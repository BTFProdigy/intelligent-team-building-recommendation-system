Abstract
To support context-based multimodal interpre-
tation in conversational systems, we have devel-
oped a semantics-based representation to
capture salient information from user inputs and
the overall conversation. In particular, we
present three unique characteristics: fine-
grained semantic models, flexible composition
of feature structures, and consistent representa-
tion at multiple levels. This representation
allows our system to use rich contexts to resolve
ambiguities, infer unspecified information, and
improve multimodal alignment. As a result, our
system is able to enhance understanding of mul-
timodal inputs including those abbreviated,
imprecise, or complex ones.
1 Introduction
Inspired by earlier works on multimodal interfaces
(e.g., Bolt, 1980; Cohen el al., 1996; Wahlster, 1991;
Zancanaro et al, 1997), we are currently building an
intelligent infrastructure, called Responsive Informa-
tion Architect (RIA) to aid users in their informa-
tion-seeking process. Specifically, RIA engages
users in a full-fledged multimodal conversation,
where users can interact with RIA through multiple
modalities (speech, text, and gesture), and RIA can
act/react through automated multimedia generation
(speech and graphics) (Zhou and Pan 2001). Cur-
rently, RIA is embodied in a testbed, called Real
HunterTM, a real-estate application to help users find
residential properties.
As a part of this effort, we are building a seman-
tics-based multimodal interpretation framework
MIND (Multimodal Interpretation for Natural Dia-
log) to identify meanings of user multimodal inputs.
Traditional multimodal interpretation has been
focused on integrating multimodal inputs together
with limited consideration on the interaction context.
In a conversation setting, user inputs could be abbre-
viated or imprecise. Only by combining multiple
inputs together often cannot reach a full understand-
ing. Therefore, MIND applies rich contexts (e.g.,
conversation context and domain context) to
enhance multimodal interpretation. In support of this
context-based approach, we have designed a seman-
tics-based representation to capture salient informa-
tion from user inputs and the overall conversation.
In this paper, we will first give a brief overview on
multimodal interpretation in MIND. Then we will
present our semantics-based representation and dis-
cuss its characteristics. Finally, we will describe the
use of this representation in context-based multimo-
dal interpretation and demonstrate that, with this rep-
resentation, MIND is able to process a variety of user
inputs including those ambiguous, abbreviated and
complex ones.
2 Multimodal Interpretation
To interpret user multimodal inputs, MIND takes
three major processes as in Figure 1: unimodal
understanding, multimodal understanding, and dis-
course understanding. During unimodal understand-
ing, MIND applies modality specific recognition and
understanding components (e.g., a speech recognizer
and a language interpreter) to identify meanings
from each unimodal input, and captures those mean-
ings in a representation called modality unit. During
multimodal understanding, MIND combines seman-
tic meanings of unimodal inputs (i.e., modality
units), and uses contexts (e.g., conversation context
and domain context) to form an overall understand-
ing of user multimodal inputs. Such an overall
understanding is then captured in a representation
called conversation unit. Furthermore, MIND also
identifies how an input relates to the overall conver-
sation discourse through discourse understanding. In
particular, MIND uses a representation called con-
versation segment to group together inputs that con-
tribute to a same goal or sub-goal (Grosz and Sidner,
1986). The result of discourse understanding is an
evolving conversation history that reflects the over-
all progress of a conversation.
Figure 2 shows a conversation fragment between a
user and MIND. In the first user input U1, the deictic
Figure 1. MIND components
gesture
speech
text
Multimodal
Interpreter
Discourse
Interpreter
Language
Interpreter
Gesture
Interpreter
Speech
Recognizer
Gesture
Recognizer
Modality Unit
(Speech
& Text)
Modality Unit
(Gesture)
Conversation Unit
Unimodal
Understanding
Discourse
Understanding
Multimodal
Understanding
Other RIA Components
C
o
n
versatio
n
H
isto
ry
Conversation
Segment
MIND
D
o
m
ain,VisualC
o
ntexts
Semantics-based Representation for Multimodal Interpretation in
Conversational Systems
Joyce Chai
IBM T. J. Watson Research Center
19 Skyline Drive
Hawthorne, NY 10532, USA
{jchai@us.ibm.com}
gesture (shown in Figure 3) is ambiguous. It is not
clear which object the user is pointing at: two houses
nearby or the town of Irvington1. The third user input
U3 by itself is incomplete since the purpose of the
input is not specified. Furthermore, in U4, a single
deictic gesture overlaps (in terms of time) with both
?this style? and ?here? from the speech input, it is hard
to determine which one of those two references
should be aligned and fused with the gesture. Finally,
U5 is also complex since multiple objects (?these two
houses?) specified in the speech input need to be uni-
fied with a single deictic gesture.
This example shows that user multimodal inputs
exhibit a wide range of varieties. They could be
abbreviated, ambiguous or complex. Fusing inputs
together often cannot reach a full understanding. To
process these inputs, contexts are important.
3 Semantics-based Representation
To support context-based multimodal interpretation,
both representation of user inputs and representation
of contexts are crucial. Currently, MIND uses three
types of contexts: domain context, conversation con-
text, and visual context. The domain context provides
domain knowledge. The conversation context reflects
the progress of the overall conversation. The visual
context gives the detailed semantic and syntactic
structures of visual objects and their relations. In this
paper, we focus on representing user inputs and the
conversation context. In particular, we discuss two
aspects of representation: semantic models that cap-
ture salient information and structures that represent
those semantic models.
3.1 Semantic Models
When two people participate in a conversation, their
understanding of each other?s purposes forms strong
constraints on how the conversation is going to pro-
ceed. Especially, in a conversation centered around
information seeking, understanding each other?s
information needs is crucial. Information needs can
be characterized by two main aspects: motivation for
seeking the information of interest and the informa-
tion sought itself. Thus, MIND uses an intention
model to capture the first aspect and an attention
model to capture the second. Furthermore, since users
can use different ways to specify their information of
interest, MIND also uses a constraint model to cap-
ture different types of constraints that are important
for information seeking.
3.1.1 Intention and Attention
Intention describes the purpose of a message. In an
information seeking environment, intention indicates
the motivation or task related to the information of
interest. An intention is modeled by three dimensions:
Motivator indicating one of the three high level pur-
poses: DataPresentation, DataAnalysis (e.g., compari-
son), and ExceptionHandling (e.g., clarification), Act
specifying whether the input is a request or a reply,
and Method indicating a specific task, e.g., Search
(activating the relevant objects based on some crite-
ria) or Lookup (evaluating/retrieving attributes of
objects).
Attention relates to objects, relations that are
salient at each point of a conversation. In an informa-
tion seeking environment, it relates to the information
sought. An attention model is characterized by six
dimensions. Base indicates the semantic type of the
information of interest (e.g., House, School, or City
which are defined in our domain ontology). Topic
specifies the granularity of the information of interest
(e.g., Instance or Collection). Focus identifies the scope
of the topic as to whether it is about a particular fea-
ture (i.e., SpecficAspect) or about all main features
(i.e., MainAspect). Aspect provides specific features of
the topic. Constraint describes constraints to be satis-
fied (described later). Content points to the actual data.
The intention and attention models were derived
based on preliminary studies of user information
needs in seeking for residential properties. The details
are described in (Chai et al, 2002).
For example, Figure 4(a-b) shows the Intention and
Attention identified from U1 speech and gesture input
respectively. Intention in Figure 4(a) indicates the user
is requesting RIA (Act: Request) to present her some
data (Motivator: DataPresentation) about attributes of
1 The generated display has multiple layers, where the house icons
are on top of the Irvington town map. Thus this deictic gesture could
either refer to the town of Irvington or houses.
Figure 2. A conversation fragment
Speech: Here is the comparison chart.
Graphics: Show a chart
R5:
Speech: Compare these two houses with the previous house.
Graphics: Point to the corner of the screen where two house icons are
displayed
U5:
Speech: This is a Victorian style house. I find seven Victorian houses
in White Plains.
Graphics: Show seven houses in White Plains
R4:
Speech: Show me houses with this style around here
Gesture: Point to a position east of Irvington on the map
U4:
Speech: This house costs 320,000 dollars.
Graphics: Highlight the house icon and show a picture
R3:
Speech: What about this one?
Gesture: Point to a house icon on the screen
U3:
Speech: The green house costs 250,000 dollars.R2:
Speech: The green one.U2:
Speech: Which house are you interested in?
Graphics: Highlight two house icons
R1:
Speech: How much is this?
Gesture: Point to the screen (not directly on any object)U1:
A collection of houses are shown on the map of Irvington
Figure 3. An example of graphics output
user points
here
certain object(s) (Method: Lookup). The Attention indi-
cates that the information of interest is about the price
(Aspect: Price) of a certain object (Focus: Instance). The
exact object is not known but is referred by a demon-
strative ?this? (in Constraint). Intention in Figure 4(b)
does not have any information since the high level
purpose and the specific task cannot be identified
from the gesture input. Furthermore, because of the
ambiguity of the deictic gesture, three Attentions are
identified. The first two Attentions are about house
instances MLS0234765 and MLS0876542 (ID from Mul-
tiple Listing Service) and the third is about the town
of Irvington.
3.1.2 Constraints
In an information seeking environment, based on the
conversation context and the graphic display, users
can refer to objects using different types of refer-
ences, for example, through temporal or spatial rela-
tions, visual cues, or simply a deictic gesture.
Furthermore, users can also search for objects using
different constraints on data properties. Therefore,
MIND models two major types of constraints: refer-
ence constraints and data constraints. Reference con-
straints characterize different types of references.
Data constraints specify relations of data properties.
A summary of our constraint model is shown in
Figure 5. Both reference constraints and data con-
straints are characterized by six dimensions. Category
sub-categorizes constraints (described later). Manner
indicates the specific way such a constraint is
expressed. Aspect indicates a feature (features) this
constraint is concerned about. Relation specifies the
relation to be satisfied between the object of interest
and other objects or values. Anchor provides a particu-
lar value, object or a reference point this constraint
relates to. Number specifies cardinal numbers that are
associated with the constraint.
Reference Constraints
Reference constraints are further categorized into
four categories: Anaphora, Temporal, Visual, and Spatial.
An anaphora reference can be expressed through pro-
nouns such as ?it? or ?them? (Pronoun), demonstra-
tives such as ?this? or ?these? (Demonstrative), here or
there (Here/There), or proper names such as ?Lyn-
hurst? (ProperNoun). An example is shown in
Figure 4(a), where a demonstrative ?this? (Manner:
Demonstrative-This) is used in the utterance ?this house?
to refer to a single house object (Number: 1). Note that
Manner also keeps track of the specific type of the
term. The subtle difference between terms can pro-
vide additional cues for resolving references. For
example, the different use of ?this? and ?that? may
indicate the recency of the referent in the user mental
model of the discourse, or the closeness of the refer-
ent to the user?s visual focus.
Temporal references use temporal relations to refer
to entities that occurred in the prior conversation.
Manner is characterized by Relative and Absolute. Rela-
tive indicates a temporal relation with respect to a cer-
tain point in a conversation, and Absolute specifies a
temporal relation regarding to the whole interaction.
Relation indicates the temporal relations (e.g., Precede
or Succeed) or ordinal relations (e.g., first). Anchor
indicates a reference point. For example, as in
Figure 6(a), a Relative temporal constraint is used
since ?the previous house? refers to the house that pre-
cedes the current focus (Anchor: Current) in the conver-
sation history. On the other hand, in the input: ?the
first house you showed me,? an Absolute temporal con-
straint is used since the user is interested in the first
house shown to her at the beginning of the entire con-
versation.
Spatial references describe entities on the graphic
display in terms of their spatial relations. Manner is
again characterized by Absolute and Relative. Absolute
indicates that entities are specified through orienta-
tions (e.g., left or right, captured by Relation) with
respect to the whole display screen (Anchor: Display-
Frame). In contrast, Relative specifies that entities are
described through orientations with respect to a par-
ticular sub-frame (Anchor: FocusFrame, e.g., an area
Figure 4. Intention and Attention for U1 unimodal inputs
Motivator: DataPresentation
Act: Request
Method: Lookup
(b) U1 gesture: pointing
Base: House
Topic: Instance
Content: {MLS0234765}
Intention
Attention
Intention
Topic: Instance
Focus: SpecificAspect
Aspect: Price
Constraint:
Attention
Category: Anaphora
Manner: Demonstrative(THIS)
Number: 1
Base: City
Topic: Instance
Content: {?Irvington?}
(a) U1 speech: ?How much is this?
Base: House
Topic: Instance
Content: {MLS0876542}
Figure 5. Constraint model
MannerCategory Aspect Relation Anchor Number
Anaphora Demonstrative,
Pronoun,
Here/There,
ProperNoun,
Temporal Relative,
Absolute
Spatial
Procede,
Succeed,
Ordinal
(e.g., first)
Visual
Attributive
Current,
Object
DisplayFrame,
FocusFrame,
Object
Orientation
(e.g., Left,
Right )
Multiple,
Cardinal-
number
(e.g., 1, 2)Relative,
Absolute
Comparative
Comparative,
Superlative,
Fuzzy
Visual-
Properties
(e.g., Color,
Highlight)
Data Features
(e.g., Price,
Size)
Less-Than,
Equals,
Greater-Than
Equals
DataValue,
ValueOfObject,
Object
DataValue,
ValueOfObject,
Object
R
efe
re
n
ce
C
o
n
straints
D
ata
Co
n
straints
-
-
-
--
Figure 6. Temporal and visual reference constraints
Base: House
Topic: Instance
Constraint:
Attention
Cateogry: Temporal
Manner: Relative
Relation: Precede
Anchor: Current
Number: 1
(a) ? the previous house? (b) ? the green house?
Base: House
Topic: Instance
Constraint:
Attention
Cateogry: Visual
Manner: Comparative
Aspect: Color
Relation: Equals
Anchor: ?Green?
Number: 1
with highlighted objects) or another object.
Visual references describe entities on the graphic
output using visual properties (such as displaying col-
ors or shapes) or visual techniques (such as high-
light). Manner of Comparative indicates a visual entity
is compared with another value (captured by Anchor).
Aspect indicates the visual entity used (such as Color
and Shape, which are defined in our domain ontol-
ogy). Relation specifies the relation to be satisfied
between the visual entity and some value. For exam-
ple, constraint used in the input ?the green house? is
shown in Figure 6(b). It is worth mentioning that dur-
ing reference resolution, the color Green will be fur-
ther mapped to the internal color encoding used by
graphics generation.
Data Constraints
Data constraints describe objects in terms of their
actual data attributes (Category: Attributive). The Man-
ner of Comparative indicates the constraint is about a
comparative relation between (aspects of) the desired
entities with other entities or values. Superlative indi-
cates the constraint is about minimum or maximum
requirement(s) for particular attribute(s). Fuzzy indi-
cates a fuzzy description on the attributes (e.g.,
?cheap house?). For example, for the input ?houses
under 300,000 dollars? in Figure 7(a), Manner is Compar-
ative since the constraint is about a ?less than? rela-
tionship (Relation: Less-Than) between the price
(Aspect: Price) of the desired object(s) and a particular
value (Anchor: ?300000 dollars?). For the input ?3 largest
houses? in Figure 7(b), Manner is Superlative since it is
about the maximum (Relation: Max) requirement on
the size of the houses (Aspect: Size).
The refined characterization of different constraints
provides rich cues for MIND to identify objects of
interest. In an information seeking environment, the
objects sought can come from different sources. They
could be entities that have been described earlier in
the conversation, entities that are visible on the dis-
play, or entities that have never been mentioned or
seen but exist in a database. Thus, fine-grained con-
straints allow MIND to determine where and how to
find the information of interest. For example, tempo-
ral constraints help MIND navigate the conversation
history by providing guidance on where to start,
which direction to follow in the conversation history,
and how many to look for.
Our fine-grained semantic models of intention,
attention and constraints characterize user informa-
tion needs and therefore enable the system to come
up with an intelligent response. Furthermore, these
models are domain independent and can be applied to
any information seeking applications (for structured
information).
3.1.3 Representing User Inputs
Given the semantic models of intention, attention and
constraints, MIND represents those models using a
combination of feature structures (Carpenter, 1992).
This representation is inspired by the earlier works
(Johnston et al, 1997; Johnston, 1998) and offers a
flexibility to accommodate complex inputs. Specifi-
cally, MIND represents intention, attention and con-
straints identified from user inputs as a result of both
unimodal understanding and multimodal understand-
ing.
During unimodal understanding, MIND applies a
decision tree based semantic parser on natural lan-
guage inputs (Jelinek et al, 1994) to identify salient
information. For the gesture input, MIND applies a
simple geometry-based recognizer. As a result, infor-
mation from each unimodal input is represented in a
modality unit. We have seen several modality units
(in Figure 4, Figure 6, and Figure 7), where intention,
attention and constraints are represented in feature
structures. Note that only features that can be instan-
tiated by information from the user input are included
in the feature structure. For example, since the exact
object cannot be identified from U1 speech input, the
Content feature is not included in its Attention structure
(Figure 4a). In addition to intention, attention and
constraints, a modality unit also keeps a time stamp
that indicates when a particular input takes place.
This time information is used for multimodal align-
ment which we do not discuss here.
Depending on the complexity of user inputs, the
representation can be composed by a flexible combi-
Figure 7. Attributive data constraints
Base: House
Topic: Collection
Constraint:
Attention
Cateogry: Attributive
Manner: Comparative
Aspect: Price
Relation: Equals
Anchor: ?300000 dollars?
(a) ?houses under 300,000 dollars?
Base: House
Topic: Collection
Constraint:
Attention
Cateogry: Attributive
Manner: Superlative
Aspect: Size
Relation: Max
Number: 3
(a) ?3 largest houses?
Figure 8. Attention structures for U4
Base: House
Topic: Collection
Constraint:
Attention (A1)
Category: Attributeive
Manner: Comparative
Aspect: Style
Relation: Equals
Anchor: *
Topic: Instance
Constraint:
Category: Anaphora
Manner: Demonstrative
(THIS)
Number: 1
Attention (A2)
Category: Attributive
Manner: Comparative
Aspect: Location
Relation: Equals
Anchor: *
Base: GeoLocation
Topic: Instance
Constraint:
Category: Anaphora
Manner: HERE
Attention (A3)Constraint:
(a) Attention structure in the modality unit for U4 speech input
Base: House
Topic: Collection
Constraint:
Attention (A1)
Category: Attributeive
Manner: Comparative
Aspect: Style
Relation: Equals
Anchor: ?Victorian?
Category: Attributive
Manner: Comparative
Aspect: Location
Relation: Equals
Anchor: ?White Plains?
Constraint:
(b) Attention structure in the conversation unit for U4 speech input
nation of different feature structures. Specifically, an
attention structure may have a constraint structure as
its feature, and on the other hand, a constraint struc-
ture may also include another attention structure.
For example, U4 in Figure 2 is a complex input,
where the speech input ?what about houses with this
style around here? consists of multiple objects with dif-
ferent relations. The modality unit created for U4
speech input is shown in Figure 8(a). The Attention
feature structure (A1) contains two attributive con-
straints indicating that the objects of interest are a
collection of houses that satisfy two attributive con-
straints. The first constraint is about the style (Aspect:
Style), and the second is about the location. Both of
these constraints are related to other objects (Manner:
Comparative), which are represented by Attention struc-
tures A2 and A3 through Anchor respectively. A2 indi-
cates an unknown object that is referred by a
Demonstrative reference constraint (this style), and A3
indicates a geographic location object referred by
HERE. Since these two references are overlapped with
a single deictic gesture, it is hard to decide which one
should be unified with the gesture input. We will
show in Section 4.3 that the fine-grained representa-
tion in Figure 8(a) allows MIND to use contexts to
resolve these two references and improve alignment.
During multimodal understanding, MIND com-
bines information from modality units together and
generates a conversation unit that represents the over-
all meaning of user multimodal inputs. A conversa-
tion unit also has the same type of intention and
attention feature structures, as well as the feature
structure for data constraints. Since references are
resolved during the multimodal understanding pro-
cess, the reference constraints are no longer present in
conversation units. For example, once two references
in Figure 8(a) are resolved during multimodal under-
standing (details are described in Section 4.3), and
MIND identifies ?this style? is ?Victorian? and ?here? is
?White Plains?, it creates a conversation unit represent-
ing the overall meanings of this input in Figure 8(b).
3.2 Representing Conversation Context
MIND uses a conversation history to represent the
conversation context based on the goals or sub-goals
of user inputs and RIA outputs. For example, in the
conversation fragment mentioned earlier (Figure 2),
the first user input (U1) initiates a goal of looking up
the price of a particular house. Due to the ambiguous
gesture input, in the next turn, RIA (R2) initiates a
sub-goal of disambiguating the house of interest. This
sub-goal contributes to the goal initiated by U1. Once
the user replies with the house of interest (U2), the
sub-goal is fulfilled. Then RIA gives the price infor-
mation (R2), and the goal initiated by U1 is accompol-
ished. To reflect this progress, our conversation
history is a hierarchical structure which consists of
conversation segments and conversation units (in
Figure 9). As mentioned earlier, a conversation unit
records user (rectangle U1, U2) or RIA (rectangle R1,
R2) overall meanings at a single turn in the conversa-
tion. These units can be grouped together to form a
conversation segment (oval DS1, DS2) based on their
goals and sub-goals. Furthermore, a conversation seg-
ment contains not only intention and attention, but
also other information such as the conversation initi-
ating participant (Initiator). In addition to conversation
segments and conversation units, a conversation his-
tory also maintains different relations between seg-
ments and between units. Details can be found in
(Chai et al, 2002).
Another main characteristic of our representation is
the consistent representation of intention and atten-
tion across different levels. Just like modality units
and conversation units, conversation segments also
consist of the same type of intention and attention
feature structures (as shown in Figure 9). This consis-
tent representation not only supports unification
based multimodal fusion, but also enables context-
based inference to enhance interpretation (described
later).
We have described our semantics-based representa-
tion and presented three characteristics: fine-grained
semantic models, flexible composition, and consis-
tent representation. Next we will show that how this
representation is used effectively in the multimodal
interpretation process.
4 The Use of Representation in Multimodal
Interpretation
As mentioned earlier, multimodal interpretation in
MIND consists of three processes: unimodal under-
standing, multimodal understanding and discourse
understanding. Here we focus on multimodal under-
standing. The key difference between MIND and ear-
lier works is the use of rich contexts to improve
understanding. Specifically, multimodal understand-
ing consists of two sub-processes: multimodal fusion
and context-based inference. Multimodal fusion fuses
intention and attention structures (from modality
units) for unimodal inputs and forms a combined rep-
resentation. Context-based inference uses rich con-
Figure 9. A fragment of a conversation history
Motivator: DataPresentation
Act: Request
Method: Lookup
Intention
Base: House
Topic: Instance
Focus: SpecificAspect
Aspect: Price
Content: {MLS0234765|
MLS0876542}
Attention
U1
Motivator: DataPresentation
Method: LookupIntention
Base: House
Topic: Instance
Focus: SpecificAspect
Aspect: Price
Content: {MLS0234765}
Attention
DS1
R2
Initiator: User
Motivator: ExceptionHandling
Method: Disambiguate
Intention
Base: House
Topic: Instance
Content: {MLS0234765 |
MLS0876542}
Attention
DS1
Initiator: RIA
R1 U2
Intention
?.
Attention
?.
texts to improve interpretation by resolving
ambiguities, deriving unspecified information, and
improving alignment.
4.1 Resolving Ambiguities
User inputs could be ambiguous. For example, in U1,
the deictic gesture is not directly on a particular
object. Fusing intention and attention structures from
each individual inputs presents some ambiguities. For
example, in Figure 4(b), there are three Attention
structures for U1 gesture input. Each of them can be
unified with the Attention structure from U1 speech
input (in Figure 4a). The result of fusion is shown in
Figure 10(a). Since the reference constraint in the
speech input (Number: 1 in Figure 4a) indicates that
only one attention structure is allowed, MIND uses
contexts to eliminate inconsistent structures. In this
case, A3 in Figure 10(a) indicates the information of
interest is about the price of the city Irvington. Based
on the domain knowledge that the city object cannot
have the price feature, A3 is filtered out. As a result,
both A1 and A2 are potential interpretation. Therefore,
the Content in those structures are combined using a
disjunctive relation as in Figure 10(b). Based on this
revised conversation unit, RIA is able to arrange the
follow-up question to further disambiguate the house
of interest (R2 in Figure 2). This example shows that,
modeling semantic information by fine-grained
dimensions supports the use of domain knowledge in
context-based inference, and can therefore resolve
some ambiguities.
4.2 Deriving Unspecified Information
In a conversation setting, user inputs are often abbre-
viated. Users tend to only provide new information
when it is their turn to interact. Sometimes, fusing
individual modalities together still cannot provide
overall meanings of those inputs. For example, after
multimodal fusion, the conversation unit for U3
(?What about this one?) does not give enough informa-
tion on what the user exactly wants. The motivation
and task of this input is not known as in Figure 11(a).
Only based on the conversation context, is MIND
able to identify the overall meaning of this input. In
this case, based on the most recent conversation seg-
ment (DS1) in Figure 9 (also as in Figure 11b), MIND
is able to derive Motivator and Method features from
DS1 to update the conversation unit for U3
(Figure 11c). As a result, this revised conversation
unit provides the overall meaning that the user is
interested in finding out the price information about
another house MLS7689432. Note that it is important
to maintain a hierarchical conversation history based
on goals and subgoals. Without such a hierarchical
structure, MIND would not be able to infer the moti-
vation of U3. Furthermore, because of the consistent
representation of intention and attention at both the
discourse level (in conversation segments) and the
input level (in conversation units), MIND is able to
directly use conversation context to infer unspecified
information and enhance interpretation.
4.3 Improving Alignment
In a multimodal environment, users could use differ-
ent ways to coordinate their speech and gesture
inputs. In some cases, one reference/object men-
tioned in the speech input coordinates with one deic-
tic gesture (U1, U3). In other cases, several references/
objects in the speech input are coordinated with one
deictic gesture (U4, U5). In the latter cases, only using
time stamps often cannot accurately align and fuse
the respective attention structures from each modal-
ity. Therefore, MIND uses contexts to improve align-
ment based on our semantics-based representation.
For example, from the speech input in U4 (?show me
houses with this style around here?), three Attention struc-
tures are generated as shown in Figure 8(a). From the
gesture input, only one Attention structure is generated
which corresponds to the city of White Plains. Since
the gesture input overlaps with both ?this style? (corre-
sponding to A2) and ?here? (corresponding to A3),
there is no obvious temporal relation indicating
which of these two references should be unified with
the deictic gesture. In fact, both A2 and A3 are poten-
tial candidates. Based on the domain context that a
city cannot have a feature Style, MIND determines
that the deictic gesture is actually resolving the refer-
Figure 10. Resolving ambiguity for U1
Motivator: DataPresentation
Act: Request
Method: Lookup
Intention
Base: House
Topic: Instance
Focus: SpecificAspect
Aspect: Price
Content:{MLS0234765}
Attention
Base: House
Topic: Instance
Focus: SpecificAspect
Aspect: Price
Content:{MLS0876542}
Base: City
Topic: Instance
Focus: SpecificAspect
Aspect: Price
Content:{?Irvington?}
Motivator: DataPresentation
Act: Request
Method: Lookup
Intention
Base: House
Topic: Instance
Focus: SpecificAspect
Aspect: Price
Content:{MLS0234765 |
MLS0876542}
Attention
A1
A2
A3
(a) Conversation unit for U1 as a
result of multimodal fusion
(b) Revised conversation unit for U1 as a
result of context-based inference
Figure 11. Deriving unspecified information for U3
Act: Request
Intention
Base: House
Topic: Instance
Content: {MLS7689432}
Attention
U3
Motivator: DataPresentation
Method: LookupIntention
Base: House
Topic: Instance
Focus: SpecificAspect
Aspect: Price
Content: {MLS0234765}
Attention
DS1
Initiator: User
(a) Conversation unit for U3 as a
result of multimodal fusion
(b) Conversation segment DS1 in
the conversation history
Motivator: DataPresentation
Act: Request
Method: Lookup
Intention
Base: House
Topic: Instance
Focus: SpecificAspect
Aspect: Price
Content: {MLS7689432}
Attention
U1
(c) Revised conversation unit for U3 as
a result of context-based inference
ence of ?here?. To resolve the reference of ?this style?,
MIND uses the visual context which indicates a
house is highlighted on the screen. A recent study
(Kehler, 2000) shows that objects in the visual focus
are often referred by pronouns, rather than by full
noun phrases or deictic gestures. Based on this study,
MIND is able to infer that most likely ?this style?
refers to the style of the highlighted house
(MLS7689432). Suppose the style is ?Victorian?, then
MIND is able to figure out that the overall meaning
of U4 is looking for houses with a Victorian style and
located in White Plains (as shown in Figure 8b).
Furthermore, for U5 (?Comparing these two houses
with the previous house?), there are two Attention struc-
tures (A1 and A2) created for the speech input as in
Figure 12(a). A1 corresponds to ?these two houses?,
where the Number feature in the reference constraint is
set 2. Although there is only one deictic gesture
which points to two potential houses (Figure 12b),
MIND is able to figure out that this deictic gesture is
actually referring to a group of two houses rather than
an ambiguous single house. Although the gesture
input in U5 is the same kind as that in U1, because of
the fine-grained information captured from the
speech input (i.e., Number feature), MIND processes
them differently. For the second reference of ?previous
house? (A2 in Figure 12a), based on the information
captured in the temporal constraint, MIND searches
the conversation history and finds the most recent
house explored (MLS7689432). Therefore, MIND is
able to reach an overall understanding of U5 that the
user is interested in comparing three houses (as in
Figure 12c).
5 Conclusion
To facilitate multimodal interpretation in conversa-
tional systems, we have developed a semantics-based
representation to capture salient information from
user inputs and the overall conversation. In this paper,
we have presented three unique characteristics of our
representation. First, our representation is based on
fine grained semantic models of intention, attention
and constraints that are important in information
seeking conversation. Second, our representation is
composed by a flexible combination of feature struc-
tures and thus supports complex user inputs. Third,
our representation of intention and attention is consis-
tent at different levels and therefore facilitates con-
text-based interpretation. This semantics-based
representation allows MIND to use contexts to
resolve ambiguities, derive unspecified information
and improve alignment. As a result, MIND is able to
process a large variety of user inputs including those
incomplete, ambiguous or complex ones.
6 Acknowledgement
The author would like to thank Shimei Pan and
Michelle Zhou for their contributions on semantic
models.
References
Bolt, R. (1980) Voice and gesture at the graphics inter-
face. Computer Graphics, pages 262-270.
Carpenter, R. (1992) The logic of typed feature struc-
tures. Cambridge University Press.
Chai, J.; Pan, S.; and Zhou, M. X. (2002) MIND: A Se-
mantics-based multimodal interpretation framework
for conversational systems. To appear in Proceedings
of International CLASS Workshop on Natural, Intelli-
gent and Effective Interaction in Multimodal Dialog
Systems.
Cohen, P.; Johnston, M.; McGee, D.; S. Oviatt, S.; Pitt-
man, J.; Smith, I.; Chen, L; and Clow, J. (1996) Quick-
set: Multimodal interaction for distributed
applications. Proc. ACM MM'96, pages 31-40.
Grosz, B. J. and Sidner, C. (1986) Attention, intentions,
and the structure of discourse. Computational Linguis-
tics, 12(3):175-204.
Jelinek, F.; Lafferty, J.; Magerman, D. M.; Mercer, R.
and Roukos, S. (1994) Decision tree parsing using a
hidden derivation model. Proc. Darpa Speech and Nat-
ural Language Workshop.
Johnston, M.; Cohen, P. R.; McGee, D.; Oviatt, S. L.;
Pittman, J. A.; and Smith, I. (1997) Unification based
multimodal integration. Proc. 35th ACL, pages 281-
288.
Johnston, M. (1998) Unification-based multimodal pars-
ing. Proc. COLING-ACL'98.
Kehler, A. (2000) Cognitive status and form of reference
in multimodal human-computer interaction. Proc.
AAAI?01, pages 685?689.
Wahlster, W. (1998) User and discourse models for mul-
timodal communication. In M. Maybury and W. Wahl-
ster, editors, Intelligent User Interfaces, pages 359-
370.
Zancanaro, M.; Stock, O.; and Strapparava, C. (1997)
Multimodal interaction for information access: Ex-
ploiting cohesion. Computational Intelligence,
13(4):439-464.
Zhou, M. X. and Pan, S. (2001) Automated authoring of
coherent multimedia discourse for conversation sys-
tems. Proc. ACM MM?01, pages 555?559.
Figure 12. Improving alignment for U5
Motivator: DataAnalysis
Act: Request
Method: Compare
Intention
Base: House
Topic: Collection
Focus: MainAspect
Constraint:
Attention
Base: House
Topic: Instance
Content:{MLS0765489}
Attention
A1
(a) Modality unit for U5 speech input
(b) Modality unit for U5 gesture input
Category: Anaphora
Manner: Demonstrative
Number: 2
Base: House
Topic: Instance
Focus: MainAspect
Constraint:
A2
Category: Temporal
Manner: Relative
Relation: Precede
Anchor: Current
Number: 1
Base: House
Topic: Instance
Content:{MLS0468709}
Motivator: DataAnalysis
Act: Request
Method: Compare
Intention
Base: House
Topic: Collection
Focus: MainAspect
Content: {MLS0468709,
MLS0765489,
MLS7689432}
Attention
A1
(c) Conversation unit for U5
A Conversational Interface for Online Shopping
Joyce Chai, Veronika Horvath, Nanda Kambhatla, Nicolas Nicolov & Margo Stys-Budzikowska
Conversational Dialog Systems
IBM T. J. Watson Research Center
30 Saw Mill River Rd, Hawthorne, NY 10532, USA
{jchai, veronika, nanda, nicolas, sm1}@us.ibm.com
ABSTRACT
We present a deployed, conversational dialog system that assists
users in finding computers based on their usage patterns and
constraints on specifications. We discuss findings from a market
survey and two user studies. We compared our system to a directed
dialog system and a menu driven navigation system. We found that
the conversational interface reduced the average number of clicks by
63% and the average interaction time by 33% over a menu driven
search system. The focus of our continuing work includes
developing a dynamic, adaptive dialog management strategy,
robustly handling user input and improving the user interface.
1. INTRODUCTION
Conversational interfaces allow users to interact with automated
systems using speech or typed in text via "conversational dialog".
For the purposes of this paper, a conversational dialog consists of a
sequence of interactions between a user and a system. The user
input is interpreted in the context of previous user inputs in the
current session and from previous sessions.
Conversational interfaces offer greater flexibility to users than
menu-driven (i.e., directed-dialog) interfaces, where users navigate
menus that have a rigid structure [5,4]. Conversational interfaces
permit users to ask queries directly in their own words. Thus, users
do not have to understand the terminology used by system designers
to label hyperlinks on a website or internalize the hierarchical
menus of a telephone system [3] or websites.
Recently, conversational interfaces for executing simple transactions
and for finding information are proliferating [7,6]. In this paper, we
present a conversational dialog system, Natural Language Assistant
(or NLA), that helps users shop for notebook computers and discuss
the results of user studies that we conducted with this system.
2. NATURAL LANGUAGE ASSISTANT
NLA assists users in finding notebooks that satisfy their needs by
engaging them in a dialog. At each turn of the dialog, NLA provides
incremental feedback about its understanding of the user's
constraints and shows products that match these constraints. By
encouraging iterative refinement of the user's query, the system
finds more user constraints and, ultimately, recommends a product
that best matches the user's criteria.
The system consists of three major modules (cf. Figure 1):
Presentation Manager, Dialog Manager, and Action Manager. The
Presentation Manager interprets user input and generates system
responses. It embodies the user interface and contains a shallow
semantic parser and a response generator. The semantic parser
identifies concepts (e.g., MULTIMEDIA) and constraints on
product attributes (e.g., hard disk size more than 20GB) from the
textual user input. The concepts mediate the mapping between user
input and available products through product specifications. They
implement the business logic.
The Dialog Manager uses the current requirements and formulates
action plans for the Action Manager to perform back-end operations
(e.g., database access1). The Dialog Manager constructs a response
to the user based on the results from the Action Manager and the
discourse history and sends the system response to the Presentation
Manager that displays it to the user. The system prompts for features
relevant in the current context. In our mixed initiative dialog
system, the user can always answer the specific question put to
him/her or provide any constraints.
The system has been recently deployed on an external website.
Figure 2 shows the start of a dialog.2
1 See [1] for a survey of natural language interfaces to databases.
2 We are demonstrating the system at HLT?2001 [2].
ManagerPresentation
Manager
telephone
PDA
web
Conversational
Dialog Manager
USER
APIs
speech,
text,..
NLP
Services
History
Action
Manager
Application
Action
Templates
etc...
APIs
Discourse
History
Figure 1. Architecture of the NLA conversational system.
3. USER STUDIES
We conducted a preliminary market survey and two user studies
described in subsections 3.1 and 3.2 respectively.
3.1 Market Survey
For understanding specific user needs and user vocabulary, we
conducted a user survey. Users were given three sets of questions.
The first set, in turn, contained three questions: "What kind of
notebook computer are you looking for?", "What features are
important to you?", and "What do you plan to use this notebook
computer for?". By applying statistical n-gram models and a
shallow noun phrase grammar to the user responses, we extracted
keywords and phrases expressing user's needs and interests. In the
second set of questions, users were asked to rank 10 randomly
selected terms from 90 notebook related terms in order of
familiarity to them. The third set of questions asked for
demographical information about users such as their gender, years
of experience with notebook computers, native language, etc. We
computed correlations between vocabulary/terms and user
demographic information. Over a 30-day period, we received 705
survey responses. From these responses, we learned 195 keywords
and phrases that were included in NLA.
3.2 Usability Testing
3.2.1 Experimental Setup
We conducted two user studies to evaluate usability of the system,
focusing on: dialog flow, ease of use, system responses, and user
vocabulary. The first user study focused on the functionality of
NLA and the second user study compared the functionality of
NLA with that of a directed dialog system and a menu driven
navigation system.
The moderators interviewed 52 users in the user studies: 18 and
34 in the two studies, respectively. All participants were
consumers or small business users with "beginner" or
"intermediate" computer skills. Each participant was asked to find
laptops for a variety of scenarios using three different systems (the
NLA, a directed dialog system and a menu driven navigation
system). Participants were asked to rate each system for each task
on a 1 to 10 scale (10 ? easiest) with respect to the ease of
navigation, clarity of terminology and their confidence in the
system responses. The test subjects were also asked whether the
system had found relevant products and were prompted to share
their impressions as to how well the system understood them and
responded to their requests.
Figure 2. The start of the dialog.
3.2.2 Results
In both studies, participants were very receptive to using natural
language dialog-based search. The users clearly preferred dialog-
based searches to non-dialog based searches3 (79% to 21% users).
Furthermore, they liked the narrowing down of a product list
based on identified constraints as the interaction proceeded. In the
first user study, comparing NLA with a menu driven system, we
found that using NLA reduced the average number of clicks by
63% and the average interaction time by 33%.
In the second user study, we compared NLA with a directed
dialog system and a menu driven search system for finding
computers. One goal of the comparative study was to find out if
there were any statistical differences in confidence, terminology
and navigation ratings across the three systems and whether they
were correlated with different categories of users. The ANOVA
analysis reveals statistical differences in terminology ratings
among the three systems for the category of beginner users only.
There were no statistical differences found in the other ratings of
navigation and confidence across the three sites for different
categories of users. Sandler's A test confirmed that the
terminology rating was significantly different for the categories of
consumers, small business owners, beginners and intermediates.
These comparative results suggest that asking questions relative to
the right level of end user experience is crucial. Asking users
questions about their lifestyle and how they were going to use a
computer accounted for a slight preference of the directed dialog
system over the NLA that uses questions presented on the basis of
understanding features and functions of computer terms.
3.2.3 Lessons from the user studies
Both user studies revealed several dimensions along which NLA
can be improved. The first user study highlighted a definite need
for system acknowledgement and feedback. The users wanted to
know whether the system had understood them. User comments
also revealed that a comparison of features across the whole pool
of products was important for them.
The focus of the second study, incorporating 34 subjects, was to
compare systems of similar functionality and to draw conclusions
about the functionality of NLA. Both the ANOVA and the
Sandler's test point out that terminology was a statistically
significant factor differentiating among the systems. We believe
that using terminology that is not overly technical would
contribute to the success of the dialog search. While the questions
asked by NLA were based on features and functionality of
notebook computers, the users preferred describing usage patterns
and life style issues rather than technical details of computers.
We also found that users' confidence in NLA decreased when the
system responses were inconsistent i.e., were not relevant to their
input. Lack of consistent visual focus on the dialog box was also a
serious drawback since it forced users to scroll in search of the
dialog box on each interaction page.
3 We define a dialog-based search as one comprising of a
sequence of interactions with a system where the system keeps
track of contextual (discourse) information.
3.2.4 Future work
Based on the results of the user studies, we are currently focused
on: developing a dynamic and adaptive dialog management
strategy, improving the robustness of the natural language
processing (NLP), and improving the user interface. Some of
issues mentioned here have been implemented in the next version
of NLA.
We are currently re-designing the questions that NLA asks users
to be simpler, and to focus on usage patterns rather than technical
features. We are also implementing a new dialog management
strategy in NLA that is more adaptive to the user's input, and
implements a mapping from high-level usage patterns to
constraints on low-level technical features.
We are integrating a statistical parser with NLA to more robustly
handle varied user input. The statistical parser should enable NLA
to scale to multiple languages and multiple domains in a more
robust and reliable fashion. We are aiming at an architecture that
separates the NLP processing from the business logic that will
make maintenance of the system easier.4
Improvements to the GUI include better acknowledgement and
feedback mechanisms as well as graphical UI issues. We now
reiterate the user's last query at the beginning of each interaction
page and also convey to the user an explanation of features
incrementally accumulated in the course of the interaction. We
have designed a more uniform, more compact and consistent UI.
In the welcome page, we have abandoned a three-step initiation
(typed input, experience level and preferences for major
specifications) keeping the emphasis on the dialog box. The user
preferences contributed to creating confusion as to the main
means of interaction (many users just clicked on the radial buttons
and did not use the full dialog functionality). We now infer the
technical specifications based the user's stated needs and usage
patterns. Our UI now has a no scrolling policy and we allow for
larger matching set of products to be visualized over a number of
pages.
4. DISCUSSION
In this paper, we have presented a conversational dialog system
for helping users shop for notebook computers. User studies
comparing our conversational dialog system with a menu driven
system have found that the conversational interface reduced the
average number of clicks by 63% and the average interaction time
by 33%. Based on our findings, it appears that for conversational
systems like ours, the sophistication of dialog management and
the actual human computer interface are more important than the
complexity of the natural language processing technique used.
This is especially true for web-based systems where user queries
are often brief and shallow linguistic processing seems to be
adequate. For web-based systems, integrating the conversational
interface with other interfaces (like menu-driven and search-
driven interfaces) for providing a complete and consistent user
experience assumes greater importance.
4 Many systems' fate has been decided not because they cannot
handle complex linguistic constructions but because of the
difficulties in porting such systems out of the research
environments.
The user studies we conducted have highlighted several directions
for further improvements for our system. We plan to modify our
interface to integrate different styles of interaction (e.g., menus,
search, browsing, etc.). We also intend to dynamically classify
each user as belonging to one or more categories of computer
shoppers (e.g., gamers, student users, home business users, etc.)
based on all the user interactions so far. We can then tailor the
whole interface to the perceived category including but not
limited to the actual questions asked, the technical knowledge
assumed by the system and the whole style of interaction.
Another area of potential improvement for the NLA is its inability
to handle any meta-level queries about itself or any deeper
questions about its domain (e.g., NLA currently can not properly
handle the queries, "How can I add memory to this model?" or
"What is DVD?"). Our long-term goal is to integrate different
sources of back-end information (databases, text documents, etc.)
and present users with an integrated, consistent conversational
interface to it.
We believe that conversational interfaces offer the ultimate kind
of personalization. Personalization can be defined as the process
of presenting each user of an automated system with an interface
uniquely tailored to his/her preference of content and style of
interaction. Thus, mixed initiative conversational interfaces are
highly personalized since they allow users to interact with systems
using the words they want, to fetch the content they want in the
style they want. Users can converse with such systems by phrasing
their initial queries at a right level of comfort to them (e.g., "I am
looking for a gift for my wife" or "I am looking for a fast
computer with DVD under 1500 dollars").
5. CONCLUSIONS
Based on our results, we conclude that conversational natural
language dialog interfaces offer powerful personalized alternatives
to traditional menu-driven or search-based interfaces to websites.
For such systems, it is especially important to present users with a
consistent interface integrating different styles of interaction and
to have robust dialog management strategies. The system feedback
and the follow up questions should strike a delicate balance
between exposing the system limitations to users, and making
users aware of the flexibility of the system. In current work we are
focusing on developing dynamic, adaptive dialog management,
robust multi-lingual NLP and improving the user interface.
6. REFERENCES
[1] Androutsopoulos, Ion, and Ritchie, Graeme. Natural
Language Interfaces to Databases ? An Introduction, Natural
Language Engineering 1.1:29-81, 1995.
[2] Budzikowska, M., Chai, J., Govindappa, S., Horvath, V.,
Kambhatla, N., Nicolov, N., and Zadrozny, W.
Conversational Sales Assistant for Online Shopping,
Demonstration at Human Language Technologies
Conference (HLT'2001), San Diego, Calif., 2001.
[3] Carpenter, Bob, and Chu-Carroll, J. Natural Language Call
Routing: A Robust, Self-organizing Approach, Proceedings
of the 5th Int. Conf. on Spoken Language Processing. 1998
[4] Chai, J., Lin, J., Zadrozny, W., Ye, Y., Budzikowska, M.,
Horvath, V., Kambhatla, N., and Wolf, C. Comparative
Evaluation of a Natural Language Dialog Based System and
a Menu-Driven System for Information Access: A Case
Study, Proceedings of RIAO 2000, Paris.
[5] Saito, M., and Ohmura, K. A Cognitive Model for Searching
for Ill-defined Targets on the Web - The Relationship
between Search Strategies and User Satisfaction, 21st Int.
Conference on Research and Development in Information
Retrieval, Australia, 1998.
[6] Walker, M., Fromer, J., and Narayanan, S. Learning Optimal
Dialogue Strategies: A Case Study of a Spoken Dialogue
Agent for Email, 36th Annual Meeting of the ACL, Montreal,
Canada, 1998.
[7] Zadrozny, W., Wolf, C., Kambhatla, N. & Ye, Y.
Conversation Machines for Transaction Processing,
Proceedings of AAAI / IAAI - 1998, Madison, Wisconsin,
U.S.A. 1998.
Conversational Sales Assistant for Online Shopping
Margo Budzikowska, Joyce Chai, Sunil Govindappa, Veronika Horvath, Nanda Kambhatla,
Nicolas Nicolov & Wlodek Zadrozny
Conversational Machines Group
IBM T. J. Watson Research Center
30 Saw Mill River Rd, Hawthorne, NY 10532, U.S.A.
{sm1, jchai, govindap, veronika, nanda, nicolas, wlodz}@us.ibm.com
ABSTRACT
Websites of businesses should accommodate both customer
needs and business requirements. Traditional menu-driven
navigation and key word search do not allow users to describe
their intentions precisely. We have developed a conversational
interface to online shopping that provides convenient,
personalized access to information using natural language
dialog. User studies show significantly reduced length of
interactions in terms of time and number of clicks in finding
products. The core dialog engine is easily adaptable to other
domains.
1. INTRODUCTION
Natural language dialog has been used in many areas, such as
for call-center/routing application (Carpenter & Chu-Carroll
1998), email routing (Walker, Fromer & Narayanan 1998),
information retrieval and database access (Androutsopoulos &
Ritchie 1995), and for telephony banking (Zadrozny et al 1998).
In this demonstration, we present a natural language dialog
interface to online shopping. Our user studies show natural
language dialog to be a very effective means for negotiating
user's requests and intentions in this domain.
2. SYSTEM ARCHITECTURE
In our system, a presentation manager captures queries from
users, employs a parser to transform the user's query into a
logical form, and sends the logical form to a dialog manager.
The presentation manager is also responsible for obtaining the
system's response from the dialog manager and presenting it to
the user using template-based generation. The dialog manager
formulates action plans for an action manager to perform back-
end tasks such as database access, business transactions, etc. The
dialog manager applies information state-based dialog strategies
to formulate responses depending on the current state, discourse
history and the action results from the action manager.
The Data Management Subsystem maintains a ?concept?
repository with common sense ?concepts? and a phrasal lexicon
that lists possible ways for referring to the concepts. Business
Rules map concepts to business specifications by defining
concepts using a propositional logic formula of constraints over
product specifications. Thus, the Business Rules reflect business
goals and decisions. The Extended Database combines product
specifications and precompiled evaluations of the concept
definitions for each product to provide a representation that
guides the natural language dialog. We are investigating
automated tools for helping developers and maintainers extract
relevant concepts and terms on the basis of user descriptions and
queries about products.
3. EVALUATION
We conducted several user studies to evaluate the usability of
NLA (Chai et al 2000). In one study, seventeen test subjects
preferred the dialog-driven navigation of NLA two to one over
menu-driven navigation. Moreover, with NLA, the average
number of clicks was reduced by 63.2% and the average time
was reduced by 33.3%. Analysis of the user queries (average
length = 5.31 words long; standard deviation = 2.62; 85% of
inputs are noun phrases) revealed the brevity and relative
linguistic simplicity of user input. Hence, shallow parsing
techniques were adequate for processing user input. In general,
sophisticated dialog management appears to be more important
than the ability to handle complex natural language sentences.
The user studies also highlighted the need to combine multiple
modalities and styles of interaction.
4. REFERENCES
[1] Androutsopoulos, Ion & Ritchie, Graeme. Natural
Language Interfaces to Databases ? An Introduction,
Natural Language Engineering 1.1:29-81, 1995.
[2] Carpenter, Bob & Chu-Carroll, Jeniffer. Natural Language
Call Routing: A Robust, Self-organizing Approach,
Proceedings of the 5th International Conference on Spoken
Language Processing, 1998.
[3] Chai, J., Lin, J., Zadrozny, W., Ye, Y., Budzikowska, M.,
Horvath, V., Kambhatla, N. & Wolf, C. Comparative
Evaluation of a Natural Language Dialog Based System
and a Menu-Driven System for Information Access: A Case
Study, Proceedings of RIAO 2000, Paris, 2000.
[4] Saito, M. & Ohmura, K. A Cognitive Model for Searching
for Ill-defined Targets on the Web ? The Relationship
between Search Strategies and User Satisfaction. 21st Int.
Conf. on Research and Development in Information
Retrieval, Australia, 1998.
[5] Walker, M., Fromer, J. & Narayanan, S. Learning Optimal
Dialogue Strategies: A Case Study of a Spoken Dialogue
Agent for Email, 36th Annual Meeting of the ACL,
Montreal, Canada, 1998.
[6] Zadrozny, W., Wolf, C., Kambhatla, N. & Ye, Y.
Conversation Machines for Transaction Processing,
Proceedings of AAAI / IAAI - 1998, Madison, Wisconsin,
U.S.A., 1998.
HTML
Application
Server
Client
HTTP
Server
HTML
Servlet
Web Server
Network
(HTTP)
Presentation
Manager
Dialog
Manager
Action
Manager
Quick
Parser
Response
Generator Vector Space Engine
Product
Database
Business Rules
Concepts
Data Management
(Off line)
User Interface
Concept
Interpreter
Explanation
ModelPresentationStrategies
Dialog
Strategies
Action
Strategies
input
output
Communication
Acts
Communication
Acts
Action Specs
Online
Interaction Discourse
Analyzer
Extended
PD
Database
Query
Discourse
History
ActionResults
State
Interpreter
Dynamic User Level and Utility Measurement for Adaptive 
Dialog in a Help-Desk System 
Preetam Maloor 
Department ofComputer Science, 
Texas A & M University, 
College Station, TX 77843, USA 
preetam@csdl.tamu.edu 
Joyee Chai 
Conversational Machines 
IBM T. J. Watson Research Center, 
Hawthorne, NY 10532, USA 
jchai@us.ibm.com 
Abstract 
The learning and self-adaptive capability in 
dialog systems has become increasingly 
important with the advances in a wide range of 
applications. For any application, particularly 
the one dealing with a technical domain, the 
system should pay attention to not only the user 
experience level and dialog goals, but more 
importantly, the mechanism to adapt he system 
behavior to the evolving state of the user. This 
paper describes a methodology that first 
identifies the user experience level and utility 
metrics of the goal and sub-goals, then 
automatically adjusts those parameters based on 
discourse history and thus directs adaptive 
dialog management. 
Introduction 
A new generation of dialog systems hould be 
viewed as learning systems rather than static 
models (Jokinen, 2000). Close-world and 
static approaches have tremendous limitations 
and often fail when the task becomes complex 
and the application environment and 
knowledge changes. Thus, the learning 
capability of a dialog system has become an 
important issue. It has been addressed in many 
different aspects including dynamic 
construction of mutual knowledge (Andersen 
et al 1999), learning of speech acts (Stolcker 
et al 1998), learning optimal strategies 
(Litman et al 1998; Litman et al 1999; 
Walker et al 1998), collaborative agent in plan 
recognition (Lesh et al 1999), etc. This paper 
addresses the dynamic user modeling and 
dialog-goal utility measurement to facilitate 
adaptive dialog behavior. 
For any dialog system dealing with a technical 
domain, such as repair support (Weis, 1997), 
help-desk support, etc, it is crucial for the 
system not only to pay attention to the user 
knowledge and experience level and dialog 
goals, but more important, to have certain 
mechanisms that adapt he system behavior in 
terms of action planning, content selection, 
and content realization to user cognitive 
limitations. Dialog strategies and management 
should be adjusted to the evolving state of the 
user. Thus a better understanding and 
modeling of user cognitive process and human 
perception is desirable. 
In this paper, we propose a methodology that 
automatically learns user experience levels 
based on sub-goal utilities and characteristics 
observed during the interaction. Those user 
levels will further feedback to update utility 
metrics and direct different dialog strategies at 
each level of dialog management: action 
planning, content selection and content 
realization. The Help-Desk is our application 
domain. This is a work in progress. We have 
built a prototype system and are currently in 
the process of evaluation of our methodology 
and hypotheses. 
1 System Overview 
The system components, hown in figure 1, 
consist of a problem space representation a d 
a set of modules and agents that utilize this 
representation. The architecture supports a 
dynamic updating process for user level and 
sub-goal utility measurement, and thus allows 
the system to adapt its dialog behavior to the 
updated environment. 
94 
~(~o r~nt ,~decdoll I oa Lq4 - 
1 1 
r I 
i i 
Figure 1. System Components 
The problem space is modeled by an Acyclic 
Problem Graph structure, which represents he 
dialog goal (i.e., final goal) and different paths 
(solutions) to the final goal. The Level 
Adjusting Agent controls the initial detection 
and dynamic shifting of user expertise level 
based on the interactions with the user. The 
Action Planner identifies the problem node 
(i.e., dialog goal) in the Acyclic Problem 
Graph and locates the optimal path to it. The 
Content Selection component uses the Level 
Adjusting Agent and the Action Planner to 
select he content for the dialog. The Content 
Realization module deals with the final 
presentation of the dialog content o the user. 
The Utility Updating Agent automatically 
updates the utility metrics of the sub-goals in 
the Acyclic Problem Graph based on the single 
and group user models that are created uring 
interactions. Different strategies are applied in 
different modules, which will be described 
later. 
2 Problem Space Modeling 
The problem space is modeled by an aeyclic 
graph named Acyclic Problem Graph. It can 
also be considered as a forest containing joint 
trees that have overlapped root nodes and 
internal nodes. Internal nodes correspond to 
sub-goals. A path traversed from a root to a 
particular node contains a potential solution to 
a goal or sub-goal related to that node. Given a 
root node, the ffurffier away from the root, the 
greater is the complexity of the goal (or sub- 
goal) represented by a node. Since multiple 
paths can lead to a node, there could be 
multiple solutions to a goal. 
Figure 2 is a fragment of an acyclic graph for 
solving problems pertaining to a Windows 
based PC. In this example, three paths 
correspond to three potential solutions to the 
problem about how to set the display 
resolution of a monitor. 
A C 
Concept:. Desklop Settings 
Remedy: (set of remedies for 
generation purpose) 
Reward: +15 
Plmishment: -45 
Timeout: 
Best.case: 10 sees (reward: +10) 
Worst-ease: 30 sees (puais lma~t:  
-2O) 
Figure 2. Acyclie Problem Graph 
Each node in the graph has the following 
fields: Concept Name, Remedy, and Utility 
Metrics that include Reward, Punishment, 
Best-case timeout and Worst-case firneout. 
Concept Name represents an instruction 
corresponding to a particular goal or sub-goal 
during the problem solving. For example, the 
concept of "Display Properties" node deals 
with manipulating the display of the monitor. 
Remedy is the template that is used to generate 
natural language responses and explanations 
corresponding to a particular goal. It also 
contains phrases and key terms used for 
language generation. 
Reward and Punishment are the utility metrics 
corresponding to each sub-goal (Winlder, 
95 
1972) depending upon the \]hypothesis of
uncertainty of understanding and the level of 
importance. Uncertainty of understanding 
implies the difficulty in following certain 
instructions or understanding certain concepts. 
For example, some technical terms require 
users to possess greater expertise in order to 
comprehend them. Some potential ways of 
initializing uncertainty ofunders.tanding are by 
observation, analysis of previously logged 
data, or surveys. The level of importance 
indicates the importance of the sub-goal for 
understanding an instruction or a concept 
towards the realization of the overall goal of 
solving the problem. One good indication of 
such importance, for example, in the Acyclic 
Problem Graph, is the branch factor of each 
node. A more difficult concept has a greater 
level of uncertainty and hence would lead to 
less punishment if the user does not 
understand it. On the other hand, if a user 
correctly understands a concept that has a high 
degree of uncertainty, he would be rewarded 
highly. Reward and punishment can be pre- 
determined and then re-adjusted later when the 
user and the group modeling progresses. 
Timeout metrics are used to indicate whether 
the user understands the instruction or the 
concept associated With the sub-goal within 
the expected period of time. The hypothesis 
that when a user has no problem of 
understanding a system instruction, the user is 
very likely to respond to the system rapidly. 
However, when the user has difficulties, 
he/she tends to spend more time on thinking 
and asking for help. There are two timeouts: 
best-case and worst-case. Each timeout has a 
reward and a punishment. Best-case time is the 
time expected by the system, in the best ease, 
that a user would take to understaud the 
instruction. The user is rewarded when actual 
time spent is less than the best-case time. 
Similarly, the worst-case time is the system 
expectation for the worst ease. If the user still 
doesn't get the instruction after the worst-ease 
time period, he is punished for it. Again, these 
values are pre-set and will be dynamically ree- 
adjusted. 
3 Dialog Management 
The Dialog Manager can be broadly classified 
into two main modules: Content Selection and 
Content Realization. 
3.1 Content Selection Module 
The Content Selection Module consists of four 
components: Level-Adjusting Agent, Utility- 
Updating Agent, Action Planner and Content 
Selector. 
& L1 The Level-Adjusting A ent 
There are three levels of user expertise that he 
dialog manager takes into consideration: 
Expert, Moderate and Novice. The agent 
controls the initial detection and dynamic 
shifting of user expertise level based on 
interactions with the user. 
If a user is using the system for the first time, a 
good indication of the initial user expertise 
level is the level of detail and technical 
complexity of the initial query. As user's 
interaction with the system continues, a profile 
of the user is constructed gradually. This 
profile could be re-used to set the initial user 
expertise when the user uses the system again. 
The dynamic shifting of user expertise l vel is 
of two kinds: local (i.e., temporary) shifting 
between local expertise levels and 
accumulated (i.e., long term) shifting between 
accumulated expertise levels. Local shifting 
adjusts the expertise level temporarily - by 
observing the user confirmation (currently an 
explicit user confirmation is expected) which 
indicates whether he/she understands a certain 
instruction. The reason for temporary 
adjustment is because we assume that the user 
is having trouble understanding only this 
particular instruction and not the overall 
solution. 
The accumulated shifting permauently adjusts 
the user expertise level depending upon two 
threshold values: EXPERTLEVEL and 
NOVICELEVEL. The user is considered an 
expert when his accumulated expertise l vel is 
above the EXPERTLEVEL and is considered 
96 
novice when that is below the 
NOVICE_LEVEL. The user is assumed to 
have moderate xpertise if he lies between 
these two thresholds. An accumulated value 
(ACCUM_VALUE) is calculated based on the 
whole dialog history. If the ACCUiVLVALUE 
of a user crosses a threshold, the accumulated 
user expertise level changes long term as it is 
assumed that there is a change in the user's 
overall understanding of the solution. 
At any point of the interaction, the system 
maintains ACCUM VALUE for the user. This 
value is used to adjust he user expertise level. 
The ACCUM VALUE is calculated based on 
the following set of features associated with 
utility metrics in each node in the discourse 
history (Wies, 1997; Jameson et al 1999): 
Sub-goal Complexity: More complex sub- 
goals have a greater level of importance and 
uncertainty of understanding, and thus have a 
high reward and a low punishment. Similarly, 
comparably simple sub-goals have a low 
reward and a high punishment. 
Accomplishing Time: this is perhaps the 
trickiest parameter as the chance of making an 
incorrect assumption is much higher. The user 
response time could be a good indication of 
user understanding. The longer the resolving 
of the user's problems lasts, the more 
unfavorable the evaluation is. Also if the user 
responds quickly, he is rewarded for it. To 
detect whether the user is distracted or not, if a 
series of timeouts occur continuously, the user 
is not paying attention to the system. 
Response Complexity: There is a reward and a 
punishment associated with each system 
response that reflects the complexity of the 
content and realization of the system 
responses. First of all, the content for response 
generation varies with different expertise 
levels. For novice users, all the content on the 
solution path will be generated as several turns 
of responses based on the number of sub-goals 
in the path. For expert users, only 40% content 
on the solution path (toward the final goal) is 
used for the generation as one response. 
Furthermore, for users with different expertise 
level, the Content Realization Module will 
generate system responses (in the prototype 
system, the system responses are mainly 
instructions that guide users to solve a help- 
desk problem) with different levels of 
syntactic omplexity and technical details. For 
example, for novice users, the system tends to 
generate responses with single instruction 
corresponding to one sub-goal, while for 
expert users, the system tends to generate 
responses with single instruction 
corresponding to multiple sub-goals on the 
solution path. The response with multiple 
instructions will have higher eward and lower 
punishment than those are associated with 
single instruction. Thus the user who gives a 
positive confirmation to a more complex 
system response will be rewarded higher than 
those who understand a simple system 
response. 
Based on the above factors, the 
ACCUM VALUE can be calculated 
depending upon the conditions using the 
following formulae: 
ACCUM VALUE = ACCUlvLVALUE + 
f/response -complexity (reward, punishment), sub-goal(reward, 
punishmen0, timeout(reward, punishment)\] 
In the prototype system, we have used the 
following: 
If a goal is accomplished by the user(indicated 
by positive user confirmation), 
ACCUM_VALUE = ACCUM...VALUE + \[response- 
complexity(reward) * sub-goal(reward)\] 
If a goal is not accomplished(indicated by 
negative user confirmation), 
ACCUM_VALUE = ACCUM. VALUE \[response- 
complexity(punishment) * sub-goal(punishment)\] 
If a goal is accomplished before best-time 
timeout value, 
ACCUM_VALUE = ACCUM_VALUE + \ [ response-  
complexity(reward) * sub-goal(best-case timeout reward)\]. 
If a goal is not accomplished before worst-time 
timeout value, 
ACCUMVALUE = ACCUM.VALUE - \[response- 
complexity(punishment) * sub-goal(worst-ease timeout punis- 
lament)\]. 
Other variations of the formula re expected to 
be explored in the future. 
97 
3.L 2 Action Planner and Content Selector 
The Action Planner identifies the final 
goal node in the Acyclic Problem Graph 
and finds the optimal path to it. The 
optimal path is selected based on the path 
utility function. The utility of a path in the 
graph is the summation of the 
reward/punishment ra io of all the nodes (sub- 
goals) in that path. 
Path utility (start-node, goal) = E (r i / Pi) 
n 
where i is a concept node in the path from the 
start node to the goal node, ri is the reward and 
pl is the punishment of the corresponding ode 
i. The number of nodes n in the path acts as 
the normalizing factor. 
Thusfor a given path, higher its path utility, 
greater is the difficulty to understand the 
concepts it contains and thus higher is the 
level of expertise required. 
The following co-operative strategies are used: 
for an expert user, select he path that has the 
maximum path utility. For a novice, select he 
one with the minimum path utility since this is 
the one containing concepts easiest to 
understand and with more steps of 
instructions. For a moderate-experience us r, 
select a path in between. (We are currently 
more focused on the experienced and novice 
users.) Content Selector is applied to select he 
appropriate nodes on the path to form the 
content of dialog. 
3.L3 Utility Updating Agent 
A set of users having very similar expertise 
levels can be classified as a group. A Utility 
Updating Agent dynamically updates utility 
metrics of sub-goals in the Acyclic Problem 
Graph based on the group interactions with the 
system. For example, Group A has a reward of 
+50 and a punishment o f -10  assigned to the 
sub-goal with associated concept of Display 
Properties. However the agent notices that the 
majority of the group understand the 
corresponding instruction very quickly without 
going into the sub-goal resolution, then the 
agent decreases the reward to +35 and 
increases the punishment to -25. This 
dynamic re-training of utility metrics in sub- 
goals would reflect the evolving user 
experience level as a whole and would 
improve the robustness of the dialog manager. 
3.2 Content Realization Module 
This module deals with the final presentation 
of the dialog content o the user. The dialog 
manager adopts different response strategies 
for each of the three expertise levels. It has 
been observed that an expert user appreciates a 
response, which is precise, to the point, and 
short. For a novice user, it has been observed 
that such a user likes system instructions that 
are step-wise, higher level of detail and 
minimum technical jargon. For a moderate- 
experience user, the strategy lies somewhere in
between which we haven't given a full 
consideration. The response strategy followed 
for each type of user is given in the table 1. 
Response \[ I.~ve~ ofd~l  of system Teclmical t~'ms in system SyntacTic Conccisencss of the 
\[ inKa'uctions and e:(planation in.~ru~ows and ~planation explanation Expertise i 
Expert / Low High High 
Moderate I Moderate Moderate Moderate 
Novice High Low Low 
Table 1. User expertise level and corresponding dialog strategies 
98 
3.3 Algorithm 
The proposed algorithm for action planning, 
content selection and content realization is 
given in Figure 3. This algorithm recursively 
applies a divide and conquer mechanism to 
accomplish the final goal by resolving sub- 
goals. Two variables (i,e., local expertise l vel 
and accumulated expertise level) are 
maintained by the Level-Adjusting Agent for 
the automated level updating. The Action 
Planner identifies the goal node and the 
solution path to it depending on the expertise 
level of the user. Based on this level, the 
Content Realization Module will first select 
the content on the path to be presented and 
then use various response strategies to 
generate and display system instructions to the 
user. For novice users, all the content on 
solution path will be used; for moderate and 
expert users, only partial content on the path 
(toward the goal) will be used. In terms of 
generation, for novice and moderate xpertise 
users, the system generates responses with 
single instruction corresponding to one sub- 
goal, while for expert users, the system tends 
to generate responses with single instruction 
corresponding to multiple sub-goals on the 
solution path. The syntax of the response 
becomes more complex as the expertise level 
increases. Depending on the response of the 
user, the Level-Adjusting Agent updates the 
user expertise level and adapts the response 
strategies accordingly. 
1) Level-Adjnsting Agent detects the initial expertise level and assigns it to both local expertise level and accumulated 
expertise l vel. 
2) Action Planner identifies the start node and goal node in the Acyclic Problem Graph and locates the appropriate path 
between the start node and the goal node. 
a~ For novice user, the path with minimum path utility is selected 
b. For expert user, the path with maximum path utility is selected 
c. For moderate user, a path in between is selected 
3) Content Realization Module generates system instructions based on the selected path by using the following response 
SU'~tegles" 
a. For an expert, the instruction is generated by using the nodes that fall within a distance of X% from the goal 
node to the root node. 
b. For a moderate-experienced user, nodes within a distance of Y% (where Y > X) are used, 
e. For a novice, all nodes from the root to the goal are used to generate the instruction 
(X and Y could be experimentally determined later) 
4) Content Realization Module displays generated insmactions tothe user. 
5) Level-Adjusting Agent receives the user confirmation and updates user expertise l vel. 
a. If the confirmation ispositive, the Level Adjusting Agent does the following: 
i. Update ACCUM_VALUE=ACCUM VALUE + \[response--complexity(reward) * sub-goal(reward)\] 
i i .  If ACCUM VALUE crosses above an expertise level threshold, upgrade accumulated expertise 
level 
iii. If the goal node is the final node, exit. Otherwise, continue to the next node. 
b. If the confirmation isnegative 
i. If current local expertise l vel is greater than novice, temporarily reduce local expertise level; else 
suspend system at current state (so that the user can take his own time in understand/rig the 
instruction or seek outside help). 
i i .  Update ACCUM_VALUE= ACCUM VALUE - \[response.complexity(punishment) * sub- 
goal(punishmen0\]. 
iii. If ACCCUIvLVALUE crosses below a level threshold, reduce accumulated experience l vel. 
iv. Record the current node and the current path 
v. Make current node as the goal node; Go to step 2. Repeat until all sub-goal nodes of this goal node 
are understood. 
6) Re-initialize local expertise level to current value of accumulated expertise level. Restore path to value stored in step 
5.b.iv. Go to step 2. Reset he start node. Continue till the final goal is reached. 
(A timer that is running on a separate hread also modifies the ACCUM_VALUE variable. On occurrence ofa tirneout, he 
following steps are followed: 
If the time spent is less than the best-case time 
ACCUM._VALUE=ACCUM_.VALUE + \[response-complexity(reward) * sub-goal(best-case timeout reward)\]. 
Go to step 5.a.ii. 
If the time spent is more than the worst-case time 
ACCUM_VALUE=ACCUM_VALUE - \[response-complexity(punishment) * sub-goal(worst.ease timeout 
punishrnent)l. 
Go to step 5.b.iii. ) 
Figure 3. Algorithm 
99 
4 An example  of  interaction 
A scenario appl icat ion for the system using the 
prob lem tree o f  F igure 2 is g iven in F igure 4. 
This set o f  interactions is between the system 
and a user  who is trying to adjust the display 
settings o f  his monitor.  
1. System: Hi, how may I help you? 
2. User: I need to adjust the desktop resolution of my monitor 
> Initial accumulated expertise l vel detected - Expert 
Goal node-Desktop Setting (as it deals with adjusting the resolution) 
Shortest part is nodes 1-2-6.7 (assuming that it has the maximum path utility value). 
Nodes 6,7 (40% rounded to nearest whole number) are selected to generate r sponse. 
> Response generated with instruction corresponding to multiple sub-goals on the solution path. 
3. System: Yon can do that by adjusting the desktop area in the desktop settings section of the display 
properties window. 
4. User: Where is desktop setting window? 
Temporarily decreasing local expertise l vel to Moderate. 
ACCUM VALUE=ACCUlVLVALUE- \[response-complexity (punishment) * Desktop- 
Setting(punishment)\] 
} Shortest path to Desktop Setting recalculated as 8-5-6-7 (assuming that it has the average path 
utility value). 
Nodes 5,6,7 (70% rounded to nearest whole number) are selected to generate r sponse. 
Response generated with single instruction corresponding to one sub-goal. 
5. System: First, go to the Control Panel window. 
...worst-case timeout occurred .... 
> Temporarily decrease local expertise level to Novice. 
> ACCUIVLVALUE=AC CU/VLVALUE \[response-complexity(punishment) * control- 
panel(worst-case timeout punishment)\] 
> ACCUM_VALUE falls below EXPERTLEVEL Make accumulated experience l vel moderate. 
> Shortest path to Control Panel recalculated as nodes 3-.4-5 (assuming it has the minimum path 
utility value). 
> All nodes used to generate r sponse. 
Response generated with single instruction corresponding to one sub-goal. 
6. System: 1. Click on "Start" icon on lower left corner of your screen. 
7. User: Ok 
> ACCUM VALUE = ACCUM LEVEL + \[response-eomplexity (reward), Start(reward)\] 
8. System: 2. Click on "Settings" label. 
9. User: Ok 
> ACCUM_VALUE=ACCUM_LEVEL + \[response-complexity(reward), Settings(reward)\] 
10. System: 3. Click on "Control Panel" label. 
> No change in ACCUM VALUE as user has been punished earlier at this step. 
local experience l vel goes back to moderate. Path reset o 8-5-6-7 selected in step 4. 
11. System: Now click on the DLsplay Properties window. 
12. User: OK 
No change in ACCUlVLVALUE as user has been punished earlier at this step. 
13. System: Select "Desktop settings" in this window. 
14. User: Ok 
No change in ACCUM_VALUE as user has been punished earlier at this step. 
15. System: Finally, adjust "display area" by moving the slide-bar. 
16. User: Ok. Thank yon 
No change in ACC"UM_VALUE as user has been punished earlier at this step. 
Profile of user stored. 
Figure 4. An example 
100 
Notice that after step 9, the user level remains 
at moderate and does not return to expert. 
This is because adecrease in accumulated user 
expertise level has taken place in step 5. Also, 
the style of the dialog is different at each level 
according to the expertise level of the user at 
that time. 
Conclusion 
This paper describes hypotheses, strategies 
and a methodology in building a dialog system 
that adapts itself to the evolving level of user 
expertise. Furthermore, the system allows 
automated sub-goal utility adjusmaent based 
on history of interactions with groups of users. 
We have implemented the algorithm described 
in this paper on a prototype system where the 
utility metrics have been initialized manually 
by a help-desk expert, based on his 
experiences of interaction with users. We are 
currently working on evaluation of hypotheses 
and the system. 
This work is still in its early stage. Our future 
work includes conducting evaluation of the 
hypotheses and the system and investigating 
machine learning techniques for improving 
utility adjustments. 
Acknowledgement 
This work was a summer project while the 
first author was doing his summer internship at 
the Conversational Machines Group at IBM T. 
J. Watson Research Center. We would like to 
thank all members in Conversational Machines 
Group for their discussions and support. 
References 
Carl Andersen, David Traum, K. Purang Darsana 
Purushothaman, Don Perlis (1999) Mixed 
Initiative Dialogue and Intelligence via Active 
Logic. In proceedings of the AAAI99 Workshop 
on Mixed-Initiative Intelligence, pp. 60-67. 
Anthony Jameson, Ralph Sch~fer, Thomas Weis, 
Andr6 Berthold and Thomas Weyrath (1999) 
MaMng Systems Sensitive to the User ~ Time and 
Working Memory Constraints, Intelligent User 
Interfaces. 
Kristiina Jokinen (2000) Learning Dialog System. 
LREC 2000 Second International Conference on 
Language Resources and Evaluation, Athens, 
Greece. 
Neal Lesh, Charles Rich, Candace Sidner (1997) 
Using plan recognition in human-computer 
collaboration. In 7tn International Conf. On User 
Modeling, Banff, Canada. 
Diane J. Litman, Shirnei Pan, Marilyn A Walker, 
(1998) Evaluating Response Strategies in a 
Web-Based Spoken Dialogue Agent. In 
Proceedings of the 36 th Annual Meeting of the 
Association for Computational Linguistics and 
the 17th International Conference on 
Computational Linguistics (COLING-ACL'98), 
Montreal, Canada, pp. 780-786. 
Diane J. Litrnan, Shimei Pan (1999) Empirically 
Evaluating an Adaptable Spoken Dialogue 
System. In Proceedings of the 7th International 
Conference on User Modeling (UM), Banff, 
Canada, pp. 55-64. 
Andros Stolcke, Elizabeth Shriberg, Rebecca Bates, 
Noah Coccaro, Daniel Jurafsky, Rachel 
Martin, Marie Meteer, Klaus Ries, Paul Taylor, 
Carol Van Ess-Dykerna (1998) Dialog act 
modeling for conversational speech. In Chu- 
Carroll J., and Green N., (Eds), Applying 
Machine Learning to Discourse Processing. 
Papers fi'om the 1998 AAAI Spring Symposium. 
Stanford, CA. 
Marilyn Walker, Jeanne Fromer, Shrikanth 
Narayanan (1998) Learning Optimal Dialog 
Strategies: A Case Study of a Spoken Dialog 
Agent for EmaiL In Proceedings of COLING- 
ACL'98, University of Montreal, Canada. 
Thomas Weis (1997) Resource-Adaptive Action 
Planning in a Dialogue System for Repair 
Support, KI. 
Robert L Winlder (1972) Introduction to Bayesian 
Inference and Decision. Holt, Rinehart and 
Winston Inc. 
101 
Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 244?253,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Incorporating Temporal and Semantic Information with Eye Gaze for
Automatic Word Acquisition in Multimodal Conversational Systems
Shaolin Qu Joyce Y. Chai
Department of Computer Science and Engineering
Michigan State University
East Lansing, MI 48824
{qushaoli,jchai}@cse.msu.edu
Abstract
One major bottleneck in conversational sys-
tems is their incapability in interpreting un-
expected user language inputs such as out-of-
vocabulary words. To overcome this problem,
conversational systems must be able to learn
new words automatically during human ma-
chine conversation. Motivated by psycholin-
guistic findings on eye gaze and human lan-
guage processing, we are developing tech-
niques to incorporate human eye gaze for au-
tomatic word acquisition in multimodal con-
versational systems. This paper investigates
the use of temporal alignment between speech
and eye gaze and the use of domain knowl-
edge in word acquisition. Our experiment re-
sults indicate that eye gaze provides a poten-
tial channel for automatically acquiring new
words. The use of extra temporal and domain
knowledge can significantly improve acquisi-
tion performance.
1 Introduction
Interpreting human language is a challenging prob-
lem in human machine conversational systems due
to the flexibility of human language behavior. When
the encountered vocabulary is outside of the sys-
tem?s knowledge, conversational systems tend to
fail. It is desirable that conversational systems can
learn new words automatically during human ma-
chine conversation. While automatic word acquisi-
tion in general is quite challenging, multimodal con-
versational systems offer an unique opportunity to
explore word acquisition. In a multimodal conversa-
tional system where users can talk and interact with
a graphical display, users? eye gaze, which occurs
naturally with speech production, provides a poten-
tial channel for the system to learn new words auto-
matically during human machine conversation.
Psycholinguistic studies have shown that eye gaze
is tightly linked to human language processing. Eye
gaze is one of the reliable indicators of what a per-
son is ?thinking about? (Henderson and Ferreira,
2004). The direction of eye gaze carries informa-
tion about the focus of the user?s attention (Just and
Carpenter, 1976). The perceived visual context in-
fluences spoken word recognition and mediates syn-
tactic processing of spoken sentences (Tanenhaus et
al., 1995). In addition, directly before speaking a
word, the eyes move to the mentioned object (Grif-
fin and Bock, 2000).
Motivated by these psycholinguistic findings, we
are investigating the use of eye gaze for automatic
word acquisition in multimodal conversation. Par-
ticulary, this paper investigates the use of tempo-
ral information about speech and eye gaze and do-
main semantic relatedness for automatic word ac-
quisition. The domain semantic and temporal in-
formation are incorporated in statistical translation
models for word acquisition. Our experiments show
that the use of domain semantic and temporal infor-
mation significantly improves word acquisition per-
formance.
In the following sections, we first describe the ba-
sic translation models for word acquisition. Then,
we describe the enhanced models that incorporate
temporal and semantic information about speech
and eye gaze for word acquisition. Finally, we
present the results of empirical evaluation.
244
(a) Raw gaze points (b) Processed gaze fixations
Figure 1: Domain scene with a user?s gaze fixations
2 Related Work
Word acquisition by grounding words to visual en-
tities has been studied in many language ground-
ing systems. For example, given speech paired with
video images of single objects, mutual information
between audio and visual signals was used to acquire
words by associating acoustic phone sequences with
the visual prototypes (e.g., color, size, shape) of ob-
jects (Roy and Pentland, 2002). Generative mod-
els were used to acquire words by associating words
with image regions given parallel data of pictures
and description text (Barnard et al, 2003). Differ-
ent from these works, in our work, the visual atten-
tion foci accompanying speech are indicated by eye
gaze. Eye gaze is an implicit and subconscious in-
put, which brings additional challenges in word ac-
quisition.
Eye gaze has been explored for word acquisition
in previous work. In (Yu and Ballard, 2004), given
speech paired with eye gaze information and video
images, a translation model was used to acquire
words by associating acoustic phone sequences with
visual representations of objects and actions. A re-
cent investigation on word acquisition from tran-
scribed speech and eye gaze in human machine con-
versation was reported in (Liu et al, 2007). In this
work, a translation model was developed to asso-
ciate words with visual objects on a graphical dis-
play. Different from these previous works, here
we investigate the incorporation of extra knowledge,
specifically speech-gaze temporal information and
domain knowledge, with eye gaze to facilitate word
acquisition.
3 Data Collection
We recruited users to interact with a simplified mul-
timodal conversational system to collect speech and
eye gaze data.
3.1 Domain
We are working on a 3D room decoration domain.
Figure 1 shows the 3D room scene that was shown
to the user in the experiments. There are 28 3D
objects (bed, chairs, paintings, lamp, etc.) in the
room scene. During the human machine conversa-
tion, the system verbally asked the user a question
(e.g., ?what do you dislike about the arrangement
of the room??) or issued a request (e.g., ?describe
the left wall?) about the room. The user provided
responses by speaking to the system.
During the experiments, users? speech was
recorded through an open microphone and users?
eye gaze was captured by an Eye Link II eye tracker.
Eye gaze data consists of the screen coordinates of
each gaze point that was captured by the eye tracker
at a sampling rate of 250hz.
3.2 Data Preprocessing
As for speech data, we collected 357 spoken utter-
ances from 7 users? experiments. The vocabulary
size is 480, among which 227 words are nouns and
adjectives. We manually transcribed the collected
speech.
As for gaze data, the first step is to identify gaze
fixation from raw gaze points. As shown in Fig-
ure 1(a), the collected raw gaze points are very noisy.
They can not be used directly for identifying gaze
fixated entities in the scene. We processed the raw
245
gaze data to eliminate invalid and saccadic gaze
points. Invalid gaze points occur when users look
off the screen. Saccadic gaze points occur during
ballistic eye movements between gaze fixations. Vi-
sion studies have shown that no visual processing
occurs in the human mind during saccades (i.e., sac-
cadic suppression) (Matin, 1974). Since eyes do not
stay still but rather make small, frequent jerky move-
ments, we average nearby gaze points to better iden-
tify gaze fixations. The processed eye gaze fixations
are shown in Figure 1(b).
1668 2096 32522692
[19] [22] [ ] [10]
[11]
[10]
[11]
[10]
[11]
This room has a chandelier
2572 2872 3170 3528 3736
speech stream
gaze stream
(ms)
(ms)
[fixated entity ID]
ts te
f: gaze fixation
( [19] ? bed_frame; [22] ? door; [10] ? bedroom; [11] ? chandelier )
Figure 2: Parallel speech and gaze streams
Figure 2 shows an excerpt of the collected speech
and gaze fixation in one experiment. In the speech
stream, each word starts at a particular timestamp.
In the gaze stream, each gaze fixation has a start-
ing timestamp ts and an ending timestamp te. Each
gaze fixation also has a list of fixated entities (3D ob-
jects). An entity e on the graphical display is fixated
by gaze fixation f if the area of e contains fixation
point of f .
Given the collected speech and gaze fixations, we
build parallel speech-gaze data set as follows. For
each spoken utterance and its accompanying gaze
fixations, we construct a pair of word sequence and
entity sequence (w, e). The word sequence w con-
sists of only nouns and adjectives in the utterance.
Each gaze fixation results in a fixated entity in the
entity sequence e. When multiple entities are fix-
ated by one gaze fixation due to the overlapping of
the entities, the forefront one is chosen. Also, we
merge the neighboring gaze fixations that contain
the same fixated entities. For the parallel speech and
gaze streams shown in Figure 2, the resulting word
sequence is w = [room chandelier] and the entity
sequence is e = [bed frame door chandelier].
4 Translation Models for Automatic Word
Acquisition
Since we are working on conversational systems
where users interact with a visual scene, we consider
the task of word acquisition as associating words
with visual entities in the domain. Given the par-
allel speech and gaze fixated entities {(w, e)}, we
formulate word acquisition as a translation problem
and use translation models to estimate word-entity
association probabilities p(w|e). The words with the
highest association probabilities are chosen as ac-
quired words for entity e.
4.1 Base Model I
Using the translation model I (Brown et al, 1993),
where each word is equally likely to be aligned with
each entity, we have
p(w|e) =
1
(l + 1)m
m?
j=1
l?
i=0
p(wj |ei) (1)
where l and m are the lengths of entity and word
sequences respectively. This is the model used in
(Liu et al, 2007) and (Yu and Ballard, 2004). We
refer to this model as Model-1 throughout the rest
of this paper.
4.2 Base Model II
Using the translation model II (Brown et al, 1993),
where alignments are dependent on word/entity po-
sitions and word/entity sequence lengths, we have
p(w|e) =
m?
j=1
l?
i=0
p(aj = i|j,m, l)p(wj |ei) (2)
where aj = i means that wj is aligned with ei.
When aj = 0, wj is not aligned with any entity (e0
represents a null entity). We refer to this model as
Model-2.
Compared to Model-1, Model-2 considers the or-
dering of words and entities in word acquisition.
EM algorithms are used to estimate the probabilities
p(w|e) in the translation models.
5 Using Speech-Gaze Temporal
Information for Word Acquisition
In Model-2, word-entity alignments are estimated
from co-occurring word and entity sequences in an
246
unsupervised way. The estimated alignments are de-
pendent on where the words/entities appear in the
word/entity sequences, not on when those words and
gaze fixated entities actually occur. Motivated by the
finding that users move their eyes to the mentioned
object directly before speaking a word (Griffin and
Bock, 2000), we make the word-entity alignments
dependent on their temporal relation in a new model
(referred as Model-2t):
p(w|e) =
m?
j=1
l?
i=0
pt(aj = i|j, e,w)p(wj |ei) (3)
where pt(aj = i|j, e,w) is the temporal alignment
probability computed based on the temporal dis-
tance between entity ei and word wj .
We define the temporal distance between ei and
wj as
d(ei, wj) =
?
??
??
0 ts(ei) ? ts(wj) ? te(ei)
te(ei) ? ts(wj) ts(wj) > te(ei)
ts(ei) ? ts(wj) ts(wj) < ts(ei)
(4)
where ts(wj) is the starting timestamp (ms) of word
wj , ts(ei) and te(ei) are the starting and ending
timestamps (ms) of gaze fixation on entity ei.
The alignment of word wj and entity ei is de-
cided by their temporal distance d(ei, wj). Based
on the psycholinguistic finding that eye gaze hap-
pens before a spoken word, wj is not allowed to
be aligned with ei when wj happens earlier than ei
(i.e., d(ei, wj) > 0). When wj happens no earlier
than ei (i.e., d(ei, wj) ? 0), the closer they are, the
more likely they are aligned. Specifically, the tem-
poral alignment probability of wj and ei in each co-
occurring instance (w, e) is computed as
pt(aj = i|j, e,w) =
{
0 d(ei, wj) > 0
exp[??d(ei,wj)]
?i exp[??d(ei,wj)]
d(ei, wj) ? 0
(5)
where ? is a constant for scaling d(ei, wj). In our
experiments, ? is set to 0.005.
An EM algorithm is used to estimate probabilities
p(w|e) in Model-2t.
?5000 ?4000 ?3000 ?2000 ?1000 0 10000
20
40
60
80
100
120
140
temporal distance of aligned word and entity (ms)
alig
nm
ent
 cou
nt
Figure 3: Histogram of truly aligned word and entity
pairs over temporal distance (bin width = 200ms)
For the purpose of evaluation, we manually anno-
tated the truly aligned word and entity pairs. Fig-
ure 3 shows the histogram of those truly aligned
word and entity pairs over the temporal distance of
aligned word and entity. We can observe in the fig-
ure that 1) almost no eye gaze happens after a spo-
ken word, and 2) the number of word-entity pairs
with closer temporal distance is generally larger than
the number of those with farther temporal distance.
This is consistent with our modeling of the tempo-
ral alignment probability of word and entity (Equa-
tion (5)).
6 Using Domain Semantic Relatedness for
Word Acquisition
Speech-gaze temporal alignment and occurrence
statistics sometimes are not sufficient to associate
words to an entity correctly. For example, suppose
a user says ?there is a lamp on the dresser? while
looking at a lamp object on a table object. Due
to their co-occurring with the lamp object, words
dresser and lamp are both likely to be associated
with the lamp object in the translation models. As
a result, word dresser is likely to be incorrectly ac-
quired for the lamp object. For the same reason, the
word lamp could be acquired incorrectly for the ta-
ble object. To solve this type of association prob-
lem, the semantic knowledge about the domain and
words can be helpful. For example, the knowledge
that the word lamp is more semantically related to
the object lamp can help the system avoid associat-
247
ing the word dresser to the lamp object. Therefore,
we are interested in investigating the use of semantic
knowledge in word acquisition.
On one hand, each conversational system has a
domain model, which is the knowledge representa-
tion about its domain such as the types of objects
and their properties and relations. On the other hand,
there are available resources about domain indepen-
dent lexical knowledge (e.g., WordNet (Fellbaum,
1998)). The question is whether we can utilize the
domain model and external lexical knowledge re-
source to improve word acquisition. To address this
question, we link the domain concepts in the domain
model with WordNet concepts, and define semantic
relatedness of word and entity to help the system ac-
quire domain semantically compatible words.
In the following sections, we first describe our
domain modeling, then define the semantic related-
ness of word and entity based on domain modeling
and WordNet semantic lexicon, and finally describe
different ways of using the semantic relatedness of
word and entity to help word acquisition.
6.1 Domain Modeling
We model the 3D room decoration domain as shown
in Figure 4. The domain model contains all do-
main related semantic concepts. These concepts are
linked to the WordNet concepts (i.e., synsets in the
format of ?word#part-of-speech#sense-id?). Each of
the entities in the domain has one or more properties
(e.g., semantic type, color, size) that are denoted by
domain concepts. For example, the entity dresser 1
has domain concepts SEM DRESSER and COLOR.
These domain concepts are linked to ?dresser#n#4?
and ?color#n#1? in WordNet.
Note that in the domain model, the domain con-
cepts are not specific to a certain entity, they are gen-
eral concepts for a certain type of entity. Multiple
entities of the same type have the same properties
and share the same set of domain concepts.
6.2 Semantic Relatedness of Word and Entity
We compute the semantic relatedness of a word w
and an entity e based on the semantic similarity be-
tween w and the properties of e. Specifically, se-
mantic relatedness SR(e, w) is defined as
SR(e, w) = max
i,j
sim(s(cie), sj(w)) (6)
?bed#n#1?
?picture#n#2? ?size#n#1?
?color#n#1?
?dresser#n#4?
COLOR
bed_framedresser_1
SIZESEM_DRESSER SEM_BED COLOR
Entities:
Domain 
concepts:
WordNet 
concepts:
Dom ain Model
Figure 4: Domain model with domain concepts linked to
WordNet synsets
where cie is the i-th property of entity e, s(c
i
e) is the
synset of property cie as designed in domain model,
sj(w) is the j-th synset of word w as defined in
WordNet, and sim(?, ?) is the similarity score of two
synsets.
We computed the similarity score of two synsets
based on the path length between them. The similar-
ity score is inversely proportional to the number of
nodes along the shortest path between the synsets as
defined in WordNet. When the two synsets are the
same, they have the maximal similarity score of 1.
The WordNet-Similarity tool (Pedersen et al, 2004)
was used for the synset similarity computation.
6.3 Word Acquisition with Word-Entity
Semantic Relatedness
We can use the semantic relatedness of word and
entity to help the system acquire semantically com-
patible words for each entity, and therefore improve
word acquisition performance. The semantic relat-
edness can be applied for word acquisition in two
ways: post process learned word-entity association
probabilities by rescoring them with semantic relat-
edness, or directly affect the learning of word-entity
associations by constraining the alignment of word
and entity in the translation models.
6.3.1 Rescoring with semantic relatedness
In the acquired word list for an entity ei, each
word wj has an association probability p(wj |ei) that
is learned from a translation model. We use the
248
semantic relatedness SR(ei, wj) to redistribute the
probability mass for each wj . The new association
probability is given by:
p?(wj |ei) =
p(wj |ei)SR(ei, wj)
?
j p(wj |ei)SR(ei, wj)
(7)
6.3.2 Semantic alignment constraint in
translation model
When used to constrain the word-entity alignment
in the translation model, semantic relatedness can be
used alone or used together with speech-gaze tempo-
ral information to decide the alignment probability
of word and entity.
? Using only semantic relatedness to constrain
word-entity alignments in Model-2s, we have
p(w|e) =
m?
j=1
l?
i=0
ps(aj = i|j, e,w)p(wj |ei)
(8)
where ps(aj = i|j, e,w) is the alignment prob-
ability based on semantic relatedness,
ps(aj = i|j, e,w) =
SR(ei, wj)
?
i SR(ei, wj)
(9)
? Using semantic relatedness and temporal infor-
mation to constrain word-entity alignments in
Model-2ts, we have
p(w|e) =
m?
j=1
l?
i=0
pts(aj = i|j, e,w)p(wj |ei)
(10)
where pts(aj = i|j, e,w) is the alignment
probability that is decided by both temporal re-
lation and semantic relatedness of ei and wj ,
pts(aj = i|j, e,w) =
ps(aj = i|j, e,w)pt(aj = i|j, e,w)
?
i ps(aj = i|j, e,w)pt(aj = i|j, e,w)
(11)
where ps(aj = i|j, e,w) is the semantic align-
ment probability in Equation (9), and pt(aj =
i|j, e,w) is the temporal alignment probability
given in Equation (5).
EM algorithms are used to estimate p(w|e) in
Model-2s and Model-2ts.
7 Grounding Words to Domain Concepts
As discussed above, based on translation models, we
can incorporate temporal and domain semantic in-
formation to obtain p(w|e). This probability only
provides a means to ground words to entities. In
conversational systems, the ultimate goal of word
acquisition is to make the system understand the se-
mantic meaning of new words. Word acquisition by
grounding words to objects is not always sufficient
for identifying their semantic meanings. Suppose
the word green is grounded to a green chair object,
so is the word chair. Although the system is aware
that green is some word describing the green chair,
it does not know that word green refers to the chair?s
color while the word chair refers to the chair?s se-
mantic type. Thus, after learning the word-entity as-
sociations p(w|e) by the translation models, we need
to further ground words to domain concepts of entity
properties.
We further apply WordNet to ground words to do-
main concepts. For each entity e, based on asso-
ciation probabilities p(w|e), we can choose the n-
best words as acquired words for e. Those n-best
words have the n highest association probabilities.
For each word w acquired for e, the grounded con-
cept c?e forw is chosen as the one that has the highest
semantic relatedness with w:
c?e = argmax
i
[
max
j
sim(s(cie), sj(w))
]
(12)
where sim(s(cie), sj(w)) is the semantic similarity
score defined in Equation (6).
8 Evaluation
We evaluate word acquisition performance of differ-
ent models on the data collected from our user stud-
ies (see Section 3).
8.1 Evaluation Metrics
The following metrics are used to evaluate the words
acquired for domain concepts (i.e., entity properties)
{cie}.
? Precision
?
e
?
i # words correctly acquired for c
i
e?
e
?
i # words acquired for c
i
e
249
? Recall
?
e
?
i # words correctly acquired for c
i
e
?
e
?
i # ground-truth
1 words of cie
? F-measure
2 ? precision ? recall
precision + recall
The metrics of precision, recall, and F-measure
are based on the n-best words acquired for the entity
properties. Therefore, we have different precision,
recall, and F-measure when n changes.
The metrics of precision, recall, and F-measure
only provide evaluation on the top n candidate
words. To measure the acquisition performance on
the entire ranked list of candidate words, we define
a new metric as follows:
? Mean Reciprocal Rank Rate (MRRR)
MRRR =
?
e
?Nei=1
1
index(wie)
?Nei=1
1
i
#e
where Ne is the number of all ground-truth
words {wie} for entity e, index(w
i
e) is the in-
dex of word wie in the ranked list of candidate
words for entity e.
Entities may have a different number of ground-
truth words. For each entity e, we calculate a Recip-
rocal Rank Rate (RRR), which measures how close
the ranks of the ground-truth words in the candidate
word list is to the best scenario where the top Ne
words are the ground-truth words for e. RRR is in
the range of (0, 1]. The higher the RRR, the better
is the word acquisition performance. The average of
RRRs across all entities gives the Mean Reciprocal
Rank Rate (MRRR).
Note that MRRR is directly based on the learned
word-entity associations p(w|e), it is in fact a mea-
sure of grounding words to entities.
1The ground-truth words were compiled and agreed upon by
two human judges.
8.2 Evaluation Results
To compare the effects of different speech-gaze
alignments on word acquisition, we evaluate the fol-
lowing models:
? Model-1 ? base model I without word-entity
alignment (Equation (1)).
? Model-2 ? base model II with positional align-
ment (Equation (2)).
? Model-2t ? enhanced model with temporal
alignment (Equation (3)).
? Model-2s ? enhanced model with semantic
alignment (Equation (8)).
? Model-2ts ? enhanced model with both tempo-
ral and semantic alignment (Equation (10)).
To compare the different ways of incorporating
semantic relatedness in word acquisition as dis-
cussed in Section 6.3.1, we also evaluate the follow-
ing models:
? Model-1-r ?Model-1 with semantic relatedness
rescoring of word-entity association.
? Model-2t-r ? Model-2t with semantic related-
ness rescoring of word-entity association.
Figure 5 shows the results of models with differ-
ent speech-gaze alignments. Figure 6 shows the re-
sults of models with semantic relatedness rescoring.
In Figure 5 & 6, n-best means the top n word candi-
dates are chosen as acquired words for each entity.
The Mean Reciprocal Rank Rates of all models are
compared in Figure 7.
8.2.1 Results of using different speech-gaze
alignments
As shown in Figure 5, Model-2 does not show a
consistent improvement compared to Model-1 when
a different number of n-best words are chosen as ac-
quired words. This result shows that it is not very
helpful to consider the index-based positional align-
ment of word and entity for word acquisition.
Figure 5 also shows that models considering
temporal or/and semantic information (Model-2t,
Model-2s, Model-2ts) consistently perform better
than the models considering neither temporal nor
250
1 2 3 4 5 6 7 8 9 100.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
n?best
prec
ision
Model?1Model?2Model?2tModel?2sModel?2ts
(a) precision
1 2 3 4 5 6 7 8 9 100.1
0.2
0.3
0.4
0.5
0.6
n?best
rec
all
Model?1Model?2Model?2tModel?2sModel?2ts
(b) recall
1 2 3 4 5 6 7 8 9 100.2
0.25
0.3
0.35
0.4
0.45
0.5
0.55
n?best
F?m
easu
re
Model?1Model?2Model?2tModel?2sModel?2ts
(c) F-measure
Figure 5: Performance of word acquisition when different types of speech-gaze alignment are applied
1 2 3 4 5 6 7 8 9 100.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
n?best
prec
ision
Model?1Model?2tModel?1?rModel?2t?r
(a) precision
1 2 3 4 5 6 7 8 9 100.1
0.2
0.3
0.4
0.5
0.6
n?best
rec
all
Model?1Model?2tModel?1?rModel?2t?r
(b) recall
1 2 3 4 5 6 7 8 9 100.2
0.25
0.3
0.35
0.4
0.45
0.5
0.55
n?best
F?m
easu
re
Model?1Model?2tModel?1?rModel?2t?r
(c) F-measure
Figure 6: Performance of word acquisition when semantic relatedness rescoring of word-entity association is applied
M?1 M?2 M?2t M?2s M?2ts M?1?r M?2t?r0.5
0.55
0.6
0.65
0.7
0.75
0.8
Models
Mea
n R
ecip
roca
l Ra
nk R
ate
Figure 7: MRRRs achieved by different models
semantic information (Model-1, Model-2). Among
Model-2t, Model-2s, and Model-2ts, it is found that
they do not make consistent differences.
As shown in Figure 7, the MRRRs of different
models are consistent with their performances on F-
measure. A t-test has shown that the difference be-
tween the MRRRs of Model-1 and Model-2 is not
statistically significant. Compared to Model-1, t-
tests have confirmed that MRRR is significantly im-
proved by Model-2t (t = 2.27, p < 0.02), Model-2s
(t = 3.40, p < 0.01), and Model-2ts(t = 2.60, p <
0.01). T-tests have shown no significant differences
among Model-2t, Model-2s, and Model-2ts.
8.2.2 Results of applying semantic relatedness
rescoring
Figure 6 shows that semantic relatedness rescor-
ing improves word acquisition. After semantic re-
latedness rescoring of the word-entity associations
learned by Model-1, Model-1-r improves the F-
measure consistently when a different number of
n-best words are chosen as acquired words. Com-
pared to Model-2t, Model-2t-r also improves the F-
measure consistently.
Comparing the two ways of using semantic relat-
edness for word acquisition, it is found that rescor-
ing word-entity association with semantic related-
ness works better. When semantic relatedness is
used together with temporal information to constrain
word-entity alignments in Model-2ts, word acqui-
251
Model Rank 1 Rank 2 Rank 3 Rank 4 Rank 5
M-1 table(0.173) dresser(0.067) area(0.058) picture(0.053) dressing(0.041)
M-2t table(0.146) dresser(0.125) dressing(0.061) vanity(0.051) fact(0.050)
M-2t-r table(0.312) dresser(0.241) vanity(0.149) desk(0.047) area(0.026)
Table 1: N-best candidate words acquired for the entity dresser 1 by different models
sition performance is not improved compared to
Model-2t. However, using semantic relatedness to
rescore word-entity association learned by Model-
2t, Model-2t-r further improves word acquisition.
As shown in Figure 7, the MRRRs of Model-
1-r and Model-2t-r are consistent with their per-
formances on F-measure. Compared to Model-2t,
Model-2t-r improves MRRR. A t-test has confirmed
that this is a significant improvement (t = 1.97, p <
0.03). Compared to Model-1, Model-1-r signifi-
cantly improves MRRR (t = 2.33, p < 0.02). There
is no significant difference between Model-1-r and
Model-2t/Model-2s/Model-2ts.
In Figures 5&6, we also notice that the recall
of the acquired words is still comparably low even
when 10 best word candidates are chosen for each
entity. This is mainly due to the scarcity of those
words that are not acquired in the data. Many of
the words that are not acquired appear less than 3
times in the data, which makes them unlikely to be
associated with any entity by the translation models.
When more data is available, we expect to see higher
recall.
8.3 An Example
Table 1 shows the 5-best words acquired by different
models for the entity dresser 1 in the 3d room scene
(see Figure 1). In the table, each word is followed by
its word-entity association probability p(w|e). The
correctly acquired words are shown in bold font.
As shown in the example, the baseline Model-1
learned 2 correct words in the 5-best list. Consid-
ering speech-gaze temporal information, Model-2t
learned one more correct word vanity in the 5-best
list. With semantic relatedness rescoring, Model-
2t-r further acquired word desk in the 5-best list
because of the high semantic relatedness of word
desk and the type of entity dresser 1. Although nei-
ther Model-1 nor Model-2t successfully acquired the
word desk in the 5-best list, the rank (=7) of the word
desk in Model-2t?s n-best list is much higher than the
rank (=21) in Model-1?s n-best list.
9 Conclusion
Motivated by the psycholinguistic findings, we in-
vestigate the use of eye gaze for automatic word ac-
quisition in multimodal conversational systems. Par-
ticularly, we investigate the use of speech-gaze tem-
poral information and word-entity semantic related-
ness to facilitate word acquisition. Our experiments
show that word acquisition is significantly improved
when temporal information is considered, which is
consistent with the previous psycholinguistic find-
ings about speech and eye gaze. Moreover, using
temporal information together with semantic relat-
edness rescoring further improves word acquisition.
Eye tracking systems are no longer bulky sys-
tems that prevent natural human machine commu-
nication. Display mounted gaze tracking systems
(e.g., Tobii) are completely non-intrusive, can toler-
ate head motion, and provide high tracking quality.
Integrating eye tracking with conversational inter-
faces is no longer beyond reach. Recent works have
shown that eye gaze can facilitate spoken language
processing in conversational systems (Qu and Chai,
2007; Prasov and Chai, 2008). Incorporating eye
gaze with automatic word acquisition provides an-
other potential approach to improve the robustness
of human machine conversation.
Acknowledgments
This work was supported by IIS-0347548 and IIS-
0535112 from the National Science Foundation.
The authors would like to thank Zahar Prasov for his
contribution on data collection. The authors would
also like to thank anonymous reviewers for their
valuable comments and suggestions.
References
Kobus Barnard, Pinar Duygulu, Nando de Freitas, David
Forsyth, David Blei, and Michael I. Jordan. 2003.
252
Matching words and pictures. Journal of Machine
Learning Research, 3:1107?1135.
Peter F. Brown, Stephen Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathematic
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19(2):263?311.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press.
Zenzi M. Griffin and Kathryn Bock. 2000. What the eyes
say about speaking. Psychological Science, 11:274?
279.
JohnM. Henderson and Fernanda Ferreira, editors. 2004.
The interface of language, vision, and action: Eye
movements and the visual world. New York: Taylor
& Francis.
Marcel A. Just and Patricia A. Carpenter. 1976. Eye fix-
ations and cognitive processes. Cognitive Psychology,
8:441?480.
Yi Liu, Joyce Y. Chai, and Rong Jin. 2007. Au-
tomated vocabulary acquisition and interpretation in
multimodal conversational systems. In Proceedings of
the 45th Annual Meeting of the Association of Compu-
tational Linguistics (ACL).
E. Matin. 1974. Saccadic suppression: a review and an
analysis. Psychological Bulletin, 81:899?917.
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. Wordnet::similarity - measuring the relat-
edness of concepts. In Proceedings of the Nineteenth
National Conference on Artificial Intelligence (AAAI-
04).
Zahar Prasov and Joyce Y. Chai. 2008. What?s in a
gaze? the role of eye-gaze in reference resolution in
multimodal conversational interfaces. In Proceedings
of ACM 12th International Conference on Intelligent
User interfaces (IUI).
Shaolin Qu and Joyce Y. Chai. 2007. An exploration
of eye gaze in spoken language processing for multi-
modal conversational interfaces. In Proceedings of the
Conference of the North America Chapter of the Asso-
ciation of Computational Linguistics (NAACL).
Deb K. Roy and Alex P. Pentland. 2002. Learning words
from sights and sounds, a computational model. Cog-
nitive Science, 26(1):113?146.
Michael K. Tanenhaus, Michael J. Spivey-Knowiton,
Kathleen M. Eberhard, and Julie C. Sedivy. 1995. In-
tegration of visual and linguistic information in spoken
language comprehension. Science, 268:1632?1634.
Chen Yu and Dana H. Ballard. 2004. A multimodal
learning interface for grounding spoken language in
sensory perceptions. ACM Transactions on Applied
Perceptions, 1(1):57?80.
253
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 217?224, Vancouver, October 2005. c?2005 Association for Computational Linguistics
A Salience Driven Approach to Robust Input Interpretation in 
Multimodal Conversational Systems  
 
Joyce Y. Chai                  Shaolin Qu              
Computer Science and Engineering 
Michigan State University  
East Lansing, MI 48824 
{jchai@cse.msu.edu,  qushaoli@cse.msu.edu} 
 
 
 
Abstract 
To improve the robustness in multimodal 
input interpretation, this paper presents a new 
salience driven approach. This approach is 
based on the observation that, during 
multimodal conversation, information from 
deictic gestures (e.g., point or circle) on a 
graphical display can signal a part of the 
physical world (i.e., representation of the 
domain and task) of the application which is 
salient during the communication.  This salient 
part of the physical world will prime what 
users tend to communicate in speech and in 
turn can be used to constrain hypotheses for 
spoken language understanding, thus 
improving overall input interpretation. Our 
experimental results have indicated the 
potential of this approach in reducing word 
error rate and improving concept identification 
in multimodal conversation.  
1 Introduction  
Multimodal conversational systems promote more 
natural and effective human machine communication 
by allowing users to interact with systems through 
multiple modalities such as speech and gesture 
(Cohen et al, 1996; Johnston et al, 2002; Pieraccini 
et al, 2004). Despite recent advances, interpreting 
what users communicate to the system is still a 
significant challenge due to insufficient recognition 
(e.g., speech recognition) and understanding (e.g., 
language understanding) performance. Significant 
improvement in the robustness of multimodal 
interpretation is crucial if multimodal systems are to 
be effective and practical for real world applications.  
Previous studies have shown that, in multimodal 
conversation, multiple modalities tend to complement 
each other (Cassell et al 1994). Fusing two or more 
modalities can be an effective means of reducing 
recognition uncertainties, for example, through 
mutual disambiguation (Oviatt 1999). For 
semantically-rich modalities such as speech and pen-
based gesture, mutual disambiguation usually 
happens at the fusion stage where partial semantic 
representations from individual modalities are 
disambiguated and combined into an overall 
interpretation (Johnston 1998, Chai et al, 2004a). 
One problem is that some critical but low probability 
information from individual modalities (e.g., 
recognized alternatives with low probabilities) may 
never reach the fusion stage. Therefore, this paper 
addresses how to use information from one modality 
(e.g., deictic gesture) to directly influence the 
semantic processing of another modality (e.g., spoken 
language understanding) even before the fusion stage.  
In particular we present a new salience driven 
approach that uses gesture to influence spoken 
language understanding. This approach is based on 
the observation that, during multimodal conversation, 
information from deictic gestures (e.g., point or 
circle) on a graphical interface can signal a part of the 
physical world (i.e., representation of the domain and 
task) of the application which is salient during the 
communication.  This salient part of the physical 
world will prime what users tend to communicate in 
speech and thus in turn can be used to constrain 
hypotheses for spoken language understanding. In 
particular, this approach incorporates a notion of 
salience from deictic gestures into language models 
for spoken language processing. Our experimental 
results indicate the potential of this approach in 
reducing word error rate and improving concept 
identification from spoken utterances. 
217
In the following sections, we first introduce the 
current architecture for multimodal interpretation. 
Then we describe our salience driven approach and 
present empirical results.  
2 
3 
Input Interpretation 
Input interpretation is the identification of semantic 
meanings in user inputs. In multimodal conversation, 
user inputs can come from multiple channels (e.g., 
speech and gesture). Thus, most work on input 
interpretation is based on semantic fusion that 
includes individual recognizers and a sequential 
integration processes as shown in Figure 1.  In this 
approach, a system first creates possible partial 
meaning representations from recognized hypotheses 
(e.g., N-best lists) independently of other modalities. 
For example, suppose a user says ?what is the price 
of this painting? and at the same time points to a 
position on the screen. The partial meaning 
representations from the speech input and the gesture 
input are shown in (a-b) in Figure 1. The system uses 
the partial meaning representations to disambiguate 
each other and combines compatible partial 
representations together into an overall semantic 
representation as in Figure1(c).  
In this architecture, the partial semantic 
representations from individual modalities are crucial 
for mutual disambiguation during multimodal fusion. 
The quality of partial semantic representations 
depends on how individual modalities are processed. 
For example, if the speech input is recognized as 
?what is the prize of this pant?, then the partial 
representation from the speech input will not be 
created in the first place. Without a candidate partial 
representation, it is not likely for multimodal fusion 
to reach an overall meaning of the input given this 
late fusion architecture. 
Thus, a problem with the semantics-based fusion 
approach is that information from multiple modalities 
is only used during the fusion stage to disambiguate 
or combine partial semantic representations. This late 
use of information from other sources in the 
pipelined process can cause the loss of some low 
probability information (e.g., recognized alternatives 
with low probabilities which did not make it to the N-
best list) which could be very crucial in terms of the 
overall interpretation.  It is desirable to use 
information from multiple sources at an earlier stage 
before partial representations are created from 
individual modalities. For example, in ((Bangalore 
and Johnston 2000), a finite-state approach was 
applied to tightly couple multimodal language 
processing (e.g., gesture and speech) and speech 
recognition to improve recognition hypotheses. To 
further address this issue, in this paper, we present a 
salience driven approach that particularly applies 
gesture information (e.g., pen-based deictic gestures) 
to robust spoken language understanding before 
multimodal fusion.  
Related Work on Salience Modeling 
We first give a brief overview on the notion of 
salience and how salience modeling is applied in 
earlier work on natural language and multimodal 
language processing.  
Linguistic salience describes the accessibility of 
entities in a speaker/hearer?s memory and its 
implication in language production and 
interpretation. Many theories on linguistic salience 
have been developed, including how the salience of 
entities affects the form of referring expressions as in 
the Givenness Hierarchy (Gundel et al, 1993) and 
the local coherence of discourse as in the Centering 
Theory (Grosz et al, 1995). Salience modeling is 
used for both language generation and language 
interpretation; the latter is more relevant to our work. 
Most salience-based interpretation has focused on 
reference resolution for both linguistic referring 
expressions (e.g., pronouns) (Lappin and Leass 1995) 
and multimodal expressions (Hul et al 1995; 
Eisenstein and Christoudias 2004).  
Speech Input Gesture Input
Speech 
Recognition
Language
Understanding
Gesture
Recognizer
Multimodal
Fusion
Semantic Representation
Gesture
Understanding
Semantic Representation Semantic Representation
What is the price of this painting Point to a position on the screen
Intent: Ask
Type: Painting
Aspect: Price
Type: Painting
Id: P23
Intent: Ask
Type: Painting
Aspect: Price
Id: P23
Type: Wall
Id: W1
(a) (b)
(c)
 
Figure 1: Semantics-based multimodal interpretation 
Visual salience considers an object salient when 
it attracts a user?s visual attention more than others. 
The cause of such attention depends on many factors 
including user intention, familiarity, and physical 
characteristics of objects. For example, an object may 
be salient when it has some properties the others do 
not have, such as it is the only one that is highlighted, 
or the only one of a certain size, category, or color 
218
(Landragin et al, 2001). Visual salience can also be 
useful in input interpretation, for example, for 
multimodal reference resolution (Kehler 2000) and 
cross-modal coreference interpretation (Byron et al, 
2005).  
We believe that salience modeling should go 
beyond reference resolution. Our view is that the 
salience not only affects the use of referring 
expressions (and thus is useful for interpreting 
referring expressions), but also influences the 
linguistic context of the referring expressions. The 
spoken utterances that contain these expressions tend 
to describe information relating to the salient objects 
(e.g., properties or actions). Therefore, our goal in 
this paper is to take salience modeling a step further 
from reference resolution, towards overall language 
understanding.  
4 
4.1 
A Salience Driven Approach 
The new salience driven approach is based on the 
cognitive theory of Conversation Implicature (Grice 
1975) and earlier empirical findings of user speech 
and gesture behavior in multimodal conversation 
(Oviatt 1999). The theory of Conversation 
Implicature (Grice 1975) states that speakers tend to 
make their contribution as informative as is required 
(for the current purpose of communication) and not 
make their contribution more informative than is 
required. In the context of multimodal conversation 
that involves speech and pen-based gesture, this 
theory indicates that users most likely will not make 
any unnecessary deictic gestures unless those 
gestures help in communicating users? intention. This 
is especially true since gestures usually take an extra 
effort from a user. When a pen-based gesture is 
intentionally delivered by a user, the information 
conveyed is often a crucial component in 
interpretation (Chai et al, 2005).  
Speech 
Recognition
Language
Understanding
Physical world representation
salient
e1 e2 e3 ???.
P(e)
d
isco
u
rse
Speech
Gesture
Gesture 
Recognition
Gesture
Understanding
Multimodal    Fusion
Semantic  Representation
Figure 2: The salience driven approach: the salience 
distribution calculated from gesture is used to tailor 
language models for spoken language understanding  
Speech and gesture also tend to complement each 
other. For example, when a speech utterance is 
accompanied by a deictic gesture (e.g., point or 
circle) on a graphical display, the speech input tends 
to issue commands or inquiries about properties of 
objects, and the deictic gestures tend to indicate the 
objects of interest. In addition, as shown in (Oviatt 
1999), the deictic gestures often occur before spoken 
utterances. Our previous work (Chai et al, 2004b) 
also showed that 85% of time gestures occurred 
before corresponding speech units. Therefore, 
gestures can be used as an earlier indicator to 
anticipate the content of communication in the 
subsequent spoken utterances.  
Overview 
The general idea of the salience based approach is 
shown in Figure 2. For each application domain, 
there is a physical world representation that captures 
domain knowledge (details are described later). A 
deictic gesture can activate several objects on the 
graphical display. This activation will signal a 
distribution of objects that are salient. The salient 
objects are mapped to the physical world 
representation to indicate a salient part of 
representation that includes relevant properties or 
tasks related to the salient objects. This salient part of 
the physical world is likely to be the potential content 
of the spoken communication, and thus can be used 
to tailor language models for spoken language 
understanding. This process is shown in the middle 
shaded box of Figure 2. It bridges gesture 
understanding and language understanding at a stage 
before multimodal fusion. Note that the use of 
gesture information can be applied at different stages: 
during speech recognition to generate hypotheses or 
post processing of recognized hypotheses before 
language understanding. In this paper, we focus on 
the latter.    
The physical world representation includes the 
following components:  
? Domain Model. This component captures the 
relevant knowledge about the domain including 
domain objects, properties of the objects, relations 
between objects, and task models related to objects. 
Previous studies have shown that domain knowledge 
219
can be used to improve spoken language 
understanding (Wai et al 2001).  Currently, we apply 
a frame-based representation where a frame 
represents an object (or a type of object) in the 
domain and frame elements represent attributes and 
tasks related to the objects. Each frame element is 
associated with a semantic tag which indicates the 
semantic content of that element. In the future, the 
domain model might also include knowledge about 
the interface, for example, visual properties and 
spatial relations between objects on the interface. 
w1 wn?? ??
Time
t2 t3 tn
)(eP
nt)|(
3tgeP
)( 3tnt?
)( 2tnt?
)( 1tnt?
wi wi+1
t1
)|(
2tgeP)|( 1tgeP
Figure 3: Salience modeling: the salience distribution 
at time tn is calculated by a joint effect of gestures 
that happen before tn.  ? Domain Grammar. This component specifies 
grammar and vocabularies used to process language 
inputs. There are two types of representation. The 
first type is a semantics-based context free grammar 
where each non-terminal symbol represents a 
semantic tag (indicating semantic information such as 
the semantic type of an object, etc). Each word (i.e., 
the terminal symbol) in the lexicon relates to one or 
more semantic tags. Some of these semantic tags are 
directly linked to the frame elements in the domain 
model since they represent certain properties or tasks. 
This grammar was manually developed.  
4.2 
The second type of representation is based on 
annotated user spoken utterances. The data are 
annotated in terms of relevant semantic information 
(i.e., using semantic tags) in the utterance and the 
intended objects of interest (which are directly linked 
to the domain model). Based on the annotated data, 
N-grams can be learned to represent the dependency 
of language in our domain.  
Based on the physical world representation, our 
approach supports the following operations:  
Salience modeling. This operation calculates a 
salience distribution of entities in the physical world. 
In our current investigation, we limit the scope of 
entities to a closed set of objects from our physical 
world representation since the system has knowledge 
about those objects. These entities could have 
different salience values depending on whether they 
are visible on the graphical display, gestured by a 
user, or mentioned in the prior conversation. In this 
paper, we focus on the salience modeling using 
gesture information only.  
Salience driven language understanding. This 
operation maps the salience distribution to the 
physical world representation and uses the salient 
world to influence spoken language understanding. 
Note that, in this paper, we are not concerned with 
acoustic models for speech recognition, but rather we 
are interested in the use of the salience distribution to 
prime language models and facilitate language 
understanding. 
Salience Modeling 
We use a vector er to represent entities in the physical 
world representation. For each entity e ek
r? , we use 
to represent its salience value at time tn.  For 
all the entities, we use P
)( kt eP n
)(e
nt
v  to represent a salience 
distribution at time tn. Figure 3 shows a sequence of 
words with corresponding gestures that occur at t1, t2, 
and t3. As shown in Figure 3, the salience distribution 
at any given time tn is influenced by a joint effect 
from this sequence of gestures that happen before tn 
etc. Depending on its time of occurrence, each 
gesture may have a different impact on the salience 
distribution at time tn. Note that although each 
gesture may have a short duration, here we only 
consider the beginning time of a gesture. Therefore, 
for an entity ek, its salience value at time tn is 
computed as follows: 
1
1
( ) ( | )
( )
( ) ( |
n i i
n
n i i
m
t t k t
i
t k m
t t t
e e i
)
g P e g
P e
g P e g
?
?
=
? =
=
?
??r
              (1) 
In Equation (1), m (m ? 1) is the number of 
gestures that have occurred before tn. The different 
impact of a gesture g  at time ti that contributes to 
the salience distribution at time tn is represented as 
the weight 
it
)(
in tt g? in Equation (1). Currently, we 
calculate the weight depending on the temporal 
distance as follows:  
)(]
2000
)(
exp[)( in
in
tt tt
tt
g
in
???=?             (2) 
Equation (2) indicates that at a given time tn 
(measured in milliseconds), the closer a gesture (at ti) 
is to the time tn, the higher impact this gesture has on 
the salience distribution (Chai et al, 2004b).  
It is worth mentioning that a deictic gesture on the 
graphic display (e.g., pointing and circling) could 
have ambiguous interpretation by itself. For example, 
220
given an interface, a point or a circle on the screen 
could result in selection of different entities with 
different probabilities. Therefore, in Equation (1), 
is the selection probability which indicates 
the likelihood of selecting an entity e given a gesture 
at time ti. This selection probability is calculated by a 
function of the distance between the location of the 
entity and the focus point of the recognized gesture 
on the display (Chai et al, 2004a). A normalization 
factor is incorporated to ensure that the summation of 
selection probabilities over all possible entities adds 
up to one.  
( | )
it
P e g
When no gesture is involved in a given input, the 
salience distribution at any given time is a uniform 
distribution. If one or more gestures are involved, 
then Equation (1) is used to calculate the salience 
distribution.  
4.3 
P W
Salience Driven Spoken Language 
Understanding 
The salience distribution of entities identified based 
on the gesture information (as described above) is 
used to constrain hypotheses for language 
understanding. More specifically, for each onset of a 
spoken word at time t (i.e., the beginning time stamp 
of a spoken word), the salience distribution at t can 
be calculated based on a sequence of gestures that 
happen before t by Equation (1). This salience 
distribution can then be used to prime language 
models for spoken language processing.   
Language Modeling 
We first give a brief background of language 
modeling. Given an observed speech utterance O, the 
goal of speech recognition is to find a sequence of 
words W* so that W P , 
where P(O|W) is the acoustic model and P(W) is the 
language model. In traditional speech recognition 
systems, the acoustic model provides the probability 
of observing the acoustic features given hypothesized 
word sequences and the language model provides the 
probability of a sequence of words. The language 
model is computed as follows: 
* arg max ( | ) ( )O W=
)|()...|()|()()( 112131211
?= nnn wwPwwwPwwPwPwP          
Using the Markov assumption, the language model 
can be approximated by a bigram model as in: 
?
=
?=
n
i
ii
n wwPwP
1
11 )|()(                                    
To improve the speech understanding results for 
spoken language interfaces, many systems have 
applied a loosely-integrated approach which 
decouples the language model from the acoustic 
model (Zue et al, 1991, Harper et al, 2000). This 
allows the development of powerful language models 
independent of the acoustic model, for example, 
utilizing topics of the utterances (Gildea and 
Hofmann 1999), syntactic or semantic labels 
(Heeman 1999), and linguistic structures (Chelba and 
Jelinek 2000, Wang and Harper 2002). Recently, we 
have seen work on language understanding based on 
environment (Schuler 2003) and language modeling 
using visual context (Roy and Mukherjee 2005). Our 
salience driven approach is inspired by this earlier 
work. Here, we do not address the acoustic model of 
speech recognition, but rather incorporate the 
salience distribution for language modeling. In 
particular, our focus is on investigating the effect of 
incorporating additional information from other 
modalities (e.g., gesture) with traditional language 
models.   
Primed Language Model 
The calculated salience distribution is used to prime 
the language model. More specifically, we use a 
class-based bigram model from (Brown et al 1992):  
)|()|()|( 11 ?? = iiiiii ccPcwPwwP                 (3) 
In Equation (3), ci is the class of the word wi, 
which could be a syntactic class or a semantic class. 
is the class transition probability, which 
reflects the grammatical formation of utterances. 
is the word class probability which 
measures the probability of seeing a word wi given a 
class ci. The class-based N-gram model can make 
better use of limited training data by clustering words 
into classes. A number of researchers have shown 
that the class-based N-gram model can successfully 
improve the performance of speech recognition 
(Jelinek 1990, Heeman 1999, Kneser and Ney 1993, 
Samuelsson and Reichl, 1999). 
)|( 1?ii ccP
)|( ii cwP
In our approach, the ?class? used in the class-
based bigram model comes from combined semantic 
and functional classes designed for our domain. For 
example, ?this? is tagged as Demonstrative, and 
?price? is tagged as AttrPrice. As shown in Equation 
(3), there are two types of parameter estimation. In 
terms of the class transition probability, as in earlier 
work, we directly use the annotated data. In terms of 
the word class distribution, we incorporate the notion 
of salience. We use the salience distribution to 
dynamically adjust the world class probability 
 as follows: )|( ii cwP
221
)(
)|(
)|,(
)|( kt
ee ki
kii
ii ePecP
ecwP
cwP
i
k
?
?
=
v
               (4) User  
index 
# of  
Inputs 
# inputs 
w/o gesture 
Baseline 
WER 
1 21 0 0.287 
2 31 0 0.335 
3 27 0 0.399 
4 10 0 0.680 
5 8 1 0.200 
6 36 0 0.387 
7 18 0 0.250 
8 25 1 0.278 
9 23 0 0.482 
10 11 0 0.117 
11 16 3 0.255 
Table 1: Related information about the evaluation 
data: user type, the number of turns per user, and the 
baseline word recognition rate.  
In Equation (4), P  is the salience value for an 
entity  at time ti (the onset of the spoken word wi), 
which can be calculated by Equation (1).  Equation 
(4) indicates that only information associated with the 
salient entities is used to estimate the word class 
distribution. In other words, the word class 
probability favors the salient physical world as 
indicated by the salience distribution
)( kt ei
ke
)(eP
it
v . More 
specifically, at time  ti, given a semantic class ci, the 
choice of word ?wi? is dependent on the salient 
physical world at the moment, which is represented 
as the salience distribution )(eP
it
v at time ti. For all wi, 
the summation of this word class probability is equal 
to one. Furthermore, given an entity ,  
and  are not dependent on time ti, but rather 
on the domain and the use of language expressions. 
Therefore they can be estimated based on the training 
data that are annotated in terms of semantic 
information and the intended objects of interest (as 
discussed in Section 4.1). Since the annotated data is 
very limited, the sparse data can become a problem 
for the maximum likelihood estimation. Therefore, a 
smoothing technique based on the Katz backoff 
model (Katz, 1987) is applied. For example, to 
calculate , if a word wi has one or more 
occurrences in the training data associated with the 
class ci and the entity , then its count is discounted 
by a fraction in the maximum likelihood estimation. 
If wi does not occur, then this approach backs off to 
the domain grammar and redistributes the remaining 
probability mass uniformly among words in the 
lexicon that are linked with class ci and entity e . 
ke )| ki ec
k
,( iwP
)| keP
,( iwP
( ic
)| ke
ke
ic
5 
                                                          
Evaluation 
We evaluated the salience model during post 
processing recognized hypotheses. Given possible 
hypotheses from a speech recognizer, we use the 
salience-based language model to identify the most 
likely sequence of words. The salience distribution 
based on gesture was used to favor words that are 
consistent with the attention indicated by gestures. 
The data collected from our previous user studies 
were used in our evaluation (Chai et al, 2004b). In 
these studies, users interacted with our multimodal 
interface using both speech and deictic gestures to 
find information about real estate properties. In 
particular, each user was asked to accomplish five 
tasks. Each of these tasks required the user to retrieve 
different types of information from our interface. For 
example, one task was to find the least expensive 
house in the most populated town. The data were 
recorded from eleven subjects including five non-
native speakers and six native speakers. Each user?s 
voice was individually trained before the study. Table 
1 shows the relevant information about the data such 
as the total number of inputs (or turns) from each 
subject, the number of speech alone inputs without 
any gesture, and the baseline recognition results 
without using salience-based post processing in terms 
of the word error rate (WER).  In total, we have 
collected 226 user inputs with an average of eight 
words per spoken utterance1. As shown in Table 1, 
the majority of inputs consisted of both speech and 
gesture. Since currently we only use gesture 
0.1
0.2
0.3
0.4
0.5
0.6
0.7
1 2 3 4 5 6 7 8 9 10 11
User index
W
o
rd
 E
rr
o
r 
R
a
te
Baseline Salience driven model
 
Figure 5: Comparison of the baseline and the result 
from post-processing in terms of WER  
1 The difference between the number of user inputs reported 
here and that in (Chai et al, 2004b) was caused by the situa-
tion where one intended user input (which was the unit for 
counting in our previous work) was split into a couple turns 
(which constitute the new counts here).  
222
information in salience modeling, our approach will 
not affect speech only inputs.  
To train the salience-based model, we applied the 
leave-one-out approach. The data from each user was 
held out as the testing data and the remaining users 
were used as the training data to acquire relevant 
probability estimations in Equation (3) and (4).  
Figure 5 shows the comparison results between 
the baseline and the salience-based model in terms of 
word error rate (WER). The word error rate as a 
result of salience-based post processing is 
significantly better than that from the baseline 
recognizer (t = 4.75, p < 0.001). The average WER 
reduction is about 12%.   
We further evaluated how the salience based 
model affects the final understanding results. This is 
because an improvement in WER may not directly 
lead to an improvement in understanding. We applied 
our semantic grammar on a sequence of words 
resulting from both the baseline and the salience-
based post-processing to identify key concepts. In 
total, there were 686 concepts from the transcribed 
speech utterances. Table 2 shows the evaluation 
results. Precision measures the percentage of correctly 
identified concepts out of the total number of 
concepts identified based on a sequence of words. 
Recall measures the percentage of correctly identified 
concepts out of the total number of intended concepts 
from user?s utterance. F-measurement combines 
precision and recall together as follows: 
1,
RecallPrecision
RecallPrecision)1(
2
2
=+
??+= ??
? whereF .  
Table 2 shows that, on average, the concept 
identification based on the word sequence resulting 
from the salience-based approach performs better 
than the baseline in terms of both precision and 
recall. Figure 6 provides two examples to show the 
difference between the baseline recognition and the 
salience-based post processing.   
The evaluation reported here is only an initial step 
based on a limited domain. The small scale in the 
number of objects and the vocabulary size can only 
demonstrate the potential of the salience-based 
approach to a limited degree.  To further understand 
the advantages and issues of this approach, we are 
currently working on a more complex domain with 
richer concepts and relations, as well as larger 
vocabularies.  
It is worth mentioning that the goal of this work is 
to explore whether salience modeling based on other 
modalities (e.g., gesture) can be used to prime 
traditional language models to facilitate spoken 
language processing. The salience driven approach 
based on additional modalities can be combined with 
more sophisticated language modeling (e.g., better 
parameter estimation) in the future.  
Example 1:
Transcription: What is the population of this town
Baseline recognition: What is the publisher of this time
Salience-based processing: what is the population of this town
Example 2:
Transcription: How much is this gray house
Baseline recognition: How much is this great house
Salience-based processing: How much is this gray house
Figure 6: Examples of utterances with baseline recogni-
tion and improved recognition from the salience-based 
processing.  
User # Baseline Salience-based 
Precision 80.3% 84.6% 
Recall 75.7% 83.8% 
F-measure 77.9% 84.2% 
Table2. Overall concept identification comparison 
between the baseline and the salience driven model. 
6 Conclusion and Future Work 
This paper presents a new salience driven approach 
to robust input interpretation in multimodal 
conversational systems. This approach takes 
advantage of rich information from multiple 
modalities. Information from deictic gestures is used 
to identify a part of the physical world that is salient 
at a given point of communication. This salient part 
of the physical world is then used to prime language 
models for spoken language understanding. Our 
experimental results have shown the potential of this 
approach in reducing word error rate and improving 
concept identification from spoken utterances in our 
application. Although currently we have only 
investigated the use of gesture information in salience 
modeling, the salience driven approach can be 
extended to include other modalities (e.g., eye gaze) 
and information (e.g., conversation context). Our 
future work will specifically investigate how to 
combine information from multiple sources in 
salience modeling and how to apply the salience 
models in different early stages of processing.  
 
 
223
Acknowledgement 
This work was supported by a CAREER grant IIS-0347548 
from the National Science Foundation. The authors would like 
to thank anonymous reviewers for their helpful comments and 
suggestions.  
References  
Bangalore, S. and Johnston, M. 2000. Integrating Multimodal 
Language Processing with Speech Recognition. In 
Proceedings of ICSLP.  
Brown, P., Della Pietra, V. J., deSouza, P. V., Lai, J. C, and 
Mercer, R. L. 1992. Class-based n-gram models of natural 
language. Computational Linguistics, 18(4):467-479.  
Byron, D., Mampilly, T., Sharma, V., and Xu, T. 2005. Utilizing 
Visual Attention for Cross-Modal Coreference Interpretation. 
Spring Lecture Notes in Computer Science: Proceedings of 
Context-05, page 83-96.  
Cassell, J., Stone, M., Douville, B., Prevost, S., Achorn, B., 
Steedman, M., Badler, N., and Pelachaud, C. 1994. Modeling 
the interaction between speech and gesture. Cognitive Science 
Society. 
Chai, J. Y., Prasov, Z., Blaim, J., and Jin, R. 2005.  Linguistic 
Theories in Efficient Multimodal Reference Resolution: an 
Empirical Investigation. The 10th International Conference on 
Intelligent User Interfaces (IUI-05), pp. 43-50, San Diego, 
CA.  
Chai, J. Y., Hong, P., Zhou, M. X, and Prasov, Z. 2004b. 
Optimization in Multimodal Interpretation. In Proceedings of 
ACL,  pp. 1-8, Barcelona, Spain. 
Chai, J. Y., Hong, P., and Zhou, M.  2004a. A Probabilistic 
Approach to Reference Resolution in Multimodal User 
Interfaces. Proceedings of 9th International Conference on 
Intelligent User Interfaces (IUI-04), pp. 70-77, Madeira, 
Portugal. 
Chelba, C. and Jelinek, F. 2000. Structured language modeling. 
Computer Speech and Language, 14(4):283?332. 
Cohen, P., Johnston, M., McGee, D., Oviatt, S., Pittman, J.; 
Smith, I., Chen, L., and Clow, J. 1996. Quickset: Multimodal 
Interaction for Distributed Applications. Proceedings of ACM 
Multimedia, 31? 40. 
Eisenstein J. and Christoudias. C. 2004. A salience-based 
approach to gesture-speech alignment. In Proceedings of 
HLT/NAACL?04.  
Gildea, D. and Hofmann, T. 1999. Topic-based language models 
using EM. In Proceedings of Eurospeech.  
Griffin, Z. M. 2001. Gaze durations during speech reflect word 
selection and phonological encoding. Cognition 82, B1-B14. 
Grosz, B. J., Joshi, A. K., and Weinstein, S. 1995. Centering: A 
framework for modeling the local coherence of discourse. 
Computational Linguistics, 21(2).  
Grice, H. P. Logic and Conversation. 1975. In Cole, P., and 
Morgan, J., eds. Speech Acts. New York, New York: 
Academic Press. 41-58. 
Gundel, J. K., Hedberg, N., and Zacharski, R. 1993. Cognitive 
Status and the Form of Referring Expressions in Discourse. 
Language 69(2):274-307.  
Harper, M.., White, C., Wang, W., Johnson, M., and Helzerman, 
R. 2000. The Effectiveness of Corpus-Induced Dependency 
Grammars for Post-processing Speech. Proceedings of the 
North American Association for Computational Linguistics, 
102-109. 
Heeman. P. 1999. POS tags and decision trees for language 
modeling. In Proceedings of the Conference on Empirical 
Methods in Natural Language Process (EMNLP). 
Huls, C., Bos, E., and Classen, W. 1995. Automatic Referent 
Resolution of Deictic and Anaphoric Expressions. 
Computational Linguistics, 21(1):59-79. 
Jelinek, F. 1990. Self-organized language modeling for speech 
recognition. In Waibel, A. and Lee, K. F. (Eds). Readings in 
Speech Recognition, pp. 450-506. 
Johnston, M. 1998. Unification-based Multimodal parsing, 
Proceedings of COLING-ACL.  
Johnston, M.,  Bangalore, S.,  Visireddy G., Stent, A., Ehlen, P., 
Walker, M., Whittaker, S., and Maloor, P. 2002. MATCH: An 
Architecture for Multimodal Dialog Systems, in Proceedings 
of the 40th ACL, Philadelphia, pp. 376-383.  
Katz, S. M. 1987. Estimation of probabilities from sparse data for 
the language model component of a speech recognizer. IEEE 
Transactions on Acoustics, Speech, and Signal Processing, 
35(3). 
Kehler, A. 2000. Cognitive Status and Form of Reference in 
Multimodal Human-Computer Interaction, Proceedings of 
AAAI?01. 
Kneser, R. and Ney, H. 1993. Improved clustering techniques for 
class-based statistical language modeling. In Eurospeech?93, 
pp. 973-976. 
Landragin, F., Bellalem, N., and Romary, L. 2001. Visual 
Salience and Perceptual Grouping in Multimodal Interactivity. 
In: First International Workshop on Information Presentation 
and Natural Multimodal Dialogue, Verona, Italy, pp. 151-155. 
Lappin, S., and Leass, H. 1994. An algorithm for pronominal 
anaphora resolution. Computational Linguistics, 20(4).  
Oviatt, S. 1999. Mutual Disambiguation of Recognition Errors in 
a Multimodal Architecture. In Proceedings of CHI. 
Pieraccini, R., Dayandhi, K., Bloom, J., Dahan, J.-G., Phillips, M., 
Goodman, B. R., Prasad, K. V., 2004. Multimodal 
Conversational Systems for Automobiles, Communications of 
the ACM, Vol. 47, No. 1, pp. 47-49 
Roy, D. and Mukherjee, N. 2005. Towards Situated Speech 
Understanding: Visual Context Priming of Language Models. 
Computer Speech and Language, 19(2): 227-248.  
Samuelsson, C. and Reichl, W. 1999. A class-based Language 
Model for Large Vocabulary Speech Recognition Extracted 
from Part-of-Speech Statistics. In IEEE ICASSP?99. 
Schuler, W. 2003. Using model-theoretic semantic interpretation 
to guide statistical parsing and word recognition in a spoken 
language interface. In  Proceedings of ACL, Sapporo, Japan. 
Wai, C., Pierraccinni, R., and Meng, H. 2001. A Dynamic 
Semantic Model for Rescoring Recognition Hypothesis. 
Proceedings of the ICASSP.  
Wang, W. and Harper. M. 2002. The superARV language model: 
In Investigating the effectiveness of tightly integrating 
multiple knowledge sources. In Proceedings  of EMNLP, 238?
247.  
Zue, V., Glass, J., Goodine, D., Leung, H., Phillips, M., Polifroni, 
J., and Seneff, S. 1991. Integration of Speech Recognition and 
Natural Language Processing in the MIT Voyager System. 
Proceedings of the ICASSP. 
224
Performance Evaluation and Error Analysis for Multimodal Reference 
Resolution in a Conversation System 
Joyce Y. Chai          Zahar Prasov              Pengyu Hong 
Department of Computer Science and Engineering
Michigan State University  
East Lansing, MI 48864 
jchai@cse.msu.edu,  prasovza@cse.msu.edu 
Department of Statistics 
Harvard University 
Cambridge, MA 02138 
hong@stat.harvard.edu 
 
Abstract 
Multimodal reference resolution is a process 
that automatically identifies what users refer 
to during multimodal human-machine 
conversation. Given the substantial work on 
multimodal reference resolution; it is important 
to evaluate the current state of the art, 
understand the limitations, and identify 
directions for future improvement. We 
conducted a series of user studies to evaluate the 
capability of reference resolution in a 
multimodal conversation system.  This paper 
analyzes the main error sources during real-time 
human-machine interaction and presents key 
strategies for designing robust multimodal 
reference resolution algorithms. 
1 Introduction* 
Multimodal systems enable users to interact with 
computers through multiple modalities such as speech, 
gesture, and gaze (Bolt 1980; Cassell et al, 1999; Cohen et 
al., 1996; Chai et al, 2002; Johnston et al, 2002). One 
important aspect of building multimodal systems is for the 
system to understand the meanings of multimodal user 
inputs. A key element of this understanding process is 
reference resolution. Reference resolution is a process that 
finds the most proper referents to referring expressions. To 
resolve multimodal references, many approaches have 
been developed, from the use of a focus space model (Neal 
et al, 1998), a centering framework (Zancanaro et al 
1997), contextual factors (Huls et al, 1995); to recent 
approaches using unification (Johnston, 1998), finite state 
machines (Johnston and Bangalore 2000), and context-
based rules (Kehler 2000).   
Given the substantial work in this area; it is important 
to evaluate the state of the art, understand the limitations, 
                                                                
* This work was supported by grant IIS-0347548 from the 
National Science Foundation and grant IRGP-03-42111 from 
Michigan State University. 
 
and identify directions for future improvement. We 
conducted a series of user studies to evaluate the capability 
of reference resolution in a multimodal conversation 
system. In particular, this paper examines two important 
aspects: (1) algorithm requirements for handling a variety 
of references, and (2) technology requirements for 
achieving good real-time performance. In the following 
sections, we first give a brief description of our system. 
Then we analyze the main error sources during real-time 
human-machine interaction and discuss the key strategies 
for designing robust reference resolution algorithms. 
2 System Description  
We implemented a multimodal conversation system to 
study multimodal user referring behavior and to evaluate 
reference resolution algorithms. Users can use both speech 
and manual gestures (e.g., point and circle) to interact with 
a map-based graphic interface to find information about 
real estate properties.  
As shown in Figure 1, our system applies a semantic 
fusion approach that combines the semantic information 
identified from each modality. A key characteristic of the 
system is that, in addition to fusing information from 
different modalities, our system systematically 
incorporates information from the conversation context 
(e.g., the focus of attention from prior conversation), the 
Speech Input Gesture Input
Speech 
Recognizer 
NL
Parser 
Gesture
Recognizer
Multimodal Interpreter
(Graph-based Reference 
Resolution Component)
Conversation
Manager
Presentation
Manager
Conversation 
Context
Visual 
Context
Domain
Context
Multimedia Output  
Figure 1:  Overview of the system 
visual context (e.g., objects on the screen that are in the 
visual focus), and the domain context (i.e., the domain 
k
m
g
re
sp
re
e
in
re
re
g
o
h
c
p
is
a
G
p
R
re
m
d
3
W
p
g
th
sy
a
th
voice from each subject was trained individually to 
minimize speech recognition errors.  
e 
s 
t 
e 
e 
 
f 
, 
e 
 
 
e 
 
. 
s 
e 
 nowledge).   
The reference resolution approach is based on a graph-
atching algorithm. Specifically, two attribute relational 
raphs are used (Tsai and Fu, 1979). One graph is called 
ferring graph that captures referring expressions from 
eech utterances. Each node, corresponding to one 
ferring expression, consists of the semantic information 
xtracted from the expression and the temporal 
formation when the expression is uttered.  Each edge 
presents the semantic and temporal relation between two 
ferring expressions. The second graph is called referent 
raph that represents all potential referents (including 
bjects selected by the gesture, objects in the conversation 
istory, and objects in the visual focus). Each node 
aptures the semantic and temporal information about a 
otential referent (e.g., the time when the potential referent 
 selected by a gesture). Each edge captures the semantic 
nd temporal relations between two potential referents. 
iven these graph representations, the reference resolution 
roblem becomes a graph-matching problem (Gold and 
angarajan, 1996). The goal is to find a match between the 
3.1 Performance Evaluation 
Table 1 summarizes the referring behavior observed in th
studies and the performance of the system. The column
indicate whether there was no gesture, one gesture (poin
or circle), or multiple gestures involved in the input. Th
rows indicate the type of referring expressions in th
speech utterances. Each table entry shows the system
performance on resolving a particular combination o
speech and gesture inputs. For example, the entry at <S2
G4> indicates that 35 inputs consist of demonstrativ
singular noun phrases (as the referring expressions) and a
single circle gesture. Out of these inputs, 27 were correctly
recognized and eight were incorrectly recognized by th
speech recognizer. Out of the 27 correctly recognized
inputs, 26 were correctly assigned referents by the system
Out of the eight incorrectly recognized inputs, reference
in two inputs were correctly resolved.  
Consistent with earlier findings (Kehler 2000), th
majority of user references were simple which onlyferring graph and the referent graph that achieves the 
aximum compatibility between the two graphs.  The 
etails of this approach are described in (Chai et al, 2004). 
 Performance Evaluation and Analysis 
e conducted several user studies to evaluate the 
erformance of real time reference resolution using the 
raph-based approach. Eleven subjects participated in 
ese studies. Each of them was asked to interact with the 
stem using both speech and gestures (point and circle) to 
ccomplish five tasks. For example, one task was to find 
e least expensive house in the most populated town. The 
involved one referring expression and one gesture as 
shown in Table 1 (i.e., S1 to S8, with column G2 and G4). 
However, we have also found that 14% (31/219) of the 
inputs were complex, which involved multiple referring 
expressions from speech utterances (see the row S9). Some 
of these inputs did not have any accompanied gesture (e.g., 
<S9, G1>). Some were accompanied by one gesture (e.g., 
<S9, G4>) or multiple gestures (e.g., <S9, G3> and <S9, 
G5>). The referents to these referring expressions could 
come from user?s gestures, or from the conversation 
context, or from the graphic display. To resolve these types 
of references, the graph-based approach is effective by 
simultaneously considering the semantic, temporal, and 
1(1), 1(0)001(1), 1(0)000S5:(these|those)(num)*(adj)*(ones)*|them
129(111), 90(26)
15(9), 16(1)
2(0) 12(4)
4(2), 0(0)
7(7), 2(1)
7(4), 12(5)
22(21), 14(3)
64(61), 27(11)
7(6), 6(1)
Total 
Num
4(4), 11(0)9(7), 5(0)63(54), 36(8)7(4), 3(2)39(37),29(14)7(5), 6(2)Total Num
0(0), 3(0)8(6), 5(0)3(1), 7(1)4(2), 0(0)00(0), 1(0)S9: multiple expressions
0(0), 3(0)00(0), 3(0)0(0), 3(2)1(0), 3(2)1(0), 0(0)S8: proper nouns
002(0), 0(0)01(1), 0(0)1(1), 0(0)S7: empty expression
006(6), 0(0)01(1), 1(0)0(0), 1(1)S6: here|there
004(2), 4(1)02(2), 6(3)1(0), 2(1)S4: it|this|that| (this|that|the)(adj)*one
3(3), 2(0)019(18), 12(3)000S3: (these|those)(num)*(adj)*Ns
1(1), 2(0)1(1), 0(0)27(26), 8(2)3(2), 0(0)29(28), 16(9)3(3), 1(0)S2: (this|that) (adj*) N
0(0), 1(0)01(0), 1(1)05(5), 3(0)1(1), 1(0)S1: the (adj)*(N | Ns)
G6
Points and 
Circles
G5
Multiple 
Circles
G4
One Circle
G3
Multiple 
Points
G2
One Point
G1
No 
Gesture
Table 1: Performance evaluation of the graph-matching approach to multimodal reference resolution. In each entry form
?a(b), c(d)?,  ?a? indicates the number of inputs in which the referring expressions were correctly recognized by the speech
recognizer; ?b? indicates the number of inputs in which the referring expressions were  correctly recognized and were cor-
rectly resolved; ?c? indicates the number of inputs in which the referring expressions were not correctly recognized; ?d?
indicates the number of inputs in which the referring expressions were not correctly recognized, but were correctly resolved.
The sum of ?a? and ?c? gives the total number of inputs with a particular combination of speech and gesture. 
 
contextual constraints.   
3.2 Error Analysis 
As shown in Table 1, out of the total 219 inputs, 137 inputs 
had their referents correctly identified (A complex input 
with multiple referring expressions was considered 
correctly resolved only if the referents to all the referring 
expressions were correctly identified). For the remaining 
82 inputs in which the referents were not correctly 
identified, the errors mainly came from five sources as 
summarized in Table 2.  
A poor performance in speech recognition is a major 
error source. Although we have trained each user?s voice 
individually, the speech recognition rate is still very low. 
Only 59% (129/219) of inputs had correctly recognized 
referring expressions. This is partly due to the fact that 
more than half of our subjects are non-native speakers.  
Fusing inputs from multiple modalities together can 
sometimes compensate for the recognition errors (Oviatt 
1996). Among 90 inputs in which referring expressions 
were incorrectly recognized, 26 of them were correctly 
assigned referents due to the mutual disambiguation. 
However, poor speech recognition still accounted for 55% 
of the total errors. A mechanism to reduce the recognition 
errors, especially by utilizing information from other 
modalities will be important to provide a robust solution 
for real time multimodal reference resolution.  
The second source of errors (20% of the total errors) 
came from insufficient language understanding, especially 
the out-of-vocabularies. For example, ?area? was not in 
our vocabulary. So the additional semantic constraint 
expressed by ?area? was not captured. Therefore, the 
system could not identify whether a house or a town was 
referred when the user uttered ?this area?. It is important 
for the system to have a capability of acquire knowledge 
(e.g., vocabulary) dynamically by utilizing information 
from other modalities and the interaction context. 
Furthermore, the errors also came from a lack of 
understanding of spatial relations (as in ?the house just 
close to the red one?) and superlatives (as in ?the most 
expensive house?). Algorithms to align visual features to 
resolve spatial references as described in (Gorniak and Roy 
2003) are desirable.  
Among all errors, 13% came from unsynchronized 
inputs. Currently, we use an idle status (i.e., 2 seconds with 
no input from either speech or gesture) as the boundary to 
delimit an interaction turn. There are two types of out of 
synchronization. The first type is unsynchronized inputs 
from the user (such as a big pause between speech and 
gesture) and the other comes from the underlying system 
implementation.  The system captures speech inputs and 
gesture inputs from two different servers through TCP/IP 
protocol. A communication delay sometimes split one 
synchronized input into two separate turns of inputs (i.e., 
one turn was speech input alone and the other turn was 
gesture input alone). A better engineering mechanism to 
synchronize inputs is desired. 
The disfluencies from the users also accounted for 
about 7% of the total errors. Recent findings indicated that 
gesture patterns could be used as an additional source to 
identify different types of speech disfluencies during 
human-human conversation (Chen et al, 2002). As 
expected, speech disfluencies did not occur that much in 
our studies. Based on our limited cases, we found that 
gesture patterns could be indicators of speech disfluencies 
when they did occur. For example, if a user says ?show me 
the red house (point to house A), the green house (still 
point to the house A)?, then the behavior of pointing to the 
same house with different speech description usually 
indicates a repair. Furthermore, gestures also involve 
disfluencies, for example, repeatedly pointing to an object 
is a gesture repetition. Failure in identifying these 
disfluencies caused problems with reference resolution. It 
is important to have a mechanism that can identify these 
disfluencies using multimodal information.  
The remaining 5% errors came from the 
implementation of our approach in order to reduce the 
complexity of graph matching.  Currently, the referent 
graph only consists of potential referents from gestures, 
objects from the prior conversation, and the objects in the 
visual focus (i.e., highlighted on the screen). Therefore, it 
is insufficient to handle cases where users only use proper 
names (without any gestures) to refer to objects visible on 
the screen.  
From the error analysis, we learned that variations in 
user inputs (e.g., variations in vocabulary and 
synchronization patterns), disfluencies in speech utterances, 
and even small changes in the input quality or the 
environment could seriously impair the real-time 
performance. The future research effort should be devoted 
to developing adaptive approaches for reference resolution 
to deal with unexpected inputs (e.g., inputs that are outside 
of system knowledge).    
3.
Th ies in 
de rence 
re andle 
te dings 
(O tudy, 
ge were 
ut % of 
ca sture 
an rred. 
Fu poral 
5%Others
7%Disfluency
13%Out of synchronization
20%Language understanding errors
55%Speech recognition errors
Percentage
 
Table 2: The distribution of error sources 3 Design Strategies   
e evaluation also indicates three important strateg
signing effective algorithms for multimodal refe
solution. The first strategy concerns with how to h
mporal relations. Consistent with the previous fin
viatt et al 1997), in most cases (85%) in our s
stures occurred before the referring expressions 
tered. However, we did find some exceptions. In 7
ses, there was no overlap between speech and ge
d speech were uttered before gestures occu
rthermore, one user could have different tem
behavior at different stages in one interaction. In our study, 
five users exhibited varied temporal alignment during the 
interaction. Therefore, to accommodate different temporal 
variations, incorporating relative temporal relations 
between different modalities based on temporal closeness 
is preferred over incorporating absolute temporal relations 
or temporal orders.  
Second, in a multimodal conversation, the potential 
objects referred to by a user could come from different 
sources. They could be the objects gestured at, objects in 
the visual focus (e.g., highlighted), objects visible on the 
screen, or objects mentioned in a prior conversation. It is 
important for reference resolution algorithms to 
simultaneously combine semantic, temporal, and 
contextual constraints. This is particularly important for 
complex inputs that involve multiple referring expressions 
and multiple gestures as described earlier. 
Third, depending on the interface design and the 
underlying architecture for multimodal systems, different 
types of uncertainties occur during the process of input 
interpretation. For example, in our interface, each house 
icon is built on top of the town icon. Therefore, a pointing 
gesture could result in several possible objects.  Once a 
touch screen is used, a finger point may result in different 
possibilities. Furthermore, most systems like ours are 
based on the pipelined architecture as shown in Figure1. 
The pipelined processes can potentially lose low 
probability information (e.g., recognized alternatives with 
low probabilities) that could be very crucial when 
incorporated with other modalities and the interaction 
context. Therefore, it is important to retain information at 
different levels and systematically incorporate the 
imprecise information.  
4 Conclusion 
This paper presents an evaluation of graph-based 
multimodal reference resolution in a conversational system. 
The evaluation indicates that, the real-time performance is 
largely dependent on speech recognition performance, 
language processing capability, disfluency detection from 
both speech and gesture, as well as the system engineering 
issues.  Furthermore, the studies identify three important 
strategies for robust multimodal reference resolution 
algorithms: (1) using relative temporal constraints based 
on temporal closeness, (2) combining temporal, semantic, 
and contextual constraints simultaneously, and (3) 
incorporating imprecise information. A successful 
approach will need to consider both algorithmic 
requirements and technology limitations.  
Acknowledgement 
The authors would like to thank Keith Houck and    
Michelle Zhou at IBM T. J. Watson Research Center for 
their support in developing the system, and the anonymous 
reviewers for their helpful comments and suggestions. 
 
References 
Bolt, R.A. 1980. Put that there: Voice and Gesture at the 
Graphics Interface. Computer Graphics14(3): 262-270. 
Cassell, J., Bickmore, T., Billinghurst, M., Campbell, L., 
Chang, K., Vilhjalmsson, H. and Yan, H. 1999. Embodi-
ment in Conversational Interfaces: Rea. In Proceedings of 
the CHI'99 Conference, pp. 520-527. Pittsburgh, PA. 
Chai, J. Y., Hong, P., and Zhou, M. X. 2004. A probabilistic 
approach to reference resolution in multimodal user inter-
faces, Proceedings of 9th International Conference on Intel-
ligent User Interfaces (IUI): 70-77. Madeira, Portugal, 
January.  
Chai, J., Pan, S., Zhou, M., and Houck, K. 2002. Context-
based Multimodal Interpretation in Conversational Systems. 
Fourth International Conference on Multimodal Interfaces. 
Chen, L., Harper, M. and Quek, F. 2002. Gesture patterns 
during speech repairs. Proceedings of International Con-
ference on Multimodal Interfaces (ICMI).  
Cohen, P., Johnston, M., McGee, D., Oviatt, S., Pittman, J., 
Smith, I., Chen, L., and Clow, J. 1996. Quickset: Multimo-
dal Interaction for Distributed Applications. Proceedings of 
ACM Multimedia, pp. 31? 40. 
Gold, S. and Rangarajan, A. 1996. A graduated assignment 
algorithm for graph-matching. IEEE Trans. Pattern Analy-
sis and Machine Intelligence, vol. 18, no. 4. 
Gorniak, P. and Roy, D. 2003.Grounded Semantic Composi-
tion for Visual Scenes. Journal of Artificial Intelligence 
Research. 
Huls, C., Bos, E., and Classen, W. 1995. Automatic Referent 
Resolution of Deictic and Anaphoric Expressions. Compu-
tational Linguistics, 21(1):59-79. 
Johnston, M. 1998. Unification-based Multimodal parsing, 
Proceedings of COLING-ACL.  
Johnston, M. and Bangalore, S. 2000. Finite-state multimodal 
parsing and understanding. Proceedings of COLING. 
Johnston, M.,  Bangalore, S.,  Visireddy G., Stent, A., Ehlen, 
P., Walker, M., Whittaker, S., and Maloor, P. 2002. 
MATCH: An Architecture for Multimodal Dialog Systems, 
in Proceedings of ACL. 
Kehler, A. 2000. Cognitive Status and Form of Reference in 
Multimodal Human-Computer Interaction, Proceedings of 
AAAI. 
Neal, J. G., Thielman, C. Y.,  Dobes, Z. Haller, S. M., and 
Shapiro, S. C. 1998. Natural Language with Integrated 
Deictic and Graphic Gestures. Intelligent User Interfaces, 
M. Maybury and W. Wahlster (eds.), 38-51.  
Oviatt, S., DeAngeli, A., and Kuhn, K. 1997. Integration and 
Synchronization of Input Modes during Multimodal Hu-
man-Computer Interaction, In Proceedings of Conference 
on Human Factors in Computing Systems: CHI '97. 
Tsai, W.H. and Fu, K.S.  1979. Error-correcting isomorphism 
of attributed relational graphs for pattern analysis. IEEE 
Transactions on Systems, Man and Cybernetics, vol. 9, pp. 
757?768. 
Zancanaro, M., Stock, O., and Strapparava, C. 1997. Multi-
modal Interaction for Information Access: Exploiting Co-
hesion. Computational Intelligence 13(7):439-464. 
Proceedings of NAACL HLT 2007, pages 284?291,
Rochester, NY, April 2007. c?2007 Association for Computational Linguistics
An Exploration of Eye Gaze in Spoken Language Processing for Multimodal
Conversational Interfaces
Shaolin Qu Joyce Y. Chai
Department of Computer Science and Engineering
Michigan State University
East Lansing, MI 48824
{qushaoli,jchai}@cse.msu.edu
Abstract
Motivated by psycholinguistic findings,
we are currently investigating the role of
eye gaze in spoken language understand-
ing for multimodal conversational sys-
tems. Our assumption is that, during hu-
man machine conversation, a user?s eye
gaze on the graphical display indicates
salient entities on which the user?s atten-
tion is focused. The specific domain infor-
mation about the salient entities is likely
to be the content of communication and
therefore can be used to constrain speech
hypotheses and help language understand-
ing. Based on this assumption, this paper
describes an exploratory study that incor-
porates eye gaze in salience modeling for
spoken language processing. Our empiri-
cal results show that eye gaze has a poten-
tial in improving automated language pro-
cessing. Eye gaze is subconscious and in-
voluntary during human machine conver-
sation. Our work motivates more in-depth
investigation on eye gaze in attention pre-
diction and its implication in automated
language processing.
1 Introduction
Psycholinguistic experiments have shown that eye
gaze is tightly linked to human language process-
ing. Eye gaze is one of the reliable indicators of
what a person is ?thinking about? (Henderson and
Ferreira, 2004). The direction of gaze carries infor-
mation about the focus of the users attention (Just
and Carpenter, 1976). The perceived visual context
influences spoken word recognition and mediates
syntactic processing (Tanenhaus et al, 1995; Roy
and Mukherjee, 2005). In addition, directly before
speaking a word, the eyes move to the mentioned
object (Griffin and Bock, 2000).
Motivated by these psycholinguistic findings, we
are currently investigating the role of eye gaze in
spoken language understanding during human ma-
chine conversation. Through multimodal interfaces,
a user can look at a graphic display and converse
with the system at the same time. Our assumption
is that, during human machine conversation, a user?s
eye gaze on the graphical display can indicate salient
entities on which the user?s attention is focused. The
specific domain information about the salient enti-
ties is likely linked to the content of communication
and therefore can be used to constrain speech hy-
potheses and influence language understanding.
Based on this assumption, we carried out an ex-
ploration study where eye gaze information is in-
corporated in a salience model to tailor a language
model for spoken language processing. Our prelim-
inary results show that eye gaze can be useful in im-
proving spoken language processing and the effect
of eye gaze varies among different users. Because
eye gaze is subconscious and involuntary in human
machine conversation, our work also motivates sys-
tematic investigations on how eye gaze contributes
to attention prediction and its implications in auto-
mated language processing.
2 Related Work
Eye gaze has been mainly used in human machine
interaction as a pointing mechanism in direct manip-
ulation interfaces (Jacob, 1990; Jacob, 1995; Zhai
et al, 1999), as a facilitator in computer supported
human human communication (Velichkovsky, 1995;
Vertegaal, 1999); or as an additional modality dur-
ing speech or multimodal communication (Starker
and Bolt, 1990; Campana et al, 2001; Kaur et al,
284
2003; Qvarfordt and Zhai, 2005). This last area of
investigation is more related to our work.
In the context of speech and multimodal commu-
nication, studies have shown that speech and eye
gaze integration patterns can be modeled reliably for
users. For example, by studying patterns of eye gaze
and speech in the phrase ?move it there?, researchers
found that the gaze fixation closest to the intended
object begins, with high probability, before the be-
ginning of the word ?move? (Kaur et al, 2003). Re-
cent work has also shown that eye gaze has a poten-
tial to improve reference resolution in a spoken dia-
log system (Campana et al, 2001). Furthermore, eye
gaze also plays an important role in managing dia-
log in conversational systems (Qvarfordt and Zhai,
2005).
Salience modeling has been used in both natural
language and multimodal language processing. Lin-
guistic salience describes entities with their accessi-
bility in a hearer?s memory and their implications in
language production and interpretation. Linguistic
salience modeling has been used for language in-
terpretations such as reference resolution (Huls et
al., 1995; Eisenstein and Christoudias, 2004). Vi-
sual salience measures how much attention an en-
tity attracts from a user based on its visual proper-
ties. Visual salience can tailor users? referring ex-
pressions and thus can be used for multimodal refer-
ence resolution (Kehler, 2000). Our recent work has
also investigated salience modeling based on deic-
tic gestures to improve spoken language understand-
ing (Chai and Qu, 2005; Qu and Chai, 2006).
3 Data Collection
We conducted user studies to collect speech and eye
gaze data. In the experiments, a static 3D bedroom
scene was shown to the user. The system verbally
asked a user a list of questions one at a time about
the bedroom and the user answered the questions by
speaking to the system. Fig.1 shows the 14 questions
in the experiments. The user?s speech was recorded
through an open microphone and the user?s eye gaze
was captured by an Eye Link II eye tracker. From 7
users? experiments, we collected 554 utterances with
a vocabulary of 489 words. Each utterance was tran-
scribed and annotated with entities that were being
talked about in the utterance.
1 Describe this room.
2 What do you like/dislike about the arrangement?
3 Describe anything in the room that seems strange to
you.
4 Is there a bed in this room?
5 How big is the bed?
6 Describe the area around the bed.
7 Would you make any changes to the area around the
bed?
8 Describe the left wall.
9 How many paintings are there in this room?
10 Which is your favorite painting?
11 Which is your least favorite painting?
12 What is your favorite piece of furniture in the room?
13 What is your least favorite piece of furniture in the
room?
14 How would you change this piece of furniture to make
it better?
Figure 1: Questions for users in experiments
The collected raw gaze data consists of the screen
coordinates of each gaze point sampled at 4 ms.
As shown in Fig.2a, this raw data is not very use-
ful for identifying fixated entities. The raw gaze
data are processed to eliminate invalid and saccadic
gaze points, leaving only pertinent eye fixations.
Invalid gaze points occur when users look off the
screen. Saccadic gaze points occur during ballis-
tic eye movements between fixations. Vision stud-
ies have shown that no visual processing occurs dur-
ing saccades (i.e., saccadic suppression). It is well
known that eyes do not stay still, but rather make
small, frequent jerky movements. In order to best
determine fixation locations, nearby gaze points are
averaged together to identify fixations. The pro-
cessed eye gaze fixations can be seen in Fig.2b.
Fig.3 shows an excerpt of the collected speech
and gaze fixation with fixated entities. In the speech
stream, each word starts at a particular timestamp. In
the gaze stream, each gaze fixation f has a starting
timestamp tf and a duration Tf . Gaze fixations can
have different durations. An entity e on the graphi-
cal display is fixated by gaze fixation f if the area of
e contains the fixation point of f . One gaze fixation
can fall on multiple entities or no entity.
4 Salience Driven Language Modeling
Our goal is to use the domain specific information
about the salient entities on a graphical display, as
indicated by the user?s eye gaze, to help recognition
of the user?s utterances. In particular, we incorporate
this salient domain information in speech recogni-
tion via salience driven language modeling.
285
(a) Raw gaze points (b) Processed gaze fixations
Figure 2: Gaze fixations on a scene
8 596 968 1668 2096 32522692
tf Tf
[19] [ ] [17] [19] [22] [ ] [10]
[11]
[10]
[11]
[10]
[11]
This room has a chandelier
2572 2872 3170 3528 3736
( [19] ? bed_8; [17] ? lamp_2; [22] ? door_1; [10] ? bedroom; [11] ? chandelier_1 )
speech stream
gaze stream
(ms)
(ms)
[fixated entity]
f: gaze fixation
Figure 3: An excerpt of speech and gaze stream data
We first briefly introduce speech recognition. The
task of speech recognition is to, given an observed
spoken utterance O, find the word sequence W ?
such that W ? = argmax
W
p(O|W )p(W ), where
p(O|W ) is the acoustic model and p(W ) is the
language model. The acoustic model provides the
probability of observing the acoustic features given
hypothesized word sequences while the language
model provides the probability of a word sequence.
The language model is represented as:
p(W ) = p(wn1 ) =
n?
k=1
p(wk|w
k?1
1 ) (1)
Using first-order Markov assumption, the above lan-
guage model can be approximated by a bigram
model:
p(wn1 ) =
n?
k=1
p(wk|wk?1) (2)
In the following sections, we first introduce the
salience modeling based on eye gaze, then present
how the gaze-based salience models can be used to
tailor language models.
4.1 Gaze-based Salience Modeling
We first define a gaze fixation set F t0+Tt0 (e), which
contains all gaze fixations that fall on entity e within
a time window t0 ? (t0 + T ):
F t0+Tt0 (e) = {f |f falls on e within t0 ? (t0 + T )}
We model gaze-based salience in two ways.
4.1.1 Gaze Salience Model 1
Salience model 1 is based on the assumption that
when an entity has more gaze fixations on it than
other entities, this entity is more likely attended by
the user and thus has higher salience:
pt0,T (e) =
#elements in F t0+Tt0 (e)
?
e(#elements in F
t0+T
t0 (e))
(3)
Here, pt0,T (e) tells how likely it is that the user is
focusing on entity ewithin time period t0 ? (t0+T )
based on how many gaze fixations are on e among
all gaze fixations that fall on entities within t0 ?
(t0 + T ).
4.1.2 Gaze Salience Model 2
Salience model 2 is based on the assumption that
when an entity has longer gaze fixations on it than
other entities, this entity is more likely attended by
the user and thus has higher salience:
pt0,T (e) =
Dt0+Tt0 (e)
?
e D
t0+T
t0 (e)
(4)
where
Dt0+Tt0 (e) =
?
f?F
t0+T
t0
(e)
Tf (5)
Here, pt0,T (e) tells how likely it is that the user is
focusing on entity e within time period t0 ? (t0+ t)
286
based on how long e has been fixated by gaze fixa-
tions among the overall time length of all gaze fixa-
tions that fall on entities within t0 ? (t0 + T ).
4.2 Salience Driven N-gram Model
Salience models can be incorporated in different lan-
guage models, such as bigram models, class-based
bigram models, and probabilistic context free gram-
mar. Among these language models, the salience
driven bigram model based on deictic gesture has
been shown to achieve best performance on speech
recognition (Qu and Chai, 2006). In our initial in-
vestigation of gaze-based salience, we incorporate
the gaze-based salience in a bigram model.
The salience driven bigram probability is given
by:
ps(wi|wi?1) = (1 ? ?)p(wi|wi?1) +
?
?
e p(wi|wi?1, e)pt0,T (e) (6)
where pt0,T (e) is the salience distribution as mod-
eled in equations (3) and (4). In applying the
salience driven bigram model for speech recogni-
tion, we set t0 as the starting timestamp of the ut-
terance and T as the duration of the utterance. The
priming weight ? decides how much the original
bigram probability will be tailored by the salient
entities indicated by eye gaze. Currently, we set
? = 0.67 empirically. We also tried learning the
priming weight with an EM algorithm. However,
we found out that the learned priming weight per-
formed worse than the empirical one in our exper-
iments. This is probably due to insufficient devel-
opment data. Bigram probabilities p(wi|wi?1) were
estimated by the maximum likelihood estimation us-
ing Katz?s backoff method (Katz, 1987) with a fre-
quency cutoff of 1. The samemethod was used to es-
timate p(wi|wi?1, e) from the users? utterance tran-
scripts with entity annotation of e.
5 Application of Salience Driven LMs
The salience driven language models can be inte-
grated into speech processing in two stages: an early
stage before a word lattice (n-best list) is generated
(Fig.4a), or in a late stage where the word lattice
(n-best list) is post-processed (Fig.4b).
For the early stage integration, the gaze-based
salience driven language model is used together with
word lattice
(n-best list)speech
eye gaze
Speech Decoder
Language 
Model
Acoustic 
Model
(a) Early stage integration
word lattice
(n-best list) n-best list
eye gaze
Rescorer
speech
Speech Decoder
Language 
Model
Acoustic 
Model
Language 
Model
(b) Late stage integration
Figure 4: Integration of gaze-based salience driven
language model in speech processing
the acoustic model to generate the word lattice, typ-
ically by Viterbi search.
For the late stage integration, the gaze-based
salience driven language model is used to rescore the
word lattice generated by a speech recognizer with
a basic language model not involving salience mod-
eling. A* search can be applied to find the n-best
paths in the word lattice.
6 Evaluation
The evaluations were conducted on data collected
from user studies (Sec. 3). We evaluated the gaze-
based salience driven bigram models when applied
for speech recognition at early and late stages.
6.1 Evaluation Results
Users? speech was first segmented, then recognized
by the CMU Sphinx-4 speech recognizer using dif-
ferent language models. Evaluation was done by
a 14-fold cross validation. We compare the per-
formances of the early and late applications of two
gaze-based salience driven language models:
? S-Bigram1 ? salience driven language model
based on salience modeling 1 (Sec. 4.1.1)
? S-Bigram2 ? salience driven language model
based on salience modeling 2 (Sec. 4.1.2)
Table 1 and Table 2 show the results of early and
late application of the salience driven language mod-
els based on eye gaze. We can see that all word error
rates (WERs) are high. In the experiments, users
were instructed to only answer systems questions
one by one. There was no flow of a real conversa-
tion. In this setting, users were more free to express
287
themselves than in the situation where users believed
they were conversing with a machine. Thus, we ob-
serve much longer sentences that often contain dis-
fluencies. Here is one example:
System: ?How big is the bed??
User: ?I would to have to offer a guess that the bed,
if I look the chair that?s beside it [pause] in a rel-
ative angle to the bed, it?s probably six feet long,
possibly, or shorter, slightly shorter.?
The high WER was mainly caused by the com-
plexity and disfluencies of users? speech. Poor
speech recording quality is another reason for the
bad recognition performance. It was found that
the trigram model performed worse than the bigram
model in the experiment. This is probably due to the
sparseness of trigrams in the corpus. The amount of
data available is too small considering the vocabu-
lary size.
Language Model Lattice-WER WER
Bigram 0.613 0.707
Trigram 0.643 0.719
S-Bigram 1 0.605 0.690
S-Bigram 2 0.604 0.689
Table 1: WER of early application of LMs
Language Model Lattice-WER WER
S-Bigram 1 0.643 0.709
S-Bigram 2 0.643 0.710
Table 2: WER of late application of LMs
The S-Bigram1 and S-Bigram2 achieved similar
results in both early application (Table 1) and late
application (Table 2). In early application, the S-
Bigram1 model performed better than the trigram
model (t = 5.24, p < 0.001, one-tailed) and the
bigram model (t = 3.31, p < 0.001, one-tailed).
The S-Bigram2model also performed better than the
trigram model (t = 5.15, p < 0.001, one-tailed)
and the bigram model (t = 3.33, p < 0.001, one-
tailed) in early application. In late application, the
S-Bigram1 model performed better than the trigram
model (t = 2.11, p < 0.02, one-tailed), so did
the S-Bigram2 model (t = 1.99, p < 0.025, one-
tailed). However, compared to the bigram model,
the S-Bigram1 model did not change the recogni-
tion performance significantly (t = 0.38, N.S., two-
tailed) in late application, neither did the S-Bigram2
model (t = 0.50, N.S., two-tailed).
We also compare performances of the salience
driven language models for individual users. In early
application (Fig.5a), both the S-Bigram1 and the S-
Bigram2 model performed better than the baselines
of the bigram and trigrammodels for all users except
user 2 and user 7. T-tests have shown that these are
significant improvements. For user 2, the S-Bigram1
model achieved the sameWER as the bigrammodel.
For user 7, neither of the salience driven language
models improved recognition compared to the bi-
gram model. In late application (Fig.5b), only for
user 3 and user 4, both salience driven language
models performed better than the baselines of the bi-
gram and trigrammodels. These improvements have
also been confirmed by t-tests as significant.
1 2 3 4 5 6 70.4
0.5
0.6
0.7
0.8
0.9
1
User ID
WE
R
bigram trigram s?bigram1 s?bigram2
(a) WER of early application
1 2 3 4 5 6 70.4
0.5
0.6
0.7
0.8
0.9
1
User ID
WE
R
bigram trigram s?bigram1 s?bigram2
(b) WER of Late application
Figure 5: WERs of LMs for individual users
Comparing early and late application of the
salience driven language models, it is observed that
early application performed better than late applica-
tion for all users except user 3 and user 4. T-tests
have confirmed that these differences are significant.
288
It is interesting to see that the effect of gaze-based
salience modeling is different among users. For
two users (i.e., user 3 and user 4), the gaze-based
salience driven language models consistently out-
performed the bigram and trigram models in both
early application and late application. However, for
some other users (e.g., user 7), this is not the case. In
fact, the gaze-based salience driven language mod-
els performed worse than the bigram model. This
observation indicates that during language produc-
tion, a user?s eye gaze is voluntary and unconscious.
This is different from deictic gesture, which is more
intentionally delivered by a user. Therefore, incor-
porating this ?unconscious? mode of modality in
salience modeling requires more in-depth research
on the role of eye gaze in attention prediction during
multimodal human computer interaction.
6.2 Discussion
Gaze-based salience driven language models are
built on the assumption that when a user is fixat-
ing on an entity, the user is saying something re-
lated to the entity. With this assumption, gaze-based
salience driven language models have the potential
to improve speech recognition by biasing the speech
decoder to favor the words that are consistent with
the entity indicated by the user?s eye gaze, especially
when the user?s utterance contains words describing
unique characteristics of the entity. These particular
characteristics could be the entity?s name or physical
properties (e.g., color, material, size).
Utterance: ?a tree growing from the floor?
Gaze salience:
p(bedroom) = 0.2414 p(plant willow) = 0.2414
p(chair soft) = 0.2414 p(door 1) = 0.1378
p(bed 8) = 0.1378
Bigram n-best list:
sheet growing from a four
sheet growing from a for
sheet growing from a floor
. . .
S-Bigram2 n-best list:
a tree growing from the floor
a tree growing from the for
a tree growing from the floor a
. . .
Figure 6: N-best lists of utterance ?a tree growing
from the floor?
Fig.6 shows an example where the S-Bigram2
model in early application improved recognition of
the utterance ?a tree growing from the floor?. In
this example, the user?s gaze fixations accompany-
ing the utterance resulted in a list of candidate enti-
ties with fixating probabilities (cf. Eqn. (4)), among
which entities bedroom and plant willow were as-
signed higher probabilities. Two n-best lists, the Bi-
gram n-best list and the S-Bigram2 n-best list, were
generated by the speech recognizer when the bigram
model and the S-Bigram2 model were applied sep-
arately. The speech recognizer did not get the cor-
rect recognition when the bigram model was used,
but got the correct result when the S-Bigram2 model
was used.
Fig.7a and 7b show the word lattices of the ut-
terance generated by the speech recognizer using
the bigram model and the S-Bigram2 model respec-
tively. The n-best lists in Fig.6 were generated from
those word lattices. In the word lattices, each path
going from the start node<s> to the end node</s>
forms a recognition hypothesis. The bigram proba-
bilities along the edges are in the logarithm of base
10. In the bigram case, the path ?<s> a tree? has a
higher language score (summation of bigram prob-
abilities along the path) than ?<s> sheet?, and ?a
floor? has a higher language score than ?a full?.
However, these correct paths ?<s> a tree? and ?a
floor? (not exactly correct, but better than ?a full?)
do not appear in the best hypothesis in the result-
ing n-best list. This is because the system tries to
find an overall best hypothesis by considering both
language and acoustic score. Because of the noisy
speech, the incorrect hypotheses may happen to have
higher acoustic confidence than the correct ones. Af-
ter tailoring the bigram model with gaze salience,
the salient entity plant willow significantly increases
the probability of ?a tree? (from -1.3594 to -0.9913)
and ?tree growing? (from -3.1009 to -1.1887), while
it decreases the probability of ?sheet growing? (from
-3.0962 to -3.4534). This probability change is made
by the entity conditional probability p(wi|wi?1, e)
in tailoring of bigram by salience (cf. Eqn. (6)).
Probability p(wi|wi?1, e), trained from the anno-
tated utterances, reflects what words are more likely
to be spoken by a user while talking about an entity
e. The increased probabilities of ?a tree? and ?tree
growing? show that word ?tree? appears more likely
than ?sheet? when the user is talking about entity
289
</s>
i
-1.5043
forest
-0.8615
floor -0.3552
four
-0.5322
for
-0.9768
full
-1.9490
a
-3.1284
-2.8274
-3.0035
-2.9066
-3.0035of
-3.3940
-3.2691
-1.0280
from
-3.6386
-3.3376
-3.5137
-3.4168
-3.5137
-1.9339
kind
-0.2312
growing
-2.2662
-3.2942
going
-1.5911sheet
-2.4272
-3.0962
tree -3.1009
-3.5780
a
-1.3594
<s>
-3.9306
-3.0275
-1.5987
(a) Word lattice with bigram model
</s>
a
-1.2861
floor
-0.2570
-1.9165
forest
-0.7782
for
-1.2683
a
-2.4966
-2.9278
-3.6009
the
-1.2151
-3.2468
-3.7353
further
-3.6961
from
-3.9011
-4.2022
-3.3477
-1.9934
-0.1622
growing
-3.4626
-3.6233
tree
-1.1887
sheet
-3.4534
a
-0.9913
<s>
-2.3655
-3.8964
-1.5618
(b) Word lattice with S-Bigram 2
Figure 7: Word lattices of utterance ?a tree growing from the floor?
?plant willow. This is in accordance with our com-
mon sense. Likewise, the salient entity bedroom, of
which floor is a component, makes the probability of
the correct hypothesis ?the floor? much higher than
other hypotheses (?the for? and ?the forest?). These
enlarged language score differences make the cor-
rect hypotheses ?a tree? and ?the floor? win out in
the searching procedure despite the noisy speech.
Utterance: ?I like the picture with like a forest in it?
Gaze salience:
p(bedroom) = 0.5960 p(chandelier 1) = 0.4040
Bigram n-best list:
and i eight that picture rid like got five
and i eight that picture rid identifiable
and i eight that picture rid like got forest
. . .
S-Bigram2 n-best list:
and i that bedroom it like upside
and i that bedroom it like a five
and i that bedroom it like a forest
. . .
Figure 8: N-best lists of utterance ?I like the picture
with like a forest in it?
Unlike the active input mode of deictic gesture,
eye gaze is a passive input mode. The salience in-
formation indicated by eye gaze is not as reliable
as the one indicated by deictic gesture. When the
salient entities indicated by eye gaze are not the
true entities the user is referring to, the salience
driven language model can worsen speech recogni-
tion. Fig.8 shows an example where the S-Bigram2
model in early application worsened the recogni-
tion of a user?s utterance ?I like the picture with like
a forest in it? because of wrong salience informa-
tion. In this example, the user was talking about a
picture entity picture bamboo. However, this entity
was not salient, only entities bedroom and chande-
lier 1 were salient. As a result, the recognition with
the S-Bigram2 model becomes worse than the base-
line. The correct word ?picture? is missing and the
wrong word ?bedroom? appears in the result.
The failure to identify the actual referred entity
picture bamboo as salient in the above example can
also be caused by the visual properties of entities.
Smaller entities on the screen are harder to be fix-
290
ated by eye gaze than larger entities. To address this
issue, more reliable salience modeling that takes into
account the visual features is needed.
7 Conclusion
This paper presents an empirical exploration of in-
corporating eye gaze in spoken language processing
via salience driven language modeling. Our prelim-
inary results have shown the potential of eye gaze in
improving spoken language processing. Neverthe-
less, this exploratory study is only the first step in
our investigation. Many interesting research ques-
tions remain. During human machine conversation,
how is eye gaze aligned with speech production?
How reliable is eye gaze for attention prediction?
Are there any other factors such as interface design
and visual properties that will affect eye gaze behav-
ior and therefore attention prediction? The answers
to these questions will affect how eye gaze should be
appropriately modeled and used for language pro-
cessing.
Eye-tracking systems are no longer bulky, sta-
tionary systems that prevent natural human ma-
chine communication. Recently developed dis-
play mounted gaze-tracking systems (e.g., Tobii) are
completely non-intrusive, can tolerate head motion,
and provide high tracking quality. These features
have been demonstrated in several successful appli-
cations (Duchowski, 2002). Integrating eye tracking
with conversational interfaces is no longer beyond
reach. We believe it is time to conduct systematic
investigations and fully explore the additional chan-
nel provided by eye gaze in improving robustness of
human machine conversation.
8 Acknowledgments
This work was supported by a Career Award IIS-
0347548 and IIS-0535112 from the National Sci-
ence Foundation. The authors would like to thank
Zahar Prasov for his contribution on data collection
and thank anonymous reviewers for their valuable
comments and suggestions.
References
E. Campana, J. Baldridge, J. Dowding, B. Hockey, R. Reming-
ton, and L. Stone. 2001. Using eye movements to determine
referents in a spoken dialogue system. In Proceedings of the
Workshop on Perceptive User Interface.
J. Chai and S. Qu. 2005. A salience driven approach to ro-
bust input interpretation in multimodal conversational sys-
tems. In Proceedings of HLT/EMNLP?05.
A. T. Duchowski. 2002. A breath-first survey of eye tracking
applications. Behavior Research methods, Instruments, and
Computers, 33(4).
J. Eisenstein and C. M. Christoudias. 2004. A salience-based
approach to gesture-speech alignment. In Proceedings of
HLT/NAACL?04.
Z. M. Griffin and K. Bock. 2000. What the eyes say about
speaking. Psychological Science, 11:274?279.
J. M. Henderson and F. Ferreira. 2004. The interface of lan-
guage, vision, and action: Eye movements and the visual
world. New York: Taylor & Francis.
C. Huls, E. Bos, and W. Classen. 1995. Automatic referent res-
olution of deictic and anaphoric expressions. Computational
Linguistics, 21(1):59?79.
R. J. K. Jacob. 1990. What you look is what you get: Eye
movement-based interaction techniques. In Proceedings of
CHI?90.
R. J. K. Jacob. 1995. Eye tracking in advanced interface design.
In W. Barfield and T. Furness, editors, Advanced Interface
Design and Virtual Environments, pages 258?288. Oxford
University Press.
M. Just and P. Carpenter. 1976. Eye fixations and cognitive
processes. Cognitive Psychology, 8:441?480.
S. Katz. 1987. Estimation of probabilities from sparse data for
the language model component of a speech recogniser. IEEE
Trans. Acous., Speech and Sig. Processing, 35(3):400?401.
M. Kaur, M. Termaine, N. Huang, J. Wilder, Z. Gacovski,
F. Flippo, and C. S. Mantravadi. 2003. Where is ?it?? event
synchronization in gaze-speech input systems. In Proceed-
ings of ICMI?03.
A. Kehler. 2000. Cognitive status and form of reference in
multimodal human-computer interaction. In Proceedings of
AAAI?00.
S. Qu and J. Chai. 2006. Salience modeling based on non-
verbal modalities for spoken language understanding. In
Proceedings of ICMI?06.
P. Qvarfordt and S. Zhai. 2005. Conversing with the user based
on eye-gaze patterns. In Proceedings of CHI?05.
D. Roy and N. Mukherjee. 2005. Towards situated speech
understanding: Visual context priming of language models.
Computer Speech and Language, 19(2):227?248.
I. Starker and R. A. Bolt. 1990. A gaze-responsive self-
disclosing display. In Proceedings of CHI?90.
M. K. Tanenhaus, M. J. Spivey-Knowlton, K. M. Eberhard,
and J. E. Sedivy. 1995. Integration of visual and linguis-
tic information in spoken language comprehension. Science,
268:1632?1634.
B. M. Velichkovsky. 1995. Communicating attention-gaze po-
sition transfer in cooperative problem solving. Pragmatics
and Cognition, 3:99?224.
R. Vertegaal. 1999. The gaze groupware system: Mediating
joint attention in multiparty communication and collabora-
tion. In Proceedings of CHI?99.
S. Zhai, C. Morimoto, and S. Ihde. 1999. Manual and gaze
input cascaded (magic) pointing. In Proceedings of CHI?99.
291
Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 146?154,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
The Role of Implicit Argumentation in Nominal SRL
Matt Gerber
Dept. of Computer Science
Michigan State University
gerberm2@msu.edu
Joyce Y. Chai
Dept. of Computer Science
Michigan State University
jchai@cse.msu.edu
Adam Meyers
Dept. of Computer Science
New York University
meyers@cs.nyu.edu
Abstract
Nominals frequently surface without overtly
expressed arguments. In order to measure the
potential benefit of nominal SRL for down-
stream processes, such nominals must be ac-
counted for. In this paper, we show that a
state-of-the-art nominal SRL system with an
overall argument F1 of 0.76 suffers a perfor-
mance loss of more than 9% when nominals
with implicit arguments are included in the
evaluation. We then develop a system that
takes implicit argumentation into account, im-
proving overall performance by nearly 5%.
Our results indicate that the degree of implicit
argumentation varies widely across nominals,
making automated detection of implicit argu-
mentation an important step for nominal SRL.
1 Introduction
In the past few years, a number of studies have
focused on verbal semantic role labeling (SRL).
Driven by annotation resources such as FrameNet
(Baker et al, 1998) and PropBank (Palmer et al,
2005), many systems developed in these studies
have achieved argument F1 scores near 80% in
large-scale evaluations such as the one reported by
Carreras and Ma`rquez (2005).
More recently, the automatic identification of
nominal argument structure has received increased
attention due to the release of the NomBank cor-
pus (Meyers, 2007a). NomBank annotates predicat-
ing nouns in the same way that PropBank annotates
predicating verbs. Consider the following example
of the verbal predicate distribute from the PropBank
corpus:
(1) Freeport-McMoRan Energy Partners will be
liquidated and [Arg1 shares of the new
company] [Predicate distributed] [Arg2 to the
partnership?s unitholders].
The NomBank corpus contains a similar instance of
the deverbal nominalization distribution:
(2) Searle will give [Arg0 pharmacists] [Arg1
brochures] [Arg1 on the use of prescription
drugs] for [Predicate distribution] [Location in
their stores].
This instance demonstrates the annotation of split ar-
guments (Arg1) and modifying adjuncts (Location),
which are also annotated in PropBank. In cases
where a nominal has a verbal counterpart, the inter-
pretation of argument positions Arg0-Arg5 is con-
sistent between the two corpora.
In addition to deverbal (i.e., event-based) nomi-
nalizations, NomBank annotates a wide variety of
nouns that are not derived from verbs and do not de-
note events. An example is given below of the parti-
tive noun percent:
(3) Hallwood owns about 11 [Predicate %] [Arg1 of
Integra].
In this case, the noun phrase headed by the predicate
% (i.e., ?about 11% of Integra?) denotes a fractional
part of the argument in position Arg1.
Since NomBank?s release, a number of studies
have applied verbal SRL techniques to the task of
nominal SRL. For example, Liu and Ng (2007) re-
ported an argument F1 of 0.7283. Although this
result is encouraging, it does not take into account
nominals that surface without overt arguments. Con-
sider the following example:
(4) The [Predicate distribution] represents [NP
available cash flow] [PP from the partnership]
[PP between Aug. 1 and Oct. 31].
146
As in (2), distribution in (4) has a noun phrase and
multiple prepositional phrases in its environment,
but not one of these constituents is an argument to
distribution in (4); rather, any arguments are implic-
itly supplied by the surrounding discourse. As de-
scribed by Meyers (2007a), instances such as (2) are
called ?markable? because they contain overt argu-
ments, and instances such as (4) are called ?unmark-
able? because they do not. In the NomBank corpus,
only markable instances have been annotated.
Previous evaluations (e.g., those by Jiang and
Ng (2006) and Liu and Ng (2007)) have been based
on markable instances, which constitute 57% of all
instances of nominals from the NomBank lexicon.
In order to use nominal SRL systems for down-
stream processing, it is important to develop and
evaluate techniques that can handle markable as well
as unmarkable nominal instances. To address this
issue, we investigate the role of implicit argumenta-
tion for nominal SRL. This is, in part, inspired by the
recent CoNLL Shared Task (Surdeanu et al, 2008),
which was the first evaluation of syntactic and se-
mantic dependency parsing to include unmarkable
nominals. In this paper, we extend this task to con-
stituent parsing with techniques and evaluations that
focus specifically on implicit argumentation in nom-
inals.
We first present our NomBank SRL system,
which improves the best reported argument F1 score
in the markable-only evaluation from 0.7283 to
0.7630 using a single-stage classification approach.
We show that this system, when applied to all nomi-
nal instances, achieves an argument F1 score of only
0.6895, a loss of more than 9%. We then present
a model of implicit argumentation that reduces this
loss by 46%, resulting in an F1 score of 0.7235 on
the more complete evaluation task. In our analyses,
we find that SRL performance varies widely among
specific classes of nominals, suggesting interesting
directions for future work.
2 Related work
Nominal SRL is related to nominal relation interpre-
tation as evaluated in SemEval (Girju et al, 2007).
Both tasks identify semantic relations between a
head noun and other constituents; however, the tasks
focus on different relations. Nominal SRL focuses
primarily on relations that hold between nominaliza-
tions and their arguments, whereas the SemEval task
focuses on a range of semantic relations, many of
which are not applicable to nominal argument struc-
ture.
Early work in identifying the argument struc-
ture of deverbal nominalizations was primarily rule-
based, using rule sets to associate syntactic con-
stituents with semantic roles (Dahl et al, 1987;
Hull and Gomez, 1996; Meyers et al, 1998). La-
pata (2000) developed a statistical model to classify
modifiers of deverbal nouns as underlying subjects
or underlying objects, where subject and object de-
note the grammatical position of the modifier when
linked to a verb.
FrameNet and NomBank have facilitated machine
learning approaches to nominal argument struc-
ture. Gildea and Jurafsky (2002) presented an early
FrameNet-based SRL system that targeted both ver-
bal and nominal predicates. Jiang and Ng (2006)
and Liu and Ng (2007) have tested the hypothe-
sis that methodologies and representations used in
PropBank SRL (Pradhan et al, 2005) can be ported
to the task of NomBank SRL. These studies report
argument F1 scores of 0.6914 and 0.7283, respec-
tively. Both studies also investigated the use of fea-
tures specific to the task of NomBank SRL, but ob-
served only marginal performance gains.
NomBank argument structure has also been used
in the recent CoNLL Shared Task on Joint Parsing
of Syntactic and Semantic Dependencies (Surdeanu
et al, 2008). In this task, systems were required to
identify syntactic dependencies, verbal and nominal
predicates, and semantic dependencies (i.e., argu-
ments) for the predicates. For nominals, the best se-
mantic F1 score was 0.7664 (Surdeanu et al, 2008);
however this score is not directly comparable to the
NomBank SRL results of Liu and Ng (2007) or the
results in this paper due to a focus on different as-
pects of the problem (see the end of section 5.2 for
details).
3 NomBank SRL
Given a nominal predicate, an SRL system attempts
to assign surrounding spans of text to one of 23
classes representing core arguments, adjunct argu-
ments, and the null or non-argument. Similarly to
147
verbal SRL, this task is traditionally formulated as
a two-stage classification problem over nodes in the
syntactic parse tree of the sentence containing the
predicate.1 In the first stage, each parse tree node is
assigned a binary label indicating whether or not it
is an argument. In the second stage, argument nodes
are assigned one of the 22 non-null argument types.
Spans of text subsumed by labeled parse tree nodes
constitute arguments of the predication.
3.1 An improved NomBank SRL baseline
To investigate the effects of implicit argumenta-
tion, we first developed a system based on previ-
ous markable-only approaches. Our system follows
many of the traditions above, but differs in the fol-
lowing ways. First, we replace the standard two-
stage pipeline with a single-stage logistic regression
model2 that predicts arguments directly. Second,
we model incorporated arguments (i.e., predicates
that are also arguments) with a simple maximum
likelihood model that predicts the most likely argu-
ment label for a predicate based on counts from the
training data. Third, we use the following heuris-
tics to resolve argument conflicts: (1) If two argu-
ments overlap, the one with the higher probability is
kept. (2) If two non-overlapping arguments are of
the same type, the one with the higher probability
is kept unless the two nodes are siblings, in which
case both are kept. Heuristic (2) accounts for split
argument constructions.
Our NomBank SRL system uses features that are
selected with a greedy forward search strategy sim-
ilar to the one used by Jiang and Ng (2006). The
top half of Table 2 (next page) lists the selected ar-
gument features.3 We extracted training nodes from
sections 2-21 of NomBank, used section 24 for de-
velopment and section 23 for testing. All parse
trees were generated by Charniak?s re-ranking syn-
tactic parser (Charniak and Johnson, 2005). Follow-
ing the evaluation methodology used by Jiang and
Ng (2006) and Liu and Ng (2007), we obtained sig-
1The syntactic parse can be based on ground-truth annota-
tion or derived automatically, depending on the evaluation.
2We use LibLinear (Fan et al, 2008).
3For features requiring the identification of support verbs,
we use the annotations provided in NomBank. Preliminary ex-
periments show a small loss when using automatic support verb
identification.
Dev. F1 Testing F1
Jiang and Ng (2006) 0.6677 0.6914
Liu and Ng (2007) - 0.7283
This paper 0.7454 0.7630
Table 1: Markable-only NomBank SRL results for ar-
gument prediction using automatically generated parse
trees. The f-measure statistics were calculated by ag-
gregating predictions across all classes. ?-? indicates
that the result was not reported.
Markable-only All-token % loss
P 0.7955 0.6577 -17.32
R 0.7330 0.7247 -1.13
F1 0.7630 0.6895 -9.63
Table 3: Comparison of the markable-only and all-
token evaluations of the baseline argument model.
nificantly better results, as shown in Table 1 above.4
3.2 The effect of implicit nominal arguments
The presence of implicit nominal arguments
presents challenges that are not taken into account
by the evaluation described above. To assess the im-
pact of implicit arguments, we evaluated our Nom-
Bank SRL system over each token in the testing
section. The system attempts argument identifica-
tion for all singular and plural nouns that have at
least one annotated instance in the training portion
of the NomBank corpus (morphological variations
included).
Table 3 gives a comparison of the results from the
markable-only and all-token evaluations. As can be
seen, assuming that all known nouns take overt argu-
ments results in a significant performance loss. This
loss is due primarily to a drop in precision caused by
false positive argument predictions made for nomi-
nals with implicit arguments.
4 Accounting for implicit arguments in
nominal SRL
A natural solution to the problem described above
is to first distinguish nominals that bear overt
arguments from those that do not. We treat this
4As noted by Carreras and Ma`rquez (2005), the discrepancy
between the development and testing results is likely due to
poorer syntactic parsing performance on the development sec-
tion.
148
A
rg
u
m
en
tf
ea
tu
re
s
# Description N S
1 12 & parse tree path from n to pred
2 Position of n relative to pred & parse tree path from n to pred *
3 First word subsumed by n
4 12 & position of n relative to pred
5 12 & 14
6 Head word of n?s parent *
7 Last word subsumed n
8 n?s syntactic category & length of parse tree path from n to pred
9 First word of n?s right sibling * *
10 Production rule that expands the parent of pred
11 Head word of the right-most NP in n if n is a PP *
12 Stem of pred
13 Parse tree path from n to the lowest common ancestor of n and pred
14 Head word of n
15 12 & n?s syntactic category
16 Production rule that expands n?s parent * *
17 Parse tree path from n to the nearest support verb *
18 Last part of speech (POS) subsumed by n *
19 Production rule that expands n?s left sibling *
20 Head word of n, if the parent of n is a PP
21 The POS of the head word of the right-most NP under n if n is a PP
... Features 22-31 are available upon request 0 3
N
o
m
in
al
fe
at
u
re
s
1 n?s ancestor subcategorization frames (ASF) (see section 4) *
2 n?s word
3 Syntactic category of n?s right sibling
4 Parse tree paths from n to each support verb *
5 Last word of n?s left sibling * *
6 Parse tree path from n to previous nominal, with lexicalized source (see section 4) *
7 Last word of n?s right sibling *
8 Production rule that expands n?s left sibling * *
9 Syntactic category of n *
10 PropBank markability score (see section 4) *
11 Parse tree path from n to previous nominal, with lexicalized source and destination *
12 Whether or not n is followed by PP *
13 Parse tree path from n to previous nominal, with lexicalized destination *
14 Head word of n?s parent *
15 Whether or not n surfaces before a passive verb * *
16 First word of n?s left sibling *
17 Parse tree path from n to closest support verb, with lexicalized destination *
18 Whether or not n is a head *
19 Head word of n?s right sibling
20 Production rule that expands n?s parent * *
21 Parse tree paths from n to all support verbs, with lexicalized destinations *
22 First word of n?s right sibling * *
23 Head word of n?s left sibling *
24 If n is followed by a PP, the head of that PP?s object *
25 Parse tree path from n to previous nominal *
26 Token distance from n to previous nominal *
27 Production rule that expands n?s grandparent *
Table 2: Features, sorted by gain in selection algorithm. & denotes concatenation. The last two columns indicate
(N)ew features (not used in Liu and Ng (2007)) and features (S)hared by the argument and nominal models.
149
as a binary classification task over token nodes.
Once a nominal has been identified as bearing
overt arguments, it is processed with the argument
identification model developed in the previous
section. To classify nominals, we use the features
shown in the bottom half of Table 2, which were
selected with the same algorithm used for the
argument classification model. As shown by Table
2, the sets of features selected for argument and
nominal classification are quite different, and many
of the features used for nominal classification have
not been previously used. Below, we briefly explain
a few of these features.
Ancestor subcategorization frames (ASF)
As shown in Table 2, the most informative feature
is ASF. For a given token t, ASF is actually a set
of sub-features, one for each parse tree node above
t. Each sub-feature is indexed (i.e., named) by its
distance from t. The value of an ASF sub-feature
is the production rule that expands the correspond-
ing node in the tree. An ASF feature with two
sub-features is depicted below for the token ?sale?:
VP: ASF2 = V P ? V,NP
V (made) NP: ASF1 = NP ? Det,N
Det (a) N (sale)
Parse tree path lexicalization A lexicalized parse
tree path is one in which surface tokens from the
beginning or end of the path are included in the path.
This is a finer-grained version of the traditional
parse tree path that captures the joint behavior of
the path and the tokens it connects. For example,
in the tree above, the path from ?sale? to ?made?
with a lexicalized source and destination would be
sale : N ? NP ? V P ? V : made. Lexicalization
increases sparsity; however, it is often preferred
by the feature selection algorithm, as shown in the
bottom half of Table 2.
PropBank markability score This feature is
the probability that the context (? 5 words) of a de-
verbal nominal is generated by a unigram language
model trained over the PropBank argument words
for the corresponding verb. Entities are normalized
Precision Recall F1
Baseline 0.5555 0.9784 0.7086
MLE 0.6902 0.8903 0.7776
LibLinear 0.8989 0.8927 0.8958
Table 4: Evaluation results for identifying nominals
with explicit arguments.
to their entity type using BBN?s IdentiFinder, and
adverbs are normalized to their related adjective us-
ing the ADJADV dictionary provided by NomBank.
The normalization of adverbs is motivated by the
fact that adverbial modifiers of verbs typically have
a corresponding adjectival modifier for deverbal
nominals.
5 Evaluation results
Our evaluation methodology reflects a practical sce-
nario in which the nominal SRL system must pro-
cess each token in a sentence. The system can-
not safely assume that each token bears overt argu-
ments; rather, this decision must be made automat-
ically. In section 5.1, we present results for the au-
tomatic identification of nominals with overt argu-
ments. Then, in section 5.2, we present results for
the combined task in which nominal classification is
followed by argument identification.
5.1 Nominal classification
Following standard practice, we train the nomi-
nal classifier over NomBank sections 2-21 using
LibLinear and automatically generated syntactic
parse trees. The prediction threshold is set to the
value that maximizes the nominal F1 score on
development section (24), and the resulting model
is tested over section 23. For comparison, we
implemented the following simple classifiers.
Baseline nominal classifier Classifies a token
as overtly bearing arguments if it is a singular or
plural noun that is markable in the training data.
As shown in Table 4, this classifier achieves nearly
perfect recall.5
MLE nominal classifier Operates similarly to
5Recall is less than 100% due to (1) part-of-speech errors
from the syntactic parser and (2) nominals that were not anno-
tated in the training data but exist in the testing data.
150
00.01
0.02
0.03
0.04
0.05
0.06
0.07
0.08
0.09
0.1
0.0
5 0.1 0.1
5 0.2 0.2
5 0.3
(0.2
5) 0
.
35 0.4 0.4
5 0.5 0.5
5 0.6
(0.5
) 0.
65 0.7 0.7
5
(0.7
5) 0
.
8
0.8
5 0.9 0.9
5 1
Observed markable probability
%
 
o
f n
o
m
in
al
 
in
st
an
ce
s
(a) Distribution of nominals. Each interval on the x-axis denotes a set of nominals that are markable between (x?5)%
and x% of the time in the training data. The y-axis denotes the percentage of all nominal instances in TreeBank that
is occupied by nominals in the interval. Quartiles are marked below the intervals. For example, quartile 0.25 indicates
that one quarter of all nominal instances are markable 35% of the time or less.
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0.0
5 0.1 0.1
5 0.2 0.2
5 0.3 0.3
5 0.4 0.4
5 0.5 0.5
5 0.6 0.6
5 0.7 0.7
5 0.8 0.8
5 0.9 0.9
5 1
Observed markable probability
Pr
ed
ic
at
e 
n
o
m
in
al
 
F1
Baseline
LibLinear
(b) Nominal classification performance with respect to the
distribution in Figure 1a. The y-axis denotes the combined
F1 for nominals in the interval.
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0.0
5 0.1 0.1
5 0.2 0.2
5 0.3 0.3
5 0.4 0.4
5 0.5 0.5
5 0.6 0.6
5 0.7 0.7
5 0.8 0.8
5 0.9 0.9
5 1
Observed markable probability
A
rg
u
m
en
t F
1
Baseline
MLE
LibLinear
(c) All-token argument classification performance with re-
spect to the distribution in Figure 1a. The y-axis denotes the
combined F1 for nominals in the interval.
Figure 1: Evaluation results with respect to the distribution of nominals in TreeBank.
the baseline classifier, but also produces a score
for the classification. The value of the score is
equal to the probability that the nominal bears overt
arguments, as observed in the training data. A
prediction threshold is imposed on this score as
determined by the development data (t = 0.23).
As shown by Table 4, this exchanges recall for
precision and leads to a significant increase in the
overall F1 score.
The last row in Table 4 shows the results for
the LibLinear nominal classifier, which significantly
outperforms the others, achieving balanced preci-
sion and recall scores near 0.9. In addition, it is
able to recover from part-of-speech errors because
it does not filter out non-noun instances; rather, it
combines part-of-speech information with other lex-
ical and syntactic features to classify nominals.
Interesting observations can be made by grouping
nominals according to the probability with which
they are markable in the corpus. Figure 1a gives
the overall distribution of markable nominals in the
training data. As shown, 50% of nominal instances
are markable only 65% of the time or less, making
nominal classification an important first step. Using
this view of the data, Figure 1b presents the over-
all F1 scores for the baseline and LibLinear nominal
151
classifiers.6 As expected, gains in nominal classi-
fication diminish as nominals become more overtly
associated with arguments. Furthermore, nominals
that are rarely markable (i.e., those in interval 0.05)
remain problematic due to a lack of positive training
instances and the unbalanced nature of the classifi-
cation task.
5.2 Combined nominal-argument classification
We now turn to the task of combined nominal-
argument classification. In this task, systems must
first identify nominals that bear overt arguments. We
evaluated three configurations based on the nominal
classifiers from the previous section. Each config-
uration uses the argument classification model from
section 3.
As shown in Table 3, overall argument classifi-
cation F1 suffers a loss of more than 9% under the
assumption that all known nouns bear overt argu-
ments. This corresponds precisely to using the base-
line nominal classifier in the combined nominal-
argument task. The MLE nominal classifier is able
to reduce this loss by 25% to an F1 of 0.7080. The
LibLinear nominal classifier reduces this loss by
46%, resulting in an overall argument classification
F1 of 0.7235. This improvement is the direct result
of filtering out nominal instances that do not bear
overt arguments.
Similarly to the nominal evaluation, we can view
argument classification performance with respect to
the probability that a nominal bears overt arguments.
This is shown in Figure 1c for the three configura-
tions. The configuration using the MLE nominal
classifier obtains an argument F1 of zero for nom-
inals below its prediction threshold. Compared to
the baseline nominal classifier, the LibLinear clas-
sifier achieves argument classification gains as large
as 150.94% (interval 0.05), with an average gain of
52.87% for intervals 0.05 to 0.4. As with nomi-
nal classification, argument classification gains di-
minish for nominals that express arguments more
overtly - we observe an average gain of only 2.15%
for intervals 0.45 to 1.00. One possible explana-
tion for this is that the argument prediction model
has substantially more training data for the nomi-
nals in intervals 0.45 to 1.00. Thus, even if the nom-
6Baseline and MLE are identical above the MLE threshold.
Nominals
Deverbal Deverbal-like Other
Baseline 0.7975 0.6789 0.6757
MLE 0.8298 0.7332 0.7486
LibLinear 0.9261 0.8826 0.8905
Arguments
Baseline 0.7059 0.6738 0.7454
MLE 0.7206 0.6641 0.7675
LibLinear 0.7282 0.7178 0.7847
Table 5: Nominal and argument F1 scores for dever-
bal, deverbal-like, and other nominals in the all-token
evaluation.
inal classifier makes a false positive prediction in the
0.45 to 1.00 interval range, the argument model may
correctly avoid labeling any arguments.
As noted in section 2, these results are not di-
rectly comparable to the results of the recent CoNLL
Shared Task (Surdeanu et al, 2008). This is due to
the fact that the semantic labeled F1 in the Shared
Task combines predicate and argument predictions
into a single score. The same combined F1 score for
our best two-stage nominal SRL system (logistic re-
gression nominal and argument models) is 0.7806;
however, this result is not precisely comparable be-
cause we do not identify the predicate role set as re-
quired by the CoNLL Shared Task.
5.3 NomLex-based analysis of results
As demonstrated in section 1, NomBank annotates
many classes of deverbal and non-deverbal nomi-
nals, which have been categorized on syntactic and
semantic bases in NomLex-PLUS (Meyers, 2007b).
To help understand what types of nominals are par-
ticularly affected by implicit argumentation, we fur-
ther analyzed performance with respect to these
classes.
Figure 2a shows the distribution of nominals
across classes defined by the NomLex resource. As
shown in Figure 2b, many of the most frequent
classes exhibit significant gains. For example, the
classification of partitive nominals (13% of all nom-
inal instances) with the LibLinear classifier results
in gains of 55.45% and 33.72% over the baseline
and MLE classifiers, respectively. For the 5 most
common classes, which constitute 82% of all nomi-
nals instances, we observe average gains of 27.47%
and 19.30% over the baseline and MLE classifiers,
152
00.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
no
m
pa
rtit
ive
no
m
like
re
lat
ion
al
no
m
ing
att
rib
ute
en
vir
on
m
en
t
ab
ility
no
m
ad
j
wo
rk-
of-
ar
t
gro
up
no
m
ad
jlike job
sh
ar
e
ev
en
t
typ
e
ve
rs
ion
ha
llm
ar
k
ab
le-
no
m fie
ld
NomLex class
%
 
o
f n
o
m
in
al
 
in
st
an
ce
s
(a) Distribution of nominals across the NomLex classes. The
y-axis denotes the percentage of all nominal instances that is
occupied by nominals in the class.
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
no
m
pa
rtit
ive
no
m
like
re
lat
ion
al
no
m
ing
att
rib
ute
en
vir
on
m
en
t
ab
ility
no
m
ad
j
wo
rk-
of-
ar
t
gro
up
no
m
ad
jlike job
sh
ar
e
ev
en
t
typ
e
ve
rs
ion
ha
llm
ar
k
ab
le-
no
m fie
ld
NomLex class
Pr
ed
ic
at
e 
n
o
m
in
al
 
F1
Baseline
MLE
LibLinear
(b) Nominal classification performance with respect to the
NomLex classes in Figure 2a. The y-axis denotes the com-
bined F1 for nominals in the class.
Figure 2: Evaluation results with respect to NomLex classes.
respectively.
Table 5 separates nominal and argument classifi-
cation results into sets of deverbal (NomLex class
nom), deverbal-like (NomLex class nom-like), and
all other nominalizations. A deverbal-like nominal
is closely related to some verb, although not mor-
phologically. For example, the noun accolade shares
argument interpretation with award, but the two are
not morphologically related. As shown by Table 5,
nominal classification tends to be easier - and ar-
gument classification harder - for deverbals when
compared to other types of nominals. The differ-
ence in argument F1 between deverbal/deverbal-like
nominals and the others is due primarily to relational
nominals, which are relatively easy to classify (Fig-
ure 2b); additionally, relational nominals exhibit a
high rate of argument incorporation, which is eas-
ily handled by the maximum-likelihood model de-
scribed in section 3.1.
6 Conclusions and future work
The application of nominal SRL to practical NLP
problems requires a system that is able to accurately
process each token it encounters. Previously, it was
unclear whether the models proposed by Jiang and
Ng (2006) and Liu and Ng (2007) would operate ef-
fectively in such an environment. The systems de-
scribed by Surdeanu et al (2008) are designed with
this environment in mind, but their evaluation did
not focus on the issue of implicit argumentation.
These two problems motivate the work presented in
this paper.
Our contribution is three-fold. First, we improve
upon previous nominal SRL results using a single-
stage classifier with additional new features. Sec-
ond, we show that this model suffers a substantial
performance degradation when evaluated over nom-
inals with implicit arguments. Finally, we identify a
set of features - many of them new - that can be used
to reliably detect nominals with explicit arguments,
thus significantly increasing the performance of the
nominal SRL system.
Our results also suggest interesting directions for
future work. As described in section 5.2, many nom-
inals do not have enough labeled training data to
produce accurate argument models. The general-
ization procedures developed by Gordon and Swan-
son (2007) for PropBank SRL and Pado? et al (2008)
for NomBank SRL might alleviate this problem.
Additionally, instead of ignoring nominals with im-
plicit arguments, we would prefer to identify the im-
plicit arguments using information contained in the
surrounding discourse. Such inferences would help
connect entities and events across sentences, provid-
ing a fuller interpretation of the text.
Acknowledgments
The authors would like to thank the anonymous re-
viewers for their helpful suggestions. The first two
authors were supported by NSF grants IIS-0535112
and IIS-0347548, and the third author was supported
by NSF grant IIS-0534700.
153
References
Collin Baker, Charles Fillmore, and John Lowe. 1998.
The Berkeley FrameNet project. In Christian Boitet
and Pete Whitelock, editors, Proceedings of the Thirty-
Sixth Annual Meeting of the Association for Computa-
tional Linguistics and Seventeenth International Con-
ference on Computational Linguistics, pages 86?90,
San Francisco, California. Morgan Kaufmann Publish-
ers.
Xavier Carreras and Llu??s Ma`rquez. 2005. Introduction
to the conll-2005 shared task: Semantic role labeling.
Eugene Charniak and Mark Johnson. 2005. Coarse-to-
fine n-best parsing and maxent discriminative rerank-
ing. In Proceedings of the 43rd Annual Meeting on
Association for Computational Linguistics.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A
library for large linear classification. Journal of Ma-
chine Learning Research, 9:1871?1874.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic la-
beling of semantic roles. Computational Linguistics,
28:245?288.
Roxana Girju, Preslav Nakov, Vivi Nastase, Stan Sz-
pakowicz, Peter Turney, and Deniz Yuret. 2007.
Semeval-2007 task 04: Classification of semantic re-
lations between nominals. In Proceedings of the 4th
International Workshop on Semantic Evaluations.
A. Gordon and R. Swanson. 2007. Generalizing seman-
tic role annotations across syntactically similar verbs.
In Proceedings of ACL, pages 192?199.
Z. Jiang and H. Ng. 2006. Semantic role labeling of
nombank: A maximum entropy approach. In Proceed-
ings of the 2006 Conference on Empirical Methods in
Natural Language Processing.
Maria Lapata. 2000. The automatic interpretation
of nominalizations. In Proceedings of the Seven-
teenth National Conference on Artificial Intelligence
and Twelfth Conference on Innovative Applications of
Artificial Intelligence, pages 716?721. AAAI Press /
The MIT Press.
Chang Liu and Hwee Ng. 2007. Learning predictive
structures for semantic role labeling of nombank. In
Proceedings of the 45th Annual Meeting of the Asso-
ciation of Computational Linguistics, pages 208?215,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Adam Meyers. 2007a. Annotation guidelines for nom-
bank - noun argument structure for propbank. Techni-
cal report, New York University.
Adam Meyers. 2007b. Those other nombank dictionar-
ies. Technical report, New York University.
Sebastian Pado?, Marco Pennacchiotti, and Caroline
Sporleder. 2008. Semantic role assignment for event
nominalisations by leveraging verbal data. In Pro-
ceedings of the 22nd International Conference on
Computational Linguistics (Coling 2008), pages 665?
672, Manchester, UK, August. Coling 2008 Organiz-
ing Committee.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated corpus of
semantic roles. Computational Linguistics, 31(1):71?
106.
Sameer Pradhan, Wayne Ward, and James H. Martin.
2005. Towards robust semantic role labeling. In Asso-
ciation for Computational Linguistics.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s Ma`rquez, and Joakim Nivre. 2008. The CoNLL
2008 shared task on joint parsing of syntactic and se-
mantic dependencies. In CoNLL 2008: Proceedings
of the Twelfth Conference on Computational Natu-
ral Language Learning, pages 159?177, Manchester,
England, August. Coling 2008 Organizing Committee.
154
Optimization in Multimodal Interpretation  
Joyce Y. Chai*         Pengyu Hong+ Michelle X. Zhou? Zahar Prasov* 
*Computer Science and Engineering 
Michigan State University  
East Lansing, MI 48824 
{jchai@cse.msu.edu,  
prasovz@cse.msu.edu} 
+Department of Statistics 
Harvard University 
Cambridge, MA 02138 
hong@stat.harvard.edu 
?Intelligent Multimedia Interaction  
  IBM T. J. Watson Research Ctr. 
Hawthorne, NY 10532 
mzhou@us.ibm.com 
Abstract 
    In a multimodal conversation, the way users 
communicate with a system depends on the 
available interaction channels and the situated 
context (e.g., conversation focus, visual feedback). 
These dependencies form a rich set of constraints 
from various perspectives such as temporal 
alignments between different modalities, 
coherence of conversation, and the domain 
semantics. There is strong evidence that 
competition and ranking of these constraints is 
important to achieve an optimal interpretation. 
Thus, we have developed an optimization approach 
for multimodal interpretation, particularly for 
interpreting multimodal references. A preliminary 
evaluation indicates the effectiveness of this 
approach, especially for complex user inputs that 
involve multiple referring expressions in a speech 
utterance and multiple gestures.   
1 Introduction 
Multimodal systems provide a natural and 
effective way for users to interact with computers 
through multiple modalities such as speech, 
gesture, and gaze (Oviatt 1996). Since the first 
appearance of ?Put-That-There? system (Bolt 
1980), a variety of multimodal systems have 
emerged, from early systems that combine speech, 
pointing (Neal et al, 1991), and gaze (Koons et al 
1993), to systems that integrate speech with pen 
inputs (e.g., drawn graphics) (Cohen et al, 1996; 
Wahlster 1998; Wu et al, 1999), and systems that 
engage users in intelligent conversation (Cassell et 
al., 1999; Stent et al, 1999; Gustafson et al, 2000; 
Chai et al, 2002; Johnston et al, 2002).   
One important aspect of building multimodal 
systems is multimodal interpretation, which is a 
process that identifies the meanings of user inputs. 
In a multimodal conversation, the way users 
communicate with a system depends on the 
available interaction channels and the situated 
context (e.g., conversation focus, visual feedback). 
These dependencies form a rich set of constraints 
from various aspects (e.g., semantic, temporal, and 
contextual). A correct interpretation can only be 
attained by simultaneously considering these 
constraints. In this process, two issues are 
important: first, a mechanism to combine 
information from various sources to form an 
overall interpretation given a set of constraints; and 
second, a mechanism that achieves the best 
interpretation among all the possible alternatives 
given a set of constraints. The first issue focuses on 
the fusion aspect, which has been well studied in 
earlier work, for example, through unification-
based approaches (Johnston 1998) or finite state 
approaches (Johnston and Bangalore, 2000). This 
paper focuses on the second issue of optimization.  
As in natural language interpretation, there is 
strong evidence that competition and ranking of 
constraints is important to achieve an optimal 
interpretation for multimodal language processing.  
We have developed a graph-based optimization 
approach for interpreting multimodal references. 
This approach achieves an optimal interpretation 
by simultaneously applying semantic, temporal, 
and contextual constraints. A preliminary 
evaluation indicates the effectiveness of this 
approach, particularly for complex user inputs that 
involve multiple referring expressions in a speech 
utterance and multiple gestures. In this paper, we 
first describe the necessities for optimization in 
multimodal interpretation, then present our graph-
based optimization approach and discuss how our 
approach addresses key principles in Optimality 
Theory used for natural language interpretation  
(Prince and Smolensky 1993).  
2 Necessities for Optimization in 
Multimodal Interpretation 
In a multimodal conversation, the way a user 
interacts with a system is dependent not only on 
the available input channels (e.g., speech and 
gesture), but also upon his/her conversation goals, 
the state of the conversation, and the multimedia 
feedback from the system. In other words, there is 
a rich context that involves dependencies from 
many different aspects established during the 
interaction. Interpreting user inputs can only be 
situated in this rich context. For example, the 
temporal relations between speech and gesture are 
important criteria that determine how the 
information from these two modalities can be 
combined. The focus of attention from the prior 
conversation shapes how users refer to those 
objects, and thus, influences the interpretation of 
referring expressions. Therefore, we need to 
simultaneously consider the temporal relations 
between the referring expressions and the gestures, 
the semantic constraints specified by the referring 
expressions, and the contextual constraints from 
the prior conversation. It is important to have a 
mechanism that supports competition and ranking 
among these constraints to achieve an optimal 
interpretation, in particular, a mechanism to allow 
constraint violation and support soft constraints.  
We use temporal constraints as an example to 
illustrate this viewpoint1.  The temporal constraints 
specify whether multiple modalities can be 
combined based on their temporal alignment. In 
earlier work, the temporal constraints are 
empirically determined based on user studies 
(Oviatt 1996). For example, in the unification-
based approach (Johnston 1998), one temporal 
constraint indicates that speech and gesture can be 
combined only when the speech either overlaps 
with gesture or follows the gesture within a certain 
time frame. This is a hard constraint that has to be 
satisfied in order for the unification to take place. 
If a given input does not satisfy these hard 
constraints, the unification fails.  
In our user studies, we found that, although the 
majority of user temporal alignment behavior may 
satisfy pre-defined temporal constraints, there are 
                                                                
1 We implemented a system using real estate as an application 
domain.  The user can interact with a map using both speech 
and gestures to retrieve information. All the user studies men-
tioned in this paper were conducted using this system.  
some exceptions. Table 1 shows the percentage of 
different temporal relations collected from our user 
studies. The rows indicate whether there is an 
overlap between speech referring expressions and 
their accompanied gestures. The columns indicate 
whether the speech (more precisely, the referring 
expressions) or the gesture occurred first. 
Consistent with the previous findings (Oviatt et al 
1997), in most cases (85% of time), gestures 
occurred before the referring expressions were 
uttered. However, in 15% of the cases the speech 
referring expressions were uttered before the 
gesture occurred. Among those cases, 8% had an 
overlap between the referring expressions and the 
ge
), 
al al 
(i ) 
in f 
in e 
1 n 
in 6 
m ?s 
sp d 
w d 
ah e 
us
sp
be
te
ac
T
0
0.2
0.4
0.6
0.8
1
1 2 3 4 5 6 7
User
Pe
rc
en
ta
ge
Non-overlap Speech First Non-overlap Gesture First
Overlap Speech First Overlap Gesture First
Figure 1: Temporal relations between speech and gesture 
for individual users 
100%85%15%Total
48%40%8%Overlap
52%45%7%Non-overlap
TotalGesture FirstSpeech First
 
Table 1: Overall temporal relations between speech and 
gesture sture and 7% had no overlap.  
Furthermore, as shown in (Oviatt et al, 2003
though multimodal behaviors such as sequenti
.e., non-overlap) or simultaneous (e.g., overlap
tegration are quite consistent during the course o
teraction, there are still some exceptions. Figur
shows the temporal alignments from seve
dividual users in our study. User 2 and User 
aintained a consistent behavior in that User 2
eech referring expressions always overlappe
ith gestures and User 6?s gesture always occurre
ead of the speech expressions. The other fiv
ers exhibited varied temporal alignment between 
eech and gesture during the interaction.  It will 
 difficult for a system using pre-defined 
mporal constraints to anticipate and 
commodate all these different behaviors.  
herefore, it is desirable to have a mechanism that 
allows violation of these constraints and support 
soft or graded constraints.  
3 A Graph-based Optimization Approach  
To address the necessities described above, we 
developed an optimization approach for 
interpreting multimodal references using graph 
matching. The graph representation captures both 
salient entities and their inter-relations. The graph 
matching is an optimization process that finds the 
best matching between two graphs based on 
constraints modeled as links or nodes in these 
graphs. This type of structure and process is 
especially useful for interpreting multimodal 
references. One graph can represent all the 
referring expressions and their inter-relations, and 
the other graph can represent all the potential 
referents. The question is how to match them 
together to achieve a maximum compatibility 
given a particular context.  
3.1 Overview  
Graph-based Representation 
Attribute Relation Graph (ARG) (Tsai and Fu, 1979) 
is used to represent information in our approach. 
An ARG consists of a set of nodes that are 
connected by a set of edges. Each node represents 
an entity, which in our case is either a referring 
expression to be resolved or a potential referent.  
Each node encodes the properties of the 
corresponding entity including: 
? Semantic information that indicates the 
semantic type, the number of potential referents, 
and the specific attributes related to the 
corresponding entity (e.g., extracted from the 
referring expressions).  
? Temporal information that indicates the time 
when the corresponding entity is introduced into 
the discourse (e.g., uttered or gestured).  
Each edge represents a set of relations between 
two entities. Currently we capture temporal 
relations and semantic type relations. A temporal 
relation indicates the temporal order between two 
related entities during an interaction, which may 
have one of the following values:  
? Precede: Node A precedes Node B if the entity 
represented by Node A is introduced into the 
discourse before the entity represented by Node B.  
? Concurrent: Node A is concurrent with Node B if 
the entities represented by them are referred to or 
mentioned simultaneously. 
? Non-concurrent: Node A is non-concurrent with 
Node B if their corresponding objects/references 
cannot be referred/mentioned simultaneously.  
? Unknown: The temporal order between two entities 
is unknown. It may take the value of any of the 
above.  
A semantic type relation indicates whether two 
related entities share the same semantic type. It 
currently takes the following discrete values: Same, 
Different, and Unknown. It could be beneficial in the 
future to consider a continuous function measuring 
the rate of compatibility instead.  
Specially, two graphs are generated. One graph, 
called the referring graph, captures referring 
expressions from speech utterances. For example, 
suppose a user says Compare this house, the green 
house, and the brown one. Figure 2 show a referring 
graph that represents three referring expressions 
from this speech input. Each node captures the 
semantic information such as the semantic type 
(i.e., Semantic Type), the attribute (Color), the 
number (Number) of the potential referents, as well 
as the temporal information about when this 
referring expression is uttered (BeginTime and 
EndTime). Each edge captures the semantic (e.g., 
SemanticTypeRelation) and temporal relations (e.g., 
TemporalRelation) between the referring expressions. 
In this case, since the green house is uttered before 
the brown one, there is a temporal Precede 
relationship between these two expressions. 
Furthermore, according to our heuristic that 
objects-to-be-compared should share the same 
semantic type, therefore, the SemanticTypeRelation 
between two nodes is set to Same.  
Node 1
this house
Node 2
the green 
house
Node 3
the brown 
one
SemanticType: House
Number.: 1
Attribute: Color = $Green
BeginTime: 32244242ms
EndTime: ?
? ?
SemanticTypeRelation: Same
TemporalRelation: Precede
Direction: Node 2 -> Node 3
Speech: Compare this house, the green house    
and the brown one
 
Figure 2: An example of a referring graph 
Similarly, the second graph, called the referent 
graph, represents all potential referents from 
multiple sources (e.g., from the last conversation, 
gestured by the user, etc). Each node captures the 
semantic and temporal information about a 
potential referent (e.g., the time when the potential 
referent is selected by a gesture). Each edge 
captures the semantic and temporal relations 
between two potential referents.  For instance, 
suppose the user points to one position and then 
points to another position. The corresponding 
referent graph is shown in Figure 3. The objects 
inside the first dashed rectangle correspond to the 
potential referents selected by the first pointing 
gesture and those inside the second dashed 
rectangle correspond to the second pointing gesture. 
Each node also contains a probability that indicates 
the likelihood of its corresponding object being 
selected by the gesture. Furthermore, the salient 
objects from the prior conversation are also 
included in the referent graph since they could also 
be the potential referents (e.g., the rightmost 
dashed rectangle in Figure 32).  
To create these graphs, we apply a grammar-
based natural language parser to process speech 
inputs and a gesture recognition component to 
process gestures. The details are described in (Chai 
et al 2004a).  
                                                                
2 Each node from the conversation context is linked to every 
node corresponding to the first pointing and the second point-
ing.  
Graph-matching Process 
Given these graph representations, interpreting 
multimodal references becomes a graph-matching 
problem. The goal is to find the best match 
between a referring graph (Gs) and a referent graph 
(Gr). Suppose ? A referring graph Gs = ?{?m}, {?mn}?, where {?m} are 
nodes and {?mn} are edges connecting nodes ?m and ?n. Nodes in Gs are named referring nodes. 
? A referent graph Gr = ?{ax}, {rxy}?, where {ax} are 
nodes and {rxy} are edges connecting nodes ax and ay. 
Nodes in Gr are named referent nodes. 
   The following equation finds a match that 
achieves the maximum compatibility between Gr 
and Gs:  
),(),(),(
),(),(),(
mnxynymxx y m n
mxmxx msr
rEdgeSimaPaP
aNodeSimaPGGQ
???
??
? ? ? ?
? ? +=  (1)   
In Equation (1), Q(Gr,Gs) measures the degree of 
the overall match between the referent graph and 
the referring graph. P(ax,?m) is the matching 
probability between a node ax in the referent graph 
and a node ?m in the referring graph. The overall 
compatibility depends on the similarities between 
nodes (NodeSim) and the similarities between 
edges (EdgeSim). The function NodeSim(ax,?m) 
measures the similarity between a referent node ax 
and a referring node ?m by combining semantic 
constraints and temporal constraints. The function 
EdgeSim(rxy,?mn) measures the similarity between 
rxy and ?mn, which depends on the semantic and 
temporal constraints of the corresponding edges. 
These functions are described in detail in the next 
section.  
We use the graduated assignment algorithm 
(Gold and Rangarajan, 1996) to maximize Q(Gr,Gs) 
in Equation (1). The algorithm first initializes 
P(ax,?m) and then iteratively updates the values of 
P(ax,?m) until it converges. When the algorithm 
converges, P(ax,?m) gives the matching 
probabilities between the referent node ax and the 
referring node ?m that maximizes the overall 
compatibility function. Given this probability 
matrix, the system is able to assign the most 
probable referent(s) to each referring expression.  
3.2 Similarity Functions  
As shown in Equation (1), the overall 
compatibility between a referring graph and a 
referent graph depends on the node similarity 
Ossining
Chappaqua
Object ID: MLS2365478
SemanticType: House
Attribute: Color = $Brown
BeginTime: 32244292 ms
SelectionProb: 0.65
? ?
Semantic Type Relation: Diff
Temporal relation: Same
Direction: 
Gesture: Point to one position and point to 
another position
First pointing Second pointing Conversation
Context
Figure 3: An example of referent graph 
function and the edge similarity function. Next we 
give a detailed account of how we defined these 
functions. Our focus here is not on the actual 
definitions of those functions (since they may vary 
for different applications), but rather a mechanism 
that leads to competition and ranking of constraints.  
Node Similarity Function 
Given a referring expression (represented as ?m 
in the referring graph) and a potential referent 
(represented as ax in the referent graph), the node 
similarity function is defined based on the 
semantic and temporal information captured in ax 
and ?m through a set of individual compatibility 
functions: 
   NodeSim(ax,?m) = Id(ax,?m) SemType(ax,?m)  
                               ?k Attrk(ax,?m) Temp(ax,?m) 
Currently, in our system, the specific return 
values for these functions are empirically 
determined through iterative regression tests.  
Id(ax,?m) captures the constraint of the 
compatibilities between identifiers specified in ax 
and ?m. It indicates that the identifier of the 
potential referent, as expressed in a referring 
expression, should match the identifier of the true 
referent. This is particularly useful for resolving 
proper nouns. For example, if the referring 
expression is house number eight, then the correct 
referent should have the identifier number eight.  
We currently define this constraint as follows: 
Id(ax,?m) = 0 if the object identities of ax and ?m 
are different. Id(ax,?m) = 100 if they are the same. 
Id(ax,?m) = 1 if at least one of the identities of ax 
and ?m is unknown. The different return values 
enforce that a large reward is given to the case 
where the identifiers from the referring expressions 
match the identifiers from the potential referents.  
SemType(ax,?m) captures the constraint of 
semantic type compatibility between  ax and ?m. It 
indicates that the semantic type of a potential 
referent as expressed in the referring expression 
should match the semantic type of the correct 
referent. We define the following: SemType(ax,?m) 
= 0 if the semantic types of ax and ?m are different. 
SemType(ax,?m) = 1 if they are the same. 
SemType(ax,?m) = 0.5 if at least one of the 
semantic types of ax and ?m is unknown. Note that 
the return value given to the case where semantic 
types are the same (i.e., ?1?) is much lower than 
that given to the case where identifiers are the 
same (i.e., ?100?). This was designed to support 
constraint ranking. Our assumption is that the 
constraint on identifiers is more important than the 
constraint on semantic types. Because identifiers 
are usually unique, the corresponding constraint is 
a greater indicator of node matching if the 
identifier expressed from a referring expression 
matches the identifier of a potential referent. 
Attrk(ax,?m) captures the domain specific 
constraint concerning a particular semantic feature 
(indicated by the subscription k). This constraint 
indicates that the expected features of a potential 
referent as expressed in a referring expression 
should be compatible with features associated with 
the true referent. For example, in the referring 
expression the Victorian house, the style feature is 
Victorian.  Therefore, an object can only be a 
possible referent if the style of that object is 
Victorian.  Thus, we define the following: Ak(ax,?m) 
= 1 if both ax and ?m share the kth feature with the 
same value. Ak(ax,?m) = 0 if both ax and ?m have 
the feature k and the values of the feature k are not 
equal. Otherwise, when the kth feature is not 
present in either ax or ?m, then Ak (ax,?m) = 0.1.  
Note that these feature constraints are dependent 
on the specific domain model for a particular 
application.  
Temp(ax,?m) captures the temporal constraint 
between a referring expression ?m and a potential 
referent ax. As discussed in Section 2, a hard 
constraint concerning temporal relations between 
referring expressions and gestures will be 
incapable of handling the flexibility of user 
temporal alignment behavior. Thus the temporal 
constraint in our approach is a graded constraint, 
which is defined as follows: 
          
)
2000
|)()(|exp(),( mxmx
BeginTimeaBeginTimeaTemp ?? ??=  
This constraint indicates that the closer a 
referring expression and a potential referent in 
terms of their temporal alignment (regardless of 
the absolute precedence relationship), the more 
compatible they are.  
Edge Similarity Function 
The edge similarity function measures the 
compatibility of relations held between referring 
expressions (i.e., an edge ?mn in the referring graph) 
and relations between the potential referents (i.e., 
an edge rxy in the referent graph). It is defined by 
two individual compatibility functions as follows: 
EdgeSim(rxy, ?mn) = SemType(rxy, ?mn) Temp(rxy, ?mn)   
   SemType(rxy, ?mn) encodes the semantic type 
compatibility between an edge in the referring 
graph and an edge in the referent graph. It is 
defined in Table 2. This constraint indicates that 
the relation held between referring expressions 
should be compatible with the relation held 
between two correct referents. For example, 
consider the utterance How much is this green house 
and this blue house. This utterance indicates that the 
referent to the first expression this green house 
should share the same semantic type as the referent 
to the second expression this blue house. As shown 
in Table 2, if the semantic type relations of rxy and 
?mn are the same, SemType(rxy, ?mn) returns 1. If 
they are different, SemType(rxy, ?mn) returns zero. If 
either rxy or ?mn is unknown, then it returns 0.5.  
   Temp(rxy, ?mn) captures the temporal 
compatibility between an edge in the referring 
graph and an edge in the referent graph. It is 
defined in Table 3. This constraint indicates that 
the temporal relationship between two referring 
expressions (in one utterance) should be 
compatible with the relations of their 
corresponding referents as they are introduced into 
the context (e.g., through gesture). The temporal 
relation between referring expressions (i.e., ?mn) is 
either Precede or Concurrent. If the temporal 
relations of rxy and ?mn are the same, then Temp(rxy, 
?mn) returns 1. Because potential references could 
come from prior conversation, even if rxy and ?mn 
are not the same, the function does not return zero 
when ?mn is Precede.  
Next, we discuss how these definitions and the 
process of graph matching address optimization, in 
particular, with respect to key principles of 
Optimality Theory for natural language 
interpretation.  
3.3 Optimality Theory 
Optimality Theory (OT) is a theory of language 
and grammar, developed by Alan Prince and Paul 
Smolensky (Prince and Smolensky, 1993). In 
Optimality Theory, a grammar consists of a set of 
well-formed constraints. These constraints are 
applied simultaneously to identify linguistic 
structures. Optimality Theory does not restrict the 
content of the constraints (Eisner 1997). An 
innovation of Optimality Theory is the conception 
of these constraints as soft, which means violable 
and conflicting.  The interpretation that arises for 
an utterance within a certain context maximizes the 
degree of constraint satisfaction and is 
consequently the best alternative (hence, optimal 
interpretation) among the set of possible 
interpretations.  
The key principles or components of Optimality 
Theory can be summarized as the following three 
components (Blutner 1998): 1) Given a set of input, 
Generator creates a set of possible outputs for each 
input. 2) From the set of candidate output, Evaluator 
selects the optimal output for that input. 3) There is 
a strict dominance in term of the ranking of constraints. 
Constraints are absolute and the ranking of the 
constraints is strict in the sense that outputs that 
have at least one violation of a higher ranked 
constraint outrank outputs that have arbitrarily 
many violations of lower ranked constraints. 
Although Optimality Theory is a grammar-based 
framework for natural language processing, its key 
principles can be applied to other representations. 
At a surface level, our approach addresses these 
main principles. 
First, in our approach, the matching matrix 
P(ax,?m) captures the probabilities of all the 
possible matches between a referring node ?m and 
a referent node ax. The matching process updates 
these probabilities iteratively. This process 
corresponds to the Generator component in 
Optimality Theory.  
Second, in our approach, the satisfaction or 
violation of constraints is implemented via return 
values of compatibility functions. These 
0.50.50.5Unknown
0.510Different
0.501Same?mn
Unknown DifferentSame
rxySemType(rxy, ?mn)
 
Table 2: Definition of SemType(rxy, ?mn) 
 
0.5010Concurrent
0.50.70.51Precede?mn
Unknown Non-concurrentConcurrentPreceding
rxyTemp(rxy, ?mn)
Table 3: Definition of Temp(rxy, ?mn) 
constraints can be violated during the matching 
process. For example, functions Id(ax,?m), 
SemType(ax,?m), and Attrk(ax,?m) return zero if the 
corresponding intended constraints are violated. In 
this case, the overall similarity function will return 
zero. However, because of the iterative updating 
nature of the matching algorithm, the system will 
still find the most optimal match as a result of the 
matching process even some constraints are 
violated. Furthermore, A function that never 
returns zero such as Temp(ax,?m) in the node 
similarity function implements a gradient 
 
 
 
 
 
 
 
. 
 
 
, 
 
 
. 
 
3.4 Evaluation  
We conducted several user studies to evaluate 
the performance of this approach. Users could 
interact with our system using both speech and 
deictic gestures. Each subject was asked to 
complete five tasks. For example, one task was to 
find the cheapest house in the most populated town. 
Data from eleven subjects was collected and 
analyzed. 
Table 4 shows the evaluation results of 219 
inputs. These inputs were categorized in terms of 
the number of referring expressions in the speech 
input and the number of gestures in the gesture 
inputs. Out of the total 219 inputs, 137 inputs had 
their referents correctly interpreted. For the 
remaining 82 inputs in which the referents were 
not correctly identified, the problem did not come 
from the approach itself, but rather from other 
sources such as speech recognition and language 
understanding errors.  These were two major error 
sources, which were accounted for 55% and 20% 
of total errors respectively (Chai et al 2004b).    
In our studies, the majority of user references 
were simple in that they involved only one 
referring expression and one gesture as in earlier 
findings (Kehler 2000). It is trivial for our 
approach to handle these simple inputs since the 
size of the graph is usually very small and there is 
only one node in the referring graph. However, we 
did find 23% complex inputs (the row S3 and the 
column G3 in Table 4), which involved multiple 
referring expressions from speech utterances 
and/or multiple gestures. Our optimization 
approach is particularly effective to interpret these 
complex inputs by simultaneously considering 
semantic, temporal, and contextual constraints.  
4 Conclusion 
As in natural language interpretation addressed 
by Optimality Theory, the idea of optimizing 
constraints is beneficial and there is evidence in 
favor of competition and constraint ranking in 
multimodal language interpretation. We developed 
a graph-based approach to address optimization for 
multimodal interpretation; in particular, 
interpreting multimodal references. Our approach 
simultaneously applies temporal, semantic, and 
contextual constraints together and achieves the 
best interpretation among all alternatives. Although 
currently the referent graph corresponds to gesture 
129(111)
90(26)
20(15),
19(2)
102(91),
65(22)
7(5),
6(2)
Total Num
15(9),
16(1)
12(8),
8(0)
3(1),
7(1)
0(0),
1(0)
S3: Multiple referring 
expressions
110(90),
74(25)
8(7),
11(2)
96(89),
58(21)
6(4),
5(2)
S2: One referring
expression
4(2),
0(0)
03(1),
0(0)
1(1),
0(0)
S1:No referring
expression
Total
Num
G3: Multi-
Gestures
G2: One 
Gesture
G1: No 
Gesture
 
Table 4: Evaluation Results. In each entry form ?a(b), c(d)?,
?a? indicates the number of inputs in which the referring
expressions were correctly recognized by the speech recog-
nizer; ?b? indicates the number of inputs in which the refer-
ring expressions were  correctly recognized and were
correctly resolved; ?c? indicates the number of inputs in
which the referring expressions were not correctly recog-
nized; ?d? indicates the number of inputs in which the refer-
ring expressions also were not correctly recognized, but
were correctly resolved. The sum of ?a? and ?c? gives the
total number of inputs with a particular combination of
speech and gesture. constraint in Optimality Theory. Given these
compatibility functions, the graph-matching
algorithm provides an optimization process to find
the best match between two graphs. This process
corresponds to the Evaluator component of
Optimality Theory.  
Third, in our approach, different compatibility
functions return different values to address the
Constraint Ranking component in Optimality Theory
For example, as discussed earlier, once ax and ?m
share the same identifier, Id(ax,?m) returns 100. If
ax and ?m share the same semantic type
SemType(ax,?m) returns 1. Here, we consider the
compatibility between identifiers is more important
than the compatibility between semantic types
However, currently we have not yet addressed the
strict dominance aspect of Optimality Theory. 
input and conversation context, it can be easily 
extended to incorporate other modalities such as 
gaze inputs.  
We have only taken an initial step to investigate 
optimization for multimodal language processing. 
Although preliminary studies have shown the 
effectiveness of the optimization approach based 
on graph matching, this approach also has its 
limitations.  The graph-matching problem is a NP 
complete problem and it can become intractable 
once the size of the graph is increased. However, 
we have not experienced the delay of system 
responses during real-time user studies. This is 
because most user inputs were relatively concise 
(they contained no more than four referring 
expressions).  This brevity limited the size of the 
graphs and thus provided an opportunity for such 
an approach to be effective. Our future work will 
address how to extend this approach to optimize 
the overall interpretation of user multimodal inputs.  
Acknowledgements 
This work was partially supported by grant IIS-
0347548 from the National Science Foundation 
and grant IRGP-03-42111 from Michigan State 
University. The authors would like to thank John 
Hale and anonymous reviewers for their helpful 
comments and suggestions.  
References 
Bolt, R.A. 1980. Put that there: Voice and Gesture at the 
Graphics Interface. Computer Graphics, 14(3): 262-270.  
Blutner, R., 1998. Some Aspects of Optimality In Natural 
Language Interpretation. Journal of Semantics, 17, 189-216. 
Cassell, J., Bickmore, T., Billinghurst, M., Campbell, L., 
Chang, K., Vilhjalmsson, H. and Yan, H. 1999. Embodi-
ment in Conversational Interfaces: Rea. In Proceedings of 
the CHI'99 Conference, 520-527.  
Chai, J., Prasov, Z, and Hong, P. 2004b. Performance Evalua-
tion and Error Analysis for Multimodal Reference Resolu-
tion in a Conversational System. Proceedings of HLT-
NAACL 2004 (Companion Volumn).  
Chai, J. Y., Hong, P., and Zhou, M. X. 2004a. A Probabilistic 
Approach to Reference Resolution in Multimodal User In-
terfaces, Proceedings of 9th International Conference on 
Intelligent User Interfaces (IUI): 70-77.  
Chai, J., Pan, S., Zhou, M., and Houck, K. 2002. Context-
based Multimodal Interpretation in Conversational Systems. 
Fourth International Conference on Multimodal Interfaces. 
Cohen, P., Johnston, M., McGee, D., Oviatt, S., Pittman, J., 
Smith, I., Chen, L., and Clow, J. 1996. Quickset: Multimo-
dal Interaction for Distributed Applications. Proceedings of 
ACM Multimedia. 
Eisner, Jason. 1997. Efficient Generation in Primitive Opti-
mality Theory. Proceedings of ACL?97. 
Gold, S. and Rangarajan, A. 1996. A Graduated Assignment 
Algorithm for Graph-matching. IEEE Trans. Pattern 
Analysis and Machine Intelligence, vol. 18, no. 4.  
Gustafson, J., Bell, L., Beskow, J., Boye J., Carlson, R., Ed-
lund, J., Granstrom, B., House D., and Wiren, M.  2000. 
AdApt ? a Multimodal Conversational Dialogue System in 
an Apartment Domain. Proceedings of 6th International 
Conference on Spoken Language Processing (ICSLP). 
Johnston, M, Cohen, P., McGee, D., Oviatt, S., Pittman, J. and 
Smith, I. 1997. Unification-based Multimodal Integration, 
Proceedings of ACL?97. 
Johnston, M. 1998. Unification-based Multimodal Parsing, 
Proceedings of COLING-ACL?98. 
Johnston, M. and Bangalore, S. 2000. Finite-state Multimodal 
Parsing and Understanding. Proceedings of COLING?00.  
Johnston, M.,  Bangalore, S.,  Visireddy G., Stent, A., Ehlen, 
P., Walker, M., Whittaker, S., and Maloor, P. 2002. 
MATCH: An Architecture for Multimodal Dialog Systems, 
Proceedings of ACL?02, Philadelphia, 376-383. 
Kehler, A. 2000. Cognitive Status and Form of Reference in 
Multimodal Human-Computer Interaction, Proceedings of 
AAAI?01, 685-689. 
Koons, D. B., Sparrell, C. J. and Thorisson, K. R. 1993. Inte-
grating Simultaneous Input from Speech, Gaze, and Hand 
Gestures. In Intelligent Multimedia Interfaces, M. Maybury, 
Ed. MIT Press: Menlo Park, CA. 
Neal, J. G., and Shapiro, S. C.  1991. Intelligent Multimedia 
Interface Technology. In Intelligent User Interfaces, J. Sul-
livan & S. Tyler, Eds. ACM: New York. 
Oviatt, S. L. 1996. Multimodal Interfaces for Dynamic Inter-
active Maps. In Proceedings of Conference on Human Fac-
tors in Computing Systems: CHI '96, 95-102.  
Oviatt, S., DeAngeli, A., and Kuhn, K., 1997. Integration and 
Synchronization of Input Modes during Multimodal Hu-
man-Computer Interaction, In Proceedings of Conference 
on Human Factors in Computing Systems: CHI '97. 
Oviatt, S., Coulston, R., Tomko, S., Xiao, B., Bunsford, R. 
Wesson, M., and Carmichael, L. 2003. Toward a Theory of 
Organized Multimodal Integration Patterns during Human-
Computer Interaction. In Proceedings of Fifth International 
Conference on Multimodal Interfaces, 44-51.  
Prince, A. and Smolensky, P. 1993. Optimality Theory. Con-
straint Interaction in Generative Grammar. ROA 537.  
http://roa.rutgers.edu/view.php3?id=845.  
Stent, A., J. Dowding, J. M. Gawron, E. O. Bratt, and R. 
Moore. 1999. The Commandtalk Spoken Dialog System. 
Proceedings of ACL?99,  183?190. 
Tsai, W.H. and Fu, K.S.  1979. Error-correcting Isomorphism 
of Attributed Relational Graphs for Pattern Analysis. IEEE 
Transactions on Systems, Man and Cybernetics., vol. 9. 
Wahlster, W., 1998. User and Discourse Models for Multimo-
dal Communication. Intelligent User Interfaces, M. 
Maybury and W. Wahlster (eds.),  359-370. 
Wu, L., Oviatt, S., and Cohen, P. 1999. Multimodal Integra-
tion ? A Statistical View, IEEE Transactions on Multime-
dia, Vol. 1, No. 4, 334-341.   
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 57?64,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Towards Conversational QA: Automatic Identification of Problematic
Situations and User Intent ?
Joyce Y. Chai Chen Zhang Tyler Baldwin
Department of Computer Science and Engineering
Michigan State University
East Lansing, MI 48824
{jchai, zhangch6, baldwi96}@cse.msu.edu
Abstract
To enable conversational QA, it is impor-
tant to examine key issues addressed in
conversational systems in the context of
question answering. In conversational sys-
tems, understanding user intent is criti-
cal to the success of interaction. Recent
studies have also shown that the capabil-
ity to automatically identify problematic
situations during interaction can signifi-
cantly improve the system performance.
Therefore, this paper investigates the new
implications of user intent and problem-
atic situations in the context of question
answering. Our studies indicate that, in
basic interactive QA, there are different
types of user intent that are tied to dif-
ferent kinds of system performance (e.g.,
problematic/error free situations). Once
users are motivated to find specific infor-
mation related to their information goals,
the interaction context can provide useful
cues for the system to automatically iden-
tify problematic situations and user intent.
1 Introduction
Interactive question answering (QA) has been
identified as one of the important directions in QA
research (Burger et al, 2001). One ultimate goal is
to support intelligent conversation between a user
and a QA system to better facilitate user informa-
tion needs. However, except for a few systems that
use dialog to address complex questions (Small et
al., 2003; Harabagiu et al, 2005), the general di-
alog capabilities have been lacking in most ques-
?This work was partially supported by IIS-0347548 from
the National Science Foundation.
tion answering systems. To move towards conver-
sational QA, it is important to examine key issues
relevant to conversational systems in the context
of interactive question answering.
This paper focuses on two issues related to con-
versational QA. The first issue is concerned with
user intent. In conversational systems, understand-
ing user intent is the key to the success of the inter-
action. In the context of interactive QA, one ques-
tion is what type of user intent should be captured.
Unlike most dialog systems where user intent can
be characterized by dialog acts such as question,
reply, and statement, in interactive QA, user in-
puts are already in the form of question. Then
the problems become whether there are different
types of intent behind these questions that should
be handled differently by a QA system and how to
automatically identify them.
The second issue is concerned with problem-
atic situations during interaction. In spoken di-
alog systems, many problematic situations could
arise from insufficient speech recognition and lan-
guage understanding performance. Recent work
has shown that the capability to automatically
identify problematic situations (e.g., speech recog-
nition errors) can help control and adapt dialog
strategies to improve performance (Litman and
Pan, 2000). Similarly, QA systems also face chal-
lenges of technology limitation from language un-
derstanding and information retrieval. Thus one
question is, in the context of interactive QA, how
to characterize problematic situations and auto-
matically identify them when they occur.
In interactive QA, these two issues are inter-
twined. Questions formed by a user not only de-
pend on his/her information goals, but are also in-
fluenced by the answers from the system. Prob-
lematic situations will impact user intent in the
57
follow-up questions, which will further influence
system performance. Both the awareness of prob-
lematic situations and understanding of user in-
tent will allow QA systems to adapt better strate-
gies during interaction and move towards intelli-
gent conversational QA.
To address these two questions, we conducted
a user study where users interacted with a con-
trolled QA system to find information of inter-
est. These controlled studies allowed us to fo-
cus on the interaction aspect rather than informa-
tion retrieval or answer extraction aspects. Our
studies indicate that in basic interactive QA where
users always ask questions and the system always
provides some kind of answers, there are differ-
ent types of user intent that are tied to differ-
ent kinds of system performance (e.g., problem-
atic/error free situations). Once users are moti-
vated to find specific information related to their
information goals, the interaction context can pro-
vide useful cues for the system to automatically
identify problematic situations and user intent.
2 Related Work
Open domain question answering (QA) systems
are designed to automatically locate answers from
large collections of documents to users? natural
language questions. In the past few years, au-
tomated question answering techniques have ad-
vanced tremendously, partly motivated by a se-
ries of evaluations conducted at the Text Retrieval
Conference (TREC) (Voorhees, 2001; Voorhees,
2004). To better facilitate user information needs,
recent trends in QA research have shifted towards
complex, context-based, and interactive question
answering (Voorhees, 2001; Small et al, 2003;
Harabagiu et al, 2005). For example, NIST initi-
ated a special task on context question answering
in TREC 10 (Voorhees, 2001), which later became
a regular task in TREC 2004 (Voorhees, 2004) and
2005. The motivation is that users tend to ask a
sequence of related questions rather than isolated
single questions to satisfy their information needs.
Therefore, the context QA task was designed to
investigate the system capability to track context
through a series of questions. Based on context
QA, some work has been done to identify clarifica-
tion relations between questions (Boni and Man-
andhar, 2003). However context QA is different
from interactive QA in that context questions are
specified ahead of time rather than incrementally
as in an interactive setting.
Interactive QA has been applied to process com-
plex questions. For analytical and non-factual
questions, it is hard to anticipate answers. Clari-
fication dialogues can be applied to negotiate with
users about the intent of their questions (Small et
al., 2003). Recently, an architecture for interactive
question answering has been proposed based on a
notion of predictive questioning (Harabagiu et al,
2005). The idea is that, given a complex ques-
tion, the system can automatically identify a set of
potential follow-up questions from a large collec-
tion of question-answer pairs. The empirical re-
sults have shown the system with predictive ques-
tioning is more efficient and effective for users to
accomplish information seeking tasks in a partic-
ular domain (Harabagiu et al, 2005).
The work reported in this paper addresses a
different aspect of interactive question answering.
Both issues raised earlier (Section 1) are inspired
by earlier work on intelligent conversational sys-
tems. Automated identification of user intent has
played an important role in conversational sys-
tems. Tremendous amounts of work has focused
on this aspect (Stolcke et al, 2000). To improve
dialog performance, much effort has also been put
on techniques to automatically detect errors during
interaction. It has shown that during human ma-
chine dialog, there are sufficient cues for machines
to automatically identify error conditions (Levow,
1998; Litman et al, 1999; Hirschberg et al, 2001;
Walker et al, 2002). The awareness of erroneous
situations can help systems make intelligent de-
cisions about how to best guide human partners
through the conversation and accomplish the tasks.
Motivated by these earlier studies, the goal of this
paper is to investigate whether these two issues can
be applied in question answering to facilitate intel-
ligent conversational QA.
3 User Studies
We conducted a user study to collect data concern-
ing user behavior in a basic interactive QA set-
ting. We are particularly interested in how users
respond to different system performance and its
implication in identifying problematic situations
and user intent. As a starting point, we charac-
terize system performance as either problematic,
which indicates the answer has some problem, or
error-free, which indicates the answer is correct.
In this section, we first describe the methodology
58
and the system used in this effort and then discuss
the observed user behavior and its relation to prob-
lematic situations and user intent.
3.1 Methodology and System
The system used in our experiments has a user in-
terface that takes a natural language question and
presents an answer passage. Currently, our inter-
face only presents to the user the top one retrieved
result. This simplification on one hand helps us
focus on the investigation of user responses to dif-
ferent system performances and on the other hand
represents a possible situation where a list of po-
tential answers may not be practical (e.g., through
PDA or telephone line).
We implemented a Wizard-of-Oz (WOZ) mech-
anism in the interaction loop to control and simu-
late problematic situations. Users were not aware
of the existence of this human wizard and were
led to believe they were interacting with a real
QA system. This controlled setting allowed us
to focus on the interaction aspect rather than in-
formation retrieval or answer extraction aspect of
question answering. More specifically, during in-
teraction after each question was issued, a ran-
dom number generator was used to decide if a
problematic situation should be introduced. If
the number indicated no, the wizard would re-
trieve a passage from a database with correct ques-
tion/answer pairs. Note that in our experiments
we used specific task scenarios (described later),
so it was possible to anticipate user information
needs and create this database. If the number in-
dicated that a problematic situation should be in-
troduced, then the Lemur retrieval engine 1 was
used on the AQUAINT collection to retrieve the
answer. Our assumption is that AQUAINT data
are not likely to provide an exact answer given our
specific scenarios, but they can provide a passage
that is most related to the question. The use of the
random number generator was to control the ratio
between the occurrence of problematic situations
and error-free situations. In our initial investiga-
tion, since we are interested in observing user be-
havior in problematic situations, we set the ratio as
50/50. In our future work, we will vary this ratio
(e.g., 70/30) to reflect the performance of state-of-
the-art factoid QA and investigate the implication
of this ratio in automated performance assessment.
1http://www-2.cs.cmu.edu/ lemur/
3.2 Experiments
Eleven users participated in our study. Each user
was asked to interact with our system to com-
plete information seeking tasks related to four
specific scenarios: the 2004 presidential debates,
Tom Cruise, Hawaii, and Pompeii. The exper-
imental scenarios were further divided into two
types: structured and unstructured. In the struc-
tured task scenarios (for topics Tom Cruise and
Pompeii), users had to fill in blanks on a dia-
gram pertaining to the given topic. Using the dia-
gram was to avoid the influence of these scenarios
on the language formation of the relevant ques-
tions. Because users must find certain informa-
tion, they were constrained in the range of ques-
tions in which they could ask, but not the way they
ask those questions. The task was completed when
all of the blanks on the diagram were filled. The
structured scenarios were designed to mimic the
real information seeking practice in which users
have real motivation to find specific information
related to their information goals. In the unstruc-
tured scenarios (for topics the 2004 presidential
debates and Hawaii), users were given a general
topic to investigate, but were not required to find
specific information. This gave the user the abil-
ity to ask a much wider range of questions than
the structured scenarios. Users were generally in
an exploration mode when performing these un-
structured tasks. They were not motivated to find
specific information and were content with any in-
formation provided by the system. In our view,
the unstructured scenarios are less representative
of the true information seeking situations.
3.3 Observations and Analysis
From our studies, a total of 44 interaction sessions
with 456 questions were collected. Figure 1 shows
an example of a fragment of interaction related to
Tom Cruise. In this example, both problematic sit-
uations applied to answers (e.g., Problematic and
Error-Free) and user intent (described later) ap-
plied to questions are annotated.
There are several observations from this data.
First, questions formed during interactive QA tend
to be self-contained and free of definite noun
phrases, pronouns, or ellipsis. Only one question
in the entire data set has a pronoun (i.e., What
are the best movies with Tom Cruise in them?).
Even in this case, the pronoun them did not re-
fer to any entities that occurred previously in the
59
# Question/Answer Annotation
Q1 When was Tom Cruise born?
Q2 What year was Tom Cruise born? Re-try
Q3 What does Tom Cruise do for a living? Continue
Q4 What is Tom Cruise's profession? Re-try
Q5 What are the best movies with Tom Cruise 
in them?
Switch
A1 ? Based on the memoir of combat veteran 
Ron Kovic, the film stars Tom Cruise as 
Kovic, whose gunshot wound in Vietnam 
left him paralyzed from the chest down.
?.a powerfully intimate portrait that 
unfolds on an epic scale, Born on the 
Fourth of July is arguably Stone's best film 
(if you can forgive its often strident tone), 
?..
Proble-
matic
A2 ?Thomas Cruise Mapother IV was born
on the 3rd of July, 1962 (eerily similar to 
his film Born on the 4th of July), in 
Syracuse, New York. He was the only boy
of four children?.
Error-Free
A3 ?you get a very nice role that shows you
differently, not the heavy, and you're
working with a George Clooney or a Tom
Cruise or a Nicolas Cage or a Martin
Scorsese. I can live with that ?
Proble-
matic
A4 ?So we may agree that Cruise?s
professional standing accounts for some 
measure of his fame?.
Proble-
matic
A5 ?Cruise proved his dramatic talents in the 
1988 drama Rain Man, where he co-
starred with Oscar-winner Dustin Hoffman. 
Oliver Stone's Born on the Fourth of July
(1989) earned him a Best Actor Oscar 
nomination for his hard-hitting portrayal of 
anti-war activist Ron Kovic?.
Error-Free
Figure 1: An example fragment of interaction
QA process. This phenomenon could be caused by
how the answers are presented. Unlike specific an-
swer entities, the answer passages provided by our
system do not support the natural use of referring
expressions in the follow-up questions. Another
possible explanation could be that in an interac-
tive environment, users seem to be more aware of
the potential limitation of a computer system and
thus tend to specify self-contained questions in a
hope to reduce the system?s inference load.
The second observation is about user behavior
in response to different system performances (i.e.,
problematic or error-free situations). We were
hoping to see different strategies users might ap-
ply to deal with the problematic situations. How-
ever, based on the data, we found that when a prob-
lem occurred, users either rephrased their ques-
tions (i.e., the same question expressed in a dif-
ferent way) or gave up the question and went on
specifying a new question. (Here we use Rephrase
and New to denote these two kinds of behaviors.)
We have not observed any sub-dialogs initiated by
Problematic Error-free Total
New Switch Continue
unstruct. 29 90 119
struct. 29 133 162
entire 58 223 281
Rephrase Re-try Negotiate
unstruct. 19 4 23
struct. 102 6 108
entire 121 10 131
Total-unst 48 94 142
Total-st 131 139 270
Total-ent 179 233 412
Table 1: Categorization of user intent with the cor-
responding number of occurrences from the un-
structured scenarios, the structured scenarios, and
the entire dataset.
the user to clarify a previous question or answer.
One possible explanation is that the current inves-
tigation was conducted in a basic interactive mode
where the system was only capable of providing
some sort of answers. This may limit users? expec-
tation in the kind of questions that can be handled
by the system. Our assumption is that, once the
QA system becomes more intelligent and able to
carry on conversation, different types of questions
(i.e., other than rephrase or new) will be observed.
This hypothesis certainly needs to be validated in
a conversational setting.
The third observation is that the rephrased ques-
tions seem to strongly correlate with problematic
situations, although not always. New questions
cannot distinguish a problematic situation from
an error-free situation. Table 1 shows the statis-
tics from our data about different combinations
of new/rephrase questions and performance situ-
ations2. What is interesting is that these different
combinations can reflect different types of user in-
tent behind the questions. More specifically, given
a question, four types of user intent can be cap-
tured with respect to the context (e.g., the previous
question and answer)
Continue indicates that the user is satisfied with
the previous answer and now moves on to this
new question.
Switch indicates that the user has given up on the
previous question and now moves on to this
2The last question from each interaction session is not in-
cluded in these statistics because there is no follow-up ques-
tion after that.
60
new question.
Re-try indicates that the user is not satisfied with
the previous answer and now tries to get a
better answer.
Negotiate indicates that the user is not satisfied
with the previous answer (although it ap-
pears to be correct from the system?s point
of view) and now tries to get a better answer
for his/her own needs.
Table 1 summarizes these different types of
intent together with the number of correspond-
ing occurrences from both structured and unstruc-
tured scenarios. Since in the unstructured sce-
narios it was hard to anticipate user?s questions
and therefore take a correct action to respond to a
problematic/error-free situation, the distribution of
these two situations is much more skewed than the
distribution for the structured scenarios. Also as
mentioned earlier, in unstructured scenarios, users
lacked the motivation to pursue specific informa-
tion, so the ratio between switch and re-try is much
larger than that observed in the structured scenar-
ios. Nevertheless, we did observe different user
behavior in response to different situations. As
discussed later in Section 5, identifying these fine-
grained intents will allow QA systems to be more
proactive in helping users find satisfying answers.
4 Automatic Identification of
Problematic Situations and User Intent
Given the discussion above, the next question is
how to automatically identify problematic situa-
tions and user intent. We formulate this as a classi-
fication problem. Given a question Qi, its answer
Ai, and the follow-up question Qi+1:
(1) Automatic identification of problematic situa-
tions is to decide whether Ai is problematic (i.e.,
correct or incorrect) based on the follow-up ques-
tion Qi+1 and the interaction context. This is a
binary classification problem.
(2) Automatic identification of user intent is to
identify the intent of Qi+1 given the interaction
context. Because we only have very limited in-
stances of Negotiate (see Table 1), we currently
merge Negotiate with Re-try since both of them
represent a situation where a better answer is re-
quested. Thus, this problem becomes a trinary
classification problem.
To build these classifiers, we identified a set of
features, which are illustrated next.
4.1 Features
Given a question Qi, its answer Ai, and the follow-
up question Qi+1, the following set of features are
used:
Target matching(TM): a binary feature indicat-
ing whether the target type of Qi+1 is the same as
the target type of Qi. Our data shows that the rep-
etition of the target type may indicate a rephrase,
which could signal a problematic situation has just
happened.
Named entity matching (NEM): a binary feature
indicating whether all the named entities in Qi+1
also appear in Qi. If no new named entity is in-
troduced in Qi+1, it is likely Qi+1 is a rephrase of
Qi.
Similarity between questions (SQ): a numeric
feature measuring the similarity between Qi+1 and
Qi. Our assumption is that the higher the simi-
larity is, the more likely the current question is a
rephrase to the previous one.
Similarity between content words of questions
(SQC): this feature is similar to the previous fea-
ture (i.e., SQ) except that the similarity measure-
ment is based on the content words excluding
named entities. This is to prevent the similarity
measurement from being dominated by the named
entities.
Similarity between Qi and Ai (SA): this feature
measures how close the retrieved passage matches
the question. Our assumption is that although a re-
trieved passage is the most relevant passage com-
pared to others, it still may not contain the answer
(e.g., when an answer does not even exist in the
data collection).
Similarity between Qi and Ai based on the con-
tent words (SAC): this feature is essentially the
same as the previous feature (SA) except that the
similarity is calculated after named entities are re-
moved from the questions and answers.
Note that since our data is currently collected
from simulation studies, we do not have the confi-
dence score from the retrieval engine associated
with every answer. In practice, the confidence
score can be used as an additional feature.
Since our focus is not on the similarity measure-
ment but rather the use of the measurement in the
classification models, our current similarity mea-
surement is based on a simple approach that mea-
sures commonality and difference between two
objects as proposed by Lin (1998). More specifi-
cally, the following equation is applied to measure
61
the similarity between two chunks of text T
1
and
T
2
:
sim
1
(T
1
, T
2
) =
? logP (T
1
? T
2
)
? logP (T
1
? T
2
)
Assume the occurrence of each word is indepen-
dent, then:
sim
1
(T
1
, T
2
) =
?
?
w?T
1
?T
2
log P (w)
?
?
w?T
1
?T
2
log P (w)
where P (w) was calculated based on the data used
in the previous TREC evaluations.
4.2 Identification of Problematic Situations
To identify problematic situations, we experi-
mented with three different classifiers: Maxi-
mum Entropy Model (MEM) from MALLET3,
SVM from SVM-Light4, and Decision Trees from
WEKA5. A leave-one-out validation was applied
where one interaction session was used for testing
and the remaining interaction sessions were used
for training.
Table 2 shows the performance of the three
models based on different combinations of fea-
tures in terms of classification accuracy. The base-
line result is the performance achieved by sim-
ply assigning the most frequently occurred class.
For the unstructured scenarios, the performance
of the classifiers is rather poor, which indicates
that it is quite difficult to make any generaliza-
tion based on the current feature sets when users
are less motivated in finding specific information.
For the structured scenarios, the best performance
for each model is highlighted in bold in Table 2.
The Decision Tree model achieves the best per-
formance of 77.8% in identifying problematic sit-
uations, which is more than 25% better than the
baseline performance.
4.3 Identification of User Intent
To identify user intent, we formulate the problem
as follows: given an observation feature vector f
where each element of the vector corresponds to
a feature described earlier, the goal is to identify
an intent c? from a set of intents I ={Continue,
Switch, Re-try/Negotiate} that satisfies the follow-
ing equation:
c? = argmaxc?IP (c|f)
3http://mallet.cs.umass.edu/index.php/
4http://svmlight.joachims.org/
5http://www.cs.waikato.ac.nz/ml/weka/
Our assumption is that user intent for a ques-
tion can be potentially influenced by the intent
from a preceding question. For example, Switch
is likely to follow Re-try. Therefore, we have im-
plemented a Maximum Entropy Markov Model
(MEMM) (McCallum et al, 2000) to take the se-
quence of interactions into account.
Given a sequence of questions Q
1
, Q
2
, up to Qt,
there is an observation feature vector f
i
associated
with each Qi. In MEMM, the prediction of user
intent ct for Qt not only depends on the observa-
tion f
t
, but also the intent ct?1 from the preceding
question Qt?1. In fact, this approach finds the best
sequence of user intent C? for Q
1
up to Qt based
on a sequence of observations f
1
, f
2
, ..., ft as fol-
lows:
C? = argmaxC?ItP (C|f1, f2, ..., ft)
where C is a sequence of intent and It is the set of
all possible sequences of intent with length t.
To find this sequence of intent C?, MEMM
keeps a variable ?t(i) which is defined to be the
maximum probability of seeing a particular se-
quence of intent ending at intent i (i ? I) for
question Qt, given the observation sequence for
questions Q
1
up to Qt:
?t(i) = maxc
1
,...,c
t?1
P (c
1
, . . . , ct?1, ct = i|f1, . . . , ft)
This variable can be calculated by a dynamic
optimization procedure similar to the Viterbi algo-
rithm in the Hidden Markov Model:
?t(i) = maxj ?t?1(j) ? P (ct = i|ct?1 = j, ft)
where P (ct = i|ct?1 = j, ft) is estimated by the
Maximum Entropy Model.
Table 3 shows the best results of identifying
user intent based on the Maximum Entropy Model
and MEMM using the leave-one-out approach.
The results have shown that both models did not
work for the data collected from unstructured sce-
narios (i.e., the baseline accuracy for intent iden-
tification is 63.4%). For structured scenarios, in
terms of the overall accuracy, both models per-
formed significantly better than the baseline (i.e.,
49.3%). The MEMM worked only slightly better
than the MEM. Given our limited data, it is not
conclusive whether the transitions between ques-
tions will help identify user intent in a basic inter-
active mode. However, we expect to see more in-
fluence from the transitions in fully conversational
QA.
62
MEM SVM DTree
Features un s ent un s ent un s ent
Baseline 66.2 51.5 56.3 66.2 51.5 56.3 66.2 51.5 56.3
TM, SQC 50.0 57.4 54.9 53.5 60.0 57.8 53.5 55.9 55.1
NEM, SQC 37.3 74.4 61.7 37.3 74.4 61.7 37.3 74.4 61.7
TM, SQ 61.3 64.8 63.6 57.0 64.1 61.7 59.9 64.4 62.9
NEM, SQC, SAC 40.8 76.7 64.3 38.0 74.4 61.9 49.3 77.8 68.0
TM, SQ, SAC 59.2 67.4 64.6 61.3 66.3 64.6 62.7 65.6 64.6
TM, NEM, SQC 54.2 75.2 68.0 54.2 75.2 68.0 53.5 74.4 67.2
TM, SQ, SA 63.4 71.9 68.9 58.5 71.5 67.0 67.6 75.6 72.8
TM, NEM, SQC, SAC 54.9 75.6 68.4 54.2 75.2 68.0 55.6 74.4 68.0
* un - unstructured, s - structured, ent - entire
Table 2: Performance of automatic identification of problematic situations
MEM MEMM
un s un s
CONTINUE P 64.4 69.7 67.3 70.8
R 96.7 85.8 80.0 88.8
F 77.3 76.8 73.1 78.7
RE-TRY P 28.6 76.2 37.1 79.0
/NEGOTIATE R 8.7 74.1 56.5 73.1
F 13.3 75.1 44.8 75.9
SWITCH P - - - 50.0
R 0 0 0 3.6
F - - - 6.7
Overall accuracy 62.7 72.2 59.9 73.7
* un - unstructured, s - structured
Table 3: Performance of automatic identification
of user intent
5 Implications of Problematic Situations
and User Intent
Automated identification of problematic situations
and user intent have potential implications in the
design of conversational QA systems. Identifica-
tion of problematic situations can be considered as
implicit feedback. The system can use this feed-
back to improve its answer retrieval performance
and proactively adapt its strategy to cope with
problematic situations. One might think that an
alternative way is to explicitly ask users for feed-
back. However, this explicit approach will defeat
the purpose of intelligent conversational systems.
Soliciting feedback after each question not only
will frustrate users and lengthen the interaction,
but also will interrupt the flow of user thoughts and
conversation. Therefore, our focus here is to inves-
tigate the more challenging end of implicit feed-
back. In practice, the explicit feedback and im-
plicit feedback should be intelligently combined.
For example, if the confidence for automatically
identifying a problematic situation or an error-free
situation is low, then perhaps explicit feedback can
be solicited.
Automatic identification of user intent also has
important implications in building intelligent con-
versational QA systems. For example, if Con-
tinue is identified during interaction, then the sys-
tem can automatically collect the question answer
pairs for potential future use. If Switch is identi-
fied, the system may put aside the question that has
not been correctly answered and proactively come
back to that question later after more information
is gathered. If Re-try is identified, the system may
avoid repeating the same answer and at the same
time may take the initiative to guide users on how
to rephrase a question. If Negotiate is identified,
the system may want to investigate the user?s par-
ticular needs that may be different from the gen-
eral needs. Overall, different strategies can be de-
veloped to address problematic situations and dif-
ferent intents. We will investigate these strategies
in our future work.
This paper reports our initial effort in investi-
gating interactive QA from a conversational point
of view. The current investigation has several
simplifications. First, our current work has fo-
cused on factoid questions where it is relatively
easy to judge a problematic or error-free situation.
However, as discussed in earlier work (Small et
al., 2003), sometimes it is very hard to judge the
truthfulness of an answer, especially for analyti-
cal questions. Therefore, our future work will ex-
amine the new implications of problematic situa-
tions and user intent for analytical questions. Sec-
63
ond, our current investigation is based on a ba-
sic interactive mode. As mentioned earlier, once
the QA systems become more intelligent and con-
versational, more varieties of user intent are an-
ticipated. How to characterize and automatically
identify more complex user intent under these dif-
ferent situations is another direction of our future
work.
6 Conclusion
This paper presents our initial investigation on
automatic identification of problematic situations
and user intent in interactive QA. Our results have
shown that, once users are motivated in finding
specific information related to their information
goals, user behavior and interaction context can
help automatically identify problematic situations
and user intent. Although our current investigation
is based on the data collected from a controlled
study, the same approaches can be applied dur-
ing online processing as the question answering
proceeds. The identified problematic situations
and/or user intent will provide immediate feed-
back for a QA system to adjust its behavior and
adapt better strategies to cope with different situa-
tions. This is an important step toward intelligent
conversational question answering.
References
Marco De Boni and Suresh Manandhar. 2003. An
analysis of clarification dialogues for question an-
swering. In Proceedings of HLT-NAACL 2003,
pages 48?55.
John Burger, Claire Cardie, Vinay Chaudhri, Robert
Gaizauskas, Sanda Harabagiu, David Israel, Chris-
tian Jacquemin, Chin-Yew Lin, Steve Maiorano,
George Miller, Dan Moldovan, Bill Ogden, John
Prager, Ellen Riloff, Amit Singhal, Rohini Shrihari,
Tomek Strzalkowski, Ellen Voorhees, and Ralph
Weishedel. 2001. Issues, tasks and program struc-
tures to roadmap research in question & answering.
In NIST Roadmap Document.
Sanda Harabagiu, Andrew Hickl, John Lehmann, and
Dan Moldovan. 2005. Experiments with interactive
question-answering. In Proceedings of the 43rd An-
nual Meeting of the Association for Computational
Linguistics (ACL?05), pages 205?214, Ann Arbor,
Michigan, June. Association for Computational Lin-
guistics.
Julia Hirschberg, Diane J. Litman, and Marc Swerts.
2001. Identifying user corrections automatically
in spoken dialogue systems. In Proceedings of
the Second Meeting of the North American Chap-
ter of the Association for Computational Linguistics
(NAACL?01).
Gina-Anne Levow. 1998. Characterizeing and recog-
nizing spoken corrections in human-computer dia-
logue. In Proceedings of the 36th Annual Meet-
ing of the Association of Computational Linguistics
(COLING/ACL-98), pages 736?742.
Dekang Lin. 1998. An information-theoretic defini-
tion of similarity. In Proceedings of International
Conference on Machine Learning, Madison, Wis-
consin, July.
Diane J. Litman and Shimei Pan. 2000. Predicting
and adapting to poor speech recognition in a spo-
ken dialogue system. In Proceedings of the Seven-
teenth National Conference on Artificial Intelligence
(AAAI-2000), pages 722?728.
Diane J. Litman, Marilyn A. Walker, and Michael S.
Kearns. 1999. Automatic detection of poor speech
recognition at the dialogue level. In Proceedings of
the 37th Annual meeting of the Association of Com-
putational Linguistics (ACL-99), pages 309?316.
Andrew McCallum, Dayne Freitag, and Fernando
Pereira. 2000. Maximum entropy markov mod-
els for information extraction and segmentation. In
Proceedings of Internatioanl Conference on Ma-
chine Learning (ICML 2000), pages 591?598.
Sharon Small, Ting Liu, Nobuyuki Shimizu, and
Tomek Strzalkowski. 2003. HITIQA: An interac-
tive question answering system: A preliminary re-
port. In Proceedings of the ACL 2003 Workshop on
Multilingual Summarization and Question Answer-
ing.
Andreas Stolcke, Klaus Ries, Noah Coccaro, Elizabeth
Shriberg, Rebecca Bates, Daniel Jurafsky, Paul Tay-
lor, Rachel Martin, Marie Meteer, and Carol Van
Ess-Dykema. 2000. Dialogue act modeling for au-
tomatic tagging and recognition of conversational
speech. In Computational Linguistics, volume 26.
Ellen Voorhees. 2001. Overview of TREC 2001 ques-
tion answering track. In Proceedings of TREC.
Ellen Voorhees. 2004. Overview of TREC 2004. In
Proceedings of TREC.
Marilyn Walker, Irene Langkilde-Geary, Helen Wright
Hastie, Jerry Wright, and Allen Gorin. 2002. Auto-
matically training a problematic dialogue predictor
for the HMIHY spoken dialog system. In Journal of
Artificial Intelligence Research.
64
Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 368?375,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Automated Vocabulary Acquisition and Interpretation
in Multimodal Conversational Systems
Yi Liu Joyce Y. Chai Rong Jin
Department of Computer Science and Engineering
Michigan State University
East Lansing, MI 48824, USA
{liuyi3, jchai, rongjin}@cse.msu.edu
Abstract
Motivated by psycholinguistic findings that
eye gaze is tightly linked to human lan-
guage production, we developed an unsuper-
vised approach based on translation models
to automatically learn the mappings between
words and objects on a graphic display dur-
ing human machine conversation. The ex-
perimental results indicate that user eye gaze
can provide useful information to establish
such mappings, which have important impli-
cations in automatically acquiring and inter-
preting user vocabularies for conversational
systems.
1 Introduction
To facilitate effective human machine conversation,
it is important for a conversational system to have
knowledge about user vocabularies and understand
how these vocabularies are mapped to the internal
entities for which the system has representations.
For example, in a multimodal conversational system
that allows users to converse with a graphic inter-
face, the system needs to know what vocabularies
users tend to use to describe objects on the graphic
display and what (type of) object(s) a user is attend-
ing to when a particular word is expressed. Here,
we use acquisition to refer to the process of acquir-
ing relevant vocabularies describing internal entities,
and interpretation to refer to the process of automat-
ically identifying internal entities given a particular
word. Both acquisition and interpretation have been
traditionally approached by either knowledge engi-
neering (e.g., manually created lexicons) or super-
vised learning from annotated data. In this paper,
we describe an unsupervised approach that relies
on naturally co-occurred eye gaze and spoken utter-
ances during human machine conversation to auto-
matically acquire and interpret vocabularies.
Motivated by psycholinguistic studies (Just and
Carpenter, 1976; Griffin and Bock, 2000; Tenenhaus
et al, 1995) and recent investigations on computa-
tional models for language acquisition and ground-
ing (Siskind, 1995; Roy and Pentland, 2002; Yu
and Ballard, 2004), we are particularly interested in
two unique questions related to multimodal conver-
sational systems: (1) In a multimodal conversation
that involves more complex tasks (e.g., both user
initiated tasks and system initiated tasks), is there
a reliable temporal alignment between eye gaze and
spoken references so that the coupled inputs can be
used for automated vocabulary acquisition and inter-
pretation? (2) If such an alignment exists, how can
we model this alignment and automatically acquire
and interpret the vocabularies?
To address the first question, we conducted an
empirical study to examine the temporal relation-
ships between eye fixations and their correspond-
ing spoken references. As shown later in section 4,
although a larger variance (compared to the find-
ings from psycholinguistic studies) exists in terms of
how eye gaze is linked to speech production during
human machine conversation, eye fixations and the
corresponding spoken references still occur in a very
close vicinity to each other. This natural coupling
between eye gaze and speech provides an opportu-
nity to automatically learn the mappings between
368
words and objects without any human supervision.
Because of the larger variance, it is difficult to
apply rule-based approaches to quantify this align-
ment. Therefore, to address the second question,
we developed an approach based on statistical trans-
lation models to explore the co-occurrence patterns
between eye fixated objects and spoken references.
Our preliminary experiment results indicate that the
translation model can reliably capture the mappings
between the eye fixated objects and the correspond-
ing spoken references. Given an object, this model
can provide possible words describing this object,
which represents the acquisition process; given a
word, this model can also provide possible objects
that are likely to be described, which represents the
interpretation process.
In the following sections, we first review some re-
lated work and introduce the procedures used to col-
lect eye gaze and speech data during human machine
conversation. We then describe our empirical study
and the unsupervised approach based on translation
models. Finally, we present experiment results and
discuss their implications in natural language pro-
cessing applications.
2 Related Work
Our work is motivated by previous work in the fol-
lowing three areas: psycholinguistics studies, multi-
modal interactive systems, and computational mod-
eling of language acquisition and grounding.
Previous psycholinguistics studies have shown
that the direction of gaze carries information about
the focus of the user?s attention (Just and Carpenter,
1976). Specifically, in human language processing
tasks, eye gaze is tightly linked to language produc-
tion. The perceived visual context influences spo-
ken word recognition and mediates syntactic pro-
cessing (Tenenhaus et al, 1995). Additionally, be-
fore speaking a word, the eyes usually move to the
objects to be mentioned (Griffin and Bock, 2000).
These psycholinguistics findings have provided a
foundation for our investigation.
In research on multimodal interactive systems, re-
cent work indicates that the speech and gaze inte-
gration patterns can be modeled reliably for indi-
vidual users and therefore be used to improve mul-
timodal system performances (Kaur et al, 2003).
Studies have also shown that eye gaze has a poten-
tial to improve resolution of underspecified referring
expressions in spoken dialog systems (Campana et
al., 2001) and to disambiguate speech input (Tanaka,
1999). In contrast to these earlier studies, our work
focuses on a different goal of using eye gaze for au-
tomated vocabulary acquisition and interpretation.
The third area of research that influenced our
work is computational modeling of language acqui-
sition and grounding. Recent studies have shown
that multisensory information (e.g., through vision
and language processing) can be combined to effec-
tively acquire words to their perceptually grounded
objects in the environment (Siskind, 1995; Roy and
Pentland, 2002; Yu and Ballard, 2004). Especially in
(Yu and Ballard, 2004), an unsupervised approach
based on a generative correspondence model was
developed to capture the mapping between spoken
words and the occurring perceptual features of ob-
jects. This approach is most similar to the transla-
tion model used in our work. However, compared
to this work where multisensory information comes
from vision and language processing, our work fo-
cuses on a different aspect. Here, instead of applying
vision processing on objects, we are interested in eye
gaze behavior when users interact with a graphic dis-
play. Eye gaze is an implicit and subconscious input
modality during human machine interaction. Eye
gaze data inevitably contain a significant amount of
noise. Therefore, it is the goal of this paper to exam-
ine whether this modality can be utilized for vocab-
ulary acquisition for conversational systems.
3 Data Collection
We used a simplified multimodal conversational sys-
tem to collect synchronized speech and eye gaze
data. A room interior scene was displayed on a com-
puter screen, as shown in Figure 1. While watching
the graphical display, users were asked to communi-
cate with the system on topics about the room dec-
orations. A total of 28 objects (e.g., multiple lamps
and picture frames, a bed, two chairs, a candle, a
dresser, etc., as marked in Figure 1) are explicitly
modeled in this scene. The system is simplified in
the sense that it only supports 14 tasks during human
machine interaction. These tasks are designed to
cover both open-ended utterances (e.g., the system
369
Figure 1: The room interior scene for user studies.
For easy reference, we give each object an ID. These
IDs are hidden from the system users.
asks users to describe the room) and more restricted
utterances (e.g., the system asks the user whether
he/she likes the bed) that are commonly supported in
conversational systems. Seven human subjects par-
ticipated in our study.
User speech inputs were recorded using the Au-
dacity software1, with each utterance time-stamped.
Eye movements were recorded using an EyeLink II
eye tracker sampled at 250Hz. The eye tracker au-
tomatically saved two-dimensional coordinates of a
user?s eye fixations as well as the time-stamps when
the fixations occurred.
The collected raw gaze data is extremely noisy.
To refine the gaze data, we further eliminated in-
valid and saccadic gaze points (known as ?saccadic
suppression? in vision studies). Since eyes do not
stay still but rather make small, frequent jerky move-
ments, we also smoothed the data by averaging
nearby gaze locations to identify fixations.
4 Empirical Study on Speech-Gaze
Alignment
Based on the data collected, we investigated the tem-
poral alignment between co-occurred eye gaze and
spoken utterances. In particular, we examined the
temporal alignment between eye gaze fixations and
the corresponding spoken references (i.e., the spo-
ken words that are used to refer to the objects on the
graphic display).
According to the time-stamp information, we can
1http://audacity.sourceforge.net/
measure the length of time gap between a user?s eye
fixation falling on an object and the corresponding
spoken reference being uttered (which we refer to
as ?length of time gap? for brevity). Also, we can
count the number of times that user fixations hap-
pen to change their target objects during this time
gap (which we refer to as ?number of fixated object
changes? for brevity). The nine most frequently oc-
curred spoken references in utterances from all users
(as shown in Table 1) are chosen for this empirical
study. For each of those spoken references, we use
human judgment to decide which object is referred
to. Then, from both before and after the onset of
the spoken reference, we find the closest occurrence
of the fixation falling on that particular object. Al-
together we have 96 such speech-gaze pairs. In 54
pairs, the eye gaze fixation occurred before the cor-
responding speech reference was uttered; and in the
other 42 pairs, the eye fixation occurred after the
corresponding speech reference was uttered. This
observation suggests that in human machine conver-
sation, eye fixation on an object does not necessarily
always proceed the utterance of the corresponding
speech reference.
Further, we computed the average absolute length
of the time gap and the average number of fixated
object changes, as well as their variances for each of
5 selected users2 as shown in Table 1. From Table 1,
it is easy to observe that: (I) A spoken reference al-
ways appears within a short period of time (usually
1-2 seconds) before or after the corresponding eye
gaze fixation. But, the exact length of the period is
far from constant. (II) It is not necessary for a user
to utter the corresponding spoken reference imme-
diately before or after the eye gaze fixation falls on
that particular object. Eye gaze fixations may move
back and forth. Between the time an object is fixated
and the corresponding spoken reference is uttered, a
user?s eye gaze may fixate on a few other objects
(reflected by the average number of eye fixated ob-
ject changes shown in the table). (III) There is a
large variance in both the length of time gap and the
number of fixated object changes in terms of 1) the
same user and the same spoken reference at differ-
ent time-stamps, 2) the same user but different spo-
2The other two users are not selected because the nine se-
lected words do not appear frequently in their utterances.
370
Spoken Average Absolute Length of Time Gap (in seconds) Average Number of Eye Fixated Object Changes
Reference User 1 User 2 User 3 User 4 User 5 User 1 User 2 User 3 User 4 User 5
bed 1.27? 1.40 1.02? 0.65 0.32? 0.21 0.59? 0.77 2.57? 3.25 2.1? 3.2 2.1? 2.2 0.4? 0.5 1.4? 2.2 5.3? 7.9
tree - 0.24? 0.24 - - - - 0.0? 0.0 - - -
window - 0.67? 0.74 - - 1.95? 3.20 - 0.0? 0.0 - - 3.3? 5.9
mirror - 1.04? 1.36 - - - - 1.0? 1.4 - - -
candle - - 3.64? 0.59 - - - - 8.5? 2.1 - -
waterfall 1.80? 1.12 - - - - 5.5? 4.9 - - - -
painting 0.10? 0.10 - - - - 0.2? 0.4 - - - -
lamp 0.74? 0.54 1.70? 0.99 0.26? 0.35 1.98? 1.72 2.84? 2.42 1.3? 1.3 1.8? 1.5 0.3? 0.6 4.8? 4.3 2.7? 2.2
door 2.47? 0.84 - - 2.49? 1.90 6.36? 2.29 5.0? 2.6 - - 6.7? 5.5 13.3? 6.7
Table 1: The average absolute length of time and the number of eye fixated object changes within the time
gap of eye gaze and corresponding spoken references. Variances are also listed. Some of the entries are not
available because the spoken references were never or rarely used by the corresponding users.
ken references, and 3) the same spoken reference but
different users. We believe this is due to the different
dialog scenarios and user language habits.
To summarize our empirical study, we find that
in human machine conversation, there still exists a
natural temporal coupling between user speech and
eye gaze, i.e. the spoken reference and the corre-
sponding eye fixation happen within a close vicinity
of each other. However, a large variance is also ob-
served in terms of these temporal vicinities, which
indicates an intrinsically more complex gaze-speech
pattern. Therefore, it is hard to directly quantify
the temporal or ordering relationship between spo-
ken references and corresponding eye fixated objects
(for example, through rules).
To better handle the complexity in the gaze-
speech pattern, we propose to use statistical transla-
tion models. Given a time window of enough length,
a speech input that contains a list of spoken refer-
ences (e.g., definite noun phrases) is always accom-
panied by a list of naturally occurred eye fixations
and therefore a list of objects receiving those fixa-
tions. All those pairs of speech references and cor-
responding fixated objects could be viewed as paral-
lel, i.e. they co-occur within the time window. This
situation is very similar to the training process of
translation models in statistical machine translation
(Brown et al, 1993), where parallel corpus is used to
find the mappings between words from different lan-
guages by exploiting their co-occurrence patterns.
The same idea can be borrowed here: by exploring
the co-occurrence statistics, we hope to uncover the
exact mapping between those eye fixated objects and
spoken references. The intuition is that, the more of-
ten a fixation is found to exclusively co-occur with a
spoken reference, the more likely a mapping should
be established between them.
5 Translation Models for Vocabulary
Acquisition and Interpretation
Formally, we denote the set of observations by
D = {wi,oi}Ni=1 where wi and oi refers to
the i-th speech utterance (i.e., a list of words
of spoken references) and the i-th corresponding
eye gaze pattern (i.e., a list of eye fixated ob-
jects) respectively. When we study the prob-
lem of mapping given objects to words (for vo-
cabulary acquisition), the parameter space ? =
{Pr(wj |ok), 1 ? j ? mw, 1 ? k ? mo} consists of
the mapping probabilities of an arbitrary word wj
to an arbitrary object ok, where mw and mo repre-
sent the total number of unique words and objects
respectively. Those mapping probabilities are sub-
ject to constraints ?mwj=1 Pr(wj |ok) = 1. Note that
Pr(wj |ok) = 0 if the corresponding word wj and ok
never co-occur in any observed list pair (wi,oi).
Let lwi and loi denote the length of lists wi and
oi respectively. To distinguish with the notations
wj and ok whose subscripts are indices for unique
words and objects respectively, we use w?i,j to de-
note the word in the j-th position of the list wi and
o?i,k to denote the object in the k-th position of the
list oi. In translation models, we assume that any
word in the list wi is mapped to an object in the cor-
responding list oi or a null object (we reserve the
position 0 for it in every object list). To denote all
the word-object mappings in the i-th list pair, we in-
troduce an alignment vector ai, whose element ai,j
takes the value k if the word w?i,j is mapped to o?i,k.
Then, the likelihood of the observations given the
371
parameters can be computed as follows
Pr(D;?) =
N
?
i=1
Pr(wi|oi) =
N
?
i=1
?
ai
Pr(wi,ai|oi)
=
N
?
i=1
?
ai
Pr(lwi |oi)
(loi + 1)l
w
i
lwi
?
j=1
Pr(w?i,j |o?ai,j )
=
N
?
i=1
Pr(lwi |oi)
(loi + 1)l
w
i
?
ai
lwi
?
j=1
Pr(w?i,j |o?ai,j )
Note that the following equation holds:
lwi
?
j=1
loi
?
k=0
Pr(w?i,j |o?i,k) =
loi
?
ai,1=1
? ? ?
loi
?
ai,lwi =1
lwi
?
j=1
Pr(w?i,j |o?ai,j )
where the right-hand side is actually the expansion
of
?
ai
?lwi
j Pr(w?i,j |o?ai,j ). Therefore, the likelihood
can be simplified as
Pr(D;?) =
N
?
i=1
Pr(lwi |oi)
(loi + 1)l
w
i
lwi
?
j=1
loi
?
k=0
Pr(w?i,j |o?i,k)
Switching to the notations wj and ok, we have
Pr(D;?)=
N
?
i=1
Pr(lwi |oi)
(loi + 1)l
w
i
mw
?
j=1
[ mo
?
k=0
Pr(wj |ok)?oi,k
]?wi,j
where ?wi,j = 1 if w?i,j ? wi and ?wi,j = 0 otherwise,
and ?oi,k = 1 if o?i,k ? oi and ?oi,k = 0 otherwise.
Finally, the translation model can be formalized
as the following optimization problem
arg max? log Pr(D;?)
s.t.
mw
?
j=1
Pr(wj |ok) = 1,?k
This optimization problem can be solved by the EM
algorithm (Brown et al, 1993).
The above model is developed in the con-
text of mapping given objects to words, i.e., its
solution yields a set of conditional probabilities
{Pr(wj |ok),?j} for each object ok, indicating how
likely every word is mapped to it. Similarly, we
can develop the model in the context of mapping
given words to objects (for vocabulary interpreta-
tion), whose solution leads to another set of prob-
abilities {Pr(ok|wj),?k} for each word wj indicat-
ing how likely every object is mapped to it. In our
experiments, both models are implemented and we
will present the results later.
6 Experiments
We experimented our proposed statistical translation
model on the collected data mentioned in Section 3.
6.1 Preprocessing
The main purpose of preprocessing is to create a
?parallel corpus? for training a translation model.
Here, the ?parallel corpus? refers to a series of
speech-gaze pairs, each of them consisting of a list
of words from the spoken references in the user ut-
terances and a list of objects that are fixated upon
within the same time window.
Specifically, we first transcribed the user speech
into scripts by automatic speech recognition soft-
ware and then refined them manually. A time-stamp
was associated with each word in the speech script.
Further, we detected long pauses in the speech script
as splitting points to create time windows, since a
long pause usually marks the start of a sentence
that indicates a user?s attention shift. In our exper-
iment, we set the threshold of judging a long pause
to be 1 second. From all the data gathered from 7
users, we get 357 such time windows (which typi-
cally contain 10-20 spoken words and 5-10 fixated
object changes).
Given a time window, we then found the objects
being fixated upon by eye gaze (represented by their
IDs as shown in Figure 1). Considering that eye gaze
fixation could occur during the pauses in speech, we
expanded each time window by a fixed length at both
its start and end to find the fixations. In our experi-
ments, the expansion length is set to 0.5 seconds.
Finally, we applied a part-of-speech tagger to
each sentence in the user script and only singled out
nouns as potential spoken references in the word list.
The Porter stemming algorithm was also used to get
the normalized forms of those nouns.
The translation model was trained based on this
preprocessed parallel data.
6.2 Evaluation Metrics
As described in Section 5, by using a statistical
translation model we can get a set of translation
probabilities, either from any given spoken word to
all the objects, or from any given object to all the
spoken words. To evaluate the two sets of trans-
lation probabilities, we use precision and recall as
372
#Rank Precision Recall #Rank Precision Recall
1 0.6667 0.2593 6 0.2302 0.5370
2 0.4524 0.3519 7 0.2041 0.5556
3 0.3810 0.4444 8 0.1905 0.5926
4 0.3095 0.4815 9 0.1799 0.6296
5 0.2667 0.5185 10 0.1619 0.6296
Table 2: Average precision/recall of mapping given
objects to words (i.e., acquisition)
#Rank Precision Recall #Rank Precision Recall
1 0.7826 0.3214 6 0.3043 0.7500
2 0.5870 0.4821 7 0.2671 0.7679
3 0.4638 0.5714 8 0.2446 0.8036
4 0.3804 0.6250 9 0.2293 0.8393
5 0.3478 0.7143 10 0.2124 0.8571
Table 3: Average precision/recall of mapping given
words to objects.(i.e., interpretation)
evaluation metrics.
Specifically, for a given object ok the trans-
lation model will yield a set of probabilities
{Pr(wj |ok),?j}. We can sort the probabilities and
get a ranked list. Let us assume that we have the
ground truth about all the spoken words to which
the given object should be mapped. Then, at a given
number n of top ranked words, the precision of map-
ping the given object ok to words is defined as
# words that ok is correctly mapped to
# words that ok is mapped to
and the recall is defined as
# words that ok is correctly mapped to
# words that ok should be mapped to
All the counting above is done within the top n rank.
Therefore, we can get different precision/recall at
different ranks. At each rank, the overall perfor-
mance can be evaluated by averaging the preci-
sion/recall for all the given objects. Human judg-
ment is used to decide whether an object-word map-
ping is correct or not, as ground truth for evaluation.
Similarly, based on the set of probabilities of map-
ping a given object with spoken words, we can
find a ranked list of objects for a given word, i.e.
{Pr(ok|wj),?k}. Thus, at a given rank the preci-
sion and recall of mapping a given word wj to ob-
jects can be measured.
6.3 Experiment Results
Vocabulary acquisition is the process of finding
the appropriate word(s) for any given object. For
the sake of statistical significance, our evaluation is
done on 21 objects that were mentioned at least 3
times by the users.
Table 2 gives the average precision/recall evalu-
ated at the top 10 ranks. As we can see, if we use
the most probable word acquired for each object,
about 66.67% of them are appropriate. With the
rank increasing, more and more appropriate words
can be acquired. About 62.96% of all the appropri-
ate words are included within the top 10 probable
words found. The results indicate that by using a
translation model, we can obtain the words that are
used by the users to describe the objects with rea-
sonable accuracy.
Table 4 presents the top 3 most probable words
found for each object. It shows that although there
may be more than one word appropriate to describe
a given object, those words with highest probabil-
ities always suggest the most popular way of de-
scribing the corresponding object among the users.
For example, for the object with ID 26, the word
candle gets a higher probability than the word
candlestick, which is in accordance with our
observation that in our user study, on most occasions
users tend to use the word candle rather than the
word candlestick.
Vocabulary interpretation is the process of find-
ing the appropriate object(s) for any given spoken
word. Out of 176 nouns in the user vocabulary,
we only evaluate those used at least three times for
statistical significance concerns. Further, abstract
words (such as reason, position) and general
words (such as room, furniture) are not eval-
uated since they do not refer to any particular objects
in the scene. Finally, 23 nouns remain for evalua-
tion.
We manually enumerated all the object(s) that
those 23 nouns refer to as the ground truth in our
evaluation. Note that a given noun can possibly
be used to refer to multiple objects, such as lamp,
since we have several lamps (with object ID 3, 8, 17,
and 23) in the experiment setting, and bed, since
bed frame, bed spread, and pillows (with object ID
19, 21, and 20 respectively) are all part of a bed.
Also, an object can be referred to by multiple nouns.
For example, the words painting, picture,
or waterfall can all be used to refer to the ob-
ject with ID 15.
373
Object Rank 1 Rank 2 Rank 3
1 paint (0.254) * wall (0.191) left (0.150)
2 pictur (0.305) * girl (0.122) niagara (0.095) *
3 wall (0.109) lamp (0.093) * floor (0.084)
4 upsid (0.174) * left (0.151) * paint (0.149) *
5 pictur (0.172) window (0.157) * wall (0.116)
6 window (0.287) * curtain (0.115) pictur (0.076)
7 chair (0.287) * tabl (0.088) bird (0.083)
9 mirror (0.161) * dresser (0.137) bird (0.098) *
12 room (0.131) lamp (0.127) left (0.069)
14 hang (0.104) favourit (0.085) natur (0.064)
15 thing (0.066) size (0.059) queen (0.057)
16 paint (0.211) * pictur (0.116) * forest (0.076) *
17 lamp (0.354) * end (0.154) tabl (0.097)
18 bedroom (0.158) side (0.128) bed (0.104)
19 bed (0.576) * room (0.059) candl (0.049)
20 bed (0.396) * queen (0.211) * size (0.176)
21 bed (0.180) * chair (0.097) orang (0.078)
22 bed (0.282) door (0.235) * chair (0.128)
25 chair (0.215) * bed (0.162) candlestick (0.124)
26 candl (0.145) * chair (0.114) candlestick (0.092) *
27 tree (0.246) * chair (0.107) floor (0.096)
Table 4: Words found for given objects. Each row
lists the top 3 most probable spoken words (being
stemmed) for the corresponding given object, with
the mapping probabilities in parentheses. Asterisks
indicate correctly identified spoken words. Note
that some objects are heavily overlapped, so the cor-
responding words are considered correct for all the
overlapping objects, such as bed being considered
correct for objects with ID 19, 20, and 21.
Word Rank 1 Rank 2 Rank 3 Rank 4
curtain 6 (0.305) * 5 (0.305) * 7 (0.133) 1 (0.121)
candlestick 25 (0.147) * 28 (0.135) 24 (0.131) 22 (0.117)
lamp 22 (0.126) 12 (0.094) 17 (0.093) * 25 (0.093)
dresser 12 (0.298) * 9 (0.294) * 13 (0.173) * 7 (0.104)
queen 20 (0.187) * 21 (0.182) * 22 (0.136) 19 (0.136) *
door 22 (0.200) * 27 (0.124) 25 (0.108) 24 (0.106)
tabl 9 (0.152) * 12 (0.125) * 13 (0.112) * 22 (0.107)
mirror 9 (0.251) * 12 (0.238) 8 (0.109) 13 (0.081)
girl 2 (0.173) 22 (0.128) 16 (0.099) 10 (0.074)
chair 22 (0.132) 25 (0.099) * 28 (0.085) 24 (0.082)
waterfal 6 (0.226) 5 (0.215) 1 (0.118) 9 (0.083)
candl 19 (0.156) 22 (0.139) 28 (0.134) 24 (0.131)
niagara 4 (0.359) * 2 (0.262) * 1 (0.226) 7 (0.045)
plant 27 (0.230) * 22 (0.181) 23 (0.131) 28 (0.117)
tree 27 (0.352) * 22 (0.218) 26 (0.100) 13 (0.062)
upsid 4 (0.204) * 12 (0.188) 9 (0.153) 1 (0.104) *
bird 9 (0.142) * 10 (0.138) 12 (0.131) 7 (0.121)
desk 12 (0.170) * 9 (0.141) * 19 (0.118) 8 (0.118)
bed 19 (0.207) * 22 (0.141) 20 (0.111) * 28 (0.090)
upsidedown 4 (0.243) * 3 (0.219) 6 (0.203) 5 (0.188)
paint 4 (0.188) * 16 (0.148) * 1 (0.137) * 15 (0.118) *
window 6 (0.305) * 5 (0.290) * 3 (0.085) 22 (0.065)
lampshad 3 (0.223) * 7 (0.137) 11 (0.137) 10 (0.137)
Table 5: Objects found for given words. Each row
lists the 4 most probable object IDs for the corre-
sponding given words (being stemmed), with the
mapping probabilities in parentheses. Asterisks in-
dicate correctly identified objects. Note that some
objects are heavily overlapped, such as the candle
(with object ID 26) and the chair (with object ID
25), and both were considered correct for the re-
spective spoken words.
Table 3 gives the average precision/recall evalu-
ated at the top 10 ranks. As we can see, if we use the
most probable object found for each speech word,
about 78.26% of them are appropriate. With the rank
increasing, more and more appropriate objects can
be found. About 85.71% of all the appropriate ob-
jects are included within the top 10 probable objects
found. The results indicate that by using a trans-
lation model, we can predict the objects from user
spoken words with reasonable accuracy.
Table 5 lists the top 4 probable objects found for
each spoken word being evaluated. A close look re-
veals that in general, the top ranked objects tend to
gather around the correct object for a given spoken
word. This is consistent with the fact that eye gaze
tends to move back and forth. It also indicates that
the mappings established by the translation model
can effectively find the approximate area of the cor-
responding fixated object, even if it cannot find the
object due to the noisy and jerky nature of eye gaze.
The precision/recall in vocabulary acquisition is
not as high as that in vocabulary interpretation, par-
tially due to the relatively small scale of our exper-
iment data. For example, with only 7 users? speech
data on 14 conversational tasks, some words were
only spoken a few times to refer to an object, which
prevented them from getting a significant portion of
probability mass among all the words in the vocab-
ulary. This degrades both precision and recall. We
believe that in large scale experiments or real-world
applications, the performance will be improved.
7 Discussion and Conclusion
Previous psycholinguistic findings have shown that
eye gaze is tightly linked with human language pro-
duction. During human machine conversation, our
study shows that although a larger variance is ob-
served on how eye fixations are exactly linked with
corresponding spoken references (compared to the
psycholinguistic findings), eye gaze in general is
closely coupled with corresponding referring ex-
pressions in the utterances. This close coupling na-
ture between eye gaze and speech utterances pro-
vides an opportunity for the system to automatically
374
acquire different words related to different objects
without any human supervision. To further explore
this idea, we developed a novel unsupervised ap-
proach using statistical translation models.
Our experimental results have shown that this ap-
proach can reasonably uncover the mappings be-
tween words and objects on the graphical display.
The main advantages of this approach include: 1) It
is an unsupervised approach with minimum human
inference; 2) It does not need any prior knowledge to
train a statistical translation model; 3) It yields prob-
abilities that indicate the reliability of the mappings.
Certainly, our current approach is built upon sim-
plified assumptions. It is quite challenging to in-
corporate eye gaze information since it is extremely
noisy with large variances. Recent work has shown
that the effect of eye gaze in facilitating spoken lan-
guage processing varies among different users (Qu
and Chai, 2007). In addition, visual properties of
the interface also affect user gaze behavior and thus
influence the predication of attention (Prasov et al,
2007) based on eye gaze. Our future work will de-
velop models to address these variations.
Nevertheless, the results from our current work
have several important implications in building ro-
bust conversational interfaces. First of all, most
conversational systems are built with static knowl-
edge space (e.g., vocabularies) and can only be up-
dated by the system developers. Our approach can
potentially allow the system to automatically ac-
quire knowledge and vocabularies based on the nat-
ural interactions with the users without human in-
tervention. Furthermore, the automatically acquired
mappings between words and objects can also help
language interpretation tasks such as reference res-
olution. Given the recent advances in eye track-
ing technology (Duchowski, 2002), integrating non-
intrusive and high performance eye trackers with
conversational interfaces becomes feasible. The
work reported here can potentially be integrated in
practical systems to improve the overall robustness
of human machine conversation.
Acknowledgment
This work was supported by funding from National
Science Foundation (IIS-0347548, IIS-0535112,
and IIS-0643494) and Disruptive Technology Of-
fice. The authors would like to thank Zahar Prasov
for his contribution to data collection.
References
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: Parameter estimation. Computa-
tional Linguistics, 19(2):263?311.
E. Campana, J. Baldridge, J. Dowding, B. A. Hockey,
R. Remington, and L. S. Stone. 2001. Using eye
movements to determine referents in a spoken dialog
system. In Proceedings of PUI?01.
A. T. Duchowski. 2002. A breath-first survey of eye
tracking applications. Behavior Research methods, In-
struments, and Computers, 33(4).
Z. M. Griffin and K. Bock. 2000. What the eyes say
about speaking. Psychological Science, 11:274?279.
M. A. Just and P. A. Carpenter. 1976. Eye fixations and
cognitive processes. Cognitive Psychology, 8:441?
480.
M. Kaur, M. Tremaine, N. Huang, J. Wilder, Z. Gacovski,
F. Flippo, and C. S. Mantravadi. 2003. Where is ?it??
Event synchronization in gaze-speech input systems.
In Proceedings of ICMI?03, pages 151?157.
Z. Prasov, J. Y. Chai, and H. Jeong. 2007. Eye gaze
for attention prediction in multimodal human-machine
conversation. In 2007 Spring Symposium on Inter-
action Challenges for Artificial Assistants, Palo Alto,
California, March.
S. Qu and J. Y. Chai. 2007. An exploration of eye gaze
in spoken language processing for multimodal con-
versational interfaces. In NAACL?07, pages 284?291,
Rochester, New York, April.
D. Roy and A. Pentland. 2002. Learning words from
sights and sounds, a computational model. Cognitive
Science, 26(1):113?1146.
J. M. Siskind. 1995. Grounding language in perception.
Artificial Intelligence Review, 8:371?391.
K. Tanaka. 1999. A robust selection system using real-
time multi-modal user-agent interactions. In Proceed-
ings of IUI?99, pages 105?108.
M. K. Tenenhaus, M. Sivey-Knowlton, E. Eberhard, and
J. Sedivy. 1995. Integration of visual and linguistic
information during spoken language comprehension.
Science, 268:1632?1634.
C. Yu and D. H. Ballard. 2004. On the integration of
grounding language and learning objects. Proceedings
of AAAI?04.
375
Combining Semantic and Temporal Constraints for Multimodal Integra-
tion in Conversation Systems 
Joyce Y. Chai 
Department of Computer 
Science and Engineering 
Michigan State University 
East Lansing, MI 48864 
jchai@cse.msu.edu 
Pengyu Hong  
Department of Statistics 
Harvard University 
Cambridge, MA 02138 
hong@stat.harvard.edu
Michelle X. Zhou 
IBM T. J. Watson Research 
Center 
19 Skyline Drive 
Hawthorne, NY 10532 
mzhou@us.ibm.com 
Abstract 
In a multimodal conversation, user refer-
ring patterns could be complex, involving 
multiple referring expressions from 
speech utterances and multiple gestures. 
To resolve those references, multimodal 
integration based on semantic constraints 
is insufficient. In this paper, we describe a 
graph-based probabilistic approach that 
simultaneously combines both semantic 
and temporal constraints to achieve a high 
performance.  
1 
                                                          
Introduction 
Multimodal conversation systems allow users to 
converse with systems through multiple modalities 
such as speech, gesture and gaze (Cohen et al, 
1996; Wahlster, 1998).  In such an environment, 
not only are more interaction modalities available, 
but also richer contexts are established during the 
interaction. Understanding user inputs, for 
example, what users refer to is important. Previous 
work on multimodal reference resolution includes 
the use of a focus space model (Neal et al, 1998), 
the centering framework (Zancanaro et al, 1997), 
context factors (Huls et al, 1995), and rules 
(Kehler 2000). These previous approaches focus 
on semantics constraints without fully addressing 
temporal constraints. In a user study1, we found 
that the majority of user referring behavior 
involved one referring expression and one gesture 
(as in [S2, G2] in Table 1). The earlier approaches 
worked well for these types of references. 
However, we found that 14.1% of the inputs were 
complex, which involved multiple referring 
expressions from speech utterances and multiple 
gestures (S3 in Table 1). To resolve those complex 
references, we have to not only apply semantic 
constraints, but also apply temporal constraints at 
the same time.  
For example, Figure 1 shows three inputs where 
the number of referring expressions is the same 
and the number of gestures is the same. The speech 
utterances and gestures are aligned along the time 
axis. The first case (Figure 1a) and the second case 
(Figure 1b) have the same speech utterance but 
different temporal alignment between the gestures 
and the speech input. The second case and the third 
case (Figure 1c) have a similar alignment, but the 
third case provides an additional constraint on the 
number of referents (from the word ?two?).  
Although all three cases are similar, but the 
objects they refer to are quite different in each 
case. In the first case, most likely ?this? refers to 
the house selected by the first point gesture and 
?these houses? refers to two houses selected by the 
other two gestures. In the second case, ?this? most 
likely refers to the highlighted house on the display 
and ?these houses? refer to three houses selected 
by the gestures. In the third case, ?this? most likely 
refers to the house selected by the first point 
gesture and ?these two houses? refers to two 
houses selected by the other two point gestures. 
1561712415Total Num
221561S3: mul. expressions
131211712S2: one expression
3012S1: no expression
Total
Num
G3
mul. gest.
G2
one gest
G1
no gest.
Table 1: Referring patterns from the user study 1 We are developing a system that helps users find real estate 
properties. So here we use real estate as the testing domain. 
Gesture input: ??...?????.??.?.???????
(a)
Time
Speech input: Compare  this     with     these    houses.
Gesture input: ??...??????????...????
(b)
Time
Speech input: Compare  this  with       these         houses.
Gesture input: ??...????.. ..??.?.??...????
(c)
Time
Speech input: Compare  this   with     these   two     houses.
 
Figure 1. Three multimodal inputs under the same 
interaction context. The timings of the point gestures 
are denoted by ???.  
Resolving these complex cases requires 
simultaneously satisfying semantic constraints 
from inputs and the interaction contexts, and the 
temporal constraints between speech and gesture.     
2 Graph-based Approach 
We use a probabilistic approach based on attrib-
uted relational graphs (ARGs) to combine semantic 
and temporal constraints for reference resolution. 
First, ARGs can adequately capture the semantic 
and temporal information (for both referring ex-
pressions and potential referents). Second, the 
graph match mechanism allows a simultaneous 
application of temporal constraints and semantic 
constraints. Specifically, we use two attributed re-
lational graphs (ARGs). One graph corresponds to 
all referring expressions in the speech utterances, 
called the referring graph. The other graph corre-
sponds to all potential referents (either coming 
from gestures or contexts), called the referent 
graph. By finding the best match between the re-
ferring graph and the referent graph, we can find 
the most possible referent(s) to each referring ex-
pression.  
An ARG consists of a set of nodes and a set of 
edges. For example, Figure 2(a) is the referring 
graph for the speech utterance in Figure 1(c). 
There are two nodes corresponding to two refer-
ring expressions ?this? and ?these two houses? re-
spectively. Each node encodes the semantic and 
temporal information of the corresponding refer-
ring expression such as the semantic type of the 
potential referent, the number, the start and end 
time the expression was uttered, etc.  The edge be-
tween two nodes indicates the semantic and tempo-
ral relations between these two expressions. 
Similarly, Figure 2(b) is the referent graph for the 
input in Figure 1(c). This referent graph consists of 
four sub-graphs.  Three sub-graphs correspond to 
three gestures respectively. Each node in these sub-
graphs corresponds to one object selected by the 
gesture. Each node encodes the semantic and tem-
poral information of the selected object, as well as 
the probability this object is actually selected. 
There is also a sub-graph corresponding to the in-
teraction context.  Each node in this sub-graph 
represents an object in the focus in the last interac-
tion turn. The sub-graphs are connected via seman-
tic type and temporal relations.  
With the ARG representations described above, 
the reference resolution problem becomes match-
ing the referent graph with the referring graph. 
Suppose we have two graphs to be matched:  
? The referent graph Gc = ?{ax}, {rxy}?, where {ax} 
is the node list and {rxy} is the edge list. The 
edge rxy connects nodes ax and ay.  
Node 1
Surface: ?this?
Base: Unknown
Number: 1
Begin Time: 32264270
End Time: 32264273
Surface: ?these two houses?
Base: House
Number: 2
Begin Time: 32264381
End Time: 32264398
Node 2
Relation: 1
Direction: Node1 ? Node2
Temporal: Preceding
Semantic type: Same
 
(a) 
 
Node 1
Node 2
Node 4
Node3 Node 5
Sub-graph of the 
1st point gesture
Node 6
Node 7
Sub-graph of the 
2nd point gesture
Sub-graph of the 
3rd point gesture
Node 8
Sub-graph of the 
interaction context
Base: House
Identifier: 4
Attr: {Price, Size, ?}
Begin Time: 32264365
End Time: 32264366
Prob: 0.4356
Node 2
Base: Town
Identifier: 1
Attr: {Area, Population ?}
Begin Time: 32264365
End Time: 32264366
Prob: 0.3321
Node 3
Relation 5
Direction: Node 1 ? Node 4
Temporal: Preceding
Semantic Type: Same
 
(b) 
 
Figure 2. The ARG representation for references in Figure 
1(c). (a) The referring graph (b) The referent graph, where 
dashed rectangles represent sub-graphs.   
? The referring graph Gs = ?{?m}, {?mn}?, where 
{?m} is the node list and {?mn} is the edge list. 
The edge ?mn connects nodes ?m and ?n.  
The match process is to maximize the following 
function: 
   
( , ) ( , ) ( , )
( , ) ( , ) ( , )
c s x m x mx m
x m y n xy mnx y m n
Q G G P a a
P a P a r
? ? ?
? ? ? ?
=? ?
? ? ? ?
+
 (1) 
with respect to P(ax,?m), the matching probabili-
ties between the referent node ax and the referring 
node ?m. 
The function Q(Gc,Gs) measures the degree of 
the overall match between the referent graph and 
the referring graph. This function not only consid-
ers the similarities between nodes as indicated by 
the function ?(ax,?m), but also considers the simi-
larities between edges as indicated by the function 
?(rxy,?mn). Both node similarity and edge similarity 
functions are further defined by a combination of 
semantic and temporal constraints. For example, 
?(ax,?m)=Sem(ax,?m)Tem(ax,?m), where Sem(ax,?m)  
measures the semantic compatibility by determin-
ing whether the semantic categories of ax and ?m 
are the same, whether their attributes are compati-
ble, and so on.  Tem(ax,?m) measures the temporal 
alignment and is empirically defined as follows: 
??
??
? ??
???
? ??=
contextfromisa
gesturefromisatimeatimeaTem
x
x
mx
mx
,1.0
,
2000
|)()(|exp),(
?
?
    
To maximize (1), we modified the graduated as-
signment algorithm (Gold and Rangarajan, 1996). 
When the algorithm converges, P(ax,?m) gives us 
the matching probabilities. Details are described in 
a separate paper.   
3 Discussion 
During the study, we collected 156 inputs. The 
system assigned time stamps to each recognized 
word in the utterance, and each gesture.  Figure 3 
shows an example of an input that consisted of two 
gesture inputs and a speech utterance ?compare 
this house with this house?. The first two lines rep-
resent two gestures. Each line gives information 
about when the gesture started and ended, as well 
as the selected objects with their probabilities. 
These data provided us information on how the 
speech and gesture were aligned (to the accuracy 
of milliseconds). These data will help us further 
validate the temporal compatibility function used 
in the matching process.  
We described an approach that uses graph 
matching algorithm to combine semantic and tem-
poral constraints for reference resolution. The 
study showed that this approach worked quite well 
(93% accuracy) when the referring expressions 
were correctly recognized by the ASR. In the fu-
ture, we plan to incorporate spatial constraints.   
References 
P. Cohen, M. Johnston, D. McGee, S. Oviatt, J. Pittman, 
I. Smith, L. Chen, and J.Clow. 1996.   Quickset: 
Multimodal Interaction for Distributed Applications. 
Proceedings of ACM Multimedia, 31-40. 
S. Gold and A. Rangarajan. 1996. A graduated 
assignment algorithm for graph matching, IEEE 
Trans. Pattern Analysis and Machine Intelligence, 
vol. 18, no. 4. 377?388.  
C. Huls, E. Bos, and W. Classen. 1995. Automatic 
Referent Resolution of Deictic and Anaphoric 
Expressions. Computational Linguistics, 21(1):59-79. 
A. Kehler. 2000. Cognitive Status and Form of 
Reference in Multimodal Human-Computer 
Interaction, Proceedings of AAAI?00. 685-689. 
J. G. Neal, C. Y. Thielman, Z. Dobes, S. M. Haller, and 
S. C. Shapiro. 1998. Natural Language with 
Integrated Deictic and Graphic Gestures. Intelligent 
User Interfaces, M. Maybury and W. Wahlster (eds.), 
38-51. 
W. H. Tsai and K. S. Fu. 1979. Error-correcting 
isomorphism of attributed relational graphs for pattern 
analysis. IEEE Trans. Sys., Man and Cyb., vol. 9, 
757?768. 
Input received on port 3334: 67275921 67277343 2    69 
23 3 1 2 67275921 67277343 39218 10000 0 0 255 
0.28571     70 23 2 2 2 67275921 67277343 39218 10000 
0 0 255 1.  
 
Input received on port 3334: 67278140 67279078 2    71 
24 4 1 2 67278140 67279078 797 10000 255 0 0 0.74545  
72 24 3 2 2 67278140 67279078 797 10000 0 0 255 1.  
 
speech input: compare_67273821 this_67274160 
House_67274490 with_67275547 this_67275847 
House_67276096 
 
Figure 3. Gesture and speech data 
W. Wahlster. 1998. User and Discourse Models for 
Multimodal Communication, Intelligent User 
Interfaces, M. Maybury and W. Wahlster (eds.), 359-
370.  
M. Zancanaro, O. Stock, and C. Strapparava. 1997. 
Multimodal Interaction for Information Access: 
Exploiting Cohesion. Computational Intelligence 
13(7):439-464. 
Discourse Structure for Context Question Answering 
Joyce Y. Chai Rong Jin 
Department of Computer Science and Engineering 
Michigan State University  
East Lansing, MI 48864 
jchai@cse.msu.edu, rongjin@cse.msu.edu 
 
 
 
Abstract 
In a real-world setting, questions are not 
asked in isolation, but rather in a cohesive 
manner that involves a sequence of related 
questions to meet user?s information needs. 
The capability to interpret and answer 
questions based on context is important. In 
this paper, we discuss the role of discourse 
modeling in context question answering.  In 
particular, we motivate a semantic-rich 
discourse representation and discuss the 
impact of refined discourse structure on 
question answering.   
1 Introduction 
In a real-world setting, questions are not asked in 
isolation, but rather in a cohesive manner that 
involves a sequence of related questions to meet 
user?s information needs. The capability to interpret 
and answer questions based on context is important. 
For example, Figure 1 shows an example of a series 
of context questions. In this example, the 
interpretation of Q2 and Q4 depends on the 
resolution of ?it? and ?this? from the context 
respectively. Although neither Q3 nor Q6 requires 
any anaphora resolution, the interpretation of Q3 
depends on Q2 while the interpretation of Q6 
depends solely on itself. Furthermore, in Q5, there 
are no explicit references. Its interpretation depends 
on a preceding question (e.g.,Q4), however, in a 
different manner.     
This example indicates that interpreting each of 
these questions and extracting answers needs to be 
situated in a particular context as the QA session 
proceeds. There are situations where a question is 
?complete? enough and its interpretation does not 
depend on the previous questions (Q6).  There are 
also situations where the interpretation of a question 
depends on preceding questions no matter whether it 
requires anaphora or ellipsis resolution. Based on 
these observations, a natural question to ask is what 
makes the use of discourse differently in different 
situations? What is the role of discourse in context 
question answering? 
To address these questions, a key issue, in our 
mind, is that every question and its answer have a 
discourse status with respect to an entire QA session. 
This discourse status includes two aspects. The first 
aspect relates to discourse roles of entities in a 
question and the corresponding answer. Entities (such 
as noun phrase, verb phrase, preposition phrase, etc) 
in a question carry distinctive roles that indicate what 
is the topic or focus of a question in terms of the 
overall information seeking discourse. Topic relates 
to the ?aboutness? of a question and focus relates to a 
specific perspective of the topic. The second aspect 
of discourse status relates to discourse transitions that 
indicate how discourse roles are changed from one 
question to another as the interaction proceeds and 
how such changes reflect the progress of user 
information needs. Both discourse roles and 
discourse transitions determine whether the context is 
useful, and if so, how to use the context to interpret a 
question. 
This paper takes an initial attempt to investigate 
the discourse status for context question answering. 
In particular, it motivates a semantic-rich discourse 
representation that captures both discourse roles of a 
question and discourse transitions between questions. 
Through examples, this paper further discusses the 
potential impact of this refined discourse structure on 
context question answering.   
Q1: What is the name of the volcano that destroyed 
the ancient city of Pompeii?  
Q2: When did it happen?  
Q3: how many people were killed?  
Q4: how tall was this volcano? 
Q5: Any pictures?  
Q6: Where is Mount Rainier?  
 
Figure 1: An example of context questions 
2 Semantic-rich Discourse Modeling 
For processing single questions, an earlier study 
shows that an impressive improvement can be 
achieved when more knowledge-intensive NLP 
techniques are applied at both question and answer 
processing level (Harabagiu et al, 2000). For context 
questions, a parallel question would be whether rich 
contextual knowledge will help interpret subsequent 
questions and extracting answers. To address this 
question, we propose a semantic-rich discourse 
modeling that captures both discourse roles of 
questions and discourse transitions between 
questions, and investigate its usefulness in context 
question answering.  
2.1 Discourse Roles  
In context question answering, each question is 
situated in a context. In addition to the semantic 
information carried by important syntactic entities 
(such as noun phrase, verb phrase, preposition 
phrase, etc), each question also carries distinctive 
discourse roles with respect to the whole question 
answering discourse. Specifically, the discourse roles 
can be categorized based on both the informational 
and intentional perspectives of discourse (Hobbs, 
1996), as well as the presentation aspect of both 
questions and answers.   
The intentional perspective relates to the purpose 
of a question. In a fully interactive question 
answering environment, instead of asking questions, 
a user may need to reply to a clarification question 
prompted by the system or may need to simply ask 
for a confirmation. Therefore, it is important to 
capture the intention from the user (Grosz and Sidner 
1986). The informational perspective relates to the 
information content of a question, in particular, the 
topic and the focus based on the semantics of the 
content. In addition to the intentional and 
informational aspects, there is also a presentational 
aspect of discourse that relates to both the input 
modality (i.e., questions) and the output modality 
(i.e., answers). For example, a user may explicitly ask 
for images or pictures of a person or event. The 
presentation aspect is particularly important to 
facilitate multimodal multimedia question answering. 
Therefore, for a given question, three types of 
discourse roles: Intent, Content, and Media can be 
captured to reflect the intentional, informational, and 
presentational perspectives of discourse respectively.   
These discourse roles can be further characterized 
by a set of features. For example, Intent can be 
represented by Act and Motivator, where Act indicates 
whether the user is requesting information from the 
system or replying to a system question. Motivator 
corresponds to the information goal as to what type 
of action is expected from the system, for example, 
whether information retrieval or confirmation (Chai 
et al 2003). We will not elaborate Intent here since it 
has been widely modeled for most dialog systems.  
 Content can be characterized as Target, Topic and 
Focus. Target indicates the expected answer type such 
as whether it should be a proposition (e.g., for why 
and how questions), or a specific type of entity (e.g., 
TIME and PLACE).  
Topic indicates the ?aboutness? or the scope 
related to a question. Focus indicates the current 
focus of attention given a particular topic. Focus 
always refers to a particular aspect of Topic. Since the 
informational perspective of discourse should capture 
the semantics of what has been conveyed, Topic and 
Focus are linked with the semantic information of a 
question, for example, semantic roles as described in 
(Gildea and Jurafsky 2002). Semantic roles concern 
with the roles of constitutes in a question in terms of 
its predicate-argument structure. The discourse roles 
link the semantic roles of individual questions 
together with respect to the discourse progress 
through Topic and Focus. 
For example, Topic can be of type Activity or Entity. 
Activity can be further categorized by ActType, 
Participant, and Peripheral.  ActType indicates the type 
of the activity; Participant indicates entities that are 
participating in the activity with different semantic 
roles. Peripheral captures auxiliary information such 
 
Content
Target: $NAME 
Topic: Activity
ActType: Destroy [Term: ?destroy?]
Participant1: Entity
SemRole: Agent
SemType: volcano
Id: ?
Term: ?the volcano? 
Participant2: Entity
SemRole: Theme
SemType: city
Id: ?Pompeii?
Term: ?Pompeii?        
Focus: Topic.Activity-Participant1
[Element: Name 
[Value: ?; Term: ?name? ]]
Intent
Act: Request
Motivator: AnsRequest
Media
Format: Text
Genre: Default
 
Figure 2: Discourse roles for Q1.   
as the time, place, purpose, and reason for such an 
activity. Entity can be categorized by SemRole, 
SemType, Id, Element, and Constraint. SemRole 
indicates the semantic role of the entity in a particular 
activity (if any). SemType represents the semantic 
type of the entity. Element indicates the specific 
features associated with the entity. Constraint 
specifies the constraints need to be satisfied to 
identify the entity, and Id specifies the particular 
identifier of the entity that particularly corresponds to 
pronouns, demonstratives, and definite noun phrases. 
Media indicates the desired information media, 
which can be further characterized as Format and 
Genre as shown in Figure 2. Format indicates whether 
it is an image, a table, or text, etc. Genre specifies the 
answer needs such as summary or list. If it is a list, 
how many should be in the list as in the question 
?number ten largest cities in the world.? 
Figure 2 shows the representation of discourse 
roles of Q1 using typed feature structures (Carpenter 
1992), where Intent indicates that the user is 
requesting for the system to retrieve an answer. Topic 
indicates the topic of Q1 is a Destroy Activity, which 
has two participants. The first participant is some 
kind of unknown volcano that takes the role of Agent 
in the activity (i.e., the destroyer). The second 
participant is the city of Pompeii that takes the role of 
Theme indicating the thing destroyed. The Focus of 
Q1 is about the name (i.e., Element) of the entity in 
the first participant (i.e., Participant1) in the Topic 
representation.  
The granularity of discourse roles can be varied. 
The finer the granularity, the better is the use of 
context for inference (as discussed later). However, 
the finer granularity also implies deeper semantic 
processing. This semantic rich representation can be 
used to generate other representations such as queries 
based on weighted terms for information retrieval or 
logical forms for deduction and inference (Waldinger 
et al, 2003). Furthermore, this representation is able 
to keep all QA sessions in a structured way to support 
inference, summarization, and collaborative fusion as 
described later.   
2.2 Discourse Transitions 
Transitions from one question to another also 
determine how context will be used in interpreting 
questions and retrieving answers. In this section, we 
use query formation as an example to illustrate the 
role of different types of discourse transitions.  
Discourse transitions also correspond to the 
intentional, informational, and presentational 
perspectives of discourse. Intentional transitions are 
closely related to Grosz and Sidner?s ?dominance? 
and ?satisfaction precedence? relations, which are 
more relevant to plan-based discourse (Grosz and 
Sidner, 1986). Here we focus on informational 
transitions and presentational transitions that are 
more relevant to QA systems since they are targeted 
for information exchange.   
Informational transitions are mainly centered 
around Topics of questions. In context question 
answering, how questions are related to each other 
depends on how ?topics? of those questions evolve. 
Currently, we categorize information transitions into 
three types: Topic Extension, Topic Exploration, and 
Topic Shift.  
 
Topic Extension 
A question concerns a similar topic as that of a 
previous question, but with different participants, 
peripheral, or constraints. It has the following 
subcategories: 
Constraint Refinement 
A question is about a similar topic with additional or 
revised constraints. For example:  
Q7: What?s the crime rate in Maryland and Virginia? 
Q8: What is it ten years ago?  
For another example:  
Q9: What?s the crime rate in Maryland and Virginia? 
Q10: What was it in Alabama and Florida? 
In both examples, both questions share the topic of 
?crime rate?, but concerning different crime rates 
with different constraints.  Interpreting the second 
question requires not only identifying constraints, but 
also the relations between constraints. In the first 
example, the constraints from Q7 need to be used to 
form a query for Q8. However, constraints from Q9 
should not be used for Q10.  
 
Participant Shift 
A question is about a similar topic with different 
participants.  
For example:  
Q11: In what country did the game of croquet 
originate?  
Q12: What about soccer? 
In this example, both questions are about the 
origination of a certain sport. The Content structure 
for both questions are the same except for the 
Participant role, which in Q11 is ?croquet? and in Q12 
is ?soccer?. Therefore, the query created for Q12 
would be {country, soccer, originate}, the keyword 
?croquet? should not appear in the query list.   
 
Topic Exploration  
Two questions are concerning the same topic, but 
with different focus (i.e., asking about different 
aspects of the topic).  For example,  
Q13: What is the name of the volcano that destroyed the 
ancient city of Pompeii? 
Q14: When did this happen?  
In this example, ?this? in Q14 refers to the same 
activity topic in Q13, but focus on the TIME 
peripheral information about the activity.  
In the following example,  
Q15: Where is Mount Rainier?  
Q16: How tall is it?  
Q15 asks about the location of Mount Rainier (which 
is an entity topic) and Q16 asks about a different 
aspect (i.e., the height) of the same entity topic. In 
both examples, significant terms representing the 
Topic from the preceding question can be merged 
with the significant terms in the current question to 
form a query.   
 
Topic Shift  
Two consecutive questions could ask about two 
different topics.  Different topic shifts indicate 
different semantic relations between two questions.  
Activity Topic shifts to another Activity Topic 
In the following example,  
Q17: What is the name of the volcano that destroyed the 
ancient city of Pompeii? 
Q18: How many people were killed?  
The topic of both questions concerns about certain 
activities. This activity shift indicates that ?kill? 
activity is a consequence of ?destroy? activity (i.e., 
Q18 is a consequence of Q17).  
Other relations can also be entailed from such a 
transition such as ?effect-cause? relation as in the 
following example (Harabagiu et al 2001):  
Q19: Which museum in Florence was damaged by a 
major bomb explosion in 1993? 
Q20: How much explosive was used?  
 
Activity Topic shifts to Entity Topic 
In the example: 
Q21: What is the name of the volcano that destroyed the 
ancient city of Pompeii? 
Q22: How tall is this volcano?  
The topic of Q21 is an activity of ?destroying? and 
the focus is the agent of the activity ?the volcano?. 
This focus becomes the topic of Q22. This transition 
indicates a further probing of a particular participant 
in an activity that can be independent of the activity 
itself. Therefore, the terms in Q21 will not be helpful 
in setting up the stage for processing Q22. Q21 
should be used only to resolve reference to the 
definite noun phrase ?this volcano?.  
Related to the presentational perspective of a QA 
discourse, we currently only identify: Media Shift.  
This relation indicates that two questions are about 
the same information content, but with different 
preference of media presentation.  
For example,  
Q25: how tall is Mount Vesuvius? 
Q26: Any pictures? 
Q26 is asking for the images of the Mount Vesuvius. 
This indicates that the backend should perform image 
retrieval rather than text retrieval.  
In summary, given two consecutive questions (Qi, 
Qi+1), a certain transition exists from Qi to Qi+1. 
These transitions determine how the context, for 
example, proceeding questions and answers can be 
used in interpreting the following question and 
identifying the potential answers. Here we only list 
several examples to show the importance of these 
transitions, which are by no means complete. We 
plan to identify a list of salient transitions for 
processing context questions as well as their 
implications (e.g., semantic relations) in interpreting 
context questions.  
2.3 Discourse Processing 
Given the above discussion, the goal of discourse 
modeling for context question answering is to 
automatically identify the discourse roles of a 
question and discourse relations between questions as 
the QA session proceeds. This may be a difficult task 
that requires rich knowledge and deep semantic 
processing. However, the recent advancement in 
semantic processing and discourse parsing has 
provided an excellent foundation for this task.  
The discourse roles are higher-level abstracts of 
the semantic roles as those provided in FrameNet 
(Baker et al, 1998) and Propbank (Kingsbury and 
Palmer 2002). Recent corpus-based approaches to 
identify semantic roles (Roth et al2002, Gildea and 
Jurafsky 2002; Gildea and Palmer 2002; Surdeanu et 
al., 2003) have been successful in identifying domain 
independent semantic relations with respect to the 
predicate-argument structure. Furthermore, recent 
work also provides discourse annotated corpora with 
rhetorical relations (Carlson, et al, 2003) and 
techniques for discourse paring for texts (Soricut and 
Marcu, 2003).  All these recent advances make the 
semantic-rich discourse modeling possible.  
For example, a collection of context questions 
(and answers) can be annotated in terms of their 
discourse roles and relations. Specifically, the 
following information can be either automatically 
identified or manually annotated: 
? Syntactic structures automatically identified from 
a parser (Collins, 1997); 
? Semantic roles of entities in the question (Gildea 
and Jurafsky 2002; Gildea and Palmer 2002; 
Surdeanu et al, 2003); 
? Discourse roles either manually annotated or 
identified by rules that map directly from semantic 
roles to discourse roles.  
? Discourse transitions automatically determined 
once discourse roles are identified for each 
question.  
? Semantic relations between questions manually 
annotated. 
? Answers provided by the system.  
Based on this information, important features can be 
identified. Different learning models such as decision 
trees or Bayesian classifier can be applied to learn the 
classifier for discourse roles and relations. Strategies 
can be built to take into account of discourse roles 
and relations from preceding questions and answers 
to process a subsequent question and extract answers. 
These models can then be applied to process new 
context questions.  
3 Refined Discourse Structure in Context 
Question Answering 
Based on the above discussion, during the question 
answering process, a discourse structure can be 
created to capture the discourse roles of each 
ActType
Activity
City
?
Volcano
Name
Theme
Agent
Entity
Destroy
?Pompeii?
Entity
Id
Id
Participant1
SemType
SemRole
Participant2 SemRole
SemType
Element
Topic
Focus
???
    
ActType
Activity
City
?Mount Vesuvius?
Volcano
Name
Theme
Agent
Entity
Destroy
?Pompeii?
Entity
Id
Id
Participant1
SemType
SemRole
Participant2 SemRole
SemType
Element
Topic
?Mount Vesuvius?
Time
?
Peripheral
Value
Focus
 
 
(a) Discourse repreentation after processing Q1                (b) Discourse representation after processing Q2             
 
 
ActType
Activity
City
?Mount Vesuvius?
Volcano
Name
Theme
Agent
Entity
Destroy
?Pompeii?
Entity
Id
Id
Participant1
SemType
SemRole
Participant2 SemRole
SemType
Element
Topic
?Mount Vesuvius?
Time
79 AD
Peripheral1
Value
Focus
Consequence
Activity
EntityKill
Patient
Person SizeOfSet ?
Peripheral2
Type
ActType Participant1
SemType
SemRole
Element
Value
Value
        
ActType
Activity
City
?Mount Vesuvius?
Volcano
Name
Theme
Agent
Entity
Destroy
?Pompeii?
Entity
Id
Id
Participant1
SemType
SemRole
Participant2 SemRole
SemType
Element
Topic
?Mount Vesuvius?
Time
79 AD
Peripheral
Value
Focus
Height
Element
Value
?
Activity
EntityKill
Patient
Person SizeOfSet Thousands
Peripheral2
Type
ActType Participant1
SemType
SemRole
Element
Value
Consequence
 
 
(c) Discourse representation after processing Q3              (d) Discourse representation after processing Q4             
question and discourse relations between questions.  
Similar to information extraction for free texts, this 
refined discourse structure captures the salient 
information extracted from the question answering 
process. This discourse provides a structured 
information space that indicates what type of 
information has been exchanged and how 
information obtained at different stages is related. In 
other words, we can also consider this representation 
as the ?mental map? of user information needs. This 
mental map will potentially provide a basis to 
improve question interpretation and answer 
extraction through inference, summarization, and 
collaborative question answering.    
3.1 Discourse Representation 
The typed feature structures can be represented as 
Directed Acyclic Graph (DAG). Thus the described 
discourse structure can be represented as semantic 
networks using DAGs. For example, Figure 3 shows 
the discourse representation after processing the each 
of the first four questions in Figure 1. In this network, 
each node is either a specific value (i.e., leaf nodes) 
or a typed feature structure itself (i.e., internal node). 
Each directed link corresponds to a particular feature. 
Note that because of the space limit, not everything 
represented in the feature structure in Figure 2 is 
shown here in the semantic network. For example, 
the type of an activity (e.g., Destroy) by itself is a 
feature structure (in Figure 2) that further consists of 
the specific term used in the question.  This term is 
not shown but is included in the semantic nets.  
As context question answering proceeds, the 
semantic network(s) for discourse grows, with 
different pointers of Topic and Focus. For example, 
Figure 3(a) represents Q1, where Topic points an 
Activity feature structure and Focus points to the Name 
Element of the Participant1 in the Activity. From Q1 to 
Q2, there is a transition of Topic Exploration which 
indicates that Q2 is about the same topic, but with a 
different focus. Therefore, in Figure 3(b), the Topic 
points to the same activity, but the Focus now points 
to the peripheral Time information of that activity.  
Next, Q3 is about a different topic involving activity 
Kill. However, since there is a consequence relation 
from Q2 to Q3, the activity asked in Q3 actually 
fulfills the Peripheral role of Consequence for the 
previous activity as shown in Figure 3(c). Finally, in 
Q4, there is a gap between Q3 and Q4, however, 
there is a transition of Probing from Q2 to Q4. Now 
the Topic becomes the Participant in Q1 as shown in 
Figure 3(d).  
3.2 Potential Impacts 
The growth of the semantic networks represents the 
overall information needs of a user and how such 
information needs are related. Since this is a 
structured representation, it can be queried and used 
to facilitate context question answering, for example, 
in the following aspects: 
? Query expansion and answer retrieval 
? Inference and summary for question answering 
? Collaborative question answering 
To process questions, most systems will first form 
a query of keywords to represent the current question 
and to retrieve relevant passages that may contain the 
potential answers. In context question answering, 
since the interpretation of a question may depend on 
preceding questions, some keywords from preceding 
questions may need to be included in the query for 
the current question. The fine-grained discourse 
structure will enhance answer retrieval through more 
controlled selection of terms from preceding 
questions and answers. For example, strategies can be 
developed to select query terms depending on the 
discourse relations. Different discourse roles and 
transitions may lead to different weighting schemes 
for query expansion.  
Furthermore, the information captured in the 
discourse structure can help make predication about 
what the user information need is and therefore 
provide more intelligent services to help user find 
answers. For example, semantic and discourse 
relations between different topics and focuses of a 
series of questions can help a system infer and predict 
the overall interest of a user. Although answers to 
each question may come from different sources, 
based on the structured discourse (e.g., in semantic 
network), the system can aggregate information and 
generate summaries.  
Another potential impact of the refined structured 
discourse is to facilitate collaborative question 
answering. Very often, various users may have a 
similar interest about a set of topics. The structured 
discourse built for one user can be used to help 
answers questions from another user. A user may 
have a certain information goal in mind, but does not 
know what types of questions to ask.  Therefore, a 
user?s question may be very general and vague, such 
as ?what happened to Pompeii?? This question needs 
to be decomposed into a set of smaller questions. The 
discourse structure that connects different aspects of 
topics together can provide some insight on how such 
decomposition should be made. Furthermore, the 
discourse structure from a skilled user can enable the 
system to intelligently direct a novice user in his 
information seeking process.  
4 Discussion 
TREC 10 Question Answering Track initiated a 
context-task that was designed to investigate the 
system capability to track context through a series of 
questions. As described in (Voorhees 2001), there 
were two unexpected results of this task. First, the 
ability to identify the correct answer to a question in 
the later series had no correlation with the capability 
to identify correct answers to preceding questions. 
Second, since the first question in a series already 
restricted answers to a small set of documents, the 
performance was determined by whether the system 
could answer a particular type of question, rather than 
the ability to track context. Because of these 
unexpected results, the context task has been stopped 
in the following TREC evaluations (Voorhees 2002).  
The reasons that TREC 10 did not achieve the 
expected results, in our opinion, lie in two aspects. 
The first aspect relates to the uniqueness of open 
domain context question answering. In open domain 
QA, first, there may be many occurrences of correct 
answers in various part(s) of document(s). Second, 
there may be multiple paths (e.g., different 
combination of key query terms) that can lead to one 
occurrence of the correct answer. Therefore, the 
correct answer to a previous question may not be 
critical in finding answers to subsequent questions. 
This phenomenon may provide an opportunity to find 
answers without explicitly modeling context (i.e., by 
keeping track of the discourse objects from answers), 
but rather identifying and using relevant context.  
For example, in the LCC system (that achieved 
the best result for the context task in TREC 10), the 
discourse was not explicitly represented (Harabagiu 
et al2001). Instead of resolving references using 
discourse information, the LCC system first identifies 
the question that contains the potential referents and 
uses those questions and the current question to 
identify the target paragraph. Thus, question 
interpretation does not depend on the answers, but 
rather depends on the context that is dynamically 
identified as a list of preceding questions. Now the 
question is whether the system will achieve even 
better results (e.g., correctly find answers to the rest 
of the eight questions) with some context 
representation?  
Another more important question to be asked is 
whether the design of the context task just happened 
to provide an opportunity to achieve good results 
without modeling the context. As discussed in 
(Harabagiu et al, 2001), answers to 85% of context 
questions actually occurred within the same 
paragraph as the answers to the previous questions. 
Therefore, just using preceding questions, the system 
was able to find the target paragraph and the final 
answer really depended on the capability to identify 
different types of answers in that paragraph. What if 
a series of questions were designed differently so that 
questions are related but answers are scattered in 
different documents or paragraphs.  Will the shallow 
processing of discourse succeed in finding the 
answers?  
Furthermore, the ultimate goal of QA systems is 
to be able to access information from different 
sources (e.g., unstructured text or structured 
database) and to provide intelligent dialog capability.  
One important question we need to address is what 
kind of discourse representation will be sufficient to 
support these capabilities? For example, to access 
structured databases, the answer to a previous 
question usually narrows down the search spaces in 
the database for subsequent questions. Thus, previous 
answers usually determine where in the database an 
answer can be found. Therefore, it is important to 
keep track of previous questions and answers in some 
kind of structure for later use.  
The second reason that TREC 10 did not achieve 
expected results relates to evaluation methodology. In 
context QA, good performance depends on two 
important components: the capability of representing 
and using the relevant context (both explicitly or 
implicitly) and the general capability of interpreting 
questions and extracting answers. The level of 
sophistication in either component will influence the 
final performance. Thus, by comparing the final 
answers to each context question, the evaluation of 
the context task in TREC 10 was not able to isolate 
the effect of one component from another. It is not 
feasible to identify that when an answer is not 
identified, whether it is because of poor 
representation and use of the discourse information or 
it is because of the general limitations of the 
capability to process certain types of questions. 
Therefore, to study the role of discourse in context 
question answering, a more controlled evaluation 
mechanism is desired. For example, one approach is 
to keep the general processing capability as a 
constant and vary the representations of discourse 
and strategies to use the discourse so that their 
different impacts on the final answer extraction can 
be learned.  
As a summary, the experience in the TREC 10 
context task is very valuable. It does not discount the 
importance of context modeling for context 
questions. But rather, it motivates a more in-depth 
investigation of the role of discourse in context 
question answering.  
5 Conclusion 
Questions are not asked in isolation, but rather in a 
cohesive manner that involves a sequence of related 
questions to meet user?s information needs. It is 
important to understand the role of discourse to 
support this cohesive question answering.  
By all means, a QA discourse can be represented 
as coarse as a list of keywords extracted from 
previous questions or as sophisticated as a fine-
grained representation as described in this paper. 
There is a balance between how much we like to 
represent the context and how far we can get there.  
Given recent advances in text-based domain 
independent semantic processing and discourse 
parsing, as well as the availability of rich semantic 
knowledge sources, we believe it is the time to start 
from the other end of the spectrum to examine the 
possibility and impact of semantic-rich discourse 
representation for open-domain question answering.     
References 
Baker, C., Fillmore, C., and Lowe, J. 1998. The Berkeley 
FrameNet project. In Proceedings of COLING/ACL, pp. 
86-90, Montreal, Canada 
Carpenter, R.1992. The Logic of Typed Feature Structures. 
Cambridge University Press.  
Chai, J., Pan, S., and Zhou, M. 2003. MIND: A Context-
based Multimodal Interpretation Framework in Conver-
sational Systems, Natural, Intelligent and Effective In-
teraction in Multimodal Dialogue Systems,  Eds. O. 
Bernsen ,  L. Dybkjaer and  J.  van Kuppevelt, Kluwer 
Academic Publishers. 
Carlson, L., Marcu, D., and Okurowski, M. 2003. Building 
a discourse-tagged corpus in the framework of Rhetori-
cal Structure Theory. In Jan van Kuppevelt and Ronnie 
Smith, editors, Current Directions in Discourse and 
Dialogue, Kluwer Academic publishers.  
Collins, M. 1997. Three Generative, Lexicalized Models 
for Statistical Parsing. In Proceedings of the 35th Annual 
meeting of the Association for Computational Linguis-
tics (ACL 1997): 16-23, Madrid, Spain.  
Fillmore, C. and Atkins, B. 1998. FrameNet and lexico-
graphic relevance, Proceedings of the First Interna-
tional Conference on Language Resources and 
Evaluation, Granada, Spain. 
Grosz, B. J. and Sidner, C. 1986. Attention, intention, and 
the structure of discourse. Computational Linguistics, 
12(3):175-204. 1986. 
Gildea, D. and Jurafsky, D. 2002. Automatic Labeling of 
Semantic Roles. Computational Linguistics, 28(3):245-
288.  
Gildea, D. and Palmer, M. 2002. The Necessity of Parsing 
for Predicate Argument Recgnition. In Proceedings of 
the 40th Meeting of the Association for Computational 
Linguistics (ACL 2002): 239-246, Philadelphia, PA.  
Harabagiu, S., Pasca, M., and Maiorano, S. Experiments 
with Open-domain Textual Question Answering. In 
Proceedings of the 18th International Conference on 
Computational Linguistics (COLING-2000), 2000 
Harabagiu, S., et al, Answering Complex, List and Con-
text Questions with LCC?s Question-Answering Server. 
In the Proceedings of TREC 2001, 2001. 
Hobbs, J. 1996. On the relations between the informational 
and Intentional Perspectives on Discourse. In Burning 
Issues in Discourse: An inter-Disciplinary Account  
(eds. E. Hovy and D. Scott), volume 151 of NATO ASI 
Series, Series F: Computer and Systems Sciences, pp 
139-157. Springer-Verlag, Berlin, Germany 
Kingsbury, P. and Palmer, M. 2002. From Treebank to 
Propbank. In Proceedings of the 3rd International Con-
ference on Language Resources and Evaluation (LREC-
2002), Las Palmas, Canary Islands, Spain.  
Roth, D., et al  2002. Question Answering via Enhanced 
Understanding of Questions. Proceedings of 
TREC2002.  
Surdeanu, M., Harabagiu, S., Williams, J., and Aarseth, P. 
2003. Using Predicate-Argument Structures for Infor-
mation Extraction. In Proceedings of the 41th Meeting of 
the Association for Computational Linguistics (ACL 
2003), Sapporo, Japan.  
Soricut, R. and Marcu, D. 2003. Sentence Level Discourse 
Parsing using Syntactic and Lexical Information. In 
Proceedings of HLT-NAACL. Edmonton, Canada. 
Waldinger, R., et al, 2003. Deductive Question Answering 
from Multiple Resources, New Directions in Question 
Answering, AAAI, 2003 
Voorhees, E. 2001. Overview of TREC 2001 Question 
Answering Track. Proceedings of TREC2001.  
Voorhees, E. 2002. Overview of TREC 2002 Question 
Answering Track. Proceedings of TREC2002.  
 
Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 188?195,
Queen Mary University of London, September 2009. c?2009 Association for Computational Linguistics
The Role of Interactivity in Human-Machine Conversation for Automatic
Word Acquisition
Shaolin Qu Joyce Y. Chai
Department of Computer Science and Engineering
Michigan State University
East Lansing, MI 48824
{qushaoli,jchai}@cse.msu.edu
Abstract
Motivated by the psycholinguistic finding
that human eye gaze is tightly linked to
speech production, previous work has ap-
plied naturally occurring eye gaze for au-
tomatic vocabulary acquisition. However,
unlike in the typical settings for psycholin-
guistic studies, eye gaze can serve differ-
ent functions in human-machine conver-
sation. Some gaze streams do not link
to the content of the spoken utterances
and thus can be potentially detrimental to
word acquisition. To address this prob-
lem, this paper investigates the incorpo-
ration of interactivity in identifying the
close coupling of speech and gaze streams
for word acquisition. Our empirical re-
sults indicate that automatic identification
of closely coupled gaze-speech streams
leads to significantly better word acquisi-
tion performance.
1 Introduction
Spoken conversational interfaces have become in-
creasingly important in many applications such
as remote interaction with robots (Lemon et al,
2002), intelligent space station control (Aist et
al., 2003), and automated training and educa-
tion (Razzaq and Heffernan, 2004). As in any con-
versational system, one major bottleneck in con-
versational interfaces is robust language interpre-
tation. To address this problem, previous multi-
modal conversational systems have utilized pen-
based or deictic gestures (Bangalore and John-
ston, 2004; Qu and Chai, 2006) to improve in-
terpretation. Besides gestures, eye movements
that naturally occur during interaction provide an-
other important channel for language understand-
ing, for example, reference resolution (Byron et
al., 2005; Prasov and Chai, 2008). Recent work
has also shown that what users look at on the inter-
face (e.g., natural scenes or generated graphic dis-
plays) during speech production provides unique
opportunities for word acquisition, namely auto-
matically acquiring semantic meanings of spoken
words by grounding them to visual entities (Liu
et al, 2007) or domain concepts (Qu and Chai,
2008).
Psycholinguistic studies have shown that eye
gaze indicates a person?s attention (Just and Car-
penter, 1976), and eye movement can facilitate
spoken language comprehension (Tanenhaus et
al., 1995; Eberhard et al, 1995). It has been
found that users? eyes move to the mentioned ob-
ject directly before speaking a word (Meyer et
al., 1998; Rayner, 1998; Griffin and Bock, 2000).
This parallel behavior of eye gaze and speech pro-
duction motivates our previous work on word ac-
quisition (Liu et al, 2007; Qu and Chai, 2008).
However, in interactive conversation, human gaze
behavior is much more complex than in the typ-
ical controlled settings used in psycholinguistic
studies. There are different types of eye move-
ments (Kahneman, 1973). The naturally occur-
ring eye gaze during speech production may serve
different functions, for example, to engage in the
conversation or to manage turn taking (Nakano et
al., 2003). Furthermore, while interacting with a
graphic display, a user could be talking about ob-
jects that were previously seen on the display or
something completely unrelated to any object the
user is looking at. Therefore using every speech-
gaze pair for word acquisition can be detrimental.
The type of gaze that is mostly useful for word
acquisition is the kind that reflects the underlying
attention and tightly links to the content of the co-
occurring speech. Thus, one important question
is how to identify the closely coupled speech and
gaze streams to improve word acquisition.
To address this question, we develop an ap-
proach that incorporates interactivity (e.g., speech,
188
user activity, conversation context) with eye gaze
to identify closely coupled speech and gaze
streams. We further use the identified speech
and gaze streams to acquire words with a trans-
lation model. Our empirical evaluation demon-
strates that automatic identification of closely cou-
pled gaze-speech streams can lead to significantly
better word acquisition performance.
2 Related Work
Previous work has explored word acquisition by
grounding words to visual entities. In (Roy and
Pentland, 2002), given speech paired with video
images of objects, mutual information between
auditory and visual signals was used to acquire
words by associating acoustic phone sequences
with the visual prototypes (e.g., color, size, shape)
of objects. Given parallel pictures and descrip-
tion texts, generative models were used to acquire
words by associating words with image regions in
(Barnard et al, 2003). Different from this previous
work, in our work, the visual attention foci accom-
panying speech are indicated by eye gaze. As an
implicit and subconscious input, eye gaze brings
additional challenges in word acquisition.
Eye gaze has been explored for word acqui-
sition in previous work. In (Yu and Ballard,
2004), given speech paired with eye gaze and
video images, a translation model was used to
acquire words by associating acoustic phone se-
quences with visual representations of objects and
actions. Word acquisition from transcribed speech
and eye gaze during human-machine conversa-
tion has been investigated recently. In (Liu et
al., 2007), a translation model was developed to
associate words with visual objects on a graphi-
cal display. In our previous work (Qu and Chai,
2008), enhanced translation models incorporat-
ing speech-gaze temporal information and domain
knowledge were developed to improve word ac-
quisition. However, none of these previous works
has investigated the role of interactivity in word
acquisition, which is the focus of this paper.
3 Data Collection
We collected speech and eye gaze data through
user studies. This data set is different from the data
set used in our previous work (Qu and Chai, 2008).
The difference lies in two aspects: 1) the data for
this investigation was collected during mixed ini-
tiative human-machine conversation whereas the
data in (Qu and Chai, 2008) was based only on
question and answering; 2) user studies were con-
ducted in a more complex domain for this investi-
gation, which resulted in a richer data set that con-
tains a larger vocabulary.
3.1 Domain
Figure 1: Treasure hunting domain
Figure 1 shows the 3D treasure hunting domain
used in our work. In this application, the user
needs to consult with a remote ?expert? (i.e., an ar-
tificial system) to find hidden treasures in a castle
with 115 3D objects. The expert has some knowl-
edge about the treasures but can not see the cas-
tle. The user has to talk to the expert for advice
regarding finding the treasures. The application is
developed based on a game engine and provides an
immersive environment for the user to navigate in
the 3D space. During the experiment, each user?s
speech was recorded, and the user?s eye gaze was
captured by a Tobii eye tracker.
3.2 Data Preprocessing
From 20 users? experiments, we collected 3709 ut-
terances with accompanying gaze fixations. We
transcribed the collected speech. The vocabulary
size of the speech transcript is 1082, among which
227 are either nouns or adjectives. The user?s
speech was also automatically recognized online
by the Microsoft speech recognizer with a word
error rate (WER) of 48.1% for the 1-best recog-
nition. The vocabulary size of the 1-best speech
recognition is 3041, among which 1643 are either
nouns or adjectives.
The collected speech and gaze streams were au-
tomatically paired together by the system. Each
time the system detected a sentence boundary (in-
dicated by a long pause of 500 milliseconds) of the
user?s speech, it paired the recognized speech with
the gaze fixations that the system had been ac-
cumulating since the previously detected sentence
189
[table_vase]
speech stream
gaze stream
[fixated entity]
ts te
gaze fixation
[vase_purple] [vase_greek3][vase_greek3] [vase_greek3][vase_greek3]
There?s orangevase in anpurplea face
Figure 2: Accompanying gaze fixations and the 1-best recognition of a user?s utterance ?There?s a purple
vase and an orange vase.? (There are two incorrectly recognized words ?in? and ?face? in the 1-best
recognition)
boundary. Figure 2 shows a pair of user speech
and accompanying stream of gaze fixations. In
the speech stream, each spoken word was times-
tamped by the speech recognizer. In the gaze
stream, each gaze fixation has a starting timestamp
ts and an ending timestamp te provided by the eye
tracker. Each gaze fixation results in a fixated en-
tity (3D object). When multiple entities are fixated
by one gaze fixation due to the overlapping of en-
tities, the one in the forefront is chosen.
Given the paired speech and gaze streams, we
build a set of parallel word sequence and gaze fix-
ated entity sequence {(w, e)} for the task of word
acquisition. In section 6, we will evaluate word
acquisition in two settings: 1) word sequence w
contains all of the nouns/adjectives in the speech
transcript, and 2) w contains all of the recognized
nouns/adjectives in the 1-best speech recognition.
4 Word Acquisition With Eye Gaze
The task of word acquisition in our application is
to ground words to the visual entities. Specifi-
cally, given the parallel word and entity sequences
{(w, e)}, we want to find the best match between
the words and the entities. Following our previ-
ous work (Qu and Chai, 2008), we formulate word
acquisition as a translation problem and use trans-
lation models for word acquisition. For each en-
tity e, we first estimate the word-entity association
probability p(w|e) with a translation model, then
choose the words with the highest probabilities as
acquired words for e.
Inspired by the psycholinguistic findings that
users? eyes move to the mentioned object before
speaking a word (Meyer et al, 1998; Rayner,
1998; Griffin and Bock, 2000), in our previous
work (Qu and Chai, 2008), we have incorpo-
rated the gaze-speech temporal information in the
translation model as follows (referred as Model-2t
through the rest of this paper):
p(w|e) =
m?
j=1
l?
i=0
pt(aj = i|j, e,w)p(wj |ei)
where l and m are the lengths of entity and word
sequences respectively. In this equation, pt(aj =
i|j, e,w) is the temporal alignment probability
representing the probability thatwj is aligned with
ei, which is further defined by:
pt(aj = i|j, e,w) =
{
0 d(ei, wj) > 0
exp[??d(ei,wj)]?
i exp[??d(ei,wj)]
d(ei, wj) ? 0
where ? is a scaling factor, and d(ei, wj) is the
temporal distance between ei and wj . Based on
the psycholinguistic finding that eye gaze happens
before a spoken word, wj is not allowed to be
aligned with ei when wj happens earlier than ei
(i.e., d(ei, wj) > 0). When wj happens no earlier
than ei (i.e., d(ei, wj) ? 0), the closer they are, the
more likely they are aligned. An EM algorithm is
used to estimate p(w|e) and ? in the model.
Our evaluation in (Qu and Chai, 2008) has
shown that Model-2t that incorporates temporal
alignment between speech and eye gaze achieves
significantly better word acquisition performance
compared to the model where no temporal align-
ment is introduced. Therefore, this model is used
for the investigation in this paper.
5 Identification of Closely Coupled
Gaze-Speech Pairs
Successful word acquisition with the translation
models relies on the tight coupling between the
gaze fixations and the speech content. As men-
tioned earlier, not all gaze-speech pairs have this
tight coupling. In a gaze-speech pair, if the speech
190
does not have any word that relates to any of the
gaze fixated entities, this instance only adds noise
to word acquisition. Therefore, we should identify
the closely coupled gaze-speech pairs and only use
them for word acquisition.
In this section, we first describe the feature ex-
traction, then evaluate the application of a logis-
tic regression classifier to predict whether a gaze-
speech pair is a closely coupled gaze-speech in-
stance ? an instance where at least one noun or
adjective in the speech stream describes some en-
tity fixated by the gaze stream. For the training of
the classifier, we manually labeled each instance
as either a coupled instance or not based on the
speech transcript and the gaze fixations.
5.1 Feature Extraction
For a gaze-speech instance, the following sets of
features are automatically extracted.
5.1.1 Speech Features (S)
The following features are extracted from
speech:
? cw ? count of nouns and adjectives.
More nouns and adjectives are expected in
the user?s utterance describing entities.
? cw/ls ? normalized noun/adjective count.
The effect of speech length ls on cw is con-
sidered.
5.1.2 Gaze Features (G)
For each fixated entity ei, let lie be its temporal
fixation length. Note that several gaze fixations
may have the same fixated entity, lie is the total
length of all the gaze fixations that fixate on entity
ei. We extract the following features from gaze
stream:
? ce ? count of different gaze fixated entities.
Fewer fixated entities are expected when the
user is describing entities while looking at
them.
? ce/ls ? normalized entity count.
The effect of temporal spoken utterance
length ls on ce is considered.
? maxi(lie) ? maximal fixation length.
At least one fixated entity?s fixation is ex-
pected to be long enough when the user is
describing entities while looking at them.
? mean(lie) ? average fixation length.
The average gaze fixation length is expected
to be longer when the user is describing enti-
ties while looking at them.
? var(lie) ? variance of fixation lengths.
The variance of the fixation lengths is ex-
pected to be smaller when the user is describ-
ing entities while looking at them.
The number of gaze fixated entities is not only
determined by the user?s eye gaze, but also af-
fected by the visual scene. Let cse be the count
of all the entities that have been visible during the
time period concurrent with the gaze stream. We
also extract the following scene related feature:
? ce/cse ? scene-normalized fixated entity
count.
The effect of the visual scene on ce is consid-
ered.
5.1.3 User Activity Features (UA)
While interacting with the system, the user?s ac-
tivity can also be helpful in determining whether
the user?s eye gaze is tightly linked to the content
of the speech. The following features are extracted
from the user?s activities:
? maximal distance of the user?s movements ?
the maximal change of user position (3D co-
ordinates) during speech.
The user is expected to move within a smaller
range while looking at entities and describing
them.
? variance of the user?s positions
The user is expected to move less frequently
while looking at entities and describing them.
5.1.4 Conversation Context Features (CC)
While talking to the system (i.e., the ?expert?),
the user?s language and gaze behavior are influ-
enced by the state of the conversation. For each
gaze-speech instance, we use the previous sys-
tem response type as a nominal feature to predict
whether this is a closely coupled gaze-speech in-
stance.
In our treasure hunting domain, there are 8 types
of system responses in 2 categories:
System Initiative Responses:
? specific-see ? the system asks whether the
user sees a certain entity, e.g., ?Do you see
another couch??.
? nonspecific-see ? the system asks whether the
user sees anything, e.g., ?Do you see any-
thing else??, ?Tell me what you see?.
191
? previous-see ? the system asks whether the
user has previously seen something, e.g.,
?Have you previously seen a similar object??.
? describe ? the system asks the user to de-
scribe in detail what the user sees, e.g., ?De-
scribe it?, ?Tell me more about it?.
? compare ? the system asks the user to com-
pare what the user sees, e.g., ?Compare these
objects?.
? repair-request ? the system asks the user to
make clarification, e.g., ?I did not understand
that?, ?Please repeat that?.
? action-request ? the system asks the user to
take action, e.g., ?Go back?, ?Try moving it?.
User Initiative Responses:
? misc ? the system hands the initiative back
to the user without specifying further require-
ments, e.g., ?I don?t know?, ?Yes?.
5.2 Evaluation of Gaze-Speech Identification
Given the extracted features and the ?closely cou-
pled? label of each instance in the training set, we
train a logistic regression classifier (le Cessie and
van Houwelingen, 1992) to predict whether an in-
stance is a closely coupled gaze-speech instance.
Since the goal of identifying closely coupled
gaze-speech instances is to improve word acqui-
sition and we are only interested in acquiring
nouns and adjectives, only the instances with rec-
ognized nouns/adjectives are used for training the
logistic regression classifier. Among the 2969 in-
stances with recognized nouns/adjectives and gaze
fixations, 2002 (67.4%) instances are labeled as
?closely coupled?. The prediction is evaluated by
a 10-fold cross validation.
Feature sets Precision Recall
Null (baseline) 0.674 1
S 0.686 0.995
G 0.707 0.958
UA 0.704 0.942
CC 0.688 0.936
G + UA 0.719 0.948
G + UA + S 0.741 0.908
G + UA + CC 0.731 0.918
G + UA + CC + S 0.748 0.899
Table 1: Gaze-speech prediction performance for
the instances with 1-best speech recognition
Table 1 shows the prediction precision and re-
call when different sets of features are used. As
seen in the table, as more features are used, the
prediction precision goes up and the recall goes
down. It is important to note that prediction pre-
cision is more critical than recall for word acqui-
sition when sufficient amount data is available.
Noisy instances where the gaze is not coupled with
the speech content will only hurt word acquisi-
tion since they will guide the translation models
to ground words to the wrong entities. Although
higher recall can be helpful, its effect is expected
to be reduced when more data becomes available.
The results show that speech features (S) and
conversation context features (CC), when used
alone, do not improve prediction precision much
compared to the baseline of predicting all in-
stances as closely coupled (with a precision of
67.4%). When used alone, gaze features (G) and
user activity features (UA) are the two most use-
ful feature sets for increasing prediction precision.
When they are used together, the prediction pre-
cision is further increased. Adding either speech
features or conversation context features to gaze
and user activity features (G + UA + S/CC) further
increases the prediction precision. Using all fea-
tures (G + UA + CC + S) achieves the highest pre-
diction precision, which is significantly better than
the baseline: z = 5.93, p < 0.001. Therefore, we
choose to use all feature sets to identify the closely
coupled gaze-speech instances for word acquisi-
tion.
To compare the effects of the automatic gaze-
speech identification on word acquisition from
various speech input (1-best speech recognition,
speech transcript), we also use the logistic re-
gression classifier with all feature sets to iden-
tify the closely coupled gaze-speech instances for
the instances with speech transcript. For the in-
stances with speech transcript, there are 2948 in-
stances with nouns/adjectives and gaze fixations,
2128 (72.2%) of them being labeled as ?closely
coupled?. The prediction precision is 77.9% and
the recall is 93.8%. The prediction precision is
significantly better than the baseline of predicting
all instances as coupled: z = 4.92, p < 0.001.
6 Evaluation of Word Acquisition
Every conversational system has an initial vocabu-
lary where words are associated with domain con-
cepts of entities. In our evaluation, we assume that
192
the system?s vocabulary has one default word for
each entity that indicates the semantic type of the
entity. For example, the word ?barrel? is the de-
fault word for the entity barrel. For each entity,
we only evaluate those new words that are not in
the system?s vocabulary.
The acquired words are evaluated against the
?gold standard? words that were manually com-
piled for each entity and its properties based on
all users? speech transcripts. For the 115 entities
in our domain, each entity has 1 to 20 ?gold stan-
dard? words. The average number of ?gold stan-
dard? words for an entity is 6.7.
6.1 Evaluation Metrics
We evaluate the n-best acquired words (words
grounded to domain concepts of entities) using
precision, recall, and F-measure. When a differ-
ent n is chosen, we will have different precision,
recall, and F-measure.
We also evaluate the whole ranked candidate
word list on Mean Reciprocal Rank Rate (MRRR)
as in (Qu and Chai, 2008):
MRRR =
?
e
?Ne
i=1 1/index(w
i
e)
?Ne
i=1 1/i
#e
where Ne is the number of all ?gold standard?
words {wie} for entity e, index(w
i
e) is the index
of word wie in the ranked list of candidate words
for entity e.
MRRR measures how close the ranks of the
?gold standard? words in the candidate word lists
are to the best-case scenario where the top Ne
words are the ?gold standard? words for e. The
higher the MRRR, the better is the acquisition per-
formance.
6.2 Evaluation Results
We evaluate the effect of the closely coupled gaze-
speech instances on word acquisition from the 1-
best speech recognition and speech transcript. The
predicted closely coupled gaze-speech instances
are generated by a 10-fold cross validation with
the logistic regression classifier.
Figure 3 shows the precision, recall, and F-
measure of the n-best words acquired from 1-best
speech recognition by Model-2t using all instances
(all), predicted coupled instances (predicted), and
true (manually labeled) coupled instances (true).
As shown in the figure, using predicted coupled
instances achieves consistently better performance
1 2 3 4 5 6 7 8 9 10
0.2
0.25
0.3
0.35
0.4
0.45
n-best
Pre
cis
ion
all
predicted
true
(a) precision
1 2 3 4 5 6 7 8 9 100.05
0.1
0.15
0.2
0.25
0.3
0.35
n-best
Re
cal
l
all
predicted
true
(b) recall
1 2 3 4 5 6 7 8 9 100.1
0.15
0.2
0.25
0.3
n-best
F-m
eas
ure
all
predicted
true
(c) F-measure
Figure 3: Performance of word acquisition on 1-
best speech recognition
than using all instances. These results show that
the identification of coupled gaze-speech predic-
tion helps word acquisition. When the true cou-
pled instances are used, the performance is further
improved. This means that reliable identification
of coupled gaze-speech instances can lead to bet-
ter word acquisition.
Figure 4 shows the precision, recall, and F-
measure of the n-best words acquired from speech
transcript by Model-2t using all instances, pre-
dicted coupled instances, and true coupled in-
stances. Consistent with the performance based
on the 1-best speech recognition, we can observe
193
1 2 3 4 5 6 7 8 9 100.25
0.3
0.35
0.4
0.45
0.5
0.55
n-best
Pre
cis
ion
all
predicted
true
(a) precision
1 2 3 4 5 6 7 8 9 100.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
0.55
n-best
Re
cal
l
all
predicted
true
(b) recall
1 2 3 4 5 6 7 8 9 100.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
n-best
F-m
eas
ure
all
predicted
true
(c) F-measure
Figure 4: Performance of word acquisition on
speech transcript
that automatic identification of coupled instances
results in better word acquisition performance and
using the true coupled instances results in even
better performance.
Table 2 presents the MRRRs achieved by
Model-2t when words are acquired from differ-
ent speech input (speech transcript, 1-best recog-
nition) with different set of instances (all in-
stances, predicted coupled instances, true coupled
instances). These results also show the consis-
tent behavior. Using predicted coupled instances
achieves significantly better MRRR than using all
instances no matter the words are acquired from 1-
best speech recognition (t = 2.59, p < 0.006) or
speech transcript(t = 3.15, p < 0.002). When the
true coupled instances are used, the performances
are further improved for both 1-best recognition
(t = 2.29, p < 0.013) and speech transcript
(t = 5.21, p < 0.001) compared to using pre-
dicted coupled instances.
Instances All Predicted True
Transcript 0.462 0.480 0.526
1-best reco 0.343 0.369 0.390
Table 2: MRRRs based on different data set
The quality of speech recognition is critical to
word acquisition performance. Comparing word
acquisition based on speech transcript and 1-best
speech recognition, as expected, word acquisition
performance on speech transcript is much better
than on recognized speech. However, the acqui-
sition performance based on speech transcript is
still comparably low. For example, the recall of
acquired words is still below 55% even when the
10 best word candidates are acquired for each en-
tity. This is mainly due to the scarcity of words.
Many words appear less than three times in the
data, which makes them unlikely to be associated
with any entity by the translation model. When
more data is available, we expect to see better ac-
quisition performance.
Note that our current evaluation is based on a
two-stage approach, i.e., first identifying closely-
coupled streams based on supervised classifica-
tion and then automatically establishing mappings
between words and entities in an unsupervised
manner. There could be other approaches to ad-
dress the word acquisition problem (e.g., super-
vised learning to directly identify whether a word
is mapped to an object). Our two-stage approach
has the advantage of requiring minimum super-
vision since the models learned from the first
stage is application-independent and is potentially
portable to different domains.
7 Conclusions
Unlike in the typical settings for psycholinguistic
studies, human eye gaze can serve different func-
tions during human machine conversation. Some
gaze and speech streams may not be tightly cou-
pled and thus can be detrimental to word acqui-
sition. Therefore, this paper describes an ap-
proach that incorporates features from the interac-
194
tion context to identify closely coupled gaze and
speech streams. Our empirical results indicate
that the word acquisition based on these automati-
cally identified gaze-speech streams achieves sig-
nificantly better performance than the word acqui-
sition based on all gaze-speech streams. Our fu-
ture work will combine gaze-based word acquisi-
tion with multiple speech recognition hypotheses
(e.g., word lattices) to further improve word acqui-
sition and language interpretation performance.
Acknowledgments
This work was supported by grants IIS-0347548
and IIS-0535112 from the National Science Foun-
dation. We thank anonymous reviewers for their
valuable comments and suggestions.
References
G. Aist, J. Dowding, B. A. Hockey, M. Rayner,
J. Hieronymus, D. Bohus, B. Boven, N. Blaylock,
E. Campana, S. Early, G. Gorrell, and S. Phan.
2003. Talking through procedures: An intelligent
space station procedure assistant. In Proceedings of
the 10th Conference of the European Chapter of the
Association for Computational Linguistics (EACL).
S. Bangalore and M. Johnston. 2004. Robust multi-
modal understanding. In Proceedings of the Inter-
national Conference on Acoustics, Speech, and Sig-
nal Processing (ICASSP).
K. Barnard, P. Duygulu, N. de Freitas, D. Forsyth,
D. Blei, and M. Jordan. 2003. Matching words and
pictures. Journal of Machine Learning Research,
3:1107?1135.
D. Byron, T. Mampilly, V. Sharma, and T. Xu. 2005.
Utilizing visual attention for cross-modal corefer-
ence interpretation. In Proceedings of the Fifth
International and Interdisciplinary Conference on
Modeling and Using Context (CONTEXT-05), pages
83?96.
K. Eberhard, M. Spivey-Knowiton, J. Sedivy, and
M. Tanenhaus. 1995. Eye movements as a win-
dow into real-time spoken language comprehension
in natural contexts. Journal of Psycholinguistic Re-
search, 24:409?436.
Z. Griffin and K. Bock. 2000. What the eyes say about
speaking. Psychological Science, 11:274?279.
M. Just and P. Carpenter. 1976. Eye fixations and cog-
nitive processes. Cognitive Psychology, 8:441?480.
D. Kahneman. 1973. Attention and Effort. Prentice-
Hall, Inc., Englewood Cliffs.
S. le Cessie and J. van Houwelingen. 1992. Ridge
estimators in logistic regression. Applied Statistics,
41(1):191?201.
O. Lemon, A. Gruenstein, and S. Peters. 2002. Col-
laborative activities and multitasking in dialogue
systems. Traitement Automatique des Langues,
43(2):131?154.
Y. Liu, J. Chai, and R. Jin. 2007. Automated vocab-
ulary acquisition and interpretation in multimodal
conversational systems. In Proceedings of the 45th
Annual Meeting of the Association of Computational
Linguistics (ACL).
A. Meyer, A. Sleiderink, and W. Levelt. 1998. View-
ing and naming objects: eye movements during
noun phrase production. Cognition, 66(22):25?33.
Y. Nakano, G. Reinstein, T. Stocky, and J. Cassell.
2003. Towards a model of face-to-face grounding.
In Proceedings of the Annual Meeting of the Associ-
ation for Computational Linguistics (ACL).
Z. Prasov and J. Chai. 2008. What?s in a gaze? the role
of eye-gaze in reference resolution in multimodal
conversational interfaces. In Proceedings of ACM
12th International Conference on Intelligent User
interfaces (IUI).
S. Qu and J. Chai. 2006. Salience modeling based
on non-verbal modalities for spoken language un-
derstanding. In Proceedings of the International
Conference on Multimodal Interfaces (ICMI), pages
193?200.
S. Qu and J. Chai. 2008. Incorporating temporal and
semantic information with eye gaze for automatic
word acquisition in multimodal conversational sys-
tems. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 244?253.
K. Rayner. 1998. Eye movements in reading and in-
formation processing - 20 years of research. Psy-
chological Bulletin, 124(3):372?422.
L. Razzaq and N. Heffernan. 2004. Tutorial dialog in
an equation solving intelligent tutoring system. In
Proceedings of the Workshop on Dialog-based In-
telligent Tutoring Systems: State of the art and new
research directions.
D. Roy and A. Pentland. 2002. Learning words from
sights and sounds, a computational model. Cogni-
tive Science, 26(1):113?146.
M. Tanenhaus, M. Spivey-Knowiton, K. Eberhard, and
J. Sedivy. 1995. Integration of visual and linguis-
tic information in spoken language comprehension.
Science, 268:1632?1634.
C. Yu and D. Ballard. 2004. A multimodal learning
interface for grounding spoken language in sensory
perceptions. ACM Transactions on Applied Percep-
tions, 1(1):57?80.
195
Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 206?215,
Queen Mary University of London, September 2009. c?2009 Association for Computational Linguistics
What do We Know about Conversation Participants: Experiments on
Conversation Entailment
Chen Zhang Joyce Y. Chai
Department of Computer Science and Engineering
Michigan State University
East Lansing, MI 48824, USA
{zhangch6, jchai}@cse.msu.edu
Abstract
Given the increasing amount of conversa-
tion data, techniques to automatically ac-
quire information about conversation par-
ticipants have become more important.
Towards this goal, we investigate the prob-
lem of conversation entailment, a task
that determines whether a given conversa-
tion discourse entails a hypothesis about
the participants. This paper describes
the challenges related to conversation en-
tailment based on our collected data and
presents a probabilistic framework that in-
corporates conversation context in entail-
ment prediction. Our preliminary exper-
imental results have shown that conver-
sation context, in particular dialogue act,
plays an important role in conversation en-
tailment.
1 Introduction
Conversation is a joint activity between its partic-
ipants (Clark, 1996). Their goals and their under-
standing of mutual beliefs of each other shape the
linguistic discourse of conversation. In turn, this
linguistic discourse provides tremendous informa-
tion about conversation participants. Given the
increasing amount of available conversation data
(e.g., conversation scripts such as meeting scripts,
court records, and online chatting), an important
question is what do we know about conversation
participants? The capability to automatically ac-
quire such information can benefit many appli-
cations, for example, development of social net-
works and discovery of social dynamics.
Related to this question, previous work has de-
veloped techniques to extract profiling informa-
tion about participants from conversation inter-
views (Jing et al, 2007) and to automatically iden-
tify dynamics between conversation participants
such as agreement/disagreement from multiparty
meeting scripts (Galley et al, 2004). We approach
this question from a different angle as a conversa-
tion entailment problem: given a conversation dis-
course D and a hypothesis H concerning its par-
ticipant, the goal is to identify whether D entails
H. For instance, in the following example, the first
hypothesis can be entailed from the dialogue seg-
ment while the second hypothesis cannot.
Example 1:
Dialogue Segment:
A: And where about were you born?
B: Up in Person Country.
Hypothesis:
(1) B was born in Person Country.
(2) B lives in Person Country.
Inspired by textual entailment (Dagan et al,
2005; Bar-Haim et al, 2006; Giampiccolo et al,
2007), conversation entailment provides an inter-
mediate step towards acquiring information about
conversation participants. What we should know
or would like to know about a participant can be
rather open. The type of information needed about
participants is also application-dependent and dif-
ficult to generalize. In conversation entailment, we
will not face this problem since hypotheses can be
used to express any type of information about a
participant one might be interested in. Although
hypotheses are currently given in our investiga-
tion, they can potentially be automatically gener-
ated based on information needs and/or theories
on cognitive status/mental models of conversation
participants. The capability to make correct entail-
ment judgements based on these hypotheses will
benefit many applications such as information ex-
traction, question answering, and summarization.
As a first step in our investigation, we collected
a corpus of conversation entailment data from
nineteen human annotators. Our data showed that
conversation entailment is more challenging than
206
the textual entailment task due to unique charac-
teristics about conversation and conversational im-
plicature. To predict entailment, we developed a
probabilisitic framework that incorporates seman-
tic representation of conversation context. Our
preliminary experimental results have shown that
conversation context, in particular dialogue acts,
play an important role in conversation entailment.
2 Related Work
Recent work has applied different approaches
to acquire information about conversation par-
ticipants based on human-human conversation
scripts, for example, to extract profiling infor-
mation from conversation interviews (Jing et al,
2007) and to identify agreement/disagreement
between participants from multiparty meeting
scripts (Galley et al, 2004). In human-machine
conversation, inference about conversation partic-
ipants has been studied as a part of user modeling.
For example, earlier work has investigated infer-
ence of user intention from utterances to control
clarification dialogue (Horvitz and Paek, 2001)
and recognition of user emotion and attitude from
utterances for intelligent tutoring systems (Litman
and Forbes-Riley, 2006). In contrast to previous
work, we propose a new angle to address informa-
tion acquisition about conversation participants,
namely, through conversation entailment.
This work is inspired by a large body of recent
work on textual entailment initiated by the PAS-
CAL RTE Challenge (Dagan et al, 2005; Bar-
Haim et al, 2006; Giampiccolo et al, 2007). Nev-
ertheless, conversation discourse is very different
from written monologue discourse. The conversa-
tion discourse is shaped by the goals of its partici-
pants and their mutual beliefs. The key distinctive
features include turn-taking between participants,
grounding between participants, and different lin-
guistic phenomena of utterances (e.g., utterances
in conversation tend to be shorter, with disfluency,
and sometimes incomplete or ungrammatical). It
is the goal of this paper to explore how techniques
developed for textual entailment can be extended
to address these unique behaviors in conversation
entailment.
3 Experimental Data
The first step in our investigation is to collect en-
tailment data to help us better understand the prob-
lem and facilitate algorithm development and eval-
uation.
3.1 Data Collection Procedure
We selected 50 dialogues from the Switchboard
corpus (Godfrey and Holliman, 1997). In each of
these dialogues, two participants discuss a topic
of interest (e.g., sports activities, corporate cul-
ture, etc.). To focus our work on the entailment
problem, we use the transcribed scripts of the di-
alogues in our experiments. We also make use of
available annotations such as syntactic structures,
disfluency markers, and dialogue acts.
We had 15 volunteer annotators read the se-
lected dialogues and create hypotheses about par-
ticipants. As a result, a total of 1096 entailment
examples were created. Each example consists of
a snippet from the dialogue (referred to as dia-
logue segment in the rest of this paper), a hypothe-
sis statement, and a truth value indicating whether
the hypothesis can be inferred from the snippet
given the whole history of that dialogue session.
During annotation, we asked the annotators to pro-
vide balanced examples for each dialogue. That is,
roughly half of the hypotheses are truly entailed
and half are not. Special attention was given to
negative entailment examples. Since any arbitrary
hypotheses that are completely irrelevant can be
negative examples, a special criteria is enforced
that any negative examples should have a major-
ity word overlap with the snippet. In addition, in-
spired by previous work (Jing et al, 2007; Galley
et al, 2004), we particularly asked annotators to
provide hypotheses that address the profiling in-
formation of the participants, their opinions and
desires, as well as the dynamic communicative re-
lations between participants.
A recent study shows that for many NLP an-
notation tasks, the reliability of a small number
of non-expert annotations is on par with that of
an expert annotator (Snow et al, 2008). It also
found that for tasks such as affection recogni-
tion, an average of four non-expert labels per item
are capable of emulating expert-level label qual-
ity. Based on this finding, in our study the en-
tailment judgement for each example was further
independently annotated by four annotators (who
were not the original contributors of the hypothe-
ses). As a result, on average each entailment ex-
ample (i.e., a pair of snippet and hypothesis) re-
ceived five judgements.
207
Figure 1: Agreement histogram of entailment
judgements
3.2 Data and Examples
Figure 1 shows a histogram of the agreements of
collected judgements. It indicates that conversa-
tion entailment is in fact a quite difficult task even
for humans. Only 53% of all the examples (586
out of 1096) are agreed upon by all human annota-
tors. The disagreement between users sometimes
is caused by language ambiguity since conversa-
tion scripts are often short and without clear sen-
tence boundaries. For example,
Example 2:
Dialogue Segment:
A: Margaret Thatcher was prime minister, uh,
uh, in India, so many, uh, women are heads
of state.
Hypothesis:
A believes that Margaret Thatcher was prime
minister of India.
In the utterance of speaker A, the prepositional
phrase in India is ambiguous because it can either
be attached to the preceding sentence, which suffi-
ciently entails the hypothesis; or it can be attached
to the succeeding sentence, which leaves it unclear
which country A believes Margaret Thatcher was
prime minister of.
Difference in recognition and handling of con-
versational implicature is another issue that led to
disagreement among annotators. For example:
Example 3:
Dialogue Segment:
A: Um, I had a friend who had fixed some, uh,
chili, buffalo chili and, about a week before
we went to see the movie.
Hypothesis:
A ate some buffalo chili.
Example 4:
Dialogue Segment:
B: Um, I?ve visited the Wyoming area. I?m
not sure exactly where Dances with Wolves
was filmed.
Hypothesis:
B thinks Dances with Wolves was filmed in
Wyoming.
In the first example, a listener could assume
that A follows the maxim of relevance. Therefore,
a natural inference that makes ?fixing of buffalo
chili? relevant is that A ate the buffalo chili. Sim-
ilarly, in the second example, the speaker A men-
tions a visit to Wyoming, which can be considered
relevant to the filming place of DANCES WITH
WOLVES. Some annotators recognized such rele-
vance and some did not.
Given the discrepencies between annotators, we
selected 875 examples which have at least 75%
agreement among the judgements in our current
investigation. We further selected one-third of this
data (291 examples) as our development data. The
experiments reported in Section 5 are based on this
development set.
3.3 Types of Hypotheses
The hypotheses collected from our study can be
categorzied into the following four types:
Fact. Facts about the participants. This includes:
(1) profiling information about individual partici-
pants (e.g., occupation, birth place, etc.); (2) activ-
ities associated with individual participants (e.g.,
A bikes to work everyday); and (3) social rela-
tions between participants (e.g., A and B are co-
workers, A and B went to college together).
Belief. Participants? beliefs and opinions about the
physical world. Any statement about the physical
world in fact is a belief of the speaker. Technically,
the state of the physical world that involves the
speaker him/herself is also a type of belief. How-
ever, here we assume a statement about oneself is
true and is considered as a fact.
Desire. Participants? desire of certain actions or
outcomes (e.g., A wants to find a university job).
These desires represent the states of the world the
participant finds pleasant (although they could be
conflicting to each other).
Intent. Participants? deliberated intent, in partic-
ular communicative intention which captures the
intent from one participant on the other partici-
pant such as whether A agrees/disagrees with B
208
on some issue, whether A intends to convince B
on something, etc.
Most of these types are motivated by the Belief-
Desire-Intention (BDI) model, which represents
key mental states and reflects the thoughts of
a conversation participant. Desire is different
from intention. The former arises subconsciously
and the latter arise from rational deliberation that
takes into consideration desires and beliefs (Allen,
1995). The fact type represents the facts about
a participant. Both thoughts and facts are criti-
cal to characterize a participant and thus impor-
tant to serve many other downstream applications.
The above four types account for 47.1%, 34.0%,
10.7%, and 8.2% of our development set respec-
tively.
4 A Probabilistic Framework
Following previous work (Haghighi et al, 2005;
de Salvo Braz et al, 2005; MacCartney et al,
2006), we approach conversation entailment using
a probabilistic framework. To predict whether a
hypothesis statement H can be inferred from a di-
alogue segment D, we estimate the probability
P (D  H|D,H)
Suppose we have a representation of a dia-
logue segment D in m clauses d1, . . . , dm and a
representation of the hypothesis H in n clauses
h1, . . . , hn. Since a hypothesis is the conjunc-
tion of the decomposed clauses, whether it can be
inferred from a segment is equivalent to whether
all of its clauses can be inferred from the seg-
ment. We further simplify the problem by assum-
ing that whether a clause is entailed from a dia-
logue segment is conditionally independent from
other clauses. Note that this conditional indepen-
dence assumption is an over-simplification, but it
gets things started. Therefore:
P (D  H|D,H)
= P (d1 . . . dm  h1 . . . hn|d1, . . . , dm, h1, . . . , hn)
= P (D  h1, . . . , D  hn|D,h1, . . . , hn)
=
n?
j=1
P (D  hj |D = d1 . . . dm, hj)
=
n?
j=1
P (d1 . . . dm  hj |d1, . . . , dm, hj) (1)
If this likelihood is above a certain threshold
(e.g., 0.5 in our experiments), then H is consid-
ered as a true entailment from D.
Given this framework, two important questions
are: (1) how to represent and automatically create
the clauses from each pair of dialogue segment and
hypothesis; and (2) how to estimate probabilities
as shown in Equation 1?
4.1 Clause Representation
Our clause representation is inspired by previ-
ous work on textual entailment (Dagan et al,
2005; Bar-Haim et al, 2006; Giampiccolo et al,
2007). Clause representation has several advan-
tages. First, it can be acquired automatically from
a parse tree (e.g., dependency parser). Second,
it can be used to facilitate both logic-based rea-
soning as in (Tatu and Moldovan, 2005; Bos and
Markert, 2005; Raina et al, 2005) or probabilis-
tic reasoning as in (Haghighi et al, 2005; de
Salvo Braz et al, 2005; MacCartney et al, 2006).
The key difference between our work and previ-
ous work on textual entailment is the representa-
tion of conversation discourse, which has not been
considered in previous work but is important for
conversation entailment, as we will see later.
More specifically, a clause is made up by two
components: Term and Predicate.
Term: A term can be an entity or an event. An
entity refers to a person, a place, an organization,
or other real world entities. This follows the con-
cept of mention in the Automatic Content Extrac-
tion (ACE) evaluation (Doddington et al, 2004).
An event refers to an action or an activity. For
example, from the sentence ?John married Eva in
1940? we can identify an event of marriage. Fol-
lowing the neo-Davidsonian representation (Par-
sons, 1990), all the events are reified as terms in
our representation.
Predicate: A predicate represents either a prop-
erty (i.e., unary) for a term or a relation (i.e., bi-
nary) between two terms. For example, an entity
company has a property of Russian as in the phrase
?a Russian company? (i.e., Russian(company)).
An event visit has a property of recently (i.e.,
recently(visit)) as in the phrase ?visit Brazil re-
cently?. From the phrase ?Prime Minister re-
cently visited Brazil?, there are binary relations:
PrimeMinister is the subject of the event visit (i.e.,
subj(visit, Prime Minister)) and Brazil is the
object of the visit (i.e., obj(visit, Brazil)).
This representation is a direct conversion from
the dependency structure and can be used to rep-
resent the semantics of utterances in the dialogue
209
segments and the semantics of hypotheses. For ex-
ample,
Example 5:
Dialogue Segment:
B: Have you seen Sleeping with the Enemy?
A: No. I?ve heard that?s really great, though.
B: You have to go see that one.
Hypothesis:
B suggests A to watch Sleeping with the Enemy.
Appendix A shows the dependency structure of
the dialogue utterances and the hypothesis from
Example 5. Appendix B shows the correspond-
ing clause representation of the dialogue segment
and the hypothesis. Note that in this represen-
tation, you and I are replaced with the respec-
tive participants. Since the clauses are generated
based on parse trees, most relational predicates are
syntactic-driven.
To facilitate conversation entailment, we fur-
ther augment the representation of a dialogue seg-
ment by incorporating conversation context. Ap-
pendix C shows the augmented representation for
Example 5. It represents the following additional
information:
? Utterance: A group of pseudo terms u1,
u2, . . . are used to represent individual utter-
ances.
? Participant: A relational clause
speaker(?, ?) is used to represent the speaker
of this utterance, e.g., speaker(u1, B).
? Content: A relational clause content(?, ?) is
used to represent the content of an utterance
where the second term is the head of the ut-
terance as identified in the parsing structure.
e.g., content(u3, heard)
? Dialogue act: A relational clause act(?, ?)
is used to represent the dialogue act of the
speaker for a particular utterance. e.g.,
act(u2, no answer). A set of 42 dialogue
acts from the Switchboard annotation are
used here (Godfrey and Holliman, 1997).
? Utterance flow: A relational clause
follow(?, ?) is used to connect each pair of
adjacent utterances. e.g., follow(u2, u1).
We currently do not consider overlap in utter-
ances, but our representation can be modified
to handle this situation by introducing
additional predicates.
4.2 Entailment Prediction
Given the clause representation for a conversation
segment and a hypothesis, the next step is to make
an entailment prediction (as in Equation 1) based
on two models: an Alignment Model and an Infer-
ence Model.
4.2.1 Alignment Model
The alignment model is to find alignments (or
matches) between terms in the clause representa-
tion for a hypothesis and those in the clause rep-
resentation for a conversation segment. We define
an alignment as a mapping function g between a
term x in the dialogue segment and a term y in the
hypothesis. g(x, y) = 1 if x and y are aligned;
otherwise g(x, y) = 0. Note that a verb can be
aligned to a noun as in g(sell, sale) = 1. It is also
possible that there are multiple terms from the seg-
ment mapped to one term in the hypothesis, or vice
versa.
For any two terms x and y, the problem of pre-
dicting the alignment function g(x, y) can be for-
mulated as a binary classification problem. We
used several features to train the classifier, which
include whether x and y are the same (or have the
same stem), whether one term is an acronym of the
other, and their WordNet and distributional simi-
larities (Lin, 1998).
Given an augmented representation with con-
versation context (as in Appendix C), we also
align event terms in the hypothesis (e.g., suggest
in Example 5) to (pseudo) utterance terms in the
dialogue segment. We call it a pseudo alignment.
This is currently done by a set of rules which asso-
ciate event terms in the hypotheses with dialogue
acts. For example, the event term suggest may be
aligned to an utterance with dialogue act of opin-
ion. Appendix D gives a correct alignment for Ex-
ample 5, in which g(u4, x1) = 1 is a pseudo align-
ment.
4.2.2 Inference Model
As shown in Equation 1, to predict the infer-
ence of the entire hypothesis, we need to calculate
the probability that the dialogue segment entails
each clause from the hypothesis. More specifi-
cally, given a clause from the hypothesis hj , a set
of clauses from the dialogue segment d1, . . . , dm,
and an alignment function g between them derived
by the method described in Section 4.2.1, we pre-
dict whether d1, . . . , dm entails hj under the align-
ment g using two different classification models,
210
depending on whether hj is a property or a rela-
tion (i.e. whether it takes one argument (hj(?)) or
two arguments (hj(?, ?))):
Given a property clause from the hypothe-
sis, hj(x), we look for all the property clauses
in the dialogue segment that describes the
same term as x, i.e. a clause set D? =
{di(x?)|di(x?) ? D, g(x?, x) = 1}. Then we pre-
dict whether hj(x) can be inferred from the
clauses in D? by binary classification, using a set
of features similar to those used in the alignment
model.
Given a relational clause from the hypothe-
sis, hj(x, y), we look for the relation between
the counterparts of x and y in the dialogue seg-
ment. That is, we find the set of terms X ? =
{x?|x? ? D, g(x?, x) = 1} and the set of terms
Y ? = {y?|y? ? D, g(y?, y) = 1} and look for the
closest relation between these two sets of terms in
the dependency structure. If there is a path be-
tween any x? ? X ? and any y? ? Y ? in the de-
pendency structure with a length smaller than a
threshold ?L, we predict that hj(x, y) can be in-
ferred. Note that our current handling of the re-
lational clauses is rather simplified. It only cap-
tures whether two terms from an hypothesis are
connected by any relation in the dialogue segment.
Appendix E shows the inference procedure of
the four hypothesis clauses in Example 5. For
each relational clause hj(x, y), the shortest path
between the correspondingX ? and Y ? has a length
of 3 or less, so each of these four clauses is en-
tailed from the dialogue segment. Based on Equa-
tion 1 we can conclude that the overall hypothesis
is entailed.
We trained the alignment model and the in-
ference model (e.g.,the threshold ?L) based on
the development data provided by the PASCAL 3
challenges on textual entailment.
5 Experimental Results
To understand unique behaviors of conversation
entailment, we focused our current experiments
on the development dataset (see Section 3.2).
We are particularly interested in how the tech-
niques for textual entailment can be improved for
conversation entailment. To do so, we applied
our entailment framework on the test data of the
PASCAL-3 RTE Challenge (Giampiccolo et al,
2007). Among 800 testing examples, our ap-
proach achieved an accuracy of 60.6%. This re-
sult is on par with the performance of the me-
dian system of accuracy 61.8% (z-test, p=0.63) in
the PASCAL-3 RTE Challenge. Our current ap-
proach is very lean on the use of external knowl-
edge. Its competitive performance sets up a rea-
sonable baseline for our investigation on conversa-
tion entailment. This same system, modified to tai-
lor linguistic characteristics of conversation (e.g.,
removal of disfluency), was used as the baseline in
our experiments.
5.1 Event Alignment
To understand the effect of conversation context
in the event alignment, we compared two configu-
rations of alignment model for events. The first
configuration is based on the clause representa-
tion of semantics of utterances (as shown in Ap-
pendix B). This is the same configuration as used
in textual entailment. The second configuration
is based on representation of both semantics from
utterances and conversation context (as shown in
Appendix C). We evaluate how well each config-
uration aligns the event terms based on the pair-
wise alignment decision: for any event term tH in
the hypothesis and any term tD in the dialogue,
whether the model can correctly predict that the
two terms should be aligned.
Figure 2(a) shows the comparison of F-measure
between the two models. Depending on the thresh-
old of alignment prediction, the precision and re-
call of the prediction vary. When the thresh-
old is lower, the models tend to give more align-
ments, resulting in lower precision and higher re-
call. When the threshold is higher, the models tend
to give fewer alignments, thus resulting in higher
precision but lower recall. When the threshold
is around 0.5, the alignment reaches its best F-
measure. Regardless of what threshold is cho-
sen, the model based on both utterance and con-
text consistently works better. Figure 2(b) shows
the breakdown based on the types of hypothesis (at
threshold 0.5). The model that incorporates con-
versation context consistently performs better for
all types. Its improvement is particularly signifi-
cant for the intent type of hypothesis.
These results are not surprising. Many event
terms in hypotheses (e.g., suggest, think, etc.) do
not have their counterparts directly expressed in
utterances in the dialogue discourse. Only through
the modeling of dialog acts, these terms can be
aligned to potential pseudo terms in the dialogue
211
segment. For the fact type hypotheses, the event
terms in the hypotheses generally have their coun-
terparts in the dialogue discourse. That explains
why the improvement for the fact type using con-
versation context is minimal.
(a) Overall comparison on F-measure
(b) Comparison for different types of hypothesis
Figure 2: Experimental results on event alignment
5.2 Entailment Prediction
Given correct alignments, we further evaluated
entailment prediction based on three configura-
tions of the inference model: (1) the same infer-
ence model learned from the textual entailment
data and tested on the PASCAL-3 RTE Challenge
(Text); (2) an improved model incorporating a
number of features relevant to dialogues (espe-
cially syntactic structure of utterances) based on
representations without conversation context as in
Appendix B (+Dialogue); (3) a further improved
model based on augmented representations of con-
versation context and using dialogue acts during
the prediction of entailment as in Appendix C
(+Context).
System Acc Prec Recall F
Text 53.6% 71.6% 29.3% 41.6%
+Dialogue 58.4% 84.1% 32.3% 46.7%
+Context 67.7% 91.7% 47.0% 62.1%
Table 1: Experimental results on entailment pre-
diction
For each configuration we present two evalua-
tion metrics: an accuracy of the overall prediction
and a precision-recall measurement for the posi-
tive entailment examples. All the evaluations are
performed on our development data, which has
56.4% of positive examples and 43.6% of negative
examples.
The evaluations results are shown in Table 1.
The system learned from textual entailment per-
forms lower than the prediction based on the
majority class (56.4%). Incorporating syntactic
features of dialogues did better but the differ-
ence is not statistically significant. Incorporat-
ing conversation context, especially dialogue acts,
achieves significantly better performance (z-test,
p < 0.005).
Table 2 shows the comparison of the three con-
figurations based on different types of hypothesis.
As expected, the basic system trained on textual
entailment is not capable for any intent type of
hypotheses. Modeling conversation context with
dialogue acts improves inference for all types of
hypothesis, with most significant improvement for
the belief, desire, and intent types of hypothesis.
6 Conclusion
This paper describes our initial investigation on
conversation entailment to address information ac-
quisition about conversation participants. Since
there are so many variables involved in the pre-
diction, our experiments have been focused on a
set of development data where most of the features
are annotated. This allowed us to study the effect
of conversation context in both alignment and en-
tailment. Our future work will enhance the cur-
rent approach by training the models based on our
development data and evaluate them on the test-
ing data. Conversation entailment is an important
task. Although the current exercise is targeted to
process conversation scripts from human-human
conversation, it can potentially benefit human ma-
chine conversation by enabling automated agents
to gain better understanding of their conversation
212
Fact Belief Desire Intent
System Acc F Acc F Acc F Acc F
Text 58.4% 51.3% 52.5% 37.3% 51.6% 34.8% 33.3% 0
+Dialogue 68.6% 62.6% 53.5% 36.1% 48.4% 33.3% 33.3% 0
+Context 70.8% 64.9% 67.7% 62.8% 58.1% 47.8% 62.5% 60.9%
Table 2: Experimental results on entailment prediction for different types of hypotheses
partners.
Acknowledgments
This work was partially supported by IIS-0347548
and IIS-0840538 from the National Science Foun-
dation. We thank the anonymous reviewers for
their valuable comments and suggestions.
References
James Allen. 1995. Natural language understanding.
The Benjamin/Cummings Publishing Company, Inc.
Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro,
Danilo Giampiccolo, Bernardo Magnini, and Idan
Szpektor. 2006. The second pascal recognising
textual entailment challenge. In Proceedings of the
Second PASCAL Challenges Workshop on Recognis-
ing Textual Entailment, Venice, Italy.
Johan Bos and Katja Markert. 2005. Recognising tex-
tual entailment with logical inference. In Proceed-
ings of HLT-EMNLP, pages 628?635.
Herbert H. Clark. 1996. Using Language. Cambridge
University Press.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2005. The pascal recognising textual entailment
challenge. In PASCAL Challenges Workshop on
Recognising Textual Entailment.
Rodrigo de Salvo Braz, Roxana Girju, Vasin Pun-
yakanok, Dan Roth, and Mark Sammons. 2005. An
inference model for semantic entailment in natural
language. In Proceedings of AAAI.
G. Doddington, A. Mitchell, M. Przybocki, and
L. Ramshaw. 2004. The automatic content extrac-
tion (ace) programctasks, data, and evaluation. In
Proceedings of the 4th International Conference on
Language Resources and Evaluation (LREC).
Michel Galley, Kathleen McKeown, Julia Hirschberg,
and Elizabeth Shriberg. 2004. Identifying agree-
ment and disagreement in conversational speech:
Use of bayesian networks to model pragmatic de-
pendencies. In Proceedings of ACL, pages 669?676.
Danilo Giampiccolo, Bernardo Magnini, Ido Dagan,
and Bill Dolan. 2007. The third pascal recogniz-
ing textual entailment challenge. In Proceedings of
the ACL-PASCAL Workshop on Textual Entailment
and Paraphrasing, pages 1?9.
John J. Godfrey and Edward Holliman. 1997.
Switchboard-1 Release 2. Linguistic Data Consor-
tium, Philadelphia.
Aria Haghighi, Andrew Ng, and Christopher Manning.
2005. Robust textual inference via graph matching.
In Proceedings of HLT-EMNLP, pages 387?394.
Eric Horvitz and Tim Paek. 2001. Harnessing mod-
els of users? goals to mediate clarification dialog in
spoken language systems. In Proceedings of the 8th
International Conference on User Modeling, pages
3?13.
Hongyan Jing, Nanda Kambhatla, and Salim Roukos.
2007. Extracting social networks and biographical
facts from conversational speech transcripts. In Pro-
ceedings of ACL, pages 1040?1047.
Dekang Lin. 1998. An information-theoretic defini-
tion of similarity. In Proceedings of International
Conference on Machine Learning, pages 296?304.
Diane Litman and Katherine Forbes-Riley. 2006. Rec-
ognizing student emotions and attitudes on the basis
of utterances in spoken tutoring dialogues with both
human and computer tutors. Speech Communica-
tion, 48(5):559?590.
Bill MacCartney, Trond Grenager, Marie-Catherine
de Marneffe, Daniel Cer, and Christopher D. Man-
ning. 2006. Learning to recognize features of valid
textual entailments. In Proceedings of HLT-NAACL,
pages 41?48.
Terence Parsons. 1990. Events in the Semantics of En-
glish. A Study in Subatomic Semantics. MIT Press.
Rajat Raina, Andrew Y. Ng, and Christopher D. Man-
ning. 2005. Robust textual inference via learning
and abductive reasoning. In Proceedings of AAAI,
pages 1099?1105.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and
Andrew Ng. 2008. Cheap and fast ? but is it good?
evaluating non-expert annotations for natural lan-
guage tasks. In Proceedings of EMNLP, pages 254?
263.
Marta Tatu and Dan Moldovan. 2005. A semantic ap-
proach to recognizing textual entailment. In Pro-
ceedings of HLT-EMNLP, pages 371?378.
213
APPENDIX
A Dependency Structure of Dialogue Utterances and Hypothesis in Example 5
Di
alo
gu
e S
eg
m
en
t:
B:
 H
av
e y
ou
se
en
Sl
ee
pin
g w
ith
 th
e E
ne
my
?
A:
 N
o. 
I 'v
e h
ea
rd
tha
t's
 re
all
y g
rea
t, t
ho
ug
h.
B:
 Y
ou
ha
ve
to 
go
se
et
ha
t o
ne
.
x 1
A
x 2
x 3
A
x 4
x 5
x 6
tho
ug
h(
?)
A
x 7
x 8
x 9
x 10
B
A
x 1
x 2
x 3
Hy
po
th
es
is:
B
su
gg
es
ts
A
to 
wa
tch
Sl
ee
pin
g w
ith
 th
e E
ne
my
.
ter
ms
pr
ed
ica
tes
B Clause Representation of Dialogue Segment and Hypothesis for Example 5s
ub
j(x
1,B
), 
ob
j(x
1,A
), 
ob
j(x
1,x
2),
 ob
j(x
2,x
3)
x 1=
su
gg
es
ts,
 x 2
=w
atc
h, 
x 3=
Sle
ep
ing
 w
ith
 th
e E
ne
my
, A
, B
Hy
po
th
es
is:
su
bj(
x 7,
A)
, o
bj(
x 7,
x 8)
, o
bj(
x 8,
x 9)
, o
bj(
x 9,
x 10
)
x 7=
ha
ve
, x
8=
go
, x
9=
se
e, 
x 10
=o
ne
, A
B:
su
bj(
x 4,
A)
, o
bj(
x 4,
x 6)
, s
ub
j(x
6,x
5),
 th
ou
gh
(x 4
)
x 4=
ha
ve
 he
ar
d, 
x 5=
tha
t, 
x 6=
is 
re
all
yg
re
at,
 A
A:
su
bj(
x 2,
A)
, o
bj(
x 2,
x 3)
, a
ux
(x 2
,x 1
)
x 1=
ha
ve
, x
2=
se
en
, 
x 3=
Sle
ep
ing
 w
ith
 th
e E
ne
my
, A
B:
Cl
au
se
s
Te
rm
s
Di
alo
gu
e S
eg
m
en
t:
C Augmented Clause Representation of Dialogue Segment in Example 5s
pe
ak
er
(u
4,B
), 
co
nte
nt(
u 4
,x 7
), 
ac
t(u
4,o
pin
ion
), 
su
bj(
x 7,
A)
, o
bj(
x 7,
x 8)
, o
bj(
x 8,
x 9)
, o
bj(
x 9,
x 10
)
u 4
,x
7=
ha
ve
, x
8=
go
, x
9=
se
e, 
x 10
=o
ne
, A
, B
B:
fol
low
(u
2,u
1),
 fo
llo
w(
u 3
,u 2
), 
fol
low
(u
4,u
3)
sp
ea
ke
r(u
2,A
), 
co
nte
nt(
u 2
,-)
, a
ct(
u 2
,no
_a
ns
we
r),
 
sp
ea
ke
r(u
3,A
), 
co
nte
nt(
u 3
,x 4
), 
ac
t(u
3,s
tat
em
en
t),
su
bj(
x 4,
A)
, o
bj(
x 4,
x 6)
, s
ub
j(x
6,x
5),
 th
ou
gh
(x 4
)
u 2
,u
3,x
4=
ha
ve
 he
ar
d, 
x 5=
tha
t, 
x 6=
is 
re
all
yg
re
at,
 A
A:
sp
ea
ke
r(u
1,B
), 
co
nte
nt(
u 1
,x 2
), 
ac
t(u
1,w
h_
qu
es
tio
n)
, 
su
bj(
x 2,
A)
, o
bj(
x 2,
x 3)
, a
ux
(x 2
,x 1
)
u 1
,x
1=
ha
ve
, x
2=
se
en
, 
x 3=
Sle
ep
ing
 w
ith
 th
e E
ne
my
, A
, B
B:
Cl
au
se
s
Te
rm
s
Di
alo
gu
e S
eg
m
en
t (
wi
th
 co
nt
ex
t r
ep
re
se
nt
at
ion
):
214
D The Alignment for Example 5
x 2=
se
en
x 3=
Sle
ep
ing
 w
ith
 th
e E
ne
my
 
BA x 5=
tha
t
x 7=
ha
ve
u 4
: a
ct(
u 4
,op
ini
on
)
Di
alo
gu
e S
eg
m
en
t
x 1=
su
gg
es
ts
x 3=
Sle
ep
ing
 w
ith
 th
e E
ne
my
 
BA x 2=
wa
tch
Hy
po
th
es
is
x 4=
ha
ve
 he
ar
d
x 8=
go
x 9=
se
e
x 10
=o
ne
x 6=
is 
re
all
yg
re
at
u 3
: a
ct(
u 3
,st
ate
me
nt)
u 2
: a
ct(
u 2
,no
_a
ns
we
r)
u 1
: a
ct(
u 1
,w
h_
qu
es
tio
n)
x 1=
ha
ve
E The Prediction of Inference for the Hypothesis Clauses in Example 5
x 3,
 x 5
, 
x 10x 3
x 2,
 x 9x 2
AA
BB
ye
s
ye
s
ye
s
ye
s
Hy
po
the
sis
 C
lau
se
 
En
tai
led
?
1
3
2
1
Pa
th 
Le
ng
th
ob
j(x
9,x
10
)
co
nte
nt(
u 4
,x 7
), 
ob
j(x
7,x
8),
 
ob
j(x
8,x
9)
co
nte
nt(
u 4
,x 7
), 
su
bj(
x 7,
A)
sp
ea
ke
r(u
4,B
)
Sh
or
tes
t P
ath
 be
tw
ee
n t
he
 
Al
ign
ed
 T
erm
s i
n t
he
 
De
pe
nd
en
cy
 St
ru
ctu
re 
of
 
Di
alo
gu
e S
eg
me
nt
x 2,
 x 9
u 4
u 4
u 4
Al
ign
ed
 T
erm
s i
n t
he
 
Di
alo
gu
e S
eg
me
nt
x 2
x 1
x 1
x 1
Te
rm
s i
n t
his
 C
lau
se
rel
ati
on
rel
ati
on
rel
ati
on
rel
ati
on
Cl
au
se
 T
yp
e
ob
j(x
2,x
3)
ob
j(x
1,x
2)
ob
j(x
1,A
)
su
bj(
x 1,
B)
 
Hy
po
the
sis
 C
lau
se
215
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 471?481,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Fusing Eye Gaze with Speech Recognition Hypotheses to
Resolve Exophoric References in Situated Dialogue
Zahar Prasov and Joyce Y. Chai
Department of Computer Science and Engineering
Michigan State University
East Lansing, MI 48824, USA
{prasovza,jchai}@cse.msu.edu
Abstract
In situated dialogue humans often utter lin-
guistic expressions that refer to extralinguistic
entities in the environment. Correctly resolv-
ing these references is critical yet challeng-
ing for artificial agents partly due to their lim-
ited speech recognition and language under-
standing capabilities. Motivated by psycholin-
guistic studies demonstrating a tight link be-
tween language production and human eye
gaze, we have developed approaches that in-
tegrate naturally occurring human eye gaze
with speech recognition hypotheses to resolve
exophoric references in situated dialogue in
a virtual world. In addition to incorporat-
ing eye gaze with the best recognized spo-
ken hypothesis, we developed an algorithm to
also handle multiple hypotheses modeled as
word confusion networks. Our empirical re-
sults demonstrate that incorporating eye gaze
with recognition hypotheses consistently out-
performs the results obtained from processing
recognition hypotheses alone. Incorporating
eye gaze with word confusion networks fur-
ther improves performance.
1 Introduction
Given a rapid growth in virtual world applications
for tutoring and training, video games and simu-
lations, and assistive technology, enabling situated
dialogue in virtual worlds has become increasingly
important. Situated dialogue allows human users to
navigate in a spatially rich environment and carry
a conversation with artificial agents to achieve spe-
cific tasks pertinent to the environment. Different
from traditional telephony-based spoken dialogue
systems and multimodal conversational interfaces,
situated dialogue supports immersion and mobility
in a visually rich environment and encourages so-
cial and collaborative language use (Byron et al,
2005; Gorniak et al, 2006). In situated dialogue, hu-
man users often need to make linguistic references,
known as exophoric referring expressions (e.g., the
book to the right), to extralinguistic entities
in the environment. Reliably resolving these ref-
erences is critical for dialogue success. However,
reference resolution remains a challenging problem,
partly due to limited speech and language process-
ing capabilities caused by poor speech recognition
(ASR), ambiguous language, and insufficient prag-
matic knowledge.
To address this problem, motivated by psycholin-
guistic studies demonstrating a close relationship
between language production and eye gaze, our
previous work has incorporated naturally occurring
eye gaze in reference resolution (Prasov and Chai,
2008). Our findings have shown that eye gaze can
partially compensate for limited language process-
ing and domain modeling. However, this work was
conducted in a setting where users only spoke to a
static visual interface. In situated dialogue, human
speech and eye gaze patterns are much more com-
plex. The dynamic nature of the environment and
the complexity of spatially rich tasks have a massive
influence on what the user will look at and say. It is
not clear to what degree prior findings can generalize
to situated dialogue. Therefore, this paper explores
new studies on incorporating eye gaze for exophoric
reference resolution in a fully situated virtual envi-
471
ronment ? a more realistic approximation of real
world interaction. In addition to incorporating eye
gaze with the best recognized spoken hypothesis, we
developed an algorithm to also handle multiple hy-
potheses modeled as word confusion networks.
Our empirical results have demonstrated the util-
ity of eye gaze for reference resolution in situ-
ated dialogue. Although eye gaze is much more
noisy given the mobility of the user, our results
have shown that incorporating eye gaze with recog-
nition hypotheses consistently outperform the re-
sults obtained from processing recognition hypothe-
ses alone. In addition, incorporating eye gaze with
word confusion networks further improves perfor-
mance. Our analysis also indicates that, although a
word confusion network appears to be more compli-
cated, the time complexity of its integration with eye
gaze is well within the acceptable range for real-time
applications.
2 Related Work
Prior work in reference resolution within situated di-
alogue has focused on using visual context to assist
reference resolution during interaction. In (Kelleher
and van Genabith, 2004) and (Byron et al, 2005), vi-
sual features of objects are used to model the focus
of attention. This attention modeling is subsequently
used to resolve references. In contrast to this line of
research, here we explore the use of human eye gaze
during real-time interaction to model attention and
facilitate reference resolution. Eye gaze provides a
richer medium for attentional information, but re-
quires processing of a potentially noisy signal.
Eye gaze has been used to facilitate human ma-
chine conversation and automated language process-
ing. For example, eye gaze has been studied in
embodied conversational discourse as a mechanism
to gather visual information, aid in thinking, or fa-
cilitate turn taking and engagement (Nakano et al,
2003; Bickmore and Cassell, 2004; Sidner et al,
2004; Morency et al, 2006; Bee et al, 2009).
Recent work has explored incorporating eye gaze
into automated language understanding such as au-
tomated speech recognition (Qu and Chai, 2007;
Cooke and Russell, 2008), automated vocabulary ac-
quisition (Liu et al, 2007; Qu and Chai, 2010), at-
tention prediction (Qvarfordt and Zhai, 2005; Fang
et al, 2009).
Motivated by previous psycholinguistic findings
that eye gaze is tightly linked with language pro-
cessing (Just and Carpenter, 1976; Tanenhous et al,
1995; Meyer and Levelt, 1998; Griffin and Bock,
2000), our prior work incorporates eye gaze into
reference resolution. Our results demonstrate that
such use of eye gaze can potentially compensate
for a conversational systems limited language pro-
cessing and domain modeling capability (Prasov and
Chai, 2008). However, this work is conducted in a
static visual environment and evaluated only on tran-
scribed spoken utterances. In situated dialogue, eye
gaze behavior is much more complex. Here, gaze
fixations may be made for the purpose of naviga-
tion or scanning the environment rather than refer-
ring to a particular object. Referring expressions can
be made to objects that are not in the user?s field of
view, but were previously visible on the interface.
Additionally, users may make egocentric spatial ref-
erences (e.g. ?the chair on the left?) which require
contextual knowledge (e.g. the users position in the
environment) in order to resolve. Therefore, the fo-
cus of our work here is on exploring these complex
user behaviors in situated dialogue and examining
how to combine eye gaze with ASR hypotheses for
improved reference resolution.
Alternative ASR hypotheses have been used in
many different ways in speech driven systems. Par-
ticularly, in (Mangu et al, 2000) multiple lattice
alignment is used for construction of word confusion
networks and in (Hakkani-Tu?r et al, 2006) word
confusion networks are used for named entity de-
tection. In the study presented here, we apply word
confusion networks (to represent ASR hypotheses)
along with eye gaze to the problem of reference res-
olution.
3 Data Collection
In this investigation, we created a 3D virtual world
(using the Irrlicht game engine1) to support situated
dialogue. We conducted a Wizard of Oz study in
which the user must collaborate with a remote arti-
ficial agent cohort (controlled by a human) to solve
a treasure hunting task. The cohort is an ?expert?
in treasure hunting and has some knowledge regard-
1http://irrlicht.sourceforge.net/
472
ing the locations of the treasure items, but cannot
see the virtual environment. The user, immersed in
the virtual world, must navigate the environment and
conduct a mixed-initiative dialogue with the agent to
find the hidden treasures. During the experiments,
a noise-canceling microphone was used to record
user speech and the Tobii 1750 display-mounted eye
tracker was used to record user eye movements.
A snapshot of user interaction with the treasure
hunting environment is shown in Figure 1. Here,
the user?s eye fixation is represented by the white
dot and saccades (eye movements) are represented
by white lines. The virtual world contains 10 rooms
with a total of 155 unique objects that encompass 74
different object types (e.g. chair or plant).
Figure 1: Snapshot of the situated treasure hunting envi-
ronment
Table 1 shows a portion of a sample dialogue be-
tween a user and the expert. Each Si represents a
system utterance and each Ui represents a user ut-
terance. We focus on resolving exophoric referring
expressions, which are enclosed in brackets here. In
our dataset, an exophoric referring expression is a
non-pronominal noun phrase that refers to an en-
tity in the extralinguistic environment. It may be
an evoking reference that initially refers to a new
object in the virtual world (e.g. an axe in utter-
ance U2) or a subsequent reference to an entity in the
virtual world which has previously been mentioned
in the dialogue (e.g. an axe in utterance U3). In
our study we focus on resolving exophoric referring
expressions because they are tightly coupled with a
user?s eye gaze behavior.
From this study, we constructed a parallel spoken
utterance and eye gaze corpus. Utterances, which
S1 Describe what you?re doing.
U1 I just came out from the room that
I started and i see [one long sword]
U2 [one short sword] and [an axe]
S2 Compare these objects.
U3 one of them is long and one of them
is really short, and i see [an axe]
Table 1: A conversational fragment demonstrating inter-
action with exophoric referring expressions.
Utterance: i just came out from the room that
i started and i see [one long sword]
Ht : . . . i 5210 see 5410 [one 5630
long 6080 sword 6460]
H1: . . . icy 5210 winds 5630 along 6080
so 6460 words 68000
H2: . . . icy 5210 [wine 5630] along 6080
so 6460 words 6800
. . .
H25: . . . icy 5210 winds 5630
[long 6080 sword 6460]
. . .
Table 2: Sample n-best list of recognition hypotheses
are separated by a long pause (500 ms) in speech,
are automatically recognized using the Microsoft
Speech SDK. Gaze fixations are characterized by
objects in the virtual world that are fixated via a
user?s eye gaze. When a fixation points to multi-
ple spatially overlapping objects, only the one in the
forefront is deemed to be fixated. The data corpus
was transcribed and annotated with 2204 exophoric
referring expressions amongst 2052 utterances from
15 users.
4 Word Confusion Networks
For each user utterance in our dataset, an n-best list
(with n = 100) of recognition hypotheses ranked
in order of likelihood is produced by the Microsoft
Speech SDK. One way to use the speech recogni-
tion results (as in most speech applications) is to use
the top ranked recognition hypothesis. This may not
be the best solution because a large amount of infor-
mation is being ignored. Table 2 demonstrates this
problem. Here, the number after the underscore de-
notes a timestamp associated with each recognized
spoken word. The strings enclosed in brackets de-
473
note recognized referring expressions. In this exam-
ple, the manual transcription of the original utter-
ance is shown by Ht. In this case, the system must
first identify one long sword as a referring ex-
pression and then resolve it to the correct set of en-
tities in the virtual world. However, not until the
twenty fifth ranked recognition hypothesis H25, do
we see a referring expression closest to the actual ut-
tered referring expression. Moreover, in utterances
with multiple referring expressions, there may not
be a single recognition hypothesis that contains all
referring expressions, but each referring expression
may be contained in some recognition hypothesis.
Thus, it is desirable to consider the entire n-best list
of hypotheses.
To address this issue, we adopted the word con-
fusion network (WCN): a compact representation
of a word lattice or n-best list (Mangu et al,
2000). A WCN captures alternative word hypothe-
ses and their corresponding posterior probabilities
in time-ordered sets. In addition to being compact,
an important feature for efficient post-processing
of recognition hypotheses for real-time systems,
WCNs are capable of representing more competing
hypotheses than either n-best lists or word lattices.
Figure 2 shows an example of a WCN for the utter-
ance ?. . . I see one long sword? along with a timeline
(in milliseconds) depicting the eye gaze fixations to
potential referent objects that correspond to the ut-
terance. The confusion network shows competing
word hypotheses along with corresponding proba-
bilities in log scale.
Using our data set, we can show that word con-
fusion networks contain significantly more words
that can compose a referring expression than the top
recognition hypothesis. The confusion network key-
word error rate (KWER) is 0.192 compared to a 1-
best list KWER of 0.318, where a keyword is a word
that can be contained in a referring expression. The
overall WER for word confusion networks and 1-
best lists are 0.315 and 0.460, respectively. The re-
ported WCN word error rates are all oracle word er-
ror rates reflecting the best WER that can be attained
using any path in the confusion network. One more
important feature of word confusion networks is that
they provide time alignment for words that occur at
approximately the same time interval in competing
hypotheses. This is not only useful for efficient syn-
tactic parsing, which is necessary for identifying re-
ferring expressions, but also critical for integration
with time aligned gaze streams.
5 Reference Resolution Algorithm
We have developed an algorithm that combines an
n-best list of speech recognition hypotheses with di-
alogue, domain, and eye-gaze information to resolve
exophoric referring expressions. There are three in-
puts to the multimodal reference resolution algo-
rithm for each utterance: (1) an n-best list of alter-
native speech recognition hypotheses (n = 100 for a
WCN and n = 1 for the top recognized hypothesis),
(2) a list of fixated objects (by eye gaze) that tempo-
rally correspond to the spoken utterance and (3) a set
of potential referent objects. Since during the trea-
sure hunting task people typically only speak about
objects that are visible or have recently been visible
on the screen, an object is considered to be a poten-
tial referent if it is present within a close proximity
(in the same room) of the user while an utterance is
spoken.
The multimodal reference resolution algorithm
proceeds with the following four steps:
Step 1: construct word confusion network A
word confusion network is constructed out of the in-
put n-best list of alternative recognition hypotheses
with the SRI Language Modeling (SRILM) toolkit
(Stolcke, 2002) using the procedure described in
(Mangu et al, 2000). This procedure aligns words
from the n-best list into equivalence classes. First,
instances of the same word containing approxi-
mately the same starting and ending timestamps are
clustered. Then, equivalence classes with common
time ranges are merged. For each competing word
hypothesis its probability is computed by summing
the posteriors of all utterance hypotheses containing
this word. In our work, instead of using the actual
posterior probability of each utterance hypothesis
(which was not available), we assigned each utter-
ance hypothesis a probability based on its position
in the ranked list. Figure 2 depicts a portion of the
resulting word confusion network (showing compet-
ing word hypotheses and their probabilities in log
scale) constructed from the n-best list in Table 2.
474
Figure 2: Sample parallel speech and eye gaze data streams, including a portion of the sample WCN
Step 2: extract referring expressions from WCN
The word confusion network is syntactically parsed
using a modified version of the CYK (Cooke and
Schwartz, 1970; Kasami, 1965; Younger, 1967)
parsing algorithm that is capable of taking a word
confusion network as input rather than a single
string. We call this the CYK-WCN algorithm. To
do the parsing, we applied a set of grammar rules
largely derived from a different domain in our pre-
vious work (Prasov and Chai, 2008). A parse chart
of the sample word confusion network is shown in
Table 3. Here, just as in the CYK algorithm the
chart is filled in from left to right then bottom to
top. The difference is that the chart has an added
dimension for competing word hypotheses. This
is demonstrated in position 15 of the WCN, where
one and wine are two nouns that constitute com-
peting words. Note that some words from the con-
fusion network are not in the chart (e.g. winds)
because they are out of vocabulary. The result of
the syntactic parsing is that the parts of speech of
all sub-phrases in the confusion network are identi-
fied. Next, a set of all exophoric referring expres-
sions (i.e. non-pronominal noun phrases) found in
the word confusion network are extracted. Each re-
ferring expression has a corresponding confidence
score, which can be computed in many many dif-
ferent ways. Currently, we simply take the mean of
the probability scores of the expression?s constituent
words. The sample WCN has four such phrases
(shown in bold in Table 3): wine at position 15 with
length 1, one long sword at position 15 with
length 3, long sword at position 16 with length
2, and sword at position 17 with length 1.
Step 3: resolve referring expressions Each re-
ferring expression rj is resolved to the top k po-
tential referent objects according to the probabil-
ity P (oi|rj), where k is determined by information
from the linguistic expressions. P (oi|rj) is deter-
mined using the following expression:
P (oi|rj) =
AS(oi)? ? Compat(oi, rj)1??
?
i
AS(oi)
? ? Compat(oi, rj)
1??
(1)
In this equation,
? AS: Attentional salience score of a particu-
475
...
5
4
length 3 NP? NUM Adj-NP
2 Adj-NP? ADJ N,
NP? Adj-NP
1 (1) N? wine, NP? N ADJ? long N? sword,
(2) NUM? one NP? N
... 14 15 16 17 18 ...
WCN position
Table 3: Syntactic parsing of word confusion network
lar object oi, which is determined based on
the gaze fixation intensity of an object at the
start time of referring expression rj . The fix-
ation intensity of an object is defined as the
amount of time that the object is fixated during
a predefined time windowW (Prasov and Chai,
2008). As in (Prasov and Chai, 2008), we set
W = [?1500..0] ms relative to the beginning
of referring expression rj .
? Compat: Compatibility score, which specifies
whether the object oi is compatible with the in-
formation specified by the referring expression
rj . Currently, the compatibility score is set to 1
if referring expression rj and object oi have the
same object type (e.g. chair), and 0 otherwise.
? ?: Importance weight, in the range [0..1], of
attentional salience relative to compatibility.
A high ? value indicates that the attentional
salience score based on eye gaze carries more
weight in deciding referents, while a low ?
value indicates that compatibility carries more
weight. In this work, we set ? = 0.5 to indicate
equal weighting between attentional salience
and compatibility. If we do not want to inte-
grate eye gaze in reference resolution, we can
set ? = 0.0. In this case, reference resolution
will be purely based on compatibility between
visual objects and information specified via lin-
guistic expressions.
Once all probabilities are calculated, each refer-
ring expression is resolved to a set of referent ob-
jects. Finally, this results in a set of (referring ex-
pression, referent object set) pairs with confidence
scores, which are determined by two components.
The first component is the confidence score of the
referring expression, which is explained in the Step
1 of the algorithm. The second component is the
probability that the referent object set is indeed the
referent of this expression (which is determined by
Equation 1). There are various ways to combine
these two components together to form an overall
confidence score for the pair. Here we simply mul-
tiply the two components. The confidence score for
the pair is used in the following step to prune un-
likely referring expressions.
Step 4: post-prune The resulting set of (referring
expression, referent object set) pairs is pruned to re-
move pairs that fall under one of the following two
conditions: (1) the pair has a confidence score equal
to or below a predefined threshold  (currently, the
threshold is set to 0 and thus keeps all resolved pairs)
and (2) the pair temporally overlaps with a higher
confidence pair. For example, in Table 3, the re-
ferring expressions one long sword and wine
overlap in position 15. Finally, the resulting (refer-
ring expression, referent object set) pairs are sorted
in ascending order according to their constituent re-
ferring expression timestamps.
6 Experimental Results
Using our data, described in Section 3, we applied
the multimodal reference resolution algorithm de-
scribed in Section 5. All of the data is used to
report the experimental results. Reference resolu-
tion model parameters are set based on our prior
work in a different domain (Prasov and Chai, 2008).
For each utterance we compare the reference reso-
lution performance with and without the integration
of eye gaze information. We also evaluate using a
476
word confusion network compared to a 1-best list to
model speech recognition hypotheses. For perspec-
tive, reference resolution with recognized speech in-
put is compared with transcribed speech.
6.1 Evaluation Metrics
The reference resolution algorithm outputs a list of
(referring expression, referent object set) pairs for
each utterance. We evaluate the algorithm by com-
paring the generated pairs to the annotated ?gold
standard? pairs using F-measure. We perform the
following two types of evaluation:
? Lenient Evaluation: Due to speech recognition
errors, there are many cases in which the al-
gorithm may not return a referring expression
that exactly matches the gold standard refer-
ring expression. It may only match based on
the object type. For example, the expressions
one long sword and sword are different,
but they match in terms of the intended object
type. For applications in which it is critical to
identify the objects referred to by the user, pre-
cisely identifying uttered referring expressions
may be unnecessary. Thus, we evaluate the ref-
erence resolution algorithm with a lenient com-
parison of (referring expression, referent object
set) pairs. In this case, two pairs are considered
a match if at least the object types specified via
the referring expressions match each other and
the referent object sets are identical.
? Strict Evaluation: For some applications it may
be important to identify exact referring ex-
pressions in addition to the objects they re-
fer to. This is important for applications that
attempt to learn a relationship between refer-
ring expressions and referenced objects. For
example, in automated vocabulary acquisition,
words other than object types must be identi-
fied so the system can learn to associate these
words with referenced objects. Similarly, in
systems that apply priming for language gen-
eration, identification of the exact referring ex-
pressions from human users could be impor-
tant. Thus, we also evaluate the reference reso-
lution algorithm with a strict comparison of (re-
ferring expression, referent object set) pairs. In
this case, a referring expression from the sys-
tem output needs to exactly match the corre-
sponding expression from the gold standard.
6.2 Role of Eye Gaze
We evaluate the effect of incorporating eye gaze
information into the reference resolution algorithm
using the top best recognition hypothesis (1-best),
the word confusion network (WCN), and the man-
ual speech transcription (Transcription). Speech
transcription, which contains no recognition errors,
demonstrates the upper bound performance of our
approach. When no gaze information is used, ref-
erence resolution solely depends on linguistic and
semantic processing of referring expressions. Table
4 shows the lenient reference resolution evaluation
using F-measure. This table demonstrates that le-
nient reference resolution is improved by incorpo-
rating eye gaze information. This effect is statisti-
cally significant in the case of transcription and 1-
best (p < 0.0001 and p < 0.009, respectively) and
marginal (p < 0.07) in the case of WCN.
Configuration Without Gaze With Gaze
Transcription 0.619 0.676
WCN 0.524 0.552
1-best 0.471 0.514
Table 4: Lenient F-measure Evaluation
Configuration Without Gaze With Gaze
Transcription 0.584 0.627
WCN 0.309 0.333
1-best 0.039 0.035
Table 5: Strict F-measure Evaluation
Table 5 shows the strict reference resolution eval-
uation using F-measure. As can be seen in the ta-
ble, incorporating eye gaze information significantly
(p < 0.0024) improves reference resolution per-
formance when using transcription and marginally
(p < 0.113) in the case of WCN optimized for strict
evaluation. However there is no difference for the 1-
best hypotheses which result in extremely low per-
formance. This observation is not surprising since 1-
best hypotheses are quite error prone and less likely
to produce the exact expressions.
477
Since eye gaze can be used to direct navigation in
a mobile environment as in situated dialogue, there
could be situations where eye gaze does not reflect
the content of the corresponding speech. In such
situations, integrating eye gaze in reference reso-
lution could be detrimental. To further understand
the role of eye gaze in reference resolution, we ap-
plied our reference resolution algorithm only to ut-
terances where speech and eye gaze are considered
closely coupled (i.e., eye gaze reflects the content of
speech). More specifically, following the previous
work (Qu and Chai, 2010), we define a closely cou-
pled utterance as one in which at least one noun or
adjective describes an object that has been fixated by
the corresponding gaze stream.
Table 6 and Table 7 show the performance based
on closely coupled utterances using lenient and strict
evaluation, respectively. In the lenient evaluation,
reference resolution performance is significantly im-
proved for all input configurations when eye gaze
information is incorporated (p < 0.0001 for tran-
scription, p < 0.015 for WCN, and p < 0.0022 for
1-best). In each case the closely coupled utterances
achieve higher performance than the entire set of ut-
terances evaluated in Table 5. Aside from the 1-best
case, the same is true when using strict evaluation
(p < 0.0006 for transcription and p < 0.046 for
WCN optimized for strict evaluation). This observa-
tion indicates that in situated dialogue, some mech-
anism to predict whether a gaze stream is closely
coupled with the corresponding speech content can
be beneficial in further improving reference resolu-
tion performance.
Configuration Without Gaze With Gaze
Transcription 0.616 0.700
WCN 0.523 0.570
1-best 0.473 0.537
Table 6: Lenient F-measure Evaluation for Closely Cou-
pled Utterances
6.3 Role of Word Confusion Network
The effect of incorporating eye gaze with WCNs
rather than 1-best recognition hypotheses into ref-
erence resolution can also be seen in Tables 4 and
5. Table 4 shows a significant improvement when
using WCNs rather than 1-best hypotheses for both
Configuration Without Gaze With Gaze
Transcription 0.579 0.644
WCN 0.307 0.345
1-best 0.045 0.038
Table 7: Strict F-measure Evaluation for Closely Coupled
Utterances
with (p < 0.015) and without (p < 0.0012) eye
gaze configurations. Similarly, Table 5 shows a sig-
nificant improvement in strict evaluation when us-
ing WCNs rather than 1-best hypotheses for both
with (p < 0.0001) and without (p < 0.0001) eye
gaze configurations. These results indicate that us-
ing word confusion networks improves both lenient
and strict reference resolution. This observation is
not surprising since identifying correct linguistic ex-
pressions will enable better search for semantically
matching referent objects.
Although WCNs lead to better performance, uti-
lizing WCNs is more computationally expensive
compared to 1-best recognition hypotheses. Never-
theless, in practice, WCN depth, which specifies the
maximum number of competing word hypotheses in
any position of the word confusion network, can be
limited to a certain value |d|. For example, in Figure
2 the depth of the shown WCN is 8 (there are 8 com-
peting word hypotheses in position 17 of the WCN).
The WCN depth can be limited by pruning word al-
ternatives with low probabilities until, at most, the
top |d| words remain in each position of the WCN.
It is interesting to observe how limiting WCN depth
can affect reference resolution performance. Figure
3 demonstrates this observation. In this figure the
resolution performance (in terms of lenient evalua-
tion) for WCNs of varying depth is shown as dashed
lines for with and without eye gaze configurations.
As a reference point, the performance when utiliz-
ing 1-best recognition hypotheses is shown as solid
lines. It can be seen that as the depth increases, the
performance also increases until the depth reaches 8.
After that, there is no performance improvement.
7 Discussion
In Section 6.2 we have shown that incorporating
eye gaze information improves reference resolu-
tion performance. Eye-gaze information is particu-
478
Figure 3: Lenient F-measure at each WCN Depth
larly helpful for resolving referring expressions that
are ambiguous from the perspective of the artificial
agent. Consider a scenario where the user utters a
referring expression that has an equivalent seman-
tic compatibility with multiple potential referent ob-
jects. For example, in a room with multiple books,
the user utters ?the open book to the right?, but only
the phrase ?the book? is recognized by the ASR. If
a particular book is fixated during interaction, there
is a high probability that it is indeed being referred
to by the user. Without eye gaze information, the se-
mantic compatibility alone could be insufficient to
resolve this referring expression. Thus, when eye
gaze information is incorporated, the main source of
performance improvement comes from better iden-
tification of potential referent objects.
In Section 6.3 we have shown that incorporating
multiple speech recognition hypotheses in the form
of a word confusion network further improves ref-
erence resolution performance. This is especially
true when exact referring expression identification
is required (F-measure of 0.309 from WCNs com-
pared to F-measure of 0.039 from 1-best hypothe-
ses). Using a WCN improves identification of low-
probability referring expressions. Consider a sce-
nario where the top recognition hypothesis of an ut-
terance contains no referring expressions or an in-
correct referring expression that has no semantically
compatible potential referent objects. If a referring
expression with a high compatibility value to some
potential referent object is present in a lower proba-
bility hypothesis, this referring expression can only
be identified when a WCN rather than a 1-best hy-
pothesis is utilized. Thus, when word confusion net-
works are incorporated, the main source of perfor-
mance improvement comes from better referring ex-
pression identification.
7.1 Computational Complexity
One potential concern of using word confusion net-
works rather than 1-best hypotheses is that they are
more computationally expensive to process. The
asymptotic computational complexity for resolving
the referring expressions using the algorithm pre-
sented in this work with a WCN is the summa-
tion of three components: (1) O(|G| ? |d|2 ? |w|3)
for confusion network construction and parsing, (2)
O(|r|?|O|?log(|O|)) for reference resolution, and (3)
O(|r|2) for selection of (referring expression, ref-
erent object set) pairs. Here, |w| is the number of
words in the input speech signal (or, more precisely,
the number of words in the longest ASR hypothesis
for a given utterance); |G| is the size of the parsing
grammar; |d| is the depth of the constructed word
confusion network; |O| is the number of potential
referent objects for each utterance; and |r| is the
number of referring expressions that are extracted
from the word confusion network.
The complexity is dominated by the word confu-
sion network construction and parsing. Also, both
the number of words in an input utterance ASR hy-
pothesis |w| and the number of referring expressions
in a word confusion network |r| are dependent on ut-
terance length. In our study, interactive dialogue is
encouraged and, thus, utterances are typically short;
with a mean length of 6.41 words and standard de-
viation of 4.35 words. The longest utterances in our
data set has 31 words. WCN depth |d| has a mean of
10.1, a standard deviation of 8.1, and a maximum 89
words. In practice, as shown in Section 6.3, limiting
|d| to 8 words achieves comparable reference resolu-
tion results as using a full word confusion network.
To demonstrate the applicability of our reference
resolution algorithm for real-time processing, we ap-
plied it on the data corpus presented in Section 3.
This corpus contains utterances with a mean input
time of 2927.5 ms and standard deviation of 1903.8
ms. On a 2.4 GHz AMD Athlon(tm) 64 X2 Dual
Core Processor, the runtimes resulted in a real time
factor of 0.0153 on average. Thus, on average, an
utterance from this corpus can be processed in just
under 45 ms, which is well within the range of ac-
479
ceptable real-time performance.
7.2 Error Analysis
As can be seen in Section 6, even when using tran-
scribed data, reference resolution performance still
has room for improvement (achieving the highest le-
nient F-measure of 0.700 when eye gaze is utilized
for resolving closely coupled utterances). In this
section, we elaborate on the potential error sources.
Specifically, we discuss two types of error: (1) a re-
ferring expression is incorrectly recognized or (2) a
recognized referring expression is not resolved to a
correct referent object set.
Given transcribed data, which simulates per-
fectly recognized utterances, all referring expression
recognition errors arise due to incorrect language
processing. Most of these errors occur because an
incorrect part of speech (POS) tag is assigned to a
word, or an out-of-vocabulary (OOV) word is en-
countered, or the parsing grammar has insufficient
coverage. A particularly interesting parsing prob-
lem occurs due to the nature of spoken language.
Since punctuation is sometimes unavailable, given
an utterance with several consecutive nouns, it is un-
clear which of these nouns should be treated as head
nouns and which should be treated as noun modi-
fiers. For example, in the utterance ?there is a desk
lamp table and two chairs? it is unclear if the itali-
cized expression should be parsed as a single phrase
or as a list of (two) phrases a desk and lamp.
Thus, some timing information should be used for
disambiguation.
Object set identification errors are more prevalent
than referring expression recognition errors. The
majority of these errors occur because a referring
expression is ambiguous from the perspective of the
conversational system and there is not enough in-
formation to choose amongst multiple potential ref-
erent objects due to limited speech recognition and
domain modeling. One reason for this is that a re-
ferring expression may be resolved to an incorrect
number of referent objects. Another reason is that a
pertinent object attribute or a distinguishing spatial
relationship between objects specified by the user
cannot be established by the system. For example,
during the utterance ?I see a vase left of the table?
there are two vases visible on the screen creating an
ambiguity if the phrase left of is not processed
correctly. This is caused by an inadequate repre-
sentation of spatial relationships and processing of
spatial language. One more reason for potential am-
biguity is the lack of pragmatic knowledge that can
support adequate inference. For example, when the
user refers to two sofa objects using the phrase ?an
armchair and a sofa?, the system lacks pragmatic
knowledge to indicate that arm chair refers to
the smaller of the two objects. Some of these errors
can be avoided when eye gaze information is avail-
able to the system. However, due to the noisy nature
of eye gaze data, many such referring expressions
remain ambiguous even when eye gaze information
is considered.
8 Conclusion
In this work, we have examined the utility of eye
gaze and word confusion networks for reference res-
olution in situated dialogue within a virtual world.
Our empirical results indicate that incorporating
eye gaze information with recognition hypotheses
is beneficial for the reference resolution task com-
pared to only using recognition hypotheses. Further-
more, using a word confusion network rather than
the top best recognition hypothesis further improves
reference resolution performance. Our findings also
demonstrate that the processing speed necessary to
integrate word confusion networks with eye gaze
information is well within the acceptable range for
real-time applications.
Acknowledgments
This work was supported by IIS-0347548 and IIS-
0535112 from the National Science Foundation. We
would like to thank anonymous reviewers for their
valuable comments and suggestions.
References
N. Bee, E. Andre?, and S. Tober. 2009. Breaking the
ice in human-agent communication: Eye-gaze based
initiation of contact with an embodied conversational
agent. In Proceedings of the 9th International Con-
ference on Intelligent Virtual Agents (IVA?09), pages
229?242. Springer.
T. Bickmore and J. Cassell, 2004. Social Dialogue with
Embodied Conversational Agents, chapter Natural, In-
480
telligent and Effective Interaction with Multimodal Di-
alogue Systems. Kluwer Academic.
D. K. Byron, T. Mampilly, and T. Sharma, V.and Xu.
2005. Utilizing visual attention for cross-modal coref-
erence interpretation. In Spring Lecture Notes in Com-
puter Science: Proceedings of CONTEXT-05, pages
83?96.
N. J. Cooke and M. Russell. 2008. Gaze-contingent au-
tomatic speech recognition. IET Signal Processing,
2(4):369?380, December.
J. Cooke and J. T. Schwartz. 1970. Programming lan-
guages and their compilers: Preliminary notes. Tech-
nical report, Courant Institute of Mathematical Sci-
ence.
R. Fang, J. Y. Chai, and F. Ferreira. 2009. Between lin-
guistic attention and gaze fixations in multimodal con-
versational interfaces. In The 11th International Con-
ference on Multimodal Interfaces (ICMI).
P. Gorniak, J. Orkin, and D. Roy. 2006. Speech, space
and purpose: Situated language understanding in com-
puter games. In Twenty-eighth Annual Meeting of
the Cognitive Science Society Workshop on Computer
Games.
Z. M. Griffin and K. Bock. 2000. What the eyes say
about speaking. In Psychological Science, volume 11,
pages 274?279.
D. Hakkani-Tu?r, F. Be?chet, G. Riccardi, and G. Tur.
2006. Beyond asr 1-best: Using word confusion net-
works in spoken language understanding. Computer
Speech and Language, 20(4):495?514.
M. A. Just and P. A. Carpenter. 1976. Eye fixations and
cognitive processes. In Cognitive Psychology, vol-
ume 8, pages 441?480.
T. Kasami. 1965. An efficient recognition and syntax-
analysis algorithm for context-free languages. Scien-
tific report AFCRL-65-758, Air Force Cambridge Re-
search Laboratory, Bedford, Massachusetts.
J. Kelleher and J. van Genabith. 2004. Visual salience
and reference resolution in simulated 3-d environ-
ments. Artificial Intelligence Review, 21(3).
Y. Liu, J. Y. Chai, and R. Jin. 2007. Automated vo-
cabulary acquisition and interpretation in multimodal
conversational systems. In Proceedings of the 45th
Annual Meeting of the Association of Computational
Linguistics (ACL).
L. Mangu, E. Brill, and A. Stolcke. 2000. Finding con-
sensus in speech recognition: word error minimization
and other applications of confusion networks. Com-
puter Speech and Language, 14(4):373?400.
A. S. Meyer and W. J. M. Levelt. 1998. Viewing and
naming objects: Eye movements during noun phrase
production. In Cognition, volume 66, pages B25?B33.
L.-P. Morency, C. M. Christoudias, and T. Darrell. 2006.
Recognizing gaze aversion gestures in embodied con-
versational discourse. In International Conference on
Multimodal Interfaces (ICMI).
Y. I. Nakano, G. Reinstein, T. Stocky, and J. Cassell.
2003. Towards a model of face-to-face grounding. In
Proceedings of the 41st Annual Meeting of the Associ-
ation for Computational Linguistics (ACL?03), pages
553?561.
Z. Prasov and J. Y. Chai. 2008. What?s in a gaze? the
role of eye-gaze in reference resolution in multimodal
conversational interfaces. In Proceedings of 13th In-
ternational Conference on Intelligent User interfaces
(IUI), pages 20?29.
S. Qu and J. Y. Chai. 2007. An exploration of eye gaze in
spoken language processing for multimodal conversa-
tional interfaces. In Proceedings of the Conference of
the North America Chapter of the Association of Com-
putational Linguistics (NAACL).
S. Qu and J. Y. Chai. 2010. Context-based word acquisi-
tion for situated dialogue in a virtual world. Journal of
Artificial Intelligence Research, 37:347?377, March.
P. Qvarfordt and S. Zhai. 2005. Conversing with the
user based on eye-gaze patterns. In Proceedings Of the
Conference on Human Factors in Computing Systems.
ACM.
C. L. Sidner, C. D. Kidd, C. Lee, and N. Lesh. 2004.
Where to look: A study of human-robot engagement.
In Proceedings of the 9th international conference
on Intelligent User Interfaces (IUI?04), pages 78?84.
ACM Press.
A. Stolcke. 2002. SRILM an extensible language model-
ing toolkit, confusion network. In International Con-
ference on Spoken Language Processing.
M. K. Tanenhous, M. Spivey-Knowlton, E. Eberhard, and
J. Sedivy. 1995. Integration of visual and linguistic
information during spoken language comprehension.
In Science, volume 268, pages 1632?1634.
D. H. Younger. 1967. Recognition and parsing of
context-free languages in time n3. Information and
Control, 10(2):189?208.
481
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 756?766,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Towards Conversation Entailment: An Empirical Investigation
Chen Zhang Joyce Y. Chai
Department of Computer Science and Engineering
Michigan State University
East Lansing, MI 48824, USA
{zhangch6, jchai}@cse.msu.edu
Abstract
While a significant amount of research has
been devoted to textual entailment, automated
entailment from conversational scripts has re-
ceived less attention. To address this limi-
tation, this paper investigates the problem of
conversation entailment: automated inference
of hypotheses from conversation scripts. We
examine two levels of semantic representa-
tions: a basic representation based on syntac-
tic parsing from conversation utterances and
an augmented representation taking into con-
sideration of conversation structures. For each
of these levels, we further explore two ways of
capturing long distance relations between lan-
guage constituents: implicit modeling based
on the length of distance and explicit mod-
eling based on actual patterns of relations.
Our empirical findings have shown that the
augmented representation with conversation
structures is important, which achieves the
best performance when combined with ex-
plicit modeling of long distance relations.
1 Introduction
Textual entailment has received increasing attention
in recent years (Dagan et al, 2005; Bar-Haim et al,
2006; Giampiccolo et al, 2007; Giampiccolo et al,
2008; Bentivogli et al, 2009). Given a segment from
a textual document, the task of textual entailment is
to automatically determine whether a given hypoth-
esis can be entailed from the segment. The capa-
bility of such kind of inference can benefit many
text-based applications such as information extrac-
tion and question answering.
Textual entailment has mainly focused on infer-
ence from written text in monologue. Recent years
also observed an increasing amount of conversa-
tional data such as conversation scripts of meetings,
call center records, court proceedings, as well as on-
line chatting. Although conversation is a form of
language, it is different from monologue text with
several unique characteristics. The key distinctive
features include turn-taking between participants,
grounding between participants, different linguistic
phenomena of utterances, and conversation impli-
catures. Traditional approaches dealing with tex-
tual entailment were not designed to handle these
unique conversation behaviors and thus to support
automated entailment from conversation scripts.
Example 1:
Conversation Segment:
B: My mother also was very very independent.
She had her own, still had her own little house
and still driving her own car,
A: Yeah.
B: at age eighty-three.
Hypothesis:
(1) B?s mother is eighty-three.
(2) B is eighty-three.
To address this limitation, our previous
work (Zhang and Chai, 2009) has initiated an
investigation on the problem of conversation en-
tailment. The problem was formulated as follows:
given a conversation discourse D and a hypothesis
H concerning its participant, the goal was to identify
whether D entails H. For instance, as in Example
1, the first hypothesis can be entailed from the
756
conversation segment while the second hypothesis
cannot. While our previous work has provided
some interesting preliminary observations, it mostly
focused on data collection and initial experiments
and analysis using a small set of development data.
It is not clear whether the previous results are
generally applicable, how different components in
the entailment framework interact with each other,
and how different representations may influence the
entailment outcome.
To reach a better understanding of conversation
entailment, we conducted a further investigation
based on the larger set of test data collected in our
previous work (Zhang and Chai, 2009). We specifi-
cally examined two levels of representations: a basic
representation based on syntactic parsing from con-
versation utterances and an augmented representa-
tion taking into consideration of conversation struc-
tures. For each of these levels, we further explored
two ways of capturing long distance relations: (1)
implicit modeling based on the length of distance
and (2) explicit modeling based on actual patterns
of relations. Our empirical findings have shown that
augmented representation with conversation struc-
tures is important in conversation entailment. Com-
bining conversation structures with explicit model-
ing of long distance relations results in the best per-
formance.
2 Related Work
Our work here is related to recent advances in tex-
tual entailment, automated processing of conversa-
tion scripts, and our initial investigation on conver-
sation entailment.
There is a large body of work on textual en-
tailment initiated by the Pascal Recognizing Tex-
tual Entailment (RTE) Challenges (Dagan et al,
2005; Bar-Haim et al, 2006; Giampiccolo et al,
2007; Giampiccolo et al, 2008; Bentivogli et al,
2009). Different approaches have been developed,
for example, based on logic proving (Tatu and
Moldovan, 2005; Bos and Markert, 2005; Raina et
al., 2005) and graph match (Haghighi et al, 2005;
de Salvo Braz et al, 2005; MacCartney et al, 2006).
Supervised learning approaches have also been ap-
plied to measure the similarities between training
and testing pairs (Zanzotto and Moschitti, 2006). In
the most recent RTE Challenge (Bentivogli et al,
2009), the best system achieves 73.5% of accuracy,
while the median performance among all partici-
pants is 60.4%. These results indicate that, while
progress has been made, textual entailment remains
a challenging problem.
As more and more conversation data becomes
available, researchers have investigated automated
processing of conversation data to acquire useful
information, for example, related to opinions (So-
masundaran et al, 2007; Somasundaran et al,
2008; Somasundaran et al, 2009), biographic at-
tributes (Garera and Yarowsky, 2009), social net-
works (Jing et al, 2007), and agreements and
disagreements between participants (Galley et al,
2004). Recent studies have also developed ap-
proaches to summarize conversations (Murray and
Carenini, 2008) and to model conversation struc-
tures (dialogue acts) from online Twitter conversa-
tions (Ritter et al, 2010). Here we address a dif-
ferent angle regarding conversation scripts, namely
conversation entailment.
In our previous work (Zhang and Chai, 2009),
we started an initial investigation on conversation
entailment. We have collected a dataset of 875
instances. Each instance consists of a conversa-
tion segment and a hypothesis (as described in Sec-
tion 1). The hypotheses are statements about conver-
sation participants and are further categorized into
four types: about their profile information, their be-
liefs and opinions, their desires, and their commu-
nicative intentions. We developed an approach that
is motivated by previous work on textual entailment.
We use clauses in the logic-based approaches as the
underlying representation of our system. Based on
this representation, we apply a two stage entailment
process similar to MacCartney et al (2006) devel-
oped for textual entailment: an alignment stage fol-
lowed by an entailment stage.
Building upon our previous work, in this paper,
we systematically examine different representations
of the conversation segment and different modeling
of long distance relations between language con-
stituents. We compare the roles of these different
representations on the performance of entailment
prediction using a larger testing dataset that was not
previously evaluated. This analysis allows better un-
derstanding of the problem and provides insight on
757
potential solutions.
3 Overall Framework
In our previous work (Zhang and Chai, 2009), con-
versation entailment is formulated as the follow-
ing: given a conversation segment D which is rep-
resented by a set of clauses D = d1 ? . . . ? dm,
and a hypothesis H represented by another set of
clauses H = h1 ? . . . ? hn, the prediction on
whether D entails H is determined by the product
of probabilities that each hypothesis clause hj is
entailed from all the conversation segment clauses
d1 . . . dm as follows. This is based on a simple as-
sumption that whether a clause is entailed from a
conversation segment is conditionally independent
from other clauses.
P (D  H|D,H)
= P (D  h1, . . . , D  hn|D,h1, . . . , hn)
=
n?
j=1
P (D  hj |D = d1 . . . dm, hj)
=
n?
j=1
P (d1 . . . dm  hj |d1, . . . , dm, hj) (1)
A clause here is similar to a sentence in first-
order predicate calculus. It is made up by terms
and predicates. A term is either: 1) an entity
described by a noun phrase, e.g., John Lennon,
mother, or she; or 2) an action or event de-
scribed by a verb phrase, e.g., marry in ?John
married Eva in 1940?. A predicate represents
either: 1) a property (i.e., unary) for a term,
e.g., Russian(company), or recently(visit);
or 2) a relation (i.e., binary) between two
terms, e.g., subj(visit, Prime Minister) and
obj(visit, Brazil) in ?Prime Minister recently vis-
ited Brazil?.
Given the clause representation, we follow the
idea similar to MacCartney et al (2006), and predict
the entailment decision in two stages of processing:
(1) an alignment model aligns terms in the hypothe-
sis to terms in the conversation segment; and (2) an
inference model predicts the entailment based on the
alignment between the hypothesis and the conversa-
tion segment.
3.1 Alignment Model
An alignment is defined as a mapping function g
between a term x in the conversation segment and a
term y in the hypothesis. g(x, y) = 1 if x and y are
aligned; otherwise g(x, y) = 0. It is possible that
multiple terms from the segment are mapped to one
term in the hypothesis (g(x1, y) = g(x2, y) = 1),
or vice versa (g(x, y1) = g(x, y2) = 1). To predict
these alignments, the problem is formulated as bi-
nary classification: given any two terms x from the
conversation and y from the hypothesis, decide the
value of their alignment function g(x, y).
3.2 Inference Model
Once an alignment between a hypothesis and a con-
versation segment is established, an inference model
is applied to predict whether the conversation seg-
ment entails the hypothesis given such alignment.
More specifically, as shown in Equation 1, given a
clause from the hypothesis hj , a set of clauses from
the conversation segment d1, . . . , dm, and an align-
ment g between them, the goal is to predict whether
d1, . . . , dm entails hj under the alignment g.
The prediction is treated differently according to
different types of clauses. If hj is a property clause
(i.e., takes one argument hj(?)), a property inference
model is applied; otherwise (i.e., relational clauses
with two arguments hj(?, ?)), a relational inference
model is applied.
In this paper we follow the same framework.
However our focus here is on the new question that
how different levels of semantic representation and
different approaches of modeling long distance rela-
tionship affect the alignment and inference models
as well as the overall entailment performance.
4 Semantic Representation
Given the clause representation described earlier,
an important question is what information from the
conversation segment should be captured and repre-
sented. To address this question, we examined two
levels of shallow semantic representation. The first
level is basic representation which only captures the
information from all the utterances in the conversa-
tion segment. The second representation includes
conversation structures (e.g., speakers and dialogue
758
acts). Next we use Example 2 to illustrate these rep-
resentations.
Example 2:
Conversation Segment:
B: Have you seen Sleeping with the Enemy?
A: No. I?ve heard that?s really great, though.
B: You have to go see that one.
Hypothesis:
B suggests A to watch Sleeping with the Enemy.
4.1 Basic Representation
The first representation is based on the syntactic
parsing from conversation utterances and we call it
a basic representation. Figure 1(a) shows an exam-
ple of dependency structures for several utterances
that are derived from the Stanford parser (Klein and
Manning, 2003), and Figure 1(b) shows the corre-
sponding clause representation. In the dependency
structure, the vertices represent entities (e.g., x1) and
actions (e.g., x3) within an utterance. They corre-
spond to terms in the clause representation. An edge
between vertices captures a dependency relation and
is represented as predicates in the clause representa-
tion. For example, the edge between x1 and x3 indi-
cates x1 is the subject of x3, which is represented by
the clause representation subj(x3, x1). Similar rep-
resentation also applies to the hypothesis as shown
in Figure 1(c), 1(d).
4.2 Augmented Representation
The second representation is built upon the basic
representation and incorporates conversation struc-
ture across turns and utterances. We call it an aug-
mented representation. Figure 2(a) shows the aug-
mented structures of the conversation segment and
Figure 2(b) shows the corresponding clause repre-
sentation. Compared to the basic representation,
there are two additional types of vertices (i.e., terms)
highlighted in the figures:
? Vertices representing utterances (e.g.,
u1 . . . u4). Their corresponding terms capture
the dialogue acts for the utterances (e.g.
u1 = yes no question). To focus our effort,
currently we only apply annotated dialogue
acts provided in the Switchboard corpus (God-
frey and Holliman, 1997). Two edges are
added to connect different utterances. The
first edge connects each utterance vertex to
the head of the corresponding utterance to
indicate the specific content of the utterance
(e.g., content(u1, x3)). The second edge con-
nects an utterance to its succeeding utterance
to indicate the temporal progression of the
conversation (e.g., follow(u2, u1)).
? Vertices representing speakers or participants
(e.g., sA, sB). One edge is added to
connect each utterance to its speaker (e.g.,
speaker(u1, sB)).
Note that since our clause representations are
mainly based on the dependency relations, they are
mostly syntactic-driven. However, it does capture
some shallow semantics such as who is the agent
(i.e., subject) or the patient (i.e., object) of an event.
The incorporation of speakers and dialogue acts in
our augmented representations provides additional
semantics of conversation discourse.
5 Modeling LDR
A critical part in predicting entailment is to recog-
nize the semantic relationship between two language
constituents, especially when these two constituents
are not directly related. In Figure 2(a), for exam-
ple, we want to recognize that x9 (You) is the (log-
ical) subject of x11 (see). Here we experimented
two ways of modeling such long distance relations
(LDR).
5.1 Implicit Modeling of LDR
The first method characterizes the relationship sim-
ply by the distance between two constituents in the
dependency structure (or augmented structure). For
example, in Figure 2(a) the distance between x11
and x9 is 3. We call this method an implicit mod-
eling of long distance relationship.
The advantage of implicit modeling is that it is
easy to implement based on the dependency struc-
ture. However, its limitation is that the distance mea-
sure does not capture sufficient information of se-
mantic relations between language constituents.
5.2 Explicit Modeling of LDR
The second way of modeling long distance relation-
ship is called explicit modeling. It uses a string to
759
B: 
Ha
ve
you
see
nS
lee
pin
g w
ith
 th
e E
nem
y?
A: 
No
. I'
ve 
hea
rd
tha
t's 
rea
lly
 gr
eat
, th
oug
h.
B: 
Yo
u h
ave
to 
go
see
tha
t o
ne.
x 9
x 13
x 12
x 11
x 10
x 4
x 1
x 3
x 2
x 5
x 8
x 6
x 7
obj
(x 3
,x 2
)
sub
j(x
3,x
1)
aux
(x 3
,x 4
)
x 1=
A
x 2=
Sle
epi
ng
wit
h t
he 
En
em
y
x 3=
see
n, x
4=h
ave
obj
(x 1
1,x
10)
obj
(x 1
2,x
11)
obj
(x 1
3,x
12)
sub
j(x
13,x
9)
x 9=
A, 
x 10
=o
ne,
x 11
=se
e, x
12=
go,
 
x 13
=h
ave
sub
j(x
7,x
6)
obj
(x 8
,x 7
)
sub
j(x
8,x
5)
x 5=
A, 
x 6=
tha
t
x 7=
is r
eal
ly g
rea
t
x 8=
hav
e h
ear
d
Cla
us
es
Te
rm
s
(a) dependency structure of the conversation
utterances
B:
 H
av
ey
ou
see
nS
lee
pin
g w
ith
 th
e E
ne
my
?
A:
 N
o. 
I'v
e h
ear
dt
ha
t's 
rea
lly
 gr
eat
, th
ou
gh
.
B:
 Y
ou
ha
ve
to 
go
see
tha
t o
ne
.
x 9
x 13
x 12
x 11
x 10
x 4
x 1
x 3
x 2
x 5
x 8
x 6
x 7
ob
j(x
3,x
2)
sub
j(x
3,x
1)
au
x(x
3,x
4)
x 1=
A
x 2=
Sle
ep
ing
wi
th 
the
 En
em
y
x 3=
see
n, 
x 4=
ha
ve
ob
j(x
11
,x 1
0)
ob
j(x
12
,x 1
1)
ob
j(x
13
,x 1
2)
sub
j(x
13
,x 9
)
x 9=
A, 
x 10
=o
ne
,
x 11
=s
ee,
 x 1
2=g
o, 
x 13
=h
av
e
sub
j(x
7,x
6)
ob
j(x
8,x
7)
sub
j(x
8,x
5)
x 5=
A, 
x 6=
tha
t
x 7=
is r
ea
lly
 gr
ea
t
x 8=
ha
ve 
he
ard
Cla
us
es
Te
rm
s
(b) basic representation of the conver-
sation segment
x 1
x 2
x 5
x 4
x 3
Bs
ugg
est
sA
to 
wa
tch
Sle
epi
ng 
wit
h t
he 
En
em
y.
sub
j
sub
j
obj
obj
B: 
Ha
ve
you
see
nS
lee
pin
g w
ith
 th
e E
nem
y?
A: 
No
. I'v
e h
ear
dt
hat
's r
eal
ly 
gre
at, 
tho
ugh
.
B: 
Yo
u h
ave
to 
go
see
tha
t o
ne.
x 9
x 13
x 12
x 11
x 10
x 4
x 1
x 3
x 2
x 5
x 8
x 6
x 7
u 1 u 2 u 3 u 4
s B s A
(c) dependency structure of the hypothesis
ob
j(x
3,x
2),
 su
bj(
x 3,
x 1)
au
x(x
3,x
4)
x 1=
A, 
x 3=
see
n, 
x 4=
ha
ve
x 2=
Sle
ep
ing
 wi
th 
the
 En
em
y
ob
j(x
11
,x 1
0),
 ob
j(x
12
,x 1
1)
ob
j(x
13
,x 1
2),
 su
bj(
x 13
,x 9
)
x 9=
A, 
x 10
=o
ne
, x
11
=s
ee
x 12
=g
o, 
x 13
=h
av
e
speaker
(u 4
,s B
)
content
(u 4
,x 1
3)
follow
(u 4
,u 3
)
u 4=
viewpoint
sub
j(x
7,x
6),
 ob
j(x
8,x
7)
sub
j(x
8,x
5)
x 5=
A, 
x 7=
is r
ea
lly
 gr
ea
t
x 6=
tha
t, x
8=h
av
e h
ea
rd
speaker
(u 3
,s A
)
content
(u 3
.x 8
)
follow
(u 3
,u 2
)
u 3=
statement
speaker
(u 2
,s A
)
follow
(u 2
,u 1
)
u 2=
no_answer
speaker
(u 1
,s B
)
content
(u 1
, x
3)
s A,
s B
u 1=
yes_no_qu
estion
Cla
us
es
Te
rm
s
sub
j(x
4,x
2)
ob
j(x
4,x
3)
sub
j(x
5,x
1)
ob
j(x
5,x
4)
x 1=
B, 
x 2=
A
x 3=
Sle
ep
ing
wi
th 
the
 En
em
y
x 4=
wa
tch
x 5=
sug
ge
sts
Cla
us
es
Te
rm
s
(d) representation of the hy-
pothesis
Figure 1: The dependency structures and corresponding basic representation of Example 2x 1
x 2
x 5
x 4
x 3
Bs
ug
ge
sts
A
to 
wa
tch
Sle
ep
ing
 w
ith
 th
e E
ne
my
.
sub
j
sub
j
ob
j
ob
j
B:
 H
av
ey
ou
see
nS
lee
pin
g w
ith
 th
e E
ne
my
?
A:
 N
o. 
I'v
e h
ear
dt
ha
t's 
rea
lly
 gr
eat
, th
ou
gh
.
B:
 Y
ou
ha
ve
to 
go
see
tha
t o
ne
.
x 9
x 13
x 12
x 11
x 10
x 4
x 1
x 3
x 2
x 5
x 8
x 6
x 7
u 1 u 2 u 3 u 4
s B s A
(a) dependency and conversation structures of the conversation
segment
obj
(x 3
,x 2
), s
ubj
(x 3
,x 1
)
aux
(x 3
,x 4
)
x 1=
A, 
x 3=
see
n, x
4=h
ave
x 2=
Sle
epi
ng 
wit
h t
he 
En
em
y
obj
(x 1
1,x
10)
, ob
j(x
12,x
11)
obj
(x 1
3,x
12)
, su
bj(
x 13
,x 9
)
x 9=
A, 
x 10
=o
ne,
 x 1
1=s
ee
x 12
=g
o, x
13=
hav
e
speaker
(u 4
,s B
)
content
(u 4
,x 1
3)
follow
(u 4
,u 3
)
u 4=
viewpoint
sub
j(x
7,x
6), 
obj
(x 8
,x 7
)
sub
j(x
8,x
5)
x 5=
A, 
x 7=
is r
eal
ly g
rea
t
x 6=
tha
t, x
8=h
ave
 he
ard
speaker
(u 3
,s A
)
content
(u 3
.x 8
)
follow
(u 3
,u 2
)
u 3=
statement
speaker
(u 2
,s A
)
follow
(u 2
,u 1
)
u 2=
no_answer
speaker
(u 1
,s B
)
content
(u 1
, x 3
)
s A,
s B
u 1=
yes_no_ques
tion
Cla
us
es
Te
rm
s
sub
j(x
4,x
2)
obj
(x 4
,x 3
)
sub
j(x
5,x
1)
obj
(x 5
,x 4
)
x 1=
B, 
x 2=
A
x 3=
Sle
epi
ng
wit
h t
he 
En
em
y
x 4=
wa
tch
x 5=
sug
ges
ts
Cla
us
es
Te
rm
s
(b) augmented representation of the conversation seg-
ment
Figure 2: The dependency and conversation structures and corresponding augmented representation of Example 2
760
describe the path from one constituent to the other:
v1e1 . . . vl?1el?1vl, where v1, . . . , vl are the vertices
on the path and e1, . . . , el?1 are the edges. Each vi
describes the type of the vertex in the dependency
structure, which is either a noun (N ), a verb (V ),
or an utterance (U ). Each ei describes whether the
edge is forward (?) or backward (?). For ex-
ample, in Figure 2(a), the path from x11 to x9 is
V ? V ? V ? N .
This kind of string representation of paths in syn-
tactic parse is known as a way of modeling ?shal-
low semantics? between any two constituents in a
language structure. It is largely used in other NLP
tasks such as semantic role labeling (Pradhan et al,
2008). The difference here is our paths are extracted
from dependency parses as opposed to traditional
constituent parses, and our paths also incorporate the
representation of conversation structures (e.g., utter-
ances and speakers).
6 Applications in Entailment Models
In this section we describe how different representa-
tions and modeling of LDR are used in the alignment
and inference models.
6.1 Applications in Alignment Model
Although a noun and a verb can potentially be
aligned, to simplify the problem, we restrict the
problem to the alignment between two nouns or two
verbs. We trained an alignment model for nouns and
one for verbs separately.
Table 1 summarizes a set of features used in the
alignment models. Most of these features are shared
by the model for noun alignment and the model for
verb alignment. These features include whether the
two strings are the same, two terms have the same
stem, the similarity between the two terms either
based on WordNet or distributional statistics (Lin,
1998).
To learn the alignment model for nouns, we anno-
tated the noun alignments for the development data
used in PASCAL RTE-3 Challenge (Giampiccolo et
al., 2007) and trained a logistic regression model
based on the features in Table 1. Cross-validation
on the same dataset shows relatively satisfying per-
formance (96.4% precision and 94.9% recall). In
this paper, we focus on the alignment between verbs
Noun Verb
Align. Align.
Verb be identification X
String equality X X
Stemmed equality X X
Acronym equality X
Named entity equality X
WordNet similarity X X
Distributional similarity X X
Subject consistency X
Object consistency X
Table 1: Features for alignment models
since it appears more difficult.
A major difference between noun alignment and
verb alignment is that, for verb alignment the con-
sistency of their arguments is also important. For
two events (described by two verbs) to be aligned, at
least their subjects (usually denoting the executers of
actions) and objects (usually denoting the receivers
of actions) should match to each other respectively.
Note that, although actions/events also depend on
other arguments or adjuncts, here we only consider
the subjects and objects and leave the consistency
check of other arguments/adjuncts to downstream
processes. Based on two different ways of model-
ing long distance relationship (as described in Sec-
tion 5), we explored two methods for modeling ar-
gument consistency (AC) in verb alignment models.
6.1.1 Implicit Modeling of AC
The first approach models argument consistency
based on implicit modeling of the relationship be-
tween a verb and its aligned subject/object. Specif-
ically, given a pair of verb terms (x, y) where x is
from the conversation segment and y is from the hy-
pothesis, let sy be the subject of y and sx be the
aligned entity of sy in the conversation (in case of
multiple alignments, sx is the one closest to x). The
subject consistency of the verbs (x, y) is then mea-
sured by the distance between sx and x in the de-
pendency structure. Similarly, the distance between
a verb and its aligned object is used as a measure of
the object consistency.
In Example 2, to decide whether the conversa-
tion term see (x11 in Figure 1(a), 1(b), and 2) and
the hypothesis term watch (x4 in Figure 1(c), 1(d))
should be aligned, we first identify the subject of x4
in the hypothesis, which is x2 (A). We then look for
761
x2?s alignments in the conversation segment, among
which x9 (You) is the closest to x11 (see). In Fig-
ure 2(a), we find the distance between x11 and x9 is
3.
Using the implicit modeling of argument consis-
tency, we follow the same approach as in our pre-
vious work (Zhang and Chai, 2009) and trained a
logistic regression model to predict verb alignment
based on the features in Table 1.
6.1.2 Explicit Modeling of AC
The second approach captures argument consis-
tency based on explicit modeling of the relationship
between a verb and its aligned subject (or object).
Given a pair of verb terms (x, y), let sy be the sub-
ject of y and sx be the aligned entity of sy in the
conversation closest to x, we use the string describ-
ing the path from x to sx as the feature to capture
subject consistency. For example, in Figure 2(a), the
path from x11 to x9 is V ? V ? V ? N .
This string representation of paths is used to cap-
ture both the subject consistency and the object con-
sistency. Since they are non-numerical features, and
the variability of their values can be extremely large,
so we applied an instance-based classification model
(e.g., k-nearest neighbor) to determine alignments
between verb terms. We measure the distance be-
tween two path features by their minimal string edit
distance, and then simply use the Euclidean distance
to measure the closeness between any two verbs.
Again this model is trained from our development
data described in Zhang and Chai (2009).
Figure 3 shows an example of alignment between
the conversation terms and hypothesis terms in Ex-
ample 2. Note that in this figure the alignment
between x5 = suggests from the hypothesis and
u4 = opinion from the conversation segment is a
pseudo alignment, which directly maps a verb term
in the hypothesis to an utterance term represented
by its dialogue act. This alignment is obtained by
following the same set of rules learned from the de-
velopment dataset as in (Zhang and Chai, 2009).
6.2 Applications in Inference Model
As mentioned earlier, once an alignment is estab-
lished, the inference model is to predict whether
each clause in the hypothesis is entailed from the
conversation segment. Two separate models were
x 4=
ha
ve
x 5=Ax 2=
Sle
ep
ing
 
wi
th 
the
 En
em
y
x 1=A x 7=
is r
ea
lly
gre
at
x 10
=o
ne
u 4=
op
ini
on
Conversation Segment
x 3=
Sle
ep
ing
 
wi
th 
the
 En
em
y
x 5=
sug
ge
sts
x 2=
A
x 1=
B
x 4=
wa
tch
Hypothesis
x 6=
tha
t
x 11
=s
ee
x 12
=g
o
x 13
=h
av
e
x 8=
ha
ve 
he
ard
u 3=
sta
tem
en
t
u 2=
no
_an
sw
er
u 1=
yes
_no
_que
sti
on
x 3=
see
n
x 9=As B s A
Figure 3: The alignment result for Example 2
used to handle the inference of property clauses
(hj(x)) and and the inference of relational clauses
(hj(x, y)). Property clauses involve less variables
and are relatively simple, so we used the same prop-
erty inference model as in (Zhang and Chai, 2009).
Here we focus on relational inference model and ex-
amine how different modeling of long distance rela-
tionship may affect relation inference.
For a relation h between x and y to be entailed
from a conversation segment, we need to find a same
or similar relation in the conversation segment be-
tween x?s and y?s counterparts (i.e., aligned entities
of x and y in the conversation segment).
More specifically, given a relational clause from
the hypothesis, hj(x, y), we find the sets of
terms X ? = {x?|x? ? D, g(x?, x) = 1} and Y ? =
{y?|y? ? D, g(y?, y) = 1}, which are aligned with x
and y, respectively. We then find the closest re-
lation between these two sets of terms, (x?, y?),
such that the distance between x? and y? is the
smallest for any x? ? X ? and y? ? Y ?. For in-
stance, in the hypothesis of Example 2 there are
terms x5=suggests and x4=watch, and a relational
clause obj(x5, x4) describing an action-object rela-
tion between them. Their counterparts in the con-
762
versation segment are X ? = {u4=viewpoint} and
Y ? = {x3=seen, x11=see}. So the closest pair of
terms between these two sets is u4 and x11. Conse-
quently, whether the target relational clause hj(x, y)
is entailed is determined by the relationship between
x? and y?. Such relationship can be modeled either
implicitly or explicitly.
6.3 Implicit modeling of relation inference
In this model we follow the simple idea that the
shorter a path is between two terms, the more likely
these two terms have a direct relationship. So we
predefine a threshold, ?L. We predict that hj(x, y) is
entailed if the distance between x? and y? is smaller
than ?L. However, as can be seen, this distance does
not reflect whether the type of relationship between
x? and y? is similar to the relationship that holds be-
tween x and y.
6.4 Explicit modeling of relation inference
In order to capture more semantics from the rela-
tion between two terms, we use explicit modeling
of the relationship between terms x? and y?. In
the previous example, the relationship between u4
and x11 is modeled by the path from u4 to x11,
U ? V ? V ? V .
Given this characterization, the prediction of
whether hj(x, y) is entailed from the conversation
segment is formulated as a binary classification
problem, using a k-nearest neighbor classification
model with following features:
1. Explicit modeling of long distance relationship,
i.e., the path from x? to y? in the dependency
structure of the conversation segment;
2. The types (N, V, or U) of x, y, x?, and y?;
3. The type of relation between x and y, for ex-
ample, obj in obj(x, y);
4. The order (i.e., before or after) between x and
y, and between x? and y?;
5. The specific type of the hypothesis.
7 Evaluation and Analysis
We evaluated different model configurations using
our data1. This dataset consists of 291 development
instances and 584 testing instances. The hypotheses
1The data is available for download at http:
//links.cse.msu.edu:8000/lair/projects/
conversationentailment_data.html.
(a) Based on basic representation
(b) Based on augmented representation
Figure 4: Evaluation of verb alignment
were categorized into four types: (1) fact: profile
and social relations of conversation participants (ac-
counted for 47% of the development data and 49%
of the testing data); (2) belief: participants? beliefs
and opinions (34% and 35%); (3) desire: partici-
pants? desire of certain actions or outcomes (11%
and 4%); (4) intent: communicative intent that cap-
tures some perlocutionary force from one participant
to the other (e.g. A stops B from doing something;
A disagreees with B on something, 8% and 12%)
Note that in our original work (Zhang and Chai,
2009), only development data were used to show
some initial observations. Here we trained our mod-
els on the development data and results shown are
from the testing data.
7.1 Evaluation of Alignment Models
The evaluation of alignment models is based on pair-
wise decision. For each pair of terms (x, y), where
x is from a conversation segment and y is from
a hypothesis, we measure whether the model cor-
rectly predicts that the two terms should or should
not be aligned. Because the alignment classification
has extremely unbalanced classes, we use precision-
recall of true alignments as evaluation metrics.
Figure 4(a) and 4(b) shows the comparison (F-
measure) of two alignment models for verb align-
763
Figure 5: Evaluation of inference models based on different representations
ment, based on the basic representation and the aug-
mented representation, respectively. Note that we
cannot directly compare the results between these
two figures since they involve different number of
alignment instances2. Nevertheless, we can see the
overall trend within each figure: the explicit model
outperforms the implicit model. This suggests that
the explicit modeling of semantic relationship be-
tween verbs and arguments works better than the im-
plicit modeling used in previous work. Furthermore,
the improvement is most noticeable when hypothe-
ses are facts (24.8% with the basic representation
and 24.1% with the augmented representation), and
least when hypotheses are intents (12.2% with the
basic representation and 6.2% with the augmented
representation).
7.2 Evaluation of Inference Models
In order to compare different inference models, in
this section (and this section only) we use gold-
standard alignment results. They are obtained from
manual annotation in our evaluation. We evaluated
two inference models, one with implicit modeling
of long distance relationship and one with explicit
modeling. Evaluations were conducted based on
both the basic representation and the augmented rep-
resentation. Figure 5 shows the four groups of eval-
uation results.
Overall speaking, the augmented representation
outperforms the basic representation for both im-
plicit modeling and explicit modeling of long dis-
tance relationship (McNemar?s tests, p < 0.05). The
explicit model performs better than implicit model
only based on augmented representation (McNe-
mar?s test, p < 0.05).
2The alignment based on the augmented representation in
Figure 4(b) also includes pseudo alignments.
Clause Rep- Relation modeling Improve-
resentation Implicit Explicit ment
Basic 53.9% 53.9% 0
Augmented 54.8% 58.7% 3.9%
Table 2: Entailment performance with different represen-
tations and LDR modeling
The results were further broken down by different
hypothesis types. For the fact type of hypotheses,
there is no difference between different represen-
tations and modeling of long distance relationship.
This is not surprising since most hypotheses about
partipants? profiling information can be inferred di-
rectly from the utterances. The augmented repre-
sentation affects the intent type of hypothesis most
significantly, so does the explicit modeling of long
distance relationship.
7.3 Interaction between Clause
Representations and LDR Modeling
It was shown in previous sections that the aug-
mented representation helps entailment prediction
compared to the basic representation. Here we want
to study how they interact with other entailment
components and what is their effect in the enhanced
modeling of long distance relations. Specifically, we
test the performance of implicit and explicit mod-
eling of long distance relations under two different
representation settings: the basic representation and
the augmented representation.
Table 2 compares the performance (accuracy) of
entailment models with different relationship mod-
eling. We can see that the explicit model makes im-
provement over the implicit model for augmented
representation (McNemar?s test, p < 0.05), while
no improvement is made for basic representation.
These evaluation results appear to suggest that there
764
is an interaction between clause representations and
semantic modeling of long distance relations: the
modeling of long distance relations between lan-
guage constituents appears only effective when con-
versation structure is incorporated in the representa-
tion.
It is interesting to see the difference in the predic-
tion performances on fact hypotheses and intent hy-
potheses. For fact, the most benefit of incorporating
explicit modeling of long distance relationship ap-
pears at the alignment stage, but not much at the in-
ference stage. However, this situation is different for
intent, where the benefit of explicitly modeling long
distance relationship mostly happened at the infer-
ence stage. This observation suggests that the effects
of different types of modeling may vary for different
types of hypotheses, which indicates that hypothesis
type dependent models may be beneficial.
8 Discussion and Conclusion
This paper presents an empirical investigation on
conversation entailment. We specifically examine
two levels of representation of conversation seg-
ments and two different ways of modeling long dis-
tance relations between language constituents. Our
findings indicate that, although traditional architec-
ture and approaches for textual entailment remain
important, additional representation and processing
that address conversation structures is critical. The
augmented representation with conversation struc-
tures, together with explicit modeling of semantic
relations between language constituents, results in
the best performance (58.7% accuracy).
The work here only represents an initial step to-
wards conversation entailment. Conversation phe-
nomena are rich and complex. Conversation entail-
ment is extremely difficult. Besides the same chal-
lenges faced by textual entailment, it is further com-
plicated by conversation implicature. Although our
current data enables us to start an initial investiga-
tion, its small size poses significant limitations on
technology development and evaluation. For ex-
ample, our studies have indicated hypothesis type-
dependent approaches may be beneficial, however
we do not have sufficient data to yield reasonable
models. A more systematical approach to collect
and create a larger set of data is crucial. Inno-
vative community-based approaches (e.g., through
web) for data collection and annotation can be pur-
sued in the future. As more techniques in semantic
processing (e.g., semantic role) become available,
future work should also capture deeper semantics,
address pragmatics, and incorporate richer world
knowledge.
Finally, as the technology in conversation entail-
ment is developed, its applications in NLP problems
should be explored. Example applications include
information extraction, question answering, summa-
rization from conversation scripts, and modeling of
conversation participants. These applications may
provide new insights on the nature of the conversa-
tion entailment problem and its potential solutions.
Acknowledgments
This work was supported by grant IIS-0347548 from
the National Science Foundation. We thank the
anonymous reviewers for their valuable comments
and suggestions.
References
Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo
Giampiccolo, Bernardo Magnini, and Idan Szpektor.
2006. The second pascal recognising textual entail-
ment challenge. In Proceedings of the Second PAS-
CAL Challenges Workshop on Recognising Textual
Entailment, Venice, Italy.
Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo
Giampiccolo, and Bernardo Magnini. 2009. The fifth
pascal recognizing textual entailment challenge. In
Proceedings of the Second Text Analysis Conference
(TAC 2009).
Johan Bos and Katja Markert. 2005. Recognising textual
entailment with logical inference. In Proceedings of
HLT-EMNLP, pages 628?635.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2005. The pascal recognising textual entailment chal-
lenge. In PASCAL Challenges Workshop on Recognis-
ing Textual Entailment.
Rodrigo de Salvo Braz, Roxana Girju, Vasin Pun-
yakanok, Dan Roth, and Mark Sammons. 2005. An
inference model for semantic entailment in natural lan-
guage. In Proceedings of AAAI.
Michel Galley, Kathleen McKeown, Julia Hirschberg,
and Elizabeth Shriberg. 2004. Identifying agreement
and disagreement in conversational speech: Use of
bayesian networks to model pragmatic dependencies.
In Proceedings of ACL, pages 669?676.
765
Nikesh Garera and David Yarowsky. 2009. Modeling la-
tent biographic attributes in conversational genres. In
Proceedings of the Joint Conference of the 47th An-
nual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing of
the AFNLP, pages 710?718, Suntec, Singapore, Au-
gust.
Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and
Bill Dolan. 2007. The third pascal recognizing tex-
tual entailment challenge. In Proceedings of the ACL-
PASCAL Workshop on Textual Entailment and Para-
phrasing, pages 1?9.
Danilo Giampiccolo, Hoa Trang Dang, Bernardog
Magnini, Ido Dagan, Elena Cabrio, and Bill Dolan.
2008. The fourth pascal recognizing textual entail-
ment challenge. In Proceedings of the First Text Anal-
ysis Conference (TAC 2008).
John J. Godfrey and Edward Holliman. 1997.
Switchboard-1 Release 2. Linguistic Data Consor-
tium, Philadelphia.
Aria Haghighi, Andrew Ng, and Christopher Manning.
2005. Robust textual inference via graph matching. In
Proceedings of HLT-EMNLP, pages 387?394.
Hongyan Jing, Nanda Kambhatla, and Salim Roukos.
2007. Extracting social networks and biographical
facts from conversational speech transcripts. In Pro-
ceedings of ACL, pages 1040?1047.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In ACL ?03: Proceedings
of the 41st Annual Meeting on Association for Compu-
tational Linguistics, pages 423?430, Morristown, NJ,
USA.
Dekang Lin. 1998. An information-theoretic definition
of similarity. In Proceedings of International Confer-
ence on Machine Learning, pages 296?304.
Bill MacCartney, Trond Grenager, Marie-Catherine
de Marneffe, Daniel Cer, and Christopher D. Man-
ning. 2006. Learning to recognize features of valid
textual entailments. In Proceedings of HLT-NAACL,
pages 41?48.
Gabriel Murray and Giuseppe Carenini. 2008. Summa-
rizing spoken and written conversations. In Proceed-
ings of the 2008 Conference on Empirical Methods in
Natural Language Processing, pages 773?782, Hon-
olulu, Hawaii, October.
Sameer S. Pradhan, Wayne Ward, and James H. Martin.
2008. Towards robust semantic role labeling. Compu-
tational Linguistics, 34(2):289?310.
Rajat Raina, Andrew Y. Ng, and Christopher D. Man-
ning. 2005. Robust textual inference via learning and
abductive reasoning. In Proceedings of AAAI, pages
1099?1105.
Alan Ritter, Colin Cherry, and Bill Dolan. 2010. Unsu-
pervised modeling of twitter conversations. In Human
Language Technologies: The 2010 Annual Conference
of the North American Chapter of the Association for
Computational Linguistics, pages 172?180, Los An-
geles, California, June.
Swapna Somasundaran, Josef Ruppenhofer, and Janyce
Wiebe. 2007. Detecting arguing and sentiment in
meetings. In Proceedings of the 8th SIGdial Workshop
on Discourse and Dialogue, Antwerp, September.
Swapna Somasundaran, Janyce Wiebe, and Josef Rup-
penhofer. 2008. Discourse level opinion interpreta-
tion. In Proceedings of the 22nd International Con-
ference on Computational Linguistics (Coling 2008),
pages 801?808, Manchester, UK, August.
Swapna Somasundaran, Galileo Namata, Janyce Wiebe,
and Lise Getoor. 2009. Supervised and unsupervised
methods in employing discourse relations for improv-
ing opinion polarity classification. In Proceedings of
the 2009 Conference on Empirical Methods in Natu-
ral Language Processing, pages 170?179, Singapore,
August.
Marta Tatu and Dan Moldovan. 2005. A semantic ap-
proach to recognizing textual entailment. In Proceed-
ings of HLT-EMNLP, pages 371?378.
Fabio Massimo Zanzotto and Alessandro Moschitti.
2006. Automatic learning of textual entailments with
cross-pair similarities. In ACL-44: Proceedings of the
21st International Conference on Computational Lin-
guistics and the 44th annual meeting of the Associ-
ation for Computational Linguistics, pages 401?408,
Morristown, NJ, USA.
Chen Zhang and Joyce Chai. 2009. What do we know
about conversation participants: Experiments on con-
versation entailment. In Proceedings of the SIGDIAL
2009 Conference, pages 206?215.
766
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 392?402,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Towards Situated Dialogue: Revisiting Referring Expression Generation
Rui Fang, Changsong Liu, Lanbo She, Joyce Y. Chai
Department of Computer Science and Engineering
Michigan State University, East Lansing, MI, 48824, USA
{fangrui,cliu,shelanbo,jchai}@cse.msu.edu
Abstract
In situated dialogue, humans and agents have
mismatched capabilities of perceiving the
shared environment. Their representations
of the shared world are misaligned. Thus
referring expression generation (REG) will
need to take this discrepancy into consider-
ation. To address this issue, we developed
a hypergraph-based approach to account for
group-based spatial relations and uncertain-
ties in perceiving the environment. Our em-
pirical results have shown that this approach
outperforms a previous graph-based approach
with an absolute gain of 9%. However, while
these graph-based approaches perform effec-
tively when the agent has perfect knowledge
or perception of the environment (e.g., 84%),
they perform rather poorly when the agent has
imperfect perception of the environment (e.g.,
45%). This big performance gap calls for new
solutions to REG that can mediate a shared
perceptual basis in situated dialogue.
1 Introduction
Situated human robot dialogue has received increas-
ing attention in recent years. In situated dialogue,
robots/artificial agents and their human partners are
co-present in a shared physical world. Robots need
to automatically perceive and make inference of the
shared environment. Due to its limited perceptual
and reasoning capabilities, the robot?s representation
of the shared world is often incomplete, error-prone,
and significantly mismatched from that of its human
partner?s. Although physically co-present, a joint
perceptual basis between the human and the robot
cannot be established (Clark and Brennan, 1991).
Thus, referential communication between the hu-
man and the robot becomes difficult.
How this mismatched perceptual basis affects ref-
erential communication in situated dialogue was in-
vestigated in our previous work (Liu et al, 2012).
In that work, the main focus is on reference resolu-
tion: given referential descriptions from human part-
ners, how to identify referents in the environment
even though the robot only has imperfect percep-
tion of the environment. Since robots need to col-
laborate with human partners to establish a joint per-
ceptual basis, referring expression generation (REG)
becomes an equally important problem in situated
dialogue. Robots have much lower perceptual capa-
bilities of the environment than humans. How can
a robot effectively generate referential descriptions
about the environment so that its human partner can
understand which objects are being referred to?
There has been a tremendous amount of work
on referring expression generation in the last two
decades (Dale, 1995; Krahmer and Deemter, 2012).
However, most existing REG algorithms were devel-
oped and evaluated under the assumption that agents
and humans have access to the same kind of domain
information. For example, many experimental se-
tups (Gatt et al, 2007; Viethen and Dale, 2008;
Golland et al, 2010; Striegnitz et al, 2012) were
developed based on a visual world for which the in-
ternal representation is assumed to be known and
can be represented symbolically. However, this as-
sumption no longer holds in situated dialogue with
robots. There are two important distinctions in situ-
ated dialogue. First, the perfect knowledge of the en-
vironment is not available to the agent ahead of time.
The agent needs to automatically make inferences to
connect recognized lower-level visual features with
392
symbolic labels or descriptors. Both recognition and
inference are error-prone and full of uncertainties.
Second, in situated dialogue the agent and the hu-
man have mismatched representations of the envi-
ronment. The agent needs to take this difference into
consideration to identify the most reliable features
for REG. Given these two distinctions, it is not clear
whether state-of-the-art REG approaches are appli-
cable under mismatched perceptual basis in situated
dialogue.
To address this issue, this paper revisits the prob-
lem of REG in the context of mismatched percep-
tual basis. We extended a well known graph-based
approach (Krahmer et al, 2003) that has shown
to be effective in previous work (Gatt and Belz,
2008; Gatt et al, 2009). We incorporated uncer-
tainties in perception into cost functions. We fur-
ther extended regular graph representation into hy-
pergraph representation to account for group-based
spatial relations that are important for visual descrip-
tions (Dhande, 2003; Tenbrink and Moratz, 2003;
Funakoshi et al, 2006; Liu et al, 2012). Our em-
pirical results demonstrate that both enhancements
lead to about a 9% absolute performance gain com-
pared to the original approach. However, while
our approache performs effectively when the agent
has perfect knowledge or perception of the environ-
ment (e.g., 84%), it performs poorly under the mis-
matched perceptual basis (e.g., 45%). This perfor-
mance gap calls for new solutions for REG that are
capable of mediating mismatched perceptual basis.
In the following sections, we first describe our
hypergraph-based representations and illustrate how
uncertainties from automated perception can be in-
corporated. We then describe an empirical study us-
ing Amazon Mechanical Turks for evaluating gener-
ated referring expressions. Finally we present evalu-
ation results and discuss potential future directions.
2 Related Work
Since the Full Brevity algorithm (Dale, 1989), many
approaches have been developed and evaluated for
REG (Dale, 1995; Krahmer and Deemter, 2012),
such as the incremental algorithm (Dale, 1995),
the locative algorithm (Kelleher and Kruijff, 2006),
and graph-based approaches (Krahmer et al, 2003;
Croitoru and Van Deemter, 2007). Most of these ap-
proaches assume the agent has access to a complete
symbolic representation of the domain. While these
approaches work well for many applications involv-
ing user interfaces, the question is whether they can
be extended to the situation where the agent has in-
complete or incorrect knowledge and needs to make
inference about the domain or the world.
Recently, there has been increasing interest in
REG for visual objects (Roy, 2002; Golland et al,
2010; Mitchell et al, 2013). Some work (Golland
et al, 2010) uses visual scenes that are generated by
computer graphics and thus the internal representa-
tion of the scene is known. Some other work focuses
on the connection between lower-level visual fea-
tures and symbolic descriptors for REG (Roy, 2002;
Mitchell et al, 2013). However, most work assumes
no vision recognition errors. It is well established
that automated recognition of visual scenes is ex-
tremely challenging. This process is error-prone
and full of uncertainties. It is not clear whether
the existing approaches can be extended to the sit-
uation where the agent has imperfect perception of
the shared environment.
An earlier work by Horacek (Horacek, 2005)
has looked into the problem of mismatched knowl-
edge between conversation partners for REG. The
approach is a direct extension of the incremental al-
gorithm (Dale, 1995). However, this work only pro-
vides a proof of concept example to illustrate the
idea. No empirical evaluation was given.
All these previous works have motivated our
present investigation. We are interested in REG un-
der mismatched perceptual basis between conversa-
tion partners, where the agent has imperfect percep-
tion and knowledge of the shared environment. In
particular, we took a well-studied graph-based ap-
proach (Krahmer et al, 2003) and extended it to in-
corporate group spatial relations and uncertainties
associated with automated perception of the envi-
ronment. The reason we chose a graph-based ap-
proach is that graph representations are widely used
in the fields of computer vision (CV) and pattern
recognition to represent spatially rich scenes. Never-
theless, the findings from this investigation provide
insight to other approaches.
393
(a) an original scene (b) the corresponding impoverished
scene
Figure 1: An original scene and its impoverished scene processed by CV algorithm
3 Hypergraph-based REG
Towards mediating a shared perceptual basis in sit-
uated dialogue, our previous work (Liu et al, 2012)
has conducted experiments to study referential com-
munication between partners with mismatched per-
ceptual capabilities. We simulated mismatched ca-
pabilities by making an original scene (Figure 1(a))
available to a director (simulating higher perceptual
calibre) and a corresponding impoverished scene
(Figure 1(b)) available to a matcher (simulating low-
ered perceptual calibre). The impoverished scene
is created by re-rendering automated recognition re-
sults of the original scene by a CV algorithm. An
example of the original scene and an impoverished
scene is shown in Figure 1. Using this setup, the di-
rector and the matcher were instructed to collaborate
with each other on some naming games. Through
these games, they collected data on how partners
with mismatched perceptual capabilities collaborate
to ground their referential communication.
The setup in (Liu et al, 2012) is intended to sim-
ulate situated dialogue between a human (like the
director) and a robot (like the matcher). The robot
has a significantly lowered ability in perception and
reasoning. The robot?s internal representation of the
shared world will be much like the impoverished
scene which contains many recognition errors. The
data from (Liu et al, 2012; Liu et al, 2013) shows
that different strategies were used by conversation
partners to produce referential descriptions. Besides
directly describing attributes or binary relations with
a relatum, they often use group-based descriptions
(e.g., a cluster of four objects on the right). This is
mainly due to the fact that some objects are simply
not recognizable to the matcher. Binary spatial rela-
tionships sometimes are difficult to describe the tar-
get object, so the matcher must resort to group infor-
mation to distinguish the target object from the rest
of the objects. For example, suppose the matcher
needs to describe the target object 5 in Figure 1(b),
he/she may have to start by indicating the group of
three objects at the bottom and then specify the re-
lationship (i.e., top) of the target object within this
group.
The importance of group descriptions has been
shown not only here, but also in previous works
on REG (Funakoshi et al, 2004; Funakoshi et al,
2006; Weijers, 2011). While the original graph-
based approach can effectively represent attributes
and binary relations between objects (Krahmer et al,
2003), it is insufficient to capture within-group or
between-group relations. Therefore, to address the
low perceptual capabilities of artificial agents, we in-
troduce hypergraphs to represent the shared environ-
ment. Our approach has two unique characteristics
compared to previous graph-based approaches: (1)
A hypergraph representation is more general than
a regular graph. Besides attributes and binary re-
lations, it can also represent group-based relations.
(2) Unlike previous work, here the generation of hy-
pergraphs are completely driven by automated per-
ception of the environment. This is done by incor-
porating uncertainties in perception and reasoning
into cost functions associated with graphs. Next we
394
give a detailed account on hypergraph representa-
tion, cost functions incorporating uncertainties, and
the search algorithm for REG.
3.1 Hypergraph Representation
A directed hypergraph G (Gallo et al, 1993) is a
tuple of the form: G = ?X, A?, in which
X = {xm}
A = {ai = (ti, hi) | ti ? X, hi ? X}
Similar to regular graphs, a hypergraph consists
of a set of nodes X and a set of arcs A. However,
different from regular graphs, each arc in A is con-
sidered as a hyperarc in the sense that it can capture
relations between any two subsets of nodes: a tail
(ti) and a head (hi). Therefore, a hypergraph is a
generalization of a regular graph. It becomes a reg-
ular graph if the cardinalities of both the tail and the
head are restricted to one for all hyperarcs. While
regular graphs are commonly used to represent bi-
nary relations between two nodes, hypergraphs pro-
vide a more general representation for n-ary rela-
tions among multiple nodes.
We use hypergraphs to represent the agent?s per-
ceived physical environment (also called scene hy-
pergraphs). More specifically, each perceived ob-
ject is represented by a node in the graph. Each per-
ceived visual attribute of an object (e.g., color, size,
type information) or a group of objects (e.g., num-
ber of objects in the group, location) is captured by
a self-looping hyperarc. Hyperarcs are also used to
capture the spatial relations between any two subsets
of nodes, whether it is a relation between two ob-
jects, or between two groups of objects, or between
one or more objects within a group of objects.
For example, Figure 2 shows a hypergraph cre-
ated for part of the impoverished scene shown in
Figure 1(b) (i.e., the upper right corner including
objects 7, 8, 9, 11, and 13). One important char-
acteristic is that, because the graph is created based
on an automated vision recognition system, the val-
ues of an attribute or a relation in the hypergraph
are numeric (except for the type attribute). For ex-
ample, the value of the color attribute is the RGB
distribution extracted from the corresponding visual
object, the value of the size attribute is the width and
height of the bounding box and the value of the lo-
cation attribute is a function of spatial coordinates.
These numerical features will be further converted to
symbolic labels with certain confidence scores (de-
scribed later in Section 3.3.2).
3.2 Hypergraph Pruning
The perceived visual scene can be represented as a
complete hypergraph, in which any pair of two sub-
sets of nodes are connected by a hyperarc. However,
such a complete hypergraph is not only inefficient
but also unnecessary. Instead of keeping all possible
n-ary relations (i.e., hyperarcs), we only retain those
relations that are likely used by humans to produce
referring expressions, based on two heuristics.
The first heuristic is based on perceptual prin-
ciples, also called the Gestalt Laws of perception
(Sternberg, 2003), which describe how people group
visually similar objects into entities or groups. Two
well known principles of perceptual grouping are
proximity and similarity (Wertheimer, 1938): ob-
jects that lie close together are often perceived as
groups; objects of similar shape, size or color are
more likely to form groups than objects differing
along these dimensions. Based on these two prin-
ciples, previous works have developed different al-
gorithms for perceptual grouping (Thrisson, 1994;
Gatt, 2006). In our investigation, we adopted Gatt?s
algorithm (Gatt, 2006), which has shown to be more
accurate for spatial grouping. Given the results
from spatial grouping, we only retain hyperarcs that
represent spatial relations between two objects, be-
tween two perceived groups, between one object and
a perceived group, or between one object and the
group it belongs to.
The second heuristic is based on the observation
that, given a certain orientation, people tend to use a
relatum that is closer to the referent than more dis-
tant relata. In other words, it is less likely to refer to
an object relative to a distant relatum when there is
a closer relatum. For example, when referring to the
stapler (object 9 in Figure 1(a) ), it is more likely to
use ?the stapler above the battery? than ?the stapler
above the cellphone?. Based on this observation, we
prune the hypergraphs by only retaining hyperarcs
between an object and their closest relata for each
possible orientation.
Figure 2 shows the resulting hypergraph for rep-
resenting a subset of objects (7, 8, 9, 11, and 13) in
Figure 1(a).
395
Figure 2: An example of hypergraph representing the per-
ceived scene (a partial scene only including object 7, 8,
9, 11, 13 for Figure 1(a)).
3.3 Symbolic Descriptors for Attributes
As mentioned earlier, the values of attributes of ob-
jects and their relations are numerical in nature. In
order for the agent to generate natural language de-
scriptions, the first step is to assign symbolic labels
or descriptors to those attributes and relations. Next
we describe how we use a lexicon with grounded se-
mantics in this process.
3.3.1 Lexicon with Grounded Semantics
Grounded semantics provides a bridge to connect
symbolic labels or words with lower level visual fea-
tures (Harnad, 1990). Previous work has developed
various approaches for grounded semantics mainly
for the reference resolution task, i.e., identifying vi-
sual objects in the environment given language de-
scriptions (Dhande, 2003; Gorniak and Roy, 2004;
Tenbrink and Moratz, 2003; Siebert and Schlangen,
2008; Liu et al, 2012). For the referring expression
generation task here, we also need a lexicon with
grounded semantics.
In our lexicon, the semantics of each category
of words is defined by a set of semantic grounding
functions that are parameterized on visual features.
For example, for the color category it is defined as a
multivariate Gaussian distribution based on the RGB
distribution. Specific words such as green, red, or
blue have different means and co-variances as the
following:
color : red = fr(~vcolor) = N(~vcolor | ?1,
?
1)
color : green = fg(~vcolor) = N(~vcolor | ?2,
?
2)
color : blue = fb(~vcolor) = N(~vcolor | ?3,
?
3)
The above functions define how likely a set of rec-
ognized visual features (i.e., ~vcolor) describing the
color dimensions (i.e., RGB distribution) is to match
the color terms red, green, and blue.
For the spatial relation terms such as above, be-
low, left, right, the semantic grounding functions
take both vertical and horizontal coordinates of two
objects, as follows 1:
spatialRel : above(a, b) = fabove(~valoc, ~vbloc)
=
{
1? |xa?xb|400 if ya < yb;
0 otherwise.
Using the above convention, we have defined se-
mantic grounding functions for size category words
(e.g., small and big) and absolute position words
(e.g., top, below, left, and right). In addition, we use
object recognition models (Zhang and Lu, 2002) to
define class type category words such as apple and
orange used in our domain.
3.3.2 Attribute Descriptors and Cost Functions
Given the lexicon with grounded semantics as de-
scribed above, the numerical attributes captured in
the scene hypergraph can be converted to symbolic
descriptors. For each attribute (e.g., color) or re-
lation, the corresponding visual feature vector (i.e.,
~vcolor) is plugged into the semantic grounding func-
tions for the corresponding category of words. The
word that best describes the attribute is chosen as the
descriptor for that attribute. For example, given an
RGB color distribution ~vcolor, we can find the color
descriptor as follows:
color : w? = argmax
red,green,blue
fw(~vcolor),
For each attribute or relation, we can find a best
descriptor in this manner. In addition, we also ob-
tain a numerical value (returned from the semantic
1The size of the overall scene is 800x800.
396
grounding functions) that measures how well this
descriptor describes the corresponding visual fea-
tures. Intuitively, one would choose a descriptor that
closely matches the visual features. Based on this
intuition, we define the cost for each attribute A as
the following:
cost(A) = 1? fw?( ~vA)
where w? is the best descriptor for the attribute.
Given an attribute, the better the descriptor
matches the extracted visual features, the lower the
cost of the corresponding hyperarc.
3.4 Graph Matching for REG
Now the hypergraph representing the perceived en-
vironment has symbolic descriptors for its attributes
and relations together with corresponding costs.
Given this representation, REG can be formulated as
a graph matching algorithm similar to that described
in (Krahmer et al, 2003). We use the same Branch
and Bound algorithm described in (Krahmer et al,
2003). In this approach, a hypothesis hypergraph
(starting with one node representing the target ob-
ject) is gradually expanded by adding in a least cost
hyperarc from the scene hypergraph. At each ex-
pansion, the hypothesis graph is matched against the
scene hypergraph to decide whether it matches any
nodes other than the target node in the scene hyper-
graph. The expansion stops if the hypothesis graph
does not cover any other nodes except for the target
node. At this point, the hypothesis graph captures all
the content (e.g., attributes and relations) required to
uniquely describe the target object. We then apply
a set of simple generation templates to generate the
surface form of referring expressions based on the
hypothesis graph.
4 Empirical Evaluations
4.1 Evaluation Setup
To evaluate the performance of this hypergraph-
based approach to REG, we conducted a compara-
tive study using crowd-sourcing. More specifically,
we created 48 different scenes similar to that in Fig-
ure 1(a). Each scene has 13 objects on average and
there are 621 objects in total. For each of these
scenes, we applied a CV algorithm (Zhang and Lu,
2002) and generated scene hypergraphs as described
in Section 3.1. We then use different generation
strategies (varied in terms of graph representations
and cost functions, to be explained in Section 4.2) to
automatically generate referring expressions to refer
to each object.
To evaluate the quality of these generated refer-
ring expressions, we applied Amazon Mechanical
Turk to solicit feedback from the crowd 2. Through
an interface, we displayed an original scene and gen-
erated referring expressions (from different genera-
tion strategies) in a random order. We asked each
turk to select the object in the scene that he/she be-
lieved was the one referred to by the shown refer-
ring expression (i.e., reference identification task).
Each referring expression received three votes from
the crowd. In total, 217 turks participated in our ex-
periment.
4.2 Generation Strategies
We applied a set of different strategies to generate
referring expressions for each object. The variations
lie in two dimensions: (1) different graph repre-
sentations: using a hypergraph to represent the per-
ceived scene as described in Section 3.1 versus us-
ing a regular graph as introduced in (Krahmer et al,
2003); and (2) different cost functions for attributes
and relations: cost functions that have been used in
previous works (Theune et al, 2007; Krahmer et al,
2008) and cost functions that incorporate uncertain-
ties of perception as described in Section 3.3.2.
Cost functions play an important role in graph-
based approaches (Krahmer et al, 2003). Previous
works have examined different types of cost func-
tions (Theune et al, 2007; Krahmer et al, 2008;
Theune et al, 2011). We adopted some commonly
used cost functions from previous work together
with the cost functions defined here. In particular,
we experimented with the following different cost
functions:
Simple Cost: The costs for all hyperarcs are set to
1. With this cost function, the graph-based algorithm
resembles the Full Brevity algorithm of Dale (Dale,
2To control the quality of crowdsourcing, we recruited par-
ticipants based on the following criteria: Participants? locations
are limited to the United States. Approval rate for each partic-
ipant?s previous work is greater than or equal to 95%, and the
number of each participant?s previous approved work is greater
than or equal to 1000.
397
1992) in that a shortest distinguishing description is
preferred.
Absolute Preferred: The costs for hyperarcs rep-
resenting absolute attributes (e.g., type, color, and
position) are set to 1. The costs for relative at-
tributes (e.g., size) and relations are set to 2. This
cost function mimics human?s preference for abso-
lute attributes over relative ones (Dale, 1995).
Relative Preferred: The costs for hyperarcs repre-
senting absolute attributes are set to 2 and for rela-
tive attributes and relations are set to 1. This cost
function has been applied previously to emphasize
the importance of spatial relations in REG (Viethen
and Dale, 2008).
Uncertainty Based: The costs for all hyperarcs are
defined by incorporating uncertainties from percep-
tion as described in Section 3.3.2.
Uncertainty Relative Preferred: To emphasize the
importance of spatial relations as demonstrated in
situated interaction (Tenbrink and Moratz, 2003;
Kelleher and Kruijff, 2006), the costs for hyperarcs
representing relative attributes and relations are di-
vided by 3. This cost function will allow the algo-
rithm to prefer spatial relations through the reduced
cost.
Note that we only tested a few (not all) com-
monly used cost functions proposed by previous
work (Krahmer et al, 2003; Theune et al, 2007;
Krahmer et al, 2008; Theune et al, 2011). For ex-
ample, we did not include the stochastic cost func-
tion which is defined based on the frequencies of at-
tribute selection from the training data (Krahmer et
al., 2003). On the one hand, we did not have a large
set of human descriptions of the impoverished scene
to learn the stochastic cost. On the other hand, it
is not clear whether human strategies of describing
the impoverished scene should be used to represent
optimal strategies for the robot. Nevertheless, the
above different cost functions will allow us to eval-
uate whether incorporating perceptual uncertainties
will make a difference in the REG performance.
4.3 Evaluation Results
As mentioned earlier, each generated referring ex-
pression received three independent votes regarding
its referent from the crowd. The referent with the
most votes is taken as the predicted referent and is
used for evaluation. If all three votes are differ-
Cost Function Regular Graph Hypergraph
Simple Costs 33.2% 33.3%
Absolute Preferred 30.1% 30.3%
Relative Preferred 31.1% 35.4%
Uncertainty Based 35.7% 37.5%
Uncertainty Rel. Prefer. 36.7% 45.2%
Table 1: Results with different cost functions
ent, then by default, it is deemed that the referent
is not correctly identified for that expression. We
use the accuracy of the referential identification task
(i.e., the percentage of generated referring expres-
sions where the referents are correctly identified) as
the metric to evaluate different generation strategies
illustrated in Section 4.2.
4.3.1 The Role of Cost Functions
Table 1 shows the results based on different cost
functions and different graph representations. There
are several observations.
First, when the agent does not have perfect knowl-
edge of the environment and has to automatically
infer the environment as in our setting here, cost
functions based on uncertainties of perception lead
to better results. This occurs for both regular graphs
and hypergraphs. This result is not surprising and
indicates that cost functions should be tied to the
agent?s ability to perceive and infer the environment.
The uncertainty based cost functions allow the agent
to prefer reliable attributes or relations.
Second, consistent with previous work (Viethen
and Dale, 2008), we observed the importance of spa-
tial relations. Especially when the perceived world
is full of uncertainties, spatial relations tend to be
more reliable. In particular, as shown in Table 1,
using hypergraphs enables generating group-based
relations and results in significantly better perfor-
mance (45.2%) compared to regular graphs (36.7%)
(p = 0.002).
Note that our current cost function only includes
uncertainties of the agent?s own perception in a sim-
plistic form. When humans and agents have mis-
matched perceptual basis, the human?s model of
comprehension and tolerance of inaccurate descrip-
tion could play a role in REG. Incorporating human
models in the cost function will require in-depth em-
pirical studies and we will leave that to our future
398
work.
4.3.2 The Role of Imperfect Perception
To further understand the role of hypergraphs in
mediating mismatched perceptions between humans
and agents, we created a perfect scene regular graph
and a perfect scene hypergraph (representing the
agent?s perfect knowledge of the environment) for
each of the 48 scenes used in the experiments. In
each of these scene graphs, the attribute and rela-
tion descriptors are manually provided. We fur-
ther applied the Absolute Preferred cost function
(which has shown competitive performance in previ-
ous work) to generate referring expressions for each
object. Again, each referring expression received
three votes from the crowd.
Table 2 shows the results comparing two con-
ditions: (1) REs generated (by the Absolute Pre-
ferred cost function) based on the perfect graphs
which represent the agent?s perfect knowledge and
perception of the environment; and (2) REs gener-
ated based on automatically created graphs (by the
Uncertainty Relative Preferred cost function) which
represent the agent?s imperfect knowledge of the
environment as a result of automated recognition
and inference. The result shows that given perfect
knowledge of the environment, hypergraphs only
perform marginally better than the regular graphs
(p = 0.07). Given imperfect knowledge of the envi-
ronment, hypergraphs significantly outperforms the
regular graphs by taking advantage of spatial group-
ing information (p = 0.002). It is worthwhile to
mention that currently we use spatial proximity to
identify groups. However, the hypergraph based ap-
proach is not restricted to spatial grouping. In the-
ory, it can represent any type of group based on dif-
ferent similarity criteria.
Furthermore, our result shows that the graph-
based approaches perform quite competitively under
the condition of perfect knowledge and perception.
Although evaluated on different data sets, this result
is consistent with results from previous work (Gatt
and Belz, 2008; Gatt et al, 2009). However, what is
more interesting here is that while graph-based ap-
proaches perform well when the agent has perfect
knowledge of the environment, as its human part-
ner, these approaches literally fall apart with close
to 40% performance degradation when applied to
Environment Regular Graph Hypergraph
Pefect Perception 80.4% 84.2%
Imperfect Perception 36.7% 45.2%
Table 2: Results of comparing perfect perception and im-
perfect perception of the shared world.
the situation where the agent?s representation of the
shared world is problematic and full of mistakes.
These results indicate that REG for automati-
cally perceived scenes can be extremely challeng-
ing. Many errors result from automated perception
and reasoning that will affect the internal representa-
tion of the world and thus the generated REs. In our
experiments here, we applied a very basic CV algo-
rithm which resulted in rather poor performance in
our data: overall, 60.3% of objects in the original
scene are mis-recognized, and 10.5% of objects are
mis-segmented. We think this poor CV performance
represents a more challenging problem.
Some errors such as recognition errors can be by-
passed using our current approach based on hyper-
graphs. For example, in Figure 1 target object 9 (a
stapler) and 13 (a key) are mis-recognized as a cup
and a pen. Using our hypergraph-based approach,
for the target object 9, instead of generating ?a small
cup? (as in the case of using regular graphs), ?a gray
object on the top within a cluster of four objects?
is generated. For the target object 13, instead of ?a
pen? as generated by regular graphs, ?a small object
on the right within a cluster of 4? is generated. Even
with recognition errors, these group-based descrip-
tions will allow the listener to identify target objects
in their representation correctly. Nevertheless, many
processing errors cannot be handled by our current
approach. For example, an object can be mistak-
enly segmented into multiple parts or several objects
can be mistakenly grouped into one object. In addi-
tion, our current semantic grounding functions are
simple. Sometimes they do not provide correct de-
scriptors for the extracted visual features. More so-
phisticated functions that better reflect human?s vi-
sual perception (Regier, 1996; Mojsilovic, 2005;
Mitchell et al, 2011) should be pursued in the fu-
ture.
399
Minimum Effort Extra Effort
Pefect Perception 84.2% 88.1%
Imperfect Perception 45.2% 51.5%
Table 3: Results of comparing minimum effort and extra
effort using hypergraphs
4.3.3 The Role of Extra Effort
While REG systems have a tendency to produce
minimal descriptions, recent psycholinguistic stud-
ies have shown that speakers do not necessarily fol-
low the Grice?s maxim of quantity, and they tend
to provide redundant properties in their descrip-
tions (Jordan and Walker, 2000; Belke and Meyer,
2002; Arts et al, 2011). With this in mind, we
conducted a very simple evaluation on the role of
extra effort. Once a set of descriptors are selected
based on the minimum cost, one additional descrip-
tor (with the least cost among the remaining at-
tributes or relations) is added to the referential de-
scription. We once again solicited the crowd feed-
back to this set of expressions generated by extra
effort. Each expression again received three votes
from the crowd.
Table 3 shows the results by comparing minimum
effort with extra effort when using hypergraphs to
generate REs. As indicated here, extra effort (by
adding one additional descriptor) leads to more com-
prehensible REs with 3.9% improvement under per-
fect perception and 6.3% improvement under imper-
fect perception (both are significant, p < 0.05). The
improvement is larger under imperfect perception.
This seems to indicate that exploring extra effort in
REG could help mediate mismatched perceptions in
situated dialogue. However, more understanding on
how to engage in such extra effort will be required
in the future.
5 Conclusion
In situated dialogue, humans and agents have mis-
matched perceptions of the shared environment. To
facilitate successful referential communication be-
tween a human and an agent, the agent needs to take
such discrepancies into consideration and generate
referential descriptions that can be understood by
its human partner. With this in mind, we re-visited
the problem of referring expression generation in the
context of mismatched perceptions between humans
and agents. In particular, we applied and extended
the state of the art graph-based approach (Krahmer
et al, 2003) in this new setting. Our empirical re-
sults have shown that, to address the agent?s limited
perceptual capability, REG algorithms will need to
take into account the uncertainties in perception and
reasoning. Group-based information appears more
reliable and thus should be modeled by an approach
that deals with automated perception of spatially
rich scenes.
While graph-based approaches have shown effec-
tive for the situation where the agent has complete
knowledge of the environment, as its human part-
ner, these approaches are often inadequate when hu-
mans and agents have mismatched representations
of the shared world. Our empirical results here call
for new solutions to address the mismatched per-
ceptual basis. Previous work indicated that referen-
tial communication is a collaborative process (Clark
and Wilkes-Gibbs, 1986; Heeman and Hirst, 1995).
Conversation partners make extra effort to collab-
orate with each other. For the situation with mis-
matched perceptual basis, a potential solution thus
should go beyond the objective of generating a mini-
mum description, and towards a collaborative model
which incorporates immediate feedback from the
conversation partner (Edmonds, 1994).
6 Acknowledgments
This work was supported by N00014-11-1-0410
from the Office of Naval Research and IIS-1208390
from the National Science Foundation.
References
Anja Arts, Alfons Maes, Leo Noordman, and Carel
Jansen. 2011. Overspecification facilitates object
identification. Journal of Pragmatics, 43(1):361?374.
E. Belke and A. S. Meyer. 2002. Tracking the time
course of multidimensional stimulus discrimination:
Analyses of viewing patterns and processing times
during ?same?-?different? decisions. European Jour-
nal of Cognitive Psychology, 14(2):237?266.
H.H. Clark and S.E. Brennan. 1991. Grounding in com-
munication. Perspectives on socially shared cognition,
13:127?149.
H. H Clark and D Wilkes-Gibbs. 1986. Referring as a
collaborative process. Cognition, 22:1?39.
400
Madalina Croitoru and Kees Van Deemter. 2007. A con-
ceptual graph approach to the generation of referring
expressions. In Proceedings of the 20th international
joint conference on Artifical intelligence, IJCAI?07,
pages 2456?2461.
Robert Dale. 1989. Cooking up referring expressions.
In Proceedings of the 27th annual meeting on Associ-
ation for Computational Linguistics, ACL ?89, pages
68?75, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Robert Dale. 1992. Generating Referring Expressions:
Constructing Descriptions in a Domain of Objects and
Processes. The MIT Press,Cambridge, Massachusetts.
Robert Dale. 1995. Computational interpretations of the
gricean maxims in the generation of referring expres-
sions. Cognitive Science, 19:233?263.
Sheel Sanjay Dhande. 2003. A computational model to
connect gestalt perception and natural language. In
Masters thesis, Massachusetts Institure of Technology.
Philip G. Edmonds. 1994. Collaboration on reference to
objects that are not mutually known. In Proceedings
of the 15th conference on Computational linguistics -
Volume 2, COLING ?94, pages 1118?1122, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Kotaro Funakoshi, Satoru Watanabe, Naoko Kuriyama,
and Takenobu Tokunaga. 2004. Generation of relative
referring expressions based on perceptual grouping. In
COLING.
Kotaro Funakoshi, Satoru Watanabe, and Takenobu
Tokunaga. 2006. Group-based generation of referring
expressions. In INLG, pages 73?80.
Giorgio Gallo, Giustino Longo, Stefano Pallottino, and
Sang Nguyen. 1993. Directed hypergraphs and ap-
plications. Discrete applied mathematics, 42(2):177?
201.
Albert Gatt and Anja Belz. 2008. Attribute selection for
referring expression generation: new algorithms and
evaluation methods. In Proceedings of the Fifth In-
ternational Natural Language Generation Conference,
INLG ?08, pages 50?58, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.
Albert Gatt, Ielka van der Sluis, and Kees van Deemter.
2007. Evaluating algorithms for the generation of re-
ferring expressions using a balanced corpus. In Pro-
ceedings of the Eleventh European Workshop on Nat-
ural Language Generation, ENLG ?07, pages 49?56,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Albert Gatt, Anja Belz, and Eric Kow. 2009. The tuna-
reg challenge 2009: overview and evaluation results.
In Proceedings of the 12th European Workshop on
Natural Language Generation, ENLG ?09, pages 174?
182, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Albert Gatt. 2006. Structuring knowledge for reference
generation: A clustering algorithm. In Proceedings of
the 11th Conference of the European Chapter of the
Association for Computational Linguistics, Associa-
tion for Computational Linguistics, pages 321?328.
Dave Golland, Percy Liang, and Dan Klein. 2010. A
game-theoretic approach to generating spatial descrip-
tions. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Processing,
EMNLP ?10, pages 410?419, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Peter Gorniak and Deb Roy. 2004. Grounded seman-
tic composition for visual scenes. Journal of Artificial
Intelligence Research, 21:429?470.
Stevan Harnad. 1990. The symbol grounding problem.
Physica D, 42:335?346.
Peter A. Heeman and Graeme Hirst. 1995. Collaborating
on referring expressions. Computational Linguistics,
21:351?382.
Helmut Horacek. 2005. Generating referential descrip-
tions under conditions of uncertainty. In Proceedings
of the 10th European Workshop on Natural Language
Generation (ENLG) pages 58-67, Aberdeen, UK.
Pamela W Jordan and Marilyn Walker. 2000. Learning
attribute selections for non-pronominal expressions.
In Proceedings of the 38th Annual Meeting on Asso-
ciation for Computational Linguistics, pages 181-190.
John D. Kelleher and Geert-Jan M. Kruijff. 2006. In-
cremental generation of spatial referring expressions
in situated dialog. In Proceedings of the 21st Interna-
tional Conference on Computational Linguistics and
the 44th annual meeting of the Association for Com-
putational Linguistics, ACL-44, pages 1041?1048,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Emiel Krahmer and Kees Van Deemter. 2012. Compu-
tational generation of referring expressions: A survey.
computational linguistics, 38(1):173?218.
Emiel Krahmer Krahmer, Sebastiaan van Erk, and Andre?
Verleg. 2003. Graph-based generation of referring
expressions. Computational Linguistics, 29(1):53?72,
March.
Emiel Krahmer, Mariet Theune, Jette Viethen, and Iris
Hendrickx. 2008. Graph: The costs of redundancy
in referring expressions. In In Proceedings of the 5th
International Conference on Natural Language Gen-
eration, Salt Fork OH, USA.
Changsong Liu, Rui Fang, and Joyce Y. Chai. 2012. To-
wards mediating shared perceptual basis in situated di-
alogue. In Proceedings of the 13th Annual Meeting of
401
the Special Interest Group on Discourse and Dialogue,
SIGDIAL ?12, pages 140?149, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Changsong Liu, Rui Fang, Lanbo She, and Joyce Y. Chai.
2013. Modeling collaborative referring for situated
referential grounding. In The 14th Annual SIGdial
Meeting on Discourse and Dialogue.
Margaret Mitchell, Kees van Deemter, and Ehud Reiter.
2011. Two approaches for generating size modifiers.
In Proceedings of the 13th European Workshop on
Natural Language Generation, ENLG ?11, pages 63?
70, Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Margaret Mitchell, Kees van Deemter, and Ehud Reiter.
2013. Generating expressions that refer to visible ob-
jects. In Proceedings of NAACL-HLT 2013, pages
1174-1184.
Aleksandra Mojsilovic. 2005. A computational model
for color naming and describing color composition
of images. IEEE Transactions on Image Processing,
14:690 ? 699.
Terry Regier. 1996. The human semantic potential. The
MIT Press,Cambridge, Massachusetts.
Deb Roy. 2002. Learning visually grounded words and
syntax of natural spoken language. Evolution of Com-
munication, 4.
Alexander Siebert and David Schlangen. 2008. A sim-
ple method for resolution of definite reference in a
shared visual context. In Proceedings of the 9th SIG-
dial Workshop on Discourse and Dialogue, SIGdial
?08, pages 84?87, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Robert Sternberg. 2003. Cognitive Psychology,Third
Edition. Thomson Wadsworth.
Kristina Striegnitz, Hendrik Buschmeier, and Stefan
Kopp. 2012. Referring in installments: a corpus
study of spoken object references in an interactive vir-
tual environment. In Proceedings of the Seventh In-
ternational Natural Language Generation Conference,
INLG ?12, pages 12?16, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.
Thora Tenbrink and Reinhard Moratz. 2003. Group-
based spatial reference in linguistic human-robot in-
teraction. Spatial Cognition and Computation, 6:63?
106.
Marie?t Theune, Pascal Touset, Jette Viethen, and Emiel
Krahmer. 2007. Cost-based attribute selection for gre
(graph-sc/graph-fp). In Proceedings of the MT Summit
XI Workshop on Using Corpora for NLG: Language
Generation and Machine Translation (UCNLG+MT).
Marie?t Theune, Ruud Koolen, Emiel Krahmer, and
Sander Wubben. 2011. Does size matter ? how much
data is required to train a reg algorithm? In Pro-
ceedings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Language
Technologies, pages 660?664, Portland, Oregon, USA,
June. Association for Computational Linguistics.
Kristinn R. Thrisson. 1994. Simulated perceptual group-
ing: An application to human-computer interaction. In
Proceedings of the Sixteenth Annual Conference of the
Cognitive Science Society, pages 876?881.
Jette Viethen and Robert Dale. 2008. The use of spa-
tial relations in referring expression generation. In
Proceedings of the Fifth International Natural Lan-
guage Generation Conference, INLG ?08, pages 59?
67, Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
S. Weijers. 2011. Referring expressions with groups as
landmarks. volume 15. University of Twente.
Max Wertheimer. 1938. Laws of organization in per-
ceptual forms. A Source Book of Gestalt Psychology.
Routledge and Kegan Paul, London.
Dengsheng Zhang and Guojun Lu. 2002. An integrated
approach to shape based image retrieval. In Proc.
of 5th Asian Conference on Computer Vision (ACCV,
pages 652?657.
402
Semantic Role Labeling of Implicit
Arguments for Nominal Predicates
Matthew Gerber?
University of Virginia
Joyce Y. Chai??
Michigan State University
Nominal predicates often carry implicit arguments. Recent work on semantic role labeling has
focused on identifying arguments within the local context of a predicate; implicit arguments,
however, have not been systematically examined. To address this limitation, we have manually
annotated a corpus of implicit arguments for ten predicates from NomBank. Through analysis
of this corpus, we find that implicit arguments add 71% to the argument structures that are
present in NomBank. Using the corpus, we train a discriminative model that is able to identify
implicit arguments with an F1 score of 50%, significantly outperforming an informed baseline
model. This article describes our investigation, explores a wide variety of features important for
the task, and discusses future directions for work on implicit argument identification.
1. Introduction
Recent work has shown that semantic role labeling (SRL) can be applied to nominal
predicates in much the same way as verbal predicates (Liu and Ng 2007; Johansson and
Nugues 2008; Gerber, Chai, and Meyers 2009). In general, the nominal SRL problem
is formulated as follows: Given a predicate that is annotated in NomBank as bear-
ing arguments, identify these arguments within the clause or sentence that contains
the predicate. As shown in our previous work (Gerber, Chai, and Meyers 2009), this
problem definition ignores the important fact that many nominal predicates do not
bear arguments in the local context. Such predicates need to be addressed in order
for nominal SRL to be used by downstream applications such as automatic question
answering, information extraction, and statistical machine translation.
Gerber, Chai, and Meyers (2009) showed that it is possible to accurately identify
nominal predicates that bear arguments in the local context. This makes the nominal
SRL system applicable to text that does not contain annotated predicates. The system
does not address a fundamental question regarding arguments of nominal predicates,
however: If an argument is missing from the local context of a predicate, might the
argument be located somewhere in the wider discourse? Most prior work on nominal
? 151 Engineer?s Way, University of Virginia, Charlottesville, VA 22904.
E-mail: matt.gerber@virginia.edu.
?? 3115 Engineering Building, Michigan State University, East Lansing, MI 48824.
E-mail: jchai@cse.msu.edu.
Submission received: 4 August 2011; revised version received: 23 December 2011; accepted for publication:
7 February 2012.
? 2012 Association for Computational Linguistics
Computational Linguistics Volume 38, Number 4
and verbal SRL has stopped short of answering this question, opting instead for an
approach that only labels local arguments and thus ignores predicates whose arguments
are entirely non-local. This article directly addresses the issue of non-local (or implicit)
argument identification for nominal predicates.
As an initial example, consider the following sentence, which is taken from the Penn
TreeBank (Marcus, Santorini, and Marcinkiewicz 1993):
(1) A SEC proposal to ease [arg1 reporting] [predicate requirements] [arg2
for some company executives] would undermine the usefulness of
information on insider trades, professional money managers contend.
The NomBank (Meyers 2007) role set for requirement is shown here:
Frame for requirement, role set 1:
arg0: the entity that is requiring something
arg1: the entity that is required
arg2: the entity of which something is being required
In Example (1), the predicate has been annotated with the local argument labels pro-
vided by NomBank. As shown, NomBank does not annotate an arg0 for this instance of
the requirement predicate; a reasonable interpretation of the sentence, however, is that
SEC is the entity that is requiring something.1 This article refers to arguments such as
SEC in Example (1) as implicit. In this work, the notion of implicit argument covers any
argument that is not annotated by NomBank.2
Building on Example (1), consider the following sentence, which directly follows
Example (1) in the corresponding TreeBank document:
(2) Money managers make the argument in letters to the agency about
[arg1 rule] [predicate changes] proposed this past summer.
The NomBank role set for change is as follows:
Frame for change, role set 1:
arg0: the entity that initiates the change
arg1: the entity that is changed
arg2: the initial state of the changed entity
arg3: the final state of the changed entity
Similarly to the previous example, Example (2) shows the local argument labels pro-
vided by NomBank. These labels only indicate that rules have been changed. For a
full interpretation, Example (2) requires an understanding of Example (1). Without
1 The Securities and Exchange Commission (SEC) is responsible for enforcing investment laws in the
United States.
2 NomBank annotates arguments in the noun phrase headed by the predicate as well as arguments
brought in by so-called support verb structures. See Meyers (2007) for details.
756
Gerber and Chai SRL of Implicit Arguments for Nominal Predicates
the sentence from Example 1, the reader has no way of knowing that the agency in
Example (2) actually refers to the same entity as SEC in Example (1). As part of the
reader?s comprehension process, this entity is identified as the filler for the arg0 role in
Example (2). This identification must occur in order for these two sentences to form a
coherent discourse.
From these examples, it is clear that the scope of implicit arguments quite naturally
spans sentence boundaries. Thus, if one wishes to recover implicit arguments as part of
the SRL process, the argument search space must be expanded beyond the traditional,
single-sentence window used in virtually all prior SRL research. What can we hope
to gain from such a fundamental modification of the problem? Consider the following
question, which targets Examples (1) and (2):
(3) Who changed the rules regarding reporting requirements?
Question (3) is a factoid question, meaning it has a short, unambiguous answer in the
targeted text. This type of question has been studied extensively in the Text Retrieval
Conference Question Answering (QA) Track (Dang, Kelly, and Lin 2007). Using the
evaluation data from this track, Pizzato and Molla? (2008) showed that SRL can improve
the accuracy of a QA system; a traditional SRL system alone, however, is not enough
to recover the implied answer to Question (3): SEC or the agency. Successful implicit
argument identification provides the answer in this case.
This article presents an in-depth study of implicit arguments for nominal predi-
cates.3 The following section surveys research related to implicit argument identifica-
tion. Section 3 describes the study?s implicit argument annotation process and the data
it produced. The implicit argument identification model is formulated in Section 4 and
evaluated in Section 5. Discussion of results is provided in Section 6, and the article
concludes in Section 7.
2. Related Work
The research presented in this article is related to a wide range of topics in cognitive
science, linguistics, and natural language processing (NLP). This is partly due to the
discourse-based nature of the problem. In single-sentence SRL, one can ignore the dis-
course aspect of language and still obtain high marks in an evaluation (for examples, see
Carreras and Ma`rquez 2005 and Surdeanu et al 2008); implicit argumentation, however,
forces one to consider the discourse context in which a sentence exists. Much has been
said about the importance of discourse to language understanding, and this section
will identify the points most relevant to implicit argumentation.
2.1 Discourse Comprehension in Cognitive Science
The traditional view of sentence-level semantics has been that meaning is composi-
tional. That is, one can derive the meaning of a sentence by carefully composing the
meanings of its constituent parts (Heim and Kratzer 1998). There are counterexamples
to a compositional theory of semantics (e.g., idioms), but those are more the exception
than the rule. Things change, however, when one starts to group sentences together
3 This article builds on our previous work (Gerber and Chai 2010).
757
Computational Linguistics Volume 38, Number 4
to form coherent textual discourses. Consider the following examples, borrowed from
Sanford (1981, page 5):
(4) Jill came bouncing down the stairs.
(5) Harry rushed off to get the doctor.
Examples (4) and (5) describe three events: bounce, rush, and get. These events are
intricately related. One cannot simply create a conjunction of the propositions bounce,
rush, and get and expect to arrive at the author?s intended meaning, which presumably
involves Jill?s becoming injured by her fall and Harry?s actions to help her. The mutual
dependence of these sentences can be further shown by considering a variant of the
situation described in Examples (4) and (5):
(6) Jill came bouncing down the stairs.
(7) Harry rushed over to kiss her.
The interpretation of Example (6) is vastly different from the interpretation of Exam-
ple (4). In Example (4), Jill becomes injured whereas in Example (6) she is quite happy.
Examples (4?7) demonstrate the fact that sentences do not have a fixed, compo-
sitional interpretation; rather, a sentence?s interpretation depends on the surrounding
context. The standard compositional theory of sentential semantics largely ignores con-
textual information provided by other sentences. The single-sentence approach to SRL
operates similarly. In both of these methods, the current sentence provides all of the
semantic information. In contrast to these methods?and aligned with the preceding
discussion?this article presents methods that rely heavily on surrounding sentences
to provide additional semantic information. This information is used to interpret the
current sentence in a more complete fashion.
Examples (4?7) also show that the reader?s knowledge plays a key role in discourse
comprehension. Researchers in cognitive science have proposed many models of reader
knowledge. Schank and Abelson (1977) proposed stereotypical event sequences called
scripts as a basis for discourse comprehension. In this approach, readers fill in a dis-
course?s semantic gaps with knowledge of how a typical event sequence might unfold.
In Examples (4) and (5), the reader knows that people typically call on a doctor only
if someone is hurt. Thus, the reader automatically fills the semantic gap caused by the
ambiguous predicate bounce with information about doctors and what they do. Similar
observations have been made by van Dijk (1977, page 4), van Dijk and Kintsch (1983,
page 303), Graesser and Clark (1985, page 14), and Carpenter, Miyake, and Just (1995).
Inspired by these ideas, the model developed in this article relies partly on large text
corpora, which are treated as repositories of typical event sequences. The model uses
information extracted from these event sequences to identify implicit arguments.
2.2 Automatic Relation Discovery
Examples (4) and (5) in the previous section show that understanding the relationships
between predicates is a key part of understanding a textual discourse. In this section, we
review work on automatic predicate relationship discovery, which attempts to extract
these relationships automatically.
758
Gerber and Chai SRL of Implicit Arguments for Nominal Predicates
Lin and Pantel (2001) proposed a system that automatically identifies relationships
similar to the following:
(8) X eats Y ? X likes Y
This relationship creates a mapping between the participants of the two predicates.
One can imagine using such a mapping to fill in the semantic gaps of a discourse that
describes a typical set of events in a restaurant. In such a discourse, the author probably
will not state directly that X likes Y; the reader might need to infer this in order to make
sense of the fact that X left a large tip for the waiter, however.
Lin and Pantel (2001) created mappings such as the one in Example (8) using a
variation of the so-called ?distributional hypothesis? posited by Harris (1985), which
states that words occurring in similar contexts tend to have similar meanings. Lin and
Pantel applied the same notion of similarity to dependency paths. For example, the
inference rule in Example 8 is identified by examining the sets of words in the two X
positions and the sets of words in the two Y positions. When the two pairs of sets are
similar, it is implied that the two dependency paths from X to Y are similar as well. In
Example (8), the two dependency paths are as follows:
X
subject???? eats object???? Y
X
subject???? likes object???? Y
One drawback of this method is that it assumes the implication is symmetric. Although
this assumption is correct in many cases, it often leads to invalid inferences. In Exam-
ple 8, it is not always true that if X likes Y then X will eat Y. The opposite?that X eating
Y implies X likes Y?is more plausible but not certain.
Bhagat, Pantel, and Hovy (2007) extended the work of Lin and Pantel (2001) to
handle cases of asymmetric relationships. The basic idea proposed by Bhagat, Pantel,
and Hovy is that, when considering a relationship of the form ?x, p1, y? ? ?x, p2, y?, if p1
occurs in significantly more contexts (i.e., has more options for x and y) than p2, then p2
is likely to imply p1 but not vice versa. Returning to Example 8, we see that the correct
implication will be derived if likes occurs in significantly more contexts than eats. The
intuition is that the more general concept (i.e., like) will be associated with more contexts
and is more likely to be implied by the specific concept (i.e., eat). As shown by Bhagat,
Pantel, and Hovy, the system built around this intuition is able to effectively identify
the directionality of many inference rules.
Zanzotto, Pennacchiotti, and Pazienza (2006) presented another study aimed at
identifying asymmetric relationships between verbs. For example, the asymmetric en-
tailment relationship X wins ?? X plays holds, but the opposite (X plays ?? X wins) does
not. This is because not all those who play a game actually win. To find evidence for
this automatically, the authors examined constructions such as the following (adapted
from Zanzotto, Pennacchiotti, and Pazienza [2006]):
(9) The more experienced tennis player won the match.
The underlying idea behind the authors? approach is that asymmetric relationships such
as X wins ?? X plays are often entailed by constructions involving agentive, nominalized
verbs as the logical subjects of the main verb. In Example (9), the agentive nominal
759
Computational Linguistics Volume 38, Number 4
?player? is logical subject to ?won?, the combination of which entails the asymmetric
relationship of interest. Thus, to validate such an asymmetric relationship, Zanzotto,
Pennacchiotti, and Pazienza (2006) examined the frequency of the ?player win? colloca-
tion using Google hit counts as a proxy for actual corpus statistics.
A number of other studies (e.g., Szpektor et al 2004, Pantel et al 2007) have been
conducted that are similar to that work. In general, such work focuses on the automatic
acquisition of entailment relationships between verbs. Although this work has often
been motivated by the need for lexical?semantic information in tasks such as automatic
question answering, it is also relevant to the task of implicit argument identification
because the derived relationships implicitly encode a participant role mapping between
two predicates. For example, given a missing arg0 for a like predicate and an explicit
arg0 = John for an eat predicate in the preceding discourse, inference rule (8) would
help identify the implicit arg0 = John for the like predicate.
The missing link between previous work on verb relationship identification and the
task of implicit argument identification is that previous verb relations are not defined
in terms of the argn positions used by NomBank. Rather, positions like subject and object
are used. In order to identify implicit arguments in NomBank, one needs inference rules
between specific argument positions (e.g., eat:arg0 and like:arg0). In the current article,
we propose methods of automatically acquiring these fine-grained relationships for
verbal and nominal predicates using existing corpora. We also propose a method of
using these relationships to recover implicit arguments.
2.3 Coreference Resolution
The referent of a linguistic expression is the real or imagined entity to which the expres-
sion refers. Coreference, therefore, is the condition of two linguistic expressions having
the same referent. In the following examples from the Penn TreeBank, the underlined
spans of text are coreferential:
(10) ?Carpet King sales are up 4% this year,? said owner Richard Rippe.
(11) He added that the company has been manufacturing carpet since 1967.
Non-trivial instances of coreference (e.g., Carpet King and the company) allow the author
to repeatedly mention the same entity without introducing redundancy into the dis-
course. Pronominal anaphora is a subset of coreference in which one of the referring
expressions is a pronoun. For example, he in Example (11) refers to the same entity as
Richard Rippe in Example (10). These examples demonstrate noun phrase coreference.
Events, indicated by either verbal or nominal predicates, can also be coreferential when
mentioned multiple times in a document (Wilson 1974; Chen and Ji 2009).
For many years, the Automatic Content Extraction (ACE) series of large-scale eval-
uations (NIST 2008) has provided a test environment for systems designed to identify
these and other coreference relations. Systems based on the ACE data sets typically take
a supervised learning approach to coreference resolution in general (Versley et al 2008)
and pronominal anaphor in particular (Yang, Su, and Tan 2008).
A phenomenon similar to the implicit argument has been studied in the context
of Japanese anaphora resolution, where a missing case-marked constituent is viewed
as a zero-anaphoric expression whose antecedent is treated as the implicit argument
of the predicate of interest. This behavior has been annotated manually by Iida et al
(2007), and researchers have applied standard SRL techniques to this corpus, resulting
760
Gerber and Chai SRL of Implicit Arguments for Nominal Predicates
in systems that are able to identify missing case?marked expressions in the surrounding
discourse (Imamura, Saito, and Izumi 2009). Sasano, Kawahara, and Kurohashi (2004)
conducted similar work with Japanese indirect anaphora. The authors used automati-
cally derived nominal case frames to identify antecedents. However, as noted by Iida
et al, grammatical cases do not stand in a one-to-one relationship with semantic roles
in Japanese (the same is true for English).
Many other discourse-level phenomena interact with coreference. For example,
Centering Theory (Grosz, Joshi, and Weinstein 1995) focuses on the ways in which
referring expressions maintain (or break) coherence in a discourse. These so-called ?cen-
tering shifts? result from a lack of coreference between salient noun phrases in adjacent
sentences. Discourse Representation Theory (DRT) (Kamp and Reyle 1993) is another
prominent treatment of referring expressions. DRT embeds a theory of coreference into
a first-order, compositional semantics of discourse.
2.4 Identifying Implicit Arguments
Past research on the actual task of implicit argument identification tends to be sparse.
Palmer et al (1986) describe what appears to be the first computational treatment of
implicit arguments. In that work, Palmer et al manually created a repository of knowl-
edge concerning entities in the domain of electronic device failures. This knowledge,
along with hand-coded syntactic and semantic processing rules, allowed the system to
identify implicit arguments across sentence boundaries. As a simple example, consider
the following two sentences (borrowed from Palmer et al [1986]):
(12) Disk drive was down at 11/16-2305.
(13) Has select lock.
Example (13) does not specify precisely which entity has select lock. The domain knowl-
edge, however, tells the system that only disk drive entities can have such a property.
Using this knowledge, the system is able to search the local context and make explicit
the implied fact that the disk drive from Example (12) has select lock.
A similar line of work was pursued by Whittemore, Macpherson, and Carlson
(1991), who offer the following example of implicit argumentation (page 21):
(14) Pete bought a car.
(15) The salesman was a real jerk.
In Example (14), the buy event is not associated with an entity representing the seller.
This entity is introduced in Example (15) as the salesman, whose semantic properties
satisfy the requirements of the buy event. Whittemore, Macpherson, and Carlson (1991)
build up the event representation incrementally using a combination of semantic prop-
erty constraints and DRT.
The systems developed by Palmer et al (1986) and Whittemore, Macpherson,
and Carlson (1991) are quite similar. They both make use of semantic constraints on
arguments, otherwise known as selectional preferences. Selectional preferences have
received a significant amount of attention over the years, with the work of Ritter,
Mausam, and Etzioni (2010) being some of the most recent. The model developed in
761
Computational Linguistics Volume 38, Number 4
the current article uses a variety of selectional preference measures to identify implicit
arguments.
The implicit argument identification systems described herein were not widely de-
ployed due to their reliance on hand-coded, domain-specific knowledge that is difficult
to create. Much of this knowledge targeted basic syntactic and semantic constructions
that now have robust statistical models (e.g., those created by Charniak and Johnson
[2005] for syntax and Punyakanok et al [2005] for semantics). With this information
accounted for, it is easier to approach the problem of implicit argumentation. Subse-
quently, we describe a series of recent investigations that have led to a surge of interest
in statistical implicit argument identification.
Fillmore and Baker (2001) provided a detailed case study of FrameNet frames as a
basis for understanding written text. In their case study, Fillmore and Baker manually
build up a semantic discourse structure by hooking together frames from the various
sentences. In doing so, the authors resolve some implicit arguments found in the dis-
course. This process is an interesting step forward; the authors did not provide concrete
methods to perform the analysis automatically, however.
Nielsen (2004) developed a system that is able to detect the occurrence of verb
phrase ellipsis. Consider the following sentences:
(16) John kicked the ball.
(17) Bill [did], too.
The bracketed text in Example (17) is a placeholder for the verb phrase kicked the ball
in Example (16), which has been elided (i.e., left out). Thus, in Example (17), Bill can
be thought of as an implicit argument to some kicking event that is not mentioned. If
one resolved the verb phrase ellipsis, then the implicit agent (Bill) would be recovered.4
Nielsen (2004) created a system able to detect the presence of ellipses, producing the
bracketing in Example (17). Ellipsis resolution (i.e., figuring out precisely which verb
phrase is missing) was described by Nielsen (2005). Implicit argument identification for
nominal predicates is complementary to verb phrase ellipsis resolution: Both work to
make implicit information explicit.
Burchardt, Frank, and Pinkal (2005) suggested that frame elements from various
frames in a text could be linked to form a coherent discourse interpretation (this is
similar to the idea described by Fillmore and Baker [2001]). The linking operation
causes two frame elements to be viewed as coreferent. Burchardt, Frank, and Pinkal
(2005) propose to learn frame element linking patterns from observed data; the authors
did not implement and evaluate such a method, however. Building on the work of
Burchardt, Frank, and Pinkal, this article presents a model of implicit arguments that
uses a quantitative analysis of naturally occurring coreference patterns.
In our previous work, we demonstrated the importance of filtering out nominal
predicates that take no local arguments (Gerber, Chai, and Meyers 2009). This approach
leads to appreciable gains for certain nominals. The approach does not attempt to
actually recover implicit arguments, however.
4 Identification of the implicit patient in Example (17) (the ball) should be sensitive to the phenomenon of
sense anaphora. If Example (16) was changed to ?a ball,? then we would have no implicit patient in
Example (17).
762
Gerber and Chai SRL of Implicit Arguments for Nominal Predicates
Most recently, Ruppenhofer et al (2009) proposed SemEval Task 10, ?Linking
Events and Their Participants in Discourse,? which evaluated implicit argument iden-
tification systems over a common test set. The task organizers annotated implicit
arguments across entire passages, resulting in data that cover many distinct predi-
cates, each associated with a small number of annotated instances. As described by
Ruppenhofer et al (2010), three submissions were made to the competition, with two of
the submissions attempting the implicit argument identification part of the task. Chen
et al (2010) extended a standard SRL system by widening the candidate window to
include constituents from other sentences. A small number of features based on the
FrameNet frame definitions were extracted for these candidates, and prediction was
performed using a log-linear model. Tonelli and Delmonte (2010) also extended a stan-
dard SRL system. Both of these systems achieved an implicit argument F1 score of less
than 0.02. The organizers and participants appear to agree that training data sparseness
was a significant problem. This is likely the result of the annotation methodology: Entire
documents were annotated, causing each predicate to receive a very small number of
annotated examples.
In contrast to the evaluation described by Ruppenhofer et al (2010), the study
presented in this article focused on a select group of nominal predicates. To help prevent
data sparseness, the size of the group was small, and the predicates were carefully
chosen to maximize the observed frequency of implicit argumentation. We annotated a
large number of implicit arguments for this group of predicates with the goal of training
models that generalize well to the testing data. In the following section, we describe
the implicit argument annotation process and resulting data set.
3. Implicit Argument Annotation and Analysis
As shown in the previous section, the existence of implicit arguments has been rec-
ognized for quite some time. This type of information, however, was not formally
annotated until Ruppenhofer et al (2010) conducted their SemEval task on implicit
argument identification. There are two reasons why we chose to create an independent
data set for implicit arguments. The first reason is the aforementioned sparsity of the
SemEval data set. The second reason is that the SemEval data set is not built on top of
the Penn TreeBank, which is the gold-standard syntactic base for all work in this article.
Working on top of the Penn TreeBank makes the annotations immediately compatible
with PropBank, NomBank, and a host of other resources that also build on the TreeBank.
3.1 Data Annotation
3.1.1 Predicate Selection. Implicit arguments are a relatively new subject of annotation in
the field. To effectively use our limited annotation resources and allow the observation
of interesting behaviors, we decided to focus on a select group of nominal predicates.
Predicates in this group were required to meet the following criteria:
1. A selected predicate must have an unambiguous role set. This criterion
corresponds roughly to an unambiguous semantic sense and is motivated
by the need to separate the implicit argument behavior of a predicate from
its semantic meaning.
2. A selected predicate must be derived from a verb. This article focuses
primarily on the event structure of texts. Nominal predicates derived
from verbs denote events, but there are other, non-eventive predicates in
763
Computational Linguistics Volume 38, Number 4
NomBank (e.g., the partitive predicate indicated by the ?%? symbol).
This criterion also implies that the annotated predicates have correlates
in PropBank with semantically compatible role sets.
3. A selected predicate should have a high frequency in the Penn TreeBank
corpus. This criterion ensures that the evaluation results say as much as
possible about the event structure of the underlying corpus. We calculated
frequency with basic counting over morphologically normalized
predicates (i.e., bids and bid are counted as the same predicate).
4. A selected predicate should express many implicit arguments. Of course,
this can only be estimated ahead of time because no data exist to compute
it. To estimate this value for a predicate p, we first calculated Np, the
average number of roles expressed by p in NomBank. We then calculated
Vp, the average number of roles expressed by the verb form of p in
PropBank. We hypothesized that the difference Vp ? Np gives an
indication of the number of implicit arguments that might be present in
the text for a nominal instance of p. The motivation for this hypothesis is as
follows. Most verbs must be explicitly accompanied by specific arguments
in order for the resulting sentence to be grammatical. The following
sentences are ungrammatical if the parenthesized portion is left out:
(18) *John loaned (the money to Mary).
(19) *John invested (his money).
Examples (18) and (19) indicate that certain arguments must explicitly
accompany loan and invest. In nominal form, these predicates can exist
without such arguments and still be grammatical:
(20) John?s loan was not repaid.
(21) John?s investment was huge.
Note, however, that Examples (20) and (21) are not reasonable things to
write unless the missing arguments were previously mentioned in the text.
This is precisely the type of noun that should be targeted for implicit
argument annotation. The value of Vp ? Np thus quantifies the desired
behavior.
Predicates were filtered according to criteria 1 and 2 and ranked according to the
product of criteria 3 and 4. We then selected the top ten, which are shown in the first
column of Table 1. The role sets (i.e., argument definitions) for these predicates can be
found in the Appendix on page 790.
3.1.2 Annotation Procedure. We annotated implicit arguments for instances of the ten se-
lected nominal predicates. The annotation process proceeded document-by-document.
For a document d, we annotated implicit arguments as follows:
1. Select from d all non-proper singular and non-proper plural nouns that are
morphologically related to the ten predicates in Table 1.
764
G
erber
an
d
C
h
ai
SR
L
of
Im
p
licitA
rgu
m
en
ts
for
N
om
in
alP
red
icates
Table 1
Annotation data analysis. Columns are defined as follows: (1) the annotated predicate, (2) the number of predicate instances that were annotated,
(3) the average number of implicit arguments per predicate instance, (4) of all roles for all predicate instances, the percentage filled by NomBank
arguments, (5) the average number of NomBank arguments per predicate instance, (6) the average number of PropBank arguments per instance of the
verb form of the predicate, (7) of all roles for all predicate instances, the percentage filled by either NomBank or implicit arguments, (8) the average
number of combined NomBank/implicit arguments per predicate instance. SD indicates the standard deviation with respect to an average.
Pre-annotation Post-annotation
Role avg. (SD)
Pred. # Pred. # Imp./pred. Role coverage (%) Noun Verb Role coverage (%) Noun role avg. (SD)
bid 88 1.4 26.9 0.8 (0.6) 2.2 (0.6) 73.9 2.2 (0.9)
sale 184 1.0 24.2 1.2 (0.7) 2.0 (0.7) 44.0 2.2 (0.9)
loan 84 1.0 22.1 1.1 (1.1) 2.5 (0.5) 41.7 2.1 (1.1)
cost 101 0.9 26.2 1.0 (0.7) 2.3 (0.5) 47.5 1.9 (0.6)
plan 100 0.8 30.8 1.2 (0.8) 1.8 (0.4) 50.0 2.0 (0.4)
investor 160 0.7 35.0 1.1 (0.2) 2.0 (0.7) 57.5 1.7 (0.6)
price 216 0.6 42.5 1.7 (0.5) 1.7 (0.5) 58.6 2.3 (0.6)
loss 104 0.6 33.2 1.3 (0.9) 2.0 (0.6) 48.1 1.9 (0.7)
investment 102 0.5 15.7 0.5 (0.7) 2.0 (0.7) 33.3 1.0 (1.0)
fund 108 0.5 8.3 0.3 (0.7) 2.0 (0.3) 21.3 0.9 (1.2)
Overall 1,247 0.8 28.0 1.1 (0.8) 2.0 (0.6) 47.8 1.9 (0.9)
(1) (2) (3) (4) (5) (6) (7) (8)
765
Computational Linguistics Volume 38, Number 4
2. By design, each selected noun has an unambiguous role set. Thus,
given the arguments supplied for a noun by NomBank, one can consult
the noun?s role set to determine which arguments are missing.5
3. For each missing argument position, search the current sentence and
all preceding sentences for a suitable implicit argument. Annotate all
suitable implicit arguments in this window.
4. When possible, match the textual bounds of an implicit argument
to the textual bounds of an argument given by either PropBank or
NomBank. This was done to maintain compatibility with these and
other resources.
In the remainder of this article, we will use iargn to refer to an implicit argument
position n. We will use argn to refer to an argument provided by PropBank or NomBank.
We will use p to mark predicate instances. Example (22) provides a sample annotation
for an instance of the investment predicate:
(22) [iarg0 Participants] will be able to transfer [iarg1 money] to [iarg2 other
investment funds]. The [p investment] choices are limited to [iarg2 a stock
fund and a money-market fund].
NomBank does not associate this instance of investment with any arguments; one can
easily identify the investor (iarg0), the thing invested (iarg1), and two mentions of the
thing invested in (iarg2) within the surrounding discourse, however.
Of course, not all implicit argument decisions are as easy as those in Example (22).
Consider the following contrived example:
(23) People in other countries could potentially consume large amounts of
[iarg0? Coke].
(24) Because of this, there are [p plans] to expand [iarg0 the company?s]
international presence.
Example (24) contains one mention of the iarg0 (the agentive planner). It might be
tempting to also mark Coke in Example (23) as an additional iarg0; the only reasonable
interpretation of Coke in 23 is as a consumable fluid, however. Fluids cannot plan things,
so this annotation should not be performed. This is a case of sense ambiguity between
Coke as a company and Coke as a drink. In all such cases, the annotator was asked to
infer the proper sense before applying an implicit argument label.
Lastly, it should be noted that we placed no restrictions on embedded argu-
ments. PropBank and NomBank do not allow argument extents to overlap. Tra-
ditional SRL systems such as the one created by Punyakanok, Roth, and Yih
(2008) model this constraint explicitly to arrive at the final label assignment; as the
5 See Appendix A for the list of role sets used in this study.
766
Gerber and Chai SRL of Implicit Arguments for Nominal Predicates
following example shows, however, this constraint should not be applied to implicit
arguments:
(25) Currently, the rules force [iarg0 executives, directors and other corporate
insiders] to report purchases and [p sales] [arg1 of [iarg0 their] companies?
shares] within about a month after the transaction.
Despite its embedded nature, the pronoun their in Example (25) is a perfectly reasonable
implicit argument (the seller) for the marked predicate. Systems should be required to
identify such arguments; thus, we included them in our annotations.
3.1.3 Inter-annotator Agreement. Implicit argument annotation is a difficult task because
it combines the complexities of traditional SRL annotation with those of coreference
annotation. To assess the reliability of the annotation process described previously, we
compared our annotations to those provided by an undergraduate linguistics student
who, after a brief training period, re-annotated a portion of the data set. For each miss-
ing argument position, the student was asked to identify the textually closest acceptable
implicit argument within the current and preceding sentences. The argument position
was left unfilled if no acceptable constituent could be found. For a missing argument
position iargn, the student?s annotation agreed with our own if both identified the same
implicit argument or both left iargn unfilled. The student annotated 480 of the 1,247
predicate instances shown in Table 1.
We computed Cohen?s chance-corrected kappa statistic for inter-annotator agree-
ment (Cohen 1960), which is based on two quantities:
po = observed probability of agreement
pc = probability of agreement by chance
The quantity 1 ? pc indicates the probability of a chance disagreement. The quantity
po ? pc indicates the probability of agreement that cannot be accounted for by chance
alone. Finally, Cohen defines ? as follows:
? =
po ? pc
1 ? pc
Cohen?s kappa thus gives the probability that a chance-expected disagreement will not
occur. When agreement is perfect, ? = 1. If the observed agreement is less than the
expected chance agreement, then ? will be negative. As noted by Di Eugenio and Glass
(2004), researchers have devised different scales to assess ?. Many NLP researchers use
the scale created by Krippendorff (1980):
? < 0.67 low agreement
0.67 ? ? < 0.8 moderate agreement
? ? 0.8 strong agreement
Di Eugenio and Glass (2004) note, however, that this scale has not been rigorously
defended, even by Krippendorff (1980) himself.
767
Computational Linguistics Volume 38, Number 4
For the implicit argument annotation data, observed and chance agreement are
defined as follows:
po =
?
iargn
agree(iargn)
N
pc =
?
iargn
PA(n) ? PB(n) ? random agree(iargn) + (1 ? PA(n)) ? (1 ? PB(n))
N (1)
where N is the total number of missing argument positions that need to be annotated,
agree is equal to 1 if the two annotators agreed on iargn and 0 otherwise, PA(n) and PB(n)
are the observed prior probabilities that annotators A and B assign the argument label n
to a filler, and random agree is equal to the probability that both annotators would select
the same implicit argument for iargn when choosing randomly from the discourse. In
Equation (1), terms to the right of + denote the probability that the two annotators
agreed on iargn because they did not identify a filler for it.
Using these values for po and pc, Cohen?s kappa indicated an agreement of 64.3%.
According to the scale of Krippendorff (1980), this value is borderline between low and
moderate agreement. Possible causes for this low agreement include the brief training
period for the linguistics student and the sheer complexity of the annotation task. If one
considers only those argument positions for which both annotators actually located an
implicit filler, Cohen?s kappa indicates an agreement of 93.1%. This shows that much
of the disagreement concerned the question of whether a filler was present. Having
agreed that a filler was present, the annotators consistently selected the same filler.
Subsequently, we demonstrate this situation with actual data. First, we present our
annotations for two sentences from the same document:
(26) Shares of UAL, the parent of [iarg1 United Airlines], were extremely active
all day Friday, reacting to news and rumors about the proposed [iarg2 $6.79
billion] buy-out of [iarg1 the airline] by an employee?management group.
(27) And 10 minutes after the UAL trading halt came news that the UAL group
couldn?t get financing for [arg0 its] [p bid].
In Example (27), the predicate is marked along with the explicit arg0 argument. Our
task is to locate the implicit iarg1 (the entity bid for) and the implicit iarg2 (the amount
of the bid). We were able to locate these entities in a previous sentence (Example (26)).
Next, we present the second annotator?s (i.e., the student?s) annotations for the same
two sentences:
(28) Shares of UAL, the parent of [iarg1 United Airlines], were extremely active
all day Friday, reacting to news and rumors about the proposed $6.79
billion buy-out of [iarg1 the airline] by an employee?management group.
(29) And 10 minutes after the UAL trading halt came news that the UAL group
couldn?t get financing for [arg0 its] [p bid].
768
Gerber and Chai SRL of Implicit Arguments for Nominal Predicates
As shown in Example (28), the second annotator agreed with our identification of the
iarg1; the second annotator did not mark an implicit iarg2, however, despite the fact that
it can be inferred. We believe this type of error can be addressed with additional train-
ing. The student?s annotations were only used to compute agreement. We performed
all training and evaluation using randomized cross-validation over the annotations we
created.
3.2 Annotation Analysis
We carried out this annotation process on the standard training (2?21), development
(24), and testing (23) sections of the Penn TreeBank. Table 1 summarizes the results. In
this section, we highlight key pieces of information found in this table.
3.2.1 Implicit Arguments are Frequent. Column (3) of Table 1 shows that most predicate
instances are associated with at least one implicit argument. Implicit arguments vary
across predicates, with bid exhibiting (on average) more than one implicit argument
per instance versus the 0.5 implicit arguments per instance of the investment and fund
predicates. It turned out that the latter two predicates have unique senses that preclude
implicit argumentation (more on this in Section 6).
3.2.2 Implicit Arguments Create Fuller Event Descriptions. Role coverage for a predicate
instance is equal to the number of filled roles divided by the number of roles in the
predicate?s role set. Role coverage for the marked predicate in Example (22) is 0/3 for
NomBank-only arguments and 3/3 when the annotated implicit arguments are also
considered. Returning to Table 1, the fourth column gives role coverage percentages
for NomBank-only arguments. The seventh column gives role coverage percentages
when both NomBank arguments and the annotated implicit arguments are considered.
Overall, the addition of implicit arguments created a 71% relative (20-point absolute)
gain in role coverage across the 1,247 predicate instances that we annotated.
3.2.3 The Vp ? Np Predicate Selection Metric Behaves as Desired. The predicate selection
method used the Vp ? Np metric to identify predicates whose instances are likely to take
implicit arguments. Column (5) in Table 1 shows that (on average) nominal predicates
have 1.1 arguments in NomBank; this compared to the 2.0 arguments per verbal form
of the predicates in PropBank (compare columns (5) and (6)). We hypothesized that
this difference might indicate the presence of approximately one implicit argument per
predicate instance. This hypothesis is confirmed by comparing columns (6) and (8):
When considering implicit arguments, many nominal predicates express approximately
the same number of arguments on average as their verbal counterparts.
3.2.4 Most Implicit Arguments Are Nearby. In addition to these analyses, we examined the
location of implicit arguments in the discourse. Figure 1 shows that approximately 56%
of the implicit arguments in our data can be resolved within the sentence containing
the predicate. Approximately 90% are found within the previous three sentences. The
remaining implicit arguments require up to 4?6 sentences for resolution. These obser-
vations are important; they show that searching too far back in the discourse is likely to
769
Computational Linguistics Volume 38, Number 4
Figure 1
Location of implicit arguments. Of all implicitly filled argument positions, the y-axis indicates
the percentage that are filled at least once within the number of sentences indicated by the x-axis
(multiple fillers may exist for the same position).
produce many false positives without a significant increase in recall. Section 6 discusses
additional implications of this skewed distribution.
4. Implicit Argument Model
4.1 Model Formulation
Given a nominal predicate instance p with a missing argument position iargn, the task
is to search the surrounding discourse for a constituent c that fills iargn. The implicit
argument model conducts this search over all constituents that are marked with a core
argument label (arg0, arg1, etc.) associated with a NomBank or PropBank predicate.
Thus, the model assumes a pipeline organization in which a document is initially
analyzed by traditional verbal and nominal SRL systems. The core arguments from
this stage then become candidates for implicit argumentation. Adjunct arguments are
excluded.
A candidate constituent c will often form a coreference chain with other constituents
in the discourse. Consider the following abridged sentences, which are adjacent in their
Penn TreeBank document:
(30) [Mexico] desperately needs investment.
(31) Conservative Japanese investors are put off by [Mexico?s] investment
regulations.
(32) Japan is the fourth largest investor in [c Mexico], with 5% of the total
[p investments].
NomBank does not associate the labeled instance of investment with any arguments, but
it is clear from the surrounding discourse that constituent c (referring to Mexico) is the
thing being invested in (the iarg2). When determining whether c is the iarg2 of investment,
770
Gerber and Chai SRL of Implicit Arguments for Nominal Predicates
Table 2
Primary feature groups used by the model. The third column gives the number of features in the
group, and the final column gives the number of features from the group that were ranked in the
top 20 among all features.
Feature group Resources used # Top 20
(1) Textual semantics PropBank, NomBank 13 4
(2) Ontologies FrameNet, VerbNet, WordNet 8 4
(3) Filler-independent Penn TreeBank 35 7
(4) Corpus statistics Gigaword, Verbal SRL, Nominal SRL 9 1
(5) Textual discourse Penn Discourse Bank 1 0
(6) Other Penn TreeBank 15 4
one can draw evidence from other mentions in c?s coreference chain. Example (30) states
that Mexico needs investment. Example (31) states that Mexico regulates investment.
These propositions, which can be derived via traditional SRL analyses, should increase
our confidence that c is the iarg2 of investment in Example (32).
Thus, the unit of classification for a candidate constituent c is the three-tuple
?p, iargn, c??, where c? is a coreference chain comprising c and its coreferent constituents.6
We defined a binary classification function Pr(+| ?p, iargn, c??) that predicts the probabil-
ity that the entity referred to by c fills the missing argument position iargn of predicate
instance p. In the remainder of this article, we will refer to c as the primary filler,
differentiating it from other mentions in the coreference chain c?. This distinction is
necessary because our evaluation requires the model to select at most one filler (i.e., c)
for each missing argument position. In the following section, we present the feature
set used to represent each three-tuple within the classification function.
4.2 Model Features
Appendix Table B.1 lists all features used by the model described in the previous
section. For convenience, Table 2 presents a high-level grouping of the features and
the resources used to compute them. The broadest distinction to be made is whether a
feature depends on elements of c?. Features in Group 3 do not, whereas all others do.
The features in Group 3 characterize the predicate?argument position being filled (p and
iargn), independently of the candidate filler. This group accounts for 43% of the features
and 35% of those in the top 20.7 The remaining features depend on elements of c? in some
way. Group 1 features characterize the tuple using the SRL propositions contained in
the text being evaluated. Group 2 features place the ?p, iargn, c?? tuple into a manually
constructed ontology and compute a value based on the structure of that ontology.
Group 4 features compute statistics of the tuple within a large corpus of semantically
analyzed text. Group 5 contains a single feature that captures the discourse structure
properties of the tuple. Group 6 contains all other features, most of which capture
the syntactic relationships between elements of c? and p. In the following sections, we
provide detailed examples of features from each group shown in Table 2.
6 We used OpenNLP for coreference identification: http://opennlp.sourceforge.net.
7 Features were ranked according to the order in which they were selected during feature selection
(Section 5.3 for details).
771
Computational Linguistics Volume 38, Number 4
4.2.1 Group 1: Features Derived from the Semantic Content of the Text. Feature 1 was often
selected first by the feature selection algorithm. This feature captures the semantic
properties of the candidate filler c? and the argument position being filled. Consider
the following Penn TreeBank sentences:
(33) [arg0 The two companies] [p produce] [arg1 market pulp, containerboard
and white paper]. The goods could be manufactured closer to customers,
saving [p shipping] costs.
Here we are trying to fill the iarg0 of shipping. Let c? contain a single mention, The two
companies, which is the arg0 of produce. Feature 1 takes a value of produce ? arg0 ? ship ?
arg0. This value, which is derived from the text itself, asserts that producers are also
shippers. To reduce data sparsity, we generalized the predicates to their WordNet synset
IDs (creating Feature 4). We also generalized the predicates and arguments to their
VerbNet thematic roles using SemLink (creating Feature 23). Although the generalized
features rely on ontologies, they do so in a trivial way that does not take advantage of
the detailed structure of the ontologies. Such structure is used by features in the next
group.
4.2.2 Group 2: Features Derived from Manually Constructed Ontologies. Feature 9 captures
the semantic relationship between predicate?argument positions by examining paths
between frame elements in FrameNet. SemLink8 maps PropBank argument positions to
their FrameNet frame elements. For example, the arg1 position of sell maps to the Goods
frame element of the Sell frame. NomBank argument positions (e.g., arg1 of sale) can
be mapped to FrameNet by first converting the nominal predicate to its verb form. By
mapping predicate?argument structures into FrameNet, one can take advantage of the
rich network of frame?frame relations provided by the resource.
The value of Feature 9 has the following general form:
(34) Frame1.FE1
Rel1???? Frame2.FE2
Rel2???? . . .
Reln?1?????? Framen.FEn
This path describes how the frame elements at either end are related. For example,
consider the frame element path between the arg1 of sell and the arg1 of buy, both of
which denote the goods being transferred:
(35) Sell.Goods Inherits????? Give.Theme Causes???? Get.Theme Inherited by??????? Buy.Goods
This path can be paraphrased as follows: things that are sold (Sell.Goods) are part of
a more general give scenario (Give.Theme) that can also be viewed as a get scenario
(Get.Theme) in which the buyer receives something (Buy.Goods). This complex world
knowledge is represented compactly using the relationships defined in FrameNet. In
our experiments, we searched all possible frame element paths of length five or less
that use the following relationships:
 Causative?of
8 http://verbs.colorado.edu/semlink.
772
Gerber and Chai SRL of Implicit Arguments for Nominal Predicates
 Inchoative?of
 Inherits
 Precedes
 Subframe?of
Feature 9 is helpful in situations such as the following (contrived):
(36) Consumers bought many [c cars] this year at reduced prices.
(37) [p Sales] are expected to drop when the discounts are eliminated.
In Example (37) we are looking for the iarg1 (thing sold) of sale. The path shown in
Example (35) indicates quite clearly that the candidate cars from Example (36), being
the entity purchased, is a suitable filler for this position. Lastly, note that the value
for Feature 9 is the actual path instead of a numeric value. When c? contains multiple
coreferential elements, this feature can be instantiated using multiple values (i.e., paths).
Ultimately, these instantiations are binarized into the LibLinear input format, so the
existence of multiple feature values does not pose a problem.
Feature 59 is similar to Feature 9 (the frame element path) except that it cap-
tures the distance between predicate?argument positions within the VerbNet hierarchy.
Consider the following VerbNet classes:
13.2 lose, refer, relinquish, remit, resign, restore, gift, hand out, pass out, shell out
13.5.1.1 earn, fetch, cash, gain, get, save, score, secure, steal
The path from earn to lose in the VerbNet hierarchy is as follows:
(38) 13.5.1.1 ? 13.5.1 ? 13.5 ? 13 ? 13.2
The path in Example (38) is four links long. Intuitively, earn and lose are related to each
other?they describe two possible outcomes of a financial transaction. The VerbNet path
quantifies this intuition, with shorter paths indicating closer relationships. This informa-
tion can be used to identify implicit arguments in situations such as the following from
the Penn TreeBank (abridged):
(39) [c Monsanto Co.] is expected to continue reporting higher [p earnings].
(40) The St. Louis-based company is expected to report that [p losses] are
narrowing.
In Example (40) we are looking for the iarg0 (i.e., entity losing something) for the loss
predicate. According to SemLink, this argument position maps to the 13.2.Agent role in
VerbNet. In Example (39), we find the candidate implicit argument Monsanto Co., which
is the arg0 to the earning predicate in that sentence. This argument position maps to
the 13.5.1.1.Agent role in VerbNet. These two VerbNet roles are related according to the
VerbNet path in Example (38), producing a value for Feature 59 of four. This relatively
small value supports an inference of Monsanto Co. as the iarg0 for loss. It is important to
note that a VerbNet path only exists when the thematic roles are identical. For example, a
VerbNet path would not exist between 13.5.1.1.Theme and 13.2.Agent because the roles
773
Computational Linguistics Volume 38, Number 4
are not compatible. Lastly, recall that c? might contain multiple coreferential elements.
In such a situation, the minimum path length is selected as the value for this feature.
4.2.3 Group 3: Filler-independent Features. Many of the features used by the model do not
depend on elements of c?. These features are usually specific to a particular predicate.
Consider the following example:
(41) Statistics Canada reported that its [arg1 industrial?product] [p price] index
dropped 2% in September.
The ?[p price] index? collocation is rarely associated with an arg0 in NomBank or with
an iarg0 in the annotated data (both argument positions denote the seller). Feature 25
accounts for this type of behavior by encoding the syntactic head of p?s right sibling.
The value of Feature 25 for Example 41 is price:index. Contrast this with the following:
(42) [iarg0 The company] is trying to prevent further [p price] drops.
The value of Feature 25 for Example (42) is price:drop. This feature captures an important
distinction between the two uses of price: the former cannot easily take an iarg0, whereas
the latter can. Many other features in Table B.1 depend only on the predicate and have
values that take the form predicate:feature value.
4.2.4 Group 4: Features Derived from Corpus Statistics. Feature 13 is inspired by the work
of Chambers and Jurafsky (2008), who investigated unsupervised learning of narrative
event sequences using pointwise mutual information (PMI) between syntactic positions.
We extended this PMI score to semantic arguments instead of syntactic dependencies.
Thus, the value for this feature is computed as follows:
pmi(?p1, argi? ,
?
p2, argj
?
) = log
Pcoref pmi(?p1, argi? ,
?
p2, argj
?
)
Pcoref pmi(?p1, argi? , ?)Pcoref pmi(
?
p2, argj
?
, ?)
(2)
We computed Equation (2) by applying verbal SRL (Punyakanok, Roth, and Yih
2008), nominal SRL (Gerber, Chai, and Meyers 2009), and coreference identification
(OpenNLP) to the Gigaword corpus (Graff 2003); because these tools are not fast enough
to process all 1,000,000 documents in the corpus, however, we selected subsets of
the corpus for each p1/p2 combination observed in our implicit argument data. We
first indexed the Gigaword corpus using the Lucene search engine.9 We then queried
this index using the simple boolean query ?p1 AND p2,? which retrieved documents
relevant to the predicates considered in Equation (2). Assuming the resulting data have
N coreferential pairs of arguments, the numerator in Equation (2) is defined as follows:
Pcoref pmi(?p1, argi? ,
?
p2, argj
?
) =
#coref (?p1, argi? ,
?
p2, argj
?
)
N (3)
In Equation (3), #coref returns the number of times the given argument positions
are found to be coreferential. In order to penalize low-frequency observations with
9 http://lucene.apache.org.
774
Gerber and Chai SRL of Implicit Arguments for Nominal Predicates
artificially high scores, we used the simple discounting method described by Pantel
and Ravichandran (2004) resulting in the following modification of Equation (3):
x = #coref (?p1, argi? ,
?
p2, argj
?
)
Pcoref pmi(?p1, argi? ,
?
p2, argj
?
) = xN ?
x
x + 1
(4)
Thus, if two argument positions are rarely observed as coreferent, the value xx+1 will
be small, reducing the PMI score. The denominator in Equation (2) is computed with a
similar discount factor:
x1 = #coref (?p1, argi? , ?)
x2 = #coref (
?
p2, argj
?
, ?)
Pcoref pmi(?p1, argi? , ?)Pcoref pmi(
?
p2, argj
?
, ?) = x1x2
(N2) min(x1,x2 )min(x1,x2 )+1
(5)
Thus, if either of the argument positions is rarely observed as coreferent with other
argument positions, the value min(x1,x2 )min(x1,x2 )+1 will be small, making the denominator of
Equation (2) large, reducing the PMI score. In general, the discount factors reduce the
PMI score for argument positions that are not frequent in the corpus.
We refer to Equation (2) as a targeted PMI score because it relies on data that have
been chosen specifically for the calculation at hand. Table 3 shows a sample of targeted
PMI scores between the arg1 of loss and other argument positions. There are two things
to note about this data: First, the argument positions listed are all naturally related to
the arg1 of loss. Second, the discount factor changes the final ranking by moving the
less frequent recoup predicate from a raw rank of 1 to a discounted rank of 3, preferring
instead the more common win predicate.
The information in Table 3 is useful in situations such as the following (contrived):
(43) Mary won [c the tennis match].
(44) [arg0 John?s] [p loss] was not surprising.
In Example (44) we are looking for the iarg1 of loss. The information in Table 3 strongly
suggests that the marked candidate c, being the arg1 of win, would be a suitable filler
Table 3
Targeted PMI scores between the arg1 of loss and other argument positions. The second column
gives the number of times that the argument position in the row is found to be coreferent with
the arg1 of the loss predicate. A higher value in this column results in a lower discount factor.
See Equation (4) for the discount factor.
Argument position #coref with loss.arg1 Raw PMI score Discounted PMI score
win.arg1 37 5.68 5.52
gain.arg1 10 5.13 4.64
recoup.arg1 2 6.99 4.27
steal.arg1 4 5.18 4.09
possess.arg1 3 5.10 3.77
775
Computational Linguistics Volume 38, Number 4
for this position. Lastly, note that if c were to form a coreference chain with other
constituents, it would be possible to calculate multiple PMI scores. In such cases, the
targeted PMI feature uses the maximum of all scores.
Feature 27 captures the selectional preference of a predicate p for the elements in c?
with respect to argument position iargn. In general, selectional preference scores denote
the strength of attraction for a predicate?argument position to a particular word or
class of words. To calculate the value for this feature, we used the information?theoretic
model proposed by Resnik (1996), which is defined as follows:
Pref (p, argn, s ? WordNet) =
Pr(s|p, argn)log
Pr(s|p, argn)
Pr(s)
Z (6)
Z =
?
si?WordNet
Pr(si|p, argn)log
Pr(si|p, argn)
Pr(si)
In Equation (6), Pref calculates the preference for a WordNet synset s in the given
predicate?argument position. Prior and posterior probabilities for s were calculated by
examining the arguments present in the Penn TreeBank combined with 20,000 docu-
ments randomly selected from the Gigaword corpus. PropBank and NomBank supplied
arguments for the Penn TreeBank, and we used the aforementioned verbal and nominal
SRL systems to extract arguments from Gigaword. The head word for each argument
was mapped to its WordNet synsets, and counts for these synsets were updated as
suggested by Resnik (1996). Note that a synset s that is not observed in the training
data will receive a score of zero because Pr(s|p, argn) will be zero.
Equation (6) computes the preference of a predicate?argument position for a synset;
a single word can map to multiple synsets if its sense is ambiguous, however. Given a
word w and its synsets s1, s2, . . . , sn, the preference of a predicate?argument position for
w is defined as follows:
Pref (p, argn,w) =
?
si Pref (p, argn, si)
n (7)
That is, the preference for a word is computed as the average preference across all possi-
ble synsets. The final value for Feature 27 is computed using the word-based preference
score defined in Equation (7). Given a candidate implicit argument c? comprising the
primary filler c and its coreferent mentions, the following value is obtained:
Pref (p, iargn, c
?) = min
f?c?
Pref (p, argn, f ) (8)
In Equation (8), each f is the syntactic head of a constituent from c?. The value of
Equation (8) is in (??,+?), with larger values indicating higher preference for c as
the implicit filler of position iargn.
Feature 33 implements the suggestion of Burchardt, Frank, and Pinkal (2005) that
implicit arguments might be identified using observed coreference patterns in a large
corpus of text. Our implementation of this feature uses the same data used for the
previous feature: arguments extracted from the Penn TreeBank and 20,000 documents
randomly selected from Gigaword. Additionally, we identified coreferent arguments
in this corpus using OpenNLP. Using this information, we calculated the probability
of coreference between any two argument positions. As with Feature 13, we used
776
Gerber and Chai SRL of Implicit Arguments for Nominal Predicates
discounting to penalize low?frequency observations, producing an estimate of
coreference probability as follows:
Corefjoint = #coref (?p1, argi? ,
?
p2, argj
?
)
Corefmarginal = #coref (?p1, argi? , ?)
Pcoref (?p1, argi? ,
?
p2, argj
?
) =
Corefjoint
Corefmarginal
?
Corefjoint
Corefjoint + 1
?
Corefmarginal
Corefmarginal + 1
(9)
In Equation (9), Pcoref should be read as ?the probability that ?p1, argi? is coref-
erential with
?
p2, argj
?
given ?p1, argi? is coreferential with something.? For example,
we observed that the arg1 for predicate reassess (the entity reassessed) is coreferential
with six other constituents in the corpus. Table 4 lists the argument positions with
which this argument is coreferential along with the raw and discounted probabilities.
The discounted probabilities can help identify the implicit argument in the following
contrived examples:
(45) Senators must rethink [c their strategy for the upcoming election].
(46) The [p reassessment] must begin soon.
In Example (46) we are looking for the iarg1 of reassess. Table 4 tells us that the marked
candidate (an arg1 to rethink) is likely to fill this missing argument position. When
c forms a coreference chain with other constituents, this feature uses the minimum
coreference probability between the implicit argument position and elements in the
chain.
4.2.5 Group 5: Features Derived from the Discourse Structure of the Text. Feature 67 identifies
the discourse relation (if any) that holds between the candidate constituent c and the
filled predicate p. Consider the following example:
(47) [iarg0 SFE Technologies] reported a net loss of $889,000 on sales of $23.4
million.
(48) That compared with an operating [p loss] of [arg1 $1.9 million] on sales of
$27.4 million in the year?earlier period.
In this case, a comparison discourse relation (signaled by the underlined text) holds be-
tween the first and second sentence. The coherence provided by this relation encourages
Table 4
Coreference probabilities between reassess.arg1 and other argument positions. See Equation (9)
for details on the discount factor.
Argument Raw coreference probability Discounted coreference probability
rethink.arg1 3/6 = 0.5 0.32
define.arg1 2/6 = 0.33 0.19
redefine.arg1 1/6 = 0.17 0.07
777
Computational Linguistics Volume 38, Number 4
an inference that identifies the marked iarg0 (the loser). The value for this feature is the
name of the discourse relation (e.g., comparison) whose two discourse units cover the
candidate (iarg0 above) and filled predicate (p above). Throughout our investigation,
we used gold-standard discourse relations provided by the Penn Discourse TreeBank
(Prasad et al 2008).
4.2.6 Group 6: Other Features. A few other features that were prominent according to our
feature selection process are not contained in the groups described thus for. Feature 2
encodes the sentence distance from c (the primary filler) to the predicate for which we
are filling the implicit argument position. The prominent position of this feature agrees
with our previous observation that most implicit arguments can be resolved within a
few sentences of the predicate (see Figure 1 on p. 770). Feature 3 is another simple yet
highly ranked feature. This feature concatenates the head of an element of c? with p
and iargn. For example, in sentences (45) and (46), this feature would have a value of
strategy ? reassess ? arg1, asserting that strategies are reassessed. Feature 5 generalizes
this feature by replacing the head word with its WordNet synset.
4.2.7 Comparison with Features for Traditional SRL. The features described thus far are
quite different from those used in previous work to identify arguments in the traditional
nominal SRL setting (see the work of Gerber, Chai, and Meyers 2009). The most impor-
tant feature used in traditional SRL?the syntactic parse tree path?is notably absent.
This difference is due to the fact that syntactic information, although present, does not
play a central role in the implicit argument model. The most important features are
those that capture semantic properties of the implicit predicate?argument position and
the candidate filler for that position.
4.3 Post-processing for Final Output Selection
Without loss of generality, assume there exists a predicate instance p with two missing
argument positions iarg0 and iarg1. Also assume that there are three candidate fillers
c1, c2, and c3 within the candidate window. The discriminative model will calculate the
probability that each candidate fills each missing argument position. Graphically:
iarg0 iarg1
c1 0.3 0.4
c2 0.1 0.05
c3 0.6 0.3
There exist two constraints on possible assignments of candidates to positions. First, a
candidate may not be assigned to more than one missing argument position. To enforce
this constraint, only the top-scoring cell in each row is retained, leading to the following:
iarg0 iarg1
c1 - 0.4
c2 0.1 -
c3 0.6 -
778
Gerber and Chai SRL of Implicit Arguments for Nominal Predicates
Second, a missing argument position can only be filled by a single candidate. To enforce
this constraint, only the top-scoring cell in each column is retained, leading to the
following:
iarg0 iarg1
c1 - 0.4
c2 - -
c3 0.6 -
Having satisfied these constraints, a threshold t is imposed on the remaining cell prob-
abilities.10 Cells with probabilities less than t are cleared. Assuming that t = 0.42, the
final assignment would be as follows:
iarg0 iarg1
c1 - -
c2 - -
c3 0.6 -
In this case, c3 fills iarg0 with probability 0.6 and iarg1 remains unfilled. The latter
outcome is desirable because not all argument positions have fillers that are present
in the discourse.
5. Evaluation
5.1 Data
All evaluations in this study were performed using a randomized cross-validation
configuration. The 1,247 predicate instances were annotated document by document.
In order to remove any confounding factors caused by specific documents, we first
randomized the annotated predicate instances. Following this, we split the predicate
instances evenly into ten folds and used each fold as testing data for a model trained on
the instances outside the fold. This evaluation set-up is an improvement versus the one
we previously reported (Gerber and Chai 2010), in which fixed partitions were used for
training, development, and testing.
During training, the system was provided with annotated predicate instances. The
system identified missing argument positions and generated a set of candidates for
each such position. A candidate three-tuple ?p, iargn, c?? was given a positive label if the
candidate implicit argument c (the primary filler) was annotated as filling the missing
argument position; otherwise, the candidate three-tuple was given a negative label.
During testing, the system was presented with each predicate instance and was required
to identify all implicit arguments for the predicate.
10 The threshold t is learned from the training data. The learning mechanism is explained in the following
section.
779
Computational Linguistics Volume 38, Number 4
Throughout the evaluation process we assumed the existence of gold-standard
PropBank and NomBank information in all documents. This factored out errors from
traditional SRL and affected the following stages of system operation:
 Missing argument identification. The system was required to figure out
which argument positions were missing. Each of the ten predicates was
associated with an unambiguous role set, so determining the missing
argument positions amounted to comparing the existing local arguments
with the argument positions listed in the predicate?s role set. Because
gold-standard local NomBank arguments were used, this stage produced
no errors.
 Candidate generation. As mentioned in Section 4.1, the set of candidates
for a missing argument position contains constituents labeled with a core
(e.g., arg0) PropBank or NomBank argument label. We used gold-standard
PropBank and NomBank arguments; it is not the case that all annotated
implicit arguments are given a core argument label by PropBank or
NomBank, however. Thus, despite the gold-standard argument labels,
this stage produced errors in which the system failed to generate a
true-positive candidate for an implicit argument position. Approximately
96% of implicit argument positions are filled by gold-standard PropBank
or NomBank arguments.
 Feature extraction. Many of the features described in Section 4.2 rely on
underlying PropBank and NomBank argument labels. For example, the
top-ranked Feature 1 relates the argument position of the candidate to the
missing argument position. In our experiments, values for this feature
contained no errors because gold-standard PropBank and NomBank labels
were used. Note, however, that many features were derived from the
output of an automatic SRL process that occasionally produced errors
(e.g., Feature 13, which used PMI scores between automatically identified
arguments). These errors were present in both the training and evaluation
stages.
We also assumed the existence of gold-standard syntactic structure when possible.
This was done in order to focus our investigation on the semantic nature of implicit
arguments.
5.2 Scoring Metrics
We evaluated system performance using the methodology proposed by Ruppenhofer
et al (2010). For each missing argument position of a predicate instance, the system
was required to either (1) identify a single constituent that fills the missing argument
position or (2) make no prediction and leave the missing argument position unfilled. To
give partial credit for inexact argument bounds, we scored predictions using the Dice
coefficient, which is defined as follows:
Dice(Predicted,True) =
2 ? |Predicted
?
True|
|Predicted|+ |True| (10)
780
Gerber and Chai SRL of Implicit Arguments for Nominal Predicates
Predicted contains the tokens that the model believes fill the implicit argument position.
True is the set of tokens from a single annotated constituent that fills the missing
argument position. The model?s prediction receives a score equal to the maximum Dice
overlap across any one of the annotated fillers (AF):
Score(Predicted) = max
True?AF
Dice(Predicted,True) (11)
Precision is equal to the summed prediction scores divided by the number of argument
positions filled by the model. Recall is equal to the summed prediction scores divided
by the number of argument positions filled in the annotated data. Predictions not cover-
ing the head of a true filler were assigned a score of zero.11 For example, consider
the following true and predicted labelings:
(49) True labeling: [iarg0 Participants] will be able to transfer [iarg1 money] to
[iarg2 other investment funds]. The [p investment] choices are limited to
[iarg2 a stock fund and a money-market fund].
(50) Predicted labeling: Participants will be able to transfer [iarg1 money] to
other [iarg2 investment funds]. The [p investment] choices are limited to a
stock fund and a money-market fund.
In the ground-truth (49) there are three implicit argument positions to fill. The hypo-
thetical system has made predictions for two of the positions. The prediction scores are:
Score(iarg1 money) = Dice(money,money) = 1
Score(iarg2 investment funds) = max{Dice(investment funds, other investment funds),
Dice(investment funds, a stock . . . money?market fund)}
= max{0.8, 0} = 0.8
Precision, recall, and F1 for the example predicate are calculated as follows:
Precision = 1.82 = 0.9
Recall = 1.83 = 0.6
F1 =
2 ? Precision ? Recall
Precision + Recall
= 0.72
We calculated the F1 score for the entire testing fold by aggregating the counts used in
the above precision and recall calculations. Similarly, we aggregated the counts across
all folds to arrive at a single F1 score for the evaluated system.
We used a bootstrap resampling technique similar to those developed by Efron and
Tibshirani (1993) to test the significance of the performance difference between various
systems. Given a test pool comprising M missing argument positions iargn along with
11 Our evaluation methodology differs slightly from that of Ruppenhofer et al (2010) in that we use the
Dice metric to compute precision and recall, whereas Ruppenhofer et al reported the Dice metric
separately from exact-match precision and recall.
781
Computational Linguistics Volume 38, Number 4
the predictions by systems A and B for each iargn, we calculated the exact p-value of the
performance difference as follows:
1. Create r random resamples from M with replacement.
2. For each resample Ri, compute the system performance difference
dRi = ARi ? BRi and store dri in D.
3. Find the largest symmetric interval [min,max] around the mean of D that
does not include zero.
4. The exact p-value equals the percentage of elements in D that are not in
[min,max].
Experiments have shown that this simple approach provides accurate estimates of
significance while making minimal assumptions about the underlying data distribution
(Efron and Tibshirani 1993). Similar randomization tests have been used to evaluate
information extraction systems (Chinchor, Lewis, and Hirschmant 1993).
5.3 LibLinear Model Configuration
Given a testing fold Ftest and a training fold Ftrain, we performed floating forward
feature subset selection using only the information contained in Ftrain. We used an
algorithm similar to the one described by Pudil, Novovicova, and Kittler (1994). As
part of the feature selection process, we conducted a grid search for the best c and w
LibLinear parameters, which govern the per-class cost of mislabeling instances from
a particular class (Fan et al 2008). Setting per-class costs helps counter the effects of
class size imbalance, which is severe even when selecting candidates from the current
and previous few sentences (most candidates are negative). We ran the feature selection
and grid search processes independently for each Ftrain. As a result, the feature set and
model parameters are slightly different for each fold.12 For all folds, we used LibLinear?s
logistic regression solver and a candidate selection window of two sentences prior. As
shown in Figure 1, this window imposes a recall upper bound of approximately 85%.
The post-processing prediction threshold t was learned using a brute-force search that
maximized the system?s performance over the data in Ftrain.
5.4 Baseline and Oracle Models
We compared the supervised model with the simple baseline heuristic defined below:
Fill iargn for predicate instance p with the nearest constituent in the two-sentence
candidate window that fills argn for a different instance of p, where all nominal
predicates are normalized to their verbal forms.
The normalization allows, for example, an existing arg0 for the verb invested to fill an
iarg0 for the noun investment. This heuristic outperformed a more complicated heuristic
that relied on the PMI score described in Section 4.2. We also evaluated an oracle model
that made gold-standard predictions for candidates within the two-sentence prediction
window.
12 See Appendix Table C.1 for a per-fold listing of features and model parameters.
782
Gerber and Chai SRL of Implicit Arguments for Nominal Predicates
5.5 Results
Table 5 presents the evaluation results for implicit argument identification. Overall,
the discriminative model increased F1 performance by 21.4 percentage points (74.1%)
compared to the baseline (p < 0.0001). Predicates with the highest number of implicit
arguments (sale and price) showed F1 increases of 13.7 and 17.5 percentage points,
respectively (p < 0.001 for both differences). As expected, oracle precision is 100% for
all predictions, and the F1 difference between the discriminative and oracle systems is
significant at p < 0.0001 for all test sets. See the Appendix for a per-fold breakdown of
results and a listing of features and model parameters used for each fold.
We also measured human performance on this task by running the undergraduate
assistant?s annotations against a small portion of the evaluation data comprising 275
filled implicit arguments. The assistant achieved an overall F1 score of 56.0% using the
same two-sentence candidate window used by the baseline, discriminative, and oracle
models. Using an infinite candidate window, the assistant increased F1 performance
to 64.2%. Although these results provide a general idea about the performance upper
bound, they are not directly comparable to the cross-validated results shown in Table 5
because the assistant did not annotate the entire data set.
6. Discussion
6.1 Training Set Size
As described in Section 3.1, implicit argument annotation is an expensive process.
Thus, it is important to understand whether additional annotation would benefit the
ten predicates considered. In order to estimate the potential benefits, we measured the
effect of training set size on system performance. We retrained the discriminative model
for each evaluation fold using incrementally larger subsets of the complete training set
for the fold. Figure 2 shows the results, which indicate minimal gains beyond 80% of
the training set. Based on these results, we feel that future work should emphasize
feature and model development over training data expansion, as gains appear to trail
off significantly.
6.2 Feature Assessment
Previously (Gerber and Chai 2010), we assessed the importance of various implicit
argument feature groups by conducting feature ablation tests. In each test, the discrimi-
native model was retrained and reevaluated without a particular group of features. We
summarize the findings of this study in this section.
6.2.1 Semantic Roles are Essential. We observed statistically significant losses when ex-
cluding features that relate the semantic roles of elements in c? to the semantic role of the
missing argument position. For example, Feature 1 appears as the top-ranked feature
in eight out of ten fold evaluations (see Appendix Table C.1). This feature is formed
by concatenating the filling predicate?argument position with the filled predicate?
argument position, producing values such as invest.arg0-lose.arg0. This value indicates
that the entity performing the investing is also the entity losing something. This type of
commonsense knowledge is essential to the task of implicit argument identification.
783
C
om
p
u
tation
alL
in
gu
istics
V
olu
m
e
38,N
u
m
ber
4
Table 5
Overall evaluation results for implicit argument identification. The second column gives the number of ground?truth implicitly filled argument
positions for the predicate instances. P, R, and F1 indicate precision, recall, and F?measure (? = 1), respectively. pexact is the bootstrapped exact
p-value of the F1 difference between two systems, where the systems are (B)aseline, (D)iscriminative, and (O)racle.
Baseline Discriminative Oracle
# Imp. args. P R F1 P R F1 pexact(B,D) P R F1 pexact(D,O)
sale 181 57.0 27.7 37.3 59.2 44.8 51.0 0.0003 100.0 72.4 84.0 <0.0001
price 138 67.1 23.3 34.6 56.0 48.7 52.1 <0.0001 100.0 78.3 87.8 <0.0001
bid 124 66.7 14.5 23.8 60.0 36.3 45.2 <0.0001 100.0 60.5 75.4 <0.0001
investor 108 30.0 2.8 5.1 46.7 39.8 43.0 <0.0001 100.0 84.3 91.5 <0.0001
cost 86 60.0 10.5 17.8 62.5 50.9 56.1 <0.0001 100.0 86.0 92.5 <0.0001
loan 82 63.0 20.7 31.2 67.2 50.0 57.3 <0.0001 100.0 89.0 94.2 <0.0001
plan 77 72.7 20.8 32.3 59.6 44.1 50.7 0.0032 100.0 87.0 93.1 <0.0001
loss 62 78.8 41.9 54.7 72.5 59.7 65.5 0.0331 100.0 88.7 94.0 <0.0001
fund 56 66.7 10.7 18.5 80.0 35.7 49.4 <0.0001 100.0 66.1 79.6 <0.0001
investment 52 28.9 10.6 15.5 32.9 34.2 33.6 0.0043 100.0 80.8 89.4 <0.0001
Overall 966 61.4 18.9 28.9 57.9 44.5 50.3 <0.0001 100.0 78.0 87.6 <0.0001
784
Gerber and Chai SRL of Implicit Arguments for Nominal Predicates
Figure 2
Effect of training set size on performance of discriminative model. The x-axis indicates the
percentage of training data used, and the y-axis indicates the overall F1 score that results.
6.2.2 Other Information is Important. Our 2010 study also found that semantic roles are
only one part of the solution. Using semantic roles in isolation also produced statistically
significant losses. This indicates that other features contribute useful information to
the task.
6.2.3 Discourse Structure Is not Essential. We also tested the effect of removing discourse
relations (Feature 67) from the model. Discourse structure has received a significant
amount of attention in NLP; it remains a very challenging problem, however, with
state-of-the-art systems attaining F1 scores in the mid-40% range (Sagae 2009). Our 2010
work as well as the updated work presented in this article used gold-standard discourse
relations from the Penn Discourse TreeBank. As shown by Sagae (2009), these relations
are difficult to extract in a practical setting. In our 2010 work, we showed that removing
discourse relations from the model did not have a statistically significant effect on
performance. Thus, this information should be removed in practical applications of the
model, at least until better uses for it can be identified.
6.2.4 Relative Feature Importance. We extended earlier findings by assessing the relative
importance of the features. We aggregated the feature rank information given in Ap-
pendix Table C.1. For each evaluation fold, each feature received a point value equal
to its reciprocal rank within the feature list. Thus, a feature appearing at rank 5 for a
fold would receive 15 = 0.2 points for that fold. We totaled these points across all folds,
arriving at the values shown in the final column of Appendix Table B.1. The scores
confirm the earlier findings. The highest scoring feature relates the semantic roles of
the candidate argument to the missing argument position. Non-semantic information
such as the sentence distance (Feature 2) also plays a key role. Discourse structure is
consistently ranked near the bottom of the list (Feature 67).
6.3 Error Analysis
Table 6 lists the errors made by the system and their frequencies. As shown, the single
most common error (type 1) occurred when a true filler was classified but an incor-
rect filler had a higher score. This occurred in approximately 31% of the error cases.
785
Computational Linguistics Volume 38, Number 4
Table 6
Implicit argument error analysis. The second column indicates the type of error that was made
and the third column gives the percentage of all errors that fall into each type.
# Description %
1 A true filler was classified but an incorrect filler scored higher 30.6
2 A true filler did not exist but a prediction was made 22.4
3 A true filler existed within the window but was not a candidate 21.1
4 A true filler scored highest but below the threshold 15.9
5 A true filler existed but not within the window 10.0
Often, though, the system did not classify a true implicit argument because such a
candidate was not generated. Without such a candidate, the system stood no chance of
making a correct prediction. Errors 3 and 5 combined (also 31%) describe this behavior.
Type 3 errors resulted when implicit arguments were not core (i.e., argn) arguments
to other predicates. To reduce class imbalance, the system only used core arguments
as candidates; this came at the expense of increased type 3 errors, however. In many
cases, the true implicit argument filled a non-core (i.e., adjunct) role within PropBank
or NomBank.
Type 5 errors resulted when the true implicit arguments for a predicate were outside
the candidate window. Oracle recall (see Table 5) indicates the nominals that suffered
most from windowing errors. For example, the sale predicate was associated with the
highest number of true implicit arguments, but only 72% of those could be resolved
within the two-sentence candidate window. Empirically, we found that extending the
candidate window uniformly for all predicates did not increase F1 performance because
additional false positives were identified. The oracle results suggest that predicate-
specific window settings might offer some advantage for predicates such as fund and
bid, which take arguments at longer ranges.
Error types 2 and 4 are directly related to the prediction confidence threshold t.
The former would be reduced by increasing t and thus filtering out bad predictions.
The latter would be reduced by lowering t and allowing more true fillers into the final
output. It is unclear whether either of these actions would increase overall performance,
however.
6.4 The Investment and Fund Predicates
In Section 4.2, we discussed the price predicate, which frequently occurs in the ?[p price]
index? collocation. We observed that this collocation is rarely associated with either an
overt arg0 or an implicit iarg0. Similar observations can be made for the investment and
fund predicates. Although these two predicates are frequent, they are rarely associated
with implicit arguments: investment takes only 52 implicit arguments and fund takes
only 56 implicit arguments (see Table 5). This behavior is due in large part to collocations
such as ?[p investment] banker,? ?stock [p fund],? and ?mutual [p fund],? which use
predicate senses that are not eventive and take no arguments. Such collocations also
violate the assumption that differences between the PropBank and NomBank argument
structure for a predicate are indicative of implicit arguments (see Section 3.1 for this
assumption).
Despite their lack of implicit arguments, it is important to account for predicates
such as investment and fund because the incorrect prediction of implicit arguments for
786
Gerber and Chai SRL of Implicit Arguments for Nominal Predicates
them can lower precision. This is precisely what happened for the investment predicate
(P = 33%). The model incorrectly identified many implicit arguments for instances such
as ?[p investment] banker? and ?[p investment] professional,? which take no arguments.
The right context of investment should help the model avoid this type of error; however
in many cases this was not enough evidence to prevent a false positive prediction.
It might be helpful to distinguish eventive nominals from non-eventive ones, given
the observation that some non-eventive nominals rarely take arguments. Additional
investigation is needed to address this type of error.
6.5 Improvements versus the Baseline
The baseline heuristic covers the simple case where identical predicates share argu-
ments in the same position. Because the discriminative model also uses this information
(see Feature 8), it is interesting to examine cases where the baseline heuristic failed
but the discriminative model succeeded. Such cases represent more difficult inferences.
Consider the following sentence:
(51) Mr. Rogers recommends that [p investors] sell [iarg2 takeover?related
stock].
Neither NomBank nor the baseline heuristic associate the marked predicate in Exam-
ple (51) with any arguments; the feature-based model was able to correctly identify
the marked iarg2 as the entity being invested in, however. This inference relied on a
number of features that connect the invest event to the sell event (e.g., Features 1, 4,
and 76). These features captured a tendency of investors to sell the things they have
invested in.
We conclude our discussion with an example of a complex extra-sentential implicit
argument. Consider the following adjacent sentences:
(52) [arg0 Olivetti] [p exported] $25 million in ?embargoed, state-of-the-art,
flexible manufacturing systems to the Soviet aviation industry.?
(53) [arg0 Olivetti] reportedly began [p shipping] these tools in 1984.
(54) [iarg0 Olivetti] has denied that it violated the rules, asserting that the
shipments were properly licensed.
(55) However, the legality of these [p sales] is still an open question.
In Example (55), we are looking for the iarg0 of sale. As shown, the discriminative model
was able to correctly identify Olivetti from Example (54) as the implied filler of this
argument position. The inference involved two key steps. First, the model identified
coreferent mentions of Olivetti in Examples (52) and (53). In these sentences, Olivetti
participates in the marked exporting and shipping events. Second, the model identified
a tendency for exporters and shippers to also be sellers (e.g., Features 1, 4, and 23 made
large contributions to the prediction). Using this knowledge, the system extracted infor-
mation that could not be extracted by the baseline heuristic or a traditional SRL system.
787
Computational Linguistics Volume 38, Number 4
6.6 Comparison with Previous Results
In a previous study, we reported initial results for the task of implicit argument identi-
fication (Gerber and Chai 2010). This article presents two major advancements versus
our prior work. First, this article presents a more rigorous evaluation set-up, which was
not used in our previous study. Our previous study used fixed partitions of training,
development, and testing data. As a result, feature and model parameter selections
overfit the development data; we observed a 23-point difference in F1 between the
development (65%) and testing (42%) partitions. The small size of the testing set alo
led to small sample sizes and large p-values during significance testing. The cross-
validated approach reported in this article alleviated both problems. The F1 difference
between training and testing was approximately 10 percentage points for all folds, and
all of the data were used for testing, leading to more accurate p-values. It is not possible
to directly compare the evaluation scores in the two studies; the methodology in the
current article is preferable for the reasons mentioned, however.
Second, this article presents a wider range of features compared with the features
described in our previous study. In particular, we experimented with corpus statistics
derived from sub-corpora that were specifically tailored to the predicate instance under
consideration. See, for example, Feature 13 in Appendix B, which computed PMI scores
between arguments found in a custom sub-corpus of text. This feature was ranked
highly by a few of the evaluation folds (see Appendix B for feature rankings).
7. Conclusions
Previous work provided a partial solution to the problem of nominals with implicit
arguments (Gerber, Chai, and Meyers 2009). The model described in that work is able
to accurately identify nominals that take local arguments, thus filtering out predicates
whose arguments are entirely implicit. This increases standard nominal SRL perfor-
mance by reducing the number of false positive argument predictions; all implicit
arguments remain unidentified, however, leaving a large portion of the corresponding
event structures unrecognized.
This article presents our investigation of implicit argument identification for nom-
inal predicates. The study was based on a manually created corpus of implicit argu-
ments, which is freely available for research purposes. Our results show that models
can be trained by incorporating information from a variety of ontological and corpus-
based sources. The study?s primary findings include the following:
1. Implicit arguments are frequent. Given the predicates in a document,
there exist a fixed number of possible arguments that can be filled
according to NomBank?s predicate role sets. Role coverage is defined as
the fraction of these roles that are actually filled by constituents in the text.
Using NomBank as a baseline, the study found that role coverage
increases by 71% when implicit arguments are taken into consideration.
2. Implicit arguments can be automatically identified. Using the annotated
data, we constructed a feature-based supervised model that is able to
automatically identify implicit arguments. This model relies heavily on
the traditional, single-sentence SRL structure of both nominal and verbal
predicates. By unifying these sources of information, the implicit argument
788
Gerber and Chai SRL of Implicit Arguments for Nominal Predicates
model provides a more coherent picture of discourse semantics than is
typical in most recent work (e.g., the evaluation conducted by Surdeanu
et al [2008]). The model demonstrates substantial gains over an informed
baseline, reaching an overall F1 score of 50% and per-predicate scores in
the mid-50s and mid-60s. These results are among the first for this task.
3. Much work remains. The study presented in the current article was very
focused: Only ten different predicates were analyzed. The goal was to
carefully examine the underlying linguistic properties of implicit
arguments. This examination produced many features that have not been
used in other SRL studies. The results are encouraging; a direct application
of the model to all NomBank predicates will require a substantial
annotation effort, however. This is because many of the most important
features are lexicalized on the predicate being analyzed and thus cannot be
generalized to novel predicates. Additional information might be
extracted from VerbNet, which groups related verbs together. Features
from this resource might generalize better because they apply to entire sets
of verbs (and verb-based nouns). Additionally, the model would benefit
from a deeper understanding of the relationships that obtain between
predicates in close textual proximity. Often, predicates themselves head
arguments to other predicates, and, as a result, borrow arguments from
those predicates following certain patterns. The work of Blanco and
Moldovan (2011) addresses this issue directly with the use of composition
rules. These rules would be helpful for implicit argument identification.
Lastly, it should be noted that the prediction model described in this article
is quite simple. Each candidate is independently classified as filling
each missing argument position, and a heuristic post-processing step is
performed to arrive at the final labeling. This approach ignores the joint
behavior of semantic arguments. We have performed a preliminary
investigation of joint implicit argument structures (Gerber, Chai, and Bart
2011); as described in that work, however, many issues remain concerning
joint implicit argument identification.
789
Computational Linguistics Volume 38, Number 4
Appendix A: Role Sets for the Annotated Predicates
Listed here are the role sets for the ten predicates used in this article.
Role set for bid:
Arg0: bidder
Arg1: thing being bid for
Arg2: amount of the bid
Role set for sale:
Arg0: seller
Arg1: thing sold
Arg2: buyer
Arg3: price paid
Arg4: beneficiary of sale
Role set for loan:
Arg0: giver
Arg1: thing given
Arg2: entity given to
Arg3: loan against
(collateral)
Arg4: interest rate
Role set for cost:
Arg1: commodity
Arg2: price
Arg3: buyer
Arg4: secondary commodity
Role set for plan:
Arg0: planner
Arg1: thing planned
Arg2: beneficiary of plan
Arg3: secondary plan
Role set for investor:
Arg0: investor
Arg1: thing invested
Arg2: thing invested in
Role set for price:
Arg0: seller
Arg1: commodity
Arg2: price
Arg3: secondary commodity
Role set for loss:
Arg0: entity losing
something
Arg1: thing lost
Arg2: entity gaining thing
lost
Arg3: source of loss
Role set for investment:
Arg0: investor
Arg1: thing invested
Arg2: thing invested in
Role set for fund:
Arg0: funder
Arg1: thing funded
Arg2: amount of funding
Arg3: beneficiary
790
Gerber and Chai SRL of Implicit Arguments for Nominal Predicates
Appendix B: Implicit Argument Features
Table B.1
Features for determining whether c fills iargn of predicate p. For each mention f (denoting a
filler) in the coreference chain c?, pf , and argf are the predicate and argument position of f .
Unless otherwise noted, all argument positions (e.g., argn and iargn) should be interpreted as the
integer label n instead of the underlying word content of the argument. The & symbol denotes
concatenation; for example, a feature value of ?p & iargn? for the iarg0 position of sale would be
?sale-0.? Features marked with an asterisk (*) are explained in Section 4.2. Features marked with
a dagger (?) require external text corpora that have been automatically processed by existing
NLP components (e.g., SRL systems). The final column gives a heuristic ranking score for the
features across all evaluation folds (see Section 6.2 for discussion).
# Feature value description Importance score
1* For every f , pf & argf & p & iargn. 8.2
2* Sentence distance from c to p. 4.0
3* For every f , the head word of f & the verbal form of p & iargn. 3.6
4* Same as 1 except generalizing pf and p to their WordNet synsets. 3.3
5* Same as 3 except generalizing f to its WordNet synset. 1.0
6 Whether or not c and p are themselves arguments to the same
predicate.
1.0
7 p & the semantic head word of p?s right sibling. 0.7
8 Whether or not any argf and iargn have the same integer argument
position.
0.7
9* Frame element path between argf of pf and iargn of p in FrameNet
(Baker, Fillmore, and Lowe 1998).
0.6
10 Percentage of elements in c? that are subjects of a copular for which p
is the object.
0.6
11 Whether or not the verb forms of pf and p are in the same VerbNet
class and argf and iargn have the same thematic role.
0.6
12 p & the last word of p?s right sibling. 0.6
13*? Maximum targeted PMI between argf of pf and iargn of p. 0.6
14 p & the number of p?s right siblings. 0.5
15 Percentage of elements in c? that are objects of a copular for which p
is the subject.
0.5
16 Frequency of the verbal form of p within the document. 0.5
17 p & the stemmed content words in a one?word window around p. 0.5
18 Whether or not p?s left sibling is a quantifier (many, most, all, etc.).
Quantified predicates tend not to take implicit arguments.
0.4
19 Percentage of elements in c? that are copular objects. 0.4
20 TF cosine similarity between words from arguments of all pf and
words from arguments of p.
0.4
21 Whether the path defined in 9 exists. 0.4
22 Percentage of elements in c? that are copular subjects. 0.4
23* For every f , the VerbNet class/role of pf /argf & the class/role of
p/iargn.
0.4
24 Percentage of elements in c? that are indefinite noun phrases. 0.4
25* p & the syntactic head word of p?s right sibling. 0.3
26 p & the stemmed content words in a two-word window around p. 0.3
27*? Minimum selectional preference between any f and iargn of p.
Uses the method described by Resnik (1996) computed over
an SRL-parsed version of the Penn TreeBank and Gigaword
(Graff 2003) corpora.
0.3
28 p & p?s synset in WordNet. 0.3
29? Same as 27 except using the maximum. 0.3
791
Computational Linguistics Volume 38, Number 4
Table B.1
(continued)
# Feature value description Importance score
30 Average per?sentence frequency of the verbal form of p within the
document.
0.3
31 p itself. 0.3
32 p & whether p is the head of its parent. 0.3
33*? Minimum coreference probability between argf of pf and iargn of p. 0.3
34 p & whether p is before a passive verb. 0.3
35 Percentage of elements in c? that are definite noun phrases. 0.3
36 Percentage of elements in c? that are arguments to other predicates. 0.3
37 Maximum absolute sentence distance from any f to p. 0.3
38 p & p?s syntactic category. 0.2
39 TF cosine similarity between the role description of iargn and the
concatenated role descriptions of all argf .
0.2
40 Average TF cosine similarity between each argn of each pf and the
corresponding argn of p, where ns are equal.
0.2
41 Same as 40 except using the maximum. 0.2
42 Same as 40 except using the minimum. 0.2
43 p & the head of the following prepositional phrase?s object. 0.2
44 Whether any f is located between p and any of the arguments
annotated by NomBank for p. When true, this feature rules out
false positives because it implies that the NomBank annotators
considered and ignored f as a local argument to p.
0.2
45 Number of elements in c?. 0.2
46 p & the first word of p?s right sibling. 0.2
47 p & the grammar rule that expands p?s parent. 0.2
48 Number of elements in c? that are arguments to other predicates. 0.2
49 Nominal form of p & iargn. 0.2
50 p & the syntactic parse tree path from p to the nearest passive verb. 0.2
51 Same as 37 except using the minimum. 0.2
52? Same as 33 except using the average. 0.2
53 Verbal form of p & iargn. 0.2
54 p & the first word of p?s left sibling. 0.2
55 Average per-sentence frequency of the nominal form of p within the
document.
0.2
56 p & the part of speech of p?s parent?s head word. 0.2
57? Same as 33 except using the maximum. 0.2
58 Same as 37 except using the average. 0.1
59* Minimum path length between argf of pf and iargn of p within
VerbNet (Kipper 2005).
0.1
60 Frequency of the nominal form of p within the document. 0.1
61 p & the number of p?s left siblings. 0.1
62 p & p?s parent?s head word. 0.1
63 p & the syntactic category of p?s right sibling. 0.1
64 p & p?s morphological suffix. 0.1
65 TF cosine similarity between words from all f and words from the
role description of iargn.
0.1
66 Percentage of elements in c? that are quantified noun phrases. 0.1
67* Discourse relation whose two discourse units cover c (the primary
filler) and p.
0.1
68 For any f , the minimum semantic similarity between pf and p using
the method described by Wu and Palmer (1994) over WordNet
(Fellbaum 1998).
0.1
792
Gerber and Chai SRL of Implicit Arguments for Nominal Predicates
Table B.1
(continued)
# Feature value description Importance score
69 p & whether or not p is followed by a prepositional phrase. 0.1
70 p & the syntactic head word of p?s left sibling. 0.1
71 p & the stemmed content words in a three-word window around p. 0.1
72 Syntactic category of c & iargn & the verbal form of p. 0.1
73 Nominal form of p & the sorted integer argument indexes (the ns)
from all argn of p.
0.1
74 Percentage of elements in c? that are sentential subjects. 0.1
75 Whether or not the integer position of any argf equals that of iargn. 0.1
76? Same as 13 except using the average. 0.1
77? Same as 27 except using the average. 0.1
78 p & p?s parent?s syntactic category. 0.1
79 p & the part of speech of the head word of p?s right sibling. 0.1
80 p & the semantic head word of p?s left sibling. 0.1
81? Maximum targeted coreference probability between argf of pf
and iargn of p. This is a hybrid feature that calculates the coreference
probability of Feature 33 using the corpus tuning method of
Feature 13.
0.1
793
C
om
p
u
tation
alL
in
gu
istics
V
olu
m
e
38,N
u
m
ber
4
Appendix C: Per-fold Implicit Argument Identification Results
Table C.1
Per-fold implicit argument identification results. Columns are defined as follows: (1) fold used for testing, (2) selected features in rank order,
(3) baseline F1, (4) LibLinear cost parameter, (5) LibLinear weight for the positive class, (6) implicit argument confidence threshold, (7) discriminative
F1, (8) oracle F1. A bias of 1 was used for all LibLinear models.
Baseline Discriminative (LibLinear) Oracle
Fold Features F1 (%) c w+ t F1 (%) F1 (%)
1 1, 2, 3, 11, 32, 8, 27, 22, 31, 10, 20, 53, 6, 16, 24, 40, 30, 38, 72, 69,
73, 19, 28, 42, 48, 64, 44, 36, 37, 12, 7
31.7 0.25 4 0.39260 47.1 86.7
2 1, 3, 2, 4, 17, 13, 28, 11, 6, 18, 25, 12, 56, 29, 16, 53, 41, 31, 46, 10, 7,
51, 15, 22
32 0.25 256 0.80629 51.5 86.9
3 4, 3, 2, 8, 7, 6, 59, 20, 9, 62, 37, 39, 41, 19, 10, 15, 11, 35, 61, 44, 42,
40, 32, 30, 16, 75, 33, 24
35.3 0.25 256 0.90879 55.8 88.1
4 1, 2, 5, 13, 8, 49, 6, 35, 34, 14, 15, 18, 36, 28, 20, 45, 3, 43, 24, 48, 10,
29, 12, 30, 33, 65, 31, 22, 61, 16, 27, 41, 60, 55, 64
27.8 0.25 4 0.38540 45.8 86.5
5 1, 2, 26, 3, 4, 23, 5, 63, 55, 6, 12, 44, 42, 65, 7, 71, 18, 15, 10, 14, 52,
34, 19, 24, 50, 58
25.8 0.125 1024 0.87629 45.9 88
6 1, 3, 2, 14, 23, 38, 25, 39, 16, 6, 21, 68, 70, 58, 9, 22, 18, 31, 60, 10,
64, 15, 66, 19, 30, 51, 56, 28
34.8 0.25 256 0.87759 55.4 90.8
7 1, 2, 4, 3, 47, 54, 43, 7, 33, 9, 67, 24, 36, 50, 40, 12, 21 22.9 0.25 256 0.81169 46.3 87.4
8 1, 3, 2, 4, 9, 7, 14, 12, 6, 46, 30, 18, 19, 36, 48, 42, 37, 45, 60, 56, 61,
51, 15, 10, 41, 40, 25, 31, 11, 39, 62, 69, 34, 16, 33, 8, 38, 20, 78, 44,
55, 80, 53, 50, 52, 49, 24, 28, 57
27.1 0.0625 512 0.92019 47.4 87.2
9 1, 5, 2, 4, 3, 21, 27, 10, 15, 9, 57, 35, 16, 25, 37, 33, 45, 24, 46, 29, 19,
34, 51, 50, 22, 48, 32, 11, 12, 58, 41, 8, 76, 18, 30, 40, 77, 6, 66, 44,
43, 79, 81, 20
23 0.0625 32 0.67719 54.1 85.5
10 4, 3, 2, 17, 1, 13, 29, 12, 11, 52, 10, 15, 6, 16, 9, 22, 7, 21, 57, 19, 74,
34, 45, 20, 66
28.4 0.0625 512 0.89769 53.2 88.5
(1) (2) (3) (4) (5) (6) (7) (8)
794
Gerber and Chai SRL of Implicit Arguments for Nominal Predicates
Acknowledgments
We would like to thank the anonymous
reviewers for their many insightful
comments and suggestions. This work
was partially supported by NSF grants
IIS-0347548 and IIS-0840538.
References
Baker, Collin, Charles Fillmore, and
John Lowe. 1998. The Berkeley FrameNet
project. In Proceedings of the Thirty-Sixth
Annual Meeting of the Association for
Computational Linguistics and Seventeenth
International Conference on Computational
Linguistics, pages 86?90, San Francisco, CA.
Bhagat, Rahul, Patrick Pantel, and Eduard
Hovy. 2007. LEDIR: An unsupervised
algorithm for learning directionality
of inference rules. In Proceedings of the
2007 Joint Conference on Empirical
Methods in Natural Language Processing
and Computational Natural Language
Learning (EMNLP-CoNLL), pages 161?170,
Prague.
Blanco, Eduardo and Dan Moldovan. 2011.
A model for composing semantic relations.
In Proceedings of the 9th International
Conference on Computational Semantics
(IWCS 2011), pages 45?54, Oxford.
Burchardt, Aljoscha, Anette Frank, and
Manfred Pinkal. 2005. Building text
meaning representations from contextually
related frames?a case study. In Proceedings
of the Sixth International Workshop on
Computational Semantics, Tilburg.
Carpenter, Patricia A., Akira Miyake,
and Marcel Adam Just. 1995. Language
comprehension: Sentence and discourse
processing. Annual Review of Psychology,
46:91?120.
Carreras, Xavier and Llu??s Ma`rquez. 2005.
Introduction to the CoNLL-2005 shared
task: Semantic role labeling. In Proceedings
of the Ninth Conference on Computational
Natural Language Learning, pages 152?164,
Ann Arbor, MI.
Chambers, Nathanael and Dan Jurafsky.
2008. Unsupervised learning of narrative
event chains. In Proceedings of the
Association for Computational Linguistics,
pages 789?797, Columbus, OH.
Charniak, Eugene and Mark Johnson.
2005. Coarse-to-fine n-best parsing
and MaxEnt discriminative reranking.
In Proceedings of the 43rd Annual Meeting of
the Association for Computational Linguistics,
pages 173?180, Ann Arbor, MI.
Chen, Desai, Nathan Schneider, Dipanjan
Das, and Noah A. Smith. 2010. Semafor:
Frame argument resolution with log-linear
models. In Proceedings of the 5th
International Workshop on Semantic
Evaluation, pages 264?267, Uppsala.
Chen, Zheng and Heng Ji. 2009. Graph-based
event coreference resolution. In Proceedings
of the 2009 Workshop on Graph-based
Methods for Natural Language Processing
(TextGraphs-4), pages 54?57, Suntec.
Chinchor, Nancy, David D. Lewis, and
Lynette Hirschmant. 1993. Evaluating
message understanding systems:
An analysis of the third message
understanding conference. Computational
Linguistics, 19(3):409?450.
Cohen, Jacob. 1960. A coefficient of
agreement for nominal scales.
Educational and Psychological
Measurement, 20(1):37?46.
Dang, Hoa Trang, Diane Kelly, and Jimmy J.
Lin. 2007. Overview of the TREC 2007
question answering track. In Proceedings
of the Fifteenth TREC. Available at
trec.nist.gov/pubs/trec15/t15
proceedings.html.
Di Eugenio, Barbara and Michael Glass.
2004. The kappa statistic: a second look.
Computational Linguistics, 30(1):95?101.
Efron, Bradley and Robert J. Tibshirani. 1993.
An Introduction to the Bootstrap. Chapman
& Hall, New York.
Fan, Rong-En, Kai-Wei Chang, Cho-Jui
Hsieh, Xiang-Rui Wang, and Chih-Jen Lin.
2008. LIBLINEAR: A Library for Large
Linear Classification. Journal of Machine
Learning Research, 9:1871?1874.
Fellbaum, Christiane. 1998. WordNet:
An Electronic Lexical Database (Language,
Speech, and Communication). The MIT Press,
Cambridge, MA.
Fillmore, C. J. and C. F. Baker. 2001.
Frame semantics for text understanding.
In Proceedings of WordNet and Other
Lexical Resources Workshop, NAACL,
Pittsburgh, PA.
Gerber, Matthew and Joyce Chai. 2010.
Beyond NomBank: A study of implicit
arguments for nominal predicates. In
Proceedings of the 48th Annual Meeting
of the Association for Computational
Linguistics, pages 1583?1592, Uppsala.
Gerber, Matthew, Joyce Chai, and Robert
Bart. 2011. A joint model of implicit
arguments for nominal predicates. In
Proceedings of the ACL 2011 Workshop on
Relational Models of Semantics, pages 63?71,
Portland, OR.
795
Computational Linguistics Volume 38, Number 4
Gerber, Matthew, Joyce Chai, and Adam
Meyers. 2009. The role of implicit
argumentation in nominal SRL. In
Proceedings of Human Language Technologies:
The 2009 Annual Conference of the North
American Chapter of the Association for
Computational Linguistics, pages 146?154,
Boulder, CO.
Graesser, Arthur C. and Leslie F. Clark.
1985. Structures and Procedures of Implicit
Knowledge. Ablex Publishing Corporation,
New York.
Graff, David. 2003. English Gigaword.
Linguistic Data Consortium,
Philadelphia, PA.
Grosz, Barbara J., Aravind K. Joshi, and
Scott Weinstein. 1995. Centering:
A framework for modeling the local
coherence of discourse. Computational
Linguistics, 21(2):203?225.
Harris, Zellig. 1985. Distributional structure.
In J. J. Katz, editor, The Philosophy of
Linguistics. Oxford University Press,
New York, pages 26?47.
Heim, Irene and Angelika Kratzer. 1998.
Semantics in Generative Grammar.
Blackwell, Oxford.
Iida, Ryu, Mamoru Komachi, Kentaro Inui,
and Yuji Matsumoto. 2007. Annotating a
Japanese text corpus with predicate-
argument and coreference relations. In
Proceedings of the Linguistic Annotation
Workshop in ACL-2007, pages 132?139,
Prague.
Imamura, Kenji, Kuniko Saito, and
Tomoko Izumi. 2009. Discriminative
approach to predicate?argument structure
analysis with zero-anaphora resolution.
In Proceedings of the ACL-IJCNLP 2009
Conference Short Papers, pages 85?88,
Suntec.
Johansson, Richard and Pierre Nugues. 2008.
Dependency-based syntactic?semantic
analysis with PropBank and NomBank.
In CoNLL 2008: Proceedings of the Twelfth
Conference on Computational Natural
Language Learning, pages 183?187,
Manchester.
Kamp, Hans and Uwe Reyle. 1993. From
Discourse to Logic. Kluwer, Dordrecht.
Kipper, Karin. 2005. VerbNet: A
Broad-coverage, Comprehensive Verb Lexicon.
Ph.D. thesis, Department of Computer
and Information Science, University of
Pennsylvania, Philadelphia.
Krippendorff, Klaus. 1980. Content Analysis:
An Introduction to Its Methodology. Sage
Publications, Thousand Oaks, CA.
Lin, Dekang and Patrick Pantel. 2001.
Discovery of inference rules for
question-answering. Natural Language
Engineering, 7(4):343?360.
Liu, Chang and Hwee Ng. 2007. Learning
predictive structures for semantic role
labeling of nombank. In Proceedings of the
45th Annual Meeting of the Association of
Computational Linguistics, pages 208?215,
Prague.
Marcus, Mitchell, Beatrice Santorini, and
Mary Ann Marcinkiewicz. 1993. Building
a large annotated corpus of English: the
Penn TreeBank. Computational Linguistics,
19:313?330.
Meyers, Adam. 2007. Annotation guidelines
for NomBank?noun argument structure
for PropBank. Technical report, New York
University.
Nielsen, Leif Arda. 2004. Verb phrase
ellipsis detection using automatically
parsed text. In COLING ?04: Proceedings
of the 20th international conference on
Computational Linguistics, pages 1093?1099,
Geneva.
Nielsen, Leif Arda. 2005. A corpus-based
study of Verb Phrase Ellipsis Identification
and Resolution. Ph.D. thesis, King?s
College, London.
NIST, 2008. The ACE 2008 Evaluation Plan.
National Institute of Standards and
Technology, Gaithersburg, MD.
Palmer, Martha S., Deborah A. Dahl,
Rebecca J. Schiffman, Lynette Hirschman,
Marcia Linebarger, and John Dowding.
1986. Recovering implicit information. In
Proceedings of the 24th Annual Meeting of the
Association for Computational Linguistics,
pages 10?19, Morristown, NJ.
Pantel, Patrick, Rahul Bhagat, Bonaventura
Coppola, Timothy Chklovski, and Eduard
Hovy. 2007. ISP: Learning inferential
selectional preferences. In Human Language
Technologies 2007: The Conference of the
North American Chapter of the Association
for Computational Linguistics; Proceedings
of the Main Conference, pages 564?571,
Rochester, NY.
Pantel, Patrick and Deepak Ravichandran.
2004. Automatically labeling semantic
classes. In HLT-NAACL 2004: Main
Proceedings, pages 321?328, Boston, MA.
Pizzato, Luiz Augusto and Diego Molla?.
2008. Indexing on semantic roles for
question answering. In COLING 2008:
Proceedings of the 2nd Workshop on
Information Retrieval for Question
Answering, pages 74?81, Manchester.
796
Gerber and Chai SRL of Implicit Arguments for Nominal Predicates
Prasad, Rashmi, Alan Lee, Nikhil Dinesh,
Eleni Miltsakaki, Geraud Campion,
Aravind Joshi, and Bonnie Webber. 2008.
Penn discourse treebank version 2.0.
Linguistic Data Consortium, University
of Pennsylvania, Philadelphia.
Pudil, P., J. Novovicova, and J. Kittler.
1994. Floating search methods in feature
selection. Pattern Recognition Letters,
15:1119?1125.
Punyakanok, Vasin, Peter Koomen,
Dan Roth, and Wen-tau Yih. 2005.
Generalized inference with multiple
semantic role labeling systems. In
Proceedings of CoNLL-2005 Shared Task,
pages 181?184, Ann Arbor, MI.
Punyakanok, Vasin, Dan Roth, and Wen-Tau
Yih. 2008. The importance of syntactic
parsing and inference in semantic role
labeling. Computational Linguistics,
34(2):257?287.
Resnik, Philip. 1996. Selectional constraints:
An information-theoretic model and its
computational realization. Cognition,
61:127?159.
Ritter, Alan, Mausam, and Oren Etzioni.
2010. A latent dirichlet alocation method
for selectional preferences. In Proceedings of
the 48th Annual Meeting of the Association for
Computational Linguistics, pages 424?434,
Uppsala.
Ruppenhofer, Josef, Caroline Sporleder,
Roser Morante, Collin Baker, and
Martha Palmer. 2009. Semeval-2010
task 10: Linking events and their
participants in discourse. In Proceedings
of the Workshop on Semantic Evaluations:
Recent Achievements and Future
Directions (SEW-2009), pages 106?111,
Boulder, CO.
Ruppenhofer, Josef, Caroline Sporleder,
Roser Morante, Collin Baker, and
Martha Palmer. 2010. Semeval-2010
task 10: Linking events and their
participants in discourse. In Proceedings
of the 5th International Workshop on
Semantic Evaluation, pages 45?50,
Uppsala.
Sagae, Kenji. 2009. Analysis of discourse
structure with syntactic dependencies
and data-driven shift-reduce parsing.
In Proceedings of the 11th International
Conference on Parsing Technologies
(IWPT?09), pages 81?84, Paris.
Sanford, A. J. 1981. Understanding Written
Language. John Wiley & Sons Ltd,
Hoboken, NJ.
Sasano, Ryohei, Daisuke Kawahara, and
Sadao Kurohashi. 2004. Automatic
construction of nominal case frames
and its application to indirect anaphora
resolution. In Proceedings of COLING 2004,
pages 1201?1207, Geneva.
Schank, Roger C. and Robert P. Abelson.
1977. Scripts, Plans, Goals and
Understanding: an Inquiry into Human
Knowledge Structures. Lawrence Erlbaum,
Hillsdale, NJ.
Surdeanu, Mihai, Richard Johansson, Adam
Meyers, Llu??s Ma`rquez, and Joakim Nivre.
2008. The CoNLL 2008 shared task on
joint parsing of syntactic and semantic
dependencies. In CoNLL 2008: Proceedings
of the Twelfth Conference on Computational
Natural Language Learning, pages 159?177,
Manchester.
Szpektor, Idan, Hristo Tanev, Ido Dagan,
and Bonaventura Coppola. 2004. Scaling
Web-based acquisition of entailment
relations. In Proceedings of Empirical
Methods in Natural Language Processing,
pages 41?48, Barcelona.
Tonelli, Sara and Rodolfo Delmonte.
2010. Venses++: Adapting a deep
semantic processing system to the
identification of null instantiations.
In Proceedings of the 5th International
Workshop on Semantic Evaluation,
pages 296?299, Uppsala.
van Dijk, T. A. 1977. Semantic macro
structures and knowledge frames
in discourse comprehension. In
M. A. Just and P. A. Carpenter, editors,
Cognitive Processes in Comprehension.
Lawrence Erlbaum, Hillsdale, NJ,
pages 3?32.
van Dijk, Teun A. and Walter Kintsch. 1983.
Strategies of Discourse Comprehension.
Academic Press, Waltham, MA.
Versley, Yannick, Simone Paolo Ponzetto,
Massimo Poesio, Vladimir Eidelman,
Alan Jern, Jason Smith, Xiaofeng Yang,
and Alessandro Moschitti. 2008.
BART: A modular toolkit for coreference
resolution. In Proceedings of the 6th
International Conference on Language
Resources and Evaluation, pages 9?12,
Marrakech.
Whittemore, Greg, Melissa Macpherson,
and Greg Carlson. 1991. Event-building
through role-filling and anaphora
resolution. In Proceedings of the 29th
Annual Meeting on Association for
Computational Linguistics, pages 17?24,
Morristown, NJ.
Wilson, N. L. 1974. Facts, events, and their
identity conditions. Philosophical Studies,
25:303?321.
797
Computational Linguistics Volume 38, Number 4
Wu, Zhibiao and Martha Palmer. 1994.
Verb semantics and lexical selection.
In Proceedings of the 32nd Annual
Meeting of the Association for
Computational Linguistics,
pages 133?138, Las Cruces, NM.
Yang, Xiaofeng, Jian Su, and Chew Lim Tan.
2008. A twin-candidate model for
learning-based anaphora resolution.
Computational Linguistics, 34(3):327?356.
Zanzotto, Fabio Massimo, Marco
Pennacchiotti, and Maria Teresa Pazienza.
2006. Discovering asymmetric entailment
relations between verbs using selectional
preferences. In ACL-44: Proceedings
of the 21st International Conference on
Computational Linguistics and the 44th
Annual Meeting of the Association for
Computational Linguistics, pages 849?856,
Morristown, NJ.
798
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 710?719,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Autonomous Self-Assessment of Autocorrections: Exploring Text Message
Dialogues
Tyler Baldwin
Department of Computer
Science and Engineering
Michigan State University
East Lansing, MI 48824
baldwi96@cse.msu.edu
Joyce Y. Chai
Department of Computer
Science and Engineering
Michigan State University
East Lansing, MI 48824
jchai@cse.msu.edu
Abstract
Text input aids such as automatic correction
systems play an increasingly important role in
facilitating fast text entry and efficient com-
munication between text message users. Al-
though these tools are beneficial when they
work correctly, they can cause significant
communication problems when they fail. To
improve its autocorrection performance, it is
important for the system to have the capabil-
ity to assess its own performance and learn
from its mistakes. To address this, this pa-
per presents a novel task of self-assessment of
autocorrection performance based on interac-
tions between text message users. As part of
this investigation, we collected a dataset of au-
tocorrection mistakes from true text message
users and experimented with a rich set of fea-
tures in our self-assessment task. Our exper-
imental results indicate that there are salient
cues from the text message discourse that al-
low systems to assess their own behaviors with
high precision.
1 Introduction
The use of SMS text messaging is widespread and
growing. Users of text messaging often rely on small
mobile devices with limited user interfaces to com-
municate with each other. To support efficient com-
munication between users, many tools to aid text in-
put such as automatic completion (autocompletion)
and automatic correction (autocorrection) have be-
come available. When they work correctly, these
tools allow users to maintain clear communication
while potentially increasing the rate at which they
input their message, improving efficiency in com-
munication. However, when these tools make a mis-
take, they can cause problematic situations. Con-
sider the following example:
A1: Euthanasia doing tonight?
B1: Euthanasia?!
A2: I typed whatcha and stupid autotype.
In this example, the automatic correction system
on person A?s phone interpreted his attempt to write
the word whatcha as an attempt to write euthanasia
(due to the keyboard adjacency of the w and e keys,
etc.). This completely changed the meaning of the
message, which confused person B. Although this
instance was eventually discovered and corrected,
the natural flow of conversation was interrupted and
the participants were forced to make extra effort to
clarify this confusion.
This example indicates that the cost of a mistake
in autocorrection is potentially high. This is exacer-
bated by the fact that users will often fail to notice
these mistakes in a timely manner, due to their focus
being on the keyboard (Paek et al, 2010) and the
quick and casual conversation style of text messag-
ing. Because of this, autocorrection systems must
have high accuracy to be useful for text messaging.
This example also indicates that, when an autocor-
rection mistake happens (i.e., mistaken correction
of euthanasia), it often causes confusion which re-
quires dialogue participants to use the follow-up dia-
logue to clarify the intent. What this suggests is that
the discourse between text message users may pro-
vide important information for autocorrection sys-
710
tems to assess whether an attempted correction is
indeed what the user intended to type.
Self-assessment of its correction performance will
allow an autocorrection system to detect correction
mistakes, learn from such mistakes, and potentially
improve its correction performance for future opera-
tions. For instance, if a system is able to identify that
its current autocorrection policy results in too many
mistakes it may choose to adopt a more cautious cor-
rection policy in the future. Additionally, if it is able
to discover not only that a mistake has taken place
but what the ideal action should have been, it will be
able to use this data to learn a more refined policy
for future attempts.
Motivated by this observation, this paper inves-
tigates the novel task of self-assessment of auto-
correction performance based on interactions be-
tween dialogue participants. In particular, we
formulate this task as the automatic identification
of correction mistakes and their corresponding in-
tended words based on the discourse. For instance,
in the previous example, the system should automat-
ically detect that the attempted correction ?euthana-
sia? is a mistake and the true term (i.e., intended
word) should have been ?whatcha?. To support our
investigation, we collected a dataset of autocorrec-
tion mistakes from true text message users. We fur-
ther experimented with a rich set of features in our
self-assessment task. Our experimental results in-
dicate that there are salient cues from the text mes-
sage discourse that potentially allow systems to as-
sess their own behavior with high precision.
In the sections that follow, we first introduce and
give an analysis of our dataset. We then highlight
the two interrelated problems that must be solved for
system self-assessment, and outline and evaluate our
approach to each of these problems. Finally, we ex-
amine the results of applying the system assessment
procedure end-to-end and discuss potential applica-
tions of autocorrection self-assessment.
2 Related Work
Spelling autocorrection systems grew naturally out
of the well studied field of spell checking. Most spell
checking systems are based on a noisy channel for-
mulation (Kernighan et al, 1990). Later refinements
allowed for string edit operations of arbitrary length
(Brill and Moore, 2000) and pronunciation modeling
(Toutanova and Moore, 2002). More recent work
has examined the use of the web as a corpus to build
a spell checking and autocorrection system without
the need for labeled training data (Whitelaw et al,
2009).
Traditional spell checking systems generally as-
sume that misspellings are unintentional. However,
much of the spelling variation that appears in text
messages may be produced intentionally. For in-
stance, text message authors make frequent use of
acronyms and abbreviations. This motivates the
task of text message normalization (Aw et al, 2006;
Kobus et al, 2008), which attempts to transform all
non-standard spellings in a text message into their
standard form. The style of misspelling in text mes-
sages is often quite different from that of standard
prose. For instance, Whitelaw et. al. (2009) applied
the Aspell spell checker1 on a corpus of mistakes in
English prose and achieved an error rate of under
5%. Conversely, the same spell checker was found
to have an error rate of over 75% on text message
data (Choudhury et al, 2007).
Autocorrection in text messaging is similar to pre-
dictive texting and word completion technologies
(Dunlop and Crossan, 2000). These technologies
attempt to reduce the number of keystrokes a user
must type (MacKenzie, 2002), potentially speeding
up text entry. There are 2 primary sources of liter-
ature on text prediction. In one (often called auto-
completion), systems attempt to predict the intended
term before the user has finished typing it (Darragh
et al, 1990; Chaudhuri and Kaushik, 2009). In the
second, the system attempts to interpret ambiguous
user input typed on a keyboard with a small number
of keys, such as the 12 key keyboards found on many
mobile phones (MacKenzie and Tanaka-Ishii, 2007).
Few studies have looked at the effects SMS writing
style has on predictive text performance. How and
Kan (2005) analyze a corpus of 10,000 text mes-
sages and conclude that changing the standard map-
ping of letters to keys on 12 key keyboards could
improve input performance on SMS data.
Although never examined in the context of auto-
correction systems, system self-assessment has been
studied in other domains. One of the most com-
1http://aspell.net/
711
Figure 1: Example text message dialogue from our cor-
pus with an automatic correction mistake
mon application domains is spoken dialogue sys-
tems (Levow, 1998; Hirschberg et al, 2001; Litman
et al, 2006), where detecting problematic situations
can help the system better adapt to user behavior.
These systems often make use of prosody and task
specific dialogue acts, two feature sources unavail-
able in general text message dialogues.
In summary, while a large body of work addresses
similar problems, to our knowledge no previous
work has looked into the aspect of self-assessment
of autocorrection based on dialogues between text
message users. The work presented in this paper
represents a first step in this direction.
3 Data Set
To support our investigation, we collected a cor-
pus of data containing true experiences with auto-
correction provided by text message users. The
website ?Damn You Auto Correct?2 (DYAC) posts
screenshots of text message conversations that con-
tain mistakes caused by phone automatic correction
systems, as sent in by cellphone users. An example
screenshot is shown is Figure 1.
Speech bubbles originating from the left of the
image in Figure 1 are messages sent by one dialogue
participant while those originating from the right of
the image are sent by the other. In this example, the
automatic correction system incorrectly decides that
the user?s attempt to write the non-standard word
form thaaaats was an attempt to write the word Tus-
saud. This confuses the reader, and several dialogue
turns are used to resolve the confusion. The author
2www.damnyouautocorrect.com
explicitly corrects her mistake by writing ?I meant
thaaaats?.
Note that, in this example, the word Tussaud
could be an autocompletion or an autocorrection
by the system. However, there may be no signifi-
cant distinction between these two operations from
a user?s point of view. These two operations could
also take place at the same time. For instance, a
system may both suggest possible completions after
the user has only typed a small number of characters
and perform autocorrection once the user presses the
space bar to go on to the next word. Therefore, for
the purposes of our discussion here, we use autocor-
rection to refer to any changes made by the system
(either by autocompletion or autocorrection) with-
out the user explicitly selecting the correction them-
selves.
Throughout the paper, we use the term attempted
correction to refer to any autocorrection made by
the system; for example, Tussaud is an attempted
correction in Figure 1. Some attempted corrections
could correct to the word that the user intended,
which will be referred to as unproblematic cor-
rections or non-erroneous corrections. Other at-
tempted corrections may mistakenly choose a word
that the user did not intend to write, which will be re-
ferred to as correction mistakes or erroneous cor-
rections. For example, Tussaud is an erroneous cor-
rection. We use the term intended word to refer to
the term that the user was attempting to type when
the autocorrection system intervened. For instance,
in the erroneous correction in Figure 1, the intended
term was thaaaats.
To build our dataset, screenshots were extracted
from the site and transcribed, and correction mis-
takes were annotated with their intended words, if
the intended word appeared in the dialogue. Be-
cause the website presents autocorrection mistakes
that submitters find to be humorous or egregious,
there may be an incentive for users to submit fal-
sified instances. To combat this, we performed an
initial filtering phase to remove instances that were
unlikely to have been produced by a typical autocor-
rection system (e.g., instances that substituted letters
that were far from each other on the keyboard and
not phonetically similar) or that were otherwise be-
lieved to be falsified. Using this methodology we
compiled a development set of 300 dialogues and an
712
Figure 2: Text message dialogue with several correction
mistakes for the same intended term.
additional 635 dialogues for evaluation.
Some dialogues contained several correction mis-
takes. It was common for multiple correction mis-
takes to be produced in an attempt at typing a single
word; an example is shown in Figure 2, in which
the intended term cookies is erroneously corrected
at first as commies and then as cockles.
We will use the term message to refer to one SMS
text message sent in the course of the conversation,
while a turn encompasses all messages sent by a user
between messages from the other participant. For
instance, the first 3 speech bubbles in Figure 2 all
represent separate messages, but they are all part of
the same turn.
While this dataset provides us with instances of
autocorrection mistakes, in order to differentiate be-
tween problematic and unproblematic correction at-
tempts we will need a dataset of unproblematic at-
tempts as well. It should be noted that, from the per-
spective of the reader, a successful autocorrection at-
tempt is equivalent to the user typing correctly with-
out any intervention from the system at all. To build
a dataset of unproblematic instances, we collected
text message conversations from pairs of users with-
out the aid of autocorrection. Users were then asked
to correct any mistakes they produced. Snippets
of these conversations that did not contain mistakes
were then extracted to act as a set of unproblematic
autocorrection instances. In total 554 snippets were
extracted. These snippets were combined with the
problematic instances from the DYAC data to make
the final dataset used for training and evaluation.
4 Autocorrection Self-Assessment
It is desirable for an autocorrection system to have
the capability to assess its own performance. For
each correction attempt it makes, if the system can
evaluate its performance based on the dialogue it can
acquire valuable information to learn from its own
mistakes and thus improve its performance for fu-
ture operations. Next we describe how we formulate
the task of self-assessment and what features can be
used for this task.
Because each correction attempt is system gener-
ated, an autocorrection system should have knowl-
edge of all correction attempts it has made. Let C
be the set of all correction attempts performed by an
autocorrection system over the course of a dialogue
and let W be the set of all words in this dialogue
which occur after the correction attempt. We model
this problem as two distinct subtasks: 1) identify at-
tempted corrections ci ? C which are erroneous (if
there are any), and 2) for each erroneous correction
ci, identify a word wj ? W which is the intended
word for ci (i.e., Intended(ci) = wj).
4.1 Identifying Erroneous Corrections
The first task involves a simple binary decision;
given an arbitrarily sized dialogue snippet contain-
ing an automatic correction attempt, we must decide
whether or not the system acted erroneously when
making the correction. We thus model the task as
a binary classification problem in which we classify
every correction attempt c ? C as either erroneous
or non-erroneous.
The proposed method follows a standard proce-
dure for supervised binary classification. First we
must build a set of labeled training data in which
each instance is represented as a vector of features
and a ground truth class label. Given this, we can
train a classifier to differentiate between the two
classes. For the purposes of this work we use a sup-
port vector machine (SVM) classifier.
4.1.1 Feature Set
In order to detect problematic corrections, we
must identify dialogue behaviors that signify an er-
ror has occurred. We examined the dialogues in
our development set to understand which dialogue
behaviors are indicative of autocorrection mistakes.
While in unproblematic dialogues users are able to
converse freely, in problematic dialogues users must
spend dialogue turns reestablishing common ground
(Clark, 1996). Our feature set will focus on two
common ways these attempts to establish common
713
ground manifest themselves: as confusion and as at-
tempts to correct the mistake.
Confusion Detection Features. Because autocor-
rection mistakes often result in misleading or se-
mantically vacuous utterances, they are apt to con-
fuse the reader, who will often express this confu-
sion in the dialogue in order to gain clarification.
These features examine the dialogue of the uncor-
rected user (the dialogue participant that reads the
automatic correction mistake, not the one that was
automatically corrected). One sign of confusion is
the use of the question mark, so one feature captured
the presence of question marks in the messages sent
by the uncorrected user. Similarly, users may often
use a block or repeated punctuation of show suprise
or confusion, so another feature detected instances
of repeated question marks and exclamation points
(???, !?!!, etc.). When confused, readers will often
retype the confusing word as a request for clarifica-
tion (e.g., Tussaud?), or simply type ?what??. We
therefore include features that detect whether or not
the corrected term appears in the first message sent
by the uncorrected user after the correction mistake
has occurred, and whether or not this message con-
tains the word ?what? as its own clause.
Clarification Detection Features. In contrast to ut-
terances of confusion which are generally produced
by the reader of the autocorrection mistake, clarifi-
cation attempts are usually initiated by the user that
was corrected. Several methods are used to indicate
that the term shown by the system was incorrect.
One convention is to use an asterisk (*) either be-
fore or after the corrected term:
A1: Indeed Sid
A2: Sir*
Another common method is to explicitly state
what was intended using phrases such as ?I meant
to type?, ?that was supposed to say?, etc. We in-
cluded several features to capture these word pat-
terns. Another method is to simply quickly reply
with the word that was intended, so we included a
feature to record whether the next message after the
correction attempt contains only a single word. As
users often feel the need to explain why the mistake
occurred, we included a feature that recorded any
mention or autocompletion, autocorrection or spell
Features Precision Recall F-Measure
All Features .861 .751 .803
-Confusion .857 .725 .786
-Clarification .848 .676 .752
-Dialogue .896 .546 .679
Baseline .568 1 .724
Table 1: Feature ablation results for identifying autocor-
rection mistakes
checking. One additional feature recorded whether
or not the corrected user?s dialogue contained words
written in all capital letters.
Dialogue Features. A few features captured infor-
mation more closely tied to the flow of the dialogue
than to confusion or clarification. In our develop-
ment set, we observed a few common dialogue for-
mats. In one, a correction mistake is immediately
followed by confusion, which is then immediately
followed by clarification. The dialogue in Figure 1
gives an example of this. To capture this form, we
included a feature that recorded whether a confusion
feature was present in the message immediately fol-
lowing the correction attempt and whether a clarifi-
cation feature was present in the message immediate
following the confusion message. Similarly, clarifi-
cation attempts are often tried immediately after the
mistake even if no confusion was present, so an ad-
ditional feature captured whether the first message
after the mistake by the corrected user was a clari-
fication attempt. Additionally, we observed that au-
tocorrection mistakes frequently appeared in the last
word in a message, which was recorded by another
binary feature. Finally, we recorded a count of how
often the corrected term appeared in the dialogue.
4.1.2 Evaluation
To build our classifier we used the SVMLight3
implementation of a support vector machine clas-
sifier with an RBF kernel. To ensure validity and
account for the relatively small size of our dataset,
evaluation was done via leave-one-out cross valida-
tion.
Results are shown in Table 1. A majority class
baseline is given for comparison. As shown, using
the entire feature set, the classifier achieves above
baseline precision of 0.861, while still producing re-
call of 0.751.
3Version 6.02, http://svmlight.joachims.org/
714
Although F-measure is reported, it is unlikely that
precision and recall should be weighted equally. Be-
cause one of the primary reasons we may wish to
detect problematic situations is to automatically col-
lect data to improve future performance by the au-
tocorrection system, it is imperative that the data
collected have high precision in order to reduce the
amount of noise present in the collected dataset.
Conversely, because problematic situation detection
can monitor a user?s input continuously for an in-
definite period of time in order to collect more data,
recall is less of a concern.
To study the effect of each feature source, we per-
formed a feature ablation study, the results of which
are included in Table 1. For each run, one feature
type was removed and the model was retrained and
reassessed. As shown, removing any feature source
has a relatively small effect on the precision but a
more substantial effect on the recall. Confusion de-
tection features seem to be the least essential, caus-
ing a comparatively small drop in precision and re-
call values when removed. Removing the dialogue
features results in the greatest drop in recall, return-
ing only slightly above half of the problematic in-
stances. However, as a result, the precision of the
classifier is higher than when all features are used.
4.2 Identifying The Intended Term
Note that one purpose of the proposed self-
assessment is to collect information online and thus
make it possible to build better models. In order
to do so, we need to know not only whether the
system acted erroneously, but also what it should
have done.Therefore, once we have extracted a set of
problematic instances (and their corresponding dia-
logues), we must identify the term which the user
was attempting to type when the system intervened.
First, assume that via the classification task de-
scribed in Section 4.1 we have identified a set of er-
roneous correction attempts, EC. Now the problem
becomes, for every erroneous correction c ? EC,
identify w ? W such that w = Intended(c). We
model this as a ranking task, in which all w ? W
are ranked by their likelihood of being the intended
term for c. We then predict that the top ranked word
is the true intended term.
4.2.1 Feature Set
To support the above processing, we explored a
diverse feature set, consisting of five different fea-
ture sources: contextual, punctuation, word form,
similarity, and pattern features, crafted from an ex-
amination of our development data. Several of the
features are related to those used in the initial clas-
sification phase. However, unlike our classification
features, these feature focus on the relationship be-
tween the erroneous correction c and a candidate in-
tended term w.
Contextual Features. Contextual features capture
relevant phenomena at the discourse level. After an
error is discovered by a user, they may type an in-
tended term several times or type it in a message by
itself in order to draw attention to it. These phe-
nomena are captured in the word repetition and only
word features. Another common discourse related
correction technique is to retype some of the origi-
nal context, which is captured by the word overlap
feature. The same author feature indicates whether
c and w are written by the same author. The author
of the original mistake is likely the one to correct it,
as they know their true intent.
Punctuation Features. Punctuation is occasionally
used by text message writers to signal a correction of
an earlier mistake, as noted previously. We included
features to capture the presence of several different
punctuation marks occurring before or after a candi-
date word such as *,?,!, etc. Each punctuation mark
is represented by a separate feature.
Word Form Features. Word form features cap-
ture variations in how a word is written. One word
form feature captures whether a word was typed in
all capital letters, a technique used by text message
writers to add emphasis. Two word form features
were designed to capture words that were potentially
unknown to the system, out-of-vocabulary words
and words with letter repetition (e.g., ?yaaay?). Be-
cause the system does not know these words, it
will consider them misspellings and may attempt to
change them to an in-vocabulary term.
Similarity Features. Our similarity feature cap-
tured the character level distance between a word
changed by the system and a candidate intended
word. We calculated the normalized levenshtein edit
distance between the two words as a measure of sim-
715
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  0.2  0.4  0.6  0.8  1
Pre
cis
ion
Recall
All Features
-Contextual
-Punctuation
-SimilarityBaseline
Figure 3: Precision-recall curve for intended term selec-
tion, including feature ablation results
ilarity.
Pattern Features. Pattern features attempt to cap-
ture phrases that are used to explicitly state a cor-
rection. These include phrases such as ?(I) meant
to write w?, ?(that was) supposed to say w?, ?(that)
should have read w?, ?(I) wrote w?, etc.
4.2.2 Evaluation
To find the most likely intended term for a cor-
rection mistake, we rank every candidate word in W
and predict that the top ranked word is the intended
term. We used the ranking mode of SVMlight to
train our ranker. By thresholding our results to only
trust predictions in which the ranker reported a high
ranking value for the top term, we were able to ex-
amine the precision at different recall levels. That
is, if the top ranked term does not meet the thresh-
old, we simply do not predict an intended term for
that instance, hurting recall but hopefully improv-
ing precision by removing instances that we are not
confident about. This thresholding process may also
allow the ranker to exclude instances in which the in-
tended term does not appear in the dialogue, which
are hopefully ranked lower than other cases. As be-
fore, evaluation was done via leave-one-out cross
validation.
Results are shown in Figure 3. As a method
of comparison we report a baseline that selects the
word with the smallest edit distance as the intended
term. As shown, using the entire feature set results
in consistently above baseline performance.
As before, we are more concerned with the pre-
cision of our predictions than the recall. It is diffi-
cult to assess the appropriate precision-recall trade-
off without an in-depth study of autocorrection us-
age by text messagers. However, a few observations
can be made from the precision-recall curve. Most
critically, we can observe that the model is able to
predict the intended term for an erroneous correc-
tion with high precision. Additionally, the precision
stays relatively stable as recall increases, suffering
a comparatively small drop in precision for an in-
crease in recall. At its highest achieved recall values
of 0.892, it maintains high precision at 0.869.
Feature ablation results are also reported in Fig-
ure 3. The most critical feature source was word
similarity; without the similarity feature the perfor-
mance is consistently worse than all other runs, even
falling below baseline performance at high recall
levels. This is not suprising, as the system?s incor-
rect guess must be at least reasonably similar to the
intended term, or the system would be unlikely to
make this mistake. Although not as substantial as
the similarity feature, the contextual and punctuation
features were also shown to have a significant effect
on overall performance. Conversely, removing word
form or pattern features did not cause a significant
change in performance (not shown in Figure 3 to en-
hance readability).
5 An End-To-End System
In order to see the actual effect of the full system,
we ran it end-to-end, with the output of the initial
erroneous correction identification phase used as in-
put when identifying the intended term. Results are
shown in Figure 4. The results of the intended term
classification task on gold standard data from Figure
3 are shown as an upper bound.
As expected, the full end-to-end system produced
lower overall performance than running the tasks in
isolation. The end-to-end system can reach a recall
level of 0.674, significantly lower than the recall of
the ground truth system. However, the system still
peaks at precision of 1, and was able to produce pre-
cision values that were competitive with the ground
truth system at lower recall levels, maintaining a pre-
cision of above 0.90 until recall reached 0.396.
It is worth mentioning that the current evalua-
716
 0
 0.2
 0.4
 0.6
 0.8
 1
 0  0.2  0.4  0.6  0.8  1
Pre
cis
ion
Recall
Gold StandardEnd To End
Figure 4: Precision-recall curve for the end-to-end sys-
tem
tion is based on a balanced dataset with roughly
even numbers of problematic and unproblematic in-
stances. It is likely that in a realistic setting an au-
tocorrection system will get many more instances
correct than wrong, leading to a data distribution
skewed in favor of unproblematic instances. This
suggests that the evaluation given here may overes-
timate the performance of a self-assessment system
in a real scenario. Although the size of our dataset
is insufficient to do a full analysis on skewed data,
we can get a rough estimate of the performance by
simply counting false positives and false negatives
unevenly. For instance, if the cost of mispredicting
a unproblematic case as problematic is nine times
more severe than the cost of missing a problematic
case, this can give us an estimate of the performance
of the system on a dataset with a 90-10 skew.
We examined the 90-10 skew case to see if the
procedure outlined here was still viable. Results of
an end-to-end system with this data skew are con-
sistently lower than the balanced data case. The
skewed data system can keep performance of 90%
or better until it reaches 13% recall, and 85% or bet-
ter until it reaches 22%. These results suggest that
the system could still potentially be utilized. How-
ever, its performance drops off steadily, to the point
where it would be unlikely to be useful at higher re-
call levels. We leave the full exploration of this to
future work, which can utilize larger data sets to get
a more accurate understanding of the performance.
6 Discussion
When an autocorrection system attempts a correc-
tion, it has perfect knowledge of the behavior of both
itself and the user. It knows the button presses the
user used to enter the term. It knows the term it
chose as a correction. It knows the surrounding con-
text; it has access to both the messages sent and re-
ceived by the user. It has a large amount of the infor-
mation it could use to improve its own performance,
if only it were able to know when it made a mis-
take. The techniques described here attempt to ad-
dress this critical system assessment step. Users may
vary in the speed and accuracy at which they type,
and input on small or virtual keyboards may vary
between users based on the size and shape of their
fingers. The self-assessment task described here can
potentially facilitate the development of autocorrec-
tion models that are tailored to specific user behav-
iors.
Here is a brief outline of how our self-assessment
module might potentially be used in building user-
specific correction models. As a user types input, the
system performs autocorrection by starting with a
general model (e.g., for all text message users). Each
time a correction is performed, the system exam-
ines the surrounding context to determine whether
the correction it chose was actually what the user
had intended to type. Over the course of several
dialogues, the system builds a corpus of erroneous
and non-erroneous correction attempts. This corpus
is then used to train a user-specific correction model
that is targeted toward system mistakes that are most
frequent with this user?s input behavior. The user-
specific model is then applied on future correction
attempts to improve overall performance. This mon-
itoring process can be continued for months or even
longer. The results from self-assessment will al-
low the system to continuously and autonomously
improve itself for a given user (Baldwin and Chai,
2012).
In order to learn a user-specific model that is ca-
pable of improving performance, it is important that
the self-assessment system provides it with training
data without a large amount of noise. This suggests
that the self-assessment system must be able to iden-
tify erroneous instances with high precision. Con-
versely, because the system can monitor user behav-
717
ior indefinitely to collect more data, the overall re-
call may not be as critical. It might then be reason-
able for a self-assessment system to be built to focus
on collecting high accuracy pairs, even if it misses
many system mistakes. Although a full examination
of this tradeoff is left for future work which may
more closely examine user input behavior, we feel
that the results presented here show promise for col-
lecting accurate data in a timely manner.
7 Conclusions and Future Work
This paper describes a novel problem of assessing
its own correction performance for an autocorrection
system based on dialogue between two text mes-
saging users. Our evaluation results indicate that
given a problematic situation caused by an auto-
correction system, the discourse between users pro-
vides important cues for the system to automati-
cally assess its own correction performance. By
exploring a rich set of features from the discourse,
our proposed approach is able to both differentiate
between problematic and unproblematic instances
and identify the term the user intended to type with
high precision, achieving significantly above base-
line performance. As discussed in Section 6, this
self-assessment task can potentially be important for
building user-specific autocorrection models to im-
prove auto-correction performance.
The results presented in this paper represent a
first look at autocorrection self-assessment. There
are several areas of future work. There is certainly
a need to examine additional feature sources. Be-
cause automatic correction mistakes can potentially
create semantically vacuous utterances, a computa-
tional semantics based approach, similar to those
used in semantic autocompletion systems (Hyvnen
and Mkel, 2006), may prove fruitful. Addition-
ally, although this work focused solely on dialogue-
related features, future work may wish to take a
closer look at the autocorrection mistakes them-
selves (e.g., which words are most likely to be mis-
takenly corrected, etc.). Lastly, although our current
work demonstrated some potential, more thorough
evaluation in realistic settings will allow a more full
understanding of the impact and limitations of the
proposed self-assessment approach.
Acknowledgments
This work was supported in part by Award #0957039
from the National Science Foundation and Award
#N00014-11-1-0410 from the Office of Naval Re-
search. The authors would like to thank the review-
ers for their valuable comments and suggestions.
References
AiTi Aw, Min Zhang, Juan Xiao, and Jian Su. 2006. A
phrase-based statistical model for sms text normaliza-
tion. In Proceedings of the COLING/ACL on Main
conference poster sessions, pages 33?40, Morristown,
NJ, USA. Association for Computational Linguistics.
Tyler Baldwin and Joyce Chai. 2012. Towards on-
line adaptation and personalization of key-target resiz-
ing for mobile devices. In Proceedings of the 2012
ACM international conference on Intelligent User In-
terfaces, IUI ?12, pages 11?20, New York, NY, USA.
ACM.
Eric Brill and Robert C. Moore. 2000. An improved
error model for noisy channel spelling correction. In
ACL ?00: Proceedings of the 38th Annual Meeting
on Association for Computational Linguistics, pages
286?293, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Surajit Chaudhuri and Raghav Kaushik. 2009. Extend-
ing autocompletion to tolerate errors. In Proceed-
ings of the 35th SIGMOD international conference on
Management of data, SIGMOD ?09, pages 707?718,
New York, NY, USA. ACM.
Monojit Choudhury, Rahul Saraf, Vijit Jain, Animesh
Mukherjee, Sudeshna Sarkar, and Anupam Basu.
2007. Investigation and modeling of the structure
of texting language. Int. J. Doc. Anal. Recognit.,
10(3):157?174.
Herbert H. Clark. 1996. Using Language. Cambridge
University Press.
J.J. Darragh, I.H. Witten, and M.L. James. 1990. The
reactive keyboard: a predictive typing aid. Computer,
23(11):41 ?49, November.
Mark Dunlop and Andrew Crossan. 2000. Predictive text
entry methods for mobile phones. Personal and Ubiq-
uitous Computing, 4:134?143. 10.1007/BF01324120.
Julia Hirschberg, Diane J. Litman, and Marc Swerts.
2001. Identifying user corrections automatically in
spoken dialogue systems. In Proceedings of the Sec-
ond Meeting of the North American Chapter of the As-
sociation for Computational Linguistics.
Yijue How and Min yen Kan. 2005. Optimizing pre-
dictive text entry for short message service on mobile
718
phones. In in Human Computer Interfaces Interna-
tional (HCII 05). 2005: Las Vegas.
Eero Hyvnen and Eetu Mkel. 2006. Semantic autocom-
pletion. In Proceedings of the first Asia Semantic Web
Conference (ASWC 2006, pages 4?9. Springer-Verlag.
Mark D. Kernighan, Kenneth W. Church, and William A.
Gale. 1990. A spelling correction program based on a
noisy channel model. In Proceedings of the 13th con-
ference on Computational linguistics, pages 205?210,
Morristown, NJ, USA. Association for Computational
Linguistics.
Catherine Kobus, Franc?ois Yvon, and Ge?raldine
Damnati. 2008. Normalizing SMS: are two metaphors
better than one ? In Proceedings of the 22nd Interna-
tional Conference on Computational Linguistics (Col-
ing 2008), pages 441?448, Manchester, UK, August.
Coling 2008 Organizing Committee.
Gina-Anne Levow. 1998. Characterizing and recogniz-
ing spoken corrections in human-computer dialogue.
In Proceedings of the 36th Annual Meeting of the As-
sociation for Computational Linguistics and 17th In-
ternational Conference on Computational Linguistics,
Volume 1, pages 736?742, Montreal, Quebec, Canada,
August. Association for Computational Linguistics.
Diane Litman, Julia Hirschberg, and Marc Swerts. 2006.
Characterizing and predicting corrections in spoken
dialogue systems. Comput. Linguist., 32:417?438,
September.
I. Scott MacKenzie and Kumiko Tanaka-Ishii. 2007.
Text Entry Systems: Mobility, Accessibility, Universal-
ity (Morgan Kaufmann Series in Interactive Technolo-
gies). Morgan Kaufmann Publishers Inc., San Fran-
cisco, CA, USA.
I. Scott MacKenzie. 2002. Kspc (keystrokes per charac-
ter) as a characteristic of text entry techniques. In Pro-
ceedings of the 4th International Symposium on Mo-
bile Human-Computer Interaction, Mobile HCI ?02,
pages 195?210, London, UK. Springer-Verlag.
Tim Paek, Kenghao Chang, Itai Almog, Eric Badger,
and Tirthankar Sengupta. 2010. A practical exami-
nation of multimodal feedback and guidance signals
for mobile touchscreen keyboards. In Proceedings of
the 12th international conference on Human computer
interaction with mobile devices and services, Mobile-
HCI ?10, pages 365?368, New York, NY, USA. ACM.
Kristina Toutanova and Robert Moore. 2002. Pronun-
ciation modeling for improved spelling correction. In
40th Annual Meeting of the Association for Computa-
tional Linguistics(ACL 2002).
Casey Whitelaw, Ben Hutchinson, Grace Y Chung, and
Ged Ellis. 2009. Using the Web for language indepen-
dent spellchecking and autocorrection. In Proceedings
of the 2009 Conference on Empirical Methods in Nat-
ural Language Processing, pages 890?899, Singapore,
August. Association for Computational Linguistics.
719
Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1583?1592,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Beyond NomBank:
A Study of Implicit Arguments for Nominal Predicates
Matthew Gerber and Joyce Y. Chai
Department of Computer Science
Michigan State University
East Lansing, Michigan, USA
{gerberm2,jchai}@cse.msu.edu
Abstract
Despite its substantial coverage, Nom-
Bank does not account for all within-
sentence arguments and ignores extra-
sentential arguments altogether. These ar-
guments, which we call implicit, are im-
portant to semantic processing, and their
recovery could potentially benefit many
NLP applications. We present a study of
implicit arguments for a select group of
frequent nominal predicates. We show that
implicit arguments are pervasive for these
predicates, adding 65% to the coverage of
NomBank. We demonstrate the feasibil-
ity of recovering implicit arguments with
a supervised classification model. Our re-
sults and analyses provide a baseline for
future work on this emerging task.
1 Introduction
Verbal and nominal semantic role labeling (SRL)
have been studied independently of each other
(Carreras and Ma`rquez, 2005; Gerber et al, 2009)
as well as jointly (Surdeanu et al, 2008; Hajic? et
al., 2009). These studies have demonstrated the
maturity of SRL within an evaluation setting that
restricts the argument search space to the sentence
containing the predicate of interest. However, as
shown by the following example from the Penn
TreeBank (Marcus et al, 1993), this restriction ex-
cludes extra-sentential arguments:
(1) [arg0 The two companies] [pred produce]
[arg1 market pulp, containerboard and white
paper]. The goods could be manufactured
closer to customers, saving [pred shipping]
costs.
The first sentence in Example 1 includes the Prop-
Bank (Kingsbury et al, 2002) analysis of the ver-
bal predicate produce, where arg0 is the agentive
producer and arg1 is the produced entity. The sec-
ond sentence contains an instance of the nominal
predicate shipping that is not associated with argu-
ments in NomBank (Meyers, 2007).
From the sentences in Example 1, the reader can
infer that The two companies refers to the agents
(arg0) of the shipping predicate. The reader can
also infer that market pulp, containerboard and
white paper refers to the shipped entities (arg1
of shipping).1 These extra-sentential arguments
have not been annotated for the shipping predi-
cate and cannot be identified by a system that re-
stricts the argument search space to the sentence
containing the predicate. NomBank also ignores
many within-sentence arguments. This is shown
in the second sentence of Example 1, where The
goods can be interpreted as the arg1 of shipping.
These examples demonstrate the presence of argu-
ments that are not included in NomBank and can-
not easily be identified by systems trained on the
resource. We refer to these arguments as implicit.
This paper presents our study of implicit ar-
guments for nominal predicates. We began our
study by annotating implicit arguments for a se-
lect group of predicates. For these predicates, we
found that implicit arguments add 65% to the ex-
isting role coverage of NomBank.2 This increase
has implications for tasks (e.g., question answer-
ing, information extraction, and summarization)
that benefit from semantic analysis. Using our an-
notations, we constructed a feature-based model
for automatic implicit argument identification that
unifies standard verbal and nominal SRL. Our re-
sults indicate a 59% relative (15-point absolute)
gain in F1 over an informed baseline. Our analy-
ses highlight strengths and weaknesses of the ap-
proach, providing insights for future work on this
emerging task.
1In PropBank and NomBank, the interpretation of each
role (e.g., arg0) is specific to a predicate sense.
2Role coverage indicates the percentage of roles filled.
1583
In the following section, we review related re-
search, which is historically sparse but recently
gaining traction. We present our annotation effort
in Section 3, and follow with our implicit argu-
ment identification model in Section 4. In Section
5, we describe the evaluation setting and present
our experimental results. We analyze these results
in Section 6 and conclude in Section 7.
2 Related work
Palmer et al (1986) made one of the earliest at-
tempts to automatically recover extra-sentential
arguments. Their approach used a fine-grained do-
main model to assess the compatibility of candi-
date arguments and the slots needing to be filled.
A phenomenon similar to the implicit argu-
ment has been studied in the context of Japanese
anaphora resolution, where a missing case-marked
constituent is viewed as a zero-anaphoric expres-
sion whose antecedent is treated as the implicit ar-
gument of the predicate of interest. This behavior
has been annotated manually by Iida et al (2007),
and researchers have applied standard SRL tech-
niques to this corpus, resulting in systems that
are able to identify missing case-marked expres-
sions in the surrounding discourse (Imamura et
al., 2009). Sasano et al (2004) conducted sim-
ilar work with Japanese indirect anaphora. The
authors used automatically derived nominal case
frames to identify antecedents. However, as noted
by Iida et al, grammatical cases do not stand in
a one-to-one relationship with semantic roles in
Japanese (the same is true for English).
Fillmore and Baker (2001) provided a detailed
case study of implicit arguments (termed null in-
stantiations in that work), but did not provide con-
crete methods to account for them automatically.
Previously, we demonstrated the importance of fil-
tering out nominal predicates that take no local ar-
guments (Gerber et al, 2009); however, this work
did not address the identification of implicit ar-
guments. Burchardt et al (2005) suggested ap-
proaches to implicit argument identification based
on observed coreference patterns; however, the au-
thors did not implement and evaluate such meth-
ods. We draw insights from all three of these
studies. We show that the identification of im-
plicit arguments for nominal predicates leads to
fuller semantic interpretations when compared to
traditional SRL methods. Furthermore, motivated
by Burchardt et al, our model uses a quantitative
analysis of naturally occurring coreference pat-
terns to aid implicit argument identification.
Most recently, Ruppenhofer et al (2009) con-
ducted SemEval Task 10, ?Linking Events and
Their Participants in Discourse?, which evaluated
implicit argument identification systems over a
common test set. The task organizers annotated
implicit arguments across entire passages, result-
ing in data that cover many distinct predicates,
each associated with a small number of annotated
instances. In contrast, our study focused on a se-
lect group of nominal predicates, each associated
with a large number of annotated instances.
3 Data annotation and analysis
3.1 Data annotation
Implicit arguments have not been annotated within
the Penn TreeBank, which is the textual and syn-
tactic basis for NomBank. Thus, to facilitate
our study, we annotated implicit arguments for
instances of nominal predicates within the stan-
dard training, development, and testing sections of
the TreeBank. We limited our attention to nom-
inal predicates with unambiguous role sets (i.e.,
senses) that are derived from verbal role sets. We
then ranked this set of predicates using two pieces
of information: (1) the average difference between
the number of roles expressed in nominal form (in
NomBank) versus verbal form (in PropBank) and
(2) the frequency of the nominal form in the cor-
pus. We assumed that the former gives an indica-
tion as to how many implicit roles an instance of
the nominal predicate might have. The product of
(1) and (2) thus indicates the potential prevalence
of implicit arguments for a predicate. To focus our
study, we ranked the predicates in NomBank ac-
cording to this product and selected the top ten,
shown in Table 1.
We annotated implicit arguments document-by-
document, selecting all singular and plural nouns
derived from the predicates in Table 1. For each
missing argument position of each predicate in-
stance, we inspected the local discourse for a suit-
able implicit argument. We limited our attention to
the current sentence as well as all preceding sen-
tences in the document, annotating all mentions of
an implicit argument within this window.
In the remainder of this paper, we will use iargn
to refer to an implicit argument position n. We
will use argn to refer to an argument provided by
PropBank or NomBank. We will use p to mark
1584
Pre-annotation Post-annotation
Role average
Predicate # Role coverage (%) Noun Verb Role coverage (%) Noun role average
price 217 42.4 1.7 1.7 55.3 2.2
sale 185 24.3 1.2 2.0 42.0 2.1
investor 160 35.0 1.1 2.0 54.6 1.6
fund 109 8.7 0.4 2.0 21.6 0.9
loss 104 33.2 1.3 2.0 46.9 1.9
plan 102 30.9 1.2 1.8 49.3 2.0
investment 102 15.7 0.5 2.0 33.3 1.0
cost 101 26.2 1.1 2.3 47.5 1.9
bid 88 26.9 0.8 2.2 72.0 2.2
loan 85 22.4 1.1 2.5 41.2 2.1
Overall 1,253 28.0 1.1 2.0 46.2 1.8
Table 1: Predicates targeted for annotation. The second column gives the number of predicate instances
annotated. Pre-annotation numbers only include NomBank annotations, whereas Post-annotation num-
bers include NomBank and implicit argument annotations. Role coverage indicates the percentage of
roles filled. Role average indicates how many roles, on average, are filled for an instance of a predicate?s
noun form or verb form within the TreeBank. Verbal role averages were computed using PropBank.
predicate instances. Below, we give an example
annotation for an instance of the investment predi-
cate:
(2) [iarg0 Participants] will be able to transfer
[iarg1 money] to [iarg2 other investment
funds]. The [p investment] choices are
limited to [iarg2 a stock fund and a
money-market fund].
NomBank does not associate this instance of in-
vestment with any arguments; however, we were
able to identify the investor (iarg0), the thing in-
vested (iarg1), and two mentions of the thing in-
vested in (iarg2).
Our data set was also independently annotated
by an undergraduate linguistics student. For each
missing argument position, the student was asked
to identify the closest acceptable implicit argu-
ment within the current and preceding sentences.
The argument position was left unfilled if no ac-
ceptable constituent could be found. For a miss-
ing argument position, the student?s annotation
agreed with our own if both identified the same
constituent or both left the position unfilled. Anal-
ysis indicated an agreement of 67% using Cohen?s
kappa coefficient (Cohen, 1960).
3.2 Annotation analysis
Role coverage for a predicate instance is equal to
the number of filled roles divided by the number
of roles in the predicate?s lexicon entry. Role cov-
erage for the marked predicate in Example 2 is
0/3 for NomBank-only arguments and 3/3 when
the annotated implicit arguments are also consid-
ered. Returning to Table 1, the third column gives
role coverage percentages for NomBank-only ar-
guments. The sixth column gives role coverage
percentages when both NomBank arguments and
the annotated implicit arguments are considered.
Overall, the addition of implicit arguments created
a 65% relative (18-point absolute) gain in role cov-
erage across the 1,253 predicate instances that we
annotated.
The predicates in Table 1 are typically associ-
ated with fewer arguments on average than their
corresponding verbal predicates. When consid-
ering NomBank-only arguments, this difference
(compare columns four and five) varies from zero
(for price) to a factor of five (for fund). When im-
plicit arguments are included in the comparison,
these differences are reduced and many nominal
predicates express approximately the same num-
ber of arguments on average as their verbal coun-
terparts (compare the fifth and seventh columns).
In addition to role coverage and average count,
we examined the location of implicit arguments.
Figure 1 shows that approximately 56% of the im-
plicit arguments in our data can be resolved within
the sentence containing the predicate. The remain-
ing implicit arguments require up to forty-six sen-
1585
0.4
0.5
0.6
0.7
0.8
0.9
1
0 2 4 6 8 10 12 18 28 46Sentences prior
I
m
p
l
i
c
i
t
 
a
r
g
u
m
e
n
t
s
 
r
e
s
o
l
v
e
d
Figure 1: Location of implicit arguments. For
missing argument positions with an implicit filler,
the y-axis indicates the likelihood of the filler be-
ing found at least once in the previous x sentences.
tences for resolution; however, a vast majority of
these can be resolved within the previous few sen-
tences. Section 6 discusses implications of this
skewed distribution.
4 Implicit argument identification
4.1 Model formulation
In our study, we assumed that each sentence in a
document had been analyzed for PropBank and
NomBank predicate-argument structure. Nom-
Bank includes a lexicon listing the possible ar-
gument positions for a predicate, allowing us to
identify missing argument positions with a simple
lookup. Given a nominal predicate instance p with
a missing argument position iargn, the task is to
search the surrounding discourse for a constituent
c that fills iargn. Our model conducts this search
over all constituents annotated by either PropBank
or NomBank with non-adjunct labels.
A candidate constituent c will often form a
coreference chain with other constituents in the
discourse. Consider the following abridged sen-
tences, which are adjacent in their Penn TreeBank
document:
(3) [Mexico] desperately needs investment.
(4) Conservative Japanese investors are put off
by [Mexico?s] investment regulations.
(5) Japan is the fourth largest investor in
[c Mexico], with 5% of the total
[p investments].
NomBank does not associate the labeled instance
of investment with any arguments, but it is clear
from the surrounding discourse that constituent c
(referring to Mexico) is the thing being invested in
(the iarg2). When determining whether c is the
iarg2 of investment, one can draw evidence from
other mentions in c?s coreference chain. Example
3 states that Mexico needs investment. Example
4 states that Mexico regulates investment. These
propositions, which can be derived via traditional
SRL analyses, should increase our confidence that
c is the iarg2 of investment in Example 5.
Thus, the unit of classification for a candi-
date constituent c is the three-tuple ?p, iargn, c??,
where c? is a coreference chain comprising c and
its coreferent constituents.3 We defined a binary
classification function Pr(+| ?p, iargn, c??) that
predicts the probability that the entity referred to
by c fills the missing argument position iargn of
predicate instance p. In the remainder of this pa-
per, we will refer to c as the primary filler, dif-
ferentiating it from other mentions in the corefer-
ence chain c?. In the following section, we present
the feature set used to represent each three-tuple
within the classification function.
4.2 Model features
Starting with a wide range of features, we per-
formed floating forward feature selection (Pudil
et al, 1994) over held-out development data com-
prising implicit argument annotations from section
24 of the Penn TreeBank. As part of the feature
selection process, we conducted a grid search for
the best per-class cost within LibLinear?s logistic
regression solver (Fan et al, 2008). This was done
to reduce the negative effects of data imbalance,
which is severe even when selecting candidates
from the current and previous few sentences. Ta-
ble 2 shows the selected features, which are quite
different from those used in our previous work to
identify traditional semantic arguments (Gerber et
al., 2009).4 Below, we give further explanations
for some of the features.
Feature 1 models the semantic role relationship
between each mention in c? and the missing argu-
ment position iargn. To reduce data sparsity, this
feature generalizes predicates and argument posi-
tions to their VerbNet (Kipper, 2005) classes and
3We used OpenNLP for coreference identification:
http://opennlp.sourceforge.net
4We have omitted many of the lowest-ranked features.
Descriptions of these features can be obtained by contacting
the authors.
1586
# Feature value description
1* For every f , the VerbNet class/role of pf /argf concatenated with the class/role of p/iargn.
2* Average pointwise mutual information between ?p, iargn? and any ?pf , argf ?.
3 Percentage of all f that are definite noun phrases.
4 Minimum absolute sentence distance from any f to p.
5* Minimum pointwise mutual information between ?p, iargn? and any ?pf , argf ?.
6 Frequency of the nominal form of p within the document that contains it.
7 Nominal form of p concatenated with iargn.
8 Nominal form of p concatenated with the sorted integer argument indexes from all argn of p.
9 Number of mentions in c?.
10* Head word of p?s right sibling node.
11 For every f , the synset (Fellbaum, 1998) for the head of f concatenated with p and iargn.
12 Part of speech of the head of p?s parent node.
13 Average absolute sentence distance from any f to p.
14* Discourse relation whose two discourse units cover c (the primary filler) and p.
15 Number of left siblings of p.
16 Whether p is the head of its parent node.
17 Number of right siblings of p.
Table 2: Features for determining whether c fills iargn of predicate p. For each mention f (denoting a
f iller) in the coreference chain c?, we define pf and argf to be the predicate and argument position of f .
Features are sorted in descending order of feature selection gain. Unless otherwise noted, all predicates
were normalized to their verbal form and all argument positions (e.g., argn and iargn) were interpreted
as labels instead of word content. Features marked with an asterisk are explained in Section 4.2.
semantic roles using SemLink.5 For explanation
purposes, consider again Example 1, where we are
trying to fill the iarg0 of shipping. Let c? contain
a single mention, The two companies, which is the
arg0 of produce. As described in Table 2, fea-
ture 1 is instantiated with a value of create.agent-
send.agent, where create and send are the VerbNet
classes that contain produce and ship, respectively.
In the conversion to LibLinear?s instance repre-
sentation, this instantiation is converted into a sin-
gle binary feature create.agent-send.agent whose
value is one. Features 1 and 11 are instantiated
once for each mention in c?, allowing the model
to consider information from multiple mentions of
the same entity.
Features 2 and 5 are inspired by the work
of Chambers and Jurafsky (2008), who inves-
tigated unsupervised learning of narrative event
sequences using pointwise mutual information
(PMI) between syntactic positions. We used a sim-
ilar PMI score, but defined it with respect to se-
mantic arguments instead of syntactic dependen-
cies. Thus, the values for features 2 and 5 are
computed as follows (the notation is explained in
5http://verbs.colorado.edu/semlink
the caption for Table 2):
pmi(?p, iargn? , ?pf , argf ?) =
log
Pcoref (?p, iargn? , ?pf , argf ?)
Pcoref (?p, iargn? , ?)Pcoref (?pf , argf ? , ?)
(6)
To compute Equation 6, we first labeled a subset of
the Gigaword corpus (Graff, 2003) using the ver-
bal SRL system of Punyakanok et al (2008) and
the nominal SRL system of Gerber et al (2009).
We then identified coreferent pairs of arguments
using OpenNLP. Suppose the resulting data has
N coreferential pairs of argument positions. Also
suppose that M of these pairs comprise ?p, argn?
and ?pf , argf ?. The numerator in Equation 6 is
defined as MN . Each term in the denominator is
obtained similarly, except that M is computed as
the total number of coreference pairs compris-
ing an argument position (e.g., ?p, argn?) and any
other argument position. Like Chambers and Ju-
rafsky, we also used the discounting method sug-
gested by Pantel and Ravichandran (2004) for low-
frequency observations. The PMI score is some-
what noisy due to imperfect output, but it provides
information that is useful for classification.
1587
Feature 10 does not depend on c? and is specific
to each predicate. Consider the following exam-
ple:
(7) Statistics Canada reported that its [arg1
industrial-product] [p price] index dropped
2% in September.
The ?[p price] index? collocation is rarely associ-
ated with an arg0 in NomBank or with an iarg0 in
our annotations (both argument positions denote
the seller). Feature 10 accounts for this type of be-
havior by encoding the syntactic head of p?s right
sibling. The value of feature 10 for Example 7 is
price:index. Contrast this with the following:
(8) [iarg0 The company] is trying to prevent
further [p price] drops.
The value of feature 10 for Example 8 is
price:drop. This feature captures an important dis-
tinction between the two uses of price: the for-
mer rarely takes an iarg0, whereas the latter often
does. Features 12 and 15-17 account for predicate-
specific behaviors in a similar manner.
Feature 14 identifies the discourse relation (if
any) that holds between the candidate constituent
c and the filled predicate p. Consider the following
example:
(9) [iarg0 SFE Technologies] reported a net loss
of $889,000 on sales of $23.4 million.
(10) That compared with an operating [p loss] of
[arg1 $1.9 million] on sales of $27.4 million
in the year-earlier period.
In this case, a comparison discourse relation (sig-
naled by the underlined text) holds between the
first and sentence sentence. The coherence pro-
vided by this relation encourages an inference that
identifies the marked iarg0 (the loser). Through-
out our study, we used gold-standard discourse re-
lations provided by the Penn Discourse TreeBank
(Prasad et al, 2008).
5 Evaluation
We trained the feature-based logistic regression
model over 816 annotated predicate instances as-
sociated with 650 implicitly filled argument posi-
tions (not all predicate instances had implicit ar-
guments). During training, a candidate three-tuple
?p, iargn, c?? was given a positive label if the can-
didate implicit argument c (the primary filler) was
annotated as filling the missing argument position.
To factor out errors from standard SRL analyses,
the model used gold-standard argument labels pro-
vided by PropBank and NomBank. As shown in
Figure 1 (Section 3.2), implicit arguments tend to
be located in close proximity to the predicate. We
found that using all candidate constituents cwithin
the current and previous two sentences worked
best on our development data.
We compared our supervised model with the
simple baseline heuristic defined below:6
Fill iargn for predicate instance p
with the nearest constituent in the two-
sentence candidate window that fills
argn for a different instance of p, where
all nominal predicates are normalized to
their verbal forms.
The normalization allows an existing arg0 for the
verb invested to fill an iarg0 for the noun in-
vestment. We also evaluated an oracle model
that made gold-standard predictions for candidates
within the two-sentence prediction window.
We evaluated these models using the methodol-
ogy proposed by Ruppenhofer et al (2009). For
each missing argument position of a predicate in-
stance, the models were required to either (1) iden-
tify a single constituent that fills the missing argu-
ment position or (2) make no prediction and leave
the missing argument position unfilled. We scored
predictions using the Dice coefficient, which is de-
fined as follows:
2 ? |Predicted
?
True|
|Predicted|+ |True|
(11)
Predicted is the set of tokens subsumed by the
constituent predicted by the model as filling a
missing argument position. True is the set of
tokens from a single annotated constituent that
fills the missing argument position. The model?s
prediction receives a score equal to the maxi-
mum Dice overlap across any one of the annotated
fillers. Precision is equal to the summed predic-
tion scores divided by the number of argument po-
sitions filled by the model. Recall is equal to the
summed prediction scores divided by the number
of argument positions filled in our annotated data.
Predictions not covering the head of a true filler
were assigned a score of zero.
6This heuristic outperformed a more complicated heuris-
tic that relied on the PMI score described in section 4.2.
1588
Baseline Discriminative Oracle
# Imp. # P R F1 P R F1 p R F1
sale 64 60 50.0 28.3 36.2 47.2 41.7 44.2 0.118 80.0 88.9
price 121 53 24.0 11.3 15.4 36.0 32.6 34.2 0.008 88.7 94.0
investor 78 35 33.3 5.7 9.8 36.8 40.0 38.4 < 0.001 91.4 95.5
bid 19 26 100.0 19.2 32.3 23.8 19.2 21.3 0.280 57.7 73.2
plan 25 20 83.3 25.0 38.5 78.6 55.0 64.7 0.060 82.7 89.4
cost 25 17 66.7 23.5 34.8 61.1 64.7 62.9 0.024 94.1 97.0
loss 30 12 71.4 41.7 52.6 83.3 83.3 83.3 0.020 100.0 100.0
loan 11 9 50.0 11.1 18.2 42.9 33.3 37.5 0.277 88.9 94.1
investment 21 8 0.0 0.0 0.0 40.0 25.0 30.8 0.182 87.5 93.3
fund 43 6 0.0 0.0 0.0 14.3 16.7 15.4 0.576 50.0 66.7
Overall 437 246 48.4 18.3 26.5 44.5 40.4 42.3 < 0.001 83.1 90.7
Table 3: Evaluation results. The second column gives the number of predicate instances evaluated.
The third column gives the number of ground-truth implicitly filled argument positions for the predicate
instances (not all instances had implicit arguments). P , R, and F1 indicate precision, recall, and F-
measure (? = 1), respectively. p-values denote the bootstrapped significance of the difference in F1
between the baseline and discriminative models. Oracle precision (not shown) is 100% for all predicates.
Our evaluation data comprised 437 predicate in-
stances associated with 246 implicitly filled ar-
gument positions. Table 3 presents the results.
Predicates with the highest number of implicit ar-
guments - sale and price - showed F1 increases
of 8 points and 18.8 points, respectively. Over-
all, the discriminative model increased F1 perfor-
mance 15.8 points (59.6%) over the baseline.
We measured human performance on this task
by running our undergraduate assistant?s annota-
tions against the evaluation data. Our assistant
achieved an overall F1 score of 58.4% using the
same candidate window as the baseline and dis-
criminative models. The difference in F1 between
the discriminative and human results had an ex-
act p-value of less than 0.001. All significance
testing was performed using a two-tailed bootstrap
method similar to the one described by Efron and
Tibshirani (1993).
6 Discussion
6.1 Feature ablation
We conducted an ablation study to measure the
contribution of specific feature sets. Table 4
presents the ablation configurations and results.
For each configuration, we retrained and retested
the discriminative model using the features de-
scribed. As shown, we observed significant losses
when excluding features that relate the seman-
tic roles of mentions in c? to the semantic role
Percent change (p-value)
Configuration P R F1
Remove 1,2,5 -35.3
(< 0.01)
-36.1
(< 0.01)
-35.7
(< 0.01)
Use 1,2,5 only -26.3
(< 0.01)
-11.9
(0.05)
-19.2
(< 0.01)
Remove 14 0.2
(0.95)
1.0
(0.66)
0.7
(0.73)
Table 4: Feature ablation results. The first column
lists the feature configurations. All changes are
percentages relative to the full-featured discrimi-
native model. p-values for the changes are indi-
cated in parentheses.
of the missing argument position (first configura-
tion). The second configuration tested the effect of
using only the SRL-based features. This also re-
sulted in significant performance losses, suggest-
ing that the other features contribute useful infor-
mation. Lastly, we tested the effect of removing
discourse relations (feature 14), which are likely
to be difficult to extract reliably in a practical set-
ting. As shown, this feature did not have a statis-
tically significant effect on performance and could
be excluded in future applications of the model.
6.2 Unclassified true implicit arguments
Of all the errors made by the system, approxi-
mately 19% were caused by the system?s failure to
1589
generate a candidate constituent c that was a cor-
rect implicit argument. Without such a candidate,
the system stood no chance of identifying a cor-
rect implicit argument. Two factors contributed to
this type of error, the first being our assumption
that implicit arguments are also core (i.e., argn)
arguments to traditional SRL structures. Approxi-
mately 8% of the overall error was due to a failure
of this assumption. In many cases, the true im-
plicit argument filled a non-core (i.e., adjunct) role
within PropBank or NomBank.
More frequently, however, true implicit argu-
ments were missed because the candidate window
was too narrow. This accounts for 12% of the
overall error. Oracle recall (second-to-last col-
umn in Table 3) indicates the nominals that suf-
fered most from windowing errors. For exam-
ple, the sale predicate was associated with the
highest number of true implicit arguments, but
only 80% of those could be resolved within the
two-sentence candidate window. Empirically, we
found that extending the candidate window uni-
formly for all predicates did not increase perfor-
mance on the development data. The oracle re-
sults suggest that predicate-specific window set-
tings might offer some advantage.
6.3 The investment and fund predicates
In Section 4.2, we discussed the price predicate,
which frequently occurs in the ?[p price] index?
collocation. We observed that this collocation
is rarely associated with either an overt arg0 or
an implicit iarg0. Similar observations can be
made for the investment and fund predicates. Al-
though these two predicates are frequent, they are
rarely associated with implicit arguments: invest-
ment takes only eight implicit arguments across its
21 instances, and fund takes only six implicit ar-
guments across its 43 instances. This behavior is
due in large part to collocations such as ?[p in-
vestment] banker?, ?stock [p fund]?, and ?mutual
[p fund]?, which use predicate senses that are not
eventive. Such collocations also violate our as-
sumption that differences between the PropBank
and NomBank argument structure for a predicate
are indicative of implicit arguments (see Section
3.1 for this assumption).
Despite their lack of implicit arguments, it is
important to account for predicates such as in-
vestment and fund because incorrect prediction of
implicit arguments for them can lower precision.
This is precisely what happened for the fund pred-
icate, where the model incorrectly identified many
implicit arguments for ?stock [p fund]? and ?mu-
tual [p fund]?. The left context of fund should help
the model avoid this type of error; however, our
feature selection process did not identify any over-
all gains from including this information.
6.4 Improvements versus the baseline
The baseline heuristic covers the simple case
where identical predicates share arguments in the
same position. Thus, it is interesting to examine
cases where the baseline heuristic failed but the
discriminative model succeeded. Consider the fol-
lowing sentence:
(12) Mr. Rogers recommends that [p investors]
sell [iarg2 takeover-related stock].
Neither NomBank nor the baseline heuristic asso-
ciate the marked predicate in Example 12 with any
arguments; however, the feature-based model was
able to correctly identify the marked iarg2 as the
entity being invested in. This inference captured a
tendency of investors to sell the things they have
invested in.
We conclude our discussion with an example of
an extra-sentential implicit argument:
(13) [iarg0 Olivetti] has denied that it violated
the rules, asserting that the shipments were
properly licensed. However, the legality of
these [p sales] is still an open question.
As shown in Example 13, the system was able to
correctly identify Olivetti as the agent in the sell-
ing event of the second sentence. This inference
involved two key steps. First, the system identified
coreferent mentions of Olivetti that participated in
exporting and supplying events (not shown). Sec-
ond, the system identified a tendency for exporters
and suppliers to also be sellers. Using this knowl-
edge, the system extracted information that could
not be extracted by the baseline heuristic or a tra-
ditional SRL system.
7 Conclusions and future work
Current SRL approaches limit the search for ar-
guments to the sentence containing the predicate
of interest. Many systems take this assumption
a step further and restrict the search to the predi-
cate?s local syntactic environment; however, pred-
icates and the sentences that contain them rarely
1590
exist in isolation. As shown throughout this paper,
they are usually embedded in a coherent and se-
mantically rich discourse that must be taken into
account. We have presented a preliminary study
of implicit arguments for nominal predicates that
focused specifically on this problem.
Our contribution is three-fold. First, we have
created gold-standard implicit argument annota-
tions for a small set of pervasive nominal predi-
cates.7 Our analysis shows that these annotations
add 65% to the role coverage of NomBank. Sec-
ond, we have demonstrated the feasibility of re-
covering implicit arguments for many of the pred-
icates, thus establishing a baseline for future work
on this emerging task. Third, our study suggests
a few ways in which this research can be moved
forward. As shown in Section 6, many errors were
caused by the absence of true implicit arguments
within the set of candidate constituents. More in-
telligent windowing strategies in addition to al-
ternate candidate sources might offer some im-
provement. Although we consistently observed
development gains from using automatic coref-
erence resolution, this process creates errors that
need to be studied more closely. It will also be
important to study implicit argument patterns of
non-verbal predicates such as the partitive percent.
These predicates are among the most frequent in
the TreeBank and are likely to require approaches
that differ from the ones we pursued.
Finally, any extension of this work is likely to
encounter a significant knowledge acquisition bot-
tleneck. Implicit argument annotation is difficult
because it requires both argument and coreference
identification (the data produced by Ruppenhofer
et al (2009) is similar). Thus, it might be produc-
tive to focus future work on (1) the extraction of
relevant knowledge from existing resources (e.g.,
our use of coreference patterns from Gigaword) or
(2) semi-supervised learning of implicit argument
models from a combination of labeled and unla-
beled data.
Acknowledgments
We would like to thank the anonymous review-
ers for their helpful questions and comments. We
would also like to thank Malcolm Doering for his
annotation effort. This work was supported in part
by NSF grants IIS-0347548 and IIS-0840538.
7Our annotation data can be freely downloaded at
http://links.cse.msu.edu:8000/lair/projects/semanticrole.html
References
Aljoscha Burchardt, Anette Frank, and Manfred
Pinkal. 2005. Building text meaning representa-
tions from contextually related frames - a case study.
In Proceedings of the Sixth International Workshop
on Computational Semantics.
Xavier Carreras and Llu??s Ma`rquez. 2005. Introduc-
tion to the CoNLL-2005 shared task: Semantic role
labeling.
Nathanael Chambers and Dan Jurafsky. 2008. Unsu-
pervised learning of narrative event chains. In Pro-
ceedings of the Association for Computational Lin-
guistics, pages 789?797, Columbus, Ohio, June. As-
sociation for Computational Linguistics.
Jacob Cohen. 1960. A coefficient of agreement
for nominal scales. Educational and Psychological
Measurement, 20(1):3746.
Bradley Efron and Robert J. Tibshirani. 1993. An In-
troduction to the Bootstrap. Chapman & Hall, New
York.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR:
A Library for Large Linear Classification. Journal
of Machine Learning Research, 9:1871?1874.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database (Language, Speech, and Commu-
nication). The MIT Press, May.
C.J. Fillmore and C.F. Baker. 2001. Frame semantics
for text understanding. In Proceedings of WordNet
and Other Lexical Resources Workshop, NAACL.
Matthew Gerber, Joyce Y. Chai, and Adam Meyers.
2009. The role of implicit argumentation in nominal
SRL. In Proceedings of the North American Chap-
ter of the Association for Computational Linguistics,
pages 146?154, Boulder, Colorado, USA, June.
David Graff. 2003. English Gigaword. Linguistic
Data Consortium, Philadelphia.
Jan Hajic?, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Anto`nia Mart??, Llu??s
Ma`rquez, Adam Meyers, Joakim Nivre, Sebastian
Pado?, Jan S?te?pa?nek, Pavel Stran?a?k, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The CoNLL-
2009 shared task: Syntactic and semantic dependen-
cies in multiple languages. In Proceedings of the
Thirteenth Conference on Computational Natural
Language Learning (CoNLL 2009): Shared Task,
pages 1?18, Boulder, Colorado, June. Association
for Computational Linguistics.
Ryu Iida, Mamoru Komachi, Kentaro Inui, and Yuji
Matsumoto. 2007. Annotating a Japanese text cor-
pus with predicate-argument and coreference rela-
tions. In Proceedings of the Linguistic Annotation
Workshop in ACL-2007, page 132139.
1591
Kenji Imamura, Kuniko Saito, and Tomoko Izumi.
2009. Discriminative approach to predicate-
argument structure analysis with zero-anaphora res-
olution. In Proceedings of the ACL-IJCNLP 2009
Conference Short Papers, pages 85?88, Suntec, Sin-
gapore, August. Association for Computational Lin-
guistics.
P. Kingsbury, M. Palmer, and M. Marcus. 2002.
Adding semantic annotation to the Penn TreeBank.
In Proceedings of the Human Language Technology
Conference (HLT?02).
Karin Kipper. 2005. VerbNet: A broad-coverage, com-
prehensive verb lexicon. Ph.D. thesis, Department
of Computer and Information Science University of
Pennsylvania.
Mitchell Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated
corpus of English: the Penn TreeBank. Computa-
tional Linguistics, 19:313?330.
Adam Meyers. 2007. Annotation guidelines for
NomBank - noun argument structure for PropBank.
Technical report, New York University.
Martha S. Palmer, Deborah A. Dahl, Rebecca J. Schiff-
man, Lynette Hirschman, Marcia Linebarger, and
John Dowding. 1986. Recovering implicit infor-
mation. In Proceedings of the 24th annual meeting
on Association for Computational Linguistics, pages
10?19, Morristown, NJ, USA. Association for Com-
putational Linguistics.
Patrick Pantel and Deepak Ravichandran. 2004.
Automatically labeling semantic classes. In
Daniel Marcu Susan Dumais and Salim Roukos, ed-
itors, HLT-NAACL 2004: Main Proceedings, pages
321?328, Boston, Massachusetts, USA, May 2 -
May 7. Association for Computational Linguistics.
Rashmi Prasad, Alan Lee, Nikhil Dinesh, Eleni Milt-
sakaki, Geraud Campion, Aravind Joshi, and Bonnie
Webber. 2008. Penn discourse treebank version 2.0.
Linguistic Data Consortium, February.
P. Pudil, J. Novovicova, and J. Kittler. 1994. Floating
search methods in feature selection. Pattern Recog-
nition Letters, 15:1119?1125.
Vasin Punyakanok, Dan Roth, and Wen-tau Yih. 2008.
The importance of syntactic parsing and infer-
ence in semantic role labeling. Comput. Linguist.,
34(2):257?287.
Josef Ruppenhofer, Caroline Sporleder, Roser
Morante, Collin Baker, and Martha Palmer. 2009.
Semeval-2010 task 10: Linking events and their
participants in discourse. In Proceedings of
the Workshop on Semantic Evaluations: Recent
Achievements and Future Directions (SEW-2009),
pages 106?111, Boulder, Colorado, June. Associa-
tion for Computational Linguistics.
Ryohei Sasano, Daisuke Kawahara, and Sadao Kuro-
hashi. 2004. Automatic construction of nominal
case frames and its application to indirect anaphora
resolution. In Proceedings of Coling 2004, pages
1201?1207, Geneva, Switzerland, Aug 23?Aug 27.
COLING.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s Ma`rquez, and Joakim Nivre. 2008. The
CoNLL 2008 shared task on joint parsing of syn-
tactic and semantic dependencies. In CoNLL 2008:
Proceedings of the Twelfth Conference on Computa-
tional Natural Language Learning, pages 159?177,
Manchester, England, August. Coling 2008 Orga-
nizing Committee.
1592
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 13?18,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Probabilistic Labeling for Efficient Referential Grounding based on
Collaborative Discourse
Changsong Liu, Lanbo She, Rui Fang, Joyce Y. Chai
Department of Computer Science and Engineering
Michigan State University
East Lansing, MI 48824
{cliu, shelanbo, fangrui, jchai}@cse.msu.edu
Abstract
When humans and artificial agents (e.g.
robots) have mismatched perceptions of
the shared environment, referential com-
munication between them becomes diffi-
cult. To mediate perceptual differences,
this paper presents a new approach us-
ing probabilistic labeling for referential
grounding. This approach aims to inte-
grate different types of evidence from the
collaborative referential discourse into a
unified scheme. Its probabilistic labeling
procedure can generate multiple ground-
ing hypotheses to facilitate follow-up dia-
logue. Our empirical results have shown
the probabilistic labeling approach sig-
nificantly outperforms a previous graph-
matching approach for referential ground-
ing.
1 Introduction
In situated human-robot dialogue, humans and
robots have mismatched capabilities of perceiving
the shared environment. Thus referential commu-
nication between them becomes extremely chal-
lenging. To address this problem, our previous
work has conducted a simulation-based study to
collect a set of human-human conversation data
that explain how partners with mismatched per-
ceptions strive to succeed in referential commu-
nication (Liu et al, 2012; Liu et al, 2013). Our
data have shown that, when conversation partners
have mismatched perceptions, they tend to make
extra collaborative effort in referential commu-
nication. For example, the speaker often refers
to the intended object iteratively: first issuing an
initial installment, and then refashioning till the
hearer identifies the referent correctly. The hearer,
on the other hand, often provides useful feedback
based on which further refashioning can be made.
This data has demonstrated the importance of in-
corporating collaborative discourse for referential
grounding.
Based on this data, as a first step we developed
a graph-matching approach for referential ground-
ing (Liu et al, 2012; Liu et al, 2013). This ap-
proach uses Attributed Relational Graph to cap-
ture collaborative discourse and employs a state-
space search algorithm to find proper ground-
ing results. Although it has made meaning-
ful progress in addressing collaborative referen-
tial grounding under mismatched perceptions, the
state-space search based approach has two ma-
jor limitations. First, it is neither flexible to ob-
tain multiple grounding hypotheses, nor flexible
to incorporate different hypotheses incrementally
for follow-up grounding. Second, the search al-
gorithm tends to have a high time complexity for
optimal solutions. Thus, the previous approach
is not ideal for collaborative and incremental di-
alogue systems that interact with human users in
real time.
To address these limitations, this paper de-
scribes a new approach to referential grounding
based on probabilistic labeling. This approach
aims to integrate different types of evidence from
the collaborative referential discourse into a uni-
fied probabilistic scheme. It is formulated un-
der the Bayesian reasoning framework to easily
support generation and incorporation of multi-
ple grounding hypotheses for follow-up processes.
Our empirical results have shown that the prob-
abilistic labeling approach significantly outper-
forms the state-space search approach in both
grounding accuracy and efficiency. This new ap-
proach provides a good basis for processing col-
laborative discourse and enabling collaborative di-
alogue system in situated referential communica-
tion.
13
2 Related Work
Previous works on situated referential grounding
have mainly focused on computational models that
connect linguistic referring expressions to the per-
ceived environment (Gorniak and Roy, 2004; Gor-
niak and Roy, 2007; Siebert and Schlangen, 2008;
Matuszek et al, 2012; Jayant and Thomas, 2013).
These works have provided valuable insights on
how to manually and/or automatically build key
components (e.g., semantic parsing, grounding
functions between visual features and words, map-
ping procedures) for a situated referential ground-
ing system. However, most of these works only
dealt with the interpretation of single referring ex-
pressions, rather than interrelated expressions in
collaborative dialogue.
Some earlier work (Edmonds, 1994; Heeman
and Hirst, 1995) proposed a symbolic reasoning
(i.e. planning) based approach to incorporate col-
laborative dialogue. However, in situated settings
pure symbolic approaches will not be sufficient
and new approaches that are robust to uncertain-
ties need to be pursued. DeVault and Stone (2009)
proposed a hybrid approach which combined sym-
bolic reasoning and machine learning for inter-
preting referential grounding dialogue. But their
?environment? was a simplistic block world and
the issue of mismatched perceptions was not ad-
dressed.
3 Data
Previously, we have collected a set of human-
human dialogues on an object-naming task (Liu
et al, 2012). To simulate mismatched perceptions
between a human and an artificial agent, two par-
ticipants were shown different versions of an im-
age: the director was shown the original image
containing some randomly placed objects (e.g.,
fruits), and the matcher was shown an impov-
erished version of the image generated by com-
puter vision. They were instructed to communi-
cate with each other to figure out the identities of
some ?named? objects (only known to the direc-
tor), such that the matcher could also know which
object has what name.
Here is an example excerpt from this dataset:
D
1
: there is basically a cluster of four objects in the upper
left, do you see that (1)
M: yes (2)
D: ok, so the one in the corner is a blue cup (3)
1
D stands for the director; M stands for the matcher.
M: I see there is a square, but fine, it is blue (4)
D: alright, I will just go with that, so and then right under
that is a yellow pepper (5)
M: ok, I see apple but orangish yellow (6)
D: ok, so that yellow pepper is named Brittany (7)
M: uh, the bottom left of those four? Because I do see a
yellow pepper in the upper right (8)
D: the upper right of the four of them? (9)
M: yes (10)
D: ok, so that is basically the one to the right of the blue
cup (11)
M: yeah (12)
D: that is actually an apple (13)
As we can see from this example, both the direc-
tor and the matcher make extra efforts to overcome
the mismatched perceptions through collaborative
dialogue. Our ultimate goal is to develop com-
putational approaches that can ground interrelated
referring expressions to the physical world, and
enable collaborative actions of the dialogue agent
(similar to the active role that the matcher played
in the human-human dialogue). For the time be-
ing, we use this data to evaluate our computa-
tional approach for referential grounding, namely,
replacing the matcher by our automatic system to
ground the director?s referring expressions.
4 Probabilistic Labeling for Reference
Grounding
4.1 System Overview
Our system first processes the data using auto-
matic semantic parsing and coreference resolu-
tion. For semantic parsing, we use a rule-based
CCG parser (Bozsahin et al, 2005) to parse each
utterance into a formal semantic representation.
For example, the utterance ?a pear is to the right
of the apple? is parsed as
[a
1
, a
2
] , [Pear(a
1
), Apple(a
2
), RightOf(a
1
, a
2
)]
which consists of a list of discourse entities (e.g.,
a
1
and a
2
) and a list of first-order-logic predicates
that specify the unary attributes of these entities
and the binary relations between them.
We then perform pairwise coreference resolu-
tion on the discourse entities to find out the dis-
course relations between entities from different ut-
terances. Formally, let a
i
be a discourse entity ex-
tracted from the current utterance, and a
j
a dis-
course entity from a previous utterance. We train a
maximum entropy classifier
2
(Manning and Klein,
2
The features we use for the classification include the dis-
tance between a
i
and a
j
, the determiners associated with
them, the associated pronouns, the syntactic roles, the ex-
tracted unary properties, etc.
14
2003) to predict whether a
i
and a
j
should refer to
the same object (i.e. positive) or to different ob-
jects (i.e. negative).
Based on the semantic parsing and pairwise
coreference resolution results, our system fur-
ther builds a graph representation to capture the
collaborative discourse and formulate referential
grounding as a probabilistic labeling problem, as
described next.
4.2 Graph Representation
We use an Attributed Relational Graph (Tsai and
Fu, 1979) to represent the referential grounding
discourse (which we call the ?dialogue graph?). It
is constructed based on the semantic parsing and
coreference resolution results. The dialogue graph
contains a set A of N nodes:
A = {a
1
, a
2
, . . . , a
N
}
in which each node a
i
represents a discourse en-
tity from the parsing results. And for each pair
of nodes a
i
and a
j
there can be an edge a
i
a
j
that
represents the physical or discourse relation (i.e.
coreference) between the two nodes.
Furthermore, each node a
i
can be assigned a set
of ?attributes?:
x
i
=
{
x
(1)
i
, x
(2)
i
, . . . , x
(K)
i
}
which are used to specify information about the
unary properties of the corresponding discourse
entity. Similarly, each edge a
i
a
j
can also be as-
signed a set of attributes x
ij
to specify informa-
tion about the binary relations between two dis-
course entities. The node attributes are from the
semantic parsing results, i.e., the unary proper-
ties associated to a discourse entity. The edge at-
tributes can be either from parsing results, such
as a spatial relation between two entities (e.g.,
RightOf(a
1
, a
2
)); Or from pairwise coreference
resolution results, i.e., two entities are coreferen-
tial (coref = +) or not (coref = ?).
Besides the dialogue graph that represents the
linguistic discourse, we build another graph to rep-
resent the perceived environment. This graph is
called the ?vision graph? (since this graph is built
based on computer vision?s outputs). It has a set ?
of M nodes:
? = {?
1
, ?
2
, . . . , ?
M
}
in which each node ?
?
represents a physical ob-
ject in the scene. Similar to the dialogue graph,
the vision graph also has edges (e.g., ?
?
?
?
), node
attributes (e.g.,
?
x
?
) and edge attributes (e.g.,
?
x
??
).
Note that the attributes in the vision graph mostly
have numeric values extracted by computer vision
algorithms, whereas the attributes in the dialogue
graph have symbolic values extracted from the lin-
guistic discourse. A set of ?symbol grounding
functions? are used to bridge between the hetero-
geneous attributes (described later).
Given these two graph representations, referen-
tial grounding then can be formulated as a ?node
labeling? process, that is to assign a label ?
i
to
each node a
i
. The value of ?
i
can be any of the
M node labels from the set ?.
4.3 Probabilistic Labeling Algorithm
The probabilistic labeling algorithm (Christmas et
al., 1995) is formulated in the Bayesian frame-
work. It provides a unified evidence-combining
scheme to integrate unary attributes, binary rela-
tions and prior knowledge for updating the label-
ing probabilities (i.e. P (?
i
= ?
?
)). The algo-
rithm finds proper labelings in an iterative manner:
it first initiates the labeling probabilities by consid-
ering only the unary attributes of each node, and
then updates the labeling probability of each node
based on the labeling of its neighbors and the rela-
tions with them.
Initialization:
Compute the initial labeling probabilities:
P
(0)
(?
i
= ?
?
) =
P (a
i
| ?
i
= ?
?
)
?
P (?
i
= ?
?
)
?
?
???
P (a
i
| ?
i
= ?
?
)
?
P (?
i
= ?
?
)
in which
?
P (?
i
= ?
?
) is the prior probability of
labeling a
i
with ?
?
. The prior probability can be
used to encode any prior knowledge about possi-
ble labelings. Especially in incremental process-
ing of the dialogue, the prior can encode previ-
ous grounding hypotheses, and other information
from the collaborative dialogue such as confirma-
tion, rejection, or replacement.
P (a
i
| ?
i
= ?
?
) is called the ?compatibility co-
efficient? between a
i
and ?
?
, which is computed
based on the attributes of a
i
and ?
?
:
P (a
i
| ?
i
= ?
?
) = P (x
i
| ?
i
= ?
?
)
?
?
k
P
(
x
(k)
i
| ?
i
= ?
?
)
and we further define
15
P(
x
(k)
i
| ?
i
= ?
?
)
= p
(
x
(k)
i
| x?
(k)
?
)
=
p
(
x?
(k)
?
|x
(k)
i
)
p
(
x
(k)
i
)
?
x
(k)
j
?L
(k)
p
(
x?
(k)
?
|x
(k)
j
)
p
(
x
(k)
j
)
where L
(k)
is the ?lexicon? for the k-th attribute of
a dialogue graph node, e.g., for the color attribute:
L
(k)
= {red, green, blue, . . .}
and p
(
x?
(k)
?
| x
(k)
i
)
is what we call a ?symbol
grounding function?, i.e., the probability of ob-
serving x?
(k)
?
given the word x
(k)
i
. It judges the
compatibilities between the symbolic attribute val-
ues from the dialogue graph and the numeric at-
tribute values from the vision graph. These sym-
bol grounding functions can be either manually
defined or automatically learned. In our current
work, we use a set of manually defined ground-
ing functions motivated by previous work (Gor-
niak and Roy, 2004).
Iteration:
Once the initial probabilities are calculated, the
labeling procedure iterates till all the labeling
probabilities have converged or the number of it-
erations has reached a specified limit. At each it-
eration and for each possible labeling, it computes
a ?support function? as:
Q
(n)
(?
i
= ?
?
) =
?
j?N
i
?
?
?
??
P
(n)
(?
j
= ?
?
)
P (a
i
a
j
| ?
i
= ?
?
, ?
j
= ?
?
)
and updates the probability of each possible label-
ing as:
P
(n+1)
(?
i
= ?
?
) =
P
(n)
(?
i
=?
?
)Q
(n)
(?
i
=?
?
)
?
?
???
P
(n)
(?
i
=?
?
)Q
(n)
(?
i
=?
?
)
The support function Q
(n)
(?
i
= ?
?
) expresses
how the labeling ?
i
= ?
?
at the n-th itera-
tion is supported by the labeling of a
i
?s neigh-
bors
3
, taking into consideration the binary rela-
tions that exist between a
i
and them. Similar to
the node compatibility coefficient, the edge com-
patibility coefficient between a
i
a
j
and ?
?
?
?
,
3
The set of indices N
i
is defined as:
N
i
= {1, 2, . . . , i? 1, i+ 1, . . . , N}
Top-1 Top-2 Top-3
Random
7.7% 15.4% 23.1%
Guess
a
S.S.S. 19.1% 19.7% 21.3%
P.L. 24.9% 36.1% 45.0%
Gain
b
5.8% 16.4% 23.7%
(p < 0.01) (p < 0.001) (p < 0.001)
P.L. using
66.4% 74.8% 81.9%annotated
coreference
a
Each image contains an average of 13 objects.
b
p-value is based on the Wilcoxon signed-rank
test (Wilcoxon et al, 1970) on the 62 dialogues.
Table 1: Comparison of the reference grounding
performances of a random guess baseline, Prob-
abilistic Labeling (P.L.) and State-Space Search
(S.S.S.), and P.L. using manually annotated coref-
erence.
namely the P (a
i
a
j
| ?
i
= ?
?
, ?
j
= ?
?
) for com-
puting Q
(n)
(?
i
= ?
?
), is also based on the at-
tributes of the two edges and their corresponding
symbol grounding functions. So we also man-
ually defined a set of grounding functions for
edge attributes such as the spatial relation (e.g.,
RightOf , Above). If an edge is used to encode
the discourse relation between two entities (i.e.,
the pairwise coreference results), the compatibility
coefficient can be defined as (suppose edge a
i
a
j
encodes a positive coreference relation between
entities a
i
and a
j
):
P (a
i
a
j
= + | ?
i
= ?
?
, ?
j
= ?
?
)
=
P
(
?
i
=?
?
,?
j
=?
?
|a
i
a
j
=+
)
P (a
i
a
j
=+)
P
(
?
i
=?
?
,?
j
=?
?
)
which can be calculated based on the results from
the coreference classifier (Section 4.1).
5 Evaluation and Discussion
Our dataset has 62 dialogues, each of which con-
tains an average of 25 valid utterances from the
director. We first applied the semantic parser and
coreference classifier as described in Section 4.1
to process each dialogue, and then built a graph
representation based on the automatic processing
results at the end of the dialogue. On average, a di-
alogue graph consists of 33 discourse entities from
the director?s utterances that need to be grounded.
We then applied both the probabilistic label-
ing algorithm and the state-space search algorithm
to ground each of the director?s discourse entities
onto an object perceived from the image. The av-
eraged grounding accuracies of the two algorithms
16
are shown in the middle part of Table 1. The first
column of Table 1 shows the grounding accura-
cies of the algorithm?s top-1 grounding hypothesis
(i.e., ?
i
= argmax
?
?
P (?
i
= ?
?
) for each i). The
second and third column then show the ?accura-
cies? of the top-2 and top-3 hypotheses
4
, respec-
tively.
As shown in Table 1, probabilistic labeling
(i.e. P.L.) significantly outperforms state-space
search (S.S.S.), especially with regard to produc-
ing meaningful multiple grounding hypotheses.
The state-space search algorithm actually only re-
sults in multiple hypotheses for the overall match-
ing, and it fails to produce multiple hypotheses
for many individual discourse entities. Multiple
grounding hypotheses can be very useful to gen-
erate responses such as clarification questions or
nonverbal feedback (e.g. pointing, gazing). For
example, if there are two competing hypotheses,
the dialogue manager can utilize them to gener-
ate a response like ?I see two objects there, are
you talking about this one (pointing to) or that one
(pointing to the other)??. Such proactive feedback
is often an effective way in referential communi-
cation (Clark and Wilkes-Gibbs, 1986; Liu et al,
2013).
The probabilistic labeling algorithm not only
produces better grounding results, it also runs
much faster (with a running-time complexity of
O
(
MN
2
)
,
5
comparing to O
(
N
4
)
of the state-
space search algorithm
6
). Figure 1 shows the av-
eraged running time of the state-space search al-
gorithm on a Intel Core i7 1.60GHz CPU with
16G RAM computer (the running time of the prob-
abilistic labeling algorithm is not shown in Fig-
ure 1 since it always takes less than 1 second to
run). As we can see, when the size of the dialogue
graph becomes greater than 15, state-space search
takes more than 1 minute to run. The efficiency of
the probabilistic labeling algorithm thus makes it
more appealing for real-time interaction applica-
tions.
Although probabilistic labeling significantly
outperforms the state-space search, the grounding
performance is still rather poor (less than 50%)
4
The accuracy of the top-2/top-3 grounding hypotheses is
measured by whether the ground-truth reference is included
in the top-2/top-3 hypotheses.
5
M is the number of nodes in the vision graph and N is
the number of nodes in the dialogue graph.
6
Beam search algorithm is applied to reduce the exponen-
tial O
(
M
N
)
to O
(
N
4
)
.
Figure 1: Average running time of the state-space
search algorithm with respect to the number of
nodes to be grounded in a dialogue graph.
even for the top-3 hypotheses. With no surprise,
the coreference resolution performance plays an
important role in the final grounding performance
(see the grounding performance of using manually
annotated coreference in the bottom part of Ta-
ble 1). Due to the simplicity of our current coref-
erence classifier and the flexibility of the human-
human dialogue in the data, the pairwise coref-
erence resolution only achieves 0.74 in precision
and 0.43 in recall. The low recall of coreference
resolution makes it difficult to link interrelated re-
ferring expressions and resolve them jointly. So it
is important to develop more sophisticated coref-
erence resolution and dialogue management com-
ponents to reliably track the discourse relations
and other dynamics in the dialogue to facilitate ref-
erential grounding.
6 Conclusion
In this paper, we have presented a probabilistic la-
beling based approach for referential grounding in
situated dialogue. This approach provides a uni-
fied scheme for incorporating different sources of
information. Its probabilistic scheme allows each
information source to present multiple hypotheses
to better handle uncertainties. Based on the in-
tegrated information, the labeling procedure then
efficiently generates probabilistic grounding hy-
potheses, which can serve as important guidance
for the dialogue manager?s decision making. In
future work, we will utilize probabilistic labeling
to incorporate information from verbal and non-
verbal communication incrementally as the dia-
logue unfolds, and to enable collaborative dia-
logue agents in the physical world.
Acknowledgments
This work was supported by N00014-11-1-0410
from the Office of Naval Research and IIS-
1208390 from the National Science Foundation.
17
References
Cem Bozsahin, Geert-Jan M Kruijff, and Michael
White. 2005. Specifying grammars for openccg: A
rough guide. Included in the OpenCCG distribution.
William J. Christmas, Josef Kittler, and Maria Petrou.
1995. Structural matching in computer vision
using probabilistic relaxation. Pattern Analysis
and Machine Intelligence, IEEE Transactions on,
17(8):749?764.
Herbert H Clark and Deanna Wilkes-Gibbs. 1986.
Referring as a collaborative process. Cognition,
22(1):1?39.
David DeVault and Matthew Stone. 2009. Learning to
interpret utterances using dialogue history. In Pro-
ceedings of the 12th Conference of the European
Chapter of the Association for Computational Lin-
guistics, pages 184?192. Association for Computa-
tional Linguistics.
Philip G Edmonds. 1994. Collaboration on reference
to objects that are not mutually known. In Pro-
ceedings of the 15th conference on Computational
linguistics-Volume 2, pages 1118?1122. Association
for Computational Linguistics.
Peter Gorniak and Deb Roy. 2004. Grounded seman-
tic composition for visual scenes. J. Artif. Intell.
Res.(JAIR), 21:429?470.
Peter Gorniak and Deb Roy. 2007. Situated lan-
guage understanding as filtering perceived affor-
dances. Cognitive Science, 31(2):197?231.
Peter A Heeman and Graeme Hirst. 1995. Collabo-
rating on referring expressions. Computational Lin-
guistics, 21(3):351?382.
Krishnamurthy Jayant and Kollar Thomas. 2013.
Jointly learning to parse and perceive: Connecting
natural language to the physical world. Transac-
tions of the Association of Computational Linguis-
tics, 1:193?206.
Changsong Liu, Rui Fang, and Joyce Chai. 2012. To-
wards mediating shared perceptual basis in situated
dialogue. In Proceedings of the 13th Annual Meet-
ing of the Special Interest Group on Discourse and
Dialogue, pages 140?149, Seoul, South Korea, July.
Association for Computational Linguistics.
Changsong Liu, Rui Fang, Lanbo She, and Joyce Chai.
2013. Modeling collaborative referring for situated
referential grounding. In Proceedings of the SIG-
DIAL 2013 Conference, pages 78?86, Metz, France,
August. Association for Computational Linguistics.
Christopher Manning and Dan Klein. 2003. Opti-
mization, maxent models, and conditional estima-
tion without magic. In Proceedings of the 2003
Conference of the North American Chapter of the
Association for Computational Linguistics on Hu-
man Language Technology: Tutorials - Volume 5,
NAACL-Tutorials ?03, pages 8?8, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Cynthia Matuszek, Nicholas FitzGerald, Luke Zettle-
moyer, Liefeng Bo, and Dieter Fox. 2012. A
joint model of language and perception for grounded
attribute learning. In John Langford and Joelle
Pineau, editors, Proceedings of the 29th Interna-
tional Conference on Machine Learning (ICML-12),
ICML ?12, pages 1671?1678, New York, NY, USA,
July. Omnipress.
Alexander Siebert and David Schlangen. 2008. A
simple method for resolution of definite reference
in a shared visual context. In Proceedings of the
9th SIGdial Workshop on Discourse and Dialogue,
pages 84?87. Association for Computational Lin-
guistics.
Wen-Hsiang Tsai and King-Sun Fu. 1979. Error-
correcting isomorphisms of attributed relational
graphs for pattern analysis. Systems, Man and Cy-
bernetics, IEEE Transactions on, 9(12):757?768.
Frank Wilcoxon, SK Katti, and Roberta A Wilcox.
1970. Critical values and probability levels for the
wilcoxon rank sum test and the wilcoxon signed
rank test. Selected tables in mathematical statistics,
1:171?259.
18
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 306?313,
The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational Linguistics
Hand Gestures in Disambiguating Types of You Expressions in Multiparty
Meetings
Tyler Baldwin
Department of Computer
Science and Engineering
Michigan State University
East Lansing, MI 48824
baldwin96@cse.msu.edu
Joyce Y. Chai
Department of Computer
Science and Engineering
Michigan State University
East Lansing, MI 48824
jchai@cse.msu.edu
Katrin Kirchhoff
Department of Electrical
Engineering
University of Washington
Seattle, WA, USA
katrin@ee.washington.edu
Abstract
The second person pronoun you serves dif-
ferent functions in English. Each of these
different types often corresponds to a dif-
ferent term when translated into another
language. Correctly identifying different
types of you can be beneficial to machine
translation systems. To address this is-
sue, we investigate disambiguation of dif-
ferent types of you occurrences in multi-
party meetings with a new focus on the
role of hand gesture. Our empirical re-
sults have shown that incorporation of ges-
ture improves performance on differentiat-
ing between the generic use of you (e.g.,
refer to people in general) and the referen-
tial use of you (e.g., refer to a specific per-
son or a group of people). Incorporation
of gesture can also compensate for limi-
tations in automated language processing
(e.g., reliable recognition of dialogue acts)
and achieve comparable results.
1 Introduction
The second person pronoun you is one of the most
prevalent words in conversation and it serves sev-
eral different functions (Meyers, 1990). For ex-
ample, it can be used to refer to a single addressee
(i.e., the singular case) or multiple addressees (i.e.,
the plural case). It can also be used to represent
people in general (i.e., the generic case) or be used
idiomatically in the phrase ?you know?.
For machine translation systems, these differ-
ent types of you often correspond to different
translations in another language. For example,
in German, there are different second-person pro-
nouns for singular vs. plural you (viz. du vs. ihr);
in addition there are different forms for formal
vs. informal forms of address (du vs. Sie) and for
the generic use (man). The following examples
demonstrate different translations of you from En-
glish (EN) into German (DE):
? Generic you
EN: Sometimes you have meetings where the
decision is already taken.
DE: Manchmal hat man Meetings wo die
Entscheidung schon gefallen ist.
? Singular you:
EN: Do you want an extra piece of paper?
DE: Mo?chtest du noch ein Blatt Papier?
? Plural you:
EN: Hope you are all happy!
DE: Ich hoffe, ihr seid alle zufrieden!
These examples show that correctly identifying
different types of You plays an important role in
the correct translation of you in different context.
To address this issue, this paper investigates the
role of hand gestures in disambiguating different
usages of you in multiparty meetings. Although
identification of you type has been investigated
before in the context of addressee identification
(Gupta et al, 2007b; Gupta et al, 2007a; Framp-
ton et al, 2009; Purver et al, 2009), our work
here focuses on two new angles. First, because of
our different application on machine translation,
rather than processing you at an utterance level to
identify addressee, our work here concerns each
occurrence of you within each utterance. Second
and more importantly, our work investigates the
role of corresponding hand gestures in the disam-
biguation process. This aspect has not been exam-
ined in previous work.
When several speakers are conversing in a sit-
uated environment, they often overtly gesture at
one another to help manage turn order or explic-
itly direct a statement toward a particular partici-
pant (McNeill, 1992). For example, consider the
following snippet from a multiparty meeting:
A: ?Why is that??
B: ?Because, um, based on what ev-
306
erybody?s saying, right, [gestures at
Speaker D] you want something sim-
ple. You [gestures at Speaker C]want
basic stuff and [gestures at Speaker A]
you want something that is easy to use.
Speech recognition might not be the
simplest thing.?
The use of gesture in this example indicates that
each instance of the pronoun you is intended to
be referential, and gives some indication of the in-
dented addressee. Without the aid of gesture, it
would be difficult even for a human listener to be
able to interpret each instance correctly.
Therefore, we conducted an empirical study on
several meeting segments from the AMI meeting
corpus. We formulated our problem as a classifica-
tion problem for each occurrence of you, whether
it is a generic, singular, or plural type. We com-
bined gesture features with several linguistic and
discourse features identified by previous work and
evaluated the role of gesture in two different set-
tings: (1) a two stage classification that first dif-
ferentiates the generic type from the referential
type and then within the referential type distin-
guishes singular and plural usages; (2) a three way
classification between generic, singular, or plural
types. Our empirical results have shown that in-
corporation of gesture improves performance on
differentiating between the generic and the refer-
ential type. Incorporation of gesture can also com-
pensate for limitations in automated language pro-
cessing (e.g., reliable recognition of dialogue acts)
and achieve comparable results. These findings
have important implications for machine transla-
tion of you expressions from multiparty meetings.
2 Related Work
Psychological research on gesture usage in
human-human dialogues has shown that speakers
gesture for a variety of reasons. While speakers of-
ten gesture to highlight objects related to the core
conversation topic (Kendon, 1980), they also ges-
ture for dialogue management purposes (Bavelas
et al, 1995). While not all of the gestures pro-
duced relate directly to the resolution of the word
you, many of them give insight into which partici-
pant is being addressed, which has a close correla-
tion with you resolution. Our investigation here is
closely related to two areas of previous work: ad-
dressee identification based on you and the use of
gestures in coreference resolution.
Addressee Identification. Disambiguation of
you type in the context of addressee identifica-
tion has been examined in several papers in re-
cent years. Gupta et. al. (2007b) examined
two-party dialogues from the Switchboard corpus.
They modeled the problem as a binary classifi-
cation problem of differentiating between generic
and referential usages (referential usages include
the singular and plural types). This work has iden-
tified several important linguistic and discourse
features for this task (which was used and ex-
tended in later work and our work here). Later
work by the same group (Gupta et al, 2007a) ex-
amined the same problem on multiparty dialogue
data. They made adjustments to their previous
methods by removing some oracle features from
annotation and applying simpler and more realis-
tic features. A recent work (Frampton et al, 2009)
has examined both the generic vs. referential and
singular vs. plural classification tasks. A main
difference is that this work incorporated gaze fea-
ture information in both classification tasks (gaze
features are commonly used in addressee identi-
fication). More recent work (Purver et al, 2009)
discovered that large gains in performance can
be achieved by including n-gram based features.
However, they found that many of the most im-
portant n-gram features were topic specific, and
thus required training data consisting of meetings
about the same topic.
Gestures in Coreference Resolution. Eisen-
stein and Davis (2006; 2007) examined corefer-
ence resolution on a corpus of speaker-listener
pairs in which the speaker had to describe the
workings of a mechanical device to the listener,
with the help of visual aids. In this gesture heavy
dataset, they found gesture data to be helpful in re-
solving references. In our previous work (2009),
we examined gestures for the identification of
coreference on multparty meeting data. We found
that gestures only provided limited help in the
coreference identification task. Given the nature
of the meetings under investigation, although ges-
tures have not been shown to be effective in gen-
eral, they are potentially helpful in recognizing
whether two linguistic expressions refer to a same
participant.
Compared to these two areas of earlier work,
our investigation here has two unique aspects.
First, as mentioned earlier, previous work on ad-
dressee identification focused the problem at the
307
utterance level. Because the goal was to find the
addressee of an utterance, the assumption was that
all instances of you in an utterance were of the
same type. However, since several instances of
you in the same utterance may translate differently,
we instead examine the classification task at the
instance level. Second, our work here specifically
investigates the role of gestures in disambiguation
of different types of you. This aspect has not been
examined in previous work.
3 Data
The dataset used in our investigation was the
AMI meeting corpus (Popescu-Belis and Estrella,
2007), the same corpus used in previous work
(Gupta et al, 2007a; Frampton et al, 2009; Purver
et al, 2009; Baldwin et al, 2009). The AMI meet-
ing corpus is a large publicly available corpus of
multiparty design meetings. AMI meeting anno-
tations contain manual speech transcriptions, as
well as annotations of several additional modali-
ties, such as focus of attention and head and hand
gesture.
For this work, six AMI meeting segments
(IS1008a, IS1008b, IS1008c, IS1008d, ES2008a,
TS3005a) were used. These instances were cho-
sen because they contained manual annotations of
hand gesture data, which was not available for all
AMI meeting segments. These six meeting seg-
ments were from AMI ?scenario? meetings, in
which meeting participants had a specific task of
designing a hypothetical remote control.
All instances of the word you and its variants
were manually annotated as either generic, singu-
lar, or plural. This produced a small dataset of 533
instances. Agreement between two human anno-
tators was high (? = 0.9). The distribution of you
types is shown in Figure 1. The most prevalent
type in our data set was the generic type, which
accounted for 47% of all instances of you present.
Of the two referential types, the singular type ac-
counted for about 60% of the referential instances.
A total of 508 gestures are present in our data
set. Table 1 shows the distribution of gestures.
As shown, ?non-communicative gestures?, make
up nearly half (46%) of the gestures produced.
These are gestures that are produced without an
overt communicative intent, such as idly tapping
on the table. The other main categorization of
gestures is ?communicative gestures?, which ac-
counts for 45% of all gestures produced and is
made up of the ?pointing at participants?, ?point-
ing at objects?, ?interact with object?, and ?other
communicative? gesture types from Table 1. A to-
tal of 17% of the gestures produced were pointing
gestures that pointed to people, a type of gesture
that would likely be helpful for you type identifica-
tion. A small percentage of the gestures produced
were not recorded by the meeting recording cam-
eras (i.e., off camera), and thus are of unknown
type.
4 Methodology
Our general methodology followed previous work
and formulated this problem as a classification
problem. We evaluated how gesture data may
help you type identification using two different ap-
proaches: (1) two stage binary classification, and
(2) a single three class classification problem. In
two stage binary classification, we first attempt
to distinguish between instances of you that are
generic and those that are referential. We then take
those cases that are referential and attempt to sub-
divide them into instances that are intended to re-
fer to a single person and those that refer to several
people.
Our feature set includes features used by Gupta
et. al. (2007a) (Hereafter referred to as Gupta) and
Frampton et. al. (2009) (Hereafter Frampton), as
well as new features incorporating gestures. We
summarize these features as follows.
Sentential Features. We used several senten-
tial features to capture important phrase patterns.
Most of our sentential features were drawn from
Gupta (2007a). These features captured the pat-
terns ?you guys?, ?you know?, ?do you? (and sim-
ilar variants), ?which you? (and variants), ?if you?,
and ?you hear? (and variants). Another sentential
feature captured the number of times the word you
appeared in the sentence. Additionally, other fea-
tures captured sentence patterns not related to you,
such as the presence of the words ?I? and ?we?.
A few other sentential features were drawn from
Frampton et. al. (2009). These include the pattern
?<auxiliary> you? (a more general version of the
?do you? feature) and a count of the number of
total words in the utterances.
Part-of-Speech Features. Several features
based on automatic part-of-speech tagging of the
sentence containing you were used. Quality of au-
tomatic tagging was not assessed. From the tagged
results, we extracted 5 features based on sentence
308
Distrib
ution 
of You
 Types
050100150200250300
Generi
c
Singula
r
Plural
Type
Count
(a) Distribution of You types
Gestu
re Dis
tributi
on
050100150200250 Non- Comm
unicative Po
inting at Pa
rticipant
s Pointi
ng at Objec
ts Other Co
mmunica
tive Inte
ract with O
bject
Off_came
ra
Type
Count
(b) Distribution of gesture types
Figure 1: Data distributions
and tag patterns: whether or not the sentence that
contained you also contained I, or we followed by
a verb tag (3 separate features), and whether or
not the sentence contains a comparative JJR (ad-
jective) tag. All of these features were adapted
from Gupta (2007a).
Dialog Act Features. We used the manually an-
notated dialogue act tags provided by the AMI cor-
pus to produce our dialogue act features. Three di-
alogue act features were used: the dialogue act tag
of the current sentence, the previous sentence, and
the sentence prior to that. Dialog act tags were in-
corporated into the feature set in one of two differ-
ent ways: 1) using the full tag set provided by the
AMI corpus, and 2) using a binary feature record-
ing if the dialogue act tag was of the elicit type.
The latter way of dialogue act incorporation rep-
resents a simpler and more realistic treatment of
dialogue acts.
Question Mark Feature. The question mark
feature captures whether or not the current sen-
tence ends in a question mark. This feature cap-
tures similar information to the elicit dialogue act
tag and was used in Gupta as an automatically ex-
tractable replacement to the manually extracted di-
alogue act tags (2007a).
Backward Looking/Forward Looking Fea-
tures. Several features adapted from Frampton et.
al. (2009) used information about previous and
next sentences and speakers. These features con-
nected the current utterance with previous utter-
ances by the other participants in the room. For
each listener, a feature was recorded that indicated
how many sentences elapsed between the current
sentence and the last/next time the person spoke.
Additionally, two features captured the number of
speakers in the previous and next five sentences.
Gesture Features. Several different features
were used to capture gesture information. Three
types of gesture data were considered: all pro-
duced gestures, only those gestures that were
manually annotated as being communicative, and
only those gestures that were manually annotated
as pointing towards another meeting participant.
For each of these types, one gesture feature cap-
tures the total number of gestures that co-occur
with the current sentence, while another feature
records only whether or not a gesture co-occurs
with the utterance of you. Since previous work
(Kendon, 1980) has indicated that gesture produc-
tion tends to precede the onset of the expression,
gestures were considered to have co-occurred with
instances if they directly overlapped with them or
preceded them by a short window of 2.5 seconds.
Note that in this investigation, we used anno-
tated gestures provided by the AMI corpus. Al-
though automated extraction of reliable gesture
features can be challenging and should be pursued
in the future, the use of manual annotation allows
us to focus on our current goal, which is to under-
stand whether and to what degree hand gestures
may help disambiguation of you Type.
It is also important to note that although previ-
ous work (Purver et al, 2009) showed that n-gram
features produced large performance gains, these
features were heavily topic dependent. The AMI
meeting corpus provides several meetings on ex-
actly the same topic, which allowed the n-gram
features to learn topic-specific words such as but-
ton, channel, and volume. However, as real world
309
Accuracy
Majority Class Baseline 53.3%
Gupta automatic 70.7%
Gupta manual 74.7%
Gupta + Frampton automatic 73.2%
Gupta + Frampton manual 74.3%
All (+ gesture) 79.0%
Table 1: Accuracy values for Generic vs. Referen-
tial Classification
meetings occur with a wider range of goals and
topics, we would like to build a topic and domain
independent model that does not require a corpus
of topic specific training data. As such, we have
excluded n-gram features from our study.
Additionally, we have not implemented gaze
features. Although previous work (Frampton et
al., 2009) showed that these features were able to
improve performance, we decided to focus solely
on gesture to the exclusion of other non-speech
modalities. However, we are currently in the pro-
cess of evaluating the overlap between gesture and
gaze feature coverage.
5 Results
Due to the small number of meeting segments in
our data, leave-one-out cross validation was pre-
formed for evaluation. Since a primary focus of
this paper is to understand whether and to what
degree gesture is able to aid in the you type iden-
tification task, experiments were run using a deci-
sion tree classifier due to its simplicity and trans-
parency 1.
5.1 Two Stage Classification
We first evaluated the role of gesture via two stage
binary classification. That is, we performed two
binary classification tasks, first differentiating be-
tween generic and referential instances, and then
further dividing the referential instances into the
singular and plural types. This provides a more
detailed analysis of where gesture may be helpful.
Results for the generic vs. referential and singu-
lar vs. plural binary classification tasks are shown
in Table 1 and Table 2, respectively. Tables 1
and 2 present several different configurations. The
1In order to get a more direct comparison to previous work
(Gupta et al, 2007a; Frampton et al, 2009), we also experi-
mented with classification via a bayesian network. We found
that the overall results were comparable to those obtained
with the decision tree.
Accuracy
Majority Class Baseline 59.5%
Gupta automatic 72.2%
Gupta manual 73.6%
Gupta + Frampton automatic 73.2%
Gupta + Frampton manual 72.5%
All (+ gesture) 74.6%
Table 2: Accuracy values for Singular vs. Plural
Classification
?Gupta? feature configurations consist of all fea-
tures used by Gupta et. al. (2007a). These in-
clude all part-of-speech features, all dialogue act
features, the question mark feature, and all sen-
tential features except the ?<auxiliary> you? fea-
ture and the word count feature. Results from two
types of processing are presented: automatic and
manual.
? Automatic feature extraction (automatic) -
The automatic configurations consist of only
features that were automatically extracted
from the text. This includes all of the features
we examined except for the dialogue act and
gesture features. These features are extracted
from meeting transcriptions.
? Manual feature extraction (manual) - Manual
configurations apply manual annotations of
dialogue acts and gestures together with the
automatically extracted features.
The Frampton configurations add the addi-
tional sentential features as well as the backward-
looking and forward-looking features. As before,
results are presented for a manual and an auto-
matic run. The final configuration (?All?) includes
the entire feature set with the addition of gesture
features. The All configuration is the only config-
uration that includes gesture features.
Although they are not directly comparable, the
results for generic vs. referential classification
shown in Table 1 appear consistent with those re-
ported by Gupta (2007a). Adding additional fea-
tures from Frampton et. al. did not produce an
overall increase in performance when dialogue act
features were present. Including gesture features
leads to a significant increase in performance (Mc-
Nemar Test, p < 0.01), an absolute increase of
4.3% over the best performing feature set that does
not include gesture. This result seems to confirm
our hypothesis that, because gestures are likely
310
Accuracy
Majority Class Baseline 46.7%
Gupta automatic 61.5%
Gupta manual 66.2%
Gupta + Frampton automatic 63.6%
Gupta + Frampton manual 70.2%
All (+ gesture) 70.4%
Table 3: Accuracy values for several different fea-
ture configurations on the three class classification
problem.
to accompany referential instances of you but not
generic instances, gesture information is able to
help differentiate between the two. Manual in-
spection of the decision tree produced indicates
that gesture features were among the most dis-
criminative features.
The results on the singular vs. plural task shown
in Table 2 are less clear. Although (Gupta et al,
2007a) did not report results on singular vs. plural
classification, their feature set produced reason-
able classification accuracy of 73.6%. Including
gesture and other features did not produce a statis-
tically significant improvement in the overall ac-
curacy. This suggests that while gesture is helpful
for predicting referentiality, it does not appear to
be a reliable predictor of whether an instance of
you is singular or plural. Inspection on the deci-
sion tree confirms that gesture features were not
seen to be highly discriminative.
5.2 Three Class Classification
The results presented for singular vs. plural classi-
fication are based on performance on the subset of
you instances that are referential, which assumes
that we are able to filter out generic references
with 100% accuracy. While this gives us an eval-
uation of how well the singular vs. plural task can
be performed without the generic references pre-
senting a confounding factor, it presents unrealis-
tic performance for a real system. To account for
this, we present results on a three class problem of
determining whether an instance of you is generic,
singular, or plural. The results are shown in Table
3. A simple majority class classifier yields accu-
racy of 46.7% (In our data, the generic class was
the majority class).
As we can see from Table 3, adding addi-
tional features gives improved performance over
the original implementation by Gupta et. al., re-
sulting in an overall accuracy of about 70%. We
also observed that the dialogue act features were
important; manual configurations produced abso-
lute gains of about 7% accuracy over fully auto-
matic configurations. The gesture feature, how-
ever, did not provide a significant increase in per-
formance over the same feature set without gesture
information.
Table 4 shows the precision, recall, and F-
measure values for each you type for several dif-
ferent configurations. As shown, the generic class
proved to be the easiest for the classifiers to iden-
tify. This is not suprising, as not only are generic
instance our majority class, but many of the fea-
tures used were originally tailored towards the two
class problem of differentiating generic instances
from the other classes. The performance on the
plural and singular classes is comparable to one
another when the basic feature set is used. How-
ever, as more features are added, the performance
on the singular class increases while the perfor-
mance on the plural class does not. This seems
to suggest that future work should attempt to in-
clude more features that are indicative of plural
instances.
When manual dialogue acts are applied, it ap-
pears incorporation of gestures does not lead to
any overall performance improvement (as shown
in Table 3). One possible explanation is that ges-
ture features as they are incorporated here do pro-
vide some disambiguating information (as shown
in the two stage classification), but this informa-
tion is subsumed by other features, such as dia-
logue acts. To test this hypothesis, we ran an ex-
periment with a feature set that contained all fea-
tures except dialogue act features. That is, a fea-
ture set that contains all of the automatic features,
as well as gesture features. Results are shown in
Table 5.
Our ?automatic + gesture? feature configuration
produced accuracy of 66.2%. When compared to
the same feature set without gesture features (the
?Gupta + Frampton automatic? row in Table 3) we
see a statistically significant (p < 0.01) absolute
accuracy improvement of about 2.6%. This seems
to suggest that gesture features are providing some
small amount of relevant information that is not
captured by our automatically extractable features.
Up until this point we have incorporated dia-
logue acts using the full set of dialogue act tags
provided by the AMI corpus. As we have men-
311
Precision Recall F-Measure
Gupta automatic Plural 0.553 0.548 0.550
Singular 0.657 0.408 0.504
Generic 0.624 0.787 0.696
Gupta manual Plural 0.536 0.513 0.524
Singular 0.675 0.503 0.576
Generic 0.704 0.839 0.766
All (+ gesture) Plural 0.542 0.565 0.553
Singular 0.745 0.604 0.667
Generic 0.754 0.835 0.792
Table 4: Precision, recall, and F-measure results for each you type based on three class classification.
Accuracy
Gupta + Frampton automatic 63.6%
Gupta + Frampton automatic + gesture 66.2%
Gupta + Frampton automatic + simple dialogue act 66.6%
Gupta + Frampton automatic + simple dialogue act + gesture 69.0%
Table 5: Accuracy for 3-way classification by combining gesture information with automatically ex-
tracted features based on the Decision Tree model
tioned, this level of granularity may not be prac-
tically extractable for use in a current state-of-
the-art system. As a result, we implemented the
simpler dialogue act incorporation method pro-
posed by (Gupta et al, 2007a), in which only
the presence or absence of the elicit dialogue act
type is considered. Using this feature with the
automatically extracted features yielded accuracy
of 66.6%, a statistically significant improvement
(p < 0.01) of an absolute 3% over a fully auto-
matic run. Furthermore, if we incorporate gesture
features with this configuration, the performance
increases to 69.0% (statistically significantly, p <
0.01). This suggests that while gesture features
may be redundant with information provided by
the full set of dialogue act tags, it is largely com-
plementary with the simpler dialogue act incorpo-
ration. The incorporation of gesture along with
simpler and more reliable dialogue acts can po-
tentially approach the performance gained by in-
corporation of more complex dialogue acts, which
are often difficult to obtain. Of course, gesture fea-
tures themselves are often difficult to obtain. How-
ever, redundancy in two potentially error-prone
feature sources can be an asset, as data from one
source may help to compensate for errors in the
other. Although addressing a different problem of
multimodal integration, previous work (Oviatt et
al., 1997) appears to indicate that this is the case.
6 Conclusion
In this paper, we investigate the role of hand ges-
tures in disambiguating types of You expressions
in multiparty meetings for the purpose of machine
translation.
Our results have shown that on the binary
generic vs. referential classification problem, the
inclusion of gesture data provides a statistically
significant increase in performance over the same
feature set without gesture. This result is consis-
tent with our hypothesis that gesture data would be
helpful because speakers are more likely to gesture
when producing referential instances of you.
To produce results more akin to those that
would be expected during incorporation in a real
machine translation system, we experimented with
the type identification problem as a three class
classification problem. It was discovered that
when a full set of dialogue act tags were used as
features, the incorporation of gesture features does
not provide an increase in performance. However,
when simpler dialogue act tags are used, the in-
corporation of gestures helps to make up for lost
performance. Since it remains a difficult prob-
lem to automatically predict complex dialog acts
with high accuracy, the incorporation of gesture
features may prove beneficial to current systems.
312
7 Acknowledgement
This work was supported by IIS-0855131 (to the
first two authors) and IIS-0840461 (to the third au-
thor) from the National Science Foundation. The
authors would like to thank anonymous reviewers
for valuable comments and suggestions.
References
Tyler Baldwin, Joyce Y. Chai, and Katrin Kirchhoff.
2009. Communicative gestures in coreference iden-
tification in multiparty meetings. In ICMI-MLMI
?09: Proceedings of the 2009 international con-
ference on Multimodal interfaces, pages 211?218.
ACM.
J. B. Bavelas, N. Chovil, L. Coates, and L. Roe. 1995.
Gestures specialized for dialogue. Personality and
Social Psychology Bulletin, 21:394?405.
Jacob Eisenstein and Randall Davis. 2006. Gesture
improves coreference resolution. In Proceedings of
the Human Language Technology Conference of the
NAACL, Companion Volume: Short Papers, pages
37?40, New York City, USA, June. Association for
Computational Linguistics.
Jacob Eisenstein and Randall Davis. 2007. Condi-
tional modality fusion for coreference resolution. In
Proceedings of the 45th Annual Meeting of the As-
sociation of Computational Linguistics, pages 352?
359, Prague, Czech Republic, June. Association for
Computational Linguistics.
Matthew Frampton, Raquel Ferna?ndez, Patrick Ehlen,
Mario Christoudias, Trevor Darrell, and Stanley Pe-
ters. 2009. Who is ?you??: combining linguis-
tic and gaze features to resolve second-person refer-
ences in dialogue. In EACL ?09: Proceedings of the
12th Conference of the European Chapter of the As-
sociation for Computational Linguistics, pages 273?
281, Morristown, NJ, USA. Association for Compu-
tational Linguistics.
Surabhi Gupta, John Niekrasz, Matthew Purver, and
Dan Jurafsky. 2007a. Resolving you in multi-party
dialog. In Proceedings of the 8th SIGdial Workshop
on Discourse and Dialogue.
Surabhi Gupta, Matthew Purver, and Dan Jurafsky.
2007b. Disambiguating between generic and refer-
ential you in dialog. In Proceedings of the 42th An-
nual Meeting of the Association for Computational
Linguistics (ACL).
Adam Kendon. 1980. Gesticulation and speech: Two
aspects of the process of utterance. In Mary Richie
Key, editor, The Relationship of Verbal and Nonver-
bal Communication, pages 207?227.
D. McNeill. 1992. Hand and Mind: What Gestures
Reveal about Thought. University of Chicago Press.
W. M. Meyers. 1990. Current generic pronoun usage.
American Speech, 65(3):228?237.
Sharon Oviatt, Antonella DeAngeli, and Karen Kuhn.
1997. Integration and synchronization of input
modes during multimodal human-computer interac-
tion. In CHI ?97: Proceedings of the SIGCHI con-
ference on Human factors in computing systems,
pages 415?422, New York, NY, USA. ACM.
Andrei Popescu-Belis and Paula Estrella. 2007. Gen-
erating usable formats for metadata and annotations
in a large meeting corpus. In Proceedings of the
45th Annual Meeting of the Association for Com-
putational Linguistics Companion Volume Proceed-
ings of the Demo and Poster Sessions, pages 93?96,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Matthew Purver, Raquel Ferna?ndez, Matthew Framp-
ton, and Stanley Peters. 2009. Cascaded lexicalised
classifiers for second-person reference resolution.
In SIGDIAL ?09: Proceedings of the SIGDIAL 2009
Conference, pages 306?309, Morristown, NJ, USA.
Association for Computational Linguistics.
313
Proceedings of the ACL 2011 Workshop on Relational Models of Semantics (RELMS 2011), pages 63?71,
Portland, Oregon, USA, June 23, 2011. c?2011 Association for Computational Linguistics
A Joint Model of Implicit Arguments for Nominal Predicates
Matthew Gerber and Joyce Y. Chai
Department of Computer Science
Michigan State University
East Lansing, Michigan, USA
{gerberm2,jchai}@cse.msu.edu
Robert Bart
Computer Science and Engineering
University of Washington
Seattle, Washington, USA
rbart@cs.washington.edu
Abstract
Many prior studies have investigated the re-
covery of semantic arguments for nominal
predicates. The models in many of these stud-
ies have assumed that arguments are indepen-
dent of each other. This assumption simpli-
fies the computational modeling of semantic
arguments, but it ignores the joint nature of
natural language. This paper presents a pre-
liminary investigation into the joint modeling
of implicit arguments for nominal predicates.
The joint model uses propositional knowledge
extracted from millions of Internet webpages
to help guide prediction.
1 Introduction
Much recent work on semantic role labeling has fo-
cused on joint models of arguments. This work is
motivated by the fact that one argument can either
promote or inhibit the presence of another argument.
Because most of this work has been done for verbal
SRL, nominal SRL has lagged behind somewhat. In
particular, the ?implicit? nominal SRL model cre-
ated by Gerber and Chai (2010) does not address
joint argument structures. Implicit arguments are
similar to standard SRL arguments, a primary differ-
ence being their ability to cross sentence boundaries.
In the model created by Gerber and Chai, implicit ar-
gument candidates are classified independently and
a heuristic post-processing method is applied to de-
rive the final structure. This paper presents a prelim-
inary joint implicit argument model.
Consider the following sentences:1
1We will use the notation of Gerber and Chai (2010), where
(1) [c1 The president] is currently struggling to
manage [c2 the country?s economy].
(2) If he cannot get it under control, [p loss] of
[arg1 the next election] might result.
In Example 2, we are searching for the iarg0 of loss
(the entity that is losing). The sentence in Exam-
ple 1 supplies two candidates c1 and c2. If one only
considers the predicate loss, then c1 and c2 would
both be reasonable fillers for the iarg0: presidents
often lose things (e.g., votes and allegiance) and
economies often lose things (e.g., jobs and value).
However, the sentence in Example 2 supplies addi-
tional information. It tells the reader that the next
election is the entity being lost. Given this infor-
mation, one would likely prefer c1 over c2 because
economies don?t generally lose elections, whereas
presidents often do. This type of inference is com-
mon in textual discourses because authors assume
a shared knowledge base with their readers. This
knowledge base contains information about events
and their typical participants (e.g., the fact that pres-
idents lose elections but economies do not).
The model presented in this paper relies on a
knowledge base constructed by automatically min-
ing semantic propositions from Internet webpages.
These propositions help to identify likely joint im-
plicit argument configurations. In the following sec-
tion, we review work on joint inference within se-
mantic role labeling. In Sections 4 and 5, we present
the joint implicit argument model and its features.
Evaluation results for this model are given in Sec-
standard nominal arguments are indicated with argn and im-
plicit arguments are indicated with iargn.
63
tion 6. The joint model contains many simplifying
assumptions, which we address in Section 7. We
conclude in Section 8.
2 Related work
A number of recent studies have shown that seman-
tic arguments are not independent and that system
performance can be improved by taking argument
dependencies into account. Consider the following
examples, due to Toutanova et al (2008):
(3) [Temporal The day] that [arg0 the ogre]
[Predicate cooked] [arg1 the children] is still
remembered.
(4) [arg1 The meal] that [arg0 the ogre]
[Predicate cooked] [Beneficiary the children]
is still remembered.
These examples demonstrate the importance of
inter-argument dependencies. The change from day
in Example 3 to meal in Example 4 affects more
than just the Temporal label: additionally, the arg1
changes to Beneficiary, even though the underlying
text (the children) does not change. To capture this
dependency, Toutanova el al. first generate an n-
best list of argument labels for a predicate instance.
They then re-rank this list using joint features that
describe multiple arguments simultaneously. The
features help prevent globally invalid argument con-
figurations (e.g., ones with multiple arg0 labels).
Punyakanok et al (2008) formulate a variety of
constraints on argument configurations. For exam-
ple, arguments are not allowed to overlap the predi-
cate, nor are they allowed to overlap each other. The
authors treat these constraints as binary variables
within an integer linear program, which is optimized
to produce the final labeling.
Ritter et al (2010) investigated joint selectional
preferences. Traditionally, a selectional preference
model provides the strength of association between
a predicate-argument position and a specific textual
expression. Returning to Examples 1 and 2, one
sees that the selectional preference for president and
economy in the iarg0 position of loss should be high.
Ritter et al extended this single-argument model
using a joint formulation of Latent Dirichlet Allo-
cation (LDA) (Blei et al, 2003). In the generative
version of joint LDA, text for the argument posi-
tions is generated from a common hidden variable.
This approach reflects the intuition behind Exam-
ples 1 and 2 and would help identify president as the
iarg0. Training data for the model was drawn from
a large corpus of two-argument tuples extracted by
the TextRunner system, which we describe next.
Both Ritter et al?s model and the model described
in this paper rely heavily on information extracted
by the TextRunner system (Banko et al, 2007).
The TextRunner system extracts tuples from Inter-
net webpages in an unsupervised fashion. One key
difference between TextRunner and other informa-
tion extraction systems is that TextRunner does not
use a closed set of relations (compare to the work
described by ACE (2008)). Instead, the relation set
is left open, leading to the notion of Open Informa-
tion Extraction (OIE). Although OIE often has lower
precision than traditional information extraction, it
is able to extract a wider variety of relations at preci-
sion levels that are often useful (Banko and Etzioni,
2008).
3 Using TextRunner to assess joint
argument assignments
Returning again to Examples 1 and 2, one can query
TextRunner in the following way:
arg0 : ?
Predicate : lose2
arg1 : election
In the TextRunner system, arg0 typically indicates
the Agent and arg1 typically indicates the Theme.
TextRunner provides many tuples in response to this
query, two of which are shown below:
(5) Usually, [arg0 the president?s party]
[Predicate loses] [arg1 seats in the mid-term
election].
(6) [arg0 The president] [Predicate lost] [arg1 the
election].
The tuples present in these sentences give strong in-
dicators about the type of entity that loses elections.
2Nominal predicates are mapped to their verbal forms using
information provided by the NomBank lexicon.
64
Given all of the returned tuples, only a single one
involves economy in the arg0 position:
(7) Any president will take credit for [arg0 a good
economy] or [Predicate lose] [arg1 an
election] over a bad one.
In Example 7, TextRunner has not analyzed the ar-
guments correctly (president should be the arg0, not
economy).3 In Section 5, we show how evidence
from the tuple lists can be aggregated such that cor-
rect analyses (5 and 6) are favored over incorrect
analyses (7). The primary contribution of this paper
is an exploration of how the aggregated evidence can
be used to identify implicit arguments (e.g., presi-
dent in Example 1).
4 Joint model formulation
To simplify the experimental setting, the model de-
scribed in this paper targets the specific situation
where a predicate instance p takes an implicit iarg0
and an implicit iarg1.4 Whereas the model proposed
by Gerber and Chai (2010) classifies candidates for
these positions independently, the model in this pa-
per classifies joint structures by evaluating the fol-
lowing binary prediction function:
P (+| ?p, iarg0, ci, iarg1, cj?) (8)
Equation 8 gives the probability of the joint assign-
ment of ci to iarg0 and cj to iarg1. Given a set of n
candidates c1, . . . , cn ? C , the best labeling is found
by considering all possible assignments of ci and cj :
argmax
(ci,cj)?CxC s.t. i 6=j
P (+| ?p, iarg0, ci, iarg1, cj?)
(9)
Consider modified versions of Examples 1 and 2:
(10) [c1 The president] is currently struggling to
manage [c2 the country?s economy].
(11) If he cannot get it under control before [c3 the
next election], a [p loss] might result.
3Banko and Etzioni (2008) cite a precision score of 88% for
their system.
4This simplifying assumption does not hold for real data,
and is addressed further in Section 7.2.
In this case, we are looking for the iarg0 as well as
the iarg1 for the loss predicate. Three candidates c1,
c2, and c3 are marked. The joint model would eval-
uate the following probabilities, taking the highest
scoring to be the final assignment:
P (+| ?loss, iarg0, president, iarg1, economy?)
*P (+| ?loss, iarg0, president, iarg1, election?)
P (+| ?loss, iarg0, economy, iarg1, president?)
P (+| ?loss, iarg0, economy, iarg1, election?)
P (+| ?loss, iarg0, election, iarg1, president?)
P (+| ?loss, iarg0, election, iarg1, economy?)
Intuitively, only the starred item should have a high
probability. In the following section, we describe
how these probabilities can be estimated using in-
formation extracted by TextRunner.
5 Joint model features
As mentioned in Section 2, the TextRunner system
has been extracting massive amounts of knowledge
in the form of tuples such as the following:
?president, lose, election?
The database of tuples can be queried by supplying
one or more of the tuple arguments. For example,
the following is a partial result list for the query
?president, lose, ??:
?Kenyan president, lose, election?
?president?s party, lose seat in, election?
?president, lose, ally?
The final position in each of these tuples (e.g.,
election) provides a single answer to the question
?What might a president lose??. Aggregation begins
by generalizing each answer to its WordNet synset
(glosses are shown after the arrows):
?Kenyan president, lose, election? ? a vote
?president?s party, lose seat in, election? (same)
?president, lose, ally? ? friendly nation
In cases where a tuple argument has multiple
WordNet senses, the tuple is mapped to the most
common sense as listed in the WordNet database.
65
Having mapped each tuple to its synset, each synset
is ranked according to the number of tuples that
it covers. For the query ?president, lose, ??, this
produces the following ranked list of WordNet
synsets (only the top five are shown, with the
number in parentheses indicating how many tuples
are covered):
1. election (77)
2. war (51)
3. vote (39)
4. people (34)
5. support (26)
...
The synsets above indicate likely answers to the pre-
vious question of ?What might a president lose??.
In a similar manner, one can answer a question
such as ?What might lose an election?? using tu-
ples extracted by TextRunner. The procedure de-
scribed above produces the following ranked list of
WordNet synsets to answer this question:
...
9. people (62)
10. Republican (51)
11. Republican party (51)
12. Hillary (50)
13. president (49)
...
In this case, the expected answer (president) ranks
13th in the list of answer synsets. It is important
to note that lower ranked answers are not necessar-
ily incorrect answers. It is a simple fact that a wide
variety of entities can lose an election. Items 9-13
are all reasonable answers to the original question
of what might lose an election.
The two symmetric questions defined and an-
swered above are closely connected to the implicit
argument situation discussed in Examples 10 and
11. In Example 11, one is searching for the implicit
iarg0 and iarg1 to the loss predicate. Candidates ci
and cj that truly fill these positions should be com-
patible with questions in the following forms:
Question: What did ci lose?
Answer: cj
Question: What entity lost cj?
Answer: ci
If either of these question-answer pairs is not satis-
fied, then the joint assignment of ci to iarg0 and cj
to iarg1 should be considered unlikely. Using the
first question-answer pair above as an example, sat-
isfaction is determined in the following way:
1. Query TextRunner for ?ci, lose, ??, retrieving
the top n tuples.
2. Map the final argument of each tuple to its
WordNet synset and rank the synsets by fre-
quency, producing the ranked list A of answer
synsets.
3. Map cj to its most common WordNet synset
synsetcj and determine whether synsetcj ex-
ists in A. If it does, the question-answer pair is
satisfied.
Some additional processing is required to determine
whether synsetcj exists in A. This is due to the hi-
erarchical organization of WordNet. For example,
suppose that synsetcj is the synset containing ?pri-
mary election? and A contains synsets paraphrased
as follows:
1. election
2. war
3. vote
...
synsetcj does not appear directly in this list; how-
ever, its existence in the list is implied by the follow-
ing hypernymy path within WordNet:
primary election is-a??? election
Intuitively, if synsetcj is connected to a highly
ranked synset in A by a short path, then one has ev-
idence that synsetcj answers the original question.
66
The evidence is weaker if the path is long, as in the
following example:
open primary is-a??? direct primary
is-a??? primary election is-a??? election
Additionally, a path between more specific synsets
(i.e., those lower in the hierarchy) indicates a
stronger relationship than a path between more gen-
eral synsets (i.e., those higher in the hierarchy).
These two situations are depicted in Figure 1. The
synset similarity metric defined by Wu and Palmer
(1994) combines the path length and synset depth
intuitions into a single numeric score that is defined
as follows:
2 ? depth(lca(synset1, synset2))
depth(synset1) + depth(synset2)
(12)
In Equation 12, lca returns the lowest common an-
cestor of the two synsets within the WordNet is-a
hierarchy.
To summarize, Equation 12 indicates the strength
of association between synsetcj (e.g., primary elec-
tion) and a ranked synset synseta from A that an-
swers a question such as ?What might a president
lose??. If the association between synsetcj and
synseta is small, then the assignment of cj to iarg1
is unlikely. The process works similarly for assess-
ing ci as the filler of iarg0. In what follows, we
quantify this intuition with features used to repre-
sent the conditioning information in Equation 8.
Feature 1: Maximum association strength. Given
the conditioning variables in Equation 8, there are
two questions that can be asked:
Question: What did ci p?
Answer: cj
Question: What entity p cj?
Answer: ci
Each of these questions produces a ranked list of
answer synsets using the approach described previ-
ously. The synset for each answer string will match
zero or more of the answer synsets, and each of these
matches will be associated with a similarity score as
defined in Equation 12. Feature 1 considers all such
similarity scores and selects the maximum. A high
value for this feature indicates that one (or both) of
the candidates (ci or cj) is likely to fill its associated
implicit argument position.
Feature 2: Maximum reciprocal rank. Of all the
answer matches described for Feature 1, Feature 2
selects the highest ranking and forms the reciprocal
rank. Thus, values for Feature 2 are in [0,1] with
larger values indicating matches with higher ranked
answer synsets.
Feature 3: Number of matches. This feature
records the total number of answer string matches
from either of the questions described for Feature 1.
Feature 4: Sum reciprocal rank. Feature 2 consid-
ers answer synset matches from either of the posed
questions; ideally, each question-answer pair should
have some influence on the probability estimate in
Equation 8. Feature 4 looks at the answer synset
matches from each question individually. The match
with highest rank for each question is selected, and
the reciprocal rank 2r1 + r2 is computed. The value
of this feature is zero if either of the questions fails
to produce a matching answer synset.
Features 5 and 6: Local classification scores. The
joint model described in this paper does not replace
the local prediction model presented by Gerber and
Chai (2010). The latter uses a wide variety of impor-
tant features that cannot be ignored. Like previous
joint models (e.g., the one described by Toutanova et
al. (2008)), the joint model works on top of the lo-
cal prediction model, whose scores are incorporated
into the joint model as feature-value pairs. Given the
local prediction scores for the iarg0 and iarg1 posi-
tions in Equation 8, the joint model forms two fea-
tures: (1) the sum of the scores for ci filling iarg0
and cj filling iarg1, and (2) the product of these two
scores.
6 Evaluation
We evaluated the joint model described in the pre-
vious sections over the manually annotated implicit
67
entity (a)
physical entity (b)
thing
body of water (c)
bay (d)
matter
abstract entity
Figure 1: Effect of depth on WordNet synset similarity. All links indicate is-a relationships. Although the link
distance from (a) to (b) equals the distance from (c) to (d), the latter are more similar due to their lower depth within
the WordNet hierarchy.
argument data created by Gerber and Chai (2010).
This dataset contains full-text implicit argument
annotations for approximately 1,200 predicate in-
stances within the Penn TreeBank. As mentioned
in Section 4, all experiments were conducted us-
ing predicate instances that take an iarg0 and iarg1
in the ground-truth annotations. We used a ten-
fold cross-validation setup and the evaluation met-
rics proposed by Ruppenhofer et al (2009), which
were also used by Gerber and Chai. For each evalu-
ation fold, features were selected using only the cor-
responding training data and the greedy selection al-
gorithm proposed by Pudil et al (1994), which starts
with an empty feature set and incrementally adds
features that provide the highest gains.
For comparison with Gerber and Chai?s model,
we also evaluated the local prediction model on the
evaluation data. Because this model predicted im-
plicit arguments independently, it continued to use
the heuristic post-processing algorithm to arrive at
the final labeling. However, the prediction threshold
t was eliminated because the system could safely as-
sume that a true filler for the iarg0 and iarg1 posi-
tions existed.
Table 1 presents the evaluation results. The first
thing to note is that these results are not comparable
to the results presented by Gerber and Chai (2010).
In general, performance is much higher because
predicate instances reliably take implicit arguments
in the iarg0 and iarg1 positions. The overall perfor-
mance increase versus the local model is relatively
small (approximately 1 percentage point); however,
the bid predicate in particular showed a substantial
increase (greater than 11 percentage points).
7 Discussion
7.1 Example improvement versus local model
The bid and investment predicates showed the
largest increase for the joint model versus the local
model. Below, we give an example of the investment
predicate for which the joint model correctly identi-
fied the iarg0 and the local model did not.
(13) [Big investors] can decide to ride out market
storms without jettisoning stock.
(14) Most often, [c they] do just that, because
stocks have proved to be the best-performing
long-term [Predicate investment], attracting
about $1 trillion from pension funds alone.
Both models identified the iarg1 as money from a
prior sentence (not shown). The local model in-
correctly predicted $1 trillion in Example 14 as the
iarg0 for the investment event. This mistake demon-
strates a fundamental limitation of the local model:
it cannot detect simple incompatibilities in the pre-
dicted argument structure. It does not know that
?money investing money? is a rare or impossible
event in the real world.
For the joint model?s prediction, consider the con-
stituent marked with c in Example 14. This con-
68
Local model Joint model
# Imp. args. P R F1 P R F1
price 40 65.0 65.0 65.0 67.5 67.5 67.5
sale 34 86.5 86.5 86.5 84.3 84.3 84.3
plan 30 60.0 60.0 60.0 56.7 56.7 56.7
bid 26 66.7 66.7 66.7 78.2 78.2 78.2
fund 18 83.3 83.3 83.3 83.3 83.3 83.3
loss 14 100.0 100.0 100.0 100.0 100.0 100.0
loan 12 63.6 58.3 60.9 50.0 50.0 50.0
investment 8 57.1 50.0 53.3 62.5 62.5 62.5
Overall 182 72.6 71.8 72.2 73.1 73.1 73.1
Table 1: Joint implicit argument evaluation results. The second column gives the total number of implicit arguments
in the ground-truth annotations. P , R, and F1 indicate precision, recall, and f-measure (? = 1) as defined by Ruppen-
hofer et al (2009).
stituent is resolved to Big investors in the preceding
sentence. Thus, the two relevant questions are as
follows:
Question: What did big investors invest?
Answer: money
Question: What entity invested money?
Answer: big investors
The first question produces the following ranked list
of answer synsets (the number in parentheses indi-
cates the number of answer tuples mapped to the
synset):
money (71)
amount (38)
million (38)
billion (22)
capital (21)
As shown, the answer string of money matches the
top-ranked answer synset. The second question pro-
duces the following ranked list of answer synsets:
company (642)
people (460)
government (275)
business (75)
investor (70)
In this case, the answer string Big investors matches
the fifth answer synset. The combined evidence
of these two question-answer pairs allows the joint
system to successfully identify Big investors as the
iarg0 of the investment predicate in Example 14.
7.2 Toward a generally applicable joint model
The joint model presented in this paper assumes that
all predicate instances take an iarg0 and iarg1. This
assumption clearly does not hold for real data (these
positions are often not expressed in the text), but re-
laxing it will require investigation of the following
issues:
1. Explicit arguments should also be considered
when determining whether a candidate c fills
an implicit argument position iargn. The mo-
tivation here is similar to that given elsewhere
in this paper: arguments (whether implicit or
explicit) are not independent. This is demon-
strated by Example 2 at the beginning of this
paper, where election is an explicit argument to
the predicate and affects the implicit argument
inference. The model developed in this paper
only considers jointly occurring implicit argu-
ments.
2. Other implicit argument positions (e.g.,
iarg2, iarg3, etc.) need to be accounted
for as well. This will present a challenge
when it comes to extracting the necessary
69
propositions from TextRunner. Currently,
TextRunner only handles tuples of the form
?arg0, p, arg1?. Other argument positions are
not directly analyzed by the system; however,
because TextRunner also returns the sentence
from which a tuple is extracted, these addi-
tional argument positions could be extracted in
the following way:
(a) For an instance of the sale predicate
with an arg0 of company, to find
likely arg2 fillers (the entity purchas-
ing the item), query TextRunner with
?company, sell, ??.
(b) Perform standard verbal SRL on the sen-
tences for the resulting tuples, identifying
any arg2 occurrences.
(c) Cluster and rank the arg2 fillers according
to the method described in this paper.
This approach combines Open Information Ex-
traction with traditional information extraction
(i.e., verbal SRL).
3. Computational complexity and probability
estimation is a problem for many joint mod-
els. The model presented in this paper quickly
becomes computationally intractable when the
number of candidates and implicit argument
positions becomes moderately large. This is
because Equation 9 considers all possible as-
signments of candidates to implicit argument
positions. With as few as thirty candidates and
five argument positions (not uncommon), one
must evaluate 30!/25! = 17, 100, 720 possible
assignments. Although this particular formula-
tion is not tractable, one based on dynamic pro-
gramming or heuristic search might give rea-
sonable results. Efficient estimation of the joint
probability via Gibbs sampling would also be a
possible approach (Resnik and Hardisty, 2010).
8 Conclusions
Many prior studies have investigated the recovery
of semantic arguments for nominal predicates. The
models in many of these studies have assumed that
the arguments are independent of each other. This
assumption simplifies the computational modeling
of semantic arguments, but it ignores the joint na-
ture of natural language. In order to take advantage
of the information provided by jointly occurring ar-
guments, the independent prediction models must be
enhanced.
This paper has presented a preliminary investiga-
tion into the joint modeling of implicit arguments
for nominal predicates. The model relies heavily
on information extracted by the TextRunner extrac-
tion system, which pulls propositional tuples from
millions of Internet webpages. These tuples encode
world knowledge that is necessary for resolving se-
mantic arguments in general and implicit arguments
in particular. This paper has proposed methods of
aggregating tuple knowledge to guide implicit argu-
ment resolution. The aggregated knowledge is ap-
plied via a re-ranking model that operates on top
of the local prediction model described in previous
work.
The performance gain across all predicate in-
stances is relatively small; however, larger gains are
observed for the bid and investment predicates. The
improvement in Example 14 shows that the joint
model is capable of correcting a bad local predic-
tion using information extracted by the TextRunner
system. This type of information is not used by the
local prediction model.
Although the results in this paper show that some
improvement is possible through the use of a joint
model of implicit arguments, a significant amount
of future work will be required to make the model
widely applicable.
References
ACE, 2008. The ACE 2008 Evaluation Plan. NIST, 1.2d
edition, August.
Michele Banko and Oren Etzioni. 2008. The tradeoffs
between open and traditional relation extraction. In
Proceedings of ACL-08: HLT, pages 28?36, Colum-
bus, Ohio, June. Association for Computational Lin-
guistics.
Michele Banko, Michael J Cafarella, Stephen Soderland,
Matt Broadhead, and Oren Etzioni. 2007. Open in-
formation extraction from the web. In Proceedings of
the 20th International Joint Conference on Artificial
Intelligence.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
70
2003. Latent dirichlet alocation. J. Mach. Learn.
Res., 3:993?1022.
Matthew Gerber and Joyce Chai. 2010. Beyond Nom-
Bank: A study of implicit arguments for nominal pred-
icates. In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics, pages
1583?1592, Uppsala, Sweden, July. Association for
Computational Linguistics.
P. Pudil, J. Novovicova, and J. Kittler. 1994. Floating
search methods in feature selection. Pattern Recogni-
tion Letters, 15:1119?1125.
Vasin Punyakanok, Dan Roth, and Wen-tau Yih. 2008.
The importance of syntactic parsing and inference in
semantic role labeling. Comput. Linguist., 34(2):257?
287.
Philip Resnik and Eric Hardisty. 2010. Gibbs sampling
for the uninitiated. Technical report, University of
Maryland, June.
Alan Ritter, Mausam, and Oren Etzioni. 2010. A latent
dirichlet alocation method for selectional preferences.
In Proceedings of the 48th Annual Meeting of the As-
sociation for Computational Linguistics.
Josef Ruppenhofer, Caroline Sporleder, Roser Morante,
Collin Baker, and Martha Palmer. 2009. Semeval-
2010 task 10: Linking events and their participants in
discourse. In Proceedings of the Workshop on Seman-
tic Evaluations: Recent Achievements and Future Di-
rections (SEW-2009), pages 106?111, Boulder, Col-
orado, June. Association for Computational Linguis-
tics.
Kristina Toutanova, Aria Haghighi, and Christopher D.
Manning. 2008. A global joint model for semantic
role labeling. Comput. Linguist., 34(2):161?191.
Zhibiao Wu and Martha Palmer. 1994. Verb seman-
tics and lexical selection. In Proceedings of the 32nd
Annual Meeting of the Association for Computational
Linguistics, pages 133?138, Las Cruces, New Mex-
ico, USA, June. Association for Computational Lin-
guistics.
71
Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 140?149,
Seoul, South Korea, 5-6 July 2012. c?2012 Association for Computational Linguistics
Towards Mediating Shared Perceptual Basis in Situated Dialogue
Changsong Liu, Rui Fang, Joyce Y. Chai
Department of Computer Science and Engineering
Michigan State University
East Lansing, MI, 48864
{cliu,fangrui,jchai}@cse.msu.edu
Abstract
To enable effective referential grounding in
situated human robot dialogue, we have con-
ducted an empirical study to investigate how
conversation partners collaborate and medi-
ate shared basis when they have mismatched
visual perceptual capabilities. In particu-
lar, we have developed a graph-based repre-
sentation to capture linguistic discourse and
visual discourse, and applied inexact graph
matching to ground references. Our empiri-
cal results have shown that, even when com-
puter vision algorithms produce many errors
(e.g. 84.7% of the objects in the environment
are mis-recognized), our approach can still
achieve 66% accuracy in referential ground-
ing. These results demonstrate that, due to its
error-tolerance nature, inexact graph matching
provides a potential solution to mediate shared
perceptual basis for referential grounding in
situated interaction.
1 Introduction
To support natural interaction between a human and
a robot, technology enabling human robot dialogue
has become increasingly important. Human robot
dialogue often involves objects and their identities
in the environment. One critical problem is inter-
pretation and grounding of references - a process
to establish mutual understanding between conver-
sation partners about intended references (Clark and
Wilkes-Gibbs, 1986). The robot needs to identify
referents in the environment that are specified by its
human partner and the partner needs to recognize
that the intended referents are correctly understood.
It is critical for the robot and its partner to quickly
and reliably reach the mutual acceptance of refer-
ences before conversation can move forward.
Despite recent progress (Scheutz et al, 2007b;
Foster et al, 2008; Skubic et al, 2004; Kruijff et al,
2007; Fransen et al, 2007), interpreting and ground-
ing references remains a very challenging problem.
In situated interaction, although a robot and its hu-
man partner are co-present in a shared environment,
they have significantly mismatched perceptual capa-
bilities (e.g., recognizing objects in the surround-
ings). Their knowledge and representation of the
shared world are significantly different. When a
shared perceptual basis is missing, grounding ref-
erences to the environment will be difficult (Clark,
1996). Therefore, a foremost question is to under-
stand how partners with mismatched perceptual ca-
pabilities mediate shared basis to achieve referential
grounding.
To address this problem, we have conducted an
empirical study to investigate how conversation part-
ners collaborate and mediate shared basis when they
have mismatched visual perceptual capabilities. In
particular, we have developed a graph-based rep-
resentation to capture linguistic discourse and vi-
sual discourse, and applied inexact graph matching
to ground references. Our empirical results have
shown that, even when the perception of the envi-
ronment by computer vision algorithms has a high
error rate (84.7% of the objects are mis-recognized),
our approach can still correctly ground those mis-
recognized objects with 66% accuracy. The results
demonstrate that, due to its error-tolerance nature,
inexact graph matching provides a potential solu-
140
tion to mediate shared perceptual basis for referen-
tial grounding in situated interaction.
In the following sections, we first describe an em-
pirical study based on a virtual environment to ex-
amine how partners mediate their mismatched visual
perceptual basis. We then provide details about our
graph matching based approach and its evaluation.
2 Related Work
There has been an increasing number of published
works on situated language understanding(Scheutz
et al, 2007a; Foster et al, 2008; Skubic et al,
2004; Huwel and Wrede, 2006), focusing on inter-
pretation of referents in a shared environment. Dif-
ferent approaches have been developed to resolve
visual referents. Gorniak and Roy present an ap-
proach that grounds referring expressions to visual
objects through semantic decomposition, using con-
text free grammar that connect linguistic structures
with underlying visual properties (Gorniak and Roy,
2004a). Recently, they have extended this work
by including action-affordances (Gorniak and Roy,
2007). This line of work has mainly focused on
grounding words to low-level visual properties. To
incorporate situational awareness, incremental ap-
proaches have been developed to prune interpreta-
tions which do not have corresponding visual ref-
erents in the environment (Scheutz et al, 2007a;
Scheutz et al, 2007b; Brick and Scheutz, 2007).
A recent work applies a bidirectional approach to
connect bottom-up incremental language processing
to top-down constrains on possible interpretation of
referents given situation awareness (Kruijff et al,
2007). Most of these previous works address utter-
ance level processing. Here, we are interested in ex-
ploring how the mismatched perceptual capabilities
influences the collaborative discourse, and develop-
ing a graph-based framework for referential ground-
ing with mismatched perceptions.
3 Empirical Study
It is very difficult to study the collaborative pro-
cess between partners with mismatched perceptual
capabilities. Subjects with truly mismatched per-
ceptual capabilities are difficult to recruit, and the
discrepancy between capabilities is difficult to mea-
sure and control. The wizard-of-oz studies with
Figure 1: Our experimental system. Two partners collab-
orate on an object naming task using this system. The
director on the left side is shown an (synthesized) origi-
nal image, while the matcher on the right side is shown
an impoverished version of the original image.
physical robots (e.g., as in (Green and Severin-
son Eklundh, 2001; Shiomi et al, 2007; Kahn et al,
2008)) are also insufficient since it is not clear what
should be the underlying principles to guide the wiz-
ard?s decisions and thus the perceived robot?s behav-
iors (Steinfeld et al, 2009). To address these prob-
lems, motivated by the Map Task (Anderson et al,
1991) and the recent encouraging results from vir-
tual simulation in Human Robot Interaction (HRI)
studies (Carpin et al, 2007; Chernova et al, 2010),
we conducted an empirical study based on virtual
simulations of mismatched perceptual capabilities.
3.1 Experimental System and Task
The setup of our experimental system is shown in
Figure 1. In the experiment, two human partners
(a director and a matcher) collaborate on an object
naming task. The mismatched perceptual capabili-
ties between partners are simulated by different ver-
sions of an image shown to them: the director looks
at an original image, while the matcher looks at an
impoverished version of the original image.
The original image (the one on the left in Fig-
ure 1) was created by randomly selecting images of
daily-life items (office supplies, fruits, etc.) from
an image database and randomly positioning them
onto a background. To create the impoverished im-
141
age (the one on the right in Figure 1), we applied
standard Computer Vision (CV) algorithms to pro-
cess the original image and then create an abstract
representation based on the outputs from the CV al-
gorithms.
More specifically, the original image was fed
into a segmentation ? feature extraction ?
recognition pipeline of CV algorithms. First, the
OTSU algorithm (Otsu, 1975) was used for image
segmentation. Then visual features such as color
and shape were extracted from the segmented re-
gions (Zhang and Lu, 2002). Finally, object recogni-
tion was done by searching the nearest neighbor (in
the shape-feature vector space) from a knowledge
base of ?known? objects. The impoverished image
was then created based on the CV algorithms? out-
puts. For example, if an object in the original image
was recognized as a pear, an abstract illustration of
pear would be displayed in the impoverished image
at the same position. Other features such like color
and size of the object were also extracted from the
original image and assigned to the illustration in the
impoverished image.
In the naming task, the director?s goal is to com-
municate the ?secret names? of some randomly se-
lected objects (i.e., target objects) in his/her image to
the matcher, so that the matcher would know which
object has what name. As shown in Figure 1, those
secret names are displayed only on the director?s
screen but not the matcher?s. Once the matcher be-
lieves that he/she correctly acquires the name of an
target object, he/she will record the name by mouse-
clicking on the target and repeating the name. A
task is considered complete when the matcher has
recorded the names of all the target objects.
3.2 Examples
Consistent with previous findings (Liu et al, 2011),
our empirical study shows that human partners tend
to combine object properties and spatial relations to
construct their referring expressions. In addition,
our empirical study has further demonstrated how
partners manage to mediate their perceptual basis
through collaborative discourse. Here are two ex-
amples from our data:
Example 1.
D1: the very top right hand corner, there is a red apple
M: ok
D: and then to the left of that red apple on the top of the
screen is a red or black cherry
M: ok
D: and then to the left of that is a brown kiwi fruit
M: ok
D: and the, the red cherry is called Richard
? ? ? ? ? ?
Example 2.
D: ok, um, so can we start in the top right
M: alright, um, the top right there are two rows of items,
they are all circular or apple shaped
D: ok, um, the item in the very top right corner does not
have a name
M: um, no name
M: um, to the left of that
D: yes, to the left of that is Richard
M: ok, are there only three items in that row
D: yes, there are only three
M: ok, this is Richard
? ? ? ? ? ?
As shown in Example 1, the most commonly used
object properties include object class, color, spatial
location, and others such as size, length and shape.
For the relations, the most common one is the pro-
jective spatial relations (Liu et al, 2010), such as
right, left, above, below. Besides, as illustrated by
Example 2, descriptions based on grouping of mul-
tiple objects are also commonly used. To mediate
their shared basis, both the director and the matcher
make extra effort to collaborate with each other. For
instance, in Example 1, the director applies install-
ment (Clark and Wilkes-Gibbs, 1986) where he ut-
ters noun phrases in episodes and the matcher ex-
plicitly accepts each installment before the director
moves forward. In Example 2, the matcher intends
to assist the grounding process by proactively pro-
viding what he perceives about the environment.
The data collected from our empirical study have
indicated that, to mediate a shared perceptual basis
and ground references, a successful method should
consider the following issues: (1) It needs to capture
the dynamics of the linguistic discourse and iden-
tify various relations among different referring ex-
pressions throughout discourse. (2) it needs to rep-
resent the perceived visual features and topological
relations between visual objects in the visual dis-
course. (3) Because the perceived visual world by
1D stands for Director and M for Matcher.
142
the matcher (who represents the lower-calibre arti-
ficial agent) very often differs from the perceived
visual world by the director (who represents the
higher-calibre human partner), reference resolution
will need some approximation without enforcing a
complete satisfaction of constraints. Based on these
considerations, we have developed a graph-based
approach for referential grounding. Next we give
a detailed account on this approach.
4 A Graph-based Approach to Referential
Grounding
In the field of image analysis and pattern recogni-
tion, Attributed Relational Graph (ARG) is a very
useful data structure to represent an image (Tsai and
Fu, 1979; Sanfeliu and Fu, 1983). In an ARG, the
underlying unlabeled graph represents the topologi-
cal structure of the scene. Then each node and edge
are labeled with a vector of attributes that represents
local features of a single node or the topological fea-
tures between two nodes. Based on the ARG rep-
resentations, an inexact graph matching is to find
a graph or a subgraph whose error-transformation
cost with the already given graph is minimum (Es-
hera and Fu, 1984).
Motivated by the representation power of ARG
and the error-correcting capability of inexact graph
matching, we developed a graph-based approach to
address the referential grounding problem. ARG
and probabilistic graph matching have been pre-
viously applied in multimodal reference resolu-
tion (Chai et al, 2004a; Chai et al, 2004b) by in-
tegrating speech and gestures. Here, although we
use similar ARG representations, our algorithm is
based on inexact graph matching and our focus is on
mediating shared perceptual basis.
4.1 Graph Representations
Figure 2 illustrates the key elements and the process
of our graph-based method. The key elements of our
method are two ARG representations, one of which
is called the discourse graph and the other called the
vision graph.
The discourse graph captures the information ex-
tracted from the linguistic discourse.2 To create the
discourse graph, the linguistic discourse first needs
2Currently we only focus on the utterances from the director.
Figure 2: An illustration of graph representations in our
method. The discourse graph is created from formal se-
mantic representations of the linguistic discourse; The vi-
sion graph is created by applying CV algorithms on the
corresponding scene. Given the two graphs, referential
grounding is to construct a node-to-node mapping from
the discourse graph to the vision graph.
to be processed by NLP components, such as the se-
mantic composition and discourse coreference res-
olution components. The output of the NLP com-
ponents are usually in the form of some formal se-
mantic representations, e.g. in the form of first-order
logic representations. The discourse graph is then
created based on the formal semantics, i.e. each
new discourse entity corresponds to a node in the
graph, one-arity predicates correspond to node at-
tributes and two-arity predicates correspond to edge
attributes. The vision graph, on the other hand, is a
representation of the visual features extracted from
the scene. Each object detected by CV algorithms
is represented as a node in the vision graph, and the
attributes of the node correspond to visual features,
such as the color, size and position of the object. The
edges between nodes represent their relations in the
physical space.
Given the discourse graph and the vision graph,
now we can formulate referential grounding as con-
structing a node-to-node mapping from the dis-
course graph to the vision graph, or in other words,
a matching between the two graphs. Note that, the
143
matching we encounter here is different from the
original graph matching problem that is often used
in the image analysis field. The original version only
considers matching between two graphs that have
the same type of values for each attribute. But in
the case of referential grounding, all the attributes in
the discourse graph possess symbolic values since
they come from formal semantic representations,
whereas the attributes in the vision graph are often
numeric values produced by CV algorithms. Our so-
lution is to introduce a set of symbol grounding func-
tions, which bridges the heterogeneous attributes of
the two graphs and makes general graph matching
algorithms applicable to referential grounding.
4.2 Inexact Graph Matching
We formulate referential grounding as a graph
matching problem, which has extended the origi-
nal graph matching approach used in image process-
ing and pattern recognition filed (Tsai and Fu, 1979;
Tsai and Fu, 1983; Eshera and Fu, 1984).
First, we give the formal definition of an ARG,
which is a doublet of the form
G = (N,E)
where
N The set of attributed-nodes of graph G,
defined as
N = {(i, a) |1 ? i ? |N | } .
E The set of directed attributed-edges of
graph G, defined as
E = {(i, j, e) |1 ? i, j ? |N | } .
(i, a) ? N Node i with a as its attribute vector,
where a = [v1, v2, ? ? ? , vK ] is a vector
of K attributes. To simplify the nota-
tion, We will denote a node as ai.
(i, j, e) ? E The directed edge from node i to node
j with e as its attribute vector, where
e = [u1, u2, ? ? ? , uL] is a vector of L
attributes. We will denote an edge as
eij .
In an ARG, the value of a node/edge attribute
vk/ul can be symbolic, numeric, or as a vector of
numeric values. For example, if v1 is used to rep-
resent the color feature of an object, then a possible
assignment could be v1 = [255, 0, 0], which is the
rgb color vector.
Suppose we represent referring expressions from
the linguistic discourse as a discourse graph G and
objects perceived from the environment as a vi-
sion graph G?, referential grounding then becomes
a graph matching problem: given G = (N,E) and
G? = (N ?, E?), in which
N = {ai |1 ? i ? I } , E = {ei1i2 |1 ? i1, i2 ? I }
N ? = {aj ? |1 ? j ? J } , E? = {e?j1j2 |1 ? j1, j2 ? J }
A matching between G and G? is to find a one-to-
one mapping between the nodes in N and the nodes
in N ?.
Note that it is not necessary for every node in
N or N ? to be mapped to a corresponding node in
the other graph. If a node is not to be mapped to
any node in the other graph, we describe it as be-
ing mapped to ?, which denotes an abstract ?null?
node. To represent the matching result, we re-order
N and N ? such that the first I ?/J ? nodes in N /N ? are
those which have been mapped to their correspond-
ing nodes in the other graph, and the nodes after
them are the unmatched nodes, i.e. those matched
with ?. Then the matching result is
M = M1 ?M2 ?M3
= {(i, j) |1 ? i ? I ?, 1 ? j ? J ? }
? {i |I ? < i ? I }
? {j |J ? < j ? J }
Here M1 is a set of I ? pairs of indices of matched
nodes. M2 and M3 are the sets of indices of all the
unmatched nodes in N and N ?, respectively. Then
M is what we call a matching between G and G?.
It is an inexact matching in the sense that we allow
bothG andG? to have a subset of nodes, i.e. M2 and
M3, that are not matched with any node in the other
graph (Conte et al, 2004). The cost of a matching
M is then defined as
C (M) = C (M1) + C (M2) + C (M3)
To complete the definition of C (M), we use M11
to denote the set of all the first indices of the matched
pairs in M1, i.e. M11 = {i |1 ? i ? I ? }, and H =(
NH , EH
) is the subgraph of G that is induced by
the subset of nodes NH = {ai |i ?M11 }, then we
have
C (M1) =
?
ai?NH
CN (ai, a?j) +
?
ei1i2?EH
CE (ei1i2 , e?j1j2)
C (M2) =
?
ai?(N?NH)
CN (ai,?) +
?
ei1i2?(E?EH)
CE (ei1i2 ,?)
144
in which CN (ai, a?j) is the cost of mapping ai
to a?j , CE (ei1i2 , e?j1j2) is the cost of mapping
ei1i2 to e?j1j2 , and CN (ai,?)/CE (ei1i2 ,?) is the
cost of mapping ai/ei1i2 to the null node/edge.
They are also called node/edge substitution cost and
node/edge insertion cost, respectively (Eshera and
Fu, 1984). Note that, in our case we let C (M3) = 0
since we have assumed that the size of G? is bigger
than the size of G.
Finally, the optimal matching between G and G?
is the one with the minimum matching cost
M? = arg min
M
C (M)
which gives us the most feasible result of grounding
the entities in the discourse graph with the objects in
the vision graph.
Given our formulation of referential grounding as
a graph matching problem, the next question is how
to find the optimal matching between two graphs.
Unfortunately, such a problem belongs to the class
of NP-complete (Conte et al, 2004). In practice,
techniques such as A? search are commonly used to
improve the efficiency, e.g. in (Tsai and Fu, 1979;
Tsai and Fu, 1983). But the memory requirement
can still be considerably large if the heuristic does
not provide a close estimate of the future matching
cost (Conte et al, 2004). In our current approach, we
use a simple beam search algorithm (Zhang, 1999)
to retain the tractability. Following the assumption
in (Eshera and Fu, 1984), we set the beam size as
hJ2, where h is the current level of the search tree
and J is the size of the bigger graph (in our caseG?).
4.3 Symbol Grounding Functions
As mentioned in Section 4.1, in referential ground-
ing the discourse graph and the vision graph pos-
sess different types of attribute values, therefore we
introduce a set of ?symbol grounding functions?,
based on which node/edge substitution and insertion
costs can be formally defined.
We start with node substitution cost to give a for-
mal definition of symbol grounding functions. As
defined in the previous section, the node substitu-
tion cost of mapping (substituting) node a with node
a? is3
CN (a, a?)
3For the ease of notation we have dropped the subscript of a
node.
Recall that in our definition of ARG, each node
is represented by a vector of attributes, i.e. a =
[v1, v2, ? ? ? , vK ] and a? = [v?1, v?2, ? ? ? , v?K ]. Thus,
we define the node substitution cost as
CN (a, a?) =
K?
k=1
? ln fk (vk, v?k)
in which fk (vk, v?k) = p (p ? [0, 1]) is what we call
the symbol grounding function for the k-th attribute.
More specifically, a symbol grounding function
for the k-th attribute takes two input arguments,
namely vk and v?k, which are the values of the k-th
attribute from node a and a? respectively. The out-
put of the function is a real number p in the range
of [0, 1], which can be interpreted as a measurement
of the compatibility between a symbol (or word) vk
and a visual feature value v?k.
Let L = {w1, w2, ? ? ? , wZ ,UNK} be the set of all
possible symbolic values of vk, then fk (vk, v?k) can
be further decomposed as
fk (vk, v?k) =
?
??????
??????
fk1 (v?k) if vk = w1;
fk2 (v?k) if vk = w2;... ...
fkZ (v?k) if vk = wZ ;
?k if vk = UNK.
Here the idea is that each value of vk may specify an
unique function that determines the compatibility of
a visual feature value v?k. For example, suppose that
we are defining a symbol grounding function for the
attribute of ?spatial location?, i.e. where is an ob-
ject located in the environment. The symbolic value
v can be in the set of {Top,Bottom, ? ? ? ,UNK}, and
the visual feature value v? is the x and y coordinates
(in pixels) of the object?s center of mass in the im-
age. A grounding function for the symbol Top can
be defined as4
fTop (v?) = fTop (x, y) =
{
1? y800 if y < 400;
0 otherwise.
Note that we have added a special symbol UNK
to represent the ?unknown? (or ?unspecified?) value
of vk. When the value of an attribute in the dis-
course graph is unknown, i.e. the speaker did not
mention anything about a particular property, the
grounding function will simply return a predefined
4Assume that the size of the image is 800? 800 pixels and
the left-top corner is the origin (0, 0)
145
Type of Error Numberof Objects
No Error 9 (5.1%)
Recognition Error 150 (84.7%)
Segmentation Error 18 (10.2%)
Total 177
Table 1: Types of errors among all the target (named)
objects. Recognition error: an object is incorrectly rec-
ognized as another type of object, or an unknown type.
Segmentation error: an object is missing, or merged with
another object.
constant, which we denote as ?. The node insertion
cost CN (a,?) is now defined as5
CN (a,?) =
K?
k=1
? ln ?k
Currently we set al the symbol grounding func-
tions? outputs for the unknown value (i.e. the ?s) to
?, which is an arbitrarily small real number (? > 0).
5 Empirical Results
Three pairs of subjects participated in our experi-
ment. Each pair (one acted as the director and the
other as the matcher) completed the naming task on
8 randomly created images. In total we collected 24
dialogues with 177 target objects to be named. Table
1 summarizes the errors made by the CV algorithms
when the 177 named objects from the original im-
ages were processed and represented in the impover-
ished images, as described in Section 3.1. As shown
in the table, only 5% of the objects were correctly
represented in the impoverished images. The other
95% of objects were either mis-recognized (about
85%) or mis-segmented (10%).
The evaluation of our approach is based on
whether the target objects are correctly grounded
by the graph matching method. To focus our cur-
rent effort on the referential grounding aspect, we
ignored all the matchers? contributions to the dia-
logues. Thus the discourse graphs were built based
on only the director?s utterances. The formal se-
mantics of each of the director?s valid utterances
was manually annotated using the DRS (Discourse
Representation Structure) representation (Bird et al,
2009). The discourse graphs were then generated
5The edge substitution/insertion cost is defined in the same
way as the node substitution/insertion cost.
Accuracy/Detection Rate
Type of Error Object-properties Object-properties
Only and Relations
No Error 66.7% (6/9) 77.8% (7/9)
Recognition Error 38.7% (58/150) 66% (99/150)
Segmentation Error 33.3% (6/18) 44.4% (8/18)
Overall 39.5% (70/177) 64.4% (114/177)
Table 2: Referential grounding performance of our
method. The accuracy/detection rates in the table were
obtained by comparing the results with annotated ground
truths.
from the annotated formal semantics. The vision
graphs were generated from the outputs of the CV
algorithms. The graph matching method was then
applied to return a (sub-) optimal matching between
the two graphs.
Table 2 shows the referential grounding perfor-
mance of our method. To better understand the ad-
vantages of the graph-based approach, we have com-
pared two settings. In the first setting, only the
object-specific properties are considered for com-
puting the comparability between a linguistic ex-
pression and a visual object, and the relations be-
tween objects are ignored. This setting is similar
to the baseline approach used in (Prasov and Chai,
2008; Prasov and Chai, 2010). In the second set-
ting, the complete graph-based approach is applied,
i.e. both the object?s properties and the relations be-
tween objects are considered. As shown in Table 2,
although the improvements of performance for the
no-error objects and mis-segmented objects are not
significant due to the small sample sizes, the perfor-
mance for the mis-recognized objects is significantly
improved by 27.3% (p < .001). The improvement
for the overall performance is also significant (by
24.9%, p < .001). The comparison between two
settings have demonstrated the importance of rep-
resenting and reasoning on relations between ob-
jects in referential grounding, and the graph-based
approach provides an ideal solution to capture rela-
tions.
In particular, even CV error rate is high (due to the
simple CV algorithms we used), our method is still
able to achieve 66% accuracy of grounding the mis-
recognized objects. Furthermore, when a referred
object is completely ?missing? in the vision graph
146
due to segmentation error6, our method is capable
to detect such discrepancy between linguistic input
and visual perception. The results have shown that
44.4% of those cases have been correctly detected.
This is also a very important aspect since informa-
tion about failures of grounding will allow the di-
alogue manager and/or the vision system to adapt
better strategies.
6 Discussions
The work presented here only represents an initial
step in our on-going investigation towards mediat-
ing shared perceptual basis in situated dialogue. It
consists of several simplifications which will be ad-
dressed in our future work.
First, the discourse graph is created only based
on contributions from the director, using manual an-
notations of formal semantics of the discourse. As
shown in the examples (Section 3.2), the collabora-
tive discourse has rich dynamics reflecting partici-
pants? collaborative behaviors. So our future work
is to model these different discourse dynamics and
take them into account in the creation of the dis-
course graph. The discourse graph will be created
after each contribution as the conversation unfolds.
When utterances are automatically processed, se-
mantics of these utterances often will not be ex-
tracted correctly or completely as in their manual
annotations. Therefore, our future work will also
explore how to efficiently match hypothesized dis-
course graphs (from automated semantic process-
ing) with vision graphs.
Second, our current symbol grounding functions
are very simple and intuitive. Our future work will
explore more sophisticated models that have theoret-
ical motivations (e.g., grounding spatial terms based
on the Attentional Vector Sum (AVS) model (Regier
and Carlson, 2001)) and enable automated acquisi-
tion of these functions (Roy, 2002; Gorniak and Roy,
2004b). In addition, we will explore context-based
symbol grounding functions where context will be
explicitly modeled. Grounding a linguistic term to a
visual feature will be influenced by contextual fac-
tors such as surroundings of the environment, the
6For example, if the director refers to ?a white ball? but
CV algorithm fails to detect that object from the environment,
then the node in the discourse graph representing ?a white ball?
should not be mapped to anything in the vision graph.
discourse history, the speaker?s individual prefer-
ence, and so on.
Lastly, as shown in our examples, the matcher
also contributes significantly to ground references.
This appears to suggest that, in situated dialogue,
lower-calibre partners (i.e., robot, and here the
matcher) also make extra effort to ground refer-
ences. The underlying motivation could be their
urge to match what they perceive from the environ-
ment to what they are told by their higher-calibre
partners (i.e., human). This motivation can be poten-
tially modeled as graph-matching and can be used
to guide the design of system responses. We will
explore this idea in the future.
7 Conclusion
In situated human robot dialogue, a robot and its
human partners have significantly mismatched capa-
bilities in perceiving the environment, which makes
grounding of references in the environment espe-
cially difficult. To address this challenge, this paper
describes an empirical study investigating how hu-
man partners mediate the mismatched perceptual ba-
sis. Based on this data, we developed a graph-based
approach and formulate referential grounding as in-
exact graph matching. Although our current investi-
gation has several simplifications, our initial empiri-
cal results have shown the potential of this approach
in mediating shared perceptual basis in situated dia-
logue.
Acknowledgments
This work was supported by Award #1050004 and
Award #0957039 from National Science Founda-
tion and Award #N00014-11-1-0410 from Office of
Naval Research.
References
A.H. Anderson, M. Bader, E.G. Bard, E. Boyle, G. Do-
herty, S. Garrod, S. Isard, J. Kowtko, J. McAllister,
J. Miller, et al 1991. The hcrc map task corpus. Lan-
guage and speech, 34(4):351?366.
S. Bird, E. Klein, and E. Loper. 2009. Natural language
processing with Python. O?Reilly Media.
T. Brick and M. Scheutz. 2007. Incremental natural
language processing for hri. In Proceeding of the
147
ACM/IEEE international conference on Human-Robot
Interaction (HRI-07), pages 263?270.
S. Carpin, M. Lewis, J. Wang, S. Balakirsky, and
C. Scrapper. 2007. USARSim: a robot simulator for
research and education. In Proceedings of the 2007
IEEE Conference on Robotics and Automation.
J.Y. Chai, P. Hong, and M.X. Zhou. 2004a. A proba-
bilistic approach to reference resolution in multimodal
user interfaces. In Proceedings of the 9th international
conference on Intelligent user interfaces, pages 70?77.
ACM.
J.Y. Chai, P. Hong, M.X. Zhou, and Z. Prasov. 2004b.
Optimization in multimodal interpretation. In Pro-
ceedings of the 42nd Annual Meeting on Association
for Computational Linguistics, page 1. Association for
Computational Linguistics.
S. Chernova, J. Orkin, and C. Breazeal. 2010. Crowd-
sourcing hri through online multiplayer games. AAAI
Symposium on Dialogue with Robots.
H. H. Clark and D. Wilkes-Gibbs. 1986. Referring as a
collaborative process. In Cognition, number 22, pages
1?39.
H. H. Clark. 1996. Using language. Cambridge Univer-
sity Press, Cambridge, UK.
D. Conte, P. Foggia, C. Sansone, and M. Vento. 2004.
Thirty years of graph matching in pattern recognition.
International journal of pattern recognition and artifi-
cial intelligence, 18(3):265?298.
M. A. Eshera and K. S. Fu. 1984. A graph distance mea-
sure for image analysis. IEEE transactions on systems,
man, and cybernetics, 14(3):398?410.
M.E. Foster, E.G. Bard, R.L. Hill, M. Guhe, J. Oberlan-
der, and A. Knoll. 2008. Generating haptic- osten-
sive referring expressions in cooperative, task-based
human-robot dialogue. Proceedings of ACM/IEEE
Human-Robot Interaction.
B. Fransen, V. Morariu, E. Martinson, S. Blis-
ard, M. Marge, S. Thomas, A. Schultz, and
D. Perzanowski. 2007. Using vision, acoustics, and
natural language for disambiguation. In Proceedings
of HRI07, pages 73?80.
P. Gorniak and D. Roy. 2004a. Grounded semantic com-
position for visual scenes. In Journal of Artificial In-
telligence Research, volume 21, pages 429?470.
P. Gorniak and D. Roy. 2004b. Grounded semantic com-
position for visual scenes. J. Artif. Intell. Res. (JAIR),
21:429?470.
P. Gorniak and D. Roy. 2007. Situated language under-
standing as filtering perceived affordances. In Cogni-
tive Science, volume 31(2), pages 197?231.
A. Green and K. Severinson Eklundh. 2001. Task-
oriented dialogue for CERO: a user centered ap-
proach. In Proceedings of 10th IEEE international
workshop on robot and human interactive communi-
cation, September.
Sonja Huwel and Britta Wrede. 2006. Situated
speech understanding for robust multi-modal human-
robot communication. In Proceedings of the In-
ternational Conference on Computational Linguistics
(COLING/ACL).
P. Kahn, N. Greier, T. Kanda, H. Ishiguro, J. Ruckert,
R. Severson, and S. Kane. 2008. Design patterns for
sociality in human-robot interaction. In Proceedings
of HRI, pages 97?104.
Geert-Jan M. Kruijff, Pierre Lison, Trevor Benjamin,
Henrik Jacobsson, and Nick Hawes. 2007. Incremen-
tal, multi-level processing for comprehending situated
dialogue in human-robot interaction. In Symposium on
Language and Robots.
C. Liu, J. Walker, and J.Y. Chai. 2010. Disambiguating
frames of reference for spatial language understanding
in situated dialogue. In AAAI Fall Symposium on Dia-
logue with Robots.
C. Liu, D. Kay, and J.Y. Chai. 2011. Awareness of part-
ners eye gaze in situated referential grounding: An em-
pirical study. In 2nd Workshop on Eye Gaze in Intelli-
gent Human Machine Interaction.
N. Otsu. 1975. A threshold selection method from gray-
level histograms. Automatica, 11:285?296.
Z. Prasov and J.Y. Chai. 2008. What?s in a gaze?: the
role of eye-gaze in reference resolution in multimodal
conversational interfaces. In Proceedings of the 13th
international conference on Intelligent user interfaces,
pages 20?29. ACM.
Z. Prasov and J.Y. Chai. 2010. Fusing eye gaze with
speech recognition hypotheses to resolve exophoric
references in situated dialogue. In Proceedings of
the 2010 Conference on Empirical Methods in Natu-
ral Language Processing, pages 471?481. Association
for Computational Linguistics.
T. Regier and L.A. Carlson. 2001. Grounding spatial lan-
guage in perception: an empirical and computational
investigation. Journal of Experimental Psychology:
General, 130(2):273.
D.K. Roy. 2002. Learning visually grounded words and
syntax for a scene description task. Computer Speech
& Language, 16(3):353?385.
A. Sanfeliu and K. S. Fu. 1983. A distance measure
between attributed relational graphs for pattern recog-
nition. IEEE transactions on systems, man, and cyber-
netics, 13(3):353?362.
M. Scheutz, P. Schermerhorn, J. Kramer, and D. Ander-
son. 2007a. First steps toward natural human-like
HRI. In Autonomous Robots, volume 22.
M. Scheutz, P. Schermerhorn, J. Kramer, and D. Ander-
son. 2007b. Incremental natural language processing
for hri. In Proceedings of HRI.
148
M. Shiomi, T. Kanda, S. Koizumi, H. Ishiguro, and
N. hagita. 2007. Group attention control for commu-
nication robots with wizard of OZ approach. In Pro-
ceedings of HRI, pages 121?128.
M. Skubic, D. Perzanowski, S. Blisard, A. Schultz,
W. Adams, M. Bugajska, and D. Brock. 2004. Spatial
language for human-robot dialogs. IEEE Transactions
on Systems, Man and Cybernetics, Part C, 34(2):154?
167.
A. Steinfeld, O. C. Jenkins, and B. Scassellati. 2009. The
oz of wizard: Simulating the human for interaction re-
search. In Proceedings of HRI, pages 101?107.
W.H. Tsai and K.S. Fu. 1979. Error-correcting isomor-
phisms of attributed relational graphs for pattern anal-
ysis. Systems, Man and Cybernetics, IEEE Transac-
tions on, 9(12):757?768.
W.H. Tsai and K.S. Fu. 1983. Subgraph error-correcting
isomorphisms for syntactic pattern. year: 1983,
13:48?62.
D. Zhang and G. Lu. 2002. An integrated approach to
shape based image retrieval. In Proc. of 5th Asian con-
ference on computer vision (ACCV), pages 652?657.
W. Zhang. 1999. State-space search: Algorithms, com-
plexity, extensions, and applications. Springer-Verlag
New York Inc.
149
Proceedings of the SIGDIAL 2013 Conference, pages 78?86,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
Modeling Collaborative Referring for Situated Referential Grounding
Changsong Liu, Rui Fang, Lanbo She, Joyce Y. Chai
Department of Computer Science and Engineering
Michigan State University
East Lansing, MI 48824
{cliu, fangrui, shelanbo, jchai}@cse.msu.edu
Abstract
In situated dialogue, because humans and
agents have mismatched capabilities of
perceiving the shared physical world, ref-
erential grounding becomes difficult. Hu-
mans and agents will need to make ex-
tra efforts by collaborating with each other
to mediate a shared perceptual basis and
to come to a mutual understanding of in-
tended referents in the environment. In
this paper, we have extended our previous
graph-matching based approach to explic-
itly incorporate collaborative referring be-
haviors into the referential grounding al-
gorithm. In addition, hypergraph-based
representations have been used to account
for group descriptions that are likely to oc-
cur in spatial communications. Our empir-
ical results have shown that incorporating
the most prevalent pattern of collaboration
with our hypergraph-based approach sig-
nificantly improves reference resolution in
situated dialogue by an absolute gain of
over 18%.
1 Introduction
As more and more applications require humans
to interact with robots, techniques to support sit-
uated dialogue have become increasingly impor-
tant. In situated dialogue, humans and artificial
agents (e.g., robots) are co-present in a shared
environment to achieve joint tasks. Their dia-
logues often involve making references to the en-
vironment. To ensure the conversation proceeds
smoothly, it is important to establish a mutual un-
derstanding of these references, a process called
referential grounding (Clark and Brennan, 1991):
the agent needs to identify what the human refers
to in the environment and the human needs to
know whether the agent?s understanding is correct;
and vice versa.
Although reference resolution (Heeman and
Hirst, 1995; Gorniak and Roy, 2004; Siebert
and Schlangen, 2008) and referential ground-
ing (Traum, 1994; DeVault et al, 2005) have been
studied in previous work, the unique characteris-
tics of situated dialogue post bigger challenges to
this problem. In situated dialogue, although hu-
mans and agents are co-present in a shared world,
they have different capabilities in perceiving the
environment (a human can perceive and reason
about the environment much better than an agent).
The shared perceptual basis, which plays an im-
portant role in facilitating referential grounding
between the human and the agent, thus is miss-
ing. Communication between the human and the
agent then becomes difficult, and they will need
to make extra efforts to jointly mediate a shared
basis and reach a mutual understanding (Clark,
1996). The goal of this paper is to investigate what
kinds of collaborative efforts may happen under
mismatched perceptual capabilities and how such
collaborations can be incorporated into our refer-
ential grounding algorithm.
Previous psycholinguistic studies have indi-
cated that grounding references is a collaborative
process (i.e., collaborative referring) (Clark and
Wilkes-Gibbs, 1986; Clark and Brennan, 1991):
The process begins with one participant present-
ing an initial referring expression. The other par-
ticipant would then either accept it, reject it, or
postpone the decision. If a presentation is not
accepted, then either one participant or the other
needs to refashion it. This new presentation (i.e.,
the refashioned expression) is then judged again,
and the process continues until the current pre-
sentation is accepted. To understand the implica-
tion of collaborative referring under the situation
of mismatched perceptual capabilities, we have
conducted experiments on human-human conver-
sation using a novel experimental setup. Our col-
lected data demonstrate an overwhelming use of
78
collaborative referring to mediate a shared percep-
tual basis.
Motivated by these observations, we have de-
veloped an approach that explicitly incorporates
collaborative referring into a graph-matching al-
gorithm for referential grounding. As the conver-
sation unfolds, our approach incrementally builds
a dialogue graph by keeping track of the contri-
butions (i.e., presentation and acceptance) from
both the human and the robot. This dialogue
graph is then matched against the perceived en-
vironment (i.e., a vision graph representing what
are perceived by the robot from the environment)
in order to resolve referring expressions from the
human. In addition, in contrast to our previous
graph-based approach (Liu et al, 2012), the new
approach applies hypergraphs: a more general
and flexible representation that can capture group-
based (n-ary) relations (whereas a regular graph
can only model binary relations between two enti-
ties). Our empirical results have shown that, incor-
porating the most prevalent pattern of collabora-
tion (i.e., agent-present-human-accept, discussed
later) with the hypergraph-based approach signif-
icantly improves reference resolution in situated
dialogue by an absolute gain of over 18%.
In the following sections, we first give a brief
discussion about the related work. We then de-
scribe our experiment setting and the patterns of
collaboration observed in the collected data. We
then illustrate how to build a dialogue graph as
the conversation unfolds, followed by the formal
definition of the hypergraph representation and
the referential grounding procedure. Finally we
demonstrate the advantage of using hypergraphs
and incorporating a prevalent collaborative behav-
ior into the graph-matching approach for reference
resolution.
2 Related Work
In an early work, Mellish (Mellish, 1985) used a
constraint satisfaction approach to identify refer-
ents that could be only partially specified. This
work illustrated the theoretical idea of how to re-
solve referring expressions based on an internal
model of a world. Heeman and Hirst (Heeman
and Hirst, 1995) presented a planning-based ap-
proach to cast Clark?s collaborative referring idea
into a computational model. They used plan con-
struction and plan inference to capture the pro-
cesses of building referring expressions and identi-
fying their referents. Previous work in situated set-
tings (Dhande, 2003; Gorniak and Roy, 2004; Fu-
nakoshi et al, 2005; Siebert and Schlangen, 2008)
mainly focused on developing/learning computa-
tional models that map words to visual features of
objects in the environment. These ?visual seman-
tics? of words were then integrated into seman-
tic composition procedures to resolve referring ex-
pressions.
These previous work has provided valuable in-
sights in computational approaches for reference
resolution. However, they mostly dealt with a sin-
gle expression or a single referent. In this pa-
per, our goal is to resolve complex referring di-
alogues that involve multiple objects in a shared
environment. In our previous work (Liu et al,
2012), we developed a graph-matching based ap-
proach to address this problem. However, the pre-
vious approach can not handle group-based rela-
tions among multiple objects. Furthermore, it did
not look into incorporating collaborative behav-
iors, which is a particularly important characteris-
tic in situated dialogue. This paper aims to address
these limitations.
3 Experiments and Observations
To investigate collaborative referring under mis-
matched perceptual capabilities, we conducted ex-
periments on human-human interaction (details of
the experimental setup can be found in (Liu et al,
2012)). In these experiments, we have two human
subjects play a set of naming games. One sub-
ject (referred to as the human-player) is provided
with an original image containing over ten objects
(Figure 1(a)). Several of these objects have se-
cret names. The other subject (referred to as the
robot-player) only has access to an impoverished
image of the same scene (Figure 1(b)) to mimic
the lower perceptual capability of a robot. The
human-player?s goal is to communicate the names
of target objects to the robot-player so that the
robot-player knows which object in his view has
what name. The impoverished image was auto-
matically created by applying standard computer
vision algorithms and thus may contain different
types of processing errors (e.g., mis-segmentation
and/or mis-recognition).
Using this setup, we have collected a set of dia-
logues. The following shows an example dialogue
segment (collected using the images shown in Fig-
ure 1):
79
Figure 1: An example of different images used in
our experiments.
H1: there is basically a cluster of four objects in the upper
left, do you see that
R2 : yes
H: ok, so the one in the corner is a blue cup
R : I see there is a square, but fine, it is blue
H: alright, I will just go with that, so and then right under
that is a yellow pepper
R : ok, I see apple but orangish yellow
H: ok, so that yellow pepper is named Brittany
R : uh, the bottom left of those four? Because I do see a
yellow pepper in the upper right
H: the upper right of the four of them?
R : yes
H: ok, so that is basically the one to the right of the blue cup
R : yeah
H: that is actually an apple, it is green, I guess it has some
amount of yellow on it, but that is a green apple and it
is named Ashley
. . . . . .
This example demonstrates two important char-
acteristics regarding referential communication
under mismatched perceptual capabilities. First,
conversation partners rely on both object-specific
properties (e.g., object class, color) and spatial
relations to describe objects in the environment.
Spatial expressions include not only the binary re-
lations (e.g., ?the one to the left of the blue cup?),
but also the group-based references (Tenbrink and
Moratz, 2003; Funakoshi et al, 2005) (e.g., ?the
upper right of the four of them?).
Second, because the shared perceptual basis
is missing here, the partners make extra efforts
to refer and ground references. For example,
the human-player go through step-by-step install-
ments (Clark and Wilkes-Gibbs, 1986) to come
to the targeted object. The robot-player often
proactively provides what he perceives from the
environment. The human-player and the robot-
player collaborate with each other through itera-
tive presentation-acceptance phases as described
in the Contribution Model proposed in (Clark and
Schaefer, 1989; Clark and Brennan, 1991).
1H stands for the human-player.
2R stands for the robot-player.
These observations indicate that, the approach
to referential grounding in situated dialogue
should capture not only binary relations but also
group-based relations. Furthermore, it should go
beyond traditional approaches that purely rely on
semantic constraints from single utterances. It
should incorporate the step-by-step collaborative
dynamics from the discourse as the conversation
proceeds.
4 Modeling Collaboration
In this section, we first give a brief description of
collaboration patterns observed in our data, and
then discuss one prevalent pattern and illustrate
how it may be taken into consideration by our
computational approach for referential grounding.
4.1 Patterns of Collaboration
Consistent with Clark?s Contribution Model, the
interactions between the human-player and the
robot-player in general fall into two phases: a pre-
sentation phase and an acceptance phase. In our
data, a presentation phase mainly consists of the
following three forms:
? A complete description: the speaker issues a
complete description in a single turn. For ex-
ample, ?there is a red apple on the top right?.
? An installment: a description is dividedinto several parts/installments, each of whichneeds to be confirmed before continuing tothe rest. For example,
A: under the big green cup we just talked about,
B: yes
A: there are two apples,
B: OK
A: one is red and one is yellow.
? A trial: a description (either completed or in-
complete) with a try marker. For example, ?Is
there a red apple on the top right??
In an acceptance phase, the addressee can either
accept or reject the current presentation. Two ma-
jor forms of accepting a presentation are observed
in our data:
? Acknowledgement: the addressee explicitly
shows his/her understanding, using assertions
(e.g., ?Yes?,?Right?, ?I see?) or continuers
(e.g., ?uh huh?, ?ok?).
? Relevant next turn: the addressee proceeds
to the next contribution that is relevant to the
current presentation. For example: A says ?I
see a red apple? and directly following that B
says ?there is also a green apple to the right
of that red one?.
80
In addition, there are also two forms of rejecting
a presentation:
? Rejection: the addressee explicitly rejects the
current presentation, for example, ?I don?t
see any apple?.
? Alternative description: the addressee
presents an alternative description. For
example, A says ?there is a red apple on the
top left,? and immediately following that B
says ?I only see a red apple on the right?.
In general, referential grounding dialogues in
our data emerge as hierarchical structures of re-
cursive presentation-acceptance phases. The ac-
ceptance to a previous presentation often repre-
sents a new presentation itself, which triggers fur-
ther acceptance. In particular, our data shows
that when mediating their shared perceptual ba-
sis, the human-player often takes into considera-
tion what the robot-player sees and uses that to
gradually lead to his intended referents. This is
demonstrated in the following example3, where
the human-player accepts (Turn 3) the robot-
player?s presentation (Turn 2) through a relevant
next turn.
(Turn 1) H: There is a kiwi fruit.
(Turn 2) R: I don?t see any kiwi fruit. I see an apple.
(Turn 3) H: Do you see a mug to the right of that apple?
(Turn 4) R: Yes.
(Turn 5) H: OK, then the kiwi fruit is to the left of that apple.
As shown later in Section 5, this is one promi-
nent collaborative strategy observed in our data.
We give this pattern a special name: agent-
present-human-accept collaboration. Next we
continue to use this example to show how the
agent-present-human-accept pattern can be incor-
porated to potentially improve reference resolu-
tion.
4.2 An Illustrating Example
In this example, the human and the robot face
a shared physical environment. The robot per-
ceives the environment through computer vision
(CV) algorithms and generates a graph represen-
tation (i.e., a vision graph), which captures the
perceived objects and their spatial relations4. As
shown in Figure 2(a), the kiwi is represented as
an unknown object in the vision graph due to in-
sufficient object recognition. Besides the vision
3This is a clean-up version of the original example to
demonstrate the key ideas.
4The spatial relations between objects are represented as
their relative coordinates in the vision graph.
graph, the robot also maintains a dialogue graph
that captures the linguistic discourse between the
human and the robot.
At Turn 1 in Figure 2(b), the human says ?there
is a kiwi fruit?. Upon receiving this utterance,
through semantic processing, a node representing
?a kiwi? is generated (i.e., x1). The dialogue graph
at this point only contains this single node. Iden-
tifying the referent of the expression ?a kiwi fruit?
is essentially a process that matches the dialogue
graph to the vision graph. Because the vision
graph does not have a node representing a kiwi ob-
ject, no high confidence match is returned at this
point. Therefore, the robot responds with a rejec-
tion as in Turn 2 (Figure 2(c)) ?I don?t see any
kiwi fruit? 5. In addition, the robot takes an extra
effort to proactively describe what is being con-
fidently perceived (i.e., ?I see an apple?). Now
an additional node y1 is added to the dialogue
graph to represent the term ?an apple? 6. Note that
when the robot generates the term ?an apple?, it
knows precisely which object in the vision graph
this term refers to. Therefore, as shown in Fig-
ure 2(c), y1 is mapped to v2 in the vision graph.
In Turn 3 (Figure 2(d)), through semantic pro-
cessing on the human?s utterance ?a mug to the
right of that apple?, two new nodes (x2 and x3)
and their relation (RightOf ) are added to the di-
alogue graph. In addition, since ?that apple?(i.e.,
x2) corefers with ?an apple? (i.e., y1) presented by
the robot in the previous turn, a coreference link
is created from x2 to y1. Importantly, in this turn
human displays his acceptance of the robot?s pre-
vious presentation (?an apple?) by coreferring to it
and building further reference based on it. This is
exactly the agent-present-human-accept strategy
described earlier. Since y1 maps to object v2 and
x2 now links to y1, it becomes equivalent to con-
sider x2 also maps to v2. We name a node such
as x2 a grounded node, since from the robot?s
point of view this node has been ?grounded? to a
perceived object (i.e., a vision graph node) via the
agent-present-human-accept pattern.
At this point, the robot matches the updated di-
alogue graph with the vision graph again and can
5Note that, since in this paper we are working with a
dataset of human-human (i.e., the human-player and the
robot-player) dialogues, decisions from the robot-player are
assumed known. We leave robot?s decision making (i.e., re-
sponse generation) into our future work.
6We use xi to denote nodes that represent expressions
from the human?s utterances and yi to represent nodes from
the robot?s utterances.
81
Figure 2: An example of incorporating collaborative efforts in an unfolding dialogue into graph representations.
successfully match x3 to v3. Note that, the match-
ing occurs here is considered constrained graph-
matching in the sense that some nodes in the dia-
logue graph (i.e., x2) are already grounded, and
the only node needs to be matched against the
vision graph is x3. Different from previous ap-
proaches that do not take dialogue dynamics into
consideration, the constrained matching utilizes
additional constraints from the collaboration pat-
terns in a dialogue and thus can improve both the
efficiency and accuracy of the matching algorithm.
This is one innovation of our approach here.
Based on such matching result, the robot re-
sponds with a confirmation as in Turn 4 Fig-
ure 2(e)). The human further elaborates in Turn
5 ?the kiwi is to the left of the apple?. Again se-
mantic processing and linguistic coreference reso-
lution will allow the robot to update the dialogue
graph as shown in Figure 2(f). Given this dialogue
graph, based on the context of the larger dialogue
graph and through constrained matching, it will
be possible to match x1 to v1 although the object
class of v1 is unknown.
This example demonstrates how the dialogue
graph can be created to incorporate the collabo-
rative referring behaviors as the conversation un-
folds and how such accumulated dialogue graph
can help referential resolution through constrained
matching. Next, we give a detailed account on
how to create a dialogue graph and briefly discuss
graph-matching for reference resolution.
4.3 Dialogue Graph
To account for different types of referring expres-
sions (i.e., object-properties, binary relations and
group-based relations), we use hypergraphs to rep-
resent dialogue graphs.
4.3.1 Hypergraph Representation
A directed hypergraph (Gallo et al, 1993) is a 2-
tuple in the form of G = (X, A), in which
X = {xm}
A = {ai = (ti, hi) | ti ? X, hi ? X}
82
(a) Dialogue Graph (b) Vision Graph
Figure 3: Example hypergraph representations
X is a set of nodes, and A is a set of ?hyperarcs?.
Similar to an arc in a regular directed graph, each
hyperarc ai in a hypergraph also has two ?ends?,
i.e., a tail (ti) and a head (hi). The tail and head
of a hyperarc are both subsets of X , thus they can
contain any number of nodes in X . Hypergraph is
a more general representation than regular graph.
It can represent not only binary relations between
two nodes, but also group-based relations among
multiple nodes.
For example, suppose the language input issued
by the human includes the following utterances:
1. There is a cluster of four objects in the upper left.
2. The one in the corner is a blue cup.
3. Under the blue cup is a yellow pepper.
4. To the right of the blue cup, which is also in the upper
right of the four objects, is a green apple.
The corresponding dialogue graph Gd =
(Xd, Ad) is shown in Figure 3(a), where Xd =
{x1, x2, x3, x4} and Ad = {a1, a2, a3}. In Ad,
for example, a1 = ({x1}, {x3}) represents the
relation ?right of? between the tail {x3} and the
head {x1}, and a3 = ({x3}, {x1, x2, x3, x4}) rep-
resents the group-based relation ?upper right? be-
tween one node and a group of nodes.
As also illustrated in Figure 3(a), we can at-
tach a set of labels (or attributes) {attrk} to a
node/hyperarc, and use them to store specific in-
formation about this node/hyperarc. The per-
ceived visual world can be represented by a
hypergraph in a similar way (i.e., a vision graph),
as shown in Figure 3(b) 7.
4.3.2 Building Dialogue Graphs
Given the hypergraph representation, a set of op-
erations can be applied to build a dialogue graph
as the conversation unfolds. It mainly consists of
three components:
7Hyperarcs of the vision graph are not shown in the figure.
A hyperarc may exist between any two subsets of objects.
Semantic Constraints. Apply a semantic parser to
extract information from human utterances. For
example, the utterance ?The kiwi is to the left of
the apple? can be parsed into a formal meaning
representation as
[x1, x2] , [Kiwi(x1), Apple(x2), LeftOf(x1, x2)]
This representation contains a list of discourse
entities introduced by the utterance, and a list of
FOL predicates specifying the properties and rela-
tions of these entities. For each discourse entity, a
node is added to the graph. Unary predicates be-
come the labels for nodes, and binary predicates
become arcs in the graph. Group-based relations
are incorporated into the graphs as hyperarcs.
Discourse Coreference. For each discourse entity
in a referring expression, identify whether it is a
new discourse entity or it corefers to a discourse
entity mentioned earlier. In our previous example
in Figure 2(d), x2 corefers with y1, thus a coref-
erence link is added to link the coreferring nodes.
Coreferring nodes are merged before matching.
Dialogue Dynamics. Different types of dialogue
dynamics can be modeled. In this paper, we only
focus on a particularly prevalent type of dynamics
as observed from our data, i.e. the agent-present-
human-accept pattern as we described in Section
4.1. When such a pattern is identified, the associ-
ated nodes (e.g., x2 in the previous example) will
be marked as grounded nodes and the mappings
to their grounded visual entities (i.e., vision graph
nodes) will be added into the dialogue graph.
Based on the above three types of operations,
the dialogue graph is updated at each turn of the
conversation.
4.3.3 Constrained Matching
Given a dialogue graph G = (X, A) and a vi-
sion graph G? = (X ?, A?), reference resolution
becomes a graph matching problem which is to
83
find a one-to-one mapping between the nodes in
X and in X ?. Due to the insufficiencies of the
NLP and the CV components, both the dialogue
graph and the vision graph are likely to contain er-
rors. Therefore, we do not require every node in
the dialog graph to be mapped to a node in the vi-
sion graph, but follow the inexact graph matching
criterion (Conte et al, 2004) to find the best match
even if they are only partial.
The matching algorithm is similar to the one
used in our previous work for regular graphs (Liu
et al, 2012), which uses a state-space search ap-
proach (Zhang, 1999). The key difference here
is to incorporate the agent-present-human-accept
collaboration pattern. The search procedure can
now start from the state that already represents
the known matching of grounded nodes (as il-
lustrated in Section 4.2), instead of starting from
the root. Thus it is constrained in a smaller and
more promising subspace to improve both effi-
ciency and accuracy.
5 Evaluation
A total of 32 dialogues collected from our ex-
periments (as described in Section 3) are used in
the evaluation. For each of these dialogues, we
have manually annotated (turn-by-turn) the formal
semantics, discourse coreferences and grounded
nodes as described in Section 4.3.2. Since the fo-
cus of this paper is on incorporating collaboration
into graph matching for referential grounding, we
use these annotations to build the dialogue graphs
in our evaluation. Vision graphs are automatically
generated by CV algorithms from the original im-
ages used in the experiments. The CV algorithms?
object recognition performance is rather low: only
5% of the objects in those images are correctly rec-
ognized. Thus reference resolution will need to
rely on relations and collaborative strategies.
The 32 dialogue graphs have a total of 384
nodes8 that are generated from human-players? ut-
terances (12 per dialogue on average), and a to-
tal of 307 nodes generated from robot-players? ut-
terances (10 per dialogue on average). Among
the 307 robot-player generated nodes, 187 (61%)
are initially presented by the robot-player and
then coreferred by human-players? following ut-
terances (i.e., relevant next turns). This indicates
8As mentioned in Section 4.3.2, multiple expressions that
are coreferential with each other and describing the same en-
tity are merged into a single node.
that the agent-present-human-accept strategy is a
prevalent way to collaborate in our experiment. As
mentioned earlier, those human-player generated
nodes which corefer to nodes initiated by robot-
players are marked as grounded nodes. In total,
187 out of the 384 human-player generated nodes
are in fact grounded nodes.
To evaluate our approach, we apply the graph-
matching algorithm on each pair of dialogue graph
and vision graph. The matching results are com-
pared with the annotated ground-truth to calcu-
late the accuracy of our approach in ground-
ing human-players? referring descriptions to vi-
sual objects. For each dialogue, we have pro-
duced matching results under four different set-
tings: with/without modeling collaborative re-
ferring (i.e., the agent-present-human-accept col-
laboration) and with/without using hypergraphs.
When collaborative referring is modeled, the
graph-matching algorithm uses the grounded
nodes to constrain its search space to match the
remaining ungrounded nodes. When collabora-
tive referring is not modeled, all the human-player
generated nodes need to be matched.
The results of four different settings (averaged
accuracies on the 32 dialogues) are shown in Ta-
ble 1. Modeling collaborative referring improves
the matching accuracies for both regular graphs
and hypergraphs. When regular graphs are used,
it improves overall matching accuracy by 11.6%
(p = 0.05, paired Wilcoxon T-test). The improve-
ment is even higher as 18.3% when hypergraphs
are used (p = 0.012, paired Wilcoxon T-test). The
results indicate that proactively describing what
the robot sees to the human to facilitate com-
munication is an important collaborative strategy
in referential grounding dialogues. Humans can
often ground the robot presented object via the
agent-present-human-accept strategy and use the
grounded object as a reference point to further
describe other intended object(s), and our graph-
matching approach is able to capture and utilize
such collaboration pattern to improve the referen-
tial grounding accuracy.
The improvement is more significant when
hypergraphs are used. A potential explanation
is that those group-based relations captured by
hypergraphs always involve multiple (more than
2) objects (nodes). If one node in a group-based
relation is grounded, all other involved nodes can
have a better chance to be correctly matched.
84
Regular graph Hypergraph
Not modeling 44.1% 47.9%collaborative referring
Modeling 55.7% 66.2%collaborative referring
Improvement 11.6% 18.3%
Table 1: Averaged matching accuracies under four
different settings.
Group 1 Group 2 Group 3
Number of dialogues 9 11 12
% of grounded nodes <30% 30%?60% >60%
Average number of 20 21 12object properties a
Average number of 11 13 8relations b
Not modeling 49.7% 49.4% 45.3%collaborative referring
Modeling 57.0% 76.6% 63.6%collaborative referring
Improvement 7.3% 27.2% 18.3%
aSpecified by human-players.
bSpecified by human-players. The number includes both
binary and group-based relations.
Table 2: Matching accuracies of three groups of
dialogues (all the matching results here are pro-
duced using hypergraphs).
Whereas in regular graphs one grounded node can
only improve the chance of one other node, since
only one-to-one (binary) relations are captured by
regular graphs.
To further investigate the effect of modeling
collaborative referring, we divide the 32 dia-
logues into three groups according to how often
the agent-present-human-accept collaboration pat-
tern happens (measured by the percentage of the
grounded nodes among all the human-player gen-
erated nodes in a dialogue). As shown at the top
part of Table 2, the agent-present-human-accept
pattern happened less often in the dialogues in
group 1 (i.e., less than 30% of human-player gen-
erated nodes are grounded nodes). In the dia-
logues in group 2, robot-players more frequently
provided proactive descriptions which led to more
grounded nodes. Robot-players were the most
proactive in the dialogues in group 3, thus this
group contains the highest percentage of grounded
nodes. Note that, although the dialogues in group
3 contain more proactive contributions from robot-
players, human-players tend to specify less num-
ber of properties and relations describing intended
objects (as shown in the middle part of Table 2).
The matching accuracies for each of the three
groups are shown at the bottom part of Table 2.
Since the agent-present-human-accept pattern ap-
pears less often in group 1, modeling collabora-
tive referring only improves matching accuracy
by 7.3%. The improvements for group 2 and
group 3 are more significant compared to group
1. However, group 3?s improvement is less than
group 2, although the dialogues in group 3 contain
more proactive contributions from robot-players.
This indicates that in some cases even with mod-
eling collaborative referring, underspecified in-
formation from human speakers (human-players
in our case) may still be insufficient to identify
the intended referents. Therefore, incorporating a
broader range of dialogue strategies to elicit ade-
quate information from humans is also important
for successful human-robot communication.
6 Conclusion
In situated dialogue, conversation partners make
extra collaborative efforts to mediate a shared per-
ceptual basis for referential grounding. It is impor-
tant to model such collaborations in order to build
situated conversational agents. As a first step, we
developed an approach for referential grounding
that takes a particular type of collaborative refer-
ring behavior, i.e. agent-present-human-accept,
into account. By incorporating this pattern into the
graph-matching process, our approach has shown
an absolute gain of over 18% in subsequent refer-
ence resolution. Extending the results in this pa-
per, our future work will address explicitly model-
ing the collaborative dynamics with a richer repre-
sentation. The dialogue graph presented in this pa-
per represents all the mentioned entities and their
relations that are currently available at any given
dialogue status. But we have not modeled the col-
laborative dynamics at the illocutionary level. Our
next step is to explicitly represent those dynam-
ics, not only for grounding human references to
the physical world, but also generating the collab-
orative behaviors for the agent.
Acknowledgments
This work was supported by N00014-11-1-0410
from the Office of Naval Research and IIS-
1208390 from the National Science Foundation.
References
Herbert H Clark and Susan E Brennan. 1991. Ground-
ing in communication. Perspectives on socially
shared cognition, 13(1991):127?149.
85
Herbert H Clark and Edward F Schaefer. 1989.
Contributing to discourse. Cognitive science,
13(2):259?294.
Herbert H Clark and Deanna Wilkes-Gibbs. 1986.
Referring as a collaborative process. Cognition,
22(1):1?39.
Herbert H Clark. 1996. Using language, volume 4.
Cambridge University Press Cambridge.
Donatello Conte, Pasquale Foggia, Carlo Sansone, and
Mario Vento. 2004. Thirty years of graph match-
ing in pattern recognition. International journal
of pattern recognition and artificial intelligence,
18(03):265?298.
David DeVault, Natalia Kariaeva, Anubha Kothari, Iris
Oved, and Matthew Stone. 2005. An information-
state approach to collaborative reference. In Pro-
ceedings of the ACL 2005 on Interactive poster and
demonstration sessions, pages 1?4. Association for
Computational Linguistics.
Sheel Sanjay Dhande. 2003. A computational model
to connect gestalt perception and natural language.
Master?s thesis, Massachusetts Institute of Technol-
ogy.
Kotaro Funakoshi, Satoru Watanabe, Takenobu Toku-
naga, and Naoko Kuriyama. 2005. Understanding
referring expressions involving perceptual grouping.
In 4th International Conference on Cyberworlds,
pages 413?420.
Giorgio Gallo, Giustino Longo, Stefano Pallottino,
and Sang Nguyen. 1993. Directed hypergraphs
and applications. Discrete applied mathematics,
42(2):177?201.
Peter Gorniak and Deb Roy. 2004. Grounded seman-
tic composition for visual scenes. J. Artif. Intell.
Res.(JAIR), 21:429?470.
Peter A Heeman and Graeme Hirst. 1995. Collabo-
rating on referring expressions. Computational Lin-
guistics, 21(3):351?382.
Changsong Liu, Rui Fang, and Joyce Y Chai. 2012.
Towards mediating shared perceptual basis in situ-
ated dialogue. In Proceedings of the 13th Annual
Meeting of the Special Interest Group on Discourse
and Dialogue, pages 140?149. Association for Com-
putational Linguistics.
Christopher S Mellish. 1985. Computer interpretation
of natural language descriptions. John Wiley and
Sons, New York, NY.
Alexander Siebert and David Schlangen. 2008. A
simple method for resolution of definite reference
in a shared visual context. In Proceedings of the
9th SIGdial Workshop on Discourse and Dialogue,
pages 84?87. Association for Computational Lin-
guistics.
Thora Tenbrink and Reinhard Moratz. 2003. Group-
based spatial reference in linguistic human-robot in-
teraction. In Proceedings of EuroCogSci, volume 3,
pages 325?330.
David R Traum. 1994. A Computational Theory
of Grounding in Natural Language Conversation.
Ph.D. thesis, University of Rochester.
Weixiong Zhang. 1999. State Space Search: Algo-
rithms, Complexity, Extensions, and Applications.
Springer.
86
Proceedings of the SIGDIAL 2014 Conference, pages 89?97,
Philadelphia, U.S.A., 18-20 June 2014.
c
?2014 Association for Computational Linguistics
Back to the Blocks World: Learning New Actions through Situated
Human-Robot Dialogue
Lanbo She
1
, Shaohua Yang
1
, Yu Cheng
2
, Yunyi Jia
2
, Joyce Y. Chai
1
, Ning Xi
2
1
Department of Computer Science and Engineering
Michigan State University
East Lansing, MI 48824, USA
{shelanbo, jchai, yangshao}@cse.msu.edu
2
Department of Electrical and Computer Engineering
Michigan State University
East Lansing, MI 48824, USA
{chengyu9, jiayunyi, xin}@egr.msu.edu
Abstract
This paper describes an approach for a
robotic arm to learn new actions through
dialogue in a simplified blocks world. In
particular, we have developed a three-
tier action knowledge representation that
on one hand, supports the connection be-
tween symbolic representations of lan-
guage and continuous sensorimotor repre-
sentations of the robot; and on the other
hand, supports the application of existing
planning algorithms to address novel situ-
ations. Our empirical studies have shown
that, based on this representation the robot
was able to learn and execute basic actions
in the blocks world. When a human is
engaged in a dialogue to teach the robot
new actions, step-by-step instructions lead
to better learning performance compared
to one-shot instructions.
1 Introduction
When a new generation of robots start to work
side-by-side with their human partners in joint
tasks (Christensen et al., 2010), they will often
encounter new objects or are required to perform
new actions. It is important for the robots to au-
tomatically learn new knowledge about the en-
vironment and the tasks from their human part-
ners. To address this issue, this paper describes
our recent work on action learning through dia-
logue. As a first step, we limit our investigation to
a simple blocks world motivated by Terry Wino-
grad?s early work (Winograd, 1972). By using
an industrial robotic arm (SCHUNK) in this small
world, we are interested in addressing the follow-
ing questions. First, human language has a dis-
crete and symbolic representation, but the robot
arm has a continuous representation for its move-
ments. Where should the connections between the
symbolic representation and the continuous repre-
sentation take place so that human language can
be used to direct the robot?s movements? Second,
when the robot learns new tasks from its human
partner, how to represent the acquired knowledge
effectively so that it can be applied in novel situa-
tions? Third, during human-robot dialogue, when
the robot fails to perform the expected actions due
to the lack of knowledge, how should the human
teach the robot new actions? through step-by-step
instructions or one-shot instructions?
With these questions in mind, we have devel-
oped a three-tier action knowledge representation
for the robotic arm. The lower level connects to
the physical arm and defines the trajectories of
executing three atomic actions supported by the
arm (i.e., open gripper, close gripper, move). The
middle level defines primitive operators such as
Open Grip, Close Grip and MoveTo in the fash-
ion of the traditional AI planner (Fikes and Nils-
son, 1971) and directly links to the lower level.
The upper-level captures the high-level actions ac-
quired by learning from the human. These high-
level actions are represented as the desired goal
states of the environment as a result of these ac-
tions. This three-tier representation allows the
robot to automatically come up with a sequence of
lower-level actions by applying existing planning
algorithms.
Based on this representation, we implemented
a dialogue system for action learning and further
conducted an empirical study with human sub-
jects. In particular, we compared the dialogue
89
Figure 1: An example setup and dialogue. Objects
are marked with labels only for the illustration pur-
pose.
based on the step-by-step instructions (i.e., one
step at a time and wait for the robot?s response
at each step before going to the next step) with
the one-shot instructions (i.e., give the instruction
with all steps at once). Our empirical results have
shown that the three-tier knowledge representation
can capture the learned new action and apply it
to novel situations. Although the step-by-step in-
structions resulted in a lengthier teaching process
compared to the one-shot instructions, they led to
better learning performance for the robot.
2 Related Work
Over forty years ago, Terry Winograd developed
SHRDLU (Winograd, 1972) to demonstrate nat-
ural language understanding using a simulated
block-moving arm. One aspect he did not address,
but mentioned in his thesis (Winograd, 1972) as
an important aspect, was learning new actions
through natural language. Motivated by Wino-
grad?s early work, we start our initial investigation
on action learning in a physical blocks world and
with a physical robotic arm. The blocks world is
the most famous domain used for planning in ar-
tificial intelligence. Thus it allows us to focus on
mechanisms that, on one hand, connect symbolic
representations of language with lower-level con-
tinuous sensorimotor representations of the robot;
and on the other hand, support the use of the plan-
ning algorithms to address novel situations.
Most previous work on following human in-
structions are based on supervised learning (Kol-
lar et al., 2010; Tellex et al., 2011; Chen et al.,
2010) or reinforcement learning (Branavan et al.,
2012; Branavan et al., 2010). These types of learn-
ing may not be adequate in time-critical situations
where only resources available to the robot is its
human partners. Thus it is desirable that humans
can engage in a natural language dialogue to teach
robots new skills. Using natural language dialogue
to learn new skills have been explored previously
by (Allen et al., 2007) where an artificial agent was
developed to acquire skills through natural lan-
guage instructions (i.e., find restaurant). But this
work only grounds language to symbolic interface
widgets on web pages.
In the robotics community, previous work has
applied learning by demonstration to teach robots
new skills (Cakmak et al., 2010). To potentially
allow natural language instructions, previous work
has also explored connecting language with lower-
level control systems (Kress-Gazit et al., 2008;
Siskind, 1999; Matuszek et al., 2012). Different
from these previous works, here we investigate the
use of natural language dialogue for learning ac-
tions. Previous work described in (Cantrell et al.,
2012; Mohan et al., 2013) is most similar to our
work. Here we focus on both grounded learning
and the use of planning for action learning.
3 Dialogue System
Figure 2: System Architecture
We developed a dialogue system to support
learning new actions. An example setup is shown
in Figure 1, in which a SCHUNK arm is used to
manipulate blocks placed on a surface. In H
1
,
the human starts to ask the robot to stack the blue
block (i.e., B
1
) on top of the red block (i.e., R
1
).
The robot does not understand the action ?stack?,
so it asks the human for instructions. Then the hu-
90
Figure 3: Example semantic representation and
action frame for the human utterance ?stack the
blue block on the red block on your right.?
man provides detailed steps to accomplish this ac-
tion (i.e., H
2
to H
8
) and also observes the robot?s
response in each step. Note that during this pro-
cess, another unknown action (i.e., ?grab? as in
H
2
) is encountered. The robot thus needs to learn
this action first. The robot is able to keep track
of the dialogue structure so that actions and sub-
actions can be learned accordingly. Once the robot
receives a confirmation from the human that the
corresponding action is successfully performed
(i.e., H
6
and H
9
), it acquires the new action and
explicitly represents it in its knowledge base for
future use. Instead of representing the acquired
knowledge as specific steps as illustrated by the
human, the acquired action is represented by the
expected final state, which represents the changes
of environment as a result of the action. The new
action can be directly applied to novel situations
by applying planning algorithms. Figure 2 shows
the system structure. Next we explain main system
modules in detail.
Natural Languge Processing: Natural language
processing modules capture semantic information
from human language inputs. In particular, the
Intention Recognizer is used to recognize
human intent (e.g., Command and Confirmation).
The Semantic Processor, implemented as
Combinatory Categorial Grammar (CCG)
1
, is
used to generate semantic representation. Current
semantic information includes the actions (e.g.,
stack) and their roles (e.g., Theme and Destina-
tion). The roles are further represented by objects?
properties (Color, Location and Spatial Relation).
An example semantic representation of ?H1: Stack
the blue block on the red block on your right.? is
shown in Figure 3.
1
We utilized OpenCCG, which could be found at:
http://openccg.sourceforge.net/
Perception Modules: Besides interpreting human
language, the robot also continuously perceives
the shared environment with its camera. Ob-
jects in video frames are recognized through vi-
sion system (Collet et al., 2011), and further repre-
sented as a Vision Graph (computed by Vision
Graph Builder), which captures objects and
their properties (in the numerical form). The robot
can also access to its own internal status, such as
the location of the gripper and whether it?s open
or closed. Combining the robot?s state and en-
vironment information, the Discrete State
Builder can represent the entire environment as
a conjunction of predicates, which will be later
used for action planning.
Referential Grounding: To make the semantic
representation meaningful, it must be grounded to
the robot?s representation of perception. We use
the graph-based approach for referential ground-
ing as described in (Liu et al., 2012)(Liu et al.,
2013). Once the references are grounded, the se-
mantic representation becomes a Grounded Action
Frame. For example, as shown in Figure 3, ?the
blue block? refers to B1 and ?the red block on your
right? refers to R1.
Dialogue Manager: The Dialogue Manager
is used to decide what dialog acts the system
should perform give a situation. It is composed by:
a representation of dialogue state, a space of sys-
tem activity and a dialogue policy. The dialogue
status is computed based on the human intention a
dialogue state captures (from semantic representa-
tion) and the Grounded Action Frame. The
current space of system activities includes asking
for instructions, confirming, executing actions and
updating its action knowledge base with new ac-
tions. The dialogue policy stores the (dialogue
state, system activities) pairs. During interaction,
the Dialogue Manager will first identify the
current dialogue state and then apply the dialogue
acts associated with that state as specified in the
dialogue policy.
Action Modules: The Action Modules are
used to realize a high-level action from the
Grounded Action Frame with the physi-
cal arm and to learn new actions. For re-
alizing high-level actions, if the action in the
Grounded Action Frame has a record in
the Action Knowledge, which keeps track
of all the knowledge about various actions, the
91
Discrete Planner will do planning to find a
sequence of primitive actions to achieve the high-
level action. Then these primitive actions will se-
quentially go through Continuous Planner
and be translated to the trajectories of arm motors.
By following these trajectories, the arm can per-
form the high-level action. For learning new ac-
tions, these modules will calculate state changes
before and after applying the action on the focus
object. Such changes of the state are generalized
and stored as knowledge representation of the new
action.
Response Generator: Currently, the Response
Generator is responsible for language genera-
tion to realize the detail sentence. In our current
investigation, the speech feedback is simple, so we
just used a set of pre-defined templates to do lan-
guage generation. And the parameters in the tem-
plates will be realized during run time.
4 Action Learning through Dialogue
To realize the action learning functionality we
have developed a set of action related processes
including an action knowledge base, action execu-
tion processes and action learning processes. Next
we give detailed explanations.
4.1 Action Modules
Figure 4: Execution example for ?Pick up the blue
block?.
As shown in Figure 4, the action knowledge
base is a three-level structure, which consists of
High-level action Knowledge, Discrete Planner
and Continuous Planner.
4.1.1 Continuous Planner
This lowest level planner defines three primitive
actions: open (i.e., open gripper), close (i.e., close
gripper) and move (i.e., move to the destination).
Each primitive action is defined as a trajectory
computing function, implemented as inverse kine-
matics. The outputs of these functions are control
commands sendt to each arm motor to keep the
arm following the trajectory.
4.1.2 Discrete Planner
The Discrete Planner is used to decompose a
high-level action into a sequence of primitive ac-
tions. In our system, it is implemented as a
STRIPS (Fikes and Nilsson, 1971) planner, which
is defined as a quadruple ?P,O, I,G?:
? P: Set of predicates describing a domain.
? O: Set of operators. Each is specified by a set
of preconditions and effects. An operator is
applicable only when its preconditions could
be entailed in a state.
? I: Initial state, the starting point of a problem.
? G: Goal state, which should be achieved if the
problem is solved.
In our system, O set includes Open Gripper,
Close Gripper and 8 different kinds of
MoveTo (She et al., 2014). And the P set
consists of two dimensions of the environment:
? Arm States: G Open/Close (i.e., whether the
gripper is open or closed), G Full/Empty
(i.e., whether the gripper has an object in it)
and G At(x) (i.e, location of the arm).
? Object States: Top Uclr/Clr(o) (i.e., whether
the block o has another block on its top),
In/Out G(o) (i.e., whether o is within the
gripper fingers or not) and On(o,x) (i.e., o is
supported by x).
The I and G are captured real-time during the
dialogue interaction.
4.1.3 High-level action Knowledge
The high-level actions represent actions specified
by the human partner. They are modeled as de-
sired goal states rather than the action sequence
taught by human. For example, the ?Stack(x,y)?
could be represented as ?On(x,y)?G Open?. If the
human specifies a high-level action out of the ac-
tion knowledge base, the dialogue manager will
verbally request for instructions to learn the action.
92
Figure 5: Learning process illustration. After hearing the stack action, the robot cannot perform. So the
human gives step by step instruction. When the instruction is completed, new knowledge of Grab(x) and
Stack(x,y) are learned in the high-level action knowledge base as the combination of the goal state of the
robotic arm and the changes of the state for the involved objects.
4.2 Action Execution
Given a Grounded Action Frame, it is
firstly checked with the high-level action knowl-
edge base. If the knowledge base has its record
(e.g., the Pickup and ClearTop in Figure 4.), a goal
state describing the action effect will be retrieved.
This goal state, together with the initial state cap-
tured from the current environment, will be sent
to the Discrete Planner. And, through au-
tomated planning, a sequence of primitive actions
will be generated to complete the task, which can
be immediately executed by the arm.
Take the ?Pick up? action frame in Figure 4
as an example. By checking the grounded ac-
tion frame with the high-level action knowledge,
a related goal state (i.e., ?G Close?Top Clr(B1)
?In G(B1)?On(B1,air)?) can be retrieved. At
the same time, the Discrete Evn Builder
translates the real world environment as a con-
junction of predicates, which serves as the ini-
tial state. Given the combination of initial state
and goal state, the STRIPS planner can search for
a path of primitive actions to solve the problem.
For example, the PickUp(B1) in Figure 4 can be
solved by Open Grip, MoveTo(B1), Close Grip
and MoveTo(air).
The primitive actions are executed by the con-
tinuous planner and control process in the lower
robotic system. For the ?open? and ?close?, they
are executed by controlling the position of the
gripper fingers. For the ?move?, a task-space tra-
jectory is first planned based on the minimum-time
motion planning algorithm to move the robot end-
effector from the current position to the final posi-
tion. A kinematic controller with redundancy res-
olution (Zhang et al., 2012) is then used to gener-
ate the joint movements for the robot to track the
planned trajectory. Achieving the end of the tra-
jectory indicates the action completion.
4.3 Action Learning
Figure 5 illustrates the system internal process of
acquiring action knowledge from the dialogue in
Figure 1.
At the beginning of the dialogue, the grounded
action frame Stack(B1, R1) captured from the first
human utterance is not in the action knowledge,
so it will be pushed to the top of the unknown ac-
tion stack as a new action waiting to be learned.
The environment state at this point is calculated as
shown in the figure. Then the robot will verbally
request instructions. During the instruction, it?s
possible that another unknown action Grab(B1) is
referred. The same as the Stack action, it will be
pushed to the top of unknown action stack waiting
to be learned.
In the next instruction, the human says ?Open
your gripper?. This sentence can be translated as
action frame Open and the goal state ?G Open?
can be retrieved from the action knowledge base.
After executing the action sequence, the grip-
per state will be changed from ?G Close? to
?G Open?, as shown in Figure 5. In the follow-
ing two instructions, the human says ?Move to the
blue block? and ?Close gripper?. Similarly, these
two instructions are translated as action frames
Move(B1) and Close, then are executed accord-
93
ingly. After executing these two steps, the state of
B1 is changed from ?Out G(B1)? to ?In G(B1)?.
At this point, the previous unknown action
Grab(B1) is achieved, so the human says ?Now
you achieve the grab action? as a signal of teach-
ing completion. After acknowledging the teach-
ing completion, the action learning module will
learn the new action representation by combining
the arm state with the state changes of the argu-
ment objects in the unknown action frame. For
example, the argument object of unknown action
Grab(B1) is B1. By comparing the original state
of B1, [(Out G B1)?(Top Clr B1)?(On B1 table)]
with the final state, [(In G B1)?(Top Clr B1)?(On
B1 table)], B1 is changed from (Out G B1) to
(In G B1). So, the learning module will gener-
alize such state changes and acquire the knowl-
edge representation of the new action Grab(x) as
G Close?In G(x).
5 Empirical Studies
The objectives of our empirical studies are two
folds. First, we aim to exam whether the current
representation can support planning algorithms
and execute the learned actions in novel situations.
Second, we aim to evaluate how extra effort from
the human partner through step-by-step instruc-
tions may affect the robot?s learning performance.
5.1 Instruction Effort
Previous work on mediating perceptual differ-
ences between humans and robots have shown that
a high collaborative effort from the robot leads to
better referential grounding (Chai et al., 2014).
Motivated by this previous work, we are inter-
ested in examining how different levels of effort
from human partners may affect the robot?s learn-
ing performance. More specifically, we model two
levels of variations:
? Collaborative Interaction: In this setting, a
human partner provides step-by-step instruc-
tions. At each step, the human will observe
the the robot?s response (i.e., arm movement)
before moving to the next step. For exam-
ple, to teach ?stack?, the human would is-
sue ?pick up the blue block?, observe the
robot?s movement, then issue ?put it on the
red block? and observe the robot movement.
By this fashion, the human makes extra effort
to make sure the robot follows every step cor-
rectly before moving on. The human partner
can detect potential problems and respond to
immediate feedback from the robot.
? Non-Collaborative Interaction: In this set-
ting, the human only provides a one-shot in-
struction. For example, to teach ?stack?,
the human first issues a complete instruction
?pick up the blue block and put it on top of
the red block? and then observes the robot?s
responses. Compared to the collaborative set-
ting, the non-collaborative setting is poten-
tially more efficient.
5.2 Experimental Tasks
Similar to the setup shown in Figure 1, in the
study, we have multiple blocks with different col-
ors and sizes placed on a flat surface, with a
SCHUNK arm positioned on one side of the sur-
face and the human subject seated on the opposite
side. The video stream of the environment is sent
to the vision system (Collet et al., 2011). With the
pre-trained object model of each block, the vision
system could capture blocks? 3D positions from
each frame. Five human subjects participated in
our experiments
2
. During the study, each sub-
ject was informed about the basic actions the robot
can perform (i.e., open gripper, close gripper, and
move to) and was instructed to teach the robot sev-
eral new actions through dialogue. Each subject
would go through the following two phases:
5.2.1 Teaching/Learning Phase
Each subject was asked to teach the following five
new actions under the two strategies (i.e., step-
by-step instructions vs. one-shot instructions):
{Pickup, Grab, Drop, ClearTop, Stack} Each time,
the subject can choose any blocks they think are
useful to teach the action. After finishing teaching
one action (either under step-by-step instructions
or under one-shot instructions), we would survey
the subject whether he/she thinks the teaching is
completed and the corresponding action is suc-
cessfully performed by the robot. We record the
teaching duration and then re-arrange the table top
setting to move to the next action.
For the teaching/learning phase, we use two
metrics for evaluation: 1) Teaching Completion
Rate(R
t
) which stands for the number of actions
successfully taught and performed by the robot;
2)Teaching Completion Duration (D
t
which mea-
sures the amount of time taken to teach an action.
2
More human subjects will be recruited to participate in
our studies.
94
5.2.2 Execution Phase
The goal of learning is to be able to apply the
learned knowledge in novel situations. To evalu-
ate such capability, for each action, we designed
10 additional setups of the environment which
are different from the environment where the ac-
tion was learned. For example, as illustrated in
Figure 6, the human teaches the pick Up action
by instructing the robot how to perform ?pick up
the blue block(i.e., B1)? under the environment
in 6(a). Once the knowledge is acquired about the
action ?pick up?, we will test the acquired knowl-
edge in a novel situation by instructing the robot to
execute ?pick up the green block(i.e., G1)? in the
environment shown in 6(b).
(a) Learning: the human
teaches the robot how to
?pick up the blue block
(i.e., B1)? during the learn-
ing phase
(b) Execution: the human
asks the robot to ?pick up
the green block (i.e., G1)?
after the robot acquires the
knowledge about ?pick up?
Figure 6: Examples of a learning and an execution
setup.
For the execution phase, we also used
two factors to evaluate: 1) Action Sequence
Generation(R
g
) which measures how many high-
level actions among the 10 execution scenarios
where the corresponding lower-level action se-
quences are correctly generated; 2) Action Se-
quence Execution(R
ge
) which measures the num-
ber of high level actions that are correctly executed
based on the lower level action sequences.
5.3 Empirical Results
Our experiments resulted in a total of 50 action
teaching dialogues. Half of these are under the
step-by-step instructions (i.e., collaborative inter-
action) and half are under one-shot instructions
(i.e., non-collaborative). As shown in Figure 7,
5 out of the 50 teaching dialogues were consid-
ered as incomplete by the human subjects and all
of them are from the Non-Collaborative setting.
For each of the 45 successful dialogues, an action
would be learned and acquired. For each of these
acquired actions, we further tested its execution
under 10 different setups.
Figure 7: The teaching completion result of the
50 teaching dialogues. ?1? stands for the dialogue
where the subject considers the teaching/learning
as complete since the robot performs the corre-
sponding action correctly; and ?0? indicates a fail-
ure in learning. The total numbers of teaching
completion are listed in the bottom row.
Figure 8: The teaching completion duration re-
sults. The durations under the non-collaborative
strategy are smaller than the collaborative strategy
in most cases.
5.3.1 Teaching Performance
The result of teaching completion is shown in Fig-
ure 7. Each subject contributes two columns: the
?Non? stands for the Non-Collaborative strategy
and the ?Col? column refers to the Collaborative
strategy. As the table shows, all the 5 uncom-
pleted teaching are from the Non-Collaborative
strategy. In most of these 5 cases, the subjects
thought the actual performed actions were differ-
ent from their expectations. For example, in one of
the ?stack? failures, the human one-shot instruc-
tion was ?move the blue block to the red block on
the left.?. She thought the arm would put the blue
block on the top of red block, open gripper and
then move away. However, based on the robot?s
knowledge, it just moved the blue block above
the red block and stopped there. So the subject
considered this teaching as incomplete. On the
other hand, in the Collaborative interactions, the
robot?s actual actions could also be different from
the subject?s expectation. But, as the instruction
95
Figure 9: Each bar represents the number of suc-
cessfully generated action sequences during test-
ing. The solid portion of each bar represents the
number of successfully executed action sequences.
The number of successfully execution is always
smaller than or equal to the generation. This is be-
cause we are dealing with dynamic environment,
and the inaccurate real-time localization will make
some correct action sequence fail to be executed.
was given step-by-step, the instructors could no-
tice the difference from the immediate feedback
and adjust their follow-up steps, which contributed
to a higher completion rate.
The duration of each teaching task is shown in
Figure 8. Bar heights represent average teaching
duration, the ranges stand for standard error of
the mean (SEM). The 5 actions are represented
by different groups. As shown in the figure, the
teaching duration under the Collaborative strategy
tends to take more time. Because in the Collab-
orative case, the human needs to plan next step
after observing the robot?s response to a previous
step. If an exception happens, a sub-dialogue is
often arranged to do correction. But in the Non-
Collaborative case, the human comes up with an
entire instruction at the beginning, which appears
more efficient.
5.3.2 Execution Performance
Figure 9 illustrates the action sequence generation
and execution results in the execution phase.
As shown in Figure 9, testing results of actions
learned under the Collaborative strategy are higher
than the ones using Non-Collaborative, this is be-
cause teaching under the Collaborative strategy is
more likely to be successful. One exception is the
Clear Top action, which has lower generation rate
under the Col setting. By examining the collected
data, we noticed that our system failed to learn the
knowledge of Clear Top in one of the 5 teaching
phases using Col setting, although the human sub-
ject labeled it as successful. Another phenomenon
shown in Figure 9 is that the generation results are
always larger than or equal with the correspond-
ing execution results. This is caused by inaccurate
localization and camera calibration, which intro-
duced exceptions during executing the action se-
quence.
6 Conclusion
This paper describes an approach to robot action
learning in a simplified blocks world. The sim-
plifications of the environment and the tasks allow
us to explore connections between symbolic repre-
sentations of natural language and continuous sen-
sorimotor representations of the robot which can
support automated planning for novel situations.
This investigation is only our first step. Many is-
sues have not been addressed. For example, the
world is full of uncertainties. Our current ap-
proach can only either succeed or fail executing
an action based on the acquired knowledge. There
is no approximation or reasoning of the uncertain
states which may affect potential execution. Also,
when the robot fails to execute an action, there is
no explanation why it fails. If the robot can artic-
ulate its internal representations regarding where
the problem occurs, the human can provide better
help or targeted teaching. These are the directions
we will pursue in our future work.
7 Acknowledgment
This work was supported by IIS-1208390 from the
National Science Foundation and N00014-11-1-
0410 from the Office of Naval Research.
References
James F. Allen, Nathanael Chambers, George Fergu-
son, Lucian Galescu, Hyuckchul Jung, Mary D.
Swift, and William Taysom. 2007. Plow: A collab-
orative task learning agent. In AAAI, pages 1514?
1519. AAAI Press.
S. R. K. Branavan, Luke S. Zettlemoyer, and Regina
Barzilay. 2010. Reading between the lines: Learn-
ing to map high-level instructions to commands. In
Proceedings of the 48th Annual Meeting of the As-
sociation for Computational Linguistics, ACL ?10,
pages 1268?1277, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
S.R.K. Branavan, Nate Kushman, Tao Lei, and Regina
Barzilay. 2012. Learning high-level planning from
96
text. In Proceedings of the 50th Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers), pages 126?135, Jeju Island,
Korea, July. Association for Computational Linguis-
tics.
Maya Cakmak, Crystal Chao, and Andrea Lockerd
Thomaz. 2010. Designing interactions for robot ac-
tive learners. IEEE T. Autonomous Mental Develop-
ment, 2(2):108?118.
R. Cantrell, K. Talamadupula, P. Schermerhorn, J. Ben-
ton, S. Kambhampati, and M. Scheutz. 2012. Tell
me when and why to do it! run-time planner model
updates via natural language instruction. In Human-
Robot Interaction (HRI), 2012 7th ACM/IEEE Inter-
national Conference on, pages 471?478, March.
Joyce Y. Chai, Lanbo She, Rui Fang, Spencer Ottarson,
Cody Littley, Changsong Liu, and Kenneth Han-
son. 2014. Collaborative effort towards common
ground in situated human robot dialogue. In Pro-
ceedings of 9th ACM/IEEE International Confer-
ence on Human-Robot Interaction, Bielefeld, Ger-
many.
David L. Chen, Joohyun Kim, and Raymond J.
Mooney. 2010. Training a multilingual sportscaster:
Using perceptual context to learn language. J. Artif.
Int. Res., 37(1):397?436, January.
H. I. Christensen, G. M. Kruijff, and J. Wyatt, editors.
2010. Cognitive Systems. Springer.
Alvaro Collet, Manuel Martinez, and Siddhartha S.
Srinivasa. 2011. The MOPED framework: Object
Recognition and Pose Estimation for Manipulation.
Richard E. Fikes and Nils J. Nilsson. 1971. Strips: A
new approach to the application of theorem proving
to problem solving. In Proceedings of the 2Nd Inter-
national Joint Conference on Artificial Intelligence,
IJCAI?71, pages 608?620, San Francisco, CA, USA.
Morgan Kaufmann Publishers Inc.
Thomas Kollar, Stefanie Tellex, Deb Roy, and Nicholas
Roy. 2010. Toward understanding natural language
directions. In Proceedings of the 5th ACM/IEEE
International Conference on Human-robot Interac-
tion, HRI ?10, pages 259?266, Piscataway, NJ, USA.
IEEE Press.
Hadas Kress-Gazit, Georgios E. Fainekos, and
George J. Pappas. 2008. Translating structured
english to robot controllers. Advanced Robotics,
22(12):1343?1359.
Changsong Liu, Rui Fang, and Joyce Chai. 2012. To-
wards mediating shared perceptual basis in situated
dialogue. In Proceedings of the 13th Annual Meet-
ing of the Special Interest Group on Discourse and
Dialogue, pages 140?149, Seoul, South Korea.
Changsong Liu, Rui Fang, Lanbo She, and Joyce Chai.
2013. Modeling collaborative referring for situated
referential grounding. In Proceedings of the SIG-
DIAL 2013 Conference, pages 78?86, Metz, France.
Cynthia Matuszek, Evan Herbst, Luke S. Zettlemoyer,
and Dieter Fox. 2012. Learning to parse nat-
ural language commands to a robot control sys-
tem. In Jaydev P. Desai, Gregory Dudek, Ous-
sama Khatib, and Vijay Kumar, editors, ISER, vol-
ume 88 of Springer Tracts in Advanced Robotics,
pages 403?415. Springer.
Shiwali Mohan, James Kirk, and John Laird. 2013. A
computational model for situated task learning with
interactive instruction. In Proceedings of ICCM
2013 - 12th International Conference on Cognitive
Modeling.
Lanbo She, Yu Cheng, Joyce Chai, Yunyi Jia, Shaohua
Yang, and Ning Xi. 2014. Teaching robots new ac-
tions through natural language instructions. In RO-
MAN.
Jeffrey Mark Siskind. 1999. Grounding the lexical se-
mantics of verbs in visual perception using force dy-
namics and event logic. J. Artif. Int. Res., 15(1):31?
90, February.
Stefanie Tellex, Thomas Kollar, Steven Dickerson,
Matthew R. Walter, Ashis Gopal Banerjee, Seth J.
Teller, and Nicholas Roy. 2011. Understanding nat-
ural language commands for robotic navigation and
mobile manipulation. In Wolfram Burgard and Dan
Roth, editors, AAAI. AAAI Press.
T. Winograd. 1972. Procedures as a representation for
data in a computer program for understanding natu-
ral language. Cognitive Psychology, 3(1):1?191.
Huatao Zhang, Yunyi Jia, and Ning Xi. 2012. Sensor-
based redundancy resolution for a nonholonomic
mobile manipulator. In IROS, pages 5327?5332.
97
