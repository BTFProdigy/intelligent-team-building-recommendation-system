Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 296?305, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational Linguistics
Choosing the Right Words:
Characterizing and Reducing Error of the Word Count Approach
H. Andrew Schwartz,1 Johannes Eichstaedt,1 Lukasz Dziurzynski,1 Eduardo Blanco,2
Margaret L. Kern,1 Stephanie Ramones,1 Martin Seligman,1 and Lyle Ungar1
1University of Pennsylvania
2Lymba Corporation
hansens@seas.upenn.edu
Abstract
Social scientists are increasingly using the
vast amount of text available on social me-
dia to measure variation in happiness and
other psychological states. Such studies count
words deemed to be indicators of happiness
and track how the word frequencies change
across locations or time. This word count ap-
proach is simple and scalable, yet often picks
up false signals, as words can appear in differ-
ent contexts and take on different meanings.
We characterize the types of errors that occur
using the word count approach, and find lex-
ical ambiguity to be the most prevalent. We
then show that one can reduce error with a
simple refinement to such lexica by automat-
ically eliminating highly ambiguous words.
The resulting refined lexica improve precision
as measured by human judgments of word oc-
currences in Facebook posts.
1 Introduction
Massive social media corpora, such as blogs, tweets,
and Facebook statuses have recently peaked the in-
terest of social scientists. Compared to traditional
samples in tens or hundreds, social media sample
sizes are orders of magnitude larger, often contain-
ing millions or billions of posts or queries. Such text
provides potential for unobtrusive, inexpensive, and
real-time measurement of psychological states (such
as positive or negative affect) and aspects of sub-
jective well-being (such as happiness and engage-
ment). Social scientists have recently begun to use
social media text in a variety of studies (Cohn et
al., 2004; Kramer, 2010; Tausczik and Pennebaker,
2010; Kamvar and Harris, 2011; Dodds et al, 2011;
Golder and Macy, 2011).
One of the most popular approaches to estimate
psychological states is by using the word count
method (Pennebaker et al, 2007), where one tracks
the frequency of words that have been judged to be
associated with a given state. Greater use of such
words is taken to index the prevalence of the cor-
responding state. For example, the use of the word
?happy? is taken to index positive emotion, and ?an-
gry? to index negative emotion. The most widely
used tool to carry out such analysis, and the one we
investigate in this paper, is Pennebaker?s Linguistic
Inquiry and Word Count, (LIWC) (Pennebaker et al,
2001; Pennebaker et al, 2007). LIWC, originally de-
veloped to analyze writing samples for emotion and
control, has grown to include a variety of lexica for
linguistic and psychosocial topics including positive
and negative emotions, pronouns, money, work, and
religion. The word count approach has high appeal
to social scientists in need of a tool to approach so-
cial media, and although others have been used (see,
for example (Gottschalk and Bechtel, 1998; Bollen
et al, 2010), LIWC?s lexica are generally perceived
as a ?tried-and-tested? list of words (Miller, 2011).
Unfortunately, the word count approach has some
drawbacks when used as indicators for psycholog-
ical states. Words are the unit of measurement, but
words can carry many different meanings depending
on context. Consider the Facebook posts below con-
taining instances of ?play?, a word associated with
positive emotion in LIWC.
296
1. so everyone should come to the play tomor-
row...
2. Does anyone what type of file i need to convert
youtube videos to play on PS3???
3. Time to go play with Chalk from the Easter
Bunny!
Out of the three instances, only (3) seems to com-
municate positive emotion. In (1), ?play? is used as
a noun rather than the expected verb, while in (2),
?play? is a verb but it is used in a sense that is not
directly associated with positive emotion. (1) and
(2) demonstrate how lexical ambiguities (i.e. multi-
ple parts-of-speech or word senses) can affect accu-
racy of words in a lexicon. Additionally, even when
appearing as the expected part of speech and word
sense, signal from a word may change due to its con-
text, such as being within the scope of a negation as
in (4), or describing something desired as in (5).
4. ...all work no play :-(
5. i sure wish i had about 50 hours a day to play
cod
Our goal is to characterize the errors of the widely
used word count approach, and show that such lex-
ica can be significantly improved by employing an
ambiguity metric to refine such lexica. Rather than
work on a new method of measuring psychological
states, we work within the bounds of word count and
ask how accurate it is and whether we can improve
it without sacrificing its simplicity and scalability.
We attempt to reduce the erroneous signal of
the word count approach while maintaining legiti-
mate signal simply by refining the lexicon. In other
words, we would like to move closer to the goal in
Figure 1, by eliminating words that often carry er-
roneous signal such as ?play?, and keeping words
which often carry the sought-after signal, such as
?cheerful?. The difficulty in doing this is that we do
not have the data to tell us which words are most
likely to carry signal (even if we had such data we
would like to develop a method that could be applied
to any newly created lexica). Instead we leverage
part-of-speech and word sense data to help us deter-
mine which words are lexically ambiguous.
Figure 1: The relationship between text expressing posi-
tive emotion (POSEMO) and text containing LIWC terms
for POSEMO.
Our approach of eliminating ambiguous words
increases the precision at the expense of recall, a
reasonable trade-off in social media where we are
working with millions or even billions of word in-
stances. Additionally, it is minimally-supervised, in
that we do not require training data on human-state;
instead we use existing hand-labeled corpora, such
as SemCor (Miller et al, 1993), for word sense in-
formation. Not requiring training data also means
our refinement is flexible; it can be applied to mul-
tiple domains and lexica, it makes few assumptions
that might introduce problems of over-fitting, and it
is parsimonious in that it merely improves an estab-
lished approach.
This paper makes two primary contributions: (1)
an analysis of the types of errors common for the
word count approach (Section 3), and (2) a general
method for refining psychosocial lexica based on the
ambiguity of words (Section 4). Before describing
these contributions, we discuss related work, mak-
ing the case for using social media in social science
and surveying some work in computational linguis-
tics. We then evaluate both the original LIWC lex-
icon and our refinement of it against human judg-
ments of expression of positive and negative emo-
tions on hand-annotated Facebook posts, and show
the benefit of lexicon refinement for estimating well-
being over time for large aggregates of posts. Fi-
nally, we discuss the implications of our work and
possible future directions.
297
2 Background
Compared to traditional approaches in the social sci-
ences, large scale analysis of social media is cheap,
near real-time, unobtrusive, and gives high cover-
age. We outline these advantages below.
Inexpensive Extracting information from sources
such Facebook and Twitter is vastly cheaper than the
more conventional polling done by companies such
as Gallup ? and by many social science researchers.
Social media data does not require phone calls to be
made or doors to be knocked on. For example, a rep-
resentative survey asking 1,000 people by a leading
polling company costs to the order of $10,0001. In
contrast, once the software exists, social media data
from tens of millions of users can be obtained and
analyzed at a fraction of the cost.
Temporal Resolution Much of the attraction of
social media stems from the fact that it captures
a written live stream of collective thought. When
Google relied on search queries to monitor health-
seeking behavior to predict influenza epidemics, the
reporting lag was a mere day, whereas traditional
CDC surveillance systems take 1-2 weeks to pub-
lish their data (Ginsberg et al, 2009). Infrastructure
based on social media and Internet use data allows
reporting and analysis systems with little to no re-
porting lag. Additionally, traditional survey designs
are typically only designed to assess psychological
states at a given point in time.
Unobtrusive Estimation Traditional self-report
survey approaches, even those implemented on the
web, suffer from social desirability, priming, and
other biases. For example, Kahneman et al (Kah-
neman et al, 2006) found that the order in which
questions are asked on questionnaires can determine
how they are answered. By looking directly into the
social worlds, many of these self-report biases can
be avoided. The traces of human interactions in so-
cial media represent the goings-on in their original
ecologies of meaning and signification. This ap-
proach diminishes the inferential distance between
the context of the phenomena and the context of
measurement ? and thus decreases the room for sys-
tematic distortion of signal.
1Gallup, Personal correspondence.
2.1 The Word Count Approach
As previously noted, the word count approach is
most often used by social scientists through the tool
known as Linguistic Inquiry and Word Count or
LIWC (Pennebaker et al, 2007). The LIWC2007
dictionary is composed of almost 4,500 words and
word stems organized across one or more word cat-
egories, including 406 positive emotion words and
499 negative emotion words. When long form texts
are analyzed with LIWC, the program simply re-
turns the percentages of words belonging to the dif-
ferent analytical categories ? the simplicity of this
approach makes it popular with non-technical social
scientists.
LIWC?s positive and negative emotion lexica have
recently begun to be used on ?short form? writing in
social media. For example, Golder and Macy (2011)
used LIWC to study diurnal and seasonal variation
in mood in a collection of 400 million Twitter mes-
sages. Kramer (2010) proposed the ?Gross National
Happiness? index and Kivran-Swaine and Naaman
(2011) examined associations between user expres-
sions of positive and negative emotions and the size
and density of social networks. A comprehensive
review can be found in Tausczik and Pennebaker
(2010).
To our knowledge there is only one work which
has evaluated LIWC?s accuracy over social media.
Bantum and Owen (2009) evaluated LIWC on a set
of posts to an Internet-based breast cancer support
group. By annotating expression of emotion within
this text, they were able to produce accuracy figures
of sensitivity (much like recall) and predictive va-
lidity (precision). Sensitivity measured how often
a word (in context) expressing positive or negative
emotion was captured by LIWC. Predictive validity
measured how often a word (in context) captured
by LIWC as measuring positive or negative emotion
was indeed expressing positive or negative emotion.
While they found a recall of 0.88, the precision was
only 0.31 ? that is, only 31% of instances contain-
ing words indexed by LIWC actually conveyed the
associated emotion. We contend that this is a major
drawback for applying LIWC to social media, be-
cause while it is not important to catch every expres-
sion of emotion out of a million Tweets, it is impor-
tant that when something is captured it is an accurate
298
estimate of the true state.
2.2 Related Work in Computational
Linguistics
Researchers have been exploring the use of lexica
that define the subjective orientation of words for
tasks such as sentiment or subjectivity analysis. A
common weakly-supervised approach starts with a
small set of sentiment knowledge (seed words as-
sociated with a given sentiment) and expands the
words into a large lexicon (Hatzivassiloglou and
McKeown, 1997; Kamps and Marx, 2002; Kim and
Hovy, 2004; Kanayama and Nasukawa, 2006; Bac-
cianella et al, 2010). We take a different approach.
Rather than expanding lexica, we start with a large
set of words and refine the set. The refinement in-
creases precision at the cost of recall, which is a
reasonable exchange when we are looking at mil-
lions or even billions of word instances. Standard
applications of sentiment analysis, such as annotat-
ing movie reviews, may not be as inclined to skip
instances, since they want to make predictions for
items which have very few reviews.
Another line of work in sentiment analysis has
created lexicons using supervised learning. One of
the first works to do so was by Pang and colleagues
(2002), who used data including author ratings of
reviews, such as IMDB movie reviews. The author
ratings become training data for sentiment classifi-
cation. Pang et al showed that human-created lexi-
cons did not perform as well as lexicons based on
simple word statistics over the training data. In-
terestingly, they found that words like ?still? were
most predictive of positive movie reviews, and that
punctuation marks of ?!? and ??? were strong signs
of negative movie reviews. Unfortunately, training
data for subjective well-being or happiness is not
yet available, preventing the use of such supervised
learning methods. Additionally, this work seeks to
experiment within the bounds of what social sci-
entists are in fact using (with publications in high-
impact venues such as Science). We thus take a dif-
ferent approach, and automatically improve human
created lexicons.
Wiebe and Cardie (2005) generalized the task of
sentiment analysis to that of discovering subjectiv-
ity such as ?opinions, emotions, sentiments, specu-
lations, evaluations, etc.?. More recently, Wilson et
POSEMO NEGEMO
term frequency term frequency
like 774,663 hate 167,109
love 797,833 miss 158,274
good 571,687 bad 151,496
friend* 406,568 bore* 140,684
happy 384,797 shit* 114,923
LOL 370,613 hurt* 98,291
well* 284,002 craz* 94,518
great 263,113 lost 94,059
haha* 240,587 damn* 93,666
best 227,381 fuck 90,212
better 212,547 stupid* 85,587
fun 216,432 kill* 83,593
please* 174,597 hell 80,046
hope 170,998 fuckin* 79,959
thank 161,827 wrong* 70,714
Table 1: Most frequent POSEMO and NEGEMO terms in
LIWC in the 12.7 million Facebook posts. ?*? indicates a
wildcard, so that ?well*? matches ?wellness?.
al. (2009) contended that the context may neutralize
or change the polarity of the subjective orientation
of a word. It is difficult to determine where concepts
of happiness such as quality of relationships or de-
gree of achievement in life fit in with subjectivity.
Thus, we do not claim to be measuring subjectivity
and instead we use the general term of ?psychologi-
cal state?, referring to ?the way something [a person]
is with respect to its main attributes? (Miller, 1993).
To the best of our knowledge, while part-of-
speech tagging and word sense disambiguation are
staple tasks in the computational linguistics commu-
nity, the utility of a lexical ambiguity metric has yet
to be explored.
3 Annotation and Analysis of Errors from
the Word Count Method
One objective of our work is to document and de-
scribe how often different types of errors occur when
using the word count approach on social media. To
do this, we first judged a sample of 1,000 instances
of LIWC terms occurring in Facebook posts to indi-
cate whether they contribute signal towards the as-
sociated LIWC category (i.e. positive emotion). We
then took instances that were deemed to carry erro-
neous signal and annotated them with a label for the
299
category agreement instances base rate
POSEMO 0.742 500 .654
NEGEMO 0.746 500 .697
TOTAL 0.744 1,000 .676
random 0.343 - -
Table 2: Inter-annotator agreement over 1,000 instances
of LIWC terms in Facebook posts. Base rate is the aver-
age of how often an annotator answered true.
type of signal error. This section describes the pro-
cess we used in generating these annotations and the
results we found.
3.1 Annotation Process
Annotating social media instances of lexica terms
provides insight into how well the word count ap-
proach works, and also yields a ?ground truth? for
evaluating our lexicon refinement methods. We ran-
domly selected for labeling a sample of 1,000 sta-
tus updates containing words from a given lexicon
drawn from a collection of 12.7 million Facebook
status updates provided by the Cambridge myPer-
sonality project (Kosinski and Stillwell, 2012).
We used terms from the LIWC positive emotion
(POSEMO) and negative emotion (NEGEMO) lex-
ica, which are the same lexica used by the works of
Kramer (2010), Kivran-Swaine and Naaman (2011),
and Golder and Macy (2011). Table 1 lists the
most frequent POSEMO and NEGEMO terms in our
Facebook sample.
As mentioned above, we did two types of annota-
tions. First, we judged whether each given instance
of a word conveyed the correct associated type of
emotion. The second task took a sample of instances
judged to have incorrect signal and labeled them
with a reason for the error; We refer to this as signal
error type.
For the first task, we had three human judges inde-
pendently evaluate the 1,000 status update instances
as to whether they were indeed correct signal. The
question the judges were told to answer was ?Does
the word contribute to the associated psychological-
state (POSEMO or NEGEMO) within the sentence
it appears??. In other words, ?would the sentence
convey less [positive emotion or negative emotion]
without this word??. Subjective feedback from the
judges indicated that it was often difficult to make
a decision, so we used three judges per instance. In
the case of conflict between judges, the ?correct? la-
bel for validation of the refined lexicon was defined
to be the majority vote. A sampling of Facebook sta-
tuses demonstrates a mixed picture of relevance for
the unrefined LIWC dictionaries:
1. has had a very good day (?good? - POSEMO)
2. is so very bored. (?bore*? - NEGEMO)
3. damn, that octopus is good, lol (?damn? -
NEGEMO)
4. thank you for his number (?numb*? -
NEGEMO)
5. I got pranked sooooo bad (?bad? - NEGEMO)
6. don?t be afraid to fail (?afraid? - NEGEMO)
7. I wish I could . . . and we could all just be happy
(?happy? - POSEMO)
Some posts clearly use positive or negative lexicon
words such as (1) and (2). Curse words can signify
negative emotion or emphasize the opposite state as
in (3), which is clearly emphasizing positive emo-
tion here. Example (5) demonstrates the word sense
issue we discussed previously. Words with wild-
cards that expand into other words with different
meanings can be particularly problematic, as the ex-
panded word can be far more frequent ? and very
different in meaning ? from the original word. For
example, ?numb*? matches ?number? in 4.
A different problem occurs when the context
of the word changes its implication for the emo-
tional state of the writer. This can either occur
through negation such as in (6) where ?afraid? sig-
nals NEGEMO, but is negated with ?don?t? or the
signal can be changed indirectly through a variety of
words indicating that the writer desires (and hence
lacks) the state, as in (7) where someone is wishing
to be ?happy?.
Table 2 shows the agreement between an-
notators calculated as
?
i agree(A
(i)
1 ,A
(i)
2 ,A
(i)
3 )
1,000 , where
agree(A1, A2, A3) was 1 when all three annota-
tions matched and 0 otherwise. Given the aver-
age positive base rate across annotators was 0.676
the chance that all three reviewers agree accord-
ing to chance (random agreement) is calculated as
300
category precision instances
POSEMO 67.9% 500
NEGEMO 72.8% 500
both 70.4% 1,000
Table 4: Accuracy of LIWC POSEMO and NEGEMO
lexica over Facebook posts.
0.6763+(1?0.676)3 = 0.343, the probability of all
three answering yes plus the probability of all three
answering no.
For the second task, we selected 100 instances
judged to be incorrect signal from the first task, and
labeled them according to the best reason for the
mistake. This task required more linguistic exper-
tise and was performed by a single annotator. La-
bels and descriptions are given in Table 3, which
breaks down the cases into lexical ambiguity, direct
or indirect negation, and other reasons such as the
stemming issue (stem plus wildcard expanding into
words indicating a different (or no) emotional state).
3.2 Analysis of Errors
Before discussing the types of errors we found when
using the word count approach, we examine LIWC?s
overall accuracy on our dataset. Table 4 shows the
precision broken down for both the positive emotion
(POSEMO) and the negative emotion (NEGEMO)
lexica. We see that the precision for NEGEMO is
slightly higher than POSEMO, indicating the terms
in that category may be more likely to indicate their
associated state.
Although the overall accuracy seems decent, one
should keep in mind our subjective judgement crite-
ria were quite tolerant, allowing any amount of con-
tribution of the corresponding signal to be consid-
ered accurate. For example, a salutation like ?Happy
New Year? was judged to be a correct use of ?happy?
to signal POSEMO, even though it clearly does not
have as strong a signal as someone saying ?I feel
deliriously happy?.
Frequencies of signal errors are given in Table
5. The most common signal error was wrong word
sense, where the word did not signal emotional
state and some other sense or definition of the word
was intended (e.g. ?u feel like ur living in a mu-
sic video?; corresponding to the sense ?to inhabit?
rather than the intended sense, ?to have life; be
category label frequency
Lexical Ambiguity
Wrong POS 15
Wrong WS 38
Signal Negation
Strict Negation 16
Desiring 6
Other
Stem Issue 5
Other 24
Table 5: Frequency of the signal error types.
alive? (Miller, 1993)). Other common signal errors
include strict negation where the word is canceled
out by a clear negative quantifier (e.g. ?Don?t be
afraid to fail?) and wrong part of speech where the
word is signaling a different part of speech than the
emotion (e.g. ?well, we cant afford to go to NYC?).
There were also various other signal error types that
include stem issues where the stem matched clearly
unintended words, desiring statuses where the status
is commenting on wanting the emotion instead of
experiencing it and other less prevalent issues such
as non-English language post, memes, or clear sar-
casm.
4 Method for Refining Lexica
The idea behind our refinement method is to remove
words that are likely to carry erroneous signal about
the underlying state or emotion of the person writ-
ing the tweet or Facebook post.2 We do so in an
indirect fashion, without actually using training data
of which posts are, in fact indicative of positive or
negative emotion. Instead, we focus on reducing er-
rors that are due to lexical ambiguity. By remov-
ing words that are often used with multiple parts of
speech or multiple senses, we can tilt the balance to-
ward precision at some cost in recall (losing some
signal from the ambiguous words). This makes the
word count approach more suitable for use in the
massive corpora afforded by social media.
4.1 Lexical Ambiguity
We address lexical ambiguity at the levels of both
part of speech (POS) and word sense. As a metric
of inverse-ambiguity, we determine the probability
that a random instance is the most frequent sense
(mfs) of the most frequent part of speech (mfp) of
2Refinement tool is available at wwbp.org.
301
category label description examples
Lexical Ambiguity
Wrong POS Not a valid signal because it is
the wrong POS
so everyone should come to the
play tomorrow...
Wrong WS Not a valid signal because it is
the wrong word sense (includes
metaphorical senses)
Does anyone what type of file i
need to convert youtube videos
to play on PS3???
Signal Negation
Strict Negation Within the scope of a negation,
where there is a clear negative
quantifier
...all work no play :-(
Desiring Within the scope of a desire /
wishing for something
i sure wish i had about 50 hours
a day to play cod
Other
Stem Issue Clearly not intended to be
matched with the given stem
numb* for NEGEMO match-
ing number
Other Any other issue or difficult to
classify
Table 3: Signal error types.
the word, denoted TSP (for top sense probability).
Given a wordw, we consider all parts of speech ofw
(POS(w)) and all senses for the most frequent part
of speech (senses(mfp(w))):
pmfp(w) =
max
[wpos?POS(w)]
fp(wpos)
?
wpos?POS(w)
fp(wpos)
pmfs(w) =
max
[wsense?senses(mfp(w))]
fs(wsense)
?
wsense?senses(mfp(w))
fs(wsense)
TSP (w) = (pmfp(w) ? pmfs(w))
2 (1)
Here, fp and fs represent the frequencies of a cer-
tain part-of-speech and a certain sense of a word,
respectively. This is the squared-probability that an
instance of w is the top sense ? the most-frequent
part-of-speech and the most-frequency sense of that
part-of-speech. The probability is squared because
both the word in the lexicon and the word occurring
in context should be the top sense (two independent
probabilities: given an instance of a word in a cor-
pus, and another instance of the word in the lexicon,
what is the probability that both are the top POS
and sense?). Frequency data is provided for parts-
of-speech from the Google N-Grams 2.0 (Lin et al,
2010) and for word senses from SemCor (Miller et
al., 1993). This aspect of the refinement is inspired
by the most frequent sense heuristic for word sense
disambiguation (McCarthy et al, 2004; Yarowsky,
1993), in which the sense of a word is chosen with-
out regard to the context, but rather is simply based
on the frequencies of senses in corpora. In our case,
we restrict ourselves this way in order for the appli-
cation of the lexicon to remain unchanged.
For some words, we were unable to find sense fre-
quency data. We decided to keep such terms, on
the assumption that a lack in available frequency in-
formation implies that the word is not very ambigu-
ous. Many of these terms include Web speak such as
?haha? or ?lol?, which we believe can carry a strong
signal for positive and negative emotion.
Lastly, since TSP is only a metric for the in-
verse ambiguity of a word, we must apply a thresh-
old to determine which words to keep. We denote
this threshold as ?, and the description of the refined
lexicon for a category, cat, is below.
lex?(cat) = {w|w ? cat ? TSP (w) > ?}
4.2 Handling Stems
Some lexica, such as the LIWC dictionary, include
word stems that are intended to match multiple
forms of a word. Stems are marked by the suffix
?*?. LIWC describes the application of stems as fol-
lows ?the asterisk, then, denotes the acceptance of
all letters, hyphens, or numbers following its ap-
302
lex cat prec size
full
POSEMO 67.9% 500
NEGEMO 72.8% 500
both 70.4% 1,000
lex0.10
POSEMO 70.9% 392
NEGEMO 71.6% 423
both 71.3% 815
lex0.50
POSEMO 75.7% 239
NEGEMO 78.9% 232
both 77.3% 471
lex0.90
POSEMO 72.5% 109
NEGEMO 78.1% 128
both 75.5% 237
Table 6: Precision (prec) and instance subset size (size)
of refinements to the LIWC POSEMO and NEGEMO lex-
ica with various ? thresholds (0.10, 0.50, 0.90)
pearance.?3 This presents a problem because, while
the creators of such lexica obviously intended stems
to match multiple forms of a word, stems also often
match completely different words, such as ?numb*?
matching ?number? or ?won*? matching ?won?t?.
We identified how often unintended matches hap-
pen in Section 3. Finding that the stemming issues
were not the biggest problem, here, we just describe
how they fit into our lexical ambiguity metric, rather
than describe a technique to rid the lexicon of stem-
ming problems. One approach might be to deter-
mine how ambiguous a stem is ? i.e. determine
how many words, parts-of-speech, and senses a stem
could be expanded into, but this ignores the fact that
the dictionary creators obviously intended the stem
to match multiple words. Instead, we expand stems
into all words that they match and replace them into
the lexica.
We base our expansion on the actual terms used
in social media. We find all words matching stems
among 1 million randomly selected Twitter mes-
sages posted over a 6-month period (August 2009
- February 2010), and restrict to those occurring at
least 20 times. Then, each word stem in the lexicon
is replaced with the expanded set of matching words.
Figure 2: The relationship between precision and size
when increasing the TSP threshold (?).
5 Evaluation
We evaluate our refinement by comparing against
human judgements of the emotion conveyed by
words in individual posts. In the case of hu-
man judgements, we find that the subset of human-
annotated instances matching the refined lexica are
more accurate than the complete set.
In section 3 we discussed the method we used to
judge instances of LIWC POSEMO and NEGEMO
words as to whether they contributed the associated
affect. Each of the 1,000 instances in our evaluation
corpus were judged three times such that the major-
ity was taken as truth. In order to validate our refined
lexica, we find the accuracy (precision) of the subset
of instances which contain the refined lexica terms.
Table 6 shows the change in precision when us-
ing the refined lexica. size represents the number of
instances from the full evaluation corpus matching
words in the refined lexica. One can see that ini-
tially precision increase as the size becomes smaller.
This is more clearly seen in Figure 2. As discussed
in the method section, our goal with the refine-
ment is improving precision, making lexica more
suitable to applications over massive social media
where one can more readily afford to skip instances
(i.e. smaller size) in order to achieve more accu-
racy. Still, removing more ambiguous words does
3?How it works?: http://www.liwc.net/howliwcworks.php
303
not guarantee improved precision at capturing the
intended psychological state; it is possible that that
all senses of an ambiguous word do in fact carry in-
tended signal or that the intended sense a low ambi-
guity word is not the most frequent.
Our maximum precision occurs with a threshold
of 0.50, where things somewhat level-out. This rep-
resents approximately a 23% reduction in error, and
verifies that we can increase precision through the
automatic lexicon refinement based on lexical ambi-
guity.
6 Conclusions
Social scientists and other researchers are starting
to measure psychological states such as happiness
through text in Facebook and Twitter. We have
shown that the widely used word count method,
where one simply counts occurrences of positive or
negative words, can often produce noisy and inaccu-
rate estimates of expressions of psychological states.
We characterized and measured the frequency of
different types of errors that occur using this ap-
proach, and found that when counting words without
considering context, it is lexical ambiguities (unin-
tended parts-of-speech or word senses) which cause
the most errors. We proposed a method for refin-
ing lexica by removing those words most likely to
be ambiguous, and showed that we can significantly
reduce error as measured by human judgements.
Acknowledgments
Support for this research was provided by the
Robert Wood Johnson Foundation?s Pioneer Portfo-
lio, through a grant to Martin Seligman, ?Exploring
Concepts of Positive Health?. We thank the review-
ers for their constructive and insightful comments.
References
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. Sentiwordnet 3.0: An enhanced lexical
resource for sentiment analysis and opinion mining. In
Proceedings of the Seventh International Conference
on Language Resources and Evaluation (LREC?10),
Valletta, Malta, may. European Language Resources
Association (ELRA).
Erin O.C. Bantum and J.E. Owen. 2009. Evaluating the
validity of computerized content analysis programs for
identification of emotional expression in cancer narra-
tives. Psychological assessment, 21(1):79.
Johan Bollen, Huina Mao, and Xiao-Jun Zeng. 2010.
Twitter mood predicts the stock market. Computer and
Information Science, 1010:1?8.
Michael A. Cohn, M.R. Mehl, and J.W. Pennebaker.
2004. Linguistic markers of psychological change sur-
rounding september 11, 2001. Psychological Science,
15(10):687.
Peter Sheridan Dodds, Kameron Decker Harris, Isabel M
Kloumann, Catherine A Bliss, and Christopher M
Danforth. 2011. Temporal patterns of happiness and
information in a global social network: Hedonomet-
rics and twitter. Diversity, page 26.
Jeremy Ginsberg, M.H. Mohebbi, R.S. Patel, L. Bram-
mer, M.S. Smolinski, L. Brilliant, et al 2009. De-
tecting influenza epidemics using search engine query
data. Nature, 457(7232):1012?4.
Scott A. Golder and M.W. Macy. 2011. Diurnal and
seasonal mood vary with work, sleep, and daylength
across diverse cultures. Science, 333(6051):1878?
1881.
Louis A. Gottschalk and RJ Bechtel. 1998. Psychiatric
content analysis and diagnosis (pcad2000).
Vasileios Hatzivassiloglou and Kathleen R. McKeown.
1997. Predicting the semantic orientation of adjec-
tives. In Annual Meeting of the Association for Com-
putational Linguistics, pages 174?181.
Daniel Kahneman, A.B. Krueger, D. Schkade,
N. Schwarz, and A.A. Stone. 2006. Would you
be happier if you were richer? a focusing illusion.
Science, 312(5782):1908.
Jaap Kamps and Maarten Marx. 2002. Words with atti-
tude. In 1st International WordNet Conference, pages
332?341, Mysore, India.
Sepandar D. Kamvar and J. Harris. 2011. We feel fine
and searching the emotional web. In Proceedings
of the fourth ACM international conference on Web
search and data mining, pages 117?126. ACM.
Hiroshi Kanayama and Tetsuya Nasukawa. 2006. Fully
automatic lexicon expansion for domain-oriented sen-
timent analysis. In Proceedings of the 2006 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, EMNLP ?06, pages 355?363, Stroudsburg,
PA, USA. Association for Computational Linguistics.
Soo-Min Kim and Eduard Hovy. 2004. Determining the
sentiment of opinions. In Proceedings of the 20th in-
ternational conference on Computational Linguistics,
COLING ?04, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Funda Kivran-Swaine and M. Naaman. 2011. Network
properties and social sharing of emotions in social
awareness streams. In Proceedings of the ACM 2011
304
conference on Computer supported cooperative work,
pages 379?382. ACM.
Michal. Kosinski and David J. Stillwell. 2012.
mypersonality research wiki. mypersonality project.
http://www.mypersonality.org/wiki/.
Adam D.I. Kramer. 2010. An unobtrusive behavioral
model of gross national happiness. In Proceedings of
the 28th international conference on Human factors in
computing systems, pages 287?290. ACM.
Dekang Lin, Kenneth Church, Heng Ji, Satoshi Sekine,
David Yarowsky, Shane Bergsma, Kailash Patil, Emily
Pitler, Rachel Lathbury, Vikram Rao, Kapil Dalwani,
and Sushant Narsale. 2010. New tools for web-scale
n-grams. In Proceedings of the Seventh International
Conference on Language Resources and Evaluation
(LREC?10), Valletta, Malta, may. European Language
Resources Association (ELRA).
Diana McCarthy, Rob Koeling, Julie Weeds, and John
Carroll. 2004. Finding predominant word senses
in untagged text. In Proceedings of the 42nd Meet-
ing of the Association for Computational Linguistics
(ACL?04), Main Volume, pages 279?286, Barcelona,
Spain, July.
George A. Miller, Claudia Leacock, Randee Tengi, and
Ross T Bunker. 1993. A semantic concordance. In
Proceedings of the ARPA Workshop on Human Lan-
guage Technology, pages 303?308. Morgan Kaufman.
George A. Miller. 1993. Five papers on wordnet. Tech-
nical Report, Princeton University.
Greg Miller. 2011. Social scientists wade into the tweet
stream. Science, 333(6051):1814?1815.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? Sentiment classification using
machine learning techniques. In Proceedings of the
2002 Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 79?86.
James W Pennebaker, Martha E Francis, and Roger J
Booth. 2001. Linguistic inquiry and word count:
Liwc 2001. Word Journal Of The International Lin-
guistic Association.
James W. Pennebaker, C.K. Chung, M. Ireland, A. Gon-
zales, and R.J. Booth. 2007. The development and
psychometric properties of liwc2007. Austin, TX,
LIWC. Net.
Yla R. Tausczik and J.W. Pennebaker. 2010. The psy-
chological meaning of words: Liwc and computerized
text analysis methods. Journal of Language and So-
cial Psychology, 29(1):24.
Janyce Wiebe and Claire Cardie. 2005. Annotating ex-
pressions of opinions and emotions in language. lan-
guage resources and evaluation. In Language Re-
sources and Evaluation.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2009. Recognizing contextual polarity: An explo-
ration of features for phrase-level sentiment analysis.
Computational Linguistics, 35:399?433, September.
David Yarowsky. 1993. One sense per collocation. In
Proceedings of the workshop on Human Language
Technology, HLT ?93, pages 266?271, Stroudsburg,
PA, USA. Association for Computational Linguistics.
305
Workshop on Computational Linguistics and Clinical Psychology: From Linguistic Signal to Clinical Reality, pages 118?125,
Baltimore, Maryland USA, June 27, 2014.
c?2014 Association for Computational Linguistics
Towards Assessing Changes in Degree of Depression through Facebook
H. Andrew Schwartz
?
Johannes Eichstaedt
?
Margaret L. Kern
?
Gregory Park
?
Maarten Sap
?
David Stillwell
?
Michal Kosinski
?
and Lyle Ungar
?
?
Psychology and Computer & Information Science, University of Pennsylvania
?
Psychometrics Centre, University of Cambridge
hansens@seas.upenn.edu
Abstract
Depression is typically diagnosed as be-
ing present or absent. However, depres-
sion severity is believed to be continu-
ously distributed rather than dichotomous.
Severity may vary for a given patient daily
and seasonally as a function of many vari-
ables ranging from life events to environ-
mental factors. Repeated population-scale
assessment of depression through ques-
tionnaires is expensive. In this paper we
use survey responses and status updates
from 28,749 Facebook users to develop a
regression model that predicts users? de-
gree of depression based on their Face-
book status updates. Our user-level pre-
dictive accuracy is modest, significantly
outperforming a baseline of average user
sentiment. We use our model to estimate
user changes in depression across seasons,
and find, consistent with literature, users?
degree of depression most often increases
from summer to winter. We then show the
potential to study factors driving individ-
uals? level of depression by looking at its
most highly correlated language features.
1 Introduction
Depression, a common mental disorder, greatly
contributes to the economic, social, and phys-
ical burden of people worldwide. Along with
other mental disorders it has been related to
early termination of education, unstable mar-
riages, teenage pregnancy, financial problems, role
impairment, heart disease, and other negative out-
comes (Kessler and Bromet, 2013; Lichtman et al.,
2014)
Currently, depression is primarily assessed
through surveys. Diagnoses require a medical or
psychological evaluation, and are typically classi-
fied into discrete categories (absent, mild, moder-
ate, severe). Clinicians rely on retrospective re-
ports by patients to monitor symptoms and treat-
ment. Unobtrusive assessments based on language
use in Facebook and social media usage could
amend both the self-help resources available to pa-
tients as well as repertoire of clinicians with richer
information. Such a tool could allow for more fre-
quent and fine grained (i.e., continuously scored)
assessment and could provide contextualized in-
formation (e.g. specific words and online activi-
ties that are contributing to the user?s depression
score).
Here, we predict and characterize one?s degree
of depression (DDep) based on their language use
in Facebook. Datasets connecting surveyed de-
pression with language in Facebook are rare at
best. To operationalize DDep, we use the depres-
sion facet scores of the ?Big 5? item pool (Gold-
berg, 1999) from the MyPersonality dataset. This
provides a continuous value outcome, for which
we fit a regression model based on ngrams, LDA
topics, and lexica usage. By predicting continuous
values, rather than classes, one can track changes
in DDep of varying size across time; we find sig-
nificantly more users? DDep increases from sum-
mer to winter than vice-versa.
Our primary contribution is the exploration
of predicting continuous-valued depression scores
from individuals? social media messages. To the
best of our knowledge this has not previously been
studied, with other social media and depression
work focused on discrete classes: present or ab-
sent. We compare our predictive model of DDep
to one derived from a state-of-the-art sentiment
lexicon and look at changes across seasons. Fi-
nally, we characterize DDep by looking at its top
ngram and topic correlates.
118
2 Background
2.1 Depression
Depression is generally characterized by persistent
low mood, poor concentration, fatigue, and little
interest in normally enjoyable activities. Depres-
sion can range from mild to severe, and can occur
as an acute episode (major depressive episode),
extend chronically over time (major depressive
disorder, persistent depressive disorder), reoccur
after a period of remission (recurrent depression),
or occur at specific periods (seasonal affective dis-
order, postpartum depression, premenstrual dys-
phoric disorder). Prevalence rates vary; the World
Health Organization estimates that over 350 mil-
lion people worldwide have a depressive disorder,
with many more reporting at least some symptoms
(Organization, 2012). In the U.S., in the World
Health Mental Survey, over half of the respondents
(62%) endorsed at least one diagnostic stem ques-
tions for depression, with 19.2% meeting criteria
for at least one major depressive episode (Kessler
et al., 2010).
Although depression has long been defined as
a single disease with a set of diagnostic criteria,
it often occurs comorbidly with other psycholog-
ical and physical disorders. Anxiety, anger, and
other psychological disorders often co-occur with
depression, and some have suggested that anx-
iety and depression are different manifestations
of the same underlying pathology (Mineka et al.,
1998). An expert panel convened by the Ameri-
can Heart Association recently recommended that
depression be considered a formal risk factor for
heart disease (Lichtman et al., 2014). Depres-
sion has been related to a range of physical con-
ditions, including asthma, cancer, cardiovascular
disease, diabetes, and chronic pain (Kessler and
Bromet, 2013), although the causal direction is
confounded; it may be that other factors cause
both depression and physical illness (Friedman
and Kern, 2014).
As noted previously, assessing degree of de-
pression as a continuous value allows us to look
at changes in depression across time. There has
been longstanding interest and discussion of sea-
sonal patterns of depression, with observations of
seasonal depressive patterns apparent in ancient
times, and the first systematic description occur-
ring in 1984 (Westrin and Lam, 2007). Com-
monly called Seasonal Affective Disorder (SAD),
the DSM-V now refers to this pattern as recur-
rent major depressive disorder with a seasonal pat-
tern. A clinical diagnosis of seasonal depression
requires that two major depressive episodes have
occurred in the past two years, with the onset and
remission showing a regular temporal pattern (pre-
dominantly with onset occurring in the fall/winter
and full remission in spring/summer).
Patients with depression often have common
symptoms of low energy, reduced or intensified
psychomotor movements, low concentration, in-
decisiveness, and thoughts of death, as well as
related symptoms such as fatigue, insomnia, and
weight gain. A challenge in diagnosis is that it re-
lies on a patient?s historical report, and other pos-
sible causes such as physical illness must be ruled
out. Further, with stigmas against mental illness
and feats about seeking treatment, many cases go
unrecognized, causing considerable burden on the
individual and society as a whole. Prevalence rates
vary, but rigorous reviews suggest a prevalence of
.4% in the U.S., although estimates have been re-
ported as high as 10% (Blazer et al., 1998; Mag-
nusson and Partonen, 2005).
There are a number of different hypotheses
about the pathophysiology of S A D, including cir-
cadian, neurotransmitter, and genetic causes (Lam
and Levitan, 2000). Reviews suggest that light
therapy is an effective and well-tolerated treat-
ment, with effects equal to or larger than antide-
pressants (Golden et al., 2005; Lam and Levitan,
2000; Thompson, 2001; Westrin and Lam, 2007).
Attempts to explain why light therapy is so ef-
fective have included shifting photoperiods (light-
dark cycles, with less light in the winter), changes
in melotonin secretion, and circadian phase shifts
(Lam and Levitan, 2000).
One related explanation for the photoperiod ef-
fect is latitude, with the prevalence of seasonal
depression increasing with growing distance from
the equator. Although there has been some support
for this hypothesis in the U.S. (Rosen et al., 1990),
findings in other countries have been mixed (Mer-
sch et al., 1999). Although latitude may play some
role, other factors such as climate, genetic vulner-
ability, and the sociocultural context may have a
stronger impact.
Altogether, inconsistent results suggest that
there is considerable variation in the magnitude,
causes and manifestations of seasonal depression,
much of which is not fully understood, in part due
to diagnostic issues (Lam and Levitan, 2000). A
119
Dislike myself.
Am often down in the dumps.
Have frequent mood swings.
Feel desperate.
Feel comfortable with myself. (-)
Seldom feel blue. (-)
Am very pleased with myself. (-)
Table 1: The seven items of the depression facet
from the 100-item International Personality Item
Pool (IPIP) proxy to the NEO-PI-R (Goldberg,
1999). (-) indicates a reverse coded item.
weekly or even daily depression assessment tool
would allow us to more fully understand the sea-
sonal and other temporal changes in depression.
We use the ?depression facet? scores de-
rived from a subset of the ?big-5? personality
items. Specifically, depression is one of sev-
eral facets (e.g. anger, depression, anxiety, self-
consciousness, impulsiveness, vulnerability) of
the neuroticism personality factor. Neuroticism
refers to individual differences in the tendency to
experience negative, distressing emotions, and be-
havioral and cognitive styles that result from this
(McCrae and John, 1992). It includes traits such
as tension, depression, frustration, guilt, and self-
consciousness, and is associated with low self-
esteem, irrational thoughts and behaviors, ineffec-
tive coping styles, and somatic complaints.
Various scales have been developed to mea-
sure neuroticism, such as the Eysenck Personality
Questionnaire (Eysenck and Eysenck, 1975) and
the NEO-PI-R (Costa and McCrae, 1992). Some
items on these scales overlap with self-reported
items that screen for depression (e.g., personality
item: ?I am often down in the dumps?; depression
screening item: ?how often have you been feel-
ing down, depressed, or hopeless??; see Table 1.),
such that the personality items effectively provide
a proxy measure of depressive tendencies.
2.2 Related Work
Depression has been linked with many online be-
haviors. In fact, even Internet usage itself seems to
vary as a function of being depressed(Katikalapudi
et al., 2012). Other behaviors include social net-
working (Moreno et al., 2011) and differences in
location sharing on Facebook (Park et al., 2013).
Most related to our work, are those using lin-
guistic features to assess various measures of de-
pression. For example, De Choudhury et al.
(2013) used online posting behavior, network
characteristics, and linguistic features when try-
ing to predict depression rather than find its corre-
lates. They used crowdsourcing to screen Twitter
users with the CES-D test (Beekman et al., 1997),
while others analyzed one year of Facebook sta-
tus updates for DSM diagnostic critera of a Major
Depressive Episode (Moreno et al., 2011). In ad-
dition, Park et al. (2013) predicted results of the
Beck Depression Inventory (Beck et al., 1961).
While previous works have made major head-
way toward automatic depression assessment tools
from social media, to the best of our knowledge,
none have tried to predict depression as a con-
tinuum rather than a discrete, present or absent,
attribute. For instance, Neuman et al. (2012)
classified blog posts based on whether they con-
tained signs of depression, and De Choudhury et
al. (2013) classified which newfound mothers
would suffer from postpartum depression.
3 Predicting Degree of Depression
3.1 Method
Dataset. We used a dataset of 28,749 nonclini-
cal users who opted into a Facebook application
(?MyPersonality?; Kosinski and Stillwell, 2012)
between June 2009 and March 2011, completed
a 100-item personality questionnaire (an Interna-
tional Personality Item Pool (IPIP) proxy to the
NEO-PI-R (Goldberg, 1999), and shared access
to their status updates containing at least 500
words. Users wrote on average of 4,236 words
(69,917,624 total word instances), and a subset of
16,507 users provided gender and age, in which
57.0% were female and the mean age was 24.8.
The dataset was divided into training and test-
ing samples. In particular, the testing sample con-
sisted of a random set of 1000 users who wrote
at least 1000 words and completed the personal-
ity measure, while the training set contained the
27,749 remaining users.
Degree of depression. We estimated user-level
degree of depression (DDep) as the average re-
sponse to seven depression facet items, which are
nested within the larger Neuroticism item pool.
For each item, users indicated how accurately
short phrases described themselves (e.g., ?often
feel blue?, ?dislike myself?; responses ranged
from 1 = very inaccurate to 5 = very accu-
120
(a)
0
50
100
150
?2 ?1 0 1 2Degree of Depression (DDep) as assessed by survey
Numb
er of u
sers
(b)
0
50
100
150
?2 ?1 0 1 2Degree of Depression (DDep) as predicted by language
Numb
er of u
sers
Figure 1: Histograms of (a) survey-assessed
and (b) predicted user-level degree of depression
DDep.
rate). Figure 1a shows the distribution of survey-
assessed DDep (standardized). The items can be
seen in Table 1.
Figure 2 shows the daily averages of survey-
assessed DDep, collapsed across years. A LOESS
smoother over the daily averages illustrates a sea-
sonal trend, with depression rising over the winter
months and dropping during the summer.
Regression modeling. In order to get a contin-
uous value output from our model, we explored
regression techniques over our training data.
Since this first work exploring regression was
concerned primarily with language content, our
features for predicting depression were based
entirely on language use (other social media
activity and friend networks may be considered in
future work). These features can be broken into
four categories:
ngrams: Ngrams of order to 1 to 3, found via Hap-
pierFunTokenizer, and restricted to those used by
at least 5% of users (resulting in 10,450 ngrams).
The features were encoded as relative frequency of
mentioning each ngram (ng):
rel freq(user, ng) =
freq(user, ng)
?
ng
?
?ngs
freq(user, ng
?
)
topics: 2000 LDA derived Facebook topics.
1
Us-
age was calculated as the probability of the topic
given the user:
usage(top|user) =
?
ng?topic
p(top|ng) ? rel freq(user, ng)
1
downloaded from wwbp.org/data.html
?0.25
0.00
0.25
Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov DecDate of Survey Administration
Surve
y?ass
essed
Degre
eofD
epress
ion
Figure 2: Seasonal trends in degree of depres-
sion as assessed by surveys. Red line is a LOESS
smoothed trend (+/- 1 SE) over the average of
scores from users who completed the survey on
that day.
lexica: 64 LIWC categories (Pennebaker et al.,
2007) as well as the sentiment lexicon from NRC
Canada (Mohammad et al., 2013).
2
Usage of a
lexicon (lex) was calculated similar to the LDA
topics, where w is the weight of the word in the
lexicon in the case of sentiment and always 1 in
the case of LIWC which has no weights:
usage(lex, user) =
?
ng?lex
w(ng, lex) ? rel freq(user, ng)
number of words: Encoded simply as the integer
value for that user.
We used penalized linear regression to fit our
features to DDep. We experimented with a few pe-
nalization types over the training set and settled on
L2 (?ridge regression?), using Principal Compo-
nents Analysis to first reduce the ngram and topic
features to 10 % of their original size. In order to
ensure users tested provided an adequate amount
of features, we only tested over those with at least
1,000 words. However, we found that including
more users in our training set at the expense of
words per user increased model accuracy. Thus,
we only required our training data users to men-
tion 500 words, essentially allowing more noise in
order to increase the number of training examples.
We also experimented with training models on
two sets of messages: all messages and the sub-
set of messages written in the same three-month
season as the survey administration (season only
2
downloaded from www.saifmohammad.com
121
Model Season test (r) All test (r)
Baseline
sentiment
.124 .149
Season .321 .340
All .351 .386
Table 2: Accuracy of various models against test
sets containing only messages from the season
and year in which the user took the survey as
well as a test using all of user?s messages. Mod-
els: Baseline
sentiment
a model based on a state-
of-the-art sentiment lexicon (Mohammad et al.,
2013); Season: model trained on messages sent
only during the same season and year in which
each user took the survey; All model trained on
all messages of each user.
messages). Because the degree of depression may
vary over time, we reasoned that messages written
closer to survey administration might better reflect
the degree of depression assessed by the survey.
When generating predictions on users in the test
set, we applied both the all messages model and
the season only messages model to features from
all messages and then to just the features from the
same season as the survey administration.
3.2 Evaluation and Results
We evaluated accuracy using the Pearson corre-
lation coefficient r between our predictions and
survey-assessed DDep. As a baseline, we built a
regression model simply using the NRC sentiment
(Mohammad et al., 2013) feature.
Accuracies are shown in Table 2. Accuracy was
highest (r = .386) when we trained a model over
all messages from users in the training set and then
applied this model to all messages by users in the
test set. Though our model allows for seasonal
change in depression, we suspect the test across all
messages was more accurate than that of only us-
ing the season in which the users depression was
assessed due to the larger amount messages and
language features provided to the model.
Both models (season-only messages, and all
messages) gave significant (p < 0.05) improve-
ment over the baseline (r = .149) and though
these accuracies may look small, it?s worth not-
ing that a correlation above r = 0.3 is often re-
garded as a strong link between a behavior and a
psychological outcome (Meyer et al., 2001). Still,
we fit many behavior variables (i.e., language use
features) to an outcome and so we might hope
0
40
80
120
160
?0.6 ?0.3 0.0 0.3 0.6Seasonal Difference (Winter?Summer) in Predicted Degree of Depression
Number
ofusers
Figure 3: Histogram of differences between winter
and summer predictions of user-level DDep. Av-
erage user-level predicted DDep values were sig-
nificantly higher in the winter months (t = 4.63,
p < .001).
for higher variance explained. We suspect hav-
ing more users to train on and taking more fea-
tures into account could improve results. For ex-
ample, people who nearly stopped writing for a
season would be thrown out of our analyses since
it is completely based on language content, even
though they are more likely to be depressed (so-
cial isolation is a common symptom in depres-
sion). Similarly, we do not use demographics in
our models, even though women are more likely
to become depressed than men.
To assess individual seasonal changes in de-
gree of depression, we predicted summer and win-
ter DDep values for each user with at least 1000
words across both summer-only and winter-only
messages, respectively. We then compared the
differences across the seasonal predictions; Fig-
ure 3 shows the distribution of user-level seasonal
differences across 676 users with sufficient lan-
guage for both seasonal predictions. In line with
the trends seen in survey data, average user-level
DDep values, as predicted by language, were sig-
nificantly higher in the winter months (t = 4.63,
p < .001).
4 Differential Language Analysis
Figure 4 shows the 100 ngrams most highly cor-
related with depression score across the 21,913
Facebook users in our dataset writing at least
1,000 words. Unlike typical word clouds, the
clouds represent language that differentiates users
scoring high on depression. The size of a word
represents its correlation with depression (larger
122
= stronger), the color its relative frequency (grey
= rarely used, blue = moderately used, red = fre-
quently used).
The f-word emerges as both the most correlated
feature (as indicated by the size of the word) and
is highly frequent (indicated by the red color). To-
gether with words such as ?pissed? and ?bloody?,
these curse words suggest hostility or aggression.
Similarly, words such has ?hate? and ?lonely? sug-
gest negative social relationships.
Perhaps surprisingly, the words ?depression?
and ?depressed? emerge as highly correlated fea-
tures. These face valid features occur infrequently
(as indicated by their grey color), yet are strongly
associated with depressive tendencies, demon-
strating the high statistical power of our approach
applied to this large dataset in identifying signif-
icant but rarely used language features. The both
frequent and highly correlated word ?why? hints at
signs of hopelessness and meaninglessness, a core
feature of depressive disorders.
As illustrated in Figure 5, extending the words
and phrase results, automatically derived topics
demonstrate substantial overlap with the major
clinical symptoms of major depressive disorder
(American Psychiatric Association et al., 2013).
Hopelessness and meaninglessness are seemingly
expressed by ?hopeless? and ?helpless?. Perhaps
the most noticable symptom of depression, de-
pressed mood, is expressed in topics mentioning
?feel?, ?crap?, ?sad?, and ?miserable?.
Depression often affects psychomotor function,
either in terms of fatigue and low energy or in-
versely as insomnia and hyperactivity. Such symp-
toms are reflected in words such as ?tired?, and
?sleep?. Depression is often expressed somati-
cally through bodily symptoms, captured through
?hurt?, ?my head? and ?pain?.
One of the most predictive questions on de-
pressive screening questionnaires asks about sui-
cidal thought, which appears with topics related to
thoughts of death, with words such as ?kill?, ?die?,
and ?dying?.
Topics also reflected hostility, aggression, and
negative relationships with other people. Loneli-
ness has emerged as one of the strongest predic-
tors of physical morbidity and mortality (Hawk-
ley and Cacioppo, 2010), and both ?lonely? and
?alone? appear as some of the most correlated sin-
gle words. Given such striking descriptive results,
future work might try to detect depression associ-
Figure 5: Top ten topics most positively correlated
with depression (from r = .14 at top to r = .11
at bottom). All are significant at a Bonferroni-
corrected threshold of p < 0.001. Word size cor-
responds to prevalence within the topics.
ated conditions as well such as insomnia, loneli-
ness, and aggression.
5 Conclusion
Depression can be viewed as a continuous con-
struct that changes over time, rather than simply as
being a disease that one has or does not have. We
showed that regression models based on Facebook
language can be used to predict an individual?s de-
gree of depression, as measured by a depression
facet survey. In line with survey seasonal trends
and the broader literature, we found that language-
based predictions of depression were higher in
the winter than the summer, suggesting that our
123
Figure 4: The 100 ngrams most correlated with DDep (ranging from r = .05 to r = .10). All are
significant at a Bonferroni-corrected threshold of p < 0.001. Ngram size corresponds to correlation
strength (larger words are more distinguishing). Color corresponds to relative frequency (red if frequent,
blue moderate, grey infrequent).
continuous predictions are capturing small, yet
meaningful within-person changes. With further
development of regression models, many users
write enough on Facebook that we could estimate
changes in their level of depression on a monthly
or even weekly basis. Such estimates, correlated
with word use over time offers potential both for
research at the group-level (?What are the social
and environmental determinants of depression??,
?How well are talk or medication-based interven-
tions working??) as well as, eventually, for med-
ical and therapeutic application at the individual
level (?How well am I doing and what depression-
relevant thoughts or behaviors have I disclosed in
the past week??).
References
APA American Psychiatric Association, Ameri-
can Psychiatric Association, et al. 2013. Diagnostic
and statistical manual of mental disorders.
Aaron T Beck, Calvin H Ward, Mock Mendelson,
Jeremiah Mock, and JK Erbaugh. 1961. An inven-
tory for measuring depression. Archives of general
psychiatry, 4(6):561.
Aartjan TF Beekman, DJH Deeg, J Van Limbeek,
AW Braam, MZ De Vries, W Van Tilburg, et al.
1997. Criterion validity of the Center for Epi-
demiologic Studies Depression scale (CES-D): re-
sults from a community-based sample of older sub-
jects in The Netherlands. Psychological medicine,
27(1):231?236.
Dan G Blazer, Ronald C Kessler, and Marvin S Swartz.
1998. Epidemiology of recurrent major and minor
depression with a seasonal pattern. The National Co-
morbidity Survey. The British Journal of Psychia-
try, 172(2):164?167.
Paul T Costa and Robert R McCrae. 1992. Profes-
sional manual: revised NEO personality inventory
(NEO-PI-R) and NEO five-factor inventory (NEO-
FFI). Odessa, FL: Psychological Assessment Re-
sources.
Munmun De Choudhury, Scott Counts, and Eric
Horvitz. 2013a. Predicting postpartum changes in
emotion and behavior via social media. In Pro-
ceedings of the 2013 ACM annual conference on
Human factors in computing systems, pages 3267?
3276. ACM.
Munmun De Choudhury, Michael Gamon, Scott
Counts, and Eric Horvitz. 2013b. Predicting de-
pression via social media. In AAAI Conference on
Weblogs and Social Media.
Hans Jurgen Eysenck and Sybil Bianca Giuletta
Eysenck. 1975. Manual of the Eysenck Personal-
ity Questionnaire (junior and adult). Hodder and
Stoughton.
Howard S Friedman and Margaret L Kern. 2014.
Personality, Well-Being, and Health*. Psychology,
65(1):719.
124
Lewis R Goldberg. 1999. A broad-bandwidth, public
domain, personality inventory measuring the lower-
level facets of several five-factor models. Personal-
ity psychology in Europe, 7:7?28.
Robert N Golden, Bradley N Gaynes, R David Ek-
strom, Robert M Hamer, Frederick M Jacobsen, Tr-
isha Suppes, Katherine L Wisner, and Charles B Ne-
meroff. 2005. The efficacy of light therapy in the
treatment of mood disorders: a review and meta-
analysis of the evidence. American Journal of Psy-
chiatry, 162(4):656?662.
Louise C Hawkley and John T Cacioppo. 2010. Lone-
liness matters: a theoretical and empirical review of
consequences and mechanisms. Annals of Behav-
ioral Medicine, 40(2):218?227.
R Katikalapudi, Sriram Chellappan, Frances Mont-
gomery, Donald Wunsch, and Karl Lutzen. 2012.
Associating Internet usage with depressive behav-
ior among college students. Technology and Society
Magazine, IEEE, 31(4):73?80.
Ronald C. Kessler and Evelyn J. Bromet. 2013. The
Epidemiology of Depression Across Cultures. An-
nual Review of Public Health, 34(1):119?138, Mar.
Ronald C Kessler, Howard G Birnbaum, Victoria
Shahly, Evelyn Bromet, Irving Hwang, Katie A
McLaughlin, Nancy Sampson, Laura Helena An-
drade, Giovanni de Girolamo, Koen Demyttenaere,
et al. 2010. Age differences in the prevalence and
co-morbidity of DSM-IV major depressive episodes:
results from the WHO World Mental Health Survey
Initiative. Depression and anxiety, 27(4):351?364.
M. Kosinski and D.J. Stillwell. 2012. myPersonality
Project. http://www.mypersonality.org/wiki/.
Raymond W Lam and Robert D Levitan. 2000. Patho-
physiology of seasonal affective disorder: a review.
Journal of Psychiatry and Neuroscience, 25(5):469.
Judith H Lichtman, Erika S Froelicher, James A Blu-
menthal, Robert M Carney, Lynn V Doering, Nancy
Frasure-Smith, Kenneth E Freedland, Allan S Jaffe,
Erica C Leifheit-Limson, David S Sheps, et al.
2014. Depression as a Risk Factor for Poor Prog-
nosis Among Patients With Acute Coronary Syn-
drome: Systematic Review and Recommendations
A Scientific Statement From the American Heart
Association. Circulation.
Andres Magnusson and Timo Partonen. 2005. Focus
Points. CNS Spectr, 10(8):625?634.
Robert R McCrae and Oliver P John. 1992. An intro-
duction to the five-factor model and its applications.
Journal of personality, 60(2):175?215.
Peter Paul A Mersch, Hermine M Middendorp, An-
toinette L Bouhuys, Domien GM Beersma, and Rut-
ger H van den Hoofdakker. 1999. Seasonal affec-
tive disorder and latitude: a review of the literature.
Journal of affective disorders, 53(1):35?48.
Gregory J Meyer, Stephen E Finn, Lorraine D Eyde,
Gary G Kay, Kevin L Moreland, Robert R Dies,
Elena J Eisman, Tom W Kubiszyn, and Geoffrey M
Reed. 2001. Psychological testing and psycholog-
ical assessment: A review of evidence and issues.
American psychologist, 56(2):128?165.
S Mineka, D Watson, and LA Clark. 1998. Comorbid-
ity of anxiety and unipolar mood disorders. Annual
review of psychology, 49:377.
Saif M. Mohammad, Svetlana Kiritchenko, and Xiao-
dan Zhu. 2013. NRC-Canada: Building the State-
of-the-Art in Sentiment Analysis of Tweets. In Pro-
ceedings of the seventh international workshop on
Semantic Evaluation Exercises (SemEval-2013), At-
lanta, Georgia, USA, June.
Megan A Moreno, Lauren A Jelenchick, Katie G Egan,
Elizabeth Cox, Henry Young, Kerry E Gannon,
and Tara Becker. 2011. Feeling bad on Face-
book: Depression disclosures by college students on
a social networking site. Depression and anxiety,
28(6):447?455.
Yair Neuman, Yohai Cohen, Dan Assaf, and Gabbi
Kedma. 2012. Proactive screening for depression
through metaphorical and automatic text analysis.
Artificial intelligence in medicine, 56(1):19?25.
World Health Organization.
2012. Depression fact sheet.
http://www.who.int/mediacentre/factsheets/fs369/en/.
Sungkyu Park, Sang Won Lee, Jinah Kwak, Meeyoung
Cha, and Bumseok Jeong. 2013. Activities on Face-
book Reveal the Depressive State of Users. Journal
of medical Internet research, 15(10).
James W. Pennebaker, C.K. Chung, M. Ireland,
A. Gonzales, and R.J. Booth. 2007. The devel-
opment and psychometric properties of LIWC2007.
Austin, TX, LIWC. Net.
Leora N Rosen, Steven D Targum, Michael Terman,
Michael J Bryant, Howard Hoffman, Siegfried F
Kasper, Joelle R Hamovit, John P Docherty, Betty
Welch, and Norman E Rosenthal. 1990. Prevalence
of seasonal affective disorder at four latitudes. Psy-
chiatry research, 31(2):131?144.
C Thompson. 2001. Evidence-based treatment. Sea-
sonal affective disorder: practice and research,
pages 151?158.
Asa Westrin and Raymond W Lam. 2007. Seasonal
affective disorder: a clinical update. Annals of Clin-
ical Psychiatry, 19(4):239?246.
125
