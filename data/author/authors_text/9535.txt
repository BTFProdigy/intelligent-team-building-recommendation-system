Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 284?293,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Arabic Named Entity Recognition using Optimized Feature Sets
Yassine Benajiba? Mona Diab Paolo Rosso?
?Natural Language Engineering Lab.,
Dept. de Sistemas Informa?ticos y Computacio?n
Universidad Polite?cnica de Valencia
{ybenajiba,prosso}@dsic.upv.es
Center of Computational Learning Systems
Columbia University
mdiab@cs.columbia.edu
Abstract
The Named Entity Recognition (NER) task
has been garnering significant attention in
NLP as it helps improve the performance
of many natural language processing applica-
tions. In this paper, we investigate the im-
pact of using different sets of features in two
discriminative machine learning frameworks,
namely, Support Vector Machines and Condi-
tional Random Fields using Arabic data. We
explore lexical, contextual and morphological
features on eight standardized data-sets of dif-
ferent genres. We measure the impact of the
different features in isolation, rank them ac-
cording to their impact for each named entity
class and incrementally combine them in or-
der to infer the optimal machine learning ap-
proach and feature set. Our system yields a
performance of F?=1-measure=83.5 on ACE
2003 Broadcast News data.
1 Introduction
Named Entity Recognition (NER) is the process by
which named entities are identified and classified in
an open-domain text. NER is one of the most im-
portant sub-tasks in Information Extraction. Thanks
to standard evaluation test beds such as the Auto-
matic Content Extraction (ACE)1, the task of NER
has garnered significant attention within the natu-
ral language processing (NLP) community. ACE
has facilitated evaluation for different languages cre-
ating standardized test sets and evaluation metrics.
NER systems are typically enabling sub-tasks within
1http://www.nist.gov/speech/tests/ace/2004/doc/ace04-
evalplan-v7.pdf
large NLP systems. The quality of the NER sys-
tem has a direct impact on the quality of the overall
NLP system. Evidence abound in the literature in
areas such as Question Answering, Machine Trans-
lation, and Information Retrieval (Babych and Hart-
ley, 2003; Ferra?ndez et al, 2004; Toda and Kataoka,
2005). The most prominent NER systems approach
the problem as a classification task: identifying the
named entities (NE) in the text and then classify-
ing them according to a set of designed features into
one of a predefined set of classes (Bender et al,
2003). The number of classes differ depending on
the data set. To our knowledge, to date, the ap-
proach is always to model the problem with a sin-
gle set of features for all the classes simultaneously.
This research, diverges from this view. We recog-
nize that different classes are sensitive to differing
features. Hence, in this study, we aspire to discover
the optimum feature set per NE class. We approach
the NER task from a multi-classification perspec-
tive. We create a classifier for each NE class inde-
pendently based on an optimal feature set, then com-
bine the different classifiers for a global NER sys-
tem. For creating the different classifiers per class,
we adopt two discriminative approaches: Support
Vector Machines (SVM)(Vapnik, 1995), and Condi-
tional Random Fields (CRF)(Lafferty et al, 2001).
We comprehensively investigate many sets of fea-
tures for each class of NEs: contextual, lexical, mor-
phological and shallow syntactic features. We ex-
plore the feature sets in isolation first. Then, we
employ the Fuzzy Borda Voting Scheme (FBVS)
(Garc??a Lapresta and Mart??nez Panero, 2002) in or-
der to rank the features according to their perfor-
284
mance per class. The incremental approach to fea-
ture selection leads to an interpretable system where
we have a better understanding of the resulting er-
rors. The paper is structured as follows: Section
2 gives a general overview of the state-of-the-art
NER approaches with a particular emphasis on Ara-
bic NER; Section 3 describes relevant character-
istics of the Arabic language illustrating the chal-
lenges posed to NER; in Section 4.1 we describe
the Support Vector Machines and Conditional Ran-
dom Fields Modeling approaches. We discuss de-
tails about our feature-set in 4.2 and describe the
Fuzzy Borda Voting Scheme in Section 4.3. Sec-
tion 5 describes the experiments and shows the re-
sults obtained; Withing Section 5, Section 5.1 gives
details about the data-sets which we use; finally, we
discuss the results and some of our insights in Sec-
tion 6 and draw some conclusions in 7.
2 Related Work
To date, the most successful language independent
approaches to English NER are systems that employ
Maximum Entropy (ME) techniques in a supervised
setting (Bender et al, 2003).
(Tran et al, 2007) show that using a Sup-
port Vector Machine (SVM) approach outperforms
(F?=1=87.75) using CRF (F?=1=86.48) on the NER
task in Vietnamese. For Arabic NER, (Benajiba
et al, 2007) show that using a basic ME approach
yields F?=1=55.23. Then they followed up with fur-
ther work in (Benajiba and Rosso, 2007), where they
model the problem as a two step classification ap-
proach applying ME, separating the NE boundary
detection from the NE classification. That mod-
ification showed an improvement in performance
yielding an F?=1=65.91. None of these studies in-
cluded Arabic specific features, all the features used
were language independent. In a later study, (Be-
najiba and Rosso, 2008) report using lexical and
morphological features in a single step model us-
ing CRF which resulted in significant improvement
over state of the art to date for Arabic NER, yield-
ing F?=1=79.21. However, the data that was used in
these evaluation sets were not standard sets. Most
recently, (Farber et al, 2004) have explored using
a structured perceptron based model that employs
Arabic morphological features. Their system ben-
efits from the basic POS tag (15 tags) information
and the corresponding capitalization information on
the gloss corresponding to the Arabic word. Exploit-
ing this information yields a significant improve-
ment in recall of 7% and an overall F?=1=69.6 on
the ACE2005 data set. The authors note the lack of
improvement in the system?s performance when us-
ing other Arabic morphological information.
3 Arabic in the context of NER
The Arabic language is a language of significant in-
terest in the NLP community mainly due to its po-
litical and economic significance, but also due to its
interesting characteristics. Arabic is a Semitic lan-
guage. It is known for its templatic morphology
where words are made up of roots, patterns, and af-
fixes. Clitics agglutinate to words. For instance, the
surface word ??EA 	J?mProceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 993?1001,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Enhancing Mention Detection
using Projection via Aligned Corpora
Yassine Benajiba
Center for Computational Learning Systems
Columbia University, NY
ybenajiba@ccls.columbia.edu
Imed Zitouni
IBM T.J. Watson Research Center
Yorktown Heights, NY
izitouni@us.ibm.com
Abstract
The research question treated in this paper
is centered on the idea of exploiting rich re-
sources of one language to enhance the per-
formance of a mention detection system of an-
other one. We successfully achieve this goal
by projecting information from one language
to another via a parallel corpus. We exam-
ine the potential improvement using various
degrees of linguistic information in a statisti-
cal framework and we show that the proposed
technique is effective even when the target
language model has access to a significantly
rich feature set. Experimental results show
up to 2.4F improvement in performance when
the system has access to information obtained
by projecting mentions from a resource-rich-
language mention detection system via a par-
allel corpus.
1 Introduction
The task of identifying and classifying entity textual
references in open-domain texts, i.e. the Mention
Detection (MD) task, has become one of the most
important subtasks of Information Extraction (IE).
It might intervene both as one step to structure nat-
ural language texts or as a text enrichment prepro-
cessing step to help other Natural Language Process-
ing (NLP) applications reach higher accuracy. Simi-
larly to the Automatic Content Extraction (ACE) 1
nomenclature, we consider that a mention can be
either named (e.g., John, Chicago), nominal (e.g.,
president, activist) or pronominal (e.g., he, she). It
has also a specific class which describes the type of
the entity it refers to. For instance, in the sentence:
1http://www.itl.nist.gov/iad/mig/tests/ace/2007/doc/ace07-
evalplan.v1.3a.pdf
Michael Bloomberg, the Mayor of NYC, declared
his war on tobacco and sugary drinks in the city.
we find the mentions ?Michael Bloomberg?, ?Mayor?
and ?his? of the same person entity. Their types
are named, nominal and pronominal, respectively.
?NYC? and ?city?, on the other hand, are mentions
of the same geopolitical (GPE) entity of type named
and nominal, respectively. Consequently, MD is a
more general and complex task than the well known
Named Entity Recognition (NER) task which aims
solely at the identification and classification of the
named mentions.
The difficulty of the MD task is directly related
to the nature of the language and the linguistic re-
sources available, i.e. it is easier to build accu-
rate MD systems for languages with a simple mor-
phology and a high amount of linguistic resources.
For this reason, we explore the idea of using an
MD system, which has been designed and built for
a resource-rich language (RRL), to help enhance
the performance of an MD system in a target lan-
guage (TL). More specifically, the goal of the re-
search work we present in this paper is to employ
the richness of English, in terms of natural lan-
guage resources, to raise the accuracy of MD sys-
tems in other languages. For instance, an English
MD system might achieve a performance of F?=1-
measure=82.7 (Zitouni and Florian, 2009) when it
resorts to a rich set of features extracted from di-
verse resources, namely: part-of-speech, chunk in-
formation, syntactic parse trees, word sense infor-
mation, WordNet information and information from
the output of other mention detection classifiers. In
this paper, our research question revolves around in-
vestigating an adequate approach to use such a sys-
tem to the benefit of other languages such as Arabic,
Chinese, French or Spanish MD systems, which also
993
have annotated resources but not of the same quan-
tity and/or quality as English.
In this paper, we have targeted English and Arabic
as the RRL and TL, respectively, because:
1. We have a very competitive English MD system;
2. The linguistic resources available for the Arabic
language allow a simulation of different TL richness
levels; and
3. The use of two languages of an utterly different
nature makes the extrapolation of the results to other
languages possible.
Our hypothesis might be expressed as follows: us-
ing an MD system resorting to a rich feature set (i.e.
the RRL MD system) to boost a MD system perfor-
mance in a TL can be very beneficial if the ?donor?
system surpasses its TL counterpart in terms of re-
sources. To test this hypothesis, we have projected
MD tags from RRL to TL via a parallel corpus, and
then extracted several linguistic features about the
automatically tagged words. Thereafter, we have
conducted experiments adding these new features to
the TL baseline MD system. In order to have a com-
plete picture on the impact of these new features, we
have used TL baseline systems resorting to a varied
amount of features, starting with a case employing
only lexical information to a case where we use all
the resources we could gather for the TL. Experi-
ments show that the gain is always statistically sig-
nificant and it reaches its maximum when only very
basic features are used in the baseline TL MD sys-
tem.
2 Mention Detection
Similarly to classical NLP tasks, such as Base
Phrase Chunking (Ramshaw and Marcus, 1999)
(BPC) or NER (Tjong Kim Sang, 2002), we formu-
late the MD task as a sequence classification prob-
lem, i.e. the classifier assigns to each token in the
text a label indicating whether it starts a specific
mention, is inside a specific mention, or is outside
any mentions. It also assigns to every non outside
mention a class to specify its type: e.g., person, or-
ganization, location, etc. In this study, we chose the
Maximum Entropy Markov Model (MEMM hence-
forth) approach because it can easily integrate arbi-
trary types of information in order to make a clas-
sification decision. To train our models, we have
used the Sequential Conditional Generalized Itera-
tive Scaling (SCGIS) technique (Goodman, 2002).
This techniques uses a Gaussian prior for regular-
ization (Chen and Rosenfeld, 2000). The features
used by our MD systems can be divided into the fol-
lowing categories:
1- Lexical: these are token n-grams directly neigh-
boring the current token on both sides, i.e. left and
right. Empirical results have shown that the optimal
span is n = 3.
2- Syntactic: they consist of the outcomes of several
Part-Of-Speech (POS) taggers and BPCs trained on
different corpora and different tag-sets in order to
provide the MD system with a wider variety of in-
formation. Our model uses the POS and BPC in-
formation appearing in window of 5 (current, two
previous, and two next) jointly with the tokens.
Both the English and the Arabic MD systems have
access to lexical and syntactic features. The former
one, however, also employs a set of features ob-
tained from the output of other MD classifiers. In
order to provide the MD system with complemen-
tary information, these classifiers are trained on dif-
ferent datasets annotated for different mention types,
e.g. dates or occupation references (not used in our
task).
3 Annotation, Projection and Feature
Extraction
We remind the reader that our main goal is to use
an RRL MD system to enhance the performance of
an MD system in another language, i.e. the TL. In
order to achieve this goal, we propose an approach
that uses an RRL-to-TL parallel corpus to bridge be-
tween these two languages. This approach performs
in three main steps, namely: annotation, projection
and feature extraction. In this section, we describe
in details each of these steps.
3.1 Annotation
This first step consists of MD tagging of the RRL
side of the parallel corpus. Because in our case study
we have chosen English as the RRL, we have used
an accurate English MD system to perform the an-
notation step. Our English MD system achieves an
F-measure of 82.7 (Zitouni and Florian, 2009) and
has achieved significantly competitive results at the
ACE evaluation campaign.
3.2 Projection
Once the RRL side of the parallel corpus is accu-
rately augmented with MD tags, the projection step
comes to transfer those tags to the TL side, Arabic
in our case study, using the word alignment informa-
tion. We illustrate the projection step with a relevant
example. Let consider the following MD tagged En-
glish sentence:
994
Bill/B-PER-NAM Clinton/I-PER-NAM is visiting
North/B-GPE-NAM Korea/I-GPE-NAM today
where ?Bill Clinton? is a named person mention and
?North Korea? is a named geopolitical entity (GPE)
one. A potential Arabic translation of this sentence
would be:
??J
? @

?J
?A?
??@ AK
P?? P?
	QK

	
??

J
	
J
?? ?J
K.
which might be transliterated as:
byl klyntwn yzwr kwryA Al$mAlyA Alywm
After projecting the English mentions to the Ara-
bic text, we obtain the following:
byl/B-PER-NAM klyntwn/I-PER-NAM yzwr
kwryA/B-GPE-NAM Al$mAlyp/I-GPE-NAM
Alywm
This tagged version of the Arabic text is provided to
the third module of the process responsible on fea-
ture extraction (see Subsection 3.3). It is, however,
pertinent to point out that the example we have used
for illustration is relatively simple in the sense that
almost all English and Arabic words have a 1-to-1
mapping. In real world translation (both human and
automatic), one should expect to see 1-to-n, n-to-1
mappings as well as unmapped words on both sides
of the parallel corpus rather frequently.
As stated by (Klementiev and Roth, 2006), the pro-
jection of NER tags is easier in comparison to pro-
jecting other types of annotations such as POS-tags
and BPC2, mainly because:
1. Not all the words are mentions: once we have pro-
jected the tags of the mentions from the RRL to TL
side, the rest of tokens are simply considered as out-
side any mentions. This is different from the POS-
tag and BPC where all the words are assigned a tag
and thus when a word is unmapped, further process-
ing is required (Yarowsky et al, 2001);
2. In case of a 1-to-n mapping, the target n
words are assigned the same class: for instance, let
consider the English GPE named mention ?North-
Korea?. The segmented version of its Arabic transla-
tion would be ?

?J
?A??
? ?@ AK
P??? (kwrya Al $mAlyp).
The projection process consists in simply assigning
the same class, i.e. GPE, to all Arabic tokens. The
problem takes another dimension, however, in the
case of propagating the POS-tags, because ?North?
is a NNP aligned with the determinant (DET) ?Al?
and the NNP ?$mAlyp?. Additional processing is
needed to handle this difference of tags on the two
2The claim is also valid for MD because it is the same type
of annotation.
sides.
3. In case of n-to-1 mapping, the TL side word is
simply assigned the class propagated from the RRL
side. For instance, if on the English side we have the
named person multi-word mention ?Ben Moussa?,
translated into the one-word mention ?????	JK. (bn-
mwsY) on the Arabic side, then projection consists
of simply assigning the person named tag to the Ara-
bic word.
However, in our research study, new challenges
arose because our RRL data are automatically an-
notated, which is different from what has been re-
ported in the research works we have mentioned be-
fore, i.e. (Yarowsky et al, 2001) and (Klementiev
and Roth, 2006), where gold annotated data were
used. In order to relax the impact of the noise intro-
duced by the English MD system, we :
1. use mention ?splits? to filter annotation errors:
We assume that when a sequence of tokens is tagged
as a mention on the RRL side, its TL counterpart
should be an uninterrupted sequence of tokens as
well. When the RRL MD system captures incor-
rectly the span of a mention, e.g. in the sentence
?Dona Karan international reputation of ...?, the
RRL MD system might mistakenly tag ?Dona Karan
international? as an organization mention instead of
tagging ?Dona Karan? as a person mention. It is pos-
sible to detect this type of errors on the TL side be-
cause ?dwnA kArAn? (Dona Karan) is distant from
?Al EAlmyp? (international), i.e. they do not form
an uninterrupted token sequence. We use this ?split?
in the mentions as information in order to not use
these mentions in the feature extraction step (see
Subsection 3.3).
2. do not use the projected mentions directly for
training: Instead, we use these tags as additional
features to our TL baseline model and allow our
MEMM classifier to weigh them according to their
relevance to each mention type.
3.3 Feature Extraction
At this point, the parallel corpus should be anno-
tated with mentions on both of its sides. Where
the RRL side is tagged using the English MD
system during the annotation step (c.f section 3.1)
while the TL side is annotated by the propagation
of these MD tags via the parallel corpus in the
projection step (c.f. section 3.2). In this third step,
the goal is to extract pertinent linguistic features
of the automatically tagged TL corpus to enhance
MD model in the TL. The explored features are as
follows:
995
1. Gazetteers: we group mentions by class in
different dictionaries. During both training and
decoding, when we encounter a token or a sequence
of tokens that is part of a dictionary, we fire its
corresponding class; the feature is fired only when
we find a complete match between sequence of
tokens in the text and in the dictionary.
2. Model-based features: it consists of building a
model on the automatically tagged TL side of the
parallel corpus. The output of this model is used
as a feature to enhance MD model in the target
language. However, it is also possible to use this
model to directly tag text in the TL. This would
be useful in cases where we do not have any TL
annotated data.
3. n-gram context features: it consists of using
the annotated corpus in the TL to collect n-gram
tokens surrounding a mention. We organize those
contexts by mention type and we use them to
tag tokens which appear in the same context
in both the training and decoding sets. These
tags will be used as additional feature in the
MD model. For instance, if we consider that
the person mention 	?
?k ?@Y? (SdAm Hsyn -
Sadam Husein) appears in the following sentence:
C ?A
	
? A?A
	
?
	
 ?

@Q

K

	?
?k ?@Y?
	
?

@ ??

@ hQ??
which might be transliterated as: SrH Ams An SdAm
Hsyn ytrAs nZAmA fA$lA and translated to English
as: declared yesterday that Sadam Husein governs
a failed system
the context n-grams that would be extracted are:
. Left n-grams: W?1=
	
?

@ (An - that),
W?2=
	
?

@ ??

@ (Ams An - yesterday that), etc.
. Right n-grams: W+1=?

@Q

K
 (ystrAs - governs),
W+2= A?A
	
?
	
 ?

@Q

K
 (ytrAs nZAmA - governs a sys-
tem), etc.
. Left and right n-grams: a joint of the two previ-
ous features, W?i and W+i.
For both training and test data we create a new
feature stream where we indicate that a token se-
quence is a mention if it appears in the same n-gram
context.
4. Head-word based features: it considers that
the lexical context in which the mention appeared
is the sequence of the parent sub-trees head words
in a parse-tree. For instance, if we consider the sen-
tence which we have used in the previous example,
the corresponding parse tree is shown in Figure 1.
The parent sub-tree heads of ?SdAm Hsyn? are
S
VPp3hhhhhhh

(((((((
SrHh3 NP
Ams
SBARp2hhhhhhhh
((((((((
Anh2 Sp1PPPP

NP
aaa
!!!
SdAm Hsyn
VP
Q
Q


ytrAsh1 NP
T
? ? ?
Figure 1: Parse tree
marked with hi on the tree. Similarly to the other
features, in both training and decoding sets, we
create a new feature stream where we tag those
token sequences which appear with the same n first
parent sub-tree head words as a person mention in
the annotated TL data.
5. Parser-based features: it attempts to use the
syntactic environment in which a mention might ap-
pear. In order to do so, for each mention in the tar-
get language corpus we consider only labels of the
parent non-terminals .We mark parent non-terminal
labels of ?SdAm Hsyn? on the tree with pi. Simi-
larly to the features described above, we create dur-
ing both training and test a new feature stream where
we indicate the token sequences which appear in the
same parent non-terminal labels.
Gazetteers and model-based features are the most
natural and expected kind of features that one would
extract from the automatically MD tagged version of
the TL text. Our motivation of using n-gram context
features, on one hand, and the head-word based and
parse-based features on the other is to: (i) contrast
the impact of local and global context features; and
(ii) experiment the possibility of employing both of
them jointly in order to test their complementarity.
4 The Target Language Mention Detection
System
- The Arabic language: In our research study, we
have intentionally chosen a TL which is differs from
English in its strategy in forming words and sen-
tences. By doing so, we are seeking to avoid ob-
taining results which are biased by the similarity of
the employed languages. For this reason, we have
996
chosen Arabic as a TL.
Due to its Semitic origins, the Arabic language is
both derivational, i.e. it uses a templatic strategy
to form a word, and highly inflectional, i.e. addi-
tional affixes might be added to a word in order to
obtain further meaning. Whereas the former char-
acteristic is common in most languages, the latter,
however, results in increasing sparseness in data
and consequently forming an obstacle to achieve a
high performance for most of the NLP tasks (Diab
et al, 2004; Benajiba et al, 2008; Zitouni et al,
2005; Zitouni and Florian, 2008). From a NLP
viewpoint, especially the supervised tasks such as
the one we are dealing with in this paper, this im-
plies that a huge amount of training data is nec-
essary in order to build a robust model. In our
study, to tackle the data sparseness problem, we have
performed the word segmentation. This segmenta-
tion pre-processing step consists of separating the
normal white-space delimited words into prefixes,
stems, and suffixes. Thus, from a modeling view-
point, the unit of analysis becomes the segments. We
use a technique similar to the one introduced in (Lee
et al, 2003) for segmentation with an accuracy of
98%.
- The Arabic MD system: Our Arabic MD system
employs the same technique presented in Section 2.
Compared to English MD model, Arabic MD sys-
tem has access to morphological information (Stem)
as we will explain next. Features used by the Arabic
MD system are divided in three categories:
1. Lexical: Similar to the lexical features used by
our English MD system (c.f. section 2);
2. Stem: This feature has been introduced in (Zitouni
et al, 2005) as stem n-grams spanning the current
stem; both preceding and following it. If the current
token xi is a stem, stem n-gram features contain the
previous n? 1 stems and the following n? 1 stems.
Stem n-gram features represent a lexical generaliza-
tion that reduce data sparseness;
3. Syntactic: it consists of the output of POS taggers
and the BPCs.
As we describe with more details in the experiments
section (see Section 6), once we have extracted the
new features from the parallel corpus, we contrast
their impact with the level of richness in features of
the TL MD system, i.e. we measure the impact of
each feature fi when the TL MD system uses: (i)
only lexical features; (ii) both lexical and stem fea-
tures; and (iii) lexical, stem and syntactic features.
5 Evaluation Data
Experiments are conducted on the Arabic ACE 2007
data. There are 379 Arabic documents and al-
most 98, 000 words. We find seven classes of men-
tions: Person (PER), Organization (ORG), Geo-
Political Entity (GPE), Location (LOC), Facility
(FAC), Vehicle (VEH) and Weapon (WEA). Since
the evaluation test sets are not publicly available,
we have split the publicly available training cor-
pus into an 85%/15% data split. We use 323 doc-
uments (80, 000 words) for training and 56 docu-
ments (18, 000 words) as a test set. This results
in 17, 634 mentions (7, 816 named, 8, 831 nominal
and 987 pronominal) for training and 3, 566 for test
(1, 673 named, 1, 682 nominal and 211 pronominal).
To facilitate future comparisons with work presented
here, and to simulate a realistic scenario, the splits
are created based on article dates: the test data is se-
lected as the latest 15% of the data in chronological
order, in each of the covered genres (newswire and
webblog). Performance on the ACE data is usually
evaluated using a special-purpose measure, i.e. the
ACE value metric. However, given that we are inter-
ested in the mention detection task only, we decided
to use the more intuitive and popular (un-weighted)
F-measure, the harmonic mean of precision and re-
call.
6 Experiments and Results
As we have stated earlier, our main goal is to in-
vestigate how an MD model of a TL might bene-
fit from additional information about the mentions
obtained by propagation from an RRL. In our re-
search study we have chosen Arabic as the TL and
English as the RRL. The English MD system we use
has access to a large set of information (Zitouni and
Florian, 2009) and has achieved a performance of
82.7F on ACE?07 data. In order to simulate differ-
ent levels of resource-richness for the TL, we have
employed four baseline systems which use different
feature-sets. Following we present these feature-sets
ranked from the resource-poorest to the resource-
richest one: 1- Lex.: lexical features; 2- Stem.:
Lex. + stem features; and 3- Syntac.: Stem. + syn-
tactic features.
For each of these baseline systems, we study the im-
pact of features extracted from the parallel corpus
(c.f. Section 3) separately. We report the following
results:
1- Base.: baseline system without the use of
parallel-data extracted features;
2- n? Lex.: Base. + n-gram context features;
997
Lex. Stem Syntac
Base. 74.14 74.47 75.53
n? Lex. 74.71 75.25 76.20
n?Head 74.63 75.29 75.93
n? Pars. 75.32 75.19 75.74
Gaz 74.90 74.79 75.66
Model 74.60 75.50 76.22
Comb. 76.01 76.74 77.18
Table 1: Obtained results when the features were ex-
tracted from a hand-aligned parallel corpus
3- n?Head: Base. + head-word based features;
4- n? Pars.: Base. + parser-related features;
5- Gaz.: Base. + automatically extracted
gazetteers from the parallel corpus;
6- Model: Base. + output of model trained on the
Arabic part of the parallel corpus;
7- Comb.: combination of all the above.
In the rest of the paper, to measure whether the im-
provement in performance of a system using fea-
tures from parallel data over baseline is statistically
significant or not, we use the stratified bootstrap re-
sampling significance test (Noreen, 1989) used in
the NER shared task of CoNLL-20023. We consider
results as statistically significant when p < 0.02.
6.1 Hand-aligned Data
In our first experiment-set, we use a hand-aligned
English-to-Arabic parallel corpus of approximately
one million words. After tagging the Arabic side
by projection we obtain 86.5K mentions. As we
have previously mentioned, in order to generate
the model-based feature, Model, we have trained a
model on the Arabic side of the parallel corpus. This
model achieved an F-measure of 57.7F. This shows
the performance that might be achieved when no hu-
man annotated data is available in the TL.
Results in Table 1 show that a significant improve-
ment is obtained when the TL is poor in resources;
for instance an improvement of ?1.9 points was
achieved when the TL used only lexical features.
The use of n ? Pars. features alone yielded 1.2
points of improvement. when the TL model uses a
rich feature-set, we still can obtain ?1.7 points im-
provement. When the TL baseline model employs
the Syntac feature-set, the greatest improvement
is obtained when we add the model-based feature.
Improvement obtained by the system using Comb.
3http://www.cnts.ua.ac.be/conll2002/ner/
features is statistically significant compared to the
baseline model. This system also outperforms sys-
tems using the new feature set separately across the
board. According to our error-analysis, the signif-
icant amount of Arabic mentions observed in the
parallel corpus, where many of them do not appear
in the training corpus, has significantly helped the
Lex., Stem and SyntacMD models to capture new
mentions and/or correct the type assigned. Some of
the relevant examples in our data are: (i) the facility
mention P?
	
??K. ?
	
?J.? (mbnY blfwr - Belvoir Build-
ing); (ii) the GPE mention ??K. A? (kAbwl - Kabul);
and (iii) the person mention 	?




J?J. ? @ (AlbEvyyn - the
Baathists). These mentions have only been tagged
correctly when we have added the new extracted fea-
tures to our model.
In other words, the error-analysis clearly points out
that one possible way to get further improvement is
to increase the parallel data in order to increase the
number of matches between (1) the number of men-
tions which are wrongly tagged by the TL MD model
and (2) the number of mentions in the TL side of the
parallel corpus. The second parameter can be, indi-
rectly, increased by increasing the size of the paral-
lel data. Getting 10 or 20 times more of parallel data
that is hand-aligned is expensive and requires sev-
eral months of human/hours work. For this reason
we opted for using an unsupervised approach by se-
lecting a parallel corpus that is automatically aligned
as we discuss in the next section.
6.2 Automatically-aligned Data
We have used for this experiment-set an Arabic-to-
English parallel data of 22 million words. The data
in this corpus is automatically aligned using a tech-
nique presented in (Ittycheriah and Roukos, 2005).
The alignment is one-to-many with a performance
around 87 F-measure.
Because we are dealing with a large amount of
data and the word alignment is done automatically,
meaning more noise, we have used the English MD
model confidence for additional filtering. Such fil-
tering consists in keeping, from the parallel corpus,
only sentences which have all tokens tagged with a
confidence greater than ?. In this paper, we use a
value of ? = 0.94, which results in a corpus of 17
million words. We notice that a lower value of ? re-
sults in a radical increase in noise. Because of space
limitation, we will report results only with this value
of ?.
Table 2 shows the obtained results for parallel-
998
Lex. Stem Syntac
Base. 74.14 74.47 75.53
n? Lex. 74.27 74.74 75.24
n?Head. 74.07 74.95 75.33
n? Pars. 75.62 75.22 76.02
Gaz 73.96 74.11 74.94
Model 74.87 75.12 75.76
Comb. 75.56 75.93 76.46
Table 2: Obtained results when the features were ex-
tracted from a automatically-aligned parallel corpus
data based features using the 17M subset. Differ-
ently from experiments using hand-aligned data, the
best results have been obtained when we have used
the parser-based feature, i.e. n ? Pars. On one
hand, the overall behavior is comparable to the one
obtained when using the 1M hand-aligned parallel
data (see Table 1), i.e. (i) the greatest improve-
ment has been obtained when the TL uses a poor
feature-set; and (ii) when the TL baseline model is
rich in resources, we still obtain 0.45 points absolute
improvement when using n ? Pars. On the other
hand, features extracted from automatically-aligned
data, in comparison with the ones extracted from the
hand aligned data, have helped the MD model to cor-
rect many of the TL baseline model false negatives.
This has been observed when the TL baseline sys-
tem uses a rich feature set as well. A side effect of
the noisy word alignment, however, was an increase
in the number of false positives. For instance, the
word H@Q?	?jJ?? (mstHDrAt - preparations) which
appeared in the following sentence:
?Q
	
k

@ H@Q?
	
?j

J??? hA???@ ?Y?
which might be transliterated as:
Edm AlsmAH lmstHDrAt AxrY
and translated to English as:
not to allow other preparations
has been tagged as an organization mention because
it has been mistakenly aligned, in the parallel cor-
pus, with the word ?A?, KO, in the sentence:

?J
?J
?j.

J? @ H@Q?
	
?j

J??

?? ?Q.??@ ?A?

??Q??
meaning:
The big cosmetics company KO.
In order to validate our results, we run our exper-
iments on a blind test-set. We have selected the
latest 5% of each genre of the hand-aligned data
Class Num. of mentions
FAC 285
GPE 2,145
LOC 239
ORG 1,135
PER 2,474
VEH 65
WEA 138
Table 3: Distribution over the classes of the blind test
mentions
Lex. Stem Syntac
Base. 74.26 73.54 73.61
n? Lex. 74.04 73.72 73.83
n?Head 74.14 73.64 73.83
n? Pars. 74.32 74.18 74.32
Gaz 71.49 72.13 73.39
Model 75.01 74.66 74.78
Table 4: Obtained results on blind test
and they have been manually annotated by a hu-
man. The blind test-set consists of 51,781 tokens of
which 6,481 are mentions. Table 3 shows the distri-
bution of these mentions over the different classes.
The results are shown in Table 4. These results con-
firm the conclusions we have deduced from the ones
previously presented in Table 2, i.e.: (i) the highest
improvement is obtained when the TL is resource-
scarce.
6.3 Combining Hand-aligned and
Automatically-aligned Data
Table 5 shows that combining both features
extracted from hand-aligned and automatically-
aligned corpora has led to better results. The im-
Lex. Stem Syntac
Base. 74.14 74.47 75.53
n? Lex. 74.60 75.08 75.58
n?Head 74.51 75.32 75.56
n? Pars. 75.46 75.90 76.22
Gaz 74.85 74.83 75.92
Model 74.83 75.59 75.40
Comb. 76.39 76.85 77.23
Table 5: Obtained results when the features were
extracted from both hand-aligned and automatically-
aligned parallel corpora
999
provement of using Comb. compared to baseline is
statistically significant. We notice again that when
the TL baseline MD model uses a richer feature set,
the obtained improvement from using RRL becomes
smaller. We also observed that automatically aligned
data helped capture most of the unseen mentions
whereas the hand-aligned features helped decrease
the number of false-alarms. It is important to notice
that when features Comb. is used with Stem base-
line model, the obtained F-measure (76.85) is 1.3
higher than the baseline model which uses lexical,
stem and syntactic features ? Syntac (75.53). The
type of errors which mostly occur and has not been
fixed neither by using hand-aligned data, automati-
cally aligned data nor the combination of both are
the nominal mentions whose class depends fully on
the context. For instance, the word
	
?
	
??? (mwZf -
employee) which was considered as O by the MD
model because it has not been seen in any of the par-
allel data in a context such as the following:
. . . 	?A? ?


Q???? @
	
?
	
???? @ ?? ?
	
?K
Q?

K
transliterated as:
tEryf $kl AlmwZf AlmSry ...
and translated as: ?defining the life of the Egyptian
employee ...?
7 Previous Works
Several research works, in different NLP tasks, have
shown that the use of an RRL to achieve a better
performance in a resource-challenged language
yields to successful results. In (Rogati et al, 2003),
authors used a statistical machine translation (MT)
system to build an Arabic stemmer. The obtained
stemmer has a performance of 87.5%. In (Ide et al,
2002), authors use the aligned versions of George
Orwell?s Nineteen Eighty-Four in seven languages
in order to determine sense distinctions which can
be used in the Word Sense Disambiguation (WSD)
task. They report that the automatically obtained
tags are at least as reliable as the one made by hu-
man annotators. Similarly, (Ng et al, 2003) report a
research study which uses an English-Chinese par-
allel corpus in order to extract sense-tagged training
data. In (Hwa et al, 2002), authors report promising
results of inducing Chinese dependency trees from
English. The obtained model outperformed the
baseline.
One of the significant differences between these
works and the one we present in this paper is that
instead of using the propagated annotation directly
as training data we use it as an additional feature and
thus allow the MEMM model to weigh each one of
them. By doing so, the model is able to distinguish
between the relevant and the irrelevant information
propagated from the RRL.
Authors in (Zitouni and Florian, 2008) attempt to
enhance an MD model of a foreign language by us-
ing an English MD system. They have used an MT
system to (i) translate the text to English; (ii) run the
English model on the translated text; (iii) and prop-
agate outcome to the original text. The approach
in (Zitouni and Florian, 2008) requires a MT system
that needs more effort and resources to build when
compared to a parallel corpus (used in our experi-
ments); not all institutions may have access to MT
and MD systems in plenty of language pairs.
8 Conclusions and Future Works
In this paper, we presented a novel approach that al-
lows to exploit the richness, in terms of resources, of
one language (English) to the benefit of a target lan-
guage (Arabic). We achieved successful results by
adopting a novel approach performing in three main
steps, namely: (i) Annotate the English side of an
English-to-Arabic parallel corpus automatically; (ii)
Project the obtained annotation from English to Ara-
bic via the parallel corpus; and (iii) Extract features
of different linguistic motivations of the automati-
cally tagged Arabic tokens. Thereafter, each of the
extracted features is used to bootstrap Arabic MD
system. We use different Arabic baseline MD mod-
els which employ different feature sets representing
different levels of richness in resources. We also use
both a one million word hand-aligned parallel cor-
pus and a 22 million word automatically aligned one
in order to study size vs. noise trade-off.
Results show that a statistically significant improve-
ment is always observed even when the Arabic base-
line MD model uses all the available resources.
When we use the hand-aligned parallel corpus, we
obtain up to 2.2 points improvement when the Ara-
bic MD model has access to very limited resources.
It decreases to 1.7 points when we use all the re-
sources we could gather for the Arabic language.
When no human-annotated data is available in the
TL, we show that we can obtain a performance of
57.6 using only mention propagation from RRL.
The results also show that a greater improvement
is achieved when using a small hand-aligned corpus
than using a 20 times bigger automatically aligned
data. However, in case both of them are available,
combining them leads to even higher results.
1000
References
Yassine Benajiba, Mona Diab, and Paolo Rosso. 2008.
Arabic named entity recognition using optimized fea-
ture sets. In Proc. of EMNLP?08, pages 284?293.
Stanley Chen and Ronald Rosenfeld. 2000. A survey of
smoothing techniques for ME models. IEEE Transac-
tion on Speech and Audio Processing.
Mona Diab, Kadri Hacioglu, and Dan Jurafsky. 2004.
Automatic tagging of arabic text: from raw text to base
phrase chunks. In Proc. of HLT/NAACL?04.
Joshua Goodman. 2002. Sequential conditional general-
ized iterative scaling. In Proceedings of ACL?02.
Rebecca Hwa, Philip Resnik, and Amy Weinberg. 2002.
Breaking the resource bottleneck for multilingual pars-
ing. In Proceedings of LREC.
Nancy Ide, Tomaz Erjavec, and Dan Tufis. 2002. Sense
discrimination with parallel corpora. In Proceedings
of the SIGLEX/SENSEVAL Workshop on Word Sense
Disambiguation, pages 54?60.
Abe Ittycheriah and Salim Roukos. 2005. A maximum
entropy word aligner for arabic-english machine trans-
lation. In Proceedings of HLT/EMNLP?05, pages 89?
96.
Alexandre Klementiev and Dan Roth. 2006. Weakly
supervised named entity transliteration and discovery
from multilingual comparable corpora. In Proceed-
ings of ACL?06, pages 817?824, Sydney, Australia.
Association for Computational Linguistics.
Young-Suk Lee, Kishore Papineni, Salim Roukos, Os-
sama Emam, and Hany Hassan. 2003. Language
model based Arabic word segmentation. In Proc. of
the ACL?03, pages 399?406.
Hwee Tou Ng, Bin Wang, and Yee Seng Chan. 2003. Ex-
ploiting parallel texts for word sense disambiguation:
An empirical study. In Proceedings of ACL?03, pages
455?462.
Eric W. Noreen. 1989. Computer-Intensive Methods for
Testing Hypotheses. John Wiley Sons.
Lance Ramshaw and Mitchell Marcus. 1999. Text
chunking using transformation-based learning. In
S. Armstrong, K.W. Church, P. Isabelle, S. Manzi,
E. Tzoukermann, and D. Yarowsky, editors, Natu-
ral Language Processing Using Very Large Corpora,
pages 157?176. Kluwer.
Monica Rogati, Scott McCarley, and Yiming Yang. 2003.
Unsupervised learning of arabic stemming using a par-
allel corpus. In Proceedings of ACL?03, pages 391?
398.
Eric. F. Tjong Kim Sang. 2002. Introduction to the conll-
2002 shared task: Language-independent named entity
recognition. In Proceedings of CoNLL-2002, pages
155?158. Taipei, Taiwan.
David Yarowsky, Grace Ngai, and Richard Wicentowski.
2001. Inducing multilingual text analysis tools via ro-
bust projection across aligned corpora. In Proceedings
of HLT?01, pages 1?8.
Imed Zitouni and Radu Florian. 2008. Mention detection
crossing the language barrier. In Proc. of the Confer-
ence on Empirical Methods in Natural Language Pro-
cessing (EMNLP), Honolulu, Hawaii, October.
Imed Zitouni and Radu Florian. 2009. Cross-language
information propagation for arabic mention detection.
ACM Transactions on Asian Language Information
Processing (TALIP), 8(4):1?21.
Imed Zitouni, Jeff Sorensen, Xiaoqiang Luo, and Radu
Florian. 2005. The impact of morphological stem-
ming on arabic mention detection and coreference res-
olution. In Proc. of the ACL Workshop on Computa-
tional Approaches to Semitic Languages, pages 63?70.
1001
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 709?712,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Arabic Mention Detection: Toward Better Unit of Analysis
Yassine Benajiba
Center for Computational Learning Systems
Columbia University
ybenajiba@ccls.columbia.edu
Imed Zitouni
IBM T. J. Watson Research Center
izitouni@us.ibm.com
Abstract
We investigate in this paper the adequate unit
of analysis for Arabic Mention Detection. We
experiment different segmentation schemes
with various feature-sets. Results show that
when limited resources are available, models
built on morphologically segmented data out-
perform other models by up to 4F points. On
the other hand, when more resources extracted
from morphologically segmented data become
available, models built with Arabic TreeBank
style segmentation yield to better results. We
also show additional improvement by combin-
ing different segmentation schemes.
1 Introduction
This paper addresses an important and basic task of
information extraction: Mention Detection (MD)1:
the identification and classification of textual refer-
ences to objects/abstractions (i.e., mentions). These
mentions can be either named (e.g. Mohammed,
John), nominal (city, president) or pronominal (e.g.
he, she). For instance, in the sentence ?President
Obama said he will visit ...? there are three men-
tions: President, Obama and he. This is similar
to the Named Entity Recognition (NER) task with
the additional twist of also identifying nominal and
pronominal mentions. We formulate the mention de-
tection problem as a classification problem, by as-
signing to each token in the text a label, indicating
whether it starts a specific mention, is inside a spe-
cific mention, or is outside all mentions. The se-
lection of the unit of analysis is an important step
toward a better classification. When processing lan-
guages, such as English, using the word itself as the
1We adopt here the ACE nomenclature:
http://www.nist.gov/speech/tests/ace/index.html
unit of analysis (after separating punctuations) leads
to a good performance (Florian et al, 2004). For
other languages, such as Chinese, character is con-
sidered as the adequate unit of analysis (Jing et al,
2003). In this paper, we investigate different seg-
mentation schemes in order to define the best unit of
analysis for Arabic MD. Arabic adopts a very com-
plex morphology, i.e. each word is composed of zero
or more prefixes, one stem and zero or more suffixes.
Consequently, the Arabic data is sparser than other
languages, such as English, and it is necessary to
?segment? the words into several units of analysis in
order to achieve a good performance.
(Zitouni et al, 2005) used Arabic morphologically
segmented data and claimed to have very competi-
tive results in ACE 2003 and ACE 2004 data. On the
other hand, (Benajiba et al, 2008) report good re-
sults for Arabic NER on ACE 2003, 2004 and 2005
data using Arabic TreeBank (ATB) segmentation. In
all published works, authors do not mention a spe-
cific motivation for the segmentation scheme they
have adopted. Only for the Machine Translation
task, (Habash and Sadat, 2006) report several results
using different Arabic segmentation schemes. They
report that the best results were obtained when the
ATB-like segmentation was used. We explore here
the four known and linguistically-motivated sorts of
segmentation: punctuation separation, ATB, mor-
phological and character-level segmentations. To
our knowledge, this is the first paper which inves-
tigates different segmentation schemes to define the
unit of analysis which best fits Arabic MD.
2 Arabic Segmentation Schemes
Character-level Segmentation: considers that each
character is a separate token.
Morphological Segmentation : aims at segmenting
709
all affixes of a word. The morphological segmenta-
tion for the word I.

J??? @? (wAlmktb ? and the of-
fice)2 could be: ?I.

J??+ ?@+ ?? (w +Al +mktb).
Arabic TreeBank (ATB) segmentation : This seg-
mentation considers splitting the word into affixes
only if it projects an independent phrasal constituent
in the parse tree. As an example, in the word shown
above I.

J??? @?, the phrasal independent constituents
are: the conjunction ? (w ? and) and the noun
I.

J??? @ (Almktb ? the office). The morphological
segmentation of this word would lead to the follow-
ing parse tree:
S
HH
CONJ
w
NP
b
b
"
"
Al +mktb
Since the ?@ (Al, the definite article) is not an in-
dependent constituent, it is not considered for ATB
segmentation. Hence, for I.

J??? @?, the ATB segmen-
tation would be I.

J??? @+ ? (w +Almktb).
Punctuation separation : it consists of separating
the punctuation marks from the word.
Both ATB and morphological segmentation sys-
tems are based on weighted finite state transducers
(WFST). The decoder implements a general Bell-
man dynamic programming search for the best path
on a lattice of segmentation hypotheses that match
the input characters (Benajiba and Zitouni, 2009).
ATB and morphological segmentation systems have
a performance of 99.4 and 98.1 F-measure respec-
tively on ATB data.
The unit of analysis when doing classification de-
pends on the used segmentation. When using the
punctuation separation or character-based segmen-
tations, the unit of analysis is the word itself (with-
out the punctuation marks attached) or the character,
respectively. The ATB and morphological segmen-
tations are language specific and are based on dif-
ferent linguistic viewpoint. When using one of these
two segmentation schemes, the unit of analysis is the
morph (i.e. prefix, stem or suffix). Our goal in this
paper is to find the unit of analysis that fits best Ara-
bic MD.
2Throughout the paper, for each Arabic example we show
between parenthesis its transliteration and English translation
separated by ???.
3 Mention Detection System
As explained earlier, we consider the MD task as a
sequence classification problem where the class we
predict for each unit of analysis (i.e., token) is the
type of the entity which it refers to. We chose the
maximum entropy (MaxEnt) classifier that can in-
tegrate arbitrary types of information and make a
classification decision by aggregating all informa-
tion available for a given classification. For more
details about the system architecture, reader may re-
fer to (Zitouni et al, 2009). The features used in our
MD system can be divided into four categories:
Lexical Features: n-grams spanning the current to-
ken; both preceding and following it. A number of
n equal to 3 turned out to be a good choice.
Stem n-gram Features: stem trigram spanning the
current stem; both preceding and following it (Zi-
touni et al, 2005).
Syntactic Features: POS tags and shallow parsing
information in a ?2 window.
Features From Other Classifiers: outputs of MD
and NER taggers trained on other data-sets different
from the one we used here. They may identify types
of mentions different from the mentions of interest
in our task. For instance, such a tagger may identify
dates or occupation references (not used in our task),
among other types. Our hypothesis is that combin-
ing classifiers from diverse sources will boost per-
formance by injecting complementary information
into the mention detection models. We also use the
two previously assigned classification tags as addi-
tional feature.
4 Data
Experiments are conducted on the Arabic ACE 2007
data. Since the evaluation tests set are not publicly
available, we have split the publicly available train-
ing corpus into an 85%/15% data split. We use 323
documents (80, 000 words, 17, 634 mentions) for
training and 56 documents (18, 000 words, 3, 566
mentions) as a test set. We are interested in 7 types
of mentions: facility, Geo-Political Entity (GPE),
location, organization, person, vehicle and weapon.
We segmented the training and test set with four dif-
ferent styles building the following corpora:
Words: a corpus which is the result of running
punctuation separation;
ATBs: a corpus obtained by running punctuation
separation and ATB segmentation;
Mophs: a corpus where we conduct punctuation
separation and morphological segmentation;
Chars: a corpus where the original text is separated
710
into a sequence of characters.
When building MD systems on Words, ATBs,
Morphs and Chars, the unit of analysis is the word,
the ATB token, the morph and the character, respec-
tively.
5 Experiments
We show in this section the experimental results
when using Arabic MD system with different seg-
mentation schemes and different feature sets. We
explore in this paper four categories of features (c.f.
Section 3):
Lexf : lexical features;
Stemf : Lexf + morphological features;
Syntf : Stemf + syntactic features;
Semf : Syntf + output of other MD classifiers.
Lexf and Stemf features are directly extracted
from the appropriate corpus based on the used seg-
mentation style. This is different for Semf : we first
run classifiers on the morphologically segmented
data. Thereafter, we project those labels to other
corpora. This is because, we use classifiers initially
trained on morphologically segmented data such as
ACE 2003, 2004 and 2005 data. In such data, two
morphs belonging to the same word or ATB token
may have 2 different mentions. During transfer, a
token will have the label of the corresponding stem
in the morphologically segmented data. One moti-
vation to not re-train classifiers on each corpus sep-
arately is to be able to extract Semf features from
classifiers with similar performance.
Table 1: Results in terms of F-measure per feature-set and
segmentation scheme
Lexf Stemf Syntf Semf
Words 66.4 66.6 69.0 77.1
ATBs 70.1 69.8 72.1 79.0
Morphs 74.1 74.5 75.5 78.3
Chars 22.3 22.4 22.5 22.6
Results in Table 1 show that classifiers built on
ATBs and Morphs have shown to perform better
than classifiers trained on data with other segmenta-
tion styles. When the system uses character as the
unit of analysis, performance is poor. This is be-
cause the token itself becomes insignificant informa-
tion to the classifier. On the other hand, when only
punctuation separation is performed (Words), the
data is significantly sparse and the obtained results
achieves high F-measure (77.1) only when outputs
of other classifiers are used. As mentioned earlier,
classifiers used to extract those features are trained
on Morphs (less sparse), which explains their re-
markable positive impact since they resolve part of
the data sparseness problem in Words. When us-
ing full morphological segmentation, the data is less
sparse, which leads to less Out-Of-Vocabulary to-
kens (OOVs): the number of OOVs in the Morphs
data is 1,518 whereas it is 2,464 in the ATBs.
As an example, the word

?
	
JJ
?Q?@ (Alrhynp ? the
hostage), which is person mention in the training
data. This word is kept unchanged after ATB seg-
mentation and is segmented to ?

?+
	?
?P
+ ?@? (Al+
rhyn +p) in Morphs. In the development set the
same word appears in its dual form without defi-
nite article, i.e. 	?


J
	
J
?P. This word is unchanged in
ATBs and is segmented to ? 	?K
+
H+
	?
?P? (rhyn
+p +yn) in Morphs. For the model built on ATBs,
this word is an OOV, whereas for the model built
on Morphs the stem has been seen as part of a per-
son mention and consequently has a better chance
to tag it correctly. These phenomena are frequent,
which make the classifier trained on Morphs more
robust for such cases. Also, we observed that mod-
els trained on ATBs perform better on long span
mentions. We think this is because a model trained
on ATBs has access to larger context. One may
argue that a similar behavior of the model built on
the Morphs might be obtained if we use a wider
context window than the one used for ATBs in or-
der to have similar contextual information. In or-
der to confirm this statement, we have carried out a
set of experiments using all features over Morphs
data for a context window up to ?5/ + 5, the ob-
tained results show no improvement. Similar behav-
ior is observed when looking to results on identi-
fied named (Nam.), nominal (Nom.) and pronomi-
nal (Pro.) mentions on ATBs and Morphs (c.f. Ta-
ble 2); we remind the reader that NER is about rec-
ognizing named mentions. When limited resources
are available (e.g. Lexf , Stemf or Syntf ), we be-
lieve that it is more effective to morphologically seg-
ment the text (Morphs) as a pre-processing step.
The use of morph as a unit of analysis reduces the
data sparseness issue and at the same time allows
better context handling when compared to character.
On the other hand, when a larger set of resources
are available (e.g., Semf ), the use of the ATB to-
ken as a unit of analysis combined with morph-
based features leads to better performance (79.0 vs.
78.3 on Morphs). This is because (1) classifiers
trained on ATBs handle better the context and (2)
the use of morph-based features (output of classi-
711
fiers trained on morphologically segmented data) re-
moves some of the data sparseness from which clas-
sifiers trained on ATBs suffer. The obtained im-
provement in performance is statistically significant
when using the stratified bootstrap re-sampling sig-
nificance test (Noreen, 1989). We consider results
as statistically significant when p < 0.02, which is
the case in this paper. For an accurate MD system,
we think it is appropriate to benefit from ATBs to-
kens and Morphs. We investigate in the following
the combination of these two segmentation styles.
Table 2: Performance in terms of F-measure per level on
ATBs and Morphs
Seg. Lexf Stemf Syntf Semf
Nam.
ATBs 68.2 69.0 72.8 79.1
Morphs 73.4 73.8 75.3 78.7
Nom.
ATBs 65.6 64.6 66.9 75.8
Morphs 71.7 72.2 72.9 75.4
Pro.
ATBs 60.7 60.1 59.9 66.3
Morphs 63.0 67.2 65.7 65.1
5.1 Combination of ATB and Morph
We trained a model on ATBs that uses output of the
model trained on Morphs as additional information
(M2Af feature). We proceed similarly by training a
model on Morphs using output of the model trained
on ATBs (A2Mf feature). We have obtained the
features by a 15-way round-robin. Table 3 shows
the obtained results.
Table 3: Results in terms of F-measure of the combina-
tion experiments
Lexf Stemf Syntf Semf
ATBs 70.1 69.8 72.1 79.0
ATBs+M2Af 70.7 70.8 73.1 79.1
Morphs 74.1 74.5 75.5 78.3
Morphs+A2Mf 74.9 75.2 75.4 78.6
Results show a significant improvement for mod-
els that are trained on ATBs using information from
Morphs in addition to Lexf , Stemf and Syntf
features. This again confirms our claim that the use
of features from morphologically segmented text re-
duces the data sparseness and consequently leads to
better performance. For Semf features, only a 0.1
F-measure points have been gained. This is because
we are already using output of classifiers trained
on morphologically segmented data, which resolve
some of the data sparseness issue. The Morphs
side shows that the obtained performance when the
ATBs output is employed together with the Stemf
(75.2) is only 0.3 points below the performance of
the system using Syntf (75.5).
6 Conclusions
We have shown a comparative study aiming at defin-
ing the adequate unit of analysis for Arabic MD.
We conducted our study using four segmentation
schemes with four different feature-sets. Results
show that when only limited resources are available,
using morphological segmentation leads to the best
results. On the other hand, model trained on ATB
segmented data become more powerful and effective
when data sparseness is reduced by the use of other
classifier outputs trained on morphologically seg-
mented data. More improvement is obtained when
both segmentation styles are combined.
References
Y. Benajiba and I. Zitouni. 2009. Morphology-
based segmentation combination for arabic men-
tion detection. Special Issue on Arabic Nat-
ural Language Processing of ACM Transac-
tions on Asian Language Information Processing
(TALIP), 8(4).
Y. Benajiba, M. Diab, and P. Rosso. 2008. Arabic
named entity recognition using optimized feature
sets. In Proc. of EMNLP?08, pages 284?293.
R. Florian, H. Hassan, A. Ittycheriah, H. Jing,
N. Kambhatla, X. Luo, N. Nicolov, and
S. Roukos. 2004. A statistical model for
multilingual entity detection and tracking. In
Proc.eedings of HLT-NAACL?04, pages 1?8.
N. Habash and F. Sadat. 2006. Combination of ara-
bic preprocessing schemes for statistical machine
translation. In Proceedings of ACL?06, pages 1?8.
H. Jing, R. Florian, X. Luo, T. Zhang, and A. Itty-
cheriah. 2003. HowtogetaChineseName(Entity):
Segmentation and combination issues. In Pro-
ceedings of EMNLP?03, pages 200?207.
E. W. Noreen. 1989. Computer-Intensive Methods
for Testing Hypotheses. John Wiley Sons.
I. Zitouni, J. Sorensen, X. Luo, and R. Florian.
2005. The impact of morphological stemming on
arabic mention detection and coreference resolu-
tion. In Proc. of the ACL Workshop on Compu-
tational Approaches to Semitic Languages, pages
63?70.
I. Zitouni, X. Luo, and R. Florian. 2009. A cascaded
approach to mention detection and chaining in
arabic. IEEE Transactions on Audio, Speech and
Language Processing, 17:935?944.
712
Proceedings of the ACL 2010 Conference Short Papers, pages 281?285,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Arabic Named Entity Recognition:
Using Features Extracted from Noisy Data
Yassine Benajiba1 Imed Zitouni2 Mona Diab1 Paolo Rosso3
1 Center for Computational Learning Systems, Columbia University
2 IBM T.J. Watson Research Center, Yorktown Heights
3 Natural Language Engineering Lab. - ELiRF, Universidad Polite?cnica de Valencia
{ybenajiba,mdiab}@ccls.columbia.edu, izitouni@us.ibm.com, prosso@dsic.upv.es
Abstract
Building an accurate Named Entity
Recognition (NER) system for languages
with complex morphology is a challeng-
ing task. In this paper, we present research
that explores the feature space using both
gold and bootstrapped noisy features to
build an improved highly accurate Arabic
NER system. We bootstrap noisy features
by projection from an Arabic-English par-
allel corpus that is automatically tagged
with a baseline NER system. The feature
space covers lexical, morphological, and
syntactic features. The proposed approach
yields an improvement of up to 1.64
F-measure (absolute).
1 Introduction
Named Entity Recognition (NER) has earned an
important place in Natural Language Processing
(NLP) as an enabling process for other tasks.
When explicitly taken into account, research
shows that it helps such applications achieve bet-
ter performance levels (Babych and Hartley, 2003;
Thompson and Dozier, 1997). NER is defined as
the computational identification and classification
of Named Entities (NEs) in running text. For in-
stance, consider the following text:
Barack Obama is visiting the Middle East.
A NER system should be able to identify Barack
Obama and Middle East as NEs and classify them
as Person (PER) and Geo-Political Entity (GPE),
respectively. The class-set used to tag NEs may
vary according to user needs. In this research,
we adopt the Automatic Content Extraction (ACE)
2007 nomenclature1.
According to (Nadeau and Sekine, 2007), opti-
mization of the feature set is the key component in
enhancing the performance of a global NER sys-
tem. In this paper we investigate the possibil-
ity of building a high performance Arabic NER
system by using a large space of available feature
sets that go beyond the explored shallow feature
sets used to date in the literature for Arabic NER.
1http://www.nist.gov/speech/tests/ace/index.htm
Given current state-of-the-art syntactic processing
of Arabic text and the relative small size of man-
ually annotated Arabic NER data, we set out to
explore a main concrete research goal: to fully ex-
ploit the level of advancement in Arabic lexical
and syntactic processing to explore deeper linguis-
tic features for the NER task. Realizing that the
gold data available for NER is quite limited in size
especially given the diverse genres in the set, we
devise a method to bootstrap additional instances
for the new features of interest from noisily NER
tagged Arabic data.
2 Our Approach
We use our state-of-the-art NER system described
in (Benajiba et al, 2008) as our baseline sys-
tem (BASE) since it yields, to our knowledge, the
best performance for Arabic NER . BASE em-
ploys Support Vector Machines (SVMs) and Con-
ditional Random Fields (CRFs) as Machine Learn-
ing (ML) approaches. BASE uses lexical, syn-
tactic and morphological features extracted using
highly accurate automatic Arabic POS-taggers.
BASE employs a multi-classifier approach where
each classifier is tagging a NE class separately.
The feature selection is performed by using an in-
cremental approach selecting the top n features
(the features are ranked according to their individ-
ual impact) at each iteration and keeping the set
that yields the best results. In case of conflict - a
word is classified with more than one class/tag si-
multaneously - the global NER system selects the
output of the classifier with the highest precision.
The following is the feature set used in (Bena-
jiba et al, 2008) and accordingly in the BASE sys-
tem. 1. Context: a?/+1 token window; 2. Lex-
ical: character n ? grams where n ranges from
1? 3; 3. Gazetteers: automatically harvested and
manually cleaned Person NE class (PER), Geopo-
litical Entity NE class (GPE), and Organization
NE class (ORG) lexica; 4. POS-tag and Base
Phrase Chunk (BPC): automatically tagged us-
ing AMIRA (Diab et al, 2007) which yields F-
measures for both tasks in the high 90?s; 5. Mor-
phological features: automatically tagged using
the Morphological Analysis and Disambiguation
for Arabic (MADA) tool to extract information
about gender, number, person, definiteness and as-
281
pect for each word (Habash and Rambow, 2005);
6. Capitalization: derived as a side effect from
running MADA. MADA chooses a specific mor-
phological analysis given the context of a given
word. As part of the morphological information
available in the underlying lexicon that MADA ex-
ploits. As part of the information present, the un-
derlying lexicon has an English gloss associated
with each entry. More often than not, if the word
is a NE in Arabic then the gloss will also be a NE
in English and hence capitalized.
We devise an extended Arabic NER system (EX-
TENDED) that uses the same architecture as
BASE but employs additional features to those in
BASE. EXTENDED defines new additional syn-
tagmatic features.
We specifically investigate the space of the sur-
rounding context for the NEs. We explore gener-
alizations over the kinds of words that occur with
NEs and the syntactic relations NEs engage in. We
use an off-the-shelf Arabic syntactic parser. State-
of-the-art for Arabic syntactic parsing for the most
common genre (with the most training data) of
Arabic data, newswire, is in the low 80%s. Hence,
we acknowledge that some of the derived syntactic
features will be noisy.
Similar to all supervised ML problems, it is de-
sirable to have sufficient training data for the rele-
vant phenomena. The size of the manually anno-
tated gold data typically used for training Arabic
NER systems poses a significant challenge for ro-
bustly exploring deeper syntactic and lexical fea-
tures. Accordingly, we bootstrap more NE tagged
data via projection over Arabic-English parallel
data. The role of this data is simply to give us more
instances of the newly defined features (namely
the syntagmatic features) in the EXTENDED sys-
tem as well as more instances for the Gazetteers
and Context features defined in BASE. It is worth
noting that we do not use the bootstrapped NE
tagged data directly as training data with the gold
data.
2.1 Syntagmatic Features
For deriving our deeper linguistic features, we
parse the Arabic sentences that contain an NE. For
each of the NEs, we extract a number of features
described as follows:
- Syntactic head-word (SHW): The idea here
is to look for a broader relevant context.
Whereas the feature lexical n-gram context fea-
ture used in BASE, and hence here for EX-
TENDED, considers the linearly adjacent neigh-
boring words of a NE, SHW uses a parse tree
to look at farther, yet related, words. For
instance, in the Arabic phrase ?SrH Ams An
Figure 1: Example for the head word and syntactic
environment feature
bArAk AwbAma ytrAs?, which means ?de-
clared yesterday that Barack Obama governs
...?, glossed ?SrH/declared Ams/yesterday An/that
bArAk/Barack AwbAmA/Obama ytrAs/governs
...?, is parsed in Figure 1. According to the phrase
structure parse, the first parent sub-tree headword
of the NE ?bArAk AwbAmA? is the verb ?ytrAs?
(governs), the second one is ?An? (that) and the
third one is the verb ?SrH? (declared). This exam-
ple illustrates that the word ?Ams? is ignored for
this feature set since it is not a syntactic head. This
is a lexicalized feature.
- Syntactic Environment (SE): This follows in the
same spirit as SHW, but expands the idea in that
it looks at the parent non-terminal instead of the
parent head word, hence it is not a lexicalized fea-
ture. The goal being to use a more abstract repre-
sentation level of the context in which a NE ap-
pears. For instance, for the same example pre-
sented in Figure 1, the first, second, and third non-
terminal parents of the NE ?bArAk AwbAmA? are
?S?, ?SBAR? and ?VP?, respectively.
In our experiments we use the Bikel implementa-
tion (Bikel, 2004) of the Collins parser (Collins,
1999) which is freely available on the web2. It is a
head-driven CFG-style parser trained to parse En-
glish, Arabic, and Chinese.
2.2 Bootstrapping Noisy Arabic NER Data
Extracting the syntagmatic features from the
training data yields relatively small number of
instances. Hence the need for additional tagged
data. The new Arabic NER tagged data is derived
via projection exploiting parallel Arabic English
data. The process depends on the availability
of two key components: a large Arabic English
parallel corpus that is sentence and word aligned,
and a robust high performing English NER
system. The process is as follows. We NE tag the
2http://www.cis.upenn.edu/?dbikel/software.html#stat-
parser
282
English side of the parallel corpus. We project
the automatically tagged NER tags from the
English side to the Arabic side of the parallel
corpus. In our case, we have access to a large
manually aligned parallel corpus, therefore the
NER projection is direct. However, the English
side of the parallel corpus is not NER tagged,
hence we use an off-the-shelf competitive robust
automatic English NER system which has a
published performance of 92% (Zitouni and
Florian, 2009). The result of these two processes
is a large Arabic NER, albeit noisy, tagged data
set. As mentioned earlier this data is used only
for deriving additional instances for training
for the syntagmatic features and for the context
and gazetteer features.3 Given this additional
source of data, we changed the lexical features
extracted from the BASE to the EXTENDED. We
added two other lexical features: CBG and NGC,
described as follows: - Class Based Gazetteers
(CBG): This feature focuses on the surface form
of the NEs. We group the NEs encountered on the
Arabic side of the parallel corpus by class as they
are found in different dictionaries. The difference
between this feature and that in BASE is that the
Gazetteers are not restricted to Wikipedia sources.
- N-gram context (NGC): Here we disregard
the surface form of the NE, instead we focus on its
lexical context. For each n, where n varies from 1
to 3, we compile a list of the ?n, +n, and ?/+ n
words surrounding the NE. Similar to the CBG
feature, these lists are also separated by NE class.
It is worth highlighting that the NCG feature is
different from the Context feature in BASE in
that the window size is different +/ ? 1 ? 3 for
EXTENDED versus +/? 1 for BASE.
3 Experiments and Results
3.1 Gold Data for training and evaluation
We use the standard sets of ACE 2003, ACE
2004 and ACE 2005.4 The ACE data is annotated
for many tasks: Entity Detection and Tracking
(EDT), Relation Detection and Recognition
(RDR), Event Detection and Recognition (EDR).
All the data sets comprise Broadcast News
(BN) and Newswire (NW) genres. ACE 2004
includes an additional NW data set from the
Arabic TreeBank (ATB). ACE 2005 includes
a different genre of Weblogs (WL). The NE
classes adopted in the annotation of the ACE
2003 data are: Person (PER), Geo Political Entity
(GPE), Organization (ORG) and Facility (FAC).
3Therefore, we did not do the full feature extraction for
the other features described in BASE for this data.
4http://www.nist.gov/speech/tests/ace/
Additionally for the ACE 2004 and 2005 data, two
NE classes are added to the ACE 2003 tag-set:
Vehicles (e.g. Rotterdam Ship) and Weapons (e.g.
Kalashnikof). We use the same split for train, de-
velopment, and test used in (Benajiba et al, 2008).
3.2 Parallel Data
Most of the hand-aligned Arabic-English parallel
data used in our experiments is from the Language
Data Consortium (LDC).5. Another set of the par-
allel data is annotated in-house by professional an-
notators. The corpus has texts of five different gen-
res, namely: newswire, news groups, broadcast
news, broadcast conversation and weblogs corre-
sponding to the data genres in the ACE gold data.
The Arabic side of the parallel corpus contains
941,282 tokens. After projecting the NE tags from
the English side to the Arabic side of the paral-
lel corpus, we obtain a total of 57,290 Arabic NE
instances. Table 1 shows the number of NEs for
each class.
Class Number of NEs Class Number of NEs
FAC 998 PER 17,964
LOC 27,651 VEH 85
ORG 10,572 WEA 20
Table 1: Number of NEs per class in the Arabic
side of the parallel corpus
3.3 Individual Feature Impact
Across the board, all the features yield improved
performance. The highest obtained result is ob-
served where the first non-terminal parent is used
as a feature, a Syntactic Environment (SE) fea-
ture, yielding an improvement of up to 4 points
over the baseline. We experiment with different
sizes for the SE, i.e. taking the first parent versus
adding neighboring non-terminal parents. We note
that even though we observe an overall increase
in performance, considering both the {first, sec-
ond} or the {first, second, and third} non-terminal
parents decreases performance by 0.5 and 1.5 F-
measure points, respectively, compared to consid-
ering the first parent information alone. The head
word features, SHW, show a higher positive im-
pact than the lexical context feature, NGC. Finally,
the Gazetteer feature, CBG, impact is comparable
to the obtained improvement of the lexical context
feature.
3.4 Feature Combination Experiments
Table 2 illustrates the final results. It shows for
each data set and each genre the F-measure ob-
tained using the best feature set and ML approach.
It shows results for both the dev and test data us-
ing the optimal number of features selected from
5All the LDC data are publicly available
283
ACE 2003 ACE 2004 ACE 2005
BN NW BN NW ATB BN NW WL
FreqBaseline 73.74 67.61 62.17 51.67 62.94 70.18 57.17 27.66
dev
All-Synt. 83.41 79.11 76.90 72.90 74.82 81.42 76.07 54.49
All 83.93 79.72 78.54 72.80 74.97 81.82 75.92 55.65
test
All-Synt. 83.50 78.90 76.70 72.40 73.50 81.31 75.30 57.30
All 84.32 79.4 78.12 72.13 74.54 81.73 75.67 58.11
Table 2: Final Results obtained with selected features contrasted against all features combined
the all the features except the syntagmatic ones
(All-Synt.) contrasted against the system in-
cluding the semantic features, i.e. All the features,
per class All . The baseline results, FreqBaseline,
assigns a test token the most frequent tag observed
for it in the gold training data, if a test token is
not observed in the training data, it is assigned the
most frequent tag which is the O tag.
4 Results Discussion
Individual feature impact results show that the
syntagmatic features are helpful for most of the
data sets. The highest improvements are obtained
for the 2003 BN and 2005 WL data-sets. The im-
provement varies significantly from one data-set
to another because it highly depends on the num-
ber of NEs which the model has not been able to
capture using the contextual, lexical, syntactic and
morphological features.
Impact of the features extracted from the paral-
lel corpus per class: The syntagmatic features
have varied in their influence on the different NE
classes. Generally, the LOC and PER classes ben-
efitted more from the head word features, SHW),
than the other classes. On the other hand for the
syntactic environment feature (SE), the PER class
seemed not to benefit much from the presence of
this feature. Weblogs: Our results show that the
random contexts in which the NEs tend to ap-
pear in the WL documents stand against obtain-
ing a significant improvement. Consequently, the
features which use a more global context (syntac-
tic environment, SE, and head word, SHW, fea-
tures) have helped obtain better results than the
ones which we have obtained using local context
namely CBG and NGC.
5 Related Work
Projecting explicit linguistic tags from another
language via parallel corpora has been widely used
in the NLP tasks and has proved to contribute sig-
nificantly to achieving better performance. Dif-
ferent research works report positive results when
using this technique to enhance WSD (Diab and
Resnik, 2002; Ng et al, 2003). In the latter two
works, they augment training data from parallel
data for training supervised systems. In (Diab,
2004), the author uses projections from English
into Arabic to bootstrap a sense tagging system
for Arabic as well as a seed Arabic WordNet
through projection. In (Hwa et al, 2002), the
authors report promising results of inducing Chi-
nese dependency trees from English. The ob-
tained model outperformed the baseline. More re-
cently, in (Chen and Ji, 2009), the authors report
their comparative study between monolingual and
cross-lingual bootstrapping. Finally, in Mention
Detection (MD), a task which includes NER and
adds the identification and classification of nom-
inal and pronominal mentions, (Zitouni and Flo-
rian, 2008) show the impact of using a MT sys-
tem to enhance the performance of an Arabic MD
model. The authors report an improvement of up
to 1.6F when the baseline system uses lexical fea-
tures only. Unlike the work we present here, their
approach requires the availability of an accurate
MT system which is a more expensive process.
6 Conclusion and Future Directions
In this paper we investigate the possibility of
building a high performance Arabic NER system
by using lexical, syntactic and morphological fea-
tures and augmenting the model with deeper lexi-
cal features and more syntagmatic features. These
extra features are extracted from noisy data ob-
tained via projection from an Arabic-English par-
allel corpus. Our results show that we achieve a
significantly high performance for almost all the
data-sets. The greatest impact of the syntagmatic
features (1.64 points of F-measure) is obtained for
the ACE 2004, BN genre. Also, the WL genre
yields an improvement of 1.16 F1 points absolute.
Acknowledgments
This work has been partially funded by DARPA GALE
project. The research of the last author was funded
by MICINN research project TEXT-ENTERPRISE 2.0
TIN2009-13391-C04-03 (Plan I+D+i).
284
References
B. Babych and A. Hartley. 2003. Improving Machine
Translation Quality with Automatic Named Entity
Recognition. In Proc. of EACL-EAMT.
Y. Benajiba, M. Diab, and P. Rosso. 2008. Ara-
bic named entity recognition using optimized feature
sets. In Proceedings of EMNLP?08, pages 284?293.
Daniel M. Bikel. 2004. On the parameter space
of generative lexicalized statistical parsing models.
University of Pennsylvania, Philadelphia, PA, USA.
Supervisor-Marcus, Mitchell P.
Z. Chen and H. Ji. 2009. Can one language bootstrap
the other: A case study of event extraction. In Pro-
ceedings of NAACL?09.
M. Collins. 1999. Head-Driven Statistical Models for
Nat- ural Language Parsing. University of Pennsyl-
vania, Philadelphia, PA, USA.
Mona Diab and Philip Resnik. 2002. An unsuper-
vised method for word sense tagging using parallel
corpora. In Proceedings of 40th Annual Meeting
of the Association for Computational Linguistics,
pages 255?262, Philadelphia, Pennsylvania, USA,
July. Association for Computational Linguistics.
M. Diab, K. Hacioglu, and D. Jurafsky, 2007. Arabic
Computational Morphology: Knowledge-based and
Empirical Methods, chapter 9. Springer.
Mona Diab. 2004. Bootstrapping a wordnet taxonomy
for arabic. In Proceedings of First Arabic Language
Technology Conference (NEMLAR), Cairo Egypt,.
N. Habash and O. Rambow. 2005. Arabic Tok-
enization, Part-of-Speech Tagging and Morpholog-
ical Disambiguation in One Fell Swoop. In Proc.
of the 43rd Annual Meeting of the Association for
Computational Linguistics (ACL?05), pages 573?
580, Ann Arbor, Michigan, June. Association for
Computational Linguistics.
R. Hwa, P. Resnik, and A. Weinberg. 2002. Break-
ing the resource bottleneck for multilingual parsing.
In In Proceedings of the Workshop on Linguistic
Knowledge Acquisition and Representation: Boot-
strapping Annotated Language Data.
D. Nadeau and S. Sekine. 2007. A Survey of Named
Entity Recognition and Classification. Linguisticae
Investigationes, 30(7).
H.-T. Ng, B. Wang, and Y.-S. Chan. 2003. Exploit-
ing parallel texts for word sense disambiguation: An
empirical study. In ACL?03, pages 455?462, Sap-
poro, Japan.
P. Thompson and C. Dozier. 1997. Name Searching
and Information Retrieval. In In Proc. of Second
Conference on Empirical Methods in Natural Lan-
guage Processing, Providence, Rhode Island.
I. Zitouni and R. Florian. 2008. Mention detection
crossing the language barrier. In Proceedings of
EMNLP?08, Honolulu, Hawaii, October.
Imed Zitouni and Radu Florian. 2009. Cross language
information propagation for arabic mention detec-
tion. Journal of ACM Transactions on Asian Lan-
guage Information Processing, December.
285
Proceedings of the 2012 Workshop on Biomedical Natural Language Processing (BioNLP 2012), pages 176?184,
Montre?al, Canada, June 8, 2012. c?2012 Association for Computational Linguistics
Grading the Quality of Medical Evidence
Binod Gyawali, Thamar Solorio
CoRAL Lab
Department of Computer and Information Sciences
University of Alabama at Birmingham, AL, USA
{bgyawali,solorio}@cis.uab.edu
Yassine Benajiba
Clinical Decision Support Solutions Department
Philips Research North America, Briarcliff Manor, NY, USA
yassine.benajiba@philips.com
Abstract
Evidence Based Medicine (EBM) is the prac-
tice of using the knowledge gained from the
best medical evidence to make decisions in
the effective care of patients. This medi-
cal evidence is extracted from medical docu-
ments such as research papers. The increas-
ing number of available medical documents
has imposed a challenge to identify the ap-
propriate evidence and to access the quality
of the evidence. In this paper, we present
an approach for the automatic grading of ev-
idence using the dataset provided by the 2011
Australian Language Technology Association
(ALTA) shared task competition. With the
feature sets extracted from publication types,
Medical Subject Headings (MeSH), title, and
body of the abstracts, we obtain a 73.77%
grading accuracy with a stacking based ap-
proach, a considerable improvement over pre-
vious work.
1 Introduction
?Evidence Based Medicine (EBM) is the conscien-
tious, explicit, and judicious use of current best evi-
dence in making decisions about the care of individ-
ual patients? (Sackett et al, 1996). EBM requires to
identify the best evidence, understand the method-
ology and strength of the approaches reported in
the evidence, and bring relevant findings into clin-
ical practice. Davidoff et al (1995) express EBM in
terms of five related ideas. Their ideas imply that
the conclusions should be derived based on the best
evidence available, the clinical decisions should be
made based on the conclusions derived, and the per-
formance of the clinical decisions should be evalu-
ated constantly. Thus, physicians practicing EBM
should be constantly aware of the new ideas and
the best methodologies available based on the most
recent literature. But the amount of clinical docu-
ments available is increasing everyday. For exam-
ple, Pubmed, a service of the US National Library of
Medicine contains more than 21 million citations for
biomedical literature from MEDLINE, life science
journals, and online books (last updated on Decem-
ber 7, 2011) 1. The abundance of digital informa-
tion makes difficult the task of evaluating the quality
of results presented and the significance of the con-
clusions drawn. Thus, it has become an important
task to grade the quality of evidence so that the most
significant evidence is incorporated into the clinical
practices.
There are several scale systems available to grade
medical evidence. Some of them are: hierarchy
of evidence proposed by Evans (2003), Grading of
Recommendations Assessment, Development, and
Evaluation (GRADE) scale by GRADE (2004), and
Strength of Recommendation Taxonomy (SORT)
scale by Ebell et al (2004). The SORT scale ad-
dresses the quality, quantity, and consistency of evi-
dence and proposes three levels of ratings: A, B, and
C. Grade A is recommended based on the consistent,
good-quality patient-oriented evidence, grade B is
based on the inconsistent or limited-quality patient-
oriented evidence, and grade C is based on consen-
sus, disease-oriented evidence, usual practice, ex-
pert opinion or case studies.
1http://www.ncbi.nlm.nih.gov/books/NBK3827/
176
The Australasian Language Technology Associa-
tion (ALTA) 2011 organized the shared task compe-
tition2 to build an automatic evidence grading sys-
tem for EBM based on the SORT grading scale. We
carry out our experiments using the data set provided
for the competition and compare the accuracy of
grading the evidence by applying basic approaches
and an ensemble (stacking) based approach of clas-
sification. We show that the later approach can
achieve 73.77% of grading accuracy, a significant
improvement over the basic approaches. We further
extend our experiments to show that, using feature
sets generated from the method and conclusion sec-
tions of the abstracts helps to obtain higher accuracy
in evidence grading than using a feature set gener-
ated from the entire body of the abstracts.
2 Related Work
To the best of our knowledge, automatic evidence
grading based on a grading scale was initiated by
Sarker et al (2011). Their work was based on the
SORT scale to grade the evidence using the corpus
developed by Molla-Aliod (2010). They showed
that using only publication types as features could
yield an accuracy of 68% while other information
like publication types, journal names, publication
years, and article titles could not significantly help
to improve the accuracy of the grading. Molla-Aliod
and Sarker (2011) worked on the evidence grading
problem of 2011 ALTA shared task and achieved
an accuracy of 62.84% using three sequential clas-
sifiers, each trained by one of the following feature
sets: word n-grams from the abstract, publication
types, and word n-grams from the title. They ap-
plied a three way classification approach where the
instances classified as A or C were removed from
the test set and labeled as such, while instances
classified as B were passed to the next classifier in
the pipeline. They repeated this process until they
reached the end of three sequential classifiers.
Most of the EBM related work is focused on ei-
ther the identification of important statements from
the medical abstracts or the classification of med-
ical abstracts to facilitate the retrieval of impor-
tant documents. Work by Demner-Fushman et al
(2006), Dawes et al (2007), Kim et al (2011) au-
2http://www.alta.asn.au/events/sharedtask2011
tomatically identify the key statements in the med-
ical abstracts and classify them into different levels
that are considered important for EBM practitioners
in making decisions. Kilicoglu et al (2009) worked
on recognizing the clinically important medical ab-
stracts using an ensemble learning method (stack-
ing). They used different combinations of feature
vectors extracted from documents to classify the ev-
idence into relevant or non relevant classes. They
approached the problem as a binary classification
problem without using any grading scales.
Systematic Reviews (SRs) are very important
to support EBM. Creating and updating SRs is
highly inefficient and needs to identify the best evi-
dence. Cohen et al (2010) used a binary classifica-
tion system to identify the documents that are most
likely to be included in creating and updating SRs.
In this work, we grade the quality of evidence
based on the SORT scale, that is different from most
of the existing works related to classification of ab-
stracts and identification of key statements of ab-
stracts. We work on the same problem as by Molla-
Aliod and Sarker (2011) but, we undertake the prob-
lem with a different approach and use different sets
of features.
3 Dataset
We use the data of 2011 ALTA shared task compe-
tition that contains three different sets: training, de-
velopment and test set. The number of evidence in-
stances present in each set is shown in Table 1. Each
data set consists of instances with grades A, B, or C
based on the SORT scale. The distribution of evi-
dence grades is shown in Table 2.
Data Set No. of Evidence Instances
Training Set 677
Development Set 178
Test Set 183
Table 1: Evidence per data set
The evidence instances were obtained from the
corpus developed by Molla-Aliod and Santiago-
Martinez (2011). The corpus was generated based
on the question and the evidence based answer for
the question along with SOR grade obtained from
the ?Clinical Inquiries? section of the Journal of
177
Grades Training
set (%)
Development
set (%)
Test set
(%)
A 31.3 27.0 30.6
B 45.9 44.9 48.6
C 22.7 28.1 20.8
Table 2: Evidence distribution per grade
Family Practice (JFP). A sample question from the
JFP Clinical Inquiries section is ?How does smoking
in the home affect children with asthma??. Each ev-
idence contains at least one or more publications de-
pending upon from which publications the evidence
was generated. Each publication is an XML file con-
taining information such as abstract title, abstract
body, publication types, and MeSH terms. Each
publication is assigned at least one publication type
and zero or more MeSH terms. The MeSH terms
vocabulary 3 is developed and maintained by the
National Library of Medicine and is used in rep-
resentation, indexing and retrieval of medical doc-
uments. Some of the medical document retrieval
work emphasizes the use of MeSH terms in the ef-
ficient retrieval of documents (Trieschnigg et al,
2009; Huang et al, 2011). MeSH terms are also
used in document summarization (Bhattacharya et
al., 2011).
Figure 1: Sample data file
Each data set contains an additional grade file
with the information related to the evidence in-
stances, their grades, and the publications. A sam-
ple of the file is shown in Figure 1. The first column
contains the evidence id, the second column contains
the grades A, B, or C of the evidence based on the
SORT scale, and the remaining columns show the
publication id of each publication in the evidence.
3http://www.nlm.nih.gov/mesh
The problem in this task is to analyze the publica-
tions in each evidence provided and classify them
into A, B or C.
The dataset available for our research has ab-
stracts in two different formats. One of them con-
tains abstracts divided into sections: background,
objective, method, result, and conclusion. The other
format contains abstracts with all the information in
a single block without any sections. A sample of an
abstract having only four sections in the given data
is shown below:
Objectives: To determine the effectiveness of a muscle
strengthening program compared to a stretching program in
women with fibromyalgia (FM).
Methods: Sixty-eight women with FM were randomly as-
signed to a 12 week, twice weekly exercise program consisting
of either muscle strengthening or stretching. Outcome measures
included muscle strength (main outcome variable), flexibility,
weight, body fat, tender point count, and disease and symptom
severity scales.
Results: No statistically significant differences between
groups were found on independent t tests. Paired t tests revealed
twice the number of significant improvements in the strengthen-
ing group compared to the stretching group. Effect size scores
indicated that the magnitude of change was generally greater in
the strengthening group than the stretching group.
Conclusions: Patients with FM can engage in a specially
tailored muscle strengthening program and experience an im-
provement in overall disease activity, without a significant exer-
cise induced flare in pain. Flexibility training alone also results
in overall improvements, albeit of a lesser degree.
In the abstract above, we see that the approaches
applied for the study are described in the method
section, and the outcome and its effectiveness are
described in the conclusion section.
4 Proposed Methodology
In this paper we propose a system to identify the
correct grade of an evidence given publications in
the evidence. We deal with the problem of evi-
dence grading as a classification problem. In evi-
dence grading, basic approaches have been shown
to have poor performance. Molla-Aliod and Sarker
(2011) showed that a basic approach of using simple
bag-of-word features and a Naive Bayes classifier
achieved 45% accuracy and proposed a sequential
approach to improve the accuracy at each step. Our
preliminary studies of applying the simple classifi-
cation approach also showed similar results. Here,
we propose a stacking based approach (Wolpert,
178
1992) of evidence grading. Stacking based approach
builds a final classifier by combining the predictions
made by multiple classifiers to improve the predic-
tion accuracy. It involves two steps. In the first step,
multiple base-level classifiers are trained with dif-
ferent feature sets extracted from a dataset and the
classifiers are used to predict the classes of a sec-
ond dataset. Then, a higher level classifier is trained
using the predictions made by the base-level clas-
sifiers on the second dataset and used to predict the
classes of the actual test data. In this approach, base-
level classifiers are trained independent of each other
and allowed to predict the classes. Based on the
predictions made by these base-level classifiers, the
higher level classifier learns from those predictions
and makes a new prediction that is the final class.
Our stacking based approach of classification uses
five feature sets. In the first step of classification, we
train five classifiers using different feature sets per
classifier and use the classifiers to predict the grades
of the development dataset. Thus, at the end of the
first step, five different predictions on the develop-
ment dataset are obtained. In the second step, a new
classifier is trained using the grades predicted by the
five classifiers as features. This new classifier is then
used to predict the grades of the test dataset.
5 Features
We extracted six sets of features from the publica-
tions to perform our experiments. They are as fol-
lows:
1. Publication types
2. MeSH terms
3. Abstract title
4. Abstract body
5. Abstract method section
6. Abstract conclusion section
For feature set 1, we extracted 30 distinct publi-
cation types from the training data. For the MeSH
terms feature set, we selected 452 unique MeSH
terms extracted from the training data. The publi-
cations contained the descriptor name of the MeSH
terms having an attribute ?majortopicyn? with value
?Y? or ?N?. As MeSH terms feature set, we selected
only those MeSH term descriptor names having ma-
jortopicyn=?Y?.
We extracted the last four sets of features from
the title, body, method, and conclusion sections of
the abstracts. Here, the body of an abstract means
the whole content of the abstract, that includes back-
ground, objective, method, result, and conclusion
sections. We applied some preprocessing steps to
generate these feature sets. We also applied a feature
selection technique to reduce the number of features
and include only the high informative features from
these feature sets. The details about preprocess-
ing and feature selection techniques are described in
Section 6.
We performed all the experiments on the basis of
evidence, i.e. we created a single feature vector per
evidence. If an evidence contained more than one
publication, we generate its features as the union of
the features extracted from all its publications.
The grades of the evidence in the SORT scale
are based on the quality of evidence, basis of ex-
periments, the methodologies used, and the types of
analysis done. Grades also depend upon the effec-
tiveness of the approach used in the experiments.
The method section of an abstract contains the in-
formation related to the basis of the experiments,
such as randomized controlled trails, systematic re-
view, cohort studies, and the methods used in their
research. The conclusion section of the abstract
usually contains the assertion statements about how
strongly the experiment supports the claims. Anal-
ysis of the contents of abstracts shows that the in-
formation needed for grading on SORT scale is typ-
ically available in the method and conclusion sec-
tions, more than in the other sections of the abstracts.
Thus, we used the method and conclusion sections
of the abstracts to generate two different feature sets
so that only the features more likely to be important
in grading using the SORT rating would be included.
Separating method and conclusion sections of
the abstracts
In order to extract features from the method and con-
clusion sections, we should separate them from the
body of abstracts, which is a challenging task for
those abstracts without section headers. Of the to-
tal number of abstracts, more than one-third of the
abstracts do not contain the section headers. In or-
der to separate these sections, we used a very simple
approach based on the number of sentences present
179
in the method and conclusion sections, and the body
of the abstracts. We used the following information
to separate the method and conclusion sections from
these abstracts: i) Order of sections in the abstracts,
ii) Average number of sentences in the method and
conclusion sections of the abstracts having sections,
and iii) Average number of sentences in the entire
body of the abstracts not having sections. All the ab-
stracts having section headers contained the sections
in the same order: background, objective, method,
result and conclusion. From the available training
dataset, we calculated:
i. The average number of sentences in the method
(4.14) and conclusion (2.11) sections of the abstracts di-
vided into sections
ii. The average number of sentences (8.78) of the ab-
stracts not having sections
Based on these values, we fragmented the ab-
stracts that do not have the section headers and sepa-
rated the method and conclusion sections from them.
Table 3 shows how the method and conclusion sec-
tions of those abstracts were generated. For exam-
ple, the fourth row of the table says that, if an ab-
stract without section headers has 6, 7 or 8 sentences
(let it be n), then the 3rd, 4th and 5th sentences were
considered as the method section, and the nth sen-
tence was considered as the conclusion section.
Total sentences in
Abstracts(n)
Method Conclusion
1 None 1
2 or 3 1 n
4 or 5 2 and 3 n
6 or 7 or 8 2, 3 and 4 n
More than 8 3, 4 and 5 n-1 and n
Table 3: Selecting method and conclusion of the abstracts
having a single block
6 Experiments and Results
This section describes the two sets of experiments
performed to compare the performance of the stack-
ing based approach and the effectiveness of the base-
level classifiers used. The first set of experiments
was done to provide a baseline comparison against
our stacking based approach. The second set con-
sists of five experiments to evaluate different con-
figurations of stack based classifiers. The basic ap-
proach of classification implies the use of a single
classifier trained by using a single feature vector.
We applied preprocessing steps to generate fea-
ture sets from the title, body, method and conclusion
sections of the abstracts. The preprocessing steps
were: detecting sentences using OpenNLP Sentence
Detector4, stemming words in each sentence using
Porter Stemmer (Porter, 1980), changing the sen-
tences into lower-case, and removing punctuation
characters from the sentences. After the preprocess-
ing step, we generated features from the unigrams,
bigrams and trigrams in each part. We removed
those features from the feature sets that contained
the stopwords listed by Pubmed5 or contained any
token having a length less than three characters. To
remove the less informative features, we calculated
the information gain of the features in the training
data using Weka (Hall et al, 2009) and selected only
the top 500 high informative features for each fea-
ture set. We used the Weka SVM classifier for all the
experiments. Based on the best result obtained af-
ter a series of experiments run with different kernel
functions and regularization parameters, we chose
the SVM classifier with a linear kernel and regular-
ization parameter equals 1 for all the experiments.
We used a binary weight for all the features.
6.1 First set of experiments
In the first set, we performed nine experiments using
the basic classification approach and one experiment
using the stacking based approach. The details of
the experiments and the combinations of the features
used in them are as shown in Table 4.
The first six experiments in the table were imple-
mented by applying a basic approach of classifica-
tion and each using only a single set of features. Ex-
periments 7, 8, and 9 were similar to the first six
experiments except, they used more than one set of
features to create the feature vector. Each feature in
the experiments 7, 8, and 9 encode the section of its
origin. For example, if feature abdomen is present
in method as well as conclusion sections, it is rep-
resented as two distinct features conc abdomen and
method abdomen. In experiment 10, we applied
4http://incubator.apache.org/opennlp
5http://www.ncbi.nlm.nih.gov/books/NBK3827
/table/pubmedhelp.T43/?report=objectonly)
180
the stacking approach of classification using five
base-level classifiers. The base-level classifiers in
this experiment are the basic classifiers used in ex-
periments 1 to 5.
Exp.
No.
Features used Exp. type
1. Publication types
Basic approach
2. MeSH terms
3. Abstract title
4. Abstract method
5. Abstract conclusion
6. Abstract body
7.
Publication types,
MeSH terms
8.
Publication types,
MeSH terms,
Abstract title,
Abstract body
9.
Publication types,
MeSH terms,
Abstract title,
Abstract method,
Abstract conclusion
10.
Publication types
Stacking based
approach
MeSH terms
Abstract title
Abstract method
Abstract conclusion
Table 4: Experiments to compare basic approaches to a
stacking based approach
Figure 2 shows the results of the 10 experiments
described in Table 4 in the same order, from 1st to
10th place and the result of the experiment by Molla-
Aliod and Sarker (2011). The results show that
the stacking based approach gives the highest ac-
curacy (73.77%), outperforming all the basic ap-
proaches applying any combination of feature sets.
The stacking based approach outperforms the base-
line of a single layered classification approach (Exp
9) that uses all the five sets of features. Molla-Aliod
and Sarker (2011) showed that a simple approach of
using a single classifier and bag-of-words features
could not achieve a good accuracy (45.9%) and pro-
posed a new approach of using a sequence of classi-
fiers to achieve a better result. Similar to their simple
approach, our basic approaches could not achieve
good results, but their performance is comparable
to Molla-Aliod and Sarker (2011)?s baseline system.
The result of our stacking based approach shows that
our approach has a better accuracy than the sequen-
cial classification approach (62.84%) proposed by
Figure 2: Comparison of accuracy of basic approaches to
a stacking based approach. X-axis shows the experiments
and Y-axis shows the accuracy of the experiments. The
first nine experiments are based on the basic approach
and the tenth experiment is based on the stacking based
approach.
Molla-Aliod and Sarker (2011).
Our stacking based approach works on two lev-
els. In the first level, the base-level classifiers pre-
dict the grades of the evidence. In the next level,
these predictions are used to train a new classifier
that learns from the predictions to identify the grades
correctly. Moreover, the five feature sets used in our
experiments were unrelated to each other. For ex-
ample, the features present in MeSH headings were
different from the features used in publication types,
and similarly, the features present in the method sec-
tion of the abstract were different from the features
present in the conclusion section. Each base-level
classifier trained by one of these feature sets is spe-
cialized in that particular feature set. Thus, using
the predictions made by these specialized base-level
classifiers to train a higher level classifier helps to
better predict the grades, this cannot be achieved by
a single classifier trained by a set of features (Exp.
1, 2, 3, 4, 5, 6), or a group of different feature sets
(Exp. 7, 8, 9).
6.2 Second set of experiments
In the second set of experiments, we compared five
experiments performed varying the base-level clas-
sifiers used in our stack based approach. Experi-
ments 1 and 2 were performed using a single base-
level classifier, that means that the second classifier
is trained on only one feature. Experiments 3 and 4
were performed by using four base-level classifiers,
and experiment 5 was performed using five base-
181
level classifiers. The 5th experiment in this set is
same as the 10th experiment in the first set. The de-
tails about the feature sets used in each experiment
are shown in Table 5.
Exp.
No.
Features used No. of Base level
classifiers
1.
Publication types,
1
MeSH terms,
Abstract title,
Abstract body
2.
Publication types,
1
MeSH terms,
Abstract title,
Abstract method,
Abstract conclusion
3.
Publication types
4
MeSH terms
Abstract title
Abstract body
4.
Publication types
4
MeSH terms
Abstract title
Abstract method,
Abstract conclusion
5.
Publication types
5
MeSH terms
Abstract title
Abstract method
Abstract conclusion
Table 5: Experiments to compare stacking based ap-
proach
Figure 3 shows the accuracy of the five experi-
ments shown in Table 5 in the same order. It shows
that the accuracy of 1st and 2nd experiments is lower
than the accuracy of 3rd, 4th, and 5th experiments.
In these two experiments, a feature vector generated
from the prediction of a single base-level classifier
is used to train the higher level classifier, that is not
sufficient to make a correct decision.
Experiments 3, 4, and 5 show a considerable im-
provement in the accuracy of the grading. Compar-
ing the results of experiments 3 and 4, we see that
the 4th experiment has higher accuracy than the 3rd
one. The difference between these experiments was
the use of features from the method and conclusion
sections of the abstracts in the 4th experiment, while
using features from the entire body of abstracts in
the 3rd experiment. The higher accuracy in the 4th
experiment shows that the method and conclusion
sections of the experiment contain high informative
text that is important for evidence grading, while
Figure 3: Comparison of accuracy of the stacking based
approaches. X-axis shows the experiments and Y-axis
shows the accuracy of the experiments. 1st and 2nd ex-
periments use only one base-level classifier, 3rd and 4th
experiment are based on four base-level classifiers and
5th one uses five base-level classifiers.
the body of abstracts may contain some information
that is not relevant to the task. The same analysis
can also be inferred from the results of experiment
8 and 9 in the first set of experiments. The high-
est accuracy obtained in the 5th experiment of apply-
ing 5 base-level classifiers shows that identifying the
sections of the abstracts containing high informative
features and using a sufficient number of base-level
classifiers can help to achieve a good accuracy in ev-
idence grading.
7 Error Analysis
The result obtained by the stacking based approach
(5th experiment in Table 5) using five base-level clas-
sifiers gave a higher error rate in predicting grades
A and C, compared to the error rate in predict-
ing grade B. Most of the error is the misclassifica-
tion of A to C and vice versa. One of the possi-
ble reasons of this might be due to the use of the
feature set extracted from the conclusion section.
Among the five base-level classifiers used in the ex-
periment, the one trained by the features extracted
from the conclusion sections has the lowest accu-
racy (5th experiment in Figure 2). We evaluated the
text contained in the conclusion section of the ab-
stracts in our dataset. The section mostly contains
the assertion statements having the words showing
strong positive/negative meanings. Conclusion of A
grade evidence mostly contains the information that
strongly asserts the claim (e.g. emollient treatment
182
significantly reduced the high-potency topical cor-
ticosteroid consumption in infants with AD), while
that of C grade evidence is not strong enough to as-
sert the claim (e.g. PDL therapy should be consid-
ered among the better established approaches in the
treatment of warts, although data from this trial sug-
gest that this approach is probably not superior). It
seems that the problem might be because of not pro-
cessing the negations appropriately. So, in order to
preserve some negation information present in the
conclusion sections, we performed another experi-
ment by merging words no, not, nor with their suc-
cessor word to create a single token from the two
words. This approach still could not reduce the mis-
classification. Thus, the simple approach of extract-
ing unigram, bigram, and trigram features from the
conclusion section might not be sufficient and might
need to include higher level analysis related to as-
sertion/certainty of the statements to reduce the mis-
classification of the evidence.
Other possible reasons of the misclassification
of the evidence might be the imbalanced data set.
Our dataset (Table 2) contains higher number of in-
stances with grade B than those with grades A and C.
Moreover, the number of publications per evidence
is not uniform, that ranges from 1 to 8 publications
per evidence in the test data. Analyzing the results,
we found that misclassification of evidence having
only one publication is higher than that of the evi-
dence having more than one publication. If an ev-
idence contains only one publication, the features
of the evidence extracted from a single publication
might not be sufficient to accurately grade the evi-
dence and might lead to misclassification.
In order to evaluate the appropriateness of our
approach in extracting the method and conclusion
sections, we performed a manual inspection of ab-
stracts. We could not revise all the abstracts to ver-
ify the approach. Thus, we randomly selected 25
abstracts without section headers from the test data
and viewed the content in them. We found that the
conclusion section was appropriately extracted in al-
most all abstracts, while the selection of method sec-
tion was partially effective. Our approach was based
on the assumption that all the abstracts having many
sentences have all the sections (background, objec-
tive, method, result, and conclusion). But we found
that the abstracts do not follow the same format, and
the start sentence of the method section is not con-
sistent. Even a long abstract might sometimes start
with the method section, and sometimes the objec-
tive section might not be present in the abstracts.
This could lead to increase the error in our grading
system.
8 Conclusion
This paper presents an approach of grading the med-
ical evidence applying a stacking based classifier
using the features from publication types, MeSH
terms, abstract body, and method, and conclusion
sections of the abstracts. The results show that
this approach achieves an accuracy of 73.77%, that
is significantly better than the previously reported
work. Here, we present two findings: 1) We show
that the stacking based approach helps to obtain a
better result in evidence grading than the basic ap-
proach of classification. 2) We also show that the
method and conclusion sections of the abstracts con-
tain important information necessary for evidence
grading. Using the feature sets generated from these
two sections helps to achieve a higher accuracy than
by using the feature set generated from the entire
body of the abstracts.
In this work, all the information available in the
method and conclusion sections of the abstracts is
treated with equal weight. Evidence grading should
not depend upon specific disease names and syn-
dromes, but should be based on how strong the facts
are presented. We would like to extend our ap-
proach by removing the words describing specific
disease names, disease syndromes, and medications,
and giving higher weight to the terms that describe
the assertion of the statements. In our current work,
we apply a simple approach to extract the method
and conclusion sections from the abstracts not hav-
ing sections. Improving the approach by using a ma-
chine learning algorithm that can more accurately
extract the sections might help to increase the accu-
racy of grading. Including the information about the
strength of assertions made in the conclusion sec-
tions could also help in boosting the accuracy. Fu-
ture work would also include testing the effective-
ness of our approach on other diverse data sets hav-
ing complex structures of the evidence, or on a dif-
ferent grading scale.
183
References
Sanmitra Bhattacharya, Viet HaThuc, and Padmini Srini-
vasan. 2011. Mesh: a window into full text for doc-
ument summarization. Bioinformatics, 27(13):i120?
i128.
Aaron M. Cohen, Kyle Ambert, and Marian McDon-
agh. 2010. A Prospective Evaluation of an Au-
tomated Classification System to Support Evidence-
based Medicine and Systematic Review. AMIA Annu
Symp Proc., 2010:121 ? 125.
Frank Davidoff, Brian Haynes, Dave Sackett, and
Richard Smith. 1995. Evidence based medicine.
BMJ, 310(6987):1085?1086, 4.
Martin Dawes, Pierre Pluye, Laura Shea, Roland Grad,
Arlene Greenberg, and Jian-Yun Nie. 2007. The iden-
tification of clinically important elements within med-
ical journal abstracts: Patient-Population-Problem,
Exposure-Intervention, Comparison, Outcome, Dura-
tion and Results (PECODR). Informatics in Primary
Care, 15(1):9?16.
Dina Demner-Fushman, Barbara Few, Susan E. Hauser,
and George Thoma. 2006. Automatically Identifying
Health Outcome Information in MEDLINE Records.
Journal of the American Medical Informatics Associa-
tion, 13(1):52 ? 60.
M. H. Ebell, J. Siwek, B. D. Weiss, S. H. Woolf, J. Sus-
man, B. Ewigman, and M. Bowman. 2004. Strength
of recommendation taxonomy (SORT): a patient-
centered approach to grading evidence in the medi-
cal literature. American Family Physician, 69(3):548?
56+.
David Evans. 2003. Hierarchy of evidence: a frame-
work for ranking evidence evaluating healthcare inter-
ventions. Journal of Clinical Nursing, 12(1):77?84.
GRADE. 2004. Grading quality of evidence and strength
of recommendations. BMJ, 328(7454):1490, 6.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA data mining software: an update.
SIGKDD Explor. Newsl., 11(1).
Minlie Huang, Aurlie Nvol, and Zhiyong Lu. 2011. Rec-
ommending MeSH terms for annotating biomedical
articles. Journal of the American Medical Informat-
ics Association, 18(5):660?667.
Halil Kilicoglu, Dina Demner-Fushman, Thomas C Rind-
flesch, Nancy L Wilczynski, and R Brian Haynes.
2009. Towards Automatic Recognition of Scientif-
ically Rigorous Clinical Research Evidence. Jour-
nal of the American Medical Informatics Association,
16(1):25?31.
Su Kim, David Martinez, Lawrence Cavedon, and Lars
Yencken. 2011. Automatic classification of sentences
to support Evidence Based Medicine. BMC Bioinfor-
matics, 12(Suppl 2):S5.
Diego Molla-Aliod and Maria Elena Santiago-Martinez.
2011. Development of a Corpus for Evidence Based
Medicine Summarisation. In Proceedings of the
Australasian Language Technology Association Work-
shop.
Diego Molla-Aliod and Abeed Sarker. 2011. Automatic
Grading of Evidence: the 2011 ALTA Shared Task.
In Proceedings of Australasian Language Technology
Association Workshop, pages 4?8.
Diego Molla-Aliod. 2010. A Corpus for Evidence
Based Medicine Summarisation. In Proceedings of the
Australasian Language Technology Association Work-
shop, volume 8.
MF Porter. 1980. An algorithm for sufx stripping. Pro-
gram, 14(3):130?137.
David L Sackett, William M C Rosenberg, J A Muir Gray,
R Brian Haynes, and W Scott Richardson. 1996. Ev-
idence based medicine: what it is and what it isn?t.
BMJ, 312(7023):71?72, 1.
Abeed Sarker, Diego Molla-Aliod, and Cecile Paris.
2011. Towards automatic grading of evidence. In Pro-
ceedings of LOUHI 2011 Third International Work-
shop on Health Document Text Mining and Informa-
tion Analysis, pages 51?58.
Dolf Trieschnigg, Piotr Pezik, Vivian Lee, Franciska
de Jong, Wessel Kraaij, and Dietrich Rebholz-
Schuhmann. 2009. MeSH Up: effective MeSH text
classification for improved document retrieval. Bioin-
formatics, 25(11):1412?1418.
David H. Wolpert. 1992. Stacked generalization. Neural
Networks, 5(2):241 ? 259.
184
