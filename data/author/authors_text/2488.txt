Proceedings of the ACL 2007 Demo and Poster Sessions, pages 57?60,
Prague, June 2007. c?2007 Association for Computational Linguistics
Support Vector Machines for Query-focused Summarization trained and
evaluated on Pyramid data
Maria Fuentes
TALP Research Center
Universitat Polite`cnica de Catalunya
mfuentes@lsi.upc.edu
Enrique Alfonseca
Computer Science Departament
Universidad Auto?noma de Madrid
Enrique.Alfonseca@gmail.com
Horacio Rodr??guez
TALP Research Center
Universitat Polite`cnica de Catalunya
horacio@lsi.upc.edu
Abstract
This paper presents the use of Support
Vector Machines (SVM) to detect rele-
vant information to be included in a query-
focused summary. Several SVMs are
trained using information from pyramids
of summary content units. Their per-
formance is compared with the best per-
forming systems in DUC-2005, using both
ROUGE and autoPan, an automatic scor-
ing method for pyramid evaluation.
1 Introduction
Multi-Document Summarization (MDS) is the task
of condensing the most relevant information from
several documents in a single one. In terms of the
DUC contests1, a query-focused summary has to
provide a ?brief, well-organized, fluent answer to a
need for information?, described by a short query
(two or three sentences). DUC participants have to
synthesize 250-word sized summaries for fifty sets
of 25-50 documents in answer to some queries.
In previous DUC contests, from 2001 to 2004, the
manual evaluation was based on a comparison with
a single human-written model. Much information
in the evaluated summaries (both human and auto-
matic) was marked as ?related to the topic, but not
directly expressed in the model summary?. Ideally,
this relevant information should be scored during the
evaluation. The pyramid method (Nenkova and Pas-
sonneau, 2004) addresses the problem by using mul-
tiple human summaries to create a gold-standard,
1http://www-nlpir.nist.gov/projects/duc/
and by exploiting the frequency of information in
the human summaries in order to assign importance
to different facts. However, the pyramid method re-
quires to manually matching fragments of automatic
summaries (peers) to the Semantic Content Units
(SCUs) in the pyramids. AutoPan (Fuentes et al,
2005), a proposal to automate this matching process,
and ROUGE are the evaluation metrics used.
As proposed by Copeck and Szpakowicz (2005),
the availability of human-annotated pyramids con-
stitutes a gold-standard that can be exploited in or-
der to train extraction models for the summary au-
tomatic construction. This paper describes several
models trained from the information in the DUC-
2006 manual pyramid annotations using Support
Vector Machines (SVM). The evaluation, performed
on the DUC-2005 data, has allowed us to discover
the best configuration for training the SVMs.
One of the first applications of supervised Ma-
chine Learning techniques in summarization was in
Single-Document Summarization (Ishikawa et al,
2002). Hirao et al (2003) used a similar approach
for MDS. Fisher and Roark (2006)?s MDS system is
based on perceptrons trained on previous DUC data.
2 Approach
Following the work of Hirao et al (2003) and
Kazawa et al (2002), we propose to train SVMs
for ranking the candidate sentences in order of rele-
vance. To create the training corpus, we have used
the DUC-2006 dataset, including topic descriptions,
document clusters, peer and manual summaries, and
pyramid evaluations as annotated during the DUC-
2006 manual evaluation. From all these data, a set
57
of relevant sentences is extracted in the following
way: first, the sentences in the original documents
are matched with the sentences in the summaries
(Copeck and Szpakowicz, 2005). Next, all docu-
ment sentences that matched a summary sentence
containing at least one SCU are extracted. Note that
the sentences from the original documents that are
not extracted in this way could either be positive (i.e.
contain relevant data) or negative (i.e. irrelevant for
the summary), so they are not yet labeled. Finally,
an SVM is trained, as follows, on the annotated data.
Linguistic preprocessing The documents from
each cluster are preprocessed using a pipe of general
purpose processors performing tokenization, POS
tagging, lemmatization, fine grained Named Enti-
ties (NE)s Recognition and Classification, anaphora
resolution, syntactic parsing, semantic labeling (us-
ing WordNet synsets), discourse marker annotation,
and semantic analysis. The same tools are used for
the linguistic processing of the query. Using these
data, a semantic representation of the sentence is
produced, that we call environment. It is a semantic-
network-like representation of the semantic units
(nodes) and the semantic relations (edges) holding
between them. This representation will be used to
compute the (Fuentes et al, 2006) lexico-semantic
measures between sentences.
Collection of positive instances As indicated be-
fore, every sentence from the original documents
matching a summary sentence that contains at least
one SCU is considered a positive example. We have
used a set of features that can be classified into three
groups: those extracted from the sentences, those
that capture a similarity metric between the sentence
and the topic description (query), and those that try
to relate the cohesion between a sentence and all the
other sentences in the same document or collection.
The attributes collected from the sentences are:
? The position of the sentence in its document.
? The number of sentences in the document.
? The number of sentences in the cluster.
? Three binary attributes indicating whether the
sentence contains positive, negative and neutral
discourse markers, respectively. For instance,
what?s more is positive, while for example and
incidentally indicate lack of relevance.
? Two binary attributes indicating whether
the sentence contains right-directed discourse
markers (that affect the relevance of fragment
after the marker, e.g. first of all), or discourse
markers affecting both sides, e.g. that?s why.
? Several boolean features to mark whether the
sentence starts with or contains a particular
word or part-of-speech tag.
? The total number of NEs included in the sen-
tence, and the number of NEs of each kind.
? SumBasic score (Nenkova and Vanderwende,
2005) is originally an iterative procedure that
updates word probabilities as sentences are se-
lected for the summary. In our case, word prob-
abilities are estimated either using only the set
of words in the current document, or using all
the words in the cluster.
The attributes that depend on the query are:
? Word-stem overlapping with the query.
? Three boolean features indicating whether the
sentence contains a subject, object or indirect
object dependency in common with the query.
? Overlapping between the environment predi-
cates in the sentence and those in the query.
? Two similarity metrics calculated by expanding
the query words using Google.
? SumFocus score (Vanderwende et al, 2006).
The cohesion-based attributes 2 are:
? Word-stem overlapping between this sentence
and the other sentences in the same document.
? Word-stem overlapping between this sentence
and the other sentences in the same cluster.
? Synset overlapping between this sentence and
the other sentences in the same document.
? Synset overlapping with other sentences in the
same collection.
Model training In order to train a traditional
SVM, both positive and negative examples are nec-
essary. From the pyramid data we are able to iden-
tify positive examples, but there is not enough ev-
idence to classify the remaining sentences as posi-
tive or negative. Although One-Class Support Vec-
tor Machine (OSVM) (Manevitz and Yousef, 2001)
can learn from just positive examples, according to
Yu et al (2002) they are prone to underfitting and
overfitting when data is scant (which happens in
2The mean, median, standard deviation and histogram of the
overlapping distribution are calculated and included as features.
58
this case), and a simple iterative procedure called
Mapping-Convergence (MC) algorithm can greatly
outperform OSVM (see the pseudocode in Figure 1).
Input: positive examples, POS, unlabeled examples U
Output: hypothesis at each iteration h?1, h?2, ..., h?k
1. Train h to identify ?strong negatives? in U :
N1 := examples from U classified as negative by h
P1 := examples from U classified as positive by h
2. Set NEG := ? and i := 1
3. Loop until Ni = ?,
3.1. NEG := NEG ? Ni
3.2. Train h?i from POS and NEG
3.3. Classify Pi by h?i:
Ni+1 = examples from Pi classified as negative
Pi+1 = examples from Pi classified as positive
5. Return {h?1, h?2, ..., h?k}
Figure 1: Mapping-Convergence algorithm.
The MC starts by identifying a small set of in-
stances that are very dissimilar to the positive exam-
ples, called strong negatives. Next, at each iteration,
a new SVM h?i is trained using the original positive
examples, and the negative examples found so far.
The set of negative instances is then extended with
the unlabeled instances classified as negative by h?i.
The following settings have been tried:
? The set of positive examples has been collected
either by matching document sentences to peer
summary sentences (Copeck and Szpakowicz,
2005) or by matching document sentences to
manual summary sentences.
? The initial set of strong negative examples for
the MC algorithm has been either built auto-
matically as described by Yu et al (2002), or
built by choosing manually, for each cluster, the
two or three automatic summaries with lowest
manual pyramid scores.
? Several SVM kernel functions have been tried.
For training, there were 6601 sentences from the
original documents, out of which around 120 were
negative examples and either around 100 or 500 pos-
itive examples, depending on whether the document
sentences had been matched to the manual or the
peer summaries. The rest were initially unlabeled.
Summary generation Given a query and a set of
documents, the trained SVMs are used to rank sen-
tences. The top ranked ones are checked to avoid re-
dundancy using a percentage overlapping measure.
3 Evaluation Framework
The SVMs, trained on DUC-2006 data, have been
tested on the DUC-2005 corpus, using the 20 clus-
ters manually evaluated with the pyramid method.
The sentence features were computed as described
before. Finally, the performance of each system
has been evaluated automatically using two differ-
ent measures: ROUGE and autoPan.
ROUGE, the automatic procedure used in DUC,
is based on n-gram co-occurrences. Both ROUGE-2
(henceforward R-2) and ROUGE-SU4 (R-SU4) has
been used to rank automatic summaries.
AutoPan is a procedure for automatically match-
ing fragments of text summaries to SCUs in pyra-
mids, in the following way: first, the text in the
SCU label and all its contributors is stemmed and
stop words are removed, obtaining a set of stem
vectors for each SCU. The system summary text is
also stemmed and freed from stop words. Next, a
search for non-overlapping windows of text which
can match SCUs is carried. Each match is scored
taking into account the score of the SCU as well as
the number of matching stems. The solution which
globally maximizes the sum of scores of all matches
is found using dynamic programming techniques.
According to Fuentes et al (2005), autoPan scores
are highly correlated to the manual pyramid scores.
Furthermore, autoPan also correlates well with man-
ual responsiveness and both ROUGE metrics.3
3.1 Results
Positive Strong neg. R-2 R-SU4 autoPan
peer pyramid scores 0.071 0.131 0.072
(Yu et al, 2002) 0.036 0.089 0.024
manual pyramid scores 0.025 0.075 0.024
(Yu et al, 2002) 0.018 0.063 0.009
Table 1: ROUGE and autoPan results using different SVMs.
Table 1 shows the results obtained, from which
some trends can be found: firstly, the SVMs
trained using the set of positive examples obtained
from peer summaries consistently outperform SVMs
trained using the examples obtained from the man-
ual summaries. This may be due to the fact that the
3In DUC-2005 pyramids were created using 7 manual sum-
maries, while in DUC-2006 only 4 were used. For that reason,
better correlations are obtained in DUC-2005 data.
59
number of positive examples is much higher in the
first case (on average 48,9 vs. 12,75 examples per
cluster). Secondly, generating automatically a set
with seed negative examples for the M-C algorithm,
as indicated by Yu et al (2002), usually performs
worse than choosing the strong negative examples
from the SCU annotation. This may be due to the
fact that its quality is better, even though the amount
of seed negative examples is one order of magnitude
smaller in this case (11.9 examples in average). Fi-
nally, the best results are obtained when using a RBF
kernel, while previous summarization work (Hirao
et al, 2003) uses polynomial kernels.
The proposed system attains an autoPan value of
0.072, while the best DUC-2005 one (Daume? III and
Marcu, 2005) obtains an autoPan of 0.081. The dif-
ference is not statistically significant. (Daume? III
and Marcu, 2005) system also scored highest in re-
sponsiveness (manually evaluated at NIST).
However, concerning ROUGE measures, the best
participant (Ye et al, 2005) has an R-2 score of
0.078 (confidence interval [0.073?0.080]) and an R-
SU4 score of 0.139 [0.135?0.142], when evaluated
on the 20 clusters used here. The proposed sys-
tem again is comparable to the best system in DUC-
2005 in terms of responsiveness, Daume? III and
Marcu (2005)?s R-2 score was 0.071 [0.067?0.074]
and R-SU4 was 0.126 [0.123?0.129] and it is better
than the DUC-2005 Fisher and Roark supervised ap-
proach with an R-2 of 0.066 and an R-SU4 of 0.122.
4 Conclusions and future work
The pyramid annotations are a valuable source of
information for training automatically text sum-
marization systems using Machine Learning tech-
niques. We explore different possibilities for apply-
ing them in training SVMs to rank sentences in order
of relevance to the query. Structural, cohesion-based
and query-dependent features are used for training.
The experiments have provided some insights on
which can be the best way to exploit the annota-
tions. Obtaining the positive examples from the an-
notations of the peer summaries is probably better
because most of the peer systems are extract-based,
while the manual ones are abstract-based. Also, us-
ing a very small set of strong negative example seeds
seems to perform better than choosing them auto-
matically with Yu et al (2002)?s procedure.
In the future we plan to include features from ad-
jacent sentences (Fisher and Roark, 2006) and use
rouge scores to initially select negative examples.
Acknowledgments
Work partially funded by the CHIL project, IST-2004506969.
References
T. Copeck and S. Szpakowicz. 2005. Leveraging pyramids. In
Proc. DUC-2005, Vancouver, Canada.
Hal Daume? III and Daniel Marcu. 2005. Bayesian summariza-
tion at DUC and a suggestion for extrinsic evaluation. In
Proc. DUC-2005, Vancouver, Canada.
S. Fisher and B. Roark. 2006. Query-focused summarization
by supervised sentence ranking and skewed word distribu-
tions. In Proc. DUC-2006, New York, USA.
M. Fuentes, E. Gonza`lez, D. Ferre?s, and H. Rodr??guez. 2005.
QASUM-TALP at DUC 2005 automatically evaluated with
the pyramid based metric autopan. In Proc. DUC-2005.
M. Fuentes, H. Rodr??guez, J. Turmo, and D. Ferre?s. 2006.
FEMsum at DUC 2006: Semantic-based approach integrated
in a flexible eclectic multitask summarizer architecture. In
Proc. DUC-2006, New York, USA.
T. Hirao, J. Suzuki, H. Isozaki, and E. Maeda. 2003. Ntt?s
multiple document summarization system for DUC2003. In
Proc. DUC-2003.
K. Ishikawa, S. Ando, S. Doi, and A. Okumura. 2002. Train-
able automatic text summarization using segmentation of
sentence. In Proc. 2002 NTCIR 3 TSC workshop.
H. Kazawa, T. Hirao, and E. Maeda. 2002. Ranking SVM and
its application to sentence selection. In Proc. 2002 Workshop
on Information-Based Induction Science (IBIS-2002).
L.M. Manevitz and M. Yousef. 2001. One-class SVM for docu-
ment classification. Journal of Machine Learning Research.
A. Nenkova and R. Passonneau. 2004. Evaluating content se-
lection in summarization: The pyramid method. In Proc.
HLT/NAACL 2004, Boston, USA.
A. Nenkova and L. Vanderwende. 2005. The impact of
frequency on summarization. Technical Report MSR-TR-
2005-101, Microsoft Research.
L. Vanderwende, H. Suzuki, and C. Brockett. 2006. Mi-
crosoft research at DUC 2006: Task-focused summarization
with sentence simplification and lexical expansion. In Proc.
DUC-2006, New York, USA.
S. Ye, L. Qiu, and T.S. Chua. 2005. NUS at DUC 2005: Under-
standing documents via concept links. In Proc. DUC-2005.
H. Yu, J. Han, and K. C-C. Chang. 2002. PEBL: Positive
example-based learning for web page classification using
SVM. In Proc. ACM SIGKDD International Conference on
Knowledge Discovery in Databases (KDD02), New York.
60
1
2
3
4
5
6
7
8
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 143?147, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational Linguistics
UPC-CORE: What Can Machine Translation Evaluation Metrics and
Wikipedia Do for Estimating Semantic Textual Similarity?
Alberto Barro?n-Ceden?o1,2 Llu??s Ma`rquez1 Maria Fuentes1 Horacio Rodr??guez1 Jordi Turmo1
1 TALP Research Center, Universitat Polite`cnica de Catalunya
Jordi Girona Salgado 1?3, 08034, Barcelona, Spain
2 Facultad de Informa?tica, Universidad Polite?cnica de Madrid
Boadilla del Monte, 28660 Madrid, Spain
albarron, lluism, mfuentes, horacio, turmo @lsi.upc.edu
Abstract
In this paper we discuss our participation to
the 2013 Semeval Semantic Textual Similarity
task. Our core features include (i) a set of met-
rics borrowed from automatic machine trans-
lation, originally intended to evaluate auto-
matic against reference translations and (ii) an
instance of explicit semantic analysis, built
upon opening paragraphs of Wikipedia 2010
articles. Our similarity estimator relies on a
support vector regressor with RBF kernel. Our
best approach required 13 machine transla-
tion metrics + explicit semantic analysis and
ranked 65 in the competition. Our post-
competition analysis shows that the features
have a good expression level, but overfitting
and ?mainly? normalization issues caused
our correlation values to decrease.
1 Introduction
Our participation to the 2013 Semantic Textual Sim-
ilarity task (STS) (Agirre et al, 2013)1 was focused
on the CORE problem: GIVEN TWO SENTENCES,
s1 AND s2, QUANTIFIABLY INFORM ON HOW SIMI-
LAR s1 AND s2 ARE. We considered real-valued fea-
tures from four different sources: (i) a set of linguis-
tic measures computed with the Asiya Toolkit for
Automatic MT Evaluation (Gime?nez and Ma`rquez,
2010b), (ii) an instance of explicit semantic analy-
sis (Gabrilovich and Markovitch, 2007), built on top
of Wikipedia articles, (iii) a dataset predictor, and
(iv) a subset of the features available in Takelab?s
Semantic Text Similarity system (?Saric? et al, 2012).
1http://ixa2.si.ehu.es/sts/
Our approaches obtained an overall modest result
compared to other participants (best position: 65 out
of 89). Nevertheless, our post-competition analysis
shows that the low correlation was caused mainly by
a deficient data normalization strategy.
The paper distribution is as follows. Section 2 of-
fers a brief overview of the task. Section 3 describes
our approach. Section 4 discuss our experiments and
obtained results. Section 5 provides conclusions.
2 Task Overview
Detecting two similar text fragments is a difficult
task in cases where the similarity occurs at seman-
tic level, independently of the implied lexicon (e.g
in cases of dense paraphrasing). As a result, simi-
larity estimation models must involve features other
than surface aspects. The STS task is proposed as
a challenge focused in short English texts of dif-
ferent nature: from automatic machine translation
alternatives to human descriptions of short videos.
The test partition also included texts extracted from
news headlines and FrameNet?Wordnet pairs.
The range of similarity was defined between 0
(no relation) up to 5 (semantic equivalence). The
gold standard values were averaged from different
human-made annotations. The expected system?s
output was composed of a real similarity value, to-
gether with an optional confidence level (our confi-
dence level was set constant).
Table 1 gives an overview of the development
(2012 training and test) and test datasets. Note
that both collections extracted from SMT data are
highly biased towards the maximum similarity val-
ues (more than 75% of the instances have a similar-
143
Table 1: Overview of sub-collections in the development and test datasets, including number of instances and distri-
bution of similarity values (in percentage) as well as mean, minimum, and maximum lengths.
similarity distribution length
dataset instances [0, 1) [1, 2) [2, 3) [3, 4) [4, 5] mean min max
dev-[train + test]
MSRpar 1,500 1.20 8.13 17.13 48.73 24.80 17.84 5 30
MSRvid 1,500 31.00 14.13 15.47 20.87 18.53 6.66 2 24
SMTEuroparl 1,193 0.67 0.42 1.17 12.32 85.4 21.13 1 72
OnWN 750 2.13 2.67 10.40 25.47 59.33 7.57 1 34
SMTnews 399 1.00 0.75 5.51 13.03 79.70 11.72 2 28
test
headlines 750 15.47 22.00 16.27 24.67 21.60 7.21 3 22
OnWN 561 36.54 9.80 7.49 17.11 29.05 7.17 5 22
FNWN 189 34.39 29.63 28.57 6.88 0.53 19.90 3 71
SMT 750 0.00 0.27 3.47 20.40 75.87 26.40 1 96
ity higher than 4) and include the longest instances.
On the other hand, the FNWN instances are shifted
towards low similarity levels (more than 60% have a
similarity lower than 2).
3 Approach
Our similarity assessment model relies upon
SVMlight?s support vector regressor, with RBF ker-
nel (Joachims, 1999).2 Our model estimation pro-
cedure consisted of two steps: parameter defini-
tion and backward elimination-based feature selec-
tion. The considered features belong to four fami-
lies, briefly described in the following subsections.
3.1 Machine Translation Evaluation Metrics
We consider a set of linguistic measures originally
intended to evaluate the quality of automatic trans-
lation systems. These measures compute the quality
of a translation by comparing it against one or sev-
eral reference translations, considered as gold stan-
dard. A straightforward application of these mea-
sures to the problem at hand is to consider s1 as the
reference and s2 as the automatic translation, or vice
versa. Some of the metrics are not symmetric so we
compute similarity between s1 and s2 in both direc-
tions and average the resulting scores.
The measures are computed with the Asiya
Toolkit for Automatic MT Evaluation (Gime?nez and
Ma`rquez, 2010b). The only pre-processing carried
out was tokenization (Asiya performs additional in-
box pre-processing operations, though). We consid-
2We also tried with linear kernels, but RBF always obtained
better results.
ered a sample from three similarity families, which
was proposed in (Gime?nez and Ma`rquez, 2010a) as
a varied and robust metric set, showing good corre-
lation with human assessments.3
Lexical Similarity Two metrics of Translation
Error Rate (Snover et al, 2006) (i.e. the esti-
mated human effort to convert s1 into s2): -TER
and -TERpA. Two measures of lexical precision:
BLEU (Papineni et al, 2002) and NIST (Dod-
dington, 2002). One measure of lexical recall:
ROUGEW (Lin and Och, 2004). Finally, four vari-
ants of METEOR (Banerjee and Lavie, 2005) (exact,
stemming, synonyms, and paraphrasing), a lexical
metric accounting for F -Measure.
Syntactic Similarity Three metrics that estimate
the similarity of the sentences over dependency
parse trees (Liu and Gildea, 2005): DP-HWCMic-4
for grammatical categories chains, DP-HWCMir-4
over grammatical relations, and DP-Or(?) over
words ruled by non-terminal nodes. Also, one mea-
sure that estimates the similarity over constituent
parse trees: CP-STM4 (Liu and Gildea, 2005).
Semantic Similarity Three measures that esti-
mate the similarities over semantic roles (i.e. ar-
guments and adjuncts): SR-Or, SR-Mr(?), and
SR-Or(?). Additionally, two metrics that es-
timate similarities over discourse representations:
DR-Or(?) and DR-Orp(?).
3Asiya is available at http://asiya.lsi.upc.edu.
Full descriptions of the metrics are available in the Asiya Tech-
nical Manual v2.0, pp. 15?21.
144
3.2 Explicit Semantic Analysis
We built an instance of Explicit Semantic Analy-
sis (ESA) (Gabrilovich and Markovitch, 2007) with
the first paragraph of 100k Wikipedia articles (dump
from 2010).Pre-processing consisted of tokenization
and lemmatization.
3.3 Dataset Prediction
Given the similarity shifts in the different datasets
(cf. Table 1), we tried to predict what dataset an in-
stance belonged to on the basis of its vocabulary. We
built binary maxent classifiers for each dataset in the
development set, resulting in five dataset likelihood
features: dMSRpar, dSMTeuroparl, dMSRvid,
dOnWN, and dSMTnews.4 Pre-processing consisted
of tokenization and lemmatization.
3.4 Baseline
We considered the features included in the Takelab
Semantic Text Similarity system (?Saric? et al, 2012),
one of the top-systems in last year competition. This
system is used as a black box. The resulting features
are named tklab n, where n = [1, 21].
Our runs departed from three increasing subsets
of features: AE machine translation evaluation met-
rics and explicit semantic analysis, AED the pre-
vious set plus dataset prediction, and AED T the
previous set plus Takelab?s baseline features (cf. Ta-
ble 3). We performed a feature normalization, which
relied on the different feature?s distribution over the
entire dataset. Firstly, features were bounded in the
range ??3??2 in order to reduce the potentially neg-
ative impact of outliers. Secondly, we normalized
according to the z-score (Nardo et al, 2008, pp. 28,
84); i.e. x = (x ? ?)/?. As a result, each real-
valued feature distribution in the dataset has ? = 0
and ? = 1. During the model tuning stage we tried
with other numerous normalization options: normal-
izing each dataset independently, together with the
training set, and without normalization at all. Nor-
malizing according to the entire dev-test dataset led
to the best results
4We used the Stanford classifier; http://nlp.
stanford.edu/software/classifier.shtml
Table 2: Tuning process: parameter definition and feature
selection. Number of features at the beginning and end
of the feature selection step included.
run parameter def. feature sel.
c ? ? corr b e corr
AE 3.7 0.06 0.3 0.8257 19 14 0.8299
AED 3.8 0.03 0.2 0.8413 24 19 0.8425
AED T 2.9 0.02 0.3 0.8761 45 33 0.8803
4 Experiments and Results
Section 4.1 describes our model tuning strategy.
Sections 4.2 and 4.3 discuss the official and post-
competition results.
4.1 Model Tuning
We used only the dev-train partition (2012 training)
for tuning. By means of a 10-fold cross validation
process, we defined the trade-off (c), gamma (?),
and tube width (?) parameters for the regressor and
performed a backward-elimination feature selection
process (Witten and Frank, 2005, p. 294), indepen-
dently for the three experiments.
The results for the cross-validation process are
summarized in Table 2. The three runs allow for cor-
relations higher than 0.8. On the one hand, the best
regressor parameters obtain better results as more
features are considered, still with very small differ-
ences. On the other hand, the low correlation in-
crease after the feature selection step shows that a
few features are indeed irrelevant.
A summary of the features considered in each ex-
periment (also after feature selection) is displayed in
Table 3. The correlation obtained over the dev-test
partition are corrAE = 0.7269, corrAED = 0.7638,
and corrAEDT = 0.8044 ?it would have appeared
in the top-10 ranking of the 2012 competition.
4.2 Official Results
We trained three new regressors with the features
considered relevant by the tuning process, but using
the entire development dataset. The test 2013 parti-
tion was normalized again by means of z-score, con-
sidering the means and standard deviations of the en-
tire test dataset. Table 4 displays the official results.
Our best approach ?AE?, was positioned in rank
65. The worst results of run AED can be explained
by the difference in the nature of the test respect to
145
Table 3: Features considered at the beginning of each run, represented as empty squares (). Filled squares ()
represent features considered relevant after feature selection.
Feature AE AED AED T Feature AE AED AED T Feature AED T
DP-HWCM c-4    METEOR-pa    tklab 7 
DP-HWCM r-4    METEOR-st    tklab 8 
DP-Or(*)    METEOR-sy    tklab 9 
CP-STM-4    ESA    tklab 10 
SR-Or(*)    dMSRpar   tklab 11 
SR-Mr(*)    dSMTeuroparl   tklab 12 
SR-Or    dMSRvid   tklab 13 
DR-Or(*)    dOnWN   tklab 14 
DR-Orp(*)    dSMTnews   tklab 15 
BLEU    tklab 1  tklab 16 
NIST    tklab 2  tklab 17 
-TER    tklab 3  tklab 18 
-TERp-A    tklab 4  tklab 19 
ROUGE-W    tklab 5  tklab 20 
METEOR-ex    tklab 6  tklab 21 
Table 4: Official results for the three runs (rank included).
run headlines OnWN FNWN SMT mean
AE (65) 0.6092 0.5679 -0.1268 0.2090 0.4037
AED (83) 0.4136 0.4770 -0.0852 0.1662 0.3050
AED T (72) 0.5119 0.6386 -0.0464 0.1235 0.3671
the development dataset. AED T obtains worst re-
sults than AE on the headlines and SMT datasets.
The reason behind this behavior can be in the dif-
ference of vocabularies respect to that stored in the
Takelab system (it includes only the vocabulary of
the development partition). This could be the same
reason behind the drop in performance with respect
to the results previously obtained on the dev-test par-
tition (cf. Section 4.1).
4.3 Post-Competition Results
Our analysis of the official results showed the main
issue was normalization. Thus, we performed a
manifold of new experiments, using the same con-
figuration as in run AE, but applying other normal-
ization strategies: (a) z-score normalization, but ig-
noring the FNWN dataset (given its shift through
low values); (b) z-score normalization, but consid-
ering independent means and standard deviations for
each test dataset; and (c) without normalizing any of
dataset (including the regressor one).
Table 5 includes the results. (a) makes evident
that the instances in FNWN represent ?anomalies?
that harm the normalized values of the rest of sub-
sets. Run (b) shows that normalizing the test sets
Table 5: Post-competition experiments results
run headlines OnWN FNWN SMT mean
AE (a) 0.6210 0.5905 -0.0987 0.2990 0.4456
AE (b) 0.6072 0.4767 -0.0113 0.3236 0.4282
AE (c) 0.6590 0.6973 0.1547 0.3429 0.5208
independently is not a good option, as the regressor
is trained considering overall normalizations, which
explains the correlation decrease. Run (c) is com-
pletely different: not normalizing any dataset ?
both in development and test? reduces the influ-
ence of the datasets to each other and allows for the
best results. Indeed, this configuration would have
advanced practically forty positions at competition
time, locating us in rank 27.
Estimating the adequate similarities over FNWN
seems particularly difficult for our systems. We ob-
serve two main factors. (i) FNWN presents an im-
portant similarity shift respect to the other datasets:
nearly 90% of the instances similarity is lower than
2.5 and (ii) the average lengths of s1 and s2 are very
different: 30 vs 9 words. These characteristics made
it difficult for our MT evaluation metrics to estimate
proper similarity values (be normalized or not).
We performed two more experiments over
FNWN: training regressors with ESA as the only
feature, before and after normalization. The correla-
tion was 0.16017 and 0.3113, respectively. That is,
the normalization mainly affects the MT features.
146
5 Conclusions
In this paper we discussed on our participation to the
2013 Semeval Semantic Textual Similarity task. Our
approach relied mainly upon a combination of au-
tomatic machine translation evaluation metrics and
explicit semantic analysis. Building an RBF support
vector regressor with these features allowed us for a
modest result in the competition (our best run was
ranked 65 out of 89).
Acknowledgments
We would like to thank the organizers of this chal-
lenging task for their efforts.
This research work was partially carried out dur-
ing the tenure of an ERCIM ?Alain Bensoussan?
Fellowship. The research leading to these results re-
ceived funding from the EU FP7 Programme 2007-
2013 (grants 246016 and 247762). Our research
work is partially supported by the Spanish research
projects OpenMT-2 and SKATER (TIN2009-14675-
C03, TIN2012-38584-C06-01).
References
Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-
Agirre, and Weiwei Guo. 2013. *SEM 2013 Shared
Task: Semantic Textual Similarity, including a Pilot on
Typed-Similarity. In *SEM 2013: The Second Joint
Conference on Lexical and Computational Semantics.
Association for Computational Linguistics.
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An Automatic Metric for MT Evaluation with Im-
proved Correlation with Human Judgments. In Gold-
stein et al (Goldstein et al, 2005), pages 65?72.
George Doddington. 2002. Automatic Evaluation
of Machine Translation Quality Using N-Gram Co-
occurrence Statistics. In Proceedings of the Second
International Conference on Human Language Tech-
nology Research, pages 138?145, San Francisco, CA.
Morgan Kaufmann Publishers Inc.
Evgeniy Gabrilovich and Shaul Markovitch. 2007. Com-
puting Semantic Relatedness Using Wikipedia-based
Explicit Semantic Analysis. In Proceedings of the
20th International Joint Conference on Artificial Intel-
ligence, pages 1606?1611, San Francisco, CA, USA.
Morgan Kaufmann Publishers Inc.
Jesu?s Gime?nez and Llu??s Ma`rquez. 2010a. Asiya:
An Open Toolkit for Automatic Machine Translation
(Meta-)Evaluation. The Prague Bulletin of Mathemat-
ical Linguistics, (94).
Jesu?s Gime?nez and Llu??s Ma`rquez. 2010b. Linguistic
Measures for Automatic Machine Translation Evalua-
tion. Machine Translation, 24(3?4):209?240.
Jade Goldstein, Alon Lavie, Chin-Yew Lin, and Clare
Voss, editors. 2005. Proceedings of the ACL Work-
shop on Intrinsic and Extrinsic Evaluation Measures
for Machine Translation and/or Summarization. Asso-
ciation for Computational Linguistics.
Thorsten Joachims, 1999. Advances in Kernel Methods ?
Support Vector Learning, chapter Making large-Scale
SVM Learning Practical. MIT Press.
Chin-Yew Lin and Franz Josef Och. 2004. Auto-
matic Evaluation of Machine Translation Quality Us-
ing Longest Common Subsequence and Skip-Bigram
Statistics. In Proceedings of the 42nd Annual Meet-
ing of the Association for Computational Linguistics
(ACL 2002), Stroudsburg, PA. Association for Com-
putational Linguistics.
Ding Liu and Daniel Gildea. 2005. Syntactic Features
for Evaluation of Machine Translation. In Goldstein
et al (Goldstein et al, 2005), pages 25?32.
Michela Nardo, Michaela Saisana, Andrea Saltelli, Ste-
fano Tarantola, Anders Hoffmann, and Enrico Giovan-
nini. 2008. Handbook on Constructing Composite In-
dicators: Methodology and User Guide. OECD Pub-
lishing.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A Method for Automatic
Evaluation of Machine Translation. In Proceedings of
the 40th Annual Meeting of the Association for Com-
putational Linguistics (ACL 2002), pages 311?318,
Philadelphia, PA. Association for Computational Lin-
guistics.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study
of Translation Edit Rate with Targeted Human An-
notation. In Proceedings of Association for Machine
Translation in the Americas, pages 223?231.
Frane ?Saric?, Goran Glavas?, Mladen Karan, Jan ?Snajder,
and Bojana Dalbelo Bas?ic?. 2012. TakeLab: Sys-
tems for Measuring Semantic Text. In First Joint
Conference on Lexical and Computational Semantics
(*SEM), pages 441?448, Montre?al, Canada. Associa-
tion for Computational Linguistics.
Ian H. Witten and Eibe Frank. 2005. Data Mining: Prac-
tical Machine Learning Tools and Techniques. Mor-
gan Kaufmann, San Francisco, CA, 2 edition.
147
