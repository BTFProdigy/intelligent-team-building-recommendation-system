Semantic Parsing Based on FrameNet
Cosmin Adrian Bejan, Alessandro Moschitti, Paul Mora?rescu,
Gabriel Nicolae and Sanda Harabagiu
University of Texas at Dallas
Human Language Technology Research Institute
Richardson, TX 75083-0688, USA
Abstract
This paper describes our method based on Support
Vector Machines for automatically assigning seman-
tic roles to constituents of English sentences. This
method employs four different feature sets, one of
which being first reported herein. The combination
of features as well as the extended training data we
considered have produced in the Senseval-3 experi-
ments an F1-score of 92.5% for the unrestricted case
and of 76.3% for the restricted case.
1 Introduction
The evaluation of the Senseval-3 task for Automatic
Labeling of Semantic Roles is based on the annota-
tions made available by the FrameNet Project (Baker
et al, 1998). The idea of automatically identifying
and labeling frame-specific roles, as defined by the
semantic frames, was first introduced by (Gildea and
Jurasfky, 2002). Each semantic frame is character-
ized by a set of target words which can be nouns,
verbs or adjectives. This helps abstracting the the-
matic roles and adding semantics to the given frame,
highlighting the characteristic semantic features.
Frames are characterized by (1) target words or
lexical predicates whose meaning includes aspects of
the frame; (2) frame elements (FEs) which represent
the semantic roles of the frame and (3) examples of
annotations performed on the British National Cor-
pus (BNC) for instances of each target word. Thus
FrameNet frames are schematic representations of
situations lexicalized by the target words (predicates)
in which various participants and conceptual roles
are related (the frame elements), exemplified by sen-
tences from the BNC in which the target words and
the frame elements are annotated.
In Senseval-3 two different cases of automatic la-
beling of the semantic roles were considered. The
Unrestricted Case requires systems to assign FE la-
bels to the test sentences for which (a) the bound-
aries of each frame element were given and the tar-
get words identified. The Restricted Case requires
systems to (i) recognize the boundaries of the FEs
for each evaluated frame as well as to (ii) assign a
label to it. Both cases can be cast as two different
classifications: (1) a classification of the role when
its boundaries are known and (2) a classification of
the sentence words as either belonging to a role or
not1.
A similar approach was used for automati-
cally identifying predicate-argument structures in
English sentences. The PropBank annotations
(www.cis.upenn.edu/?ace) enable training for two
distinct learning techniques: (1) decision trees (Sur-
deanu et al, 2003) and (2) Support Vector Machines
(SVMs) (Pradhan et al, 2004). The SVMs produced
the best results, therefore we decided to use the same
learning framework for the Senseval-3 task for Auto-
matic Labeling of Semantic Roles. Additionally, we
have performed the following enhancements:
? we created a multi-class classifier for each frame,
thus achieving improved accuracy and efficiency;
? we combined some new features with features from
(Gildea and Jurasfky, 2002; Surdeanu et al, 2003;
Pradhan et al, 2004);
? we resolved the data sparsity problem generated
by limited training data for each frame, when using
the examples associated with any other frame from
FrameNet that had at least one FE shared with each
frame that was evaluated;
? we crafted heuristics that improved mappings from
the syntactic constituents to the semantic roles.
We believe that the combination of these four exten-
sions are responsible for our results in Senseval-3.
The remainder of this paper is organized as fol-
lows. Section 2 describes our methods of classify-
ing semantic roles whereas Section 3 describes our
method of identifying role boundaries. Section 4 de-
tails our heuristics and Section 5 details the exper-
imental results. Section 6 summarizes the conclu-
sions.
1The second classification represents the detection of role
boundaries. The semantic parsing defined as two different clas-
sification tasks was introduced in (Gildea and Jurasfky, 2002).
                                             Association for Computational Linguistics
                        for the Semantic Analysis of Text, Barcelona, Spain, July 2004
                 SENSEVAL-3: Third International Workshop on the Evaluation of Systems
2 Semantic role classification
The result of the role classifier on a sentence, as il-
lustrated in Figure 1, is the identification of semantic
roles of the FEs when the boundaries of each FE are
known. To be able to assign the labels of each FE, we
used three sets of features. Feature Set 1, illustrated
in Figure 2 was used in the work reported in (Gildea
and Jurasfky, 2002).
VP
S
VP
NP
People were fastening
TARGETAgent
NP
a rope
PP
to the ring
NP
GoalItem
Figure 1: Sentence with annotated frame elements.
? POSITION (pos) ? Indicates if the constituent appears  
before or after the the predicate in the sentence.
? VOICE (voice) ? This feature distinguishes between
active or passive voice for the predicate phrase.
are preserved.
of the evaluated phrase. Case and morphological information
? HEAD WORD (hw) ? This feature contains the head word
? PARSE TREE PATH (path): This feature contains the path
in the parse tree between the predicate phrase and the 
labels linked by direction symbols (up or down), e.g.
? PHRASE TYPE (pt): This feature indicates the syntactic
noun phrases only, and it indicates if the NP is dominated
by a sentence phrase (typical for subject arguments with
active?voice predicates), or by a verb phrase (typical 
for object arguments).
? GOVERNING CATEGORY (gov) ? This feature applies to
type of the phrase labeled as a frame element, e.g.
target word, expressed as a sequence of nonterminal
? TARGET WORD ? In our implementation this feature
(2) LEMMA which represents the target normalized to lower 
the case and morphological information preserved; and 
consists of two components: (1) WORD: the word itself with
case and infinitive form for the verbs or singular for nouns. 
NP for Agent in Figure 1.
NP    S    VP    VP for Agent in Figure 1.
Figure 2: Feature Set 1 (FS1)
Feature Set 2 was introduced in (Surdeanu et al,
2003) and it is illustrated in Figure 3. The CON-
TENT WORD (cw) feature illustrated in Figure 3
applies to PPs, SBARs and VPs, as it was reported
in (Surdeanu et al, 2003). For example, if the PP is
?in the past month?, instead of using ?in?, the head
of the PP, as a feature, ?month?, the head of the NP
is selected since it is more informative. Similarly, if
the SBAR is ?that occurred yesterday?, instead of us-
ing the head ?that? we select ?occurred?, the head of
the VP. When the VP ?to be declared? is considered,
?declared? is selected over ?to?.
Feature set 3 is a novel set of features introduced
in this paper and illustrated in Figure 4. Some
of the new features characterize the frame, e.g.
the frame name (FRAME-NAME); the frame FEs,
(NUMBER-FEs); or the target word associated with
the frame (TAGET-TYPE). Additional characteriza-
tion of the FEs are provided by by the GRAMMATI-
CAL FUNCTION feature and by the list of grammat-
ical functions of all FEs recognized in each sentence(
LIST Grammatical Function feature).
BOOLEAN NAMED ENTITY FLAGS ? A feature set comprising: 
? neOrganization: set to 1 if an organization is recognized in the phrase
? neLocation: set to 1 a location is recognized in the phrase
? nePerson: set to 1 if a person name is recognized in the phrase
? neMoney: set to 1 if a currency expression is recognized in the phrase
? nePercent: set to 1 if a percentage expression is recognized in the phrase
? neTime: set to 1 if a time of day expression is recognized in the phrase
? neDate: set to 1 if a date temporal expression is recognized in the phrase 
word from the constituent, different from the head word.
? CONTENT WORD (cw) ? Lexicalized feature that selects an informative 
PART OF SPEECH OF HEAD WORD (hPos) ? The part of speech tag of
the head word.
PART OF SPEECH OF CONTENT WORD (cPos) ?The part of speech 
tag of the content word.
NAMED ENTITY CLASS OF CONTENT WORD (cNE) ? The class of 
the named entity that includes the content word
Figure 3: Feature Set 2 (FS2)
In FrameNet, sentences are annotated with the
name of the sub-corpus. There are 12,456 possi-
ble names of sub-corpus. For the 40 frames eval-
uated in Senseval-3, there were 1442 names asso-
ciated with the example sentences in the training
data and 2723 names in the test data. Three of
the most frequent sub-corpus names are: ?V-trans-
other? (frequency=613), ?N-all? (frequency=562)
and ?V-trans-simple?(frequency=560). The name
of the sub-corpus indicates the relations between
the target word and some of its FEs. For ex-
ample, the ?V-trans-other? name indicated that the
target word is a transitive verb, and thus its FEs
are likely to have other roles than object or indi-
rect object. A sentence annotated with this sub-
corpus name is: ?Night?s coming, you can see
the black shadow on [Self?mover the stones] that
[TARGET rush] [Pathpast] and [Pathbetween your
feet.?]. For this sentence both FEs with the role of
Path are neither objects or indirect objects of the
transitive verb.
Feature SUPPORT VERBS considers the usage of
support expressions in FrameNet. We have found
that whenever adjectives are target words, their se-
mantic interpretation depends on their co-occurrence
with verbs like ?take?, ?become? or ?is?. Support
verbs are defined as those verbs that combine with a
state-noun, event-noun or state-adjective to create a
verbal predicate, allowing arguments of the verb to
serve as FEs of the frame evoked by the noun or the
adjective.
The CORENESS feature takes advantage of a
more recent implementation concept of core FEs
(vs. non-core FEs) in FrameNet. More specifi-
cally, the FrameNet developers classify frame ele-
ments in terms of how central they are to a particular
frame, distinguishing three levels: core, peripheral
and extra-thematic.
The features were used to produce two types of
examples: positive and negative examples. For each
FE of a frame, aside from the positive examples ren-
dered by the annotations, we considered as negative
examples all the annotations of the other FEs for the
same frame. The positive and the negative examples
were used for training the multi-class classifiers.
SUPPORT_VERBS that are recognized for adjective or noun target words
target word. The values of this feature are either (1) The POS of the head
of the VP containing the target word or (2) NULL if the target word does
not belong to a VP
or ADJECTIVE
LIST_CONSTITUENT (FEs): This feature represents a list of the syntactic
Grammatical Function: This feature indicates whether the FE is:
? an External Argument (Ext)
? an Object (Obj)
? a Complement (Comp)
? a Modifier (Mod)
? Head noun modified by attributive adjective (Head)
? Genitive determiner (Gen)
? Appositive (Appos)
LIST_Grammatical_Function: This feature represents a list of the 
grammatical functions of the FEs recognized in the sentence.
in each sentence.
FRAME_NAME: This feature indicates the name of the semantic frame 
for which FEs are labeled
COVERAGE: This feature indicates whether there is a syntactic structure
in the parse tree that perfectly covers the FE
a conceptually necessary participant of a frame. For example, in the 
are: (1) core; (2) peripheral and (3) extrathemathic. FEs that mark notions
such as Time, Place, Manner and Degree are peripheral. Extrathematic
FEs situate an event against a backdrop of another event, by evoking 
a larger frame for which the target event fills a role.
SUB_CORPUS: In FrameNet, sentences are annotated with the name
of the subcorpus they belong to. For example, for a verb target word,
to a FE included in a relative clause headed by a wh?word.
(2) a hyponym of sense 1 of PERSON in WordNet
(1) a personal pronoun or
HUMAN: This feature indicates whether the syntactic phrase is either
TARGET?TYPE: the lexical class of the target word, e.g. VERB, NOUN
consituents covering each FE of the frame recognized in a sentence. 
For the example illustrated in Figure 1, the list is: [NP, NP, PP]
NUMBER_FEs: This feature indicates how many FEs were recognized 
have the role of predicate for the FEs. For example, if the target word is
"clever" in the sentence "Smith is very clever, but he?s no Einstein", the 
the FE "Smith" is an argument of the support verb "is"? rather than of the
CORENESS: This feature indicates whether the FE instantiates
REVENGE frame, Punishment is a core element. The values 
V?swh represents a subcorpus in which the target word is a predicate
Figure 4: Feature Set 3 (FS3)
Our multi-class classification allows each FE to be
initially labeled with more than one role when sev-
eral classifiers decide so. For example, for the AT-
TACHING frame, an FE may be labeled both as Goal
and as Item if the classifiers for the Goal and Item
select it as a possible role. To choose the final label,
we select the classification which was assigned the
largest score by the SVMs.
PARSE TREE PATH WITH UNIQUE DELIMITER ?  This feature removes
the direction in the path, e.g. VBN?VP?ADVP
PARTIAL PATH ? This feature uses only the path from the constituent to
the lowest common ancestor of the predicate and the constituent
FIRST WORD ? First word covered by constituent
FIRST POS ? POS of first word covered by constituent
LEFT CONSTITUENT ? Left sibling constituent label
RIGHT HEAD ? Right sibling head word
RIGHT POS HEAD ? Right sibling POS of head word
LAST POS ? POS of last word covered by the constituent
LEFT HEAD ? Left sibling head word
LEFT POS HEAD ? Left sibling POS of head word
RIGHT CONSTITUENT ? Right sibling constituent label
PP PREP ? If constituent is labeled PP get first word in PP
DISTANCE ? Distance in the tree from constituent to the target word
LAST WORD ? Last word covered by the constituent
Figure 5: Feature Set 4 (FS4)
3 Boundary Detection
The boundary detection of each FE was required
in the Restricted Case of the Senseval-3 evalua-
tion. To classify a word as belonging to an FE
or not, we used all the entire Feature Set 1 and
2. From the Feature Set 3 we have used only
four features: the Support- Verbs feature; the
Target-Type feature, the Frame-Name feature
and the Sub Corpus feature. For this task we have
also used Feature Set 4, which were first introduced
in (Pradhan et al, 2004). The Feature Set 4 is il-
lustrated in Figure 5. After the boundary detec-
tion was performed, the semantic roles of each FE
were assigned using the role classifier trained for the
Restricted Case
4 Heuristics
Frequently, syntactic constituents do not cover ex-
actly FEs. For the Unrestricted Case we imple-
mented a very simple heuristic: when there is no
parse-tree node that exactly covers the target role r
but a subset of adjacent nodes perfectly match r,
we merge them in a new NPmerge node. For the
Restricted Case, a heuristic for adjectival and nomi-
nal target words w adjoins consecutive nouns that are
in the same noun phrase as w.
5 Experimental Results
In the Senseval-3 task for Automatic Labeling of
Semantic Roles 24,558 sentences from FrameNet
were assigned for training while 8,002 for testing.
We used 30% of the training set (7367 sentences)
as a validation-set for selecting SVM parameters
that optimize accuracy. The number of FEs for
which labels had to be assigned were: 51,010 for
the training set; 15,924 for the validation set and
16,279 for the test set. We used an additional set
of 66,687 sentences (hereafter extended data) as ex-
tended data produced when using the examples as-
sociated with any other frame from FrameNet that
had at least one FE shared with any of the 40
frames evaluated in Senseval-3. These sentences
were parsed with the Collins? parser (Collins, 1997).
The classifier experiments were carried out using the
SVM-light software (Joachims, 1999) available at
http://svmlight.joachims.org/with a poly-
nomial kernel2 (degree=3).
5.1 Unrestricted Task Experiments
For this task we devised four different experiments
that used four different combination of features: (1)
FS1 indicates using only Feature Set 1; (2) +H in-
dicates that we added the heuristics; (3) +FS2+FS3
indicates that we add the feature Set 2 and 3; and
(4) +E indicates that the extended data has also been
used. For each of the four experiments we trained 40
multi-class classifiers, (one for each frame) for a total
of 385 binary role classifiers. The following Table il-
lustrates the overall performance over the validation-
set. To evaluate the results we measure the F1-score
by combining the precision P with the recall R in the
formula F1 = 2?P?RP+R .
FS1 +H +H+FS2+FS3 +H+FS2+FS3+E
84.4 84.9 91.7 93.1
5.2 Restricted Task Experiments
In order to find the best feature combination for this
task we carried out some preliminary experiments
over five frames. In Table 1, the row labeled B lists
the F1-score of boundary detection over 4 different
feature sets: FS1, +H, +FS4 and +E, the extended
data. The row labeled R lists the same results for the
whole Restricted Case.
Table 1: Restrictive experiments on validation-set.
+FS1 +H +H+FS2+FS3 +H+FS4+E
B 80.29 80.48 84.76 84.88
R 74.9 75.4 78 78.9
Table 1 illustrates the overall performance (bound-
ary detection and role classification) of automatic se-
mantic role labeling. The results listed in Tables 1
and 2 were obtained by comparing the FE bound-
aries identified by our parser with those annotated in
FrameNet. We believe that these results are more
2In all experiments and for any classifier, we used the default
SVM-light regularization parameter (e.g., C = 1 for normalized
kernels) and a cost-factor j = 100 to adjust the rate between
Precision and Recall.
indicative of the performance of our systems than
those obtained when using the scorer provided by
Senseval-3. When using this scorer, our results have
a precision of 89.9%, recall of 77.2% and an F1-
score of 83.07% for the Restricted Case.
Table 2: Results on the test-set.
Precision Recall F1
Unrestricted Case 94.5 90.6 92.5
Boundary Detection 87.3 75.1 80.7
Restricted Case 82.4 71.1 76.3
To generate the final Senseval-3 submissions we
selected the most accurate models (for unrestricted
and restricted tasks) of the validation experiments.
Then we re-trained such models with all training data
(i.e. our training plus validation data) and the set-
ting (parameters, heuristics and extended data) de-
rived over the validation-set. Finally, we run all clas-
sifiers on the test-set of the task. Table 2 illustrates
the final results for both sub-tasks.
6 Conclusions
In this paper we describe a method for automatically
labeling semantic roles based on support vector ma-
chines (SVMs). The training benefits from an ex-
tended data set on which multi-class classifiers were
derived. The polynomial kernel of the SVMs en-
able the combination of four feature sets that pro-
duced very good results both for the Restricted Case
and the Unrestricted Case. The paper also describes
some heuristics for mapping syntactic constituents
onto FEs.
References
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet project. In Proceedings
of the COLING-ACL, Montreal, Canada.
Michael Collins. 1997. Three generative, lexicalized
models for statistical parsing. In Proceedings of the
ACL-97, pages 16?23.,
Daniel Gildea and Daniel Jurasfky. 2002. Automatic la-
beling of semantic roles. Computational Linguistic,
28(3):496?530.
T. Joachims. 1999. Making Large-Scale SVM Learning
Practical. In B. Schlkopf, C. Burges, and MIT-Press.
A. Smola (ed.), editors, Advances in Kernel Methods -
Support Vector Learning.
Sameer Pradhan, Kadri Hacioglu, Valeri Krugler, Wayne
Ward, James H. Martin, and Daniel Jurafsky. 2004.
Support vector learning for semantic argument classifi-
cation. Journal of Machine Learning Research.
Mihai Surdeanu, Sanda M. Harabagiu, John Williams,
and John Aarseth. 2003. Using predicate-argument
structures for information extraction. In Proceedings
of (ACL-03).
A Semantic Kernel for Predicate Argument Classification
Alessandro Moschitti and Cosmin Adrian Bejan
University of Texas at Dallas
Human Language Technology Research Institute
Richardson, TX 75083-0688, USA
alessandro.moschitti@utdallas.edu
ady@hlt.utdallas.edu
Abstract
Automatically deriving semantic structures
from text is a challenging task for machine
learning. The flat feature representations, usu-
ally used in learning models, can only partially
describe structured data. This makes difficult
the processing of the semantic information that
is embedded into parse-trees.
In this paper a new kernel for automatic clas-
sification of predicate arguments has been de-
signed and experimented. It is based on sub-
parse-trees annotated with predicate argument
information from PropBank corpus. This ker-
nel, exploiting the convolution properties of
the parse-tree kernel, enables us to learn which
syntactic structures can be associated with the
arguments defined in PropBank. Support Vec-
tor Machines (SVMs) using such a kernel clas-
sify arguments with a better accuracy than
SVMs based on linear kernel.
1 Introduction
Several linguistic theories, e.g. (Jackendoff, 1990), claim
that semantic information in natural language texts is
connected to syntactic structures. Hence, to deal with nat-
ural language semantics, the learning algorithm should be
able to represent and process structured data. The classi-
cal solution adopted for such tasks is to convert syntax
structures in a flat feature representation, which is suit-
able for a given learning model. The main drawback is
structures may not be properly represented by flat fea-
tures as: (1) these latter may not be able to capture the
required properties or (2) the feature designer may not
know what structure properties enable the processing of
semantic information.
In particular, these problems arise for semantic infor-
mation represented via predicate argument structures de-
fined on syntactic parse trees. For example, Figure 1
shows the parse tree of the sentence: "Paul gives
a lecture in Rome" along with the annotation of
predicate arguments.
A predicate may be a verb or a noun or an adjective
whereas generally Arg 0 stands for agent, Arg 1 for di-
rect object or theme or patient and ArgM may indicate
locations, as in our example. A standard for predicate ar-
gument annotation is provided in the PropBank project
(Kingsbury and Palmer, 2002). It has produced one
million word corpus annotated with predicate-argument
structures on top of the Penn Treebank 2 Wall Street Jour-
nal texts. In this way, for a large number of the Penn
TreeBank parse-trees, there are available predicate anno-
tations in a style similar to that shown in Figure 1.
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Predicate 
Arg. 0 
Arg. M 
S
N
NP 
D N 
VP 
V Paul 
in 
gives 
a lecture 
PP 
IN N 
Rome 
Arg. 1 
Figure 1: Predicate arguments in a parse-tree representation.
In PropBank only verbs are considered to be predicates
whereas arguments are labeled sequentially from Arg 0
to Arg 91. In addition to these core arguments, adjunctive
arguments are marked up. They include functional tags,
e.g. ArgM-DIR indicates a directional, ArgM-LOC in-
dicates a locative and ArgM-TMP stands for a temporal.
An example of PropBank markup is:
1Other arguments are: Arg 2 for indirect object or benefac-
tive or instrument or attribute or end state, Arg 3 for start point
or benefactive or attribute, Arg4 for end point and so on.
[Arg10 Analysts ] have been [predicate1 expecting ] [Arg11
a GM-Jaguar pact ] that would [predicate2 give ] [Arg22 the
U.S. car maker ] [Arg21 an eventual 30% state in the British
Company ].
Automatically recognizing the boundaries and classi-
fying the type of arguments allows Natural Language
Processing systems (e.g. Information Extraction, Ques-
tion Answering or Summarization) to answer questions
such as ?Who?, ?When?, ?What?, ?Where?, ?Why?, and
so on.
Given the importance of this task for Natural Lan-
guage Processing applications, several machine learning
approaches for argument identification and classification
have been developed (Gildea and Jurasky, 2002; Sur-
deanu et al, 2003; Hacioglu et al, 2003; Chen and Ram-
bow, 2003; Gildea and Hockenmaier, 2003). Their com-
mon characteristic is the adoption of feature spaces that
model predicate-argument structures in a flat representa-
tion. The major problem of this choice is that there is
no linguistic theory that supports the selection of syntac-
tic features to recognize semantic structures. As a con-
sequence, researchers are still trying to extend the basic
features with other ones, e.g. (Surdeanu et al, 2003), to
improve the flat feature space.
Convolution kernels are a viable alternative to flat fea-
ture representation that aims to capture the structural in-
formation in term of sub-structures. The kernel functions
can be used to measure similarities between two objects
without explicitly evaluating the object features. That
is, we do not need to understand which syntactic feature
may be suited for representing semantic data. We need
only to define the similarity function between two seman-
tic structures. An example of convolution kernel on the
parse-tree space is given in (Collins and Duffy, 2002).
The aim was to design a novel syntactic parser by look-
ing at the similarity between the testing parse-trees and
the correct parse-trees available for training.
In this paper, we define a kernel in a semantic struc-
ture space to learn the classification function of predicate
arguments. The main idea is to select portions of syn-
tactic/semantic trees that include the target <predicate,
argument> pair and to define a kernel function between
these objects. If our similarity function is well defined the
learning model will converge and provide an effective ar-
gument classification.
Experiments on PropBank data show not only that
Support Vector Machines (SVMs) trained with the pro-
posed semantic kernel converge but also that they have a
higher accuracy than SVMs trained with a linear kernel
on the standard features proposed in (Gildea and Jurasky,
2002). This provides a piece of evidence that convolution
kernel can be used to learn semantic linguistic structures.
Moreover, interesting research lines on the use of ker-
nel for NLP are enabled, e.g. question classification in
Question/Answering or automatic template designing in
Information Extraction.
The remaining of this paper is organized as follows:
Section 2 defines the Predicate Argument Extraction
problem and the standard solution to solve it. In Section
3 we present our approach based on the parse-tree kernel
whereas in Section 4 we show our comparative results
between SVMs using standard features and the proposed
kernel. Finally, Section 5 summarizes the conclusions.
2 Automatic Predicate-Argument
extraction
Given a sentence in natural language, all the predicates
associated with its verbs have to be identified along with
their arguments. This problem can be divided in two sub-
tasks: (a) detection of the target argument boundaries,
i.e. all its compounding words, and (b) classification of
the argument type, e.g. Arg0 or ArgM.
A direct approach to learn both detection and classifi-
cation of predicate arguments is summarized by the fol-
lowing steps:
1. Given a sentence from the training-set, generate a
full syntactic parse-tree;
2. let P and A be the set of predicates and the set of
parse-tree nodes (i.e. the potential arguments), re-
spectively;
3. for each pair <p, a> ? P ?A:
? extract the feature representation set, Fp,a;
? if the subtree rooted in a covers exactly the
words of one argument of p, put Fp,a in T+
(positive examples), otherwise put it in T?
(negative examples).
For example, in Figure 1, for each combination of the
predicate give with the nodes N, S, VP, V, NP, PP, D or
IN the instances F?give?,a are generated. In case the node
a exactly covers Paul, a lecture or in Rome, it will be a
positive instance otherwise it will be a negative one, e.g.
F?give?,?IN?.
The above T+ and T? sets can be re-organized as posi-
tive T+argi and negative T?argi examples for each argument
i. In this way, an individual ONE-vs-ALL classifier for
each argument i can be trained. We adopted this solution
as it is simple and effective (Pradhan et al, 2003). In the
classification phase, given a sentence of the test-set, all
its Fp,a are generated and classified by each individual
classifier. As a final decision, we select the argument as-
sociated with the maximum value among the scores pro-
vided by the SVMs2, i.e. argmaxi?S Ci, where S is
the target set of arguments.
2This is a basic method to pass from binary categorization
2.1 Standard feature space
The discovering of relevant features is, as usual, a com-
plex task, nevertheless there is a common consensus on
the basic features that should be adopted. These stan-
dard features, firstly proposed in (Gildea and Jurasky,
2002), refer to a flat information derived from parse trees,
i.e. Phrase Type, Predicate Word, Head Word, Governing
Category, Position and Voice. Table 1 presents the stan-
dard features and exemplifies how they are extracted from
a given parse tree.
- Phrase Type: This feature indicates the syntactic type
of the phrase labeled as a predicate argument, e.g. NP
for Arg1 in Figure 1.
- Parse Tree Path: This feature contains the path in
the parse tree between the predicate and the argument
phrase, expressed as a sequence of nonterminal labels
linked by direction (up or down) symbols, e.g. V ? VP
? NP for Arg1 in Figure 1.
- Position: Indicates if the constituent, i.e. the potential
argument, appears before or after the predicate in the
sentence, e.g. after for Arg1 and before for Arg0 (see
Figure 1).
- Voice: This feature distinguishes between active or
passive voice for the predicate phrase, e.g. active for
every argument (see Figure 1).
- Head Word: This feature contains the head word of the
evaluated phrase. Case and morphological information
are preserved, e.g. lecture for Arg1 (see Figure 1).
- Governing Category: This feature applies to noun
phrases only, and it indicates if the NP is dominated by
a sentence phrase (typical for subject arguments with
active voice predicates), or by a verb phrase (typical for
object arguments), e.g. the NP associated with Arg1 is
dominated by a verbal phrase VP (see Figure 1).
- Predicate Word: In our implementation this feature
consists of two components: (1) the word itself with
the case and morphological information preserved, e.g.
gives for all arguments; and (2) the lemma which rep-
resents the verb normalized to lower case and infinitive
form, e.g. give for all arguments (see Figure 1).
Table 1: Standard features extracted from parse-trees.
For example, the Parse Tree Path feature represents the
path in the parse-tree between a predicate node and one of
its argument nodes. It is expressed as a sequence of non-
terminal labels linked by direction symbols (up or down),
e.g. in Figure 1, V?VP?NP is the path between the pred-
icate to give and the argument 1, a lecture. If two pairs
<p1, a1> and <p2, a2> have a Path that differs even for
one character (e.g. a node in the parse-tree) the match
will not be carried out, preventing the learning algorithm
to generalize well on unseen data. In order to address also
into a multi-class categorization problem; several optimization
have been proposed, e.g. (Goh et al, 2001).
this problem, next section describes a novel kernel space
for predicate argument classification.
3 A semantic kernel for argument
classification
We consider the predicate argument structures annotated
in PropBank as our semantic space. Many semantic struc-
tures may constitute the objects of our space. Some possi-
bilities are: (a) the selection of the whole sentence parse-
tree, in which the target predicate is contained or (b) the
selection of the sub-tree that encloses the whole predi-
cate annotation (i.e. all its arguments). However, both
choices would cause an exponential explosion on the po-
tential sub-parse-trees that have to be classified during
the testing phase. In fact, during this phase we do not
know which are the arguments associated with a predi-
cate. Thus, we need to build all the possible structures,
which contain groups of potential arguments for the tar-
get predicate. More in detail, assuming that S is the set of
PropBank argument types, and m is the maximum num-
ber of entries that the target predicate can have, we have
to evaluate
(|S|
m
)
argument combinations for each target
predicate.
In order to define an efficient semantic space we se-
lect as objects only the minimal sub-structures that in-
clude one predicate with only one of its arguments. For
example, Figure 2 illustrates the parse-tree of the sen-
tence "Paul delivers a lecture in formal
style". The circled substructures in (a), (b) and (c) are
our semantic objects associated with the three arguments
of the verb to deliver, i.e. <deliver, Arg0>, <deliver,
Arg1> and <deliver, ArgM>. In this formulation, only
one of the above structures is associated with each pred-
icate/argument pair, i.e. Fp,a contain only one of the cir-
cled sub-trees.
We note that our approach has the following properties:
? The overall semantic feature space F contain sub-
structures composed of syntactic information em-
bodied by parse-tree dependencies and semantic in-
formation under the form of predicate/argument an-
notation.
? This solution is efficient as we have to classify at
maximum |A| nodes for each predicate, i.e. the set
of the parse-tree nodes of a testing sentence.
? A constituent cannot be part of two different argu-
ments of the target predicate, i.e. there is no over-
lapping between the words of two arguments. Thus,
two semantic structures Fp1,a1 and Fp2,a23, asso-
3Fp,a was defined as the set of features of our objects
<p, a>. Since in our kernel we have only one element in Fp,a
with an abuse of notation we use it to indicate the objects them-
selves.
 S
N
NP 
D N 
VP 
V Paul 
in 
delivers 
a 
   talk 
PP 
IN 
  NP 
jj 
Fdeliver, Arg0 
 formal 
 N 
      style 
Arg. 0 
a) S
N
NP 
D N 
VP 
V Paul 
in 
delivers 
a 
   talk 
PP 
IN 
  NP 
jj 
 formal 
 N 
      style 
Fdeliver, Arg1 
b) S
N
NP 
D N 
VP 
V Paul 
in 
delivers 
a 
   talk 
PP 
IN 
  NP 
jj 
 formal 
 N 
      style 
Arg. 1 
Fdeliver, ArgM 
 
c) 
Arg. M 
Figure 2: Semantic feature space for predicate argument classification.
ciated with two different arguments, cannot be in-
cluded one in the other. This property is important
because, a convolution kernel would not be effective
to distinguish between an object and its sub-parts.
Once defined our semantic space we need to design a
kernel function to measure a similarity between two ob-
jects. These latter may still be seen as described by com-
plex features but such a similarity is carried out avoiding
the explicit feature computation. For this purpose we de-
fine a mapping ? : F ? F ? such as:
~x = (x1, ..., x|F |) ? ?(~x) = (?1(~x), .., ?|F ?|(~x)),
where F ? allows us to design an efficient semantic kernel
K(~x, ~z) =<?(~x) ? ?(~z)>.
3.1 The Semantic Kernel (SK)
Given the semantic objects defined in the previous sec-
tion, we design a convolution kernel in a way similar
to the parse-tree kernel proposed in (Collins and Duffy,
2002). Our feature set F ? is the set of all possible sub-
structures (enumerated from 1 to |F ?|) of the semantic
objects extracted from PropBank. For example, Figure
3 illustrates all valid fragments of the semantic structure
Fdeliver,Arg1 (see also Figure 2). It is worth noting that
the allowed sub-trees contain the entire (not partial) pro-
duction rules. For instance, the sub-tree [NP [D a]] is
excluded from the set of the Figure 3 since only a part
of the production NP ? D N is used in its generation.
However, this constraint does not apply to the production
VP? V NP PP along with the fragment [VP [V NP]] as
the subtree [VP [PP [...]]] is not considered part of the
semantic structure.
Even if the cardinality of F ? will be very large the eval-
uation of the kernel function is polynomial in the number
of parse-tree nodes.
More precisely, a semantic structure ~x is mapped in
?(~x) = (h1(~x), h2(~x), ...), where the feature function
hi(~x) simply counts the number of times that the i-
th sub-structure of the training data appears in ~x. Let
 
 
 
 
 
 
 
 
 
 
NP 
D N 
a 
  talk 
NP 
D N 
NP 
D N 
a 
D N 
a   talk 
NP 
D N NP 
D N 
VP 
V 
delivers 
a 
   talk 
V 
delivers 
NP 
D N 
VP 
V 
a 
   talk 
NP 
D N 
VP 
V 
NP 
D N 
VP 
V 
a 
NP 
D
VP 
V 
   talk 
N 
a 
NP 
D N 
VP 
V 
delivers 
   talk 
NP 
D N 
VP 
V 
delivers NP D N 
VP 
V 
delivers 
NP 
VP 
V NP 
VP 
V 
delivers 
  talk 
Figure 3: All 17 valid fragments of the semantic structure as-
sociated with Arg 1 (see Figure 2).
Ii(n) be the indicator function: 1 if the sub-structure
i is rooted at node n and 0 otherwise. It follows that
h(~x) = ?n?N Ii(n), where N is the set of the ~x?s nodes.
Therefore, the kernel4 function is:
K(~x, ~z) = ~h(~x) ? ~h(~z) =
=
?
i
( ?
nx?Nx
Ii(nx)
)( ?
nz?Nz
Ii(nz)
) =
=
?
nx?Nx
?
nz?Nz
?
i
Ii(nx)Ii(nz) (1)
where Nx and Nz are the nodes in x and z, respec-
tively. In (Collins and Duffy, 2002), it has been shown
that Eq. 1 can be computed in O(|Nx| ? |Nz|) by eval-
uating ?(nx, nz) =
?
i Ii(nx)Ii(nz) with the following
recursive equations:
? if the production at nx and nz are different then
?(nx, nz) = 0;
4Additionally, we carried out the normalization in the kernel
space, thus the final kernel is K?(~x, ~z) = K(~x,~z)?
K(~x,~x)?K(~z,~z)
.
? if the production at nx and nz are the same, and nx
and nz are pre-terminals then
?(nx, nz) = 1; (2)
? if the production at nx and nz are the same, and nx
and nz are not pre-terminals then
?(nx, nz) =
nc(nx)?
j=1
(1 + ?(ch(nx, j), ch(nz, j))),
(3)
where nc(nx) is the number of children of nx and
ch(n, i) is the i-th child of the node n. Note that as
the productions are the same ch(nx, i) = ch(nz, i).
This kind of kernel has the drawback of assigning more
weight to larger structures while the argument type does
not depend at all on the size of its structure. In fact two
sentences such as:
(1) [Arg0 Paul ][predicate delivers ] [Arg1 a lecture] and(2) [Arg0 Paul ][predicate delivers ][Arg1 a plan on the de-
tection of theorist groups active in the North Iraq]
have the same argument type with a very different size.
To overcome this problem we can scale the relative im-
portance of the tree fragments with their size. For this
purpose a parameter ? is introduced in equations 2 and 3
obtaining:
?(nx, nz) = ? (4)
?(nx, nz) = ?
nc(nx)?
j=1
(1+?(ch(nx, j), ch(nz, j))) (5)
It is worth noting that even if the above equations
define a kernel function similar to the one proposed in
(Collins and Duffy, 2002), the substructures on which SK
operates are different from the parse-tree kernel. For ex-
ample, Figure 3 shows that structures such as [VP [V]
[NP]], [VP [V delivers ] [NP]] and [VP [V] [NP [DT
N]]] are valid features, but these fragments (and many
others) are not generated by a complete production, i.e.
VP? V NP PP. As a consequence they are not included
in the parse-tree kernel representation of the sentence.
3.2 Comparison with Standard Features
We have synthesized the comparison between stan-
dard features and the SK representation in the follow-
ing points. First, SK estimates a similarity between
two semantic structures by counting the number of
sub-structures that are in common. As an example,
the similarity between the two structures in Figure 2,
F?delivers?,Arg0 and F?delivers?,Arg1, is equal to 1 since
they have in common only the [V delivers] substruc-
ture. Such low value depends on the fact that different
argument types tend to appear in different structures.
On the contrary, if two structures differ only for a few
nodes (especially terminal or near terminal nodes) the
similarity remains quite high. For example, if we change
the tense of the verb to deliver (Figure 2) in delivered,
the [VP [V delivers] NP] subtree will be transformed
in [VP [VBD delivered] NP], where the NP is un-
changed. Thus, the similarity with the previous structure
will be quite high as: (1) the NP with all sub-parts will
be matched and (2) the small difference will not highly
affect the kernel norm and consequently the final score.
This conservative property does not apply to the Parse
Tree Path feature which is very sensible to small changes
in the tree-structure, e.g. two predicates, expressed in dif-
ferent tenses, generate two different Path features.
Second, some information contained in the standard
features is embedded in SK: Phrase Type, Predicate Word
and Head Word explicitly appear as structure fragments.
For example, in Figure 3 are shown fragments like [NP
[DT] [N]] or [NP [DT a] [N talk]] which explicitly en-
code the Phrase Type feature NP for Arg 1 in Figure 2.b.
The Predicate Word is represented by the fragment [V
delivers] and the Head Word is present as [N talk].
Finally, Governing Category, Position and Voice can-
not be expressed by SK. This suggests that a combination
of the flat features (especially the named entity class (Sur-
deanu et al, 2003)) with SK could furthermore improve
the predicate argument representation.
4 The Experiments
For the experiments, we used PropBank
(www.cis.upenn.edu/?ace) along with Penn-
TreeBank5 2 (www.cis.upenn.edu/?treebank)
(Marcus et al, 1993). This corpus contains about 53,700
sentences and a fixed split between training and testing
which has been used in other researches (Gildea and
Jurasky, 2002; Surdeanu et al, 2003; Hacioglu et al,
2003; Chen and Rambow, 2003; Gildea and Hocken-
maier, 2003; Gildea and Palmer, 2002; Pradhan et al,
2003). In this split, Sections from 02 to 21 are used for
training, section 23 for testing and sections 1 and 22 as
developing set. We considered all PropBank arguments
from Arg0 to Arg9, ArgA and ArgM even if only Arg0
from Arg4 and ArgM contain enough training/testing
data to affect the global performance. In Table 2 some
characteristics of the corpus used in our experiments are
reported.
The classifier evaluations were carried out using
the SVM-light software (Joachims, 1999) available at
http://svmlight.joachims.org/ with the de-
fault linear kernel for the standard feature evaluations.
5We point out that we removed from the Penn TreeBank the
special tags of noun phrases like Subj and TMP as parsers usu-
ally are not able to provide this information.
Table 2: Characteristics of the corpus used in the experiments.
Number of Args Number of unique
train. test-set Std. features
Arg0 34,955 2,030 12,520
Arg1 44,369 2,714 14,442
Arg2 10,491 579 6,422
Arg3 2,028 106 1,591
Arg4 1,611 67 918
ArgM 30,464 1,930 7,647
Total 123,918 7,426 21,710
For processing our semantic structures, we implemented
our own kernel and we used it inside SVM-light.
The classification performances were evaluated using
the f1 measure for single arguments as each of them has
a different Precision and Recall and by using the accu-
racy for the final multi-class classifier as the global Pre-
cision = Recall = accuracy. The latter measure allows
us to compare the results with previous literature works,
e.g. (Gildea and Palmer, 2002; Surdeanu et al, 2003; Ha-
cioglu et al, 2003; Chen and Rambow, 2003; Gildea and
Hockenmaier, 2003).
To evaluate the effectiveness of our new kernel we di-
vided the experiments in 3 steps:
? The evaluation of SVMs trained with standard fea-
tures in a linear kernel, for comparison purposes.
? The estimation of the ? parameter (equations 4 and
5) for SK from the validation-set .
? The performance measurement of SVMs, using SK
along with ? computed in the previous step.
Additionally, both Linear and SK kernels were evalu-
ated using different percentages of training data to com-
pare the gradients of their learning curves.
4.1 SVM performance on Linear and Semantic
Kernel
The evaluation of SVMs using a linear kernel on the stan-
dard features did not raise particular problems. We used
the default regularization parameter (i.e., C = 1 for nor-
malized kernels) and we tried a few cost-factor values
(i.e., j ? {0.1, 1, 2, 3, 4, 5}) to adjust the rate between
precision and recall. Given the huge amount of training
data, we used only 30% of training-set in these valida-
tion experiments. Once the parameters were derived, we
learned 6 different classifiers (one for each role) and mea-
sured their performances on the test-set.
For SVM, using the Semantic Kernel, we derived that
a good ? parameter for the validation-set is 0.4. In Figure
4 we report the curves, f1 function of ?, for the 3 largest
(in term of training examples) arguments on the test-set.
0.82
0.85
0.88
0.91
0.94
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9?
f1
Arg0Arg1ArgM
Figure 4: SVM f1 for Arg0, Arg1 and ArgM with respect to
different ? values.
We note that the maximal value from the validation-set is
also the maximal value from the test-set for every argu-
ment. This suggests that: (a) it is easy to detect an optimal
parameter and (b) there is a common (to all arguments) ?-
value which defines how much the size of two structures
impacts on their similarity. Moreover, some experiments
using ? greater than 1 have shown a remarkable decrease
in performance, i.e. a correct ? seems to be essential to
obtain a good generalization6 of the training-set.
Table 3: f1 of SVMs using linear and semantic kernel com-
pared to literature models for argument classification.
Args SVM SVM Prob. C4.5
STD SK STD STD EXT
Arg0 87.79 88.35 - - -
Arg1 82.43 86.25 - - -
Arg2 54.10 68.52 - - -
Arg3 31.65 49.46 - - -
Arg4 62.81 66.66 - - -
ArgM 91.97 94.07 - - -
multi-class
accuracy 84.07 86.78 82.8 78.76 83.74
Table 3 reports the performances of SVM trained with
the standard features (STD column) and with the Seman-
tic Kernel (SK column). In columns Prob. and C4.5 are
reported the results for argument classification achieved
in (Gildea and Palmer, 2002) and (Surdeanu et al, 2003).
This latter used C4.5 model on standard feature set (STD
sub-column) and on an extended feature set (EXT sub-
column). We note that: (a) SVM performs better than
the probabilistic approach and C4.5 learning model inde-
pendently of the adopted features and (b) the Semantic
Kernel considerably improves the standard feature set.
In order to investigate if SK generalizes better than the
6For example, ? = 1 would generate low kernel values be-
tween small and large structures. This is in contrast with the
observation in Section 3.1, i.e. argument type is independent of
its constituent size.
linear kernel, we measured the performances by select-
ing different percentages of training data. Figure 5 shows
the curves for the three roles Arg0, Arg1 and ArgM, re-
spectively for linear and semantic kernel whereas Figure
6 shows the multi-class classifier f1 plots.
0.7
0.73
0.76
0.79
0.82
0.85
0.88
0.91
0.94
0 15 30 45 60 75 90% Training Data
f1
Arg0-SK Arg1-SK ArgM-SK
Arg0-STD Arg1-STD ArgM-STD
Figure 5: Arg0, Arg1 and ArgM evaluations over SK and the
linear kernel of standard features with respect to different per-
centages of training data.
0.7
0.73
0.76
0.79
0.82
0.85
0.88
0 10 20 30 40 50 60 70 80 90 100
% Trai ing Data
Acc
ura
cy
SK
STD
Figure 6: Accuracy of the multi-class classifier using standard
features and SK with respect to different percentages of training
data.
We note that not only SK produces higher accuracy
but also the gradient of the learning curves is higher: for
example, Figure 6 shows that with only 20% of training
data, SVM using SK approaches the accuracy of SVM
trained with all data on standard features.
Additionally, we carried out some preliminary exper-
iments for argument identification (boundary detection),
but the learning algorithm was not able to converge. In
fact, for this task the non-inclusion property (discussed
in Section 3) does not hold. A constituent ai, which has
incorrect boundaries, can include or be included in the
correct argument ac. Thus, the similarity K(ai, ac) be-
tween ai and ac is quite high preventing the algorithm to
learn the structures of correct arguments.
4.2 Discussion and Related Work
The material of the previous sections requires a discus-
sion of the following points: firstly, in Section 3.2 we
have noted that some standard features are explicitly
coded in SK but Governing Category, Position and Voice
features are not expressible as a single fragment of a se-
mantic structure. For example, to derive the Position of
an argument relatively to the target predicate is required a
visit of the tree. No parse-tree information, i.e. node tags
or edges, explicitly indicates this feature. A similar ratio-
nale applies to Governing Category and Voice, even if for
the latter some tree fragments may code the to be feature.
Since these three features have been proved important for
role classification we argue that either (a) SK implicitly
produces this kind of information or (b) SK is able to pro-
vide a different but equally effective information which
allows it to perform better than the standard features. In
this latter case, it would be interesting to study which
features can be backported from SK to the linear kernel
to obtain a fast and improved system (Cumby and Roth,
2003). As an example, the fragment [VP [V NP]] defines
a sort of sub-categorization frame that may be used to
cluster together syntactically similar verbs.
Secondly, it is worth noting that we compared SK
against a linear kernel of standard features. A recent
study, (Pradhan et al, 2003), has suggested that a poly-
nomial kernel with degree = 2 performs better than the
linear one. Using such a kernel, the authors obtained
88% in classification but we should take into account
that they also used a larger set of flat features, i.e. sub-
categorization information (e.g. VP? V NP PP for the
tree in Figure 1), Named Entity Classes and a Partial Path
feature.
Thirdly, this is one of the first massive use of convo-
lution kernels for Natural Language Processing tasks, we
trained SK and tested it on 123,918 and 7,426 arguments,
respectively. For training each large argument (in term
of instances) were required more than 1.5 billion of ker-
nel iterations. This was a little time consuming (about
a couple of days for each argument on a Intel Pentium
4, 1,70 GHz, 512 Mbytes Ram) as the SK computation
complexity is quadratic in the number of semantic struc-
ture nodes7. This prevented us to carry out cross/fold val-
idation. An important aspect is that a recent paper (Vish-
wanathan and Smola, 2002) assesses that the tree-kernel
complexity can be reduced to linear one; this would make
our approach largely applicable.
Finally, there is a considerable work in Natural Lan-
guage Processing oriented kernel (Collins and Duffy,
2002; Lodhi et al, 2000; Ga?rtner, 2003; Cumby and
Roth, 2003; Zelenko et al, 2003) about string, parse-
7More precisely, it is O(|Fp,a|2) where Fp,a is the largest
semantic structure of the training data.
tree, graph, and relational kernels but, to our knowledge,
none of them was used to derive semantic information
on the form of predicate argument structures. In particu-
lar, (Cristianini et al, 2001; Kandola et al, 2003) address
the problem of semantic similarity between two terms by
using, respectively, document sets as term context and
the latent semantic indexing. Both techniques attempt
to cluster together terms that express the same meaning.
This is quite different in means and purpose of our ap-
proach that derives more specific semantic information
expressed as argument/predicate relations.
5 Conclusions
In this paper, we have experimented an original kernel
based on semantic structures from PropBank corpus. The
results have shown that:
? the Semantic Kernel (SK) can be adopted to classify
predicate arguments defined in PropBank;
? SVMs using SK performs better than SVMs trained
with the linear kernel of standard features; and
? the higher gradient in the accuracy/training percent-
age plots shows that SK generalizes better than the
linear kernel.
Finally, SK suggests that some features, contained in
the fragments of semantic structures, should be back-
ported in a flat feature space. Conversely, the good per-
formance of the linear kernel suggests that standard fea-
tures, e.g. Head Word, Predicate Word should be empha-
sized in the definition of a convolution kernel for argu-
ment classification. Moreover, other selections of predi-
cate/argument substructures (able to capture different lin-
guistic relations) as well as kernel combinations (e.g. flat
features with SK) could furthermore improve semantic
shallow parsing.
6 Acknowledgements
This research has been sponsored by the ARDA
AQUAINT program. In addition, we would like to thank
prof. Sanda Harabagiu to support us with interesting ad-
vices. Many thanks to the anonymous reviewers for their
professional and committed suggestions.
References
John Chen and Owen Rambow. 2003. Use of deep linguistic
features for the recognition and labeling of semantic argu-
ments. In Proceedings EMNLP03.
Michael Collins and Nigel Duffy. 2002. New ranking algo-
rithms for parsing and tagging: Kernels over discrete struc-
tures, and the voted perceptron. In Proceedings of ACL02.
Nello Cristianini, John Shawe-Taylor, and Huma Lodhi. 2001.
Latent semantic kernels. In Proceedings of ICML01, pages
66?73, Williams College, US. Morgan Kaufmann Publish-
ers, San Francisco, US.
Chad Cumby and Dan Roth. 2003. Kernel methods for rela-
tional learning. In Proceedings of ICML03.
Thomas Ga?rtner. 2003. A survey of kernels for structured data.
SIGKDD Explor. Newsl., 5(1):49?58.
Daniel Gildea and Julia Hockenmaier. 2003. Identifying se-
mantic roles using combinatory categorial grammar. In Pro-
ceedings of EMNLP03.
Daniel Gildea and Daniel Jurasky. 2002. Automatic labeling of
semantic roles. Computational Linguistic, 28(3):496?530.
Daniel Gildea and Martha Palmer. 2002. The necessity of pars-
ing for predicate argument recognition. In Proceedings of
ACL02, Philadelphia, PA.
King-Shy Goh, Edward Chang, and Kwang-Ting Cheng. 2001.
SVM binary classifier ensembles for image classification.
Proceedings of CIKM01, pages 395?402.
Kadri Hacioglu, Sameer Pradhan, Wayne Ward, Jim Martin, and
Dan Jurafsky. 2003. Shallow semantic parsing using Sup-
port Vector Machines. Technical report.
R. Jackendoff. 1990. Semantic Structures, Current Studies
in Linguistics series. Cambridge, Massachusetts: The MIT
Press.
T. Joachims. 1999. Making large-scale SVM learning
practical. In B. Schlkopf, C. Burges, and MIT-Press.
A. Smola (ed.), editors, Advances in Kernel Methods - Sup-
port Vector Learning.
J. Kandola, N. Cristianini, and J. Shawe-Taylor. 2003. Learn-
ing semantic similarity. In Advances in Neural Information
Processing Systems, volume 15.
Paul Kingsbury and Martha Palmer. 2002. From TreeBank to
PropBank. In Proceedings of LREC02, Las Palmas, Spain.
Huma Lodhi, Craig Saunders, John Shawe-Taylor, Nello Cris-
tianini, and Christopher Watkins. 2000. Text classification
using string kernels. In NIPS, pages 563?569.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz. 1993.
Building a large annotated corpus of english: The Penn Tree-
Bank. Computational Linguistics, 19:313?330.
Sameer Pradhan, Kadri Hacioglu, Wayne Ward, James H. Mar-
tin, and Daniel Jurafsky. 2003. Semantic role parsing:
Adding semantic structure to unstructured text. In Proceed-
ings of ICDM03.
Mihai Surdeanu, Sanda M. Harabagiu, John Williams, and John
Aarseth. 2003. Using predicate-argument structures for in-
formation extraction. In Proceedings of ACL03, Sapporo,
Japan.
S.V.N. Vishwanathan and A.J. Smola. 2002. Fast kernels on
strings and trees. In Proceedings of Neural Information Pro-
cessing Systems.
D. Zelenko, C. Aone, and A. Richardella. 2003. Kernel meth-
ods for relation extraction. Journal of Machine Learning Re-
search.
Intentions, Implicatures and Processing of Complex Questions
Sanda M. Harabagiu and Steven J. Maiorano and Alessandro Moschitti and Cosmin A. Bejan
University of Texas at Dallas
Human Language Technology Research Institute
Richardson, TX 75083-0688, USA
Abstract
In this paper we introduce two methods for
deriving the intentional structure of complex
questions. Techniques that enable the deriva-
tion of implied information are also presented.
We show that both the intentional structure
and the implicatures enabled by it are essen-
tial components of Q/A systems capable of suc-
cessfully processing complex questions. The
results of our evaluation support the claim that
there are multiple interactions between the pro-
cess of answer finding and the coercion of in-
tentions and implicatures.
1 Introduction
The Problem of Question Intentions.
When using a Question Answering system to find
information, the user cannot separate the intentions
and beliefs from the formulation of the question. A
direct consequence of this phenomenon is that the user
incorporates his or her intentions and beliefs into the
interrogation. For example, when asking the question:
Q1: What kind of assistance has North Korea received
from the USSR/Russia for its missile program?
the user associate with the question a number of in-
tentions, that maybe expressed a set of intended ques-
tions. Each intended question, in turn generates implied
information, that maybe expresses as implied questions.
For question Q1, a list of intended questions and implied
questions is detailed in Table1.
Most of the intended questions are similar with the
questions evaluated in TREC1. For example questions
1The Text REtrieval Conferences (TREC) are evaluation
workshops in which Information Retrieval tasks are annually
tested. Since 1999 the performance of question answering sys-
tems are measured in the TREC QA track.
Qi1, Qi2 and Qi3 are so-called definition questions, since
they ask about defining properties of an object. However
unlike the TREC definition questions, these questions ex-
press unstated intentions of the questioner and need to be
processed in the context of the original complex question
Q1. Questions Qi4 and Qi5 are factoid questions, request-
ing information about facts or events. Qi6 asks about the
source of information that enables the answers of ques-
tion Q1.
Questions Qi1, Qi2, Qi3, Qi4 and Qi5 result from the in-
tentional structure generated when processing question
Q1 or questions similar to it. When intended questions
are generated, their sequential processing (a) represents a
decomposition of the complex question and (b) generates
a scenario for finding information; thus questions like Q1
are also known as scenario questions.
Intentions and Implicatures.
As Table 1 suggests, the implied information takes the
form of alternatives that guide the answers to intended
questions. For example, question Qm11 lists alternatives
for the answer to Qi1 whereas Qm21 lists components of
the answer of Qi1. Implicatures may also involve tem-
poral inference, e.g. the implied questions pertaining
to Qi3 and Qi4. Additionally, the reliability of infor-
mation is commonly an implicature in the case of sce-
nario questions, since the causal and temporal inference
is based on the quality and correctness of the available
data sources. Neither intentions or implicatures are rec-
ognizable at syntactic or semantic level, but they both
play an important role in the question interpretation. In-
terpretations disregard the implied information or the user
intentions determine the extraction of incorrect answers,
thus influence the performance of Q/A systems.
Our solution.
In this paper we present two different mechanisms of
deriving the question implicatures. Both methods start
from the syntactic and semantic content of the interro-
gation. The first method considers only the semantic
Intended Questions Implied Questions
Qi1 :What is the USSR/Russia? Qm11 :Is this the Soviet/Russian government?
Qm12 :Does it include private firms, state-owned firms, educational
institutions, and individuals?
Qi2 :What is North Korea? Qm21 :Is this the North Korean government only?
Qm22 :Does it include private firms, state-owned firms, educational
institutions, and individuals?
Qi3 :What is assistance? Qm31 :Is it the transfer of complete missile systems, licensing agreements,
components, materials, or plans?
Qm32 :Is it the training of personnel?
Qm33 :What kind of training?
Qm34 :Does transfer include data, and, if so, what kind of data?
Qm35 :Does transfer include financial assistance, and, if so, what kind of
financial assistance?
Qi4 :What are the missiles in the North Qm41 :Are any based upon Soviet/Russian designs?
Korean inventory? Qm42 :If so, which ones?
Qm43 :What was the development timeline of the missiles?
Qm44 :Did any timeline differ significantly from others?
Qm45 :Did North Korea receive assistance from other sources besides
USSR/Russia to develop these missiles?
Qi5 :When did North Korea receive assistance Qm51 :Was any intended assistance halted, stopped or intercepted?
from the USSR/Russia?
Qi6 :What are the sources of information? Qm61 :Are the sources reliable?
Qm62 :Is some information contradictory?
Table 1: Question decomposition associated with question Q1
meaning of the words used in the question whereas the
second method considers the predicate-argument struc-
ture of the question and candidate answers as a form of
shallow semantics that enables the inference of the inten-
tional structure. Question implicatures are derived from
lexico-semantic paths retrieved from the WordNet lexico-
semantic database. These paths bring forward new con-
cepts, that may be associated with the question implica-
tures when testing the paths against the conversational
maxims introduced by Grice in (Grice, 1975a). For ex-
ample, if the user asks ?Will Prime Minister Mori survive
the crisis??, the first method detects the user?s belief that
the position of the Prime Minister is in jeopardy, since
the concept DANGER is coerced although none of the
question words directly imply it.
The second method generates the intentional structure
of the question, enabling a more structured representa-
tion of the pragmatics of question interpretation. The in-
tentional structure is based on a study that we have con-
ducted for capturing the motivations of a group of users
when asking series of questions in several scenarios. We
show how the intentional structures that we have gathered
guide the coercion of knowledge that helps to support the
acceptance of rejection of computational implicatures.
The derivation of intentional structures is made possi-
ble by predicate-argument structures that are recognized
both at the question level and at the candidate answer
level. In this paper we show how richer semantic ob-
jects can be derived around predicate-argument structures
and how inferential mechanisms can be associated with
such semantic objects for obtaining correct answers. The
rest of the paper is organized as follows. In Section 2
we describe several forms of complex questions that re-
quire the derivation of computational implicatures. Sec-
tion 3 details the models of Question Answering that we
considered and Section 4 shows our methods of deriving
predicate-argument structures and their usage in identi-
fying answers for questions. Section 5 details the inten-
tional structures whereas Section 6 summarizes the con-
clusions.
2 Question Complexity
Since 1999, the TREC QA evaluations focused on fac-
toid questions, such as ?In what year did Joe Di Maggio
compile his 56-game hitting streak?? or ?Name a film in
which Jude Law acted.?. The answers to most of these
questions belong to semantic categories associated with
each question class. For example, questions asking about
a date or a year can be answered because Named Entity
Recognizers identify a temporal expression in a candidate
text span. Similarly, names of people or organizations
are provided as answers to questions such as ?Who is the
first Russian astronaut?? or ?What is the largest software
company in the world??. Most Named Entity Recogniz-
ers detect names of PEOPLE, ORGANIZATIONS, LOCA-
TIONS, DATES, PRICES and NUMBERS. For factoid Q/A,
the list of name categories needs to be extended, as re-
ported in (Harabagiu et al, 2003) for recognizing many
What kind od assistance has North Korea received fromComplex Question:
What kind of assistance has X received from Y for Z?Question PATTERN:
X=North Korea FOCUS=Z=misile program
Intended Questions:
Definition Questions:
What is Y? "What is USSR/Russia?"
What is assistance? 
Elaboration of FOCUS:
Reliability: "What are the sources of information?"
the USSR/Russia for its missile program?
assistance from the USSR/Russia?"
(1) RESULTATIVE
(2) TEMPORAL
North Korean inventory?"
"When did North Korea receive
Y=USSR/Russia
What is X? "What is North Korea?"
"What are the missiles in the
Figure 1: Decomposition of scenario question into intended questions
more types of names, e.g. names of movies, names of
diseases, names of battles. Moreover, the semantic cat-
egories of the extended set of names need to be incor-
porated into an answer type taxonomy that enables the
recognition of (a) the expected answer type and (b) the
question class. The taxonomy of expected answer types
is useful because the answer is not always a name; it can
be a lexicalized concept or a concept that is expressed by
a paraphrase.
The TREC evaluations have also considered two more
classes of questions: (1) list questions and (2) definition
questions. The list questions have answers that are typ-
ically assembled from different documents. Such ques-
tions are harder to answer than factoid questions be-
cause the systems must detect duplications. Example of
list questions are ?Name singers performing the role of
Donna Elvira in performances of Mozart?s ?Don Gio-
vani?.? or ?What companies manufacture golf clubs??.
Definition questions require a different form of process-
ing that factoid questions because no taxonomy of answer
types needs to be used. The expected answer type is a def-
inition, which cannot be represented by a single concept.
Q/A systems assume that definitions are given by follow-
ing a set of linguistic patterns that need to be matched for
extracting the answer. Example of definition questions
are ?What is a golden parachute?? or ?What is ETA in
Spain??.
In (Echihabi and Marcu, 2003) a noisy channel model
for Q/A was introduced. This model is based on the
idea that if a given sentence SA contains an answer sub-
string A to a question Q, then SA can be re-written into
Q through a sequence of stochastic operators. Not only a
justification of the answer is produced, but the conditional
probability P(Q?SA) re-ranks all candidate answers.
A different viewpoint of Q/A was reported in (Itty-
cheriah et al, 2000). Finding the answers A to a ques-
tion Q was considered a classification problem that maxi-
mizes the conditional probability P(A?Q). This model is
not tractable currently, because (a) the search space is too
large for a text collection like the TREC or the AQUAINT
corpora; and (b) the training data is insufficient. There-
fore, Q/A is modeled by the distribution P(C?A,Q)
where C measures the ?correctness? of A to question
Q. By using a hidden variable E that represents the ex-
pected answer type, P(C?A,Q) = ?E p(C,E?Q,A) =
?E p(C?E,Q,A) * p(E?Q,A). Both distributions are
modeled by using the maximum entropy.
All three forms of questions are also useful when pro-
cessing complex questions, determined by a scenario re-
sulting from a problem-solving situation. As illustrated
in Figure 1, a scenario question may be associated with a
pattern. One of the pattern variables represents the focus
of the question. The notion of the question focus was first
introduced by (Lehnert, 1978). The focus represents the
most important concept of the question; a concept deter-
mining the domain of question. In the case of question
Q1, the focus is missile program. The identification of
the focus is based on the predicate-structure of the ques-
tion pattern and on the order of the arguments. Figure 3
shows both the question pattern associated with Q1 and
its predicate-argument structure. The argument with the
role of purpose is ranked highest, and thus it determines
the question focus.
With the exception of the focus, all arguments from
the predicate-argument structure may be used for gener-
ating definition questions. The focus is elaborated upon.
Several forms of elaborations are possible. One is a tem-
poral one, as illustrated in Figure 1. Other are resultative,
causative or manner-based. For example, the knowledge
that assistance in a missile program results in an inven-
When did North Korea receive from USSR/Russiaassistance
WRB VBD NNP NNP NNPVB NN
NPB
IN
PP
NPB
VP
NPB
SQ
WHADVP
SBARQ
OBJECTBENEFICIARY SOURCEDATE
when=DATE
Step 2(a): Binary Semantic Dependencies
receive North Korea USSR/Russia assistance
TypeExpected Answer
Step 2(b): Predicate?Argument Structures
Predicate: receive
Arguments: assistance=OBJECT
North Korea=BENEFICIARY
USSR/Russia=SOURCE
When=DATE=Expected Answer Type
Question: When did North Korea receive assistance from USSR/Russia?
Step 1: Syntactic Parse
Figure 2: Deriving the Expected Answer Type
Predicate?argument structure:
Predicate:
Arguments:
receive
Purpose: Z
Beneficiary: X Source: Y
Object: assistance
Question Pattern: What kind of assistance has X received
from Y for Z?
Figure 3: Predicate-argument structure
tory of missiles allows for resultative elaboration. Further
knowledge needs to be coerced for generating the implied
questions as possible follow-ups to intended questions.
The relationship between intended questions and im-
plied questions is marked by the presence of multiple
references, e.g. the pronouns it and this or any and
ones. The generation of implied questions is made pos-
sible by knowledge that is coerced from the intended
questions. For example, when asking Qi1 :?What is
the USSR/Russia?? the coercion process abstracts away
from the concept that needs to be defined, i.e. a coun-
try. The implied question requests confirmation of
the metonymy resolution involving USSR/Russia.This
named entity may represent a country but most likely it
refers to its government or, as Qm12 suggests, organiza-
tions or individuals acting on behalf of the country. Both
Qm11 and Qm12 , implied questions derived from the in-
tended question Qi1, refer to the metonymy by using the
pronouns this and it respectively. Different forms of coer-
cion are used for Qi3 because in this case the knowledge
is associated with the predicate. The implied questions
associated with the focus, i.e. the intended question Qi4,
coerce the design and development predicates which are
associated with the missiles as well as the timelines of
possible additional assistance.
3 Models of Question Answering
The processing of questions is typically performed as a
sequence of three processes: (1) Question Processing; (2)
Document Processing and (3) Answer Extraction. In the
case of factoid questions , question processing involves
the classification of questions with the purpose of pre-
dicting what semantic class the answer should belong
to. Thus we may have questions asking about PEOPLE,
ORGANIZATIONS, TIME or LOCATIONS. Since open-
domain Q/A systems process questions regardless of the
domain of interest, question processing must be based on
an extended ontology of answer types. The identification
of the expected answer type is based either on binary se-
mantic dependencies extracted from the syntactic parse of
the question (Harabagiu et al, 2001) or on the predicate-
argument structure of the question. In both cases, the re-
lation to the question stem (i.e. what, who, when) enables
the classification. Figure 2 illustrates a factoid question
generated as an intended question and the derivation of
its expected answer type.
However, many times the expected answer type needs
to be identified from an ontology that has high lexico-
semantic coverage. Many Q/A systems use the WordNet
database for this purpose. In contrast, definition ques-
tions do not require the identification of the expected an-
Answer Pattern:
Answer:
Question Pattern:
Question?Point, a Definition
has killed nearly 800 people since
taking up arms in 1968
for Basque Homeland and Freedom ?
ETA, a Basque language acronym
What is Question?Point in Country?
What is in SpainETA
NNP
NPB
IN
PP
NNP
NPB
NP
VBZ
SQ
WP
WHNP
SBARQ
Definition Question: What is ETA in Spain?
Question Parse:
Figure 4: Patterns for Processing Definition Questions
swer type, since they always request a definition. How-
ever, definition questions are matched against a set of pat-
terns, which enables the extraction of the definition from
the candidate answers. Figure 4 illustrates a definition
question, the pattern it matched as well as the extracted
answer.
Both factoid and definition questions can be answered
only if candidate passages are available. The retrieval of
these passages is made possible by keywords that are se-
lected from the question words. The Documents Process-
ing module implements a search engine that returns pas-
sages that are likely to contain the expected answer type
in the case of factoid questions or the definition pattern
in the case of definition questions. The answer extraction
module optimizes the extraction of the correct answer by
unifying the question information with the answer infor-
mation. The unification may be based on pattern match-
ing; on machine learning algorithms based on the ques-
tion and answer features or on abductive reasoning that
justifies the answer correctness.
Current state-of-the-art QA systems search for the can-
didate answer by assuming that the answers are single
concepts, that can be recognized from a hierarchy or by
a Named Entity Recognizer. This is a serious limitation,
but it works well for the factoid, list or definition ques-
tions evaluated in TREC.
The three modules of current Q/A systems reflect the
three functions that need to be considered by any Q/A
model: (1) understanding what the question asks; (2)
identify candidate text passage that might contain the an-
swer; and (3) the extraction of the correct answer. Cur-
rently, the expected answer type represents what ques-
tion asks about: a semantic concept, e.g. the name of a
person, location or organization, kinds of diseases, types
of animals or plants. Generally these semantic concepts
are lexicalized in a single word or in 2-word collocations.
Clearly, this represents a limitation, since often the ques-
tions ask for more than a single concept. As we have
seen in Table1, there is additional intended and implied
information that is requested. Therefore new models of
Question/Answering need to incorporate these additional
forms of knowledge.
When definition questions are processed in current
Q/A systems, they are matched against a pattern, which is
different from the question patterns associated with com-
plex questions similar to those illustrated in Figure 1. In
the case of a definition question like ?What is ETA in
Spain??, the pattern identifies the question-point (QP) as
ETA- the concept that needs to be defined and Spain as
its context. The definition question pattern also contains
several surface-form patterns that are matched in the can-
didate paragraphs. One such pattern is recognized in an
apposition, by [QP, a AP] where AP represents the an-
swer phrase. In the following passage:
?ETA, a Basque language acronym for Basque Homeland
and Freedom - has killed nearly 800 people since taking
up arms in 1968.?
the exact answer representing the definition is identified
in AP: Basque language acronym for Basque Homeland
and Freedom. The fact that Basque country is a region in
Spain allows a justification of the question context.
In this paper, by considering the intentional informa-
tion and the implied information that can be derived when
processing questions, we introduce a novel model of Q/A,
which has access to rich semantic structures and enables
the retrieval of more accurate answers as well as inference
processes that explain the validity and contextual cover-
age of answers.
Figure 5 shows the structure of the novel model of Q/A
we propose. Both Question Processing and Document
Processing have the recognition of predicate-argument
structures as a crux of their models. As reported in (Sur-
deanu et al, 2003), the recognition of predicate-argument
structures depends on features made available by full syn-
tactic parses and by Named Entity Recognizers. As we
shall show in this paper, the predicate-argument struc-
tures enable the recognition of question pattern, the ques-
tion focus and the intentional structure associated with
Question
Syntactic Parse Named EntityRecognition
Identification of
Predicate?Argument Structure
Structure
Recognition of Answer
based on extended
Indexing & Retrieval
lexico?semantic knowledge
Named EntityRecognitionSyntactic Parse
Intentional Structure
Identification ofPredicate?ArgumentStructures Question Pattern
Recognition of
Identification ofQuestion Focus
Recognition of Answer Structure
Keyword Extraction
Validation of Implied Information
Answer Structure
Recognition of
Recognition and extention
of intentional structure
Reference Resolution
Question Processing Answer ProcessingDocument Processing
Answer
Figure 5: Novel Question/Answering Architecture.
a question. When the intentions are known, the answer
structure can be identified and the keywords extracted.
For better retrieval of candidate answers, documents are
indexed and retrieved based on the predicate-argument
structures as well as on complex semantic structure asso-
ciated with different question patterns. Similarly, the in-
tentional structures are used for indexing/retrieving can-
didate passages. The Answer Processing function in-
volves the recognition of the answer structure and inten-
tional structure. Often this requires reference resolution.
The implied information coerced from both the question
and the candidate answer is also validated before decid-
ing on the answer correctness.
4 Predicate-Argument Structures
To identify predicate-argument structures in questions
and passages, we have: (1) used the Proposition Bank or
PropBank as training data; and (2) a mode for predicting
argument roles similar to the one employed by (Gildea
and Jurafsky, 2002).
PropBank is a one million word corpus annotated with
predicate-argument structures on top of the Penn Tree-
bank 2 Wall Street Journal texts. For any given predicate,
the expected arguments are labeled sequentially from Arg
0 to Arg 4. Generally, Arg 0 stands for agent, Arg 1 for
direct object or theme or patient, Arg 2 for indirect object
or benefactive or instrument or attribute or end state, Arg
3 for start point or benefactive or attribute and Arg4 for
end point. In addition to these core arguments, adjunc-
tative arguments are marked up. They include functional
tags from Treebank, e.g. ArgM-DIR indicates a direc-
tional, ArgM-LOC indicates a locative, and ArgM-TMP
stands for a temporal.
An example of PropBank markup is:
[Arg10 Analysts ] have been [predicate1 expecting ] [Arg11
a GM-Jaguar pact ] that would [predicate2 give ] [Arg22 the
U.S. car maker ] [Arg21 an eventual 30% state in the British
Company ].
The model of identifying the arguments of each pred-
icate consists of two tasks: (1) the recognition of the
boundaries of each argument in the syntactic parse tree;
(2) the identification of the argument role. Each task can
be cast as a separate classifier. Next section describes
our approach based on Support Vector Machines (SVM)
(Vapnik, 1995).
4.1 Automatic Predicate-Argument extraction
Given a sentence in natural language, all the predicates
associated with its verbs have to be identified along with
their arguments. This problem can be divided in two sub-
tasks: (a) detection of the target argument boundaries,
i.e. all its compounding words, and (b) classification of
the argument type, e.g. Arg0 or ArgM.
A direct approach to learn both detection and classifi-
cation of predicate arguments is summarized by the fol-
lowing steps:
1. Given a sentence from the training-set, generate a
full syntactic parse-tree;
2. let P and A be the set of predicates and the set of
parse-tree nodes (i.e. the potential arguments), re-
spectively;
3. for each pair <p, a> ? P ?A:
? extract the feature representation set, Fp,a;
? if the subtree rooted in a covers exactly the
words of one argument of p, put Fp,a in T+
(positive examples), otherwise put it in T?
(negative examples).
0.68
0.71
0.74
0.77
0.8
0.83
1 2 3 4 5
Polynomial Degree
(a)
F1
Arg0
Arg1
ArgM
0.65
0.68
0.71
0.74
0.77
0.8
1 2 3 4 5
Polynomial Degree
(b)
F1
Figure 6: Single classifiers and Multi-classifier performance for argument extraction.
The above T+ and T? sets can be re-organized as pos-
itive T+argi and negative T?argi examples for each argu-
ment i. In this way, an individual ONE-vs-ALL SVM
classifier for each argument i can be trained. We adopted
this solution as it is simple and effective (Pradhan et al,
2003). In the classification phase, given a sentence of the
test-set, all its Fp,a are generated and classified by each
individual SVM classifier. As a final decision, we select
the argument associated with the maximum value among
the scores provided by the SVMs2, i.e. argmaxi?S Ci,
where S is the target set of arguments.
The discovering of relevant features is a complex task.
Nevertheless there is a common consensus on the basic
features that should be adopted. These standard features,
first proposed in (Gildea and Jurafsky, 2002), are derived
from parse trees as illustrated by Table 2.
4.2 Parsing Sentence into Predicate Argument
Structures
For the experiments, we used PropBank
(www.cis.upenn.edu/?ace) along with Penn-
TreeBank3 2 (www.cis.upenn.edu/?treebank)
(Echihabi and Marcu, 2003). This corpus contains about
53,700 sentences and a fixed split between training and
testing which has been used in other researches (Gildea
and Jurafsky, 2002; Surdeanu et al, 2003; Hacioglu et
al., 2003; Chen and Rambow, 2003; Gildea and Hock-
enmaier, 2003; Gildea and Palmer, 2002; Pradhan et al,
2003). In this split, Sections from 02 to 21 are used for
training, section 23 for testing and sections 1 and 22 as
developing set. We considered all PropBank arguments
from Arg0 to Arg9, ArgA and ArgM even if only Arg0
from Arg4 and ArgM contain enough training/testing
2This is a basic method to pass from binary categorization
into a multi-class categorization problem; several optimization
have been proposed, e.g. (Goh et al, 2001).
3We point out that we removed from the Penn TreeBank the
special tags of noun phrases like Subj and TMP as parsers usu-
ally are not able to provide this information.
data to affect the global performance.
The classifier evaluations were carried out using
the SVM-light software (Joachims, 1999) available at
http://svmlight.joachims.org/ with the de-
fault polynomial kernel according to a degree d ?
{1, 2, 3, 4, 5}. The performances were evaluated using
the F1 measure for both single argument classifiers and
the multi-class classifier.
- PHRASE TYPE (pt): This feature indicates the syntactic
type of the phrase labeled as a predicate argument.
- PARSE TREE PATH (path): This feature contains the path
in the parse tree between the predicate phrase and the argu-
ment phrase, expressed as a sequence of nonterminal labels
linked by direction (up or down).
- POSITION (pos) Indicates if the constituent appears be-
fore or after the predicate in the sentence.
- VOICE (voice) This feature distinguishes between active
or passive voice for the predicate phrase.
- HEAD WORD (hw) This feature contains the head word
of the evaluated phrase. Case and morphological informa-
tion are preserved.
- GOVERNING CATEGORY (gov) This feature applies to
noun phrases only, and it indicates if the NP is dominated by
a sentence phrase (typical for subject arguments with active
voice predicates), or by a verb phrase (typical for object
arguments).
- PREDICATE WORD In our implementation this feature
consists of two components: (1) VERB: the word itself
with the case and morphological information preserved; and
(2) LEMMA which represents the verb normalized to lower
case and infinitive form.
Table 2: Standard Features used in Predicate Argument
Extraction.
Figure 6 illustrates the F1 measures for the overall ar-
gument extraction task (i.e. identification and classifica-
tion) according to different polynomial degrees. Figure
6(a) illustrates the F1-performance of single classifiers
for the arguments Arg0, Arg1 and ArgM. Figure 6(b) il-
lustrates the performance for all the arguments (i.e. the
multi-classifier). In general, we were able to recognize
predicate argument structures with an F1-score of 80%.
4.3 Using Predicate-Argument Structures in
Question Answering.
Predicate-argument structures are useful for identifying
candidate answers. Since they recognize long-distance
dependencies between a predicate and one its arguments,
they enable (1) the identification of the exact boundaries
of an answer; and (2) they unify the predicate-argument
relation sought by question with those recognized in can-
didate passages.
Moreover, they are very useful in situations when the
expected answer type of the question could not be recog-
nized. There are two causes when the expected answer
type cannot be identified:
Case1: the answer class is a name that cannot be correctly
classified by an available Named Entity Recognizer, be-
cause its class name is not encoded.
Case2: the answer class cannot be found in the Answer
Type hierarchy. The example from Figure 7 shows an in-
stance of case 1. In this figure, the TREC question Q2054
has a predicate that can be unified with PREDICATES
from the answer passage. The Arg1 of the predicate is
the expected answer, which is identified as ?the Declara-
tion of Independence?. The Arg0 in the question is But-
ton Gwinnett, whereas in the answer, it is underspecified,
and should be resolved to who. This relative pronoun has
Button Gwinnett as one of its antecedents.
In Figure 8 the second case is illustrated. The question
asked about the first argument of the predicate ?measure?,
when its Arg2 = ?a theodolite?. In the answer, Predicate
2, with its infinite form, has as Arg 2 the same ?theodo-
lite?. However, the predicates are lexicalized by different
verbs. In WordNet, the first sense of the verb ?measure?
as the verb ?determine? as a hypernym, therefore Arg1 =
?wind speeds? is the correct answer.
5 Intentional Structures
The correct interpretation of many questions requires
the inference of implicit information, that is not directly
stated in the question, but merely implied. The mecha-
nisms of recognizing the intentions of the questioner are
helpful means of identifying the implied information. For
example, in the question QI :?Will Prime Minister Mori
survive the crisis??, the user does not literally mean ?Will
Prime Minister Mori be still alive when the political cri-
sis is over ??, but rather (s)he implies her/his belief that
the current political crisis might cost the Japanese Prime
Minister his job. It is very unlikely that any expert knowl-
edge base covering Japanese politics will encode knowl-
edge covering all situations of political crisis and the pos-
sible outcomes of the prime minister. However, this prag-
matic knowledge is essential for the correct interpretation
of the question.
Q2054:
Answer: Button Gwinnett, George Walton and Lyman Hall were
Georgians who could have been hanged as traitors for
What document did Button Gwinnett sign on the upper left
hand side?
signing the Declaration of Independence on July 4, 1776.
Predicate?argument structure:
PREDICATE1: were
ARG1(PREDICATE1): Georgians
who
PREDICATE2: could have been hanged
ARG2(PREDICATE2): as traitors
PREDICATE3: signing
ARG1(PREDICATE3): The Declaration of Independence
ARGM?LOC(PREDICATE3): on July 4, 1776
ARG0: Button Gwinnett, George Walton and Lyman Hall
ARGM?LOC: on the upper left hand side
ARG0: Button Gwinnett
PREDICATE: sign
ARG1: What document Question Type
Predicate?argument structure:
Figure 7: Answer extraction from predicate-argument struc-
tures: Case1
measurePREDICATE:
What does a theodolite measure?
Predicate?argument structure:
ARG1: What
ARG2: a theodolite
Answer: The theodolite ? a 1940s gadget, no longer in production,
wind speeds.
that uses a helium balloon and trigonometry to determine
Predicate?argument structure:
a 1940s gadget
that
PREDICATE1: uses
PREDICATE2: to determine
ARG1(PREDICATE2): wind speeds
Q2145:
ARG1(PREDICATE1): The theodolite
ARG2(PREDICATE1): a helium balloon and trigonometry
Figure 8: Answer extraction from predicate-argument struc-
tures: Case 2
The design of advanced Question&Answering systems
capable of grasping the intention of a professional analyst
when (s)he poses a question depends both on the knowl-
edge of the domain referred by the question as well as on
a variety of rules and conventions that allow the commu-
nication of intentions and beliefs in addition to the literary
meaning of the question. Access to domain knowledge is
granted by a combination of retrieval mechanisms that
bring forward relevant document passages from unstruc-
tured collections of documents, specialized knowledge
Text Information RetrievalEngine
QUERY: Prime & Minister & Mori &DANGER (word)
Japanese Factual PoliticsKnowledge Base
Text Retrieval
"vote of non-confidence against 
Prime Minister Mori"
political crisis
survive crisis
adversity DANGER
WordNet 1.6
resignation
removal
strike
vote
vote of non-confidence =
   DANGER(Position)
continue in existence
Question: Will Prime Minister Mori survive the political crisis ?
DANGER ( Prime Minister Mori continues in Position)
Figure 9: Intentional Structure derived from Lexico-Semantic Knowledge.
bases and/or database access mechanisms. The research
proposed in this project focuses on the derivation and us-
age of pragmatic knowledge that supports the recognition
of question implications, also known as implicatures (cf.
(Grice, 1975b)).
5.1 Intentional structures Derived from
Lexico-Semantic Knowledge
The novel idea of this research is to link computa-
tional implicatures, similar to those defined by Grice
(Grice, 1975b), to inferences that can be drawn from
general lexico-semantic knowledge bases such as Word-
Net of FrameNet. Incipient work was described in
(Sanda Harabagiu and Yukawa, 1996), where a method
of using lexico-semantic path for recognizing textual im-
plicatures was presented. To our knowledge, this is the
only computational model of implicatures that was de-
veloped and tested on a large lexico-semantic knowledge
base (e.g. WordNet), enabling successful recognition of
implicatures.
The model proposed in (Sanda Harabagiu and Yukawa,
1996) uncovered a relationship between (a) the coherence
of a text segment; (b) its cohesion expressed by the lexical
paths and (c) the implicatures that can be drawn, mostly
to account for pragmatic knowledge. This relationship
can be extended across documents and across topics, to
learn patterns of textual and Q&A implicatures and the
methods of deriving knowledge that enables their recog-
nition.
The derivation of pragmatic knowledge combines in-
formation from three different sources:
(1) lexical knowledge bases (e.g. WordNet),
(2) expert knowledge bases that can be rapidly formatted
for many domains (e.g. Japanese political knowledge);
and
(3) knowledge supported from the textual information
available from documents. The methodology of combin-
ing these three sources of information is novel.
For question QI , the starting point is the concept iden-
tified as a cue for the expected answer type through meth-
ods described in (Harabagiu et al, 2000). This con-
cept is lexicalized by the verb-object pair survive-crisis.
Verb survive has four distinct senses in the WordNet 1.6
database, whereas noun crisis has two senses. The poly-
semy of the expected answer type increases the difficulty
of the derivation of pragmatic knowledge, but it does not
presupposes the word sense disambiguation of the ex-
pression. The information available in the glosses defin-
ing the WordNet synsets provides helpful information for
expanding the multi-word term defining the expected an-
swer type. By measuring the similarity between the two
senses of the noun crisis and the words encountered as
objects or prepositional attachments in the glosses of the
various senses of the verb survive, we distinguish the
noun adversity and the example cancer as expressing the
closest semantic orientation to the first sense of noun cri-
sis. The similarity is measured by counting the number
of common hypernyms and gloss concepts of hypernyms
of two synsets. Figure 9 illustrates the concepts related
to the question QI , as derived from WordNet lexico-
semantic knowledge base.
The fact that surviving a political crisis has a dangerous
component, indicated by the noun adversity, may also be
supported by inferences drawn from an expert knowledge
base, showing that a political crisis may be dangerous for
political figures in power. However, at this point, the ob-
ject of the dangerous situation is not specified. But sev-
eral concepts indicating dangerous political situations can
be inferred from the expert knowledge base and used in
the query for text evidence. Only when text passages in-
volving Prime Minister Mori are retrieved, clarifications
of the situation are brought to attention: a vote of non-
confidence against the prime minister is considered. This
new information helps inference from the expert knowl-
edge base. The expert knowledge base modeling the
 Intentional Structure of Questions
0* Evidence (     1-possess      (          2-Iraq,      3-biological weapons   ))
4* Means of finding (0)
a. reports 
b. inspections
c. assessments ? patterns of inspections
5* Source (0)
a. authority
b. reliability ? may, would
6* Consequence (0)
a. Enablement
b. Hiding/Presenting finding evidence 
Question: Does Iraq   have biological weapons  ?
x                           y
: have( Iraq, biological weapons )
Predicate-
Argument
Structure
Question
pattern
Does x have y ?
possess (x, y) 
Topic (3)
biological  weapons
a. Types of topic
b. Components
- chemical agents
- mustard gas, VX, sarin
c. Usage
- rockets, artillery shells
a. discover(1,2,3)
b. stockpile(2,3)
c. use(2,3)
d. 1-possess
a. develop(2,3)
b. acquire(2,3)
a. inspections( _,2,3)
b. ban( _,2,3)
Source/      fact/          reliability
reporter     evidence
5.a              0              5.b
coercion
Structure
Figure 10: Intentional structure derived from predicate-argument structures.
Japanese factional politics confirms that this is a danger-
ous situation for the Prime Minister and that in fact his
position is in jeopardy. Due to this inference from the
expert knowledge base, the concept POSITION replaces
noun existence from the gloss of the second sense of verb
survive, and the pragmatic knowledge required for the in-
terpretation of the implicature is assembled:
The interactions between the three information sources
derives the pragmatic knowledge on which relies the im-
plication of the question. The user had an inherent belief
that Prime Minister Mori might be replaced, and (s)he
queries the Q&A system not only to find information but
also to find support for his/her belief. The intentional
structure is represented as a set of concepts and the re-
lations that span them, as illustrated in Figure 9.
5.2 Coercion of Intentions
A second method of deriving the intentional structure of a
question is based on the predicate-argument structure that
is derived from the question and the candidate answers.
Figure 10 illustrates the Intentional Structure of one
such question. The structure of the intentions is deter-
mined by the predicate-argument structure of the ques-
tion and by its pattern. Generally, when asking whether
X posses Y, we want to find (1) evidence of this fact;
(2) we explore different means of finding the informa-
tion; (3) we are interested in the source of information
and (4) the enablers or inhibitors of finding the informa-
tion as well as the consequences of knowing it are of in-
terest. We assign a different index to each object from
the predicate-argument structure, and do the same for
each element of the intentional structure. For instance,
in Figure 2, source(0) is interpreted as source(index=0)
= source(evidence). Another feature of the intentional
structure is determined by the coercions that are associ-
ated with both forms of indexed objects. For example,
the coercion of evidence shows the most typical ways
of finding evidence in the context of the topic of the
question. Figure 2 lists such possibilities as (a) discov-
ering, (b) stockpiling, (c) using and even (d) possess-
ing. These possibilities are inserted in the context of the
topic, since they make use of the indexes for associat-
ing meaning to their representations. In fact, option (a)
discover(1,2,3) reads as discover(index=1, index=2, in-
dex=3) =discover(possesses(Iraq, biological weapons)).
Whereas option (b) stockpile(2,3) can be similarly inter-
preted as stockpile(Iraq, biological weapons). Note that
one of the indexed objects is the topic. The structure of
the topic is define along three semantic dimensions: (1)
hyponyms or examples of other types of the same cate-
gory as the topic; (2) the meronyms or components; and
(3) the functionality or the usage. The derivation of such
a large set of intentional structures helped us learn how
to coerce pragmatic knowledge. We have developed a
probabilistic approach extending the metonymy work of
(Lapata and Lascarides, 2003).
Lapata and Lascarides report a model of interpretation
of verbal metonymy as the point distribution P (e, o, v) of
three variables: the metonymy verb v, its object, and the
sought after interpretation i. For example a verb ? ob-
ject relation that needs to be metonymycally interpreted,
is enjoy ? movie. In this case v = enjoy, o = movie
and i ? {making, watching, directing}. The variables
of the distribution re ordered as <i, v, o> to help factor-
ing P (i, v, o) = P (i) ? P (v|i) ? P (o|i, v). Each of the
probabilities P (i), P (v|i) and P (o|i, v) can be estimated
using maximum likelihood. As it is illustrated in Fig-
ure 10, we have extended this model to account for: (1)
coercion of topic information; (2) coercion of evidence
of a fact; (3) interpretation of predicate and (4) inter-
pretation of arguments. Since the verb ? object rela-
tion translates in one of the predicate-argument relations,
we have coerced the predicate interpretations in the same
way as (Lapata and Lascarides, 2003), but we allowed
for any predicate-argument relation. Argument coercions
were produced by searching the most likely predicates
that used the same arguments. The topic model also in-
corporated topic signatures, similar to these reported in
(E.H. Hovy and Ravichandran, 2002).
6 Conclusions
In this paper we have described the problem of interpret-
ing the question intentions and proposed two methods of
generating the intentional structure of questions. The first
method is based on lexico-semantic chains between con-
cepts that are related to the question. The second method
generates intentional structures by using the predicate-
argument structures of questions and the topic represen-
tation of questions. To derive both forms of intentional
structures, we have relied on information available from
WordNet and on the parsing of questions and answers
in predicate-argument structures. Our experiments show
that the intentional structure may determine a different in-
terpretation of the question, and thus different keywords
can be used to retrieve the answers. Answer extraction
also depends on the semantic relations between the co-
erced interpretations of predicates and arguments. By
selecting a set of 100 questions for test, we have eval-
uated the correctness of the extracted answers when (1)
no intentional knowledge was coerced; (2) implicatures
were derived from lexico-semantic knowledge and (3)
intentional structures were derived based on predicate-
argument structures. An increase of 8structures and one
of 22the impact of each element of the intentional struc-
ture on the Q/A processing.
References
John Chen and Owen Rambow. 2003. Use of deep lin-
guistic features for the recognition and labeling of se-
mantic arguments. In Proceedings of the 2003 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing.
Abdessamad Echihabi and Daniel Marcu. 2003. A
noisy-channel approach to question answering. In Pro-
ceedings of the 41st Annual Meeting of the ACL, Sap-
poro, Japan.
Chin-Yew Lin E.H. Hovy, U. Hermjakob and Deepak
Ravichandran. 2002. Using knowledge to facilitate
pinpointing of factoid answers. In Proceedings of the
19th International Conference on Computational Lin-
guistics (COLING 2002).
Daniel Gildea and Julia Hockenmaier. 2003. Identifying
semantic roles using combinatory categorial grammar.
In Proceedings of the 2003 Conference on Empirical
Methods in Natural Language Processing.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic la-
beling of semantic roles. Computational Linguistic,
28(3):254?288.
Daniel Gildea and Martha Palmer. 2002. The neces-
sity of parsing for predicate argument recognition. In
Proceedings of the 40th Annual Conference of the
Association for Computational Linguistics (ACL-02),
Philadelphia, PA.
King-Shy Goh, Edward Chang, and Kwang-Ting Cheng.
2001. SVM binary classifier ensembles for image clas-
sification. In Proceedings of the tenth international
conference on Information and knowledge manage-
ment, pages 395?402.
Paul H. Grice. 1975a. Logic and conversation. In P. Cole
and New York J.L. Morgan (ed.), Academic Press, edi-
tors, Syntax and Sematics Vol.3:Speech Acts, pages 41?
58.
Paul J. Grice. 1975b. Syntax and Semantics Vol.3:Speech
Acts. P. Cole and J. Morgan, editors.
Kadri Hacioglu, Sameer Pradhan, Wayne Ward, Jim Mar-
tin, and Dan Jurafsky. 2003. Shallow semantic parsing
using support vector machines. Technical report.
Sanda Harabagiu, Marius Pas?ca, and Steven Maiorano.
2000. Experiments with open-domain textual question
answering. In Proceedings of the 18th International
Conference on Computational Linguistics (COLING-
2000), pages 292?298, Saarbrucken, Germany,.
Sanda M. Harabagiu, Dan I. Moldovan, Marius Pasca,
Rada Mihalcea, Mihai Surdeanu, Razvan C. Bunescu,
Roxana Girju, Vasile Rus, and Paul Morarescu. 2001.
The role of lexico-semantic feedback in open-domain
textual question-answering. In Meeting of the ACL,
pages 274?281.
S. Harabagiu, D. Moldovan, C. Clark, M. Bodwen,
J. Williams, and J. Bensley. 2003. Answer mining by
combining extraction techniques with abductive rea-
soning. In Notebook of the Twelveth Text REtrieval
Converence (TREC-2003), pages 46?53.
A. Ittycheriah, M. Franz, W. Zhu, and A. Ratnaparkhi.
2000. IBM?s statistical question answering system.
In Proceedings of the 9th Text REtrieval Conference,
Gaithersburg, MD.
T. Joachims. 1999. T. Joachims, Making large-Scale
SVM Learning Practical. In B. Scho?lkopf and C.
Burges and A. Smola (ed.), MIT-Press., editor, Ad-
vances in Kernel Methods - Support Vector Learning.
Maria Lapata and Alex Lascarides. 2003. A probabilistic
account of logical metonymy. Computational Linguis-
tics, 29:2:263?317.
Wendy Lehnert. 1978. The process of question answer-
ing. In Lawrence Erlbaum Assoc., Hillsdale.
Sameer Pradhan, Kadri Hacioglu, Wayne Ward, James H.
Martin, and Daniel Jurafsky. 2003. Semantic role
parsing: Adding semantic structure to unstructured
text. In Proceedings of the International Conference
on Data Mining (ICDM-2003).
Dan Moldovan Sanda Harabagiu and Takashi Yukawa.
1996. Testing gricean constraints on a wordnet-based
coherence evaluation system. In Working Notes of the
AAAI-96 Spring Symposium on Computational Impli-
cature, Stanford, CA.
Mihai Surdeanu, Sanda Harabagiu, John Williams, and
Paul Aarseth. 2003. Using predicate-argument struc-
tures for information extraction. In Proceedings of the
41th Annual Conference of the Association for Compu-
tational Linguistics (ACL-03), pages 8?15.
V. Vapnik. 1995. The Nature of Statistical Learning The-
ory. Springer.
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 460?463,
Prague, June 2007. c?2007 Association for Computational Linguistics
UTD-SRL: A Pipeline Architecture for Extracting Frame
Semantic Structures
Cosmin Adrian Bejan and Chris Hathaway
Human Language Technology Research Institute
The University of Texas at Dallas
Richardson, TX 75083-0688, USA
{ady,chris}@hlt.utdallas.edu
Abstract
This paper describes our system for the task
of extracting frame semantic structures in
SemEval?2007. The system architecture
uses two types of learning models in each
part of the task: Support Vector Machines
(SVM) and Maximum Entropy (ME). De-
signed as a pipeline of classifiers, the seman-
tic parsing system obtained competitive pre-
cision scores on the test data.
1 Introduction
The SemEval?2007 task for extracting frame se-
mantic structures relies on the human annotated
data available in the FrameNet (FN) database. The
Berkeley FrameNet project (Baker et al, 1998) is
an ongoing effort of building a semantic lexicon for
English based on the theory of frame semantics. In
frame semantics, the meaning of words or word ex-
pressions, also called target words (TW), comprises
aspects of conceptual structures, or frames, that de-
scribe specific situations. The semantic roles, or
frame elements (FE), associated with a target word
are locally defined in the frame evoked by the tar-
get word. Currently, the FN lexicon includes more
than 135,000 sentences extracted from the British
National Corpus containing more than 6,100 target
words that evoke more than 825 semantic frames.
For this task, we extended our previous work at
Senseval-3 (Bejan et al, 2004) by (1) experiment-
ing with additional features, (2) adding new classifi-
cation sub-tasks to accomplish all the requirements,
and (3) integrating these sub-tasks into a pipeline ar-
chitecture.
2 System Description
Given a sentence, the frame semantic structure ex-
traction task consists of recognizing the word ex-
pressions that evoke semantic frames, assigning the
correct frame to them and, for each target word,
detecting and labeling the corresponding frame el-
ements properly. The task also requires the de-
termination of syntactic realizations associated to a
frame element, such as grammatical function (GF)
and phrase type (PT). The following illustrates a
sentence example annotated with frame elements to-
gether with their corresponding grammatical func-
tions and phrase types for the target word ?tie?:
FE = Content2
GF = Dep
PT = PP
FE = Content1
GF = Ext
PT = NP
AEOI?s activities and facilities  have been  tied   to several universities .
Frame = Make_Cognitive_Connection
evokes
To extract semantic structures similar to those il-
lustrated in the example we divide the SemEval?
2007 task into four sub-tasks: (1) target word frame
disambiguation (TWFD); (2) FE boundary detection
(FEBD); (3) GF label classification (GFLC) and (4)
FE label classification (FELC). The sub-tasks TWFD
and GFLC are natural extensions of the approach de-
scribed in (Bejan et al, 2004) for the task of se-
mantic role labeling at Senseval-03. We design ma-
chine learning classifiers specific for each of the four
sub-tasks and arrange them in a pipeline architecture
such that a classifier can use information predicted
by its previous classifiers. The system architecture
is illustrated in Figure 1. In the data processing step,
we parse each sentence into a syntactic tree using the
Collins parser and extract named entities using an in
460
Target word listTest data
Named Entity
Recognizer
Test Data
Target word
Frame
FE Boundary
GF
SVM model ME model SVM model ME model SVM model ME modelSVM model ME model
ME trainSVM train
Feature Extractor
ME train
one multi?class classifier
SVM train
Feature Extractor
ME trainSVM train
Feature Extractor
one binary classifier
ME trainSVM train
Feature Extractor
556 multi?class classifiers 489 multi?class classifiers
Test Data
Train Data
FeatureFeature
Test Data
Target word
Frame
FeatureFeature
Test Data
Target word
Frame
FE Boundary
FN Annotation
FE Boundary
Predictor
Frame
Predictor Predictor
FE Label
Predictor
Extractor Extractor Extractor Extractor
GF Label
Syntactic
Parser
GF Label Classification FE Label ClassificationFE Boundary DetectionFrame Disambiguation
FrameNet
Lexicon
Data Processing
Figure 1: System architecture.
house implementation of a named entity recognizer.
We also extract from the FN lexicon mappings of
target words and the semantic frames they evoke.
Various features corresponding to constituents
were extracted and passed to SVM and ME clas-
sifiers. For example, in Figure 2, the frame dis-
activities
NNS
AEOI
NNP POS
?s
NP
and
CC
facilities
NNS
NP
JJ
several
Inhibit_movement
Rope_manipulation
Attaching
Closure
Activity_finish
Finish_competition
Immobilization
Make_cognitive_connection
Knot_creation
Forming_relationships
Frame Disambiguation
positive negative
FE Boundary Detection
Head
NULL
Obj
Quant
Appositive
Dep
Ext
Gen
GF Classification
VBP
have
VP
VP
VBN
VBN
been
S
tied NPto
PP
NNS
universities
VP Concept_1
Concept_2
Evidence
Cognizer
Concepts
Time
Place
Circumstances
Frequency
FE Classif.
Figure 2: Classification examples for each sub-task.
ambiguation sub-task extracts features correspond-
ing to the constituent tied in order to predict the
right frame between the semantic frames that can be
evoked by this target word. In this figure, the correct
categories for each sub-task are shown in boldface.
The complete set of features extracted for all the
classification sub-tasks is illustrated in Figure 3.
These represent a subset of features used in previ-
ous works (Gildea and Jurafsky, 2002; Florian et al,
2002; Surdeanu et al, 2003; Xue and Palmer, 2004;
Bejan et al, 2004; Pradhan et al, 2005) for auto-
matic semantic role labeling and word sense disam-
biguation. Figure 3 also indicates whether or not a
feature is selected for a specific classification task.
In the remaining part of this section we describe
in detail each classification sub-task and the features
that have the most salient effect on improving the
corresponding classifiers.
2.1 Frame Disambiguation
In FrameNet, some target words can evoke multiple
semantic frames. In order to extract the semantic
structure of an ambiguous target word, the first step
is to assign the correct frame to the target word in
a given context. This task is similar with the word
sense disambiguation task.
We select from the FN lexicon 556 target words
that evoke at least two semantic frames and have at
least five sentences annotated for each frame, and
assemble a multi-class classifier for each ambiguous
target word. As described in Figure 3, for this task
we extract features used in word sense disambigua-
tion (Florian et al, 2002), lexical features of the tar-
get word, and NAMED ENTITY FLAGS associated
with the root node in a syntactic parse tree. For
the rest of the ambiguous target words that have less
than five sentences annotated we randomly choose a
frame as being the correct frame in a given context.
2.2 Frame Element Identification
The idea of splitting the automatic semantic role la-
beling task into FE boundary detection and FE label
classification was first proposed in (Gildea and Ju-
rafsky, 2002) and then adopted by other works in
this task. The problem of detecting the FE bound-
aries is cast as the problem of deciding whether or
not a constituent is a valid candidate for a FE.
461
TW
FD
G
FL
C
Feature DescriptionNO NO Feature Description
TW
FD
G
FL
C
FE
BD
FE
BD
FE
LC
FE
LC
CW: The content word of the constituent computed as described in
(Surdeanu et al, 2003);
v20
CW POS: The POS corresponding to the content word;v21
CW STEM: Stemmed content word;v22
GOVERNING CATEGORY: Test whether the noun phrase constituents arevv23
dominated by verbal phrases or sentence phrases;
SYNTACTIC DISTANCE: The length of the syntactic path;v24
PP FIRST WORD: If the constituent is a prepositional phrase, return the first
word in the phrase;
v25
HUMAN: Test whether the constituent phrase is either a personal pronoun
or a hyponym of first sense of PERSON synset in WordNet;
v26
CONSTITUENTS NUMBER: The number of candidate FEs;v27
CONSTITUENTS LIST: Constituents labels list of the candidate FEs;v28
SAME CLAUSE: Test whether the constituent is in the same clause withv29
the target word;
GF: The grammatical function of a candidate frame element;v30
GF LIST: The list of grammatical functions associated to the candidate FEs;v31
FRAME: The name of the semantic frame that is evoked by the target word;vvv32
NP SISTER: Determine whether the constituent has a noun phrase sister;v33
FIRST/LAST WORD: Return the first/last word of the constituent phrase;v34
FIRST/LAST POS: Return the first/last POS in the constituent;vv35
LEFT/RIGHT SISTER LABEL: Return the left/right sibling constituent label;v36
LEFT/RIGHT SISTER HEAD: Return the left/right sibling head word;v37
LEFT/RIGHT SISTER STEM HEAD: Return the left/right sibling stemmedv38
head word;
LEFT/RIGHT SISTER POS HEAD: Return the left/right sibling head POS;v39
HW POS: The syntactic head POS of the constituent;
HW STEM: The stem word of the constituent?s head word;
v v
v
18
19
TW STEM & HW STEM: Join of TW STEM and HW STEM;
TW STEM & PHRASE TYPE: Join of TW STEM and PHRASE TYPE;
v
v
40
41
VOICE & POSITION: Join of VOICE and POSITION.v42
TW UNIGRAMS: The words, stem words and part of speech (POS) unigramsv01
that are adjacent to target word expressions;
TW BIGRAMS: The words, stem words and POS bigrams that are adjacent to02
target word expressions;
TW WORD: The target word expression;03
TW STEM: The stem word(s) of the target word expression;v v04
v
TW POS: The POS of the target word;v
TW CLASS: The lexical class of the target word, e.g. verb, noun, adjective;vv06
05
NAMED ENTITY FLAGS: Set of binary features indicating whether a consti?vv07
tuent contains, is contained or exactly identifies a named entity;
VERB WSD: If the target word is a verb, extract the head noun of the direct
object and the prepositional object included in the verbal phrase;
v08
v
NOUN WSD: If the target word is a noun, extract the head word of the verbal
phrase that is in a verb?subject or verb?object relation with the noun;
09 v
ADJECTIVE WSD: If the target word is an adjective, extract the head noun
that is modified by the adjective;
10 v
PHRASE TYPE: The syntactic category of the constituent;vv11
DIRECTED PATH: Path in the syntactic parse tree between the constituent
and the target word preserving the movement direction;
vvv12
UNDIRECTED PATH: Same syntactic path as DIRECTED PATH without13 v
preserving the movement direction;
PARTIAL PATH: Path from the constituent to the earlier common ancestor of
the target word and the constituent;
v14
POSITION: Test whether the constituent contains the target word, or appears
before or after the target word;
vv v15
VOICE: Test if the verbal target word has active or passive construction;vv16
HW: The head word of the constituent;v vv17
Figure 3: Feature set for extracting frame semantic structures.
We consider a binary classifier over the entire FN
data and extract features for each constituent from a
syntactic parse tree. Because this experimental setup
allows training the binary classifier on a large set of
examples, the best feature combination consists of
a restrained number of features. Most of these fea-
tures are from the set proposed by (Gildea and Juraf-
sky, 2002). Another feature that improved the pre-
diction of FE boundaries in every feature selection
experiment is the FRAME feature. Since the frame
disambiguation is executed before the FE boundary
detection in the pipeline architecture, we can use the
FRAME feature at this step. This feature helps the
binary classifier distinguish between frame element
structures from different semantic frames.
2.3 Grammatical Function Classification
Once we identify the candidate boundaries for frame
elements, the next step is to assign the grammat-
ical functions to these boundaries. In FrameNet,
the grammatical functions represent the manner in
which the frame elements satisfy grammatical con-
straints with respect to the target word.
For this task we train a multi-class classifier over
the entire lexicon to predict seven categories of GFs
that exist in FN. In addition, we assign the NULL
category for those FEs that double as target words.
The features are extracted only for the constituents
that are identified as FEs in the previous FE bound-
ary identification sub-task. The best feature set in
this phase includes the features proposed by (Gildea
and Jurafsky, 2002) and the FRAME feature.
2.4 Frame Element Classification
The task of FE classification is to assign FE labels to
every constituent identified as FE. In order to predict
the frame elements, which are locally defined for
each semantic frame, we built 489 multi-class clas-
sifiers, where each classifier corresponds to a frame
in FrameNet. This partitioning of the FN lexicon has
the advantage of increasing the overall classification
performance and efficiently learning the frame ele-
ments labels. On the other hand, this approach suf-
fers from the lack of annotated data in some frames
and hence it requires using a large set of features.
The advantage of designing the classifiers in a
pipeline architecture is best illustrated in this sub-
task. Some of the most effective features for FE
classification are extracted using information from
previous sub-tasks: FRAME feature is made avail-
able by the TWFD sub-task, CONSTITUENTS NUM-
BER and CONSTITUENTS LIST are made available
by the FEBD sub-task, and GF and GF LIST are
made available by the GFLC sub-task.
462
3 Experimental Results
We report experimental results on all four classi-
fication sub-tasks. In our experiments we trained
two types of classification models for each sub-task:
SVM and ME. In order to optimize the performance
measure of each sub-task and to find the best config-
uration of classification models we used 20% of the
sub-tasks training data as validation data. Table 1
lists the best configuration of classification models
as well as the best sub-task results when running
the experiments on the validation data. For frame
disambiguation, we obtained 76.71% accuracy com-
pared to a baseline of 60.72% accuracy that always
predicts the most annotated frame for each of the
556 target words. The results for GFLC and FELC
sub-tasks listed in Table 1 were achieved by using
gold FE boundaries.
Frame Disambiguation
GF Label Classification
FE Label Classification
FE Boundary Detection
Task
SVM
SVM
76.71
Best Model
96.00
88.93
ME
ME
Accuracy
F1?measureRecall
73.65
Precision
87.08 79.80
Table 1: Task results on the validation set.
The SemEval?2007 organizers provided fully an-
notated training files, a scorer to evaluate these
training files, and testing files containing flat sen-
tences. In the evaluation process, a semantic depen-
dency graph corresponding to a fully system anno-
tated sentence is created and then matched with its
gold dependency graph. The matching process not
only evaluates every semantic structure of a target
word, but also considers frame-to-frame and FE-to-
FE graph relations between the semantic structures.
In addition, various scoring options were consid-
ered: exact or partial frame matching, partial credit
for evaluating the named entities, evaluation of the
flat frame elements labels, and an option for match-
ing only the frames in evaluation. The evaluation for
flat frame elements labels is similar with the evalu-
ation performed at Senseval-3. The only difference
is that for this scorer the FE boundaries must match
exactly.
In Table 2, we present the averaged precision,
recall and F1 measures for evaluating the seman-
tic dependency graphs and detecting the semantic
frames on the testing files. The ?Options? col-
umn represents the configuration parameters of the
scorer: (E)xact/(P)artial frame matching, seman-
tic (D)ependency or (L)abels only evaluation, and
(Y)es/(N)o named entity evaluation.
E D N
P D N
E L Y
P L Y
E D Y
P D Y
E L N
P L N
Semantic Dependency Evaluation
F1?measureRecallPrecision
51.10
50.29
54.78
51.85
51.38
56.13
55.56
56.59
27.74
27.05
29.48
27.59
26.95
29.45
30.19
30.14
35.88
35.11
38.26
35.94
35.29
38.57
39.04
39.25
69.16
71.69
80.35
69.16
71.69
80.35
77.82
77.82
42.73
44.43
49.79
42.73
44.43
49.79
48.09
48.09
52.71
54.74
61.35
52.71
54.74
61.35
59.32
59.32
Precision Recall F1?measure
Options Frame Detection Evaluation
Table 2: System results on the test set.
Although the system achieved good precision
scores on the test data, the recall values caused the
system to obtain unsatisfactory F1-measure values.
We expect that the recall will increase by consid-
ering various heuristics for a better mapping of the
frame elements to constituents in parse trees.
4 Conclusions
We described a system that participated in SemEval?
2007 for the task of extracting frame semantic struc-
tures. We showed that a pipeline architecture of the
SVM and ME classifiers as well as an adequate se-
lection of the classification models can improve the
performance measures of each sub-task.
References
Collin F. Baker, Charles J. Fillmore, and John B. Lowe. 1998.
The Berkeley FrameNet project. In Proceedings of the
COLING-ACL, Montreal, Canada.
Cosmin Adrian Bejan, Alessandro Moschitti, Paul Mora?rescu,
Gabriel Nicolae, and Sanda Harabagiu. 2004. Semantic
Parsing Based on FrameNet. In Senseval-3: Workshop on
the Evaluation of Systems for the Semantic Analysis of Text.
Radu Florian, Silviu Cucerzan, Charles Schafer, and David
Yarowsky. 2002. Combining classifiers for word sense dis-
ambiguation. Natural Language Engineering.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic Labeling
of Semantic Roles. Computational Linguistic.
Sameer Pradhan, Kadri Hacioglu, Valeri Krugler, Wayne Ward,
James H. Martin, and Daniel Jurafsky. 2005. Support vec-
tor learning for semantic argument classification. Journal of
Machine Learning Research.
Mihai Surdeanu, Sanda M. Harabagiu, John Williams, and Paul
Aarseth. 2003. Using predicate-argument structures for in-
formation extraction. In Proceedings of ACL.
Nianwen Xue and Marta Palmer. 2004. Calibrating features for
semantic role labeling. In Proceedings of EMNLP.
463
Unsupervised Event Coreference Resolution
Cosmin Adrian Bejan?
Vanderbilt University
Sanda Harabagiu??
University of Texas at Dallas
The task of event coreference resolution plays a critical role in many natural language pro-
cessing applications such as information extraction, question answering, and topic detection
and tracking. In this article, we describe a new class of unsupervised, nonparametric Bayesian
models with the purpose of probabilistically inferring coreference clusters of event mentions
from a collection of unlabeled documents. In order to infer these clusters, we automatically
extract various lexical, syntactic, and semantic features for each event mention from the doc-
ument collection. Extracting a rich set of features for each event mention allows us to cast
event coreference resolution as the task of grouping together the mentions that share the same
features (they have the same participating entities, share the same location, happen at the same
time, etc.).
Some of the most important challenges posed by the resolution of event coreference in an
unsupervised way stem from (a) the choice of representing event mentions through a rich set
of features and (b) the ability of modeling events described both within the same document and
across multiple documents. Our first unsupervised model that addresses these challenges is a
generalization of the hierarchical Dirichlet process. This new extension presents the hierarchi-
cal Dirichlet process?s ability to capture the uncertainty regarding the number of clustering
components and, additionally, takes into account any finite number of features associated with
each event mention. Furthermore, to overcome some of the limitations of this extension, we
devised a new hybrid model, which combines an infinite latent class model with a discrete time
series model. The main advantage of this hybrid model stands in its capability to automatically
infer the number of features associated with each event mention from data and, at the same
time, to perform an automatic selection of the most informative features for the task of event
coreference. The evaluation performed for solving both within- and cross-document event coref-
erence shows significant improvements of these models when compared against two baselines for
this task.
? Department of Biomedical Informatics, School of Medicine, Vanderbilt University, 400 Eskind Biomedical
Library, 2209 Garland Avenue, Nashville, TN 37232, USA. E-mail: adi.bejan@vanderbilt.edu.
?? Human Language Technology Research Institute, Department of Computer Science, University of Texas
at Dallas, 800 West Campbell Road, Richardson, TX 75080, USA. E-mail: sanda@hlt.utdallas.edu.
Submission received: 6 February 2012; revised submission received: 9 May 2013; accepted for publication:
28 June 2013.
doi:10.1162/COLI a 00174
? 2014 Association for Computational Linguistics
Computational Linguistics Volume 40, Number 2
1. Introduction
Event coreference resolution consists of grouping together the text expressions that refer
to real-world events (also called event mentions) into a set of clusters such that all the
mentions from the same cluster correspond to a unique event. The problem of event
coreference is not new. It was originally studied in philosophy, where researchers tried
to determine when two events are identical and when they are different. One relevant
theory in this direction was proposed by Davidson (1969), who argued that two events
are identical if they have the same causes and effects. Later on, a different theory was
proposed by Quine (1985), who considered that each event is associated with a physical
object (which is well defined in space and time), and therefore, two events are identical
if their corresponding objects have the same spatiotemporal location. According to
Malpas (2009), in the same year, Davidson abandoned his suggestion to embrace the
Quinean theory on event identity (Davidson 1985).
Resolving event coreference is an essential requirement for many natural language
processing (NLP) applications. For instance, in topic detection and tracking, event
coreference resolution is required in order to identify new seminal events in broadcast
news that have not been mentioned before (Allan et al. 1998). In information extraction,
event coreference information was used for filling predefined template structures from
text documents (Humphreys, Gaizauskas, and Azzam 1997). In question answering, a
novel method of mapping event structures was used in order to provide answer justi-
fication (Narayanan and Harabagiu 2004). The same idea of mapping event structures
was used in a graph-matching approach for enhancing textual entailment (Haghighi,
Ng, and Manning 2005). Event coreference information was also used for detecting
contradictions in text (de Marneffe, Rafferty, and Manning 2008).
Previous NLP approaches for solving event coreference relied on supervised learn-
ing methods that explore various linguistic properties in order to decide if a pair of
event mentions is coreferential or not (Humphreys, Gaizauskas, andAzzam 1997; Bagga
and Baldwin 1999; Ahn 2006; Chen and Ji 2009; Chen, Su, and Tan 2010b). In spite
of being successful for a particular labeled corpus, in general, these pairwise models
are dependent on the domain or language that they are trained on. For instance, in
order to adapt a supervised system to run over a collection of documents written in
a different language or belonging to a different domain of interest, at least a minimal
annotation effort needs to be performed (Daume? III 2007). Furthermore, because these
models are dependent on local pairwise decisions, they are unable to capture a global
event distribution at the topic- or document-collection level.
To address these limitations, we departed from the idea of using supervised ap-
proaches for event coreference resolution and explored how a new class of unsuper-
vised, nonparametric Bayesian models can be used to probabilistically infer coreference
clusters of event mentions from a collection of unlabeled documents. In addition,
because an event can be mentioned multiple times in a document collection and its
mentions may occur both in the same document or across multiple documents, we
designed our unsupervised models to solve the two subproblems of within-document
and cross-document event coreference resolution. In order to evaluate the unsupervised
models for these two subproblems, we annotated a new data set encoding both within-
and cross-document event coreference information.
Besides our contribution of using unsupervised methods to solve within- and
cross-document event coreference, in this article we present novel Bayesian models
that provide a more flexible framework for representing data than current models. By
starting from the generic problem of clustering observable linguistic objects (i.e., event
312
Bejan and Harabagiu Unsupervised Event Coreference Resolution
mentions) encoded into a large collection of text documents where the clusters (i.e.,
events) can be shared across documents, we devised our unsupervised models such
that they provide solutions to the following four desiderata:
1) We prefer the number of clusters (denoted by K) to be probabilistically inferred
from data rather than to be assigned to an a priori fixed value. This desideratum
of allowing K to be a free parameter in the Bayesian models devised for our
problem constitutes a more realistic approach because, in general, document
collections encode an unspecified number of latent linguistic structures.
2) We redefine the task of finding clusters of mentions that refer to the same events
as the task of identifying those mentions that share the same event participants
and the same event properties. For example, the same entity must participate in
all the event mentions that are coreferential; also, all the coreferential mentions
must have the same spatiotemporal location. These characteristics extracted for
each event mention from text are also called linguistic features and, in general,
the event mentions corresponding to each of these clusters are characterized by
a large set of features. Because of this, we desire that the generative process
associated with each Bayesian model to automatically adapt every time a new
feature is added in the feature extraction phase.
3) Although each event mention is represented as a feature-rich linguistic object,
there is no guarantee that all the features that describe event mentions have a
positive impact for the task of event coreference. Some of these features may be
redundant or may increase the complexity of the Bayesian models solving this
task and, consequently, they may contribute to lowering the overall performance
of event coreference. To address these problems, we wish to incorporate into the
Bayesian models a feature selection mechanism that is able to automatically build
a set of the most salient features from the initial feature set such that only these
salient features will participate in the process of clustering event mentions. In this
regard, we assume that a feature is salient if it corresponds to a large number of
samples in the generative process. We denote the size of the salient feature set
by M. Furthermore, in spite of the fact that the initial feature space describing
event mentions can have an unbounded number of features, we want the set of
salient features to be finite (i.e., M?finite) at any given point in time during the
generative process corresponding to each Bayesian model.
4) Finally, we also want our Bayesian models to capture the structural dependencies
of the observable objects. In this way, the models can take advantage of the se-
quential order in which the event mentions are generated inside each document.
We believe that these four desiderata constitute a more natural approach for clustering
complex linguistic objects from a large collection of documents and relax many of the
constraints imposed in the current clustering tasks.
It is worth pointing out that the generic problem described here can be instanti-
ated by tasks not only from the area of computational linguistics, but also from other
research areas as well. For instance, in biomedical informatics, clinical researchers can
use the new Bayesian models to perform studies over various cohorts of patients. In
this configuration, the observations to be clustered correspond to patients, and the
features associated with the patients can be extracted from clinical reports or can be
represented by structured clinical information (e.g., white blood cells, temperature,
313
Computational Linguistics Volume 40, Number 2
heart rate, respiratory rate, sputum culture). Another instance of the generic problem
described here is from data mining. In this domain, clustering tasks can be performed
over structured information stored in large tables (e.g., products, restaurants, hotels).
For this type of problem, each object is associated with a row in a table and the features
correspond to table columns.
2. Related Work
Unlike entity coreference resolution, event coreference resolution is a relatively less-
studied task. One rationale is that events are expressed in many more varied linguistic
constructs. For example, event mentions are typically predications that require more
complex lexico-semantic processing, and furthermore, the capability of extracting fea-
tures that characterize them has been available only since semantic parsers based on
PropBank (Palmer, Gildea, and Kingsbury 2005) and FrameNet (Baker, Fillmore, and
Lowe 1998) corpora have been developed. In contrast, entity coreference resolution
has been intensively studied and many successful techniques for identifying mention
clusters have been developed (Cardie and Wagstaf 1999; Haghighi and Klein 2009;
Stoyanov et al. 2009; Haghighi and Klein 2010; Raghunathan et al. 2010; Rahman and
Ng 2011).
Even if entity coreference resolution has received much attention from the compu-
tational linguistic researchers, there is only limited work that incorporates event-related
information to solve entity coreference, typically by considering the verbs that are
present in the context of a referring entity as features. For instance, Haghighi and Klein
(2010) include the governor of the head of nominal mentions as features in their model.
Rahman and Ng (2011) used event-related information by looking at which semantic
role the entity mentions can have and the verb pairs of their predicates. More recently,
Lee et al. (2012) proposed an approach to jointly model event and entity coreference by
allowing information from event coreference to help entity coreference, and the other
way around. Their supervised method uses a high-precision entity resolution method
based on a collection of deterministic models (called sieves) to produce both entity and
event clusters that are optimally merged using linear regression. A similar technique
that treated entity and event coreference resolution jointly was reported in He (2007)
using narrative clinical data.
Research that aimed at resolving only event coreference was initiated by the
template merging task required in MUC evaluations and was primarily focused on
scenario-specific events (Humphreys, Gaizauskas, andAzzam 1997; Bagga and Baldwin
1999). More recently, various supervised approaches using a mention-pair probabilistic
framework (Ahn 2006), spectral graph clustering (Chen and Ji 2009), and tree kernel?
based methods (Chen, Su, and Tan 2010b) have been used to solve event coreference.
Tree kernel?based methods have also been used to solve a special case of event coref-
erence resolution called event pronoun resolution (Chen, Su, and Tan 2010a; Kong and
Zhou 2011). To the best of our knowledge, the framework for solving event coreference
presented in this article, extending the approach reported in Bejan and colleagues (Bejan
et al. 2009; Bejan and Harabagiu 2010), is the only line of research on event coreference
resolution that uses fully unsupervised methods and is based on Bayesian models.
Over the past years, Bayesian models have been extensively used for the purpose
of solving similar problems or subproblems of the generic problem presented in the
previous section. In 2003, Blei, Ng, and Jordan proposed a parametric approach, called
latent Dirichlet allocation (LDA), for automatically learning probability distributions
of words corresponding to a specific number of latent classes (or topics) from a large
314
Bejan and Harabagiu Unsupervised Event Coreference Resolution
collection of text documents. In this latent class model, documents are expressed as
probabilistic mixtures of topics, while each topic has assigned a multinomial distribu-
tion over the words from the entire document collection. This approach also uses an
exchangeability assumption by modeling the documents as bags of words. The LDA
model and variations of it have been used in many applications such as topic modeling
(Blei, Ng, and Jordan 2003; Griffiths and Steyvers 2004), word sense disambiguation
(Boyd-Graber, Blei, and Zhu 2007), object categorization from a collection of images
(Sivic et al. 2005, 2008), image classification into scene categories (Li and Perona 2005),
discovery of event scenarios from text documents (Bejan 2008; Bejan and Harabagiu
2008b), and attachment of attributes to a concept ontology (Reisinger and Pas?ca 2009).
The LDA model, although attractive, has the disadvantage of requiring a priori knowl-
edge regarding the number of latent classes.
A more suitable approach for solving our problem is the hierarchical Dirichlet
process (HDP) model described in Teh et al. (2006). Like LDA, this model considers
problems that involve groups of data, where each observable object is sampled from
a mixture model and each mixture component is shared across groups. However, the
HDP mixture model is a nonparametric generalization of LDA that is also able to
automatically infer the number of clustering components K (the first desideratum for
our problem). It consists of a set of Dirichlet processes (DPs) (Ferguson 1973), in which
each DP is associated with a group of data. In addition, these DPs are coupled through
a common random base measure which is itself distributed according to a DP. Due to
the fact that a DP provides a nonparametric prior for the number of classes K, the HDP
setting allows for this number to be unbounded in each group. More recently, various
other applications have been proposed to improve the existing HDP inference algo-
rithms (Wang, Paisley, and Blei 2011; Bryant and Sudderth 2012). HDP has been used
in a wide variety of applications such as maneuvering target tracking (Fox, Sudderth,
and Willsky 2007), visual scene analysis (Sudderth et al. 2008), information retrieval
(Cowans 2004), entity coreference resolution (Haghighi and Klein 2007; Ng 2008), event
coreference resolution (Bejan et al. 2009; Bejan and Harabagiu 2010), word segmentation
(Goldwater, Griffiths, and Johnson 2006), and construction of stochastic context-free
grammars (Finkel, Grenager, and Manning 2007; Liang et al. 2007).
Although infinite latent class models like HDP have the advantage of automatically
inferring the number of categorical outcomes K, they are still limited in representing
feature-rich objects. Specifically, in their original form, they are not able to model the
data such that each observable object can be generated from a combination of multiple
features. For example, in HDP, each data point is represented only by its corresponding
word. For this reason, we built new Bayesian models on top of already-existing models
with the main goal of providing a more flexible framework for representing data.
The first model extends the HDP model such that it takes into account additional
linguistic features associated with event mentions. This extension is performed by
using a conditional independence assumption between the observed random variables
corresponding to object features. Thus, instead of considering as features only thewords
that express the event mentions (which is the way an observable object is represented
in the original HDP model), we devised an HDP extension that is also able to represent
features such as location, time, and agent for each event mention. This extension was
inspired from the fully generative Bayesian model proposed by Haghighi and Klein
(2007). However, Haghighi and Klein?s model was strictly customized for the task of
entity coreference resolution. As also noted in Ng (2008) and Poon and Domingos
(2008), whenever new features need to be considered in Haghighi and Klein?s model,
the extension becomes a challenging task. Also, Daume? III and Marcu (2005) performed
315
Computational Linguistics Volume 40, Number 2
related work in this direction by proposing a generative model for solving supervised
clustering problems.
As an alternative to the HDP model, an important extension of latent class models
that are able to represent feature-rich objects is the Indian buffet process (IBP) model
presented in Griffiths and Ghahramani (2005). The IBP model defines a distribution
over infinite binary sparse matrices that can be used as a nonparametric prior on the
features associated with observable objects. Moreover, extensions of this model were
considered in order to provide a more flexible approach for modeling the data. For
example, the Markov Indian buffet process (mIBP) (Van Gael, Teh, and Ghahramani
2008) was defined as a distribution over an unbounded set of binary Markov chains,
where each chain can be associated with a binary latent feature that evolves over time
according to Markov dynamics. Also, the phylogenetic Indian buffet process (pIBP)
(Miller, Griffiths, and Jordan 2008) was created as a non-exchangeable, nonparametric
prior for latent feature models, where the dependencies between objects were expressed
as tree structures. Examples of applications that utilized these models are: identification
of protein complexes (Chu et al. 2006), modeling of dyadic data (Meeds et al. 2006),
modeling of choice behavior (Go?ru?r, Ja?kel, and Rasmussen 2006), and event coreference
resolution (Bejan et al. 2009; Bejan and Harabagiu 2010).
Our extension of the HDP model still does not fulfill all the desiderata for the
generic problem introduced in Section 1. It still requires a mechanism to automati-
cally select a finite set of salient features that will be used in the clustering process
(third desideratum) as well as a mechanism for capturing the structural dependencies
between objects (fourth desideratum). To overcome these limitations, we created two
additional models. First, we incorporated the mIBP framework into our HDP extension
to create the mIBP?HDP model. And second, we coupled an infinite latent feature
model with an infinite latent class model into a new discrete time series model. For
the infinite latent feature model, we chose the infinite factorial hidden Markov model
(iFHMM) (Van Gael, Teh, and Ghahramani 2008) coupled with the mIBP mechanism
in order to represent the latent features as an infinite set of parallel Markov chains; for
the infinite latent class model, we chose the infinite hidden Markov model (iHMM)
(Beal, Ghahramani, and Rasmussen 2002). We call this new hybrid the iFHMM?iHMM
model.
2.1 Contribution
This article represents an extension of our previous work on unsupervised event
coreference resolution (Bejan et al. 2009; Bejan and Harabagiu 2010). In this work, we
present more details on the problem of solving both within- and cross-document event
coreference as well as describe a generic framework for solving this type of problem
in an unsupervised way. As data sets, we consider three different resources, including
our own corpus (which is the only corpus available that encodes event coreference
annotations across and within documents). In the next section, we provide additional
information on how we performed the annotation of this corpus. Another major contri-
bution of this article is an extended description of the unsupervised models for solving
event coreference. In particular, we focused on providing further explanations about
the implementation of the mIBP framework as well as its integration into the HDP and
iHMM models. Finally, in this work, we significantly extended the experimental results
section, which also includes a novel set of experiments performed over the OntoNotes
English corpus (LDC-ON 2007).
316
Bejan and Harabagiu Unsupervised Event Coreference Resolution
3. Event Coreference Data Sets
Because our nonparametric Bayesian models are also unsupervised, they do not require
the data set(s) on which they are trained to be annotated with event coreference infor-
mation. The only requirement for them to infer coreference clusters of event mentions
is to have the observable objects (i.e., the event mentions) identified in the order they
occur in the documents as well as to have all the linguistic features associated with
these objects extracted. However, in order to see how well these models perform, we
need to compare their results with manually annotated clusters of event mentions. For
this purpose, we evaluated our models on three different data sets annotated with event
coreference information.
The first data set was used for the event coreference evaluations performed in the
automatic content extraction (ACE) task (LDC-ACE 2005). This resource contains only a
restricted set of event types such as LIFE, BUSINESS, CONFLICT, and JUSTICE. As a second
data set, we used the OntoNotes English corpus (release 2.0), a more diverse resource
that provides a larger coverage of event (and entity) annotations. The utilization of
the ACE and OntoNotes corpora for evaluating our event coreference models is, how-
ever, limited because these resources provide only within-document event coreference
annotations. For this reason, as a third data set, we created the EventCorefBank (ECB)
corpus1 to increase the diversity of event types and to be able to evaluate our models
for both within- and cross-document event coreference resolution. Recently, Lee et al.
(2012) extended the EventCorefBank corpus with entity coreference information and
additional annotations of event coreference.
One important step in the creation process of the ECB corpus consists of finding
sets of related documents that describe the same seminal event2 such that the an-
notation of coreferential event mentions across documents is possible. In this regard,
we searched the Google News archive3 for various topics whose description contains
keywords such as commercial transaction, attack, death, sports, announcement, terrorist act,
election, arrest, natural disaster, and so on, and manually selected sets of Web documents
describing the same seminal event for each of these topics. In a subsequent step, for
every Web document, we automatically tokenized and split the textual content into
sentences, and saved the preprocessed data in a uniquely identified text file. Next,
we manually annotated a limited set of events in each text file in accordance with the
TimeML specification (Pustejovsky et al. 2003a). To mark the event mentions and the
coreferential relations between them we utilized the Callisto4 and Tango5 annotation
tools, respectively. Additional details regarding the annotation process for creating the
ECB resource are described in Bejan and Harabagiu (2008a).
Several annotation fragments from ECB are shown in Example (1). In this example,
event mentions are annotated at the sentence level, sentences are grouped into docu-
ments, and the documents describing the same seminal event are organized into topics.
The topics shown in Example (1) describe the seminal event of arresting sea pirates by a
1 The ECB corpus is available at http://www.hlt.utdallas.edu/?ady/data/ECB1.0.tar.gz.
2 A seminal event in a document is the event that triggers the topic of the document and has
interconnections with the majority of events from its surrounding textual context. Furthermore, the
set of documents describing the same seminal event defines a topic. A more detailed description
of seminal events can be found in topic detection and tracking literature (Allan 2002).
3 http://news.google.com.
4 http://callisto.mitre.org.
5 Tango is a tool designed for annotating relations between the event mentions encoded in the TimeML
format and is available at http://timeml.org/site/tango/tool.html.
317
Computational Linguistics Volume 40, Number 2
Topic 12
Document 3
s1: In another anti-piracy operation, Navy warship on Saturday repulsed an attack on a merchant
vessel in the Gulf of Aden and [nabbed]em1 23 Somali and Yemeni sea brigands.
Topic 43
Document 3
s4: AMD agreed to [buy]em2 Markham, Ontario-based ATI for around $5.4 billion in cash and stock,
the companies announced Monday.
s5: The [acquisition]em3 would turn AMD into one of the world?s largest providers of graphics
chips.
Topic 44
Document 2
s1: Hewlett-Packard is negotiating to [buy]em4 technology services provider Electronic Data Sys-tems.
? ? ?
s8: With a market value of about $115 billion, HP could easily use its own stock to finance the
[purchase]em5 .
s9: If the [deal]em6 is completed, it would be HP?s biggest [acquisition]em7 since it [bought]em8
Compaq Computer Corp. for $19 billion in 2002.
Document 5
s2: Industry sources have confirmed to eWEEK that Hewlett-Packard will [acquire]em9 Electronic
Data Systems for about $13 billion.
Topic 55
Document 2
s2: Despite his [arrest]em10 on suspicion of driving under the influence yesterday, Chargers receiver
Vincent Jackson will play in Sunday?s AFC divisional playoff game at Pittsburgh.
Document 3
s2: San Diego Chargers receiver Vincent Jackson was [arrested]em11 on suspicion of drunk driving
on Tuesday morning, five days before a key NFL playoff game.
s3: Police [apprehended]em12 Jackson in San Diego at 2:30 a.m. and booked him for the misdemeanor
before his release.
Example 1
Examples of event mention annotations.
Navywarship (topic 12), the event of buying ATI byAMD (topic 43), the event of buying
EDS by HP (topic 44), and the event of arresting a reputed football player (topic 55).
When taken out of context, the event mentions annotated in this example refer only to
two generic events: arrest and buy. On the other hand, when these mentions are contex-
tually associated with the event properties expressed in Example (1), five individuated
events can be distinguished: e1={em2, em3}, e2={em4?7, em9}, e3={em8}, e4={em1}, and
e5={em10, em11, em12}. For example, em4?7 are event mentions referring to the same real
event (of buying EDS by HP), whereas em2 (buy) and em4(buy) correspond to different
318
Bejan and Harabagiu Unsupervised Event Coreference Resolution
individuated events because they have a different AGENT (i.e., BUYER(em2)=AMD is dif-
ferent from BUYER(em4)=HP). Similarly, thementions em1(nabbed) and em12 (apprehended)
do not corefer because they correspond to different spatial and temporal locations (e.g.,
LOCATION(em1)=Gulf of Aden is different from LOCATION(em12)=San Diego).
This organization of event mentions leads to the idea of creating an event hier-
archy as the one illustrated in Figure 1. Specifically, this figure depicts the hierarchy
of the events described in Example (1). In this hierarchy, the nodes on the first level
correspond to event mentions (e.g., em11 corresponds to arrested), the nodes on the
second level correspond to individuated events (e.g., e5 subsumes all the event mention
nodes that refer to the arrest of Vincent Jackson), and, finally, the nodes on the third
level correspond to generic events (e.g., the node arrest contains all possible arrest
events). In this article, our focus is to discover the nodes on the second level of this
hierarchy.
As can be seen from Example (1), solving the event coreference problem poses
many interesting challenges. For instance, in order to solve the coreference chain of
event mentions that refer to the event e2, we need to take into account the following
issues: (i) a coreference chain can encode both within- and cross-document coreference
information; (ii) twomentions from the same chain can have different word classes (e.g.,
em4(buy)?verb, em5(purchase)?noun); (iii) not all the mentions from the same chain are
synonymous (e.g., em4(buy) and em9(acquire)), although a semantic relation might exist
between them (e.g., in WordNet [Fellbaum 1998], the genus of buy is acquire); (iv) not
all the properties associated with an event mention are expressed in text (e.g., all the
properties of em5(purchase) are omitted). In Section 7, we discuss additional challenges
of the event coreference problem that are not observed in Example (1).
4. Linguistic Features for Event Coreference Resolution
The main idea for solving event coreference is to identify the event mentions (from the
same or different documents) that share the same characteristics (e.g., all the mentions
in a cluster convey the same meaning in text, have the same participants, and happen
in the same space and temporal location). Moreover, finding clusters of event mentions
that share the same characteristics is identical to finding clusters of mention features
that correspond to the same real event. For instance, Figure 2 depicts five clusters of
linguistic features that characterize the five individuated events from Example (1). As
can be observed, each individuated event corresponds to a subset of features that are
usually common to all the mentions referring to it. For this purpose, we extracted var-
ious linguistic features associated with each event mention from the ACE, OntoNotes,
and ECB corpora.
individuated
events
generic
events
event
mentions em9em7em6em5em4 em1em8
e4e3e2
em12em11em10
e5
em2 em3
e1
arrest
event
buy
Figure 1
Fragment from the event hierarchy.
319
Computational Linguistics Volume 40, Number 2
Before describing in detail all the categories of linguistic features considered for
solving event coreference, we would like to emphasize that we make a clear distinction
between the notions of feature type and feature value throughout this article. A feature
type is represented by a characteristic that can be extracted with a specific methodology
and is associated with at least two feature values. For instance, the feature values
corresponding to the feature type WORD consist of all the distinct words extracted from
a given data set. In order to differentiate between the same values of different feature
types, we inserted to the notation of each feature value the name of its corresponding
feature type (e.g., WORD:play).
4.1 Lexical Features (LF)
We capture the lexical context of an event mention by extracting the following features:
the head word (HW), the lemmatized head word (HL), the lemmatized left and right
words surrounding the mention (LHL, RHL), and the HL features corresponding to
the left and right mentions (LHE, RHE). For instance, the lexical features extracted
for the event mention em8(bought) from our example are HW:bought, HL:buy, LHL:it,
RHL:Compaq, LHE:acquisition, and RHE:acquire.
4.2 Class Features (CF)
This category of features aims to group mentions into several types of classes: the part-
of-speech of the HW feature (POS), the word class of the HW feature (HWC), and the event
class of themention (EC). The HWC feature type is associatedwith the following four fea-
ture values: VERB, NOUN, ADJECTIVE, and OTHER. As feature values for the EC feature
type, we consider the seven event classes defined in the TimeML specification language
(Pustejovsky et al. 2003a): OCCURRENCE, PERCEPTION, REPORTING, ASPECTUAL, STATE,
I ACTION, and I STATE. To extract all these event classes for all the event mentions, we
used an event identifier trained on the TimeBank corpus (Pustejovsky et al. 2003b), a
linguistic resource encoding temporal elements such as events, time expressions, and
temporal relations. More details about this event identifier are described in Bejan (2007).
4.3 WordNet Features (WF)
In our efforts to create clusters of attributes corresponding to event mentions as close
as possible to the true attribute clusters of the individuated events, we built two sets
of word clusters using the entire lexical information from the WordNet database. After
Event properties: Event properties: Event properties:
Event properties:Event properties:
Buyer: HP
Seller: Compaq
Money: $19 billion
Time: 2002
e3 Suspect: sea brigands
Authority: Navy warship
Location: Gulf of Aden
Time: Saturday
e4 Suspect: Vincent Jackson
Authority: police
Location: San Diego
Time: Tuesday
e5
Money: $13 billion
Buyer: HP
Seller: EDS
e2
Buyer: AMD
Seller: ATI
Money: $5.4 billion
e1
Figure 2
Linguistic features associated with the individuated events encoded in Example (1).
320
Bejan and Harabagiu Unsupervised Event Coreference Resolution
creating these sets of clusters, we associated each event mention with only one cluster
from each set. For the first set, we used the transitive closure of the WordNet SYNONY-
MOUS relation to form clusters with all the words from WordNet (WNS). For instance,
the verbs buy and purchase correspond to the same cluster ID because there exist a chain
of SYNONYMOUS relations between them inWordNet. For the second set, we considered
as grouping criteria the categorization of words from the WordNet lexicographer?s files
(WNL). In addition, for each word that is not represented in WordNet, we created a new
cluster ID in each set of clusters.
4.4 Semantic Features (SF)
To extract features that characterize participants and properties of event mentions,
we used the semantic parser described in Bejan and Hathaway (2007). One category
of semantic features that we identified for event mentions is the predicate argument
structures encoded in the PropBank annotations (Palmer, Gildea, and Kingsbury 2005).
The predicate argument structures in PropBank are represented by events (or verbs)
and by the semantic roles (or predicate arguments) associated with these events. For
example, ARG0 annotates a specific type of semantic role which represents the AGENT,
DOER, or ACTOR of a specific event. Another argument is ARG1, which plays the role
of the PATIENT, THEME, or EXPERIENCER of an event. In Example (1), for instance,
the predicate arguments associated with the event mention em8(bought) are ARG0:[it],
ARG1:[Compaq Computer Corp.], ARG3:[ for $19 billion], and ARG-TMP:[in 2002].
Event mentions are not only expressed as verbs in text, but they also can occur as
nouns and adjectives. Therefore, for a better coverage of semantic features, we also used
the semantic annotations encoded in the FrameNet corpus (Baker, Fillmore, and Lowe
1998). FrameNet annotates word expressions capable of evoking conceptual structures,
or semantic frames, which describe specific situations, objects, or events. The semantic
roles associated with a word in FrameNet, or frame elements, are locally defined for
the semantic frame evoked by the word. In general, the words annotated in FrameNet
are expressed as verbs, nouns, and adjectives.
To preserve the consistency of the semantic role features, we aligned the frame
elements to the predicate arguments by running the PropBank semantic parser on the
manual annotations from FrameNet as well as running the FrameNet parser on the
PropBank annotations. Moreover, to obtain a better alignment for each semantic role, we
ran both parsers on a large amount of unlabeled text. The result of this process is a map
with all frame elements statistically aligned to all predicate arguments. For instance,
in 99.7% of the cases the frame element BUYER of the semantic frame COMMERCE BUY
is mapped to ARG0, and in the remaining 0.3% of the cases to ARG1. Additionally, we
used this map to create a more general semantic feature that assigns a frame element
label to each predicate argument. Examples of semantic features for the em8 mention
are ARG0:BUYER, ARG1:GOODS, ARG3:MONEY, and ARG-TMP:TIME.
Another two semantic features used in our experiments are: (1) the semantic frame
(FR) evoked by every mention in the data set, since in general, frames are able to capture
properties of generic events (Lowe, Baker, and Fillmore 1997); and (2) the WNS feature
applied to the head word of every semantic role (e.g., WSARG0, WSARG1).
4.5 Feature Combinations (FC)
We also explored various combinations of the given features. For instance, the feature
resulting from the combination of the HW and HWC feature types for em8(bought) in
321
Computational Linguistics Volume 40, Number 2
Example (1) is HW+HWC:bought+VERB. Examples of additional feature combinations we
experimented with are HL+FR, HW+POS, FR+POS+EC, FE+ARG1, and so forth.
5. Finite Feature Models
In this section, we first present HDP, a nonparametric Bayesian model that is capable
of clustering objects based on one feature type (i.e., WORD); then, we introduce a novel
extension of this model that describes an algorithm for clustering objects characterized
by multiple feature types.
The HDP models take as input a collection of I documents, where each document
i has Ji event mentions. Each event mention is characterized by L feature types (FT),
and each feature type is represented by a finite vocabulary of feature values (fv). For
example, the feature values extracted from an event coreference data set and associated
with the feature type HW constitute all possible head words of the event mentions
annotated in the data set. Therefore, we can represent the observable properties of an
event mention as a vector of pairs ?(FT1 : fv1i), . . . , (FTL : fvLi)?, where each feature value
index i ranges in the feature value space of its corresponding feature type. In the
description of these models, we also consider Z: the set of indicator random variables
for indices of events (i.e., an array of size equal with the number of event mentions in the
document collection where Zi,j represents the event index of the event mention j from
the document i);?z: the set of parameters associated with an event z;?: a notation for all
model parameters; and X: a notation for all random variables that represent observable
features. As already introduced in Section 1, we denote by K the total number of latent
events.
Given a document collection annotated with event mentions, the goal is to find
the best assignment of event indices Z?, which maximize the posterior probability
P(Z |X). In a Bayesian approach, this probability is computed by integrating out all
model parameters:
P(Z|X) =
?
P(Z,?|X)d? =
?
P(Z|X,?)P(?|X)d? (2)
5.1 The HDP1f Model
The one feature model, denoted here as HDP1f , constitutes the simplest representation
of anHDPmodel. In thismodel, depicted graphically in Figure 3(a), the observable com-
ponents are characterized by only one feature type (e.g., the head lemma corresponding
to each event mention). The distribution over events associated with each document,
?, is generated by a Dirichlet process with a concentration parameter ?>0. Because
this setting enables a clustering of event mentions at the document level, it is desirable
that events be shared across documents and the number of events, K, be inferred from
data. To ensure this flexibility, a global nonparametric DP prior with a hyperparameter
? and a global base measure H can be considered for ? (Teh et al. 2006). The global
distribution drawn from this DP prior, denoted as ?0 in Figure 3(a), encodes the event
mixing weights. Thus, the same global events are used for each document, but each
event has a document specific distribution ?i that is drawn from a DP prior centered
on ?0.
322
Bejan and Harabagiu Unsupervised Event Coreference Resolution
FRi
POSi
?
?
H
Zi
?
??
?
Xi
?
?
HLi
H
?
H
Zi
HLi FRi
?
?
?
?
H
Zi
HLi
?
?
?
?
(b)
?
?
?0
?
IJi
(a)
Ji
?0
?
I
?
?
?
?
(d)
?
?
?0
?
IJi
Zi
(c)
?0
?
Ji I
L
Figure 3
Graphical representation of four HDP models. Each node corresponds to a random variable. In
particular, shaded nodes denote observable variables. Each rectangle captures the replication of
the structure it contains. The number of replications is indicated in the bottom-right corner of the
rectangle. The model depicted in (a) is an HDP model using one feature type; the model in (b)
employs the HL and FR feature types; (c) illustrates a flat representation of a limited number of
feature types in a generalized framework (henceforth, HDPflat); and (d) captures a simple
example of structured network topology of three feature types (henceforth, HDPstruct).
To infer the true posterior probability of P(Z|X), we followed Teh et al. (2006)
and used a Gibbs sampling algorithm (Geman and Geman 1984) based on the direct
assignment sampling scheme. In this sampling scheme, the ? and ? parameters are
integrated out analytically. The formula for sampling an event index for mention j from
document i, Zi,j, is given by:6
P(Zi,j | Z?i,j,HL) ? P(Zi,j | Z?i,j)P(HLi,j | Z,HL?i,j) (3)
where HLi,j is the head lemma of event mention j from document i.
6 Z?i,j represents a notation for Z? {Zi,j}.
323
Computational Linguistics Volume 40, Number 2
First, in the generative process of an event mention, an event index z is sam-
pled by using a mechanism that facilitates sampling from a prior for infinite mixture
models called the Chinese restaurant franchise (CRF) representation, as reported in
(Teh et al. 2006):
P(Zi,j = z | Z?i,j,?0) ?
{
??u0 , if z = znew
nz + ??z0, otherwise
(4)
In this formula, nz is the number of event mentions with event index z, znew is a new
event index not used already in Z?i,j, ?z0 are the global mixing proportions associated
with the K events, and ?u0 is the weight for the unknown mixture component.
Next, to generate the mention head lemma (in this model, X = ?HL?), the event z is
associated with a multinomial emission distribution over the HL feature values having
the parameters ?=??hlZ ?. We assume that this emission distribution is drawn from a
symmetric Dirichlet distribution with concentration ?HL:
P(HLi,j = hl | Z,HL?i,j) ? nhl,z + ?HL (5)
where HLi,j is the head lemma of mention j from document i, and nhl,z is the number of
times the feature value hl has been associated with the event index z in (Z,HL?i,j).
5.2 The HDPflat Model
A model in which observable components are represented only by one feature type has
the tendency to cluster these components based on their corresponding feature values.
This model may produce good results for tasks such as topic discovery where the
linguistic objects rely only on lexical information. Because event coreference involves
clustering complex objects characterized by a large number of features, it is desirable to
extend the HDP1f model with a generalized model where additional feature types can
be easily incorporated. Moreover, this extension should allow multiple feature types to
be added simultaneously.
To facilitate this extension, we assume that the feature variables are conditionally in-
dependent given Z. This assumption considerably reduces the complexity of computing
P(Z |X). For example, if we want to incorporate into the previous model the feature
type associated with the semantic frame evoked by every event mention (i.e., FR), the
formula becomes:
P(Zi,j |HL,FR) ? P(Zi,j)P(HLi,j,FRi,j |Z)
? P(Zi,j)P(HLi,j |Z)P(FRi,j |Z)
(6)
In this formula, we omit the conditioning components of Z, HL, and FR for the sake
of clarity. The graphical representation corresponding to this model is illustrated in
Figure 3(b). In general, if X consists of L feature variables, the inference formula for
the Gibbs sampler is defined as:
P(Zi,j |X) ? P(Zi,j)
?
FT?X
P(FTi,j |Z) (7)
324
Bejan and Harabagiu Unsupervised Event Coreference Resolution
The graphical model for this general setting is depicted in Figure 3(c). Drawing an
analogy, the graphical representation involving Z and feature variables resembles the
graphical representation of a naive Bayes classifier.
5.3 The HDPstruct Model
When dependencies between feature type variables exist (e.g., in our case, frame
elements are dependent on the semantic frames that define them, and frames are
dependent on the words that evoke them), various global distributions are involved
for computing P(Z |X). For the model depicted in Figure 3(d), for instance, the posterior
probability is given by:
P(Zi,j |X) ? P(Zi,j)P(FRi,j |HLi,j,?)
?
FT?X
P(FTi,j |Z) (8)
In this model, P(FRi,j |HLi,j,?) is a global distribution parameterized by ?, and FT is
a feature type variable from the set X=?HL,POS,FR?. However, one limitation of
this particular model is that it requires domain knowledge in order to establish the
dependencies between the feature type variables.
For all the HDP extended models, we computed the prior and likelihood factors
as described in the HDP1f model. In the inference mechanism, we assigned soft counts
to those likelihood factors whose corresponding feature values cannot be extracted for
a given event mention (e.g., unspecified predicate arguments). It is worth noting that
there exist event mentions for which not all the features can be extracted. For instance,
the feature types corresponding to the left and right lemmatized headwords (denoted in
Section 4 as LHE and RHE, respectively) are missing for the first and last event mentions
in a document. Also, many semantic roles can be absent for an event mention in a given
context.
6. Infinite Feature Models
One of the main limitations of the HDP extensions presented in the previous section
is that these models have limited capabilities in representing the observable objects
characterized by a large number of feature types. This is because, in order to sample
the event indices into the set of indicator random variables Z, the HDP models need to
store in memory large matrices that encode the significant statistics for the observable
components associated with each cluster. More specifically, in order to compute the
likelihood factors in Equation (5), for each feature type FTi, i = 1 . . . L, we assigned a
counting matrix having the number of rows equal with the number of distinct feature
values corresponding to FTi and K + 1 columns, where K represents the number of
inferred events. For instance, the counting matrix corresponding to the head lemma
feature type (HL) stores the number of times each feature value of the HL feature type
has been associated with each event index during the HDP generative process. The
number nhl,z in Equation (5), for example, is stored in a cell of this matrix.
Just to have an idea of how much memory the HDP models require to infer the
events from OntoNotes, we made the following calculation. In OntoNotes, we automat-
ically identified a total number of 81,938 event mentions for which we extracted 454,170
distinct feature values. For all data sets, we considered L = 132 feature types, which
means that, on average, each feature type is associated with approximately 3,440 feature
325
Computational Linguistics Volume 40, Number 2
values. Because K is bounded by the total number of event mentions considered (i.e.,
the case when each event mention is associated with a different event), the maximum
value that it can reach when inferring the event indices from OntoNotes is 81,938. If we
consider that each cell from the counting matrices associated with each feature type is
represented into the memory by one byte, the total space required to store only one such
matrix is, on average, 81,938?3,440 bytes. By a simple computation, the total amount of
memory to store all 132 matrices is? 34.6 gigabytes (GB). Furthermore, by adding more
data, the amount of memory needed by the HDP models increases considerably. For
instance, if we consider all three data sets (with a total number of 148,402 eventmentions
and 832,611 distinct feature values), the memory space required increases to 115 GB.
Because in our implementation we used the int type (4 bytes) to represent the counting
matrices, the total amount of memory required by the HDP extensions to infer the event
indices from OntoNotes and all three data sets when considering all 132 feature types is
in fact 4? 34.6 = 138.4 GB and 4? 115 = 460.3 GB, respectively.
Due to this limitation, the HDP extensions will be able to run only using a restricted,
manually selected set of feature types.7 Therefore, the existence of a novel methodology
that is able to consider a much smaller subset of representative feature values from the
entire feature space is necessary. For this purpose, we devised two novel approaches
that provide a more flexible representation of the data by modeling event mentions
with an infinite number of features and by using a mechanism to automatically select
a finite set of the most salient features for each mention in the inference process. The
first approach uses the Markov Indian buffet process (mIBP) to represent each object
as a sparse subset of a potentially unbounded set of latent features (Griffiths and
Ghahramani 2006; Ghahramani, Griffiths, and Sollich 2007; Van Gael et al. 2008), and
combines it with the HDP extension presented in the previous section. We call this
hybrid the mIBP?HDP model. The second approach uses the infinite factorial hidden
Markov model (iFHMM), which is an extension of mIBP, and combines it with the
infinite hidden Markov model (iHMM) to form the iFHMM?iHMM model.
6.1 The mIBP?HDP Model
In this section, we describe a model that is able to represent event mentions charac-
terized by an unbounded set of feature values into the HDP framework. Although the
feature space describing event mentions is unbounded, this approach is able to model
the uncertainty in the number of feature values M that will be used for clustering event
mentions and, at the same time, is able to guarantee that this number is finite at any
point in time during the generative process. First, we use mIBP to describe a mechanism
for assigning to each event mention a sparse subset of feature values from the set of M
observed feature values used in the clustering process. We will use the set of notations
introduced in this description when presenting both mIBP?HDP and iFHMM?iHMM
models. Then, we will show how this mechanism is coupled into the HDP framework.
6.1.1 The Markov Indian Buffet Process. The Markov Indian buffet process (Van Gael, Teh,
and Ghahramani 2008) defines a distribution over an unbounded set of independent
hiddenMarkov chains, where each chain is associated with a binary latent feature value
7 Because, in general, most of the counts corresponding to each feature value are assigned to a single
cluster, a partial solution for this problem would be an efficient way of managing the sparsity in
the counting matrices. However, the main issue of representing the entire set of features into the
HDP models remains unaddressed.
326
Bejan and Harabagiu Unsupervised Event Coreference Resolution
that evolves over time according to Markov dynamics. Specifically, if we denote by M
the total number ofMarkov chains associated with the latent feature values and by T the
number of observations, mIBP defines a probability distribution over a binary matrix F
with an unbounded number of rows M (M??) and T columns.
In our framework, we usemIBP to incrementally build the set ofM observed feature
values that will be used for clustering event mentions (denoted as {f 1, f 2, . . . , fM}), as
well as to determine which of these feature values will be selected to explain each event
mention. The sequence of observations is associated with the sequence of event men-
tions, y1, y2, . . . , yT, and each latent feature value in the mIBP framework is associated
with one observed feature value from the unbounded set of features that characterize
our event mentions. It is worth mentioning that, at any given time point during the
mIBP generative process, from the unbounded set of observed features, we index only
these M observed feature values that correspond to the set of hidden feature values.
The selection of the observed feature values which will represent each event men-
tion in the clustering process is determined by the indicator random variables of the
binary matrix F. For instance, the selection of the observed feature value f i for the event
mention yt is indicated by an assignment of the binary random variable Fit to 1 in the
mIBP generative process. More specifically, the set of observed feature values that will
represent the event mention yt is indicated in the matrix by the column vector of binary
random variables Ft=?F1t ,F2t , . . . ,FMt ?. Therefore, F decomposes the event mentions and
represents them as feature value factors, which can then be associated with hidden
variables in an iFHMM model as described in Van Gael, Teh, and Ghahramani (2008).
The transition probabilities of the binary Markov chain associated with a latent
feature value, Fm=?Fm1 ,Fm2 , . . . ,FmT ?, are given by the following transition matrix:
W(m) =
(
1? am am
1? bm bm
)
(9)
where W(m)ij =P(Fmt+1= j |Fmt = i), the parameters am?Beta(??/M, 1) and bm?Beta(??, ??),
and the initial state Fm0 =0. In the mIBP process, the hidden variable associated with
an observed feature value fm and an event mention yt is generated from the following
Bernoulli distribution:
Fmt ?Bernoulli(a
1?Fmt?1
m b
Fmt?1
m ) (10)
Based on these definitions, we computed the probability of the feature matrix F8 (in
which the parameters a and b are integrated out analytically) by recording the number
of 0?0, 0?1, 1?0, and 1?1 transitions for each binary chain m into the counting
variables c00m , c01m , c10m , and c11m , respectively. For example, the c11m associated with the
feature value representing the VERB class (fm = HWC:VERB) counts how many times
this feature value was assigned to the event mention yt when it was also assigned to the
previous event mention yt?1 during the generative process.
The stochastic process that derives the probability distribution in terms of these
variables is defined as follows. In the first step, the process assigns a value of 1 to a
number of Poisson(??) latent features for the first component. In our implementation,
this statement is equivalent with the process of randomly selecting for the first event
8 Technical details for computing this probability are described in Van Gael, Teh, and Ghahramani (2008).
327
Computational Linguistics Volume 40, Number 2
mention a number of Poisson(??) observed feature values. In the general case, the sam-
pling of the binary variable from the mth Markov chain and associated with the tth event
mention depends on the value assigned to the hidden variable in the previous t? 1 step:
P(Fmt = 1 |Fmt?1=1) =
c11m + ??
?? + ?? + c10m + c11m
P(Fmt = 1 |Fmt?1=0) =
c01m
c00m + c01m
(11)
As a result, in our implementation, the observed feature value fm is selected for the tth
event mention according to the probabilities presented in Equation (11). For example,
in order to select the feature value which indicates that the tth event mention has the
OCCURRENCE event class (i.e., fm = EC:OCCURRENCE), we need to determine whether
or not the event mention t? 1 from the document collection selected this feature value.
In the cases when EC:OCCURRENCE was previously selected for the event mention t? 1
(Fmt?1=1), we select this feature value according to P(Fmt = 1 |Fmt?1=1). Otherwise, the
selection is determined according to P(Fmt = 1 |Fmt?1=0). Furthermore, in the tth step of
the generative process, the same sampling mechanism is repeated until all M latent
feature values are generated. After sampling all these feature values for the tth event
mention, an additional number of Poisson(??/t) new feature values are assigned to this
mention, and M gets incremented accordingly.
As an observation regarding the mIBP generative process, it has been shown that
M grows logarithmically with the number of observed components (in our case, event
mentions) (Ghahramani, Griffiths, and Sollich 2007; Doshi-Velez 2009). This type of
growth is desirable because it provides a scalable solution for our models to work in
an efficient way on fairly large data sets.
6.1.2 Integration of mIBP into HDP. One direct application of the mIBP model is to
integrate it into the framework of the HDP extension model described in the previous
section. In this way, the new nonparametric extensionwill have the benefits of capturing
the uncertainty regarding the number of mixture components that are characterized by
a potentially infinite number of feature values. However, to make this hybrid work, we
have to devise a mechanism in which only a finite set of relevant feature values will be
selected to explain each observation (i.e., event mention) in the HDP inference process.
Our idea of selecting a finite set of representative feature values for each event
mention is based on a heuristic approach that is used after running the mIBP generative
process. Specifically, by considering the event mention yt, fm one of the feature values
that characterizes yt, qm the number of times fm was selected for all mentions during
mIBP, and vt a threshold variable for yt such that vt?Uniform(1,max{qm | Fmt =1}), we
define the finite set of feature values Bt corresponding to the observation yt as:
Bt = {fm | Fmt = 1 ? qm ? vt} (12)
A pictorial representation of this idea is illustrated in Figure 4, where only the
feature values fm with the corresponding counts qm above the threshold indicated by
vt are selected in Bt. The finiteness of this feature set is based on the observation that,
at any time point during the generative process of the mIBP model, only a finite set of
latent features have assigned a value of 1 for an event mention. Furthermore, based on
328
Bejan and Harabagiu Unsupervised Event Coreference Resolution
fMf 1 f 2 f 3 fm fm+1fm?1f 4 . . .
vt
qm
Figure 4
Graphical representation of the mechanism for filtering the feature values associated with the
event mention yt. After this mechanism is applied, yt will be represented only by the feature
values fm for which their corresponding counts qm are above the threshold variable vt.
the assumption that the more a feature value is selected during the mIBP generative
process the more relevant it is for the event coreference task, each set Bt contains the
most informative feature values that are able to explain its corresponding event mention
yt. This last property is ensured by the second constraint imposed when building each
set Bt (i.e., qm ? vt). Due to the fact that the threshold variables are sampled using a
uniform distribution, we denote this model as mIBP?HDPuniform.
The feature values selected by this mechanism are used to represent the event men-
tions in the clustering process of the HDP. The main difference from the original imple-
mentation of the HDP extensions is that, in this new model, instead of representing the
event mentions by the entire set of feature values from the initial feature space (which
can be as large as possible), only a restricted subset of these feature values is considered.
Furthermore, due to the random process of selecting the feature values, the number of
feature values associated with each event mention can vary significantly. We adapted
the implementation of the HDP framework to this modification by truncating all count-
ing matrices such that they will represent only the feature values selected in mIBP. More
specifically, we removed from each counting matrix the rows corresponding to all the
feature values that were not selected during the mIBP generative process. Because M
grows as O(logT), it now becomes feasible for the HDP extension models to represent
event mentions using the entire set of feature types. It is important to mention that this
modification does not affect the implementation of the Gibbs sampler in the HDP frame-
work because we always normalize the probabilities corresponding to the likelihood
factors in Equation (5) when computing the posterior distribution over event indices.
Moreover, using the assumption that the relevance of a feature value is proportional
with the number of times it was selected during the mIBP generative process, we
explored additional heuristics for building the sets of feature values Bt for each event
mention. In general, we chose these new heuristics to be biased towards selecting more
relevant feature values fm for each event mention yt (i.e., their counts qm to be closer to
max{qm | Fmt =1}). One such heuristic is based on the method that considers for each
event mention yt all feature values fm with the counts qm ? 1 (i.e., vt = 1). In this case,
each set Bt contains all the observed feature values selected for each event mention yt
during the mIBP process, and therefore it represents a subset of the set of observed fea-
ture values {f 1, f 2, . . . , fM}. It is worth mentioning that all the subsets of {f 1, f 2, . . . , fM}
329
Computational Linguistics Volume 40, Number 2
are finite due to the fact that M is finite at any given point in time during mIBP. In
consequence, all the Bt sets derived using this heuristic are finite. Because no feature
value is filtered out after it was assigned to an event mention during mIBP, we denote
the model implementing this heuristic as mIBP?HDPunfiltered. Starting from the distri-
bution of the counting variables qm corresponding to those feature values fm selected
during the mIBP generative process for an event mention yt, another heuristic considers
for building each set Bt only the feature values with the counts above the median of
this distribution (mIBP?HDPmedian). Finally, the last heuristic we experimented with is
based on the idea of sampling the threshold variables vt directly from the distribution
of the counting variables associated with each event mention yt (mIBP?HDPdiscrete). The
implementation of these three heuristics is possible due to the observation that in the
mIBP?HDP framework the size of each set Bt is not required to be known in advance.
6.2 The iFHMM?iHMM Model
Over the years, the hidden Markov model (HMM) (Rabiner 1989) has proven to be one
of the most commonly used statistical tools for modeling time series data. Due to the
efficiency in estimating its parameters, various HMM generalizations were proposed
for a better representation of the latent structure encoded in this type of data. Figure 5
illustrates a hierarchy of HMM extensions whose main criteria of expansion is based on
relaxing the constraints on the parameters M (the number of state chains) and K (the
number of clustering components). In the factorial hidden Markov model (FHMM),
Ghahramani and Jordan (1997) introduced the idea of factoring the hidden state space
into a finite number of state variables, in which each of these variables has its own
Markovian dynamics. Later on, Van Gael, Teh, and Ghahramani (2008) introduced the
K ? finite
M = 1
8M
8K
8M
8K
FHMM
iHMM
iFHMM
HMM
iFHMM ? iHMM
K ? finite
M ? finite
M = 1
K ? finite
(Ghahramani and Jordan, 1987)
(Van Gael et al., 2008)
(Beal et al., 2002)
(Rabiner, 1989)
Y1
F21
F11
FM1 FM2
Y2
F22
F12
FMT
YT
F2T
F1T
F20
F10
FM0
Y1
F21
F11
FM1 FM2
Y2
F22
F12
FMT
YT
F2T
F1T
FM0
F20
F10
ST
YT
YT
F1T
FMT
ST
F2T
F10
Y1
F11
Y2
F12
S0
FM0 FM1 FM2
S1 S2
F20 F21 F22
ST
YT
S0 S1 S2
Y1 Y2
S0 S1 S2
Y1 Y2
Figure 5
Extensions of the hidden Markov model.
330
Bejan and Harabagiu Unsupervised Event Coreference Resolution
infinite factorial hidden Markov model (iFHMM) with the purpose of allowing the
number of parallel Markov chains M to be learned from data. Although the iFHMM
provides a more flexible representation of the latent structure, it cannot be used as a
framework where the number of clustering components K is infinite. In this direction,
Beal, Ghahramani, and Rasmussen (2002) proposed the infinite hidden Markov model
(iHMM) in order to perform inferences with an infinite number of states K. To further
increase the representational power for modeling discrete time series data, we introduce
a novel nonparametric extension that combines the best of the iFHMM and iHMM
models (denoted as iFHMM?iHMM) and lets both parameters M and K to be learned
from data.
As shown in Figure 5, the graphical representation of this new model consists
of a sequence of hidden state variables, (s1, . . . , sT ), that corresponds to the sequence
of event mentions (y1, . . . , yT ). Each hidden state st can be assigned to one of the K
latent events, st?{1, . . . ,K}, and each mention yt is represented by a column vector of
binary random variables ?F1t ,F2t , . . . ,FMt ?. One element of the transition probability pi is
defined as piij=P(st= j |st?1= i), and a mention yt is generated according to a likelihood
model F that is parameterized by a state-dependent parameter ?st (yt |st?F (?st )). The
observation parameters ? are independent and identically distributed drawn from a
prior base distribution H.
6.2.1 Inference. The main idea of the inference mechanism corresponding to this new
model is illustrated in Figure 6. As depicted in this figure, each step in the generative
process of the new hybrid model is performed in two consecutive phases. In the first
phase, the binary random variables associated with each feature value from the iFHMM
framework are sampled using the mIBP mechanism, and consequently, the most salient
feature values are selected for each event mention (Figure 6: Phase I). Of note, the Bt
sets of feature values associated with each event mention yt are determined using the
same set of heuristics as described in Section 6.1. In the second phase, the feature values
sampled so far, which become observable during this phase, are used in an adapted
beam sampling algorithm (Van Gael et al. 2008) to infer the clustering components or
latent events (Figure 6: Phase II).
Because we utilized the same mechanism for determining the sets of relevant fea-
ture values for each event mention (as described in Section 6.1), in this section we focus
on describing our implementation of the beam sampling algorithm. The beam sampling
algorithm (Van Gael et al. 2008) combines the ideas of slice sampling (Neal 2003) and
dynamic programming for an efficient sampling of state trajectories. Because in time
series models the transition probabilities have independent priors (Beal, Ghahramani,
and Rasmussen 2002), Van Gael et al. (2008) also used the HDP mechanism to allow
couplings across transitions. For sampling the whole hidden state trajectory s, this
algorithm uses a forward filtering-backward sampling technique.
As described in Van Gael et al. (2008), in the forward step, an auxiliary variable ut
is sampled for each mention yt, ut?Uniform(0,pist?1st ). The auxiliary variables u are used
to filter only those trajectories s for which pist?1st ?ut, for all t. Also, in this step, for all t,
the probabilities P(st |y1:t,u1:t) are computed as follows:
P(st | y1:t,u1:t) ? P(yt | st)
?
st?1:ut<pist?1st
P(st?1 | y1:t?1,u1:t?1) (13)
In this formula, the dependencies involving parameters pi and ? are omitted for clarity.
331
Computational Linguistics Volume 40, Number 2
Y1 Y2 YT
F10 F11 F12 F1T
F20 F21 F22 F2T
FM0 FM1 FM2 FMT
S0 S1 S2 ST
Y1 Y2 YT
S0 S1 S2 ST
FM0 FM1 FM2 FMT
F20 F21 F22 F2T
F10 F11 F12 F1T
Phase II
Phase I
Figure 6
A step in the generative process of the iFHMM?iHMM model is performed in two phases:
(Phase I) sample the feature values for each event mention, and (Phase II) sample the
latent events.
In the backward step, first, the event for the last state sT is directly sampled from
P(sT |y1:T,u1:T ) and then, for all t : T ? 1, 1, each state st given st+1 is sampled using the
following formula:
P(st | st+1, y1:T,u1:T ) ? P(st | y1:t,u1:t)P(st+1 | st,ut+1) (14)
To sample the emission distribution ? efficiently and to ensure that each mention is
characterized by a finite set of representative features, in our implementation of the
beam sampling algorithm, we set the base distribution H to be conjugate with the
data distribution F in a Dirichlet-multinomial model with the multinomial parameters
(o1, . . . , oK ) defined as:
ok =
T
?
t=1
?
fm?Bt
nmk (15)
where nmk counts how many times the feature value fm was assigned in the generative
process to event k, and Bt stores a finite set of feature values for yt as defined in
332
Bejan and Harabagiu Unsupervised Event Coreference Resolution
Section 6.1. As can be noticed, the multinomial parameters defined here are finite due to
the fact that each set of feature values Bt is finite and the number of event mentions T is
fixed. This allows us to define a proper emission distribution for the new hybrid model.
In a similar manner to the notations of the mIBP?HDP model, we make notations of the
iFHMM?iHMM model according to the heuristic used for selecting the feature values.
7. Evaluation
In this section, we present the evaluation framework of the Bayesian models for
both within-document (WD) and cross-document (CD) coreference resolution. We start
by briefly describing the experimental set-up and coreference evaluation measures,
and then continue by showing the experimental results on the ACE, OntoNotes, and
EventCorefBank data sets. Finally, we conclude with an analysis of the most common
errors made by the Bayesian models.
7.1 The Experimental Set-up
In the data processing phase, we extracted the linguistic features described in Section 4
for each event mention annotated in the three data sets. As a result of this phase, in
the ACE corpus, we identified 6,553 event mentions grouped into 4,946 events, and in
the OntoNotes corpus, we identified 11,433 event mentions grouped into 3,393 events.
Likewise, in the new ECB corpus, we distinguished 1,744 event mentions, 1,302 within-
document events, 339 cross-document events, and 43 seminal events (or topics). Table 1
lists additional statistics extracted from these three data sets after performing this phase.
It is also worth mentioning that for processing OntoNotes we devoted additional
efforts. This is because, in spite of the fact that OntoNotes provides coreference
annotations for both entity and eventmentions, the annotations from this data set do not
specify which of the mentions refer to entities and which of them refer to events. There-
fore, in order to identify only the event mentions fromOntoNotes, we first ran our event
identifier (Bejan 2007) and then marked as event mentions only those mentions anno-
tated in this data set that overlap with the mentions extracted by the event identifier.
Table 1
Statistics of the ACE, OntoNotes, and ECB corpora.
ACE OntoNotes ECB
Number of true mentions 6,553 11,433 1,744
Number of system mentions 45,289 81,938 21,175
Number of within-document events 4,946 3,393 1,302
Number of cross-document events ? ? 339
Number of documents 745 1,540 482
Number of seminal events ? ? 43
Average number of true mentions/within-document event 1.32 3.37 1.34
Average number of true mentions/document 8.79 7.42 3.62
Average number of true mentions/seminal event ? ? 40.55
Average number of system mentions/document 60.79 53.2 43.93
Average number of within-document events/document 6.63 2.20 2.70
Average number of within-document events/seminal event ? ? 30.27
Average number of cross-document events/seminal event ? ? 7.88
Average number of documents/seminal event ? ? 11.20
Number of distinct feature values for system mentions 391,798 454,170 237,197
333
Computational Linguistics Volume 40, Number 2
Using this procedure, we marked a number of 4,940 mentions as event mentions
from the total number of 67,500 mentions annotated in OntoNotes. In a second step of
processing OntoNotes, we extended the number of event mentions to 11,433 bymarking
all the mentions that share the same cluster with at least one event mention from the set
of 4,940 previously identified event mentions. From the 6,493 event mentions marked in
this step, the majority of them correspond to nouns (4,707) and to the it pronoun (767).
Although only a small subset of eventmentions wasmanually annotatedwith event
coreference information in the three data sets (also called the set of true or gold event
mentions), during the generative process, we considered all possible event mentions
that are expressed in the data sets for every specific event. We believe this is a more
realistic approach, in spite of the fact that we evaluated only the manually annotated
events. For this purpose, we ran the event identifier described in Bejan (2007) on the
ACE, OntoNotes, and ECB corpora, and extracted 45,289, 81,938, and 21,175 event men-
tions, respectively. It is also worth mentioning that the set of event mentions obtained
from running the event identifier (also called the set of system event mentions) on
ACE and ECB includes more than 98% from the set of true event mentions. In terms of
feature space dimensionality over the two data sets, we performed experiments with a
set of 132 feature types, where each feature type consists, on average, of 6,300 distinct
feature values.
In the evaluation phase, we considered only the true mentions from the ACE test
data set and from the test sets of a five-fold cross validation scheme on the OntoNotes
and ECB data sets. For evaluating the cross-document coreference annotations from
EventCorefBank, we adopted the same approach as described in Bagga and Baldwin
(1999) by merging all the documents from the same topic into a meta-document and
then scoring this document as performed for within-document evaluation. To compute
the final results of our experiments, we averaged the results over five runs of the
generative models.
7.2 Coreference Resolution Metrics
Because there is no agreement on the best coreference resolution metric, we used four
metrics for our evaluation: the link-based MUC metric (Vilain et al. 1995), the mention-
based B3 metric (Bagga and Baldwin 1998), the entity-based CEAF metric (Luo 2005), and
the pairwise (PW) metric. These metrics report results in terms of recall (R), precision
(P), and F-score (F) by comparing the true set of coreference chains T (i.e., the manually
annotated coreference chains) against the set of chains predicted by a coreference res-
olution system S . Here, a coreference link represents a pair of coreferential mentions
whereas a coreference chain represents all the event mentions from the same cluster
with coreference links between consecutive mentions.
The MUC recall computes the number of common coreference links in T and S
divided by the number of links in T , and the MUC precision computes the number of
common links in T and S divided by the number of links in S . As was previously noted
(Luo et al. 2004; Denis and Baldridge 2008; Finkel and Manning 2008), this metric favors
the systems that group mentions into smaller number of clusters (or, in other words,
systems that predict large coreference chains) and does not take into account single
mention clusters. For instance, a system that groups all entity mentions into the same
cluster achieves a MUC score that surpasses any published results of known systems
developed for the task of entity coreference resolution.
The B3 metric was designed to overcome some of the MUC metric?s shortcomings.
This metric computes the recall and precision for each mention and then estimates the
334
Bejan and Harabagiu Unsupervised Event Coreference Resolution
overall score by averaging over all mention scores. For a given mention m, the scorer
compares the true coreference chain that contains the mentionm (Tm) against the system
chain that contains the same mention m (Sm). Thus, the recall for m is the ratio of the
number of common elements in Sm and Tm over the number of elements in Tm. Similarly,
the precision corresponding to the mention m is the ratio of the number of common
elements in Sm and Tm over the number of elements in Sm. Because this metric computes
the precision and recall for each mention, it will penalize in precision the systems that
predict a small number of clusters. Because of the same reason, this metric includes
single mention clusters in the evaluation.
The Constrained Entity-Alignment F-Measure (CEAF) scorer finds the best align-
ment between the set of true coreference chains T and the set of predicted coreference
chains S . This is equivalent to finding the best mapping in a weighted bipartite graph.
We computed the weight of a pair of coreference chains (Ti, Sj), with Ti ? T and Sj ? S ,
by using the ?4 similarity measure described in Luo et al. (2004). Therefore, the CEAF
recall and precision measures are computed as the overall similarity score of the best
alignment divided by the self-similarity score of the coreference links in T and S ,
respectively.
The last coreference metric that we considered, the PW metric, finds correspon-
dences between all mentions pairs (mi, mj) from the true and system chains with the
coreference chains linking the mentions mi and mj in the system and true chains,
respectively. As can be noticed, this metric overpenalizes those systems that predict too
many or too few clusters when compared with the number of true clusters.
7.3 Experimental Results
Tables 2, 3, 4, and 5 list the results performed by our proposed baselines (rows 1?2),
by the HDP models (rows 3?8), by the mIBP?HDP model (row 9), and by the iFHMM?
iHMM model (rows 10?13). We discuss the performance achieved by these models in
the remaining part of this section.
Table 2
Results for WD coreference resolution on the ACE data set.
Model configuration MUC B
3 CEAF PW
R P F R P F R P F R P F
1 BLeclass 94.3 33.1 49.0 97.9 25.0 39.9 14.7 64.4 24.0 93.5 8.2 15.2
2 BLsyn 71.9 29.6 41.5 89.3 36.7 52.1 25.1 64.8 36.2 63.8 10.5 18.1
3 HDP1f (HL) 62.2 43.1 50.9 86.0 70.6 77.5 62.3 76.4 68.6 50.5 27.7 35.8
4 HDPflat (LF) 52.5 51.1 51.8 82.9 82.6 82.7 74.9 75.8 75.3 42.4 41.9 42.1
5 (LF+CF) 48.9 53.1 51.0 82.0 84.9 83.4 77.8 75.3 76.6 37.9 45.1 41.2
6 (LF+CF+WF) 53.8 53.9 53.9 83.3 83.6 83.4 76.3 76.2 76.3 42.2 43.9 43.0
7 (LF+CF+WF+SF) 53.5 54.2 53.9 83.4 84.2 83.8 76.9 76.5 76.7 43.3 47.1 45.1
8 HDPstruct (HL?FR?FEA) 61.9 49.0 54.7 86.2 76.9 81.3 69.0 77.5 73.0 53.2 38.1 44.4
9 mIBP-HDPunfiltered 48.7 41.9 45.1 81.7 76.4 79.0 68.8 73.8 71.2 37.4 28.9 32.6
10 iFHMM-iHMMunfiltered 50.7 52.0 51.4 82.8 83.6 83.2 75.8 75.0 75.4 41.4 42.6 42.0
11 iFHMM-iHMMdiscrete 52.5 50.2 51.3 83.1 81.5 82.3 73.7 75.1 74.4 41.9 40.1 41.0
12 iFHMM-iHMMmedian 52.8 49.6 51.1 83.0 81.3 82.1 73.2 75.2 74.2 40.7 39.0 39.8
13 iFHMM-iHMMuniform 48.7 48.8 48.7 81.9 82.2 82.1 74.6 74.5 74.5 37.2 39.0 38.1
335
Computational Linguistics Volume 40, Number 2
Table 3
Results for WD coreference resolution on the OntoNotes data set.
Model configuration MUC B
3 CEAF PW
R P F R P F R P F R P F
1 BLeclass 77.6 68.3 72.7 71.2 50.3 58.9 38.1 56.1 45.4 57.1 42.1 47.9
2 BLsyn 54.3 61.8 57.8 52.9 63.2 57.6 50.7 39.5 44.4 30.6 42.5 34.7
3 HDP1f (HL) 78.3 72.2 75.1 77.1 54.1 63.6 40.4 50.4 44.9 67.4 44.0 52.9
4 HDPflat (LF) 70.3 77.6 73.8 81.6 58.3 67.9 40.0 42.6 41.2 81.2 41.5 54.5
5 (LF+CF) 72.1 76.4 74.2 74.8 62.7 68.2 43.4 38.3 40.7 72.8 43.7 54.5
6 (LF+CF+WF) 79.6 77.1 78.2 81.7 57.3 67.1 39.6 43.1 41.1 81.0 41.9 54.6
7 (LF+CF+WF+SF) 72.1 77.6 74.7 74.9 64.0 68.8 39.4 50.7 44.3 74.5 44.1 55.0
8 HDPstruct (HL?FR?FEA) 84.7 78.1 81.2 85.6 55.2 67.1 67.4 37.3 48.0 79.1 40.1 51.4
7.3.1 Baseline Results. A simple baseline for event coreference, which was proposed by
Ahn (2006), consists of grouping event mentions by their event classes (BLeclass). To com-
pute this baseline, we grouped mentions into clusters according to their corresponding
EC feature value. In consequence, this baseline categorizes events into a small number
of clusters, since the event identifier for extracting the EC features is trained to predict
the seven event classes annotated in TimeBank. A second baseline that we implemented
groups two event mentions if there is a (transitive) SYNONYMOUS relation between their
corresponding head lemmas (BLsyn). To implement this baseline, we used the clusters
built over the WordNet SYNONYMOUS relations as described in Section 4. Similarly to
the MUC results reported for entity coreference resolution, the baselines that group event
mentions into very few clusters are overestimated by the MUC metric (e.g., the MUC
F-scores of BLeclass in Table 5).
7.3.2 HDP Results. Due to memory limitations, we evaluated the HDP models on a
restricted set of manually selected feature types. For the HDP1f model, which plays
Table 4
Results for WD coreference resolution on the ECB data set.
Model configuration MUC B
3 CEAF PW
R P F R P F R P F R P F
1 BLeclass 92.2 39.8 55.6 97.7 55.8 71.0 44.5 80.1 57.2 93.7 25.4 39.8
2 BLsyn 75.0 34.3 47.0 91.5 57.4 70.5 45.7 75.9 57.0 65.3 21.9 32.6
3 HDP1f (HL) 46.9 54.8 50.4 84.3 89.0 86.5 83.4 79.6 81.4 36.6 53.4 42.6
4 HDPflat (LF) 34.7 85.7 49.4 81.4 98.2 89.0 92.7 77.2 84.2 24.7 82.8 37.7
5 (LF+CF) 36.1 83.4 50.1 81.5 98.0 89.0 92.8 77.9 84.7 24.6 80.7 37.4
6 (LF+CF+WF) 38.0 90.2 53.2 82.0 98.9 89.6 93.7 78.4 85.3 26.8 89.9 41.0
7 (LF+CF+WF+SF) 37.8 92.9 53.4 82.1 99.2 89.8 93.9 78.2 85.3 27.0 92.4 41.3
8 HDPstruct (HL?FR?FEA) 47.4 82.7 60.1 84.3 97.1 90.2 92.7 81.1 86.5 34.4 83.0 48.6
9 mIBP-HDPunfiltered 38.2 68.8 48.9 82.1 95.3 88.2 90.3 78.5 84.0 26.5 67.9 37.7
10 iFHMM-iHMMunfiltered 38.9 84.4 52.9 82.6 97.7 89.5 92.7 78.5 85.0 28.5 82.4 41.8
11 iFHMM-iHMMdiscrete 40.2 85.2 54.6 82.6 98.1 89.7 93.2 79.0 85.5 29.7 85.4 44.0
12 iFHMM-iHMMmedian 39.5 84.3 53.6 82.6 97.8 89.5 92.9 78.8 85.3 29.3 83.7 43.0
13 iFHMM-iHMMuniform 39.5 85.2 53.9 82.5 98.1 89.6 93.1 78.8 85.3 29.4 86.6 43.7
336
Bejan and Harabagiu Unsupervised Event Coreference Resolution
Table 5
Results for CD coreference resolution on the ECB data set.
Model configuration MUC B
3 CEAF PW
R P F R P F R P F R P F
1 BLeclass 90.5 61.1 72.9 93.8 49.6 64.9 36.6 72.7 48.7 90.7 28.6 43.3
2 BLsyn 80.9 55.1 65.5 84.6 48.1 61.3 32.8 63.6 43.3 66.2 26.0 37.3
3 HDP1f (HL) 47.7 70.5 56.8 67.0 86.2 75.3 76.2 57.1 65.2 34.9 58.9 43.5
4 HDPflat (LF) 41.1 90.5 56.5 63.8 97.3 77.0 84.9 54.3 66.1 27.2 88.5 41.5
5 (LF+CF) 43.8 90.7 59.0 64.6 97.3 77.6 85.3 55.6 67.2 27.6 88.7 42.0
6 (LF+CF+WF) 46.2 93.0 61.6 65.8 98.0 78.7 86.7 57.1 68.8 29.6 93.0 44.8
7 (LF+CF+WF+SF) 44.4 95.3 60.5 65.0 98.7 78.3 86.9 56.0 68.0 29.2 95.1 44.4
8 HDPstruct (HL?FR?FEA) 51.9 89.5 65.7 69.3 95.8 80.4 86.2 60.1 70.8 37.5 85.6 52.1
9 mIBP-HDPunfiltered 40.0 79.8 53.2 63.1 94.1 75.5 82.7 54.6 65.7 26.1 77.0 38.9
10 iFHMM-iHMMunfiltered 48.2 89.8 62.7 67.2 96.4 79.1 85.6 58.0 69.1 32.5 87.7 47.2
11 iFHMM-iHMMdiscrete 47.0 88.4 61.3 66.2 96.2 78.4 84.8 57.2 68.3 32.2 88.1 47.1
12 iFHMM-iHMMmedian 48.3 89.6 62.7 67.0 96.5 79.0 86.1 58.3 69.5 33.1 88.1 47.9
13 iFHMM-iHMMuniform 48.4 89.0 62.7 67.0 96.4 79.0 85.5 58.0 69.1 33.3 88.3 48.2
the role of baseline for the HDPflat and HDPstruct models, we considered HL as the
most representative feature type for performing the clustering of event mentions. In
this configuration, the HDP1f model outperforms the BLeclass and BLsyn baselines. For
the HDPflat models (rows 4?7 in Tables 2?5), we classified the experiments according
to the set of manually selected feature types. We found that the best configuration of
features for this model consists of a combination of feature types from all the categories
of features described in Section 4 (row 7 in Tables 2?5). For the experiments of the
HDPstruct model, we considered the set of features of the best HDPflat experiment as
well as the conditional dependencies between the HL, FR, and FEA feature types.
In general, the HDPflat model achieved the best performance results on the ACE
test data set (the results in Table 2), whereas the HDPstruct model, which also encounters
dependencies between feature types, proved to be more effective on the ECB data set
for both within- and cross-document event coreference evaluation (as shown in Tables 4
and 5). On the OntoNotes data set, as listed in Table 3, HDPflat shows better results
than HDPstruct when considering the B3 and PW metrics, whereas HDPstruct outperforms
HDPflat when considering the MUC and CEAF metrics. Moreover, the results of the
HDPflat and HDPstruct models show an F-score increase by 4?10 percentage points over
the HDP1f model, and therefore prove that the HDP extensions provide a more flexible
representation for clustering objects characterized by rich properties than the original
HDP model.
We also plot the evolution of the generative process associated with an HDP model.
For instance, Figure 7 shows that the HDPflat model corresponding to the experiment
from row 7 in Table 2 converges in 350 iteration steps to a posterior distribution over
event mentions from the ACE corpus with around 2,000 latent events.
7.3.3 mIBP?HDP Results. In spite of its advantage of working with a potentially infinite
number of features in an HDP framework, the mIBP?HDP model (row 9 in Tables 2,
4, and 5) did not achieve a satisfactory performance in comparison with the other pro-
posedmodels. However, the results were obtained by automatically selecting only 2% of
337
Computational Linguistics Volume 40, Number 2
0 50 100 150 200 250 300 350
1000
1200
1400
1600
1800
2000
2200
Number of iterations
N
um
be
r o
f c
at
eg
or
ie
s 
(K
)
0 50 100 150 200 250 300 350
?4.4
?4.2
?4
?3.8
?3.6
?3.4
?3.2
?3
?2.8
x 105
Number of iterations
Lo
g 
po
st
er
io
r p
ro
ba
bi
lity
Figure 7
Evolution of the number of categories and the log of the posterior probability for the
HDPflat model.
distinct feature values from the entire set of values extracted from both corpora. When
compared with the restricted set of features considered by the HDPflat and HDPstruct
models, the percentage of values selected by the mIBP?HDP model is only 6%.
7.3.4 iFHMM?iHMM Results. The results achieved by the iFHMM?iHMM model using
automatic selection of feature values remain competitive against the results of the
HDP models, where the feature types were manually tuned. When comparing the
strategies for filtering feature values in the iFHMM?iHMM framework, we could not
find a distinct separation between the results obtained by the iFHMM?iHMMunfiltered,
iFHMM?iHMMdiscrete, iFHMM?iHMMmedian, and iFHMM?iHMMuniform models. As ob-
served from Tables 2, 4, and 5, most of the iFHMM?iHMM results fall in between the
HDPflat andHDPstruct results. Moreover, the results listed in these tables indicate that the
iFHMM?iHMM model is a better framework than the HDP framework for capturing
the event mention dependencies simulated by the mIBP feature sampling scheme.
A study of the impact on the performance results of the parameter ?? that controls
the number of feature values selected in the iFHMM?iHMM framework is presented
in Figure 8. The results plotted in this figure show a small variation in performance for
different values of ?? indicating that the iFHMM?iHMM model is able to successfully
handle the feature values that introduce additional noise in the data. Figure 8 also
shows that the iFHMM?iHMMmodel achieves the best results on the ACE data set for a
relative small value of ?? (?? = 10), which corresponds to 0.05% feature values sampled
from the total number of feature values considered. However, because the number of
event mentions in the ECB corpus is smaller than the number of mentions in the ACE
corpus, the iFHMM?iHMM model utilizes a larger number of features values (0.91% of
feature values selected for ?? = 150) extracted from the new corpus in order to obtain
most of its best results.
The experiments depicted in Figure 8 were performed by using the unfiltered
heuristic for selecting feature values in the iFHMM?iHMM model. Similar results were
338
Bejan and Harabagiu Unsupervised Event Coreference Resolution
Figure 8
Performance results and feature reduction bars of the iFHMM?iHMM models for various ??.
339
Computational Linguistics Volume 40, Number 2
obtained when considering the rest of the heuristics integrated in the iFHMM?iHMM
framework. Also, the iFHMM?iHMM experiments using the unfiltered, discrete, median,
and uniform heuristics from Tables 2?5 were performed by setting ?? to 10, 100, 100, and
50, respectively. For the parameters ?? and ??, we considered a default value of 0.5.
To gain a deeper insight into the behavior of the iFHMM?iHMM model, we show
in Figure 9 the performance results obtained by this model for different sets of feature
types. For this purpose, we ran the iFHMM?iHMMuniform model with a fixed value of
the ?? parameter (?? = 50) on increasing fractions of feature types.9 The results confirm
the fact that the sampling scheme of the feature values used in the iFHMM?iHMM
framework does not guarantee the selection of the most salient features. However, the
constant trend in the performance values shown in Figure 9 proves that iFHMM?iHMM
is a robust generative model for handling noisy and redundant features. For instance,
noisy features for our problem can be generated from errors in semantic parsing, event
class extraction, POS tagging, and disambiguation of polysemous semantic frames.
To strengthen this statement, we also compare in Table 6 the results obtained by an
iFHMM?iHMM model that considers all the feature values associated with an observ-
able object (iFHMM?iHMMall) against the iFHMM?iHMM models that use the mIBP
sampling scheme and the unfiltered, discrete, median, and uniform heuristics. Because
of the memory limitation constraints, we performed the experiments listed in Table 6
by selecting only a subset of feature types from the ones that proved to be salient in
the HDP experiments. As listed in Table 6, all the iFHMM?iHMM models that used a
heuristic approach for selecting feature values significantly outperform the iFHMM?
iHMMall model; therefore, this proves that all the feature selection approaches consid-
ered in the iFHMM?iHMM framework are able to successfully filter out a significant
number of noisy and redundant feature values.
7.4 Error Analysis
We performed an error analysis by manually inspecting both system and gold-
annotated data in order to track the most common errors made by our models. One
frequent error occurs when a more complex form of semantic inference is needed to
find a correspondence between two event mentions of the same individuated event.
For instance, because all properties and participants of em3(acquisition) are omitted in
Example (1), and no common features exist between em2(buy) and em3(acquisition) to
indicate a similarity between these mentions, they will most probably be assigned to
different clusters. This example also suggests the need for a better modeling of the
discourse salience for event mentions.
Another common error is made when matching the semantic roles corresponding
to coreferential event mentions. Although we simulated entity coreference by using
various semantic features, the task of matching participants and properties associ-
ated with coreferential event mentions is not completely solved. This is because, in
many coreferential cases, partonomic relations between semantic roles need to be in-
ferred.10 Examples of such relations extracted from ECB are Israeli forces PART OF?????Israel,
an Indian warship PART OF?????the Indian navy, his cell PART OF?????Sicilian jail. Similarly for event
properties, many coreferential examples do not specify a clear location and time interval
9 The selection of features into the increasing fractions of feature types was randomly performed. The
fraction corresponding to the 100% experiment in Figure 9 contains all 132 feature types.
10 This observation was also reported in Hasler and Orasan (2009).
340
Bejan and Harabagiu Unsupervised Event Coreference Resolution
30
40
50
60
70
80
90
52.74
83.01
76.04
41.91
10 20 30 40 50 60 70 80 90 100
Percent of feature types
F1
?m
ea
su
re
ACE      |      WD
 
 
MUC B3 CEAF PW
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
10 20 30 40 50 60 70 80 90 100
0.18 0.19 0.20
0.21 0.21 0.20 0.22 0.22 0.21 0.21
Percent of feature types
Pe
rc
en
t o
f f
ea
tu
re
 v
al
ue
s
      ACE      |      WD
40
50
60
70
80
90
55.34
89.88
85.66
45.63
10 20 30 40 50 60 70 80 90 100
Percent of feature types
F1
?m
ea
su
re
ECB      |      WD
 
 
MUC B3 CEAF PW
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
10 20 30 40 50 60 70 80 90 100
0.23 0.25
0.30 0.31 0.32 0.32
0.34 0.32 0.33 0.32
Percent of feature types
Pe
rc
en
t o
f f
ea
tu
re
 v
al
ue
s
      ECB      |      WD
35
40
45
50
55
60
65
70
75
80
85
90
63.31
79.32
69.55
49.32
10 20 30 40 50 60 70 80 90 100
Percent of feature types
F1
?m
ea
su
re
ECB      |      CD
 
 
MUC B3 CEAF PW
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
10 20 30 40 50 60 70 80 90 100
0.23 0.25
0.30 0.31 0.32 0.32
0.34 0.32 0.33 0.32
Percent of feature types
Pe
rc
en
t o
f f
ea
tu
re
 v
al
ue
s
      ECB      |      CD
Figure 9
Performance results and feature reduction bars of the iFHMM?iHMM models for various sets
of features.
341
Computational Linguistics Volume 40, Number 2
Table 6
Feature non-sampling vs. feature sampling in iFHMM?iHMM models.
Model configuration MUC B
3 CEAF PW
R P F R P F R P F R P F
ACE (within-document event coreference)
1 iFHMM-iHMMall 72.4 30.9 43.3 89.3 39.8 55.0 30.2 68.8 42.0 62.7 9.1 15.9
2 iFHMM-iHMMunfiltered 53.5 45.8 49.4 83.3 77.7 80.4 70.6 75.9 73.2 42.1 34.6 38.0
3 iFHMM-iHMMdiscrete 54.3 50.0 52.0 83.8 80.7 82.2 73.0 75.8 74.4 43.9 39.1 41.4
4 iFHMM-iHMMmedian 53.8 48.9 51.2 83.5 80.2 81.8 72.2 75.3 73.7 42.7 38.2 40.3
5 iFHMM-iHMMuniform 51.5 47.8 49.6 82.8 80.7 81.7 72.8 75.2 73.9 41.4 39.3 40.3
ECB (within-document event coreference)
6 iFHMM-iHMMall 64.6 34.0 44.5 89.5 62.5 73.6 53.3 76.5 62.8 60.7 22.9 33.2
7 iFHMM-iHMMunfiltered 40.0 76.9 52.4 82.6 96.6 89.0 92.0 79.1 85.1 28.4 75.6 41.0
8 iFHMM-iHMMdiscrete 41.7 77.2 54.0 83.1 96.7 89.4 91.6 79.2 84.9 30.5 79.0 43.9
9 iFHMM-iHMMmedian 39.0 80.0 52.5 82.5 97.3 89.3 92.8 78.9 85.3 29.2 78.8 42.0
10 iFHMM-iHMMuniform 40.4 73.4 51.8 82.7 96.0 88.9 91.1 79.0 84.6 29.3 74.9 41.6
ECB (cross-document event coreference)
11 iFHMM-iHMMall 70.4 54.7 61.5 79.3 54.4 64.5 43.3 61.3 50.7 59.6 26.2 36.4
12 iFHMM-iHMMunfiltered 49.3 84.3 62.1 67.2 94.5 78.5 84.7 59.2 69.6 32.8 82.5 46.8
13 iFHMM-iHMMdiscrete 48.8 84.8 61.8 67.6 94.8 78.9 83.8 58.3 68.8 34.3 85.3 48.9
14 iFHMM-iHMMmedian 47.6 86.2 61.4 66.7 95.2 78.4 84.5 57.7 68.5 32.2 83.7 46.3
15 iFHMM-iHMMuniform 49.9 82.9 62.2 67.7 93.6 78.4 83.6 59.2 69.2 33.6 79.5 46.9
(e.g., Jabaliya refugee camp PART OF?????Gaza, Tuesday PART OF?????this week). In future work, we plan
to build relevant clusters using partonomies and taxonomies such as the WordNet
hierarchies built from MERONYMY/HOLONYMY and HYPERNYMY/HYPONYMY rela-
tions, respectively.11
8. Conclusion
We have described a new class of unsupervised, nonparametric Bayesian models
designed for the purpose of solving the problem of event coreference resolution. Spe-
cifically, we have shown how already existing models can be extended in order to
relax some of their limitations and how to better represent the event mentions from
a particular document collection. In this regard, we have focused on devising models
for which the number of clusters and the number of feature values corresponding to
event mentions can be automatically inferred from data.
Our experimental results for solving the problem of event coreference proved that
these models are able to successfully handle such types of requirements on a real data
application. Based on these results, we also demonstrated that the new HDP extension,
which is able to model observable objects characterized by multiple properties, is a
better fit for this type of problem than the original HDP model. Moreover, we believe
that the HDP extension can be used for solving clustering problems that involve a small
number of feature types and a priori known facts about the salience of these feature
11 This task is not trivial, because if applying the transitive closure on these relations, all words will end up
being part of the same cluster with entity for instance.
342
Bejan and Harabagiu Unsupervised Event Coreference Resolution
types. On the other hand, when no such prior information is known with respect to the
number of feature types, or the total number of features is relatively large, we believe
that the iFHMM?iHMMmodel is a more suitable choice. The main reason is because the
new hybrid model is able to perform an automatic selection of feature values. As shown
in our experiments, this model was capable of achieving competitive results even when
only 2% of feature values were selected from the entire set of features encoded in the
ACE, OntoNotes, and ECB data sets.
Acknowledgments
The authors would like to thank the
anonymous reviewers, whose insightful
comments and suggestions considerably
improved the quality of this article.
References
Ahn, David. 2006. The stages of event
extraction. In Proceedings of the Workshop
on Annotating and Reasoning about Time
and Events, pages 1?8, Sydney.
Allan, James, editor. 2002. Topic Detection
and Tracking: Event-Based Information
Organization. Kluwer Academic
Publishers.
Allan, James, Jaime Carbonell, George
Doddington, Jonathan Yamron, and
Yiming Yang. 1998. Topic detection
and tracking pilot study: Final report.
In Proceedings of the Broadcast News
Understanding and Transcription Workshop,
pages 194?218, Lansdowne, VA.
Bagga, Amit and Breck Baldwin. 1998.
Algorithms for scoring coreference chains.
In Proceedings of the 1st International
Conference on Language Resources and
Evaluation (LREC-1998), pages 563?566,
Granada.
Bagga, Amit and Breck Baldwin. 1999.
Cross-document event coreference:
Annotations, experiments, and
observations. In Proceedings of the ACL
Workshop on Coreference and its Applications,
pages 1?8, College Park, MD.
Baker, Collin F., Charles J. Fillmore, and
John B. Lowe. 1998. The Berkeley
FrameNet project. In Proceedings of the
36th Annual Meeting of the Association
for Computational Linguistics and 17th
International Conference on Computational
Linguistics (COLING-ACL), pages 86?90,
Montreal.
Beal, Matthew J., Zoubin Ghahramani,
and Carl Edward Rasmussen. 2002.
The infinite hidden Markov model.
In Advances in Neural Information
Processing Systems 14 (NIPS),
pages 577?584, Vancouver.
Bejan, Cosmin Adrian. 2007. Deriving
chronological information from texts
through a graph-based algorithm. In
Proceedings of the 20th Florida Artificial
Intelligence Research Society International
Conference (FLAIRS), Applied Natural
Language Processing Track, pages 259?260,
Key West, FL.
Bejan, Cosmin Adrian. 2008. Unsupervised
discovery of event scenarios from texts.
In Proceedings of the 21st Florida Artificial
Intelligence Research Society International
Conference (FLAIRS), Applied Natural
Language Processing Track, pages 124?129,
Coconut Grove, FL.
Bejan, Cosmin Adrian and Sanda Harabagiu.
2008a. A linguistic resource for
discovering event structures and resolving
event coreference. In Proceedings of the
Sixth International Conference on Language
Resources and Evaluation (LREC),
pages 2,881?2,887, Marrakech.
Bejan, Cosmin Adrian and Sanda Harabagiu.
2008b. Using clustering methods
for discovering event structures.
In Proceedings of the Association for the
Advancement of Artificial Intelligence
(AAAI), pages 1,776?1,777, Chicago, IL.
Bejan, Cosmin Adrian and Sanda Harabagiu.
2010. Unsupervised event coreference
resolution with rich linguistic features.
In Proceedings of the 48th Annual Meeting of
the Association for Computational Linguistics
(ACL), pages 1,412?1,422, Uppsala.
Bejan, Cosmin Adrian and Chris Hathaway.
2007. UTD-SRL: A pipeline architecture
for extracting frame semantic structures.
In Proceedings of the Fourth International
Workshop on Semantic Evaluations
(SemEval), pages 460?463, Prague.
Bejan, Cosmin Adrian, Matthew Titsworth,
Andrew Hickl, and Sanda Harabagiu.
2009. Nonparametric Bayesian models for
unsupervised event coreference resolution.
In Advances in Neural Information Processing
Systems 23 (NIPS), pages 73?81, Vancouver.
Blei, David, Andrew Ng, and Michael
Jordan. 2003. Latent Dirichlet allocation.
Journal of Machine Learning Research,
pages 993?1,022.
343
Computational Linguistics Volume 40, Number 2
Boyd-Graber, Jordan, David Blei, and
Xiaojin Zhu. 2007. A topic model for word
sense disambiguation. In Proceedings of the
2007 Joint Conference on Empirical Methods
in Natural Language Processing and
Computational Natural Language Learning
(EMNLP-CoNLL), pages 1,024?1,033,
Prague.
Bryant, Michael and Erik B. Sudderth. 2012.
Truly nonparametric online variational
inference for hierarchical Dirichlet
processes. In Advances in Neural
Information Processing Systems 25 (NIPS),
pages 2,708?2,716, Lake Tahoe, NV.
Cardie, Claire and Kiri Wagstaf. 1999.
Noun phrase coreference as clustering.
In Proceedings of the 1999 Conference on
Empirical Methods in Natural Language
Processing (EMNLP), pages 82?89,
College Park, MD.
Chen, Bin, Jian Su, and Chew Lim Tan.
2010a. A twin-candidate based approach
for event pronoun resolution using
composite kernel. In Proceedings of
the 23rd International Conference on
Computational Linguistics (COLING),
pages 188?196, Beijing.
Chen, Bin, Jian Su, and Chew Lim Tan.
2010b. Resolving event noun phrases to
their verbal mentions. In Proceedings of the
2010 Conference on Empirical Methods in
Natural Language Processing (EMNLP),
pages 872?881, Cambridge, MA.
Chen, Zheng and Heng Ji. 2009. Graph-based
event coreference resolution. In Proceedings
of the 2009 Workshop on Graph-based
Methods for Natural Language Processing
(TextGraphs-4), pages 54?57, Singapore.
Chu, Wei, Zoubin Ghahramani, Roland
Krause, and David Wild. 2006. Identifying
protein complexes in high-throughput
protein interaction screens using an
infinite latent feature model. In Pacific
Symposium on Biocomputing (PSB-11),
pages 231?242, Maui, HI.
Cowans, Philip. 2004. Information retrieval
using hierarchical Dirichlet processes.
In Proceedings of the 27th Annual
International ACM SIGIR Conference on
Research and Development in Information
Retrieval, pages 564?565, Sheffield.
Daume? III, Hal. 2007. Frustratingly easy
domain adaptation. In Proceedings of the
45th Annual Meeting of the Association
of Computational Linguistics (ACL),
pages 256?263, Prague.
Daume? III, Hal and Daniel Marcu. 2005.
A Bayesian model for supervised
clustering with the Dirichlet process
prior. Journal of Machine Learning Research
(JMLR), 6:1551?1577.
Davidson, Donald, 1969. The individuation
of events, pages 216?234. In N. Rescher
et al., editors, Essays in Honor of Carl G.
Hempel. Dordrecht: Reidel. Reprinted in
D. Davidson, editor, Essays on Actions and
Events. 2001, Oxford: Clarendon Press.
Davidson, Donald, 1985. Reply to Quine on
Events. In E. LePore and B. McLaughlin,
eds., Actions and Events: Perspectives on the
Philosophy of Donald Davidson. Blackwell,
Oxford, pages 172?176.
de Marneffe, Marie-Catherine, Anna N.
Rafferty, and Christopher D. Manning.
2008. Finding contradictions in text. In
Proceedings of the 46th Annual Meeting of the
Association for Computational Linguistics:
Human Language Technologies (ACL-HLT),
pages 1,039?1,047, Columbus, OH.
Denis, Pascal and Jason Baldridge. 2008.
Specialized models and ranking for
coreference resolution. In Proceedings of the
2008 Conference on Empirical Methods in
Natural Language Processing (EMNLP?08),
pages 660?669, Honolulu, HI.
Doshi-Velez, Finale. 2009. The Indian Buffet
Process: Scalable Inference and Extensions.
Ph.D. thesis, Department of Engineering,
University of Cambridge.
Fellbaum, Christiane. 1998. WordNet:
An Electronic Lexical Database. MIT Press,
Cambridge, MA.
Ferguson, Thomas S. 1973. A Bayesian
analysis of some nonparametric problems.
The Annals of Statistics, 1(2):209?230.
Finkel, Jenny Rose, Trond Grenager, and
Christopher Manning. 2007. The infinite
tree. In Proceedings of the 45th Annual
Meeting of the Association for Computational
Linguistics, pages 272?279, Prague.
Finkel, Jenny Rose and Christopher
Manning. 2008. Enforcing transitivity in
coreference resolution. In Proceedings of the
46th Annual Meeting of the Association for
Computational Linguistics: Human Language
Technologies (ACL-HLT), Short Papers,
pages 45?48, Columbus, OH.
Fox, E. B., E. B. Sudderth, and A. S. Willsky.
2007. Hierarchical Dirichlet processes
for tracking maneuvering targets.
In Proceedings of International Conference
on Information Fusion, pages 1,415?1,422,
Quebec.
Geman, Stuart and Donald Geman. 1984.
Stochastic relaxation, Gibbs distributions
and the Bayesian restoration of images.
IEEE Transactions on Pattern Analysis and
Machine Intelligence, 6:721?741.
344
Bejan and Harabagiu Unsupervised Event Coreference Resolution
Ghahramani, Zoubin, T. L. Griffiths,
and Peter Sollich, 2007. Bayesian
Nonparametric Latent Feature
Models. In Bayesian Statistics 8, edited
by J. M. Bernardo et al., pages 201?225.
Oxford University Press.
Ghahramani, Zoubin and Michael Jordan.
1997. Factorial hidden Markov models.
Machine Learning, 29:245?273.
Goldwater, Sharon, Thomas L. Griffiths,
and Mark Johnson. 2006. Contextual
dependencies in unsupervised word
segmentation. In Proceedings of the 21st
International Conference on Computational
Linguistics and 44th Annual Meeting of the
Association for Computational Linguistics,
pages 673?680, Sydney.
Go?ru?r, Dilan, Frank Ja?kel, and Carl Edward
Rasmussen. 2006. A choice model
with infinitely many latent features.
In Proceedings of the 23rd Annual
International Conference on Machine Learning
(ICML), pages 361?368, Pittsburgh, PA.
Griffiths, Thomas and Mark Steyvers. 2004.
Finding scientific topics. In Proceedings
of the National Academy of Sciences,
pages 5,228?5,235.
Griffiths, Tom and Zoubin Ghahramani. 2005.
Infinite latent feature models and the Indian
buffet process. Technical Report 2005-10,
Gatsby Computational Neuroscience Unit,
University College London.
Griffiths, Tom and Zoubin Ghahramani.
2006. Infinite latent feature models and the
indian buffet process. In Advances in Neural
Information Processing Systems 18 (NIPS),
pages 475?482, Vancouver.
Haghighi, Aria and Dan Klein. 2007.
Unsupervised coreference resolution
in a nonparametric Bayesian model.
In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics
(ACL), pages 848?855, Prague.
Haghighi, Aria and Dan Klein. 2009. Simple
coreference resolution with rich syntactic
and semantic features. In Proceedings of the
2009 Conference on Empirical Methods in
Natural Language Processing (EMNLP),
pages 1,152?1,161, Singapore.
Haghighi, Aria and Dan Klein. 2010.
Coreference resolution in a modular,
entity-centered model. In Human Language
Technologies: The 2010 Annual Conference
of the North American Chapter of the
Association for Computational Linguistics,
pages 385?393, Los Angeles, CA.
Haghighi, Aria, Andrew Ng, and
Christopher Manning. 2005. Robust
textual inference via graph matching.
In Proceedings of the Human Language
Technology Conference and Conference on
Empirical Methods in Natural Language
Processing (HLT-EMNLP), pages 387?394,
Vancouver.
Hasler, Laura and Constantin Orasan. 2009.
Do coreferential arguments make event
mentions coreferential? In Proceedings of
the 7th Discourse Anaphora and Anaphor
Resolution Colloquium (DAARC 2009),
pages 151?163, Goa.
He, Tian. 2007. Coreference resolution on
entities and events for hospital discharge
summaries. Master?s thesis, Department
of Electrical Engineering and Computer
Science, Massachusetts Institute of
Technology.
Humphreys, Kevin, Robert Gaizauskas, and
Saliha Azzam. 1997. Event coreference for
information extraction. In Proceedings of the
Workshop on Operational Factors in Practical,
Robust Anaphora Resolution for Unrestricted
Texts, 35th Meeting of ACL, pages 75?81,
Madrid.
Kong, Fang and Guodong Zhou. 2011.
Improve tree kernel-based event pronoun
resolution with competitive information.
In Proceedings of the Twenty-Second
International Joint Conference on Artificial
Intelligence (IJCAI), pages 1,814?1,819,
Barcelona.
LDC-ACE. 2005. ACE (Automatic Content
Extraction) English Annotation Guidelines
for Events, version 5.4.3 2005.07.01. LDC
Catalog Number: LDC2006T06.
LDC-ON. 2007. OntoNotes Release 2.0. LDC
Catalog Number: LDC2008T04.
Lee, Heeyoung, Marta Recasens, Angel
Chang, Mihai Surdeanu, and Dan Jurafsky.
2012. Joint entity and event coreference
resolution across documents. In Proceedings
of the 2012 Joint Conference on Empirical
Methods in Natural Language Processing and
Computational Natural Language Learning,
pages 489?500, Jeju Island.
Li, Fei-Fei and Pietro Perona. 2005. A
Bayesian hierarchical model for learning
natural scene categories. In Proceedings of
the 2005 IEEE Computer Society Conference
on Computer Vision and Pattern Recognition
(CVPR) - Volume 2, pages 524?531,
San Diego, CA.
Liang, Percy, Slav Petrov, Michael Jordan,
and Dan Klein. 2007. The infinite PCFG
using hierarchical Dirichlet processes.
In Empirical Methods in Natural Language
Processing and Computational Natural
Language Learning (EMNLP/CoNLL),
pages 688?697, Prague.
345
Computational Linguistics Volume 40, Number 2
Lowe, John B., Collin F. Baker, and Charles J.
Fillmore. 1997. A frame-semantic approach
to semantic annotation. In Proceedings of the
SIGLEX Workshop on Tagging Text with
Lexical Semantics: Why, What, and How?,
pages 18?24, Washington, DC.
Luo, Xiaoqiang. 2005. On coreference
resolution performance metrics.
In Proceedings of the Human Language
Technology Conference and Conference on
Empirical Methods in Natural Language
Processing (EMNLP-2005), pages 25?32,
Vancouver.
Luo, Xiaoqiang, Abe Ittycheriah, Hongyan
Jing, Nanda Kambhatla, and Salim
Roukos. 2004. A mention-synchronous
coreference resolution algorithm based
on the bell tree. In Proceedings of the
42nd Meeting of the Association for
Computational Linguistics (ACL?04),
Main Volume, pages 135?142, Barcelona.
Malpas, Jeff. 2009. Donald Davidson. In the
Stanford Encyclopedia of Philosophy (Fall
2009 Edition), edited by Edward N. Zalta.
Available at http://plato.stanford.edu/
archives/fall2009/entries/davidson/.
Meeds, Edward, Zoubin Ghahramani,
Radford Neal, and Sam Roweis. 2006.
Modeling dyadic data with binary
latent factors. In Advances in Neural
Information Processing Systems 19 (NIPS),
pages 977?984, Vancouver.
Miller, Kurt, Thomas Griffiths, and Michael
Jordan. 2008. The phylogenetic Indian
buffet process: A non-exchangeable
nonparametric prior for latent features.
In Proceedings of the Twenty-Fourth
Conference on Uncertainty in Artificial
Intelligence (UAI), pages 403?410, Helsinki.
Narayanan, Srini and Sanda Harabagiu.
2004. Question answering based on
semantic structures. In Proceedings
of the 20th International Conference on
Computational Linguistics (COLING),
pages 693?701, Geneva.
Neal, Radford M. 2003. Slice Sampling.
The Annals of Statistics, 31:705?741.
Ng, Vincent. 2008. Unsupervised models for
coreference resolution. In Proceedings of the
2008 Conference on Empirical Methods in
Natural Language Processing (EMNLP),
pages 640?649, Honolulu, HI.
Palmer, Martha, Daniel Gildea, and Paul
Kingsbury. 2005. The proposition bank:
An annotated corpus of semantic roles.
Computational Linguistics, 31(1):71?105.
Poon, Hoifung and Pedro Domingos. 2008.
Joint unsupervised coreference resolution
with Markov logic. In Proceedings of the
2008 Conference on Empirical Methods in
Natural Language Processing (EMNLP),
pages 650?659, Honolulu, HI.
Pustejovsky, James, Jose Castano, Bob Ingria,
Roser Sauri, Rob Gaizauskas, Andrea
Setzer, and Graham Katz. 2003a. TimeML:
Robust specification of event and temporal
expressions in text. In Proceedings of
the Fifth International Workshop on
Computational Semantics (IWCS),
pages 337?353, Tilburg.
Pustejovsky, James, Patrick Hanks, Roser
Sauri, Andrew See, Robert Gaizauskas,
Andrea Setzer, Dragomir Radev, Beth
Sundheim, David Day, Lisa Ferro, and
Marcia Lazo. 2003b. The TimeBank
Corpus. In Corpus Linguistics,
pages 647?656.
Quine, W. V. O., 1985. Events and Reification.
In E. LePore and B. P. McLaughlin,
editors, Actions and Events: Perspectives
on the Philosophy of Donald Davidson.
Blackwell, Oxford, pages 162?171.
Reprinted in R. Casati and A. C. Varzi,
editors, Events. 1996, Aldershot,
Dartmouth, pages 107?116.
Rabiner, Lawrence R. 1989. A tutorial on
hidden Markov models and selected
applications in speech recognition. In
Proceedings of the IEEE, pages 257?286.
Raghunathan, Karthik, Heeyoung Lee,
Sudarshan Rangarajan, Nate Chambers,
Mihai Surdeanu, Dan Jurafsky, and
Christopher Manning. 2010. A multi-pass
sieve for coreference resolution.
In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language
Processing (EMNLP), pages 492?501,
Cambridge, MA.
Rahman, Altaf and Vincent Ng. 2011.
Coreference resolution with world
knowledge. In Proceedings of the 49th
Annual Meeting of the Association for
Computational Linguistics: Human
Language Technologies, pages 814?824,
Portland, OR.
Reisinger, Joseph and Marius Pas?ca. 2009.
Latent variable models of concept-attribute
attachment. In Proceedings of the Joint
Conference of the 47th Annual Meeting of
the ACL and the 4th International Joint
Conference on Natural Language Processing
of the AFNLP, pages 620?628, Singapore.
Sivic, Josef, Bryan Russell, Alexei Efros,
Andrew Zisserman, and William Freeman.
2005. Discovering object categories in
image collections. In Proceedings of the 10th
IEEE International Conference on Computer
Vision (ICCV), pages 370?377, Beijing.
346
Bejan and Harabagiu Unsupervised Event Coreference Resolution
Sivic, Josef, Bryan Russell, Andrew
Zisserman, William Freeman, and
Alexei Efros. 2008. Unsupervised
discovery of visual object class hierarchies.
In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition,
pages 1?8, Anchorage, AK.
Stoyanov, Veselin, Nathan Gilbert,
Claire Cardie, and Ellen Riloff. 2009.
Conundrums in noun phrase coreference
resolution: Making sense of the state
of the art. In Proceedings of the
Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International
Joint Conference on Natural Language
Processing of the AFNLP, pages 656?664,
Singapore.
Sudderth, Erik B., Antonio Torralba,
William T. Freeman, and Alan S. Willsky.
2008. Describing visual scenes using
transformed objects and parts.
International Journal of Computer
Vision, 77:291?330.
Teh, Yee Whye, Michael Jordan, Matthew
Beal, and David Blei. 2006. Hierarchical
Dirichlet processes. Journal of the American
Statistical Association, 101(476):1566?1581.
Van Gael, Jurgen, Y. Saatci, Yee Whye Teh,
and Zoubin Ghahramani. 2008. Beam
sampling for the infinite hidden Markov
model. In Proceedings of the 25th Annual
International Conference on Machine Learning
(ICML), pages 1,088?1,095, Helsinki.
Van Gael, Jurgen, Yee Whye Teh, and Zoubin
Ghahramani. 2008. The infinite factorial
hidden Markov model. In Advances in
Neural Information Processing Systems 21
(NIPS), pages 1697?1704, Vancouver.
Vilain, Marc, John Burger, John Aberdeen,
Dennis Connolly, and Lynette Hirschman.
1995. A model-theoretic coreference
scoring scheme. In Proceedings of MUC-6,
pages 45?52, Columbia, MD.
Wang, Chong, John Paisley, and David Blei.
2011. Online variational inference for
the hierarchical Dirichlet process.
In Proceedings of the 14th International
Conference on Artificial Intelligence and
Statistics (AISTATS), pages 752?760, Ft.
Lauderdale, FL.
347

Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1412?1422,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Unsupervised Event Coreference Resolution with Rich Linguistic Features
Cosmin Adrian Bejan
Institute for Creative Technologies
University of Southern California
Marina del Rey, CA 90292, USA
Sanda Harabagiu
Human Language Technology Institute
University of Texas at Dallas
Richardson, TX 75083, USA
Abstract
This paper examines how a new class of
nonparametric Bayesian models can be ef-
fectively applied to an open-domain event
coreference task. Designed with the pur-
pose of clustering complex linguistic ob-
jects, these models consider a potentially
infinite number of features and categorical
outcomes. The evaluation performed for
solving both within- and cross-document
event coreference shows significant im-
provements of the models when compared
against two baselines for this task.
1 Introduction
The event coreference task consists of finding
clusters of event mentions that refer to the same
event. Although it has not been extensively stud-
ied in comparison with the related problem of en-
tity coreference resolution, solving event coref-
erence has already proved its usefulness in vari-
ous applications such as topic detection and track-
ing (Allan et al, 1998), information extraction
(Humphreys et al, 1997), question answering
(Narayanan and Harabagiu, 2004), textual entail-
ment (Haghighi et al, 2005), and contradiction de-
tection (de Marneffe et al, 2008).
Previous approaches for solving event corefer-
ence relied on supervised learning methods that
explore various linguistic properties in order to de-
cide if a pair of event mentions is coreferential
or not (Humphreys et al, 1997; Bagga and Bald-
win, 1999; Ahn, 2006; Chen and Ji, 2009). In
spite of being successful for a particular labeled
corpus, these pairwise models are dependent on
the domain or language that they are trained on.
Moreover, since event coreference resolution is a
complex task that involves exploring a rich set of
linguistic features, annotating a large corpus with
event coreference information for a new language
or domain of interest requires a substantial amount
of manual effort. Also, since these models are de-
pendent on local pairwise decisions, they are un-
able to capture a global event distribution at topic
or document collection level.
To address these limitations and to provide a
more flexible representation for modeling observ-
able data with rich properties, we present two
novel, fully generative, nonparametric Bayesian
models for unsupervised within- and cross-
document event coreference resolution. The first
model extends the hierarchical Dirichlet process
(Teh et al, 2006) to take into account additional
properties associated with observable objects (i.e.,
event mentions). The second model overcomes
some of the limitations of the first model. It
uses the infinite factorial hidden Markov model
(Van Gael et al, 2008b) coupled to the infinite
hidden Markov model (Beal et al, 2002) in or-
der to (1) consider a potentially infinite number
of features associated with observable objects, (2)
perform an automatic selection of the most salient
features, and (3) capture the structural dependen-
cies of observable objects at the discourse level.
Furthermore, both models are designed to account
for a potentially infinite number of categorical out-
comes (i.e., events). These models provide addi-
tional details and experimental results to our pre-
liminary work on unsupervised event coreference
resolution (Bejan et al, 2009).
2 Event Coreference
The problem of determining if two events are iden-
tical was originally studied in philosophy. One
relevant theory on event identity was proposed by
Davidson (1969) who argued that two events are
identical if they have the same causes and effects.
Later on, a different theory was proposed by Quine
(1985) who considered that each event refers to
a physical object (which is well defined in space
and time), and therefore, two events are identical
1412
if they have the same spatiotemporal location. In
(Davidson, 1985), Davidson abandoned his sug-
gestion to embrace the Quinean theory on event
identity (Malpas, 2009).
2.1 An Example
In accordance with the Quinean theory, we con-
sider that two event mentions are coreferential if
they have the same event properties and share the
same event participants. For instance, the sen-
tences from Example 1 encode event mentions that
refer to several individuated events. These sen-
tences are extracted from a newly annotated cor-
pus with event coreference information (see Sec-
tion 4). In this corpus, we organize documents
that describe the same seminal event into topics.
In particular, the topics shown in this example de-
scribe the seminal event of buying ATI by AMD
(topic 43) and the seminal event of buying EDS
by HP (topic 44).
Although all the event mentions of interest em-
phasized in boldface in Example 1 evoke the same
generic event buy, they refer to three individu-
ated events: e1 = {em1, em2}, e2 = {em3?6,
em8}, and e3 = {em7}. For example, em1(buy)
and em3(buy) correspond to different individuated
events since they have a different AGENT ([BU-
YER(em1)=AMD] 6= [BUYER(em3)=HP]). This
organization of event mentions leads to the idea of
creating an event hierarchy which has on the first
level, event mentions, on the second level, individ-
uated events, and on the third level, generic events.
In particular, the event hierarchy corresponding to
the event mentions annotated in our example is il-
lustrated in Figure 1.
Solving the event coreference problem poses
many interesting challenges. For instance, in or-
der to solve the coreference chain of event men-
tions that refer to the event e2, we need to take
into account the following issues: (i) a coreference
chain can encode both within- and cross-document
coreference information; (ii) two mentions from
the same chain can have different word classes
(e.g., em3(buy)?verb, em4(purchase)?noun); (iii)
not all the mentions from the same chain are syn-
onymous (e.g., em3(buy) and em8(acquire)), al-
though a semantic relation might exist between
them (e.g., in WordNet (Fellbaum, 1998), the
genus of buy is acquire); (iv) partial (or all) prop-
erties and participants of an event mention can be
omitted in text (e.g., em4(purchase)). In Section
Topic 43
Document 3
s4: AMD agreed to [buy]em1 Markham, Ontario-based
ATI for around $5.4 billion in cash and stock, the
companies announced Monday.
s5: The [acquisition]em2 would turn AMD into one of
the world?s largest providers of graphics chips.
Topic 44
Document 2
s1: Hewlett-Packard is negotiating to [buy]em3 technol-
ogy services provider Electronic Data Systems.
s8: With a market value of about $115 billion, HP
could easily use its own stock to finance the [pur-
chase]em4 .
s9: If the [deal]em5 is completed, it would be HP?s
biggest [acquisition]em6 since it [bought]em7 Com-
paq Computer Corp. for $19 billion in 2002.
Document 5
s2: Industry sources have confirmed to eWEEK that
Hewlett-Packard will [acquire]em8 Electronic Data
Systems for about $13 billion.
Example 1: Examples of event mention annotations.
buy
em7
e2 e3e1
em5 em6em3em2em1 em4 em8
Figure 1: Fragment from the event hierarchy.
5, we discuss additional aspects of the event coref-
erence problem that are not revealed in Example 1.
2.2 Linguistic Features
The events representing coreference clusters of
event mentions are characterized by a large set of
linguistic features. To compute an accurate event
distribution for event coreference resolution, we
associate the following categories of linguistic fea-
tures with each annotated event mention.
Lexical Features (LF) We capture the lexical con-
text of an event mention by extracting the follow-
ing features: the head word (HW), the lemmatized
head word (HL), the lemmatized left and right
words surrounding the mention (LHL,RHL), and
the HL features corresponding to the left and right
mentions (LHE,RHE). For instance, the lexical fea-
tures extracted for the event mention em7(bought)
from our example are HW:bought, HL:buy, LHL:it,
RHL:Compaq, LHE:acquisition, and RHE:acquire.
Class Features (CF) These features aim to group
mentions into several types of classes: the part-
of-speech of the HW feature (POS), the word class
of the HW feature (HWC), and the event class of
the mention (EC). The HWC feature can take one
of the following values: VERB, NOUN, ADJEC-
1413
TIVE, and OTHER. As values for the EC feature,
we consider the seven event classes defined in
the TimeML specification language (Pustejovsky
et al, 2003a): OCCURRENCE, PERCEPTION, RE-
PORTING, ASPECTUAL, STATE, I ACTION, and
I STATE. In order to extract the event classes cor-
responding to the event mentions from a given
dataset, we employed the event extractor described
in (Bejan, 2007). This extractor is trained on
the TimeBank corpus (Pustejovsky et al, 2003b),
which is a TimeML resource encoding temporal
elements such as events, time expressions, and
temporal relations.
WordNet Features (WF) In our efforts to create
clusters of event mention attributes as close as pos-
sible to the true attribute clusters of the individu-
ated events, we build two sets of word clusters us-
ing the entire lexical information from the Word-
Net database. After creating these sets of clusters,
we then associate each event mention with only
one cluster from each set. The first set uses the
transitive closure of the WordNet SYNONYMOUS
relation to form clusters with all the words from
WordNet (WNS). For instance, the verbs buy and
purchase correspond to the same cluster ID be-
cause there exist a chain of SYNONYMOUS rela-
tions between them in WordNet. The second set
considers as grouping criteria the categorization
of words from the WordNet lexicographer?s files
(WNL). In addition, for each word that is not cov-
ered in WordNet, we create a new cluster ID in
each set of clusters.
Semantic Features (SF) To extract features that
characterize participants and properties of event
mentions, we use the semantic parser described
in (Bejan and Hathaway, 2007). One category of
semantic features that we identify for event men-
tions is the predicate argument structures encoded
in PropBank annotations (Palmer et al, 2005).
In PropBank, the predicate argument structures
are represented by events expressed as verbs in
text and by the semantic roles, or predicate argu-
ments, associated with these events. For example,
ARG0 annotates a specific type of semantic role
which represents the AGENT, DOER, or ACTOR
of a specific event. Another argument is ARG1,
which plays the role of the PATIENT, THEME,
or EXPERIENCER of an event. In particular, the
predicate arguments associated to the event men-
tion em8(bought) from Example 1 are ARG0:[it],
ARG1:[Compaq Computer Corp.], ARG3:[for $19
billion], and ARG-TMP:[in 2002].
Event mentions are not only expressed as verbs
in text, but also as nouns and adjectives. There-
fore, for a better coverage of semantic features,
we also employ the semantic annotations encoded
in the FrameNet corpus (Baker et al, 1998).
FrameNet annotates word expressions capable of
evoking conceptual structures, or semantic frames,
which describe specific situations, objects, or
events (Fillmore, 1982). The semantic roles as-
sociated with a word in FrameNet, or frame ele-
ments, are locally defined for the semantic frame
evoked by the word. In general, the words anno-
tated in FrameNet are expressed as verbs, nouns,
and adjectives.
To preserve the consistency of semantic role
features, we align frame elements to predicate ar-
guments by running the PropBank semantic parser
on the manual annotations from FrameNet; con-
versely, we also run the FrameNet parser on the
manual annotations from PropBank. Moreover, to
obtain a better alignment of semantic roles, we
run both parsers on a large amount of unlabeled
text. The result of this process is a map with all
frame elements statistically aligned to all predi-
cate arguments. For instance, in 99.7% of the
cases the frame element BUYER of the semantic
frame COMMERCE BUY is mapped to ARG0, and
in the remaining 0.3% of the cases to ARG1. Ad-
ditionally, we use this map to create a more gen-
eral semantic feature which assigns to each predi-
cate argument a frame element label. In particular,
the features for em8(acquire) are FEA0:BUYER,
FEA1:GOODS, FEA3:MONEY, and FEATMP:TIME.
Two additional semantic features used in our ex-
periments are: (1) the semantic frame (FR) evoked
by every mention;1 and (2) the WNS feature ap-
plied to the head word of every semantic role (e.g.,
WSA0, WSA1).
Feature Combinations (FC) We also explore var-
ious combinations of the features presented above.
Examples include HW+HWC, HL+FR, FR+ARG1,
LHL+RHL, etc.
It is worth noting that there exist event mentions
for which not all the features can be extracted. For
example, the LHE and RHE features are missing
for the first and last event mentions in a document,
respectively. Also, many semantic roles can be ab-
sent for an event mention in a given context.
1 The reason for extracting this feature is given by the fact
that, in general, frames are able to capture properties of
generic events (Lowe et al, 1997).
1414
3 Nonparametric Bayesian Models
As input for our models, we consider a collection
of I documents, where each document i has Ji
event mentions. For features, we make the dis-
tinction between feature types and feature values
(e.g., POS is a feature type and has values such
as NN and VB). Each event mention is charac-
terized by L feature types, FT, and each feature
type is represented by a finite vocabulary of fea-
ture values, fv. Thus, we can represent the ob-
servable properties of an event mention as a vec-
tor of L feature type ? feature value pairs ?(FT1 :
fv1i), . . . , (FTL : fvLi)?, where each feature value
index i ranges in the feature value space associated
with a feature type.
3.1 A Finite Feature Model
We present an extension of the hierarchical Dirich-
let process (HDP)model which is able to represent
each observable object (i.e., event mention) by a
finite number of feature types L. Our HDP ex-
tension is also inspired from the Bayesian model
proposed by Haghighi and Klein (2007). How-
ever, their model is strictly customized for entity
coreference resolution, and therefore, extending it
to include additional features for each observable
object is a challenging task (Ng, 2008; Poon and
Domingos, 2008).
In the HDP model, a Dirichlet process (DP)
(Ferguson, 1973) is associated with each docu-
ment, and each mixture component (i.e., event) is
shared across documents. To describe its exten-
sion, we consider Z the set of indicator random
variables for indices of events, ?z the set of param-
eters associated with an event z, ? a notation for
all model parameters, and X a notation for all ran-
dom variables that represent observable features.2
Given a document collection annotated with event
mentions, the goal is to find the best assignment
of event indices Z?, which maximize the poste-
rior probability P (Z|X). In a Bayesian approach,
this probability is computed by integrating out all
model parameters:
P (Z|X)=
?
P (Z, ?|X)d?=
?
P (Z|X, ?)P (?|X)d?
Our HDP extension is depicted graphically in
Figure 2(a). Similar to the HDP model, the dis-
tribution over events associated with each docu-
ment, ?, is generated by a Dirichlet process with a
2 In this subsection, the feature term is used in context of a
feature type.
concentration parameter ?> 0. Since this setting
enables a clustering of event mentions at the doc-
ument level, it is desirable that events be shared
across documents and the number of events K be
inferred from data. To ensure this flexibility, a
global nonparametric DP prior with a hyperparam-
eter ? and a global base measure H can be consid-
ered for ? (Teh et al, 2006). The global distri-
bution drawn from this DP prior, denoted as ?0
in Figure 2(a), encodes the event mixing weights.
Thus, same global events are used for each docu-
ment, but each event has a document specific dis-
tribution ?i that is drawn from a DP prior centered
on the global weights ?0.
To infer the true posterior probability of
P (Z|X), we follow (Teh et al, 2006) and use
the Gibbs sampling algorithm (Geman and Ge-
man, 1984) based on the direct assignment sam-
pling scheme. In this sampling scheme, the pa-
rameters ? and ? are integrated out analytically.
Moreover, to reduce the complexity of comput-
ing P (Z|X), we make the na??ve Bayes assump-
tion that the feature variables X are conditionally
independent given Z. This allows us to factorize
the joint distribution of feature variables X condi-
tioned on Z into product of marginals. Thus, by
Bayes rule, the formula for sampling an event in-
dex for mention j from document i, Zi,j , is:3
P (Zi,j | Z?i,j,X) ? P (Zi,j | Z?i,j)
?
X?X
P (Xi,j |Z,X?i,j)
whereXi,j represents the feature value of a feature
type corresponding to the event mention j from the
document i.
In the process of generating an event mention,
an event index z is first sampled by using a mech-
anism that facilitates sampling from a prior for in-
finite mixture models called the Chinese restau-
rant franchise (CRF) representation, as reported in
(Teh et al, 2006):
P (Zi,j = z | Z?i,j, ?0) ?
{
??u0 , if z = znew
nz + ??z0 , otherwise
Here, nz is the number of event mentions with
event index z, znew is a new event index not used
already in Z?i,j , ?z0 are the global mixing propor-
tions associated with the K events, and ?u0 is the
weight for the unknown mixture component.
Next, to generate a feature value x (with the fea-
ture type X) of the event mention, the event z is
3 Z?i,j represents a notation for Z? {Zi,j}.
1415
HZi
?
??
? ?
?
Xi
(a)
?0
?
Ji I
L
?
?
HLi
FRi
POSi
?
?
H ?
F20 F
2
1 F
2
2 F
2
T
?
?
?0
?
IJi
Zi
(b)
F10
Y1
F11
Y2
F12
YT
F1T
S0
FM0 F
M
1 F
M
2 F
M
T
S1 S2 ST
Phase 1
Phase 2
(c)
Figure 2: Graphical representation of our models: nodes correspond to random variables; shaded nodes denote observable
variables; a rectangle captures the replication of the structure it contains, where the number of replications is indicated in the
bottom-right corner. The model in (a) illustrates a flat representation of a limited number of features in a generalized framework
(henceforth, HDPflat). The model in (b) captures a simple example of structured network topology of three feature variables
(henceforth, HDPstruct). The dependencies involving parameters ? and ? in these models are omitted for clarity. The model
from (c) shows the representation of the iFHMM-iHMM model as well as the main phases of its generative process.
associated with a multinomial emission distribu-
tion over the feature values of X having the pa-
rameters ?= ??xZ?. We assume that this emission
distribution is drawn from a symmetric Dirichlet
distribution with concentration ?X :
P (Xi,j = x | Z,X?i,j) ? nx,z + ?X
where Xi,j is the feature type of the mention j
from the document i, and nx,z is the number of
times the feature value x has been associated with
the event index z in (Z,X?i,j). We also apply the
Lidstone?s smoothing method to this distribution.
In cases when only a feature type is considered
(e.g., X = ?HL?), the HDPflat model is identical
with the original HDP model. We denote this one
feature model by HDP1f .
When dependencies between feature variables
exist (e.g., in our case, frame elements are de-
pendent on the semantic frames that define them,
and frames are dependent on the words that evoke
them), various global distributions are involved for
computing P (Z|X). For the model depicted in
Figure 2(b), for instance, the posterior probability
is given by:
P (Zi,j)P (FRi,j |HLi,j,?)
?
X?X
P (Xi,j |Z)
In this formula, P (FRi,j|HLi,j ,?) is a global dis-
tribution parameterized by ?, and X is a feature
variable from the set X = ?HL,POS,FR?. For
the sake of clarity, we omit the conditioning com-
ponents of Z, HL, FR, and POS.
3.2 An Infinite Feature Model
To relax some of the restrictions of the first model,
we devise an approach that combines the infinite
factorial hidden Markov model (iFHMM)with the
infinite hidden Markov model (iHMM) to form
the iFHMM-iHMM model.
The iFHMM framework uses the Markov In-
dian buffet process (mIBP) (Van Gael et al,
2008b) in order to represent each object as a sparse
subset of a potentially unbounded set of latent fea-
tures (Griffiths and Ghahramani, 2006; Ghahra-
mani et al, 2007; Van Gael et al, 2008a).4 Specif-
ically, the mIBP defines a distribution over an un-
bounded set of binary Markov chains, where each
chain can be associated with a binary latent fea-
ture that evolves over time according to Markov
dynamics. Therefore, if we denote by M the to-
tal number of feature chains and by T the num-
ber of observable components, the mIBP defines
a probability distribution over a binary matrix F
with T rows, which correspond to observations,
and an unbounded number of columns M , which
correspond to features. An observation yt con-
tains a subset from the unbounded set of features
{f1, f2, . . . , fM} that is represented in the matrix
by a binary vector Ft =?F 1t , F 2t , . . . , FMt ?, where
F it = 1 indicates that f i is associated with yt. In
other words, F decomposes the observations and
represents them as feature factors, which can then
be associated with hidden variables in an iFHMM
model as depicted in Figure 2(c).
4 In this subsection, a feature will be represented by a (fea-
ture type:feature value) pair.
1416
Although the iFHMM allows a more flexible
representation of the latent structure by letting the
number of parallel Markov chains M be learned
from data, it cannot be used as a framework where
the number of clustering components K is infi-
nite. On the other hand, the iHMM represents
a nonparametric extension of the hidden Markov
model (HMM) (Rabiner, 1989) that allows per-
forming inference on an infinite number of states
K . To further increase the representational power
for modeling discrete time series data, we propose
a nonparametric extension that combines the best
of the two models, and lets the parameters M and
K be learned from data.
As shown in Figure 2(c), each step in the new
iHMM-iFHMM generative process is performed
in two phases: (i) the latent feature variables from
the iFHMM framework are sampled using the
mIBP mechanism; and (ii) the features sampled so
far, which become observable during this second
phase, are used in an adapted version of the beam
sampling algorithm (Van Gael et al, 2008a) to in-
fer the clustering components (i.e., latent events).
In the first phase, the stochastic process for sam-
pling features in F is defined as follows. The first
component samples a number of Poisson(??) fea-
tures. In general, depending on the value that was
sampled in the previous step (t? 1), a feature fm
is sampled for the tth component according to the
P (Fmt = 1 |Fmt?1 = 1) and P (Fmt = 1 |Fmt?1 = 0)
probabilities.5 After all features are sampled for
the tth component, a number of Poisson(??/t)
new features are assigned for this component, and
M gets incremented accordingly.
To describe the adapted beam sampler, which
is employed in the second phase of the generative
process, we introduce additional notations. We de-
note by (s1, . . . , sT ) the sequence of hidden states
corresponding to the sequence of event mentions
(y1, . . . , yT ), where each state st belongs to one
of the K events, st ? {1, . . . ,K}, and each men-
tion yt is represented by a sequence of latent fea-
tures ?F 1t , F 2t , . . . , FMt ?. One element of the tran-
sition probability pi is defined as piij =P (st = j |
st?1 = i), and a mention yt is generated according
to a likelihood model F that is parameterized by a
state-dependent parameter ?st (yt | st ? F(?st)).
The observation parameters ? are drawn indepen-
dently from an identical prior base distribution H .
The beam sampling algorithm combines the
5 Technical details for computing these probabilities are de-
scribed in (Van Gael et al, 2008b).
ideas of slice sampling and dynamic program-
ming for an efficient sampling of state trajectories.
Since in time series models the transition probabil-
ities have independent priors (Beal et al, 2002),
Van Gael and colleagues (2008a) also used the
HDP mechanism to allow couplings across transi-
tions. For sampling the whole hidden state trajec-
tory s, this algorithm employs a forward filtering-
backward sampling technique.
In the forward step of our adapted beam sam-
pler, for each mention yt, we sample features us-
ing the mIBP mechanism and the auxiliary vari-
able ut ? Uniform(0, pist?1st). As explained in
(Van Gael et al, 2008a), the auxiliary variables u
are used to filter only those trajectories s for which
pist?1st ? ut for all t. Also, in this step, we com-
pute the probabilities P (st |y1:t, u1:t) for all t:
P (st|y1:t,u1:t)?P (yt|st)
?
st?1:ut<pist?1st
P (st?1|y1:t?1,u1:t?1)
Here, the dependencies involving parameters pi
and ? are omitted for clarity.
In the backward step, we first sample the
event for the last state sT directly from P (sT |
y1:T , u1:T ) and then, for all t : T?1 . . . 1, we sam-
ple each state st given st+1 by using the formula
P (st | st+1, y1:T , u1:T) ? P (st | y1:t, u1:t)P (st+1 |
st, ut+1). To sample the emission distribution
? efficiently, and to ensure that each mention is
characterized by a finite set of representative fea-
tures, we set the base distribution H to be con-
jugate with the data distribution F in a Dirichlet-
multinomial model with the multinomial parame-
ters (o1, . . . , oK) defined as:
ok =
T
?
t=1
?
fm?Bt
nmk
In this formula, nmk counts how many times the
feature fm was sampled for the event k, and Bt
stores a finite set of features for yt.
The mechanism for building a finite set of rep-
resentative features for the mention yt is based on
slice sampling (Neal, 2003). Letting qm be the
number of times the feature fm was sampled in the
mIBP, and vt an auxiliary variable for yt such that
vt ? Uniform(1,max{qm : Fmt = 1}), we define
the finite feature set Bt for the observation yt as
Bt = {fm : Fmt = 1?qm ? vt}. The finiteness of
this feature set is based on the observation that, in
the generative process of the mIBP, only a finite set
1417
of features are sampled for a component. We de-
note this model as iFHMM-iHMMuniform. Also,
it is worth mentioning that, by using this type of
sampling, only the most representative features of
yt get selected in Bt.
Furthermore, we explore the mechanism for
selecting a finite set of features associated with
an observation by: (1) considering all the ob-
servation?s features whose corresponding feature
counter qm ? 1 (unfiltered); (2) selecting only
the higher half of the feature distribution consist-
ing of the observation?s features that were sampled
at least once in the mIBP model (median); and
(3) sampling vt from a discrete distribution of the
observation?s features that were sampled at least
once in the mIBP (discrete).
4 Experiments
Datasets One dataset we employed is the au-
tomatic content extraction (ACE) (ACE-Event,
2005). However, the utilization of the ACE corpus
for the task of solving event coreference is lim-
ited because this resource provides only within-
document event coreference annotations using a
restricted set of event types such as LIFE, BUSI-
NESS, CONFLICT, and JUSTICE. Therefore, as a
second dataset, we created the EventCorefBank
(ECB) corpus6 to increase the diversity of event
types and to be able to evaluate our models for
both within- and cross-document event corefer-
ence resolution. One important step in the cre-
ation process of this corpus consists in finding sets
of related documents that describe the same semi-
nal event such that the annotation of coreferential
event mentions across documents is possible. For
this purpose, we selected from the GoogleNews
archive7 various topics whose description contains
keywords such as commercial transaction, attack,
death, sports, terrorist act, election, arrest, natu-
ral disaster, etc. The entire annotation process for
creating the ECB resource is described in (Bejan
and Harabagiu, 2008). Table 1 lists several basic
statistics extracted from these two corpora.
Evaluation For a more realistic approach, we not
only trained the models on the manually annotated
event mentions (i.e., true mentions), but also on all
the possible mentions encoded in the two datasets.
To extract all event mentions, we ran the event
identifier described in (Bejan, 2007). The men-
tions extracted by this system (i.e., system men-
6 ECB is available at http://www.hlt.utdallas.edu/?ady.
7 http://news.google.com/
ACE ECB
Number of topics ? 43
Number of documents 745 482
Number of within-topic events ? 339
Number of cross-document events ? 208
Number of within-document events 4946 1302
Number of true mentions 6553 1744
Number of system mentions 45289 21175
Number of distinct feature values 391798 237197
Table 1: Statistics of the ACE and ECB corpora.
tions) were able to cover all the true mentions from
both datasets. As shown in Table 1, we extracted
from ACE and ECB corpora 45289 and 21175 sys-
tem mentions, respectively.
We report results in terms of recall (R), preci-
sion (P), and F-score (F) by employing the men-
tion-based B3 metric (Bagga and Baldwin, 1998),
the entity-based CEAF metric (Luo, 2005), and the
pairwise F1 (PW) metric. All the results are av-
eraged over 5 runs of the generative models. In
the evaluation process, we considered only the
true mentions of the ACE test dataset, and the
event mentions of the test sets derived from a 5-
fold cross validation scheme on the ECB dataset.
For evaluating the cross-document coreference an-
notations, we adopted the same approach as de-
scribed in (Bagga and Baldwin, 1999) by merg-
ing all the documents from the same topic into a
meta-document and then scoring this document as
performed for within-document evaluation. For
both corpora, we considered a set of 132 feature
types, where each feature type consists on average
of 3900 distinct feature values.
Baselines We consider two baselines for event
coreference resolution (rows 1&2 in Tables 2&3).
One baseline groups each event mention by its
event class (BLeclass). Therefore, for this baseline,
we cluster mentions according to their correspond-
ing EC feature value. Similarly, the second base-
line uses as grouping criteria for event mentions
their corresponding WNS feature value (BLsyn).
HDP Extensions Due to memory limitations, we
evaluated the HDP models on a restricted set of
manually selected feature types. In general, the
HDP1f model with the feature type HL, which
plays the role of a baseline for the HDPflat and
HDPstruct models, outperforms both baselines on
the ACE and ECB datasets. For the HDPflat mod-
els (rows 4?7 in Tables 2&3), we classified the ex-
periments according to the set of feature types de-
scribed in Section 2. Our experiments reveal that
the best configuration of features for this model
1418
Model configuration B
3 CEAF PW B3 CEAF PW
R P F R P F R P F R P F R P F R P F
ECB | WD ECB | CD
1 BLeclass 97.7 55.8 71.0 44.5 80.1 57.2 93.7 25.4 39.8 93.8 49.6 64.9 36.6 72.7 48.7 90.7 28.6 43.3
2 BLsyn 91.5 57.4 70.5 45.7 75.9 57.0 65.3 21.9 32.6 84.6 48.1 61.3 32.8 63.6 43.3 66.2 26.0 37.3
3 HDP1f (HL) 84.3 89.0 86.5 83.4 79.6 81.4 36.6 53.4 42.6 67.0 86.2 75.3 76.2 57.1 65.2 34.9 58.9 43.5
4 HDPflat (LF) 81.4 98.2 89.0 92.7 77.2 84.2 24.7 82.8 37.7 63.8 97.3 77.0 84.9 54.3 66.1 27.2 88.5 41.5
5 (LF+CF) 81.5 98.0 89.0 92.8 77.9 84.7 24.6 80.7 37.4 64.6 97.3 77.6 85.3 55.6 67.2 27.6 88.7 42.0
6 (LF+CF+WF) 82.0 98.9 89.6 93.7 78.4 85.3 26.8 89.9 41.0 65.8 98.0 78.7 86.7 57.1 68.8 29.6 93.0 44.8
7 (LF+CF+WF+SF) 82.1 99.2 89.8 93.9 78.2 85.3 27.0 92.4 41.3 65.0 98.7 78.3 86.9 56.0 68.0 29.2 95.1 44.4
8 HDPstruct (HL?FR?FEA) 84.3 97.1 90.2 92.7 81.1 86.5 34.4 83.0 48.6 69.3 95.8 80.4 86.2 60.1 70.8 37.5 85.6 52.1
9 iFHMM-iHMMunfiltered 82.6 97.7 89.5 92.7 78.5 85.0 28.5 82.4 41.8 67.2 96.4 79.1 85.6 58.0 69.1 32.5 87.7 47.2
10 iFHMM-iHMMdiscrete 82.6 98.1 89.7 93.2 79.0 85.5 29.7 85.4 44.0 66.2 96.2 78.4 84.8 57.2 68.3 32.2 88.1 47.1
11 iFHMM-iHMMmedian 82.6 97.8 89.5 92.9 78.8 85.3 29.3 83.7 43.0 67.0 96.5 79.0 86.1 58.3 69.5 33.1 88.1 47.9
12 iFHMM-iHMMuniform 82.5 98.1 89.6 93.1 78.8 85.3 29.4 86.6 43.7 67.0 96.4 79.0 85.5 58.0 69.1 33.3 88.3 48.2
Table 2: Results for within-document (WD) and cross-document (WD) coreference resolution on the ECB dataset.
B3 CEAF PW
R P F R P F R P F
ACE | WD
1 97.9 25.0 39.9 14.7 64.4 24.0 93.5 8.2 15.2
2 89.3 36.7 52.1 25.1 64.8 36.2 63.8 10.5 18.1
3 86.0 70.6 77.5 62.3 76.4 68.6 50.5 27.7 35.8
4 82.9 82.6 82.7 74.9 75.8 75.3 42.4 41.9 42.1
5 82.0 84.9 83.4 77.8 75.3 76.6 37.9 45.1 41.2
6 83.3 83.6 83.4 76.3 76.2 76.3 42.2 43.9 43.0
7 83.4 84.2 83.8 76.9 76.5 76.7 43.3 47.1 45.1
8 86.2 76.9 81.3 69.0 77.5 73.0 53.2 38.1 44.4
9 82.8 83.6 83.2 75.8 75.0 75.4 41.4 42.6 42.0
10 83.1 81.5 82.3 73.7 75.1 74.4 41.9 40.1 41.0
11 83.0 81.3 82.1 73.2 75.2 74.2 40.7 39.0 39.8
12 81.9 82.2 82.1 74.6 74.5 74.5 37.2 39.0 38.1
Table 3: Results for WD coreference resolution on ACE.
consists of a combination of feature types from
all the categories of features (row 7). For the
HDPstruct experiments, we considered the set of
features of the best HDPflat experiment as well as
the dependencies between HL, FR, and FEA. Over-
all, we can assert that HDPflat achieved the best
performance results on the ACE test dataset (Ta-
ble 3), whereas HDPstruct proved to be more ef-
fective on the ECB dataset (Table 2). Moreover,
the results of the HDPflat and HDPstruct models
show an F-score increase by 4-10% over HDP1f ,
and therefore, the results prove that the HDP ex-
tension provides a more flexible representation for
clustering objects with rich properties.
We also plot the evolution of our generative
processes. For instance, Figure 3(a) shows that
the HDPflat model corresponding to row 7 in Ta-
ble 3 converges in 350 iteration steps to a posterior
distribution over event mentions from ACE with
around 2000 latent events. Additionally, our ex-
periments with different values of the ? parame-
ter for the Lidstone?s smoothing method indicate
that this smoothing method is useful for improv-
ing the performance of the HDP models. How-
ever, we could not find a ? value in our experi-
ments that brings a major improvement over the
non-smoothed HDPmodels. Figure3(b) shows the
performances of HDPstruct on ECBwith various ?
values.8 The HDP results from Tables 2&3 corre-
spond to a ? value of 10?4 and 10?2 for HDPflat
and HDPstruct, respectively.
iFHMM-iHMM In spite of the fact that the
iFHMM-iHMM model employs automatic feature
selection, its results remain competitive against
the results of the HDP models, where the fea-
ture types were manually tuned. When compar-
ing the strategies for filtering feature values in this
framework, we could not find a distinct separation
between the results obtained by the unfiltered,
discrete, median, and uniform models. As ob-
served from Tables 2&3, most of the iFHMM-
iHMM results fall in between the HDPflat and
HDPstruct results. The results were obtained by
automatically selecting only up to 1.5% of distinct
feature values. Figure 3(c) shows the percents of
features employed by this model for various val-
ues of the parameter ?? that controls the number
of sampled features. The best results (also listed
in Tables 2&3) were obtained for ?? = 10 (0.05%)
on ACE and ?? = 150 (0.91%) on ECB.
To show the usefulness of the sampling schemes
considered for this model, we also compare in
Table 4 the results obtained by an iFHMM-
iHMM model that considers all the feature values
associated with an observable object (iFHMM-
iHMMall) against the iFHMM-iHMMmodels that
employ the mIBP sampling scheme together with
the unfiltered, discrete, median, and uniform
filtering schemes. Because of the memory limi-
tation constraints, we performed the experiments
listed in Table 4 by selecting only a subset from
8 A configuration ? = 0 in the Lidstone?s smoothing method
is equivalent with a non-smoothed version of the model on
which it is applied.
1419
1000
1500
2000
2500
HDPflat     |      ACE      |      WD
N
um
be
r o
f c
at
eg
or
ie
s
0 50 100 150 200 250 300 350
?4.5
?4
?3.5
?3
?2.5 x 10
5
Number of iterations
Lo
g?
lik
el
ih
oo
d
(a)
30
40
50
60
70
80
90
100
90.27
86.53
48.62
0 10?7 10?6 10?4 10?3 10?2 101 102?
F1
?m
ea
su
re
HDP
struct
     |      ECB      |      WD
 
 
B3 CEAF PW
(b)
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
10 50 100 150 200 250
0.07
0.32
0.63
0.91
1.20
1.47
??
N
um
be
r o
f f
ea
tu
re
 v
al
ue
s 
(%
)
   iFHMM?iHMM      |      ECB      |      WD&CD
(c)
Figure 3: (a) Evolution of K and log-likelihood in the HDPflat model. (b) Evaluation of the Lidstone?s smoothing method in
the HDPstruct model. (c) Counts of features employed by the iFHMM-iHMM model for various ?? values.
Model B
3 CEAF PW
R P F R P F R P F
ACE | WD
all 89.3 39.8 55.0 30.2 68.8 42.0 62.7 9.1 15.9
unfiltered 83.3 77.7 80.4 70.6 75.9 73.2 42.1 34.6 38.0
discrete 83.8 80.7 82.2 73.0 75.8 74.4 43.9 39.1 41.4
median 83.5 80.2 81.8 72.2 75.3 73.7 42.7 38.2 40.3
uniform 82.8 80.7 81.7 72.8 75.2 73.9 41.4 39.3 40.3
ECB | WD
all 89.5 62.5 73.6 53.3 76.5 62.8 60.7 22.9 33.2
unfiltered 82.6 96.6 89.0 92.0 79.1 85.1 28.4 75.6 41.0
discrete 83.1 96.7 89.4 91.6 79.2 84.9 30.5 79.0 43.9
median 82.5 97.3 89.3 92.8 78.9 85.3 29.2 78.8 42.0
uniform 82.7 96.0 88.9 91.1 79.0 84.6 29.3 74.9 41.6
ECB | CD
all 79.3 54.4 64.5 43.3 61.3 50.7 59.6 26.2 36.4
unfiltered 67.2 94.5 78.5 84.7 59.2 69.6 32.8 82.5 46.8
discrete 67.6 94.8 78.9 83.8 58.3 68.8 34.3 85.3 48.9
median 66.7 95.2 78.4 84.5 57.7 68.5 32.2 83.7 46.3
uniform 67.7 93.6 78.4 83.6 59.2 69.2 33.6 79.5 46.9
Table 4: Feature non-sampling vs. feature sampling in the
iFHMM-iHMM model.
the feature types which proved to be salient in
the HDP experiments. As listed in Table 4,
all the iFHMM-iHMM models that used a fea-
ture sampling scheme significantly outperform
the iFHMM-iHMMall model; this proves that all
the sampling schemes considered in the iFHMM-
iHMM framework are able to successfully filter
out noisy and redundant feature values.
The closest comparison to prior work is the
supervised approach described in (Chen and Ji,
2009) that achieved a 92.2% B3 F-measure on the
ACE corpus. However, for this result, ground truth
event mentions as well as a manually tuned coref-
erence threshold were employed.
5 Error Analysis
One frequent error occurs when a more complex
form of semantic inference is needed to find a cor-
respondence between two event mentions of the
same individuated event. For instance, since all
properties and participants of em3(deal) are omit-
ted in our example and no common features ex-
ist between em3(buy) and em1(buy) to indicate a
similarity between these mentions, they will most
probably be assigned to different clusters. This ex-
ample also suggests the need for a better modeling
of the discourse salience for event mentions.
Another common error is made when match-
ing the semantic roles corresponding to coref-
erential event mentions. Although we simu-
lated entity coreference by using various seman-
tic features, the task of matching participants of
coreferential event mentions is not completely
solved. This is because, in many coreferen-
tial cases, partonomic relations between seman-
tic roles need to be inferred.9 Examples of
such relations extracted from ECB are Israeli
forces PART OF?????Israel, an Indian warship PART OF?????the
Indian navy, his cell PART OF?????Sicilian jail. Simi-
larly for event properties, many coreferential ex-
amples do not specify a clear location and time
interval (e.g., Jabaliya refugee camp PART OF?????Gaza,
Tuesday PART OF?????this week). In future work, we
plan to build relevant clusters using partonomies
and taxonomies such as the WordNet hierarchies
built from MERONYMY/HOLONYMY and HYPER-
NYMY/HYPONYMY relations, respectively.10
6 Conclusion
We have presented two novel, nonparametric
Bayesian models that are designed to solve com-
plex problems that require clustering objects char-
acterized by a rich set of properties. Our experi-
ments for event coreference resolution proved that
these models are able to solve real data applica-
tions in which the feature and cluster numbers are
treated as free parameters, and the selection of fea-
ture values is performed automatically.
9 This observation was also reported in (Hasler and Orasan,
2009). 10 This task is not trivial since, if applying the tran-
sitive closure on these relations, all words will end up being
part from the same cluster with entity for instance.
1420
References
ACE-Event. 2005. ACE (Automatic Content Extrac-
tion) English Annotation Guidelines for Events, ver-
sion 5.4.3 2005.07.01.
David Ahn. 2006. The stages of event extraction.
In Proceedings of the Workshop on Annotating and
Reasoning about Time and Events, pages 1?8.
James Allan, Jaime Carbonell, George Doddington,
Jonathan Yamron, and Yiming Yang. 1998. Topic
Detection and Tracking Pilot Study: Final Report.
In Proceedings of the Broadcast News Understand-
ing and Transcription Workshop, pages 194?218.
Amit Bagga and Breck Baldwin. 1998. Algorithms
for Scoring Coreference Chains. In Proceedings of
the 1st International Conference on Language Re-
sources and Evaluation (LREC-1998).
Amit Bagga and Breck Baldwin. 1999. Cross-
Document Event Coreference: Annotations, Exper-
iments, and Observations. In Proceedings of the
ACL Workshop on Coreference and its Applications,
pages 1?8.
Collin F. Baker, Charles J. Fillmore, and John B. Lowe.
1998. The Berkeley FrameNet project. In Pro-
ceedings of the 36th Annual Meeting of the Associ-
ation for Computational Linguistics and 17th Inter-
national Conference on Computational Linguistics
(COLING-ACL).
Matthew J. Beal, Zoubin Ghahramani, and Carl Ed-
ward Rasmussen. 2002. The Infinite Hidden
Markov Model. In Advances in Neural Information
Processing Systems 14 (NIPS).
Cosmin Adrian Bejan and Sanda Harabagiu. 2008.
A Linguistic Resource for Discovering Event Struc-
tures and Resolving Event Coreference. In Proceed-
ings of the Sixth International Conference on Lan-
guage Resources and Evaluation (LREC).
Cosmin Adrian Bejan and Chris Hathaway. 2007.
UTD-SRL: A Pipeline Architecture for Extracting
Frame Semantic Structures. In Proceedings of the
Fourth International Workshop on Semantic Evalu-
ations (SemEval), pages 460?463.
Cosmin Adrian Bejan, Matthew Titsworth, Andrew
Hickl, and Sanda Harabagiu. 2009. Nonparametric
Bayesian Models for Unsupervised Event Corefer-
ence Resolution. In Advances in Neural Information
Processing Systems 23 (NIPS).
Cosmin Adrian Bejan. 2007. Deriving Chronologi-
cal Information from Texts through a Graph-based
Algorithm. In Proceedings of the 20th Florida Ar-
tificial Intelligence Research Society International
Conference (FLAIRS), Applied Natural Language
Processing track.
Zheng Chen and Heng Ji. 2009. Graph-based Event
Coreference Resolution. In Proceedings of the
2009 Workshop on Graph-based Methods for Natu-
ral Language Processing (TextGraphs-4), pages 54?
57.
Donald Davidson, 1969. The Individuation of Events.
In N. Rescher et al, eds., Essays in Honor of Carl G.
Hempel, Dordrecht: Reidel. Reprinted in D. David-
son, ed., Essays on Actions and Events, 2001, Ox-
ford: Clarendon Press.
Donald Davidson, 1985. Reply to Quine on Events,
pages 172?176. In E. LePore and B. McLaughlin,
eds., Actions and Events: Perspectives on the Phi-
losophy of Donald Davidson, Oxford: Blackwell.
Marie-Catherine de Marneffe, Anna N. Rafferty, and
Christopher D. Manning. 2008. Finding Contra-
dictions in Text. In Proceedings of the 46th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies (ACL-
HLT), pages 1039?1047.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press.
Thomas S. Ferguson. 1973. A Bayesian Analysis
of Some Nonparametric Problems. The Annals of
Statistics, 1(2):209?230.
Charles J. Fillmore. 1982. Frame Semantics. In Lin-
guistics in the Morning Calm.
Stuart Geman and Donald Geman. 1984. Stochas-
tic relaxation, Gibbs distributions and the Bayesian
restoration of images. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 6:721?741.
Zoubin Ghahramani, T. L. Griffiths, and Peter Sollich,
2007. Bayesian Statistics 8, chapter Bayesian non-
parametric latent feature models, pages 201?225.
Oxford University Press.
Tom Griffiths and Zoubin Ghahramani. 2006. Infinite
Latent Feature Models and the Indian Buffet Pro-
cess. In Advances in Neural Information Processing
Systems 18 (NIPS), pages 475?482.
Aria Haghighi and Dan Klein. 2007. Unsuper-
vised Coreference Resolution in a Nonparametric
Bayesian Model. In Proceedings of the 45th An-
nual Meeting of the Association of Computational
Linguistics (ACL), pages 848?855.
Aria Haghighi, Andrew Ng, and Christopher Man-
ning. 2005. Robust Textual Inference via Graph
Matching. In Proceedings of Human Language
Technology Conference and Conference on Empiri-
cal Methods in Natural Language Processing (HLT-
EMNLP), pages 387?394.
Laura Hasler and Constantin Orasan. 2009. Do
coreferential arguments make event mentions coref-
erential? In Proceedings of the 7th Discourse
Anaphora and Anaphor Resolution Colloquium
(DAARC 2009).
1421
Kevin Humphreys, Robert Gaizauskas, and Saliha Az-
zam. 1997. Event coreference for information ex-
traction. In Proceedings of the Workshop on Opera-
tional Factors in Practical, Robust Anaphora Reso-
lution for Unrestricted Texts, 35th Meeting of ACL,
pages 75?81.
John B. Lowe, Collin F. Baker, and Charles J. Fillmore.
1997. A frame-semantic approach to semantic an-
notation. In Proceedings of the SIGLEX Workshop
on Tagging Text with Lexical Semantics: Why, What,
and How?, pages 18?24.
Xiaoqiang Luo. 2005. On coreference resolution per-
formance metrics. In Proceedings of the Human
Language Technology Conference and Conference
on Empirical Methods in Natural Language Pro-
cessing (EMNLP-2005), pages 25?32.
Jeff Malpas. 2009. Donald Davidson. In The
Stanford Encyclopedia of Philosophy (Fall 2009
Edition), Edward N. Zalta (ed.), http://plato.stan
ford.edu/archives/fall2009/entries/davidson/.
Srini Narayanan and Sanda Harabagiu. 2004. Ques-
tion Answering Based on Semantic Structures. In
Proceedings of the 20th International Conference on
Computational Linguistics (COLING), pages 693?
701.
Radford M. Neal. 2003. Slice Sampling. The Annals
of Statistics, 31:705?741.
Vincent Ng. 2008. Unsupervised Models for Corefer-
ence Resolution. In Proceedings of the 2008 Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP), pages 640?649.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The Proposition Bank: An Annotated Cor-
pus of Semantic Roles. Computational Linguistics,
31(1):71?105.
Hoifung Poon and Pedro Domingos. 2008. Joint
Unsupervised Coreference Resolution with Markov
Logic. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Processing
(EMNLP), pages 650?659.
James Pustejovsky, Jose Castano, Bob Ingria, Roser
Sauri, Rob Gaizauskas, Andrea Setzer, and Gra-
ham Katz. 2003a. TimeML: Robust Specification
of Event and Temporal Expressions in Text. In
Proceedings of the Fifth International Workshop on
Computational Semantics (IWCS).
James Pustejovsky, Patrick Hanks, Roser Sauri, An-
drew See, Robert Gaizauskas, Andrea Setzer,
Dragomir Radev, Beth Sundheim, David Day, Lisa
Ferro, and Marcia Lazo. 2003b. The TimeBank
Corpus. In Corpus Linguistics, pages 647?656.
W. V. O. Quine, 1985. Events and Reification, pages
162?171. In E. LePore and B. P. McLaughlin, eds.,
Actions and Events: Perspectives on the philosophy
of Donald Davidson, Oxford: Blackwell. Reprinted
in R. Casati and A. C. Varzi, eds., Events, 1996,
pages 107?116, Aldershot: Dartmouth.
Lawrence R. Rabiner. 1989. A Tutorial on Hid-
den Markov Models and Selected Applications in
Speech Recognition. In Proceedings of the IEEE,
pages 257?286.
Yee Whye Teh, Michael Jordan, Matthew Beal, and
David Blei. 2006. Hierarchical Dirichlet Pro-
cesses. Journal of the American Statistical Associa-
tion, 101(476):1566?1581.
Jurgen Van Gael, Y. Saatci, Yee Whye Teh, and Zoubin
Ghahramani. 2008a. Beam Sampling for the Infi-
nite Hidden Markov Model. In Proceedings of the
25th Annual International Conference on Machine
Learning (ICML), pages 1088?1095.
Jurgen Van Gael, Yee Whye Teh, and Zoubin Ghahra-
mani. 2008b. The Infinite Factorial Hidden Markov
Model. In Advances in Neural Information Process-
ing Systems 21 (NIPS).
1422
Proceedings of the 2013 Workshop on Biomedical Natural Language Processing (BioNLP 2013), pages 10?17,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Identification of Patients with Acute Lung Injury from  
Free-Text Chest X-Ray Reports 
 
 
Meliha Yetisgen-Yildiz 
University of Washington 
Seattle, WA 98195 
melihay@uw.edu 
 
Cosmin Adrian Bejan 
University of Washington 
Seattle, WA 98195 
bejan@uw.edu 
 
Mark M. Wurfel 
University of Washington 
Seattle, WA 98195 
mwurfel@uw.edu 
 
  
 
Abstract 
Identification of complex clinical phenotypes 
among critically ill patients is a major chal-
lenge in clinical research. The overall research 
goal of our work is to develop automated ap-
proaches that accurately identify critical illness 
phenotypes to prevent the resource intensive 
manual abstraction approach. In this paper, we 
describe a text processing method that uses 
Natural Language Processing (NLP) and su-
pervised text classification methods to identify 
patients who are positive for Acute Lung Inju-
ry (ALI) based on the information available in 
free-text chest x-ray reports. To increase the 
classification performance we enhanced the 
baseline unigram representation with bigram 
and trigram features, enriched the n-gram fea-
tures with assertion analysis, and applied sta-
tistical feature selection. We used 10-fold 
cross validation for evaluation and our best 
performing classifier achieved 81.70% preci-
sion (positive predictive value), 75.59% recall 
(sensitivity), 78.53% f-score, 74.61% negative 
predictive value, 76.80% specificity in identi-
fying patients with ALI. 
1 Introduction 
Acute lung injury (ALI) is a critical illness con-
sisting of acute hypoxemic respiratory failure 
with bilateral pulmonary infiltrates that is associ-
ated with pulmonary and non-pulmonary risk 
factors. ALI and its more severe form, acute res-
piratory distress syndrome (ARDS), represent a 
major health problem with an estimated preva-
lence of 7% of intensive care unit admissions 
(Rubenfeld et al, 2005) for which the appropri-
ate treatment is often instituted too late or not at 
all (Ferguson et al, 2005; Rubenfeld et al, 
2004). Early detection of ALI syndrome is essen-
tial for appropriate application of the only thera-
peutic intervention demonstrated to improve 
mortality in ALI, lung protective ventilation 
(LPV).   
The identification of ALI requires recognition 
of a precipitating cause, either due to direct lung 
injury from trauma or pneumonia or secondary to 
another insult such as sepsis, transfusion, or pan-
creatitis. The consensus criteria for ALI include 
the presence of bilateral pulmonary infiltrates on 
chest radiograph, representing non-cardiac pul-
monary edema as evidenced by the absence of 
left atrial hypertension (Pulmonary Capillary 
Wedge Pressure < 18 mmHg (2.4 kPa)) or ab-
sence of clinical evidence of congestive heart 
failure, and oxygenation impairment as defined 
by an arterial vs. inspired oxygen level ratio 
(PaO2/FiO2) <300 mmHg (40 kPa))  (Argitas et 
al., 1998; Dushianthan et al, 2011; Ranieri et al, 
2012).  
In this paper, we describe a text processing 
approach to identify patients who are positive for 
ALI based only on the free-text chest x-ray re-
ports. 
2 Related Work 
Several studies demonstrated the value of Natu-
ral Language Processing (NLP) in a variety of 
health care applications including phenotype ex-
traction from electronic medical records (EMR) 
(Demner-Dushman et al, 2009). Within this do-
main, chest x-ray reports have been widely stud-
ied to extract different types of pneumonia (Tep-
per et al, 2013; Elkin et al, 2008; Aronsky et al, 
2001; Fiszman et al, 2000). Chest x-ray reports 
have also been studied for ALI surveillance by 
other researchers. Two of the prior studies relied 
on rule-based keyword search approaches. He-
rasevich et al (2009) included a free text Boole-
an query containing trigger words bilateral, infil-
trate, and edema. Azzam et al (2009) used a 
more extensive list of trigger words and phrases 
to identify the presence of bilateral infiltrates and 
10
ALI. In another study, Solti et al (2009) repre-
sented the content of chest x-ray reports using 
character n-grams and applied supervised classi-
fication to identify chest x-ray reports consistent 
with ALI. In our work, different from prior re-
search, we proposed a fully statistical approach 
where (1) the content of chest x-ray reports was 
represented by token n-grams, (2) statistical fea-
ture selection was applied to select the most in-
formative features, and (3) assertion analysis was 
used to enrich the n-gram features. We also im-
plemented Azzam et al?s approach based on the 
information available in their paper and used it as 
a baseline to compare performance results of our 
approach to theirs. 
3 Methods  
The overall architecture of our text processing 
approach for ALI identification is illustrated in 
Figure 1. In the following sections, we will de-
scribe the main steps of the text processing ap-
proach as well as the annotated chest x-ray cor-
pus used in training and test. 
3.1 Chest X-ray Corpora 
To develop the ALI extractor, we created a cor-
pus composed of 1748 chest x-ray reports gener-
ated for 629 patients (avg number of re-
ports=2.78, min=1, max=3). Subjects for this 
corpus were derived from a cohort of intensive 
care unit (ICU) patients at Harborview Medical 
Center that has been described previously (Gla-
van et al, 2011). We selected 629 subjects who 
met the oxygenation criteria for ALI 
(PaO2/FiO2<300 mmHg) and then three con-
secutive chest radiographs were pulled from the 
radiology database. Three Critical Care Medicine 
specialists reviewed the chest radiograph images 
for each patient and annotated the radiographs as 
consistent (positive) or not-consistent (negative) 
with ALI. We assigned ALI status for each sub-
ject based on the number of physician raters call-
ing the chest radiographs consistent or not con-
sistent with ALI. Table 1 shows the number of 
physicians with agreement on the radiograph in-
terpretation. There were 254 patients in the posi-
tive set (2 or more physicians agreeing on ALI 
positive) and 375 patients in the negative set (2 
or more physicians agreeing on ALI negative). 
Table 1 includes the distribution of patients over 
the positive and negative classes at different 
agreement levels. We will refer to this annotated 
corpus as the development set in the remaining 
of the paper.  
For validation, we used a second dataset gen-
erated in a similar fashion to the development 
set.  We obtained chest radiographs for 55 sub-
jects that were admitted to ICU and who met ox-
ygenation criteria for ALI (1 radiograph and re-
port per patient). A specialized chest radiologist 
annotated each report for the presence of ALI. 
There were 21 patients in the positive set and 34 
in the negative set. We will refer to this corpus as 
the validation set in the remaining of the paper. 
The retrospective review of the reports in both 
corpora was approved by the University of 
Washington Human Subjects Committee of Insti-
tutional Review Board who waived the need for 
informed consent. 
3.2 Pre-processing ? Section and Sentence 
Segmentation 
Although radiology reports are in free text for-
mat, they are somewhat structured in terms of 
sections. We used a statistical section segmenta-
tion approach we previously built to identify the 
boundaries of the sections and their types in our 
corpus of chest x-ray reports (Tepper et al, 
2012). The section segmenter was trained and 
tested with a corpus of 100 annotated radiology 
reports and produced 93% precision, 91% recall 
and 92% f-score (5-fold cross validation).  
R diology
Reports
Data Processor
Sections,
Sentences
Ranked
n-grams
ALI Learner ALI Predictor
Training
instances
Test
instances
Yes No
Assertion Cl ssifier
Feature Extractor
As ertion
cl sses
Top n-grams
 
Figure 1 Overall system architecture of ALI ex-
tractor. 
Annotation Agreement Patient Count 
ALI positive 
patients 
3 147 
2 107 
ALI negative 
patients 
3 205 
2 170 
Table 1 Agreement levels 
11
After identifying the report sections, we used the 
OpenNLP 1  sentence chunker to identify the 
boundaries of sentences in the section bodies. 
This pre-processing step identified 8,659 sec-
tions and 15,890 sentences in 1,748 reports of the 
development set and 206 sections and 414 sen-
tences in 55 reports of the validation set. We 
used the section information to filter out the sec-
tions with clinician signatures (e.g., Interpreted 
By, Contributing Physicians, Signed By). We 
used the sentences to extract the assertion values 
associated with n-gram features as will be ex-
plained in a later section. 
3.3 Feature Selection 
Representing the information available in the 
free-text chest x-ray reports as features is critical 
in identifying patients with ALI. In our represen-
tation, we created one feature vector for each 
patient. We used unigrams as the baseline repre-
sentation. In addition, we used bigrams and tri-
grams as features. We observed that the chest x-
ray reports in our corpus are short and not rich in 
terms of medical vocabulary usage. Based on this 
observation, we decided not to include any medi-
cal knowledge-based features such as UMLS 
concepts or semantic types. Table 2 summarizes 
the number of distinct features for each feature 
type used to represent the 1,748 radiology reports 
for 629 patients. 
As can be seen from the table, for bigrams and 
trigrams, the feature set sizes is quite high. Fea-
ture selection algorithms have been successfully 
applied in text classification in order to improve 
the classification accuracy (Wenqian et al, 
2007). In previous work, we applied statistical 
feature selection to the problem of pneumonia 
detection from ICU reports (Bejan et al, 2012). 
By significantly reducing the dimensionality of 
the feature space, they improved the efficiency of 
the pneumonia classifiers and provided a better 
understanding of the data. 
We used statistical hypothesis testing to de-
termine whether there is an association between 
a given feature and the two categories of our 
problem (i.e, positive and negative ALI). Specif-
ically, we computed the ?2 statistics (Manning 
                                                 
1 OpenNLP. Available at: http://opennlp.apache.org/ 
and Schutze, 1999) which generated an ordering 
of features in the training set. We used 10-fold 
cross validation (development set) in our overall 
performance evaluation. Table 3 lists the top 15 
unigrams, bigrams, and trigrams ranked by ?2 
statistics in one of ten training sets we used in 
evaluation. As can be observed from the table, 
many of the features are closely linked to ALI. 
Once the features were ranked and their corre-
sponding threshold values (N) were established, 
we built a feature vector for each patient. Specif-
ically, given the subset of N relevant features 
extracted from the ranked list of features, we 
considered in the representation of a given pa-
tient?s feature vector only the features from the 
subset of relevant features that were also found 
in the chest x-ray reports of the patient. There-
fore, the size of the feature space is equal to the 
size of relevant features subset (N) whereas the 
length of each feature vector will be at most this 
value.  
3.4 Assertion Analysis 
We extended our n-gram representation with as-
sertion analysis. We built an assertion classifier 
(Bejan et al, 2013) based on the annotated cor-
pus of 2010 Integrating Biology and the Beside 
(i2b2) / Veteran?s Affairs (VA) NLP challenge 
(Uzuner et al, 2011). The 2010 i2b2/VA chal-
lenge introduced assertion classification as a 
Unigram Bigram Trigram 
Diffuse diffuse lung opacities con-
sistent with 
Atelectasis lung opacities diffuse lung opaci-
ties 
Pulmonary pulmonary edema change in diffuse 
Consistent consistent with lung opacities 
consistent 
Edema opacities consistent in diffuse lung 
Alveolar in diffuse with pulmonary 
edema 
Opacities diffuse bilateral consistent with 
pulmonary 
Damage with pulmonary low lung volumes 
Worsening alveolar damage or alveolar damage 
Disease edema or pulmonary edema 
pneumonia 
Bilateral low lung diffuse lung dis-
ease 
Clear edema pneumonia edema pneumonia 
no 
Severe or alveolar diffuse bilateral 
opacities 
Injury lung disease lungs are clear 
Bibasilar pulmonary opacities lung volumes with 
Table 3 Top 15 most informative unigrams, bigrams, 
and trigrams for ALI classification according to ?2 
statistics. 
Feature Type # of Distinct Features 
Unigram (baseline) 1,926 
Bigram 10,190 
Trigram 17,798 
Table 2 Feature set sizes of the development set. 
12
shared task, formulated such that each medical 
concept mentioned in a clinical report (e.g., 
asthma) is associated with a specific assertion 
category (present, absent, conditional, hypothet-
ical, possible, and not associated with the pa-
tient). We defined a set of novel features that 
uses the syntactic information encoded in de-
pendency trees in relation to special cue words 
for these categories. We also defined features to 
capture the semantics of the assertion keywords 
found in the corpus and trained an SVM multi-
class classifier with default parameter settings. 
Our assertion classifier outperformed the state-
of-the-art results and achieved 79.96% macro-
averaged F-measure and 94.23% micro-averaged 
F-measure on the i2b2/VA challenge test data.  
For each n-gram feature (e.g., pneumonia), we 
used the assertion classifier to determine whether 
it is present or absent based on contextual infor-
mation available in the sentence the feature ap-
peared in (e.g., Feature: pneumonia, Sentence: 
There is no evidence of pneumonia, congestive 
heart failure, or other acute process., Assertion: 
absent). We added the identified assertion value 
to the feature (e.g., pneumonia_absent). The fre-
quencies of each assertion type in our corpus are 
presented in Table 4. Because chest x-rays do not 
include family history, there were no instances of 
not associated with the patient. We treated the 
three assertion categories that express hedging 
(conditional, hypothetical, possible) as the pre-
sent category. 
3.5 Classification  
For our task of classifying ALI patients, we 
picked the Maximum Entropy (MaxEnt) algo-
rithm due to its good performance in text classi-
fication tasks (Berger et al, 1996). In our exper-
iments, we used the MaxEnt implementation in a 
machine learning package called Mallet2.  
4 Results 
4.1 Metrics  
We evaluated the performance by using precision 
(positive predictive value), recall (sensitivity), 
negative predictive value, specificity, f-score, 
and accuracy. We used 10-fold cross validation 
to measure the performance of our classifiers on 
the development set. We evaluated the best per-
forming classifier on the validation set.  
4.2 Experiments with Development Set  
We designed three groups of experiments to ex-
plore the effects of (1) different n-gram features, 
(2) feature selection, (3) assertion analysis of 
features on the classification of ALI patients. We 
defined two baselines to compare the perfor-
mance of our approaches. In the first baseline, 
we implemented the Azzam et. al.?s rule-based 
approach (2009). In the second baseline, we only 
represented the content of chest x-ray reports 
with unigrams. 
4.3 N-gram Experiments  
Table 5 summarizes the performance of n-gram 
features. When compared to the baseline uni-
gram representation, gradually adding bigrams 
(uni+bigram) and trigrams (uni+bi+trigram) to 
the baseline increased the precision and specifici-
ty by 4%. Recall and NPV remained the same. 
Azzam et. al.?s rule-based baseline generated 
higher recall but lower precision when compared 
to n-gram features. The best f-score (64.45%) 
was achieved with the uni+bi+trigram represen-
tation. 
4.4 Feature Selection Experiments  
To understand the effect of large feature space on 
classification performance, we studied how the 
performance of our system evolves for various 
threshold values (N) on the different combina-
tions of ?2 ranked unigram, bigram, and trigram 
features. Table 6 includes a subset of the results 
we collected for different values of N. As listed 
                                                 
2 Mallet. Available at: http://mallet.cs.umass.edu 
Assertion Class Frequency  
Present 206,863 
Absent 13,961 
Conditional 4 
Hypothetical 330 
Possible 3,980 
Table 4 Assertion class frequencies. 
System configuration TP TN FP FN 
Precision/ 
PPV 
Recall/ 
Sensitivity 
NPV Specificity F-Score Accuracy 
Baseline#1?Azzam et. al. (2009) 201 184 191 53 51.27 79.13 77.64 49.07 62.23 61.21 
Baseline#2?unigram 156 288 87 98 64.20 61.42 74.61 76.80 62.78 70.59 
Uni+bigram 156 296 79 98 66.38 61.42 75.13 78.93 63.80 71.86 
Uni+bi+trigram 155 303 72 99 68.28 61.02 75.37 80.80 64.45 72.81 
Table 5 Performance evaluation on development set with no feature selection. TP: True positive, TN: True nega-
tive, FP: False positive, FN: False negative, PPV: Positive predictive value, NPV: Negative predictive value. The 
row with the heighted F-Score is highlighted. 
 
 
13
in this table, for N=100, the unigram represen-
tation performed better than uni+bigram, 
uni+bi+trigram feature combinations; however, 
as N increased, the performance of 
uni+bi+trigram performed better, reaching the 
best f-score (78.53%) at N=800. When compared 
to the two defined baselines, the performance 
results of uni+bi+trigram at N=800 were signifi-
cantly better than those of the baselines.  
4.5 Assertion Analysis Experiments  
We ran a series of experiments to understand the 
effect of assertion analysis on the classification 
performance. We used the best performing clas-
N Feature configuration TP TN FP FN 
Precision/ 
PPV 
Recall/  
Sensitivity 
NPV Specificity F-Score Accuracy 
100 
Unigram 191 316 59 63 76.40 75.20 83.38 84.27 75.79 80.60 
Uni+bigram 180 313 62 74 74.38 70.87 80.88 83.47 72.58 78.38 
Uni+bi+trigram 183 317 58 71 75.93 72.05 81.70 84.53 73.94 79.49 
200 
Unigram 189 312 63 65 75.00 74.41 82.76 83.20 74.70 79.65 
Uni+bigram 183 321 54 71 77.22 72.05 81.89 85.60 74.54 80.13 
Uni+bi+trigram 190 322 53 64 78.19 74.80 83.42 85.87 76.46 81.40 
300 
Unigram 185 311 64 69 74.30 72.83 81.84 82.93 73.56 78.86 
Uni+bigram 188 322 53 66 78.01 74.02 82.99 85.87 75.96 81.08 
Uni+bi+trigram 187 331 44 67 80.95 73.62 83.17 88.27 77.11 82.35 
400 
Unigram 179 315 60 75 74.90 70.47 80.77 84.00 72.62 78.54 
Uni+bigram 184 319 56 70 76.67 72.44 82.01 85.07 74.49 79.97 
Uni+bi+trigram 184 325 50 70 78.63 72.44 82.28 86.67 75.41 80.92 
500 
Unigram 177 310 65 77 73.14 69.69 80.10 82.67 71.37 77.42 
Uni+bigram 178 321 54 76 76.72 70.08 80.86 85.60 73.25 79.33 
Uni+bi+trigram 187 325 50 67 78.90 73.62 82.91 86.67 76.17 81.40 
600 
Unigram 179 305 70 75 71.89 70.47 80.26 81.33 71.17 76.95 
Uni+bigram 177 320 55 77 76.29 69.69 80.60 85.33 72.84 79.01 
Uni+bi+trigram 189 325 50 65 79.08 74.41 83.33 86.67 76.67 81.72 
700 
Unigram 176 308 67 78 72.43 69.29 79.79 82.13 70.82 76.95 
Uni+bigram 180 323 52 74 77.59 70.87 81.36 86.13 74.07 79.97 
Uni+bi+trigram 189 328 47 65 80.08 74.41 83.46 87.47 77.14 82.19 
800 
Unigram 172 311 64 82 72.88 67.72 79.13 82.93 70.20 76.79 
Uni+bigram 180 327 48 74 78.95 70.87 81.55 87.20 74.69 80.60 
Uni+bi+trigram 192 332 43 62 81.70 75.59 84.26 88.53 78.53 83.31 
900 
Unigram 174 311 64 80 73.11 68.50 79.54 82.93 70.73 77.11 
Uni+bigram 182 328 47 72 79.48 71.65 82.00 87.47 75.36 81.08 
Uni+bi+trigram 187 333 42 67 81.66 73.62 83.25 88.80 77.43 82.67 
1000 
Unigram 177 313 62 77 74.06 69.69 80.26 83.47 71.81 77.90 
Uni+bigram 185 326 49 69 79.06 72.83 82.53 86.93 75.82 81.24 
Uni+bi+trigram 190 327 48 64 79.83 74.80 83.63 87.20 77.24 82.19 
Table 6 Performance evaluation on development set with feature selection. TP: True positive, TN: True neg-
ative, FP: False positive, FN: False negative, PPV: Positive predictive value, NPV: Negative predictive value. 
The row with the heighted F-Score is highlighted. 
 
Assertion configuration TP TN FP FN 
Precision/ 
PPV 
Recall/ 
Sensitivity 
NPV Specificity F-Score Accuracy 
Assertion_none 192 332 43 62 81.70 75.59 84.26 88.53 78.53 83.31 
Assertion_all 188 328 47 66 80.00 74.02 83.25 87.47 76.89 82.03 
Assertion_top_10 191 328 47 63 80.25 75.20 83.89 87.47 77.64 82.51 
Assertion_top_20 190 329 46 64 80.51 74.80 83.72 87.73 77.55 82.51 
Assertion_top_30 190 331 44 64 81.20 74.80 83.80 88.27 77.87 82.83 
Assertion_top_40 190 328 47 64 80.17 74.80 83.67 87.47 77.39 82.35 
Assertion_top_50 190 330 45 65 80.85 74.51 83.54 88.00 77.55 82.54 
Table 7 Performance evaluation on development set with the assertion feature (uni+bi+trigram at N=800). 
TP: True positive, TN: True negative, FP: False positive, FN: False negative, PPV: Positive predictive value, 
NPV: Negative predictive value. The row with the heighted F-Score is highlighted. 
 
System configuration TP TN FP FN 
Precision/ 
PPV 
Recall/  
Sensitivity 
NPV Specificity F-Score Accuracy 
Baseline#1?Azzam et. al. (2009) 10 18 16 11 38.46 47.62 62.07 52.94 42.55 50.91 
Baseline#2?unigram 12 29 5 9 70.53 57.14 76.32 85.29 63.16 74.55 
Uni+bi+trigram at k=800 9 30 4 12 69.23 42.86 71.43 88.24 52.94 70.91 
Table 8 Performance evaluation on validation set. TP: True positive, TN: True negative, FP: False positive, 
FN: False negative, PPV: Positive predictive value, NPV: Negative predictive value. The row with the 
heighted F-Score is highlighted. 
14
sifier with uni+bi+trigram at N=800 in our ex-
periments. We applied assertion analysis to all 
800 features as well as only a small set of top 
ranked 10?k (1?k?5) features which were ob-
served to be closely related to ALI (e.g., diffuse, 
opacities, pulmonary edema). We hypothesized 
applying assertion analysis would inform the 
classifier on the presence and absence of those 
terms which would potentially decrease the false 
positive and negative counts. 
Table 7 summarizes the results of our experi-
ments. When we applied assertion analysis to all 
800 features, the performance slightly dropped 
when compared to the performance with no as-
sertion analysis. When assertion analysis applied 
to only top ranked features, the best f-score per-
formance was achieved with assertion analysis 
with top 30 features; however, it was still slightly 
lower than the f-score with no assertion analysis.  
The differences are not statistically significant. 
4.6 Experiments with Validation Set  
We used the validation set to explore the general-
izability of the proposed approach. To accom-
plish this we run the best performing classifier 
(uni+bi+trigram at N=800) and two defined 
baselines on the validation set. We re-trained the 
uni+bi+trigram at N=800 classifier and unigram 
baseline on the complete development set. 
Table 8 includes the performance results. The 
second baseline with unigrams performed the 
best and Azzam et. al.?s baseline performed the 
worst in identifying the patients with ALI in the 
validation set. 
5 Discussion 
Our best system achieved an f-score of 78.53 
(precision=81.70, recall=75.59) on the develop-
ment set. While the result is encouraging and 
significantly better than the f-score of a previous-
ly published system (f-score=62.23, preci-
sion=51.27, recall=79.13), there is still room for 
improvement. 
There are several important limitations to our 
current development dataset. First, the annotators 
who are pulmonary care specialists used only the 
x-ray images to annotate the patients. However, 
the classifiers were trained based on the features 
extracted from the radiologists? free-text inter-
pretation of the x-ray images. In one false posi-
tive case, the radiologist has written ?Bilateral 
diffuse opacities, consistent with pulmonary 
edema. Bibasilar atelectasis.? in the chest x-ray 
report, however all three pulmonary care special-
ists annotated the case as negative based on their 
interpretation of images. Because the report con-
sisted of many very strong features indicative of 
ALI, our classifier falsely identified the patient 
as positive with a very high prediction probabil-
ity 0.96. Second, although three annotators anno-
tated the development set, there was full agree-
ment on 42.12% (107/254) of the positive pa-
tients and 45.33% (170/375) of the negative pa-
tients. Table 9 includes the false positive and 
negative statistics of the best performing classifi-
er (uni+bi+trigrams at N=800). As can be seen 
from the table, the classifier made more mistakes 
on patients where the annotator agreement was 
not perfect. The classifier predicted 13 of the 28 
false positives and 23 of the 39 false negatives 
with probabilities higher than 0.75. When we 
investigated the reports of those 13 false posi-
tives, we observed that the radiologists used 
many very strong ALI indicative features (e.g., 
diffuse lung opacities, low lung volumes) to de-
scribe the images. On the contrary, radiologists 
did not use as many ALI indicative features in 
the reports of 23 false negative cases. 
In our experiments on the development set, we 
demonstrated the positive impact of statistical 
feature selection on the overall classification per-
formance. We achieved the best f-score, when 
we used only 2.67% (800/29,914) of the com-
plete n-gram feature space. We enriched the 
highly ranked features with assertion analysis. 
However, unlike feature selection, assertion 
analysis did not improve the overall perfor-
mance. To explore the reasons, we analyzed re-
ports from our corpus and found out that the cur-
rent six assertion classes (present, absent, condi-
tional, hypothetical, possible) were not sufficient 
to capture true meaning in many cases. For ex-
ample, our assertion classifier assigned the class 
present to the bigram bibasilar opacities based 
on the sentence ?There are bibasilar opacities 
that are unchanged?. Although present was the 
correct assignment for bibasilar opacities, the 
more important piece of information was the 
change of state in bibasilar opacities for ALI 
diagnosis. X-rays describe a single snapshot of 
time but the x-ray report narrative makes explicit 
Error Type Agreement Frequency Percentage 
False Positives 
3 15 10.20% (15/147) 
2 28 26.17% (28/107) 
False Negatives 
3 24 11.70% (24/205) 
2 39 22.94% (39/170) 
Table 9 False positive and false negative statistics at 
different agreement levels. 
15
or, more often implicit references to a previous 
x-ray. In this way, the sequence of x-ray reports 
is used not only to assess a patient?s health at a 
moment in time but also to monitor the change. 
We recently defined a schema to annotate change 
of state for clinical events in chest x-ray reports 
(Vanderwende et al, 2013). We will use this an-
notation schema to create an annotated corpus 
for training models to enrich the assertion fea-
tures for ALI classification.  
The results on the validation set revealed that 
the classification performance degraded signifi-
cantly when training and test data do not come 
from the same dataset. There are multiple rea-
sons to this effect. First, the two datasets had dif-
ferent language characteristics. Although both 
development and validation sets included chest 
x-ray reports, only 2,488 of the 3,305 (75.28%) 
n-gram features extracted from the validation set 
overlapped with the 29,914 n-gram features ex-
tracted from the development set. We suspect 
that this is the main reason why our best per-
forming classifier with feature selection trained 
on the development set did not perform as well 
as the unigram baseline on the validation set. 
Second, the validation set included only 55 pa-
tients and each patient had only one chest x-ray 
report unlike the development set where each 
patient had 2.78 reports on the average. In other 
words, the classifiers trained on the development 
set with richer content made poor predictions on 
the validation set with more restricted content. 
Third, because the number of patients in the val-
idation set was too small, each false positive and 
negative case had a huge impact on the overall 
performance. 
6 Conclusion 
In this paper, we described a text processing ap-
proach to identify patients with ALI from the 
information available in their corresponding free-
text chest x-ray reports. To increase the classifi-
cation performance, we (1) enhanced the base-
line unigram representation with bigram and tri-
gram features, (2) enriched the n-gram features 
with assertion analysis, and (3) applied statistical 
feature selection. Our proposed methodology of 
ranking all the features using statistical hypothe-
sis testing and selecting only the most relevant 
ones for classification resulted in significantly 
improving the performance of a previous system 
for ALI identification. The best performing clas-
sifier achieved 81.70% precision (positive pre-
dictive value), 75.59% recall (sensitivity), 
78.53% f-score, 74.61% negative predictive val-
ue, 76.80% specificity in identifying patients 
with ALI when using the uni+bi+trigram repre-
sentation at N=800. Our experiments showed 
that assertion values did not improve the overall 
performance. For future work, we will work on 
defining new semantic features that will enhance 
the current assertion definition and capture the 
change of important events in radiology reports.  
Acknowledgements 
The work is partly supported by the Institute of 
Translational Health Sciences (UL1TR000423), 
and Microsoft Research Connections. We would 
also like to thank the anonymous reviewers for 
helpful comments. 
References  
Aronsky D, Fiszman M, Chapman WW, Haug PJ. 
Combining decision support methodologies to di-
agnose pneumonia. AMIA Annu Symp Proc., 
2001:12-16. 
Artigas A, Bernard GR, Carlet J, Dreyfuss D, Gatti-
noni L, Hudson L, Lamy M, Marini JJ, Matthay 
MA, Pinsky MR, Spragg R, Suter PM. The Ameri-
can-European Consensus Conference on ARDS, 
part 2: Ventilatory, pharmacologic, supportive 
therapy, study design strategies, and issues related 
to recovery and remodeling. Acute respiratory dis-
tress syndrome. Am J Respir Crit Care Med. 
1998;157(4 Pt1):1332-47. 
Azzam HC, Khalsa SS, Urbani R, Shah CV, Christie 
JD, Lanken PN, Fuchs BD. Validation study of an 
automated electronic acute lung injury screening 
tool. J Am Med Inform Assoc. 2009; 16(4):503-8.  
Bejan CA, Xia F, Vanderwende L, Wurfel M, Yet-
isgen-Yildiz M. Pneumonia identification using 
statistical feature selection. J Am Med Inform As-
soc. 2012; 19(5):817-23. 
Bejan CA, Vanderwende L, Xia F, Yetisgen-Yildiz 
M. Assertion Modeling and its role in clinical phe-
notype identification. J Biomed Inform. 2013; 
46(1):68-74. 
Berger AL, Pietra SAD, Pietra VJD. A maximum 
entropy approach to natural language processing. 
Journal of Computational Linguistics. 1996; 
22(1):39-71. 
Demner-Fushman D, Chapman WW, McDonald CJ. 
What can natural language processing do for clini-
cal decision support? J Biomed Inform. 2009; 
42(5):760-72. 
Dushianthan A, Grocott MPW, Postle AD, Cusack R. 
Acute respiratory distress syndrome and acute lung 
injury. Postgrad Med J. 2011; 87:612-622. 
16
Elkin PL, Froehling D, Wahner-Roedler D, Trusko B, 
Welsh G, Ma H, Asatryan AX, Tokars JI, Rosen-
bloom ST, Brown SH. NLP-based identification of 
pneumonia cases from free-text radiological re-
ports. AMIA Annu Symp Proc. 2008; 6:172-6. 
Ferguson ND, Frutos-Vivar F, Esteban A, Fern?ndez-
Segoviano P, Aramburu JA, N?jera L, Stewart TE. 
Acute respiratory distress syndrome: underrecogni-
tion by clinicians and diagnostic accuracy of three 
clinical definitions. Crit Care Med. 2005; 
33(10):2228-34. 
Fiszman M, Chapman WW, Aronsky D, Evans RS, 
Haug PJ. Automatic detection of acute bacterial 
pneumonia from chest X-ray reports. J Am Med In-
form Assoc. 2000;7(6):593-604. 
Glavan BJ, Holden TD, Goss CH, Black RA, Neff 
MJ, Nathens AB, Martin TR, Wurfel MM; 
ARDSnet Investigators. Genetic variation in the 
FAS gene and associations with acute lung injury. 
Am J Respir Crit Care Med. 2011;183(3):356-63. 
Herasevich V, Yilmaz M, Khan H, Hubmayr RD, 
Gajic O. Validation of an electronic surveillance 
system for acute lung injury. Intensive Care Med. 
2009; 35(6):1018-23. 
Manning CD, Schutze H. Foundations of statistical 
natural language processing. MIT Press 1999. 
Ranieri VM, Rubenfeld GD, Thompson BT, Ferguson 
ND, Caldwell E, Fan E, Camporota L, Slutsky AS. 
Acute Respiratory Distress Syndrome. The Berlin 
Definition. JAMA. 2012; 307(23): 2526-2533. 
Rubenfeld GD, Caldwell E, Peabody E, Weaver J, 
Martin DP, Neff M, Stern EJ, Hudson LD. Inci-
dence and outcomes of acute lung injury. N Engl J 
Med. 2005; 353(16):1685-93. 
Rubenfeld GD, Cooper C, Carter G, Thompson BT, 
Hudson LD. Barriers to providing lung-protective 
ventilation to patients with acute lung injury. Crit 
Care Med. 2004; 32(6):1289-93. 
Solti I, Cooke CR, Xia F, Wurfel MM. Automated 
Classification of Radiology Reports for Acute 
Lung Injury: Comparison of Keyword and Ma-
chine Learning Based Natural Language Pro-
cessing Approaches. Proceedings (IEEE Int Conf 
Bioinformatics Biomed). 2009;314-319. 
Tepper M, Capurro D, Xia F, Vanderwende L, Yet-
isgen-Yildiz M. Statistical Section Segmentation in 
Free-Text Clinical Records. Proceedings of the In-
ternational Conference on Language Resources and 
Evaluation (LREC), Istanbul, May 2012.  
Tepper M, Evans HL, Xia F, Yetisgen-Yildiz M. 
Modeling Annotator Rationales with Application 
to Pneumonia Classification. Proceedings of Ex-
panding the Boundaries of Health Informatics Us-
ing AI Workshop of AAAI'2013, Bellevue, WA; 
2013. 
Uzuner O, South BR, Shen S, DuVall SL. 2010 
i2b2/VA challenge on concepts, assertions, and re-
lations in clinical text. J Am Med Inform Assoc. 
2011; 18(5):552?556. 
Vanderwende L, Xia F, Yetisgen-Yildiz M. Annotat-
ing Change of State for Clinical Events.  
Proceedings of the 1st Workshop on EVENTS: 
Definition, Detection, Coreference, and Represen-
tation Workshop of NAACL?2013, Atlanta, June 
2013. 
Wenqian W, Houkuan H, Haibin Z et al A novel fea-
ture selection algorithm for text categorization. Ex-
pert Syst Appl 2007;33:1?5. 
17
