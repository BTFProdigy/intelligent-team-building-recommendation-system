Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 313?320
Manchester, August 2008
Tracking the Dynamic Evolution of Participant Salience in a Discussion
Ahmed Hassan
University of Michigan
hassanam@umich.edu
Anthony Fader
University of Michigan
afader@umich.edu
Michael H. Crespin
University of Georgia
crespin@uga.edu
Kevin M. Quinn
Harvard University
kquinn@fsa.harvard.edu
Burt L. Monroe
Pennsylvania State University
burtmonroe@psu.edu
Michael Colaresi
Michigan State University
colaresi@msu.edu
Dragomir R. Radev
University of Michigan
radev@umich.edu
Abstract
We introduce a technique for analyzing the
temporal evolution of the salience of par-
ticipants in a discussion. Our method can
dynamically track how the relative impor-
tance of speakers evolve over time using
graph based techniques. Speaker salience
is computed based on the eigenvector cen-
trality in a graph representation of partici-
pants in a discussion. Two participants in a
discussion are linked with an edge if they
use similar rhetoric. The method is dy-
namic in the sense that the graph evolves
over time to capture the evolution inher-
ent to the participants salience. We used
our method to track the salience of mem-
bers of the US Senate using data from the
US Congressional Record. Our analysis
investigated how the salience of speakers
changes over time. Our results show that
the scores can capture speaker centrality
in topics as well as events that result in
change of salience or influence among dif-
ferent participants.
1 Introduction
There are several sources of data that record
speeches or participations in debates or discus-
sions among a group of speakers or participants.
Those include parliamentary records, blogs, and
news groups. This data represents a very important
and unexploited source of information that con-
tains several trends and ideas. In any debate or
discussion, there are certain types of persons who
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
influence other people and pass information or ad-
vice to them. Those persons are often regarded
as experts in the field or simply influential peo-
ple and they tend to affect the ideas and rhetoric
of other participants. This effect can be tracked
down by tracking the similarity between different
speeches. We can then imagine a debate with many
people arguing about many different things as a
network of speeches or participations interacting
with each other. We can then try to identify the
most salient or important participants by identify-
ing the most central speeches in this network and
associating them with their speakers. When we
have a large dataset of debates and conversations
that expand over a long period of time, the salience
of participants becomes a dynamic property that
changes over time. To capture this dynamic nature
of the process, the graph of speeches must evolve
over time such that we have a different graph at
each instance of time that reflects the interaction
of speeches at this instant.
We apply our method to the US Congressional
Record. The US Congressional Record documents
everything said and done in the US Congress
House and Senate. The speeches in this data set
are made by a large number of people over a long
period of time. Using political speeches as test
data for the proposed method adds an extra layer
of meaning onto the measure of speakers salience.
Speaker salience of the Congress members can re-
flect the importance or influence in the US leg-
islative process. The way salience scores evolve
over time can answer several interesting issues like
how the influence of the speakers vary with major-
ity status and change of party control. It can also
study the dynamics of the relative distribution of
attention to each topic area in different time peri-
ods.
313
The rest of this paper will proceed as follows.
Section 2 reviews some related work. In Section 3,
we describe how the data can be clustered into dif-
ferent topic clusters. In Section 4, we describe
our method for computing the salience of different
participant in a discussion, we also describe how
to the network of speakers varies over time. Sec-
tion 5 describes the experimental setup. Finally,
we present the conclusions in Section 6.
2 Related Work
Several methods have been proposed for identify-
ing the most central nodes in a network. Degree
centrality, closeness, and betweenness (Newman,
2003) are among the most known methods for
measuring centrality of nodes in a network. Eigen-
vector centrality is another powerful method that
that has been applied to several types of networks.
For example it has been used to measure cen-
trality in hyperlinked web pages networks (Brin
and Page, 1998; Kleinberg, 1998), lexical net-
works (Erkan and Radev, 2004; Mihalcea and Ta-
rau, 2004; Kurland and Lee, 2005; Kurland and
Lee, 2006), and semantic networks (Mihalcea et
al., 2004).
The interest of applying natural language pro-
cessing techniques in the area of political science
has been recently increasing.
(Quinn et al, 2006) introduce a multinomial
mixture model to cluster political speeches into
topics or related categories. In (Porter et al, 2005),
a network analysis of the members and committees
of the US House of Representatives is performed.
The authors prove that there are connections link-
ing some political positions to certain committees.
This suggests that there are factors affecting com-
mittee membership and that they are not deter-
mined at random. In (Thomas et al, 2006), the au-
thors try to automatically classify speeches, from
the US Congress debates, as supporting or oppos-
ing a given topic by taking advantage of the voting
records of the speakers. (Fader et al, 2007) in-
troduce MavenRank , which is a method based on
lexical centrality that identifies the most influen-
tial members of the US Senate. It computes a sin-
gle salience score for each speaker that is constant
over time.
In this paper, we introduce a new method for
tracking the evolution of the salience of partici-
pants in a discussion over time. Our method is
based on the ones described in (Erkan and Radev,
2004; Mihalcea and Tarau, 2004; Fader et al,
2007), The objective of this paper is to dynami-
cally rank speakers or participants in a discussion.
The proposed method is dynamic in the sense that
the computed importance varies over time.
3 Topic Clusters
Before applying the proposed method to a data
set with speeches in multiple topics, we first need
to divide the speech documents into topic clus-
ters. We used the model described in (Quinn et al,
2006) for this purpose. The model presented in this
paper assumes that the probabilities of a document
belonging to a certain topic varies smoothly over
time and the words within a given document have
exactly the same probability of being drawn from
a particular topic (Quinn et al, 2006). These two
properties make the model different than standard
mixture models (McLachlan and Peel, 2000) and
the latent Dirichlet alocation model of (Blei et al,
2003). The model of (Quinn et al, 2006) is most
closely related to the model of (Blei and Lafferty,
2006), who present a generalization of the model
used by (Quinn et al, 2006).
The output from the topic model is a D ? K
matrix Z where D is the number of speeches , K
is the number of topics and the element z
dk
repre-
sents the probability of the dth speech being gen-
erated by topic k. We then assign each speech d
to the kth cluster where k = argmax
j
z
dj
. If the
maximum value is not unique, one of the clusters
having the maximum value is arbitrary selected.
4 Speaker Centrality
In this section we describe how to build a network
of speeches and use it to identify speaker centrality.
We also describe how to generate different projec-
tions of the network at different times, and how
to use those projection to get dynamic salience
scores.
4.1 Computing Speaker Salience
The method we used is similar to the methods de-
scribed in (Erkan and Radev, 2004; Mihalcea and
Tarau, 2004; Kurland and Lee, 2005), which were
originally used for ranking sentences and docu-
ments in extractive summarization and information
retrieval systems.
A collection of speeches can be represented as
a network where similar speeches are linked to
each other. The proposed method is based on
314
the premise that important speeches tend to be
lexically similar to other important speeches, and
important speeches tend to belong to important
speakers. Hence given a collection of speeches and
a similarity measure, we can build a network and
define the centrality score of a speech recursively
in terms of the scores of other similar speeches.
Later, we can compute the salience of a speaker
as the sum of the centrality measure of all his
speeches.
To measure the similarity between two
speeches, we use the bag-of-words model to repre-
sent each sentence as an N-dimensional vector of
tf-idf scores, where N is the number of all possible
words in the target language. The similarity
between two speeches is then computed using the
cosine similarity between the two vectors.
A vector of term frequencies is used to represent
each speech. Those term frequencies are weighted
according to the relative importance of the given
term in the cluster.
The vectors representing speeches contain term
frequencies (or tf), which are weighted according
to their inverse document frequencies to account
for the relative importance of the given term in the
cluster. The inverse document frequency of a term
w is given by (Sparck-Jones, 1972)
idf(w) = log
(
N
n
w
)
(1)
where n
w
is the number of speeches in the clus-
ter containing the term w, and N is the number of
documents in the cluster. We calculated idf values
specific to each topic, rather than to all speeches.
We preferred to use topic-specific idf values be-
cause the relative importance of words may vary
from one topic to the other.
The tf-idf cosine similarity measure is computed
as the cosine of the angle between the tf-idf vec-
tors. It is defined as follows:
P
w?u,v
tf
u
(w) tf
v
(w) idf(w)
2
?
P
w?u
(tf
u
(w) idf(w))
2
?
P
w?v
(tf
v
(w) idf(w))
2
, (2)
The choice of tf-idf scores to measure speech
similarity is an arbitrary choice. Some other possi-
ble similarity measures are edit distance, language
models (Kurland and Lee, 2005), or generation
probabilities (Erkan, 2006).
The recursive definition of the score of any
speech s in the speeches network is given by
p(s) =
?
t?adj[s]
p(t)
deg(t)
(3)
where deg(t) is the degree of node t, and adj[s] is
the set of all speeches adjacent to s in the network.
This can be rewritten in matrix notation as:
p = pB (4)
where p = (p(s
1
), p(s
2
), . . . , p(s
N
)) and the ma-
trix B is the row normalized similarity matrix of
the graph
B(i, j) =
S(i, j)
?
k
S(i, k)
(5)
where S(i, j) = sim(s
i
, s
j
). Equation (4) shows
that the vector of salience scores p is the left eigen-
vector of B with eigenvalue 1.
The matrix B can be thought of as a stochastic
matrix that acts as transition matrix of a Markov
chain. An element X(i, j) of a stochastic matrix
specifies the transition probability from state i to
state j in the corresponding Markov chain. And
the whole process can be seen as a Markovian ran-
dom walk on the speeches graph. To help the ran-
dom walker escape from periodic or disconnected
components, (Brin and Page, 1998) suggests re-
serving a small escape probability at each node
that represents a chance of jumping to any node
in the graph, making the Markov chain irreducible
and aperiodic, which guarantees the existence of
the eigenvector.
Equation (4) can then be rewritten, assuming a
uniform escape probability, as:
p = p[dU+ (1 ? d)B] (6)
where N is the total number of nodes, U is a
square matrix with U(i, j) = 1/N for all i, j, and
d is the escape probability chosen in the interval
[0.1, 0.2] (Brin and Page, 1998).
4.2 Dynamic Salience Scores
We use the time stamps associated with the data to
compute dynamic salience scores p
T
(u) that iden-
tify central speakers at some time T . To do this,
we create a speech graph that evolves over time.
Let T be the current date and let u and v be two
speech documents that occur on days t
u
and t
v
.
Our goal is to discount the lexical similarity of u
and v based on how far apart they are. One way
to do this is by defining a new similarity measure
s(u, v;T ) as:
s(u, v;T ) = tf-idf-cosine(u, v) ? f(u, v;T ) (7)
315
where f(u, v;T ) is a function taking values in
[0, 1].
If f(u, v;T ) = 1 for all u, v, and T , then time is
ignored when calculating similarity and p
T
(u) =
p(u). On the other hand, suppose we let
f(u, v;T ) =
{
1 if t
u
= t
v
= T ,
0 else.
(8)
This removes all edges that link a speech, occur-
ring at some time T , to all other speeches occur-
ring at some time other than T and the ranking al-
gorithm will be run on what is essentially the sub-
graph of documents restricted to time T (although
the isolated speech documents will receive small
non-zero scores because of the escape probability
from Section 4.1). These two cases act as the ex-
treme boundaries of possible functions f : in the
first case time difference has no effect on document
similarity, while in the second case two documents
must occur on the same day to be similar.
We use the following time weight functions in
our experiments. In each case, we assume that the
speeches represented by speech documents u and v
have already occurred, that is, t
u
, t
v
? T . We will
use the convention that f(u, v;T ) = 0 if t
u
> T
or t
v
> T for all time weight functions, which
captures the idea that speeches that have not yet
occurred have no influence on the graph at time T .
Also define
age(u, v;T ) = T ? min{t
u
, t
v
} (9)
which gives the age of the oldest speech document
from the pair u, v at time T .
? Exponential: Given a parameter a > 0, define
f
exp,a
(u, v;T ) = e
?a age(u,v;T )
. (10)
This function will decrease the impact of sim-
ilarity as time increases in an exponential
fashion. a is a parameter that controls how
fast this happens, where a larger value of a
makes earlier speeches have a small impact
on current scores and a smaller value of a
means that earlier speeches will have a larger
impact on current scores.
? Linear: Given b > 0, define
f
lin,d
(u, v;T ) =
?
?
?
?
?
1 ?
1
b
age(u, v;T )
if age(u, v;T ) ? b
0 if age(u, v;T ) > b
(11)
Figure 1: The Dynamic boundary cases for Sena-
tor Santorum.
This function gives speech documents that
occur at time T full weight and then decreases
their weight linearly towards time T + b,
where it becomes 0.
? Boundary: Given d ? 0, define
f
bnd,d
(u, v;T ) =
{
1 if age(u, v;T ) ? d
0 if age(u, v;T ) > d
(12)
This function gives speech documents occur-
ring within d days of T the regular tf-idf sim-
ilarity score, but sets the similarity of speech
documents occurring outside of d days to 0.
The case when d = 0 is one of the boundary
cases explained above.
Figure 1 gives an example of different time
weighting functions for Senator Rick Santorum
(R - Pennsylvania) on topic 22 (Abortion) during
1997, the first session of the 105th Congress. The
dashed line shows the case when time has no ef-
fect on similarity (his score is constant over time),
while the solid line shows the case where only
speeches on the current day are considered simi-
lar (his score spikes only on days where he speaks
and is near zero otherwise). The dotted line shows
the case when the influence of older speeches de-
creases exponentially, which is more dynamic than
the first case but smoother than the second case.
5 Experiments and Results
5.1 Data
We used the United States Congressional Speech
corpus (Monroe et al, 2006) in our experiment.
316
This corpus is in XML formatted version of the
electronic United States Congressional Record
from the Library of Congress
1
. The Congressional
Record is a verbatim transcript of the speeches
made in the US House of Representatives and Sen-
ate and includes tens of thousands of speeches per
year (Monroe et al, 2006). The data we used cover
the period from January 2001 to January 2003.
5.2 Experimental Setup
We used results from (Quinn et al, 2006) to get
topic clusters from the data, as described in Sec-
tion 3. The total number of topics was 42. The
average sized topic cluster had several hundred
speech documents (Quinn et al, 2006).
We set up a pipeline using a Perl implementa-
tion of the proposed method We ran it on the topic
clusters and ranked the speakers based on the cen-
trality scores of their speeches. The graph nodes
were speech documents. A speaker?s score was
determined by the average of the scores of the
speeches given by that speaker. After comparing
the different time weighting function as shown in
Figure 1, we decided to use the exponential time
weight function for all the experiments discussed
below. Exponential time weighting function de-
creases the impact of similarity as time increases
in an exponential fashion. It also allows us to con-
trol the rate of decay using the parameter a.
5.3 Baseline
We compare the performance of our system to
a simple baseline that calculates the salience of
a speaker as a weighted count of the number of
times he has spoken. The baseline gives high
weight to recent speeches . The weight decreases
as the speeches gets older. The salience score of a
speaker is calculate as follows:
BS(i) =
?
d
?
d
0
?d
? S
i
d
(13)
Where BS(i) is the baseline score of speaker i,
? is the discounting factor, d
0
is the current date,
and S
i
d
is the number of speeches made by speaker
i at date d. We used ? = 0.9 for all our experi-
ments.
5.4 Results
One way to evaluate the dynamic salience scores,
is to look at changes when party control of the
1
http://thomas.loc.gov
chamber switches. Similar to (Hartog and Mon-
roe, 2004), we exploit the party switch made by
Senator Jim Jeffords of Vermont and the result-
ing change in majority control of the Senate dur-
ing the 107th Congress as a quasi-experimental
design. In short, Jeffords announced his switch
on May 24, 2001 from Republican to Independent
status, effective June 6, 2001. Jeffords stated that
he would vote with the Democrats to organize the
Senate, giving the Democrats a one-seat advantage
and change control of the Senate from the Repub-
licans back to the Democrats. This change of ma-
jority status during the 107th Congress allows us
to ignore many of the factors that could potentially
influence dynamic salience scores at the start of a
new congress.
On average, we expect committee chairs or a
member of the majority party to be the most im-
portant speaker on each topic followed by ranking
members or a member of the minority party. If
our measure is capturing dynamics in the central-
ity of Senators, we expect Republicans to be more
central before the Jeffords switch and Democrats
becoming central soon afterwards, assuming the
topic is being discussed on the Senate floor. We
show that the proposed technique captures several
interesting events in the data and also show that the
baseline explained above fails to capture the same
set of events.
Figure 2(a) shows the dynamic salience scores
over time for Senator John McCain (R - Arizona)
and Senator Carl Levin (D - Michigan) on topic
5 (Armed Forces 2) for the 107th Senate. Mc-
Cain was the most salient speaker for this topic
until June 2001. Soon after the change in major-
ity status a switch happened and Levin, the new
chair of Senate Armed Services, replaced McCain
as the most salient speaker. On the other hand,
Figure 2(b) shows the baseline scores for the same
topic and same speakers. We notice here that the
baseline failed to capture the switch of salience
near June 2001.
We can also observe similar behavior in Fig-
ure 3(a). This figure shows how Senate Majority
Leader Trent Lott (R - Mississippi) was the most
salient speaker on topic 35 (Procedural Legisla-
tion) until July 2001. Topic 35 does not map to
a specific committee but rather is related to ma-
neuvering bills through the legislative process on
the floor, a job generally delegated to members in
the Senate leadership. Just after his party gained
317
 
0
 
0.1
 
0.2
 
0.3
 
0.4
 
0.5
 
0.6
 
0.7
 
0.8 Jan0
1Mar
01Ma
y01J
ul01
Sep01
Nov01
Jan02
Mar02
May02
Jul02
Sep02
Nov02
Jan03
LexRank
Time
Dynam
ic Lexr
ank, Se
nate 10
7 Arme
d Force
s 2 (Infra
structure
)??, Expo
nential, a
=0.02, th
=
MCCA
IN
LEVIN
CARL
(a) Dynamic Lexrank
 
0
 
2
 
4
 
6
 
8
 
10 Jan01
Mar01
May01
Jul01
Sep01
Nov01
Jan02
Mar02
May02
Jul02
Sep02
Nov02
Jan03
LexRank
Time
Baselin
e, Sena
te 107 
Armed
 Forces
 2 (Infras
tructure)
MCCA
IN
LEVIN
CARL
(b) Baseline
Figure 2: The Switch of Speakers Salience near Jun 2001 for Topic 5(Armed Forces 2).
 
0
 
0.1
 
0.2
 
0.3
 
0.4
 
0.5
 
0.6
 
0.7
 
0.8 Jan0
1Mar
01Ma
y01J
ul01
Sep01
Nov01
Jan02
Mar02
May02
Jul02
Sep02
Nov02
Jan03
LexRank
Time
Dynam
ic Lexr
ank, Se
nate 10
7 Proce
dural 4
 (Legisla
ton 2)??, E
xponenti
al, a=0.0
2, th= REID LOTT
(a) Dynamic Lexrank
 
0
 
5
 
10
 
15
 
20 Jan01
Mar01
May01
Jul01
Sep01
Nov01
Jan02
Mar02
May02
Jul02
Sep02
Nov02
Jan03
LexRank
Time
Baselin
e, Sena
te 107 
Proced
ural 4 (L
egislaton
 2)??
REID LOTT
(b) Baseline
Figure 3: The Switch of Speakers Salience near Jun 2001 for Topic 35(Procedural Legislation).
majority status, Senator Harry Reid (D - Nevada)
became the most salient speaker for this topic. This
is consistent with Reid?s switch from Assistant mi-
nority Leader to Assistant majority Leader. Again
the baseline scores for the same topic and speakers
in Figure 3(b) fails to capture the switch.
An even more interesting test would be to check
whether the Democrats in general become more
central than Republicans after the Jeffords switch.
Figure 4(a) shows the normalized sum of the
scores of all the Democrats and all the Republicans
on topic 5 (Armed Forces 2) for the 107th Senate.
The figure shows how the Republicans were most
salient until soon after the Jeffords switch when the
Democrats regained the majority and became more
salient. We even discovered similar behavior when
we studied how the average salience of Democrats
and Republicans change across all topics. This is
shown in Figure 5(a) where we can see that the
Republicans were more salient on average for all
topics until June 2001. Soon after the change in
majority status, Democrats became more central.
Figures 4(b) and 5(b) show the same results using
the baseline system. We notice that the number of
speeches made by the Democrats and the Repub-
licans is very similar in most of the times. Even
when one of the parties has more speeches than
the other, it does not quite reflect the salience of
the speakers or the parties in general.
An alternative approach to evaluate the dynamic
scores is to exploit the cyclical nature of the leg-
islative process as some bills are re-authorized on
a fairly regular time schedule. For example, the
farm bill comes due about every five years. As a
new topic is coming up for debate, we expect the
saliency scores for relevant legislators to increase.
Figure 6 shows the dynamic scores of Senator
Thomas Harkin (D - Iowa), and Senator Richard
318
 
0
 
0.2
 
0.4
 
0.6
 
0.8 1 Jan0
1Mar
01Ma
y01J
ul01
Sep01
Nov01
Jan02
Mar02
May02
Jul02
Sep02
Nov02
Jan03
LexRank
Time
Dynam
ic Lexr
ank, Se
nate 10
7 Arme
d Force
s 2??
Repub
licans Democ
rats
(a) Dynamic Lexrank
 
0
 
5
 
10
 
15
 
20
 
25
 
30 Jan01
Mar01
May01
Jul01
Sep01
Nov01
Jan02
Mar02
May02
Jul02
Sep02
Nov02
Jan03
LexRank
Time
Baselin
e, Sena
te 107 
Armed
 Forces
 2 (Infras
tructure)
Democ
rates
Repub
licans
(b) Baseline
Figure 4: The Switch of Speakers Salience near Jun 2001 for Topic 5(Armed Forces 2), Republicans vs
Democrats.
 
10
 
12
 
14
 
16
 
18
 
20 Jan01
Mar01
May01
Jul01
Sep01
Nov01
Jan02
Mar02
May02
Jul02
Sep02
Nov02
Jan03
LexRank
Time
Dynam
ic Lexr
ank, Se
nate 10
7
Repub
licans Democ
rats
(a) Dynamic Lexrank
 
0
 
100
 
200
 
300
 
400
 
500
 
600
 
700 Jan0
1Mar
01Ma
y01J
ul01
Sep01
Nov01
Jan02
Mar02
May02
Jul02
Sep02
Nov02
Jan03
LexRank
Time
Baselin
e, Sena
te 107
Democ
rates
Repub
licans
(b) Baseline
Figure 5: The Switch of Speakers Salience near Jun 2001 for All Topics, Republicans vs Democrats.
Lugar (R - Indiana) during the 107th senate on
topic 24 (Agriculture). The two senators were
identified, by the proposed method, as the most
salient speakers for this topic, as expected, since
they both served as chairmen of the Senate Com-
mittee on Agriculture, Nutrition, and Forestry
when their party was in the majority during the
107th Senate. This committee was in charge of
shepherding the Farm Bill through the Senate. The
scores of both senators on the agriculture topic sig-
nificantly increased starting late 2001 until June
2002. The debate began on the bill starting in
September of 2001 and it was not passed until May
2002.
6 Conclusion
We presented a graph based method for analyz-
ing the temporal evolution of the salience of par-
ticipants in a discussion. We used this method to
track the evolution of salience of speakers in the
US Congressional Record. We showed that the
way salience scores evolve over time can answer
several interesting issues. We tracked how the in-
fluence of the speakers vary with majority status
and change of party control. We also show how
a baseline system that depends on the number of
speeches fails to capture the interesting events cap-
tured by the proposed system. We also studied the
dynamics of the relative distribution of attention to
each topic area in different time periods and cap-
tured the cyclical nature of the legislative process
as some bills are re-authorized on a fairly regular
time schedule.
319
 
0
 
0.1
 
0.2
 
0.3
 
0.4
 
0.5 Jan0
1Mar
01Ma
y01J
ul01
Sep01
Nov01
Jan02
Mar02
May02
Jul02
Sep02
Nov02
Jan03
LexRank
Time
Dynam
ic Lexr
ank, Se
nate 10
7 Agric
ulture??
, Expon
ential, 
a=0.02
, th= LUG
AR HARKI
N
Figure 6: The Farm Bill Discussions on the Rela-
tive Distribution of Attention to Topic 24 (Agricul-
ture).
Acknowledgments
This paper is based upon work supported by
the National Science Foundation under Grant No.
0527513, ?DHB: The dynamics of Political Rep-
resentation and Political Rhetoric?. Any opinions,
findings, and conclusions or recommendations ex-
pressed in this paper are those of the authors and
do not necessarily reflect the views of the National
Science Foundation.
References
Blei, David and John Lafferty. 2006. Dynamic topic
models. In ICML 2006.
Blei, David, Andrew Ng, and Michael Jordan. 2003.
Latent dirichlet alocation. Journal of Machine
Learning Research, 3:993?1022.
Brin, Sergey and Lawrence Page. 1998. The anatomy
of a large-scale hypertextual Web search engine.
CNIS, 30(1?7):107?117.
Erkan, G?unes? and Dragomir Radev. 2004. Lexrank:
Graph-based centrality as salience in text summa-
rization. Journal of Artificial Intelligence Research
(JAIR).
Erkan, Gunes. 2006. Language model-based document
clustering using random walks. In HLT/NAACL
2006, pages 479?486. Association for Computa-
tional Linguistics.
Fader, Anthony, Dragomir Radev, Michael Crespin,
Burt Monroe, Kevin Quinn, and Michael Colaresi.
2007. Mavenrank: Identifying influential members
of the us senate using lexical centrality. In EMNLP
2007.
Hartog, Chris Den and Nathan Monroe. 2004. The
value of majority status: The effect of jeffords?s
switch on asset prices of republican and democratic
firms. Legislative Studies Quarterly, 33:63?84.
Kleinberg, Jon. 1998. Authoritative sources in a hyper-
linked environment. In the ACM-SIAM Symposium
on Discrete Algorithms, pages 668?677.
Kurland, Oren and Lillian Lee. 2005. PageRank with-
out hyperlinks: Structural re-ranking using links in-
duced by language models. In SIGIR 2005, pages
306?313.
Kurland, Oren and Lillian Lee. 2006. Respect my au-
thority! HITS without hyperlinks, utilizing cluster-
based language models. In SIGIR 2006, pages 83?
90.
McLachlan, Geoffrey and David Peel. 2000. Finite
Mixture Models. New York: Wiley.
Mihalcea, Rada and Paul Tarau. 2004. TextRank:
Bringing order into texts. In EMNLP 2004.
Mihalcea, Rada, Paul Tarau, and Elizabeth Figa. 2004.
Pagerank on semantic networks, with application
to word sense disambiguation. In COLING 2004,
pages 1126?1132.
Monroe, Burt, Cheryl Monroe, Kevin Quinn, Dragomir
Radev, Michael Crespin, Michael Colaresi, Anthony
Fader, Jacob Balazer, and Steven Abney. 2006.
United states congressional speech corpus. Depart-
ment of Political Science, The Pennsylvania State
University.
Newman, Mark. 2003. A measure of betweenness
centrality based on random walks. Technical Report
cond-mat/0309045, Arxiv.org.
Porter, Mason, Peter Mucha, Miark Newman, and
Casey Warmbrand. 2005. A network analysis of
committees in the U.S. House of Representatives.
PNAS, 102(20).
Quinn, Kevin, Burt Monroe, Michael Colaresi, Michael
Crespin, and Dragomir Radev. 2006. An automated
method of topic-coding legislative speech over time
with application to the 105th-108th U.S. senate. In
Midwest Political Science Association Meeting.
Sparck-Jones, Karen. 1972. A statistical interpretation
of term specificity and its application in retrieval.
Journal of Documentation, 28(1):11?20.
Thomas, Matt, Bo Pang, and Lillian Lee. 2006. Get
out the vote: Determining support or opposition from
Congressional floor-debate transcripts. In EMNLP
2006, pages 327?335.
320
Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational
Natural Language Learning, pp. 658?666, Prague, June 2007. c?2007 Association for Computational Linguistics
MavenRank: Identifying Influential Members of the US Senate Using
Lexical Centrality
Anthony Fader
University of Michigan
afader@umich.edu
Dragomir Radev
University of Michigan
radev@umich.edu
Michael H. Crespin
The University of Georgia
crespin@uga.edu
Burt L. Monroe
The Pennsylvania State University
burtmonroe@psu.edu
Kevin M. Quinn
Harvard University
kevin quinn@harvard.edu
Michael Colaresi
Michigan State University
colaresi@msu.edu
Abstract
We introduce a technique for identifying the
most salient participants in a discussion. Our
method, MavenRank is based on lexical cen-
trality: a random walk is performed on a
graph in which each node is a participant in
the discussion and an edge links two partici-
pants who use similar rhetoric. As a test, we
used MavenRank to identify the most influ-
ential members of the US Senate using data
from the US Congressional Record and used
committee ranking to evaluate the output.
Our results show that MavenRank scores are
largely driven by committee status in most
topics, but can capture speaker centrality in
topics where speeches are used to indicate
ideological position instead of influence leg-
islation.
1 Introduction
In a conversation or debate between a group of
people, we can think of two remarks as interact-
ing if they are both comments on the same topic.
For example, if one speaker says ?taxes should
be lowered to help business,? while another argues
?taxes should be raised to support our schools,? the
speeches are interacting with each other by describ-
ing the same issue. In a debate with many people
arguing about many different things, we could imag-
ine a large network of speeches interacting with each
other in the same way. If we associate each speech
in the network with its speaker, we can try to iden-
tify the most important people in the debate based
on how central their speeches are in the network.
To describe this type of centrality, we borrow a
term from The Tipping Point (Gladwell, 2002), in
which Gladwell describes a certain type of person-
ality in a social network called a maven. A maven
is a trusted expert in a specific field who influences
other people by passing information and advice. In
this paper, our goal is to identify authoritative speak-
ers who control the spread of ideas within a topic. To
do this, we introduce MavenRank, which measures
the centrality of speeches as nodes in the type of net-
work described in the previous paragraph.
Significant research has been done in the area
of identifying central nodes in a network. Vari-
ous methods exist for measuring centrality, includ-
ing degree centrality, closeness, betweenness (Free-
man, 1977; Newman, 2003), and eigenvector cen-
trality. Eigenvector centrality in particular has
been successfully applied to many different types
of networks, including hyperlinked web pages (Brin
and Page, 1998; Kleinberg, 1998), lexical net-
works (Erkan and Radev, 2004; Mihalcea and Ta-
rau, 2004; Kurland and Lee, 2005; Kurland and
Lee, 2006), and semantic networks (Mihalcea et al,
2004). The authors of (Lin and Kan, 2007) extended
these methods to include timestamped graphs where
nodes are added over time and applied it to multi-
document summarization. In (Tong and Faloutsos,
2006), the authors use random walks on a graph as
a method for finding a subgraph that best connects
some or all of a set of query nodes. In our paper,
we introduce a new application of eigenvector cen-
trality for identifying the central speakers in the type
of debate or conversation network described above.
Our method is based on the one described in (Erkan
658
and Radev, 2004) and (Mihalcea and Tarau, 2004),
but modified to rank speakers instead of documents
or sentences.
In our paper, we apply our method to analyze the
US Congressional Record, which is a verbatim tran-
script of speeches given in the United States House
of Representatives and Senate. The Record is a
dense corpus of speeches made by a large number
of people over a long period of time. Using the tran-
scripts of political speeches adds an extra layer of
meaning onto the measure of speaker centrality. The
centrality of speakers in Congress can be thought of
as a measure of relative importance or influence in
the US legislative process. We can also use speaker
centrality to analyze committee membership: are the
central speakers on a given issue ranking members
of a related committee? Is there a type of impor-
tance captured through speaker centrality that isn?t
obvious in the natural committee rankings?
There has been growing interest in using tech-
niques from natural language processing in the area
of political science. In (Porter et al, 2005) the
authors performed a network analysis of members
and committees of the US House of Representatives.
They found connections between certain commit-
tees and political positions that suggest that com-
mittee membership is not determined at random.
In (Thomas et al, 2006), the authors use the tran-
scripts of debates from the US Congress to auto-
matically classify speeches as supporting or oppos-
ing a given topic by taking advantage of the vot-
ing records of the speakers. In (Wang et al, 2005),
the authors use a generative model to simultane-
ously discover groups of voters and topics using
the voting records and the text from bills of the
US Senate and the United Nations. The authors
of (Quinn et al, 2006) introduce a multinomial mix-
ture model to perform unsupervised clustering of
Congressional speech documents into topically re-
lated categories. We rely on the output of this model
to cluster the speeches from the Record in order to
compare speaker rankings within a topic to related
committees.
We take advantage of the natural measures of
prestige in Senate committees and use them as a
standard for comparison with MavenRank. Our hy-
pothesis is that MavenRank centrality will capture
the importance of speakers based on the natural
committee rankings and seniority. We can test this
claim by clustering speeches into topics and then
mapping the topics to related committees. If the hy-
pothesis is correct, then the speaker centrality should
be correlated with the natural committee rankings.
There have been other attempts to link floor par-
ticipation with topics in political science. In (Hall,
1996), the author found that serving on a commit-
tee can positively predict participation in Congress,
but that seniority was not a good predictor. His
measure only looked at six bills in three commit-
tees, so his method is by far not as comprehensive
as the one that we present here. Our approach with
MavenRank differs from previous work by provid-
ing a large scale analysis of speaker centrality and
bringing natural language processing techniques to
the realm of political science.
2 Data
2.1 The US Congressional Speech Corpus
The text used in the experiments is from the United
States Congressional Speech corpus (Monroe et
al., 2006), which is an XML formatted version of
the electronic United States Congressional Record
from the Library of Congress1. The Congressional
Record is a verbatim transcript of the speeches made
in the US House of Representatives and Senate be-
ginning with the 101st Congress in 1998 and in-
cludes tens of thousands of speeches per year. In
our experiments we focused on the records from the
105th and 106th Senates. The basic unit of the US
Congressional Speech corpus is a record, which cor-
responds to a single subsection of the print version
of the Congressional Record and may contain zero
or more speakers. Each paragraph of text within
a record is tagged as either speech or non-speech
and each paragraph of speech text is tagged with the
unique id of the speaker. Figure 1 shows an example
record file for the sixth record on July 14th, 1997 in
the 105th Senate.
In our experiments we use a smaller unit of anal-
ysis called a speech document by taking all of the
text of a speaker within a single record. The cap-
italization and punctuation is then removed from
the text as in (Monroe et al, 2006) and then the
1http://thomas.loc.gov
659
text stemmed using Porter?s Snowball II stemmer2.
Figure 1 shows an example speech document for
speaker 15703 (Herb Kohl of Wisconsin) that has
been generated from the record in Figure 1.
In addition to speech documents, we also use
speaker documents. A speaker document is the
concatenation of all of a speaker?s speech docu-
ments within a single session and topic (so a sin-
gle speaker may have multiple speaker documents
across topics). For example within the 105th Senate
in topic 1 (?Judicial Nominations?), Senator Kohl
has four speech documents, so the speaker document
attributed to him within this session and topic would
be the text of these four documents treated as a sin-
gle unit. The order of the concatenation does not
matter since we will look at it as a vector of weighted
term frequencies (see Section 3.2).
2.2 Topic Clusters
We used the direct output of the 42-topic model of
the 105th-108th Senates from (Quinn et al, 2006)
to further divide the speech documents into topic
clusters. In their paper, they use a model where the
probabilities of a document belonging to a certain
topic varies smoothly over time and the words within
a given document have exactly the same probabil-
ity of being drawn from a particular topic. These
two properties make the model different than stan-
dard mixture models (McLachlan and Peel, 2000)
and the latent Dirichlet alocation model of (Blei et
al., 2003). The model of (Quinn et al, 2006) is most
closely related to the model of (Blei and Lafferty,
2006), who present a generalization of the model
used by (Quinn et al, 2006). Table 1 lists the 42
topics and their related committees.
The output from the topic model is a D ? 42 ma-
trix Z where D is the number of speech documents
and the element zdk represents the probability of the
dth speech document being generated by topic k.
We clustered the speech documents by assigning a
speech document d to the kth cluster where
k = argmax
j
zdj .
If the maximum value is not unique, we arbitrarily
assign d to the lowest numbered cluster where zdj is
2http://snowball.tartarus.org/
algorithms/english/stemmer.html
a maximum. A typical topic cluster contains several
hundred speech documents, while some of the larger
topic clusters contain several thousand.
2.3 Committee Membership Information
The committee membership information that we
used in the experiments is from Stewart and
Woon?s committee assignment codebook (Stewart
and Woon, 2005). This provided us with a roster
for each committee and rank and seniority informa-
tion for each member. In our experiments we use
the rank within party and committee seniority mem-
ber attributes to test the output of our pipeline. The
rank within party attribute orders the members of a
committee based on the Resolution that appointed
the members with the highest ranking members hav-
ing the lowest number. The chair and ranking mem-
bers always receive a rank of 1 within their party. A
committee member?s committee seniority attribute
corresponds to the number of years that the member
has served on the given committee.
2.4 Mapping Topics to Committees
In order to test our hypothesis that lexical centrality
is correlated with the natural committee rankings,
we needed a map from topics to related commit-
tees. We based our mapping on Senate Rule XXV,3
which defines the committees, and the descriptions
on committee home pages. Table 1 shows the map,
where a topic?s related committees are listed in ital-
ics below the topic name. Because we are matching
short topic names to the complex descriptions given
by Rule XXV, the topic-committee map is not one
to one or even particularly well defined: some top-
ics are mapped to multiple committees, some top-
ics are not mapped to any committees, and two dif-
ferent topics may be mapped to the same commit-
tee. This is not a major problem because even if a
one to one map between topics and committees ex-
isted, speakers from outside a topic?s related com-
mittee are free to participate in the topic simply by
giving a speech. Therefore there is no way to rank
all speakers in a topic using committee information.
To test our hypotheses, we focused our attention on
topics that have at least one related committee. In
Section 4.3 we describe how the MavenRank scores
3http://rules.senate.gov/senaterules/
rule25.php
660
<?xml version="1.0" standalone="no"?>
<!DOCTYPE RECORD SYSTEM "record.dtd">
<RECORD>
<HEADER>
<CHAMBER>Senate</CHAMBER>
<TITLE>NOMINATION OF JOEL KLEIN TO BE ASSISTANT ATTORNEY
GENERAL IN CHARGE OF THE ANTITRUST DIVISION </TITLE>
<DATE>19970714</DATE>
</HEADER>
<BODY>
<GRAF>
<PAGEREF></PAGEREF>
<SPEAKER>NULL</SPEAKER>
<NONSPEECH>NOMINATION OF JOEL KLEIN TO BE ASSISTANT
ATTORNEY GENERAL IN CHARGE OF THE ANTITRUST DIVISION
(Senate - July 14, 1997)</NONSPEECH>
</GRAF>
<GRAF>
<PAGEREF>S7413</PAGEREF>
<SPEAKER>15703</SPEAKER>
<SPEECH> Mr. President, as the ranking Democrat on the
Antitrust Subcommittee, let me tell you why I support Mr.
Klein?s nomination, why he is a good choice for the job,
and why we ought to confirm him today.
</SPEECH>
</GRAF>
. . .
<GRAF>
<PAGEREF>S7414</PAGEREF>
<SPEAKER>UNK1</SPEAKER>
<SPEECH> Without objection, it is so ordered. </SPEECH>
</GRAF>
</BODY>
</RECORD>
mr presid a the rank democrat on the antitrust subcommitte
let me tell you why i support mr klein nomin why he i a
good choic for the job and why we ought to confirm him
todai
first joel klein i an accomplish lawyer with a distinguish
career he graduat from columbia univers and harvard law
school and clerk for the u court of appeal here in
washington then for justic powel just a importantli he i
the presid choic to head the antitrust divis and i believ
that ani presid democrat or republican i entitl to a strong
presumpt in favor of hi execut branch nomine second joel
klein i a pragmatist not an idealogu hi answer at hi confirm
hear suggest that he i not antibusi a some would claim the
antitrust divis wa in the late 1970 nor anticonsum a some
argu the divis wa dure the 1980 instead he will plot a middl
cours i believ that promot free market fair competit and
consum welfar
the third reason we should confirm joel klein i becaus no on
deserv to linger in thi type of legisl limbo here in congress
we need the input of a confirm head of the antitrust divis
to give u the administr view on a varieti of import polici
matter defens consolid electr deregul and telecommun merger
among other we need someon who can speak with author for the
divis without a cloud hang over hi head
more than that without a confirm leader moral at the
antitrust divis i suffer and given the pace at which the
presid ha nomin and the senat ha confirm appointe if we fail
to approv mr klein it will be at least a year befor we confirm
a replac mayb longer and mayb never so we need to act now we
can?t afford to let the antitrust divis continu to drift
final mr presid i have great respect for the senat from south
carolina a well a the senat from nebraska and north dakota
thei have been forc advoc for consum on telecommun matter and
. . .
Figure 1: A sample of the text from record 105.sen.19970714.006.xml and the speech document for Senator
Herb Kohl of Wisconsin (id 15703) generated from it. The ?. . . ? represents omitted text.
1 Judicial Nominations 15 Health 2 (Economics - Seniors) 27 Procedural 1 (Housekeeping 1)
Judiciary Health, Education, Labor, and Pensions 28 Procedural 2 (Housekeeping 2)
2 Law & Crime 1 (Violence / Drugs) Veterans? Affairs 29 Campaign Finance
Judiciary Agriculture, Nutrition, and Forestry Rules and Administration
3 Banking / Finance Aging (Special Committee) 30 Law & Crime 2 (Federal)
Banking, Housing, and Urban Affairs Finance Judiciary
4 Armed Forces 1 (Manpower) 16 Gordon Smith re Hate Crime 31 Child Protection
Armed Services 17 Debt / Deficit / Social Security Health, Education, Labor, and Pensions
5 Armed Forces 2 (Infrastructure) Appropriations Agriculture, Nutrition, and Forestry
Armed Services Budget 32 Labor 1 (Workers, esp. Retirement)
6 Symbolic (Tribute - Living) Finance Health, Education, Labor, and Pensions
7 Symbolic (Congratulations - Sports) Aging (Special Committee) Aging (Special Committee)
8 Energy 18 Supreme Court / Constitutional Small Business and Entrepreneurship
Energy and Natural Resources Judiciary 33 Environment 2 (Regulation)
9 Defense (Use of Force) 19 Commercial Infrastructure Environment and Public Works
Armed Services Commerce, Science, and Transportation Agriculture, Nutrition, and Forestry
Homeland Security and Governmental Affairs 20 Symbolic (Remembrance - Military) Energy and Natural Resources
Intelligence (Select Committee) 21 International Affairs (Diplomacy) 34 Procedural 3 (Legislation 1)
10 Jesse Helms re Debt Foreign Relations 35 Procedural 4 (Legislation 2)
11 Environment 1 (Public Lands) 22 Abortion 36 Procedural 5 (Housekeeping 3)
Energy and Natural Resources Judiciary 37 Procedural 6 (Housekeeping 4)
Agriculture, Nutrition, and Forestry Health, Education, Labor, and Pensions 38 Taxes
12 Health 1 (Medical) 23 Symbolic (Tribute - Constituent) Finance
Health, Education, Labor, and Pensions 24 Agriculture 39 Symbolic (Remembrance - Nonmilitary)
13 International Affairs (Arms Control) Agriculture, Nutrition, and Forestry 40 Labor 2 (Employment)
Foreign Relations 25 Intelligence Health, Education, Labor, and Pensions
14 Social Welfare Intelligence (Select Committee) Small Business and Entrepreneurship
Agriculture, Nutrition, and Forestry Homeland Security and Governmental Affairs 41 Foreign Trade
Banking, Housing, and Urban Affairs 26 Health 3 (Economics - General) Finance
Health, Education, Labor, and Pensions Health, Education, Labor, and Pensions Banking, Housing, and Urban Affairs
Finance Finance 42 Education
Health, Education, Labor, and Pensions
Table 1: The numbers and names of the 42 topics from (Quinn et al, 2006) with our mappings to related
committees (listed below the topic name, if available).
661
of speakers who are not members of related commit-
tees were taken into account when we measured the
rank correlations.
3 MavenRank and Lexical Similarity
The following sections describe MavenRank, a mea-
sure of speaker centrality, and tf-idf cosine similar-
ity, which is used to measure the lexical similarity of
speeches.
3.1 MavenRank
MavenRank is a graph-based method for finding
speaker centrality. It is similar to the methods
in (Erkan and Radev, 2004; Mihalcea and Tarau,
2004; Kurland and Lee, 2005), which can be used
for ranking sentences in extractive summaries and
documents in an information retrieval system. Given
a collection of speeches s1, . . . , sN and a measure
of lexical similarity between pairs sim(si, sj) ? 0,
a similarity graph can be constructed. The nodes
of the graph represent the speeches and a weighted
similarity edge is placed between pairs that exceed
a similarity threshold smin. MavenRank is based on
the premise that important speakers will have cen-
tral speeches in the graph, and that central speeches
should be similar to other central speeches. A recur-
sive explanation of this concept is that the score of
a speech should be proportional to the scores of its
similar neighbors.
Given a speech s in the graph, we can express the
recursive definition of its score p(s) as
p(s) =
?
t?adj[s]
p(t)
wdeg(t)
(1)
where adj[s] is the set of all speeches adjacent to
s and wdeg(t) =
?
u?adj[t] sim(t, u), the weighted
degree of t. Equation (1) captures the idea that the
MavenRank score of a speech is distributed to its
neighbors. We can rewrite this using matrix notation
as
p = pB (2)
where p = (p(s1), p(s2), . . . , p(sN )) and the ma-
trixB is the row normalized similarity matrix of the
graph
B(i, j) =
S(i, j)
?
k S(i, k)
(3)
where S(i, j) = sim(si, sj). Equation (2) shows
that the vector of MavenRank scores p is the left
eigenvector of B with eigenvalue 1.
We can prove that the eigenvector p exists by us-
ing a techinque from (Page et al, 1999). We can
treat the matrix B as a Markov chain describing
the transition probabilities of a random walk on the
speech similarity graph. The vector p then repre-
sents the stationary distribution of the random walk.
It is possible that some parts of the graph are dis-
connected or that the walk gets trapped in a com-
ponent. These problems are solved by reserving
a small escape probability at each node that repre-
sents a chance of jumping to any node in the graph,
making the Markov chain irreducible and aperiodic,
which guarantees the existence of the eigenvector.
Assuming a uniform escape probability for each
node on the graph, we can rewrite Equation (2) as
p = p[dU+ (1? d)B] (4)
where U is a square matrix with U(i, j) = 1/N
for all i and j, N is the number of nodes, and
d is the escape probability chosen in the interval
[0.1, 0.2] (Brin and Page, 1998). Equation (4) is
known as PageRank (Page et al, 1999) and is used
for determining prestige on the web in the Google
search engine.
3.2 Lexical Similarity
In our experiments, we used tf-idf cosine similarity
to measure lexical similarity between speech docu-
ments. We represent each speech document as a vec-
tor of term frequencies (or tf), which are weighted
according to the relative importance of the given
term in the cluster. The terms are weighted by their
inverse document frequency or idf. The idf of a term
w is given by (Sparck-Jones, 1972)
idf(w) = log
(
N
nw
)
(5)
where N is the number of documents in the corpus
and nw is the number of documents in the corpus
containing the term w. It follows that very common
words like ?of? or ?the? have a very low idf, while
the idf values of rare words are higher. In our experi-
ments, we calculated the idf values for each topic us-
ing all speech documents across sessions within the
662
 20
 30
 40
 50
 60
 70
 80
 90
 100
Abortion ChildProtection Education Workers,Retirement
SantorumBoxerKennedy
Figure 2: MavenRank percentiles for three speakers
over four topics.
given topic. We calculated topic-specific idf values
because some words may be relatively unimportant
in one topic, but important in another. For example,
in topic 22 (?Abortion?), the idf of the term ?abort?
is near 0.20, while in topic 38 (?Taxes?), its idf is
near 7.18.
The tf-idf cosine similarity measure
tf-idf-cosine(u, v) is defined as
P
w?u,v tfu(w) tfv(w) idf(w)
2
?P
w?u(tfu(w) idf(w))
2
?P
w?v(tfv(w) idf(w))
2
, (6)
which is the cosine of the angle between the tf-idf
vectors.
There are other alternatives to tf-idf cosine sim-
ilarity. Some other possible similarity measures
are document edit distance, the language models
from (Kurland and Lee, 2005), or generation proba-
bilities from (Erkan, 2006). For simplicity, we only
used tf-idf similarities in our experiments, but any of
these measures could be used in this case.
4 Experiments and Results
4.1 Data
We used the topic clusters from the 105th Senate
as training data to adjust the parameter smin and
observe trends in the data. We did not run experi-
ments to test the effect of different values of smin on
MavenRank scores, but our chosen value of 0.25 has
shown to give acceptable results in similar experi-
ments (Erkan and Radev, 2004). We used the topic
clusters from the 106th Senate as test data. For the
speech document networks, there was an average of
351 nodes (speech documents) and 2142 edges per
topic. For the speaker document networks, there was
an average of 63 nodes (speakers) and 545 edges per
topic.
4.2 Experimental Setup
We set up a pipeline using a Perl implementation
of tf-idf cosine similarity and MavenRank. We ran
MavenRank on the topic clusters and ranked the
speakers based on the output. We used two different
types granularities of the graphs as input: one where
the nodes are speech documents and another where
the nodes are speaker documents (see Section 2.1).
For the speech document graph, a speaker?s score is
determined by the sum of the MavenRank scores of
the speeches given by that speaker.
4.3 Evaluation Methods
To evaluate our output, we estimate independent
ordinary least squares linear regression models of
MavenRank centrality for topics with at least one re-
lated committee (there are 29 total):
MavenRankik = ?0k + ?skSeniorityik +
+?rkRankingMemberjk + ik (7)
where i indexes Senators, k indexes topics,
Seniorityik is the number of years Senator i has
served on the relevant committee for topic k (value
zero for those not on a relevant committee) and
RankingMemberjk has the value of one only for
the Chair and ranking minority member of a rele-
vant committee. We are interested primarily in the
overall significance of the estimated model (indicat-
ing committee effects) and, secondarily, in the spe-
cific source of any committee effect in seniority or
committee rank.
4.4 Results
Table 2 summarizes the results. ?Maven? status on
most topics does appear to be driven by committee
status, as expected. There are particularly strong ef-
fects of seniority and rank in topics tied to the Judi-
ciary, Foreign Relations, and Armed Services com-
mittees, as well as legislation-rich areas of domestic
policy. Perhaps of greater interest are the topics that
do not have committee effects. These are of three
distinct types. The first are highly politicized top-
ics for which speeches are intended not to influence
663
Topic p(F )a p(?s > 0)b p(?r > 0)c Topic p(F ) p(?s > 0) p(?r > 0)
Seniority and Ranking Status Both Significant Seniority and Ranking Status Jointly Significant
2 Law & Crime 1 [Violent] < .001 0.016 < .001 26 Health 3 [Economics] 0.001 0.106 0.064
18 Constitutional < .001 0.003 < .001 32 Labor 1 [Workers] 0.007 0.156 0.181
33 Environment 2 [Regulation] 0.007 0.063 0.056
Seniority Significant 3 Banking / Finance 0.042 0.141 0.579
12 Health 1 [Medical] < .001 < .001 0.567
42 Education < .001 < .001 0.337 No Significant Effects of Committee Status
41 Trade < .001 < .001 0.087 11 Environment 1 [Public Lands] 0.104 0.102 0.565
21 Int?l Affairs [Nonmilitary] < .001 0.007 0.338 22 Abortion 0.419 0.609 0.252
9 Defense [Use of Force] 0.002 0.001 0.926 5 Armed Forces 2 [Infrastructure] 0.479 0.267 0.919
19 Commercial Infrastructure 0.007 0.032 0.332 24 Agriculture 0.496 0.643 0.425
40 Labor 2 [Employment] 0.029 0.010 0.114 17 Debt / Social Security 0.502 0.905 0.295
38 Taxes 0.037 0.033 0.895 15 Health 2 [Seniors] 0.706 0.502 0.922
25 Intelligence 0.735 0.489 0.834
Ranking Status Significant 29 Campaign Finance 0.814 0.748 0.560
30 Crime 2 [Federal] < .001 0.334 < .001 31 Child Protection 0.856 0.580 0.718
8 Energy < .001 0.145 < .001
1 Judicial Nominations < .001 0.668 < .001
14 Social Welfare < .001 0.072 0.005
13 Int?l Affairs [Arms] < .001 0.759 0.001
4 Armed Forces 1 [Manpower] 0.007 0.180 0.049
aF-test for joint significance of committee variables.
bT-test for significance of committee seniority.
cT-test for significance of chair or ranking member status.
Table 2: Significance tests for ordinary least squares (OLS) linear regressions ofMavenRank scores (Speech-
documents graph) on committee seniority (in years) and ranking status (chair or ranking member), 106th
Senate, topic-by-topic. Results for the speaker-documents graph are similar.
legislation as much as indicate an ideological or par-
tisan position, so the mavens are not on particular
committees (abortion, children, seniors, the econ-
omy). The second are ?distributive politics? topics
where many Senators speak to defend state or re-
gional interests, so debate is broadly distributed and
there are no clear mavens (agriculture, military base
closures, public lands). Third are topics where there
are not enough speeches for clear results, because
most debate occurred after 1999-2000 (post-9/11
intelligence reform, McCain-Feingold campaign fi-
nance reform).
Alternative models, using measures of centrality
based on the centroid were also examined. Dis-
tance to centroid provides broadly similar results as
MavenRank, with several marginal significance re-
sults reversed in each direction. Cosine similarity
with centroid, on the other hand, appears to have no
relationship with committee structure.
Figure 2 shows the MavenRank percentiles (us-
ing the speech document network) for Senators Rick
Santorum, Barbara Boxer, and Edward Kennedy
across a few topics in the 106th Senate. These
sample scores conform to the expected rankings for
these speakers. In this session, Santorum was the
sponsor of a bill to ban partial birth abortions and
was a spokesman for Social Security reform, which
support his high ranking in abortion and work-
ers/retirement. Boxer acted as the lead opposition
to Santorum?s abortion bill and is known for her
support of child abuse laws. Kennedy was ranking
member of the Health, Education, Labor, and Pen-
sions committee and the Judiciary committee (which
was involved with the abortion bill).
4.5 MavenRank in Other Contexts
MavenRank is a general method for finding central
speakers in a discussion and can be applied to areas
outside of political science. One potential applica-
tion would be analyzing blog posts to find ?Maven?
bloggers by treating blogs as speakers and posts as
speeches. Similarly, MavenRank could be used to
find central participants in a newsgroup, a forum, or
a collection of email conversations.
5 Conclusion
We have presented a technique for identifying lexi-
cally central speakers using a graph based method
called MavenRank. To test our method for find-
ing central speakers, we analyzed the Congressional
664
Record by creating a map from the clusters of
speeches to Senate committees and comparing the
natural ranking committee members to the output of
MavenRank. We found evidence of a possible rela-
tionship between the lexical centrality and commit-
tee rank of a speaker by ranking the speeches us-
ing MavenRank and computing the rank correlation
with the natural ordering of speakers. Some spe-
cific committees disagreed with our hypothesis that
MavenRank and committee position are correlated,
which we propose is because of the non-legislative
aspects of those specific committees. The results
of our experiment suggest that MavenRank can in-
deed be used to find central speakers in a corpus of
speeches.
We are currently working on applying our meth-
ods to the US House of Representatives and other
records of parliamentary speech from the United
Kingdom and Australia. We have also developed a
dynamic version of MavenRank that takes time into
account when finding lexical centrality and plan on
using it with the various parliamentary records. We
are interested in dynamic MavenRank to go further
with the idea of tracking how ideas get propagated
through a network of debates, including congres-
sional records, blogs, and newsgroups.
Acknowledgments
This paper is based upon work supported by
the National Science Foundation under Grant No.
0527513, ?DHB: The dynamics of Political Rep-
resentation and Political Rhetoric?. Any opinions,
findings, and conclusions or recommendations ex-
pressed in this paper are those of the authors and do
not necessarily reflect the views of the National Sci-
ence Foundation.
References
David Blei and John Lafferty. 2006. Dynamic topic
models. In Machine Learning: Proceedings of the
Twenty-Third International Conference (ICML).
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. Journal of Machine
Learning Research, 3:993?1022.
Sergey Brin and Lawrence Page. 1998. The anatomy of
a large-scale hypertextual Web search engine. Com-
puter Networks and ISDN Systems, 30(1?7):107?117.
Gu?nes? Erkan and Dragomir R. Radev. 2004. Lexrank:
Graph-based centrality as salience in text summa-
rization. Journal of Artificial Intelligence Research
(JAIR).
Gunes Erkan. 2006. Language model-based document
clustering using random walks. In Proceedings of
the Human Language Technology Conference of the
NAACL, Main Conference, pages 479?486, New York
City, USA, June. Association for Computational Lin-
guistics.
L. C. Freeman. 1977. A set of measures of central-
ity based on betweenness. Sociometry, 40(1):35?41,
March.
Malcolm Gladwell. 2002. The Tipping Point: How Little
Things Can Make a Big Difference. Back Bay Books,
January.
Richard L. Hall. 1996. Participation in Congress. Yale
University Press.
Jon M. Kleinberg. 1998. Authoritative sources in a hy-
perlinked environment. In Proceedings of the 9th An-
nual ACM-SIAM Symposium on Discrete Algorithms,
pages 668?677.
Oren Kurland and Lillian Lee. 2005. PageRank without
hyperlinks: Structural re-ranking using links induced
by language models. In Proceedings of SIGIR, pages
306?313.
Oren Kurland and Lillian Lee. 2006. Respect my author-
ity! HITS without hyperlinks, utilizing cluster-based
language models. In Proceedings of SIGIR, pages 83?
90.
Ziheng Lin and Min-Yen Kan. 2007. Timestamped
graphs: Evolutionary models of text for multi-
document summarization. In Proceedings of the Sec-
ond Workshop on TextGraphs: Graph-Based Algo-
rithms for Natural Language Processing, pages 25?32,
Rochester, NY, USA. Association for Computational
Linguistics.
Geoffrey McLachlan and David Peel. 2000. Finite Mix-
ture Models. New York: Wiley.
Rada Mihalcea and Paul Tarau. 2004. TextRank: Bring-
ing order into texts. In Proceedings of the Ninth Con-
ference on Empirical Methods in Natural Language
Processing (EMNLP ?04).
Rada Mihalcea, Paul Tarau, and Elizabeth Figa. 2004.
Pagerank on semantic networks, with application to
word sense disambiguation. In Proceedings of the
Twentieth International Conference on Computational
Linguistics (COLING ?04), pages 1126?1132.
665
Burt L. Monroe, Cheryl L. Monroe, Kevin M. Quinn,
Dragomir Radev, Michael H. Crespin, Michael P. Co-
laresi, Anthony Fader, Jacob Balazer, and Steven P.
Abney. 2006. United states congressional speech cor-
pus. Department of Political Science, The Pennsylva-
nia State University.
Mark E. J. Newman. 2003. A measure of betweenness
centrality based on random walks. Technical Report
cond-mat/0309045, Arxiv.org.
Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry
Winograd. 1999. The PageRank citation ranking:
Bringing order to the Web. Technical Report 1999-66,
Stanford Digital Library Technologies Project, Stan-
ford University, November 11,.
Mason A. Porter, Peter J. Mucha, M. E. J. Newman, and
Casey M. Warmbrand. 2005. A network analysis of
committees in the u.s. house of representatives. PNAS,
102(20), May.
Kevin M. Quinn, Burt L. Monroe, Michael Colaresi,
Michael H. Crespin, and Dragomir R. Radev. 2006.
An automated method of topic-coding legislative
speech over time with application to the 105th-108th
U.S. senate. In Midwest Political Science Association
Meeting.
K. Sparck-Jones. 1972. A statistical interpretation of
term specificity and its application in retrieval. Jour-
nal of Documentation, 28(1):11?20.
Charles Stewart and Jonathan Woon. 2005. Con-
gressional committee assignments, 103rd to 105th
congresses, 1993?1998: Senate, july 12, 2005.
http://web.mit.edu/17.251/www/
data_page.html.
Matt Thomas, Bo Pang, and Lillian Lee. 2006. Get
out the vote: Determining support or opposition from
Congressional floor-debate transcripts. In Proceed-
ings of EMNLP, pages 327?335.
Hanghang Tong and Christos Faloutsos. 2006. Center-
piece subgraphs: problem definition and fast solutions.
In Tina Eliassi-Rad, Lyle H. Ungar, Mark Craven, and
Dimitrios Gunopulos, editors, KDD, pages 404?413.
ACM.
Xuerui Wang, Natasha Mohanty, and Andrew McCallum.
2005. Group and topic discovery from relations and
their attributes. In NIPS.
666
