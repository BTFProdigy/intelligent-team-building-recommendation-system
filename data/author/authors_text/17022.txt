Proceedings of the 4th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 81?86,
Atlanta, Georgia, 14 June 2013. c?2013 Association for Computational Linguistics
From newspaper to microblogging: What does it take to find opinions?
Wladimir Sidorenko and Jonathan Sonntag and Manfred Stede
Applied Computational Linguistics
University of Potsdam/Germany
sidarenk|sonntag|stede@uni-potsdam.de
Nina Kru?ger and Stefan Stieglitz
Dept. of Information Systems
University of Mu?nster/Germany
nina.krueger|stefan.stieglitz@uni-muenster.de
Abstract
We compare the performance of two lexicon-
based sentiment systems ? SentiStrength
(Thelwall et al, 2012) and SO-CAL (Taboada
et al, 2011) ? on the two genres of newspaper
text and tweets. While SentiStrength has been
geared specifically toward short social-media
text, SO-CAL was built for general, longer
text. After the initial comparison, we suc-
cessively enrich the SO-CAL-based analysis
with tweet-specific mechanisms and observe
that in some cases, this improves the perfor-
mance. A qualitative error analysis then iden-
tifies classes of typical problems the two sys-
tems have with tweets.
1 Introduction: Twitter, SentiStrength and
SO-CAL
In recent years, microblogging has been an attrac-
tive new target for sentiment analysis. The question
studied in this paper is how the methods used for
?standard? newspaper text can be transferred to mi-
croblogs. We focused on the Twitter network be-
cause of its widespread use, and because Twitter
communication, in response to emerging issues, is
fast and especially ad hoc, making it an effective
platform for the sharing and discussion of crisis-
related information (Bruns/Burgess, 2011). Further-
more, Twitter is characterized by a high topicality of
content (Milstein al., 2008).
Specifically, we present experiments involving
two sentiment analysis systems that both employ
a combination of polarity lexicon and sentiment
composition rules: (i) SentiStrength (Thelwall et
al., 2012), a system that is geared toward short
social-media text, and (ii) SO-CAL (Taboada et al,
2011), ?Semantic Orientation Calculator?, a general-
purpose system that was designed primarily to work
on the level of complete texts. While both are
lexicon-based approaches, there are certain differ-
ences in the roles of the various submodules. For our
purposes here, it is important that SentiStrength was
designed to cope specifically with ?user-generated
content?. Among the features of the system, as
stated by Thelwall et al, the following four are espe-
cially important for tweets: (i) a simple spelling cor-
rection algorithm deletes repeated letters when the
word is not found in the dictionary; (ii) repeated let-
ters lead to a boost in sentiment value; (iii) an emoti-
con list supplements the polarity lexicon; (iv) pos-
itive sentences ending in an exclamation mark re-
ceive an additional boost, and multiple exclamation
marks further strengthen the polarity.
SO-CAL, on the other hand, does not include
social-media-specific measures. In contrast, it was
designed for determining semantic orientation on
the text level; in our experiments here, we are thus
using it for the non-intended purpose of sentence-
level sentiment, on tweet ?sentences?.
Next, we review related work on twitter sentiment
analysis (Section 2), and describe the data sets for
our experiments in Section 3. Then we investigate
the relative performance of SentiStrength and SO-
CAL on newspaper text and on tweets (Section 4),
including experiments with preprocessing steps. In
Section 5, we present observations from a qualitative
evaluation, and we interpret the results and conclude
in Section 6.
81
2 Related work
Following the work on ?standard? text, sentiment
classification on tweets is often treated as a two-step
task, e.g., (Barbosa/Feng, 2010): subjectivity classi-
fication followed by polarity classification. For sub-
jectivity classification, (Pak/Paroubek, 2010) found
that the distribution of POS tags is a useful feature,
due to, for example, the presence of modal verbs in
subjective tweets.
For polarity assignment, one approach is to au-
tomatically build large sets of training data and
then train classifiers on token n-grams; in this vein,
(Pak/Paroubek, 2010) found that in their approach,
bigrams outperform unigrams and trigrams, and
they report f-measures around 0.6 for the three-
way pos/neg/neutral classification. The other, non-
learning, approach is to rely on a polarity wordlist
(or a collection of several, as in (Joshi et al, 2011;
Mukherjee et al, 2012)). Mukherjee et al report
an accuracy of 66.69% for pos/neg, and 56.17% for
pos/neg/neut classification.
Typical preprocessing steps employed by the
approaches discussed are the correction of mis-
spellings, the replacement of URLs and hashtags,
the translation of emoticons and of slang words.
Sometimes, stop word removal and stemming is
used; sometimes deliberately not. Few authors eval-
uate the influence of the various measures; one ex-
ception is (Mukherjee et al, 2012).
A recent branch of research deals with fine-
grained target-specific analysis (as proposed re-
cently by (Jiang et al, 2011)). In our work, how-
ever, we tackle the more coarse-grained problem
of assigning a single sentiment value to a complete
tweet. However, we will return to the issue of target-
specificity in our conclusions.
An interesting result from analysing the state of
the art is that apparently no consensus has been
reached yet on the question of ?extra difficulty? of
tweet sentiment analysis. While everybody agrees
that tweets are noisy and can pose considerable diffi-
culty to any standard linguistically-inspired analysis
tool, it is not clear to what extent this is a problem
for sentiment analysis. Some authors argue that the
noise renders the task more difficult than the anal-
ysis of longer text, while others maintain that the
brevity of tweets is in fact an advantage, because ? as
(Bermingham/Smeaton, 2010) put it, ?the short doc-
ument length introduces a succinctness to the con-
tent?, and thus ?the focused nature of the text and
higher density of sentiment-bearing terms may ben-
efit automated sentiment analysis techniques.? In
their evaluation, the classification of microblogs in-
deed yields better results than that of blogs.
In correspondence with this open question, there
are only few investigations so far on the performance
differences for existing sentiment tools operating on
newspaper versus social media text. To shed more
light on the issue, we chose to run a set of com-
parative experiments with the two aforementioned
lexicon/rule-based systems, on both newspaper and
twitter corpora.
3 Data sets
MPQA The well-known MPQA corpus1 (Wiebe
et al, 2005) of newspaper text has fine-grained an-
notations of ?private states? at phrase level. For our
purposes these need to be reduced to a more coarse-
grained labelling of sentence-level sentiment. To
avoid ambiguity, we ignored those sentences that in-
clude both positive and negative sentiment annota-
tions. From the remaining sentences, we selected
100 positive and negative sentences each, where the
former target-specific sentiment is now taken to rep-
resent sentence-level sentiment. The data set is a
difficult one, given that we are dealing with isolated
sentences from newspaper reports.
Qantas To track Twitter data we used a self-
developed prototype (see (Stieglitz/Kaufhold,
2011)). We concentrate our analysis on Qantas, an
Australian leading carrier for long-haul air travel,
for which we assume substantial interest in public
communication. We furthermore expect that ?
caused by some management crises in 2011 ? online
communication around Qantas-related topics is
characterized by a strong emotional investment of
stakeholders.
The tracking tool captures all those tweets that
contain the keyword ?Qantas? in their content, in the
username of the sender, or in a URL. After spam re-
moval, we had a dataset of some 27,000 tweets, col-
lected between mid-May and mid-November 2011.
1http://mpqa.cs.pitt.edu/
82
Topic #pos #neut #neg #irrelevant
Apple 219 581 377 164
Google 218 604 61 498
Microsoft 93 671 138 513
Twitter 68 647 78 611
Table 1: Distribution of tweets and labels across subcor-
pora
For evaluation purposes, 300 Tweets have been man-
ually annotated by two annotators in parallel, using
a polarity scale ranging from -2 to 2. 190 Tweets of
those (63%) received identical labels, and we used
only this set in our experiments described below.
That means we also discarded cases of ?minor? dis-
agreement such as a -1/-2 annotation.
Sanders The Sanders corpus2 is a corpus consist-
ing of 5513 tweets of various languages which have
been annotated for sentiment. The tweets have been
sampled by the search terms ,,@apple?, ,,#google?,
,,#microsoft? and ,,#twitter?. Each tweet is accom-
panied by a date-time stamp and the target of its po-
larity. Possible polarity values are positive, negative,
neutral (simple factual statements / questions with-
out strong emotions / neither positive nor negative /
both positive and negative), and irrelevant (spam /
non-English). The positive and negative tweets thus
contain judgements on the companies or their prod-
ucts/services. Along with the corpus comes an anno-
tation scheme and statistics about the corpus. Some
numbers of the size and distribution within the cor-
pus are given in Table 1.
According to the annotation guidelines, positive
and negative labels were only assigned to clear cases
of sentiment. Ambigious tweets have been anno-
tated as neutral.
4 Experiments and results
4.1 Performance on MPQA sentences
In order to establish a basis for the comparison, we
first ran a small comparative evaluation on ?stan-
dard? text, i.e., on the sentences from the MPQA
newspaper corpus. The results, given in Table 2,
show that both systems perform considerably better
2http://www.sananalytics.com/lab/
twitter-sentiment/
SentiStrength SO-CAL
acc pos 0.2727 0.4717
acc neg 0.7071 0.6542
weighted avg 0.4899 0.5634
Table 2: Accuracy on MPQA sentences
Senti- SO-CAL SO-CAL
Strength preproc.
Qantas
acc 0.3754 0.3953 0.3887
acc pos 0.3091 0.2545 0.2545
acc neg 0.2857 0.2857 0.2857
acc neut 0.6164 0.6781 0.6644
avg sentiment 1.1075 1.2756 1.3316
Sanders total
acc 0.5945 0.5899 0.5790
acc pos 0.6171 0.5694 0.6032
acc neg 0.4572 0.5301 0.5519
acc neut 0.6230 0.6092 0.5802
avg sentiment 0.8517 1.3761 1.5233
Sanders twitter
acc 0.4985 0.5804 0.5387
acc pos 0.4286 0.3750 0.4821
acc neg 0.4590 0.4754 0.5246
acc neut 0.5099 0.6121 0.5245
avg sentiment 0.8393 1.4054 1.6978
Table 3: Accuracy on tweet corpora
on negative than on positive sentences, and overall
there is a slight advantage for SO-CAL.
4.2 Performance on Qantas and Sanders tweets
In Table 3, we show the system performance on the
Twitter corpora: Qantas, the complete Sanders cor-
pus, and the Sanders subcorpus with target ?Twit-
ter?. We ran evaluations on all four separate sub-
corpora, but only ?Twitter? showed interesting dif-
ferences from the results for the total corpus, and
that is why they are included in the table. The ?acc?
row gives the overall weighted accuracy. ?Avg senti-
ment? is the absolute value of the sentiment strength
determined by SentiStrength and SO-CAL; notice
that these should not be compared between the two
systems, as they do not operate on the same scale.
(We will return to the role of sentiment strength in
Section 6.)
83
4.3 Preprocessing steps
Since SO-CAL was not intended for analyzing Twit-
ter data, we implemented three preprocessing steps
to study whether noise effects of this text genre can
be reduced. Similarly to the steps suggested by
(Mukherjee et al, 2012), we first unified all URLs,
e-mail addresses and user names by replacing them
with unique tokens. Additionally, in step 1 all hash
marks were stripped from words, and emoticons
were mapped to special tokens representing their
emotion categories. These special tokens were then
added to the polarity lexicons used by SO-CAL.
In step 2, social media specific slang expressions
and abbreviations like ?2 b? (for ?to be?) or ?im-
sry? (for ?I am sorry?) were translated to their ap-
propriate standard language forms. For this, we used
a dictionary of 5,424 expressions that we gathered
from publicly available resources.3
In the last step, we tackled two typical spelling
phenomena: the omission of final g in gerund forms
(goin), and elongations of characters (suuuper). For
the former, we appended the character g to words
ending with -in if these words are unknown to vo-
cabulary,4 while the corresponding ?g?-forms are in-
vocabulary words (IVW). For the latter problem,
we first tried to subsequently remove each repeat-
ing character until we hit an IVW. For cases re-
sisting this treatment, we adopted the method sug-
gested by (Brody/Diakopoulos, 2011) and generated
a squeezed form of the prolongated word, subse-
quently looking it up in a probability table that has
previously been gathered from a training corpus.
Altogether, SO-CAL does not benefit from pre-
processing in the Qantas corpus, but it does help for
the pos/neg tweets from the Sanders corpus, espe-
cially for the Twitter subcorpus. The observation
that the accuracy on neutral tweets decreases while
the average sentiment increases will be discussed
in Section 6. We also measured the effects of the
three individual steps in isolation, and the only note-
worthy result is that SentiStrength, when subjected
to our ?extra? preprocessing, benefits slightly from
slang normalization for the Qantas corpus, and from
3http://www.noslang.com/dictionary/,
http://onlineslangdictionary.com/, http:
//www.urbandictionary.com/
4For vocabulary check, we used the open Hunspell dictio-
nary (http://hunspell.sourceforge.net/).
noise cleaning for some parts of the Sanders corpus.
5 Qualitative evaluation
Having computed the success rates, we then per-
formed a small qualitative evaluation: What are the
main reasons for the misclassifications on tweets? In
addition, we wanted to know why the Qantas corpus
yielded much worse results than the Sanders corpus,
and thus we looked into its results.
5.1 Problems for SO-CAL
We chose SO-CAL?s judgements as the basis for this
evaluation and randomly selected 120 tweets from
the Sanders corpus that were not correctly classi-
fied. The distribution across the manual annotations
pos/neg/neut was 40/40/40.
In Table 4, we provide a classification of the rea-
sons for problems. The first group are cases where
we would not agree with the annotation and thus
cannot blame SO-CAL. The second group includes
problems that are beyond the scope of the system
and hence, strictly speaking, not its fault. Among the
typos, there are cases of misspelled opinion words,
but also a few where the typo leads to problems with
SO-CALs linguistic analysis and in consequence to
a misclassification. The slang words include items
like ?wow!? but also shorthands such as ?thx?. Most
important are ?domain formulae?: expressions that
require inferences in order to identify the sentiment.
An example is ?I now use X instead of TARGET?.
We encounter these most often in negative tweets,
where complaints are expressed, as in ?My phone
can send but not receive texts.?
In the third group, we find problems that are or
could be in the scope of SO-CAL. Occasionally,
negation or irrealis rules misfire. Gaps in the lex-
icon are noticeable especially on the positive side
(examples: ?loving?, ?better?, ?thanks to?). ?Lex-
ical ambiguity? refers to words that may or may
not carry polarity; by far the most frequent example
here is ?new?, which SO-CAL labels positive, but in
technology-related tweets often is neutral. Also in
neutral tweets, we often find high complexity, i.e.,
cases where both positive and negative judgements
are mixed. And finally, a fair number of problems
stems from sentiment expressed on the wrong target
of the tweet.
84
Problem Pos Neg Neut
Annotation ambig. 15% 0% 2%
Typo 3% 5% 10%
Slang words 12% 10% 0%
Sarcasm 0% 2% 0%
Domain formula 23% 60% 5%
Wrong rule 3% 5% 3%
Lexicon gap 30% 12% 0%
Lexical ambiguity 5% 5% 50%
Complexity 0% 0% 18%
Wrong target 8% 0% 12%
Table 4: SO-CAL error types on 120 Sanders tweets
Problem Pos Neg Neut
Annotation ambig. 45% 25% 12%
Typo 18% 0% 0%
Slang words 0% 0% 0%
Sarcasm 0% 16% 0%
Domain formula 9% 42% 4%
Wrong rule 9% 0% 10%
Lexicon gap 9% 16% 0%
Lexical ambiguity 0% 0% 16%
Complexity 9% 0% 16%
Spam / news 0% 0% 41%
Table 5: Error types on 75 Qantas tweets
5.2 Observations on the Qantas corpus
The analysis of 75 Qantas tweets that have been mis-
classified by both SentiStength and SO-CAL yielded
the results in Table 5: Again, many annotation cases
are ambiguous, and domain formulae are the ma-
jor problem with negative tweets. Sarcasm is much
more frequent than in the Sanders corpus. The cen-
tral problem for neutral tweets stems from the fact
that spam and tweets containing headlines and URLs
of news messages have been annotated as neutral,
but these may very well contain polarity-bearing
words, which are then detected by the systems.
6 Interpretation and Conlusions
News versus tweets. Since the Sanders corpus is
much larger than Qantas, we regard it as the tweet
representative for the comparison to MPQA (a dif-
ficult data set, as argued above). For positive text,
both SentiStrength and SO-CAL yield better re-
sults on tweets, while for negative texts, the results
on tweets are much lower than on news sentences.
Within the news genre, however, both systems per-
form much better on negative than on positive text.
So we conclude a ?polarity flip? in the performance
of both systems when going from news to tweets.
Differences among tweets. Based on the Sanders
corpus, the SentiStrength and SO-CAL results are
a little better than those reported by (Mukherjee et
al., 2012), who achieved 56.17% accuracy for the
three-way classification. As SO-CAL does not in-
clude tweet-specific analysis, we may conclude that
the utility of such genre-specific measures is in fact
limited. ? An interesting question is why the ?Twit-
ter? subcorpus of Sanders behaves so different from
the others: While overall accuracy is the same, the
figures for the three categories differ widely. Also,
SO-CAL here benefits heavily from preprocessing
on the non-neutral tweets. One factor is the large
proportion of neutral tweets (see Table 1); besides,
we find that these tweets are not as target-related as
those for Apple, Google, Microsoft; it seems that
users often drop a ?#twitter? without actually talking
about Twitter or its service.
Preprocessing. Of the four measures taken by
SentiStrength to account for tweet problems (see
Sct. 1), SO-CAL already implements the exclama-
tion mark boost; the other three were added in our
own preprocessing, but we did only minimal spell-
checking. Overall, SO-CAL does not profit as much
as we had expected, but we find a fair improvement
(0.57?0.6) for the positive Sanders tweets. For neu-
tral tweets, performance actually decreases.
The role of targets An interesting observation is
that adding preprocessing to SO-CAL leads to de-
tecting ?more? sentiment: The average sentiment
values increase for all the corpora in Table 3. At the
same time, the accuracy on neutral tweets decreases,
which indicates that ?spurious? sentiment is being
detected. The most likely reason is that SO-CAL in-
deed profits from tweet-preprocessing but then de-
tects sentiment that is unrelated to the target and
therefore not annotated in the gold data. An im-
portant direction for future work therefore is to pay
more attention to target-specific sentiment identifi-
cation, cf. (Jiang et al, 2011).
85
Acknowledgments
This work was funded by German Ministry for Edu-
cation and Research (BMBF), grant 01UG1232D.
References
L. Barbosa and J. Feng. 2010. Robust sentiment detec-
tion on twitter from biased and noisy data. Proc. of
COLING (Posters), Beijing.
A. Bermingham and A. Smeaton. 2010. Classifying
Sentiment in Microblogs: Is Brevity an Advantage?
Proc. of the 20th ACM Conference on Information and
Knowledge Management (CIKM), Toronto.
S. Brody and N. Diakopoulos. 2011.
Cooooooooooooooollllllllllllll!!!!!!!!!!!!!! Using
Word Lengthening to Detect Sentiment in Microblogs.
Proceedings of the 2011 Conference on Empirical
Methods in Natural Language Processing (EMNLP),
pp. 562?570, Edinburgh.
A. Bruns and J.E. Burgess. 2011. The Use of Twitter
Hashtags in the Formation of Ad Hoc Publics. 6th
European Consortium for Political Research General
Conference, Reykjavik, Iceland, pp. 25-27.
L. Jiang, M. Yu, M. Zhou, X. Liu and T. Zhao.
2011. Target-dependent twitter sentiment classifica-
tion. Proc. of the 49th Annual Meeting of the ACL,
pp. 151-160, Portland/OR.
A. Joshi, Balamurali A R, P. Bhattacharyya and R. Mo-
hanty. 2011. C-Feel-It: a sentiment analyzer for
micro-blogs. Proc. of the ACL-HLT 2011 System
Demonstrations, pp. 127-132, Portland/OR.
S. Milstein, A. Chowdhury, G. Hochmuth, B. Lorica and
R. Magoulas. 2008. Twitter and the Micro-Messaging
Revolution: Communication, Connections, and Imme-
diacy - 140 Characters at a Time.
S. Mukherjee, A. Malu, A.R. Balamurali and P. Bhat-
tacharyya. 2012. TwiSent: a multistage system for
analyzing sentiment in twitter. Proc. of the 21st ACM
Conference on Information and Knowledge Manage-
ment (CIKM).
A. Pak and P. Paroubek. 2010. Twitter as a corpus
for sentiment analysis and opinion mining. Proc. of
LREC, Valletta/Malta.
S. Stieglitz and C. Kaufhold. 2011. Automatic Full
Text Analysis in Public Social Media ? Adoption of
a Software Prototype to Investigate Political Commu-
nication. Proc. of the 2nd International Conference on
Ambient Systems, Networks and Technologies (ANT-
2011) / The 8th International Conference on Mobile
Web Information Systems (MobiWIS 2011), Procedia
Computer Science 5, Elsevier, 776-781.
M. Taboada, J. Brooke, M. Tofiloski, K. Voll, and M.
Stede. 2011. Lexicon-based methods for sentiment
analysis. Computational Linguistics, 37(2):267?307.
M. Thelwall, K. Buckley, and G. Paltoglou. 2012. Sen-
timent strength detection for the social Web. Journal
of the American Society for Information Science and
Technology, 63(1):163?173.
J. Wiebe, T. Wilson, and C. Cardie. 2005. Annotating ex-
pressions of opinions and emotions in language. Lan-
guage Resources and Evaluation, 39(2?3):165?210.
86
Proceedings of the 7th Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities, pages 55?64,
Sofia, Bulgaria, August 8 2013. c?2013 Association for Computational Linguistics
Towards a Tool for Interactive Concept Building for Large Scale Analysis
in the Humanities
Andre Blessing1 Jonathan Sonntag2 Fritz Kliche3
Ulrich Heid3 Jonas Kuhn1 Manfred Stede2
1Institute for Natural Language Processing
Universitaet Stuttgart, Germany
2Institute for Applied Computational Linguistics
University of Potsdam, Germany
3Institute for Information Science and Natural Language Processing
University of Hildesheim, Germany
Abstract
We develop a pipeline consisting of var-
ious text processing tools which is de-
signed to assist political scientists in find-
ing specific, complex concepts within
large amounts of text. Our main focus is
the interaction between the political scien-
tists and the natural language processing
groups to ensure a beneficial assistance for
the political scientists and new application
challenges for NLP. It is of particular im-
portance to find a ?common language? be-
tween the different disciplines. Therefore,
we use an interactive web-interface which
is easily usable by non-experts. It inter-
faces an active learning algorithm which
is complemented by the NLP pipeline to
provide a rich feature selection. Political
scientists are thus enabled to use their own
intuitions to find custom concepts.
1 Introduction
In this paper, we give examples of how NLP meth-
ods and tools can be used to provide support for
complex tasks in political sciences. Many con-
cepts of political science are complex and faceted;
they tend to come in different linguistic realiza-
tions, often in complex ones; many concepts are
not directly identifiable by means of (a small set
of) individual lexical items, but require some in-
terpretation.
Many researchers in political sciences either
work qualitatively on small amounts of data which
they interpret instance-wise, or, if they are in-
terested in quantitative trends, they use compara-
tively simple tools, such as keyword-based search
in corpora or text classification on the basis of
terms only; this latter approach may lead to im-
precise results due to a rather unspecific search as
well as semantically invalid or ambigious search
words. On the other hand, large amounts of e.g.
news texts are available, also over longer periods
of time, such that e.g. tendencies over time can
be derived. The corpora we are currently working
on contain ca. 700,000 articles from British, Irish,
German and Austrian newspapers, as well as (yet
unexplored) material in French.
Figure 1 depicts a simple example of a quantita-
tive analysis.1 The example shows how often two
terms, Friedensmission(?peace operation?), and
Auslandseinsatz(?foreign intervention?) are used
in the last two decades in newspaper texts about
interventions and wars. The long-term goal of the
project is to provide similar analysis for complex
concepts. An example of a complex concept is
the evocation of collective identities in political
contexts, as indirect in the news. Examples for
such collective identities are: the Europeans, the
French, the Catholics.
The objective of the work we are going to dis-
cuss in this paper is to provide NLP methods and
tools for assisting political scientists in the ex-
ploration of large data sets, with a view to both,
a detailed qualitative analysis of text instances,
and a quantitative overview of trends over time,
at the level of corpora. The examples discussed
here have to do with (possibly multiple) collective
identities. Typical context of such identities tend
to report communication, as direct or as indirect
speech. Examples of such contexts are given in 1.
(1) Die
The
Europa?er
Europeans
wu?rden
would
die
the
Lu?cke
gap
fu?llen,
fill,
1The figure shows a screenshot of our web-based
prototype.
55
Figure 1: The screenshot of our web-based system shows a simple quantitative analysis of the frequency
of two terms in news articles over time. While in the 90s the term Friedensmission (peace operation) was
predominant a reverse tendency can be observed since 2001 with Auslandseinsatz (foreign intervention)
being now frequently used.
sagte
said
Ru?he.
Ru?he.
,,The Europeans would fill the gap, Ru?he said.?
The tool support is meant to be semi-automatic,
as the automatic tools propose candidates that
need to be validated or refused by the political sci-
entists.
We combine a chain of corpus processing tools
with classifier-based tools, e.g. for topic clas-
sifiers, commentary/report classifiers, etc., make
the tools interoperable to ensure flexible data ex-
change and multiple usage scenarios, and we em-
bed the tool collection under a web (service) -
based user interface.
The remainder of this paper is structured as fol-
lows. In section 2, we present an outline of the ar-
chitecture of our tool collection, and we motivate
the architecture. Section 3 presents examples of
implemented modules, both from corpus process-
ing and search and retrieval of instances of com-
plex concepts. We also show how our tools are re-
lated to the infrastructural standards in use in the
CLARIN community. In section 4, we exemplify
the intended use of the methods with case studies
about steps necessary for identifying evocation:
being able to separate reports from comments, and
strategies for identifying indirect speech. Section
6 is devoted to a conclusion and to the discussion
of future work.
2 Project Goals
A collaboration between political scientists and
computational linguists necessarily involves find-
ing a common language in order to agree on
the precise objectives of a project. For exam-
ple, social scientists use the term codebook for
manual annotations of text, similar to annotation
schemes or guidelines in NLP. Both disciplines
share methodologies of interactive text analysis
which combine term based search, manual an-
notation and learning-based annotation of large
amounts of data. In this section, we give a brief
56
summary of the goals from the perspective of each
of the two disciplines, and then describe the text
corpus that is used in the project. Section 3 will
describe our approach to devising a system archi-
tecture that serves to realize the goals.
2.1 Social Science Research Issue
Given the complexity of the underlying research
issues (cf. Section 1) and the methodological tra-
dition of manual text coding by very well-trained
annotators in the social science and particular in
political science, our project does not aim at any
fully-automatic solution for empirical issues in po-
litical science. Instead, the goal is to provide as
much assistance to the human text analyst as possi-
ble, by means of a workbench that integrates many
tasks that otherwise would have to be carried out
with different software tools (e.g., corpus prepro-
cessing, KWIC searches, statistics). In our project,
the human analyst is concerned specifically with
manifestations of collective identities in newspa-
per texts on issues of war and military interven-
tions: who are the actors in political crisis man-
agement or conflict? How is this perspective of
responsible actors characterized in different news-
papers (with different political orientation; in dif-
ferent countries)? The analyst wants to find doc-
uments that contain facets of such constellations,
which requires search techniques involving con-
cepts on different levels of abstraction, ranging
from specific words or named entities (which may
appear with different names in different texts) to
event types (which may be realized with different
verb-argument configurations). Thus the text cor-
pus should be enriched with information relevant
to such queries, and the workbench shall provide
a comfortable interface for building such queries.
Moreover, various types and (possibly concurrent)
layers of human annotations have to complement
the automatic analysis, and the manual annota-
tion would benefit from automatic control of code-
book2 compliance and the convergence of coding
decisions.
2.2 Natural Language Processing Research
Issue
Large collections of text provide an excellent op-
portunity for computational linguists to scale their
methods. In the scenario of a project like ours, this
becomes especially challenging, because standard
2or, in NLP terms: annotation scheme.
automatic analysis components have to be com-
bined with manual annotation or interactive inter-
vention of the human analyst.
In addition to this principled challenge, there
may be more mundane issues resulting from pro-
cessing corpora whose origin stretches over many
years. In our case, the data collection phase coin-
cided with a spelling reform in German-speaking
countries. Many aspects of spelling changed twice
(in 1996 and in 2006), and thus it is the responsi-
bility of the NLP branch of the project to provide
an abstraction over such changes and to enable to-
day?s users to run a homogeneous search over the
texts using only the current spelling. While this
might be less important for generic web search ap-
plications, it is of great importance for our project,
where the overall objective is a combination of
quantitative and qualitative text analysis.
In our processing chain, we first need to harmo-
nize the data formats so that the processing tools
operate on a common format. Rather than defin-
ing these from scratch, we aim at compatibility
with the standardization efforts of CLARIN3 and
DARIAH4, two large language technology infras-
tructure projects in Europe that in particular target
eHumanities applications. One of the objectives
is to provide advanced tools to discover, explore,
exploit, annotate, analyse or combine textual re-
sources. In the next section we give more details
about how we interact which the CLARIN-D in-
frastructure (Boehlke et al, 2013).
3 Architecture
The main goal is to provide a web-based user-
interface to the social scientist to avoid any soft-
ware installation. Figure 2 presents the workflow
of the different processing steps in this project.
The first part considers format issues that occur
if documents from different sources are used. The
main challenge is to recognize metadata correctly.
Date and source name are two types of metadata
which are required for analyses in the social sci-
ences. But also the separation of document con-
tent (text) and metadata is important to ensure that
only real content is processed with the NLP meth-
ods. The results are stored in a repository which
uses a relational database as a back-end. All fur-
ther modules are used to add more annotations to
the textual data. First a complex linguistic pro-
3http://www.clarin.eu/
4http://www.dariah.eu/
57
cessing chain is used to provide state-of-the-art
corpus linguistic annotations (see Section 3.2 for
details). Then, to ensure that statistics over oc-
currence counts of words, word combinations and
constructions are valid and not blurred by the mul-
tiple presence of texts or text passages in the cor-
pus, we filter duplicates. Duplicates can occur
if our document set contains the same document
twice or if two documents are very similar, e.g.
they differ in only one sentence.
Raw documents
Repository:MetadataStructural dataTextual data Topic filter
Duplicate filter
Linguistic analysisSentence splitter Tokenizer
Web-basedUserinterface
Tagger ParserCoref NER
ImportExploration Workbench
Concept detection
Complex Concept Builder
Figure 2: Overview of the complete processing
chain.
We split the workflow for the user into two
parts: The first part is only used if the user im-
ports new data into the repository. For that he
can use the exploration workbench (Section 3.1).
Secondly, all steps for analyzing the data are done
with the Complex Concept Builder (Section 3.2).
3.1 Exploration Workbench
Formal corpus inhomogeneity (e.g. various data
formats and inconsistent data structures) are a ma-
jor issue for researchers working on text corpora.
The web-based ?Exploration Workbench? allows
for the creation of a consistent corpus from vari-
ous types of data and prepares data for further pro-
cessing with computational linguistic tools. The
workbench can interact with to existing computa-
tional linguistic infrastructure (e.g. CLARIN) and
provides input for the repository also used by the
Complex Concept Builder.
The workbench converts several input formats
(TXT, RTF, HTML) to a consistent XML repre-
sentation. The conversion tools account for differ-
ent file encodings and convert input files to Uni-
code (UTF-8). We currently work on newspa-
per articles wrapped with metadata. Text mining
components read out those metadata and identify
text content in the documents. Metadata appear
at varying positions and in diverse notations, e.g.
for dates, indications of authors or newspaper sec-
tions. The components account for these varia-
tions and convert them to a consistent machine
readable format. The extracted metadata are ap-
pended to the XML representation. The result-
ing XML is the starting point for further compu-
tational linguistic processing of the source docu-
ments.
The workbench contains a tool to identify text
duplicates and semi-duplicates via similarity mea-
sures of pairs of articles (Kantner et al, 2011).
The method is based on a comparison of 5-grams,
weighted by significance (tf-idf measure (Salton
and Buckley, 1988)). For a pair of documents it
yields a value on a ?similarity scale? ranging from
0 to 1. Values at medium range (0.4 to 0.8) are
considered semi-duplicates.
Data cleaning is important for the data-driven
studies. Not only duplicate articles have a nega-
tive impact, also articles which are not of interest
for the given topic have to be filtered out. There
are different approaches to classify articles into a
range of predefined topics. In the last years LDA
(Blei et al, 2003; Niekler and Ja?hnichen, 2012)
is one of the most successful methods to find top-
ics in articles. But for social scientists the cate-
gories typically used in LDA are not sufficient. We
follow the idea of Dualist (Settles, 2011; Settles
and Zhu, 2012) which is an interactive method for
classification. The architecture of Dualist is based
on MALLET (McCallum, 2002) which is easily
integrable into our architecture. Our goal is to
design the correct feature to find relevant articles
for a given topic. Word features are not sufficient
since we have to model more complex features (cf.
Section 2.1).
The workbench is not exclusively geared to the
data of the current project. We chose a modular
set-up of the tools of the workbench and provide
user-modifiable templates for the extraction of var-
ious kinds of metadata, in order to keep the work-
bench adaptable to new data and to develop tools
suitable for data beyond the scope of the current
corpus.
58
3.2 Complex Concept Builder
A central problem for political scientists who in-
tend to work on large corpora is the linguistic va-
riety in the expression of technical terms and com-
plex concepts. An editorial or a politician cited
in a news item can mobilize a collective identity
which can be construed from e.g. regional or so-
cial affiliation, nationality or religion. A reason-
able goal in the context of the search for collec-
tive identity evocation contexts is therefore to find
all texts which (possibly) contain collective iden-
tities. Moreover, while we are training our inter-
active tools on a corpus on wars and military in-
terventions the same collective identities might be
expressed in different ways in a corpus i.e. on the
Eurocrisis.
From a computational point of view, many dif-
ferent tools need to be joined to detect interest-
ing texts. An example application could be a case
where a political scientist intends to extract news-
paper articles that cite a politician who tries to
rally support for his political party. In order to
detect such text, we need a system to identify di-
rect and indirect speech and a sentiment system to
determine the orientation of the statement. These
systems in turn need various kinds of preprocess-
ing starting from tokenization over syntactic pars-
ing up to coreference resolution. The Complex
Concept Builder is the collection of all these sys-
tems with the goal to assist the political scientists.
So far, the Complex Concept Builder imple-
ments tokenization (Schmid, 2009), lemmatisation
(Schmid, 1995), part-of-speech tagging (Schmid
and Laws, 2008), named entity detection (Faruqui
and Pado?, 2010), syntactical parsing (Bohnet,
2010), coreference analysis for German (Lappin
and Leass, 1994; Stuckardt, 2001), relation extrac-
tion (Blessing et al, 2012) and sentiment analysis
for English (Taboada et al, 2011).
It is important for a researcher of the humanities
to be able to adapt existing classification systems
according to his own needs. A common procedure
in both, NLP and political sciences, is to annotate
data. Therefore, one major goal of the project and
the Complex Concept Builder is to provide ma-
chine learning systems with a wide range of pos-
sible features ? including high level information
like sentiment, text type, relations to other texts,
etc. ? that can be used by non-experts for semi-
automatic annotation and text selection. Active
learning is used to provide immediate results that
can then be improved continuously. This aspect
of the Complex Concept Builder is especially im-
portant because new or adapted concepts that may
be looked for can be found without further help of
natural language processing experts.
3.3 Implementation
We decided to use a web-based platform for our
system since the social scientist needs no software
installation and we are independent of the used
operating system. Only a state-of-the-art web-
browser is needed. On the server side, we use a
tomcat installation that interacts with our UIMA
pipeline (Ferrucci and Lally, 2004). A HTML-
rendering component designed in the project (and
parametrizable) allows for a flexible presentation
of the data. A major issue of our work is interac-
tion. To solve this, we use JQuery and AJAX to
dynamically interact between client- and server-
side.
4 Case Study
In this section we explore the interaction between
various sub-systems and how they collaborate to
find complex political concepts. The following
Section 4.1 describes the detection of direct and
indirect speech and its evaluation follows in Sec-
tion 4.2. Section 4.3 is a general exploration of a
few selected sub-systems which require, or benefit
from direct and indirect speech. Finally, Section
4.4 discusses a specific usage scenario for indirect
speech.
4.1 Identifying Indirect Speech
The Complex Concept Builder provides analy-
ses on different linguistic levels (currently mor-
phosyntax, dependency syntax, named entities) of
annotation. We exploit this knowledge to identify
indirect speech along with a mentioned speaker.
Our indirect speech recognizer is based on three
conditions: i) Consider all sentences that contain
at least one word which is tagged as subjunctive
(i.e. ?*.SUBJ?) by the RFTagger. ii) This verb
has to be a direct successor of another verb in the
dependency tree. iii) This verb needs to have a
subject.
Figure 3 depicts the dependency parse tree of
sentence 2.
(2) Der Einsatz werde wegen der Risiken fu?r die
unbewaffneten Beobachter ausgesetzt, teilte
59
Einsatzmission
theDer
,,
ausgesetztstopped
werde
wegenbecause of
Risikorisks
teilteinformed
will be Missionschefhead of mission
MoodMood
RobertRobert
mitam
SaturdaySamstag
on
..
SBOC
VFIN.Aux.3.Sg.Pres.Subj
VFIN.Full.3.Sg.Past.IndRFTags
Figure 3: Dependency parse of a sentence that
contains indirect speech (see Sentence 2).
Missionschef Robert Mood am Samstag mit.
The mission will be stopped because of the risks to the
unarmed observers, informed Head of Mission Robert
Mood on Saturday.
The speaker of the indirect speech in Sentence
2 is correctly identified as Missionschef (Head of
Mission) and the corresponding verb is teilte mit
(from mitteilen) (to inform).
The parsing-based analysis helps to identify the
speaker of the citation which is a necessary in-
formation for the later interpretation of the cita-
tion. As a further advantage, such an approach
helps to minimize the need of lexical knowledge
for the identification of indirect speech. Our er-
ror analysis below will show that in some cases
a lexicon can help to avoid false positives. A lexi-
con of verbs of communication can easily be boot-
strapped by using our approach to identify candi-
dates for the list of verbs which then restrict the
classifier in order to achieve a higher precision.
4.2 Indirect Speech Evaluation
For a first impression, we present a list of sen-
tences which were automatically annotated as pos-
itive instances by our indirect speech detector.
The sentences were rated by political scientists.
Additionally, for each sentence we extracted the
speaker and the used verb of speech. We man-
ually evaluated 200 extracted triples (sentence,
speaker, verb of speech): The precision of our
system is: 92.5%
Examples 2, 3 and 4 present good candidates
which are helpful for further investigations on col-
lective identities. In example 3 Cardinal Lehmann
is a representative speaker of the Catholic commu-
nity which is a collective identity. Our extracted
sentences accelerate the search for such candidates
which amounts to looking manually for needles in
a haystack.
example speaker verb of speech
(2) Robert Mood teilte (told)
(3) Kardinal Karl Lehmann sagte (said)
(4) Sergej Ordzhonikidse sagte (said)
(5) Bild (picture) tru?ben (tarnish)
(6) sein (be) sein (be)
Examples 5 and 6 show problems of our first
approach. In this case, the speaker is not a person
or an organisation, and the verb is not a verb of
speech.
(3) Ein Angriffskrieg jeder Art sei ? sit-
tlich verwerflich ?, sagte der Vorsitzende
der Bischoffskonferenz, Kardinal Karl
Lehmann.
Any kind of war of aggression is ?morally reprehen-
sible,? said the chairman of the Bishops? Conference,
Cardinal Karl Lehmann.
(4) Derartige Erkla?rungen eines Staatschefs
seien im Rahmen der internationalen
Beziehungen inakzeptabel, sagte der UN-
Generaldirektor Sergej Ordzhonikidse
gestern in Genf.
Such statements of heads of states are unacceptable in
the context of international relations, said UN General
Director Sergei Ordzhonikidse in Geneva yesterday.
(5) Wu?rden die Wahlen verschoben, tru?bte sich
das gescho?nte Bild.
Would the elections be postponed, the embellished im-
age would tarnish.
(6) Dies sei alles andere als einfach, ist aus Of-
fizierskreisen zu ho?ren.
This is anything but simple, is to hear from military
circles.
60
EinsatzEimosheDrhsatzs,uDigao
psgasatzEdidsowshgbcdsatzhsu hdo
pgddsgDsatzEcihsoEbchsgwsatzfhgdso
ciwsatzciRsofshksatzfgDDo
wsd asatzspuciEglsoshlrcDsatzdsDDo
MSMMy.SMMy BMSMMyB.SMMyOMSMMyO.SMMyCMSMMyC.SMMyVMSMMy
p EdtFEsktEussbctRshwE
Figure 4: 10 most used verbs (lemma) in indirect
speech.
4.3 Using Indirect Speech
Other modules benefit from the identification of
indirect speech, as can be seen from Sentence 7.
The sentiment system assigns a negative polarity
of ?2.15 to the sentence. The nested sentiment
sources, as described by (Wiebe et al, 2005), of
this sentence require a) a direct speech with the
speaker ?Mazower? and b) an indirect speech with
the speaker ?no one? to be found.5
(7) ?There were serious arguments about what
should happen to the Slavs and Poles in east-
ern Europe,? says Mazower, ?and how many
of them should be sent to the camps and what
proportion could be Germanised . . . No one
ever came out and directly said Hitler had got
it wrong, but there was plenty of implied crit-
icism through comparisons with the Roman
empire. [...]?6
A collective identity evoked in Sentence 7 is
?the Germans?? although the term is not explic-
itly mentioned. This collective identity is de-
scribed as non-homogeneous in the citation and
can be further explored manually by the political
scientists.
The following are further applications of the
identified indirect speeches a) using the frequency
of speeches per text as a feature for classifica-
tion; e.g. a classification system for news re-
ports/commentaries as described in Section 4.4 b)
a project-goal is to find texts in which collective
5The reported sentiment value for the whole sentence is
applicable only to the direct speech. The indirect speech (i.e.
?Hitler had got it wrong?) needs a more fine-grained polarity
score. Since our Complex Concept Builder is very flexible, it
is trivial to score each component separately.
6http://www.guardian.co.uk/education/2008/jul
/01/academicexperts.highereducationprofile
identities are mobilised by entities of political de-
bate (i.e. persons, organisations, etc.); the detec-
tion of indirect speech is mandatory for any such
analysis.
4.4 Commentary/Report Classification
A useful distinction for political scientists dealing
with newspaper articles is the distinction between
articles that report objectively on events or back-
grounds and editorials or press commentaries.
We first extracted opinionated and objective
texts from DeReKo corpus (Stede, 2004; Kupietz
et al, 2010). Some texts were removed in order to
balance the corpus. The balanced corpus contains
2848 documents and has been split into a develop-
ment and a training and test set. 570 documents
were used for the manual creation of features. The
remaining 2278 documents were used to train and
evaluate classifiers using 10-fold cross-validation
with the WEKA machine learning toolkit (Hall et
al., 2009) and various classifiers (cf. Table 1).
The challenge is that the newspaper articles
from the training and evaluation corpus come from
different newspapers and, of course, from differ-
ent authors. Commentaries in the yellow press
tend to have a very different style and vocabulary
than commentaries from broadsheet press. There-
fore, special attention needs to be paid to the in-
dependence of the classifier from different authors
and different newspapers. For this reason, we use
hand-crafted features tailored to this problem. In
return, this means omitting surface-form features
(i.e. words themselves).
The support vector machine used the SMO al-
gorithm (Platt and others, 1998) with a polynomial
kernel K(x, y) =< x, y > e with e = 2. All other
algorithms were used with default settings.
precision recall f-score
SVM 0.819 0.814 0.813
Naive Bayes 0.79 0.768 0.764
Multilayer Percep-
tron
0.796 0.795 0.794
Table 1: Results of a 10-fold cross-validation for
various machine learning algorithms.
A qualitative evaluation shows that direct and
indirect speech is a problem for the classifier.
Opinions voiced via indirect speech should not
lead to a classification as ?Commentary?, but
should be ignored. Additionally, the number of
61
uses of direct and indirect speech by the author can
provide insight into the intention of the author. A
common way to voice one?s own opinion, without
having to do so explicitly, is to use indirect speech
that the author agrees with. Therefore, the number
of direct and indirect speech uses will be added
to the classifier. First experiments indicate that the
inclusion of direct and indirect speech increase the
performance of the classifier.
5 Related Work
Many approaches exist to assist social scientists in
dealing with large scale data. We discuss some
well-known ones and highlight differences to the
approach described above.
The Europe Media Monitor (EMM) (Stein-
berger et al, 2009) analyses large amounts of
newspaper articles and assists anyone interested in
news. It allows its users to search for specific top-
ics and automatically clusters articles from differ-
ent sources. This is a key concept of the EMM,
because it collects about 100, 000 articles in ap-
proximately 50 languages per day and it is impos-
sible to scan through these by hand. EMM users
are EU institutions, national institutions of the EU
member states, international organisations and the
public (Steinberger et al, 2009).
The topic clusters provide insight into ?hot?
topics by simply counting the amount of articles
per cluster or by measuring the amount of news on
a specific topic with regards to its normal amount
of news. Articles are also data-mined for geo-
graphical information, e.g. to update in which
geographical region the article was written and
where the topic is located. Social network infor-
mation is gathered and visualised as well.
Major differences between the EMM and our
approach are the user group and the domain of
the corpus. The complex concepts political sci-
entists are interested in are much more nuanced
than the concepts relevant for topic detection and
the construction of social networks. Additionally,
the EMM does not allow its users to look for their
own concepts and issues, while this interactivity
is a central contribution of our approach (cf. Sec-
tions 1, 2.1 and 3.2).
The CLARIN-D project also provides a web-
based platform to create NLP-chains. It is called
WebLicht (Hinrichs et al, 2010), but in its cur-
rent form, the tool is not immediately usable for
social scientists as the separation of metadata and
textual data and the encoding of the data is hard
for non-experts. Furthermore, WebLicht does not
yet support the combination of manual and au-
tomatic annotation needed for text exploration in
the social science. Our approach is based on the
webservices used by WebLicht. But in contrast to
WebLicht, we provide two additional components
that simplify the integration (exploration work-
bench) and the interpretation (complex concept
builder) of the research data. The former is in-
tended, in the medium term, to be made available
in the CLARIN framework.
6 Conclusion and Outlook
We developed and implemented a pipeline of var-
ious text processing tools which is designed to as-
sist political scientists in finding specific, complex
concepts within large amounts of text. Our case
studies showed that our approach can provide ben-
eficial assistance for the research of political sci-
entists as well as researcher from other social sci-
ences and the humanities. A future aspect will be
to find metrics to evaluate our pipeline. In recently
started annotation experiments on topic classifica-
tion Cohen?s kappa coefficient (Carletta, 1996) is
mediocre. It may very well be possible that the
complex concepts, like multiple collective identi-
ties, are intrinsically hard to detect, and the anno-
tations cannot be improved substantially.
The extension of the NLP pipeline will be an-
other major working area in the future. Examples
are sentiment analysis for German, adding world
knowledge about named entities (e.g. persons and
events), identification of relations between enti-
ties.
Finally, all these systems need to be evaluated
not only in terms of f-score, precision and recall,
but also in terms of usability for the political scien-
tists. This also includes a detailed investigation of
various political science concepts and if they can
be detected automatically or if natural language
processing can help the political scientists to de-
tect their concepts semi-automatically. The defini-
tion of such evaluation is an open research topic in
itself.
Acknowledgements
The research leading to these results has been
done in the project eIdentity which is funded from
the Federal Ministry of Education and Research
(BMBF) under grant agreement 01UG1234.
62
References
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. J. Mach. Learn.
Res., 3:993?1022, March.
Andre Blessing, Jens Stegmann, and Jonas Kuhn.
2012. SOA meets relation extraction: Less may be
more in interaction. In Proceedings of the Work-
shop on Service-oriented Architectures (SOAs) for
the Humanities: Solutions and Impacts, Digital Hu-
manities, pages 6?11.
Volker Boehlke, Gerhard Heyer, and Peter Wittenburg.
2013. IT-based research infrastructures for the hu-
manities and social sciences - developments, exam-
ples, standards, and technology. it - Information
Technology, 55(1):26?33, February.
Bernd Bohnet. 2010. Top accuracy and fast depen-
dency parsing is not a contradiction. In Proceedings
of the 23rd International Conference on Computa-
tional, pages 89?97.
Jean Carletta. 1996. Assessing agreement on classi-
fication tasks: The kappa statistic. Computational
Linguistics, 22(2):249?254.
Manaal Faruqui and Sebastian Pado?. 2010. Train-
ing and evaluating a german named entity recog-
nizer with semantic generalization. In Proceedings
of KONVENS 2010, Saarbru?cken, Germany.
D. Ferrucci and A. Lally. 2004. UIMA: an architec-
tural approach to unstructured information process-
ing in the corporate research environment. Natural
Language Engineering, 10(3-4):327?348.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H Witten.
2009. The weka data mining software: an update.
ACM SIGKDD Explorations Newsletter, 11(1):10?
18.
Erhard W. Hinrichs, Marie Hinrichs, and Thomas Za-
strow. 2010. WebLicht: Web-Based LRT Services
for German. In Proceedings of the ACL 2010 System
Demonstrations, pages 25?29.
Cathleen Kantner, Amelie Kutter, Andreas Hilde-
brandt, and Mark Puettcher. 2011. How to get rid
of the noise in the corpus: Cleaning large samples
of digital newspaper texts. International Relations
Online Working Paper, 2, July.
Marc Kupietz, Cyril Belica, Holger Keibel, and An-
dreas Witt. 2010. The german reference corpus
dereko: a primordial sample for linguistic research.
In Proceedings of the 7th conference on interna-
tional language resources and evaluation (LREC
2010), pages 1848?1854.
Shalom Lappin and Herbert J Leass. 1994. An algo-
rithm for pronominal anaphora resolution. Compu-
tational linguistics, 20(4):535?561.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://www.cs.umass.edu/ mccallum/mallet.
Andreas Niekler and Patrick Ja?hnichen. 2012. Match-
ing results of latent dirichlet alocation for text.
In Proceedings of ICCM 2012, 11th International
Conference on Cognitive Modeling, pages 317?322.
Universita?tsverlag der TU Berlin.
John Platt et al 1998. Sequential minimal optimiza-
tion: A fast algorithm for training support vector
machines. technical report msr-tr-98-14, Microsoft
Research.
Gerard Salton and Christopher Buckley. 1988. Term-
weighting approaches in automatic text retrieval. In-
formation processing & management, 24(5):513?
523.
Helmut Schmid and Florian Laws. 2008. Estima-
tion of conditional probabilities with decision trees
and an application to fine-grained POS tagging. In
Proceedings of the 22nd International Conference
on Computational Linguistics (Coling 2008), pages
777?784, Manchester, UK, August.
Helmut Schmid. 1995. Improvements in part-of-
speech tagging with an application to german. In
Proceedings of the ACL SIGDAT-Workshop, pages
47?50.
Helmut Schmid, 2009. Corpus Linguistics: An In-
ternational Handbook, chapter Tokenizing and Part-
of-Speech Tagging. Handbooks of Linguistics and
Communication Science. Walter de Gruyter, Berlin.
Burr Settles and Xiaojin Zhu. 2012. Behavioral fac-
tors in interactive training of text classifiers. In Pro-
ceedings of the 2012 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
563?567. Association for Computational Linguis-
tics.
Burr Settles. 2011. Closing the loop: Fast, inter-
active semi-supervised annotation with queries on
features and instances. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, pages 1467?1478. Association for Com-
putational Linguistics.
Manfred Stede. 2004. The potsdam commentary
corpus. In Proceedings of the 2004 ACL Work-
shop on Discourse Annotation, DiscAnnotation ?04,
pages 96?102, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Ralf Steinberger, Bruno Pouliquen, and Erik Van
Der Goot. 2009. An introduction to the europe me-
dia monitor family of applications. In Proceedings
of the Information Access in a Multilingual World-
Proceedings of the SIGIR 2009 Workshop, pages 1?
8.
63
Roland Stuckardt. 2001. Design and enhanced evalua-
tion of a robust anaphor resolution algorithm. Com-
putational Linguistics, 27(4):479?506.
Maite Taboada, Julian Brooke, Milan Tofiloski, Kim-
berly Voll, and Manfred Stede. 2011. Lexicon-
based methods for sentiment analysis. Computa-
tional linguistics, 37(2):267?307.
Janyce Wiebe, Theresa Wilson, and Claire Cardie.
2005. Annotating expressions of opinions and emo-
tions in language. Language Resources and Evalu-
ation, 39(2-3):165?210.
64
Proceedings of the 2nd Workshop on EVENTS: Definition, Detection, Coreference, and Representation, pages 35?44,
Baltimore, Maryland, USA, June 22-27, 2014.
c?2014 Association for Computational Linguistics
Conceptual and Practical Steps in
Event Coreference Analysis of Large-scale Data
Fatemeh Torabi Asr
1
, Jonathan Sonntag
2
, Yulia Grishina
2
and Manfred Stede
2
1
MMCI Cluster of Excellence, Saarland University, Germany
fatemeh@coli.uni-saarland.de
2
Applied Computational Linguistics, University of Potsdam, Germany
sonntag|grishina|stede@uni-potsdam.de
Abstract
A simple conceptual model is employed
to investigate events, and break the task
of coreference resolution into two steps:
semantic class detection and similarity-
based matching. With this perspective an
algorithm is implemented to cluster event
mentions in a large-scale corpus. Results
on test data from AQUAINT TimeML,
which we annotated manually with coref-
erence links, reveal how semantic conven-
tions vs. information available in the con-
text of event mentions affect decisions in
coreference analysis.
1 Introduction
In a joint project with political scientists, we are
concerned with various tasks of indexing the con-
tent of a large corpus of newspaper articles. To
supplement other NLP tools and as an interest-
ing information for the political scientists by itself,
we are interested in keeping track of discussions
around headline events such as attacks and crises.
The main challenges in the project include:
1. proposing a definition of event identity, and
2. finding the actual mentions in natural text,
to construct clusters of, so-called, coreferential
events. We refer to the former task as a formal
convention, a vital step in order for useful results
to be delivered to the human text analysts. The lat-
ter is basically an information extraction task once
a clear problem specification is obtained.
The main objective of the paper is to shed
light on each of the above tasks by applying a
three-layer event ontology
1
. Terminologies from
1
The term ontology is used to refer to a conceptual model
of events and connections between them rather than a partic-
ular knowledge base implementation.
earlier theories (Davidson, 1969) up until recent
work (Hovy et al., 2013a) are combined to draw an
integrated picture of the event coreference prob-
lem. The semantic layer is established with the
help of WordNet synsets. Related entities and
timestamps are considered as fundamental event
attributes that in practice can be resolved from the
context of a mention. We implement an incremen-
tal event clustering algorithm with respect to the
adapted ontology of events and use a minimal lin-
guistic procedure to extract values from text for
every event attribute. This system is being devel-
oped to work within a pipeline annotation project
where incremental clustering performs efficiently
on large-scale data.
In order to evaluate our proposed method, we
have manually annotated a random selection of
event mentions in the AQUAINT TimeML cor-
pus (UzZaman et al., 2013). Performance of the
automatic system in pair-wise coreference reso-
lution is comparable to that of more sophisti-
cated clustering methods, which at the same time
consider a variety of linguistic features (Bejan
and Harabagiu, 2010). The differences between
the human annotator pair-wise decisions and the
output of our clustering algorithm reveal inter-
esting cases where coreference labeling is per-
formed based upon the adapted semantic conven-
tion rather than information available in the text
about time, location and participants of an event
instance. In the following, we provide an overview
of the adapted ontology, background on event
coreference, and finally our implementation and
experiments within the proposed framework on
real data as well as the annotated corpus. We point
to related work at the various appropriate places in
the paper.
2 An Object Oriented Ontology
The general impression one gets by a review of
the coreference literature, is that at the semantic
35
formalism level, events are engaged with a higher
degree of complexity and more variety than en-
tities. That is probably because of the concrete
nature of entities: intuitively, an event happens,
whereas, an entity exists. As a subject matter, the
latter is more straightforward to get decomposed
into smaller components and be identified by cer-
tain feature attributes. The ontology explained in
this chapter is general in the sense that one could
(perhaps should) start understanding it by exam-
ples about entities.
A realized entity belongs to a class of enti-
ties sharing the same set of attributes. For ex-
ample, president Obama, as long as being talked
in a political context is considered as an instance
of the class PRESIDENT, comprising attributes
such as Country, Party and Duration of
presidency. Any other president can be compared
against Obama, with respect to the attribute values
associated with them. Therefore, Bush is a differ-
ent instance of the class PRESIDENT regarding
the fact that a different political Party as well
as a different presidential Duration are assigned
to him. Detecting mentions of these PRESIDENT
instances in text corpora would be a technical task
once the semantic representation was fixed. At this
level, instead we face questions like, whether or
not a named entity somewhere in the text detected
by our text processor, e.g., ?Barack Hossein?, is
referring to the one PRESIDENT instance that we
named above as Obama.
Figure 1 illustrates similar levels of abstraction
for event classes, event instances, and event men-
tions. The distinction between the second and the
third layer are more obvious and previously con-
sidered as clearly in other frameworks. The dis-
tinction between the first and the second layer,
though, is often left implicit, even in recently pub-
lished event annotation guidelines. For example in
a Grounded Annotation for Events (GAF, Fokkens
et al. 2013), event mentions are clearly distin-
guished from instances. However, the first two
layers have been taken as one, i.e., the semantic
layer. In their work, event type which is an artifact
of the adapted semantic ontology (SEM, Klyne
and Carroll 2004), implicitly works similar to the
classes in our definition. Nevertheless, these three
layers are intuitively separable and familiar for lin-
guists working on event and entity recognition.
Bejan and Harabagiu (2010), for example, intro-
duce the event coreference resolution with an ex-
ample put into a similar three-layer hierarchy, de-
spite their purely data-driven approach leaving off
prior semantic specifications. Here, we explain
each layer of the model separately. Issues specific
to coreference detection will be presented in the
following section.
2.1 Event Classes
The first layer of the ontology determines event
type definitions. Each class can have totally dif-
ferent attributes depending on the interests of a
particular study. Some events might be identi-
fied only by their time and place, while others by
participants of prioritized importance. A very flat
semantic representation would attribute all types
of events with a fixed set of entities, e.g.: par-
ticipants, time and location. Note, however, that
structural and semantic differences exist among
events of different natures, even if these complex
phenomena are reduced into something more fa-
miliar and tangible such as verb frames (Fillmore
et al., 2003). For example, a KILLING event is es-
sentially attributed with its Agent and Patient,
while salient attributes of an EARTHQUAKE
include Location, Magnitude, Time and
Human Impacts, in a typical news context.
This becomes even more clear when event types
are taken and compared against one another from
different genres of text (Pivovarova et al., 2013;
Shaw, 2013). A scientific attitude toward the
analysis of EARTHQUAKE events might character-
ize them with Natural Impacts rather than
Human Impacts. Thus, the first layer of the
model needs to be designed with respect to the
specific information extraction goals of the partic-
ular study, be it a pure linguistic or an application-
oriented one.
Ambiguities about the granularity of attributes,
subevent-ness, scope and most importantly, iden-
tity between event instances are dealt with at the
definition layer for and between classes. For ex-
ample, if the modeler wants to allow coreference
between instances of KILLING and SHOOTING
to indicate some type of coreference between an
event and its possible subevent then this needs to
be introduced at the class level, along with a pro-
cedure to compare instances of the two classes,
which possess different sets of attribute
2
. Remarks
2
The same applies even to a more flexible case, when
the modeler wants to allow coreference between KILLING
and DYING instances (e.g., if a KILLING?s Patient is the
same as a DYING?s Theme).
36
  
Class KILLINGAgent;Patient;Time;Location;
Class SHOOTINGAgent;Patient;Time;Location;Weapon;
Class EARTQUAKEMagnitude;Human Impacts;Time;Location;
Shooting instance 1Agent: Lee Harvey OswaldPatient:  John Fitzgerald KennedyWeapon: a rifleTime: 22.11.1963Location: Dealey Plaza, Dallas
Mention 4? Shortly after noon on November 22, 1963, President John F. Kennedy was assassinated as he rode in a motorcade through Dealey Plaza. ?
Earthquake instance 1Magnitude: 6.6 to 7Human Impacts:  injury and deathTime: 20.04.2013Location: Sichuan, China
Mentions 2 and 3? Lushan, China (CNN) -- A strong earthquake that struck the southwestern Chinese province of Sichuan this weekend has killed 186 people, sent nearly 8,200 to hospitals and created a dire dearth of drinking water, Chinese state-run Xinhua reported Sunday. Earlier reports had said as many as 11,200 people were injured. ?
1 n m q
        Formalism                                  Realization                                           Text 
Killing instance 1Agent: Lee Harvey OswaldPatient:  John Fitzgerald KennedyTime: 22.11.1963Location: Dealey Plaza, Dallas
Killing instance 2Agent: an earthquakePatient: local peopleTime: 20.04.2013Location: Sichuan, China
Mention 1? President Kennedy was killed three days before he was to make these amendments public.?
Figure 1: A three-layer ontology of events: classes, instances and mentions
of Hovy et al. (2013b) on different types of iden-
tity according to lexicosyntactic similarity, syn-
onymy and paraphrasing indicate that the model-
ers have a wide choice of identity definition for
event types. In section 4.3 we explain how to adapt
an extended version of synonymy in order to de-
fine event classes prior to similarity-based cluster-
ing of the mentions.
2.2 Event Instances
Layer 2 indicates perfect instantiation, representa-
tive of the human common sense intuition of phe-
nomena in real world. Instances in this layer corre-
spond to the Davidsonian notion of events as con-
crete objects with certain locations in space-time,
something that is happening, happened, or will
happen at some point (Davidson, 1969). There-
fore, links from classes to instances represent
a one-to-many relation. Every instance of the
EARTHQUAKE is determined with a unique set
of attribute values. Two EARTHQUAKE instanti-
ations with exactly similar attribute values are just
identical. In order to keep a clear and simple rep-
resentation specific to the study of coreference,
the model does not allow any connection or rela-
tion between two event instances unless via their
classes. Note that in Figure 1, for each realized
object, only attributes included in the formalism
layer are presented with their values, while in re-
ality events occur with possibly infinite number of
attributes.
2.3 Event Mentions
Facing an event mention in the text, one should
first determine its class and then the unique event
instance, to which the mention points. Detection
of the class depends on the semantic layer defi-
nitions, while discovering the particular instance
that the mention is talking about relies on the at-
tribute values extractable from the mention con-
text.
Usually, mentions provide only partial informa-
tion about their target event instance. They can
be compared against one another and (if available)
against a fully representative mention, which most
clearly expresses the target event by providing all
necessary attribute values. Fokkens et al. (2013)
refer to such a mention as the trigger event. Some-
times it is possible that the context is even more in-
formative than necessary to resolve the unique real
world corresponding event (see details about the
impact of the earthquake in mention 3, Figure 1).
In natural text a mention can refer to more than
one event instance of the same type, for example
when a plural case is used: ? ... droughts, floods
and earthquakes cost China 421 billion yuan in
2013?. Hovy et al. (2013b) propose partial coref-
erence between singular and plural mentions. In
37
our model plural mentions are not treated seman-
tically differently, they only point to several in-
stances, thus, are coreferential with any single
mention of them as long as the attribute values al-
low
3
.
With respect to the above discussion, links from
layer 2 to 3 represent many-to-many relations: an
event instance can have several mentions in the
text, and a single mention can point to more than
one event instance at a time.
3 Towards Coreference Analysis
In terms of method, two different approaches have
been tried in the literature under the notion of
event coreference resolution (Chen and Ji, 2009;
Bejan and Harabagiu, 2010; Lee et al., 2012;
Hovy et al., 2013b). The first and most theoreti-
cally founded strategy is to decide for every pair
of event mentions, whether or not they refer to
the same event instance. Since in this approach
decisions are independently made for every pair
of event mentions, a clear formalism is needed to
determine exactly what types of coreference are
possible and how they are detected by looking
at textual mentions (Chen and Ji, 2009; Hovy
et al., 2013b). Some related work on predicate
alignment also fit into this category of research
(Roth and Frank, 2012; Wolfe et al., 2013).
Alternatively, in automatic event clustering, the
objective is basically discovering event instances:
all we know about an event in the world is the
collective information obtained from mentions
referring to that in a text corpus. Each cluster
in the end ideally represents a unique event in
reality with all its attribute values (Bejan and
Harabagiu, 2010; Lee et al., 2012). Some formal
and technical differences exist between the two
approaches.
Boolean choice: traditionally, clusters shape with
the idea that all mentions within a cluster are of
the same identity. Every randomly chosen pair
of mentions are coreferent if they are found in a
single cluster at the end, and non-coreferent oth-
erwise. Therefore, taking this approach implies a
level of formalism, which rules out partial coref-
erence. On the other hand, pair-wise classifica-
tion could consider partial coreference whenever
3
The other type of quasi-identity discussed by Hovy et al.
(2013b) engaged with sub-events is handled in the semantic
level.
two event mentions are neither identical nor totally
different (Hovy et al., 2013b). Soft-clustering can
compensate some deficiencies of traditional clus-
tering approaches
4
.
Transitivity: all mentions in a single cluster
are coreferential, whereas pair-wise labels allow
for non-transitive relations among event mentions.
Depending on the specific goal of a study, this
could be an advantage or a disadvantage. Lack
of transitivity could be considered as an error if it
is not consciously permitted in the underlying se-
mantic formalism.
Complexity and coverage: event mentions can
appear in noisy or sparse context where informa-
tion for detection of their target event instance is
not available. Dealing with such cases is usually
easier in a clustering framework where similarity
scores are calculated against the collective infor-
mation obtained from a population of mentions,
rather than an individual occurrence. Classifica-
tion approaches could comparatively handle this
only if sufficiently representative labeled data is
available for training.
Exploration: a general advantage of cluster anal-
ysis is that it provides an exploratory framework
to assess the nature of similar input records, and
at the end it results in a global distributional
representation. This is specially desired here,
since computational research on event coreference
is in its early ages. Evaluation corpora and
methodology are still not established, thus, the
problem is not yet in the phase of ?look for higher
precision?!
The method we are going to propose in the next
section combines a rule-based initial stage with a
similarity-based clustering procedure. This is par-
tially inspired by the work of Rao et al. (2010),
where entity coreference links are looked up in
high-volume streaming data. They employ a lex-
icon of named entities for cluster nomination to
reduce the search space. Once a mention is visited
only the candidates among all incrementally con-
structed clusters up to that point are examined. In-
cremental clustering strategies are in general suit-
able for a pipeline project by efficiently providing
single visits of every mention in its context. Fea-
ture values of a mention can be extracted from the
document text, used for clustering, and combined
4
For example, multi-assignment would allow plural men-
tions to take part in several different clusters, each represen-
tative of one event instance.
38
into the feature representation of the assigned clus-
ter in a compressed format.
4 Event Coreference System
The original data in our study is a text corpus au-
tomatically annotated with several layers of syn-
tactic and semantic information (Blessing et al.,
2013). The English portion includes news and
commentary articles of several British and Amer-
ican publishers from 1990 to 2012. An approx-
imate average of 100 event mentions per docu-
ment with the large number of total documents per
month (avg. 1200) requires us to think of different
ways to reduce the search space and also design a
low-complexity coreference resolution algorithm.
4.1 Partitioning
In cross-document analysis, typically, a topic-
based document partitioning is performed prior to
the coreference chain detection (Lee et al., 2012;
Cybulska and Vossen, 2013). Since we are in-
terested to track discussions about a certain event
possibly appearing in different contexts, this tech-
nique is not desired as coreference between men-
tions of a single real word event in two differ-
ent topics would remain unknown. For example,
when an articles reviews several instances of a cer-
tain event type such as different attacks that has
happened in a wide temporal range and in differ-
ent locations, such articles would not be included
in any of the individual topics each focused on
one event instance. As an alternative to the pre-
vious approach, we perform a time-window par-
titioning based on the article publication date be-
fore feeding the data into the coreference analysis
algorithm. Larger windows would capture more
coreference links: this is a parameter that can be
set with respect to the available resources in trade-
off with the desired search scope. In the future, we
would like to invent an efficient procedure to com-
bine the resulting clusters from consecutive time-
windows in order to further enhance the recall of
the system.
4.2 Event Mention and Feature Identification
In order to extract event mentions we use the
ClearTK UIMA library (Ogren et al., 2008), check
the PoS of the head word in the extracted text
span and take all verbal and nominal mentions
into account. In the current implementation all
event classes are identified by a fixed set of at-
tributes including Timestamps and Related
Entities. While being very coarse-grained,
this way of attribution is quite intuitive: events
are identified by times, places and participants
directly or vaguely attached to them. Temporal
expressions are extracted also by ClearTK and
normalized using SUTime (Chang and Manning,
2012). Named entities of all types except Date
are used which are obtained from previous work
on the same dataset (Blessing et al., 2013).
4.3 The Two-step Algorithm
Having all required annotations, we select a
time window and perform the following two
steps for event mentions of the TimeML classes
Occurrence, I-Action, Perception and
Aspectual
5
.
1) Semantic class identification: WordNet
synsets provide a rich resource in order to be
adapted as event classes (Fellbaum, 1999). They
cover a large lexicon and the variety of rela-
tional links between words enables us to specify
a clear semantic convention for the coreference
system. In addition to the mentions coming from
the same synset, we allow coreference between
events belonging to two different synsets that are
directly connected via hypernymy or morphose-
mantic links. While every WordNet synset com-
prises words only from a single part of speech,
morphosemantic relations allow the model to es-
tablish cross-PoS identity among words sharing
a stem with the same meaning which is desired
here: observe (verb) and observation (noun)
6
. A
Java library is employed to access WordNet anno-
tations (Finlayson, 2014).
2) Similarity-based clustering: A mention is
compared against previously constructed clus-
ters with respect to the attribute values that are
extractable from its context. In order to fill
the Timestamps attribute we have employed a
back-off strategy: first we look at all time expres-
sions in the same paragraph where the event men-
tion appears, if we found enough temporal infor-
mation, that would suffice. Otherwise, we look
into the content of the entire article for tempo-
ral expressions. The Related Entities at-
5
Other types, namely, Report, State and I-State
events are not interesting for us, therefore such mentions are
simply skipped.
6
When a mention is visited all compatible synsets accord-
ing to the head lemma are tried because in the current imple-
mentation we do not perform word sense disambiguation.
39
tribute is filled similarly by looking at the named
entities in the context of the event mention. The
first step is a procedure to candidate clusters con-
taining mentions of related types. If no cluster
is a candidate, a singleton cluster is created and
its class is added to the index of visited event
types (synsets). If candidate clusters already ex-
ist, we calculate the feature-based similarity score
for each. If the best score is below a threshold a
new singleton cluster is created but in this case for
the reason that, perhaps, not a new type but a new
event instance is visited.
5 Manual Annotation and Evaluation
The Event Coreference Bank, which is the largest
available corpus with cross-document corefer-
ence labels, supports only a within topic evalu-
ation (ECB, Bejan and Harabagiu 2010). In or-
der to perform a more realistic evaluation of the
method presented in this paper, we selected a sub-
set of events from the AQUAINT TimeML cor-
pus and annotated those with coreferentiality. The
AQUAINT TimeML data has recently served as
one of the benchmarks in the TempEval shared
task (UzZaman et al., 2013) and is available for
public use
7
. It contains 73 news report docu-
ments from four topics, annotated with 4431 event
mentions and 652 temporal expressions which
make it suitable for our task. Two main differ-
ences between our annotation and the ECB data
are: 1) event mentions here are selected semi-
randomly
8
and across topics rather than topic-
based, 2) they are shown pair-wise to the anno-
tator (in order to catch the transitivity patterns
after the analysis), whereas, in the ECB, event
mentions are clustered. Furthermore, the data
already comes with manually assigned mention
boundaries, event types, temporal expressions and
links between events and temporal expressions, all
according to the TimeML standards (Hobbs and
Pustejovsky, 2003). These serve exactly as fea-
tures that our algorithm uses for construction of
clusters. We only had to perform named entity
recognition automatically to have data ready for
evaluation of the model. The manual annotation
7
http://www.cs.york.ac.uk/
semeval-2013/task1
8
Since the number of coreferential mentions is much
smaller than non-coreferent ones, we adapted a heuristic mea-
sure to make sure that we will have some similar mentions
among the 100 records. Therefore, we would call it a semi-
random selection, still different from the fully selective strat-
egy employed for ECB.
of 4950 pairs resulting from 100 selected event
mentions (
100!
2!(100?2)!
) was done with the help of a
simple user interface, which showed each of the
two event mentions within its context to the an-
notator and asked for pushing yes, no or next
(undecided) button to proceed to the next pair.
After studying the annotation guideline published
by Cybulska and Vossen (2014), our expert spent
some hours during a week for the job. Decisions
made in shorter than 500 ms were revised after-
wards. There was one no answer which the an-
notator found unsure after revision, as it resulted
in a transitivity violation, but we left it unchanged
due to the nature of pair-wise decisions. In the end
we came up with a total of 36 yes, and 4914 no
pairs.
6 Experiments
This section provides an insight into how clusters
of event mentions are created for a portion of our
large news corpus. We also run the algorithm on
the manually annotated data to perform an error
analysis.
6.1 Construction of Event Clusters
News text from New York Times and Washing-
ton Post are combined to demonstrate a show-
case of clustering for a time-window of two weeks
(250 articles)
9
. Figure 2 shows the creation curve
of event classes (type index entries) and event
instances (clusters) as the number of the vis-
ited mentions increases. Comparison between the
number of mentions with that of clusters indicates
that a great deal of event instances are mentioned
only once in the text. Since, for each mention, all
compatible synsets are added to the type index (if
not there already) during the early stages of clus-
tering the number of the type index entries is times
the number of visited mentions. In the middle
to the end phases the type index contains a large
collection of event classes, also a decent number
of non-singleton clusters (repeatedly mentioned
event instances) are created. Statistics of the type
of clusters obtained after performing the algorithm
on the processed mentions are presented in Ta-
ble 1. A significant number of non-singleton clus-
ters contain mentions only from a single paragraph
or a single article, which is expected given the type
9
This collection is processed within a few minutes on a
normal PC by the proposed algorithm starting with zero clus-
ters.
40
Figure 2: Number of clusters and the type index entries as mentions are visited in 250 articles
of features; remember that Timestamps and
Named Entities are looked up in a paragraph
scope. Clusters containing mentions from several
articles, namely, the popular ones are most inter-
esting for us as they would be representative of the
systems performance on cross-document corefer-
ence analysis. By looking at those we found that
the named entities have a very important role in
finding similar subtopics within and between doc-
uments. Temporal expressions are less helpful as
they are rare, and otherwise introduce some noise
when documents are already being processed in
a specific publication time-window. For example,
the word today which appears in most articles of
the same day (and would be normalized to that
day?s date, e.g., ?1990.01.12?) would gather men-
tions of a general event type, e.g., meet, although,
they might not be pointing to the same instance.
The employed semantic convention establishes a
balance between efficiency and recall of the sys-
tem. Nevertheless, it sometimes allows clustering
of intuitively unrelated actions. In order to en-
hance the clustering performance in terms of the
precision, we have a parameter to give priority to
within synset coreference.
Cluster type Freq. Avg. content
Singleton 12895 1
Single paragraph 1360 2.36
Single article 807 3.95
Popular 182 2.99
Table 1: Different types of resulting clusters
6.2 Error Analysis
We fed all event mentions from the AQUAINT
TimeML corpus into the algorithm exactly in the
same way that we did in case of our large news
corpora. The algorithm has a few parameters
which we set by looking at samples of resulting
clusters prior to the measurement on the labeled
portion. This is a minimal NLP system given that
neither syntactic/semantic dependency of entities
to the event head word nor the type of attachment
to temporal expressions in the context are taken
into account. Nevertheless, we obtain 51.3% pre-
cision and 55.6% recall for the pair-wise corefer-
ence resolution task on the annotated data. The
resulting F-score of 53.4% is comparable with the
best F-scores reported in the work of Bejan and
Harabagiu (52.1% on ECB for the similar task)
while they use a rich linguistic feature set, as well
as a more sophisticated clustering method.
Coreference Total Related class Same doc.
True positive 20 100% 25%
True negative 4895 16% 2%
False positive 19 100% 36%
False negative 16 33% 7%
Total 4950 15% 2%
Table 2: Pair-wise decisions
Table 2 shows false positive and negative answers
separately. As reflected in the results, positive
labels are given only to mention pairs of related
classes (headwords need to share a synset, or are
related via hypernym and morphosemantic links
in WordNet). 36% of positive labels are given to
pairs within some article which is expected given
that common contextual features are easy to find
for them. In such cases, usually linguistic features
are needed to resolve participants or the relative
temporality of one mention against the other:
a. some people are born rich, some are born
poor.
b. the bullet bounced off a cabinet and rico-
cheted into the living room.
41
In some cases, on the other hand, the disagreement
depends on the semantic approach to the defini-
tion of identity, and therefore, is more controver-
sial. The human annotator has apparently been
more conservative to annotate coreference when
the head words of the mentions were a bit different
in meaning, whereas the system?s decision bene-
fited from some flexibility:
a. the immigration service decided the boy
should go home. / they made a reasonable
decision Wednesday in ruling that...
b. if he goes, he will immediately become...
It is not clear, for example, whether ruling is a sub-
event of the decision or exactly the same event. A
similar distinction needs to be made in case of the
false negative labels. The automatic clustering is
not able to detect coreference mostly in case of
sparse context, where enough information is not
available to resolve the similarity. That is why
false negative happens more frequently for men-
tions coming from different articles (specifically
paragraphs sharing few named entities) and only
7% of the time when they happen within a docu-
ment:
a. the Clinton administration has pushed for
the boy?s return. / his son said he didn?t
want to go.
Sparse context results either in the creation of a
singleton cluster for the mention or careless as-
signment to some wrong cluster, which in the fu-
ture would decrease the chance of meeting coref-
erent mentions. False negatives happening for
mentions of unrelated semantic classes are due to
the missing links between possibly synonym words
in WordNet, one of the issues that need to be in-
vestigated and cured in the future work.
7 Conclusion
This paper presented a variety of material concern-
ing event coreference resolution:
1. A general ontology is explained that can be
employed in different studies on events.
2. An algorithm is designed, regardingly, to
gather coreferential event in a large corpus.
3. A set of event mentions in AQUAINT
TimeML is annotated with pair-wise corefer-
ence tags within and between topics
10
.
4. An implementation of the method consider-
ing simple and scalable features is tested on
real data and the annotated corpus.
5. Finally, we performed an error analysis of the
automatically assigned labels to identify fu-
ture directions.
Separating the semantic layer definition of coref-
erence from textual attribution of event mentions
has two benefits in our framework. First, it pro-
vides us with an efficient partitioning procedure
to reduce the search space. Second, it makes the
model flexible to allow for different possible se-
mantic conventions which could vary from one
application to another. Our adaptation of Word-
Net synsets allows for integrative future exten-
sion of the model ? e.g., to capture metaphori-
cal and subevent relations based on Methonymy
and Entailment links. The intuition of using
named entities for identification of important real-
world events resulted in balanced precision and re-
call on the test data. In the future, we would like to
investigate the effect of linguistic features on im-
proving the performance of the algorithm. In par-
ticular, it would be interesting to see whether exact
specification of event head arguments would out-
perform the vague attribution with related entities.
The state-of-the-art result in the supervised predi-
cate alignment approach is a hint for rich linguistic
features to be helpful (Wolfe et al., 2013). On the
other hand, depending on the adapted event iden-
tity definition, coreferential events might not re-
ally share identical arguments (Hasler and Orasan,
2009). There are differences between real data
collections and the available annotated corpora,
including ours, which needs to be investigated as
well. For example, small collections do not in-
clude enough same-class event mentions pointing
to different event instances, and it brings about
unrealistic evaluations. Furthermore, annotation
guidelines are usually biased towards a specific
theory of event identity which affect the resulting
data in one way or another. Some applications de-
mand different semantic conventions perhaps with
broader/narrower definition of identity. This is a
dilemma that needs to be resolved through more
theoretical studies in touch with real world prob-
lems such as the one we introduced in this paper.
10
The annotation is available at: http://www.coli.
uni-saarland.de/
?
fatemeh/resources.htm
42
References
Bejan, C. A. and Harabagiu, S. (2010). Unsuper-
vised event coreference resolution with rich lin-
guistic features. In Proceedings of the 48th An-
nual Meeting of the Association for Computa-
tional Linguistics, pages 1412?1422. Associa-
tion for Computational Linguistics.
Blessing, A., Sonntag, J., Kliche, F., Heid, U.,
Kuhn, J., and Stede, M. (2013). Towards a
tool for interactive concept building for large
scale analysis in the humanities. In Proceed-
ings of the 7th Workshop on Language Technol-
ogy for Cultural Heritage, Social Sciences, and
Humanities, pages 55?64, Sofia, Bulgaria. As-
sociation for Computational Linguistics.
Chang, A. X. and Manning, C. (2012). Sutime:
A library for recognizing and normalizing time
expressions. In LREC, pages 3735?3740.
Chen, Z. and Ji, H. (2009). Graph-based event
coreference resolution. In Proceedings of the
2009 Workshop on Graph-based Methods for
Natural Language Processing, pages 54?57.
Association for Computational Linguistics.
Cybulska, A. and Vossen, P. (2013). Semantic re-
lations between events and their time, locations
and participants for event coreference resolu-
tion. In RANLP, volume 2013, page 8.
Cybulska, A. and Vossen, P. (2014). Guidelines for
ecb+ annotation of events and their coreference.
Technical report, Technical Report NWR-2014-
1, VU University Amsterdam.
Davidson, D. (1969). The individuation of events.
In Essays in honor of Carl G. Hempel, pages
216?234. Springer.
Fellbaum, C. (1999). WordNet. Wiley Online Li-
brary.
Fillmore, C. J., Johnson, C. R., and Petruck, M. R.
(2003). Background to framenet. International
journal of lexicography, 16(3):235?250.
Finlayson, M. A. (2014). Java libraries for ac-
cessing the princeton wordnet: Comparison and
evaluation. In Proceedings of the 7th Global
Wordnet Conference, pages 78?85.
Fokkens, A., van Erp, M., Vossen, P., Tonelli, S.,
van Hage, W. R., SynerScope, B., Serafini, L.,
Sprugnoli, R., and Hoeksema, J. (2013). Gaf: A
grounded annotation framework for events. In
NAACL HLT, volume 2013, page 11.
Hasler, L. and Orasan, C. (2009). Do corefer-
ential arguments make event mentions corefer-
ential. In Proc. the 7th Discourse Anaphora
and Anaphor Resolution Colloquium (DAARC
2009).
Hobbs, J. and Pustejovsky, J. (2003). Annotating
and reasoning about time and events. In Pro-
ceedings of AAAI Spring Symposium on Logical
Formalizations of Commonsense Reasoning.
Hovy, E., Mitamura, T., and Palmer, M. (2013a).
The 1st workshop on events: Definition, detec-
tion, coreference, and representation.
Hovy, E., Mitamura, T., Verdejo, F., Araki, J.,
and Philpot, A. (2013b). Events are not sim-
ple: Identity, non-identity, and quasi-identity.
NAACL HLT 2013, page 21.
Klyne, G. and Carroll, J. J. (2004). Resource
description framework (rdf): Concepts and ab-
stract syntax. w3c recommendation, 10 feb.
2004.
Lee, H., Recasens, M., Chang, A., Surdeanu,
M., and Jurafsky, D. (2012). Joint entity and
event coreference resolution across documents.
In Proceedings of the 2012 Joint Conference
on Empirical Methods in Natural Language
Processing and Computational Natural Lan-
guage Learning, pages 489?500. Association
for Computational Linguistics.
Ogren, P. V., Wetzler, P. G., and Bethard, S. J.
(2008). Cleartk: A uima toolkit for statisti-
cal natural language processing. Towards En-
hanced Interoperability for Large HLT Systems:
UIMA for NLP, 32.
Pivovarova, L., Huttunen, S., and Yangarber, R.
(2013). Event representation across genre.
NAACL HLT 2013, page 29.
Rao, D., McNamee, P., and Dredze, M. (2010).
Streaming cross document entity coreference
resolution. In Proceedings of the 23rd Inter-
national Conference on Computational Linguis-
tics: Posters, pages 1050?1058. Association for
Computational Linguistics.
Roth, M. and Frank, A. (2012). Aligning predicate
argument structures in monolingual comparable
texts: A new corpus for a new task. In Proceed-
ings of the First Joint Conference on Lexical
and Computational Semantics-Volume 1: Pro-
ceedings of the main conference and the shared
43
task, and Volume 2: Proceedings of the Sixth In-
ternational Workshop on Semantic Evaluation,
pages 218?227. Association for Computational
Linguistics.
Shaw, R. (2013). A semantic tool for historical
events. NAACL HLT 2013, page 38.
UzZaman, N., Llorens, H., Derczynski, L., Verha-
gen, M., Allen, J., and Pustejovsky, J. (2013).
Semeval-2013 task 1: Tempeval-3: Evaluat-
ing time expressions, events, and temporal rela-
tions. In Second joint conference on lexical and
computational semantics (* SEM), volume 2,
pages 1?9.
Wolfe, T., Van Durme, B., Dredze, M., Andrews,
N., Beller, C., Callison-Burch, C., DeYoung,
J., Snyder, J., Weese, J., Xu, T., et al. (2013).
Parma: A predicate argument aligner.
44
