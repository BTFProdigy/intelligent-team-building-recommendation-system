Preface
Mike Rosner
Dept. CSAI
University of Malta
MSD06
Malta
mros@cs.um.edu.mt
This workshop is something of an accident. The
original intention was to have two separate
workshops, one devoted to educational issues in
NL technologies, and the other to infrastructures
for sharing computational language resources.
Each of these proposed workshops had a history
of their own.
The general problems of sharing, re-use and
distribution of resources and tools has been with
us for a long time. Only recently, however, have
we seen the emergence of archives not only of
significant size, but of a variety that reflects an
amazing degree of specialisation in terms of
both type of content, type of language, type of
user and so on. The development of such
"designer" archives is due partly to the
availability of cheap computing power and mass
storage (that even University laboratories can
afford!), partly to a shift in perspective towards
empirical methods, and partly to the ever-
increasing ease with which such materials can
be accessed via Internet. Good examples of
existing initiatives in this area are, on the data
side, ELRA/ELDA, LDC, TELRI and elsnet
resource catalogues and repositories. On the
tools side we have, amongst others, the ACL
Natural Language Software Registry (hosted at
DFKI) which was set up as a repository for tools
for the distinct fields of Human Language
Technology (HLT).
Mere proliferation of data is not necessarily
conducive to improved practice, however, and at
a workshop at ACL 2000 in Hong Kong
dedicated to Infrastructures for Global
Collaboration there was an agreement between
the main professional organisations in NLP and
Speech (ACL and ISCA), and elsnet, and the
other meeting participants, that it would be
useful to aim at a broadly supported, joint
repository or catalogue for tools and materials
for the language and speech communities. That
is roughly the background to the infrastructures
workshop.
The education workshop has a somewhat
different history. The first point to note is that
teaching in this area, whether it be from the
perspective of Computational Linguistics, NLP
or AI, involves a high proportion (approximately
40% according to DeSmedt's (1999) survey) of
practical exercises and projects, the organisation
of which depends on the availability
computational infrastructure. All of those
involved in teaching eventually discover the
same basic problem: there are relatively few
resources, both tools and data, that are directly
useable as the basis for instruction, compared to
the total volume of resources that are actually
available. That is, in general, the educationally
useful yield that results from trawling the net, is
pitifully low, and may be getting lower as the
supply increases. The main reason for this is that
most of the material has evolved to fulfil the
needs of research, not education. Consequently,
the best strategy is often to just go on
reinventing our own wheels for our own
purposes: hardly an optimal use of existing
resources.
An ELSNET-sponsored workshop was held at
EACL99 in Bergen to address these issues.
Some excellent individual efforts at providing
tools that buck the trend were presented there
(see Rosner 1999). However, the main
conclusion, which, incidentally, is echoed in
DeSmedt's survey, is that certain non-transient
infrastructures needed to be instigated to raise
the public perception of educational issues in
Computational Linguistics. There was talk of
founding an entity within EACL that could take
on the function of coordinating training-related
activities - as has already happened in ISCA.
Well, this has not happened yet, but it is salutary
to note that something else has: one non-
transient infrastructure, founded in January 2000
under the auspices of elsnet, is the European
Masters in Language and Speech, which is
probably the first example of a tertiary level
"qualification" that transcends national
boundaries.
A second conclusion at the EACL workshop
was the need for a repository of shared
materials, appropriately indexed for educational
usage. Some effort towards that end has been
achieved through JEWELS, an EU-supported
website for educational materials in Language
and Speech.
It seems clear that another infrastructure that is
badly needed to support such initiatives is an
entity whose primary tasks would be the
creation of a maintainable multilingual
taxonomy of the subject matter in order to
enable a consistent naming policy to be agreed
upon concerning both courses, and the
corresponding resources.
This workshop will build on the consensus
reached at these previous workshops. As can be
seen from the presentations, there are two clear
foci: one upon sharable tools, the other on
instruments for sharing tools and resources in
general. A third theme concerns how to build
upon existing initiatives as sources of data or
inspiration.
The main goal of the workshop is to discuss
methods for the improvement and extension of
existing repositories; the educational uses of
repositories; the closer interlinking between
different kinds of repositories (tools and
resources); global infrastructures for the
achievement of joint actions. However, we
expect the scope of the workshop to be much
wider than that, as the issues addressed are of
general interest to everybody who believes that
sharing tools and resources is essential for the
progress of research and education in our field.
To conclude, some words of thanks: to Thierry
Declerck, Steven Krauwer and the other
members of the workshop committee who all
doubled as referees, for their help, patience and
comments.
Our final acknowledgement goes to elsnet, but
not just for sponsoring our invited speaker. If
one looks carefully at many of the other
initiatives that have been mentioned in this short
introduction, it clear that the support shown for
this workshop is no mere coincidence, but part
of a dastardly long-term strategy to promote the
status of our theme and also related ones within,
and somewhat beyond the bounds of
Computational Linguistics.
Long may this strategy continue!
References
de Smedt. K. et. al. European Studies on
Computational Linguistics, University of Bergen
1999.
Rosner M. (ed). Proceedings of the Workshop on
Computer and Internet Supported Education
in Language and Speech Technology, EACL-99.
ACL NL Software Registry, http://registry.dfki.de
LDC (Linguistic Data Consortium)
http://www.ldc.upenn.edu
TELRI (Trans European Language Resources
Infrastructure) http://www.telri.de
ELSNET http://www.elsnet.org/resources.html
ELRA/ELDA http://www.icp.inpg.fr/ELRA
JEWELS website http://www.elsnet.org/jewels.html
.

Proceedings of the Second ACL Workshop on Effective Tools and Methodologies for Teaching NLP and CL, pages 43?48,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Language Technology from a European Perspective 
 
 
Hans Uszkoreit, 
Valia Kordoni 
Vladislav Kubon Michael Rosner Sabine Kirchmeyer-
Andersen 
Dept. of Computational 
Linguistics 
UFAL MFF UK Dept. of Computer Sci-
ence and A.I. 
Dept. of Computational 
Linguistics 
Saarland University Charles University University of Malta Copenhagen Business 
School 
D-66041, Saarbruecken, 
Germany 
Prague, Czech Republic Msida, Malta Copenhagen, Denmark 
{uszkoreit, 
kordoni}@coli.uni-
sb.de 
vk@ufal.mff.cuni.cz mike.rosner 
@um.edu.mt 
ska.id@cbs.dk 
 
 
 
 
Abstract 
This paper describes the cooperation of 
four European Universities aiming at at-
tracting more students to European master 
studies in Language and Communication 
Technologies. The cooperation has been 
formally approved within the framework 
of the new European program ?Erasmus 
Mundus? as a Specific Support Action in 
2004. The consortium also aims at creat-
ing a sound basis for a joint master pro-
gram in the field of language technology 
and computer science. 
1 European higher education: Erasmus 
Mundus 
The Erasmus Mundus programme [1] is a co-
operation and mobility program in the field of 
higher education. It aims to enhance quality in 
European higher education and to promote inter-
cultural understanding through co-operation with 
non-EU countries. 
The program is intended to strengthen European 
co-operation and international links in higher edu-
cation by supporting high-quality European Mas-
ters Courses, by enabling students and visiting 
scholars from around the world to engage in post-
graduate study at European universities, as well as 
by encouraging the outgoing mobility of European 
students and scholars towards non-EU countries. 
The Erasmus Mundus program comprises four 
concrete actions: 
 
ACTION 1 - Erasmus Mundus Masters Courses: 
high-quality integrated courses at masters level 
offered by a consortium of at least three universi-
ties in at least three different European countries.  
 
ACTION 2 - Erasmus Mundus scholarships: a 
scholarship scheme for non-EU-country graduate 
students and scholars from the whole world. 
 
ACTION 3 - Partnerships: Erasmus Mundus Mas-
ters Courses selected under Action 1 also have the 
possibility of establishing partnerships with non-
EU-country higher education institutions.  
 
ACTION 4 - Enhancing attractiveness: projects 
aimed at enhancing the attractiveness of the Euro-
pean higher education.  
2 LATER 
One of the projects approved for funding (and the 
only one in the field of language technology) in the 
2004 call is called LATER ? Language Technol-
ogy Erasmus Mundus [2]. 
LATER falls under action 4 of the program and 
hence addresses the need to enhance the attractive-
ness of European higher education in Language 
43
Technology and Communication (LCT). This need 
will be met through dissemination of the combined 
LCT-related expertise in of a consortium of Uni-
versities whose members are as follows 
 
Saarland University in Saarbruecken (CoLi) 
The Department of Computational Linguistics 
and Phonetics (CoLi) of Saarland University (co-
ordinator) has an excellent international reputation 
for graduate training in Language Technologies, 
and for leading-edge basic research in this area. 
CoLi offers a new M.Sc. program in Language 
Science and Technology [3]. This is an active pro-
gram of basic, applied and cognitive research, 
which combines with state-of-the-art facilities to 
provide students with a rich and stimulating envi-
ronment for their research. Moreover, CoLi offers 
a European Ph.D. program in Language Technol-
ogy and Cognitive Systems. In the past 15 years, 
CoLi has provided postgraduate research training 
to 100 early-stage researchers [4]. 
 
Charles University, Prague (?FAL) 
The Institute of Formal and Applied Linguistics 
(?FAL) at the Faculty of Mathematics and Physics 
of the Charles University in Prague offers a five-
year master program in Computer Science with 
several specialized branches. One of the branches 
of this program is the masters in Computational 
and Formal Linguistics [7]. It focuses mainly on 
the following four topics: formal description of 
natural language, grammars and automata in lin-
guistics, methods of artificial intelligence in lin-
guistics, as well as methods of automatic natural 
language processing.  
 
University of Malta (UoM) 
The Department of Computer Science and Arti-
ficial Intelligence at the University of Malta, estab-
lished in 1993, teaches both Bachelors and Masters 
degree programs. The 4-year BSc. (Hons) scheme 
include several streams relevant to Language 
Technology including NLP and Computational 
Linguistics itself, Information Retrieval, Semantic 
Web, Internet and Agent technologies. The De-
partment also runs a, one-year research oriented 
M.Sc. program [10]. The areas of specialization 
include the development of computational tools, 
techniques and resources for Maltese, the only se-
mitic language to enjoy official EU status.  
 
Copenhagen Business School (CBS) 
The Department of Computational Linguistics 
is part of the Faculty of Modern Languages at the 
Copenhagen Business School. The Department is 
actively involved in research in the following four 
core fields: formal descriptions of the Danish lan-
guage, modeling of knowledge relevant for LSP, 
LSP databases, and Machine Translation. Embed-
ded in this context is the Master of Language Ad-
ministration (MLA) [9] that the Department of 
Computational Linguistics of the Copenhagen 
Business School offers in co-operation with the 
University of Southern Denmark in Roskilde  
3 Overall aims of the project 
The overall aim of the project is to export the 
common educational experience currently embod-
ied within existing Masters programs of the con-
sortium to scholars and students of non-EU 
countries. 
This aim will be realized by several different 
classes of activity under the rubrics of (i) work-
shops (ii) distance learning tools and (iii) coordina-
tion of a common Master program. We discuss 
these in the following sections. 
3.1 Workshops 
One of the most important types of activities of 
the project is organizing workshops and courses 
both for students from non-EU countries and for 
their teachers. The effect of these events is at least 
twofold ? the students from countries or regions 
which do not have an access to any higher degree 
education in LCT get a chance to broaden their 
perspective by listening to lectures of prominent 
scientists and lecturers. The courses will also help 
the consortium to establish better contacts with 
non-EU Universities, teachers, and students which 
will turn out to be invaluable when disseminating 
the common European Master program in Lan-
guage Technology discussed further below. 
Both ?FAL and CoLi have a long tradition in 
respect of offering such courses to students from 
the broadest possible range of countries. 
?FAL has devoted a huge effort in the past to 
raise funding for the organization, once or twice a 
year, of a series of lectures by prominent scientists 
and lecturers from all over the world. This series of 
lectures, the Vilem Mathesius courses [6], have 
become well-known, especially among the Central 
44
and East European students of computational and 
general linguistics.  
This year?s course, held in March under the aus-
pices of LATER, was able to support  the atten-
dance of 50 students from Russia, Ukraine, 
Albania, Bosnia, Serbia, Croatia and Georgia to 
lectures by prominent individuals including two 
ACL award winners. 
At CoLi, the Computational Linguistics Collo-
quium is also a traditional event attracting the at-
tention of both well-known lecturers and a number 
of master and postgraduate students from various 
countries. A second series of lectures in the frame 
of our project was held at the University of Saar-
landes in Saarbruecken in January. 
A third event, organized by the CBS, will take 
place in June. The first day consists of information 
seminar on content management and language 
technology to promote CBS? newly-launched In-
ternational Master of Language Administration, 
whilst the second will be devoted to diffusion of a 
various issues connected to the Erasmus Mundus 
course.  
Finally, a fourth event, in the form of a work-
shop with invited guest lecturers, is being organ-
ized at the University of Malta that will take place 
in September 2005. The theme of the workshop 
will be Machine Translation which is currently 
very topical given the newly-achieved official 
European status that the local language now enjoys.  
3.2 Coordination of Masters Programs 
A second important aim of the LATER project 
is the definition, coordination and implementation 
of an integrated European Masters Programme in 
LCT by creating a common basis that will appeal 
to both European and non-EU students. 
The rationale behind the creation of such a pro-
gramme is the assumption that LCT now occupies 
a central position in research and education in 
Europe, being a key enabling technology for nu-
merous applications related to the information so-
ciety, although the shortage of qualified 
researchers and developers is slowing down the 
speed of innovation in Europe. 
The proposed programme addresses this short-
age by creating a directed education and training 
opportunity for the next generation of LCT innova-
tors in that will in turn bring educational, social 
and economic benefits. Some specific aims of 
Erasmus Mundus are also addressed: European 
education in LCT will be promoted worldwide and 
its competitiveness increased, increasing at the 
same time the competitiveness of European IT in-
dustries, creating a multilingual information soci-
ety that is accessible for all, and turning the 
``information overload'' into a wealth of accessible 
and useful knowledge. 
3.3 Distance learning tools 
A third aim of LATER is the development of 
effective methods of hosting and integrating non-
EU students, for example by developing distance 
learning tools and joint distance education modules, 
in order to facilitate outreach by online dissemina-
tion of courses. An example of such modules, as 
well as for computer-based tools, is being devel-
oped on the basis of the virtual courses CoLi has 
developed in the last 3 years in the framework of 
the MiLCA project (Medienintensive Lehrmodule 
in der Computerlinguistik-Ausbildung1). 
We also plan to explore the use of collaboration 
technologies based on Sitescape [16], that have 
been developed at CBS for academic collaboration, 
for the management of certain aspects of the pro-
posed Masters programme.
The fruits of various initiatives already under 
way at UoM will be exploited and extended during 
the life of the proposed course. These include in-
teractive web based course delivery [13], just-in-
time support based on P2P architectures [14], 
XML-based frameworks for online courses [15], 
the latter being developed within as a part of the 
Mediterranean Virtual University (MVU) 
EUMEDIS project [17]. 
 
4 Integrated European LCT Masters 
Programme 
Whilst many agree with the above assessment of 
the importance of LCT, they disagree on the defi-
nition of ?integrated course?. Fortunately, we can 
turn to the comprehensive definition supplied by 
the EU call, the central element of which is ?a 
jointly developed curriculum or full recognition by 
the consortium of modules which are developed 
                                                          
1 for more see http://milca.sfs.uni-
tuebingen.de/index.html. 
45
and delivered separately, but make up a common 
standard Masters course.? 
Again, some turn away in horror at the notion of 
a standard curriculum in this area, the claim being 
that there is already enough standardization in the 
world, so why add to it? The point is, any pro-
gramme dealing with LCT has to address the fact 
that it is highly interdisciplinary, including, at the 
core, computer science, computational and theo-
retical linguistics, and mathematics, and at the pe-
riphery, a wide variety of other subjects including 
electrical engineering, psychology, cognitive sci-
ence artificial intelligence etc.  
With such a large number of disciplines in-
volved, it is practically impossible for a single 
University to excel in all of them. However if more 
than one University is involved, various kinds of 
curriculum sharing can be envisaged and so a 
much higher level of coverage becomes entirely 
achievable.  
Put another way, curriculum sharing, together 
with common admission and assessment proce-
dures envisaged, allows delivery of a complex 
course to be handled by what is effectively a ?su-
peruniversity?. 
4.1 Integration in practice 
To put this idea into practice we are proposing 
that students will get the chance to attend a two 
years? master program at two universities chosen 
from a larger consortium, which is currently being 
put together. It includes the four original partners 
of the LATER project and the following new part-
ners: University of Amsterdam (UvA) in the Neth-
erlands, Free University of Bolzano-Bozen (FUB) 
in Italy, the Universities of Nancy 1 and Nancy 2 
in France, Roskilde University in Denmark and 
Utrecht University in the Netherlands. 
Studying in multi-national groups at two uni-
versities in Europe, with English as instruction 
language, accompanied by language classes in an-
other European language, will contribute to the 
students' preparation for the increasing globaliza-
tion of science, commerce and industry. The 
course also will also prepare students for follow-up 
Ph.D. studies provided by the participating partners 
and others. 
The proposed programme follows the Bologna 
model for higher education in Europe and com-
prises 120 ECTS2 credits, 30 of which make up the 
Masters dissertation, and 90 of which are course-
work credits structured as follows: 
? Compulsory modules in Computer Science (28 
ECTS) 
? Compulsory modules in Language Technology 
(28 ECTS) 
? Advanced modules in Language Technology, 
Computational Linguistics and Computer Sci-
ence (34 ECTS) 
Coursework is distributed over three semesters, 
while the dissertation is supposed to be completed 
in the fourth semester  
It is important to underline that this structure 
permits a considerable degree of variation. First, a 
module might be ?implemented? by different set of 
courses at different Universities. Secondly, the ad-
vanced modules are electives, based on the specific 
strengths in research and teaching of individual 
partner institutions. There is no requirement that 
the advanced modules offered by different Univer-
sities should coincide. 
Let us now introduce individual modules in 
more detail. Parentheses indicate ECTS credits. 
Computer Science Modules 
The Computer Science Modules are as follows:  
 
? Logic, Computability and Complexity (? 9) 
Topics: Logic & inference; Computability the-
ory; Complexity theory; Discrete mathematics 
? Formal Languages and Algorithms (? 9) 
Topics: Formal grammars and languages hier-
archy; Parsing and compiler design; Search 
techniques and constraint resolution; Auto-
mated Learning 
? Data Structures, Data Organization and 
Processing (? 6) 
Topics: Algebraic data types; Relational data-
bases; Semi-structured data and XML; Informa-
tion retrieval; Digital libraries 
? Advanced Modules and Applications(? 6) 
Topics: Artificial Intelligence, Knowledge 
?epresentation, Automated Reasoning, 
Semantic Web, Neural Networks, Machine 
Learning etc. Students are expected to obtain at 
least 9 ECTS credits from each of the first two 
                                                          
2 European Credit Transfer System: a standard measure that is 
used in Europe for comparing the size of courses. 
46
modules and 6 ECTS credits from each of the 
remaining two modules.  
Language Technology Modules 
The Language Technology Modules are these: 
? Foundations of Language Technology (? 6) 
Topics: Statistical methods; Symbolic methods; 
Cognition; Corpus Linguistics; Text and 
speech; Foundations of Linguistics 
? Computational Syntax and Morphology (? 9) 
Topics: Finite state methods; Probabilistic ap-
proaches; Formal grammars; Tagging; Chunk-
ing; Parsing 
? Computational Semantics, Pragmatics and 
Discourse (? 6) 
Topics: Syntax-semantics interface; Semantic 
construction; Dialogue; Formal semantics 
? Advanced Modules and Applications 
(? 6) Topics: Machine Translation, Informa-
tion Retrieval, Speech Recognition, Question 
Answering, Psycholinguistics etc.. 
4.2 Main issues to be addressed  
Although it was not explicitly mentioned in the 
previous text, the integration of existing master 
programmes is done exclusively pair-wise. The 
students can?t study at three universities (although 
the rules of the Erasmus Mundus programme allow 
such triangular cooperation). The restrictions 
within our consortia go even further ? the students 
do not have a free choice of a combination of any 
two universities from within the consortium, they 
must choose one of the pairs offered by the consor-
tium. 
The reason for such a restriction is pretty simple 
- it turned out that although all members of the 
consortia in principle provide education both in 
Computer Science and in Computational Linguis-
tics, they differ in the balance between these two 
fields. Within the consortium, there are universities 
with a strong stress on a Computer Science courses, 
aiming at a complex education including the sound 
theoretical background in the field, while other 
universities offer a more practically oriented edu-
cational scheme, stressing the concepts attracting a 
wider audience, e.g. various types of web tech-
nologies, databases, data mining etc.  
As a result of this, each university participates in 
an average of four bilateral partnerships. We think 
that the fact that the consortium consists of univer-
sities which are not identical greatly increases the 
variety of options available. They have a chance to 
choose those universities which are best suited to 
their preferences whether these are in terms of sub-
ject area emphasis or geographical region.  
The preparation of the integrated Master pro-
gramme doesn?t stop at matching the universities 
and lectures offered. Erasmus Mundus is not just a 
cooperation, it is really a completely new scheme 
which must also address practical issues as grades, 
examination procedures, admission procedure, tui-
tion fees, defense of the thesis, local specialties 
existing at some partner universities etc.
The proposed Masters programme is something 
new. It is the first attempt to create a comprehen-
sive Masters degree in this subject area that con-
forms to all the legalistic requirements of each 
participating University. Students completing the 
course will possess a Masters degree delivered by 
two of the participant Universities. This is in con-
trast to the existing European Master in Language 
and Speech [11], which is implemented through a 
certification procedure that does not replace any 
legal degree that a student may obtain from a Uni-
versity.
 
5 Conclusion 
Although the process of establishing a new Euro-
pean Master programme in Language Technology 
was really very complicated, time consuming and 
painful, there are definitely already at this stage 
very positive results. 
In order to submit a proposal, our consortium 
has managed to overcome all formal and structural 
differences among all partners, it has found a rea-
sonable model of cooperation, it has developed a 
high-quality master programme open both to Euro-
pean and non-EU students. 
The wide variety of modules and topics offered 
combined with a relatively high degree of freedom 
of choice for students allows for individual pairs of 
partner universities to promote those courses and 
fields in which they excel. The students are of 
course offered individual guidance from consor-
tium members in order to allow them to identify 
that pair of universities which best suits their indi-
vidual needs and preferences 
47
The strategy we have chosen ? the initial coop-
eration of a smaller consortium in the LATER pro-
ject, promoting LTC education among the students 
from outside the EU and testing our ability both to 
offer a coordinated high-quality education and to 
attract a reasonable amount of interested students, 
has turned to be a sound one. It also helped to 
solve some issues in the larger consortium based 
on the experience from the smaller one. 
References  
[1] http://europa.eu.int/comm/education
/programmes/mundus/index_en.html 
(Erasmus Mundus web page) 
[2] http://europa.eu.int/comm/education
/programmes/mundus/projects/2004/47
.pdf (The description of the LATER pro-
ject) 
[3] http://www.coli.uni-
saarland.de/msc/ (the MSc website at 
the University of Saarlandes in Saar-
bruecken) 
[4] http://www.coli.uni-
saarland.de/kvv/ (courses at the Dept. 
of Computational Linguistics at the Uni-
versity of Saarlandes in Saarbruecken) 
[5] http://www.coli.uni-
saarland.de/courses/late2/ (the web 
page of the Language Technology II 
course in Saarbruecken) 
[6] http://ufal.mff.cuni.cz/vmc/vmc_ls2
0.html (the web page of the Vilem 
Mathesius Lecture Series) 
[7] http://www.mff.cuni.cz/toUTF8.en/st
udium/bcmgr/ok/i1b53.htm (the master 
programme in Mathematical Linguistics at 
the Charles University in Prague) 
[8] http://web.cbs.dk/stud_pro/clmdatau
k.shtml (the master program at the Co-
penhagen Business School) 
[9] http://uk.cbs.dk/mla (Master of Lan-
guage Administration at the Copenhagen 
Business School) 
[10] http://www.cs.um.edu.mt/rese
arch/pgEnquiries.html (the master 
program at the University of Malta) 
[11] http://www.cstr.ed.ac.uk/e
uromasters (European Masters in 
Language and Speech) 
[12] A.Burchardt, S. Walter and M. 
Pinkal. 2004. "MiLCA -- Distance Educa-
tion in Computational Linguistics". In 
Szucs, Andras and Bo, Ingeborg 
(eds.),  New Challenges and Partnerships 
in an Enlarged European Union ? Proc. 
2004 EDEN Conference, Budapest, pp. 
351-356. 
[13] Ellul, C., 2002, ?Just-in-Time Lec-
ture Delivery, Management and Student 
Support System?, BSc. Project report, 
Dept. CSAI, University of Malta. 
[14] Bezzina, R., 2002, ?Peer-to-Peer 
Just-in-Time Support for Curriculum based 
Learning?, BSc. Project report, Dept. 
CSAI, University of Malta. 
[15] Cachia, E., and Micallef, M., forth-
coming, ?A Universal XML/XSLT 
Framework for Online Courses?, Proc. In-
ternational Conference  on IT-Based 
Higher Education And Training (ITHET)?, 
Dominican Republic. 
[16] www.sitescape.com : SiteScape 
corporate website. 
[17] http://www.eumedis.net/en/project/
22: Mediterranean Virtual University 
(MVU) description. 
48
Coling 2008: Proceedings of the workshop on Multi-source Multilingual Information Extraction and Summarization, pages 25?32
Manchester, August 2008
MultiSum
Query-Based Multi-Document Summarisation
Michael Rosner
Dept. Artificial Intelligence
University of Malta
mike.rosner@um.edu.mt
Carl Camilleri
Dept. Artificial Intelligence
University of Malta
ccam0002@um.edu.mt
Abstract
This paper describes a generic, open-
domain multi-document summarisation
system which combines new and exist-
ing techniques in a novel way. The sys-
tem is capable of automatically identify-
ing query-related online documents and
compiling a report from the most use-
ful sources, whilst presenting the result in
such a way as to make it easy for the re-
searcher to look up the information in its
original context.
1 Introduction
Although electronic resources have several in-
herent advantages over traditional research me-
dia, they also introduce several drawbacks, such
as Information Overload (Edmunds and Morris,
2000),which has become synonymous with the in-
formation retrieval phase of any research-related
task. Another problem which is directly related
to the one just described is that of Source Iden-
tification (Eppler and Mengis, 2004). This refers
to the problem of having relevant results intermin-
gled with results that are less relevant, or actually
irrelevant.
Lastly, the researcher usually has to also manu-
ally traverse the relevant sources of information in
order to form an answer to the research query.
These problems have led to the study of vari-
ous areas in computing, all of which aim to try and
minimise the manual effort of information retrieval
and extraction, one of which is Multi-Document
Summarisation (MDS).
The core aim of any MDS system is that of pro-
cessing multiple sources of information and out-
putting a relatively brief but broad report or sum-
c
? 2008. Licensed under the Creative Commons
Attribution-Non ommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
mary. Uses of MDS systems vary widely, from
summarisation of closed-domain documents, such
as news documents (Evans et al, 2004), to aggre-
gation of information from several sources in an
open domain.
2 Aims and Objectives
MDS techniques can be used in various tools that
may help addressing the problems described in
Section 1. On the other hand, a brief study of the
relevant literature indicates that the majority of the
work done in this area concerns closed-domains
such as news summarisation, which is perhaps the
reason why such tools have not yet become more
popular. The objectives of this study are thus
twofold.
? The primary objective is that of design-
ing, implementing and evaluating an open-
domain, query-based MDS system which is
capable of compiling an acceptably-coherent
report from the most relevant online sources
of information, whilst making it easy for the
reader to access the full source of information
in its original context.
? A secondary objective of this study is Search
Engine Optimisation (SEO): We require the
system to produce summaries which, if pub-
lished on the Internet, would be deemed rel-
evant to the original query by search en-
gine ranking algorithms. This is measured
by keyoword density in the summary. Suc-
cess on this objective addresses the problem
of Source Identification since the summary
would at the very least serve as a gateway to
the other relevant sources from which it was
formed.
Unsurprisingly, one of the problems that has to
be overcome in the field of summarisation and par-
ticularly in an open-domain system such as ours is
25
the quality of output, as measured by a number of
different linguistic and non-linguistic criteria (see
Section 5). We have adopted a number of novel
techniques to address this such as
? Multi-Layered Architecture
? Sentence Ordering Model
? Heuristic Sentence Filtering
? Paragraph Clustering
3 Background
3.1 Search Engine Ranking Criteria
Search engine ranking algorithms vary, and are
continuously being optimised in order to provide
better and more accurate results. However, some
guidelines that outline factors which web masters
need to take into account have been established (cf.
Google (2007), Vaughn (2007)).
When ranking documents for a particular search
query, ranking algorithms take into account both
on-page and off-page factors. Off-page factors
comprise mainly the number and quality of in-
bound links to a particular page, whilst on-page
factors comprise various criteria, most important
of which is the relevance of the content to the
search query.
3.2 Multi-Document Summarisation
Several different approaches and processes have
been developed in automatic MDS systems. These
vary according to the problem domain, which usu-
ally defines particular formats for both input and
output. However, five basic sub-systems of any
MDS system can be identified (Mani, 2001).
1. Unit Identification During this first phase,
input documents are parsed and tokenised
into ?units?, which can vary from single
words to whole documents, according to the
application problem.
2. Unit Matching (Clustering) The second
stage involves grouping similar units together.
In the context of MDS, similar units usu-
ally mean either identical or informationally-
equivalent strings (Clarke, 2004), with the
purpose of discovering the main themes in the
different units and identify the most salient
ones.
3. Unit Filtering The filtering stage eliminates
units residing in clusters which are deemed to
be non-salient.
4. Compacting During this phase, it is often as-
sumed that different clusters contain similar
units. Thus, a sample of units from different
clusters is chosen.
5. Presentation/Summary Generation The
last phase of the MDS process involves using
the output from the Compacting stage, and
generating a summary. Usually, na??ve string
concatenation does not produce coherent
summaries and thus, techniques such as
named entity normalisation and sentence
ordering criteria are used at this stage.
3.3 Clustering Techniques
As outlined in Section 3.2, MDS often makes use
of clustering techniques in order to group together
similar units. Clustering can be defined as a pro-
cess which performs ?unsupervised classification
of patterns into groups based on their similarity?
(Clarke, 2004).
A particular clustering technique typically con-
sists of three main components:
1. Pattern Representation
2. Similarity Measure
3. Clustering Algorithm
The very generic nature of our problem domain
requires a clustering technique which is both suit-
able and without scenario-dependant parameters.
Fung?s algorithm (Fung et al, 2003), comprising a
pre-processing stage and a further three-phase core
process, uses the following concepts, and is briefly
described in Figure 1.
ItemSet A set of words occurring together
within a document. An ItemSet composed of k
words is called a k-ItemSet.
Global Support The Global Support of a word
item is the number of documents from the docu-
ment collection it appears in (cf. document fre-
quency).
Cluster Support The Cluster Support of a word
item is the number of documents within a cluster it
appears in.
26
1. Pre-Processing - stem, remove stop
words and convert to TFxIDF represen-
tation
2. Discover Global Frequent ItemSets
3. For each Global Frequent ItemSet (GFI)
create a corresponding cluster, contain-
ing all documents that contain all items
found within the GFI associated with
each cluster. This GFI will act as a ?la-
bel? to the cluster.
4. Make Clusters Disjoint
Figure 1: Hierarchical Document Clustering Using
Frequent Itemsets
Frequent ItemSet An ItemSet occurring in a
pre-determined minimum portion of a document
collection. The pre-defined minimum is referred
to as the Minimum Support, and is usually deter-
mined empirically according to the application.
Global Frequent ItemSet An ItemSet which is
frequent within the whole document collection.
The words within a Global Frequent ItemSet are
referred to as Global Frequent Items, whilst the
minimum support is referred to as the Minimum
Global Support.
Cluster Frequent ItemSet An ItemSet which
is frequent within a cluster. In this context, the
minimum support is referred to as the Minimum
Cluster Support.
With these definitions, it is now possible to de-
scribe into more detail the core non-trivial phases
of the algorithm.
3.3.1 Discovering Global Frequent ItemSets
From the definition of an ItemSet, it can be con-
cluded that the set of ItemSets is the power set of
all features
1
within the document collection. Given
even a small document collection, enumerating all
the possible ItemSets and checking which of them
are Global Frequent would be intractable. In order
to discover Global Frequent ItemSets, the authors
recommend the use of the Apriori Candidate Gen-
eration algorithm, a data mining algorithm pro-
posed by Agrawal and Srikant (1994). This algo-
1
Features here constitute distinct, single words found in
the whole document collection. In practice, stemming is ap-
plied before feature extraction.
rithm defines a way to reduce the number of candi-
date frequent ItemSets generated. The generation
algorithm basically operates on the principle that,
given a set of frequent k-1-ItemSets, a set of can-
didate frequent k-ItemSets can be generated such
that each candidate is composed of frequent k-1-
ItemSets.
Agrawal and Srikant (1994) also mention a sim-
ilar algorithm proposed by Mannila et al (1994).
As illustrated in Figure 2, this algorithm consists
of first generating candidates, and then pruning the
result based on a principle similar to that men-
tioned.
3.3.2 Making Clusters Disjoint
The purpose of the last phase of the algorithm is
converting a fuzzy cluster result to its crisp equiva-
lent. In order to identify the best cluster for a doc-
ument contained in multiple clusters, the authors
define the scoring function illustrated in the equa-
tion of Figure 3, where x is a global and cluster-
frequent item in doc
j
, x
?
a global frequent but not
cluster frequent item in doc
j
, and n(x) a weighted
frequency (TF.IDF) of feature x in doc
j
.
Using this function, the best cluster for a partic-
ular document is that which maximises the score.
In case of a tie, the most specific cluster (having
the largest number of labels) is chosen.
4 Procedure
The system was designed in two parts, namely a
simple web-based user interface and a server pro-
cess responsible for iterating sequentially over user
queries and performing the content retrieval and
summarisation tasks. The following sections de-
scribe the various sub-systems that compose the
server process.
4.1 Content Retrieval
The Content Retrieval sub-system is responsible
for retrieving web documents related to a user?s
query. This is done simply by querying a search
engine and retrieving the top ranked documents
2
.
Although throughout the course of this study the
system was configured to use only Google as its
document source, the number of search engines
that can be queried is arbitrary, and the system can
2
It was empirically determined that retrieving the top 30
ranked documents achieved the best results. Considering
less documents meant that, in most scenarios, main relevant
sources were missed, whilst considering more documents
caused the infiltration of irrelevant information
27
1. Join
C
k
= {X ?X
?
|X,X
?
? L
k?1
, |X ?X
?
| = k ? 2}
2. Prune
C
k
= {X ? C
?
k
|X contains k members of L
k?1
}
Figure 2: Candidate Generation Algorithm by Mannila et al (1994)
Score(C
i
? doc
j
) =
?
x
n(x)? cluster support(x)?
?
x
?
n(x
?
)? global support(x
?
)
Figure 3: Definition of Scoring Function
be given a set of parameters to query a particular
search engine.
4.2 Content Extraction
The Content Extraction module is responsible for
transforming the retrieved HTML documents into
raw text. However, a simple de-tagging process
is not sufficient. This module was designed so as
to be able to identify the main content of a web
document, and leave out other clutter such as nav-
igation menus and headings. Finn et al (2001) in-
troduce a generic method to achieve this, by trans-
lating the content extraction problem to an optimi-
sation problem. The authors observe that, essen-
tially, an HTML document consists of two types of
elements, that is, actual text and HTML tags. Thus,
such a document can easily be encoded as a binary
string B, where 0 represents a natural word, whilst
1 represents and HTML tag. Figure 4 shows a typ-
ical graphical representation obtained when cumu-
lative HTML tag tokens are graphed against the
cumulative number of tokens in a typical HTML
document.
Finn et al (2001) suggest that, typically, the
plateau that can be discerned in such a graph con-
tains the actual document content. Therefore, in
order to extract the content, the start and end point
of the plateau (marked with black boxes in Figure
, and referred to hereafter as i and j respectively)
must be identified.
The optimisation problem now becomes max-
imisation of the number of HTML tags below i
and above j, in parallel with maximisation of the
number of natural language words between i and
j. The maximisation formula proposed by the au-
thors is given by Equation 1.
Figure 4: Total HTML Tokens VS Total Tokens
(Finn et al, 2001)
T
i,j
=
i?1
?
n=0
B
n
+
j
?
n=i
(1?B
n
)+
N?1
?
n=j+1
(1?B
n
) (1)
Our Content Extraction module is further de-
composed into three sub-modules. The first is a
pre-processing module, which parses out the body
of the HTML document, and removes superfluous
content such as scripts and styling sections. The
second and core sub-module consists namely of an
implementation of the content extraction method
introduced by Finn et al (2001), which is pri-
marily responsible for identifying the main con-
tent section of the input document. The last post-
processing module then ensures that the output
from the previous sub-modules is converted to raw
text, by performing an HTML detagging process-
ing and also inserting paragraph marks in places
where they are explicit cf. HTML <p> tag) or
where an element from a predefined set of HTML
text break delimiters occurs.
28
4.3 Summarisation
The overall design of the core summarisation mod-
ule is loosely based upon the two-tiered MDS
architecture introduced by Torralbo et al (2005)
The following sections map our system to a sim-
ilar two-tiered architecture, and explain how each
module operates.
Document Identification
Document Identification is trivial, since docu-
ments are explicitly defined by the content retrieval
module, the output of which is basically a set of
query-related text documents.
Document Filtering
The job of Document Filtering is partially done
at the very beginning by the search engine. How-
ever, our system further refines the document col-
lection by pre-processing each document, apply-
ing a noise
3
removal procedure, stemming and stop
word and rare word removal. Each document is
then converted to a bag of words, or the Vector
Space Model, where each word is associated with
its corresponding TF?IDF measure. Any docu-
ment which, after pre-processing, ends up with an
empty bag of words, is filtered out from the doc-
ument collection. Furthermore, in order to ensure
the robustness of the system especially in subse-
quent intensive processing, documents which are
longer than 5 times the average document length
are truncated.
Paragraph Identification
As outlined in Section 4.2, the Content Extrac-
tion sub-system inserts paragraph indicators in the
text wherever appropriate. Thus, the paragraph
identification phase is trivial, and entails only split-
ting the content of a document at the indicated po-
sitions.
Paragraph Clustering and Filtering
In contrast to the technique of Torralbo et al
(2005), a paragraph filtering module was intro-
duced in order to select only the most informa-
tive, query-related paragraphs. To achieve this,
we implemented the clustering technique out-
lined in Section 3.3 in order to obtain clusters of
thematically-similar paragraphs, using the Global
Frequent ItemSet generation technique from Man-
nila et al (1994) and setting the Minimum Global
3
?Noise? refers to any character which is not in the En-
glish alphabet.
1. For each paragraph p
k
(a) Initialise the target summary Sum
k
as an empty text
(b) Let p = p
k
(c) Remove the first sentence s from p,
and add it at the end of Sum
k
.
(d) Calculate the similarity between s
and the first sentence of all the para-
graphs, using the size of the inter-
section of the two vectors of words
as a similarity metric.
(e) Let p be the paragraph whose first
sentence maximises the similarity,
and go back to step (c) with that
paragraph. If the best similarity is
0, stop.
2. Choose the longest one of the k different
summaries.
Figure 5: Summary Generation Algorithm (Tor-
ralbo et al, 2005)
Support and Minimum Cluster Support parameters
to 35 and 50 respectively.
The filtering technique then consists of simply
choosing the largest cluster. This is based on the
intuition that most of the paragraphs having the
central theme as their main theme will get clus-
tered together. Therefore, choosing the largest
cluster of paragraphs would filter out irrelevant
paragraphs. This paragraph filtering method may
filter out paragraphs which are actually relevant,
however, we rely on the redundancy of informa-
tion usually found in information obtained from
the web. Thus, the paragraph filtering gives more
importance to filtering out all the irrelevant para-
graphs.
Summary Generation
The role of the summary generation module is
to generate a report from a cluster of paragraphs.
We based our summary generation method on that
used by Torralbo et al (2005), which is illustrated
in Figure 5. However, in order to make it more
applicable to our problem domain and increase the
output quality, we introduced some improvements.
Sentence Ordering Model We introduced a
probabilistic sentence ordering model which en-
ables the algorithm to choose the sentence that
29
maximises the probability given the previous sen-
tence. The sentence ordering model, based on a
method of probabilistic text structuring introduced
by Lapata (2003), is trained upon the whole doc-
ument collection. We used Minipar (Lin, 1993),
a dependency-based parser, in order to identify
verbs, nouns, verb dependencies and noun depen-
dencies. Using counts of these features and Sim-
ple Good-Turing smoothing (Gale and Sampson,
1995), we were able to construct a probabilistic
sentence ordering model such that, during sum-
mary generation, given the previous sentence, we
are able to identify the sentence which is the most
likely to occur from the pool of sentences appear-
ing at the beginning of the remaining paragraphs.
Sentence Filtering We also introduced at this
stage a method to filter out sentences that decrease
the coherency and fluency of the resultant sum-
mary. This is based on two criteria:
1. Very low probability of occurrence
If the most likely next-occurring sentence that
is chosen and removed from a paragraph still
has a very low probabilistic score, it is not
added to the output summary.
2. Heuristics
We also introduce a set of heuristics to filter
out sentences having a wrong construction or
sentences which would not make sense in a
given context. These heuristics include:
(a) Sentences with no verbs
(b) Sentences starting with an adverb and
occurring at a paragraph transition
within the summary
(c) Sentences occurring at a context switch
4
within the summary and starting with a
word matched with a select list of words
that usually occur as anaphora
(d) Malformed sentences (including sen-
tences not starting with a capital letter
and very short sentences)
5 Evaluation
5.1 Automatic Evaluation
5.1.1 Coherence Evaluation
In order to evaluate the local coherence of the re-
ports generated by the system, we employed an au-
4
Context Switch refers to scenarios where a candidate sen-
tence comes from a different document than that of the last
sentence in the summary.
tomatic coherence evaluation method introduced
by Barzilay and Lapata (2005)
5
. The main objec-
tive of this part of the evaluation phase was to de-
termine the effect on output quality when param-
eters are varied, namely the minimum cluster sup-
port parameter for the clustering algorithm, and the
key phrase popularity.
From this evaluation, we empirically deter-
mined that the optimum minimum cluster support
threshold for this application is 50, whilst the qual-
ity of the output is directly proportional to the key-
word popularity.
5.1.2 Keyword Density Evaluation
Here we focused on determining whether the
secondary objective was achieved (cf. section 2).
We measured the frequency of occurrence of the
keyword phrase within the output, or more specifi-
cally, the keyword density. The average key phrase
density achieved by the system was 1.32%, when
taking into account (i) the original keyword phrase
and its constituent keywords, and (ii) secondary
keyword phrases and their constituents.
5.2 Manual Quality Evaluation
In order to measure the quality of the output and
determine whether the objectives of the study was
achieved, three users were introduced to the sys-
tem and asked to grade the system, on a scale of
1-5, on several criteria. Table 1 illustrates the re-
sults obtained from this evaluation.
6 Conclusions
6.1 Interpretation of Results
In this section we will identify some conclusions
elicited from the results obtained from the evalua-
tion phase and illustrated in Section 5.
Automatic Coherence Evaluation The auto-
matic coherence evaluation tests, although, in this
application, the level of ?coherence? indicated did
not match that of manual evaluation, provided
nonetheless a standard by which different outputs
from the system using different parameters and ap-
plication scenarios could be compared. From the
results, we could empirically determine that the
optimal value for the cluster support parameter was
around 50%. Furthermore, unsurprisingly, the sys-
tem tends to produce output of a higher quality
5
Data required to set up the automatic coherence eval-
uation model was available from the author?s website
http://people.csail.mit.edu/regina/coherence/.
30
Grammaticality Non-Redundancy Referential Clarity Focus Structure Naturalness Usefulness
Average 3.62 2.21 4.03 4.28 3.27 2.76 4.78
Table 1: Results of Manual Evaluation
in scenarios where the keyword phrase is popular,
and thus more data is available.
SEO Evaluation From an SEO perspective, it
was predictable that the system would produce
query-related text, since its data source is ob-
tained from query-related search engine results.
However, the resulting average keyword density
achieved is significant, and is at a level which is
totally acceptable by most search engine ranking
algorithms
6
.
Manual Quality Evaluation Due to limited re-
sources, the results of the manual evaluation pro-
cedure were not statistically significant since only
three users were involved in evaluating six sum-
maries. However, allowing for a factor of sub-
jectivity, some conclusions could still be elicited,
namely:
1. The system did not perform well enough to
have its output rated as high as a manual sum-
marisation procedure. This can be concluded
from the low rating on the output Naturalness
criterion, as well as from the presence of re-
peated and irrelevant content in some of the
output summaries.
2. The system performed acceptably well in
generating reports that were adequately co-
herent and high-level enough to give an
overview of concepts represented by users?
queries. This can be concluded from the av-
erage scores achieved in the Focus and Refer-
ential Clarity criteria.
3. The evaluators were also asked to give a grade
indicating whether this system and similar
tools would actually be useful. A positive
grade was obtained on this criterion, indicat-
ing that the system achieved the MDS objec-
tive, enabling users to get a brief overview
of the topic as well as facilitating document
identification.
When comparing these results to those achieved
by Torralbo et al (2005), we can elicit two main
conclusions:
6
Very high keyword density (more than a threshold of 2%
- 5%) is usually considered as a spammy technique known as
keyword stuffing.
1. Although our system achieved lower rank-
ings on the Non-Redundancy, Structure and
Grammaticality criteria, these rankings were
not unacceptable. We could attributed this to
the more generic domain in which our sys-
tem operates, where it is not possible to in-
troduce fixed heuristics such as those used by
Torralbo et al (2005) for avoiding repeated
information by replacing a term definition by
its corresponding acronym. Such heuristics
tend to be relevant in the context of such a
term definition system.
2. Our system achieved higher grades on the
Referential Clarity and Focus criteria. Given
the fact that the system of Torralbo et al
(2005) retrieves results from search engines
in a similar way used by our system, the im-
provement Focus might be attributed to the
fact that our paragraph filtering methodol-
ogy tends to perform well in selecting only
the most relevant parts of the document base.
Furthermore, the improved grade achieved in
the Referential Clarity criterion might arise
from the more advanced sentence ordering
methodology used, as well as to the different
heuristic-based sentence filtering techniques
employed by our summary generation mod-
ule.
6.2 Limitations
The main limitation is that the quality of the output
is very susceptible to the quality and amount of re-
sources available. However, we also noticed a se-
vere fall in quality where results were largely com-
posed of business-oriented portals, which tend to
lack textual information. Furthermore, the output
summary is largely dictated by the results of search
engines. Therefore, the queries submitted to the
system must be formulated similarly to those sub-
mitted to search engines, since the system would
fail to generate a focused summary for queries
which, when submitted to traditional search en-
gines, return irrelevant results.
The system performance is also limited by the
quality and number of external components being
referenced, which are not state of the art and which
31
introduce performance bottlenecks by imposing a
batch-processing regime.
6.3 Final Conclusions
Our system combines several existing techniques
in a novel way. New techniques, such as our
Heuristic-Based Sentence Filtering algorithm, are
also introduced.
The primary objective of creating an MDS was
achieved albeit with limited ?coherency?. How-
ever, our system was considered a useful research
tool - supporting the hypothesis that a partially co-
herent but understandable report with minimum
effort is arguably better than a perfectly coherent
one, if the latter is unrealistically laborious to pro-
duce.
The secondary SEO objective was also
achieved, to the extent that the system generated
query-related content that has a natural level of
key phrase density. Such content has the potential
of being considered query-related also by search
engine ranking algorithms, if published within the
right context.
7 Future Work
There remains much is to be done. We propose:
? To increase the output quality and natural-
ness by focusing on an a sub-system for
anaphora identification and resolution which
would complement our probabilistic sentence
ordering model.
? To widen the scope by applying the system to
sources of information other than web docu-
ments.
? To convert our batch-processing system to an
interactive one by incorporating all the re-
quired tools within the same environment.
References
Agrawal, R. and Ramakrishnan Srikant. 1994. Fast
algorithms for mining association rules in large
databases. In VLDB?94, Proc. of 20th International
Conference on Very Large Data Bases, Sept. 1994,
Santiago de Chile, Chile, pages 487?499. Morgan
Kaufmann.
Barzilay, R. and M. Lapata. 2005. Modeling local
coherence: an entity-based approach. In ACL ?05:
Proc. 43rd Annual Meeting of the ACL, pages 141?
148, Morristown, NJ, USA. ACL.
Clarke, J. 2004. Clustering techniques for multi-
document summarisation. Master?s thesis, Univer-
sity of Sheffield.
Edmunds, A. and A. Morris. 2000. The problem of
information overload in business organisations: a re-
view of the literature. Int. Journal of Information
Management, 20(1):17?28.
Eppler, M.J. and J. Mengis. 2004. The Concept of
Information Overload: A Review of Literature from
Organization Science, Accounting, Marketing, MIS,
and Related Disciplines. The Information Society,
20(5):325?344.
Evans, D.K., J.L. Klavans, and K.R. McKeown. 2004.
Columbia Newsblaster: Multilingual News Summa-
rization on the Web. Proc. HLT Conference and the
NAACL Annual Meeting.
Finn, A., N. Kushmerick, and B. Smyth. 2001. Fact
or fiction: Content classification for digital libraries.
In DELOS Workshop: Personalisation and Recom-
mender Systems in Digital Libraries.
Fung, B.C.M., K. Wang, and M. Ester. 2003. Hier-
archical Document Clustering Using Frequent Item-
sets. Proc. of the SIAM International Conference on
Data Mining, 30.
Gale, W.A. and G. Sampson. 1995. Good-Turing Fre-
quency Estimation Without Tears. Journal of Quan-
titative Linguistics, 2(3):217?237.
Google. 2007. Google Webmaster Guidelines.
http://www.google.com/support/webmasters
/bin/answer.py?answer=35769.
Lapata, M. 2003. Probabilistic text structuring: Ex-
periments with sentence ordering. Proc. of the 41st
Meeting of the ACL, pages 545?552.
Lin, Dekang. 1993. Principle-based parsing without
overgeneration. In Meeting of the ACL, pages 112?
120.
Mani, I. 2001. Automatic Summarization. Computa-
tional Linguistics, 28(2).
Mannila, Heikki, Hannu Toivonen, and A. Inkeri
Verkamo. 1994. Efficient algorithms for discov-
ering association rules. In Fayyad, Usama M. and
Ramasamy Uthurusamy, editors, AAAI Workshop
on Knowledge Discovery in Databases (KDD-94),
pages 181?192, Seattle, Washington. AAAI Press.
Torralbo, R., E. Alfonseca, A. Moreno-Sandoval, and
J.M. Guirao. 2005. Automatic generation of
term definitions using multidocument summarisa-
tion from the Web. In Proc. Workshop on Crossing
Barriers in Text Summarisation Research, RANLP
Borovets.
Vaughn. 2007. Google Ranking Factors
- SEO Checklist. http://www.vaughns-1-
pagers.com/internet/google-ranking-factors.htm.
32
