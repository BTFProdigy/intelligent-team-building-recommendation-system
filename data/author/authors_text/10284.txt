Coling 2008: Companion volume ? Posters and Demonstrations, pages 79?82
Manchester, August 2008
Generation under Space Constraints 
C?cile Paris, Nathalie Colineau, 
Andrew Lampert 
CSIRO ? ICT Centre 
Locked Bag 17 
North Ryde, NSW 1670, Australia 
FirstName.LastName@csiro.au 
Joan Giralt Duran 
 
Barcelona School of Informatics 
Technical University of Catalonia 
Barcelona, Spain 
joangi@gmail.com 
 
 
Abstract 
Reasoning about how much to generate 
when space is limited is a challenge for 
generation systems. This paper presents 
two algorithms that exploit the discourse 
structure to decide which content to drop 
when there are space restrictions, in the 
context of producing documents from 
pre-authored text fragments. We analyse 
the effectiveness of both algorithms and 
show that the second is near optimal.  
1 Introduction 
Many organisations employ content management 
systems to store information, typically at the 
paragraph level. The use of such systems enables 
the application of NLG techniques without the 
cost of acquiring a knowledge base or forming 
text from first principles. But it brings its own 
challenge: how to produce a coherent and well 
structured text satisfying specific space 
requirements when a system has no control over 
the text at the sentence level? 
The ability to reason about space constraints 
becomes more pressing as the amount of 
available information increases and delivery 
channels become more diverse in terms of space 
requirements (e.g., web browsers, email, PDAs). 
We, as humans, address this problem by short-
ening our sentences or restricting the content we 
include. We achieve the former by manipulating 
vocabulary and syntax. This requires careful at-
tention to the text at sentence level and often 
does not reclaim significant amount of space.  
                                                 
? CSIRO 2008. Licensed under the Creative Com-
mons Attribution-Noncommercial-Share Alike 3.0 
Unported license (http://creativecommons.org/ 
licenses/by-nc-sa/3.0/). Some rights reserved. 
We achieve the latter by dropping those pieces of 
content whose contribution to the communicative 
goal is most limited. This approach can reduce a 
text?s length significantly but requires an 
understanding of the text?s discourse structure. 
In our application domain, we answer people?s 
information needs by retrieving content from a 
repository of pre-authored text fragments and 
delivering that content via a variety of media 
(e.g., web, paper, email), each with their own 
space constraints. In this paper, we show how we 
exploit the discourse structure to determine what 
should be realised to best fit some specific space. 
In particular, we present two algorithms that per-
form this reasoning and analyse their compara-
tive performance. 
2 Related Work 
NLG systems have exploited the discourse struc-
ture for a number of tasks ? e.g., to generate ap-
propriate cue phrases (e.g., Scott and de Souza, 
1990) or reason about layout (e.g., Bateman et 
al., 2001). Our system uses the discourse 
structure to reason about how much to realise to 
fit a specific space. It produces one discourse tree 
that is then realised for different delivery chan-
nels, each with its own space requirements.  
Like other systems (e.g., Moore and Paris, 
1993), our system specifies the RST relations 
(Mann and Thompson, 1988) that hold between 
text spans during discourse planning. It then ex-
ploits the RST principle of nuclearity to decide 
what to realise. The intuition is that nuclei are 
important while satellites can be dropped.  This 
intuition has been exploited in some systems to 
produce summaries (e.g., Sparck-Jones, 1993; 
Marcu, 1998). Our purpose is different, however. 
We do not aim to produce a summary but a text 
that fits into some space requirements, 
79
  
 
Figure 1. A brochure generated by our system 
potentially only slightly shortening a text. Our 
task brings new challenges, e.g., filling the space 
optimally and producing a balanced text.  
Our system exploits the notion that some rela-
tions are more important than others. O?Donnell 
(1997) used this principle to produce documents 
of variable length. In his approach, sentence 
fragments were manually marked up with RST 
and the text was manipulated at or below the sen-
tence level.  In our work, we cannot manipulate 
text at sentence level nor manually mark up the 
documents to be shortened. 
3 Reasoning about Space Constraints 
We focus on applications in which the generated 
text is delivered through several channels. One 
such application is SciFly, which produces 
tailored brochures about our organisation. Given 
a query from a user (topic(s) of interest), the 
system consults a staff directory and a repository 
of text fragments to gather relevant information. 
The fragments, written by our marketing team, 
are self contained and comprised of one or two 
paragraphs. SciFly integrates all the relevant 
information into a coherent whole (see Figure 1) 
using the meta-data describing each fragment?s 
content. A text fragment can be used with differ-
ent rhetorical relations in different brochures. 
The system produces a 2-page paper brochure, a 
web output, and a PDF version of the paper bro-
chure is emailed to the user with a summary in 
the email body. All outputs are generated from 
the same discourse structure by our algorithm. 
Our need to deliver the brochure via multiple 
channels led us to design algorithms that reason 
about the content to be expressed and the space 
available for each channel. The system follows a 
two-stage approach: during discourse planning, 
content and organisation are selected, and a dis-
course tree is built. The tree includes the top 
level communicative goal, intermediate goals 
and the rhetorical relations that exist between 
text spans, encoding both the purpose of each 
fragment and how they relate to each other. Then, 
at the presentation stage, the system reasons 
about this structure  to decide what to realise 
when there is too much content for some channel.  
We implemented and tested two algorithms 
for this reasoning. Both algorithms embody the 
principle of nuclearity and exploit the notion that 
some relations are more important than others. 
An importance value is assigned to relations 
based on their contribution to the communicative 
goal. Table 1 shows our assignments, which are 
based on judgments from our marketing staff. 
To explain the algorithms, we represent the 
discourse tree using an abstract view, as shown 
in Figure 2. Each node is a communicative goal. 
White nodes indicate nuclei. Satellites are shaded 
in grey corresponding to the importance of the 
rhetorical relation linking them to the nucleus. 
The number inside each node is the approximate 
amount of content that node produces (in lines). 
80
Shading Discourse Relations Importance 
Black Illustration, Background, 
Circumstance, Elaboration 
Low  
Low-Medium 
Dark 
Grey 
Context, Motivation, 
Evidence, Summary , 
Justification,  
Medium 
Light 
Grey 
Preparation, Enablement Medium-High 
High 
Table 1. Importance score for some relations1 
Each node is the root of a subtree (empty if the 
node is a leaf) which generates some content. In 
both algorithms, the system computes for each 
node an approximation of the space required for 
that content in number of lines (an approximation 
as it depends on style, line-wrapping and other 
formatting attributes in the final output). This is 
computed bottom-up in an iterative manner by 
looking at the retrieved content at each node. 
 
Figure 2. Discourse tree with space annotations 
3.1 Simple Algorithm  
The first algorithm is simple. It checks whether 
the top level node would result in too much 
content given the space requirements of the 
output channel (e.g., lines of content per page). If 
yes, the system traverses the tree, selects satel-
lites with the lowest importance value and drops 
them with their sub-trees. The algorithm repeats 
this process until the total amount of content fits 
the required space. We deployed the SciFly 
system with this algorithm at a trade fair in 2005 
and 2006 and measured the experience visitors 
had with the system. On average, people rated 
the system positively but noted that there was 
sometimes a lot of blank space in the brochures, 
when they felt that more information could have 
been included. This is because our simple 
algorithm drops many sub-trees at once, thus 
potentially deleting a lot of content in each step. 
This led us to our enhanced algorithm. 
                                                 
1 In our system, we consider 5 levels of importance. We 
have merged levels here to avoid too many shades of grey. 
3.2 Enhanced Algorithm 
We redesigned the algorithm to take into account 
the depth of a node in addition to its rhetorical 
status. We assign each node a weight, computed 
by adding the weight of the node?s parent and the 
penalty score of the rhetorical relation, which is 
(inversely) related to its importance score. Pen-
alty scores range from 1 to 6, in increments of 1: 
A nucleus has a score of 1 to take the tree depth 
into account, high importance relations have a 
score of 2, and low importance relations have a 
score of 6. In a discourse tree, a child node is 
always heavier than its parent. The larger the 
weight, the less important the node is to the 
overall comunicative goal. The system orders the 
nodes by their weight, and the heaviest nodes are 
dropped first. Thus, nodes deeper in the tree and 
linked by discourse relations with lower 
importance get removed first. Nodes are dropped 
one by one, until the top level node has an 
amount of content that satisfies the space 
constraint. This provides finer control over the 
amount of realised content and avoids the 
limitation of the first algorithm. 
 
Figure 3. Ordered list of (satellite) cluster nodes 
Sometimes, a discourse structure contains 
parallel sub-structures (e.g., bulletted points) 
that, if pruned unevenly, result in unbalanced text 
that seems odd. In such cases, the discourse 
structure typically contains several sub-trees with 
the same structure. In SciFly, such parallel 
structures occur when describing a list of 
projects. These are generated during discourse 
planning by a plan containing a foreach 
statement, e.g., (foreach project in project-list 
(describe project)). To address this situation, the 
system annotates all sub-structures issued from 
such a foreach statement. When constructing the 
ordered list of satellites, nodes at the same depth 
in the sub-structures are clustered together, as 
shown in Figure 3, taking into account their 
relationship to the nucleus. When dropping 
81
nodes, the whole cluster is deleted toegether, 
rather than node by node. So, in Figure 3, the 
whole cluster of weight 10 is dropped first, then 
the cluster of weight 8, etc. This prevents one 
sub-structure from being pruned more than its 
sibling structures and ensures the resulting 
brochure is balanced.  
4 Evaluation 
We evaluated the algorithms to assess their 
comparative effectiveness, based on a test set of 
1507 automatically generated brochures about 
randomly selected topics. We observed that 
82.5% of the brochures generated with the 
enhanced algorithm filled over 96% of the 
available space (leaving at most 8 lines of empty 
space), compared to 29% of brochures generated 
with the simple algorithm. In addition, 96.5% of 
the brochures generated with the new algorithm 
filled at least 90% of the space, compared with 
44.5% of brochures with the simple algorithm.  
We also found that 75% of brochures included 
more content using the enhanced algorithm (an 
average of 32 additional lines), but 12% of the 
brochures contained less content. We examined 
the latter in detail and found that, for these cases, 
the difference was on average 4 lines, and that 
the reduction was due to our treatment of parallel 
discourse structures, thus representing a desirable 
loss of content to create balanced brochures. 
We also performed a user evaluation to verify 
that the improvement in space usage had not de-
creased users? satisfaction. We asked users to 
compare pairs of brochures (simple algorithm vs. 
enhanced algorithm), indicating their preference 
if any. Seventeen users participated in the 
evaluation and were presented with seven pairs 
of brochures. To control any order effect, the 
pairs were randomly presented from user to user, 
and, in each pair, each brochure was randomly 
assigned a left-right configuration. Participants 
mostly preferred the brochures from the en-
hanced algorithm, or found the brochures equiva-
lent, thus showing that our more effective use of 
space had not decreased users? satisfaction.   
Overall, our results show that our enhanced 
algorithm is close to optimal in terms of 
conveying a message appropriately while filling 
up the space and producing a coherent and 
balanced text.  
5 Conclusions 
Reasoning about how much to generate when 
space is limited presents an important challenge 
for generation systems. Most systems either 
control their generation process to avoid 
producing large amounts of text at the onset, or 
control the generation at the sentence level. In 
our application, we cannot resort to any of these 
approaches as we generate text reusing existing 
text fragments and need to produce one discourse 
tree with all the appropriate available content and 
then select what to realise to output on several 
delivery channels. To satisfy space constraints, 
we implemented and tested two algorithms that 
embody the notions of nuclearity and importance 
of information to decide which content to keep 
and which to withhold. Our approach produces 
documents that fill most of the available space 
while maintaining users? satisfaction. 
Acknowledgements 
We thank M. Raji for her work on the user ex-
periment, the members of our group and K. 
Vander Linden for their input, and everyone who 
participated in our evaluation. 
References 
John Bateman, T. Kamps, K. K. Reichenberger, K. 
and J. Kleinz. 2001. Constructive text, diagram and 
layout generation for information presentation: the 
DArt_bio system. Computational Linguistics, 27 
(3): 409?449. 
William C. Mann and Sandra A. Thompson. 1988. 
Rhetorical Structure Theory: Toward a functional 
theory of text organisation. Text 8(3):243?281. 
Daniel Marcu. 1998. To build text summaries of high 
quality, nuclearity is not sufficient. In Working 
Notes of the AAAI-98 Spring Symposium on Intelli-
gent Text Summarization, Stanford, CA, 1?8. 
Johanna D. Moore and C?cile L. Paris. 1993. Planning 
Text for Advisory Dialogues: Capturing Intentional 
and Rhetorical Information. Computational 
Linguistics, 19 (4):651?694, Cambridge, MA. 
Donia R. Scott and Clarisse S. de Souza. 1990. 
Getting the message across in RST-based text 
generation. In Dale, Mellish & Zock (eds). Current 
Research in NLG. London: Academic Press. 119?
128. 
Mick O?Donnell (1997). Variable Length On-Line 
Document Generation. Proceedings of EWNLG. 
Gerhard-Mercator University, Duisburg, Germany.  
Karen Spark Jones (1993). What might be in a 
summary? Information Retrieval 93: Von der 
Modellierung zur Anwendung. (Ed: Knorz, Krause 
and Womse-Hacker), Konstanz: Universitatsverlag 
Konstanz, 9?26. 
82
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 919?928,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Segmenting Email Message Text into Zones
Andrew Lampert ??
?CSIRO ICT Centre
PO Box 76
Epping 1710, Australia
andrew.lampert@csiro.au
Robert Dale ?
rdale@ics.mq.edu.au
Ce?cile Paris ?
?Centre for Language Technology
Macquarie University
North Ryde 2109, Australia
cecile.paris@csiro.au
Abstract
In the early days of email, widely-used
conventions for indicating quoted reply
content and email signatures made it easy
to segment email messages into their func-
tional parts. Today, the explosion of dif-
ferent email formats and styles, coupled
with the ad hoc ways in which people vary
the structure and layout of their messages,
means that simple techniques for identify-
ing quoted replies that used to yield 95%
accuracy now find less than 10% of such
content. In this paper, we describe Zebra,
an SVM-based system for segmenting the
body text of email messages into nine zone
types based on graphic, orthographic and
lexical cues. Zebra performs this task with
an accuracy of 87.01%; when the num-
ber of zones is abstracted to two or three
zone classes, this increases to 93.60% and
91.53% respectively.
1 Introduction
Email message bodies consist of different func-
tional parts such as email signatures, quoted re-
ply content and advertising content. We refer to
these as email zones. Many language process-
ing tools stand to benefit from better knowledge
of this message structure, facilitating focus on rel-
evant content in specific parts of a message. In
particular, access to zone information would al-
low email classification, summarisation and anal-
ysis tools to separate or filter out ?noise? and focus
on the content in specific zones of a message that
are relevant to the application at hand. Email con-
tact mining tools such as that developed by Culotta
et al (2004), for example, might access the email
signature zone, while tools that attempt to iden-
tify tasks or action items in email (e.g., (Bellotti
et al, 2003; Corston-Oliver et al, 2004; Bennett
and Carbonell, 2007; Lampert et al, 2007)) might
restrict themselves to the sender-authored and for-
warded content. Despite previous work on this
problem, there are no available tools that can re-
liably extract or identify the different functional
zones of an email message.
While there is no agreed standard set of email
zones, there are clearly different functional parts
within the body text of email messages. For ex-
ample, the content of an email disclaimer is func-
tionally different from the sender-authored content
and from the quoted reply content automatically
included from previous messages in the thread of
conversation. Of course, there are different dis-
tinctions that can be drawn between zones; in
this paper we explore several different categorisa-
tions based on our proposed set of nine underlying
email zones.
Although we focus on content in the body of
email messages, we recognise the presence of use-
ful information in the semi-structured headers, and
indeed make use of header information such as
sender and recipient names in segmenting the un-
structured body text.
Segmenting email messages into zones is a
challenging task. Accurate segmentation is ham-
pered by the lack of standard syntax used by dif-
ferent email clients to indicate different message
parts, and by the ad hoc ways in which people vary
the structure and layout of their messages. When
replying to a message, for example, it is often use-
ful to include all or part of the original message
that is being replied to. Different email clients in-
dicate quoted material in different ways. By de-
fault, some prefix every line of the quoted message
with a character such as ?>? or ?|?, while others in-
dent the quoted content or insert the quoted mes-
sage unmodified, prefixed by a message header.
Sometimes the new content is above the quoted
content (a style known as ?top-posting?); in other
cases, the new content may appear after the quoted
919
content (bottom-posting) or interleaved with the
quoted content (inline replying). Confounding the
issue further is that users are able to configure their
email client to suit their individual tastes, and can
change both the syntax of quoting and their quot-
ing style (top, bottom or inline replying) on a per-
message basis.
To address these challenges, in this paper we
describe Zebra, our email zone classification sys-
tem. First we describe how Zebra builds and im-
proves on previous work in Section 2. Section 3
then presents our set of email zones, along with
details of the email data we use for system train-
ing and experiments. In Section 4 we describe two
approaches to zone classification, one that is line-
based and one that is fragment-based. The perfor-
mance of Zebra across two, three and nine email
zone classification tasks is presented and analysed
in Section 5.
2 Related Work
Segmenting email messages into zones requires
both text segmentation and text classification. The
main focus of most work on text segmentation
is topic-based segmentation of news text (e.g.,
(Hearst, 1997; Beeferman et al, 1997)), but there
have been some previous attempts at identifying
functional zones in email messages.
Chen et al (1999) looked at both linguistic and
two-dimensional layout cues for extracting struc-
tured content from email signature zones in email
messages. The focus of their work was on extract-
ing information from already identified signature
blocks using a combination of two-dimensional
structural analysis and one-dimensional grammat-
ical constraints; the intended application domain
was as a component in a system for email text-
to-speech rendering. The authors claim that their
system can be modified to also identify signature
blocks within email messages, but their system
performs this task with a recall of only 53%. No
attempt is made to identify functional zones other
than email signatures.
Carvalho and Cohen?s (2004) Jangada system
attempted to identify email signatures within plain
text email messages and to extract email signa-
tures and reply lines. Unfortunately, the 20 News-
groups corpus1 they worked with contains 15-
year-old Usenet messages which are much more
homogeneous in their syntax than contemporary
1http://people.csail.mit.edu/jrennie/20Newsgroups/
email, particularly in terms of how quoted text
from previous messages is indicated. As a result,
using a very simple metric (a line-initial ?>? char-
acter) to identify reply lines achieves more than
95% accuracy. In contrast, this same simple met-
ric applied to the Enron email data we annotated
detects less than 10% of actual reply or forward
lines.
Usenet messages are also markedly different
from contemporary email when it comes to email
signatures. Most Usenet clients produced mes-
sages which conformed to RFC3676 (Gellens,
2004), a standard that formalised a ?long-standing
convention in Usenet news . . . of using two hy-
phens -- as the separator line between the body
and the signature of a message.? Unfortunately,
this convention has long since ceased to be ob-
served in email messages. Carvalho and Cohen?s
email signature detection approach also benefits
greatly from a simplifying assumption that signa-
tures are found in the last 10 lines of an email mes-
sage. While this holds true for their Usenet mes-
sage data, it is no longer the case for contemporary
email.
In attempting to use Carvalho and Cohen?s sys-
tem to identify signature blocks and reply lines
in our own work, we identified similar shortcom-
ings to those noted by Estival et al (2007). In
particular, Jangada did not accurately identify for-
warded or reply content in email data from the
Enron email corpus. We believe that the use of
older Usenet-style messages to train Jangada is a
significant factor in the systematic errors the sys-
tem makes in failing to identify quoted reply, for-
warded and signature content in messages format-
ted in the range of message formats and styles pop-
ularised by Microsoft Outlook. These errors are
a fundamental problem with Jangada, especially
since Outlook is the most common client used to
compose messages in our annotated email collec-
tion drawn from the Enron corpus. More gen-
erally, we note that Outlook is the most popular
email client in current use, with an estimated 350?
400 million users worldwide,2 representing any-
where up to 40% of all email users.3
More recently, as part of their work on profiling
2Xobni Co-founder Adam Smith and former Engi-
neering VP Gabor Cselle have both published Outlook
user statistics. See http://www.xobni.com/asmith/archives/66
and http://gaborcselle.com/blog/2008/05/xobnis-journey-to-
right-product.html.
3http://www.campaignmonitor.com/stats/email-clients/
920
authors of email messages, Estival et al (2007)
classified email bodies into five email zones. Their
paper does not provide results for five-zone classi-
fication, but they report accuracy of 88.16% using
a CRF classifier to distinguish three zones: reply,
author and signature. We use their classification
scheme as the starting point for our own set of
email zones.
3 Email Zones
As noted earlier, we refer to the different func-
tional components of email messages as email
zones. The zones we propose refine and extend
the five categories ? Author Text, Signature, Ad-
vertisement (automatically appended advertising),
Quoted Text (extended quotations such as song
lyrics or poems), and Reply Lines (including for-
warded and reply text) ? identified by Estival et
al. (2007).
We consider that each line of text in the body
of an email message belongs to one of nine more
fine-grained email zones. We intend our nine
email zones to be abstracted and adapted to suit
different tasks. To illustrate, we present the
zones below abstracted into three classes: sender-
authored content, boilerplate content, and content
quoted from other conversations. This is the zone
partition we use to generate the three-zone results
reported in Section 5. This categorisation is use-
ful for problems such as finding action items in
email messages: such detection tools would look
in text from the sender-authored message zones
for new action item information, and could also
look in quoted conversation content to link new
action item information (such as reported comple-
tions) to previous action item content.
Our nine email zones can also be reduced to a
binary scheme to distinguish text authored by the
sender from text authored by others. This distinc-
tion is useful for problems such as author attribu-
tion or profiling tasks. In this two-class case, the
sender-authored zones would be Author, Greeting,
Signoff and Signature, while the other-authored
zones would be Reply, Forward, Disclaimer, Ad-
vertising and Attachment. This is the partition
of zones we use in our two-zone experiments re-
ported in Section 5.
3.1 Sender Zones
Sender zones contain text written by the current
email sender. The Greeting and Signoff zones are
sub-zones of the Author zone, usually appearing
as the first and last items respectively in the Author
zone. Thus, our proposed sender zones are:
1. Author: New content from the current email
sender. This specifically excludes any text
authored by the sender that is included from
previous messages.
2. Greeting: Terms of address and recipient
names at the beginning of a message (e.g.,
Dear/Hi/Hey Noam).
3. Signoff: The message closing (e.g.,
Thanks/Cheers/Regards, John).
3.2 Quoted Conversation Zones
Quoted conversation zones include both content
quoted in reply to previous messages in the same
conversation thread and forwarded content from
other conversations.4 Our quoted conversation
zones are:
4. Reply: Content quoted from a previous mes-
sage in the same conversation thread, includ-
ing any embedded signatures, attachments,
advertising, disclaimers, author content and
forwarded content. Content in a reply content
zone may include previously sent content au-
thored by the current sender.
5. Forward: Content from an email message
outside the current conversation thread that
has been forwarded by the current email
sender, including any embedded signatures,
attachments, advertising, disclaimers, author
content and reply content.
3.3 Boilerplate Zones
Boilerplate zones contain content that is reused
without modification across multiple email mes-
sages. Our proposed boilerplate zones are:
6. Signature: Content containing contact or
other information that is automatically in-
serted in a message. In contrast to disclaimer
or advertising content, signature content is
usually templated content written once by
the email author, and automatically or semi-
automatically included in email messages. A
4Although we recognise the need for the Quoted Text zone
proposed by Estival et al (2007), no such data occurs in our
collection of annotated email messages. We therefore omit
this zone from our current set.
921
user may also use a Signature in place of a
Signoff; in such cases, we still mark the text
as a Signature.
7. Advertising: Advertising material in an
email message. Such material often appears
at the end of a message (e.g., Do you Ya-
hoo!?), but may also appear prefixed or in-
line with the content of the message, (e.g., in
sponsored mailing lists).
8. Disclaimer: Legal disclaimers and privacy
statements, often automatically appended.
9. Attachment: Automated text indicating or
referring to attached documents, such as that
shown in line 16 of Figure 1. Note that this
zone does not apply to manually authored ref-
erence to attachments, nor to the actual con-
tent of attachments (which we do not clas-
sify).
3.4 Email Data and Annotation
The training data for our zone classifier consists of
11881 annotated lines from almost 400 email mes-
sages drawn at random from the Enron email cor-
pus (Klimt and Yang, 2004).5 We use the database
dump of the corpus released by Andrew Fiore and
Jeff Heer.6 This version of the corpus has been
processed to remove duplicate messages and to
normalise sender and recipient names, resulting in
just over 250,000 email messages. No attachments
are included. Following Estival et al (2007), we
used only a single annotator since the task revealed
itself to be relatively uncontroversial. Each line in
the body text of selected messages was marked by
the annotator (one of the authors) as belonging to
one of the nine zones. After removing blank lines,
which we do not attempt to classify, we are left
with 7922 annotated lines as training data for Ze-
bra. The frequency of each zone within this anno-
tated dataset is shown in Table 3.
Figure 1 shows an example of an email mes-
sage with each line annotated with the appropriate
email zone. Two zone annotations are shown for
each line (in separate columns), one using the nine
fine-grained zones and the second using the ab-
stracted three-zone scheme described in Section 3.
Note, however, that not all of the nine fine-grained
5This annotated dataset is available from
http://zebra.thoughtlets.org/.
6http://bailando.sims.berkeley.edu/enron/enron.sql.gz
zones, nor all of the three abstracted zones, are ac-
tually present in this particular message.
4 Zone Segmentation and Classification
Our email zone classification system is based
around an SVM classifier using features that cap-
ture graphic, orthographic and lexical information
about the content of an email message.
To classify the zones in an email message, we
experimented with two approaches. The first em-
ploys a two-stage approach that segments a mes-
sage into zone fragments and then classifies those
fragments. Our second method simply classifies
lines independently, returning a classification for
each non-blank line in an email message. Our hy-
pothesis was that classifying larger text fragments
would lead to better performance due to the text
fragments containing more cues about the zone
type.
4.1 Zone Fragment Classification
Zone fragment classification is a two-step process.
First it predicts the zone boundaries using a simple
heuristic, then it classifies the resulting zone frag-
ments, the sets of content lines that lie between
these hypothesised boundaries.
In order to determine how well we can detect
zone boundaries, we first need to establish the cor-
rect zone boundaries in our collection of zone-
annotated email messages.
4.1.1 Zone Boundaries
A zone boundary is defined as a continuous collec-
tion of one or more lines that separate two differ-
ent email zones. Lines that separate two zones and
are blank, contain only whitespace or contain only
punctuation characters are called buffer lines.
Since classification of blank lines between
zones is often ambiguous, empty or whitespace-
only buffer lines are not included as content in any
zone, and thus are not classified. Instead, they are
treated as strictly part of the zone boundary. In
Figure 1, these lines are shown without any zone
annotation. Zone boundary lines that are included
as content in a zone have their zone annotation
styled in bold and underlined. The important point
here is that zone boundaries are specific to a zone
classification scheme. For nine-zone classifica-
tion of the message in Figure 1, there are six zone
boundaries: line 2, lines 10?11, line 12, line 15,
lines 17?20, and lines 30?33. For three-zone clas-
922
Figure 1: An example email message marked with both nine- and three-zone annotations.
sification, the only zone boundary consists of line
12, separating the sender and boilerplate zones.
Based on these definitions, there are three dif-
ferent types of zone boundaries:
1. Blank boundaries contain only empty or
whitespace-only buffer lines. Lines in these
zone boundaries are strictly separate from the
zone content. An example is Line 12 in Fig-
ure 1, for both the three- and nine-zone clas-
sification.
2. Separator boundaries contain only
buffer lines, but must contain at least
one punctuation-character buffer line that is
retained as content in one or both zones. In
Figure 1, an example is the zone boundary
containing lines 17?20 that separates the
Attachment and Disclaimer zones for nine-
zone classification, since line 20 is retained
as part of the Disclaimer zone content.
3. Adjoining boundaries consist of the last
content line of the earlier zone and the first
content line of the following zone. These
boundaries occur where no buffer lines ex-
ist between the two zones. An example is
the zone boundary containing lines 10 and 11
that separates the Author and Signoff zones in
Figure 1 for nine-zone classification.
923
4.1.2 Hypothesising Zone Boundaries
To identify zone boundaries in unannotated email
data, we employ a very simple heuristic approach.
Specifically, we consider every line in the body of
an email message that matches any of the follow-
ing criteria to be a zone boundary:
1. A blank line;
2. A line containing only whitespace; or
3. A line beginning with four or more repeated
punctuation characters, optionally prefixed
by whitespace.
Our efforts to apply more sophisticated
machine-learning techniques to identifying zone
boundaries could not match the 90.15% recall
achieved by this simple heuristic. The boundaries
missed by the simple heuristic are all adjoining
boundaries, where two zones are not separated
by any buffer lines. An example of a boundary
that is not detected by our heuristic is the zone
boundary between the Author and Signoff zones
in Figure 1 formed by lines 10 and 11.
Obviously, our simple boundary heuristic de-
tects actual boundaries as well as spurious
boundaries that do not actually separate differ-
ent email zones. Unsurprisingly, the number of
spurious boundaries is large. The precision of
our simple heuristic across our annotated set of
email messages is 22.5%, meaning that less than
1 in 4 hypothesised zone boundaries is an actual
boundary. The underlying email zones average
more than 12 lines in length, including just over
8 lines of non-blank content. Due to the num-
ber of spurious boundaries, fragments contain less
than half this amount ? approximately 3 lines of
non-blank content on average. One of the most
common types of spurious boundaries detected are
the blank lines that frequently separate paragraphs
within a single zone.
For three-zone classification, the set of pre-
dicted boundaries remains the same, but there are
less actual boundaries to find, so recall increases to
96.3%. However, because many boundaries from
the nine-zone classification are not boundaries for
the three-zone classification, precision decreases
to 14.7%.
4.1.3 Classifying Zone Fragments
Having segmented the email message into candi-
date zone fragments, we classify these fragments
using the SMO implementation provided by Weka
(Witten and Frank, 2005) with the features de-
scribed in Section 4.3.
Although our boundary detection heuristic has
better than 90% recall, the small number of ac-
tual boundaries that are not detected result in some
zone fragments containing lines from more than
one underlying email zone. In these cases, we con-
sider the mode of all annotation values for lines
in the fragment (i.e., the most frequent zone an-
notation) to be the gold-standard zone type for
the fragment. This, of course, may mean that we
somewhat unfairly penalise the accuracy of our au-
tomated classification when Zebra detects a zone
that is indeed present in the fragment, but is not
the most frequent zone.
4.2 Line Classification
Our line-based classification approach simply ex-
tracts all non-blank lines from an email message
and classifies lines one-by-one, using the same
features as for fragment-based classification. This
approach is the same as the signature and reply
line classification approach used by Carvalho and
Cohen (2004).
4.3 Classification Features
We use a variety of graphic, orthographic and lex-
ical features for classification in Zebra. The same
features are applied in both the line-based and the
fragment-based zone classification (to either indi-
vidual lines or zone fragments). In the description
of our features, we refer to both single lines and
zone fragments (collections of contiguous lines) as
text fragments.
4.3.1 Graphic Features
Our graphic features capture information about the
presentation and layout of text in an email mes-
sage, independent of the actual words used. This
information is a crucial source of information for
identifying zones. Such information includes how
the text is organised and ordered, as well as the
?shape? of the text. The specific features we em-
ploy are:
? the number of words in the text fragment;
? the number of Unicode code points (i.e.,
characters) in the text fragment;
? the start position of the text fragment (equal
to one for the first line in the message, two for
the second line and increasing monotonically
924
through the message; we also normalise the
result for message length);
? the end position of the text fragment (calcu-
lated as above and again normalised for mes-
sage length);
? the average line length (in characters) within
the text fragment (equal to the line length for
line-based text fragments);
? the length of the text fragment (in characters)
relative to the previous fragment;
? the length of the text fragment (in characters)
relative to the following fragment;
? the number of blank lines preceding the text
fragment; and
? the number of blank lines following the text
fragment.
4.3.2 Orthographic Features
Our orthographic features capture information
about the use of distinctive characters or charac-
ter sequences including punctuation, capital let-
ters and numbers. Like our graphic features, or-
thographic features tend to be independent of the
words used in an email message. The specific or-
thographic features we employ include:
? whether all lines start with the same character
(e.g., ?>?);
? whether a prior text fragment in the message
contains a quoted header;
? whether a prior text fragment in the message
contains repeated punctuation characters;
? whether the text fragment contains a URL;
? whether the text fragment contains an email
address;
? whether the text fragment contains a se-
quence of four or more digits;
? the number of capitalised words in the text
fragment;
? the percentage of capitalised words in the text
fragment;
? the number of non-alpha-numeric characters
in the text fragment;
? the percentage of non-alpha-numeric charac-
ters in the text fragment;
? the number of numeric characters in the text
fragment;
? the percentage of numeric characters in the
text fragment;
? whether the message subject line contains a
reply syntax marker such as Re: ; and
? whether the message subject line contains a
forward syntax marker such as Fw:.
4.3.3 Lexical Features
Finally, our lexical features capture information
about the words used in the email text. We use
unigrams to capture information about the vocab-
ulary and word bigram features to capture short
range word order information. More specifically,
the lexical features we apply to each text fragment
include:
? each word unigram, calculated with a mini-
mum frequency threshold cutoff of three, rep-
resented as a separate binary feature;
? each word bigram, calculated with a mini-
mum frequency threshold cutoff of three, rep-
resented as a separate binary feature;
? whether the text fragment contains the
sender?s name;
? whether a prior text fragment in the message
contains the sender?s name;
? whether the text fragment contains the
sender?s initials; and
? whether the text fragment contains a recipi-
ent?s name.
Features that look for instances of sender or recip-
ient names are less likely to be specific to a par-
ticular business or email domain. These features
use regular expressions to find name occurrences,
based on semi-structured information in the email
message headers. First, we extract and normalise
the names from the email headers to identify the
relevant person?s given name and surname. Our
features then capture whether one or both of the
given name or surname are present in the current
text fragment. Features which detect user initials
make use of the same name normalisation code to
retrieve a canonical form of the user?s name, from
which their initials are derived.
5 Results and Discussion
Table 1 shows Zebra?s accuracy in classifying
email zones. The results are calculated using 10-
fold cross-validation. Accuracy is shown for three
tasks ? nine-, three- and two-zone classification
? using both line and zone-fragment classifica-
tion. Performance is compared against a majority
class baseline in each case.
Zebra?s performance compares favourably with
previously published results. While it is difficult to
925
2 Zones 3 Zones 9 Zones
Zebra Baseline Zebra Baseline Zebra Baseline
Lines 93.60% 61.14% 91.53% 58.55% 87.01% 30.94%
Fragments 92.09% 62.18% 91.37% 59.44% 86.45% 30.36%
Table 1: Classification accuracy compared against a majority baseline
2 Zones 3 Zones 9 Zones
Zebra Baseline Zebra Baseline Zebra Baseline
Lines 90.62% 61.14% 86.56% 58.55% 81.05% 30.94%
Fragments 91.14% 62.18% 89.44% 59.44% 82.55% 30.36%
Table 2: Classification accuracy, without word n-gram features, compared against a majority baseline
directly compare, since not all systems are freely
available and they are not trained or tested over the
same data, our three-zone classification (identify-
ing sender, boilerplate and quoted reply content) is
very similar to the three-zone task for which (Es-
tival et al, 2007) report 88.16% accuracy for their
system and 64.22% accuracy using Carvalho and
Cohen?s Jangada system. Zebra outperforms both,
achieving 91.53% accuracy using a line-based ap-
proach. In the two-zone task, where we attempt
to identify sender-authored lines, Zebra achieves
93.60% accuracy and an F-measure of 0.918, ex-
ceeding the 0.907 F-measure reported for Estival
et al?s system tuned for exactly this task.
Interestingly, the line-based approach provides
slightly better performance than the fragment-
based approach for each of the two-zone, three-
zone and nine-zone classification tasks. As noted
earlier, our original hypothesis was that zone frag-
ments would contain more information about the
sequence and text shape of the original message,
and that this would lead to better performance for
fragment-based classification.
When we restrict our feature set to those that
look only at the text of the line or zone fragment,
the fragment-based approach does perform better
than the line-based one. Using only word uni-
gram features, for example, our fragment classi-
fier achieves 78.7% accuracy. Using the same fea-
tures, the line-based classifier achieves only 57.5%
accuracy. When we add further features that cap-
ture sequence and shape information from outside
the text fragment being classified (e.g., the length
of a text segment compared to the text segment
before and after, and whether a segment occurs
after another segment containing repeated punc-
tuation or the sender?s name), the line-based ap-
proach achieves a greater increase in accuracy than
the fragment-based approach. This presumably is
because individual lines intrinsically have less in-
formation about the message context, and so ben-
efit more from the information added by the new
features.
We also experimented with removing all word
unigram and bigram features to explore the classi-
fier?s portability across different domains. This re-
moved all vocabulary and word order information
from our feature set. In doing so, our feature set
was reduced to less than thirty features, consist-
ing of mostly graphic and orthographic informa-
tion. The few remaining lexical features captured
only the presence of sender and recipient names,
which are independent of any particular email do-
main. As expected, performance did drop, but not
dramatically. Table 2 shows that average perfor-
mance without n-grams (across two-, three- and
nine-zone tasks) for line-based classification drops
by 4.67%. In contrast, fragment-based classifica-
tion accuracy drops by less than half this amount
? an average of 2.26%. This suggests that, as we
originally hypothesised, there are additional non-
lexical cues in zone fragments that give informa-
tion about the zone type. This makes the zone
fragment approach potentially more portable for
use across email data from different enterprise do-
mains.
Of course, classification accuracy gives only a
limited picture of Zebra?s performance. Table 4
shows precision and recall results for each zone in
the nine-zone line-based classification task. Per-
926
Total Author Signature Disclaim Advert Greet Signoff Reply Fwd Attach
Author 2415 2197 56 9 4 14 31 43 53 8
Signature 383 93 203 4 0 0 20 28 31 4
Disclaim 97 30 4 52 0 0 0 2 9 0
Advert 83 47 1 1 20 0 0 7 7 0
Greet 85 8 0 0 0 74 2 0 1 0
Signoff 195 30 5 0 0 0 147 11 2 0
Reply 2451 49 10 3 2 1 10 2222 154 0
Fwd 2187 72 13 7 8 1 3 125 1958 0
Attach 26 4 0 0 0 0 0 1 1 20
Table 3: Confusion Matrix for 9 Zone Line Classification
formance clearly varies significantly across the
different zones. For Author, Greeting, Reply and
Forward zones, performance is good, with F-
measure > 0.8. This is encouraging, given that
many email tools, such as action-item detection
and email summarisation would benefit from an
ability to separate author content from reply con-
tent and forwarded content. The Advertising, Sig-
nature and Disclaimer zones show the poorest per-
formance, particularly in terms of Recall. The
Advertising and Disclaimer zones are almost cer-
tainly hindered by a lack of training data; they are
two of the smallest zones in terms of number of
lines of training data. The relatively poor Signa-
ture class performance is more interesting. Given
the potential confusion between Signoff content
and Signatures that function as Signoffs, one might
expect confusion between Signoff and Signature
zones, but Table 3 shows this is not the case.
Instead, there is significant confusion between
Signature and Author content, with almost 25%
of Signature lines misclassified as Author lines.
When word n-grams are removed from the fea-
ture set, the number of these misclassifications in-
creases to almost 50%. These results reinforce our
observation that the task of email signature extrac-
tion is much more difficult that it was in the days
of Usenet messages.
6 Conclusion
Identifying functional zones in email messages is
a challenging task, due in large part to the diver-
sity in syntax used by different email software, and
the dynamic manner in which people employ dif-
ferent styles in authoring email messages. Zebra,
our system for segmenting and classifying email
message text into functional zones, achieves per-
Zone Precision Recall F-Measure
Author 0.868 0.910 0.889
Signature 0.695 0.530 0.601
Disclaimer 0.684 0.536 0.601
Advertising 0.588 0.241 0.342
Greeting 0.822 0.871 0.846
Signoff 0.690 0.754 0.721
Reply 0.911 0.907 0.909
Forward 0.884 0.895 0.889
Attachment 0.625 0.769 0.690
Table 4: Precision and recall for nine-zone line
classification
formance that exceeds comparable systems, and
that is at a level to be practically useful to email
researchers and system builders. In addition to re-
leasing our annotated email dataset, the Zebra sys-
tem will also be available for others to use7.
Because we employ a non-sequential learn-
ing algorithm, we encode sequence information
into the feature set. In future work, we plan
to determine the effectiveness of using a sequen-
tial learning algorithm like Conditional Random
Fields (CRF). We note, however, that Carvalho
and Cohen (2004) demonstrate that using a non-
sequential learning algorithm with sequential fea-
tures, as we do, has the potential to meet or exceed
the performance of sequential learning algorithms.
Acknowledgments
The authors are grateful to the anonymous review-
ers for their insightful comments and suggestions.
7See http://zebra.thoughtlets.org for access to the anno-
tated data and Zebra system
927
References
Douglas Beeferman, Adam Berger, and John Lafferty.
1997. Text segmentation using exponential models.
In Proceedings of the 2nd Conference on Empiri-
cal Methods in Natural Language Processing, pages
35?46, Providence, RI.
Victoria Bellotti, Nicolas Ducheneaut, Mark Howard,
and Ian Smith. 2003. Taking email to task: The
design and evaluation of a task management centred
email tool. In Computer Human Interaction Confer-
ence, CHI, pages 345?352, Ft Lauderdale, Florida,
USA, April 5-10.
Paul N Bennett and Jaime G Carbonell. 2007. Com-
bining probability-based rankers for action-item de-
tection. In Proceedings of NAACL HLT 2007, pages
324?331, Rochester, NY, April.
Vitor R Carvalho and William W Cohen. 2004. Learn-
ing to extract signature reply lines from email. In
Proceedings of First Conference on Email and Anti-
Spam (CEAS), Mountain View, CA, July 30-31.
Hao Chen, Jianying Hu, and Richard W Sproat. 1999.
Integrating geometrical and linguistic analysis for
email signature block parsing. ACM Transactions
on Information Systems, 17(4):343?366, October.
ISSN: 1046-8188.
Simon H. Corston-Oliver, Eric Ringger, Michael Ga-
mon, and Richard Campbell. 2004. Task-focused
summarization of email. In ACL-04 Workshop: Text
Summarization Branches Out, pages 43?50, July.
Aron Culotta, Ron Bekkerman, and Andrew McCal-
lum. 2004. Extracting social networks and contact
information from email and the web. In Proceedings
of the Conference on Email and Anti-Spam (CEAS).
Dominique Estival, Tanja Gaustad, Son Bao Pham,
Will Radford, and Ben Hutchinson. 2007. Author
profiling for English emails. In Proceedings of the
10th Conference of the Pacific Association for Com-
putational Linguistics, pages 263?272, Melbourne,
Australia, Sept 19-21.
R. Gellens. 2004. RFC3676: The text/plain format and
delsp parameters, February.
Marti A. Hearst. 1997. Texttiling: Segmenting text
into multi-paragraph subtopic passages. Computa-
tional Linguistics, 23(1):33?64.
Bryan Klimt and Yiming Yang. 2004. Introducing the
Enron corpus. In Proceedings of the Conference on
Email and Anti-Spam (CEAS).
Andrew Lampert, Ce?cile Paris, and Robert Dale. 2007.
Can requests-for-action and commitments-to-act be
reliably identified in email messages? In Proceed-
ings of the 12th Australasian Document Comput-
ing Symposium, pages 48?55, Melbourne, Australia,
December 10.
Ian Witten and Eiba Frank. 2005. Data Mining: Prac-
tical machine learning tools and techniques. Mor-
gan Kaufmann, San Francisco, 2nd edition.
928
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 984?992,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Detecting Emails Containing Requests for Action
Andrew Lampert ??
?CSIRO ICT Centre
PO Box 76
Epping 1710
Australia
andrew.lampert@csiro.au
Robert Dale
?Centre for Language Technology
Macquarie University 2109
Australia
rdale@science.mq.edu.au
Cecile Paris
CSIRO ICT Centre
PO Box 76
Epping 1710
Australia
cecile.paris@csiro.au
Abstract
Automatically finding email messages that
contain requests for action can provide valu-
able assistance to users who otherwise strug-
gle to give appropriate attention to the ac-
tionable tasks in their inbox. As a speech
act classification task, however, automatically
recognising requests in free text is particularly
challenging. The problem is compounded by
the fact that typical emails contain extrane-
ous material that makes it difficult to isolate
the content that is directed to the recipient of
the email message. In this paper, we report
on an email classification system which iden-
tifies messages containing requests; we then
show how, by segmenting the content of email
messages into different functional zones and
then considering only content in a small num-
ber of message zones when detecting requests,
we can improve the accuracy of message-level
automated request classification to 83.76%, a
relative increase of 15.9%. This represents
an error reduction of 41% compared with the
same request classifier deployed without email
zoning.
1 Introduction
The variety of linguistic forms that can be used
to express requests, and in particular the frequency
with which indirect speech acts are used in email, is
a major source of difficulty in determining whether
an email message contains one or more requests.
Another significant problem arises from the fact that
whether or not a request is directed at the recipient of
the email message depends on where in the message
the request is found. Most obviously, if the request is
part of a replied-to message that is contained within
the current message, then it is perhaps more likely
that this request was directed at the sender of the
current message. However, separating out content
intended for the recipient from other extraneous con-
tent is not as simple as it might appear. Segmenting
email messages into their different functional parts
is hampered by the lack of standard syntax used by
different email clients to indicate different message
parts, and by the ad hoc ways in which people vary
the structure and layout of messages.
In this paper, we present our results in classifying
messages according to whether or not they contain
requests, and then show how a separate classifier
that aims to determine the nature of the zones that
make up an email message can improve upon these
results. Section 2 contains some context and moti-
vation for this work before we briefly review rele-
vant related work in Section 3. Then, in Section 4,
we describe a first experiment in request classifica-
tion using data gathered from a manual annotation
experiment. In analysing the errors made by this
classifier, we found that a significant number of er-
rors seemed to arise from the inclusion of content in
parts of a message (e.g., quoted reply content) that
were not authored by the current sender, and thus
were not relevant other than as context for interpret-
ing the current message content. Based on this anal-
ysis, we hypothesised that segmenting messages into
their different functional parts, which we call email
zones, and then using this information to consider
only content from certain parts of a message for re-
quest classification, would improve request classifi-
984
cation performance.
To test this hypothesis, we developed an SVM-
based automated email zone classifier configured
with graphic, orthographic and lexical features; this
is described in more detail in (Lampert et al, 2009).
Section 5 describes how we improve request classi-
fication performance using this email zone classifier.
Section 6 summarises the performance of our re-
quest classifiers, with and without automated email
zoning, along with an analysis of the contribution of
lexical features to request classification, discussion
of request classification learning curves, and a de-
tailed error analysis that explores the sources of re-
quest classification errors. Finally, in Section 7, we
offer pointers to future work and some concluding
remarks.
2 Background and Motivation
Previous research has established that users rou-
tinely use email for managing requests in the work-
place ? e.g., (Mackay, 1988; Ducheneaut and Bel-
lotti, 2001). Such studies have highlighted how
managing multiple ongoing tasks through email
leads to information overload (Whittaker and Sid-
ner, 1996; Bellotti et al, 2003), especially in the
face of an ever-increasing volume of email. The
result is that many users have difficulty giving ap-
propriate attention to requests hidden in their email
which require action or response. A particularly lu-
cid summary of the requirements placed on email
users comes from work by Murray (1991), whose
ethnographic research into the use of electronic mes-
saging at IBM highlighted that:
[Managers] would like to be able to track
outstanding promises they have made,
promises made to them, requests they?ve
made that have not been met and requests
made of them that they have not fulfilled.
This electronic exchange of requests and commit-
ments has previously been identified as a fundamen-
tal basis of the way work is delegated and com-
pleted within organisations. Winograd and Flores
were among the first to recognise and attempt to
exploit this with their Coordinator system (Wino-
grad and Flores, 1986). Their research into organ-
isational communication concluded that ?Organisa-
tions exist as networks of directives and commis-
sives?. It is on this basis that our research explores
the use of requests (directive speech acts) and com-
mitments (commissive speech acts) in email. In this
paper, we focus on requests; feedback from users
of the request and commitment classifier plug-in for
Microsoft Outlook that we have under development
suggests that, at least within the business context of
our current users, requests are the more important of
the two phenomena.
Our aim is to create tools that assist email users
to identify and manage requests contained in incom-
ing and outgoing email. We define a request as an
utterance that places an obligation on an email re-
cipient to schedule an action; perform (or not per-
form) an action; or to respond with some speech
act. A simple example might be Please call when
you have a chance. A more complicated request is
David will send you the latest version if there have
been any updates. If David (perhaps cc?ed) is a re-
cipient of an email containing this second utterance,
the utterance functions as a (conditional) request for
him, even though it is addressed as a commitment to
a third-party. In real-world email, requests are fre-
quently expressed in such subtle ways, as we discuss
in Section 4.
A distinction can be drawn between message-
level identification?i.e., the task of determining
whether an email message contains a request ?
and utterance-level identification?i.e., determin-
ing precisely where and how the request is ex-
pressed. In this paper, we focus on the task of
message-level identification, since utterance-level
identification is a significantly more problematic
task: it is often the case that, while we might agree
that a message contains a request or commitment,
it is much harder to determine the precise extent of
the text that conveys this request (see (Lampert et
al., 2008b) for a detailed discussion of some of the
issues here).
3 Related Work
Our request classification work builds on influential
ideas proposed by Winograd and Flores (1986) in
taking a language/action perspective and identifying
speech acts in email. While this differs from the ap-
proach of most currently-used email systems, which
985
routinely treat the content of email messages as ho-
mogeneous bags-of-words, there is a growing body
of research applying ideas from Speech Act Theory
(Austin, 1962; Searle, 1969) to analyse and enhance
email communication.
Khosravi and Wilks (1999) were among the first
to automate message-level request classification in
email. They used cue-phrase based rules to clas-
sify three classes of requests: Request-Action,
Request-Information and Request-Permission. Un-
fortunately, their approach was quite brittle, with the
rules being very specific to the computer support do-
main from which their email data was drawn.
Cohen, Carvalho and Mitchell (2004) developed
machine learning-based classifiers for a number of
email speech acts. They performed manual email
zoning, but didn?t explore the contribution this made
to the performance of their various speech act clas-
sifiers. For requests, they report peak F-measure of
0.69 against a majority class baseline accuracy of
approximately 66%. Cohen, Carvalho and Mitchell
found that unweighted bigrams were particularly
useful features in their experiments, out-performing
other features applied. They later applied a series of
text normalisations and n-gram feature selection al-
gorithms to improve performance (Carvalho and Co-
hen, 2006). We apply similar normalisations in our
work. While difficult to compare due to the use of a
different email corpus that may or may not exclude
annotation disagreements, our request classifier per-
formance exceeds that of the enhanced classifier re-
ported in (Carvalho and Cohen, 2006).
Goldstein and Sabin (2006) have also worked on
related email classification tasks. They use verb
classes, along with a series of hand-crafted form-
and phrase-based features, for classifying what they
term email genre, a task which overlaps signifi-
cantly with email speech act classification. Their
results are difficult to compare since they include a
mix of form-based classifications like response with
more intent-based classes such as request. For re-
quests, the results are rather poor, with precision of
only 0.43 on a small set of personal mail.
The SmartMail system (Corston-Oliver et al,
2004) is probably the most mature previous work
on utterance-level request classification. SmartMail
attempted to automatically extract and reformulate
action items from email messages for the purpose of
adding them to a user?s to-do list. The system em-
ployed a series of deep linguistic features, including
phrase structure and semantic features, along with
word and part-of-speech n-gram features. The au-
thors found that word n-grams were highly predic-
tive for their classification task, and that there was
little difference in performance when the more ex-
pensive deep linguistic features were added. Based
on this insight, our own system does not employ
deeper linguistic features. Unfortunately, the re-
sults reported reveal only the aggregate performance
across all classes, which involves a mix of both
form-based classes (such as signature content ad-
dress lines and URL lines), and intent-based classes
(such as requests and promises). It is thus very dif-
ficult to directly compare the results with our sys-
tem. Additionally, the experiments were performed
over a large corpus of messages that are not avail-
able for use by other researchers. In contrast, we
use messages from the widely-available Enron email
corpus (Klimt and Yang, 2004) for our own experi-
ments.
While several of the above systems involve man-
ual processes for removing particular parts of mes-
sage bodies, none employ a comprehensive, auto-
mated approach to email zoning.
We focus on the combination of email zoning
and request classification tasks and provide details
of how email zoning improves request classification
? a task not previously explored. To do so, we re-
quire an automated email zone classifier. We exper-
imented with using the Jangada system (Carvalho
and Cohen, 2004), but found similar shortcomings
to those noted by Estival et al (2007). In particular,
Jangada did not accurately identify forwarded or re-
ply content in email messages from the email Enron
corpus that we use. We achieved much better perfor-
mance with our own Zebra zone classifier (Lampert
et al, 2009); it is this system that we use for email
zoning throughout this paper.
4 Email Request Classification
Identifying requests requires interpretation of the in-
tent that lies behind the language used. Given this, it
is natural to approach the problem as one of speech
act identification. In Speech Act Theory, speech
acts are categories like assertion and request that
986
capture the intentions underlying surface utterances,
providing abstractions across the wide variety of dif-
ferent ways in which instances of those categories
might be realised in linguistic form. In this paper
we focus on the speech acts that represent requests,
where people are placing obligations upon others via
actionable content within email messages.
The task of building automated classifiers is dif-
ficult since the function of conveying a request does
not neatly map to a particular set of language forms;
requests often involve what are referred to as indi-
rect speech acts. While investigating particular sur-
face forms of language is relatively unproblematic,
it is widely recognised that ?investigating a collec-
tion of forms that represent, for example, a partic-
ular speech act leads to the problem of establish-
ing which forms constitute that collection? (Archer
et al, 2008). Email offers particular challenges as
it has been shown to exhibit a higher frequency of
indirect speech acts than other media (Hassell and
Christensen, 1996). We approach the problem by
gathering judgments from human annotators and us-
ing this data to train supervised machine learning al-
gorithms.
Our request classifier works at the message-level,
marking emails as requests if they contain one or
more request utterances. As noted earlier, we define
a request as an utterance from the email sender that
places an obligation on a recipient to schedule an
action (e.g., add to a calendar or task list), perform
an action, or respond. Requests may be conditional
or unconditional in terms of the obligation they im-
pose on the recipient. Conditional requests require
action only if a stated condition is satisfied. Previous
annotation experiments have shown that conditional
requests are an important phenomena and occur fre-
quently in email (Scerri et al, 2008; Lampert et al,
2008a). Requests may also be phrased as either a
direct or indirect speech act.
Although some linguists distinguish between
speech acts that require a physical response and
those that require a verbal or information response,
e.g., (Sinclair and Coulthard, 1975), we follow
Searle?s approach and make no such distinction. We
thus consider questions requiring an informational
response to be requests, since they place an obliga-
tion on the recipient to answer.1
Additionally, there are some classes of request
which have been the source of systematic human
disagreement in our previous annotation experi-
ments. One such class consists of requests for
inaction. Requests for inaction, sometimes called
prohibitives (Sadock and Zwicky, 1985), prohibit
action or request negated action. An example is:
Please don?t let anyone else use the computer in the
office. As they impose an obligation on the sender,
we consider requests for inaction to be requests.
Similarly, we consider that meeting announcements
(e.g., Today?s Prebid Meeting will take place in
EB32c2 at 3pm) and requests to read, open or oth-
erwise act on documents attached to email messages
(e.g., See attached) are also requests.
Several complex classes of requests are particu-
larly sensitive to the context for their interpretation.
Reported requests are one such class. Some reported
requests, such as Paul asked if you could put to-
gether a summary of your accomplishments in an
email, clearly function as a request. Others do not
impose an obligation on the recipient, e.g., Sorry for
the delay; Paul requested your prize to be sent out
late December. The surrounding context must be
used to determine the intent of utterances like re-
ported requests. Such distinctions are often difficult
to automate.
Other complex requests include instructions.
Sometimes instructions are of the kind that one
might ?file for later use?. These tend to not be
marked as requests. Other instructions, such as Your
user id and password have been set up. Please fol-
low the steps below to access the new environment,
are intended to be executed more promptly. Tem-
poral distance between receipt of the instruction and
expected action is an important factor to distinguish
between requests and non-requests. Another influ-
encing property is the likelihood of the trigger event
that would lead to execution of the described ac-
tion. While the example instructions above are likely
to be executed, instructions for how to handle sus-
pected anthrax-infected mail are (for most people)
unlikely to be actioned.
Further detail and discussion of these and other
1Note, however, that not all questions are requests. Rhetori-
cal questions are perhaps the most obvious class of non-request
questions.
987
challenges in defining and interpreting requests in
email can be found in (Lampert et al, 2008b). In
particular, that paper includes analysis of a series of
complex edge cases that make even human agree-
ment in identifying requests difficult to achieve.
4.1 An Email Request Classifier
Our request classifier is based around an SVM clas-
sifier, implemented using Weka (Witten and Frank,
2005). Given an email message as input, complete
with header information, our binary request classi-
fier predicts the presence or absence of request ut-
terances within the message.
For training our request classifier, we use email
from the database dump of the Enron email corpus
released by Andrew Fiore and Jeff Heer.2 This ver-
sion of the corpus has been processed to remove du-
plicate messages and to normalise sender and recipi-
ent names, resulting in just over 250,000 email mes-
sages. No attachments are included.
Our request classifier training data is drawn from
a collection of 664 messages that were selected at
random from the Enron corpus. Each message was
annotated by three annotators, with overall kappa
agreement of 0.681. From the full dataset of 664
messages, we remove all messages where annota-
tors disagreed for training and evaluating our request
classifier, in order to mitigate the effects of annota-
tion noise, as discussed in (Beigman and Klebanov,
2009). The unanimously agreed data set used for
training consists of 505 email messages.
4.2 Request Classification Features
The features we use in our request classifier are:
? message length in characters and words;
? number and percentage of capitalised words;
? number of non alpha-numeric characters;
? whether the subject line contains markers of
email replies or forwards (e.g. Re:, Fw:);
? the presence of sender or recipient names;
? the presence of sentences that begin with a
modal verb (e.g., might, may, should, would);
? the presence of sentences that begin with a
question word (e.g, who, what, where, when,
why, which, how);
2http://bailando.sims.berkeley.edu/enron/enron.sql.gz
? whether the message contains any sentences
that end with a question mark; and
? binary word unigram and word bigram fea-
tures for n-grams that occur at least three times
across the training set.
Before generating n-gram features, we normalise
the message text as shown in Table 1, in a manner
similar to Carvalho and Cohen (2006). We also add
tokens marking the start and end of sentences, de-
tected using a modified version of Scott Piao?s sen-
tence splitter (Piao et al, 2002), and tokens marking
the start and end of the message.
Symbol Used Pattern
numbers Any sequence of digits
day Day names or abbreviations
pronoun-object Objective pronouns: me, her, him, us, them
pronoun-subject Subjective pronouns: I, we, you, he, she, they
filetype .doc, .pdf, .ppt, .txt, .xls, .rtf
multi-dash 3 or more sequential ?-? characters
multi-underscore 3 or more sequential ? ? characters
Table 1: Normalisation applied to n-gram features
Our initial request classifier achieves an accuracy of
72.28%. Table 2 shows accuracy, precision, recall
and F-measure results, calculated using stratified 10-
fold cross-validation, compared against a majority
class baseline. Given the well-balanced nature of
our training data (52.08% of messages contain a re-
quest), this is a reasonable basis for comparison.
Majority Baseline No Zoning Classifier
Request Non-Request Request Non-Request
Accuracy 52.08% 72.28%
Precision 0.521 0.000 0.729 0.716
Recall 1.000 0.000 0.745 0.698
F-Measure 0.685 0.000 0.737 0.707
Table 2: Request classifier results without email zoning
An error analysis of the predictions from our initial
request classifier uncovered a series of classification
errors that appeared to be due to request-like sig-
nals being picked up from parts of messages such as
email signatures and quoted reply content. It seemed
likely that our request classifier would benefit from
an email zone classifier that could identify and ig-
nore such message parts.
988
5 Improving Request Classification with
Email Zoning
Requests in email do not occur uniformly across the
zones that make up the email message. There are
specific zones of a message in which requests are
likely to occur.
Unfortunately, accurate classification of email
zones is difficult, hampered by the lack of standard
syntax used by different email clients to indicate dif-
ferent message parts, and by the ad hoc ways in
which people vary the structure and layout of their
messages. For example, different email clients indi-
cate quoted material in a variety of ways. Some pre-
fix every line of the quoted message with a character
such as ?>? or ?|?, while others indent the quoted
content or insert the quoted message unmodified,
prefixed by a message header. Sometimes the new
content is above the quoted content (a style known
as top-posting); in other cases, the new content may
appear after the quoted content (bottom-posting) or
interleaved with the quoted content (inline reply-
ing). Confounding the issue further is that users are
able to configure their email client to suit their in-
dividual tastes, and can change both the syntax of
quoting and their quoting style (top, bottom or in-
line replying) on a per message basis.
Despite the likelihood of some noise being in-
troduced through mis-classification of email zones,
our hypothesis was that even imperfect information
about the functional parts of a message should im-
prove the performance of our request classifier.
Based on this hypothesis, we integrated Zebra
(Lampert et al, 2009), our SVM-based email zone
classifier, to identify the different functional parts of
email messages. Using features that capture graphic,
orthographic and lexical information, Zebra classi-
fies and segments the body text into nine different
email zones: author content (written by the cur-
rent sender), greetings, signoffs, quoted reply con-
tent, forwarded content, email signatures, advertis-
ing, disclaimers, and automated attachment refer-
ences. Zebra has two modes of operation, classi-
fying either message fragments ? whitespace sepa-
rated sets of contiguous lines ? or individual lines.
We configure Zebra for line-based zone classifica-
tion, and use it to extract only lines classified as au-
thor, greeting and signoff text. We remove the con-
tent of all other zones before we evaluate features
for request classification.
6 Results and Discussion
Classifying the zones in email messages and ap-
plying our request classifier to only relevant mes-
sage parts significantly increases the performance
of the request classifier. As noted above, without
zoning, our request classifier achieves accuracy of
72.28% and a weighted F-measure (weighted be-
tween the F-measure for requests and non-requests
based on the relative frequency of each class) of
0.723. Adding the zone classifier, we increase the
accuracy to 83.76% and the weighted F-measure to
0.838. This corresponds to a relative increase in
both accuracy and weighted F-measure of 15.9%,
which in turn corresponds to an error reduction of
more than 41%. Table 3 shows a comparison of
the results of the non-zoning and zoning request
classifiers, generated using stratified 10-fold cross-
validation. In a two-tailed paired t-test, run over ten
iterations of stratified 10-fold cross-validation, the
increase in accuracy, precision, recall and f-measure
were all significant at p=0.01.
No Zoning With Zoning
Request Non-Request Request Non-Request
Accuracy 72.28% 83.76%*
Precision 0.729 0.716 0.849* 0.825*
Recall 0.745 0.698 0.837* 0.839*
F-Measure 0.737 0.707 0.843* 0.832*
Table 3: Request classifier results with and without email
zoning (* indicates a statistically significant difference at
p=0.01)
6.1 Lexical Feature Contribution
As expected, lexical information is crucial to re-
quest classification. When we experimented with re-
moving all lexical (n-gram) features, the non-zoning
request classifier accuracy dropped to 57.62% and
the zoning request classifier accuracy dropped to
61.78%. In contrast, when we apply only n-gram
features, we achieve accuracy of 71.49% for the
non-zoning classifier and 83.36% for the zoning
classifier. Clearly, lexical information is critical for
accurate request classification, regardless of whether
email messages are zoned.
989
Using Information Gain, we ranked the n-gram
features in terms of their usefulness. Table 4 shows
the top-10 unigrams and bigrams for our non-zoning
request classifier. Using these top-10 n-grams (plus
our non-n-gram features), we achieve only 66.34%
accuracy. These top-10 n-grams do not seem to
align well with linguistic intuitions, illustrating how
the noise from irrelevant message parts hampers per-
formance. In particular, there were several similar,
apparently automated messages that were annotated
(as non-requests) which appear to be the source of
several of the top-10 n-grams. This strongly sug-
gests that without zoning, the classifier is not learn-
ing features from the training set at a useful level of
generality.
Word Unigrams Word Bigrams
Word 1 Word 2
pronoun-object let pronoun-object
please pronoun-object know
iso start-sentence no
pronoun-subject start date
hourahead hour :
attached ; hourahead
let hourahead hour
westdesk start-sentence start
parsing westdesk /
if iso final
Table 4: Top 10 useful n-grams for our request classifier
without zoning, ranked by Information Gain
In contrast, once we add the zoning classifier, the
top-10 unigrams and bigrams appear to correspond
much better with linguistic intuitions about the lan-
guage of requests. These are shown in Table 5. Us-
ing these top-10 n-grams (plus our non-n-gram fea-
tures), we achieve 80% accuracy. This suggests that,
even with our relatively small amount of training
data, the zone classifier helps the request classifier
to extract fairly general n-gram features.
Interestingly, although lexical features are very
important, the top three features ranked by Informa-
tion Gain are non-lexical: message length in words,
the number of non-alpha-numeric characters in the
message and the number of capitalised words in the
message.
Word Unigrams Word Bigrams
Word 1 Word 2
please ? end-sentence
? pronoun-object know
pronoun-object let pronoun-object
if start-sentence please
pronoun-subject if pronoun-subject
let start-sentence thanks
to please let
know pronoun-subject have
thanks thanks comma
do start date
Table 5: Top 10 useful n-grams for our request classifier
with zoning, ranked by Information Gain
6.2 Learning Curves
Figure 1 shows a plot of accuracy, precision and
recall versus the number of training instances
used to build the request classifier. These re-
sults are calculated over zoned email bodies, us-
ing the average across ten iterations of stratified
10-fold cross-validation for each different sized
set of training instances, implemented via the
FilteredClassifier with the Resample fil-
ter in Weka. Given our pool of 505 agreed mes-
sage annotations, we plot the recall and precision for
training instance sets of size 50 to 505 messages.
There is a clear trend of increasing performance
as the training set size grows. It seems reasonable to
assume that more data should continue to facilitate
better request classifier performance. To this end,
we are annotating more data as part of our current
and future work.
6.3 Error Analysis
To explore the errors made by our request classifier,
we examined the output of our zoning request clas-
sifier using our full feature set, including all word
n-grams.
Approximately 20% of errors relate to requests
that are implicit, and thus more difficult to detect
from surface features. Another 10% of errors are
due to attempts to classify requests in inappropri-
ate genres of email messages. In particular, both
marketing messages and spam frequently include
request-like, directive utterances which our annota-
tors all agreed would not be useful to mark as re-
990
Figure 1: Learning curve showing recall, accuracy and
precision versus the number of training instances
quests for an email user. Not unreasonably, our clas-
sifier is sometimes confused by the content of these
messages, mistakenly marking requests where our
annotators did not. We intend to resolve these classi-
fication errors by filtering out such messages before
we apply the request classifier.
Another 5% of errors are due to request content
occurring in zones that we ignore. The most com-
mon case is content in a forwarded zone. Sometimes
email senders forward a message as a form of task
delegation; because we ignore forwarded content,
our request classifier misses such requests. We did
experiment with including content from forwarded
zones (in addition to the author, greeting and sig-
noff zones), but found that this reduced the perfor-
mance of our request classifier, presumably due to
the additional noise from irrelevant content in other
forwarded material. Forwarded messages are thus
somewhat difficult to deal with. One possible ap-
proach would be to build sender-specific profiles that
might allow us to deal with forwarded content (and
potentially content from other zones) differently for
different users, essentially learning to adapt to the
different styles of different email users.
A further 5% of errors involve errors in the zone
classifier, which leads to incorrect zone labels be-
ing applied to zone content that we would wish to
include for our request classifier. Examples include
author content being mistakenly identified as signa-
ture content. In such cases, we incorrectly remove
relevant content from the body text that is passed
to our request classifier. Improvements to the zone
classifier would resolve these issues.
As part of our annotation task, we also asked
coders to mark the presence of pleasantries. We
define a pleasantry as an utterance that could be a
request in some other context, but that does not func-
tion as a request in the context of use under consid-
eration. Pleasantries are frequently formulaic, and
do not place any significant obligation on the recip-
ient to act or respond. Variations on the phrase Let
me know if you have any questions are particularly
common in email messages. The context of the en-
tire email message needs to be considered to distin-
guish between when such an utterance functions as
a request and when it should be marked as a pleas-
antry. Of the errors made by our request classifier,
approximately 5% involve marking messages con-
taining only pleasantries as containing a request.
The remaining errors are somewhat diverse.
Close to 5% involve errors interpreting requests as-
sociated with attached files. The balance of almost
50% of errors involve a wide range of issues, from
misspellings of key words such as please to a lack
of punctuation cues such as question marks.
7 Conclusion
Request classification, like any form of automated
speech act recognition, is a difficult task. Despite
this inherent difficulty, the automatic request clas-
sifier we describe in this paper correctly labels re-
quests at the message level in 83.76% of email mes-
sages from our annotated dataset. Unlike previous
work that has attempted to automate the classifi-
cation of requests in email, we zone the messages
without manual intervention. This improves accu-
racy by 15.9% relative to the performance of the
same request classifier without the assistance of an
email zone classifier to focus on relevant message
parts. Although some zone classification errors are
made, error analysis reveals that only 5% of errors
are due to zone misclassification of message parts.
This suggests that, although zone classifier perfor-
mance could be further improved, it is likely that
focusing on improving the request classifier using
the existing zone classifier performance will lead to
greater performance gains.
991
References
Dawn Archer, Jonathan Culpeper, and Matthew Davies,
2008. Corpus Linguistics: An International Hand-
book, chapter Pragmatic Annotation, pages 613?642.
Mouton de Gruyter.
John L Austin. 1962. How to do things with words. Har-
vard University Press.
Eyal Beigman and Beata Beigman Klebanov. 2009.
Learning with annotation noise. In Proceedings of the
Joint Conference of the 47th Annual Meeting of the
ACL and the 4th IJCNLP, pages 280?287, Singapore.
Victoria Bellotti, Nicolas Ducheneaut, Mark Howard,
and Ian Smith. 2003. Taking email to task: The
design and evaluation of a task management centred
email tool. In Computer Human Interaction Confer-
ence, CHI, pages 345?352, Ft Lauderdale, Florida.
Vitor R Carvalho and William W Cohen. 2004. Learning
to extract signature reply lines from email. In Pro-
ceedings of First Conference on Email and Anti-Spam
(CEAS), Mountain View, CA, July 30-31.
Vitor R. Carvalho and William W. Cohen. 2006. Improv-
ing email speech act analysis via n-gram selection. In
Proceedings of HLT/NAACL 2006 - Workshop on Ana-
lyzing Conversations in Text and Speech, pages 35?41,
New York.
William W. Cohen, Vitor R. Carvalho, and Tom M.
Mitchell. 2004. Learning to classify email into
?speech acts?. In Conference on Empirical Meth-
ods in Natural Language Processing, pages 309?316,
Barcelona, Spain.
Simon H. Corston-Oliver, Eric Ringger, Michael Gamon,
and Richard Campbell. 2004. Task-focused summa-
rization of email. In ACL-04 Workshop: Text Summa-
rization Branches Out, pages 43?50.
Nicolas Ducheneaut and Victoria Bellotti. 2001. E-mail
as habitat: an exploration of embedded personal infor-
mation management. Interactions, 8(5):30?38.
Dominique Estival, Tanja Gaustad, Son Bao Pham, Will
Radford, and Ben Hutchinson. 2007. Author profiling
for English emails. In Proceedings of the 10th Con-
ference of the Pacific Association for Computational
Linguistics, pages 263?272, Melbourne, Australia.
Jade Goldstein and Roberta Evans Sabin. 2006. Using
speech acts to categorize email and identify email gen-
res. In Proceedings of the 39th Hawaii International
Conference on System Sciences, page 50b.
Lewis Hassell and Margaret Christensen. 1996. Indi-
rect speech acts and their use in three channels of com-
munication. In Proceedings of the First International
Workshop on Communication Modeling - The Lan-
guage/Action Perspective, Tilburg, The Netherlands.
Hamid Khosravi and Yorick Wilks. 1999. Routing email
automatically by purpose not topic. Journal of Natural
Language Engineering, 5:237?250.
Bryan Klimt and Yiming Yang. 2004. Introducing the
Enron corpus. In Proceedings of the Conference on
Email and Anti-Spam (CEAS).
Andrew Lampert, Robert Dale, and Ce?cile Paris. 2008a.
The nature of requests and commitments in email mes-
sages. In Proceedings of EMAIL-08: the AAAI Work-
shop on Enhanced Messaging, pages 42?47, Chicago.
Andrew Lampert, Robert Dal e, and Ce?cile Paris. 2008b.
Requests and commitments in email are more complex
than you think: Eight reasons to be cautious. In Pro-
ceedings of Australasian Language Technology Work-
shop (ALTA2008), pages 55?63, Hobart, Australia.
Andrew Lampert, Robert Dale, and Ce?cile Paris. 2009.
Segmenting email message text into zones. In Pro-
ceedings of Empirical Methods in Natural Language
Processing, pages 919?928, Singapore.
Wendy E. Mackay. 1988. More than just a communica-
tion system: Diversity in the use of electronic mail. In
ACM conference on Computer-supported cooperative
work, pages 344?353, Portland, Oregon, USA.
Denise E. Murray. 1991. Conversation for Action: The
Computer Terminal As Medium of Communication.
John Benjamins Publishing Co.
Scott S L Piao, Andrew Wilson, and Tony McEnery.
2002. A multilingual corpus toolkit. In Proceedings
of 4th North American Symposium on Corpus Linguis-
tics, Indianapolis.
Jerry M. Sadock and Arnold Zwicky, 1985. Language
Typology and Syntactic Description. Vol.I Clause
Structure, chapter Speech act distinctions in syntax,
pages 155?96. Cambridge University Press.
Simon Scerri, Myriam Mencke, Brian David, and
Siegfried Handschuh. 2008. Evaluating the ontology
powering smail ? a conceptual framework for seman-
tic email. In Proceedings of the 6th LREC Conference,
Marrakech, Morocco.
John R. Searle. 1969. Speech Acts: An Essay in the
Philosophy of Language. Cambridge University Press.
John Sinclair and Richard Malcolm Coulthard. 1975. To-
wards and Analysis of Discourse - The English used by
Teachers and Pupils. Oxford University Press.
Steve Whittaker and Candace Sidner. 1996. Email over-
load: exploring personal information management of
email. In ACM Computer Human Interaction confer-
ence, pages 276?283. ACM Press.
Terry Winograd and Fernando Flores. 1986. Under-
standing Computers and Cognition. Ablex Publishing
Corporation, Norwood, New Jersey, USA, 1st edition.
ISBN: 0-89391-050-3.
Ian Witten and Eiba Frank. 2005. Data Mining: Prac-
tical machine learning tools and techniques. Morgan
Kaufmann, San Francisco, 2nd edition.
992
