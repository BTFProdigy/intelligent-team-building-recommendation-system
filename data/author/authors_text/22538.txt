Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2070?2081,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Recall Error Analysis for Coreference Resolution
Sebastian Martschat and Michael Strube
Heidelberg Institute for Theoretical Studies gGmbH
Schloss-Wolfsbrunnenweg 35, 69118 Heidelberg, Germany
(sebastian.martschat|michael.strube)@h-its.org
Abstract
We present a novel method for coreference
resolution error analysis which we apply
to perform a recall error analysis of four
state-of-the-art English coreference reso-
lution systems. Our analysis highlights
differences between the systems and iden-
tifies that the majority of recall errors for
nouns and names are shared by all sys-
tems. We characterize this set of com-
mon challenging errors in terms of a broad
range of lexical and semantic properties.
1 Introduction
Coreference resolution is the task of determining
which mentions in a text refer to the same entity.
State-of-the-art approaches include both learning-
based (Fernandes et al., 2012; Bj?orkelund and
Farkas, 2012; Durrett and Klein, 2013) and de-
terministic models (Lee et al., 2013; Martschat,
2013). These approaches achieve state-of-the-art
performance mainly relying on morphosyntactic
and lexical factors. However, consider the follow-
ing example.
In order to improving the added value
of oil products, the second phase project
of the Qinghai Petroleum Bureau?s
Ge?ermu oil refinery has been put into
production. This will further improve
the factory?s oil products structure.
Due to the lack of any string overlap, most
state-of-the-art systems will miss the link between
the factory and the Qinghai Petroleum Bureau?s
Ge?ermu oil refinery. The information that factory
is a hypernym of refinery, however, may be useful
to resolve such links.
The aim of this paper is to quantify and char-
acterize such recall errors made by state-of-the-
art coreference resolution systems. By doing so,
we provide a solid foundation for work on em-
ploying knowledge sources for improving recall
for coreference resolution (Ponzetto and Strube,
2006; Rahman and Ng, 2011; Ratinov and Roth,
2012; Bansal and Klein, 2012, inter alia). In par-
ticular, we make the following contributions:
We present a novel framework for coreference
resolution error analysis. This yields a formal
foundation for previous work on link-based error
analysis (Uryupina, 2008; Martschat, 2013) and
complements work on transformation-based error
analysis (Kummerfeld and Klein, 2013).
We apply the method proposed in this paper to
perform a recall error analysis of four state-of-
the-art systems, encompassing deterministic and
learning-based approaches. In particular, we iden-
tify and characterize a set of challenging errors
common to all systems, and discuss strengths and
weaknesses of each system regarding specific er-
ror types. We also present a brief precision error
analysis.
A toolkit which implements the framework pro-
posed in this paper is available for download.
1
2 A Link-Based Analysis Framework
In this section we discuss challenges in corefer-
ence resolution error analysis and devise an error
analysis framework to overcome these challenges.
2.1 Motivation
Suppose a document contains the entity BARACK
OBAMA, which is referenced by four mentions
in the following order: Obama, he, the president
and his. A typical output of a current system not
equipped with world knowledge will consist of
two entities: {Obama, he} and {the president, his}
Obviously, the system made a recall error. But,
due to the complex nature of the coreference reso-
lution task, it is not clear how to represent the re-
1
http://smartschat.de/software
2070
(a)
m
1
Obama
m
2
he
m
3
the president
m
4
his
(b)
m
1
m
2
m
3
the president
m
4
n
1
n
2
n
3
(c)
m
1
m
2
m
3
the president
m
4
(d)
m
1
m
2
m
3
the president
m
4
Figure 1: (a) a reference entity r, represented as a complete one-directional graph, (b) a set S of three
system entities, (c) the partition r
S
, (d) a spanning tree for r.
call error: is it missing the link between the pres-
ident and Obama? Can the error be attributed to
deficiencies in pronoun resolution?
Linguistically motivated error representations
would facilitate both understanding of current
challenges and make system development faster
and easier. The aim of this section is to devise
such representations.
2.2 Formalizing Coreference Resolution
To start with, we give a formal description of the
coreference resolution task following the termi-
nology used for the ACE (Mitchell et al., 2004)
and OntoNotes (Weischedel et al., 2013) projects.
A mention is a linguistic realization of a reference
to an entity. Two mentions corefer if they refer to
the same entity. Hence, coreference is reflexive,
symmetric and transitive, and therefore an equiva-
lence relation. The task of coreference resolution
is to predict equivalence classes of mentions in a
document according to the coreference relation.
In order to extract errors, we need to compare
the reference equivalence classes, given by the
annotation, with the system equivalence classes
obtained from system output. The key question
now is how we represent these equivalence classes
of mentions. Adapting common terminology, we
also refer to the equivalence classes as entities.
2.3 Representing Entities
The most straightforward entity representation ig-
nores any structure and models an entity as a set
of mentions. This representation was utilized for
error analysis by Kummerfeld and Klein (2013),
who extract errors by transforming reference into
system entities. In this set-based representation,
we can only extract whether two mentions corefer
at all. More fine-grained information, for example
about antecedent information, is not accessible.
We therefore propose to employ a structured en-
tity representation, which explicitly models links
established by the coreference relation between
mentions. This leads to a link-based error repre-
sentation which formalizes the methods presented
in Uryupina (2008) and Martschat (2013).
We employ for representation a complete one-
directional graph. That is, we represent an en-
tity e over mentions {m
1
, . . . ,m
n
} as a graph
e = (N,A), where N = {m
1
, . . . ,m
n
} and
A = {(m
k
,m
j
) | k > j}. The indices respect
the mention ordering. Mentions earlier in the text
have a lower index. An example graph for an en-
tity over four mentions m
1
, . . . ,m
4
(such as the
BARACK OBAMA entity) is depicted in Figure 1a.
In this graph, we express all coreference relations
between all pairs of mentions.
2
Using this representation, we can represent a set
of entities as a set of graphs. In particular, given a
document we consider the set of reference entities
R given by the annotation, and the set of system
entities S, given by the system output. In order to
extract errors, we compare the graphs in R with
the graphs in S.
In the following, we discuss how to compute re-
call errors for a reference entity r ? R with respect
to the system entities S. For computing precision
errors, we just switch the roles of R and S.
2.4 Comparing Reference and System
Entities
As we represent entities as sets of links between
mentions, errors can be quantified as differences in
the links. For example, if an edge (representing a
link) from some reference entity r ? R is missing
2
We could also use an undirected instead of a one-
directional graph, but using a one-directional graph conve-
niently models sequential information, which simplifies no-
tation and the algorithms we will present.
2071
in all system entities in S, this is a recall error.
In order to formalize this, we employ the notion
of a partition of an entity. Let r ? R be some ref-
erence entity, and let S be a set of system entities.
The partition of r by S, written r
S
, is obtained
by taking all edges in r that also appear in S. r
S
consists of all connected components of r (we will
refer to these as subentities) that are also in S. All
edges in r that are not in r
S
are candidates for re-
call errors, as these were not in any entity in S.
Figure 1b shows a set S of three system entities:
two consist of two mentions, one of three men-
tions. In our running example, this corresponds
to the system output {Obama, he} and {the pres-
ident, his} plus some spurious mentions, which
are colored gray. The graph r
S
for our example
is shown in Figure 1c. The two edges correspond
to the correctly recognized links (he, Obama) and
(his, the president). All edges in r (Figure 1a)
missing from this graph are candidates for errors.
2.5 Spanning Trees
However, taking all edges in r missing in r
S
as er-
rors leads to unintuitive results. In the BARACK
OBAMA example, this would lead to four errors
being extracted: (the president, Obama), (his,
Obama), (the president, he) and (his, he). But,
in order to correctly predict the BARACK OBAMA
entity, a coreference resolution system only needs
to predict three correct links, i.e. it has to provide a
spanning tree of the entity?s graph representation.
Therefore, to extract errors, we compute a span-
ning tree T
r
of r, and take all edges in T
r
that do
not appear in r
S
as errors. Figure 1d shows an ex-
ample spanning tree for the running example en-
tity r. The dashed edge, which corresponds to the
link (the president, Obama), does not appear in r
S
and is therefore extracted as an error.
The strategies for computing a spanning tree
may differ for recall and precision errors. Hence,
our extraction algorithm is parametrized by two
procedures ST
rec
(e, P ) and ST
prec
(e, P ) which,
given an entity e and a set of entities P , output
a spanning tree T
e
of e. The whole algorithm for
error extraction is summarized in Algorithm 1.
3 Spanning Tree Algorithms
In the last section we presented a framework for
link-based error analysis, which extracts errors by
comparing entity spanning trees to entity parti-
tions. Therefore we can accommodate different
Algorithm 1 Error Extraction from a Corpus
Input: A corpus C, algorithms ST
rec
, ST
prec
for
computing spanning trees.
function ERRORS(C, ST
rec
, ST
prec
)
recall errors = [ ]
precision errors = [ ]
for d ? C do
Let R
d
be the reference entities and S
d
be the system entities of document d.
for r ? R
d
do
Add all edges in ST
rec
(r, S
d
) not in
r
S
d
to recall errors.
for s ? S
d
do
Add all edges in ST
prec
(s,R
d
) not in
s
R
d
to precision errors.
Output: recall errors, precision errors
notions of errors by varying the algorithm for com-
puting spanning trees. We now present some span-
ning tree algorithms for extracting recall and pre-
cision errors.
3.1 Recall Errors
We first observe that for error extraction, the struc-
ture of the spanning trees of the subentities appear-
ing in r
S
does not play a role. Edges present in r
S
are not candidates for errors, since they appear in
both the reference entity r and the system output
S. Therefore, it does not matter which edges from
the subentities are in the spanning tree.
Hence, to build the spanning tree, we first
choose arbitrary spanning trees for the subentities
in the partition. We choose the remaining edges
according to the spanning tree algorithm.
Having settled on this, we only have to decide
which edges to choose that connect the trees rep-
resenting the subentities. There are many possible
choices for this. For example, the graph in Fig-
ure 1c has four candidate edges which connect the
trees for the subentities.
We can reduce the number of candidate edges
by only considering the first mention (with respect
to textual order) in a subentity as the source of
an edge to be added. This makes sense since all
other mentions in that subentity were correctly re-
solved to be coreferent with some preceding men-
tion. We still have to decide on the target of the
edge. In Figure 1c, we have two choices for edges:
(m
3
,m
1
) and (m
3
,m
2
). We now present two
methods for choosing edges.
2072
Choosing Edges by Distance. The most
straight-forward way to decide on an edge is to
take the edge with smallest mention distance
between source and target. This is the approach
taken by Martschat (2013).
Choosing Edges by Accessibility. However, the
distance-based approach may lead to unintuitive
results. Let us consider again the BARACK
OBAMA example from Figure 1. When choosing
edges by distance, we would extract the error (the
president, he). However, such links with a non-
pronominal anaphor and a pronominal antecedent
are difficult to process and considered unreliable
(Ng and Cardie, 2002; Bengtson and Roth, 2008).
On the other hand, the missed link (the president,
Obama) constitutes a well-defined hyponymy re-
lation which can be found in knowledge bases and
is easily interpretable by humans.
Uryupina (Uryupina, 2007; Uryupina, 2008)
presents a recall error analysis where she takes
the ?intuitively easiest? missing link to analyze
(Uryupina, 2007, p. 196). How can we formal-
ize such an intuition? We will employ a no-
tion grounded in accessibility theory (Ariel, 1988).
Names and nouns refer to less accessible entities
than pronouns do. For such anaphors, we prefer
descriptive (name/nominal) antecedents. Inspired
by Ariel?s degrees of accessibility, we choose a
target for a given anaphor m
i
as follows:
? If m
i
is a pronoun, choose the closest preced-
ing mention.
? If m
i
is not a pronoun, choose the closest
preceding proper name. If no such mention
exists, choose the closest preceding common
noun. If no such mention exists, choose the
closest preceding mention.
Applied to the example from Figure 1, this algo-
rithm extracts the error (the president, Obama).
3
3.2 Precision Errors
Virtually all approaches to coreference resolu-
tion obtain entities by outputting pairs of anaphor
and antecedent, subject to the constraint that one
anaphor has at most one antecedent.
We use this information to build spanning trees
for system entities: these spanning trees con-
sist of exactly the edges which correspond to
anaphor/antecedent pairs in the system output.
3
A similar procedure was used by Ng and Cardie (2002)
to extract meaningful antecedents when training a corefer-
ence resolution system.
4 Data and Systems
We now discuss data and coreference resolution
systems which we will employ for our analysis.
4.1 Data
We analyze the errors of the systems on the En-
glish development data of the CoNLL?12 shared
task on multilingual coreference resolution (Prad-
han et al., 2012). This corpus contains 343 docu-
ments, spanning seven genres: bible texts, broad-
cast conversation, broadcast news, magazine texts,
news wire, telephone conversations and web logs.
4.2 Systems
State-of-the-art approaches to coreference resolu-
tion encompass various paradigms, ranging from
deterministic pairwise systems to learning-based
structured prediction models. Hence, we want to
conduct our analysis on a representative sample of
the state of the art, which should be publicly avail-
able. Therefore, we decided on two deterministic
and two learning-based systems:
? StanfordSieve
4
(Lee et al., 2013) was the
winning system of the CoNLL?11 shared
task. It employs a multi-sieve approach by
making more confident decisions first.
? Multigraph
5
(Martschat, 2013) is a deter-
ministic pairwise system which is based on
Martschat et al. (2012), the second-ranking
system in the English track of the CoNLL?12
shared task. It uses a subset of features as
hard constraints and chooses an antecedent
for a mention by summing up the remaining
boolean features.
? IMSCoref
6
(Bj?orkelund and Farkas, 2012)
ranked second overall in the CoNLL?12
shared task (third for English). It stacks mul-
tiple decoders and relies on a combination of
standard pairwise and lexicalized features.
? BerkeleyCoref
7
(Durrett and Klein, 2013) is
a state-of-the-art system that uses mainly lex-
icalized features and a latent antecedent rank-
ing architecture. It outperforms Stanford-
Sieve and IMSCoref on the CoNLL?11 data.
4
Part of Stanford CoreNLP, available at http://nlp.
stanford.edu/software/corenlp.shtml. We
use version 3.4.
5
http://smartschat.de/software
6
http://www.ims.uni-stuttgart.de/
forschung/ressourcen/werkzeuge/IMSCoref.
en.html . We use the CoNLL 2012 system.
7
http://nlp.cs.berkeley.edu/
berkeleycoref.shtml
2073
System MUC B
3
CEAF
e
Average
Fernandes et al. 69.46 57.83 54.43 60.57
Martschat 66.22 55.47 51.90 57.86
StanfordSieve 64.96 54.49 51.24 56.90
Multigraph 69.13 58.61 56.06 61.28
IMSCoref 67.15 55.19 50.94 57.76
BerkeleyCoref 70.27 59.29 56.11 61.89
Table 1: Comparison of the systems with Fernan-
des et al. (2012) and with Martschat (2013) on
CoNLL?12 English development data.
For Multigraph, we modified the system described
in Martschat (2013) slightly to allow for the in-
corporation of distance (similar to Cai and Strube
(2010)). Inspired by Lappin and Leass (1994), we
add salience weights for subjects and objects to
the model to improve third-person pronoun reso-
lution. We also extended the feature set by a sub-
string feature. Furthermore, motivated by Chen
and Ng (2012), we added a lexicalized feature for
non-pronominal mentions that were coreferent in
at least 50% of the cases in the training data.
StanfordSieve was run with its standard CoNLL
shared task settings. The learning-based sys-
tems were trained on the CoNLL?12 training data.
We trained IMSCoref with its standard settings,
and trained BerkeleyCoref with the final feature
set from Durrett and Klein (2013) for twenty it-
erations. We evaluate the systems on English
CoNLL?12 development data and compare it with
the winning system of the CoNLL?12 shared task
(Fernandes et al., 2012) and with Martschat (2013)
in Table 1, using the reference implementation v7
of the CoNLL scorer (Pradhan et al., 2014).
BerkeleyCoref performs best according to all
metrics, followed by Multigraph. StanfordSieve
is the worst performing system: the gap to Berke-
leyCoref is five points in average score.
4.3 Discussion
Although we analyze recent systems on a recently
published coreference data set, we believe that the
results of our analysis will have implications for
coreference in general. The data set is the largest
and most genre-diverse coreference corpus so far.
The systems we investigate represent major di-
rections in coreference resolution model research,
and make use of large and diverse feature sets pro-
posed in the literature (Ng, 2010).
5 A Comparative Analysis
The coreference resolution systems presented in
the previous section are a representative sample of
the state of the art. Therefore, by analyzing the
errors they make, we can learn about remaining
challenges in coreference resolution and analyze
the qualitative differences between the systems.
The results of such an analysis will deepen our
understanding of coreference resolution and will
suggest promising directions for further research.
5.1 Experimental Settings
Previous studies identified the presence of recall
errors as a main bottleneck for improving per-
formance (Raghunathan et al., 2010; Durrett and
Klein, 2013; Kummerfeld and Klein, 2013). This
is also evidenced by the CoNLL shared tasks on
coreference resolution (Pradhan et al., 2011; Prad-
han et al., 2012), where most competitive systems
had higher precision than recall. This indicates
that an analysis of recall errors helps to understand
and improve the state of the art. Hence, we focus
on analyzing recall errors, and complement this by
a brief analysis of precision errors.
We analyze errors of the four systems presented
in the previous section on the CoNLL?12 English
development data. To extract recall errors we em-
ploy the spanning tree algorithm which chooses
edges by accessibility. We obtain precision errors
from the pairwise output of the systems.
5.2 A Recall Error Analysis of StanfordSieve
Since StanfordSieve is currently the most-widely
used coreference resolution system, it serves as a
good starting point for our analysis. Remember
that we represent each error as a pair of anaphor
and antecedent. For an initial analysis, we cate-
gorize each error by mention type, distinguishing
between proper name, common noun, pronoun,
demonstrative pronoun and verb.
8
StanfordSieve makes 5245 recall errors. To put
this number into context, we compare it with the
maximum number of recall errors a system can
make. This count is obtained by extracting recall
errors from the output of a system that puts each
mention in its own entity, which yields 14609 er-
rors. In Table 2 we present a detailed analysis.
For each pair of mention type of anaphor and an-
8
We obtain the type from the part-of-speech tag of the
mention?s head. Furthermore, we treat every mention whose
head has a NER label in the data as a proper name.
2074
Name Noun Pron. Dem. Verb
Name
Errors 1006 181 43 0 0
Maximum 3578 206 56 2 0
Noun
Errors 517 1127 46 14 91
Maximum 742 2063 51 14 91
Pron.
Errors 483 761 543 45 53
Maximum 1166 1535 4596 92 53
Dem.
Errors 23 86 41 31 117
Maximum 27 93 43 46 117
Verb
Errors 1 20 2 4 10
Maximum 1 20 2 4 11
Table 2: Number of StanfordSieve?s recall er-
rors according to mention type, compared to the
maximum possible number of errors. Rows are
anaphors, columns antecedents.
tecedent, the table displays the number of recall
errors and of maximum errors possible.
StanfordSieve gets almost none of the links in-
volving verbal or demonstrative mentions correct.
This is due to the system not attempting to handle
event coreference, and performing very poorly for
demonstratives. On the other hand, recall for pro-
noun resolution is quite good, at least when con-
sidering non-verbal antecedents. While Stanford-
Sieve makes 1885 recall errors when the anaphor
is a pronoun, it successfully resolves most of such
links present in the corpus. Finally, let us consider
the links involving only proper names and com-
mon nouns. In total, these amount to 6589 links
in the corpus (around 45% of all links). Stanford-
Sieve misses 2831 of these links. Pairs of proper
names seem to be easier to resolve than pairs of
common nouns. Links between a common noun
and a proper name are less frequent, but much
more difficult: most of the links are missing.
5.3 Analysis of the Other Systems
In the previous section we identified various char-
acteristics of the errors made by StanfordSieve:
only (comparatively) few errors are made for pro-
noun resolution and name coreference, while other
types of nominal anaphora and coreference of
demonstrative/verbal mentions pose a challenge
for the system. Do the other systems in our study
also have these characteristics? In order to answer
Proportion
System Total Anaphor Pron. Name/Noun
StanfordSieve 5245 36% 54%
Multigraph 4630 32% 56%
IMSCoref 5220 32% 58%
BerkeleyCoref 4635 32% 56%
Table 3: Recall error numbers for all systems.
this question, we repeated the analysis for the three
other systems described in Section 4. We summa-
rize the results in Table 3. We only report num-
bers for pronoun resolution and name/noun coref-
erence, as all systems do not resolve verbal men-
tions and perform poorly for demonstratives.
StanfordSieve makes the most recall errors,
closely followed by IMSCoref. Multigraph
and BerkeleyCoref make around 600 errors less.
While the total number of errors differs between
the systems, the distributions are similar. In par-
ticular, around 55% of recall errors made involve
only proper names and common nouns. The num-
ber is a bit higher for IMSCoref. We conclude
that, despite variations in performance, both de-
terministic and learning-based state-of-the-art sys-
tems have similar weaknesses regarding recall.
The results displayed in Table 3 suggest vari-
ous opportunities for future research. In this pa-
per, we will focus on analyzing name/noun recall
errors, as these constitute a large fraction of all re-
call errors. Future work should address the pro-
noun resolution errors and a characterization of the
verbal/demonstrative errors.
5.4 Analysis of the Name/Noun Recall Errors
We now turn towards a fine-grained analysis of the
name/noun recall errors.
Table 4 displays the number of such recall errors
made by each system, according to the mention
types of anaphor and antecedent. We are interested
in errors common to all systems, and in qualitative
differences of errors between the systems.
5.4.1 Common Errors
Let us first analyze the errors common to all sys-
tems. Our analysis is driven by the question how
these can be characterized, and which knowledge
is missing to resolve such links. We discuss the
errors depending on the mention types of anaphor
and antecedent. The lower part of Table 4 displays
the number of common errors for each category.
2075
Number of Recall Errors (Anaphor-Antecedent)
Description Name-Name Noun-Name Name-Noun Noun-Noun
StanfordSieve 1006 517 181 1127
Multigraph 753 501 189 1152
IMSCoref 1082 500 188 1264
BerkeleyCoref 910 456 171 1072
Common errors 475 371 147 835
Correct boundaries idenfified 257 273 108 563
excl. IMSCoref 156 222 97 475
Table 4: Name/Noun recall errors for all systems.
Common All
Type % Type %
ORG 25% PERSON 26%
PERSON 19% GPE 26%
GPE 16% ORG 20%
DATE 14% NONE 14%
NONE 9% DATE 6%
Table 5: Distribution of top five named entity
types of common name-name recall errors and all
possible name-name recall errors.
Furthermore, in order to assess the impact of
mention detection, the table shows the number of
common errors where boundaries for both men-
tions were identified correctly by some system.
We can see that boundary identification is a diffi-
cult problem, especially for proper name pairs: for
48% of such errors, no system found the correct
boundaries of both mentions participating in the
error. The number of errors where correct bound-
aries could be found drops significantly after ex-
cluding IMSCoref. This is due to the mention ex-
traction strategy of IMSCoref: the other systems
in our study discard the shorter mention when two
mentions have the same head, IMSCoref keeps
both mentions. Hence, the system is able to cor-
rectly identify some mentions even in the presence
of parsing or preprocessing errors. However, as
a result, IMSCoref has to process many spurious
mentions, which makes learning more difficult.
We conclude that mention detection still consti-
tutes a challenge. We now proceed to a detailed
analysis of errors common to all systems. In pass-
ing we will discuss difficulties in mention detec-
tion with regard to specific error types.
Errors between Pairs of Proper Names. The
systems share 475 recall errors between pairs of
proper names. In Table 5, we compare the distri-
bution of gold named entity types of these errors
with the distribution of gold named entity types of
all possible errors (obtained via a singleton sys-
tem). We see that especially difficult classes of
links are pairs with type ORG or DATE.
Let us now consider lexical features of the er-
rors.
9
In 154 errors, the strings match completely,
but the correct resolution was mostly prevented by
annotation inconsistencies (e.g. China instead of
China?s) or propagated parsing and NER errors,
which lead to deficiencies in mention extraction.
For 217 errors, at least one token appears in both
mention strings, as in the ?Cole? and the ?USS
Cole?. This shows the insufficiency of the features
which hint to alias relations, may it be heuristics or
learned lexical similarities (for 109 of the 217 er-
rors, both mention boundaries were identified cor-
rectly by at least one system). Disambiguation
with respect to knowledge bases could provide a
principled way to identify name variations.
We classified the remaining 104 errors manu-
ally, see Table 6. For a couple of categories such
as identifying acronyms, spelling variations and
aliases, disambiguation could also help. Many er-
rors happen for date mentions, which suggests the
use of temporal tagging features.
Errors for Noun-Name Pairs. We now inves-
tigate the errors where the anaphor is a common
noun and the antecedent is a proper name. 371 er-
rors are common to all systems. The high fraction
of common errors shows that this is an especially
challenging category. We again start by investigat-
ing how the distribution of the named entity type
9
When computing these, we ignored case and ignored all
tokens with part-of-speech tag DT or POS.
2076
Description Occ. Example
Acronyms 20 National Ice Hockey League and
NHL
Alias 24 Florida and the Sunshine State
Annotation 2 Annotation errors (pronoun as
name)
Context 2 Paula Coccoz and juror number ten
Date 29 1989 and last year?s
Metonymy 12 South Afria and Pretoria
Roles 8 Al Gore and the Vice President
Spelling 7 Hsiachuotzu and Hsiachuotsu
Table 6: Classification of common name-name re-
call errors without common tokens.
Common All
Type % Type %
ORG 28% ORG 27%
PERSON 22% GPE 22%
GPE 19% PERSON 18%
NONE 7% NONE 11%
DATE 5% DATE 5%
Table 7: Distribution of top five named entity
types of common noun-name recall errors and all
possible noun-name recall errors.
of the antecedent differs when we compare com-
mon errors to all possible errors. The results are
shown in Table 7. Links with a proper name an-
tecedent of type PERSON are especially difficult.
They constitute 22% of the common errors, but
only 18% of all possible errors.
Most mentions are in a hyponymy relation, like
the prime minister and Mr. Papandreou. This con-
firms that harnessing such relations could improve
coreference resolution (Rahman and Ng, 2011;
Uryupina et al., 2011). For 65 of the errors (18%)
there is lexical overlap: the head of the anaphor is
contained in the proper name antecedent, as in the
entire park and the Ocean Park.
When categorizing all common errors accord-
ing to the head of the anaphor, we observe 204 dif-
ferent heads. 142 heads appear only once, but the
top ten heads make up 88 of the 371 errors. The
most frequent heads are company (15), group (12),
government, country and nation (each 9). This
suggests that even with few reliable hyponymy re-
lations recall could be significantly improved.
We observe similar trends when the anaphor is
a proper name and the antecedent is a noun.
Reference System
System Stanford MG IMS Berkeley
Stanford - 51 47 61
MG 17 - 42 60
IMS 26 54 - 54
Berkeley 12 42 25 -
Table 8: Comparison of noun-name recall errors.
Entries are errors made by the system in the row,
while the participating mentions are coreferent ac-
cording to the the system in the column.
Errors between Pairs of Common Nouns. 835
errors between pairs of common nouns are shared
by all systems. For 174 of these, the anaphor is
an indefinite noun phase, which makes resolution
a lot harder, since most coreference resolution sys-
tems classify these as non-anaphoric and therefore
do not attempt resolution.
For further analysis, we split all 835 errors in
two categories, distinguishing whether the head
matches between the mentions or not. In 341 cases
the heads match. For many of these cases, parsing
errors propagate and prevent the systems from rec-
ognizing the correct mention boundaries.
In order to get a better understanding of the er-
rors for nouns with different heads, we randomly
extracted 50 of the 494 pairs and investigated the
relation that holds between the heads. In 23 cases,
the heads were related via hyponymy. In 10 cases
they were synonyms. The remaining 17 cases
involve many different phenomena, for example
meronymy. This confirms findings from previous
research (Vieira and Poesio, 2000).
Hence, looking up lexical relations, especially
hyponymy, might be helpful to solve these cases.
5.4.2 Differences between the Systems
In order to analyze differences between the sys-
tems, we compare the recall errors they make.
The information how recall errors differ between
systems will enable us to understand individual
strengths and weaknesses.
Exemplarily, we will have a look at the differ-
ences in the errors when the anaphor is a common
noun and the antecedent is a proper name. By sys-
tem design and by the total error numbers (Table
4) we expect the learning-based systems to have a
slight advantage over the deterministic systems.
In Table 8 we compare noun-name recall errors
made by each system. Entries are errors made by
2077
Number and Proportion of Precision Errors (Anaphor-Antecedent)
Description Name-Name Noun-Name Name-Noun Noun-Noun
StanfordSieve 1038 31% 64 59% 65 72% 944 48%
Multigraph 1131 30% 76 51% 24 56% 743 42%
IMSCoref 834 26% 74 59% 46 64% 1050 54%
BerkeleyCoref 810 24% 191 67% 60 62% 1015 48%
Common errors 158 1 2 167
Table 9: Name/Noun precision errors for all systems. The percentages are the proportion of precision
errors with respect to all decision of the system in that category.
the system in the row, while the participating men-
tions are coreferent according to the the system in
the column. The numbers confirm our hypothesis,
but also show that the deterministic systems are
able to recover a few links missed by the learning-
based systems.
For example, BerkeleyCoref recovers 60 links
that could not be found by Multigraph, including
34 links without any common token, such as the
airline and Pan Am. Multigraph recovers only 42
links not found by BerkeleyCoref, 21 without any
common token. Qualitatively, StanfordSieve and
Multigraph are able to resolve a few links thanks
to their engineered substring match, such as the
judge and Dallas District Judge Jack Hampton.
We also conducted similar investigations for
common noun and proper name pairs. For com-
mon nouns, the trends are similar: the learning-
based systems have an advantage over the deter-
ministic systems. However, only few relations be-
tween nouns with different heads are learned ?
compared to StanfordSieve, BerkeleyCoref recov-
ers only 11 such pairs, such as the man and an
expert in the law. Recall of the deterministic sys-
tems is further hampered by their strict checks for
modifier agreement, which they employ to keep
precision high. Both systems miss for example the
link from the anaphor the Milosevic regime to the
regime, since the nominal modifier of the anaphor
does not appear in the antecedent.
For proper names, Multigraph employs so-
phisticated alias heuristics which help to resolve
matches such as Marshall Ye Ting?s and his grand-
father Ye Ting. This explains the corresponding
low number in Table 4. The lexicalized features
of Multigraph, IMSCoref and BerkeleyCoref help
to learn aliases when there is no string match, es-
pecially for the bible part of the corpus (resolving
links such as Jesus and the Son of Man).
5.5 Precision Errors
In the above analysis we identified common
name/noun recall errors and discussed strengths
and weaknesses of each system. Let us comple-
ment this analysis by a brief discussion of corre-
sponding precision errors.
Table 9 gives an overview. It displays the num-
ber of precision errors for each category, and the
proportion of these errors compared to all deci-
sions in that category. We can see some general
trends from this table: first, more decisions lead to
a higher proportion of errors. This shows the dif-
ficulty of balancing recall and precision. Second,
proper name coreference seems much easier than
common noun coreference. Coreference involving
different mention types is a lot harder ? the sys-
tems only attempt few decisions, most of them are
wrong. This confirms findings from our recall er-
ror analysis. Third, the fraction of common errors
is very low, which indicates that precisions errors
stem from various sources, which are handled dif-
ferently by each system.
6 Related Work
We now discuss related work in coreference res-
olution error analysis and in the related field of
coreference resolution evaluation metrics.
Error Analysis. While many papers on coref-
erence resolution briefly discuss errors made and
resolved by the system under consideration, only
few concentrate on error analysis. Uryupina
(2008) presents a manual error analysis on the
small MUC-7 test set; Martschat (2013) performs
an automatic coarse-grained error classification on
CoNLL data. By extending and formalizing the
approach of Martschat (2013), we are able to per-
form a large-scale investigation of recall errors
made by state-of-the-art systems.
2078
Kummerfeld and Klein (2013) devise a method
to extract errors from transformations of reference
to system entities. They apply this method to a
variety of systems and aggregate errors over these
systems. By aggregating, they are not able to ana-
lyze differences. They furthermore focus on de-
scribing many different error classes, instead of
closely investigating particular phenomena.
Evaluation Metrics. We extract recall and pre-
cision errors. How does our error analysis frame-
work relate to coreference resolution evaluation
metrics, which quantify recall and precision er-
rors? We first observe a fundamental difference:
evaluation metrics deal with scoring coreference
chains, they provide no means of extracting recall
or precision errors. Therefore our analysis com-
plements insights obtained via evaluation metrics.
We follow Chen and Ng (2013) and distinguish
between linguistically agnostic metrics, which do
not employ linguistic information during scoring,
and linguistically informed metrics, which employ
linguistic information similar as we do when com-
puting spanning trees.
We limit the discussion of linguistically ag-
nostic metrics to the three most popular evalua-
tion metrics whose average constitutes the official
score in the CoNLL shared tasks on coreference
resolution: MUC (Vilain et al., 1995), B
3
(Bagga
and Baldwin, 1998) and CEAF
e
(Luo, 2005).
10
Our framework bears most similarities to the
MUC metric, as both are based on the same link-
based entity representation. In particular, when we
divide the number of errors extracted from an en-
tity by the size of a spanning tree for that entity, we
obtain a score linearly related
11
to the MUC score
for that entity (recall for reference entities, preci-
sion for system entities). B
3
and CEAF
e
are not
founded on a link-based structure. B
3
computes
recall by computing the relative overlap of refer-
ence and system entity for each reference mention,
and then normalizes by the number of mentions.
CEAF
e
computes an optimal entity alignment with
respect to the relative overlap, and then normalizes
by the number of entities. As the metrics are not
link-based, they do not provide means to extract
link-based errors. We leave determining whether
the framework of these metrics exhibits a useful
notion of errors to future work.
10
These are linguistically agnostic since they do not differ
between different mention or entity types when evaluating.
11
via the transformation x 7? 1? x
Recent work considered devising evaluation
metrics which take linguistic information into
account. Chen and Ng (2013) inject linguis-
tic knowledge into existing evaluation metrics by
weighting links in an entity representation graph.
Tuggener (2014) devises scoring algorithms tai-
lored for particular applications by redefining the
notion of a correct link. While both of these works
focus on scoring, they weight or explicitly define
links in the reference and system entities, thereby
they in principle allow error extraction. However,
the authors do not attempt this and it is not clear
whether the errors extracted that way are useful for
analysis and system development.
7 Conclusions
We presented a novel link-based framework for
coreference resolution error analysis, which ex-
tends and complements previous work. We ap-
plied the framework to analyze recall errors of four
state-of-the-art systems on a large English bench-
mark dataset. Concentrating on errors involving
only proper names and common nouns, we identi-
fied a core set of challenging errors common to all
systems in our study.
We characterized the common errors among a
broad range of properties. In particular, our anal-
ysis highlights and quantifies the usefulness of
world knowledge. Furthermore, by comparing the
recall errors made by each system, we identified
individual strengths and weaknesses. A brief pre-
cision error analysis highlighted the hardness of
resolving noun-name and noun-noun links.
The presented method and findings help to iden-
tify challenges in coreference resolution and to in-
vestigate ways to overcome these challenges.
Acknowledgments
This work has been funded by the Klaus Tschira
Foundation, Heidelberg, Germany. The first au-
thor has been supported by a HITS Ph.D. scholar-
ship.
References
Mira Ariel. 1988. Referring and accessibility. Journal
of Linguistics, 24(1):65?87.
Amit Bagga and Breck Baldwin. 1998. Algorithms
for scoring coreference chains. In Proceedings
of the 1st International Conference on Language
Resources and Evaluation, Granada, Spain, 28?30
May 1998, pages 563?566.
2079
Mohit Bansal and Dan Klein. 2012. Coreference se-
mantics from web features. In Proceedings of the
50th Annual Meeting of the Association for Compu-
tational Linguistics (Volume 1: Long Papers), Jeju
Island, Korea, 8?14 July 2012, pages 389?398.
Eric Bengtson and Dan Roth. 2008. Understanding
the value of features for coreference resolution. In
Proceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing, Waikiki,
Honolulu, Hawaii, 25?27 October 2008, pages 294?
303.
Anders Bj?orkelund and Rich?ard Farkas. 2012. Data-
driven multilingual coreference resolution using re-
solver stacking. In Proceedings of the Shared Task
of the 16th Conference on Computational Natural
Language Learning, Jeju Island, Korea, 12?14 July
2012, pages 49?55.
Jie Cai and Michael Strube. 2010. End-to-end coref-
erence resolution via hypergraph partitioning. In
Proceedings of the 23rd International Conference
on Computational Linguistics, Beijing, China, 23?
27 August 2010, pages 143?151.
Chen Chen and Vincent Ng. 2012. Combining the
best of two worlds: A hybrid approach to multilin-
gual coreference resolution. In Proceedings of the
Shared Task of the 16th Conference on Computa-
tional Natural Language Learning, Jeju Island, Ko-
rea, 12?14 July 2012, pages 56?63.
Chen Chen and Vincent Ng. 2013. Linguistically
aware coreference evaluation metrics. In Proceed-
ings of the 6th International Joint Conference on
Natural Language Processing, Nagoya, Japan, 14?
18 October 2013, pages 1366?1374.
Greg Durrett and Dan Klein. 2013. Easy victories
and uphill battles in coreference resolution. In Pro-
ceedings of the 2013 Conference on Empirical Meth-
ods in Natural Language Processing, Seattle, Wash.,
18?21 October 2013, pages 1971?1982.
Eraldo Fernandes, C??cero dos Santos, and Ruy Milidi?u.
2012. Latent structure perceptron with feature in-
duction for unrestricted coreference resolution. In
Proceedings of the Shared Task of the 16th Confer-
ence on Computational Natural Language Learning,
Jeju Island, Korea, 12?14 July 2012, pages 41?48.
Jonathan K. Kummerfeld and Dan Klein. 2013. Error-
driven analysis of challenges in coreference reso-
lution. In Proceedings of the 2013 Conference on
Empirical Methods in Natural Language Process-
ing, Seattle, Wash., 18?21 October 2013, pages 265?
277.
Shalom Lappin and Herbert J. Leass. 1994. An algo-
rithm for pronominal anaphora resolution. Compu-
tational Linguistics, 20(4):535?561.
Heeyoung Lee, Angel Chang, Yves Peirsman,
Nathanael Chambers, Mihai Surdeanu, and Dan Ju-
rafsky. 2013. Deterministic coreference resolu-
tion based on entity-centric, precision-ranked rules.
Computational Linguistics, 39(4):885?916.
Xiaoqiang Luo. 2005. On coreference resolution
performance metrics. In Proceedings of the Hu-
man Language Technology Conference and the 2005
Conference on Empirical Methods in Natural Lan-
guage Processing, Vancouver, B.C., Canada, 6?8
October 2005, pages 25?32.
Sebastian Martschat, Jie Cai, Samuel Broscheit,
?
Eva
M?ujdricza-Maydt, and Michael Strube. 2012. A
multigraph model for coreference resolution. In
Proceedings of the Shared Task of the 16th Confer-
ence on Computational Natural Language Learning,
Jeju Island, Korea, 12?14 July 2012, pages 100?106.
Sebastian Martschat. 2013. Multigraph clustering for
unsupervised coreference resolution. In 51st Annual
Meeting of the Association for Computational Lin-
guistics: Proceedings of the Student Research Work-
shop, Sofia, Bulgaria, 5?7 August 2013, pages 81?
88.
Alexis Mitchell, Stephanie Strassel, Shudong Huang,
and Ramez Zakhary. 2004. ACE 2004 multilingual
training corpus. LDC2005T09, Philadelphia, Penn.:
Linguistic Data Consortium.
Vincent Ng and Claire Cardie. 2002. Improving ma-
chine learning approaches to coreference resolution.
In Proceedings of the 40th Annual Meeting of the As-
sociation for Computational Linguistics, Philadel-
phia, Penn., 7?12 July 2002, pages 104?111.
Vincent Ng. 2010. Supervised noun phrase corefer-
ence research: The first fifteen years. In Proceed-
ings of the 48th Annual Meeting of the Associa-
tion for Computational Linguistics, Uppsala, Swe-
den, 11?16 July 2010, pages 1396?1411.
Simone Paolo Ponzetto and Michael Strube. 2006.
Exploiting semantic role labeling, WordNet and
Wikipedia for coreference resolution. In Proceed-
ings of the Human Language Technology Confer-
ence of the North American Chapter of the Associa-
tion for Computational Linguistics, New York, N.Y.,
4?9 June 2006, pages 192?199.
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,
Martha Palmer, Ralph Weischedel, and Nianwen
Xue. 2011. CoNLL-2011 Shared Task: Modeling
unrestricted coreference in OntoNotes. In Proceed-
ings of the Shared Task of the 15th Conference on
Computational Natural Language Learning, Port-
land, Oreg., 23?24 June 2011, pages 1?27.
Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,
Olga Uryupina, and Yuchen Zhang. 2012. CoNLL-
2012 Shared Task: Modeling multilingual unre-
stricted coreference in OntoNotes. In Proceedings
2080
of the Shared Task of the 16th Conference on Com-
putational Natural Language Learning, Jeju Island,
Korea, 12?14 July 2012, pages 1?40.
Sameer Pradhan, Xiaoqiang Luo, Marta Recasens, Ed-
uard Hovy, Vincent Ng, and Michael Strube. 2014.
Scoring coreference partitions of predicted men-
tions: A reference implementation. In Proceed-
ings of the 52nd Annual Meeting of the Association
for Computational Linguistics (Volume 2: Short Pa-
pers), Baltimore, Md., 22?27 June 2014, pages 30?
35.
Karthik Raghunathan, Heeyoung Lee, Sudarshan Ran-
garajan, Nathanael Chambers, Mihai Surdeanu, Dan
Jurafsky, and Christopher Manning. 2010. A multi-
pass sieve for coreference resolution. In Proceed-
ings of the 2010 Conference on Empirical Methods
in Natural Language Processing, Cambridge, Mass.,
9?11 October 2010, pages 492?501.
Altaf Rahman and Vincent Ng. 2011. Coreference res-
olution with world knowledge. In Proceedings of
the 49th Annual Meeting of the Association for Com-
putational Linguistics (Volume 1: Long Papers),
Portland, Oreg., 19?24 June 2011, pages 814?824.
Lev Ratinov and Dan Roth. 2012. Learning-based
multi-sieve co-reference resolution with knowledge.
In Proceedings of the 2012 Conference on Empirical
Methods in Natural Language Processing and Nat-
ural Language Learning, Jeju Island, Korea, 12?14
July 2012, pages 1234?1244.
Don Tuggener. 2014. Coreference resolution evalua-
tion for higher level applications. In Proceedings of
the 14th Conference of the European Chapter of the
Association for Computational Linguistics, Gothen-
burg, Sweden, 26?30 April 2014, pages 231?235.
Olga Uryupina, Massimo Poesio, Claudio Giuliano,
and Kateryna Tymoshenko. 2011. Disambigua-
tion and filtering methods in using web knowledge
for coreference resolution. In Proceedings of the
24th International Florida Artificial Intelligence Re-
search Society Conference, Palm Beach, USA, 18?
20 May 2011, pages 317?322.
Olga Uryupina. 2007. Knowledge acquisition for
coreference resolution. Ph.D. thesis, Saarland Uni-
versity, Saarbr?ucken, Germany.
Olga Uryupina. 2008. Error analysis for learning-
based coreference resolution. In Proceedings of
the 6th International Conference on Language Re-
sources and Evaluation, Marrakech, Morocco, 26
May ? 1 June 2008, pages 1914?1919.
Renata Vieira and Massimo Poesio. 2000. An
empirically-based system for processing definite de-
scriptions. Computational Linguistics, 26(4):539?
593.
Marc Vilain, John Burger, John Aberdeen, Dennis Con-
nolly, and Lynette Hirschman. 1995. A model-
theoretic coreference scoring scheme. In Proceed-
ings of the 6th Message Understanding Conference
(MUC-6), pages 45?52, San Mateo, Cal. Morgan
Kaufmann.
Ralph Weischedel, Martha Palmer, Mitchell Marcus,
Eduard Hovy, Sameer Pradhan, Lance Ramshaw,
Nianwen Xue, Ann Taylor, Jeff Kaufman, Michelle
Franchini, Mohammed El-Bachouti, Robert Belvin,
and Ann Houston. 2013. OntoNotes release 5.0.
LDC2013T19, Philadelphia, Penn.: Linguistic Data
Consortium.
2081
Proceedings of the ACL Student Research Workshop, pages 81?88,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Multigraph Clustering for Unsupervised Coreference Resolution
Sebastian Martschat
Heidelberg Institute for Theoretical Studies gGmbH
Schloss-Wolfsbrunnenweg 35
69118 Heidelberg, Germany
sebastian.martschat@h-its.org
Abstract
We present an unsupervised model for
coreference resolution that casts the prob-
lem as a clustering task in a directed la-
beled weighted multigraph. The model
outperforms most systems participating in
the English track of the CoNLL?12 shared
task.
1 Introduction
Coreference resolution is the task of determining
which mentions in a text refer to the same en-
tity. With the advent of machine learning and
the availability of annotated corpora in the mid
1990s the research focus shifted from rule-based
approaches to supervised machine learning tech-
niques. Quite recently, however, rule-based ap-
proaches regained popularity due to Stanford?s
multi-pass sieve approach which exhibits state-
of-the-art performance on many standard coref-
erence data sets (Raghunathan et al, 2010) and
also won the CoNLL-2011 shared task on coref-
erence resolution (Lee et al, 2011; Pradhan et
al., 2011). These results show that carefully
crafted rule-based systems which employ suitable
inference schemes can achieve competitive perfor-
mance. Such a system can be considered unsuper-
vised in the sense that it does not employ training
data for optimizing parameters.
In this paper we present a graph-based approach
for coreference resolution that models a document
to be processed as a graph. The nodes are men-
tions and the edges correspond to relations be-
tween mentions. Coreference resolution is per-
formed via graph clustering. Our approach be-
longs to a class of recently proposed graph models
for coreference resolution (Cai and Strube, 2010;
Sapena et al, 2010; Martschat et al, 2012) and
is designed to be a simplified version of existing
approaches. In contrast to previous models be-
longing to this class we do not learn any edge
weights but perform inference on the graph struc-
ture only which renders our model unsupervised.
On the English data of the CoNLL?12 shared task
the model outperforms most systems which partic-
ipated in the shared task.
2 Related Work
Graph-based coreference resolution. While
not developed within a graph-based framework,
factor-based approaches for pronoun resolution
(Mitkov, 1998) can be regarded as greedy clus-
tering in a multigraph, where edges representing
factors for pronoun resolution have negative or
positive weight. This yields a model similar to
the one presented in this paper though Mitkov?s
work has only been applied to pronoun resolu-
tion. Nicolae and Nicolae (2006) phrase coref-
erence resolution as a graph clustering problem:
they first perform pairwise classification and then
construct a graph using the derived confidence val-
ues as edge weights. In contrast, work by Culotta
et al (2007), Cai and Strube (2010) and Sapena
et al (2010) omits the classification step entirely.
Sapena et al (2010) and Cai and Strube (2010)
perform coreference resolution in one step using
graph partitioning approaches. These approaches
participated in the recent CoNLL?11 shared task
(Pradhan et al, 2011; Sapena et al, 2011; Cai
et al, 2011b) with excellent results. The ap-
proach by Cai et al (2011b) has been modified by
Martschat et al (2012) and ranked second in the
English track at the CoNLL?12 shared task (Prad-
han et al, 2012). The top performing system at
the CoNLL?12 shared task (Fernandes et al, 2012)
81
also represents the problem as a graph by per-
forming inference on trees constructed using the
multi-pass sieve approach by Raghunathan et al
(2010) and Lee et al (2011), which in turn won
the CoNLL?11 shared task.
Unsupervised coreference resolution. Cardie
and Wagstaff (1999) present an early approach to
unsupervised coreference resolution based on a
straightforward clustering approach. Angheluta et
al. (2004) build on their approach and devise more
sophisticated clustering algorithms. Haghighi and
Klein (2007), Ng (2008) and Charniak and El-
sner (2009) employ unsupervised generative mod-
els. Poon and Domingos (2008) present a Markov
Logic Network approach to unsupervised corefer-
ence resolution. These approaches reach competi-
tive performance on gold mentions but not on sys-
tem mentions (Ng, 2008). The multi-pass sieve
approach by Raghunathan et al (2010) can also be
viewed as unsupervised.
3 A Multigraph Model
We aim for a model which directly represents the
relations between mentions in a graph structure.
Clusters in the graph then correspond to entities.
3.1 Motivation
To motivate the choice of our model, let us con-
sider a simple made-up example.
Leaders met in Paris to discuss recent
developments. They left the city today.
We want to model that Paris is not a likely candi-
date antecedent for They due to number disagree-
ment, but that Leaders and recent developments
are potential antecedents for They. We want to
express that Leaders is the preferred antecedent,
since Leaders and They are in a parallel construc-
tion both occupying the subject position in their
respective sentences.
In other words, our model should express the
following relations for this example:
? number disagreement for (They, Paris), which
indicates that the mentions are not coreferent,
? the anaphor being a pronoun for (They, Lead-
ers), (They, recent developments) and (They,
Paris), which is a weak indicator for corefer-
ence if the mentions are close to each other,
? syntactic parallelism for (They, Leaders): both
mentions are in a parallel construction in adja-
cent sentences (both in the subject slot), which
is also a weak coreference indicator.
We denote these relations as N Number,
P AnaPron and P Subject respectively. The
graphical structure depicted in Figure 1 mod-
els these relations between the four mentions
Leaders, Paris, recent developments and They.
Leaders
recent de-
velopments
They
ParisP AnaPron
P Subject
P AnaPron
N
Nu
mb
er
PA
na
Pro
n
Figure 1: An example graph modeling relations
between mentions.
A directed edge from a mention m to n indi-
cates that n precedes m and that there is some rela-
tion between m and n that indicates coreference or
non-coreference. Labeled edges describe the rela-
tions between the mentions, multiple relations can
hold between a pair. Edges may be weighted.
3.2 Multigraphs for Coreference Resolution
Formally, the model is a directed labeled weighted
multigraph. That is a tuple D = (R, V,A,w)
where
? R is the set of labels (in our case relations such
as P Subject that hold between mentions),
? V is the set of nodes (the mentions extracted
from a document),
? A ? V ? V ? R is the set of edges (relations
between two mentions),
? w is a mapping w : A? R? {??} (weights
for edges).
Many graph models for coreference resolution op-
erate on A = V ?V . Our multigraph model allows
us to have multiple edges with different labels be-
tween mentions.
To have a notion of order we employ a directed
graph: We only allow an edge from m to n if m
appears later in the text than n.
To perform coreference resolution for a docu-
ment d, we first construct a directed labeled multi-
graph (Section 3.3). We then assign a weight to
each edge (Section 3.4). The resulting graph is
82
clustered to obtain the mentions that refer to the
same entity (Section 3.5).
3.3 Graph Construction
Given a set M of mentions extracted from a doc-
ument d, we set V = M , i.e. the nodes of the
graph are the mentions. To construct the edges
A, we consider each pair (m,n) of mentions with
n ? m. We then check for every relation r ? R
if r holds for the pair (m,n). If this is the case
we add the edge (m,n, r) to A. For simplicity,
we restrict ourselves to binary relations that hold
between pairs of mentions (see Section 4).
The graph displayed in Figure 1 is the graph
constructed for the mentions Leaders, Paris, re-
cent developments and They from the example
sentence at the beginning of this Section, where
R = {P AnaPron, P Subject, N Number}.
3.4 Assigning Weights
Depending on whether a relation r ? R is indica-
tive for non-coreference (e.g. number disagree-
ment) or for coreference (e.g. string matching) it
should be weighted differently. We therefore di-
vide R into a set of negative relations R? and a
set of positive relations R+.
Previous work on multigraphs for coreference
resolution disallows any edge between mentions
for which a negative relations holds (Cai et al,
2011b; Martschat et al, 2012). We take a sim-
ilar approach and set w(m,n, r) = ?? for
(m,n, r) ? A when r ? R?1.
Work on graph-based models similar to ours re-
port robustness with regard to the amount of train-
ing data used (Cai et al, 2011b; Cai et al, 2011a;
Martschat et al, 2012). Motivated by their obser-
vations we treat every positive relation equally and
set w(m,n, r) = 1 for (m,n, r) ? A if r ? R+.
In contrast to previous work on similar graph
models we do not learn any edge weights from
training data. We compare this unsupervised
scheme with supervised variants empirically in
Section 5.
3.5 Clustering
To describe the clustering algorithm used in this
work we need some additional terminology. If
there exists an edge (m,n, r) ? A we say that n is
a child of m.
1We experimented with different weighting schemes
for negative relations on development data (e.g. setting
w(m,n, r) = ?1) but did not observe a gain in performance.
In the graph constructed according to the pro-
cedure described in Section 3.3, all children of a
mention m are candidate antecedents for m. The
relations we employ are indicators for coreference
(which get a positive weight) and indicators for
non-coreference (which get a negative weight).
We aim to employ a simple and efficient cluster-
ing scheme on this graph and therefore choose
1-nearest-neighbor clustering: for every m, we
choose as antecedent m?s child n such that the sum
of edge weights is maximal and positive. We break
ties by choosing the closest mention.
In the unsupervised setting described in Section
3.4 this algorithm reduces to choosing the child
that is connected via the highest number of posi-
tive relations and via no negative relation.
For the graph depicted in Figure 1 this algorithm
computes the clusters {They, Leaders}, {Paris}
and {recent developments}.
4 Relations
The graph model described in Section 3 is based
on expressing relations between pairs of mentions
via edges built from such relations. We now de-
scribe the relations currently used by our system.
They are well-known indicators and constraints
for coreference and are taken from previous work
(Cardie and Wagstaff, 1999; Soon et al, 2001;
Rahman and Ng, 2009; Lee et al, 2011; Cai et al,
2011b). All relations operate on pairs of mentions
(m,n), where m is the anaphor and n is a candi-
date antecedent. If a relation r holds for (m,n),
the edge (m,n, r) is added to the graph. We final-
ized the set of relations and their distance thresh-
olds on development data.
4.1 Negative Relations
Negative relations receive negative weights. They
allow us to introduce well-known constraints such
as agreement into our model.
(1) N Gender, (2) N Number: Two mentions do
not agree in gender or number. We compute
number and gender for common nouns us-
ing the number and gender data provided by
Bergsma and Lin (2006).
(3) N SemanticClass: Two mentions do not
agree in semantic class (we only use the top
categories Object, Date and Person from
WordNet (Fellbaum, 1998)).
(4) N ItDist: The anaphor is it or they and the
sentence distance to the antecedent is larger
83
than one.
(5) N Speaker12Pron: Two first person pro-
nouns or two second person pronouns with dif-
ferent speakers, or one first person pronoun
and one second person pronoun with the same
speaker2.
(6) N ContraSubObj: Two mentions are in the
subject/object positions of the same verb, the
anaphor is a non-possessive/reflexive pronoun.
(7) N Mod: Two mentions have the same syntac-
tic heads, and the anaphor has a nominal mod-
ifier which does not occur in the antecedent.
(8) N Embedding: Two mentions where one em-
beds the other, which is not a reflexive or pos-
sessive pronoun.
(9) N 2PronNonSpeech: Two second person
pronouns without speaker information and not
in direct speech.
4.2 Positive Relations
Positive relations are coreference indicators which
are added as edges with positive weights.
(10) P NonPron StrMatch: Applies only if the
anaphor is definite or a proper name3. This re-
lation holds if after discarding stop words the
strings of mentions completely match.
(11) P HeadMatch: If the syntactic heads of
mentions match.
(12) P Alias: If mentions are aliases of each other
(i.e. proper names with partial match, full
names and acronyms, etc.).
(13) P Speaker12Pron: If the speaker of the sec-
ond person pronoun is talking to the speaker
of the first person pronoun (applies only to
first/second person pronouns).
(14) P DSPron: One mention is a speak verb?s
subject, the other mention is a first person pro-
noun within the corresponding direct speech.
(15) P ReflPronSub: If the anaphor is a reflexive
pronoun, and the antecedent is the subject of
the sentence.
(16) P PossPronSub: If the anaphor is a posses-
sive pronoun, and the antecedent is the subject
of the anaphor?s sentence or subclause.
(17) P PossPronEmb: The anaphor is a posses-
2Like all relations using speaker information, this relation
depends on the gold speaker annotation layer in the corpus.
3This condition is necessary to cope with the high-recall
output of the mention tagger.
sive pronoun embedded in the antecedent.
(18) P AnaPron: If the anaphor is a pronoun and
none of the mentions is a first or second per-
son pronoun. This relation is restricted to a
sentence distance of 3.
(19) P VerbAgree: If the anaphor is a third per-
son pronoun and has the same predicate as the
antecedent. This relation is restricted to a sen-
tence distance of 1.
(20) P Subject, (21) P Object: The anaphor is a
third person pronoun and both mentions are
subjects/objects. These relations are restricted
to a sentence distance of 1.
(22) P Pron StrMatch: If both mentions are
pronouns and their strings match.
(23) P Pron Agreement: If both mentions are
different pronoun tokens but agree in number,
gender and person.
5 Evaluation
5.1 Data and Evaluation Metrics
We use the data provided for the English track of
the CoNLL?12 shared task on multilingual coref-
erence resolution (Pradhan et al, 2012) which is
a subset of the upcoming OntoNotes 5.0 release
and comes with various annotation layers provided
by state-of-the-art NLP tools. We used the official
dev/test split for development and evaluation. We
evaluate the model in a setting that corresponds
to the shared task?s closed track, i.e. we use only
WordNet (Fellbaum, 1998), the number and gen-
der data of Bergsma and Lin (2006) and the pro-
vided annotation layers. To extract system men-
tions we employ the mention extractor described
in Martschat et al (2012).
We evaluate our system with the coreference
resolution evaluation metrics that were used for
the CoNLL shared tasks on coreference, which are
MUC (Vilain et al, 1995), B3 (Bagga and Bald-
win, 1998) and CEAFe (Luo, 2005). We also re-
port the unweighted average of the three scores,
which was the official evaluation metric in the
shared tasks. To compute the scores we employed
the official scorer supplied by the shared task or-
ganizers.
5.2 Results
Table 1 displays the performance of our model and
of the systems that obtained the best (Fernandes
et al, 2012) and the median performance in the
84
MUC B3 CEAFe average
R P F1 R P F1 R P F1
CoNLL?12 English development data
best 64.88 74.74 69.46 66.53 78.28 71.93 54.93 43.68 48.66 63.35
median 62.3 62.8 62.0 66.7 71.8 69.1 46.4 44.9 45.6 58.9
this work (weights fraction) 64.00 68.56 66.20 66.59 75.67 70.84 50.48 45.52 47.87 61.63
this work (weights MaxEnt) 63.72 65.78 64.73 66.60 73.76 70.00 47.46 45.30 46.36 60.36
this work (unsupervised) 64.01 68.58 66.22 67.00 76.45 71.41 51.10 46.16 48.51 62.05
CoNLL?12 English test data
best 65.83 75.91 70.51 65.79 77.69 71.24 55.00 43.17 48.37 63.37
median 62.08 63.02 62.55 66.23 70.45 68.27 45.74 44.74 45.23 58.68
this work (weights fraction) 64.25 68.31 66.22 65.44 74.20 69.54 49.18 44.71 46.84 60.87
this work (weights MaxEnt) 63.58 64.70 64.14 65.63 72.09 68.71 45.58 44.41 44.99 59.28
this work (unsupervised) 63.95 67.99 65.91 65.47 74.93 69.88 49.83 45.40 47.51 61.10
Table 1: Results of different systems on the CoNLL?12 English data sets.
CoNLL?12 shared task, which are denoted as best
and median respectively. best employs a struc-
tured prediction model with learned combinations
of 70 basic features. We also compare with two
supervised variants of our model which use the
same relations and the same clustering algorithm
as the unsupervised model: weights fraction sets
the weight of a relation to the fraction of posi-
tive instances in training data (as in Martschat et
al. (2012)). weights MaxEnt trains a mention-pair
model (Soon et al, 2001) via the maximum en-
tropy classifier implemented in the BART toolkit
(Versley et al, 2008) and builds a graph where
the weight of an edge connecting two mentions
is the classifier?s prediction4. We use the official
CoNLL?12 English training set for training.
Our unsupervised model performs considerably
better than the median system from the CoNLL?12
shared task on both data sets according to all met-
rics. It also seems to be able to accommodate well
for the relations described in Section 4 since it out-
performs both supervised variants5. The model
performs worse than best, the gap according to B3
and CEAFe being considerably smaller than ac-
cording to MUC. While we observe a decrease of
1 point average score when evaluating on test data
the model still would have ranked fourth in the En-
glish track of the CoNLL?12 shared task with only
0.2 points difference in average score to the sec-
ond ranked system.
4The classifier?s output is a number p ? [0, 1]. In order to
have negative weights we use the transformation p? = 2p?1.
5Compared with the supervised variants all improvements
in F1 score are statistically significant according to a paired
t-test (p < 0.05) except for the difference in MUC F1 to
weights fraction.
6 Error Analysis
In order to understand weaknesses of our model
we perform an error analysis on the development
data. We distinguish between precision and recall
errors. For an initial analysis we split the errors
according to the mention type of anaphor and an-
tecedent (name, nominal and pronoun).
6.1 Precision Errors
Our system operates in a pairwise fashion. We
therefore count one precision error whenever the
clustering algorithm assigns two non-coreferent
mentions to the same cluster. Table 2 shows the
NAM NOM PRO
NAM 3413 (21%) 67 (66%) 11 (46%)
NOM 43 (67%) 2148 (49%) 9 (89%)
PRO 868 (32%) 1771 (55%) 5308 (24%)
Table 2: Number of clustering decisions made ac-
cording to mention type (rows anaphor, columns
antecedent) and percentage of wrong decisions.
number of clustering decisions made according to
the mention type and in brackets the fraction of de-
cisions that erroneously assign two non-coreferent
mentions to the same cluster. We see that two main
sources of error are nominal-nominal pairs and the
resolution of pronouns. We now focus on gain-
ing further insight into the system?s performance
for pronoun resolution by investigating the perfor-
mance per pronoun type. The results are displayed
in Table 3. We obtain good performance for I and
my which in the majority of cases can be resolved
unambiguously by the speaker relations employed
by our system. The relations we use also seem
85
Anaphor all anaphoric
I 1260 (13%) 1239 (11%)
my 192 (14%) 181 (9%)
he 824 (14%) 812 (13%)
. . . . . .
they 764 (29%) 725 (26%)
. . . . . .
you 802 (41%) 555 (15%)
it 1114 (64%) 720 (44%)
Table 3: Precision statistics for pronouns. Rows
are pronoun surfaces, columns number of cluster-
ing decisions and percentage of wrong decisions
for all and only anaphoric pronouns respectively.
to work well for he. In contrast, the local, shal-
low approach we currently employ is not able to
resolve highly ambiguous pronouns such as they,
you or it in many cases. The reduction in error rate
when only considering anaphoric pronouns shows
that our system could benefit from an improved
detection of expletive it and you.
6.2 Recall Errors
Estimating recall errors by counting all missing
pairwise links would consider each entity many
times. Therefore, we instead count one recall er-
ror for a pair (m,n) of anaphor m and antecedent
n if (i) m and n are coreferent, (ii) m and n are
not assigned to the same cluster, (iii) m is the first
mention in its cluster that is coreferent with n, and
(iv) n is the closest mention coreferent with m that
is not in m?s cluster.
This can be illustrated by an example. Consid-
ering mentions m1, . . . ,m5, assume that m1, m3,
m4 and m5 are coreferent but the system clusters
are {m2,m3} and {m4,m5}. We then count two
recall errors: one for the missing link from m3 to
m1 and one for the missing link from m4 to m3.
According to this definition we count 3528 re-
call errors on the development set. The distribu-
tion of errors is displayed in Table 4. We see that
NAM NOM PRO
NAM 321 220 247
NOM 306 797 330
PRO 306 476 525
Table 4: Number of recall errors according to
mention type (rows anaphor, columns antecedent).
the main source of recall errors are missing links
of nominal-nominal pairs. We randomly extracted
50 of these errors and manually assigned them to
different categories.
29 errors: missing semantic knowledge. In these
cases lexical or world knowledge is needed to
build coreference links between mentions with dif-
ferent heads. For example our system misses the
link between the sauna and the hotbox sweatbox.
14 errors: too restrictive N Mod. In these cases
the heads of the mentions matched but no link was
built due to N Mod. An example is the missing
link between our island?s last remaining forest of
these giant trees and the forest of Chilan.
4 errors: too cautious string match. We only
apply string matching for common nouns when the
noun is definite.
Three errors could not be attributed to any of the
above categories.
7 Conclusions and Future Work
We presented an unsupervised graph-based model
for coreference resolution. Experiments show that
our model exhibits competitive performance on
the English CoNLL?12 shared task data sets.
An error analysis revealed that two main
sources of errors of our model are the inaccurate
resolution of highly ambiguous pronouns such as
it and missing links between nominals with dif-
ferent heads. Future work should investigate how
semantic knowledge and more complex relations
capturing deeper discourse properties such as co-
herence or information status can be added to the
model. Processing these features efficently may
require a more sophisticated clustering algorithm.
We are surprised by the good performance of
this unsupervised model in comparison to the
state-of-the-art which uses sophisticated machine
learning techniques (Fernandes et al, 2012) or
well-engineered rules (Lee et al, 2011). We are
not sure how to interpret these results and want to
leave different interpretations for discussion:
? our unsupervised model is really that good
(hopefully),
? the evaluation metrics employed are to be
questioned (certainly),
? efficiently making use of annotated training
data still remains a challenge for the state-of-
the-art (likely).
Acknowledgments
This work has been funded by the Klaus Tschira
Foundation, Germany. The author has been sup-
ported by a HITS PhD scholarship.
86
References
Roxana Angheluta, Patrick Jeuniaux, Rudradeb Mitra,
and Marie-Francine Moens. 2004. Clustering al-
gorithms for noun phrase coreference resolution. In
Proceedings of the 7e`mes Journe?es Internationales
d?Analyse Statistique des Donne?es Textuelles, Lou-
vain La Neuve, Belgium, 10?12 March 2004, pages
60?70.
Amit Bagga and Breck Baldwin. 1998. Algorithms
for scoring coreference chains. In Proceedings
of the 1st International Conference on Language
Resources and Evaluation, Granada, Spain, 28?30
May 1998, pages 563?566.
Shane Bergsma and Dekang Lin. 2006. Bootstrap-
ping path-based pronoun resolution. In Proceedings
of the 21st International Conference on Computa-
tional Linguistics and 44th Annual Meeting of the
Association for Computational Linguistics, Sydney,
Australia, 17?21 July 2006, pages 33?40.
Jie Cai and Michael Strube. 2010. End-to-end coref-
erence resolution via hypergraph partitioning. In
Proceedings of the 23rd International Conference
on Computational Linguistics, Beijing, China, 23?
27 August 2010, pages 143?151.
Jie Cai, E?va Mu?jdricza-Maydt, Yufang Hou, and
Michael Strube. 2011a. Weakly supervised graph-
based coreference resolution for clinical data. In
Proceedings of the 5th i2b2 Shared Tasks and Work-
shop on Challenges in Natural Language Processing
for Clinical Data, Washington, D.C., 20-21 October
2011.
Jie Cai, E?va Mu?jdricza-Maydt, and Michael Strube.
2011b. Unrestricted coreference resolution via
global hypergraph partitioning. In Proceedings of
the Shared Task of the 15th Conference on Computa-
tional Natural Language Learning, Portland, Oreg.,
23?24 June 2011, pages 56?60.
Claire Cardie and Kiri Wagstaff. 1999. Noun phrase
coreference as clustering. In Proceedings of the
1999 SIGDAT Conference on Empirical Methods in
Natural Language Processing and Very Large Cor-
pora, College Park, Md., 21?22 June 1999, pages
82?89.
Eugene Charniak and Micha Elsner. 2009. EM works
for pronoun anaphora resolution. In Proceedings of
the 12th Conference of the European Chapter of the
Association for Computational Linguistics, Athens,
Greece, 30 March ? 3 April 2009, pages 148?156.
Aron Culotta, Michael Wick, and Andrew McCal-
lum. 2007. First-order probabilistic models for
coreference resolution. In Proceedings of Human
Language Technologies 2007: The Conference of
the North American Chapter of the Association for
Computational Linguistics, Rochester, N.Y., 22?27
April 2007, pages 81?88.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge,
Mass.
Eraldo Fernandes, C??cero dos Santos, and Ruy Milidiu?.
2012. Latent structure perceptron with feature in-
duction for unrestricted coreference resolution. In
Proceedings of the Shared Task of the 16th Confer-
ence on Computational Natural Language Learning,
Jeju Island, Korea, 12?14 July 2012, pages 41?48.
Aria Haghighi and Dan Klein. 2007. Unsupervised
coreference resolution in a nonparametric Bayesian
model. In Proceedings of the 45th Annual Meeting
of the Association for Computational Linguistics,
Prague, Czech Republic, 23?30 June 2007, pages
848?855.
Heeyoung Lee, Yves Peirsman, Angel Chang,
Nathanael Chambers, Mihai Surdeanu, and Dan Ju-
rafsky. 2011. Stanford?s multi-pass sieve corefer-
ence resolution system at the CoNLL-2011 shared
task. In Proceedings of the Shared Task of the
15th Conference on Computational Natural Lan-
guage Learning, Portland, Oreg., 23?24 June 2011,
pages 28?34.
Xiaoqiang Luo. 2005. On coreference resolution
performance metrics. In Proceedings of the Hu-
man Language Technology Conference and the 2005
Conference on Empirical Methods in Natural Lan-
guage Processing, Vancouver, B.C., Canada, 6?8
October 2005, pages 25?32.
Sebastian Martschat, Jie Cai, Samuel Broscheit, E?va
Mu?jdricza-Maydt, and Michael Strube. 2012. A
multigraph model for coreference resolution. In
Proceedings of the Shared Task of the 16th Confer-
ence on Computational Natural Language Learning,
Jeju Island, Korea, 12?14 July 2012, pages 100?106.
Ruslan Mitkov. 1998. Robust pronoun resolution with
limited knowledge. In Proceedings of the 17th In-
ternational Conference on Computational Linguis-
tics and 36th Annual Meeting of the Association
for Computational Linguistics, Montre?al, Que?bec,
Canada, 10?14 August 1998, pages 869?875.
Vincent Ng. 2008. Unsupervised models for corefer-
ence resolution. In Proceedings of the 2008 Con-
ference on Empirical Methods in Natural Language
Processing, Waikiki, Honolulu, Hawaii, 25?27 Oc-
tober 2008, pages 640?649.
Cristina Nicolae and Gabriel Nicolae. 2006. BestCut:
A graph algorithm for coreference resolution. In
Proceedings of the 2006 Conference on Empirical
Methods in Natural Language Processing, Sydney,
Australia, 22?23 July 2006, pages 275?283.
Hoifung Poon and Pedro Domingos. 2008. Joint unsu-
pervised coreference resolution with Markov Logic.
In Proceedings of the 2008 Conference on Empirical
Methods in Natural Language Processing, Waikiki,
Honolulu, Hawaii, 25?27 October 2008, pages 650?
659.
87
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,
Martha Palmer, Ralph Weischedel, and Nianwen
Xue. 2011. CoNLL-2011 Shared Task: Modeling
unrestricted coreference in OntoNotes. In Proceed-
ings of the Shared Task of the 15th Conference on
Computational Natural Language Learning, Port-
land, Oreg., 23?24 June 2011, pages 1?27.
Sameer Pradhan, Alessandro Moschitti, and Nianwen
Xue. 2012. CoNLL-2012 Shared Task: Modeling
multilingual unrestricted coreference in OntoNotes.
In Proceedings of the Shared Task of the 16th Con-
ference on Computational Natural Language Learn-
ing, Jeju Island, Korea, 12?14 July 2012, pages 1?
40.
Karthik Raghunathan, Heeyoung Lee, Sudarshan Ran-
garajan, Nathanael Chambers, Mihai Surdeanu, Dan
Jurafsky, and Christopher Manning. 2010. A multi-
pass sieve for coreference resolution. In Proceed-
ings of the 2010 Conference on Empirical Methods
in Natural Language Processing, Cambridge, Mass.,
9?11 October 2010, pages 492?501.
Altaf Rahman and Vincent Ng. 2009. Supervised mod-
els for coreference resolution. In Proceedings of the
2009 Conference on Empirical Methods in Natural
Language Processing, Singapore, 6?7 August 2009,
pages 968?977.
Emili Sapena, Llu??s Padro?, and Jordi Turmo. 2010. A
global relaxation labeling approach to coreference
resolution. In Proceedings of Coling 2010: Poster
Volume, Beijing, China, 23?27 August 2010, pages
1086?1094.
Emili Sapena, Llu??s Padro?, and Jordi Turmo. 2011.
RelaxCor participation in CoNLL shared task on
coreference resolution. In Proceedings of the
Shared Task of the 15th Conference on Computa-
tional Natural Language Learning, Portland, Oreg.,
23?24 June 2011, pages 35?39.
Wee Meng Soon, Hwee Tou Ng, and Daniel
Chung Yong Lim. 2001. A machine learning ap-
proach to coreference resolution of noun phrases.
Computational Linguistics, 27(4):521?544.
Yannick Versley, Simone Paolo Ponzetto, Massimo
Poesio, Vladimir Eidelman, Alan Jern, Jason Smith,
Xiaofeng Yang, and Alessandro Moschitti. 2008.
BART: A modular toolkit for coreference resolu-
tion. In Companion Volume to the Proceedings of
the 46th Annual Meeting of the Association for Com-
putational Linguistics, Columbus, Ohio, 15?20 June
2008, pages 9?12.
Marc Vilain, John Burger, John Aberdeen, Dennis Con-
nolly, and Lynette Hirschman. 1995. A model-
theoretic coreference scoring scheme. In Proceed-
ings of the 6th Message Understanding Conference
(MUC-6), pages 45?52, San Mateo, Cal. Morgan
Kaufmann.
88
Proceedings of the Joint Conference on EMNLP and CoNLL: Shared Task, pages 100?106,
Jeju Island, Korea, July 13, 2012. c?2012 Association for Computational Linguistics
A Multigraph Model for Coreference Resolution
Sebastian Martschat, Jie Cai, Samuel Broscheit, E?va Mu?jdricza-Maydt, Michael Strube
Natural Language Processing Group
Heidelberg Institute for Theoretical Studies gGmbH
Heidelberg, Germany
(sebastian.martschat|jie.cai|michael.strube)@h-its.org
Abstract
This paper presents HITS? coreference reso-
lution system that participated in the CoNLL-
2012 shared task on multilingual unrestricted
coreference resolution. Our system employs a
simple multigraph representation of the rela-
tion between mentions in a document, where
the nodes correspond to mentions and the
edges correspond to relations between the
mentions. Entities are obtained via greedy
clustering. We participated in the closed tasks
for English and Chinese. Our system ranked
second in the English closed task.
1 Introduction
Coreference resolution is the task of determining
which mentions in a text refer to the same entity.
This paper describes HITS? system for the CoNLL-
2012 Shared Task on multilingual unrestricted coref-
erence resolution, where the goal is to build a system
for coreference resolution in an end-to-end multilin-
gual setting (Pradhan et al, 2012). We participated
in the closed tasks for English and Chinese and fo-
cused on English. Our system ranked second in the
English closed task.
Being conceptually similar to and building upon
Cai et al (2011b), our system is based on a directed
multigraph representation of a document. A multi-
graph is a graph where two nodes can be connected
by more than one edge. In our model, nodes rep-
resent mentions and edges are built from relations
between the mentions. The entities to be inferred
correspond to clusters in the multigraph.
Our model allows for directly representing any
kind of relations between pairs of mentions in a
graph structure. Inference over this graph can har-
ness structural properties and the rich set of encoded
relations. In order to serve as a basis for further
work, the components of our system were designed
to work as simple as possible. Therefore our system
relies mostly on local information between pairs of
mentions.
2 Architecture
Our system is implemented on top of the BART
toolkit (Versley et al, 2008). To compute the coref-
erence clusters in a document, we first extract a set
of mentions M = {m1, . . . ,mn} ordered according
to their position in the text (Section 2.1). We then
build a directed multigraph where the set of nodes
is M and edges are induced by relations between
mentions (Section 2.4). The relations we use in our
system are coreference indicators like string match-
ing or alias (Section 3). For every relation R, we
compute a weight wR using the training data (Sec-
tion 2.3). We then assign the weight wR to any edge
that is induced by the relation R. Depending on dis-
tance and connectivity properties of the graph the
weights may change (Section 2.4.1). Given the con-
structed graph with edge weights, we go through the
mentions according to their position in the text and
perform greedy clustering (Section 2.6). For Chi-
nese, we employ spectral clustering (Section 2.5) as
adopted in Cai et al (2011b) before the greedy clus-
tering step to reduce the number of candidate an-
tecedents for a mention. The components of our sys-
tem are described in the following subsections.
100
2.1 Mention Extraction
Noun phrases are extracted from the provided parse
and named entity annotation layers. For embedded
mentions with the same head, we only keep the men-
tion with the largest span.
2.1.1 English
For English we identify eight different mention
types: common noun, proper noun, personal pro-
noun, demonstrative pronoun, possessive pronoun,
coordinated noun phrase, quantifying noun phrase
(some of ..., 17 of ...) and quantified noun phrase
(the armed men in one of the armed men). The head
for a common noun or a quantified noun is com-
puted using the SemanticHeadFinder from the Stan-
ford Parser1. The head for a proper noun starts at
the first token tagged as a noun until a punctuation,
preposition or subclause is encountered. Coordina-
tions have the CC tagged token as head and quanti-
fying noun phrases have the quantifier as head.
In a postprocessing step we filter out adjectival
use of nations and named entities with semantic
class Money, Percent or Cardinal. We discard men-
tions whose head is embedded in another mention?s
head. Pleonastic pronouns are identified and dis-
carded via a modified version of the patterns used
by Lee et al (2011).
2.1.2 Chinese
For Chinese we detect four mention types: com-
mon noun, proper noun, pronoun and coordination.
The head detection for Chinese is provided by the
SunJurafskyChineseHeadFinder from the Standford
Parser, except for proper nouns whose head is set to
the mention?s rightmost token.
The remaining processing is similar to the men-
tion detection for English.
2.2 Preprocessing
We extract the information in the provided an-
notation layers and transform the predicted con-
stituent parse trees into dependency parse trees.
We work with two different dependency represen-
tations, one obtained via the converter implemented
1http://nlp.stanford.edu/software/
lex-parser.shtml
in Stanford?s NLP suite2, the other using LTH?s
constituent-to-dependency conversion tool3. For
pronouns, we determine number and gender using
tables containing a mapping of pronouns to their
gender and number.
2.2.1 English
For English, number and gender for common
nouns are computed via a comparison of head
lemma to head and using the number and gender
data of Bergsma and Lin (2006). Quantified noun
phrases are always plural. We compute semantic
classes via a WordNet (Fellbaum, 1998) lookup.
2.2.2 Chinese
For Chinese, we simply determine number and
gender by searching for the corresponding desig-
nators, since plural mentions mostly end with ?,
while ?? (sir) and ?? (lady) often suggest gen-
der information. To identify demonstrative and defi-
nite noun phrases, we check whether they start with
a definite/demonstrative indicator (e.g. ? (this) or
? (that)). We use lists of named entities extracted
from the training data to determine named entities
and their semantic class in development and testing
data.
2.3 Computing Weights for Relations
We compute weights for relations using simple de-
scriptive statistics on training documents. Since this
is a robust approach to learning weights for the type
of graph model we employ (Cai et al, 2011b; Cai
et al, 2011a), we use only a fraction of the available
training data. We took a random subset consisting of
around 20% for English and 15% for Chinese of the
training data. For every document in this set and ev-
ery relation R, we go through the set M of extracted
mentions and compute for every pair (mi,mj) with
i > j whether R holds for this pair. The weight wR
for R is then the number of coreferent pairs with R
divided by the number of all pairs with R.
2.4 Graph Construction
The set of relations we employ consists of two sub-
sets: negative relations R? which enforce that no
2http://nlp.stanford.edu/software/
stanford-dependencies.shtml
3http://nlp.cs.lth.se/software/treebank_
converter/
101
edge is built between two mentions, and positive re-
lations R+ that induce edges. Again, we go through
M in a left-to-right fashion. If for two mentions mi,
mj with i > j a negative relation R? holds, no edge
between mi and mj can be built. Otherwise we add
an edge from mi to mj for every positive relation
R+ such that R+(mi,mj) is true. The structure ob-
tained by this construction is a directed multigraph.
We handle copula relations similar to Lee et al
(2011): if mi is this and the pair (mi,mj) is in a
copula relation (like This is the World), we remove
mj and replace mj in all edges involving it by mi.
For Chinese, we handle ?role appositives? as intro-
duced by Haghighi and Klein (2009) analogously.
2.4.1 Assigning Weights to Edges
Initially, any edge (mi,mj) induced by the rela-
tion R has the weight wR computed as described
in Section 2.3. If R is a transitive relation, we di-
vide the weight by the number of mentions con-
nected by this relation. This corresponds to the way
edge weights are assigned during the spectral em-
bedding in Cai et al (2011b). If R is a relation sen-
sitive to distance like compatibility between a com-
mon/proper noun and a pronoun, the weight is al-
tered according to the distance between mi and mj .
2.4.2 An Example
We demonstrate the graph construction by a sim-
ple example. Consider a document consisting of the
following three sentences.
Barack Obama and Nicolas Sarkozy met
in Toronto yesterday. They discussed the
financial crisis. Sarkozy left today.
Let us assume that our system identifies Barack
Obama (m1), Nicolas Sarkozy (m2), Barack Obama
and Nicolas Sarkozy (m3), They (m4) and Sarkozy
(m5) as mentions. We consider these mentions and
the relations N Number, P Nprn Prn, P Alias and
P Subject described in Section 3. The graph con-
structed according to the algorithm described in this
section is displayed in Figure 1.
Observe the effect of the negative relation
N Number: while P Nprn Prn holds for the pair
Barack Obama (m1) and They (m4), the mentions
do not agree in number. Hence N Number holds for
this pair and no edge from m4 to m1 can be built.
m2 m5
m3 m4
P Alias
P Nprn Prn
P Subject
Figure 1: An example graph. Nodes represent mentions,
edges are induced by relations between the mentions.
2.5 Spectral Clustering
For Chinese we apply spectral clustering before the
final greedy clustering phase. In order to be able to
apply spectral clustering, we make the graph undi-
rected and merge parallel edges into one edge, sum-
ming up all weights. Due to the way edge weights
are computed, the resulting undirected simple graph
corresponds to the graph Cai et al (2011b) use as
input to the spectral clustering algorithm. Spectral
clustering is now performed as in Cai et al (2011b).
2.6 Greedy Clustering
To describe our clustering algorithm, we use some
additional terminology: if there exists an edge from
m to n we say that m is a parent of n and that n is a
child of m.
In the last step, we go through the mentions from
left to right. Let mi be the mention in focus. For
English, we consider all children of mi as possible
antecedents. For Chinese we restrict the possible an-
tecedents to all children that are in the same cluster
obtained by spectral clustering.
If mi is a pronoun, we determine mj such that
the sum over all weights of edges from mi to mj is
maximized. We then assign mi and mj to the same
entity. In English, if mi is a parent of a noun phrase
m that embeds mj , we instead assign mi and m to
the same entity.
For Chinese, all other noun phrases are assigned
to the same entity as all their children in the cluster
obtained by spectral clustering. For English, we are
more restrictive: definites and demonstratives are as-
signed to the same cluster as their closest (according
to the position of the mentions in the text) child.
Negative relations may also be applied as con-
straints in this phase. Before assigning mi to the
same entity as a set of mentions C, we check for
102
every m ? C and every negative relation R?
that we want to incorporate as a constraint whether
R?(mi,m) holds. If yes, we do not assign mi to the
same entity as the mentions in C.
2.7 Complexity
Our algorithms for weight computation, graph con-
struction and greedy clustering look at all pairs of
mentions in a document and perform simple calcu-
lations, which leads to a time complexity of O
(
n2
)
per document, where n is the number of mentions
in a document. When performing spectral cluster-
ing, this increases to O
(
n3
)
. Since we deal with
at most a few hundred mentions per document, the
cubic running time is not an issue.
3 Relations
In our system relations serve as templates for build-
ing or disallowing edges between mentions. We
distinguish between positive and negative relations:
negative relations disallow edges between mentions,
positive relations build edges between mentions.
Negative relations can also be used as constraints
during clustering, while positive relations may also
be applied as ?weak? relations: in this case, we only
add the induced edge when the two mentions under
consideration are already included in the graph after
considering all the non-weak relations.
Most of the relations presented here were already
used in our system for last year?s shared task (Cai et
al., 2011b). The set of relations was enriched mainly
to resolve pronouns in dialogue and to resolve pro-
nouns that do not carry much information by them-
selves like it and they.
3.1 Negative Relations
(1) N Gender, (2) N Number: Two mentions do
not agree in gender or number.
(3) N SemanticClass: Two mentions do not agree
in semantic class (only the Object, Date and Per-
son top categories derived from WordNet (Fell-
baum, 1998) are used).
(4) N ItDist: The anaphor is it or they and the sen-
tence distance to the antecedent is larger than
one.
(5) N BarePlural: Two mentions that are both bare
plurals.
(6) N Speaker12Prn: Two first person pronouns
or two second person pronouns with different
speakers, or one first person pronoun and one
second person pronoun with the same speaker.
(7) N DSprn: Two first person pronouns in direct
speech assigned to different speakers.
(8) N ContraSubjObj: Two mentions are in the
subject and object positions of the same verb,
and the anaphor is a non-possessive pronoun.
(9) N Mod: Two mentions have the same syntac-
tic heads, and the anaphor has a pre- or post-
modifier which does not occur in the antecedent
and does not contradict the antecedent.
(10) N Embedding: Two mentions where one em-
beds the other, which is not a reflexive or posses-
sive pronoun.
(11) N 2PrnNonSpeech: Two second person pro-
nouns without speaker information and not in di-
rect speech.
3.2 Positive Relations
(12) P StrMatch Npron, (13) P StrMatch Pron:
After discarding stop words, if the strings of
mentions completely match and are not pro-
nouns, the relation P StrMatch Npron holds.
When the matched mentions are pronouns,
P StrMatch Pron holds.
(14) P HeadMatch: If the syntactic heads of men-
tions match.
(15) P Nprn Prn: If the antecedent is not a pro-
noun and the anaphor is a pronoun. This relation
is restricted to a sentence distance of 1.
(16) P Alias: If mentions are aliases of each other
(i.e. proper names with partial match, full names
and acronyms, etc.).
(17) P Speaker12Prn: If the speaker of the second
person pronoun is talking to the speaker of the
first person pronoun. The mentions contain only
first or second person pronouns.
(18) P DSPrn: If one mention is subject of a speak
verb, and the other mention is a first person pro-
noun within the corresponding direct speech.
(19) P ReflPrn: If the anaphor is a reflexive pro-
noun, and the antecedent is the subject of the
sentence.
103
(20) P PossPrn: If the anaphor is a possessive pro-
noun, and the antecedent is the subject of the
sentence or the subclause.
(21) P GPEIsA: If the antecedent is a Named En-
tity of GPE entity type and the anaphor is a def-
inite expression of the same type.
(22) P PossPrnEmbedding: If the anaphor is a
possessive pronoun and is embedded in the an-
tecedent.
(23) P VerbAgree: If the anaphor is a pronoun and
has the same predicate as the antecedent.
(24) P Subject & (25) P Object: If both mentions
are subjects/objects (applies only if the anaphor
is it or they).
(26) P SemClassPrn: If the anaphor is a pronoun,
the antecedent is not a pronoun, and both have
semantic class Person.
For English, we used all relations except for (21) and
(26). Relations (1), (2) and (10) were incorporated
as constraints during greedy clustering. For Chinese,
we used relations (1) ? (6), (12) ? (15), (21) and (26).
(26) was incorporated as a weak relation.
4 Results
We submitted to the closed tasks for English and
Chinese. The results on the English development
set and testing set are displayed in Table 1 and Table
2 respectively. To indicate the progress we achieved
within one year, Table 3 shows the performance of
our system on the CoNLL ?11 development data set
compared to last year?s results (Cai et al, 2011b).
The Overall number is the average of MUC, B3
and CEAF (E), MD is the mention detection score.
Overall, we gained over 5% F1 some of which can
be attributed to improved mention detection.
Metric R P F1
MD 73.96 75.69 74.81
MUC 64.93 68.69 66.76
B3 68.42 75.77 71.91
CEAF (M) 61.23 61.23 61.23
CEAF (E) 49.61 45.60 47.52
BLANC 77.81 80.75 79.19
Overall 62.06
Table 1: Results on the English CoNLL ?12 development
set
Metric R P F1
MD 74.23 76.10 75.15
MUC 65.21 68.83 66.97
B3 66.50 74.69 70.36
CEAF (M) 59.61 59.61 59.61
CEAF (E) 48.64 44.72 46.60
BLANC 73.29 78.94 75.73
Overall 61.31
Table 2: Results on the English CoNLL ?12 testing set
Metric R P F1 2011 F1
MD 70.84 73.08 71.94 66.28
MUC 60.80 65.09 62.87 55.19
B3 68.37 75.89 71.94 68.52
CEAF (M) 60.42 60.42 60.42 54.44
CEAF (E) 50.40 46.11 48.16 43.19
BLANC 75.44 79.26 77.19 72.13
Overall 60.99 55.63
Table 3: Results on the English CoNLL ?11 development
set compared to Cai et al (2011b)
Table 4 and Table 5 display our results on Chinese
development data and testing data respectively.
Metric R P F1
MD 52.45 71.50 60.51
MUC 45.90 67.07 54.50
B3 58.94 84.26 69.36
CEAF (M) 53.60 53.60 53.60
CEAF (E) 50.73 34.24 40.89
BLANC 66.17 83.11 71.45
Overall 54.92
Table 4: Results on the Chinese CoNLL ?12 development
set
Metric R P F1
MD 48.49 74.02 58.60
MUC 42.71 67.80 52.41
B3 55.37 85.24 67.13
CEAF (M) 51.30 51.30 51.30
CEAF (E) 51.81 32.46 39.92
BLANC 63.96 82.81 69.18
Overall 53.15
Table 5: Results on the Chinese CoNLL ?12 testing set
Because none of our team members has knowl-
edge of the Arabic language we did not attempt to
104
run our system on the Arabic datasets and therefore
our official score for this language is considered to
be 0. The combined official score of our submission
is (0.0 + 53.15 + 61.31)/3 = 38.15. In the closed
task our system was the second best performing sys-
tem for English and the eighth best performing sys-
tem for Chinese.
5 Error analysis
We did not attempt to resolve event coreference and
did not incorporate world knowledge which is re-
sponsible for many recall errors our system makes.
Since we use a simple greedy strategy for clus-
tering that goes through the mentions left-to-right,
errors in clustering propagate, which gives rise to
cluster-level inconsistencies. We observed a drop in
performance when using more negative relations as
constraints. A more sophisticated clustering strat-
egy that allows a more refined use of constraints is
needed.
5.1 English
Our detection of copula and appositive relations is
quite inaccurate, which is why we limit the incor-
poration of copulas to cases where the antecedent is
this and left appositives out.
We aim for high precision regarding the usage of
the negative relation N Modifier. This leads to some
loss in recall. For example, our system does not as-
sign the just-completed Paralympics and the 12-day
Paralympics to the same entity. Such cases require a
more involved reasoning scheme to decide whether
the modifiers are actually contradicting each other.
Non-referring pronouns constitute another source
of errors. While we improved detection of pleonas-
tic it compared to last year?s system, a lot of them
are not filtered out. Our system also does not distin-
guish well between generic and non-generic uses of
you and we, which hurts precision.
5.2 Chinese
Since each Chinese character carries its own mean-
ing, there are multiple ways to express the same en-
tity by combining different characters into a word.
Both syntactic heads and modifiers can be replaced
by similar words or by abbreviated versions. From
??? (outside people) to ???? (outside eth-
nic group) the head is replaced, while from ??
? (Diana) to ?? ?? ? ?? (charming Di
Princess) the name is abbreviated.
Modifier replacement is more difficult to cope
with, our system does not recognize that ?? ?
??? (starting-over counting-votes job) and??
?? (verifying-votes job) are coreferent. It is also
not trivial to separate characters from words (e.g. by
separating ? and ?) to resolve such cases, since
it will introduce too much noise as a consequence.
In order to tackle this problem, a smart scheme to
propagate similarities from partial words to the en-
tire mentions and a knowledge base upon which re-
liable similarities can be retrieved are necessary.
In contrast to English there is no strict enforce-
ment of using definite noun phrases when referring
to an antecedent in Chinese. Both ???? (the
talk) and?? (talk) can corefer with the antecedent
??????????? (Clinton?s talk during
Hanoi election). This makes it very difficult to dis-
tinguish generic expressions from referential ones.
In the submitted version of our system, we simply
ignore the nominal anaphors which do not start with
definite articles or demonstratives.
6 Conclusions
In this paper we presented a graph-based model for
coreference resolution. It captures pairwise relations
between mentions via edges induced by relations.
Entities are obtained by graph clustering. Discrim-
inative information can be incorporated as negative
relations or as constraints during clustering.
We described our system?s architecture and the re-
lations it employs, highlighting differences and sim-
ilarities to our system from last year?s shared task.
Designed to work as a basis for further work, our
system works mainly by exploring the relationship
between pairs of mentions. Due to its modular archi-
tecture, our system can be extended by components
taking global information into account, for example
for weight learning or clustering.
We focused on the closed task for English in
which our system achieved competitive perfor-
mance, being ranked second out of 15 participants.
Acknowledgments. This work has been funded
by the Klaus Tschira Foundation, Heidelberg, Ger-
many. The first and the second authors have been
supported by a HITS PhD. scholarship.
105
References
Shane Bergsma and Dekang Lin. 2006. Bootstrapping
path-based pronoun resolution. In Proceedings of the
21st International Conference on Computational Lin-
guistics and 44th Annual Meeting of the Association
for Computational Linguistics, Sydney, Australia, 17?
21 July 2006, pages 33?40.
Jie Cai, E?va Mu?jdricza-Maydt, Yufang Hou, and Michael
Strube. 2011a. Weakly supervised graph-based coref-
erence resolution for clinical data. In Proceedings of
the 5th i2b2 Shared Tasks and Workshop on Chal-
lenges in Natural Language Processing for Clinical
Data, Washington, D.C., 20-21 October 2011. To ap-
pear.
Jie Cai, E?va Mu?jdricza-Maydt, and Michael Strube.
2011b. Unrestricted coreference resolution via global
hypergraph partitioning. In Proceedings of the Shared
Task of the 15th Conference on Computational Natu-
ral Language Learning, Portland, Oreg., 23?24 June
2011, pages 56?60.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge,
Mass.
Aria Haghighi and Dan Klein. 2009. Simple coreference
resolution with rich syntactic and semantic features.
In Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing, Singapore,
6?7 August 2009, pages 1152?1161.
Heeyoung Lee, Yves Peirsman, Angel Chang, Nathanael
Chambers, Mihai Surdeanu, and Dan Jurafsky. 2011.
Stanford?s multi-pass sieve coreference resolution sys-
tem at the CoNLL-2011 shared task. In Proceedings
of the Shared Task of the 15th Conference on Compu-
tational Natural Language Learning, Portland, Oreg.,
23?24 June 2011, pages 28?34.
Sameer Pradhan, Alessandro Moschitti, and Nianwen
Xue. 2012. CoNLL-2012 Shared Task: Modeling
multilingual unrestricted coreference in OntoNotes. In
Proceedings of the Shared Task of the 16th Confer-
ence on Computational Natural Language Learning,
Jeju Island, Korea, 12?14 July 2012. This volume.
Yannick Versley, Simone Paolo Ponzetto, Massimo Poe-
sio, Vladimir Eidelman, Alan Jern, Jason Smith,
Xiaofeng Yang, and Alessandro Moschitti. 2008.
BART: A modular toolkit for coreference resolution.
In Companion Volume to the Proceedings of the 46th
Annual Meeting of the Association for Computational
Linguistics, Columbus, Ohio, 15?20 June 2008, pages
9?12.
106
