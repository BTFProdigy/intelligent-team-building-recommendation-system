Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 288?295,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Supertagged Phrase-Based Statistical Machine Translation
Hany Hassan
School of Computing,
Dublin City University,
Dublin 9, Ireland
hhasan@computing.dcu.ie
Khalil Sima?an
Language and Computation,
University of Amsterdam,
Amsterdam, The Netherlands
simaan@science.uva.nl
Andy Way
School of Computing,
Dublin City University,
Dublin 9, Ireland
away@computing.dcu.ie
Abstract
Until quite recently, extending Phrase-based
Statistical Machine Translation (PBSMT)
with syntactic structure caused system per-
formance to deteriorate. In this work we
show that incorporating lexical syntactic de-
scriptions in the form of supertags can yield
significantly better PBSMT systems. We de-
scribe a novel PBSMT model that integrates
supertags into the target language model
and the target side of the translation model.
Two kinds of supertags are employed: those
from Lexicalized Tree-Adjoining Grammar
and Combinatory Categorial Grammar. De-
spite the differences between these two ap-
proaches, the supertaggers give similar im-
provements. In addition to supertagging, we
also explore the utility of a surface global
grammaticality measure based on combina-
tory operators. We perform various experi-
ments on the Arabic to English NIST 2005
test set addressing issues such as sparseness,
scalability and the utility of system subcom-
ponents. Our best result (0.4688 BLEU)
improves by 6.1% relative to a state-of-the-
art PBSMT model, which compares very
favourably with the leading systems on the
NIST 2005 task.
1 Introduction
Within the field of Machine Translation, by far the
most dominant paradigm is Phrase-based Statistical
Machine Translation (PBSMT) (Koehn et al, 2003;
Tillmann &Xia, 2003). However, unlike in rule- and
example-based MT, it has proven difficult to date to
incorporate linguistic, syntactic knowledge in order
to improve translation quality. Only quite recently
have (Chiang, 2005) and (Marcu et al, 2006) shown
that incorporating some form of syntactic structure
could show improvements over a baseline PBSMT
system. While (Chiang, 2005) avails of structure
which is not linguistically motivated, (Marcu et al,
2006) employ syntactic structure to enrich the en-
tries in the phrase table.
In this paper we explore a novel approach towards
extending a standard PBSMT system with syntactic
descriptions: we inject lexical descriptions into both
the target side of the phrase translation table and the
target language model. Crucially, the kind of lexical
descriptions that we employ are those that are com-
monly devised within lexicon-driven approaches to
linguistic syntax, e.g. Lexicalized Tree-Adjoining
Grammar (Joshi & Schabes, 1992; Bangalore &
Joshi, 1999) and Combinary Categorial Grammar
(Steedman, 2000). In these linguistic approaches, it
is assumed that the grammar consists of a very rich
lexicon and a tiny, impoverished1 set of combina-
tory operators that assemble lexical entries together
into parse-trees. The lexical entries consist of syn-
tactic constructs (?supertags?) that describe informa-
tion such as the POS tag of the word, its subcatego-
rization information and the hierarchy of phrase cat-
egories that the word projects upwards. In this work
we employ the lexical entries but exchange the al-
gebraic combinatory operators with the more robust
1These operators neither carry nor presuppose further lin-
guistic knowledge beyond what the lexicon contains.
288
and efficient supertagging approach: like standard
taggers, supertaggers employ probabilities based on
local context and can be implemented using finite
state technology, e.g. Hidden Markov Models (Ban-
galore & Joshi, 1999).
There are currently two supertagging approaches
available: LTAG-based (Bangalore & Joshi, 1999)
and CCG-based (Clark & Curran, 2004). Both the
LTAG (Chen et al, 2006) and the CCG supertag
sets (Hockenmaier, 2003) were acquired from the
WSJ section of the Penn-II Treebank using hand-
built extraction rules. Here we test both the LTAG
and CCG supertaggers. We interpolate (log-linearly)
the supertagged components (language model and
phrase table) with the components of a standard
PBSMT system. Our experiments on the Arabic?
English NIST 2005 test suite show that each of the
supertagged systems significantly improves over the
baseline PBSMT system. Interestingly, combining
the two taggers together diminishes the benefits of
supertagging seen with the individual LTAG and
CCG systems. In this paper we discuss these and
other empirical issues.
The remainder of the paper is organised as fol-
lows: in section 2 we discuss the related work on en-
riching PBSMT with syntactic structure. In section
3, we describe the baseline PBSMT system which
our work extends. In section 4, we detail our ap-
proach. Section 5 describes the experiments carried
out, together with the results obtained. Section 6
concludes, and provides avenues for further work.
2 Related Work
Until very recently, the experience with adding syn-
tax to PBSMT systems was negative. For example,
(Koehn et al, 2003) demonstrated that adding syn-
tax actually harmed the quality of their SMT system.
Among the first to demonstrate improvement when
adding recursive structure was (Chiang, 2005), who
allows for hierarchical phrase probabilities that han-
dle a range of reordering phenomena in the correct
fashion. Chiang?s derived grammar does not rely on
any linguistic annotations or assumptions, so that the
?syntax? induced is not linguistically motivated.
Coming right up to date, (Marcu et al, 2006)
demonstrate that ?syntactified? target language
phrases can improve translation quality for Chinese?
English. They employ a stochastic, top-down trans-
duction process that assigns a joint probability to
a source sentence and each of its alternative trans-
lations when rewriting the target parse-tree into a
source sentence. The rewriting/transduction process
is driven by ?xRS rules?, each consisting of a pair
of a source phrase and a (possibly only partially)
lexicalized syntactified target phrase. In order to
extract xRS rules, the word-to-word alignment in-
duced from the parallel training corpus is used to
guide heuristic tree ?cutting? criteria.
While the research of (Marcu et al, 2006) has
much in common with the approach proposed here
(such as the syntactified target phrases), there re-
main a number of significant differences. Firstly,
rather than induce millions of xRS rules from par-
allel data, we extract phrase pairs in the standard
way (Och & Ney, 2003) and associate with each
phrase-pair a set of target language syntactic struc-
tures based on supertag sequences. Relative to using
arbitrary parse-chunks, the power of supertags lies
in the fact that they are, syntactically speaking, rich
lexical descriptions. A supertag can be assigned to
every word in a phrase. On the one hand, the cor-
rect sequence of supertags could be assembled to-
gether, using only impoverished combinatory opera-
tors, into a small set of constituents/parses (?almost?
a parse). On the other hand, because supertags are
lexical entries, they facilitate robust syntactic pro-
cessing (using Markov models, for instance) which
does not necessarily aim at building a fully con-
nected graph.
A second major difference with xRS rules is that
our supertag-enriched target phrases need not be
generalized into (xRS or any other) rules that work
with abstract categories. Finally, like POS tagging,
supertagging is more efficient than actual parsing or
tree transduction.
3 Baseline Phrase-Based SMT System
We present the baseline PBSMT model which we
extend with supertags in the next section. Our
baseline PBSMT model uses GIZA++2 to obtain
word-level alignments in both language directions.
The bidirectional word alignment is used to obtain
phrase translation pairs using heuristics presented in
2http://www.fjoch.com/GIZA++.html
289
(Och & Ney, 2003) and (Koehn et al, 2003), and the
Moses decoder was used for phrase extraction and
decoding.3
Let t and s be the target and source language
sentences respectively. Any (target or source) sen-
tence x will consist of two parts: a bag of elements
(words/phrases etc.) and an order over that bag. In
other words, x = ??x, Ox?, where ?x stands for the
bag of phrases that constitute x, andOx for the order
of the phrases as given in x (Ox can be implemented
as a function from a bag of tokens ?x to a set with a
finite number of positions). Hence, we may separate
order from content:
argmax
t
P (t|s) = argmax
t
P (s | t)P (t) (1)
= arg max
??t,Ot?
TM
? ?? ?
P (?s | ?t)
distortion
? ?? ?
P (Os | Ot)
LM
? ?? ?
Pw(t) (2)
Here, Pw(t) is the target language model, P (Os|Ot)
represents the conditional (order) linear distortion
probability, and P (?s|?t) stands for a probabilis-
tic translation model from target language bags of
phrases to source language bags of phrases using a
phrase translation table. As commonly done in PB-
SMT, we interpolate these models log-linearly (us-
ing different ?weights) together with a word penalty
weight which allows for control over the length of
the target sentence t:
arg max
??t,Ot?
P (?s | ?t) P (Os | Ot)
?o
Pw(t)
?lm exp|t|?w
For convenience of notation, the interpolation factor
for the bag of phrases translation model is shown in
formula (3) at the phrase level (but that does not en-
tail any difference). For a bag of phrases ?t consist-
ing of phrases ti, and bag ?s consisting of phrases
si, the phrase translation model is given by:
P (?s | ?t) =
Y
si
ti
P (si|ti)
P (si| ti) = Pph(si|ti)
?t1Pw(si|ti)
?t2Pr(ti|si)
?t3 (3)
where Pph and Pr are the phrase-translation proba-
bility and its reverse probability, and Pw is the lexi-
cal translation probability.
3http://www.statmt.org/moses/
4 Our Approach: Supertagged PBSMT
We extend the baseline model with lexical linguis-
tic representations (supertags) both in the language
model as well as in the phrase translation model. Be-
fore we describe how our model extends the base-
line, we shortly review the supertagging approaches
in Lexicalized Tree-Adjoining Grammar and Com-
binatory Categorial Grammar.
4.1 Supertags: Lexical Syntax
NP
D
The
NP
NP
N
purchase
NP
NP
N
price
S
NP VP
V
includes
NP
NP
N
taxes
Figure 1: An LTAG supertag sequence for the sen-
tence The purchase price includes taxes. The sub-
categorization information is most clearly available
in the verb includes which takes a subject NP to its
left and an object NP to its right.
Modern linguistic theory proposes that a syntactic
parser has access to an extensive lexicon of word-
structure pairs and a small, impoverished set of oper-
ations to manipulate and combine the lexical entries
into parses. Examples of formal instantiations of this
idea include CCG and LTAG. The lexical entries are
syntactic constructs (graphs) that specify informa-
tion such as POS tag, subcategorization/dependency
information and other syntactic constraints at the
level of agreement features. One important way of
portraying such lexical descriptions is via the su-
pertags devised in the LTAG and CCG frameworks
(Bangalore & Joshi, 1999; Clark & Curran, 2004).
A supertag (see Figure 1) represents a complex,
linguistic word category that encodes a syntactic
structure expressing a specific local behaviour of a
word, in terms of the arguments it takes (e.g. sub-
ject, object) and the syntactic environment in which
it appears. In fact, in LTAG a supertag is an elemen-
tary tree and in CCG it is a CCG lexical category.
Both descriptions can be viewed as closely related
functional descriptions.
The term ?supertagging? (Bangalore & Joshi,
1999) refers to tagging the words of a sentence, each
290
with a supertag. When well-formed, an ordered se-
quence of supertags can be viewed as a compact
representation of a small set of constituents/parses
that can be obtained by assembling the supertags
together using the appropriate combinatory opera-
tors (such as substitution and adjunction in LTAG
or function application and combination in CCG).
Akin to POS tagging, the process of supertagging
an input utterance proceeds with statistics that are
based on the probability of a word-supertag pair
given their Markovian or local context (Bangalore
& Joshi, 1999; Clark & Curran, 2004). This is the
main difference with full parsing: supertagging the
input utterance need not result in a fully connected
graph.
The LTAG-based supertagger of (Bangalore &
Joshi, 1999) is a standard HMM tagger and consists
of a (second-order) Markov language model over su-
pertags and a lexical model conditioning the proba-
bility of every word on its own supertag (just like
standard HMM-based POS taggers).
The CCG supertagger (Clark & Curran, 2004) is
based on log-linear probabilities that condition a su-
pertag on features representing its context. The CCG
supertagger does not constitute a language model
nor are the Maximum Entropy estimates directly in-
terpretable as such. In our model we employ the
CCG supertagger to obtain the best sequences of su-
pertags for a corpus of sentences from which we ob-
tain language model statistics. Besides the differ-
ence in probabilities and statistical estimates, these
two supertaggers differ in the way the supertags are
extracted from the Penn Treebank, cf. (Hocken-
maier, 2003; Chen et al, 2006). Both supertaggers
achieve a supertagging accuracy of 90?92%.
Three aspects make supertags attractive in the
context of SMT. Firstly, supertags are rich syntac-
tic constructs that exist for individual words and so
they are easy to integrate into SMT models that can
be based on any level of granularity, be it word-
or phrase-based. Secondly, supertags specify the
local syntactic constraints for a word, which res-
onates well with sequential (finite state) statistical
(e.g. Markov) models. Finally, because supertags
are rich lexical descriptions that represent under-
specification in parsing, it is possible to have some
of the benefits of full parsing without imposing the
strict connectedness requirements that it demands.
4.2 A Supertag-Based SMT model
We employ the aforementioned supertaggers to en-
rich the English side of the parallel training cor-
pus with a single supertag sequence per sentence.
Then we extract phrase-pairs together with the co-
occuring English supertag sequence from this cor-
pus via the same phrase extraction method used in
the baseline model. This way we directly extend
the baseline model described in section 3 with su-
pertags both in the phrase translation table and in
the language model. Next we define the probabilistic
model that accompanies this syntactic enrichment of
the baseline model.
Let ST represent a supertag sequence of the same
length as a target sentence t. Equation (2) changes
as follows:
argmax
t
?
ST
P (s | t, ST )PST (t, ST ) ?
arg max
?t,ST ?
TM w.sup.tags
? ?? ?
P (?s | ?t,ST )
distortion
? ?? ?
P (Os | Ot)
?o
LM w.sup.tags
? ?? ?
PST (t, ST )
word?penalty
? ?? ?
exp|t|?w
The approximations made in this formula are of two
kinds: the standard split into components and the
search for the most likely joint probability of a tar-
get hypothesis and a supertag sequence cooccuring
with the source sentence (a kind of Viterbi approach
to avoid the complex optimization involving the sum
over supertag sequences). The distortion and word
penalty models are the same as those used in the
baseline PBSMT model.
Supertagged Language Model The ?language
model? PST (t, ST ) is a supertagger assigning prob-
abilities to sequences of word?supertag pairs. The
language model is further smoothed by log-linear
interpolation with the baseline language model over
word sequences.
Supertags in Phrase Tables The supertagged
phrase translation probability consists of a combina-
tion of supertagged components analogous to their
counterparts in the baseline model (equation (3)),
i.e. it consists of P (s | t, ST ), its reverse and
a word-level probability. We smooth this proba-
bility by log-linear interpolation with the factored
291
John bought quickly sharesNNP_NN VBD_(S[dcl]\NP)/NP RB|(S\NP)\(S\NP) NNS_N
2 Violations
Figure 2: Example CCG operator violations: V = 2
and L = 3, and so the penalty factor is 1/3.
backoff version P (s | t)P (s | ST ), where we im-
port the baseline phrase table probability and ex-
ploit the probability of a source phrase given the tar-
get supertag sequence. A model in which we omit
P (s | ST ) turns out to be slightly less optimal than
this one.
As in most state-of-the-art PBSMT systems, we
use GIZA++ to obtain word-level alignments in both
language directions. The bidirectional word align-
ment is used to obtain lexical phrase translation pairs
using heuristics presented in (Och & Ney, 2003) and
(Koehn et al, 2003). Given the collected phrase
pairs, we estimate the phrase translation probability
distribution by relative frequency as follows:
P?ph(s|t) =
count(s, t)
?
s count(s, t)
For each extracted lexical phrase pair, we extract the
corresponding supertagged phrase pairs from the su-
pertagged target sequence in the training corpus (cf.
section 5). For each lexical phrase pair, there is
at least one corresponding supertagged phrase pair.
The probability of the supertagged phrase pair is es-
timated by relative frequency as follows:
Pst(s|t, st) =
count(s, t, st)
?
s count(s, t, st)
4.3 LMs with a Grammaticality Factor
The supertags usually encode dependency informa-
tion that could be used to construct an ?almost parse?
with the help of the CCG/LTAG composition oper-
ators. The n-gram language model over supertags
applies a kind of statistical ?compositionality check?
but due to smoothing effects this could mask cru-
cial violations of the compositionality operators of
the grammar formalism (CCG in this case). It is
interesting to observe the effect of integrating into
the language model a penalty imposed when formal
compostion operators are violated. We combine the
n-gram language model with a penalty factor that
measures the number of encountered combinatory
operator violations in a sequence of supertags (cf.
Figure 2). For a supertag sequence of length (L)
which has (V ) operator violations (as measured by
the CCG system), the language model P will be ad-
justed as P? = P ? (1 ? VL ). This is of course no
longer a simple smoothed maximum-likelihood es-
timate nor is it a true probability. Nevertheless, this
mechanism provides a simple, efficient integration
of a global compositionality (grammaticality) mea-
sure into the n-gram language model over supertags.
Decoder The decoder used in this work is Moses,
a log-linear decoder similar to Pharaoh (Koehn,
2004), modified to accommodate supertag phrase
probabilities and supertag language models.
5 Experiments
In this section we present a number of experiments
that demonstrate the effect of lexical syntax on trans-
lation quality. We carried out experiments on the
NIST open domain news translation task from Ara-
bic into English. We performed a number of ex-
periments to examine the effect of supertagging ap-
proaches (CCG or LTAG) with varying data sizes.
Data and Settings The experiments were con-
ducted for Arabic to English translation and tested
on the NIST 2005 evaluation set. The systems were
trained on the LDC Arabic?English parallel corpus;
we use the news part (130K sentences, about 5 mil-
lion words) to train systems with what we call the
small data set, and the news and a large part of
the UN data (2 million sentences, about 50 million
words) for experiments with large data sets.
The n-gram target language model was built us-
ing 250M words from the English GigaWord Cor-
pus using the SRILM toolkit.4 Taking 10% of the
English GigaWord Corpus used for building our tar-
get language model, the supertag-based target lan-
guage models were built from 25M words that were
supertagged. For the LTAG supertags experiments,
we used the LTAG English supertagger5 (Bangalore
4http://www.speech.sri.com/projects/srilm/
5http://www.cis.upenn.edu/?xtag/gramrelease.html
292
& Joshi, 1999) to tag the English part of the parallel
data and the supertag language model data. For the
CCG supertag experiments, we used the CCG su-
pertagger of (Clark & Curran, 2004) and the Edin-
burgh CCG tools6 to tag the English part of the par-
allel corpus as well as the CCG supertag language
model data.
The NIST MT03 test set is used for development,
particularly for optimizing the interpolation weights
using Minimum Error Rate training (Och, 2003).
Baseline System The baseline system is a state-
of-the-art PBSMT system as described in sec-
tion 3. We built two baseline systems with two
different-sized training sets: ?Base-SMALL? (5 mil-
lion words) and ?Base-LARGE? (50 million words)
as described above. Both systems use a trigram lan-
guage model built using 250 million words from
the English GigaWord Corpus. Table 1 presents the
BLEU scores (Papineni et al, 2002) of both systems
on the NIST 2005 MT Evaluation test set.
System BLEU Score
Base-SMALL 0.4008
Base-LARGE 0.4418
Table 1: Baseline systems? BLEU scores
5.1 Baseline vs. Supertags on Small Data Sets
We compared the translation quality of the baseline
systems with the LTAG and CCG supertags systems
(LTAG-SMALL and CCG-SMALL). The results are
System BLEU Score
Base-SMALL 0.4008
LTAG-SMALL 0.4205
CCG-SMALL 0.4174
Table 2: LTAG and CCG systems on small data
given in Table 2. All systems were trained on the
same parallel data. The LTAG supertag-based sys-
tem outperforms the baseline by 1.97 BLEU points
absolute (or 4.9% relative), while the CCG supertag-
based system scores 1.66 BLEU points over the
6http://groups.inf.ed.ac.uk/ccg/software.html
baseline (4.1% relative). These significant improve-
ments indicate that the rich information in supertags
helps select better translation candidates.
POS Tags vs. Supertags A supertag is a complex
tag that localizes the dependency and the syntax in-
formation from the context, whereas a normal POS
tag just describes the general syntactic category of
the word without further constraints. In this experi-
ment we compared the effect of using supertags and
POS tags on translation quality. As can be seen
System BLEU Score
Base-SMALL 0.4008
POS-SMALL 0.4073
LTAG-SMALL .0.4205
Table 3: Comparing the effect of supertags and POS
tags
in Table 3, while the POS tags help (0.65 BLEU
points, or 1.7% relative increase over the baseline),
they clearly underperform compared to the supertag
model (by 3.2%).
The Usefulness of a Supertagged LM In these
experiments we study the effect of the two added
feature (cost) functions: supertagged translation and
language models. We compare the baseline system
to the supertags system with the supertag phrase-
table probability but without the supertag LM. Ta-
ble 4 lists the baseline system (Base-SMALL), the
LTAG system without supertagged language model
(LTAG-TM-ONLY) and the LTAG-SMALL sys-
tem with both supertagged translation and language
models. The results presented in Table 4 indi-
System BLEU Score
Base-SMALL 0.4008
LTAG-TM-ONLY 0.4146
LTAG-SMALL .0.4205
Table 4: The effect of supertagged components
cate that the improvement is a shared contribution
between the supertagged translation and language
models: adding the LTAG TM improves BLEU
score by 1.38 points (3.4% relative) over the base-
line, with the LTAG LM improving BLEU score by
293
a further 0.59 points (a further 1.4% increase).
5.2 Scalability: Larger Training Corpora
Outperforming a PBSMT system on small amounts
of training data is less impressive than doing so on
really large sets. The issue here is scalability as well
as whether the PBSMT system is able to bridge the
performance gap with the supertagged system when
reasonably large sizes of training data are used. To
this end, we trained the systems on 2 million sen-
tences of parallel data, deploying LTAG supertags
and CCG supertags. Table 5 presents the compari-
son between these systems and the baseline trained
on the same data. The LTAG system improves by
1.17 BLEU points (2.6% relative), but the CCG sys-
tem gives an even larger increase: 1.91 BLEU points
(4.3% relative). While this is slightly lower than
the 4.9% relative improvement with the smaller data
sets, the sustained increase is probably due to ob-
serving more data with different supertag contexts,
which enables the model to select better target lan-
guage phrases.
System BLEU Score
Base-LARGE 0.4418
LTAG-LARGE 0.4535
CCG-LARGE 0.4609
Table 5: The effect of more training data
Adding a grammaticality factor As described in
section 4.3, we integrate an impoverished grammat-
icality factor based on two standard CCG combi-
nation operations, namely Forward and Backward
Application. Table 6 compares the results of the
baseline, the CCG with an n-gram LM-only system
(CCG-LARGE) and CCG-LARGE with this ?gram-
maticalized? LM system (CCG-LARGE-GRAM).
We see that bringing the grammaticality tests to
bear onto the supertagged system gives a further im-
provement of 0.79 BLEU points, a 1.7% relative
increase, culminating in an overall increase of 2.7
BLEU points, or a 6.1% relative improvement over
the baseline system.
5.3 Discussion
A natural question to ask is whether LTAG and CCG
supertags are playing similar (overlapping, or con-
System BLEU Score
Base-LARGE 0.4418
CCG-LARGE 0.4609
CCG-LARGE-GRAM 0.4688
Table 6: Comparing the effect of CCG-GRAM
flicting) roles in practice. Using an oracle to choose
the best output of the two systems gives a BLEU
score of 0.441, indicating that the combination pro-
vides significant room for improvement (cf. Ta-
ble 2). However, our efforts to build a system that
benefits from the combination using a simple log-
linear combination of the two models did not give
any significant performance change relative to the
baseline CCG system. Obviously, more informed
ways of combining the two could result in better per-
formance than a simple log-linear interpolation of
the components.
Figure 3 shows some example system output.
While the baseline system omits the verb giving ?the
authorities that it had...?, both the LTAG and CCG
found a formulation ?authorities reported that? with
a closer meaning to the reference translation ?The
authorities said that?. Omitting verbs turns out to
be a problem for the baseline system when trans-
lating the notorious verbless Arabic sentences (see
Figure 4). The supertagged systems have a more
grammatically strict language model than a standard
word-level Markov model, thereby exhibiting a pref-
erence (in the CCG system especially) for the inser-
tion of a verb with a similar meaning to that con-
tained in the reference sentence.
6 Conclusions
SMT practitioners have on the whole found it dif-
ficult to integrate syntax into their systems. In this
work, we have presented a novel model of PBSMT
which integrates supertags into the target language
model and the target side of the translation model.
Using LTAG supertags gives the best improve-
ment over a state-of-the-art PBSMT system for a
smaller data set, while CCG supertags work best on
a large 2 million-sentence pair training set. Adding
grammaticality factors based on algebraic composi-
tional operators gives the best result, namely 0.4688
BLEU, or a 6.1% relative increase over the baseline.
294
Reference: The authorities said he was allowed to contact family members by phone from the armored vehicle he was in.
Baseline: the authorities that it had allowed him to communicate by phone with his family of the armored car where
LTAG: authorities reported that it had allowed him to contact by telephone with his family of armored car where
CCG: authorities reported that it had enabled him to communicate by phone his family members of the armored car where
Figure 3: Sample output from different systems
Source: wmn AlmErwf An Al$Eb AlSyny mHb llslAm . Ref: It is well known that the Chinese people are peace loving .
Baseline: It is known that the Chinese people a peace-loving .
LTAG: It is known that the Chinese people a peace loving . CCG: It is known that the Chinese people are peace loving .
Figure 4: Verbless Arabic sentence and sample output from different systems
This result compares favourably with the best sys-
tems on the NIST 2005 Arabic?English task. We
expect more work on system integration to improve
results still further, and anticipate that similar in-
creases are to be seen for other language pairs.
Acknowledgements
We would like to thank Srinivas Bangalore and
the anonymous reviewers for useful comments on
earlier versions of this paper. This work is par-
tially funded by Science Foundation Ireland Princi-
pal Investigator Award 05/IN/1732, and Netherlands
Organization for Scientific Research (NWO) VIDI
Award.
References
S. Bangalore and A. Joshi, ?Supertagging: An Ap-
proach to Almost Parsing?, Computational Linguistics
25(2):237?265, 1999.
J. Chen, S. Bangalore, and K. Vijay-Shanker, ?Au-
tomated extraction of tree-adjoining grammars
from treebanks?. Natural Language Engineering,
12(3):251?299, 2006.
D. Chiang, ?A Hierarchical Phrase-Based Model for Sta-
tistical Machine Translation?, in Proceedings of ACL
2005, Ann Arbor, MI., pp.263?270, 2005.
S. Clark and J. Curran, ?The Importance of Supertagging
for Wide-Coverage CCG Parsing?, in Proceedings of
COLING-04, Geneva, Switzerland, pp.282?288, 2004.
J. Hockenmaier, Data and Models for Statistical Parsing
with Combinatory Categorial Grammar, PhD thesis,
University of Edinburgh, UK, 2003.
A. Joshi and Y. Schabes, ?Tree Adjoining Grammars and
Lexicalized Grammars? in M. Nivat and A. Podelski
(eds.) Tree Automata and Languages, Amsterdam, The
Netherlands: North-Holland, pp.409?431, 1992.
P. Koehn, ?Pharaoh: A Beam Search Decoder for phrase-
based Statistical Machine TranslationModels?, in Pro-
ceedings of AMTA-04, Berlin/Heidelberg, Germany:
Springer Verlag, pp.115?124, 2004.
P. Koehn, F. Och, and D. Marcu, ?Statistical Phrase-
Based Translation?, in Proceedings of HLT-NAACL
2003, Edmonton, Canada, pp.127?133, 2003.
D. Marcu, W. Wang, A. Echihabi and K. Knight, ?SPMT:
Statistical Machine Translation with Syntactified Tar-
get Language Phrases?, in Proceedings of EMNLP,
Sydney, Australia, pp.44?52, 2006.
D. Marcu andW.Wong, ?A Phrase-Based, Joint Probabil-
ity Model for Statistical Machine Translation?, in Pro-
ceedings of EMNLP, Philadelphia, PA., pp.133?139,
2002.
F. Och, ?Minimum Error Rate Training in Statistical Ma-
chine Translation?, in Proceedings of ACL 2003, Sap-
poro, Japan, pp.160?167, 2003.
F. Och and H. Ney, ?A Systematic Comparison of Var-
ious Statistical Alignment Models?, Computational
Linguistics 29:19?51, 2003.
K. Papineni, S. Roukos, T. Ward and W-J. Zhu, ?BLEU:
A Method for Automatic Evaluation of Machine
Translation?, in Proceedings of ACL 2002, Philadel-
phia, PA., pp.311?318, 2002.
L. Rabiner, ?A Tutorial on Hidden Markov Models and
Selected Applications in Speech Recognition?, in A.
Waibel & F-K. Lee (eds.) Readings in Speech Recog-
nition, San Mateo, CA.: Morgan Kaufmann, pp.267?
296, 1990.
M. Steedman, The Syntactic Process. Cambridge, MA:
The MIT Press, 2000.
C. Tillmann and F. Xia, ?A Phrase-based Unigram Model
for Statistical Machine Translation?, in Proceedings of
HLT-NAACL 2003, Edmonton, Canada. pp.106?108,
2003.
295
TIPS: A Translingual Information Processing System
Y. Al-Onaizan, R. Florian, M. Franz, H. Hassan, Y. S. Lee, S. McCarley, K.
Papineni, S. Roukos, J. Sorensen, C. Tillmann, T. Ward, F. Xia
IBM T. J. Watson Research Center
Yorktown Heights
Abstract
Searching online information is
increasingly a daily activity for many
people. The multilinguality of online
content is also increasing (e.g. the
proportion of English web users, which
has been decreasing as a fraction the
increasing population of web users, dipped
below 50% in the summer of 2001). To
improve the ability of an English speaker
to search mutlilingual content, we built a
system that supports cross-lingual search
of an Arabic newswire collection and
provides on demand translation of Arabic
web pages into English. The cross-lingual
search engine supports a fast search
capability (sub-second response for typical
queries) and achieves state-of-the-art
performance in the high precision region
of the result list. The on demand statistical
machine translation uses the Direct
Translation model along with a novel
statistical Arabic Morphological Analyzer
to yield state-of-the-art translation quality.
The on demand SMT uses an efficient
dynamic programming decoder that
achieves reasonable speed for translating
web documents.
Overview
Morphologically rich languages like Arabic
(Beesley, K. 1996) present significant challenges
to many natural language processing applications
as the one described above because a word often
conveys complex meanings decomposable into
several morphemes (i.e. prefix, stem, suffix). By
segmenting words into morphemes, we can
improve the performance of natural language
systems including machine translation (Brown et
al. 1993) and information retrieval (Franz, M.
and McCarley, S. 2002). In this paper, we
present a cross-lingual English-Arabic search
engine combined with an on demand Arabic-
English statistical machine translation system
that relies on source language analysis for both
improved search and translation. We developed
novel statistical learning algorithms for
performing Arabic word segmentation (Lee, Y.
et al2003) into morphemes and morphological
source language (Arabic) analysis (Lee, Y. et al
2003b). These components improve both mono-
lingual (Arabic) search and cross-lingual
(English-Arabic) search and machine
translation. In addition, the system supports
either document translation or convolutional
models for cross-lingual search (Franz, M. and
McCarley, S. 2002).
The overall demonstration has the following
major components:
1. Mono-lingual search: uses Arabic word
segmentation and an okapi-like search
engine for document ranking.
2. Cross-lingual search: uses Arabic word
segmentation and morphological
analysis along with a statistical
morpheme translation matrix in a
convolutional model for document
ranking. The search can also use
document translation into English to
rank the Arabic documents. Both
approaches achieve similar precision in
the high precision region of retrieval.
The English query is also
morphologically analyzed to improve
performance.
3. OnDemand statistical machine
translation: this component uses both
analysis components along with a direct
channel translation model with a fast
dynamic programming decoder
(Tillmann, C. 2003). This system
                                                               Edmonton, May-June 2003
                                                              Demonstrations , pp. 1-2
                                                         Proceedings of HLT-NAACL 2003
achieves state-of-the-art Arabic-English
translation quality.
4. Arabic named entity detection and
translation: we have 31 categories of
Named Entities (Person, Organization,
etc.) that we detect and highlight in
Arabic text and provide the translation
of these entities into English. The
highlighted named entities help the user
to quickly assess the relevance of a
document.
All of the above functionality is available
through a web browser. We indexed the Arabic
AFP corpus about 330k documents for the
demonstration. The resulting search engine
supports sub-second query response. We also
provide an html detagging capability that allows
the translation of Arabic web pages while trying
to preserve the original layout as much as
possible in the on demand SMT component. The
Arabic Name Entity Tagger is currently run as an
offline process but we expect to have it online by
the demonstration time. We aslo include two
screen shots of the demonstration system.
Acknowledgments
This work was partially supported by the
Defense Advanced Research Projects Agency
and monitored by SPAWAR under contract No.
N66001-99-2-8916. The views and findings
contained in this material are those of the authors
and do not necessarily reflect the position of
policy of the Government and no official
endorsement should be inferred.
References
Beesley, K. 1996. Arabic Finite-State
Morphological Analysis and Generation.
Proceedings of COLING-96, pages 89? 94.
Brown, P., Della Pietra, S., Della Pietra, V., and
Mercer, R. 1993. The mathematics of statistical
machine translation: Parameter Estimation.
Computational Linguistics, 19(2): 263?311.
Franz, M. and McCarley, S. 2002. Arabic
Information Retrieval at IBM. Proceedings
of TREC 2002, pages 402?405.
Lee, Y., Papineni, K., Roukos, S.,
Emam, O., and Hassan, H. 2003. Language
Model Based Arabic Word Segmentation.
Submitted for publication.
Lee, Y., Papineni, K., Roukos, S., Emam,
O., and Hassan, H. 2003b. Automatic
Induction of Morphological Analysis for
Statistical Machine Translation. Manuscript in
preparation.
Tillmann, C., 2003. Word Reordering and a
DP Beam Search Algorithm for Statistical
Machine Translation. Computational
Linguistics, 29(1): 97-133.
A Statistical Model for Multilingual Entity Detection and Tracking
R. Florian, H. Hassan   , A. Ittycheriah, H. Jing
N. Kambhatla, X. Luo, N. Nicolov, and S. Roukos
I.B.M. T.J. Watson Research Center
Yorktown Heights, NY 10598
{raduf,abei,hjing,nanda,xiaoluo, nicolas,roukos}@us.ibm.com

hanyh@eg.ibm.com
Abstract
Entity detection and tracking is a relatively new
addition to the repertoire of natural language
tasks. In this paper, we present a statistical
language-independent framework for identify-
ing and tracking named, nominal and pronom-
inal references to entities within unrestricted
text documents, and chaining them into clusters
corresponding to each logical entity present in
the text. Both the mention detection model
and the novel entity tracking model can use
arbitrary feature types, being able to integrate
a wide array of lexical, syntactic and seman-
tic features. In addition, the mention detec-
tion model crucially uses feature streams de-
rived from different named entity classifiers.
The proposed framework is evaluated with sev-
eral experiments run in Arabic, Chinese and
English texts; a system based on the approach
described here and submitted to the latest Au-
tomatic Content Extraction (ACE) evaluation
achieved top-tier results in all three evaluation
languages.
1 Introduction
Detecting entities, whether named, nominal or pronom-
inal, in unrestricted text is a crucial step toward under-
standing the text, as it identifies the important concep-
tual objects in a discourse. It is also a necessary step for
identifying the relations present in the text and populating
a knowledge database. This task has applications in in-
formation extraction and summarization, information re-
trieval (one can get al hits for Washington/person and not
the ones for Washington/state or Washington/city), data
mining and question answering.
The Entity Detection and Tracking task (EDT hence-
forth) has close ties to the named entity recognition
(NER) and coreference resolution tasks, which have been
the focus of attention of much investigation in the recent
past (Bikel et al, 1997; Borthwick et al, 1998; Mikheev
et al, 1999; Miller et al, 1998; Aberdeen et al, 1995;
Ng and Cardie, 2002; Soon et al, 2001), and have been
at the center of several evaluations: MUC-6, MUC-7,
CoNLL?02 and CoNLL?03 shared tasks. Usually, in com-
putational linguistic literature, a named entity represents
an instance of a name, either a location, a person, an or-
ganization, and the NER task consists of identifying each
individual occurrence of such an entity. We will instead
adopt the nomenclature of the Automatic Content Extrac-
tion program1 (NIST, 2003a): we will call the instances
of textual references to objects or abstractions mentions,
which can be either named (e.g. John Mayor), nominal
(e.g. the president) or pronominal (e.g. she, it). An entity
consists of all the mentions (of any level) which refer to
one conceptual entity. For instance, in the sentence
President John Smith said he has no comments.
there are two mentions: John Smith and he (in the order
of appearance, their levels are named and pronominal),
but one entity, formed by the set {John Smith, he}.
In this paper, we present a general statistical frame-
work for entity detection and tracking in unrestricted text.
The framework is not language specific, as proved by ap-
plying it to three radically different languages: Arabic,
Chinese and English. We separate the EDT task into a
mention detection part ? the task of finding all mentions
in the text ? and an entity tracking part ? the task of com-
bining the detected mentions into groups of references to
the same object.
The work presented here is motivated by the ACE eval-
uation framework, which has the more general goal of
building multilingual systems which detect not only enti-
ties, but also relations among them and, more recently,
events in which they participate. The EDT task is ar-
guably harder than traditional named entity recognition,
because of the additional complexity involved in extract-
ing non-named mentions (nominals and pronouns) and
the requirement of grouping mentions into entities.
We present and evaluate empirically statistical mod-
els for both mention detection and entity tracking prob-
lems. For mention detection we use approaches based on
Maximum Entropy (MaxEnt henceforth) (Berger et al,
1996) and Robust Risk Minimization (RRM henceforth)
1For a description of the ACE program see
http://www.nist.gov/speech/tests/ace/.
(Zhang et al, 2002). The task is transformed into a se-
quence classification problem. We investigate a wide ar-
ray of lexical, syntactic and semantic features to perform
the mention detection and classification task including,
for all three languages, features based on pre-existing sta-
tistical semantic taggers, even though these taggers have
been trained on different corpora and use different seman-
tic categories. Moreover, the presented approach implic-
itly learns the correlation between these different seman-
tic types and the desired output types.
We propose a novel MaxEnt-based model for predict-
ing whether a mention should or should not be linked to
an existing entity, and show how this model can be used
to build entity chains. The effectiveness of the approach
is tested by applying it on data from the above mentioned
languages ? Arabic, Chinese, English.
The framework presented in this paper is language-
universal ? the classification method does not make any
assumption about the type of input. Most of the fea-
ture types are shared across the languages, but there are a
small number of useful feature types which are language-
specific, especially for the mention detection task.
The paper is organized as follows: Section 2 describes
the algorithms and feature types used for mention detec-
tion. Section 3 presents our approach to entity tracking.
Section 4 describes the experimental framework and the
systems? results for Arabic, Chinese and English on the
data from the latest ACE evaluation (September 2003), an
investigation of the effect of using different feature types,
as well as a discussion of the results.
2 Mention Detection
The mention detection system identifies the named, nom-
inal and pronominal mentions introduced in the previous
section. Similarly to classical NLP tasks such as base
noun phrase chunking (Ramshaw and Marcus, 1994), text
chunking (Ramshaw and Marcus, 1995) or named entity
recognition (Tjong Kim Sang, 2002), we formulate the
mention detection problem as a classification problem,
by assigning to each token in the text a label, indicating
whether it starts a specific mention, is inside a specific
mention, or is outside any mentions.
2.1 The Statistical Classifiers
Good performance in many natural language process-
ing tasks, such as part-of-speech tagging, shallow pars-
ing and named entity recognition, has been shown to de-
pend heavily on integrating many sources of information
(Zhang et al, 2002; Jing et al, 2003; Ittycheriah et al,
2003). Given the stated focus of integrating many feature
types, we are interested in algorithms that can easily in-
tegrate and make effective use of diverse input types. We
selected two methods which satisfy these criteria: a linear
classifier ? the Robust Risk Minimization classifier ? and
a log-linear classifier ? the Maximum Entropy classifier.
Both methods can integrate arbitrary types of informa-
tion and make a classification decision by aggregating all
information available for a given classification.
Before formally describing the methods2, we introduce
some notations: let
 	



be the set of pre-
dicted classes,  be the example space and 

be the feature space. Each example Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1182?1191,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
A Syntactified Direct Translation Model with Linear-time Decoding
Hany Hassan
Cairo TDC
IBM
Cairo, Egypt
hanyh@eg.ibm.com
Khalil Sima?an
Language and Computation
University of Amsterdam
Amsterdam, The Netherlands
k.simaan@uva.nl
Andy Way
School of Computing
Dublin City University
Dublin, Ireland
away@computing.dcu.ie
Abstract
Recent syntactic extensions of statisti-
cal translation models work with a syn-
chronous context-free or tree-substitution
grammar extracted from an automatically
parsed parallel corpus. The decoders ac-
companying these extensions typically ex-
ceed quadratic time complexity.
This paper extends the Direct Transla-
tion Model 2 (DTM2) with syntax while
maintaining linear-time decoding. We
employ a linear-time parsing algorithm
based on an eager, incremental interpre-
tation of Combinatory Categorial Gram-
mar (CCG). As every input word is pro-
cessed, the local parsing decisions resolve
ambiguity eagerly, by selecting a single
supertag?operator pair for extending the
dependency parse incrementally. Along-
side translation features extracted from
the derived parse tree, we explore syn-
tactic features extracted from the incre-
mental derivation process. Our empiri-
cal experiments show that our model sig-
nificantly outperforms the state-of-the art
DTM2 system.
1 Introduction
Syntactic structure is gradually showing itself to
constitute a promising enrichment of state-of-the-
art Statistical Machine Translation (SMT) models.
However, it would appear that the decoding algo-
rithms are bearing the brunt of this improvement in
terms of time and space complexity. Most recent
extensions work with a synchronous context-free
or tree-substitution grammar extracted from an au-
tomatically parsed parallel corpus. While attrac-
tive in many ways, the decoders that are needed
for these types of grammars usually have time
and space complexities that are far beyond linear.
Leaving pruning aside, there is a genuine ques-
tion as to whether syntactic structure necessarily
implies more complex decoding algorithms. This
paper shows that this need not necessarily be the
case.
In this paper we extend the Direct Translation
Model (DTM2) (Ittycheriah and Roukos, 2007)
with target language syntax while maintaining
linear-time decoding. With this extension we
make three novel contributions to SMT. Our first
contribution is to define a linear-time syntactic
parser that works as incrementally as standard
SMT decoders (Tillmann and Ney, 2003; Koehn,
2004a). At every word position in the target lan-
guage string, this parser spans at most a single
parse-state to augment the translation states in
the decoder. The parse state summarizes previ-
ous parsing decisions and imposes constraints on
the set of valid future extensions such that a well-
formed sequence of parse states unambiguously
defines a dependency structure. This approach
is based on an incremental interpretation of the
mechanisms of Combinatory Categorial Grammar
(CCG) (Steedman, 2000).
Our second contribution lies in extending the
DMT2 model with a novel set of syntactically-
oriented feature functions. Crucially, these feature
functions concern the derived (partial) dependency
structure as well as local aspects of the derivation
process, including such information as the CCG
lexical categories (supertag), the CCG operators
and the intermediate parse states. This accom-
plishment is interesting both from a linguistic and
technical point of view.
Our third contribution is the extension of the
standard phrase-based decoder with the syntactic
structure and definition of new grammar-specific
pruning techniques that control the size of the
search space. Interestingly, because it is eager,
the incremental parser used in this work is hard
pushed to perform at a parsing level close to state-
1182
of-the-art cubic-time parsers. Nevertheless, the
parsing information it provides allows for signif-
icant improvement in translation quality.
We test the new model, called the Dependency-
based Direct Translation Model (DDTM), on stan-
dard Arabic?English translation tasks used in the
community, including LDC and GALE data. We
show that our DDTM system provides significant
improvements in BLEU (Papineni et al, 2002) and
TER (Snover et al, 2006) scores over the already
extremely competitive DTM2 system. We also
provide results of manual, qualitative analysis of
the system output to provide insight into the quan-
titative results.
This paper is organized as follows. Section 2
reviews the related work. Section 3 discusses the
DTM2 baseline model. Section 4 presents the gen-
eral workings of the incremental CCG parser lay-
ing the foundations for integrating it into DTM2.
Section 5 details our own DDTM, the dependency-
based extension of the DTM2 model. Section 6
reports on extensive experiments and their results.
Section 7 provides translation output to shed fur-
ther detailed insight into the characteristics of the
systems. Finally, Section 8 concludes, and dis-
cusses future work.
2 Related Work
In (Marcu et al, 2006), it is demonstrated that
?syntactified? target language phrases can im-
prove translation quality for Chinese?English. A
stochastic, top-down transduction process is em-
ployed that assigns a joint probability to a source
sentence and each of its alternative syntactified
translations; this is done by specifying a rewrit-
ing process of the target parse-tree into a source
sentence.
Likewise, the model in (Zollmann and Venu-
gopal, 2006) extends (Chiang, 2005) by augment-
ing the hierarchical phrases with syntactic cate-
gories derived from parsing the target side of a
parallel corpus. They use an existing parser to
parse the target side of the parallel corpus in or-
der to extract a syntactically motivated, bilingual
synchronous grammar as in (Chiang, 2005).
The above-mentioned approaches for incor-
porating syntax into Phrase-based SMT (Marcu
et al, 2006; Zollmann and Venugopal, 2006)
share common drawbacks. Firstly, they are
based on syntactic phrase-structure parse trees
incorporated into a Synchronous CFG or Tree-
Substitution Grammar, which makes for a diffi-
cult match with non-constituent phrases that are
common within Phrase-based SMT. These ap-
proaches usually resort to ad hoc solutions to
enrich the non-constituent phrases with syntactic
structures. Secondly, they deploy chart-based de-
coders with a high computational cost compared
with the phrase-based beam search decoders, e.g.,
(Tillmann and Ney, 2003; Koehn, 2004a). Thirdly,
due to the large parse space, some of the pro-
posed approaches are forced to employ small lan-
guage models compared to what is usually used
in phrase-based systems. To circumvent these
computational limitations, various pruning tech-
niques are usually needed, e.g., (Huang and Chi-
ang, 2007).
Other recent approaches, e.g., (Birch et al,
2007; Hassan et al, 2007; Hassan et al, 2008a)
incorporate a linear-time supertagger into SMT to
take the role of a syntactic language model along-
side the standard language model. While these ap-
proaches share with our work the use of lexical-
ized grammars, they never seek to build a full de-
pendency tree or employ syntactic features in or-
der to directly influence the reordering probabili-
ties in the decoder. In the current work, we ex-
pand our previous work in (Hassan et al, 2007;
Hassan et al, 2008a) to introduce the capabilities
of building a full dependency structure and em-
ploying syntactic features to influence the decod-
ing process.
Recently, (Shen et al, 2008) introduced an ap-
proach for incorporating a dependency-based lan-
guage model into SMT. They proposed to extract
String-to-Dependency trees from the parallel cor-
pus. As the dependency trees are not constituents
by nature, they handle non-constituent phrases as
well. While this work is in the same general
direction as our work, namely aiming at incor-
porating dependency parsing into SMT, there re-
main three major differences. Firstly, (Shen et al,
2008) resorted to heuristics to extract the String-
to-Dependency trees, whereas our approach em-
ploys the well formalized CCG grammatical the-
ory. Secondly, their decoder works bottom-up
and uses a chart parser with a limited language
model capability (3-grams), while we build on the
efficient, linear-time decoder commonly used in
phrase-based SMT. Thirdly, (Shen et al, 2008)
deploys the dependency language model to aug-
ment the lexical language model probability be-
1183
tween two head words but never seek a full de-
pendency graph. In contrast, our approach inte-
grates an incremental parsing capability, that pro-
duces the partial dependency structures incremen-
tally while decoding, and thus provides for better
guidance for the search of the decoder for more
grammatical output. To the best of our knowledge,
our approach is the first to incorporate incremental
dependency parsing capabilities into SMT while
maintaining the linear-time and -space decoder.
3 Baseline: Direct Translation Model 2
The Direct Translation Model (DTM) (Papineni
et al, 1997) employs the a posteriori conditional
distribution P (T |S) of a target sentence T given
a source sentence S, instead of the common in-
version into P (S|T ) based on the source chan-
nel approach (Brown et al, 1990). DTM2, in-
troduced in (Ittycheriah and Roukos, 2007), ex-
presses the phrase-based translation task in a uni-
fied log-linear probabilistic framework consisting
of three components: (i) a prior conditional dis-
tribution P
0
(.|S), (ii) a number of feature func-
tions ?
i
() that capture the translation and language
model effects, and (iii) the weights of the features
?
i
that are estimated under MaxEnt (Berger et al,
1996), as in (1):
P (T |S) =
P
0
(T, J |S)
Z
exp
?
i
?
i
?
i
(T, J, S) (1)
Here J is the skip reordering factor for the phrase
pair captured by ?
i
() and represents the jump from
the previous source word, and Z is the per source
sentence normalization term. The prior probabil-
ity P
0
is the prior distribution for the phrase prob-
ability which is estimated using the phrase nor-
malized counts commonly used in conventional
Phrase?based SMT systems, e.g., (Koehn et al,
2003).
DTM2 differs from other Phrase?based SMT
models in that it extracts from a word-aligned par-
allel corpus only a non-redundant set of minimal
phrases in the sense that no two phrases overlap
with each other.
Baseline DTM2 Features: The baseline em-
ploys the following five types of features (beside
the language model):
? Lexical Micro Features examining source
and target words of the phrases,
? Lexical Context Features encoding the
source and target phrase context (i.e. previ-
ous and next source and previous target),
? Source Morphological Features encoding
morphological and segmentation characteris-
tics of source words.
? Part-of-Speech Features encoding source and
target POS tags as well as the POS tags of the
surrounding contexts of phrases.
The DTM2 approach based on MaxEnt provides
a flexible framework for incorporating other avail-
able feature types as we demonstrate below.
DTM2 Decoder: The decoder for the baseline is
a beam search decoder similar to decoders used in
standard phrase-based log-linear systems such as
(Tillmann and Ney, 2003) and (Koehn, 2004a).
The main difference between the DTM2 decoder
and the standard Phrase?based SMT decoders is
that DTM2 deploys Maximum Entropy probabilis-
tic models to obtain the translation costs and var-
ious feature costs by deploying the features de-
scribed above in a discriminative MaxEnt fashion.
In the rest of this paper we adopt the DTM2 for-
malization of translation as a discriminative task,
and we describe the CCG-based incremental de-
pendency parser that we use for extending the
DTM2 decoder, and then list a new set of syntac-
tic dependency feature functions that extend the
DTM2 feature set. We also discuss pruning and
other details of the approach.
4 The Incremental Dependency Parser
As it processes an input sentence left-to-right
word-by-word, the incremental dependency model
builds?for each prefix of the input sentence?a
partial parse that is a subgraph of the partial parse
that it builds for a longer prefix. The dependency
graph is constructed incrementally, in that the sub-
graph constructed at a preceding step is never al-
tered or revised in any later steps. The following
schematic view in (2) exhibits the general work-
ings of this parser:
S
0
o
1
w
1
,st
1
//
S
1
o
2
w
2
,st
2
//
S
2
S
i
o
i
w
i
,st
i
//
S
i+1
S
n
(2)
The syntactic process is represented by a sequence
of transitions between adjacent syntactic states S
i
.
1184
A transition from state S
i?1
to S
i
scans the cur-
rent word w
i
and stochastically selects a com-
plex lexical descriptor/category st
i
and an oper-
ator o
i
given the local context in the transition se-
quence. The syntactic state S
i
summarizes all the
syntactic information about fragments that have
already been processed and registers the syntac-
tic arguments which are to be expected next. Only
an impoverished deterministic procedure (called a
?State-Realizer?) is needed in order to compose a
state S
i
with the previous states S
0
. . . S
i?1
in or-
der to obtain a fully connected intermediate depen-
dency structure at every position in the input.
To implement the incremental parsing scheme
described above we use the parser described in
(Hassan et al, 2008b; Hassan et al, 2009), which
is based on Combinatory Categorial Grammar
(CCG) (Steedman, 2000). We only briefly de-
scribe this parser as its full description is beyond
the scope of this paper. The notions of a supertag
as a lexical category and the process of supertag-
ging are both crucial here (Bangalore and Joshi,
1999). Fortunately, CCG specifies the desired kind
of lexical categories (supertags) st
i
for every word
and a small set of combinatory operators o
i
that
combine the supertag st
i
with a previous parse
state S
i?1
into the next parse state S
i
. In terms
of CCG representations, the parse state is a CCG
composite category which specifies either a func-
tor and the arguments it expects to the right of the
current word, or is itself an argument for a functor
that will follow it to the right. At the first word in
the sentence, the parse state consists solely of the
supertag of that word.
Attacks rocked Riyadh
S
0
NP (S\NP)/NP NP
> NOP
S
1
: NP
> TRFC
S
2
: S/NP
> FA
S
3
: S
Figure 1: A sentence and possible supertag-,
operator- and state-sequences. NOP: No Oper-
ation; TRFC: Type Raise-Forward Composition;
FA: Forward Application. The CCG operators
used show that Attacks and Riyadh are both
dependents of rocked.
Figure 1 exhibits an example of the workings of
this parser. Practically speaking, after POS tag-
ging the input sentence, the parser employs two
components:
? A Supertag-Operator Tagger which proposes
a supertag?operator pair for the current word,
? A deterministic State-Realizer, which real-
izes the current state by applying the current
operator to the previous state and the current
supertag.
The Supertag-Operator Tagger is a probablistic
component while the State-Realizer is a determin-
istic component. The generative model underlying
this component concerns the probability P (W,S)
of a word sequence W = wn
1
and a parse-state
sequence S = Sn
1
, with associated supertag se-
quence ST = stn
1
and operator sequence O = on
1
,
which represents a possible derivation. Note that
given the choice of supertags st
i
and operator o
i
,
the state S
i
is calculated deterministically by the
State-Realizer.
A generative version of this model is described
in (3):
P (W,S) =
n
?
i=1
Word Predictor
? ?? ?
P (w
i
|W
i?1
S
i?1
)
.
Supertagger
? ?? ?
P (st
i
|W
i
) .
Operator Tagger
? ?? ?
P (o
i
|W
i
, S
i?1
, ST
i
) (3)
In (3):
? P (W,S) represents the product of the pro-
duction probabilities at each parse-state and
is similar to the structured language model
representation introduced in (Chelba, 2000).
? P (w
i
|W
i?1
S
i?1
) is the probability of w
i
given the previous sequence of words W
i?1
and the previous sequence of states S
i?1
,
? P (st
i
|W
i
): is the supertag st
i
probability
given the word sequence W
i
up to the cur-
rent position. Basically, this represents a se-
quence tagger (a ?supertagger?).
? P (o
i
|W
i
, S
i?1
, ST
i
) represents the probabil-
ity of the operator o
i
given the previous
words, supertags and state sequences up to
the current position. This represents a CCG
operator tagger.
The different local conditional components (for
every i) in (3) are estimated as discriminative
MaxEnt submodels trained on a corpus of incre-
mental CCG derivations. This corpus was ex-
tracted from the CCGbank (Hockenmaier, 2003)
1185
by transforming every normal form derivation into
strictly left-to-right CCG derivations, with the
CCG operators only slightly redesigned to allow
incrementality while still satisfying the dependen-
cies in the CCGbank (cf. (Hassan et al, 2008b;
Hassan et al, 2009)).
As mentioned before, the State-Realizer is a
deterministic function. Starting at the first word
with (obviously) a null previous state, the realizer
performs the following deterministic steps for
each word in turn: (i) set the current supertag
and operator to those of the current word; (ii) at
the current state, apply the current operator to the
previous state and current supertag; (iii) add edges
to the dependency graphs between words that are
linked as CCG arguments; and (iv) if not at the
end of the sentence, set the previous state to the
current one, then set the current word to the next
one, and iterate from (i).
It is worth noting that the proposed dependency
parser is deterministic in the sense that it maintains
only one parse state per word. This characteris-
tic is crucial for its incorporation into a large-scale
SMT system to avoid explosion of the translation
space during decoding.
5 Dependency-based DTM (DDTM)
In this section we extend the DTM2 model with
incremental target dependency-based syntax. We
call the resulting model the Dependency-based Di-
rect Translation Model (DDTM). This extension
takes place by (i) extracting syntactically enriched
minimal phrase pairs, (ii) including a new set of
syntactic feature functions among the exponen-
tial model features, and (iii) adapting the decoder
for dealing with syntax, including various pruning
strategies and enhancements. Next we describe
each extension in turn.
5.1 Phrase Table: Incremental Syntax
The target-side sentences in the word-aligned par-
allel corpus used for training are parsed using
the incremental dependency parser described in
section 4. This results in a word-aligned par-
allel corpus where the words of the target sen-
tences are tagged with supertags and operators.
From this corpus we extract the set of minimal
phrase pairs using the method described in (Itty-
cheriah and Roukos, 2007), extracting along with
every target phrase the associated sequences of su-
pertags and operators. As shown in (4), a source
phrase s
1
, . . . , s
n
translates into a target phrase
w
1
, . . . , w
m
where every word w
i
is labeled with
a supertag st
i
, and a possible parsing operator o
i
appearing with it in the parsed parallel corpus:
s
1
...s
n
//
[w
1
, st
1
, o
1
]...[w
m
, st
m
, o
m
] (4)
Hence, our phrase table associates with every
target phrase an incremental parsing subgraph.
These subgraphs along with their probabilities
represent our phrase table augmented with incre-
mental dependency parsing structure.
This representation turns the complicated prob-
lem of MT with incremental parsing into a sequen-
tial classification problem in which the classifier
deploys various features from the source sentence
and the candidate target translations to specify a
sequence of decisions that finally results in an out-
put target string along with its associated depen-
dency graph. The classification decisions are per-
formed in sequence step-by-step while traversing
the input string to provide decisions on possible
words, supertags, operators and states. A beam
search decoder simultaneously decides which se-
quence is the most probable.
5.2 DDTM Features
The exponential model and the MaxEnt frame-
work used in DTM2 and DDTM enabled us to ex-
plore the utility of incremental syntactic parsing
within a rich feature space. In our DDTM sys-
tem, we implemented a set of features alongside
the baseline DTM2 features that were discussed in
Section 3. The features described here encode all
the probabilistic components in (3) within a log
linear interpretation along with some more empir-
ically intuitive features.
? Supertag-Word features: these features ex-
amine the target phrase words with their as-
sociated supertags and is related to the Su-
pertagger component in (3).
? Supertag sequence features: these features
encode n-gram supertags (equivalent to the n-
gram supertags Language Model). This fea-
ture is related to the supertagger component
as well.
? Supertag-Operator features: these features
encode supertags and associated operators
which is related to the Operator Tagger com-
ponent in (3).
1186
? Supertag-State features: these features regis-
ter state and supertag co-occurrences.
? State sequence features: these features en-
code n-gram state features and are equiva-
lent to an n-gram Language Model over parse
state sequences which is related to the multi-
plication in (3).
? Word-State sequence features: these fea-
tures encode words and states co-occurrences
which is related to the Word Predictor com-
ponent in (3).
The exponential model and the MaxEnt frame-
work used in DTM2 and DDTM enable us to ex-
plore the utility of incremental syntactic parsing
with the use of minimal phrases within a rich fea-
ture space.
5.3 DDTM Decoder
In order to support incremental dependency pars-
ing, we extend the DTM2 decoder in three ways:
firstly, by constructing the syntactic states during
decoding; secondly, by extending the hypothesis
structures to incorporate the syntactic states and
the partial dependency derivations; and thirdly, by
modifying the pruning strategy to handle the large
search space.
At decoding time, each hypothesis state is as-
sociated with a parse-state which is constructed
while decoding using the incremental parsing ap-
proach introduced in ((Hassan et al, 2008b; Has-
san et al, 2009)). The previous state, the se-
quences of supertags and CCG incremental opera-
tors are deployed in a deterministic manner to re-
alize the parse-states as well as the intermediate
dependency graphs between words.
Figure 2 shows the DDTM decoder while de-
coding a sentence with the English translation ?At-
tacks rocked Riyadh?. Each hypothesis is asso-
ciated with a parse-state S
i
and a partial depen-
dency graph (shown for some states only). More-
over, each transition is associated with an opera-
tor o that combines the previous state and the cur-
rent supertag st to construct the next state S
i
. The
decoder starts from a null state S
1
and then pro-
ceeds with a possible expansion with the word ?at-
tacks?, supertag NP and operator NOP to pro-
duce the next hypothesis with state S
2
and cate-
gory NP . Further expansion for that path with the
verb ?rocked?, supertag ?(S\NP )/NP and oper-
ator TRFC will produce the state S
5
with cat-
egory S/NP . The partial dependency graph for
state S
5
is shown above the state where a depen-
dency relation between the two words is estab-
lished. Furthermore, another expansion with the
word ?Riyadh?, supertag NP and operator FA
produces state S
7
with category S and a completed
dependency graph as shown above the state. An-
other path which spans the states S
1
, S
3
, S
6
and
S
8
ends with a state category S/NP and a partial
dependency graph as shown under state S
8
where
the dependency graph is still missing its object
(e.g. ?Riyadh attacks rocked the Saudi Govt.?).
The addition of parse-states may result in a very
large search space due to the fact that the same
phrase/word may have many possible supertags
and many possible operators. Moreover, the same
word sequences may have many parse-state se-
quences and, therefore, many hypotheses that rep-
resent the same word sequence. The search space
is definitely larger than the baseline search space.
We adopt the following three pruning heuristics to
limit the search space.
5.3.1 Grammatical Pruning
Any hypothesis which does not constitute a valid
parse-state is discarded, i.e. if the previous parse-
state and the current supertag sequence cannot
construct a valid state using the associated oper-
ator sequence, then the expansion is discarded.
Therefore, this pruning strategy maintains only
fully connected graphs and discards any partially
connected graphs that might result during the de-
coding process.
As shown in Figure 2, the expansion from state
S
1
to state S
4
(with the dotted line) is pruned and
not expanded further because the proposed expan-
sion is the verb ?attacks?, supertag (S\NP )/NP
and operator TRFC . Since the previous state is
NULL, it cannot be combined with the verb using
the TRFC operator. This would produce an un-
defined state and thus the hypothesis is discarded.
5.3.2 Supertags and Operators Threshold
We limit the supertag and operator variants per tar-
get phrase to a predefined number of alternatives.
We tuned this on the MT03 DevSet for the best
accuracy while maintaining a manageable search
space. The supertags limit was set to four alterna-
tives while the operators limit was set to three.
As shown in Figure 2, each word can have many
alternatives with different supertags. In this exam-
ple the word ?attacks? has two forms, namely a
1187
e:
a : --------
P:1
S1:NULL
e: attacks
a: *----
P:=.162
ST=NP
S2=NP
e: attacks
a: *-------
P:=.092
ST=(S\NP)/NP
S4= UNDEF
O:TRFC
e: Riyadh
a: -*------
P:=.142
ST=NP/NP
S3=NP/NP
e: rocked
a: --*--
P:=.083
ST=(S\NP)/NP
S5=S/NP
O:NOP
O:NOP
O:TRFC
e: rocked
a: --*------
P:=.01
ST=(S\NP)/NP
S8=S/NP
attacks
attacks rocked
e: Riyadh
a: --*--
P:=.04
ST=NP
S7=S
O:FC
attacks rocked Riyadh
e: attacks
a: *-------
P:=.07
ST=NP
S6=NP
O:TRFC
Riyadh attacks rocked
O:FA
Figure 2: DDTM Decoder: each hypothesis has a parse state and a partial dependency structure.
noun and a verb, with different supertags and op-
erators. The proposed thresholds limit the possible
alternatives to a reasonable number.
5.3.3 Merging Hypotheses
Standard Phrase?based SMT decoders merge
translation hypotheses if they cover the same
source words and share the same n-gram lan-
guage model history. Similarly, DDTM decoder
merges translation hypotheses if they cover the
same source words, share the same n-gram lan-
guage model history and share the same parse-
state history. This helps in reducing the search
space by merging paths that will not constitute a
part of the best path.
6 Experiments
We conducted experiments on an Arabic-to-
English translation task using LDC parallel data
and GALE parallel data. We used the UN paral-
lel corpus and LDC news corpus together with the
GALE parallel corpus, totaling 7.8M parallel sen-
tences. The 5-gram Language Model was trained
on the English Gigaword Corpus and the English
part of the parallel corpus. Our baseline system is
similar to the system described in (Ittycheriah and
Roukos, 2007). We report results on NIST MT05
and NIST MT06 evaluations test sets using BLEU
and TER as automatic evaluation metrics.
To train the DDTM model, we use the incre-
mental parser introduced in (Hassan et al, 2008b;
Hassan et al, 2009) to parse the target side of the
parallel training data. Each sentence is associated
with supertag, operator and parse-state sequences.
We then train models with different feature sets.
Results: We compared the baseline DTM2 (It-
tycheriah and Roukos, 2007) with our DDTM sys-
tem with the features listed above. We examine
the effect of all features on system performance.
In this set of experiments we used LDC parallel
data only which is composed of 3.7M sentences
and the results are reported on MT05 test set. Each
of the examined systems deploys DTM2 features
in addition to a number of newly added syntactic
features. The systems examined are:
? DTM2: Direct Translation model 2 baseline.
? D-SW: DTM2 + Supertag-Word features.
? D-SLM: DTM2 + Supertag-Word and su-
pertag n-gram features.
? D-SO: DTM2+ Supertag-Operator features.
? D-SS : DTM2 + supertags and states features
with parse-state construction.
? D-WS : DTM2 + words and states features
with parse-state construction.
? D-STLM: DTM2 + state n-gram features
with parse-state construction.
? DDTM: fully fledged system with all fea-
tures that proved useful above which are:
Supertag-Word features, supertag n-gram
1188
features, supertags and states features and
state n-gram features .
System BLEU Score on MT05
DTM2-Baseline 52.24
D-SW 52.28
D-SLM 52.29
D-SO 52.01
D-SS 52.39
D-WS 52.03
D-STLM 52.53
DDTM 52.61
Table 1: DDTM Results with various features.
As shown in Table 1, the DTM baseline system
demonstrates a very high BLEU score, unsurpris-
ingly given its top-ranked performance in two re-
cent major MT evaluation campaigns. Among the
features we tried, supertags and n-gram supertags
systems (D-SW and D-SLM systems) give slight
yet statistically insignificant improvements. On
the other hand, the states n-gram sequence features
(D-SS and DDTM systems) give small yet statis-
tically significant improvements (as calculated via
bootstrap resampling (Koehn, 2004b)). The D-WS
system shows a small degradation in performance,
probably due to the fact that the states-words inter-
actions are quite sparse and could not be estimated
with good evidence. Similarly, the D-SO system
shows a small degradation in performance. When
we investigated the features types, we found out
that all features that deploy the operators had bad
effect on the model. We think this is due to the fact
that the operator set is a small set with high evi-
dence in many training instances such that it has
low discriminative power on it is own. However,
it implicitly helps in producing the state sequence
which proved useful.
System DTM2-Baseline DDTM
MT05 (BLEU) 55.28 55.66
MT05 (TER) 38.79 38.48
MT06 (BLEU) 43.56 43.91
MT06 (TER) 49.08 48.65
Table 2: DDTM Results on MT05 and MT06.
We examined a combination of the best fea-
tures in our DDTM system on a larger training
data comprising 7.8M sentences from both NIST
and GALE parallel corpora. Table 2 shows the
results on both MT05 and MT06 test sets. As
shown, DDTM significantly outperforms the state-
of-the-art baseline system. It is worth noting that
DDTM outperforms this baseline even when very
large amounts of training data are used. Despite
the fact that the actual scores are not so different,
we found that the baseline translation output and
the DDTM translation outout are significantly dif-
ferent. We measured this by calculating the TER
between the baseline translation and the DDTM
translation for the MT05 test set, and found this
to be 25.9%. This large difference has not been
realized by the BLEU or TER scores in compari-
son to the baseline. We believe that this is due to
the fact that most changes that match the syntac-
tic constraints do not bring about the best match
where the automatic evaluation metrics are con-
cerned. Accordingly, in the next section we de-
scribe the outcome of a detailed manual analysis
of the output translations.
7 Manual Analysis of Results
Although the BLEU score does not mark a large
improvement by the dependency-based system
over the baseline system, human inspection of the
data gives us important insights into the pros and
cons of the dependency-based model. We ana-
lyzed a randomly selected set of 100 sentences
from the MT05 test set. In this sample, the base-
line and the DDTM system perform similarly in
68% of the sentences. The outputs of both system
are similar though not identical. In these cases,
the systems may choose equivalent paraphrases.
However, the translations using syntactic struc-
tures are rather similar. It is worth noting that the
DDTM system tends to produce more concise sys-
ntactic structures which may lead to less BLUE
score due to penalizing the translation length al-
though the translation might be equivelent to the
baseline if not better.
In 28% of the sentences, the DDTM system pro-
duces remarkably better translations. The exam-
ples here illustrate the behaviour of the baseline
and the DDTM systems which can be observed
consistently throughout the test set. We only high-
light some of the examples for illustration pur-
poses. DDTM manages to insert verbs which are
deleted by any standard phrase-based SMT sys-
tem. DDTM prefers to deploy verbs since they
have complex and more detailed syntactic struc-
tures which give better and more likely state se-
1189
quences. Furthermore, the DDTM system avoids
longer noun phrases and instead uses some prepo-
sitions in-between. Again, this is probably due to
the fact that like verbs, prepositions have a com-
plex syntactic description that give rise to more
likely state sequences.
On the other hand, the baseline produced better
translation in 8% of the analysis sample. We ob-
served that the baseline is doing better mainly in
two cases. The first when the produced translation
is very poor and producing poor sysntatctic struc-
ture due to out of vocabularies or hard to trans-
late sentences. The second case is with sentences
with long noun phrases, in such cases the DDTM
system prefres to introduce verbs or prepositions
in the middle of long noun phrase and thus the
baseline would produce better translations. This
is maybe due to the fact that noun phrases have
relatively simple structure in CCG such that it did
not help in constructing long noun phrases.
Source: ??Q???  ZAJ
.
? Yg A

?Qk
.


HA??j
	
?? ??
	
X Y?K
.
?
	
?
	
k?
Reference: He then underwent medical examinations by a po-
lice doctor .
Baseline: He was subjected after that tests conducted by doc-
tors of the police .
DDTM: Then he underwent tests conducted by doctors of the
police .
Source: 	?



KPA J


?
.
	
?A??j
.
? ?J


?  ZA??
	
?AK


Q ?
	
Q? Y

??
	
?




J
	
j
	
j
	
??
Reference: Riyadh was rocked tonight by two car bomb at-
tacks..
Baseline: Riyadh rocked today night attacks by two booby -
trapped cars.
DDTM: Attacks rocked Riyadh today evening in two car
bombs.
Figure 3: DDTM provides better syntactic struc-
ture with more concise translations.
Figure 3 shows two examples where DDTM
provides better and more concise syntactic struc-
ture. As we can see, there is not much agree-
ment between the reference and the proposed
translation. However, longer translations enhance
the possibility of picking more common n-gram
matches via the BLEU score and so increases the
chance of better scores. This well-known bias
does not favour the more concise output derived
by our DDTM system, of course.
8 Conclusion and Future Work
In this paper, we presented a novel model of de-
pendency phrase-based SMT which integrates in-
cremental dependency parsing into the transla-
tion model while retaining the linear decoding as-
sumed in conventional Phrase?based SMT sys-
tems. To the best of our knowledge, this model
constitutes the first effective attempt at integrating
a linear-time dependency parser that builds a con-
nected tree incrementally into SMT systems with
linear-time decoding. Crucially, it turns out that
incremental dependency parsing based on lexical-
ized grammars such as CCG and LTAG can pro-
vide valuable incremental parsing information to
the decoder even if their output is imperfect. We
believe this robustness in the face of imperfect
parser output to be a property of the probabilistic
formulation and statistical estimation used in the
Direct Translation Model. A noteworthy aspect of
our proposed approach is that it integrates features
from the derivation process as well as the derived
tree. We think that this is possible due to the im-
portance of the notion of a derivation in linguistic
frameworks such as CCG and LTAG.
Future work will attempt further extensions of
our DDTM system to allow for the exploitation
of long-range aspects of the dependency struc-
ture. We will work on expanding the features
set of DDTM system to leverage features from
the constructed dependency structure itself. Fi-
nally, we will work on enabling the deployment
of source side dependency structures to influence
the construction of the target dependency structure
based on a bilingually enabled dependency pars-
ing mechanism using the discriminative modeling
capabilities.
Acknowledgments
We would like to thank Salim Roukos, IBM TJ
Watson Research Center, for fruitful, insightful
discussions and for his support during this work.
We also would like to thank Abe Ittycheriah, IBM
TJ Watson Research Center, for providing the
DTM2 baseline and for his support during de-
veloping this system. Finally, we would like to
thank the anonymous reviewers for their helpful
and constructive comments.
1190
References
Bangalore, S. and Joshi, A. (1999). ?Supertagging: An
Approach to Almost Parsing?, Computational Lin-
guistics 25(2):237?265, 1999.
Berger, A. and Della Pietra, S. and Della Pietra, V.J.
(1996). Maximum Entropy Approach to Natural
Language Processing Computational Linguistics,
22(1): 39?71, 1996.
Birch, A., Osborne, M. and Koehn, P. (2007). CCG Su-
pertags in Factored Statistical Machine Translation.
In Proceedings of the Second Workshop on Statisti-
cal Machine Translation, ACL-07, pp.9?16, 2007.
Brown, P., Cocke,J., Della Pietra, S., Jelinek, F., Della
Pietra, V.J. Lafferty, R. Mercer and Roossin, P. ?A
Statistical Approach to Machine Translation? Com-
putational Linguistics 16(2):79?85, 1990.
Chelba, C. (2000). Exploiting Syntactic Structure for
Natural Language Modeling. PhD thesis, Johns
Hopkins University, Baltimore, MD.
Chiang, D. (2005). A Hierarchical Phrase-Based
Model for Statistical Machine Translation. In 43rd
Annual Meeting of the Association for Computa-
tional Linguistics (ACL05), pp.263?270, Ann Arbor,
MI.
Hassan, H., Sima?an, K., and Way, A.. (2009). Lex-
icalized Semi-Incremental Dependency Parsing. In
Proceedings of RANLP 2009, the International Con-
ference on Recent Advances in Natural Language
Processing, Borovets, Bulgaria (to appear).
Hassan, H., Sima?an, K., and Way, A. (2008a). Syntac-
tically Lexicalized Phrase-Based Statistical Transla-
tion. IEEE Transactions on Audio, Speech and Lan-
guage Processing, 6(7):1260?1273.
Hassan, H., Sima?an, K., and Way, A.. (2008b). A Syn-
tactic Language Model Based on Incremental CCG
Parsing. In Proceedings IEEE Workshop on Spoken
Language Technology (SLT) 2008, Goa, India.
Hassan, H., Sima?an, K., and Way, A. (2007). Inte-
grating Supertags into Phrase-based Statistical Ma-
chine Translation. In Proceedings of the ACL-2007,
Prague, Czech Republic, pp.288?295, 2007.
Hockenmaier, J. (2003). Data and Models for Statisti-
cal Parsing with Combinatory Categorial Grammar,
Ph.D Thesis, University of Edinburgh, UK, 2003.
Huang, L. and Chiang, D. (2007). Forest Rescoring:
Faster Decoding with Integrated Language Models.
In Proceedings of the ACL-2007, Prague, Czech Re-
public, 2007.
Ittycheriah, A. and Roukos, S. (2007). Direct trans-
lation model 2. In Human Language Technologies
2007: The Conference of the North American Chap-
ter of the Association for Computational Linguis-
tics; Proceedings of the Main Conference, pp.57?64,
Rochester, NY.
Koehn, P. (2004a). Pharaoh: A Beam Search De-
coder for phrase-based Statistical Machine Transla-
tion Models. Machine Translation: From Real Users
to Research. In Proceedings of 6th Conference of the
Association for Machine Translation in the Ameri-
cas, AMTA 2004, pp.115?124, Washington, DC.
Koehn, P. (2004b). Statistical Significance Tests for
Machine Translation Evaluation. In Proceedings of
the 2004 Conference on Empirical Methods in Nat-
ural Language Processing (EMNLP), pp.388?395,
Edmonton, AB, Canada.
Koehn, P. Och, F.J. and Marcu, D. (2003). Statisti-
cal phrase-based translation. In Proceedings of the
Joint Human Language Technology Conference and
the Annual Meeting of the North American Chap-
ter of the Association for Computational Linguistics
(HLT-NAACL 2003), pp.127?133, Edmonton, AL,
Canada.
Marcu, D., Wang, W., Echihabi, A., and Knight, K.
(2006). SPMT: Statistical Machine Translation with
Syntactified Target Language Phrases. In Proceed-
ings of the 2006 Conference on Empirical Methods
in Natural Language Processing (EMNLP 2006),
pp.44?52, Sydney, Australia.
Papineni, K., Roukos, S., and Ward, T. (1997).
Feature-Based Language Understanding. In Pro-
ceedings of 5th European Conference on Speech
Communication and Technology EUROSPEECH
?97 , pp.1435?1438, Rhodes, Greece.
Papineni, K., Roukos, S., Ward, T. and Zhu, W-J.
(2002). BLEU: a Method for Automatic Evalua-
tion of Machine Translation. In 40th Annual Meet-
ing of the Association for Computational Linguistics
(ACL?02), pp.311?318, Philadelphia, PA.
Shen, L., Xu, J., and Weischedel, R. (2008). A new
string-to-dependency machine translation algorithm
with a target dependency language model. In Pro-
ceedings of ACL-08: HLT, pp.577?585, Columbus,
OH.
Snover, M., Dorr, B., Schwartz, R., Micciulla, L. and
Makhoul, J. (2006) A Study of Translation Edit Rate
with Targeted Human Annotation. In AMTA 2006:
Proceedings of the 7th Conference of the Association
for Machine Translation in the Americas, pp.223?
231, Cambridege, MA.
Steedman, M. (2000). The Syntactic Process. MIT
Press, Cambridge, MA.
Tillmann, C. and Ney, H. (2003). Word reordering and
a dynamic programming beam search algorithm for
statistical machine translation. Computational Lin-
guistics, 29(1):97?133.
Zollmann, A. and Venugopal, A. (2006). Syntax aug-
mented machine translation via chart parsing. In
Proceedings of the Workshop on Statistical Machine
Translation, HLT/NAACL, pp.138?141, New York,
NY.
1191
Language Independent Text Correction using Finite State Automata
Ahmed Hassan? Sara Noeman
IBM Cairo Technology Development Center
Giza, Egypt
hasanah,noemans,hanyh@eg.ibm.com
Hany Hassan
Abstract
Many natural language applications, like
machine translation and information extrac-
tion, are required to operate on text with
spelling errors. Those spelling mistakes
have to be corrected automatically to avoid
deteriorating the performance of such ap-
plications. In this work, we introduce a
novel approach for automatic correction of
spelling mistakes by deploying finite state
automata to propose candidates corrections
within a specified edit distance from the mis-
spelled word. After choosing candidate cor-
rections, a language model is used to assign
scores the candidate corrections and choose
best correction in the given context. The
proposed approach is language independent
and requires only a dictionary and text data
for building a language model. The ap-
proach have been tested on both Arabic and
English text and achieved accuracy of 89%.
1 Introduction
The problem of detecting and correcting misspelled
words in text has received great attention due to
its importance in several applications like text edit-
ing systems, optical character recognition systems,
and morphological analysis and tagging (Roche and
Schabes, 1995). Other applications, like machine
translation and information extraction, operate on
text that might have spelling errors. The automatic
detection, and correction of spelling erros should be
of great help to those applications.
The problem of detecting and correcting mis-
spelled words in text is usually solved by checking
whether a word already exists in the dictionary or
not. If not, we try to extract words from the dictio-
nary that are most similar to the word in question.
?Now with the University of Michigan Ann Arbor, has-
sanam@umich.edu
Those words are reported as candidate corrections
for the misspelled word.
Similarity between the misspelled word and dic-
tionary words is measured by the Levenshtein edit
distance (Levenshtein, 1966; Wagner and M.Fisher,
1974). The Levenshtein edit distance is usu-
ally calculated using a dynamic programming tech-
nique with quadratic time complexity (Wagner and
M.Fisher, 1974). Hence, it is not reasonable to com-
pare the misspelled word to each word in the dictio-
nary while trying to find candidate corrections.
The proposed approach uses techniques from fi-
nite state theory to detect misspelled words and to
generate a set of candidate corrections for each mis-
spelled word. It also uses a language model to select
the best correction from the set of candidate correc-
tions using the context of the misspelled word. Us-
ing techniques from finite state theory, and avoiding
calculating edit distances makes the approach very
fast and efficient. The approach is completely lan-
guage independent, and can be used with any lan-
guage that has a dictionary and text data to building
a language model.
The rest of this paper will proceed as follows.
Section 2 will present an overview of related work.
Section 3 will discuss the different aspects of the
proposed approach. Section 4 presents a perfor-
mance evaluation of the system. Finally a conclu-
sion is presented in section 5.
2 Related Work
Several solutions were suggested to avoid comput-
ing the Levenshtein edit distance while finding can-
didate corrections. Most of those solutions select
a number of dictionary words that are supposed to
contain the correction, and then measure the dis-
tance between the misspelled word and all selected
words. The most popular of those methods are the
similarity keys methods (Kukich, 1992; Zobel and
Dart, 1995; De Beuvron and Trigano, 1995). In
913
those methods, the dictionary words are divided into
classes according to some word features. The input
word is compared to words in classes that have sim-
ilar features only.
In addition to the techniques discussed above,
other techniques from finite state automata have
been recently proposed. (Oflazer, 1996) suggested
a method where all words in a dictionary are treated
as a regular language over an alphabet of letters. All
the words are represented by a finite state machine
automaton. For each garbled input word, an exhaus-
tive traversal of the dictionary automaton is initiated
using a variant of Wagner-Fisher algorithm (Wag-
ner and M.Fisher, 1974) to control the traversal of
the dictionary. In this approach Levenshtein dis-
tance is calculated several times during the traversal.
The method carefully traverses the dictionary such
that the inspection of most of the dictionary states
is avoided. (Schulz and Mihov, 2002) presents a
variant of Oflazers?s approach where the dictionary
is also represented as deterministic finite state au-
tomaton. However, they avoid the computation of
Levenshtein distance during the traversal of the dic-
tionary automaton. In this technique, a finite state
acceptor is constructed for each input word. This
acceptor accepts all words that are within an edit dis-
tance k from the input word. The dictionary automa-
ton and the Levenshtein-automaton are then tra-
versed in parallel to extract candidate corrections for
the misspelled word. The authors present an algo-
rithm that can construct a deterministic Levenshtein-
automaton for an arbitrary word of degrees 1, and
2 which corresponds to 1 or 2 errors only. They
suggest another algorithm that can construct a non-
deterministic Levenshtein-automaton for any other
degree. They report results using a Levenshtein-
automaton of degree 1(i.e. words having a single
insertion, substitution, or deletion) only.
The method we propose in this work also assumes
that the dictionary is represented as a determinis-
tic finite state automaton. However, we completely
avoid computing the Levenshtein-distance at any
step. We also avoid reconstructing a Levenshtein-
automaton for each input word. The proposed
method does not impose any constraints on the
bound k, where k is the edit distance between the
input word and the candidate corrections. The ap-
proach can adopt several constraints on which char-
Figure 1: An FSM representation of a word list
acters can substitute certain other characters. Those
constraints are obtained from a phonetic and spatial
confusion matrix of characters.
The purpose of context-dependent error correc-
tion is to rank a set of candidate corrections tak-
ing the misspelled word context into account. A
number of approaches have been proposed to tackle
this problem that use insights from statistical ma-
chine learning (Golding and Roth, 1999), lexical
semantics (Hirst and Budanitsky, 2005), and web
crawls (Ringlstetter et al, 2007).
3 Error Detection and Correction in Text
Using FSMs
The approach consists of three main phases: detect-
ing misspelled words, generating candidate correc-
tions for them, and ranking corrections. A detailed
description of each phase is given in the following
subsections.
3.1 Detecting Misspelled Words
The most direct way for detecting misspelled words
is to search the dictionary for each word, and report
words not found in the dictionary. However, we can
make use of the finite state automaton representation
of the dictionary to make this step more efficient.
In the proposed method, we build a finite state ma-
chine (FSM) that contains a path for each word in
the input string. This FSM is then composed with
the dictionary FSM. The result of the composition
is merely the intersection of the words that exist in
both the input string and the dictionary. If we calcu-
lated the difference between the FSM containing all
words and this FSM, we get an FSM with a path for
914
each misspelled word. Figure 1 illustrate an FSM
that contain all words in an input string.
3.2 Generating Candidate Corrections
The task of generating candidate corrections for mis-
spelled words can be divided into two sub tasks:
Generating a list of words that have edit distance
less than or equal k to the input word, and select-
ing a subset of those words that also exist in the dic-
tionary. To accomplish those tasks, we create a sin-
gle transducer(Levenshtein-transducer) that is when
composed with an FSM representing a word, gen-
erates all words withing any edit distance k from
the input word. After composing the misspelled
word with Levenshtein-transducer, we compose the
resulting FSM with the dictionary FSM to filter out
words that do not exist in the dictionary.
3.2.1 Levenshtein-transducers for primitive
edit distances
To generate a finite state automaton that con-
tain all words within some edit distance to the in-
put word, we use a finite state transducer that al-
lows editing its input according to the standard
Levenshtein-distance primitive operations: substitu-
tion, deletion, and insertion.
A finite-state transducers (FST) is a a 6-tuple
(Q,?1,?2, ?, i, F ), where Q is a set of states, ?1
is the input alphabet, ?2 is the output alphabet, i is
the initial state, F ? Q is a set of final states, and ?
is a transition function (Hopcroft and Ullman, 1979;
Roche and Shabes, 1997). A finite state acceptor is a
special case of an FST that has the same input/output
at each arc.
Figure 2 illustrates the Levenshtein-transducer for
edit distance 1 over a limited set of vocabulary (a,b,
and c). We can notice that we will stay in state zero
as long as the output is identical to the input. On
the other hand we can move from state zero, which
corresponds to edit distance zero, to state one, which
corresponds to edit distance one, with three different
ways:
? input is mapped to a different output (input is
consumed and a different symbol is emitted)
which corresponds to a substitution,
? input is mapped to an epsilon (input is con-
sumed and no output emitted) which corre-
sponds to a deletion, and
Figure 2: A Levenshtein-transducer (edit distance 1)
? an epsilon is mapped to an output (output is
emitted without consuming any input) which
corresponds to an insertion.
Once we reach state 1, the only possible transitions
are those that consume a symbol and emit the same
symbol again and hence allowing only one edit op-
eration to take place.
When we receive a new misspelled word, we rep-
resent it with a finite state acceptor that has a single
path representing the word, and then compose it with
the Levenshtein-transducer. The result of the com-
position is a new FSM that contains all words with
edit distance 1 to the input word.
3.2.2 Adding transposition
Another non-primitive edit distance operation that
is frequently seen in misspelled words is transposi-
tion. Transposition is the operation of exchanging
the order of two consecutive symbols (ab ? ba).
Transposition is not a primitive operation because
it can be represented by other primitive operations.
However, this makes it a second degree operation.
As transposition occurs frequently in misspelled
words, adding it to the Levenshtein-transducer as a
single editing operation would be of great help.
915
Figure 3: A Levenshtein-transducer for edit distance
1 with transposition
To add transposition, as a single editing opera-
tion, to the Levenshtein-transducer we add arcs be-
tween states zero and one that can map any symbol
sequence xy to the symbol sequence yx, where x,
and y are any two symbols in the vocabulary. Fig-
ure 3 shows the Levenshtein-transducer with degree
1 with transposition over a limited vocabulary (a and
b).
3.2.3 Adding symbol confusion matrices
Adding a symbol confusion matrix can help re-
duce the number of candidate corrections. The con-
fusion matrix determines for each symbol a set of
symbols that may have substituted it in the garbled
word. This matrix can be used to reduce the num-
ber of candidate corrections if incorporated into the
Levenshtein-transducer. For any symbol x, we add
an arc x : y between states zero, and one in the trans-
ducer where y ? Confusion Matrix(x) rather
than for all symbols y in the vocabulary.
The confusion matrix can help adopt the meth-
ods to different applications. For example, we can
build a confusion matrix for use with optical char-
acter recognition error correction that captures er-
rors that usually occur with OCRs. When used with
a text editing system, we can use a confusion ma-
trix that predicts the confused characters according
to their phonetic similarity, and their spatial location
on the keyboard.
Figure 4: A Levenshtein-transducer for edit distance
2 with transposition
3.2.4 Using degrees greater than one
To create a Levenshtein-transducer that can gen-
erate all words within edit distance two of the input
word, we create a new state (2) that maps to two edit
operations, and repeat all arcs that moves from state
0 to state 1 to move from state 1 to state 2.
To allow the Levenshtein-transducer of degree
two to produce words with edit distance 1 and 2 from
the input word, we mark both state 1, and 2 as final
states. We may also favor corrections with lower
edit distances by assigning costs to final states, such
that final states with lower number of edit operations
get lower costs. A Levenshtein-transducer of degree
2 for the limited vocabulary (a and b) is shown in
figure 4.
3.3 Ranking Corrections
To select the best correction from a set of candidate
corrections, we use a language model to assign a
probability to a sequence of words containing the
corrected word. To get that word sequence, we go
back to the context where the misspelled word ap-
peared, replace the misspelled word with the candi-
date correction, and extract n ngrams containing the
candidate correction word in all possible positions
in the ngram. We then assign a score to each ngram
using the language model, and assign a score to the
candidate correction that equals the average score of
all ngrams. Before selecting the best scoring cor-
rection, we penalize corrections that resulted from
higher edit operations to favor corrections with the
minimal number of editing operations.
916
Edit 1/with trans. Edit 1/no trans. Edit 2/with trans. Edit 2 / no trans.
word len. av. time av. correcs. av. time av. correcs. av. time av. correcs. av. time av. correcs.
3 3.373273 18.769 2.983733 18.197 73.143538 532.637 69.709387 514.174
4 3.280419 4.797 2.796275 4.715 67.864291 136.230 66.279842 131.680
5 3.321769 1.858 2.637421 1.838 73.718353 33.434 68.695935 32.461
6 3.590046 1.283 2.877242 1.277 75.465624 11.489 69.246055 11.258
7 3.817453 1.139 2.785156 1.139 78.231015 6.373 72.2057 6.277
8 4.073228 1.063 5.593761 1.062 77.096026 4.127 73.361455 4.066
9 4.321661 1.036 3.124661 1.036 76.991945 3.122 73.058418 3.091
10 4.739503 1.020 3.2084 1.020 75.427416 2.706 72.2143 2.685
11 4.892105 1.007 3.405101 1.007 77.045616 2.287 71.293116 2.281
12 5.052191 0.993 3.505089 0.993 78.616536 1.910 75.709801 1.904
13 5.403557 0.936 3.568391 0.936 81.145124 1.575 78.732955 1.568
Table 1: Results for English
Edit 1/with trans. Edit 1/no trans. Edit 2/with trans. Edit 2 / no trans.
word len. av. time av. correcs. av. time av. correcs. av. time av. correcs. av. time av. correcs.
3 5.710543 31.702 4.308018 30.697 83.971263 891.579 75.539547 862.495
4 6.033066 12.555 4.036479 12.196 80.481281 308.910 71.042372 296.776
5 7.060306 6.265 4.360373 6.162 79.320644 104.661 69.71572 100.428
6 9.08935 4.427 4.843784 4.359 79.878962 51.392 74.197127 48.991
7 8.469497 3.348 5.419919 3.329 82.231107 24.663 70.681298 23.781
8 10.078842 2.503 5.593761 2.492 85.32005 13.586 71.557569 13.267
9 10.127946 2.140 6.027077 2.136 83.788916 8.733 76.199034 8.645
10 11.04873 1.653 6.259901 1.653 92.671732 6.142 81.007893 6.089
11 12.060286 1.130 7.327353 1.129 94.726469 4.103 77.464609 4.084
12 13.093397 0.968 7.194902 0.967 95.35985 2.481 82.40306 2.462
13 13.925067 0.924 7.740105 0.921 106.66238 1.123 78.966914 1.109
Table 2: Results for Arabic
4 Experimental Setup
4.1 Time Performance
The proposed method was implemented in C++ on
a 2GHz processor machine under Linux. We used
11,000 words of length 3,4,..., and 13, 1,000 word
for each word length, that have a single error and
computed correction candidates. We report both the
average correction time, and the average number of
corrections for each word length. The experiment
was run twice on different test data, one with con-
sidering transposition as primitive operation, and the
other without. We also repeated the experiments for
edit distance 2 errors, and also considered the two
cases where transposition is considered as a primi-
tive operation or not. Table 1 shows the results for
an English dictionary of size 225,400 entry, and Ta-
ble 2 shows the results for an Arabic dictionary that
has 526,492. entries.
4.2 Auto-correction accuracy
To measure the accuracy of the auto-correction pro-
cess, we used a list of 556 words having common
spelling errors of both edit distances 1 and 2. We put
a threshold on the number of characters per word to
decide whether it will be considered for edit distance
1 or 2 errors. When using a threshold of 7, the spell
engine managed to correct 87% of the words. This
percentage raised to 89% when all words were con-
sidered for edit distance 2 errors. The small degra-
dation in the performance occured because in 2% of
the cases, the words were checked for edit distance 1
errors although they had edit distance 2 errors. Fig-
ure 6 shows the effect of varying the characters limit
on the correction accuracy.
Figure 5 shows the effect of varying the weight as-
signed to corrections with lower edit distances on the
accuracy. As indicated in the figure, when we only
consider the language model weight, we get accura-
cies as low as 79%. As we favor corrections with
lower edit distances the correction accuracy raises,
but occasionally starts to decay again when empha-
sis on the low edit distance is much larger than that
on the language model weights.
Finally, we repeated the experiments but with us-
917
Figure 5: Effect of increasing lower edit distance
favoring factor on accuracy
Figure 6: Effect of increasing Ed1/Ed2 char limits
on accuracy
ing a confusion matrix, as 3.2.3. We found out that
the average computation time dropped by 78% ( be-
low 1 ms for edit distance 1 errors) at the price of
losing only 8% of the correction accuracy.
5 Conclusion
In this work, we present a finite state automata based
spelling errors detection and correction method.
The new method avoids calculating the edit dis-
tances at all steps of the correction process. It also
avoids building a Levenshtein-automata for each in-
put word. The method is multilingual and may work
for any language for which we have an electronic
dictionary, and a language model to assign probabil-
ity to word sequences. The preliminary experimen-
tal results show that the new method achieves good
performance for both correction time and accuracy.
The experiments done in this paper can be extended
in several directions. First, there is still much room
for optimizing the code to make it faster especially
the FST composition process. Second, we can allow
further editing operations like splitting and merging.
References
Francois De Bertrand De Beuvron and Philippe Trigano.
1995. Hierarchically coded lexicon with variants. In-
ternational Journal of Pattern Recognition and Artifi-
cial Intelligence, 9:145?165.
Andrew Golding and Dan Roth. 1999. A winnow-based
approach to context-sensitive spelling correction. Ma-
chine learning, 34:107?130.
Graeme Hirst and Alexander Budanitsky. 2005. Cor-
recting real-word spelling errors by restoring lexical
cohesion. Natural Language Engineering, 11:87?111.
J.E. Hopcroft and J.D. Ullman. 1979. Introduction to au-
tomata theory, languages, and computation. Reading,
Massachusetts: Addison-Wesley.
Karen Kukich. 1992. Techniques for automatically cor-
recting words in text. ACM Computing Surveys, pages
377?439.
V.I. Levenshtein. 1966. Binary codes capable of correct-
ing deletions, insertions, and reversals. Soviet Physics
- Doklady.
Kemal Oflazer. 1996. Error-tolerant finite-state recog-
nition with applications to morphological analysis
and spelling correction. Computational Linguistics,
22:73?89.
Christoph Ringlstetter, Max Hadersbeck, Klaus U.
Schulz, and Stoyan Mihov. 2007. Text correction
using domain dependent bigram models from web
crawls. In Proceedings of the International Joint Con-
ference on Artificial Intelligence (IJCAI-2007) Work-
shop on Analytics for Noisy Unstructured Text Data.
Emmanuel Roche and Yves Schabes. 1995. Determinis-
tic part-of-speech tagging with finite-state transducers.
Computational Linguistics, 2:227253.
Emmanuel Roche and Yves Shabes. 1997. Finite-state
language processing. Cambridge, MA, USA: MIT
Press.
Klaus Schulz and Stoyan Mihov. 2002. Fast string cor-
rection with levenshtein-automata. International Jour-
nal of Document Analysis and Recognition (IJDAR),
5:67?85.
R.A. Wagner and M.Fisher. 1974. The string-to-string
correction problem. Journal of the ACM.
Justin Zobel and Philip Dart. 1995. Finding approximate
matches in large lexicons. Software Practice and Ex-
perience, 25:331?345.
918
Language Model Based Arabic Word Segmentation 
 
Young-Suk Lee     Kishore Papineni      Salim Roukos 
IBM T. J. Watson Research Center 
Yorktown Heights, NY 10598 
 
Ossama Emam    Hany Hassan 
IBM Cairo Technology Development Center 
P.O.Box 166, El-Ahram, Giza, Egypt
  
Abstract 
 
We approximate Arabic?s rich 
morphology by a model that a word 
consists of a sequence of morphemes in 
the pattern prefix*-stem-suffix* (* 
denotes zero or more occurrences of a 
morpheme). Our method is seeded by a 
small manually segmented Arabic corpus 
and uses it to bootstrap an unsupervised 
algorithm to build the Arabic word 
segmenter from a large unsegmented 
Arabic corpus. The algorithm uses a 
trigram language model to determine the 
most probable morpheme sequence for a 
given input. The language model is 
initially estimated from a small manually 
segmented corpus of about 110,000 
words. To improve the segmentation 
accuracy, we use an unsupervised 
algorithm for automatically acquiring 
new stems from a 155 million word 
unsegmented corpus, and re-estimate the 
model parameters with the expanded 
vocabulary and training corpus. The 
resulting Arabic word segmentation 
system achieves around 97% exact match 
accuracy on a test corpus containing 
28,449 word tokens. We believe this is a 
state-of-the-art performance and the 
algorithm can be used for many highly 
inflected languages provided that one can 
create a small manually segmented 
corpus of the language of interest.  
 
 
 
1   Introduction 
 
Morphologically rich languages like       
Arabic present significant challenges to many 
natural language processing applications 
because a word often conveys complex 
meanings decomposable into several 
morphemes (i.e. prefix, stem, suffix).   By 
segmenting words into morphemes, we can 
improve the performance of natural language 
systems including machine translation (Brown 
et al 1993) and information retrieval (Franz, 
M. and McCarley, S. 2002). In this paper, we 
present a general word segmentation algorithm 
for handling inflectional morphology capable 
of segmenting a word into a prefix*-stem-
suffix* sequence, using a small manually 
segmented corpus and a table of 
prefixes/suffixes of the language. We do not 
address Arabic infix morphology where many 
stems correspond to the same root with various 
infix variations; we treat all the stems of a 
common root as separate atomic units. The use 
of a stem as a morpheme (unit of meaning) is 
better suited than the use of a root for the 
applications we are considering in information 
retrieval and machine translation (e.g. different 
stems of the same root translate into different 
English words.) Examples of Arabic words and 
their segmentation into prefix*-stem-suffix* are 
given in Table 1, where '#' indicates a 
morpheme being a prefix, and '+' a suffix.1 As  
                                                          
1 Arabic is presented in both native and Buckwalter 
transliterated Arabic whenever possible. All native 
Arabic is to be read from right-to-left, and transliterated 
Arabic is to be read from left-to-right. The convention of 
shown in Table 1, a word may include multiple 
prefixes, as in   ???? (l: for, Al: the),  or multiple 
suffixes, as in   ????? (t: feminine singular, h: his).  
A word may also consist only of a stem, as in 
 ?????  (AlY, to/towards). 
  The algorithm implementation involves (i) 
language model training on a morpheme-
segmented corpus, (ii) segmentation of input 
text into a sequence of morphemes using the 
language model parameters, and (iii) 
unsupervised acquisition of new stems from a 
large unsegmented corpus. The only linguistic 
resources required include  a small manually 
segmented corpus ranging from 20,000 words 
to 100,000 words, a table of prefixes and 
suffixes of the language and  a large 
unsegmented corpus.   
  In Section 2, we discuss related work. In 
Section 3, we describe the segmentation 
algorithm.  In Section 4, we discuss the  
unsupervised algorithm for new stem 
acquisition. In Section 5, we present 
experimental results. In Section 6, we 
summarize the paper. 
 
2   Related Work 
 
Our work adopts major components of the 
algorithm from (Luo & Roukos 1996): 
language model (LM) parameter estimation 
from a segmented corpus and input 
segmentation on the basis of LM probabilities.  
However, our work diverges from their work 
in two crucial respects: (i) new technique of 
computing all possible segmentations of a 
word into prefix*-stem-suffix* for decoding, 
and  (ii) unsupervised algorithm for new stem 
acquisition based on a stem candidate's 
similarity to stems occurring in the training 
corpus. 
  (Darwish 2002) presents a  supervised 
technique which identifies the root of an 
Arabic word by stripping away the prefix and 
the suffix of the word on the basis of manually 
acquired dictionary of word-root pairs and the 
likelihood that a prefix and a suffix would 
occur with the template from which the root is 
derived. He reports 92.7% segmentation 
accuracy on a 9,606 word evaluation corpus.  
His technique pre-supposes at most one prefix 
and one suffix per stem regardless of the actual 
number and meanings of prefixes/suffixes 
associated with the stem.  (Beesley 1996)  
presents a finite-state morphological analyzer 
for Arabic, which displays the root, pattern, 
and prefixes/suffixes. The analyses are based 
on manually acquired lexicons and rules.  
Although his analyzer is comprehensive in the 
types of knowledge it presents, it has been 
criticized for their extensive development time 
and lack of robustness, cf. (Darwish 2002). 
                                                                                    
marking a prefix with '#" and a suffix with '+' will be 
adopted throughout the paper. 
  (Yarowsky and Wicentowsky 2000) 
presents a minimally supervised morphological 
analysis with a  performance of over 99.2% 
accuracy for the 3,888 past-tense test cases in 
English. The core algorithm lies in the 
estimation of a probabilistic alignment 
between inflected forms and root forms. The 
probability estimation is based on the lemma 
alignment by frequency ratio similarity among 
different inflectional forms derived from the 
same lemma, given a table of inflectional 
parts-of-speech, a list of the canonical suffixes 
for each part of speech, and a list of the 
candidate noun, verb and adjective roots of the 
language.  Their algorithm does not handle 
multiple affixes per word. 
  (Goldsmith 2000) presents an unsupervised 
technique based on the expectation-
maximization algorithm and minimum 
description length to segment exactly one 
suffix per word, resulting in an F-score of 81.8 
for suffix identification in English according to 
(Schone and Jurafsky 2001). (Schone and 
Jurafsky 2001) proposes an unsupervised 
algorithm capable of automatically inducing 
the morphology of inflectional languages using 
only text corpora. Their algorithm combines 
cues from orthography, semantics, and 
contextual information to induce 
morphological relationships in German, Dutch, 
and English, among others. They report F-
scores between 85 and 93 for suffix analyses 
and between 78 and 85 for circumfix analyses 
in these languages. Although their algorithm 
captures prefix-suffix combinations or 
circumfixes, it does not handle the multiple 
affixes per word we observe in Arabic.
 2
                Words            Prefixes                 Stems             Suffixes 
    Arabic    Translit.   Arabic  Translit.    Arabic    Translit.   Arabic   Translit. 
 ????????????? ?   AlwlAyAt  #??    Al#       ????       wlAy      ?? +    +At 
      ???????????    HyAth           ??????     HyA  ?  +? +    +t +h 
 ?????????????    llHSwl  #?#  ??     l# Al#    ??????     HSwl   
         ?????        AlY           ?????      AlY   
 Table 1  Segmentation of Arabic Words into Prefix*-Stem-Suffix* 
 
3  Morpheme Segmentation 
 
3.1 Trigram Language Model 
 
Given an Arabic sentence, we use a trigram 
language model on morphemes to segment it 
into a sequence of morphemes {m1, m2, ?,mn}. 
The input to the morpheme segmenter is a 
sequence of Arabic tokens ? we use a 
tokenizer that looks only at white space and 
other punctuation, e.g. quotation marks, 
parentheses, period, comma, etc.  A sample of 
a manually segmented corpus is given below2. 
Here multiple occurrences of prefixes and 
suffixes per word are marked with an 
underline. 
 
???? # ??? ??????? ???? ?? ?? ??# ?
??? # ???? ?? # ? ??+??? ?? ???? # ??
? ?????? ??? ?+???? ??? ???? # ?? #
 ??? ???+? +? ???? +???? ?? ???  #
??? #?# ??? # ????? ?# ?????? ?? ?? 
?? ??+???? # ? ??????# ??? ???? ? #
 ? ??? ?? ???? ???? ?????? +????? .
????? ?? ?????? #  ?? ???? ??#?# ?# ?
??????? ??????? ????? ???? # ??
??? # ???? ??? ??# ??????? ?? ??
 ?? ?+?? + ??? ???? ??? #? # ????? 
 ?? ?????????+???? ???? 
 
w# kAn AyrfAyn Al*y Hl fy Al# mrkz Al# 
Awl fy jA}z +p Al# nmsA Al# EAm Al# 
mADy Ely syAr +p fyrAry $Er b# AlAm fy 
bTn +h ADTr +t +h Aly Al# AnsHAb mn Al#  
tjArb w# hw s# y# Ewd Aly lndn l# AjrA' Al# 
fHwS +At Al# Drwry +p Hsb mA A$Ar fryq  
 
                                                          
2 A manually segmented Arabic corpus containing about 
140K word tokens has been provided by LDC 
(http://www.ldc.upenn.edu). We divided this corpus into 
training and the development test sets as described in 
Section 5. 
 
 
jAgwAr. w# s# y# Hl sA}q Al# tjArb fy 
jAgwAr Al# brAzyly lwsyAnw bwrty mkAn 
AyrfAyn fy Al# sbAq gdA Al# AHd Al*y s# 
y# kwn Awly xTw +At +h fy EAlm sbAq +At 
AlfwrmwlA 
 
Many instances of prefixes and suffixes in 
Arabic are meaning bearing and correspond to 
a word in English such as pronouns and 
prepositions.  Therefore, we choose a 
segmentation into multiple prefixes and 
suffixes. Segmentation into one prefix  and one 
suffix per word, cf. (Darwish 2002), is not very 
useful for applications like statistical machine 
translation, (Brown et al 1993), for which an 
accurate word-to-word alignment between the 
source and the target languages is critical for 
high quality translations. 
  The trigram language model probabilities 
of morpheme sequences, p(mi|mi-1, mi-2), are 
estimated from the morpheme-segmented 
corpus. At token boundaries, the morphemes 
from previous tokens constitute the histories of 
the current morpheme in the trigram language 
model.  The trigram model is smoothed using 
deleted interpolation with the bigram and 
unigram models, (Jelinek 1997), as in (1): 
 
(1) p(m3 | m1 ,m2) =  ?3 p(m3 |m1 ,m2) + ?2 
p(m3 |m2) + ?3 p(m3), where ?1+?2 +?3 = 1. 
 
  A small morpheme-segmented corpus 
results in a relatively high out of vocabulary 
rate for the stems. We describe below an 
unsupervised acquisition of new stems from a 
large unsegmented Arabic corpus.  However, 
we first describe the segmentation algorithm.   
 
3.2  Decoder for Morpheme Segmentation 
 
 3
We take the unit of decoding to be a sentence 
that has been tokenized using white space and 
punctuation.  The task of a decoder is to find 
the morpheme sequence which maximizes the 
trigram probability of the input sentence, as in 
(2): 
 
(2)  SEGMENTATIONbest = Argmax IIi=1, N 
p(mi|mi-1mi-2), N = number of morphemes in 
the input. 
 
Search algorithm for (2) is informally 
described for each word token as follows: 
 
Step 1: Compute all possible segmentations of 
the token  (to be elaborated in 3.2.1). 
Step 2: Compute the trigram language model 
score of each segmentation.  For some 
segmentations of a token, the stem may be an 
out of vocabulary item. In that case, we use an 
?UNKNOWN? class in the trigram language 
model with the model probability given by 
p(UNKNOWN|mi-1, mi-2) * UNK_Fraction, where 
UNK_Fraction is 1e-9 determined on empirical 
grounds. This allows us to segment new words 
with a high accuracy even with a relatively 
high number of unknown stems in the 
language model vocabulary, cf. experimental 
results in Tables 5 & 6. 
Step 3: Keep the top N highest scored 
segmentations. 
 
3.2.1  Possible Segmentations of  a Word 
 
Possible segmentations of a word token are 
restricted to those derivable from a table of 
prefixes and suffixes of the language for 
decoder speed-up and improved accuracy.   
  Table 2 shows examples of atomic (e.g. ??, 
??) and multi-component (e.g.  ??????     ,???????) 
prefixes and suffixes, along with their 
component morphemes in native Arabic.3 
 
                                                          
3 We have acquired the prefix/suffix table from a 110K 
word manually segmented LDC corpus (51 prefixes & 72 
suffixes) and from IBM-Egypt (additional 14 prefixes & 
122 suffixes). The performance improvement by the 
additional prefix/suffix list ranges from 0.07% to 0.54% 
according to the manually segmented training corpus 
size. The smaller the manually segmented corpus size is, 
the bigger the performance improvement by adding 
additional prefix/suffix list is. 
         Prefixes          Suffixes 
      ??          ??       ??# ??+ 
    ??????        ?#  ??# ?????+   ???     ??+ 
 ???????     ?#  ?#   ??# ?????+?? + ??  
          Table 2  Prefix/Suffix Table 
 
Each token is assumed to have the structure 
prefix*-stem-suffix*, and is compared against 
the prefix/suffix table for segmentation. Given 
a word token, (i) identify all of the matching 
prefixes and suffixes from the table, (ii) further 
segment each matching prefix/suffix at each 
character position, and (iii) enumerate all 
prefix*-stem-suffix* sequences derivable from 
(i) and (ii).  
  Table 3 shows all of its possible 
segmentations of the token ???????  
(wAkrrhA; 'and I repeat it'),4 where ? indicates 
the null prefix/suffix and the Seg Score is the 
language model probabilities of each 
segmentation S1 ... S12. For this token, there 
are two matching prefixes #?(w#) and 
#??(wA#) from the prefix table, and two 
matching suffixes ?+(+A) and ??+(+hA)  
from the suffix table. S1, S2, & S3 are the 
segmentations given the null prefix ? and 
suffixes ?, +A, +hA. S4, S5, & S6 are the 
segmentations given the prefix w# and suffixes 
?, +A, +hA. S7, S8, & S9 are the 
segmentations given the prefix wA# and 
suffixes ?, +A, +hA. S10, S11, & S12 are the 
segmentations given the prefix sequence w# 
A# derived from the prefix wA# and  suffixes 
?, +A, +hA. As illustrated by S12, derivation 
of sub-segmentations of the matching 
prefixes/suffixes enables the system to identify 
possible segmentations which would have been 
missed otherwise. In this case, segmentation 
including the derived prefix sequence               
??+??? # ?# ? (w# A# krr +hA) happens to 
be the correct one.  
 
3.2.2. Prefix-Suffix Filter 
 
While the number of possible segmentations is 
maximized by sub-segmenting matching 
                                                          
4 A sentence in which the token occurs is as follows:  ?????
??????? ???????? ???? ?? ????? ????? ????? ?? ???????? ??????? 
(qlthA wAkrrhA fAlm$klp lyst fy AlfnT AlxAm wAnmA fy 
Alm$tqAt AlnfTyp.) 
 4
prefixes and suffixes, some of illegitimate sub-
segmentations are filtered out on the basis of 
the knowledge specific to the manually 
segmented corpus. For instance, sub-
segmentation of the suffix hA into +h +A is 
ruled out because there is no suffix sequence 
+h +A in the training corpus. Likewise, sub-
segmentation of the prefix Al into A# l# is 
filtered out. Filtering out improbable 
prefix/suffix sequences improves the 
segmentation accuracy, as shown in Table 5. 
 
 Prefix Stem Suffix Seg Scores 
S1 ? wAkrrhA ? 2.6071e-05 
S2 ? wAkrrh +A 1.36561e-06 
S3 ? wAkrr +hA 9.45933e-07 
S4 w# AkrrhA ? 2.72648e-06 
S5 w# Akrrh +A 5.64843e-07 
S6 w# Akrr +hA 4.52229e-05 
S7 wA# krrhA ? 7.58256e-10 
S8 wA# krrh +A 5.09988e-11 
S9 wA# krr +hA 1.91774e-08 
S10 w# A# krrhA ? 7.69038e-07 
S11 w# A# krrh +A 1.82663e-07 
S12 w# A# krr +hA 0.000944511 
Table 3 Possible Segmentations of  
??????? (wAkrrhA) 
 
4  Unsupervised Acquisition  of  New  
Stems 
 
Once the seed segmenter is developed on the 
basis of a manually segmented corpus,  the 
performance may be improved by iteratively 
expanding the stem vocabulary  and retraining 
the language model on a large automatically 
segmented Arabic corpus.  
  Given a small manually segmented corpus 
and a large unsegmented corpus, segmenter 
development proceeds as follows. 
 
Initialization: Develop the seed segmenter 
Segmenter0 trained on the manually segmented 
corpus Corpus0, using the language model 
vocabulary, Vocab0, acquired from Corpus0.  
Iteration: For i = 1 to N, N = the number of 
partitions of the unsegmented corpus 
 i. Use Segmenteri-1 to segment Corpusi. 
 ii.  Acquire new stems from the newly 
segmented Corpusi. Add the new stems to 
Vocabi-1, creating an expanded vocabulary 
Vocabi.  
 iii. Develop Segmenteri trained on Corpus0 
through Corpusi with Vocabi.   
Optimal Performance Identification:  
Identify the Corpusi and Vocabi, which result 
in the best performance, i.e. system training 
with Corpusi+1 and Vocabi+1 does not improve 
the performance any more. 
  Unsupervised acquisition of new stems 
from an automatically segmented new corpus 
is a three-step process: (i)  select new stem 
candidates on the basis of a frequency 
threshold, (ii) filter out new stem candidates  
containing a sub-string with a high likelihood 
of being a prefix, suffix, or prefix-suffix. The 
likelihood of a sub-string being a prefix, suffix, 
and prefix-suffix of a token is computed as in  
(5) to (7), (iii) further filter out new stem 
candidates on the basis of contextual 
information, as in (8). 
 
(5)  Pscore = number of tokens with prefix P / 
number of tokens starting with sub-string P 
(6)  Sscore = number of tokens with suffix S / 
number of tokens ending with sub-string S 
(7)  PSscore = number of tokens with prefix P 
and suffix S / number of tokens starting with 
sub-string P and ending with  sub-string S 
 
Stem candidates containing a sub-string with a 
high prefix, suffix, or prefix-suffix likelihood 
are filtered out. Example sub-strings with the 
prefix, suffix, prefix-suffix likelihood 0.85 or 
higher in a 110K word manually segmented 
corpus are given in Table 4. If a token starts 
with the sub-string ???  (sn), and end with  ???  
(hA), the sub-string's likelihood of being the 
prefix-suffix of the token is 1.  If a token starts 
with the sub-string  ????  (ll), the sub-string's 
likelihood of being the prefix of the token is 
0.945, etc. 
 
        Arabic Transliteration      Score 
 ??? +  stem # ???     sn# stem+hA      1.0 
     ?+ stem # ?????  Al# stem+p      0.984        
         stem # ????   ll# stem      0.945 
  ??+  stem         stem+At      0.889 
    Table 4 Prefix/Suffix Likelihood Score 
 
 5
(8) Contextual Filter: (i) Filter out stems co-
occurring with prefixes/suffixes not present in 
the training corpus. (ii) Filter out stems whose 
prefix/suffix distributions are highly 
disproportionate to those seen in the training 
corpus.  
   According to (8), if a stem is followed by 
a potential suffix +m, not present in the 
training corpus, then it is filtered out as an 
illegitimate stem. In addition, if a stem is 
preceded by a prefix and/or followed by a 
suffix with a significantly higher proportion 
than that observed in the training corpus, it is 
filtered out. For instance, the probability for 
the suffix +A to follow a stem is less than 50% 
in the training corpus regardless of the stem 
properties, and therefore, if a candidate stem is 
followed by +A with the probability of over 
70%, e.g. mAnyl +A, then it is filtered out as 
an illegitimate stem. 
 
5  Performance Evaluations 
 
We present experimental results illustrating the 
impact of three factors on segmentation error 
rate: (i) the base algorithm, i.e. language model 
training and decoding, (ii) language model 
vocabulary and training corpus size, and (iii) 
manually segmented training corpus size.  
Segmentation error rate is defined in (9). 
 
(9)  (number of incorrectly segmented tokens /  
       total number of tokens)  x  100 
 
  Evaluations have been performed on a 
development test corpus containing 28,449 
word tokens.  The test set is extracted from 
20001115_AFP_ARB.0060.xml.txt through 
20001115_AFP_ARB.0236.xml.txt of the 
LDC Arabic Treebank: Part 1 v 2.0 Corpus. 
Impact of the core algorithm and the 
unsupervised stem acquisition has been 
measured on segmenters developed from 4 
different sizes of manually segmented seed 
corpora: 10K, 20K, 40K, and 110K words.    
  The experimental results are shown in 
Table 5. The baseline performances are 
obtained by assigning each token the most 
frequently occurring segmentation in the 
manually segmented training corpus. The 
column headed by '3-gram LM' indicates the 
impact of the segmenter using only trigram 
language model probabilities for decoding. 
Regardless of the manually segmented training 
corpus size, use of  trigram language model 
probabilities reduces the word error rate of the 
corresponding baseline by approximately 50%. 
The column headed by '3-gram LM + PS 
Filter' indicates the impact of the core 
algorithm plus Prefix-Suffix Filter discussed in 
Section 3.2.2. Prefix-Suffix Filter reduces the 
word error rate ranging from 7.4% for the 
smallest (10K word) manually segmented 
corpus to 21.8% for the largest (110K word) 
manually segmented corpus ?- around 1% 
absolute reduction for all segmenters. The 
column headed by '3-gram LM + PS Filter + 
New Stems' shows the impact of unsupervised 
stem acquisition from a 155 million word 
Arabic corpus.  Word error rate reduction due 
to the unsupervised stem acquisition is 38% for 
the segmenter developed from the 10K word 
manually segmented corpus and 32% for the 
segmenter developed from 110K word 
manually segmented corpus. 
  Language model vocabulary size (LM VOC 
Size) and the unknown stem ratio (OOV ratio) 
of various segmenters is given in Table 6. For 
unsupervised stem acquisition, we have set the 
frequency threshold at 10 for every 10-15 
million word corpus, i.e. any new morphemes 
occurring more than 10 times in a 10-15 
million word corpus are considered to be new 
stem candidates. Prefix, suffix, prefix-suffix 
likelihood score to further filter out illegitimate 
stem candidates was set at 0.5 for the 
segmenters developed from 10K, 20K, and 
40K manually segmented corpora, whereas it 
was set at 0.85 for the segmenters developed 
from a 110K manually segmented corpus.  
Both the frequency threshold and the optimal 
prefix, suffix, prefix-suffix likelihood scores 
were determined on empirical grounds. 
Contextual Filter stated in (8) has been applied 
only to the segmenter developed from 110K 
manually segmented training corpus.5 
Comparison of Tables 5 and 6 indicates a high 
correlation between the segmentation error rate 
and the unknown stem ratio.  
                                                          
5 Without the Contextual Filter, the  error rate of the 
same segmenter is 3.1%. 
 6
   
 
Manually Segmented 
Training Corpus Size 
      Baseline  3-gram LM  3-gram LM +  
PS Filter 
3-gram LM + PS 
Filter + New Stems 
        10K Words    26.0%        14.7%            13.6%          8.5% 
        20K Words       19.7%        9.1%            8.0%          5.9% 
        40K Words        14.3%        7.6%            6.5%          5.1% 
      110K Words        11.0%        5.5%            4.3%           2.9% 
Table 5 Impact of Core Algorithm and LM Vocabulary Size on Segmentation Error Rate 
 
                       3-gram LM  3-gram LM + PS Filter + New Stems Manually Segmented 
Training Corpus Size     LM VOC Size      OOV Ratio    LM VOC Size      OOV Ratio 
         10K Words           2,496          20.4%          22,964           7.8% 
         20K Words           4,111          11.4%          25,237           5.3% 
         40K Words           5,531            9.0%          21,156           4.7% 
       110K Words           8,196            5.8%          25,306           1.9% 
             Table 6 Language Model Vocabulary Size and Out of Vocabulary Ratio 
  
                                  3-gram LM + PS Filter + New Stems Manually Segmented 
Training Corpus Size   Unknown Stem          Alywm     Other Errors  Total # of Errors 
         10 K Words    1,844  (76.9%)        98 (4.1%)     455 (19.0%)          2,397 
         20 K Words    1,174  (71.1%)        82 (5.0%)     395 (23.9%)          1,651 
         40 K Words    1,005  (69.9%)        81 (5.6%)     351 (24.4%)          1,437 
       110 K Words       333  (39.6%)        82 (9.8%)     426 (50.7%)             841 
Table 7 Segmentation Error Analyses
  
Table 7 gives the error analyses of four 
segmenters according to three factors: (i) 
errors due to unknown stems, (ii) errors 
involving  ?????????? (Alywm), and (iii) errors due to 
other factors. Interestingly, the segmenter 
developed from a 110K manually segmented 
corpus has the lowest percentage of ?unknown 
stem? errors at 39.6% indicating that our 
unsupervised acquisition of new stems is 
working well, as well as suggesting to use a 
larger unsegmented corpus for unsupervised 
stem acquisition.  
    ?????????? (Alywm) should be segmented 
differently depending on its part-of-speech to 
capture the semantic ambiguities. If it is an 
adverb or a proper noun, it is segmented as 
 ?????????? 'today/Al-Youm', whereas if it is a noun, 
it is segmented as ?? # ??????   'the day.'  Proper 
segmentation of   ?????????? primarily requires its 
part-of-speech information, and cannot be 
easily handled by morpheme trigram models 
alone. 
  Other errors include over-segmentation of  
foreign words such as  ???????????????  (bwtyn) as  ?# 
 ??????????  and  ?????????????  (lytr)  'litre' as ? # ?# ????? .  
These errors are attributed to the segmentation 
ambiguities of these tokens:  ??????????????? is 
ambiguous between ' ??????????????? (Putin)' and '?# 
 ?????????? (by aorta)'.   ?????????????  is ambiguous 
between ' ????????????? (litre)' and ' ? # ?# ?????  (for him 
to harm)'. These errors may also be corrected 
by incorporating part-of-speech information 
for disambiguation. 
  To address the segmentation ambiguity 
problem, as illustrated by ' ??????????????? (Putin)' vs. 
' ? # ??????????  (by aorta)', we have developed a 
joint model for segmentation and part-of-
speech tagging for which the best 
segmentation of an input sentence is obtained 
according to the formula (10), where ti is the 
part-of-speech of morpheme mi, and N is the 
number of morphemes in the input sentence. 
 
(10) SEGMENTATIONbest = Argmax ?i=1,N  
p(mi|mi-1 mi-2) p(ti|ti-1 ti-2) p(mi|ti) 
 
By using the joint model, the segmentation 
word error rate of the best performing 
segmenter has been reduced by about 10% 
 7
from 2.9% (cf. the last column of Table 5) to 
2.6%. 
   
5  Summary and Future Work 
 
We have presented a robust word segmentation 
algorithm which segments a word into a 
prefix*-stem-suffix* sequence, along with 
experimental results. Our Arabic word 
segmentation system implementing the 
algorithm achieves around 97% segmentation 
accuracy on a development test corpus 
containing 28,449 word tokens. Since the 
algorithm can identify any number of prefixes 
and suffixes of a given token, it is generally 
applicable to various language families 
including agglutinative languages (Korean, 
Turkish, Finnish), highly inflected languages 
(Russian, Czech) as well as semitic languages 
(Arabic, Hebrew). 
   Our future work includes (i) application 
of the current technique to other highly 
inflected languages, (ii) application of the 
unsupervised stem acquisition technique on 
about 1 billion word unsegmented Arabic 
corpus, and (iii) adoption of a novel 
morphological analysis technique to handle 
irregular morphology, as realized in Arabic 
broken plurals  ????????? (ktAb) 'book' vs.  ???????? 
(ktb) 'books'. 
 
Acknowledgment 
 
This work was partially supported by the 
Defense Advanced Research Projects Agency 
and monitored by SPAWAR under contract No. 
N66001-99-2-8916. The views and findings 
contained in this material are those of the 
authors and do not necessarily reflect the 
position of policy of the Government and no 
official endorsement should be inferred. We 
would like to thank Martin Franz for discussions 
on language model building, and his help with 
the use of ViaVoice language model toolkit. 
 
References 
 
Beesley, K. 1996. Arabic Finite-State 
 Morphological Analysis and Generation. 
 Proceedings of COLING-96, pages 89?  94. 
Brown, P., Della Pietra, S., Della Pietra, V., 
 and Mercer, R. 1993. The mathematics  of 
 statistical machine translation:  Parameter 
 Estimation. Computational  Linguistics, 
 19(2): 263?311. 
Darwish, K. 2002. Building a Shallow  Arabic 
 Morphological Analyzer in  One  Day. 
 Proceedings of the  Workshop on 
 Computational  Approaches to Semitic 
 Languages,  pages 47?54.  
Franz, M. and McCarley, S. 2002. Arabic 
 Information Retrieval at IBM.  Proceedings 
 of TREC 2002, pages 402? 405. 
Goldsmith, J. 2000. Unsupervised  learning 
 of  the morphology of a natural  language.   
 Computational Linguistics, 27(1). 
Jelinek, F. 1997. Statistical Methods for 
 Speech Recognition. The MIT Press. 
Luo, X. and Roukos, S. 1996. An Iterative 
 Algorithm to Build Chinese Language 
 Models. Proceedings of ACL-96, pages 
 139?143. 
Schone, P. and Jurafsky, D. 2001. 
 Knowledge-Free Induction of  Inflectional 
 Morphologies. Proceedings  of  North 
 American Chapter of  Association for 
 Computational  Linguistics. 
Yarowsky, D. and Wicentowski, R. 2000. 
 Minimally supervised morphological 
 analysis by multimodal alignment. 
 Proceedings of ACL-2000, pages 207? 216. 
Yarowsky, D, Ngai G. and Wicentowski, R. 
 2001. Inducting Multilingual Text  Analysis 
 Tools via Robust Projection  across Aligned 
 Corpora. Proceedings of  HLT 2001, pages 
 161?168. 
 
 
 
 8
Proceedings of the ACL Workshop on Computational Approaches to Semitic Languages, pages 25?30,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Examining the Effect of Improved Context Sensitive Morphology on 
Arabic Information Retrieval 
 
 
 
Kareem Darwish Hany Hassan and Ossama Emam 
Dept. of Information Engineering & Technology IBM Technology Development Center 
German University in Cairo P.O. Box 166 
5th District, New Cairo, Cairo, Egypt El-Ahram, Giza, Egypt 
 and  
IBM Technology Development Center 
{hanyh,emam}@eg.ibm.com 
P.O. Box 166, El-Ahram, Giza, Egypt  
kareem@darwish.org  
 
 
 
 
Abstract 
This paper explores the effect of 
improved morphological analysis, 
particularly context sensitive morphology, 
on monolingual Arabic Information 
Retrieval (IR).  It also compares the effect 
of context sensitive morphology to non-
context sensitive morphology.  The results 
show that better coverage and improved 
correctness have a dramatic effect on IR 
effectiveness and that context sensitive 
morphology further improves retrieval 
effectiveness, but the improvement is not 
statistically significant. Furthermore, the 
improvement obtained by the use of 
context sensitive morphology over the use 
of light stemming was not significantly 
significant. 
1 Introduction 
Due to the morphological complexity of the Arabic 
language, much research has focused on the effect 
of morphology on Arabic Information Retrieval 
(IR).  The goal of morphology in IR is to conflate 
words of similar or related meanings.  Several 
early studies suggested that indexing Arabic text 
using roots significantly increases retrieval 
effectiveness over the use of words or stems [1, 3, 
11].  However, all the studies used small test 
collections of only hundreds of documents and the 
morphology in many of the studies was done 
manually.   
Performing morphological analysis for Arabic IR 
using existing Arabic morphological analyzers, 
most of which use finite state transducers [4, 12, 
13], is problematic for two reasons.  First, they 
were designed to produce as many analyses as 
possible without indicating which analysis is most 
likely.  This property of the analyzers complicates 
retrieval, because it introduces ambiguity in the 
indexing phase as well as the search phase of 
retrieval.  Second, the use of finite state 
transducers inherently limits coverage, which the 
number of words that the analyzer can analyze, to 
the cases programmed into the transducers.  
Darwish attempted to solve this problem by 
developing a statistical morphological analyzer for 
Arabic called Sebawai that attempts to rank 
possible analyses to pick the most likely one [7].  
He concluded that even with ranked analysis, 
morphological analysis did not yield statistically 
significant improvement over words in IR.  A later 
study by Aljlayl et al on a large Arabic collection 
of 383,872 documents suggested that lightly 
stemmed words, where only common prefixes and 
suffixes are stripped from them, were perhaps 
better index term for Arabic [2].  Similar studies by 
Darwish [8] and Larkey [14] also suggested that 
light stemming is indeed superior to morphological 
analysis in the context of IR.   
25
However, the shortcomings of morphology might 
be attributed to issues of coverage and correctness.  
Concerning coverage, analyzers typically fail to 
analyze Arabized or transliterated words, which 
may have prefixes and suffixes attached to them 
and are typically valuable in IR.  As for 
correctness, the presence (or absence) of a prefix 
or suffix may significantly alter the analysis of a 
word.  For example, for the word ?Alksyr? is 
unambiguously analyzed to the root ?ksr? and stem 
?ksyr.?  However, removing the prefix ?Al? 
introduces an additional analysis, namely to the 
root ?syr? and the stem ?syr.?  Perhaps such 
ambiguity can be reduced by using the context in 
which the word is mentioned.  For example, for the 
word ?ksyr? in the sentence ?sAr ksyr? (and he 
walked like), the letter ?k? is likely to be a prefix. 
The problem of coverage is practically eliminated 
by light stemming.  However, light stemming 
yields greater consistency without regard to 
correctness.  Although consistency is more 
important for IR applications than linguistic 
correctness, perhaps improved correctness would 
naturally yield great consistency.  Lee et al [15] 
adopted a trigram language model (LM) trained on 
a portion of the manually segmented LDC Arabic 
Treebank in developing an Arabic morphology 
system, which attempts to improve the coverage 
and linguistic correctness over existing statistical 
analyzers such as Sebawai [15].  The analyzer of 
Lee et al will be henceforth referred to as the 
IBM-LM analyzer.  IBM-LM's analyzer combined 
the trigram LM (to analyze a word within its 
context in the sentence) with a prefix-suffix filter 
(to eliminate illegal prefix suffix combinations, 
hence improving correctness) and unsupervised 
stem acquisition (to improve coverage).  Lee et al 
report a 2.9% error rate in analysis compared to 
7.3% error reported by Darwish for Sebawai [7]. 
This paper evaluates the IBM-LM analyzer in the 
context of a monolingual Arabic IR application to 
determine if in-context morphology leads to 
improved retrieval effectiveness compared to out-
of-context analysis.  To determine the effect of 
improved analysis, particularly the use of in-
context morphology, the analyzer is used to 
produce analyses of words in isolation (with no 
context) and in-context.  Since IBM-LM only 
produces stems, Sebawai was used to produce the 
roots corresponding to the stems produced by 
IBM-LM.  Both are compared to Sebawai and light 
stemming. 
The paper will be organized as follows:  Section 2 
surveys related work; Section 3 describes the IR 
experimental setup for testing the IBM-LM 
analyzer; Section 4 presents experimental results; 
and Section 5 concludes the paper.  
2 Related Work 
Most early studies of character-coded Arabic text 
retrieval relied on relatively small test collections 
[1, 3, 9, 11].  The early studies suggested that 
roots, followed by stems, were the best index terms 
for Arabic text.  More recent studies are based on a 
single large collection (from TREC-2001/2002) [9, 
10]. The studies examined indexing using words, 
word clusters [14], terms obtained through 
morphological analysis (e.g., stems and roots [9]), 
light stemming [2, 8, 14], and character n-grams of 
various lengths [9, 16].  The effects of normalizing 
alternative characters, removal of diacritics and 
stop-word removal have also been explored [6, 
19].  These studies suggest that perhaps light 
stemming and character n-grams are the better 
index terms.   
Concerning morphology, some attempts were 
made to use statistics in conjunction with rule-
based morphology to pick the most likely analysis 
for a particular word or context.  In most of these 
approaches an Arabic word is assumed to be of the 
form prefix-stem-suffix and the stem part may or 
may not be derived from a linguistic root.  Since 
Arabic morphology is ambiguous, possible 
segmentations (i.e. possible prefix-stem-suffix 
tuples) are generated and ranked based on the 
probability of occurrence of prefixes, suffixes, 
stems, and stem template.  Such systems that use 
this methodology include RDI?s MORPHO3 [5] 
and Sebawai [7].  The number of manually crafted 
rules differs from system to system.  Further 
MORPHO3 uses a word trigram model to improve 
in-context morphology, but uses an extensive set of 
manually crafted rules.  The IBM-LM analyzer 
uses a trigram language model with a minimal set 
of manually crafted rules [15].  Like other 
statistical morphology systems, the IBM-LM 
analyzer assumes that a word is constructed as 
prefix-stem-suffix.  Given a word, the analyzer 
generates all possible segmentations by identifying 
all matching prefixes and suffixes from a table of 
26
prefixes and suffixes.  Then given the possible 
segmentations, the trigram language model score is 
computed and the most likely segmentation is 
chosen.  The analyzer was trained on a manually 
segmented Arabic corpus from LDC.  
3 Experimental Design  
IR experiments were done on the LDC 
LDC2001T55 collection, which was used in the 
Text REtrieval Conference (TREC) 2002 cross-
language track.  For brevity, the collection is 
referred to as the TREC collection.  The collection 
contains 383,872 articles from the Agence France 
Press (AFP) Arabic newswire.  Fifty topics were 
developed cooperatively by the LDC and the 
National Institute of Standards and Technology  
(NIST), and relevance judgments were developed 
at the LDC by manually judging a pool of 
documents obtained from combining the top 100 
documents from all the runs submitted by the 
participating teams to TREC?s cross-language 
track in 2002.  The number of known relevant 
documents ranges from 10 to 523, with an average 
of 118 relevant documents per topic [17].  This is 
presently the best available large Arabic 
information retrieval test collection.  The TREC 
topic descriptions include a title field that briefly 
names the topic, a description field that usually 
consists of a single sentence description, and a 
narrative field that is intended to contain any 
information that would be needed by a human 
judge to accurately assess the relevance of a 
document [10].  Queries were formed from the 
TREC topics by combining the title and 
description fields.  This is intended to model the 
sort of statement that a searcher might initially 
make when asking an intermediary, such as a 
librarian, for help with a search. 
Experiments were performed for the queries with 
the following index terms:   
? w:  words.   
? ls:  lightly stemmed words, obtained using Al-
Stem [17]1. 
? SEB-s:  stems obtained using Sebawai. 
? SEB-r:  roots obtained using Sebawai. 
                                                        
1 A slightly modified version of Leah Larkey?s Light-10 light 
stemmer [8] was also tried, but the stemmer produced very 
similar results to Al-Stem. 
? cIBM-LMS:  stems obtained using the IBM-
LM analyzer in context.  Basically, the entire 
TREC collection was processed by the 
analyzer and the prefixes and suffixes in the 
segmented output were removed. 
? cIBM-SEB-r:  roots obtained by analyzing the 
in-context stems produced by IBM-LM using 
Sebawai. 
? IBM-LMS:  stems obtained using the IBM-LM 
analyzer without any contextual information.  
Basically, all the unique words in the 
collection were analyzed one by one and the 
prefixes and suffixes in the segmented output 
were removed. 
? IBM-SEB-r:  roots obtained by analyzing the 
out-of-context stems produced by IBM-LM 
using Sebawai. 
All retrieval experiments were performed using the 
Lemur language modeling toolkit, which was 
configured to use Okapi BM-25 term weighting 
with default parameters and with and without blind 
relevance feedback (the top 20 terms from the top 
5 retrieved documents were used for blind 
relevance feedback).   To observe the effect of 
alternate indexing terms mean uninterpolated 
average precision was used as the measure of 
retrieval effectiveness.  To determine if the 
difference between results was statistically 
significant, a Wilcoxon signed-rank test, which is a 
nonparametric significance test for correlated 
samples, was used with p values less than 0.05 to 
claim significance.   
4 Results and Discussion 
Figure 1 shows a summary of the results for 
different index terms.  Tables 1 and 2 show 
statistical significance between different index 
terms using the p value of the Wilcoxon test.  
When comparing index terms obtained using IBM-
LM and Sebawai, the results clearly show that 
using better morphological analysis produces 
better retrieval effectiveness.  The dramatic 
difference in retrieval effectiveness between 
Sebawai and IBM-LM highlight the effect of errors 
in morphology that lead to inconsistency in 
analysis.  When using contextual information in 
analysis (compared to analyzing words in isolation 
? out of context) resulted in only a 3% increase in 
mean average precision when using stems (IBM-
LMS), which is a small difference compared to the 
27
effect of blind relevance feedback (about 6% 
increase) and produced mixed results when using 
roots (IBM-SEB-r).  Nonetheless, the improvement 
for stems was almost statistically significant with p 
values of 0.063 and 0.054 for the cases with and 
without blind relevance feedback.  Also 
considering that improvement in retrieval 
effectiveness resulted from changing the analysis 
for only 0.12% of the words in the collection (from 
analyzing them out of context to analyzing them in 
context)2 and that the authors of IBM-LM report 
about 2.9% error rate in morphology, perhaps 
further improvement in morphology may lead to 
further improvement in retrieval effectiveness.  
However, further improvements in morphology 
and retrieval effectiveness are likely to be difficult.  
One of difficulties associated with developing 
better morphology is the disagreement on what 
constitutes ?better? morphology.  For example, 
should ?mktb? and ?ktb? be conflated?   ?mktb? 
translates to office, while ktb translates to books.  
Both words share the common root ?ktb,? but they 
are not interchangeable in meaning or usage.  One 
                                                        
2 Approximately 7% of unique tokens had two or more differ-
ent analysis in the collection when doing in-context morphol-
ogy.  In tokens with more than one analysis, one of the 
analyses was typically used more than 98% of the time.  
would expect that increasing conflation would 
improve recall at the expense of precision and 
decreasing conflation would have the exact 
opposite effect.  It is known that IR is more 
tolerant of over-conflation than under-conflation 
[18].  This fact is apparent in the results when 
comparing roots and stems.  Even though roots 
result in greater conflation than stems, the results 
for stems and roots are almost the same.  Another 
property of IR is that IR is sensitive to consistency 
of analysis.  In the case of light stemming, 
stemming often mistakenly removes prefixes and 
suffixes leading to over conflation, for which IR is 
tolerant, but the mistakes are done in a consistent 
manner.  It is noteworthy that sense 
disambiguation has been reported to decrease 
retrieval effectiveness [18]. However, since 
improving the correctness of morphological 
analysis using contextual information is akin to 
sense disambiguation, the fact that retrieval results 
improved, though slightly, using context sensitive 
morphology is a significant result. 
In comparing the IBM-LM analyzer (in context or 
out of context) to light stemming (using Al-Stem), 
although the difference in retrieval effectiveness is 
small and not statistically significant, using the 
IBM-LM analyzer, unlike using Al-Stem, leads to 
Figure 1.  Comparing index term with and without blind relevance feedback using mean average 
precision 
0.
30
8
0.3
030.3
20
0.2
56
0.
31
8
0.3
27
0.
29
7
0.
30
0
0.
21
7
0.
21
7
0.
28
7
0.
26
3
0.3
19
0.
33
2
0.
26
7
0.
27
8
0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
0.40
w ls SEB-s SEB-r IBM-
LMS
IBM-
SEB-r
cIBM-
LMS
cIBM-
SEB-r
Index Terms
no feedback
with feedback
28
statistically significant improvement over using 
words.  Therefore there is some advantage, though 
only a small one, to using statistical analysis over 
using light stemming.  The major drawback to 
morphological analysis (specially in-context 
analysis) is that it requires considerably more 
computing time than light stemming3. 
5 Conclusion 
The paper investigated the effect of improved 
morphological analysis, especially context 
sensitive morphology, in Arabic IR applications 
compared to other statistical morphological 
analyzers and light stemming.  The results show 
that improving morphology has a dramatic effect 
on IR effectiveness and that context sensitive 
morphology slightly improved Arabic IR over non-
context sensitive morphology, increasing IR 
                                                        
3 The processing of the TREC collection using the in-context 
IBM-LM required 16 hours on a 2.4 GHz Pentium 4 machine 
with 1 Gigabyte of RAM compared to 10 minutes to perform 
light stemming.  
effectiveness by approximately 3%.  The 
improvement is almost statistically significant.  
Developing better morphology could lead to 
greater retrieval effectiveness, but improving 
analyzers is likely to be difficult and would require 
careful determination of the proper level of 
conflation.  In overcoming some of the difficulties 
associated with obtaining ?better? morphology (or 
more fundamentally the proper level of word 
conflation), adaptive morphology done on a per 
query term basis or user feedback might prove 
valuable.  Also, the scores that were used to rank 
the possible analyses in a statistical morphological 
analyzer may prove useful in further improving 
retrieval.  Other IR techniques, such as improved 
blind relevance feedback or combination of 
evidence approaches, can also improve 
monolingual Arabic retrieval. 
Perhaps improved morphology is particularly 
beneficial for other IR applications such as cross-
language IR, in which ascertaining proper 
translation of words is particularly important, and 
ls SEB-s SEB-r 
IBM-
LMS 
IBM-
SEB-r 
cIBM-
LMS 
cIBM-
SEB-r 
 
0.055 0.475 0.671 0.038 0.027 0.019 0.049 w 
 0.004 0.023 0.560 0.359 0.946 0.505 ls 
  0.633 0.005 0.001 0.001 0.012 SEB-s 
   0.039 0.007 0.020 0.064 SEB-r 
    0.0968 0.063 0.758 
IBM-
LMS 
     0.396 0.090 
IBM-
SEB-r 
      0.001 
cIBM-
LMS 
Table 1. Wilcoxon p values (shaded=significant) , with blind  relevance feedback. 
ls SEB-s SEB-r 
IBM-
LMS 
IBM-
SEB-r 
cIBM-
LMS 
cIBM-
SEB-r 
 
0.261 0.035 0.065 0.047 0.135 0.011 0.016 w 
 0.000 0.000 0.968 0.757 0.515 0.728 ls 
  0.269 0.000 0.000 0.000 0.000 SEB-s 
   0.000 0.000 0.000 0.000 SEB-r 
    0.732 0.054 0.584 
IBM-
LMS 
     0.284 0.512 
IBM-
SEB-r 
      0.005 
cIBM-
LMS 
Table 2. Wilcoxon p values (shaded=significant) , without blind relevanc e feedback 
 
29
in-document search term highlighting for display 
to a user.  
References  
1.  Abu-Salem, H., M. Al-Omari, and M. Evens.  
Stemming Methodologies Over Individual Query 
Words for Arabic Information Retrieval. JASIS, 
1999. 50(6): p.  524-529. 
2.  Aljlayl, M., S. Beitzel, E. Jensen, A. Chowdhury, D. 
Holmes, M. Lee, D.  Grossman, and O. Frieder. IIT 
at TREC-10. In TREC. 2001. Gaithersburg, MD. 
3.  Al-Kharashi, I. and M Evens.  Comparing Words, 
Stems, and Roots as Index Terms in an Arabic 
Information Retrieval System. JASIS, 1994. 45(8): p. 
548 - 560. 
4.  Antworth, E. PC-KIMMO: a two-level processor for 
morphological analysis. In Occasional Publications 
in Academic Computing. 1990. Dallas, TX: Summer 
Institute of Linguistics. 
5.  Ahmed, Mohamed Attia.  A Large-Scale 
Computational Processor of the Arabic Morphology, 
and Applications.  A Master?s Thesis, Faculty of 
Engineering, Cairo University, Cairo, Egypt, 2000. 
6. Chen, A. and F. Gey. Translation Term Weighting 
and Combining Translation Resources in Cross-
Language Retrieval. In TREC, 2001. Gaithersburg, 
MD. 
7. Darwish, K. Building a Shallow Morphological 
Analyzer in One Day.  ACL Workshop on 
Computational Approaches to Semitic Languages. 
2002. 
8.  Darwish, K. and D. Oard.  CLIR Experiments at 
Maryland for  TREC 2002:   Evidence Combination 
for Arabic-English Retrieval.  In TREC. 2002.  
Gaithersburg, MD. 
9. Darwish, K. and D. Oard. Term Selection for 
Searching Printed Arabic. SIGIR, 2002. Tampere, 
Finland. p. 261 - 268. 
10.  Gey, F. and D. Oard. The TREC-2001 Cross-
Language Information Retrieval Track: Searching 
Arabic Using English, French or Arabic Queries.  
TREC, 2001. Gaithersburg, MD. p. 16-23. 
11.  Hmeidi, I., G. Kanaan, and M. Evens.  Design and 
Implementation of Automatic Indexing for 
Information Retrieval with Arabic Documents. 
JASIS, 1997. 48(10):  p. 867 - 881. 
12.  Kiraz, G. Arabic Computation Morphology in the 
West.  In The 6th International  Conference and 
Exhibition on Multi-lingual Computing. 1998. 
Cambridge.   
13.  Koskenniemi, K., Two Level Morphology:  A 
General Computational Model for Word-form 
Recognition and Production. 1983, Department of 
General Linguistics, University of Helsinki. 
14. Larkey, L., L. Ballesteros, and M. Connell. 
Improving Stemming for Arabic Information 
Retrieval:  Light Stemming and Co-occurrence 
Analysis.  SIGIR 2002.  p. 275-282, 2002.  
15.  Lee, Y., K. Papineni, S. Roukos, O. Emam, and H. 
Hassan. Language Model Based Arabic Word 
Segmentation. In the Proceedings of the 41st Annual 
Meeting of the Association for Computational 
Linguistics, July 2003, Sapporo, Japan.  p. 399 - 406.  
16. Mayfield, J., P. McNamee, C. Costello, C. Piatko, 
and A. Banerjee. JHU/APL at TREC 2001:  
Experiments in Filtering and in Arabic, Video, and 
Web Retrieval. In TREC 2001. Gaithersburg, MD. p. 
322-329. 
17.  Oard, D. and F. Gey. The TREC 2002 
Arabic/English CLIR Track. In TREC 2002. 
Gaithersburg, MD. 
18.  Sanderson, M.  Word sense disambiguation and 
information  retrieval. In Proceedings  of the 17th 
ACM SIGIR Conference, p. 142-151, 1994 
19.  Xu, J., A. Fraser, and R. Weischedel. 2001 Cross-
Lingual Retrieval at BBN.  In TREC, 2001. 
Gaithersburg, MD. p. 68 - 75. 
 
30
Proceedings of the ACL Workshop on Computational Approaches to Semitic Languages, pages 87?93,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
An Integrated Approach for Arabic-English Named Entity Translation 
 
 
Hany Hassan Jeffrey Sorensen 
IBM Cairo Technology Development Center IBM T.J. Watson Research Center 
Giza - Egypt 
                Yorktown Heights 
P.O. Box 166 Al-Ahram  NY 10598 
hanyh@eg.ibm.com sorenj@us.ibm.com 
 
 
Abstract 
Translation of named entities (NEs), 
such as person names, organization names 
and location names is crucial for cross lin-
gual information retrieval, machine trans-
lation, and many other natural language 
processing applications. Newly named en-
tities are introduced on daily basis in 
newswire and this greatly complicates the 
translation task. Also, while some names 
can be translated, others must be transliter-
ated, and, still, others are mixed. In this 
paper we introduce an integrated approach 
for named entity translation deploying 
phrase-based translation, word-based trans-
lation, and transliteration modules into a 
single framework.  While Arabic based, 
the approach introduced here is a unified 
approach that can be applied to NE transla-
tion for any language pair. 
1 Introduction 
Named Entities (NEs) translation is crucial for ef-
fective cross-language information retrieval 
(CLIR) and for Machine Translation. There are 
many types of NE phrases, such as: person names, 
organization names, location names, temporal ex-
pressions, and names of events. In this paper we 
only focus on three categories of NEs: person 
names, location names and organization names, 
though the approach is, in principle, general 
enough to accommodate any entity type. 
NE identification has been an area of significant 
research interest for the last few years. NE transla-
tion, however, remains a largely unstudied prob-
lem. NEs might be phonetically transliterated (e.g. 
persons names) and might also be mixed between 
phonetic transliteration and semantic translation as 
the case with locations and organizations names. 
There are three distinct approaches that can be 
applied for NE translation, namely:  a 
transliteration approach, a word based translation 
approach and a phrase based translation approach. 
The transliteration approach depends on phonetic 
transliteration and is only appropriate for out of 
vocabulary and completely unknown words. For 
more frequently used words, transliteration does 
not provide sophisticated results. A word based 
approach depends upon traditional statistical 
machine translation techniques such as IBM 
Model1 (Brown et al, 1993) and may not always 
yield satisfactory results due to its inability to 
handle difficult many-to-many phrase translations. 
A phrase based approach could provide a good 
translation for frequently used NE phrases though 
it is inefficient for less frequent words. Each of the 
approaches has its advantages and disadvantages. 
In this paper we introduce an integrated ap-
proach for combining phrase based NE translation, 
word based NE translation, and NE transliteration 
in a single framework. Our approach attempts to 
harness the advantages of the three approaches 
while avoiding their pitfalls. We also introduce and 
evaluate a new approach for aligning NEs across 
parallel corpora, a process for automatically ex-
tracting new NEs translation phrases, and a new 
transliteration approach. As is typical for statistical 
MT, the system requires the availability of general 
parallel corpus and Named Entity identifiers for 
the NEs of interest. 
Our primary focus in this paper is on translating 
NEs out of context (i.e. NEs are extracted and 
translated without any contextual clues). Although 
87
this is a more difficult problem than translating 
NEs in context, we adopt this approach because it 
is more generally useful for CLIR applications. 
The paper is organized as follows, section 2 
presents related work, section 3 describes our inte-
grated NE translation approach, section 4 presents 
the word based translation module, the phrase 
based module, the transliteration module, and sys-
tem integration and decoding, section 5 provides 
the experimental setup and results and finally sec-
tion 6 concludes the paper. 
2 Related Work 
The Named Entity translation problem was previ-
ously addressed using two different approaches: 
Named Entity phrase translation (which includes 
word-based translation) and Named Entity translit-
eration. Recently, many NE phrase translation ap-
proaches have been proposed. Huang et al   
(Huang et al, 2003) proposed an approach to ex-
tract NE trans-lingual equivalences based on the 
minimization of a linearly combined multi-feature 
cost. However this approach used a bilingual dic-
tionary to extract NE pairs and deployed it itera-
tively to extract more NEs.  Moore (Moore, 2003), 
proposed an approach deploying a sequence of cost 
models. However this approach relies on ortho-
graphic clues, such as strings repeated in the 
source and target languages and capitalization, 
which are only suitable for language pairs with 
similar scripts and/or orthographic conventions.  
Most prior work in Arabic-related translitera-
tion has been developed for the purpose of ma-
chine translation and for Arabic-English 
transliteration in particular. Arbabi (Arbabi et al, 
1998) developed a hybrid neural network and 
knowledge-based system to generate multiple Eng-
lish spellings for Arabic person names. Stalls and 
Knight (Stalls and Knight, 1998) introduced an 
approach for Arabic-English back transliteration 
for names of English origin; this approach could 
only back transliterate to English the names that 
have an available pronunciation. Al-Onaizan and 
Knight (Al-Onaizan and Knight, 2002) proposed a 
spelling-based model which directly maps English 
letter sequences into Arabic letter sequences. Their 
model was trained on a small English Arabic 
names list without the need for English pronuncia-
tions. Although this method does not require the 
availability of English pronunciation, it has a seri-
ous limitation because it does not provide a mecha-
nism for inserting the omitted short vowels in 
Arabic names. Therefore it does not perform well 
with names of Arabic origin in which short vowels 
tend to be omitted. 
3 Integrated Approach for Named Entity 
Translation  
We introduce an integrated approach for Named 
Entity (NE) translation using phrase based transla-
tion, word based translation and transliteration ap-
proaches in a single framework. Our unified 
approach could handle, in principle, any NE type 
for any languages pair. 
The level of complication in NE translation de-
pends on the NE type, the original source of the 
names, the standard de facto translation for certain 
named entities and the presence of acronyms. For 
example persons names tend to be phonetically 
transliterated, but different sources might use dif-
ferent transliteration styles depending on the origi-
nal source of the names and the idiomatic 
translation that has been established. Consider the 
following two names: 
?
     
 : jAk $yrAk?  ?Jacques Chirac? 
? 	

   
  :jAk strw?  ?Jack Straw? 
Although the first names in both examples are the 
same in Arabic, their transliterations should be dif-
ferent.  One might be able to distinguish between 
the two by looking at the last names.  This example 
illustrates why transliteration may not be good for 
frequently used named entities.  Transliteration is 
more appropriate for unknown NEs. 
For locations and organizations, the translation 
can be a mixture of translation and transliteration. 
For example: 
 ffProceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 501?508,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Unsupervised Information Extraction Approach Using Graph Mutual 
Reinforcement  
 
 
Hany Hassan Ahmed Hassan Ossama Emam 
 
IBM Cairo Technology Development Center 
Giza, Egypt 
P.O. Box 166 Al-Ahram 
 
hanyh@eg.ibm.com hasanah@eg.ibm.com emam@eg.ibm.com 
 
  
 
Abstract 
Information Extraction (IE) is the task of 
extracting knowledge from unstructured 
text. We present a novel unsupervised 
approach for information extraction 
based on graph mutual reinforcement. 
The proposed approach does not require 
any seed patterns or examples. Instead, it 
depends on redundancy in large data sets 
and graph based mutual reinforcement to 
induce generalized ?extraction patterns?. 
The proposed approach has been used to 
acquire extraction patterns for the ACE 
(Automatic Content Extraction) Relation 
Detection and Characterization (RDC) 
task. ACE RDC is considered a hard task 
in information extraction due to the ab-
sence of large amounts of training data 
and inconsistencies in the available data. 
The proposed approach achieves superior 
performance which could be compared to 
supervised techniques with reasonable 
training data.  
1 Introduction 
In this paper we propose a novel, and completely 
unsupervised approach for information extrac-
tion. We present a general technique; however 
we focus on relation extraction as an important 
task of Information Extraction. The approach 
depends on constructing generalized extraction 
patterns, which could match many instances, and 
deploys graph based mutual reinforcement to 
weight the importance of these patterns. The mu-
tual reinforcement is used to automatically iden-
tify the most informative patterns, where patterns 
that match many instances tend to be correct. 
Similarly, instances matched by many patterns 
tend to be correct. The intuition is that large un-
supervised data is redundant, i.e. different in-
stances of information could be found many 
times in different contexts and by different repre-
sentation. The problem can therefore be seen as 
hubs (instances) and authorities (patterns) prob-
lem which can be solved using the Hypertext 
Induced Topic Selection (HITS) algorithm 
(Kleinberg, 1998). 
HITS is an algorithmic formulation of the no-
tion of authority in web pages link analysis, 
based on a relationship between a set of relevant 
?authoritative pages? and a set of ?hub pages?. 
The HITS algorithm benefits from the following 
observation:  when a page (hub) links to another 
page (authority), the former confers authority 
over the latter.  
By analogy to the authoritative web pages 
problem, we could represent the patterns as au-
thorities and instances as hubs, and use mutual 
reinforcement between patterns and instances to 
weight the most authoritative patterns. Highly 
weighted patterns are then used in extracting in-
formation.  
The proposed approach does not need any 
seeds or examples. Human involvement is only 
needed in determining the entities of interest; the 
entities among which we are seeking relations. 
The paper proceeds as follows: in Section 2 
we discuss previous work followed by a brief 
definition of our general notation in Section 3. A 
detailed description of the proposed approach 
then follows in Section 4. Section 5 discusses the 
application of the proposed approach to the prob-
501
lem of detecting semantic relations from text. 
Section 6 discusses experimental results while 
the conclusion is presented in Section 7. 
2 Previous Work 
Most of the previous work on Information Ex-
traction (IE) focused on supervised learning. Re-
lation Detection and Characterization (RDC) was 
introduced in the Automatic Content Extraction 
Program (ACE) (ACE, 2004). The approaches 
proposed to the ACE RDC task such as kernel 
methods (Zelenko et al, 2002) and Maximum 
Entropy methods (Kambhatla, 2004) required the 
availability of large set of human annotated cor-
pora which are tagged with relation instances. 
However human annotated instances are limited, 
expensive, and time consuming to obtain, due to 
the lack of experienced human annotators and the 
low inter-annotator agreements. 
Some previous work adopted weakly super-
vised or unsupervised learning approaches. 
These approaches have the advantage of not 
needing large tagged corpora but need seed ex-
amples or seed extraction patterns. The major 
drawback of these approaches is their depend-
ency on seed examples or seed patterns which 
may lead to limited generalization due to de-
pendency on handcrafted examples. Some of 
these approaches are briefed here: 
 (Brin,98) presented an approach for extracting 
the authorship information as found in books de-
scription on the World Wide Web. This tech-
nique is based on dual iterative pattern relation 
extraction wherein a relation and pattern set is 
iteratively constructed. This approach has two 
major drawbacks: the use of handcrafted seed 
examples to extract more examples similar to 
these handcrafted seed examples and the use of a 
lexicon as the main source for extracting infor-
mation. 
(Blum and Mitchell, 1998) proposed an ap-
proach based on co-training that uses unlabeled 
data in a particular setting. They exploit the fact 
that, for some problems, each example can be 
described by multiple representations. 
(Riloff & Jones, 1999) presented the Meta-
Bootstrapping algorithm that uses an un-
annotated training data set and a set of seeds to 
learn a dictionary of extraction patterns and a 
domain specific semantic lexicon. Other works 
tried to exploit the duality of patterns and their 
extractions for the purpose of inferring the se-
mantic class of words like (Thelen & Riloff, 
2002) and (Lin et al 2003). 
(Muslea et al, 1999) introduced an inductive 
algorithm to generate extraction rules based on 
user labeled training examples. This approach 
suffers from the labeled data bottleneck. 
(Agichtein et. al, 2000) presented an approach 
using seed examples to generate initial patterns 
and to iteratively obtain further patterns. Then 
ad-hoc measures were deployed to estimate the 
relevancy of the patterns that have been newly 
obtained. The major drawbacks of this approach 
are:  its dependency on seed examples leads to 
limited capability of generalization, and the esti-
mation of patterns relevancy requires the de-
ployment of ad-hoc measures. 
(Hasegawa et. al. 2004) introduced unsuper-
vised approach for relation extraction depending 
on clustering context words between named enti-
ties; this approach depends on ad-hoc context 
similarity between phrases in the context and 
focused on certain types of relations. 
(Etzioni et al 2005) proposed a system for 
building lists of named entities found on the web. 
Their system uses a set of eight domain-
independent extraction patterns to generate can-
didate facts. 
All approaches, proposed so far, suffer from 
either requiring large amount of labeled data or 
the dependency on seed patterns (or examples) 
that result in limited generalization. 
3 General Notation 
In graph theory, a graph is a set of objects called 
vertices joined by links called edges. A bipartite 
graph, also called a bigraph, is a special graph 
where the set of vertices can be divided into two 
disjoint sets with no two vertices of the same set 
sharing an edge.  
The Hypertext Induced Topic Selection 
(HITS) algorithm is an algorithm for rating, and 
therefore ranking, web pages. The HITS algo-
rithm makes use of the following observation: 
when a page (hub) links to another page (author-
ity), the former confers authority over the latter. 
HITS uses two values for each page, the "author-
ity value" and the "hub value". "Authority value" 
and "hub value" are defined in terms of one an-
other in a mutual recursion. An authority value is 
computed as the sum of the scaled hub values 
that point to that authority. A hub value is the 
sum of the scaled authority values of the authori-
ties it points to. 
A template, as we define for this work, is a se-
quence of generic forms that could generalize 
502
over the given instances. An example template 
is:  
GPE POS  (PERSON)+ 
 
GPE: Geographical Political En-
tity 
POS: possessive ending 
PERSON: PERSON Entity 
 
This template could match the sentence: 
?France?s President Jacque Chirac...?.  This tem-
plate is derived from the representation of the 
Named Entity tags, Part-of-Speech (POS) tags 
and semantic tags. The choice of the template 
representation here is for illustration purpose 
only; any combination of tags, representations 
and tagging styles might be used.  
A pattern is more specific than a template. A 
pattern specifies the role played by the tags (first 
entity, second entity, or relation). An example of 
a pattern is:  
    
GPE(E2)  POS   (PERSON)+(E1) 
 
This pattern indicates that the word(s) with the 
tag GPE in the sentence represents the second 
en-tity (Entity 2) in the relation, while the 
word(s) tagged PERSON represents the first en-
tity (Entity 1) in this relation, the ?+? symbol 
means that the (PERSON) entity is repetitive (i.e. 
may consist of several tokens).  
A tuple, in our notation during this paper, is 
the result of the application of a pattern to un-
structured text. In the above example, one result 
of applying the pattern to some raw text is the 
following tuple: 
 
Entity 1: Jacque Chirac 
Entity 2: France 
Relation: EMP-Executive 
4 The Approach 
The unsupervised graph-based mutual rein-
forcement approach, we propose, depends on the 
construction of generalized ?extraction patterns? 
that could match many instances. The patterns 
are then weighted according to their importance 
by deploying graph based mutual reinforcement 
techniques. This duality in patterns and extracted 
information (tuples) could be stated that patterns 
could match different tuples, and tuples in turn 
could be matched by different patterns. The pro-
posed approach is composed of two main steps 
namely, initial patterns construction and pattern 
weighting or induction. Both steps are detailed in 
the next sub-sections. 
4.1 Initial Patterns Construction 
As shown in Figure 1, several syntactic, lexical, 
and semantic analyzers could be applied to the 
unstructured text. The resulting analyses could be 
employed in the construction of extraction pat-
terns. It is worth mentioning that the proposed 
approach is general enough to accommodate any 
pattern design; the introduced pattern design is 
for illustration purposes only. 
 
 
 
 
Initially, we need to start with some templates 
and patterns to proceed with the induction proc-
ess. Relatively large amount of text data is 
tagged with different taggers to produce the pre-
viously mentioned patterns styles. An n-gram 
language model is built on this data and used to 
construct weighted finite state machines.  
Paths with low cost (high language model 
probabilities) are chosen to construct the initial 
set of templates; the intuition is that paths with 
low cost (high probability) are frequent and 
could represent potential candidate patterns. 
The resulting initial set of templates is applied 
to a very large text data to produce all possible 
patterns. The number of candidate initial patterns 
could be reduced significantly by specifying the 
candidate types of entities; for example we might 
specify that the first entity could be PEROSN or 
PEOPLE while the second entity could be OR-
GANIZATION, LOCATION, COUNTRY and 
etc...  
The candidate patterns are then applied to the 
tagged stream and the unstructured text to collect 
a set of patterns and matched tuples pairs.  
The following procedure briefs the Initial Pat-
tern Construction Step: 
? Select a random set of text data. 
American vice President   Al Gore said today... 
PEOPLE    O         O       PERSON   O    O... 
ADJ     NOUN_PHRASE   NNP  VBD CD... 
PEOPLE NOUN_PHRASE  PERSON  VBD CD... 
Entities 
POS 
Tagged 
Stream 
Figure 1:  An example of the output of analys-
ers applied to the unstructured text  
 
503
? Apply various taggers on text data and con-
struct templates style. 
? Build n-gram language model on template 
style data. 
? Construct weighted finite state machines 
from the n-gram language model. 
? Choose n-best paths in the finite state ma-
chines. 
? Use best paths as initial templates. 
? Apply initial templates on large text data. 
? Construct initial patterns and associated tu-
ples sets. 
4.2 Pattern Induction 
The inherent duality in the patterns and tuples 
relation suggests that the problem could be inter-
preted as a hub authority problem. This problem 
could be solved by applying the HITS algorithm 
to iteratively assign authority and hub scores to 
patterns and tuples respectively. 
 
 
Patterns and tuples are represented by a bipar-
tite graph as illustrated in figure 2. Each pattern 
or tuple is represented by a node in the graph. 
Edges represent matching between patterns and 
tuples. The pattern induction problem can be 
formulated as follows: Given a very large set of 
data D containing a large set of patterns P which 
match a large set of tuples T, the problem is to 
identify P
~
, the set of patterns that match the set 
of the most correct tuples  T
~
. The intuition is 
that the tuples matched by many different pat-
terns tend to be correct and the patterns matching 
many different tuples tend to be good patterns. In 
other words; we want to choose, among the large 
space of patterns in the data, the most informa-
tive, highest confidence patterns that could iden-
tify correct tuples; i.e. choosing the most ?au-
thoritative? patterns in analogy with the hub au-
thority problem. However, both P
~
and T
~
are un-
known. The induction process proceeds as fol-
lows:  each pattern p in P is associated with a 
numerical authority weight av which expresses 
how many tuples match that pattern. Similarly, 
each tuple t in T has a numerical hub weight ht 
which expresses how many patterns were 
matched by this tuple. The weights are calculated 
iteratively as follows: 
( ) ( )( )
=
+
=
pT
u i
i
i
H
uhpa
1 )(
)(
)1(
 (1) 
( ) ( )( )
=
+
=
tP
u i
i
i
A
ua
th
1 )(
)(
)1(
 (2) 
where T(p) is the set of tuples matched by p, P(t) 
is the set of patterns matching t, ( )pa i )1( +  is the 
authoritative weight of pattern p  at iteration  
)1( +i , and ( )th i )1( +  is the hub weight of tuple t  
at iteration  )1( +i  . H(i) and A(i) are normaliza-
tion factors defined as: 
 
( )( ) 
= =
=
||
1 1
)()( P
p
pT
u
ii uhH  (3) 
( )( ) 
= =
=
||
1 1
)()( T
v
tP
u
ii uaA
 (4) 
 
Highly weighted patterns are identified and used 
for extracting relations. 
4.3 Tuple Clustering 
The tuple space should be reduced to allow more 
matching between pattern-tuple pairs. This space 
reduction could be accomplished by seeking a 
tuple similarity measure, and constructing a 
weighted undirected graph of tuples. Two tuples 
are linked with an edge if their similarity meas-
ure exceeds a certain threshold. Graph clustering 
algorithms could be deployed to partition the 
graph into a set of homogeneous communities or 
clusters. To reduce the space of tuples, we seek a 
matching criterion that group similar tuples to-
gether. Using WordNet, we can measure the se-
mantic similarity or relatedness between a pair of 
concepts (or word senses), and by extension, be-
tween a pair of sentences. We use the similarity 
P
P
P
P
P
T
T
T
T
T
P
P
T
T
Patterns Tuples
Figure 2: A bipartite graph represent-
ing patterns and tuples 
504
measure described in (Wu and Palmer, 1994) 
which finds the path length to the root  node 
from the least common subsumer (LCS) of the 
two word senses which is the most specific word 
sense they share as an ancestor. The similarity 
score of two tuples, ST, is calculated as follows: 
 
2
2
2
1 EET SSS +=    (5) 
 
where SE1, and SE2 are the similarity scores of the 
first entities in the two tuples, and their second 
entitles respectively. 
The tuple matching procedure assigns a simi-
larity measure to each pair of tuples in the data-
set. Using this measure we can construct an undi-
rected graph G. The vertices of G are the tuples. 
Two vertices are connected with an edge if the 
similarity measure between their underlying tu-
ples exceeds a certain threshold. It was noticed 
that the constructed graph consists of a set of 
semi isolated groups as shown in figure 3. Those 
groups have a very large number of inter-group 
edges and meanwhile a rather small number of 
intra-group edges. This implies that using a 
graph clustering algorithm would eliminate those 
weak intra-group edges and produce separate 
groups or clusters representing similar tuples. We 
used Markov Cluster Algorithm (MCL) for graph 
clustering (Dongen, 2000). MCL is a fast and 
scalable unsupervised clustering algorithm for 
graphs based on simulation of stochastic flow. 
 
 
 
Figure 3: Applying Clustering Algorithms to Tu-
ple graph  
 
An example of a couple of tuples that could be 
matched by this technique is: 
United Stated(E2) presi-
dent(E1) 
US(E2) leader(E1) 
  
A bipartite graph of patterns and tuple clusters 
is constructed. Weights are assigned to patterns 
and tuple clusters by iteratively applying the 
HITS algorithm and the highly ranked patterns 
are then used for relation extraction.  
5 Experimental Setup 
5.1 ACE Relation Detection and Charac-
terization 
In this section, we describe Automatic Content 
Extraction (ACE). ACE is an evaluation con-
ducted by NIST to measure Entity Detection and 
Tracking (EDT) and Relation Detection and 
Characterization (RDC). The EDT task is con-
cerned with the detection of mentions of entities, 
and grouping them together by identifying their 
coreference. The RDC task detects relations be-
tween entities identified by the EDT task. We 
choose the RDC task to show the performance of 
the graph based unsupervised approach we pro-
pose. To this end we need to introduce the notion 
of mentions and entities. Mentions are any in-
stances of textual references to objects like peo-
ple, organizations, geopolitical entities (countries, 
cities ?etc), locations, or facilities. On the other 
hand, entities are objects containing all mentions 
to the same object. Here, we present some exam-
ples of ACE entities and relations: 
Spain?s Interior Minister 
announced this evening the 
arrest of separatist organi-
zation Eta?s presumed leader 
Ignacio Garcia Arregui. Ar-
regui, who is considered to 
be the Eta organization?s 
top man, was arrested at 
17h45 Greenwich. The Spanish 
judiciary suspects Arregui 
of ordering a failed attack 
on King Juan Carlos in 1995. 
 
In this fragment, all the underlined phrases are 
mentions to ?Eta? organization, or to ?Garcia 
Arregui?. There is a management relation be-
tween ?leader? which references to ?Gar-
cia Arregui? and ?Eta?. 
5.2 Patterns Construction and Induction 
We used the LDC English Gigaword Corpus, 
AFE source from January to August 1996 as a 
source for unstructured text. This provides a total 
of 99475 documents containing 36 M words.  In 
the performed experiments, we focus on two 
types of relations EMP-ORG relations and GPE-
AFF relations which represent almost 50% of all 
relations in RDC ? ACE task. 
T
T T
T
T
T
T
T
T
T
TT
T T
T
T T
T
T
T
T
T
T
T
T
T
T T
Before Clustering After Clustering
505
POS (part of speech) tagger and mention tagger 
were applied to the data, the used pattern design 
consists of a mix between the part of speech 
(POS) tags and the mention tags for the words in 
the unsupervised data. We use the mention tag, if 
it exists; otherwise we use the part of speech tag. 
An example of the analyzed text and the pre-
sumed associated pattern is shown: 
 
Text: Eta?s presumed leader 
Arregui ? 
Pos: NNP POS JJ NN NNP 
Mention: ORG 0 0 0 PERSON 
Pattern: ORG(E2) POS JJ 
NN(R) PERSON(E1) 
 
An n-gram language model, 5-gram model and 
back off to lower order n-grams, was built on the 
data tagged with the described patterns? style. 
Weighted finite states machines were constructed 
with the language model probabilities. The n-best 
paths, 20 k paths, were identified and deployed 
as the initial template set. Sequences that do not 
contain the entities of interest, and hence cannot 
represent relations, were automatically filtered 
out. This resulted in an initial templates set of 
around 3000 element. This initial templates set 
was applied on the text data to establish initial 
patterns and tuples pairs. Graph based mutual 
reinforcement technique was deployed with 10 
iterations on the patterns and tuples pairs to 
weight the patterns. 
We conducted two groups of experiments, the 
first with simple syntactic tuple matching, and 
the second with semantic tuple clustering as de-
scribed in section 4.3 
6 Results and Discussion 
We compare our results to a state-of-the-art su-
pervised system similar to the system described 
in (Kambhatla, 2004). Although it is unfair to 
make a comparison between a supervised system 
and a completely unsupervised system, we chose 
to make this comparison to test the performance 
of the proposed unsupervised approach on a real 
task with defined test set and state-of-the-art per-
formance. The supervised system was trained on 
145 K words which contain 2368 instances of the 
two relation types we are considering. 
The system performance is measured using 
precision, recall and F-Measure with various 
amounts of induced patterns. Table 1 presents the 
precision, recall and F-measure for the two rela-
tions using the presented approach with the utili-
zation of different amount of highly weighted 
patterns. Table 2 presents the same results using 
semantic tuple matching and clustering, as de-
scribed in section 4.3.  
 
No. of  
Patterns Precision Recall F-Measure 
1500 35.9 66.3 46.58 
1000 41.2 59.7 48.75 
700 43.1 58.1 49.49 
500 46 56.5 50.71 
400 46.9 52.9 49.72 
200 50.1 44.9 47.36 
 
Table 1:  The effect of varying the number of 
induced patterns on the system performance 
(syntactic tuple matching) 
 
No. of  
Patterns Precision Recall F-Measure 
1500 36.1 67.2 46.97 
1000 43.7 59.6 50.43 
700 44.1 59.3 50.58 
500 46.3 57.2 51.18 
400 47.3 57.6 51.94 
200 48.1 45.9 46.97 
 
Table 2:  The effect of varying the number of 
induced patterns on the system performance (se-
mantic tuple matching) 
0
10
20
30
40
50
60
70
80
Sup 67.1 54.2 59.96
Unsup-Syn 46 56.5 50.71
Unsup-Sem 47.3 57.6 51.94
Precision Recall F Measure
 
 
Figure 4:  A comparison between the supervised 
system (Sup), the unsupervised system with syn-
tactic tuple matching (Unsup-Syn), and with se-
mantic tuple matching (Unsup-Sem) 
 
Best F-Measure is achieved using relatively 
small number of induced patterns (400 and  500 
patterns) while using more patterns increases the 
recall but degrades the precision. 
Table 2 indicates that the semantic clustering 
of tuples did not provide significant improve-
506
ment; although better performance was achieved 
with less number of patterns (400 patterns). We 
think that the deployed similarity measure and it 
needs further investigation to figure out the rea-
son for that. 
Figure 4 presents the comparison between the 
proposed unsupervised systems and the reference 
supervised system. The unsupervised systems 
achieves good results even in comparison to  a 
state-of-the-art supervised system. 
Sample patterns and corresponding matching 
text are introduced in Table 3 and Table 4. Table 
3 shows some highly ranked patterns while Table 
4 shows examples of low ranked patterns. 
 
Pattern Matches 
GPE (PERSON)+ Peruvian President Alberto Fu-jimori 
GPE (PERSON)+ Zimbabwean President Robert Mugabe 
GPE (PERSON)+ PLO leader Yasser Arafat 
GPE POS (PERSON)+ Zimbabwe 's President Robert Mugabe 
GPE JJ PERSON    American clinical neuropsy-
chologist 
GPE JJ PERSON    American diplomatic personnel 
PERSON IN JJ GPE candidates for local government 
ORGANIZATION PER-
SON Airways spokesman 
ORGANIZATION PER-
SON      Ajax players 
PERSON IN DT (OR-
GANIZATION)+  
chairman of the opposition par-
ties 
(ORGANIZATION)+ 
PERSON    opposition parties chairmans 
 
Table3: Examples of patterns with high weights 
 
Pattern Matches 
GPE CC (PERSON)+ Barcelona and Johan 
Cruyff 
GPE , CC PERSON Paris , but Riccardi 
GPE VBZ VBN PERSON Pyongyang has accepted 
Gallucci 
GPE VBZ VBN PERSON Russia has abandoned us 
GPE VBZ VBN P PER-
SON 
Rwanda 's defeated Hutu 
GPE VBZ VBN PERSON state has pressed Arafat 
GPE VBZ VBN TO VB 
PERSON 
Taiwan has tried to keep 
Lee 
(PERSON)+ VBD GPE 
ORGANIZATION 
Alfred Streim told Ger-
man radio 
(PERSON)+ VBD GPE 
ORGANIZATION 
Dennis Ross met Syrian 
army 
(PERSON)+ VBD GPE 
ORGANIZATION 
Van Miert told EU indus-
try 
 
Table4: Examples of patterns with low weights 
7 Conclusion and Future Work 
In this work, a general framework for unsuper-
vised information extraction based on mutual 
reinforcement in graphs has been introduced. We 
construct generalized extraction patterns and de-
ploy graph based mutual reinforcement to auto-
matically identify the most informative patterns. 
We provide motivation for our approach from a 
graph theory and graph link analysis perspective. 
Experimental results have been presented sup-
porting the applicability of the proposed ap-
proach to ACE Relation Detection and Charac-
terization (RDC) task, demonstrating its applica-
bility to hard information extraction problems. 
The proposed approach achieves remarkable re-
sults comparable to a state-of-the-art supervised 
system, achieving 51.94 F-measure compared to 
59.96 F-measure of the state-of-the-art super-
vised system which requires huge amount of hu-
man annotated data. The proposed approach 
represents a powerful unsupervised technique for 
information extraction in general and particularly 
for relations extraction that requires no seed pat-
terns or examples and achieves significant per-
formance. 
In our future work, we plan to focus on general-
izing the approach for targeting more NLP prob-
lems. 
8 Acknowledgements 
We would like to thank Salim Roukos for his 
invaluable suggestions and support. We would 
also like to thank Hala Mostafa for helping with 
the early investigation of this work. Finally we 
would like to thank the anonymous reviewers for 
their constructive criticism and helpful com-
ments. 
References 
ACE. 2004. The NIST ACE evaluation website. 
http://www.nist.gov/speech/tests/ace/ 
Eugene Agichtein and Luis Gravano. 2000.  Snow-
ball: Extracting Relations from Large Plain-Text 
Collections. Proceedings of the 5th ACM Confer-
ence on Digital Libraries (DL 2000). 
   Sergy Brin. 1998. Extracting Patterns and Relations 
from the World Wide Web. Proceedings of the 1998 
International Workshop on the Web and Data-
bases? 
Stijn van Dongen. 2000. A Cluster Algorithm for 
Graphs. Technical Report INS-R0010, National 
Research Institute for Mathematics and Computer 
Science in the Netherlands. 
507
Stijn van Dongen. 2000. Graph Clustering by Flow 
Simulation. PhD thesis, University of Utrecht 
Oren Etzioni, Michael Cafarella, Doug Downey, Ana-
Maria Popescu, Tal Shaked, Stephen Soderland, 
Daniel S. Weld, and Alexander Yates. 2004. Web-
scale information extraction in KnowItAll (prelimi-
nary results). In Proceedings of the 13th World 
Wide Web Conference, pages 100-109. 
Oren Etzioni, Michael Cafarella, Doug Downey, Ana-
Maria Popescu, Tal Shaked, Stephen Soderland, 
Daniel S. Weld, and Alexander Yates. 2005. Unsu-
pervised Named-Entity Extraction from the Web: 
An Experimental Study. Artificial Intelligence, 
2005. 
Radu Florian, Hany Hassan, Hongyan Jing, Nanda 
Kambhatla, Xiaqiang Luo, Nicolas Nicolov, and 
Salim Roukos. 2004. A Statistical Model for multi-
lingual entity detection and tracking. Proceedings 
of the Human Language Technologies Conference 
(HLT-NAACL 2004). 
Dayne Freitag, and Nicholas Kushmerick. 2000. 
Boosted wrapper induction. The 14th European 
Conference on Artificial Intelligence Workshop on 
Machine Learning for Information Extraction 
Rayid Ghani and Rosie Jones. 2002. A Comparison of 
Efficacy and Assumptions of Bootstrapping Algo-
rithms for Training Information Extraction Sys-
tems. Workshop on Linguistic Knowledge Acquisi-
tion and Representation: Bootstrapping Annotated 
Data at the Linguistic Resources and Evaluation 
Conference (LREC 2002). 
Takaaki Hasegawa, Satoshi Sekine, Ralph Grishman. 
2004. Discovering Relations among Named Enti-
ties from Large Corpora. Proceedings of The 42nd 
Annual Meeting of the Association for Computa-
tional Linguistics (ACL 2004). 
Taher Haveliwala. 2002. Topic-sensitive PageRank. 
Proceedings of the 11th International World Wide 
Web Conference 
Thorsten Joachims. 2003. Transductive Learning via 
Spectral Graph Partitioning. Proceedings of the In-
ternational Conference on Machine Learning 
(ICML 2003). 
Nanda Kambhatla. 2004. Combining Lexical, Syntac-
tic, and Semantic Features with Maximum Entropy 
Models for Information Extraction. Proceedings of 
The 42nd Annual Meeting of the Association for 
Computational Linguistics (ACL 2004). 
John Kleinberg. 1998. Authoritative Sources in a Hy-
perlinked Environment. Proceedings of the 9th 
ACM-SIAM Symposium on Discrete Algorithms. 
N. Kushmerick, D.S. Weld, R.B. Doorenbos. 1997. 
Wrapper Induction for Information Extraction. 
Proceedings of the International Joint Conference 
on Artificial Intelligence.  
Winston Lin, Roman Yangarber, Ralph Grishman. 
2003. Bootstrapped Learning of Semantic Classes 
from Positive and Negative Examples. Proceedings 
of the 20th International Conference on Machine 
Learning (ICML 2003) Workshop on The Contin-
uum from Labeled to Unlabeled Data in Machine 
Learning and Data Mining. 
Ion Muslea, Steven Minton, and Craig 
Knoblock.1999.  A hierarchical approach to wrap-
per induction. Proceedings of the Third Interna-
tional Conference on Autonomous Agents. 
Ted Pedersen, Siddharth Patwardhan, and Jason 
Michelizzi. 2004, WordNet::Similarity - Measuring 
the Relatedness of Concepts. Proceedings of Fifth 
Annual Meeting of the North American Chapter of 
the Association for Computational Linguistics 
(NAACL 2004) 
Ellen Riloff and Rosie Jones. 2003. Learning diction-
aries for information extraction by multilevel boot-
strapping. Proceedings of the Sixteenth national 
Conference on Artificial Intelligence (AAAI 1999). 
Michael Thelen and Ellen Riloff. 2002. A Bootstrap-
ping Method for Learning Semantic Lexicons using 
Extraction Pattern Contexts. Proceedings of the 
2002 Conference on Empirical Methods in Natural 
Language Processing (EMNLP 2002). 
Scott White, and Padhraic Smyth. 2003. Algorithms 
for Discoveing Relative Importance in Graphs. 
Proceedings of Ninth ACM SIGKDD International 
Conference on Knowledge Discovery and Data 
Mining. 
Zhibiao Wu, and Martha Palmer. 1994. Verb seman-
tics and lexical selection. Proceedings of the 32nd 
Annual Meeting of the Association for Computa-
tional Linguistics (ACL 1994). 
Xiaojin Zhu, Zoubin Ghahramani, and John Lafferty. 
2003. Semi-supervised Learning using Gaussian 
Fields and Harmonic Functions. Proceedings of 
the 20th International Conference on Machine 
Learning (ICML 2003). 
 
 
508
Workshop on TextGraphs, at HLT-NAACL 2006, pages 9?16,
New York City, June 2006. c?2006 Association for Computational Linguistics
Graph Based Semi-Supervised Approach for Information Extraction 
 
 
Hany Hassan Ahmed Hassan Sara Noeman 
 
IBM Cairo Technology Development Center 
Giza, Egypt 
                                                              P.O. Box 166 Al-Ahram 
 
hanyh@eg.ibm.com hasanah@eg.ibm.com noemans@eg.ibm.com 
 
 
 
 
Abstract 
Classification techniques deploy supervised 
labeled instances to train classifiers for 
various classification problems. However 
labeled instances are limited, expensive, 
and time consuming to obtain, due to the 
need of experienced human annotators.  
Meanwhile large amount of unlabeled data 
is usually easy to obtain. Semi-supervised 
learning addresses the problem of utilizing 
unlabeled data along with supervised la-
beled data, to build better classifiers.  In 
this paper we introduce a semi-supervised 
approach based on mutual reinforcement in 
graphs to obtain more labeled data to en-
hance the classifier accuracy. The approach 
has been used to supplement a maximum 
entropy model for semi-supervised training 
of the ACE Relation Detection and Charac-
terization (RDC) task. ACE RDC is con-
sidered a hard task in information 
extraction due to lack of large amounts of 
training data and inconsistencies in the 
available data. The proposed approach pro-
vides 10% relative improvement over the 
state of the art supervised baseline system. 
1 Introduction 
Classification techniques use labeled data to train 
classifiers for various classification problems.  Yet 
they often face a shortage of labeled training data. 
Labeled instances are often difficult, expensive, 
and /or time consuming to obtain. Meanwhile large 
numbers of unlabeled instances are often available. 
Semi-supervised learning addresses the problem of 
how unlabeled data can be usefully employed, 
along with labeled data, to build better classifiers. 
In this paper we propose a semi-supervised ap-
proach for acquiring more training instances simi-
lar to some labeled instances. The approach 
depends on constructing generalized extraction 
patterns, which could match many instances, and 
deploying graph based mutual reinforcement to 
weight the importance of these patterns.  The mu-
tual reinforcement is used to automatically identify 
the most informative patterns; where patterns that 
match many instances tend to be correct. Similarly, 
instances matched by many patterns also tend to be 
correct. The labeled instances should have more 
effect in the mutual reinforcement weighting proc-
ess. The problem can therefore be seen as hubs 
(instances) and authorities (patterns) problem 
which can be solved using the Hypertext Induced 
Topic Selection (HITS) algorithm (Kleinberg, 
1998 ). 
HITS is an algorithmic formulation of the notion 
of authority in web pages link analysis, based on a 
relationship between a set of relevant ?authorita-
tive pages? and a set of ?hub pages?. The HITS 
algorithm benefits from the following observation:  
when a page (hub) links to another page (author-
ity), the former confers authority over the latter.  
By analogy to the authoritative web pages prob-
lem, we could represent the patterns as authorities 
and instances as hubs, and use mutual reinforce-
ment between patterns and instances to weight the 
most authoritative patterns. Instances from unsu-
9
pervised data matched with the highly weighted 
patterns are then used in retraining the system.  
The paper proceeds as follows: in Section 2 we 
discuss previous work followed by a brief defini-
tion of our general notation in Section 3. A detailed 
description of the proposed approach then follows 
in Section 4. Section 5 discusses the application of 
the proposed approach to the problem of detecting 
semantic relations from text. Section 6 discusses 
experimental results while the conclusion is pre-
sented in Section 7. 
2 Previous Work 
(Blum and Mitchell, 1998) proposed an approach 
based on co-training that uses unlabeled data in a 
particular setting. They exploit the fact that, for 
some problems, each example can be described by 
multiple representations. They develop a boosting 
scheme which exploits conditional independence 
between these representations.  
(Blum and Chawla, 2001) proposed  a general 
approach utilizing unlabeled data by constructing a 
graph on all the data points based on distance rela-
tionships among examples, and then to use the 
known labels to perform a graph partitioning using  
the minimum cut that agrees with the labeled data. 
(Zhu et al, 2003) extended this approach by pro-
posing a  cut based on the assumption that labels 
are generated according to a Markov Random 
Field on the graph , (Joachims, 2003) presented  an 
algorithm based on spectral graph partitioning. 
(Blum et al, 2004) extended the min-cut  approach 
by adding randomness to the graph structure, their 
algorithm addresses several shortcomings of the 
basic mincut approach, yet it may not help in cases 
where the graph does not have small cuts for a 
given classification problem. 
3 Background  
In graph theory, a graph is a set of objects called 
vertices joined by links called edges. A bipartite 
graph, also called a bigraph, is a special graph 
where the set of vertices can be divided into two 
disjoint sets with no two vertices of the same set 
sharing an edge.  
The Hypertext Induced Topic Selection (HITS) 
algorithm is an algorithm for rating, and therefore 
ranking, web pages. The HITS algorithm makes 
use of the following observation: when a page 
(hub) links to another page (authority), the former 
confers authority over the latter. HITS uses two 
values for each page, the "authority value" and the 
"hub value". "Authority value" and "hub value" are 
defined in terms of one another in a mutual recur-
sion. An authority value is computed as the sum of 
the scaled hub values that point to that authority. A 
hub value is the sum of the scaled authority values 
of the authorities it points to. 
A template, as we define for this work, is a se-
quence of generic forms that could generalize over 
the given training instance. An example template 
is:  
COUNTRY  NOUN_PHRASE PERSON 
VERB_PHRASE  
This template could represent the sentence: 
?American vice President Al Gore visited ...?.  
This template is derived from the representation of 
the Named Entity tags, Part-of-Speech (POS) tags 
and semantic tags. The choice of the template rep-
resentation here is for illustration purpose only; 
any combination of tags, representations and tag-
ging styles might be used.  
A pattern is more specific than a template. A 
pattern specifies the role played by the tags (first 
entity, second entity, or relation). An example of a 
pattern is: 
COUNTRY(E2) NOUN_PHRASE(R) PERSON(E1)   
VERB_PHRASE  
This pattern indicates that the word(s) with the 
tag COUNTRY in the sentence represents the sec-
ond entity (Entity 2) in the relation, while the 
word(s) tagged PERSON represents the first entity 
(Entity 1) in this relation. Finally, the word(s) with 
the tag NOUN_PHRASE represents the relation 
between the two previous entities.   
A tuple, in our notation during this paper, is the 
result of the application of a pattern to unstructured 
text. In the above example, one result of applying 
the pattern to some raw text is the following tuple: 
Entity 1:  Al Gore 
Entity 2: United States 
Relation: vice President 
4 The Approach 
The semi-supervised graph-based approach we 
propose depends on the construction of generalized 
extraction patterns that could match many training 
instances. The patterns are then weighted accord-
ing to their importance by deploying graph based 
10
mutual reinforcement techniques. Patterns derived 
from the supervised training instances should have 
a superior effect in the reinforcement weighting 
process. This duality in patterns and tuples relation 
could be stated that patterns could match different 
tuples, and tuples in turn could be matched by dif-
ferent patterns. The proposed approach is com-
posed of two main steps namely, pattern extraction 
and pattern weighting or induction. Both steps are 
detailed in the next subsections. 
4.1 Patterns Extraction 
As shown in Figure 1, several syntactic, lexical, 
and semantic analyzers could be applied to the 
training instances. The resulting analyses could be 
employed in the construction of extraction pat-
terns. Any extraction pattern could match different 
relations and hence could produce several tuples. 
As an example let?s consider the pattern depicted 
in figure 1: 
 
 
Figure 1:  An example of a pattern and its possible 
tuples. 
 
PEOPLE_Inhabitant(E2) NOUN_PHRASE(R) 
PERSON(E1) VERB_PHRASE  
This pattern could extract the tuple: 
Entity 1: Al Gore 
Entity 2: American  
Relation: vice President 
Another tuple that could be extracted by the same 
pattern is:  
Entity 1: Berlusconi 
Entity 2: Italian  
Relation: Prime Minister 
On the other hand, many other patterns could ex-
tract the same information in the tuple from differ-
ent contexts. It is worth mentioning that the 
proposed approach is general enough to accommo-
date any pattern design; the introduced pattern de-
sign is for illustration purposes only. 
To further increase the number of patterns that 
could match a single tuple, the tuple space might 
be reduced i.e. by grouping tuples conveying the 
same information content together into a single 
tuple. This will be detailed further in the experi-
mental setup section. 
4.2   Pattern Induction 
The inherent duality in the patterns and tuples rela-
tion suggests that the problem could be interpreted 
as a hub authority problem. This problem could be 
solved by applying the HITS algorithm to itera-
tively assign authority and hub scores to patterns 
and tuples respectively. 
 
Figure 2: A bipartite graph representing patterns 
and tuples 
 
Patterns and tuples are represented by a bipartite 
graph as illustrated in figure 2. Each pattern or tu-
ple is represented by a node in the graph. Edges 
represent matching between patterns and tuples.  
The pattern induction problem can be formu-
lated as follows: Given a very large set of data D 
containing a large set of patterns P which match a 
P
P
P
P
P
T
T
T
T
T
P
P
T
T
Patterns Tuples
American vice President   Al Gore said today... 
Word: American 
Entity: PEOPLE 
POS : ADJ 
Sem: Inhabitant 
Word: vice president 
Entity:  
POS: NOUN_PHRASE 
Sem:  
Word: Al Gore 
Entity: PERSON 
POS: 
Sem: 
PEOPLE_Inhabitant    NOUN_PHRASE        PERSON 
VERB_PHRASE 
 
Entity 1:  Al Gore 
Entity 2: American 
Relation: vice President 
American vice Presi-
dent   Al Gore said 
today? 
Italian Prime Minister 
Berlusconi  visited?.. 
Entity 1: Berlusconi  
Entity 2: Italian 
Relation: prime minister 
11
large set of tuples T, the problem is to identify P
~
, 
the set of patterns that match the set of the most 
correct tuplesT
~
. The intuition is that the tuples 
matched by many different patterns tend to be cor-
rect and the patterns matching many different tu-
ples tend to be good patterns. In other words; we 
want to choose, among the large space of patterns 
in the data, the most informative, highest confi-
dence patterns that could identify correct tuples; 
i.e. choosing the most ?authoritative? patterns in 
analogy with the hub authority problem. However, 
both P
~
andT
~
are unknown. The induction process 
proceeds as follows:  each pattern p in P is associ-
ated with a numerical authority weight av which 
expresses how many tuples match that pattern. 
Similarly, each tuple t in T has a numerical hub 
weight ht which expresses how many patterns were 
matched by this tuple. The weights are calculated 
iteratively as follows: 
( ) ( )( )
=
+
=
pT
u i
i
i
H
uhpa
1 )(
)(
)1(
 (1) 
( ) ( )( )
=
+
=
tP
u i
i
i
A
ua
th
1 )(
)(
)1(
 (2) 
where T(p) is the set of tuples matched by p, P(t) is 
the set of patterns matching t, ( )pa i )1( +  is the au-
thoritative weight of pattern p  at iteration  )1( +i , 
and ( )th i )1( +  is the hub weight of tuple t  at itera-
tion  )1( +i  . H(i) and A(i) are normalization fac-
tors defined as: 
 
( )( ) 
= =
=
||
1 1
)()( P
p
pT
u
ii uhH  (3) 
( )( ) 
= =
=
||
1 1
)()( T
v
tP
u
ii uaA
 (4) 
Patterns with weights lower than a predefined 
threshold are rejected, and examples associated 
with highly ranked patterns are then used in unsu-
pervised training. 
It is worth mentioning that both T and P contain 
supervised and unsupervised examples, however 
the proposed method could assign weights to the 
correct examples (tuples and patterns) in a com-
pletely unsupervised setup. For semi-supervised 
data some supervised examples are provided, 
which are associated in turn with tuples and pat-
terns.  
We adopt the HITS extension introduced in 
(White and Smyth, 2003) to extend HITS with Pri-
ors. By analogy, we handle the supervised exam-
ples as priors to the HITS induction algorithm.  
A prior probabilities vector pr ={pr1, . . . , prn}  
is defined such that the probabilities sum to 1,  
where prv denotes the relative importance (or 
?prior bias?) we attach to node v. A pattern Pi is 
assigned a prior pri=1/n if pattern Pi matches a 
supervised tuple, otherwise pri is set to zero, n is 
the total number of patterns that have a supervised 
match. We also define a ?back probability? 
 
, 0   
 
   1 which determines how often we bias the su-
pervised nodes: 
( ) ( ) ( )( ) ppTu i
i
i pr
H
uhpa *1
1 )(
)(
)1( ?? +



?= 
=
+
 (5) 
( ) ( ) ( )( ) ttPu i
i
i pr
A
ua
th *1
1 )(
)(
)1( ?? +



?= 
=
+
  (6) 
where T(p) is the set of tuples matched by p , P(t) 
is the set of patterns matching t, and H(i) and A(i) 
are normalization factors defined as in  equations 
(3) and (4) 
 
Thus each node in the graph (pattern or tuple) has 
an associated prior weight depending on its super-
vised data. The induction process proceeds to itera-
tively assign weights to the patterns and tuples. In 
the current work we used 5.0=? . 
5 Experimental Setup  
5.1 ACE Relation Detection and Characteri-
zation 
In this section, we describe Automatic Content 
Extraction (ACE). ACE is an evaluation conducted 
by NIST to measure Entity Detection and Tracking 
(EDT) and Relation Detection and Characteriza-
tion (RDC). The EDT task is concerned with the 
detection of mentions of entities, and grouping 
them together by identifying their coreference. The 
RDC task detects relations between entities identi-
fied by the EDT task. We choose the RDC task to 
show the performance of the graph based semi-
supervised information extraction approach we 
propose. To this end we need to introduce the no-
tion of mentions and entities. Mentions are any 
instances of textual references to objects like peo-
12
ple, organizations, geo-political entities (countries, 
cities ?etc), locations, or facilities. On the other 
hand, entities are objects containing all mentions to 
the same object. 
 
Type Subtype Number of Instances 
User-Owner 
Inventor ART 
Other 
331 
DISC DISC 143 
Employ-Exec 
Employ-Staff 
Employ-Undetermined 
Member-of-Group 
Subsidiary 
EMP-ORG 
Other 
1673 
Ethnic 
Ideology Other-AFF 
Other 
153 
Citizen-Resident 
Based-in GPE-AFF 
Other 
695 
Business 
Family PER-SOC 
Other 
358 
Located 
Near PHYS 
Part-Whole 
1411 
 
Table 1. Types and subtypes of ACE relations 
 
Table 1 lists the types and subtypes of relations 
for the ACE RDC task. Here, we present an exam-
ple for those relations: 
 
Spain?s Interior Minister an-
nounced this evening the ar-
rest of separatist 
organization Eta?s presumed 
leader Ignacio Garcia Ar-
regui. Arregui, who is con-
sidered to be the Eta 
organization?s top man, was 
arrested at 17h45 Greenwich. 
The Spanish judiciary sus-
pects Arregui of ordering a 
failed attack on King Juan 
Carlos in 1995. 
In this fragment, all the underlined phrases are 
mentions to Eta organization, or to ?Garcia Ar-
regui?. There is a management relation between 
leader which references to ?Garcia Arregui? and 
Eta. 
5.2 Baseline System 
The base line system uses a Maximum Entropy 
model that combines diverse lexical, syntactic and 
semantic features derived from text, like the sys-
tem described in (Nanda, 2004). The system was 
trained on the ACE training data provided by LDC. 
The training set contained 145K words, and 4764 
instances of relations, the number of instances cor-
responding to each relation is shown in Table 1. 
The test set contained around 41K words, and 
1097 instances of relations. The system was evalu-
ated using standard ACE evaluation procedure. 
ACE evaluation procedure assigns the system an 
ACE value for each relation type and a total ACE 
value. The ACE value is a standard NIST metric 
for evaluating relation extraction. The reader is 
referred to the ACE web site (ACE, 2004) for more 
details.  
5.3 Pattern Construction 
We used the baseline system described in the pre-
vious section to label a large amount of unsuper-
vised data. The data comes from LDC English 
Gigaword corpus, Agence France Press English 
Service (AFE). The data contains around 3M 
words, from which 80K instances of relations have 
been extracted. 
We start by extracting a set of patterns that rep-
resent the supervised and unsupervised data. We 
consider each relation type separately and extract a 
pattern for each instance in the selected relation. 
The pattern we used consists of a mix between the 
part of speech (POS) tags and the mention tags for 
the words in the training instance. We use the men-
tion tag, if it exists; otherwise we use the part of 
speech tag. An example of a pattern is: 
 
Text: Eta?s presumed leader 
Arregui ? 
Pos: NNP POS JJ NN NNP 
Mention: ORG 0 0 0 PERSON 
Pattern: ORG(E2) POS JJ NN(R) 
PERSON(E1) 
13
5.4 Tuples Clustering 
As discussed in the previous section, the tuple 
space should be reduced to allow more matching 
between pattern-tuple pairs. This space reduction 
could be accomplished by seeking a tuple similar-
ity measure, and constructing a weighted undi-
rected graph of tuples. Two tuples are linked with 
an edge if their similarity measure exceeds a cer-
tain threshold. Graph clustering algorithms could 
be deployed to partition the graph into a set of ho-
mogeneous communities or clusters. To reduce the 
space of tuples, we seek a matching criterion that 
group similar tuples together. Using WordNet, we 
can measure the semantic similarity or relatedness 
between a pair of concepts (or word senses), and 
by extension, between a pair of sentences. We use 
the similarity measure described in (Wu and 
Palmer, 1994) which finds the path length to the 
root  node from the least common subsumer (LCS) 
of the two word senses which is the most specific 
word sense they share as an ancestor. The similar-
ity score of two tuples, ST, is calculated as follows:. 
2
2
2
1 EET SSS +=   (9) 
where SE1, and SE2 are the similarity scores of the 
first entities in the two tuples, and their second en-
titles respectively. 
The tuple matching procedure assigns a similarity 
measure to each pair of tuples in the dataset. Using 
this measure we can construct an undirected graph 
G. The vertices of G are the tuples. Two vertices 
are connected with an edge if the similarity meas-
ure between their underlying tuples exceeds a cer-
tain threshold. It was noticed that the constructed 
graph consists of a set of semi isolated groups as 
shown in figure 3. Those groups have a very large 
number of inter-group edges and meanwhile a 
rather small number of intra-group edges. This im-
plies that using a graph clustering algorithm would 
eliminate those weak intra-group edges and pro-
duce separate groups or clusters representing simi-
lar tuples. We used Markov Cluster Algorithm 
(MCL) for graph clustering (Dongen, 2000). MCL 
is a fast and scalable unsupervised cluster algo-
rithm for graphs based on simulation of stochastic 
flow. 
A bipartite graph of patterns and tuple clusters is 
constructed. Weights are assigned to patterns and 
tuple clusters by iteratively applying the HITS with 
Priors? algorithm. Instances associated with highly 
ranked patterns are then added to the training data 
and the model is retrained. Samples of some highly 
ranked patterns and corresponding matching text 
are introduced in Table 2. 
 
 
Figure 3: Applying Clustering Algorithms to Tuple 
graph  
 
Pattern Matches 
GPE PERSON 
PERSON PERSON 
Zimbabwean President 
Robert Mugabe 
GPE POS PERSON 
PERSON 
Zimbabwe 's President 
Robert Mugabe 
GPE JJ PERSON American diplomatic per-
sonnel 
PERSON IN JJ GPE candidates for local gov-
ernment 
ORGANIZATION 
PERSON Airways spokesman 
ORGANIZATION 
PERSON      Ajax players 
PERSON IN DT JJ  
ORGANIZATION  
chairman of the opposition 
parties 
ORGANIZATION 
PERSON    parties chairmans 
 
Table 2: Examples of patterns with high weights 
6 Results and Discussion 
We train several models like the one described in 
section 5.2 on different training data sets. In all 
experiments, we use both the LDC ACE training 
data and the labeled unsupervised data induced 
with the graph based approach we propose. We use 
the ACE evaluation procedure and ACE test cor-
pus, provided by LDC, to evaluate all models. 
We incrementally added labeled unsupervised 
data to the training data to determine the amount of 
data after which degradation in the system per-
formance occurs. We sought this degradation point 
separately for each relation type. Figure 4 shows 
the effect of adding labeled unsupervised data on 
T
T T
T
T
T
T
T
T
T
TT
T T
T
T T
T
T
T
T
T
T
T
T
T
T T
Before Clustering After Clustering
14
the ACE value for each relation separately. We 
notice from figure 4 and table 1 that relations with 
a small number of training instances had a higher 
gain in performance compared to relations with a 
large number of training instances. This implies 
that the proposed approach achieves significant 
improvement when the number of labeled training 
instances is small but representative. 
. 
0
10
20
30
40
50
60
0 50 100 200 300 400 500
Number of Added Documents
A
CE
 
Va
lu
e
EMP-ORG
PER-SOC
ART
PHYS
GPE-AFF
OTHER-
AFF
 
 
Figure 4: The effect of adding labeled unsuper-
vised data on the ACE value for each relation. The 
average number of relations per document is 4. 
 
From figure 4, we determined the number of 
training instances resulting in the maximum boost 
in performance for each relation. We added the 
training instances corresponding to the maximum 
boost in performance for all relations to the super-
vised training data and trained a new model on 
them. Figure 5 compares the ACE values for each 
relation in the base line model and the final model 
The total system ACE value has been improved 
by 10% over the supervised baseline system. All 
relation types, except the DSC relation, had sig-
nificant improvement ranging from 7% to 30% 
over the baseline supervised system. The DISC 
relation type had a small degradation; noting that it 
already has a low ACE value with the baseline sys-
tem. We think this is due to the fact that the DISC 
relation has few and inconsistent examples in the 
supervised data set. 
To assess the usefulness of the smoothing 
method employing WordNet distance, we repeated 
the experiment on EMP-ORG relation without it. 
We found out that it contributed to almost 30% of 
the total achieved improvement. We also repeated 
the experiment but with considering hub scores 
instead of authority scores. We added the examples 
associated with highly ranked tuples to the training 
set. We noticed that using hub scores yielded very 
little variation in the ACE value (i.e. 0.1 point for 
EMP-ORG relation). 
0
10
20
30
40
50
AC
E 
Va
lu
e
BaseLine 36.7 6 33.1 22.3 23.6 42.2 26.4 30.5
Final Model 39.6 4.2 35.8 24.7 30.8 46.6 28.2 33.5
ART DISC EMP-ORG
GPE-
AFF
OTHE
R-AFF
PER-
SOC PHYS TOTAL
 
 
Figure 5: A comparison of base line ACE values, 
and final ACE values for each relation. 
 
To evaluate the quality and representativeness of 
the labeled unsupervised data, acquired using the 
proposed approach, we study the effect of replac-
ing supervised data with unsupervised data while 
holding the amount of training data fixed. Several 
systems have been built using mixture of the su-
pervised and the unsupervised data. In Figure 6, 
the dotted line shows the degradation in the system 
performance when using a reduced amount of su-
pervised training data only, while the solid line 
shows the effect of replacing supervised training 
data with unsupervised labeled data on the system 
performance. We notice from Figure 6 that the un-
supervised data could replace more than 50% of 
the supervised data without any degradation in the 
system performance. This is an indication that the 
induced unsupervised data is good for training the 
classifier.  
 
26
27
28
29
30
31
32
33
34
0% 25% 50% 75%
100% 75% 50% 25%
Percentage of Unsupervised/Supervised 
Data
A
CE
 
Va
lu
e Sup + Unsup
Data
Sup Data Only
Unsupervised
Supervised
 
 
Figure 6: The effect of removing portions of the 
supervised data on the ACE value. And the effect 
15
of replacing portions of the supervised data with 
labeled training data. 
7 Conclusion 
We introduce a general framework for semi-
supervised learning based on mutual reinforcement 
in graphs. We construct generalized extraction pat-
terns and deploy graph based mutual reinforcement 
to automatically identify the most informative pat-
terns. We provide motivation for our approach 
from a graph theory and graph link analysis per-
spective. 
We present experimental results supporting the 
applicability of the proposed approach to ACE Re-
lation Detection and Characterization (RDC) task, 
demonstrating its applicability to hard information 
extraction problems. Our approach achieves a sig-
nificant improvement over the base line supervised 
system especially when the number of labeled in-
stances is small. 
8 Acknowledgements 
We would like to thank Nanda Kambhatla for pro-
viding the ACE baseline system. We would also 
like to thank Salim Roukos for several invaluable 
suggestions and guidance. Finally we would like to 
thank the anonymous reviewers for their construc-
tive criticism and helpful comments. 
References  
ACE. 2004. The NIST ACE evaluation website. 
http://www.nist.gov/speech/tests/ace/ 
Avrim Blum, and Tom Mitchell. 1998. Combining La-
beled and Unlabeled data with Co-training. Proceed-
ings of the 11th Annual Conference on 
Computational Learning Theory. 
 Avrim Blum and Shuchi Chawla. 2001. Learning From 
Labeled and Unlabeled Data Using Graph Mincuts. 
Proceedings of International Conference on Machine 
Learning (ICML). 
Avrim Blum, John Lafferty, Mugizi Rwebangira, and 
Rajashekar Reddy. 2004. Semi-supervised Learning 
Using Randomized Mincuts. Proceedings of the In-
ternational Conference on Machine Learning 
(ICML).  
Stijn van Dongen. 2000. A Cluster Algorithm for 
Graphs. Technical Report INS-R0010, National Re-
search Institute for Mathematics and Computer Sci-
ence in the Netherlands. 
Stijn van Dongen. 2000. Graph Clustering by Flow 
Simulation. PhD thesis, University of Utrecht 
Radu Florian, Hany Hassan, Hongyan Jing, Nanda 
Kambhatla, Xiaqiang Luo, Nicolas Nicolov, and 
Salim Roukos. 2004. A Statistical Model for multi-
lingual entity detection and tracking. Proceedings of 
the Human Language Technologies Conference 
(HLT-NAACL?04). 
Dayne Freitag, and Nicholas Kushmerick. 2000. 
Boosted wrapper induction. The 14th European Con-
ference on Artificial Intelligence Workshop on Ma-
chine Learning for Information Extraction 
Taher Haveliwala. 2002. Topic-sensitive PageRank. 
Proceedings of the 11th International World Wide 
Web Conference 
Thorsten Joachims. 2003. Transductive Learning via 
Spectral Graph Partitioning. Proceedings of the In-
ternational Conference on Machine Learning 
(ICML). 
John Kleinberg. 1998. Authoritative Sources in a Hy-
perlinked Environment. Proceedings of the. 9th 
ACM-SIAM Symposium on Discrete Algorithms. 
Nanda Kambhatla. 2004. Combining Lexical, Syntactic, 
and Semantic Features with Maximum Entropy Mod-
els for Information Extraction. Proceedings of the 
42nd Annual Meeting of the Association for Compu-
tational Linguistics 
Ted  Pedersen, Siddharth Patwardhan, and Jason Mich-
elizzi, 2004, WordNet::Similarity - Measuring the 
Relatedness of Concepts. Proceedings of Fifth An-
nual Meeting of the North American Chapter of the 
Association for Computational Linguistics (NAACL-
2004) 
Scott White, and Padhraic Smyth. 2003. Algorithms for 
Discoveing Relative Importance in Graphs. Proceed-
ings of Ninth ACM SIGKDD International Confer-
ence on Knowledge Discovery and Data Mining. 
Zhibiao Wu, and Martha Palmer. 1994. Verb semantics 
and lexical selection. Proceedings of the 32nd An-
nual Meeting of the Association for Computational 
Linguistics. 
Xiaojin Zhu, Zoubin Ghahramani, and John Lafferty. 
2003. Semi-supervised Learning using Gaussian 
Fields and Harmonic Functions. Proceedings of the 
20th International Conference on Machine Learning. 
16
Proceedings of the 5th Workshop on Important Unresolved Matters, pages 25?32,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Arabic Cross-Document Person Name Normalization 
Walid Magdy, Kareem Darwish, Ossama Emam, and Hany Hassan 
Human Language Technologies Group 
IBM Cairo Technology Development Center 
P.O. Box 166 El-Ahram, Giza, Egypt 
{wmagdy, darwishk, emam, hanyh}@eg.ibm.com 
 
Abstract 
This paper presents a machine learning 
approach based on an SVM classifier 
coupled with preprocessing rules for cross-
document named entity normalization.  The 
classifier uses lexical, orthographic, 
phonetic, and morphological features.  The 
process involves disambiguating different 
entities with shared name mentions and 
normalizing identical entities with different 
name mentions.  In evaluating the quality of 
the clusters, the reported approach achieves 
a cluster F-measure of 0.93.  The approach 
is significantly better than the two baseline 
approaches in which none of the entities are 
normalized or entities with exact name 
mentions are normalized.  The two baseline 
approaches achieve cluster F-measures of 
0.62 and 0.74 respectively.  The classifier 
properly normalizes the vast majority of 
entities that are misnormalized by the 
baseline system. 
1. Introduction: 
Much recent attention has focused on the 
extraction of salient information from unstructured 
text.  One of the enabling technologies for 
information extraction is Named Entity 
Recognition (NER), which is concerned with 
identifying the names of persons, organizations, 
locations, expressions of times, quantities, ... etc. 
(Chinchor, 1999; Maynard et al, 2001;  Sekine, 
2004; Joachims, 2002).  The NER task is 
challenging due to the ambiguity of natural 
language and to the lack of uniformity in writing 
styles and vocabulary used across documents 
(Solorio, 2004). 
Beyond NER, considerable work has focused 
on the tracking and normalization of entities that 
could be mentioned using different names (e.g. 
George Bush, Bush) or nominals (e.g. the 
president, Mr., the son) (Florian et al, 2004).  
Most of the named entity tracking work has 
focused on intra-document normalization with 
very limited work on cross-documents 
normalization. 
Recognizing and tracking entities of type 
?Person Name? are particularly important for 
information extraction.  Yet they pose interesting 
challenges that require special attention.  The 
problems can result from: 
1. A Person?s name having many variant spellings 
(especially when it is transliterated into a 
foreign language).  These variations are 
typically limited in the same document, but are 
very common across different documents from 
different sources (e.g. Mahmoud Abbas = 
Mahmod Abas, Mohamed El-Baradei = 
Muhammad AlBaradey ? etc). 
2. A person having more than one name (e.g. 
Mahmoud Abbas = Abu Mazen). 
3. Some names having very similar or identical 
names but refer to completely different persons 
(George H. W. Bush ? George W. Bush). 
4. Single token names (e.g. Bill Clinton = Clinton 
? Hillary Clinton). 
This paper will focus on Arabic cross-document 
normalization of named entities of type ?person 
name,? which would involve resolving the 
aforementioned problems.  As illustrated in Figure 
1, the task involves normalizing a set of person 
entities into a set of classes each of which is 
25
formed of at least one entity.  For N input entities, 
the output of normalization process will be M 
classes, where M ? N.  Each class would refer to 
only one person and each class would contain all 
entities referring to that person. 
For this work, intra-document normalization is 
assumed and an entity refers to a normalized set of 
name mentions and nominals referring to a single 
person in a single document.  Florian et al (2004) 
were kind enough to provide the authors access to 
an updated version of their state-of-the-art Named 
Entity Recognition and Tracking (NERT) system, 
which achieves an F-measure of 0.77 for NER, 
and an F-measure of 0.88 for intra-document 
normalization assuming perfect NER.  Although 
the NERT systems is efficient for relatively short 
documents, it is computational impractical for 
large documents, which precludes using the NERT 
system for cross-document normalization through 
combining the documents into one large 
document.  The main challenges of this work stem 
from large variations in the spelling of 
transliterated foreign names and the presence of 
many common Arabic names (such as 
Muhammad, Abdullah, Ahmed ?etc.), which 
increases the ambiguity in identifying the person 
referred to by the mentioned name.  Further, the 
NERT system output system contains many NER 
errors and intra-document normalization errors. 
In this paper, cross-document normalization 
system employs a two-step approach.  In the first 
step, preprocessing rules are used to remove errant 
named entities.  In the second step, a support 
vector machine (SVM) classifier is used to 
determine if two entities from two different 
documents need to be normalized.  The classifier 
is trained on lexical, orthographic, phonetic, and 
morphological features. 
The paper is organized as follows: Section 2 
provides a background on cross-document NE 
normalization; Section 3 describes the 
preprocessing steps and data used for training and 
testing;  Section 4 describes the normalization 
methodology; Section 5 describes the 
experimental setup;  Section 6 reports and 
discusses experimental results;  and Section 7 
concludes the paper and provides possible future 
directions. 
2. Background 
While considerable work has focused on named 
entity normalization within a single document, 
little work has focused on the challenges 
associated with resolving person name references 
across multiple documents.  Most of the work 
done in cross-document normalization focused on 
the problem of determining if two instances with 
the same name from different documents referring 
to the same person (Fleischman and Hovy, 2004).  
Fleischman and Hovy (2004) focused on 
distinguishing between individuals having 
identical names, but they did not extend 
normalization to different names referring to the 
same individual.  Their task is a subtask of what is 
examined in this paper.  They used a large number 
of features to accomplish their work, depending 
mostly on language specific dictionaries and 
wordnet.  Some these resources are not available 
for Arabic and many other languages.  Mann and 
Yarowsky (Mann and Yarowsky, 2003) examined 
the same problem but they treated it as a clustering 
task.  They focused on information extraction to 
build biographical profiles (date of birth, place of 
birth, etc.), and they wanted to disambiguate 
biographies belonging to different authors with 
identical names. 
Dozier and Zielund (Dozier and Zielund, 2004) 
reported on cross-document person name 
normalization in the legal domain.  They used a 
 
Figure 1 Normalization Model 
E1 
E3 
E7 E5 
E2 
E4 E6 
E8 
 
Normalization 
E1 
E4 E8 
E2 
E3 E7 
E5 
E6 
26
finite state machine that identifies paragraphs in a 
document containing the names of attorneys, 
judges, or experts and a semantic parser that 
extracts from the paragraphs template information 
about each named individual.  They relied on 
reliable biographies for each individual.  A 
biography would typically contain a person?s first 
name, middle name, last name, firm, city, state, 
court, and other information.  They used a 
Bayesian network to match the name mentions to 
the biographical records. 
Bhattacharya and Getoor (Bhattacharya and 
Getoor, 2006) introduced a collective decision 
algorithm for author name entity resolution, where 
decisions are not considered on an independent 
pairwise basis.  They focused on using relational 
links among the references and co-author 
relationships to infer collaboration groups, which 
would disambiguate entity names.  Such explicit 
links between co-authors can be extracted directly.  
However, implicit links can be useful when 
looking at completely unstructured text.  Other 
work has extended beyond entities of type ?person 
name? to include the normalization of location 
names (Li et al, 2002) and organizations (Ji and 
Grishman. 2004). 
3.  Preprocessing  and the Data Set 
For this work, a set of 7,184 person name entities 
was constructed.  Building new training and test 
sets is warranted, because the task at hand is 
sufficiently different from previously reported 
tasks in the literature.  The entities were 
recognized from 2,931 topically related documents 
(relating to the situation in the Gaza and Lebanon 
during July of 2006) from different Arabic news 
sources (obtained from searching the Arabic 
version of news.google.com).  The entities were 
recognized and normalized (within document) 
using the NERT system of Florian et al(2004).  
As shown in Figure 2, each entity is composed of 
a set of name mentions (one or more) and a set of 
nominal mentions (zero or more).  
The NERT system achieves an F-score of 0.77 
with precision of 0.82 and recall of 0.73 for person 
name mention and nominal recognition and an F-
score of 0.88 for tracking (assuming 100% 
recognition accuracy).  The produced entities may 
suffer from the following: 
1. Errant name mentions: Two name mentions 
referring to two different entities are 
concatenated into an errant name mention (e.g. 
?Bush Blair?, ?Ahmadinejad Bush?).  These 
types of errors stem from phrases such as ?The 
meeting of Bush Blair? and generally due to 
lack of sufficient punctuation marks. 
2. NE misrecognitions: Regular words are 
recognized as person name mentions and are 
embedded into person entities (e.g. Bush = 
George Bush = said). 
3. Errant entity tracking: name mentions of 
different entities are recognized as different 
mentions of the same entity (e.g. Bush = 
Clinton = Ahmadinejad). 
4. Lack of nominal mentions: Many entities do 
not contain any nominal mentions, which 
increases the entity ambiguity (especially 
when there is only one name mention 
composed of a single token).  
To overcome these problems, entities were 
preprocessed as follows: 
1. Errant name mentions such as ?Bush Blair? 
were automatically removed.  In this step, a 
dictionary of person name mentions was built 
from the 2,931 documents collection from 
which the entities were recognized and 
normalized along with the frequency of 
appearance in the collection.  For each entity, 
all its name mentions are checked in the 
dictionary and their frequencies are compared 
to each other.  Any name mention with a 
frequency less than 1/30 of the frequency of 
the name mention with the highest frequency 
is automatically removed (1/30 was picked 
based on manual examination of the training 
set).   Figure 2 Entity Description  
27
2. Name mentions formed of a single token 
consisting of less than 3 characters are 
removed.  Such names are almost always 
misrecognized name entities. 
3. Name entities with 10 or more different name 
mentions are automatically removed.  The 
NERT system often produces entities that 
include many different name mentions 
referring to different persons as one.  Such 
entities are errant because they over normalize 
name mentions.  Persons are referred to using 
a limited number of name mentions. 
4. Nominal mentions are stemmed using a 
context sensitive Arabic stemmer (Lee et al 
2003) to overcome the morphological 
complexity of Arabic.  For example, ?JKL?? = 
?president?, ? O?JKLQ ? = ?the president?, 
? O??JKLQ ? = ?and the president?, ? SKL?TU ? = ?its 
presidents? ? etc are stemmed to ?JKL?? = 
?president?.  
 
Cross-document entities are compared in a 
pairwise manner and binary decision is taken on 
whether they are the same.  Therefore, the 
available 7,184 entities lead to nearly 26 million 
pairwise comparisons (For N entities, the number 
of pair wise comparisons = 2
)1( ?NN ). 
Entity pairs were chosen to be included in the 
training set if they match any of the following 
criteria: 
1. Both entities have one shared name mention. 
2. Both entities have shared nominal mentions. 
3. A name mention in one of the entities is a 
substring of a name mention in the other 
entity. 
4. Both entities have nearly identical name 
mentions (small edit distance between both 
mentions). 
The resulting set was composed of 19,825 
pairs, which were manually judged to determine if 
they should be normalized or not.  These criteria 
skew the selection of pairs towards more 
ambiguous cases, which would be better 
candidates to train the intended SVM classifier, 
where the items near the boundary dividing the 
hyperplane are the most important.  For the 
training set, 18,503 pairs were normalized, and 
1,322 pairs were judged as different.  
Unfortunately, the training set selection criteria 
skewed the distribution of training examples 
heavily in favor of positive examples.  It would 
interesting to examine other training sets where 
the distribution of positives and negatives is 
balanced or skewed in favor of negatives. 
The test set was composed of 470 entities that 
were manually normalized into 253 classes, of 
which 304 entities were normalized to 87 classes 
and 166 entities remained unnormalized (forming 
single-entity classes).  Using 470 entities leads to 
110,215 pairwise comparisons.  The test set, which 
was distinct from the training set, was chosen 
using the same criteria as the training set.  Further, 
all duplicate (identical) entities were removed 
from the test set.  The selection criteria insure that 
the test set is skewed more towards ambiguous 
cases.  Randomly choosing entities would have 
made the normalization too easy.   
4. Normalization Methodology 
SVMLight, an SVM classifier (Joachims, 2002), 
was used for classification with a linear kernel and 
default parameters.  The following training 
features were employed: 
1. The percentage of shared name mentions 
between two entities calculated as: 
Name Commonality = 
? ??>< ???
?
???
?
namescommon j
i
j
i
f
f
f
f
2
2
1
1 ,min  
 where f1i is the frequency of the shared name 
mention in first entity, and f2i is the frequency 
of the shared name mention in the second 
entity.  ? f1i is the number of name mentions 
appearing in the entity. 
2. The maximum number of tokens in the shared 
name mentions, i.e. if there exists more than 
one shared name mention then this feature is 
the number of tokens in the longest shared 
name mention. 
3. The percentage of shared nominal mentions 
between two entities, and it is calculated as the 
name commonality but for nominal mentions.  
4. The smallest minimum edit distance 
(Levenshtein distance with uniform weights) 
between any two name mentions in both 
entities (Cohen et al, 2003) and this feature is 
only enabled when name commonality 
between both entities equals to zero. 
28
5. Phonetic edit distance, which is similar to edit 
distance except that phonetically similar 
characters, namely {(? ? t, ? ? T), (? ? k, ? ? 
q),(? ? d, ? ? D),(? ? v, ? ? s, ? ? S), (? ? *, 
? ? z, ? ? Z),(? ? j, ? ? g),(i?  ? p, k? ? h),(? ? <, 
n ? |, ? ,< ? ? ? A)1}, are normalized, vowels are 
removed, and spaces between tokens are 
removed. 
6. The number of tokens in the pair of name 
mentions that lead to the minimum edit 
distance. 
Some of the features might seem duplicative.  
However, the edit distance and phonetic edit 
distance are often necessary when names are 
transliterated into Arabic and hence may have 
different spellings and consequently no shared 
name mentions.  Conversely, given a shared name 
mention between a pair of entities will lead to zero 
edit distance, but the name commonality may also 
be very low indicating two different persons may 
have a shared name mention.  For example 
?Abdullah the second? and ?Abdullah bin 
Hussein? have the shared name mention 
?Abdullah? that leads to zero edit distance, but 
they are in fact two different persons.  In this case, 
the name commonality feature can be indicative of 
the difference.  Further, nominals are important in 
differentiating between identical name mentions 
that in fact refer to different persons (Fleischman 
and Hovy, 2004).  The number of tokens feature 
indicates the importance of the presence of 
similarity between two name mentions, as the 
similarity between name mentions formed of one 
token cannot be indicative for similarity when the 
number of tokens is more than one. 
Further, it is assumed that entities are transitive 
and are not available all at once, but rather the 
system has to normalize entities incrementally as 
they appear.  Therefore, for a given set of entity 
pairs, if the classifier deems that Entityi = Entityj 
and Entityj = Entityk, then Entityi is set to equal 
Entityk even if the classifier indicates that Entityi ? 
Entityk, and all entities (i, j, and k) are merged into 
one class.   
                                                 
1 Buckwalter transliteration scheme is used throughout 
the paper 
5. Experimental Setup 
Two baselines were established for the 
normalization process.  In the first, no entities are 
normalized, which produces single entity classes 
(?no normalization? condition).  In the second, any 
two entities having two identical name mentions in 
common are normalized (?surface normalization? 
condition).  For the rest of the experiments, focus 
was given to two main issues: 
1. Determining the effect of the different features 
used for classification. 
2. Determining the effect of varying the number 
of training examples.  
To determine the effect of different features, 
multiple classifiers were trained using different 
features, namely: 
? All features: all the features mentioned above 
are used,  
? Edit distance removed:  edit distance features 
(features 4, 5, and 6) are removed,  
? Number of tokens per name mention removed:  
the number of shared tokens and the number 
of tokens leading to the least edit distance 
(features 2 and 6) are removed.   
To determine the effect of training examples, 
the classifier was trained using all features but 
with a varying number of training example pairs, 
namely all 19,825 pairs, a set of randomly picked 
5,000 pairs, and a set of randomly picked 2,000 
pairs.   
For evaluation, 470 entities in test set were 
normalized into set of classes with different 
thresholds for the SVM classifier.  The quality of 
the clusters was evaluated using purity, entropy, 
and Cluster F-measure (CF-measure) in the 
manner suggested by Rosell et al (2004).  For the 
cluster quality measures, given cluster i (formed 
using automatic normalization) and each cluster j 
(reference normalization formed manually), cluster 
precision (p) and recall (r) are computed as 
follows: 
i
ij
ij n
n
p = , and 
j
ij
ij n
n
r = , where ni number of 
entities in cluster i, nj number of entities in cluster 
j, and nij number of shared entities between cluster 
i and j.  
The CF-measure for an automatic cluster i 
against a manually formed reference cluster j is:  
29
ijij
ijij
ij pr
pr
CF +
??= 2 , and the CF-measure for a 
reference cluster j is: 
}{max ijij CFCF = .  
The final CF-measure is computed over all the 
reference clusters as follows: ?= j jij CFn
n
CF . 
Purity of (?i) of an automatically produced 
cluster i is the maximum cluster precision obtained 
when comparing it with all the reference clusters 
as follows: }{max ijji p=? , and the weighted 
average purity over all clusters is: 
?= i i
i
ij
n
n ?? , where n is the total number of 
entities in the set to be normalized (470 in this 
case). 
As for entropy of a cluster, it is calculated as:  
??= j ijiji ppE log , and the average entropy 
as: 
?= i i
i
i E
n
nE . 
The CF-measure captures both precision and 
recall while purity and entropy are precision 
oriented measures (Rosell et al, 2004). 
6. Results and Discussion 
Figure 3 shows the purity and CF-measure for the 
two baseline conditions (no normalization, and 
surface normalization) and for the normalization 
system with different SVM thresholds.  Since 
purity is a precision measure, purity is 100% when 
no normalization is done.  The CF-measure is 62% 
and 74% for baseline runs with no normalization 
and surface normalization respectively.  As can be 
seen from the results, the baseline run based on 
exact matching of name mentions in entities 
achieves low CF-measure and low purity.  Low 
CF-measure values stem from the inability to 
match identical entities with different name 
mentions, and the low purity value stems from not 
disambiguating different entities with shared name 
mentions.  Some notable examples where the 
surface normalization baseline failed include:  
1. The normalization of the different entities 
referring to the Israeli soldier who is 
imprisoned in Gaza with different Arabic 
spellings for his name, namely ?tKuv ?Twux? 
(jlEAd $lyT), ?tKOTv ?Twux? (jlEAd $AlyT), 
?zKuv ?|}~O?? (the soldier $lyt), and so forth.  
2. The separation between ??T?O?? ? |?? ?u?O?? 
(King Abdullah the Second) and ? ?? ?? |?? ?u?O?
???wO? |??? (King Abdullah ibn Abdul-Aziz) 
that have a shared name mention ???|?? ?u?O?? 
(King Abdullah). 
3. The normalization of the different entities 
representing the president of Palestinian 
Authority with different name mentions, 
namely ???T? ???? (Abu Mazen) and ? ?????
?T??? (Mahmoud Abbas).   
 
The proposed normalization technique 
properly normalized the aforementioned examples.  
Given different SVM thresholds, Figure 3 shows 
that the purity of resultant classes increases as the 
SVM threshold increases since the number of 
normalized entities decreases as the threshold 
increases.  The best CF-measure of 93.1% is 
obtained at a threshold of 1.4 and as show in Table 
1 the corresponding purity and entropy are 97.2% 
and 0.056 respectively.  The results confirm the 
success of the approach. 
Table 1 highlights the effect of removing 
different training feature and the highest CF-
measures (at different SVM thresholds) as a result.  
The table shows that using all 6 features produced 
the best results and the removal of the shared 
names and tokens (features 2 and 6) had the most 
adverse effect on normalization effectiveness.  The 
adverse effect is reasonable especially given that 
some single token names such as ?Muhammad? 
and ?Abdullah? are very common and matching 
one of these names across entities is an insufficient 
indicator that they are the same.  Meanwhile, the 
exclusion of edit distance features (features 4, 5, 
and 6) had a lesser but significant adverse impact 
on normalization effectiveness.  Table 1 reports 
the best results obtained using different thresholds.  
Perhaps, a separate development set should be 
used for ascertaining the best threshold. 
Table 2 shows that decreasing the number of 
training examples (all six features are used) has a 
noticeable but less pronounced effect on 
normalization effectiveness compared to removing 
training features. 
30
 
Table 1 Quality of clusters as measured by purity (higher values are better), entropy (lower values are 
better), and CF-measure (higher values are better) for different feature sets.  Values are shown for max 
CF-measure.  Thresholds were tuned for max CF-measure for each feature configuration separately 
Training Data Purity Maximum CF-Measure Entropy Threshold 
No Normalization 100.0% 62.6% 0.000 - 
Baseline 83.4% 74.7% 0.151 - 
All Features 97.2% 93.1% 0.056 1.4 
Edit Distance removed 99.4% 85.5% 0.010 1.0 
# of tokens/name removed 96.6% 77.8% 0.071 1.5 
 
Normalization Evaluation
60%
65%
70%
75%
80%
85%
90%
95%
100%
No
No
rm
al
iza
tio
n
Ba
se
lin
e
1.
0
1.
1
1.
2
1.
3
1.
4
1.
5
1.
6
1.
7
1.
8
1.
9
2.
0
SVM Threshold
Purity
CF-Measure
 
Figure 3 Purity and cluster F-measure versus SVM Threshold 
 
Table 2 Effect of number of training examples on normalization effectiveness 
Training Data Purity Maximum CF-Measure Entropy Threshold 
20k training pairs 97.2% 93.1% 0.056 1.4 
5k training pairs 97.4% 90.5% 0.053 1.5 
2k training pairs 98.5% 90.3% 0.031 1.6 
 
7. Conclusion: 
This paper presented a two-step approach to cross-
document named entity normalization.  In the first 
step, preprocessing rules are used to remove errant 
named entities.  In the second step, a machine 
learning approach based on an SVM classifier to 
disambiguate different entities with matching 
name mentions and to normalize identical entities 
with different name mentions.  The classifier was 
trained on features that capture name mentions and 
nominals overlap between entities, edit distance, 
and phonetic similarity.  In evaluating the quality 
of the clusters, the reported approach achieved a 
cluster F-measure of 0.93.  The approach 
outperformed that two baseline approaches in 
which no normalization was done or normalization 
was done when two entities had matching name 
31
mentions.  The two approaches achieved cluster F-
measures of 0.62 and 0.74 respectively. 
For future work, implicit links between entities 
in the text can serve as the relational links that 
would enable the use of entity attributes in 
conjunction with relationships between entities.  
An important problem that has not been 
sufficiently explored is cross-lingual cross-
document normalization.  This problem would 
pose unique and interesting challenges.  The 
described approach could be generalized to 
perform normalization of entities of different types 
across multilingual documents.  Also, the 
normalization problem was treated as a 
classification problem.  Examining the problem as 
a clustering (or alternatively an incremental 
clustering) problem might prove useful.  Lastly, 
the effect of cross-document normalization should 
be examined on applications such as information 
extraction, information retrieval, and relationship 
and social network visualization.   
References: 
Bhattacharya I. and Getoor L. ?A Latent Dirichlet 
Allocation Model for Entity Resolution.? 6th SIAM 
Conference on Data Mining (SDM), Bethesda, USA, 
April 2006. 
Chinchor N., Brown E., Ferro L., and Robinson P.  
?Named Entity Recognition Task Definition.? 
MITRE, 1999. 
Cohen W., Ravikumar P., and Fienberg S. E. ?A 
Comparison of String Distance Metrics for Name-
Matching Tasks.?  In Proceedings of the 
International Joint Conference on Artificial 
Intelligence, 2003. 
Dozier C. and Zielund T. ?Cross-document Co-
Reference Resolution Applications for People in the 
Legal Domain.? In 42nd Annual Meeting of the 
Association for Computational Linguistics, 
Reference Resolution Workshop, Barcelona, Spain. 
July 2004. 
Fleischman M. B. and Hovy E. ?Multi-Document 
Person Name Resolution.?  In 42nd Annual Meeting 
of the Association for Computational Linguistics, 
Reference Resolution Workshop, Barcelona, Spain. 
July 2004. 
Ji H. and Grishman R. ?Applying Coreference to 
Improve Name Recognition?. In 42nd Annual 
Meeting of the Association for Computational 
Linguistics, Reference Resolution Workshop, 
Barcelona, Spain. July (2004). 
Ji H. and Grishman R. "Improving Name Tagging by 
Reference Resolution and Relation Detection." ACL 
2005 
Joachims T. ?Learning to Classify Text Using Support 
Vector Machines.? Ph.D. Dissertation, Kluwer, 
(2002). 
Joachims T. ?Optimizing Search Engines Using Click-
through Data.?  Proceedings of the ACM Conference 
on Knowledge Discovery and Data Mining (KDD), 
(2002).  
Lee Y. S., Papineni K., Roukos S., Emam O., Hassan 
H. ?Language Model Based Arabic Word 
Segmentation.?  In ACL 2003, pp. 399-406, (2003). 
Li H., Srihari R. K., Niu C., and Li W. ?Location 
Normalization for Information Extraction.?  
Proceedings of the 19th international conference on 
Computational linguistics, pp. 1-7, 2002 
Li H., Srihari R. K., Niu C., and Li W. ?Location 
Normalization for Information Extraction.?  
Proceedings of the sixth conference on applied 
natural language processing, 2000. pp. 247 ? 254. 
Mann G. S. and Yarowsky D. ?Unsupervised Personal 
Name Disambiguation.? Proceedings of the seventh 
conference on Natural language learning at HLT-
NAACL 2003.  pp. 33-40. 
Maynard D., Tablan V., Ursu C., Cunningham H., and 
Wilks Y. ?Named Entity Recognition from Diverse 
Text Types.? Recent Advances in Natural Language 
Processing Conference, (2001). 
Palmer D. D. and Day D. S. ?A statistical Profile of the 
Named Entity Task?.  Proceedings of the fifth 
conference on Applied natural language processing, 
pp. 190-193, (1997). 
R. Florian R., Hassan H., Ittycheriah A., Jing H., 
Kambhatla N., Luo X., Nicolov N., and Roukos S. 
?A Statistical Model for Multilingual Entity 
Detection and Tracking.?  In HLT-NAACL, 2004. 
Rosell M., Kann V., and Litton J. E.  ?Comparing 
Comparisons: Document Clustering Evaluation 
Using Two Manual Classifications.?  In ICON 2004 
Sekine S. ?Named Entity: History and Future?.  Project 
notes, New York University, (2004). 
Solorio T. ?Improvement of Named Entity Tagging by 
Machine Learning.?  Ph.D. thesis, National Institute 
of Astrophysics, Optics and Electronics, Puebla, 
Mexico, September 2005. 
 
32
BioNLP 2007: Biological, translational, and clinical language processing, pages 89?96,
Prague, June 2007. c?2007 Association for Computational Linguistics
BioNoculars: Extracting Protein-Protein Interactions from Biomedical Text
Amgad Madkour, *Kareem Darwish, Hany Hassan, Ahmed Hassan, Ossama Emam
Human Language Technologies Group
IBM Cairo Technology Development Center
P.O.Box 166 El-Ahram, Giza, Egypt
{amadkour,hanyh,hasanah,emam}@eg.ibm.com,*kareem@darwish.org
Abstract
The vast number of published medical doc-
uments is considered a vital source for rela-
tionship discovery. This paper presents a sta-
tistical unsupervised system, called BioNoc-
ulars, for extracting protein-protein interac-
tions from biomedical text. BioNoculars
uses graph-based mutual reinforcement to
make use of redundancy in data to construct
extraction patterns in a domain independent
fashion. The system was tested using MED-
LINE abstract for which the protein-protein
interactions that they contain are listed in the
database of interacting proteins and protein-
protein interactions (DIPPPI). The system
reports an F-Measure of 0.55 on test MED-
LINE abstracts.
1 Introduction
With the ever-increasing number of published
biomedical research articles and the dependency
of new research and previously published research,
medical researchers and practitioners are faced with
the daunting prospect of reading through hundreds
or possibly thousands of research articles to sur-
vey advances in areas of interest. Much work has
been done to ease access and discovery of articles
that match the interest of researchers via the use
of search engines such as PubMed, which provides
search capabilities over MEDLINE, a collection of
more than 15 million journal paper abstracts main-
tained by the National Library of Medicine (NLM).
However, with the addition of abstracts from more
than 5,000 medical journals to MEDLINE every
year, the number of articles containing information
that is pertinent to users needs has grown consider-
ably. These 5,000 journals constitute only a subset
of the published biomedical research. Further, med-
ical articles often contain redundant information and
only subsections of articles are typically of direct in-
terest to researchers. More advanced information
extraction tools have been developed to effectively
distill medical articles to produce key pieces of in-
formation from articles while attempting to elimi-
nate redundancy. These tools have focused on areas
such as protein-protein interaction, gene-disease re-
lationship, and chemical-protein interaction (Chun
et al, 2006). Many of these tools have been used
to extract key pieces of information from MED-
LINE. Most of the reported information extraction
approaches use sets of handcrafted rules in conjunc-
tion with manually curated dictionaries and ontolo-
gies.
This paper presents a fully unsupervised statisti-
cal technique to discover protein-protein interaction
based on automatically discoverable repeating pat-
terns in text that describe relationships. The paper
is organized as follows: section 2 surveys related
work; section 3 describes BioNoculars; Section 4
describes the employed experimental setup; section
5 reports and comments on experimental results; and
section 6 concludes the paper.
2 Background
The background will focus primarily on the tagging
of Biomedical Named Entities (BNE), such genes,
gene-products, proteins, and chemicals and the Ex-
89
traction of protein-protein interactions from text.
2.1 BNE Tagging
Concerning BNE tagging, the most common ap-
proaches are based on hand-crafted rules, statisti-
cal classifiers, or a hybrid of both (usually in con-
junction with dictionaries of BNE). Rule-based sys-
tems (Fukuda et al, 1998; Hanisch et al, 2003; Ya-
mamoto et al, 2003) that use dictionaries tend to
exhibit high precision in tagging named entities but
generally with lower tagging recall. They tend to
lag the latest published research and are sensitive
to the expression of the named entities. Dictionar-
ies of BNE are typically laborious and expensive to
build, and they are dependant on nomenclatures and
specific species. Statistical approaches (Collier et
al., 2000; Kazama et al, 2002; Settles, 2004) typ-
ically improve recall at the expense of precision,
but are more readily retargetable for new nomen-
clatures and organisms. Hybrid systems (Tanabe
and Wilbur, 2002; Mika and Rost, 2004) attempt to
take advantage of both approaches. Although these
approaches tend to generate acceptable recognition,
they are heavily dependent on the type of data on
which they are trained.
(Fukuda et al, 1998) proposed a rule-based pro-
tein name extraction system called PROPER (PRO-
tein Proper-noun phrase Extracting Rules) system,
which utilizes a set of rules based on the surface
form of text in conjunction with a Part-Of-Speech
(POS) tagging to identify what looks like a protein
without referring to any specific BNE dictionary.
They reported a 94.7% precision and a 98.84% re-
call for the identification of BNEs. The results that
they achieved seem to be too specific to their train-
ing and test sets.
(Hanisch et al, 2003) proposed a rule-based
protein and gene name extraction system called
ProMiner, which is based on the construction of a
general-purpose dictionary along with different dic-
tionaries of synonyms and an automatic curation
procedure based on a simple token model of protein
names. Results showed that their system achieved a
0.80 F-measure score in the name extraction task on
the BioCreative test set (BioCreative).
(Yamamoto et al, 2003) proposed the use of mor-
phological analysis to improve protein name tag-
ging. Their approach tags proteins based on mor-
pheme chunking to properly determine protein name
boundary. They used the GENIA corpus for training
and testing and obtained an F-measure score of 0.70
for protein name tagging.
(Collier et al, 2000) used a machine learning ap-
proach to protein name extraction based on a linear
interpolation Hidden Markov Model (HMM) trained
using bi-grams. They focused on finding the most
likely protein sequence classes (C) for a given se-
quence of words (W), by maximizing the probabil-
ity of C given W, P(C?W). Unlike traditional dic-
tionary based methods, the approach uses no manu-
ally crafted patterns. However, their approach may
misidentify term boundaries for phrases containing
potentially ambiguous local structures such as co-
ordination and parenthesis. They reported an F-
measure score of 0.73 for different mixtures of mod-
els tested on 20 abstracts.
(Kazama et al, 2002) proposed a machine learn-
ing approach to BNE tagging based on support vec-
tor machines (SVM), which was trained on the GE-
NIA corpus. Their preliminary results of the system
showed that the SVM with the polynomial kernel
function outperforms techniques of Maximum En-
tropy based systems.
Yet another BNE tagging system is ABNER (Set-
tles, 2005), which utilizes machine learning, namely
conditional random fields, with a variation of or-
thographic and contextual features and no seman-
tic or syntactic features. ABNER achieves an F-
measure score of 0.71 on the NLPA 2004 shared
task dataset corpus and 0.70 on the BioCreative cor-
pus.and scored an F1-measure of 51.8set.
(Tanabe and Wilbur, 2002) used a combination
of statistical and knowledge-based strategies, which
utilized automatically generated rules from transfor-
mation based POS tagging and other generated rules
from morphological clues, low frequency trigrams,
and indicator terms. A key step in their method is
the extraction of multi-word gene and protein names
that are dominant in the corpus but inaccessible to
the POS tagger. The advantage of such an approach
is that it is independent of any biomedical domain.
However, it can miss single word gene names that
do not occur in contextual gene theme terms. It
can also incorrectly tag compound gene names, plas-
mids, and phages.
(Mika and Rost, 2004) developed NLProt, which
90
combines the use of dictionaries, rules-based filter-
ing, and machine learning based on an SVM classi-
fier to tag protein names in MEDLINE. The NLProt
system used rules for pre-filtering and the SVM for
classification, and it achieved a precision of 75% and
recall 76%.
2.2 Relationship Extraction
As for the extraction of interactions, most efforts in
extraction of biomedical interactions between enti-
ties from text have focused on using rule-based ap-
proaches due to the familiarity of medical terms that
tend to describe interactions. These approaches have
proven to be successful with notably good results. In
these approaches, most researchers attempted to de-
fine an accurate set of rules to describe relationship
types and patterns and to build ontologies and dic-
tionaries to be consulted in the extraction process.
These rules, ontologies, and dictionaries are typi-
cally domain specific and are often not generalizable
to other problems.
(Blaschke et al, 1999) reported a domain spe-
cific approach for extracting protein-protein interac-
tions from biomedical text based on a set of pre-
defined patterns and words describing interactions.
Later work attempted to automatically extract inter-
actions, which are referenced in the database of in-
teracting proteins (Xenarios et al, 2000), from the
text mentioning the interactions (Blaschke and Va-
lencia, 2001). They achieved surprisingly low recall
(25%), which they attributed to problems in properly
identifying protein names in the text.
(Koike et al, 2005) developed a system called
PRIME, which was used to extract biological func-
tions of genes, proteins, and their families. Their
system used a shallow parser and sentence struc-
ture analyzer. They extracted so-called ACTOR-
OBJECT relationships from the shallow parsed sen-
tences using rule based sentence structure analysis.
The identification of BNEs was done by consulting
the GENA gene name dictionary and family name
dictionary. In extracting the biological functions of
genes and proteins, their system reported a recall of
64% and a precision of 94%.
Saric et al developed a system to extract gene
expression regulatory information in yeast as well
as other regulatory mechanisms such phosphoryla-
tion (Saric et al, 2004; Saric et al, 2006). They
used a rule based named entity recognition module,
which recognizes named entities via cascading finite
state automata. They reported a precision of 83-90%
and 86-95% for the extraction of gene expression
and phosphorylation regulatory information respec-
tively.
(Leroy and Chen, 2005) used linguistic parsers
and Concept Spaces, which use a generic co-
occurrence based technique that extracts relevant
medical phrases using a noun chunker. Their system
employed UMLS (Humphreys and Lindberg, 1993),
GO (Ashburner et al, 2000), and GENA (Koike and
Takagi, 2004) to further improve extraction. Their
main purpose was entity identification and cross ref-
erence to other databases to obtain more knowledge
about entities involved in the system.
Other extraction approaches such as the one re-
ported on by (Cooper and Kershenbaum, 2005) uti-
lized a large manually curated dictionary of many
possible combinations of gene/protein names and
aliases from different databases and ontologies.
They annotated their corpus using a dictionary-
based longest matching technique. In addition, they
used filtering with a maximum entropy based named
entity recognizer in order to remove the false posi-
tives that were generated from merging databases.
The problem with this approach is the resulting in-
consistencies from merging databases, which could
hurt the effectiveness of the system. They reported
a recall of 87.1 % and a precision of 78.5% in the
relationship extraction task.
Work by (Mack et al, 2004) used the Munich In-
formation Center for Protein Sequences (MIPS) for
entity identification. Their system was integrated in
the IBM Unstructured Information Management Ar-
chitecture (UIMA) framework (Ferrucci and Lally,
2004) for tokenization, identification of entities, and
extraction of relations. Their approach was based on
a combination of computational linguistics, statis-
tics, and domain specific rules to detect protein in-
teractions. They reported a recall of 61% and a pre-
cision of 97%.
(Hao et al, 2005) developed an unsupervised ap-
proach, which also uses patterns that were deduced
using minimum description lengths. They used pat-
tern optimization techniques to enhance the patterns
by introducing most common keywords that tend to
describe interactions.
91
(Jo?rg et. al., 2005) developed Ali Baba which
uses sequence alignments applied to sentences an-
notated with interactions and part of speech tags.It
also uses finite state automata optimized with a ge-
netic algorithm in its approach. It then matches the
generated patterns against arbitrary text to extract in-
teractions and their respective partners. The system
scored an F1-measure of 51.8% on the LLL?05 eval-
uation set.
The aforementioned systems used either rule-
based approaches, which require manual interven-
tion from domain experts, or statistical approaches,
either supervised or semi-supervised, which also re-
quire manually curated training data.
3 BioNoculars
BioNoculars is a relationship extraction system that
based on a fully unsupervised technique suggested
by (Hassan et al, 2006) to automatically extract
protein-protein interaction from medical articles. It
can be retargeted to different domains such as pro-
tein interactions in diseases. The only requirement
is to compile domain specific taggers and dictionar-
ies, which would aid the system in performing the
required task.
The approach uses an unsupervised graph-based
mutual reinforcement, which depends on the con-
struction of generalized extraction patterns that
could match instances of relationships (Hassan et
al., 2006). Graph-based mutual reinforcement is
similar to the idea of hubs and authorities in web
pages depicted by the HITS algorithm (Kleinberg,
1998). The basic idea behind the algorithm is that
the importance of a page increases when more and
more good pages link to it. The duality between pat-
terns and extracted information (tuples) leads to the
fact that patterns could express different tuples, and
tuples in turn could be expressed by different pat-
terns. Tuple in this context contains three elements,
namely two proteins and the type of interaction be-
tween them. The proposed approach is composed of
two main steps, namely initial pattern construction
and then pattern induction.
For pattern construction, the text is POS tagged
and BNE tagged. The tags of Noun Phrases or se-
quences of nouns that constitute a BNE are removed
and replaced with a BNE tag. Then, an n-gram lan-
guage model is built on the tagged text (using tags
only) and is used to construct weighted finite state
machines. Paths with low cost (high language model
probabilities) are chosen to construct the initial set
of patterns; the intuition is that paths with low cost
(high probability) are frequent and could represent
potential candidate patterns. The number of candi-
date initial patterns could be reduced significantly
by specifying the candidate types of entities of in-
terest. In the case of BioNoculars, the focus was
on relationships between BNEs of type PROTEIN.
The candidate patterns are then applied to the tagged
stream to produce in-sentence relationship tuples.
As for pattern induction, due to the duality in the
patterns and tuples relation, patterns and tuples are
represented by a bipartite graph as illustrated in Fig-
ure 1.
Figure 1: A bipartite graph representing patterns and
tuples
Each pattern or tuple is represented by a node in
the graph. Edges represent matching between pat-
terns and tuples. The pattern induction problem can
be formulated as follows: Given a very large set of
data D containing a large set of patterns P, which
match a large set of tuples T, the problem is to iden-
tify , which is the set of patterns that match the set
of the most correct tuples T. The intuition is that
the tuples matched by many different patterns tend
to be correct and the patterns matching many differ-
ent tuples tend to be good patterns. In other words,
BioNoculars attempts to choose from the large space
of patterns in the data the most informative, high-
est confidence patterns that could identify correct tu-
ples; i.e. choosing the most authoritative patterns in
analogy with the hub-authority problem. The most
authoritative patterns can then be used for extracting
relations from free text. The following pattern-tuple
pairs show how patterns can match tuples in the cor-
pus:
(protein) (verb) (noun) (prep.) (protein)
92
Cla4 induces phosphorylation of Cdc24
(protein) (I-protein) (Verb) (prep.) (protein)
NS5A interacts with Cdk1
The proposed approach represents an unsuper-
vised technique for information extraction in general
and particularly for relations extraction that requires
no seed patterns or examples and achieves signifi-
cant performance. Given enough domain text, the
extracted patterns can support many types of sen-
tences with different styles (such passive and active
voice) and orderings (the interaction of X and Y vs.
X interacts with Y).
One of the critical prerequisites of the above-
mentioned approach is the use of a POS tagger,
which is tuned for biomedical text, and a BNE tag-
ger to properly identify BNEs. Both are critical for
determining the types of relationships that are of in-
terest. For POS tagging, a decision tree based tagger
developed by (Schmid, 1994) was used in combi-
nation with a model, which was trained on a cor-
rected/revised GENIA corpus provided by (Saric et
al., 2004) and was reported to achieve 96.4% tagging
accuracy (Saric et al, 2006). This POS tagger will
be referred to as the Schmid tagger. For BNE tag-
ging, ABNER was used. The accuracy of ABNER
is approximately state of the art with precision and
recall of 74.5% and 65.9% respectively with training
done using the BioCreative corpora (BioCreative).
Nonetheless we still face entity identification prob-
lems such as missed identifications in the text which
in turn affects our results considerably. We do be-
lieve if we use a better identification method , we
would yield better results.
4 Experimental Setup
Experiments aimed at extracting protein-protein
interactions for Bakers yeast (Sacharomyces
Cerevesiae) to assess BioNoculars (Cherry et al,
1998). The experiments were performed using
109,440 MEDLINE abstracts that contained the
varying names of the yeast, namely Sacharomyces
cerevisiae, S. Cerevisiae, Bakers yeast, Brewers
yeast and Budding yeast. MEDLINE abstracts
typically summarize the important aspects of papers
possibly including protein-protein interactions if
they are of relevance to the article. The goal was
to deduce the most appropriate extraction patterns
that can be later used to extract relations from any
document. All the MEDLINE abstracts were used
for pattern extraction except for 70 that were set
aside for testing. There were no test documents in
the training set. To build ground-truth, the test set
was semi-manually POS and BNE tagged. They
were also annotated with the interactions that are
contained in the text. There was a condition that
all the abstracts that are used for testing must have
entries in the Database of Interacting Proteins and
Protein-Protein Interactions (DIPPPI), which is
a subset of the Database of Interacting Proteins
(DIP) (Xenarios et al, 2000) restricted to proteins
from yeast. DIPPPI lists the known protein-protein
interactions in the MEDLINE abstracts. There were
297 protein-protein interactions in the test set of 70
abstracts. One of the disadvantages of DIPPPI is
that the presence of interactions is indicated without
mentioning their types or from which sentences
they were extracted. Although BioNoculars is able
to guess the sentence from which an interaction was
extracted and the type of interaction, this informa-
tion was ignored when evaluating against DIPPPI.
Unfortunately, there is no standard test set for the
proposed task, and most of the evaluation sets are
proprietary. The authors hope that others can benefit
from their test set, which is freely available.
The abstracts used for pattern extraction were
POS tagged using the Schmid tagger and BNE tag-
ging was done using ABNER. The patterns were re-
stricted to only those with protein names. For extrac-
tion of interaction tuples, the test set was POS and
BNE tagged using the Schmid tagger and ABNER
respectively. A varying number of final patterns
were then used to extract tuples from the test set and
the average recall and precision were computed. An-
other setup was used in which the relationships were
filtered using preset keywords for relationships such
as inhibits, interacts, and activates to properly com-
pare BioNoculars to systems in the literature that use
such keywords. The keywords were obtained from
the (Hakenberg et al, 2005) and (Temkin and Gilder,
2003). One of the generated pattern-tuple pairs was
as follows:
(PROTEIN) (Verb) (Conjunction) (PROTEIN)
NS5A interacts with Cdk1
One consequence of tuple extraction is generation
of redundant tuples, which contain the same enti-
93
Pattern Count 30 59 78 103 147 192 205 217
Recall 0.51 0.70 0.76 0.81 0.84 0.89 0.89 0.93
Precision 0.47 0.42 0.43 0.35 0.30 0.26 0.26 0.16
FMeasure 0.49 0.53 0.55 0.49 0.44 0.40 0.40 0.27
Table 1: Recall, Precision, and F-measure for extrac-
tion of tuples using a varying number of top rated
patterns
ties and relations. Consequently, all protein aliases
and full text names were resolved to a unified nam-
ing scheme and the unified scheme was used to re-
place all variations of protein names in patterns. All
potential protein-protein interactions that BioNocu-
lars extracted were compared to those in the DIPPPI
databases.
5 Results and Discussion
For the first set of experiments, the experimental
setup described above was used without modifica-
tion. Table 1 and Figure 2 report on the resulting
recall and precision when taking different number
of highest rated patterns. The highest rated 217 pat-
terns were divided on a linear scale into 8 clusters
based on their relative weights.
Figure 2: Recall, Precision, and F-measure for tuple
extraction using a varying number of top patterns
As expected, Figure 2 clearly shows an inverse
relationship between precision and recall. This is
because using more extraction patterns yields more
tuples thus increasing recall at the expense of pre-
cision. The F-measure (with ? = 1) peeks at 78
patterns, which seems to provide the best score
given that precision and recall are equally important.
However, the technique seems to favor recall, reach-
ing a recall of 93% when using all 217 patterns. The
Pattern Count 30 59 78 103 147 192 205 217
Recall 0.31 0.44 0.46 0.48 0.64 0.73 0.74 0.78
Precision 0.31 0.36 0.35 0.34 0.39 0.35 0.35 0.37
FMeasure 0.31 0.40 0.40 0.40 0.48 0.47 0.48 0.50
Table 2: Recall, Precision, and Recall for extraction
of tuples using a varying number of top rated patters
keyword filtering
low precision levels warrant thorough investigation.
In the second set of experiments, extracted tuples
were filtered using preset keywords indicating inter-
actions. Table 2 and Figure 3 show the results of the
experiments.
Figure 3: Recall, Precision, and F-measure for tu-
ple extraction using a varying number of top patterns
with keyword filtering
The results show that filtering with keywords led
to lower recall, but precision remained fairly steady
as the number of patterns changed. Nonetheless, the
best precision in Figure 3 is lower than the best pre-
cision in Figure 2 and the maximum F-measure for
this set of experiments is lower than the maximum
F-measure when no filtering was used. The BioNoc-
ulars system with no filtering can be advantageous
for recall oriented applications. The use of no filter-
ing suggests that some interaction may be expressed
in more generic forms or patterns. An intermediate
solution would be to increase the size of the list of
most commonly occurring keywords to filter the ex-
tracted tuples further.
Currently, ABNER, which is used by the system,
has a precision of 75.4% and a recall of 65.9%. Per-
haps improved tagging may improve the extraction
effectiveness.
The effectiveness of BioNoculars needs to be
94
thoroughly compared to existing systems via the use
of standard test sets, which are not readily available.
Most of previously reported work has been tested
on proprietary test sets or sets that are not publicly
available. The creation of standard publicly avail-
able test set can prompt research in this area.
6 Conclusion and Future Work
This paper presented a system for extracting
protein-protein interaction from biomedical text call
BioNoculars. BioNoculars uses a statistical un-
supervised learning algorithm, which is based on
graph mutual reinforcement and data redundancy
to extract extraction patterns. The system is re-
call oriented and is able to properly extract 93% of
the interaction mentions from test MEDLINE ab-
stracts. Nonetheless, the systems precision remains
low. Precision can be enhanced by using keywords
that describe interactions to filter to the resulting in-
teraction, but this would be at the expense of recall.
As for future work, more attention should be fo-
cused on improving extraction patterns. Currently,
the system focuses on extracting interactions be-
tween exactly two proteins. Some of the issues that
need to be handled include complex relationship (X
and Y interact with A and B), linguistic variabil-
ity (passive vs. active voice; presence of superflu-
ous words such as modifiers, adjectives, and prepo-
sitional phrases), protein lists (W interacts with X,
Y, and Z), nested interactions (W, which interacts
with X, also interacts with Y). Resolving these is-
sues would require an investigation of how patterns
can be generalized in automatic or semi-automatic
ways. Further, the identification of proteins in the
text requires greater attention. Also, the BioNocu-
lars approach can be combined with other rule-based
approaches to produce better results.
References
Ashburner, M., C. A. Ball, J. A. Blake, D. Botstein, H.
Butler, J. M. Cherry, A. P. Davis, K. Dolinski, S. S.
Dwight, J. T. Eppig, M. A. Harris, D. P. Hill, L. Issel-
Tarver, A. Kasarskis, S. Lewis, J. C. Matese, J. E.
Richardson, M. Ringwald, G. M. Rubin, and G. Sher-
lock. 2000. Gene ontology: tool for the unification of
biology. Nature Genetics,volume 25 pp.25-29.
BioCreative. 2004. [Online].
Blaschke C., M. A. Andrade, C. Ouzounis, and A. Valen-
cia. 1999. Automatic Extraction of Biological Infor-
mation from Scientific Text: Protein-Protein Interac-
tions. ISMB99, pp. 60-67.
Blaschke, C. and A. Valencia. 2001. Can Bibliographic
Pointers for Known Biological Protein Interactions
as a Case Study. Comparative and Functional Ge-
nomics,vol. 2: 196-206.
Cherry, J. M., C. Adler, C. Ball, S. A. Chervitz, S. S.
Dwight, E. T. Hester, Y. Jia, G. Juvik, T. Roe, M.
Schroeder, S. Weng, and D. Botstein. 1998. SGD:
Saccharomyces Genome Database. Nucleic Acids Re-
search, 26, 73-9.
Chun, H. W., Y. Tsuruka, J. D. Kim, R. Shiba, N. Nagata,
T. Hishiki, and J. Tsujii. 2006. Extraction of Gene-
Disease Relations from MEDLINE Using Domain Dic-
tionaries and Machine Learning. Pacific Symposium
on Biocomputing 11:4-15.
Collier, N., C. Nobata, and J. Tsujii. 2000. Extracting
the Names of Genes and Gene Products with a Hidden
Markov Model. COLING, 2000, pp. 201207.
Cooper, J. and A. Kershenbaum. 2005. Discovery of
protein-protein interactions using a combination of
linguistic, statistical and graphical information. BMC
Bioinformatics.
DIPPPI http://www2.informatik.hu-berlin.de/ haken-
ber/corpora. 2006.
Ferrucci, D. and A. Lally. 2004. UIMA: an architec-
tural approach to unstructured information processing
in the corporate research environment. Natural Lan-
guage Engineering 10, No. 3-4, 327-348.
Fukuda, K., T. Tsunoda, A. Tamura, and T. Takagi. 1998.
Toward information extraction: identifying protein
names from biological papers. PSB, pages 705716.
Hakenberg, J., C. Plake, U. Leser, H. Kirsch, and D.
Rebholz-Schuhmann. 2005. LLL?05 Challenge:
Genic Interaction Extraction with Alignments and Fi-
nite State Automata. Proc Learning Language in Logic
Workshop (LLL?05) at ICML 2005, pp. 38-45. Bonn,
Germany.
Hanisch, D., J. Fluck, HT. Mevissen, and R. Zimmer.
2003. Playing biologys name game: identifying pro-
tein names in scientific text. PSB, pages 403414.
Hao, Y., X. Zhu, M. Huang, and M. Li. 2005. Discov-
ering patterns to extract protein-protein interactions
from the literature: Part II. Bioinformatics, Vol. 00
no. 0 2005 pages 1-7.
95
Hassan, H., A. Hassan, and O. Emam. 2006. Un-
supervised Information Extraction Approach Using
Graph Mutual Reinforcement. Proceedings of Em-
pirical Methods for Natural Language Processing (
EMNLP ).
Humphreys B. L. and D. A. B. Lindberg. 1993. The
UMLS project: making the conceptual connection be-
tween users and the information they need. Bulletin of
the Medical Library Association, 1993; 81(2): 170.
Jo?rg Hakenberg, Conrad Plake, Ulf Leser. 2005. Genic
Interaction Extraction with Alignments and Finite
State Automata. Proc Learning Language in Logic
Workshop (LLL?05) at ICML 2005, pp. 38-45. Bonn,
Germany (August 2005)
Kazama, J., T. Makino, Y. Ohta, and J. Tsujii. 2002. Tun-
ing Support Vector Machines for Biomedical Named
Entity Recognition. ACL Workshop on NLP in
Biomedical Domain, pages 18.
Kleinberg, J. 1998. Authoritative sources in a hy-
perlinked environment. In Proc. Ninth Ann. ACM-
SIAM Symp. Discrete Algorithms, pages 668-677,
ACM Press, New York.
Koike A. and T. Takagi. 2004. Gene/protein/family
name recognition in biomedical literature. BioLINK
2004: Linking Biological Literature, Ontologies, and
Database, pp. 9-16.
Koike, A., Y. Niwa, and T. Takagi 2005. Automatic
extraction of gene/protein biological functions from
biomedical text. Bioinformatics, Vol. 21, No. 7.
Leroy, G. and H. Chen. 2005. Genescene: An Ontology-
enhanced Integration of Linguistic and Co-Occurance
based Relations in Biomedical Text. JASIST Special
Issue on Bioinformatics.
Mack, R. L., S. Mukherjea, A. Soffer, N. Uramoto, E. W.
Brown, A. Coden, J. W. Cooper, A. Inokuchi, B. Iyer,
Y. Mass, H. Matsuzawa, L. V. Subramaniam. 2004.
Text analytics for life science using the Unstructured
Information Management Architecture. IBM Systems
Journal 43(3): 490-515.
Mika, S. and B. Rost. 2004. NLProt: extracting pro-
tein names and sequences from papers. Nucleic Acids
Research, 32 (Web Server issue): W634W637.
Saric, J., L. J. Jensen, R. Ouzounova, I. Rojas, and P.
Bork. 2004. Extracting regulatory gene expression
networks from PUBMED. Proceedings of the 42nd
Annual Meeting of the Association for Computational
Linguistics, Barcelona, Spain, pp.191-198.
Saric, J., L. J. Jensen, R. Ouzounova, I. Rojas, and P.
Bork. 2006. Extraction of regulatory gene/protein
networks from Medline. Bioinformatics Vol.22 no
6,pp. 645-650.
Schmid, H. 1994. Probabilistic Part-of-Speech Tagging
Using Decision Trees. In the International Conference
on New Methods in Language Processing, Manch-
ester, UK.
Settles, B. 2004. Biomedical Named Entity Recognition
Using Conditional Random Fields and Rich Feature
Sets. In Proceedings of the International Joint Work-
shop on Natural Language Processing in Biomedicine
and its Applications (NLPBA), Geneva, Switzerland,
pages 104-107.
Settles, B. 2005. ABNER: an open source tool for au-
tomatically tagging genes, proteins, and other entity
names in text. Bioinformatics, 21(14): 3191-3192.
Tanabe L., and W. J. Wilbur. 2002. Tagging gene
and protein names in biomedical text. Bioinformatics,
18(8):11241132.
Temkin, J. M. and M. R. Gilder. 2003. Extraction
of protein interaction information from unstructured
text using a context-free grammar. Bioinformatics
19(16):2046-2053.
Xenarios I, Rice DW, Salwinski L, Baron MK, Marcotte
EM, Eisenberg D. 2000. DIP: the Database of Inter-
acting Proteins. Nucleic Acids Res 28: 289291.
Yamamoto, K., T. Kudo, A. Konagaya, Y. Matsumoto.
2003. Protein Name Tagging for Biomedical Annota-
tion in Text. Proceedings of the ACL 2003 Workshop
on Natural Language Processing in Biomedicine, pp.
65-72.
96
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1577?1586,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Social Text Normalization using Contextual Graph Random Walks
Hany Hassan
Microsoft Research
Redmond, WA
hanyh@microsoft.com
Arul Menezes
Microsoft Research
Redmond, WA
arulm@microsoft.com
Abstract
We introduce a social media text normal-
ization system that can be deployed as a
preprocessing step for Machine Transla-
tion and various NLP applications to han-
dle social media text. The proposed sys-
tem is based on unsupervised learning of
the normalization equivalences from unla-
beled text. The proposed approach uses
Random Walks on a contextual similarity
bipartite graph constructed from n-gram
sequences on large unlabeled text corpus.
We show that the proposed approach has a
very high precision of (92.43) and a rea-
sonable recall of (56.4). When used as
a preprocessing step for a state-of-the-art
machine translation system, the translation
quality on social media text improved by
6%. The proposed approach is domain and
language independent and can be deployed
as a preprocessing step for any NLP appli-
cation to handle social media text.
1 Introduction
Social Media text is usually very noisy and con-
tains a lot of typos, ad-hoc abbreviations, pho-
netic substitutions, customized abbreviations and
slang language. The social media text is evolving
with new entities, words and expressions. Natural
language processing and understanding systems
such as Machine Translation, Information Extrac-
tion and Text-to-Speech are usually trained and
optimized for clean data; therefore such systems
would face a challenging problem with social me-
dia text.
Various social media genres developed distinct
characteristics. For example, SMS developed a
nature of shortening messages to avoid multiple
keystrokes. On the other hand, Facebook and in-
stant messaging developed another genre where
more emotional expressions and different abbre-
viations are very common. Somewhere in be-
tween, Twitter?s statuses come with some brevity
similar to SMS along with the social aspect of
Facebook. On the same time, various social me-
dia genres share many characteristics and typo
styles. For example, repeating letters or punctu-
ation for emphasizing and emotional expression
such as ??goooood morniiing??. Using phonetic
spelling in a generalized way or to reflect a lo-
cal accent; such as ??wuz up bro?? (what is up
brother). Eliminating vowels such as ??cm to c
my luv??. Substituting numbers for letters such as
??4get?? (forget) , ??2morrow?? (tomorrow), and
??b4?? (before). Substituting phonetically sim-
ilar letters such as ??phone?? (fon). Slang ab-
breviations which usually abbreviates multi-word
expression such as ??LMS?? (like my status) ,
??idk?? (i do not know), ??rofl?? (rolling on floor
laughing).
While social media genres share many charac-
teristics, they have significant differences as well.
It is crucial to have a solution for text normaliza-
tion that can adapt to such variations automati-
cally. We propose a text normalization approach
using an unsupervised method to induce normal-
ization equivalences from noisy data which can
adapt to any genre of social media.
In this paper, we focus on providing a solu-
tion for social media text normalization as a pre-
processing step for NLP applications. However,
this is a challenging problem for several reasons.
First, it is not straightforward to define the Out-of-
Vocabulary (OOV) words. Traditionally, an OOV
word is defined as a word that does not exist in
the vocabulary of a given system. However, this
definition is not adequate for the social media text
which has a very dynamic nature. Many words
and named entities that do not exist in a given vo-
cabulary should not be considered for normaliza-
tion. Second, same OOV word may have many
1577
appropriate normalization depending on the con-
text and on the domain. Third, text normalization
as a preprocessing step should have very high pre-
cision; in other words, it should provide conser-
vative and confident normalization and not over-
correct. Moreover, the text normalization should
have high recall, as well, to have a good impact on
the NLP applications.
In this paper, we introduce a social media text
normalization system which addresses the chal-
lenges mentioned above. The proposed system is
based on constructing a lattice from possible nor-
malization candidates and finding the best normal-
ization sequence according to an n-gram language
model using a Viterbi decoder. We propose an
unsupervised approach to learn the normalization
candidates from unlabeled text data. The proposed
approach uses RandomWalks on a contextual sim-
ilarity graph constructed form n-gram sequences
on large unlabeled text corpus. The proposed ap-
proach is very scalable, accurate and adaptive to
any domain and language. We evaluate the ap-
proach on the normalization task as well as ma-
chine translation task.
The rest of this paper is organized as follows:
Section(2) discusses the related work, Section(3)
introduces the text normalization system and the
baseline candidate generators, Section(4) intro-
duces the proposed graph-based lexicon induction
approach, Section(5) discusses the experiments
and output analysis, and finally Section(6) con-
cludes and discusses future work.
2 Related Work
Early work handled the text normalization prob-
lem as a noisy channel model where the normal-
ized words go through a noisy channel to produce
the noisy text. (Brill and Moore, 2000) introduced
an approach for modeling the spelling errors as
a noisy channel model based on string to string
edits. Using this model gives significant perfor-
mance improvements compared to previously pro-
posed models. (Toutanova and Moore, 2002) im-
proved the string to string edits model by mod-
eling pronunciation similarities between words.
(Choudhury et al, 2007) introduced a supervised
HMM channel model for text normalization which
has been expanded by (Cook and Stevenson, 2009)
to introduce unsupervised noisy channel model
using probabilistic models for common abbrevi-
ation and various spelling errors types. Some
researchers used Statistical Machine Translation
approach for text normalization; formalizing the
problem as a translation from the noisy forms to
the normalized forms. (Aw et al, 2006) proposed
an approach for normalizing Short Messaging Ser-
vice (SMS) texts by translating it into normal-
ized forms using Phrase-based SMT techniques on
character level. The main drawback of these ap-
proaches is that the noisy channel model cannot
accurately represent the errors types without con-
textual information.
More recent approaches tried to handle the text
normalization problem using normalization lexi-
cons which map the noisy form of the word to a
normalized form. For example, (Han et al, 2011)
proposed an approach using a classifier to identify
the noisy words candidate for normalization; then
using some rules to generate lexical variants and a
small normalization lexicon. (Gouws et al, 2011)
proposed an approach using an impoverished nor-
malization lexicon based on string and distribu-
tional similarity along with a dictionary lookup
approach to detect noisy words. More recently,
(Han et al, 2012) introduced a similar approach
by generating a normalization lexicon based on
distributional similarity and string similarity. This
approach uses pairwise similarity where any two
words that share the same context are considered
as normalization equivalences. The pairwise ap-
proach has a number of limitations. First, it does
not take into account the relative frequencies of
the normalization equivalences that might share
different contexts. Therefore, the selection of the
normalization equivalences is performed on pair-
wise basis only and is not optimized over the
whole data. Secondly, the normalization equiva-
lences must appear in the exact same context to
be considered as a normalization candidate. These
limitations affect the accuracy and the coverage of
the produced lexicon.
Our approach also adopts a lexicon based ap-
proach for text normalization, we construct a lat-
tice from possible normalization candidates and
find the best normalization sequence according
to an n-gram language model using a Viterbi de-
coder. The normalization lexicon is acquired from
unlabeled data using random walks on a contex-
tual similarity graph constructed form n-gram se-
quences on large unlabeled text corpus. Our ap-
proach has some similarities with (Han et al,
2012) since both approaches utilize a normaliza-
1578
tion lexicon acquired form unlabeled data using
distributional and string similarities. However, our
approach is significantly different since we acquire
the lexicon using random walks on a contextual
similarity graph which has a number of advantages
over the pairwise similarity approach used in (Han
et al, 2012). Namely, the acquired normalization
equivalence are optimized globally over the whole
data, the rare equivalences are not considered as
good candidates unless there is a strong statistical
evidence across the data, and finally the normal-
ization equivalences may not share the same con-
text. Those are clear advantages over the pairwise
similarity approach and result in a lexicon with
higher accuracy as well as wider coverage. Those
advantages will be clearer when we describe the
proposed approach in details and during evalua-
tion and comparison to the pairwise approach.
3 Text Normalization System
In this paper, we handle text normalization as a
lattice scoring approach, where the translation is
performed from noisy text as the source side to
the normalized text as the target side. Unlike con-
ventional MT systems, the translation table is not
learned from parallel aligned data; instead it is
modeled by the graph-based approach of lexicon
generation as we will describe later. We construct
a lattice from possible normalization candidates
and find the best normalization sequence accord-
ing to an n-gram language model using a Viterbi
decoder.
In this paper, we restrict the normalization lexi-
con to one-to-one word mappings, we do not con-
sider multi words mapping for the lexicon induc-
tion. To identify OOV candidates for normaliza-
tion; we restrict proposing normalization candi-
dates to the words that we have in our induced
normalization lexicon only. This way, the system
would provide more confident and conservative
normalization. We move the problem of identi-
fying OOV words to training time; at training time
we use soft criteria to identify OOV words.
3.1 Baseline Normalization Candidates
Generation
We experimented with two normalization candi-
date generators as baseline systems. The first is a
dictionary based spelling correction similar to As-
pell1. In this experiment we used the spell checker
1http://aspell.net/
to generate all possible candidates for OOV words
and then applied the Viterbi decoder on the con-
structed lattice to score the best correction candi-
dates using a language model.
Our second candidates generator is based on
a trie approximate string matching with K errors
similar to the approach proposed in (Chang et al,
2010), where K errors can be caused by substi-
tution, insertion, or deletion operations. In our
implementation, we customized the errors opera-
tions to accommodate the nature of the social me-
dia text. Such as lengthening, letter substitution,
letter-number substitution and phonetic substitu-
tion. This approach overcomes the main problem
of the dictionary-based approach which is provid-
ing inappropriate normalization candidates to the
errors styles in the social media text.
As we will show in the experiments in
Section(5), dictionary-based normalization meth-
ods proved to be inadequate for social media do-
main normalization for many reasons. First, they
provide generic corrections which are inappropri-
ate for social media text. Second, they usually pro-
vide corrections with the minimal edit distance for
any word or named entity regardless of the nature
of the words. Finally, the previous approaches do
not take into account the dynamics of the social
media text where new terms can be introduced on
a daily basis.
4 Normalization Lexicons using
Graph-based Random Walks
4.1 Bipartite Graph Representation
The main motivation of this approach is that
normalization equivalences share similar context;
which we call contextual similarity. For instance,
assume 5-gram sequences of words, two words
may be normalization equivalences if their n-gram
context shares the same two words on the left and
the same two words on the right. In other words,
they are sharing a wild card pattern such as (word
1 word 2 * word 4 word 5).
This contextual similarity can be represented as
a bipartite graph with the first partite representing
the words and the second partite representing the
n-gram contexts that may be shared by words. A
word node can be either normalized word or noisy
word. Identifying if a word is normalized or noisy
(candidate for normalization) is crucial since this
decision limits the candidate noisy words to be
normalized. We adopted a soft criteria for iden-
1579
C2
making4
makin
2 mking1
tkin
1
C3 23
C1 taking1
takin21
C4 145
Figure 1: Bipartite Graph Representation, left
nodes represent contexts, gray right nodes repre-
sent the noisy words and white right nodes rep-
resent the normalized words. Edge weight is the
co-occurrence count of a word and its context.
tifying noisy words. A vocabulary is constructed
from a large clean corpus. Any word that does not
appear in this vocabulary more than a predefined
threshold (i.e. 10 times) is considered as a can-
didate for normalization (noisy word). Figure(1)
shows a sample of the bipartite graphG(W,C,E),
where noisy words are shown as gray nodes.
Algorithm 4.1: CONSTRUCTBIPARTITE(text)
comment: Construct Bipartite Graph
output (G(W,C,E))
comment: Extract all n-gram sequences
Ngrams? EXTRACTNGRAMS(TextCorpus)
for each n ? Ngrams
do?
????????????
????????????
comment: Check for center word
if ISNOISY(CenterWord)
W ? ADDSOURCENODE(CenterWord)
else
W ? ADDABSORBINGNODE(CenterWord)
comment: add the context pattern
C ? ADD(Context)
comment: edge weight
E ? ADD(Context,Word, count)
The bipartite graph, G(W,C,E), is composed
of W which includes all nodes representing nor-
malized words and noisy words, C which includes
all nodes representing shared context, and finally
E which represents the edges of the graph con-
necting word nodes and context nodes. The weight
on the edge is simply the number of occurrences
of a given word in a context. While construct-
ing the graph, we identify if a node represents a
noisy word (N) (called source node) or a normal-
ized word (M) (called absorbing node). The bi-
partite graph is constructed using the procedure in
Algorithm(4.1).
4.2 Lexicon generation using Random Walks
Our proposed approach uses Markov Random
Walks on the bipartite graph in Figure(1) as de-
fined in (Norris, 1997). The main objective is to
identify pairs of noisy and normalized words that
can be considered as normalization equivalences.
In principal, this is similar to using random walks
for semi-supervised label propagation which has
been introduced in (Szummer and Jaakkola, 2002)
and then used in many other applications. For
example, (Hughes and Ramage, 2007) used ran-
dom walks on Wordnet graph to measure lexical
semantic relatedness between words. (Das and
Petrov, 2011) used graph-based label propagation
for cross-lingual knowledge transfers to induce
POS tags between two languages. (Minkov and
Cohen, 2012) introduced a path constrained graph
walk algorithm given a small number of labeled
examples to assess nodes relatedness in the graph.
In this paper, we apply the label propagation ap-
proach to the text normalization problem.
Consider a random walk on the bipartite graph
G(W,C,E) starting at a noisy word (source
node) and ending at a normalized word (absorb-
ing node). The walker starts from any source
node Ni belonging to the noisy words then move
to any other connected node Mj with probability
Pij . The transition between each pair of nodes
is defined by a transition probability Pij which
represents the normalized probability of the co-
occurrence counts of the word and the correspond-
ing context. Though the counts are symmetric, the
probability is not symmetric. This is due to the
probability normalization which is done according
to the nodes connectivity. Therefore, the transition
probability between any two nodes i, j is defined
as:
Pij = Wij/
?
?k
Wik (1)
For any non-connected pair of nodes, Pij =0. It
is worth noting that due to the bipartite graph rep-
resentation; any word node, either noisy (source)
or normalized (absorbing), is only connected to
context nodes and not directly connected to any
other word node.
1580
The algorithm repeats independent random
walks for K times where the walks traverse the
graph randomly according to the transition prob-
ability distribution in Eqn(1); each walk starts
from the source noisy node and ends at an absorb-
ing normalized node, or consumes the maximum
number of steps without hitting an absorbing node.
For any random walk the number of steps taken
to traverse between any two nodes is called the
hitting time (Norris, 1997). Therefore, the hit-
ting time between a noisy and a normalized pair
of nodes (n,m) with a walk r is hr(n,m). We
define the cost between the two nodes as the aver-
age hitting time H(n,m) of all walks that connect
those two nodes:
H(n,m) =
?
?r
hr(n,m)/R (2)
Consider the bipartite graph in Figure(1), as-
sume a random walk starting at the source node
representing the noisy word ?tkin? then moves to
the context node C1 then to the absorbing node
representing the normalized word ?taking?. This
random walk will associate ?tkin? with ?taking?
with a walk of two steps (hits). Another random
walk that can connect the two words is [?tkin?
? C4? ?takin?? C1? ?taking?], which has
4 steps (hits). In this case, the cost of this pair
of nodes is the average number of hits connecting
them which is 3.
It is worth noting that the random walks are
selected according to the transition probability in
Eqn(1); therefore, the more probable paths will be
picked more frequently. The same pair of nodes
can be connected with many walks of various steps
(hits), and the same noisy word can be connected
to many other normalized words.
We define the contextual similarity probabil-
ity of a normalization equivalence pair n,m as
L(n,m). Which is the relative frequency of the
average hitting of those two nodes, H(n,m), and
all other normalized nodes linked to that noisy
word. Thus L(n,m), is calculated as:
L(n,m) = H(n,m)/
?
i
H(n,mi) (3)
Furthermore, we add another similarity cost be-tween a noisy word and a normalized word based
on the lexical similarity cost, SimCost(n,m),
which we will describe in the next section. Thefinal cost associated with a pair is:
Cost(n,m) = ?1L(n,m) + ?2SimCost(n,m) (4)
Algorithm 4.2: INDUCELEXICON(G)
output (Lexicon)
INIT((Lexicon))
for each n ?W ? G(W,C,E)
do?
???????????????????
???????????????????
comment: for noisy nodes only
if ISNOISY(n)?
????????
????????
INIT(Rn)
comment: do K random walks
for i? 0 to K
do
Rn? RANDOMWALK(n)
comment: Calculate Avg. hits and normalize
Ln? NORMALIZE(Rn)
comment: Calculate Lexical Sim Cost
Ln? SIMCOST(Ln)
Ln? PRUNE(Ln)
Lexicon? ADD(Ln)
We used uniform interpolation, both ?1 and ?2
equals 1. The final Lexicon is constructed using
those entries and if needed we prune the list to take
top N according to the cost above. The algorithm
is outlined in 4.2.
4.3 Lexical Similarity Cost
We use a similarity function proposed in (Con-
tractor et al, 2010) which is based on Longest
Common Subsequence Ratio (LCSR) (Melamed,
1999). This cost function is defined as the ratio
of LCSR and Edit distance between two strings as
follows:
SimCost(n,m) = LCSR(n,m)/ED(n,m) (5)
LCSR(n,m) = LCS(n,m)/MaxLenght(n,m) (6)
We have modified the Edit Distance calculation
ED(n,m) to be more adequate for social media text.
The edit distance is calculated between the conso-
nant skeleton of the two words; by removing all
vowels, we used Editex edit distance as proposed
in (Zobel and Philip, 1996), repetition is reduced
to a single letter before calculating the edit dis-
tance, and numbers in the middle of words are sub-
stituted by their equivalent letters.
5 Experiments
5.1 Training and Evaluation Data
We collected large amount of social media data to
generate the normalization lexicon using the ran-
1581
dom walk approach. The data consists of 73 mil-
lion Twitter statuses. All tweets were collected
from March/April 2012 using the Twitter Stream-
ing APIs2. We augmented this data with 50 mil-
lion sentences of clean data from English LDC Gi-
gaword corpus 3. We combined both data, noisy
and clean, together to induce the normalization
dictionary from them. While the Gigaword clean
data was used to train the language model to score
the normalized lattice.
We constructed a test set of 1000 sentences of
social media which had been corrected by a na-
tive human annotator, the main guidelines were to
normalize noisy words to its corresponding clean
words in a consistent way according to the evi-
dences in the context. We will refer to this test
set as SM-Test. Furthermore, we developed a test
set for evaluating the effect of the normalization
system when used as a preprocessing step for Ma-
chine translation. The machine translation test set
is composed of 500 sentences of social media En-
glish text translated to normalized Spanish text by
a bi-lingual translator.
5.2 Evaluating Normalization Lexicon
Generation
We extracted 5-gram sequences from the com-
bined noisy and clean data; then we limited the
space of noisy 5-gram sequences to those which
contain only one noisy word as the center word
and all other words, representing the context, are
not noisy. As we mentioned before, we identify
whether the word is noisy or not by looking up
a vocabulary list constructed from clean data. In
these experiments, the vocabulary is constructed
from the Language Model data (50M sentences of
the English Gigaword corpus). Any word that ap-
pears less than 10 times in this vocabulary is con-
sidered noisy and candidate for normalization dur-
ing the lexicon induction process. It is worth not-
ing that our notion of noisy word does not mean it
is an OOV that has to be corrected; instead it in-
dicates that it is candidate for correction but may
be opted not to be normalized if there is no con-
fident normalization for it. This helps to maintain
the approach as a high precision text normaliza-
tion system which is highly preferable as an NLP
preprocessing step.
We constructed a lattice using normalization
2https://dev.twitter.com/docs/streaming-apis
3http://www.ldc.upenn.edu/Catalog/LDC2011T07
candidates and score the best Viterbi path with 5-
gram language model. We experimented with two
candidate generators as baseline systems, namely
the dictionary-based spelling correction and the
trie approximate match with K errors; where K=3.
For both candidate generators the cost function for
a given candidate is calculated using the lexical
similarity cost in Eqn(5). We compared those ap-
proaches with our newly proposed unsupervised
normalization lexicon induction; for this case the
cost for a candidate is the combined cost of the
contextual similarity probability and the lexical
similarity cost as defined in Eqn(4). We examine
the effect of data size and the steps of the random
walks on the accuracy and the coverage of the in-
duced dictionary.
We constructed the bipartite graph with the n-
gram sequences as described in Algorithm 4.1.
Then the Random Walks Algorithm in 4.2 is ap-
plied with 100 walks. The total number of word
nodes is about 7M nodes and the total number
of context nodes is about 480M nodes. We used
MapReduce framework to implement the pro-
posed technique to handle such large graph. We
experimented with the maximum number of ran-
dom walk steps of 2, 4 and 6; and with different
portions of the data as well. Finally, we pruned
the lexicon to keep the top 5 candidates per noisy
word.
Table(1) shows the resulting lexicons from dif-
ferent experiments.
Lexicon Lexicon Data Steps
Lex1 123K 20M 4
Lex2 281K 73 M 2
Lex3 327K 73M 4
Lex4 363K 73M 6
Table 1: Generated Lexicons, steps are the Ran-
dom Walks maximum steps.
As shown in Table(1), we experimented with
different data sizes and steps of the random walks.
The more data we have the larger the lexicon we
get. Also larger steps increase the induced lexi-
con size. A random walk step size of 2 means that
the noisy/normalized pair shares the same context;
while a step size of 4 or more means that they may
not share the same context. Next, we will exam-
ine the effect of lexicon size on the normalization
task.
1582
5.3 Text Normalization Evaluation
We experimented different candidate generators
and compared it to the unsupervised lexicon ap-
proach. Table(2) shows the precision and recall on
a the SM-Test set.
System Candidates Precision Recall F-Measure
Base1 Dict 33.9 15.1 20.98
Base2 Trie 26.64 27.65 27.13
RW1 Lex1 88.76 59.23 71.06
RW2 Lex2 90.66 54.06 67.73
RW3 Lex3 92.43 56.4 70.05
RW4 Lex4 90.87 60.73 72.8
Table 2: Text Normalization with different lexi-
cons
In Table(2), the first baseline is using a dictio-
nary based spell checker; which gets low precision
and very low recall. Similarly the trie approximate
string match is doing a similar job with better re-
call though the precision is worst. Both of the
baseline approaches are inadequate for social me-
dia text since both will try to correct any word that
is similar to a word in the dictionary. The Trie ap-
proximate match is doing better job on the recall
since the approximate match is based on phonetic
and lexical similarities.
On the other hand, the induced normalization
lexicon approach is doing much better even with
a small amount of data as we can see with sys-
tem RW1 which uses Lex1 generated from 20M
sentences and has 123K lexicon entry. Increas-
ing the amount of training data does impact the
performance positively especially the recall. On
the other hand, increasing the number of steps has
a good impact on the recall as well; but with a
considerable impact on the precision. It is clear
that increasing the amount of data and keeping the
steps limit at ??4?? gives better precision and cov-
erage as well. This is a preferred setting since the
main objective of this approach is to have better
precision to serve as a reliable preprocessing step
for Machine Translation and other NLP applica-
tions.
5.4 Comparison with Pairwise Similarity
We present experimental results to compare our
proposed approach with (Han et al, 2012) which
used pairwise contextual similarity to induce a
normalization lexicon of 40K entries, we will refer
to this lexicon as HB-Dict. We compare the per-
formance of HB-Dict and our induced dictionary
(system RW3). We evaluate both system on SM-
Test test set and on (Han et al, 2012) test set of
548 sentences which we call here HB-Test.
System Precision Recall F-Measure
SM-Test
HB-Dict 71.90 26.30 38.51
RW3 92.43 56.4 70.05
HB-Test
HB-Dict 70.0 17.9 26.3
RW3 85.37 56.4 69.93
Table 3: Text Normalization Results
As shown in Table(3), RW3 system signifi-
cantly outperforms HB-Dict system with the lex-
icon from (Han et al, 2012) on both test sets for
both precision and recall. The contextual graph
random walks approach helps in providing high
precision lexicon since the sampling nature of the
approach helps in filtering out unreliable normal-
ization equivalences. The random walks will tra-
verse more frequent paths; which would lead to
more probable normalization equivalence. On the
other hand, the proposed approach provides high
recall as well which is hard to achieve with higher
precision. Since the proposed approach deploys
random walks to sample paths that can traverse
many steps, this relaxes the constraints that the
normalization equivalences have to share the same
context. Instead a noisy word may share a con-
text with another noisy word which in turn shares
a context with a clean equivalent normalization
word. Therefore, we end up with a lexicon that
have much higher recall than the pairwise simi-
larity approach since it explores equivalences be-
yond the pairwise relation. Moreover, the random
walk sampling emphasis the more frequent paths
and hence provides high precision lexicon.
5.5 Output Analysis
Table(4) shows some examples of the induced nor-
malization equivalences, the first part shows good
examples where vowels are restored and phonetic
similar words are matched. Remarkably the cor-
rection ??viewablity?? to ??visibility?? is interest-
ing since the system picked the more frequent
form. Moreover, the lexicon contains some entries
with foreign language words normalized to its En-
glish translation. On the other hand, the lexicon
has some bad normalization such as ??unrecycled
?? which should be normalized to ??non recycled??
but since the system is limited to one word cor-
rection it did not get it. Another interesting bad
normalization is ??tutting?? which is new type of
1583
dancing and should not be corrected to ??tweet-
ing??.
Noisy Clean Remarks
tnght tonight Vowels restored
darlin darling g restored
urung orange phonetic similarity
viewablity visibility good correction
unrecycled recycled negation ignored
tutting tweeting tutting is dancing type
Table 4: Lexicon Samples
Table 5 lists a number of examples and their
normalization using both Baseline1 and RW3. At
the first example, RW3 got the correct normaliza-
tion as ?interesting? which apparently is not the
one with the shortest edit distance, though it is
the most frequent candidate at the generated lex-
icon. The baseline system did not get it right; it
got a wrong normalization with shorter edit dis-
tance. Example(2) shows the same effect by get-
ting ?cuz? normalized to ?because?. At Exam-
ple(3), both the baseline and RW3 did not get
the correct normalization of ?yur? to ?you are?
which is currently a limitation in our system since
we only allow one-to-one word mapping in the
generated lexicons not one-to-many or many-to-
many. At Example(4), RW3 did not normalize
?dure? to ?sure? ; however the baseline normal-
ized it by mistake to ?dare?. This shows a char-
acteristic of the proposed approach; it is very con-
servative in proposing normalization which is de-
sirable as a preprocessing step for NLP applica-
tions. This limitation can be marginalized by pro-
viding more data for generating the lexicon. Fi-
nally, Example 4 shows also that the system nor-
malize ?gr8?which is mainly due to having a flex-
ible similarity cost during the normalization lexi-
con construction.
1. Source: Mad abt dt so mch intesting
Baseline1: Mad at do so much ingesting
RW3: Mad about that so much interesting
2. Source: i?l do cuz ma parnts r ma lyf
Baseline1: I?ll do cut ma parents r ma life
RW3: I?ll do because my parents are my life
3. Source: yur cuuuuute
Baseline1: yur cuuuuute
RW3: your cute
4. Source: I?m dure u will get a gr8 score
Baseline1: I?m dare you will get a gr8 score
RW3: I?m dure you will get a great score
Table 5: Normalization Examples
5.6 Machine Translation Task Evaluation
The final evaluation of the text normalization sys-
tem is an extrinsic evaluation where we evaluate
the effect of the text normalization task on a so-
cial media text translating from English to Span-
ish using a large scale translation system trained
on general domain data. The system is trained
on English-Spanish parallel data from WMT 2012
evaluation 4. The data consists of about 5M paral-
lel sentences on news, europal and UN data. The
system is a state of the art phrase based system
similar to Moses (Hoang et al, 2007). We used
The BLEU score (Papineni et al, 2002) to evaluate
the translation accuracy with and without the nor-
malization. Table(6) shows the translation evalua-
tion with different systems. The translation with
normalization was improved by about 6% from
29.02 to 30.87 using RW3 as a preprocessing step.
System BLEU Impreovemnet
No Normalization 29.02 0%
Baseline1 29.13 0.37%
HB-Dict 29.76 3.69%
RW3 30.87 6.37%
Table 6: Translation Results
6 Conclusion and Future Work
We introduced a social media text normalization
system that can be deployed as a preprocessor
for MT and various NLP applications to han-
dle social media text. The proposed approach is
very scalable, adaptive to any domain and lan-
guage. We show that the proposed unsupervised
approach provides a normalization system with
very high precision and a reasonable recall. We
compared the system with conventional correction
approaches and with recent previous work; and we
showed that it highly outperforms other systems.
Finally, we have used the system as a preprocess-
ing step for a machine translation system which
improved the translation quality by 6%.
As an extension to this work, we will extend the
approach to handle many-to-many normalization
pairs; also we plan to apply the approach to more
languages. Furthermore, the approach can be eas-
ily extended to handle similar problems such as ac-
cent restoration and generic entity normalization.
4http://www.statmt.or/wmt12
1584
Acknowledgments
We would like to thank Lee Schwartz and Will
Lewis for their help in constructing the test sets
and in the error analysis. We would also like to
thank the anonymous reviewers for their helpful
and constructive comments.
References
AiTi Aw, Min Zhang, Juan Xiao, and Jian Su. 2006.
A phrase-based statistical model for SMS text nor-
malization. In Proceedings of the 21st International
Conference on Computational Linguistics and 44th
Annual Meeting of the Association for Computa-
tional Linguistics, pages 3340, Sydney, Australia.
Eric Brill and Robert C. Moore. 2000. An improved er-
ror model for noisy channel spelling correction, In
ACL 2000: Proceedings of the 38th Annual Meeting
on Association for Computational Linguistics, En-
glewood Cliffs, NJ, USA.
Ye-In Chang and Jiun-Rung Chen and Min-Tze Hsu
2010. A hash trie filter method for approximate
string matching in genomic databases Applied In-
telligence, 33:1, pages 21:38, Springer US.
Monojit Choudhury, Rahul Saraf, Vijit Jain, Animesh
Mukherjee, Sudeshna Sarkar, and Anupam Basu
2007. Investigation and modeling of the structure of
texting language. International Journal of Document
Analysis and Recognition, vol. 10, pp. 157:174.
Danish Contractor and Tanveer Faruquie and Venkata
Subramaniam 2010. Unsupervised cleansing of
noisy text. In COLING ?10 Proceedings of the 23rd
International Conference on Computational Linguis-
tics, pages 189:196.
Paul Cook and Suzanne Stevenson. 2009. An unsu-
pervised model for text message normalization.. In
CALC 09: Proceedings of the Workshop on Compu-
tational Approaches to Linguistic Creativity, pages
71:78, Boulder, USA.
Dipanjan Das and Slav Petrov 2011 Unsuper-
vised part-of-speech tagging with bilingual graph-
based projections Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
600:609, Portland, Oregon
Stephan Gouws, Dirk Hovy, and Donald Metzler.
2011. Unsupervised mining of lexical variants from
noisy text. In Proceedings of the First workshop on
Unsupervised Learning in NLP, pages 82:90, Edin-
burgh, Scotland.
Bo Han and Timothy Baldwin. 2011. Lexical normal-
isation of short text messages: Makn sens a twit-
ter. In Proceedings of the 49th Annual Meeting
of the Association for Computational Linguistics:
Human Language Technologies (ACL-HLT 2011),
pages 368:378, Portland, Oregon, USA.
Bo Han and Paul Cook and Timothy Baldwin 2012.
Automatically Constructing a Normalisation Dic-
tionary for Microblogs. Proceedings of the 2012
Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL 2012), pages
421:432, Jeju Island, Korea.
Hieu Hoang and Alexandra Birch and Chris Callison-
burch and Richard Zens and Rwth Aachen and
Alexandra Constantin and Marcello Federico and
Nicola Bertoldi and Chris Dyer and Brooke Cowan
andWade Shen and Christine Moran and Ondrej Bo-
jar 2007. Moses: Open source toolkit for statistical
machine translation.
Thad Hughes and Daniel Ramage 2007. Lexical se-
mantic relatedness with random graph walks Pro-
ceedings of Conference on Empirical Methods in
Natural Language Processing EMNLP, pp. 581589,
Prague
Fei Liu and Fuliang Weng and Bingqing Wang and
Yang Liu 2011. Insertion, Deletion, or Substi-
tution? Normalizing Text Messages without Pre-
categorization nor Supervision Proceedings of the
49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technolo-
gies, pages 19:24, Portland, Oregon
Dan Melamed 1999. Bitext Maps and Alignment via
Pattern Recognition. In Computational Linguistics,
25, pages 107:130.
Einat Minkov and William Cohen Graph Based
Similarity Measures for Synonym Extraction from
Parsed Text In Proceedings of the TextGraphs work-
shop 2012
J. Norris 1997. Markov Chains. Cambridge Univer-
sity Press.
Kishore Papineni and Salim Roukos and Todd Ward
and Wei-jing Zhu 2002. BLEU: a Method for Au-
tomatic Evaluation of Machine Translation. in Pro-
ceedings of ACL-2002: 40th Annual meeting of the
Association for Computational Linguistics. , pages
311:318.
Richard Sproat, Alan W. Black, Stanley Chen, Shankar
Kumar, Mari Ostendorf, and Christopher Richards.
Normalization of non-standard words. 2001.
Xu Sun and Jianfeng Gao and Daniel Micol and Chris
Quirk 2010. Learning Phrase-Based Spelling Error
Models from Clickthrough Data. Proceedings of the
48th Annual Meeting of the Association for Compu-
tational Linguistics, pages 266:274, Sweeden.
Martin Szummer and Tommi 2002. Partially labeled
classification with markov random walks. In Ad-
vances in Neural Information Processing Systems,
pages 945:952.
1585
Kristina Toutanova and Robert C. Moore. Pronunci-
ation modeling for improved spelling correction..
2002. In Proceedings of the 40th Annual Meeting
on Association for Computational Linguistics, ACL
, pages 144151, Philadelphia, USA.
Justin Zobel and Philip Dart 1996. Phonetic string
matching: Lessons from information retrieval. in
Proceedings of the Eighteenth ACM SIGIR Inter-
national Conference on Research and Development
in Information Retrieval, pages 166:173, Zurich,
Switzerland.
1586
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 676?686,
Baltimore, Maryland, USA, June 23-25 2014.
c
?2014 Association for Computational Linguistics
Graph-based Semi-Supervised Learning of Translation Models from
Monolingual Data
Avneesh Saluja
?
Carnegie Mellon University
Pittsburgh, PA 15213, USA
avneesh@cs.cmu.edu
Hany Hassan, Kristina Toutanova, Chris Quirk
Microsoft Research
Redmond, WA 98502, USA
hanyh,kristout,chrisq@microsoft.com
Abstract
Statistical phrase-based translation learns
translation rules from bilingual corpora,
and has traditionally only used monolin-
gual evidence to construct features that
rescore existing translation candidates. In
this work, we present a semi-supervised
graph-based approach for generating new
translation rules that leverages bilingual
and monolingual data. The proposed tech-
nique first constructs phrase graphs using
both source and target language mono-
lingual corpora. Next, graph propaga-
tion identifies translations of phrases that
were not observed in the bilingual cor-
pus, assuming that similar phrases have
similar translations. We report results
on a large Arabic-English system and a
medium-sized Urdu-English system. Our
proposed approach significantly improves
the performance of competitive phrase-
based systems, leading to consistent im-
provements between 1 and 4 BLEU points
on standard evaluation sets.
1 Introduction
Statistical approaches to machine translation
(SMT) use sentence-aligned, parallel corpora to
learn translation rules along with their probabil-
ities. With large amounts of data, phrase-based
translation systems (Koehn et al, 2003; Chiang,
2007) achieve state-of-the-art results in many ty-
pologically diverse language pairs (Bojar et al,
2013). However, the limiting factor in the suc-
cess of these techniques is parallel data availabil-
ity. Even in resource-rich languages, learning re-
liable translations of multiword phrases is a chal-
lenge, and an adequate phrasal inventory is crucial
?
This work was done while the first author was interning
at Microsoft Research
for effective translation. This problem is exacer-
bated in the many language pairs for which par-
allel resources are either limited or nonexistent.
While parallel data is generally scarce, monolin-
gual resources exist in abundance and are being
created at accelerating rates. Can we use monolin-
gual data to augment the phrasal translations ac-
quired from parallel data?
The challenge of learning translations from
monolingual data is of long standing interest,
and has been approached in several ways (Rapp,
1995; Callison-Burch et al, 2006; Haghighi et
al., 2008; Ravi and Knight, 2011). Our work in-
troduces a new take on the problem using graph-
based semi-supervised learning to acquire trans-
lation rules and probabilities by leveraging both
monolingual and parallel data resources. On the
source side, labeled phrases (those with known
translations) are extracted from bilingual corpora,
and unlabeled phrases are extracted from mono-
lingual corpora; together they are embedded as
nodes in a graph, with the monolingual data de-
termining edge strengths between nodes (?2.2).
Unlike previous work (Irvine and Callison-Burch,
2013a; Razmara et al, 2013), we use higher order
n-grams instead of restricting to unigrams, since
our approach goes beyond OOV mitigation and
can enrich the entire translation model by using
evidence from monolingual text. This enhance-
ment alone results in an improvement of almost
1.4 BLEU points. On the target side, phrases ini-
tially consisting of translations from the parallel
data are selectively expanded with generated can-
didates (?2.1), and are embedded in a target graph.
We then limit the set of translation options for
each unlabeled source phrase (?2.3), and using
a structured graph propagation algorithm, where
translation information is propagated from la-
beled to unlabeled phrases proportional to both
source and target phrase similarities, we esti-
mate probability distributions over translations for
676
Source! Target!
el gato!
los gatos!
un gato! cat!
the cat! the cats!
a cat!
Target! Prob.!
the cat! 0.7!
cat! 0.15!
?! ?!
felino!
canino! el perro!
Target! Prob.!
canine! 0.6!
dog! 0.3!
?! ?!
Target! Prob.!
the cats! 0.8!
cats! 0.1!
?! ?!
Target! Prob.!
the dog! 0.9!
dog! 0.05!
?! ?!
canine!
dog!
the dog!
catlike!
Figure 1: Example source and target graphs used in our approach. Labeled phrases on the source side are black (with their
corresponding translations on the target side also black); unlabeled and generated (?2.1) phrases on the source and target sides
respectively are white. Labeled phrases also have conditional probability distributions defined over target phrases, which are
extracted from the parallel corpora.
the unlabeled source phrases (?2.4). The addi-
tional phrases are incorporated in the SMT sys-
tem through a secondary phrase table (?2.5). We
evaluated the proposed approach on both Arabic-
English and Urdu-English under a range of sce-
narios (?3), varying the amount and type of mono-
lingual corpora used, and obtained improvements
between 1 and 4 BLEU points, even when using
very large language models.
2 Generation & Propagation
Our goal is to obtain translation distributions for
source phrases that are not present in the phrase
table extracted from the parallel corpus. Both par-
allel and monolingual corpora are used to obtain
these probability distributions over target phrases.
We assume that sufficient parallel resources ex-
ist to learn a basic translation model using stan-
dard techniques, and also assume the availability
of larger monolingual corpora in both the source
and target languages. Although our technique ap-
plies to phrases of any length, in this work we con-
centrate on unigram and bigram phrases, which
provides substantial computational cost savings.
Monolingual data is used to construct separate
similarity graphs over phrases (word sequences),
as illustrated in Fig. 1. The source similarity graph
consists of phrase nodes representing sequences of
words in the source language. If a source phrase
is found in the baseline phrase table it is called a
labeled phrase: its conditional empirical probabil-
ity distribution over target phrases (estimated from
the parallel data) is used as the label, and is sub-
sequently never changed. Otherwise it is called an
unlabeled phrase, and our algorithm finds labels
(translations) for these unlabeled phrases, with the
help of the graph-based representation. The la-
bel space is thus the phrasal translation inventory,
and like the source side it can also be represented
in terms of a graph, initially consisting of target
phrase nodes from the parallel corpus.
For the unlabeled phrases, the set of possible
target translations could be extremely large (e.g.,
all target language n-grams). Therefore, we first
generate and fix a list of possible target transla-
tions for each unlabeled source phrase. We then
propagate by deriving a probability distribution
over these target phrases using graph propagation
techniques. Next, we will describe the generation,
graph construction and propagation steps.
2.1 Generation
The objective of the generation step is to popu-
late the target graph with additional target phrases
for all unlabeled source phrases, yielding the full
set of possible translations for the phrase. Prior to
generation, one phrase node for each target phrase
occurring in the baseline phrase table is added to
the target graph (black nodes in Fig. 1?s target
graph). We only consider target phrases whose
source phrase is a bigram, but it is worth noting
that the target phrases are of variable length.
The generation component is based on the ob-
servation that for structured label spaces, such as
translation candidates for source phrases in SMT,
even similar phrases have slightly different labels
(target translations). The exponential dependence
677
of the sizes of these spaces on the length of in-
stances is to blame. Thus, the target phrase inven-
tory from the parallel corpus may be inadequate
for unlabeled instances. We therefore need to en-
rich the target or label space for unknown phrases.
A na??ve way to achieve this goal would be to ex-
tract all n-grams, from n = 1 to a maximum n-
gram order, from the monolingual data, but this
strategy would lead to a combinatorial explosion
in the number of target phrases.
Instead, by intelligently expanding the target
space using linguistic information such as mor-
phology (Toutanova et al, 2008; Chahuneau et al,
2013), or relying on the baseline system to gener-
ate candidates similar to self-training (McClosky
et al, 2006), we can tractably propose novel trans-
lation candidates (white nodes in Fig. 1?s target
graph) whose probabilities are then estimated dur-
ing propagation. We refer to these additional can-
didates as ?generated? candidates.
To generate new translation candidates using
the baseline system, we decode each unlabeled
source bigram to generate its m-best translations.
This set of candidate phrases is filtered to include
only n-grams occurring in the target monolingual
corpus, and helps to prune passed-through OOV
words and invalid translations. To generate new
translation candidates using morphological infor-
mation, we morphologically segment words into
prefixes, stem, and suffixes using linguistic re-
sources. We assume that a morphological ana-
lyzer which provides context-independent analysis
of word types exists, and implements the functions
STEM(f ) and STEM(e) for source and target word
types. Based on these functions, source and target
sequences of words can be mapped to sequences
of stems. The morphological generation step adds
to the target graph all target word sequences from
the monolingual data that map to the same stem
sequence as one of the target phrases occurring in
the baseline phrase table. In other words, this step
adds phrases that are morphological variants of ex-
isting phrases, differing only in their affixes.
2.2 Graph Construction
At this stage, there exists a list of source bigram
phrases, both labeled and unlabeled, as well as a
list of target language phrases of variable length,
originating from both the phrase table and the gen-
eration step. To determine pairwise phrase similar-
ities in order to embed these nodes in their graphs,
we utilize the monolingual corpora on both the
source and target sides to extract distributional
features based on the context surrounding each
phrase. For a phrase, we look at the pwords before
and the p words after the phrase, explicitly distin-
guishing between the two sides, but not distance
(i.e., bag of words on each side). Co-occurrence
counts for each feature (context word) are accu-
mulated over the monolingual corpus, and these
counts are converted to pointwise mutual infor-
mation (PMI) values, as is standard practice when
computing distributional similarities. Cosine sim-
ilarity between two phrases? PMI vectors is used
for similarity, and we take only the k most simi-
lar phrases for each phrase, to create a k-nearest
neighbor similarity matrix for both source and tar-
get language phrases. These graphs are distinct,
in that propagation happens within the two graphs
but not between them.
While accumulating co-occurrence counts for
each phrase, we also maintain an inverted index
data structure, which is a mapping from features
(context words) to phrases that co-occur with that
feature within a window of p.
1
The inverted index
structure reduces the graph construction cost from
?(n
2
), by only computing similarities for a sub-
set of all possible pairs of phrases, namely other
phrases that have at least one feature in common.
2.3 Candidate Translation List Construction
As mentioned previously, we construct and fix
a set of translation candidates, i.e., the label set
for each unlabeled source phrase. The probabil-
ity distribution over these translations is estimated
through graph propagation, and the probabilities
of items outside the list are assumed to be zero.
We obtain these candidates from two sources:
2
1. The union of each unlabeled phrase?s la-
beled neighbors? labels, which represents the
set of target phrases that occur as transla-
tions of source phrases that are similar to
the unlabeled source phrase. For un gato in
Fig. 1, this source would yield the cat and
cat, among others, as candidates.
2. The generated candidates for the unlabeled
phrase ? the ones from the baseline system?s
1
The q most frequent words in the monolingual corpus
were removed as keys from this mapping, as these high en-
tropy features do not provide much information.
2
We also obtained the k-nearest neighbors of the transla-
tion candidates generated through these methods by utilizing
the target graph, but this had minimal impact.
678
decoder output, or from a morphological gen-
erator (e.g., a cat and catlike in Fig. 1).
The morphologically-generated candidates for a
given source unlabeled phrase are initially de-
fined as the target word sequences in the mono-
lingual data that have the same stem sequence
as one of the baseline?s target translations for a
source phrase which has the same stem sequence
as the unlabeled source phrase. These candidates
are scored using stem-level translation probabili-
ties, morpheme-level lexical weighting probabili-
ties, and a language model, and only the top 30
candidates are included.
After obtaining candidates from these two pos-
sible sources, the list is sorted by forward lexical
score, using the lexical models of the baseline sys-
tem. The top r candidates are then chosen for each
phrase?s translation candidate list.
In Figure 2 we provide example outputs of
our system for a handful of unlabeled source
phrases, and explicitly note the source of the trans-
lation candidate (?G? for generated, ?N? for labeled
neighbor?s label).
2.4 Graph Propagation
A graph propagation algorithm transfers label in-
formation from labeled nodes to unlabeled nodes
by following the graph?s structure. In some appli-
cations, a label may consist of class membership
information, e.g., each node can belong to one of
a certain number of classes. In our problem, the
?label? for each node is actually a probability dis-
tribution over a set of translation candidates (target
phrases). For a given node f , let e refer to a can-
didate in the label set for node f ; then in graph
propagation, the probability of candidate e given
source phrase f in iteration t + 1 is:
P
t+1
(e|f) =
X
j2N (f)
T
s
(j|f)P
t
(e|j) (1)
where the setN (f) contains the (labeled and unla-
beled) neighbors of node f , and T
s
(j|f) is a term
that captures how similar nodes f and j are. This
quantity is also known as the propagation proba-
bility, and its exact form will depend on the type
of graph propagation algorithm used. For our pur-
poses, node f is a source phrasal node, the set
N (f) refers to other source phrases that are neigh-
bors of f (restricted to the k-nearest neighbors as
in ?2.2), and the aim is to estimate P (e|f), the
probability of target phrase e being a phrasal trans-
lation of source phrase f .
A classic propagation algorithm that has been
suitably modified for use in bilingual lexicon in-
duction (Tamura et al, 2012; Razmara et al, 2013)
is the label propagation (LP) algorithm of Zhu et
al. (2003). In this case, T
s
(f, j) is chosen to be:
T
s
(j|f) =
w
s
f,j
P
j
0
2N (f)
w
s
f,j
0
(2)
where w
s
f,j
is the cosine similarity (as computed
in ?2.2) between phrase f and phrase j on side s
(the source side).
As evident in Eq. 2, LP only takes into account
source language similarity of phrases. To see this
observation more clearly, let us reformulate Eq. 1
more generally as:
P
t+1
(e|f) =
X
j2N (f)
T
s
(j|f)
X
e
0
2H(j)
T
t
(e
0
|e)P
t
(e
0
|j) (3)
where H(j) is the translation candidate set for
source phrase j, and T
t
(e
0
|e) is the propagation
probability between nodes or phrases e and e
0
on the target side. We have simply replaced
P
t
(e|j) with
P
e
0
2H(j)
T
t
(e
0
|e)P
t
(e
0
|j), defining it
in terms of j?s translation candidate list.
Note that in the original LP formulation the tar-
get side information is disregarded, i.e., T
t
(e
0
|e) =
1 if and only if e = e
0
and 0 otherwise. As a
result, LP is suboptimal for our needs, since it is
unable to appropriately handle generated transla-
tion candidates for the unlabeled phrases. These
translation candidates are usually not present as
translations for the labeled phrases (or for the la-
beled phrases that neighbor the unlabeled one in
question). When propagating information from
the labeled phrases, such candidates will obtain
no probability mass since e 6= e
0
. Thus, due to
the setup of the problem, LP naturally biases away
from translation candidates produced during the
generation step (?2.1).
2.4.1 Structured Label Propagation
The label set we are considering has a similarity
structure encoded by the target graph. How can
we exploit this structure in graph propagation on
the source graph? In Liu et al (2012), the authors
generalize label propagation to structured label
propagation (SLP) in an effort to work more el-
egantly with structured labels. In particular, the
definition of target similarity is similar to that of
source similarity:
T
t
(e
0
|e) =
w
t
e,e
0
P
e
00
2H(j)
w
t
e,e
00
(4)
679
Therefore, the final update equation in SLP is:
P
t+1
(e|f) =
X
j2N (f)
T
s
(j|f)
X
e
0
2H(j)
T
t
(e
0
|e)P
t
(e
0
|j) (5)
With this formulation, even if e 6= e
0
, the simi-
larity T
t
(e
0
|e) as determined by the target phrase
graph will dictate propagation probability. We re-
normalize the probability distributions after each
propagation step to sum to one over the fixed list
of translation candidates, and run the SLP algo-
rithm to convergence.
3
2.5 Phrase-based SMT Expansion
After graph propagation, each unlabeled phrase
is labeled with a categorical distribution over
the set of translation candidates defined in ?2.3.
In order to utilize these newly acquired phrase
pairs, we need to compute their relevant features.
The phrase pairs have four log-probability fea-
tures with two likelihood features and two lexical
weighting features. In addition, we use a sophis-
ticated lexicalized hierarchical reordering model
(HRM) (Galley and Manning, 2008) with five fea-
tures for each phrase pair.
We utilize the graph propagation-estimated for-
ward phrasal probabilities P(e|f) as the forward
likelihood probabilities for the acquired phrases;
to obtain the backward phrasal probability for a
given phrase pair, we make use of Bayes? Theo-
rem:
P(f |e) =
P(e|f)P(f)
P(e)
where the marginal probabilities of source and tar-
get phrases e and f are obtained from the counts
extracted from the monolingual data. The baseline
system?s lexical models are used for the forward
and backward lexical scores. The HRM probabil-
ities for the new phrase pairs are estimated from
the baseline system by backing-off to the average
values for phrases with similar length.
3 Evaluation
We performed an extensive evaluation to exam-
ine various aspects of the approach along with
overall system performance. Two language pairs
were used: Arabic-English and Urdu-English. The
Arabic-English evaluation was used to validate the
decisions made during the development of our
3
Empirically within a few iterations and a wall-clock time
of less than 10 minutes in total.
method and also to highlight properties of the
technique. With it, in ?3.2 we first analyzed the
impact of utilizing phrases instead of words and
SLP instead of LP; the latter experiment under-
scores the importance of generated candidates. We
also look at how adding morphological knowledge
to the generation process can further enrich per-
formance. In ?3.3, we then examined the effect of
using a very large 5-gram language model train-
ing on 7.5 billion English tokens to understand the
nature of the improvements in ?3.2. The Urdu to
English evaluation in ?3.4 focuses on how noisy
parallel data and completely monolingual (i.e., not
even comparable) text can be used for a realistic
low-resource language pair, and is evaluated with
the larger language model only. We also exam-
ine how our approach can learn from noisy parallel
data compared to the traditional SMT system.
Baseline phrasal systems are used both for com-
parison and for generating translation candidates
for unlabeled phrases as described in ?2.1. The
baseline is a state-of-the-art phrase-based system;
we perform word alignment using a lexicalized
hidden Markov model, and then the phrase ta-
ble is extracted using the grow-diag-final
heuristic (Koehn et al, 2003). The 13 baseline
features (2 lexical, 2 phrasal, 5 HRM, and 1 lan-
guage model, word penalty, phrase length feature
and distortion penalty feature) were tuned using
MERT (Och, 2003), which is also used to tune
the 4 feature weights introduced by the secondary
phrase table (2 lexical and 2 phrasal, other fea-
tures being shared between the two tables). For
all systems, we use a distortion limit of 4. We use
case-insensitive BLEU (Papineni et al, 2002) to
evaluate translation quality.
3.1 Datasets
Bilingual corpus statistics for both language pairs
are presented in Table 2. For Arabic-English, our
training corpus consisted of 685k sentence pairs
from standard LDC corpora
4
. The NIST MT06
and MT08 Arabic-English evaluation sets (com-
bining the newswire and weblog domains for both
sets), with four references each, were used as
tuning and testing sets respectively. For Urdu-
English, the training corpus was provided by the
LDC for the NIST Urdu-English MT evaluation,
and most of the data was automatically acquired
from the web, making it quite noisy. After fil-
tering, there are approximately 65k parallel sen-
4
LDC2007T08 and LDC2008T09
680
Parameter Description Value
m m-best candidate list size when bootstrapping candidates in generation stage. 100
p Window size on each side when extracting features for phrases. 2
q Filter the q most frequent words when storing the inverted index data structure for graph construction.
Both source and target sides share the same value.
25
k Number of neighbors stored for each phrase for both source and target graphs. This parameter controls
the sparsity of the graph.
500
r Maximum size of translation candidate list for unlabeled phrases. 20
Table 1: Parameters, explanation of their function, and value chosen.
tences; these were supplemented by an additional
100k dictionary entries. Tuning and test data con-
sisted of the MT08 and MT09 evaluation corpora,
once again a mixture of news and web text.
Corpus Sentences Words (Src)
Ar-En Train 685,502 17,055,168
Ar-En Tune (MT06) 1,664 33,739
Ar-En Test (MT08) 1,360 42,472
Ur-En Train 165,159 1,169,367
Ur-En Tune (MT08) 1,864 39,925
Ur-En Test (MT09) 1,792 39,922
Table 2: Bilingual corpus statistics for the Arabic-English
and Urdu-English datasets used.
Table 3 contains statistics for the monolingual
corpora used in our experiments. From these cor-
pora, we extracted all sentences that contained at
least one source or target phrase match to com-
pute features for graph construction. For the Ara-
bic to English experiments, the monolingual cor-
pora are taken from the AFP Arabic and English
Gigaword corpora and are of a similar date range
to each other (1994-2010), rendering them compa-
rable but not sentence-aligned or parallel.
Corpus Sentences Words
Ar Comparable 10.2m 290m
En I Comparable 29.8m 900m
Ur Noisy Parallel 470k 5m
En II Noisy Parallel 470k 4.7m
Ur Non-Comparable 7m 119m
En II Non-Comparable 17m 510m
Table 3: Monolingual corpus statistics for the Arabic-English
and Urdu-English evaluations. The monolingual corpora can
be sub-divided into comparable, noisy parallel, and non-
comparable components. En I refers to the English side of
the Arabic-English corpora, and En II to the English side of
the Urdu-English corpora.
For the Urdu-English experiments, completely
non-comparable monolingual text was used for
graph construction; we obtained the Urdu side
through a web-crawler, and a subset of the AFP
Gigaword English corpus was used for English. In
addition, we obtained a corpus from the ELRA
5
,
which contains a mix of parallel and monolingual
data; based on timestamps, we extracted a compa-
rable English corpus for the ELRA Urdu monolin-
gual data to form a roughly 470k-sentence ?noisy
parallel? set. We used this set in two ways: ei-
ther to augment the parallel data presented in Table
2, or to augment the non-comparable monolingual
data in Table 3 for graph construction.
For the parameters introduced throughout the
text, we present in Table 1 a reminder of their in-
terpretation as well as the values used in this work.
3.2 Experimental Variations
In our first set of experiments, we looked at the im-
pact of choosing bigrams over unigrams as our ba-
sic unit of representation, along with performance
of LP (Eq. 2) compared to SLP (Eq. 4). Re-
call that LP only takes into account source sim-
ilarity; since the vast majority of generated can-
didates do not occur as labeled neighbors? labels,
restricting propagation to the source graph dras-
tically reduces the usage of generated candidates
as labels, but does not completely eliminate it. In
these experiments, we utilize a reasonably-sized
4-gram language model trained on 900m English
tokens, i.e., the English monolingual corpus.
Table 4 presents the results of these variations;
overall, by taking into account generated candi-
dates appropriately and using bigrams (?SLP 2-
gram?), we obtained a 1.13 BLEU gain on the
test set. Using unigrams (?SLP 1-gram?) actu-
ally does worse than the baseline, indicating the
importance of focusing on translations for sparser
bigrams. While LP (?LP 2-gram?) does reason-
ably well, its underperformance compared to SLP
underlines the importance of enriching the trans-
lation space with generated candidates and han-
dling these candidates appropriately.
6
In ?SLP-
5
ELRA-W0038
6
It is relatively straightforward to combine both unigrams
and bigrams in one source graph, but for experimental clarity
we did not mix these phrase lengths.
681
HalfMono?, we use only half of the monolingual
comparable corpora, and still obtain an improve-
ment of 0.56 BLEU points, indicating that adding
more monolingual data is likely to improve the
system further. Interestingly, biasing away from
generated candidates using all the monolingual
data (?LP 2-gram?) performs similarly to using
half the monolingual corpora and handling gener-
ated candidates properly (?SLP-HalfMono?).
BLEU
Setup Tune Test
Baseline 39.33 38.09
SLP 1-gram 39.47 37.85
LP 2-gram 40.75 38.68
SLP 2-gram 41.00 39.22
SLP-HalfMono 2-gram 40.82 38.65
SLP+Morph 2-gram 41.02 39.35
Table 4: Results for the Arabic-English evaluation. The LP
vs. SLP comparison highlights the importance of target side
enrichment via translation candidate generation, 1-gram vs.
2-gram comparisons highlight the importance of emphasiz-
ing phrases, utilizing half the monolingual data shows sensi-
tivity to monolingual corpus size, and adding morphological
information results in additional improvement.
Additional morphologically generated candi-
dates were added in this experiment as detailed in
?2.3. We used a simple hand-built Arabic morpho-
logical analyzer that segments word types based
on regular expressions, and an English lexicon-
based morphological analyzer. The morphological
candidates add a small amount of improvement,
primarily by targeting genuine OOVs.
3.3 Large Language Model Effect
In this set of experiments, we examined if the
improvements in ?3.2 can be explained primar-
ily through the extraction of language model char-
acteristics during the semi-supervised learning
phase, or through orthogonal pieces of evidence.
Would the improvement be less substantial had we
used a very large language model?
To answer this question we trained a 5-gram
language model on 570M sentences (7.6B tokens),
with data from various sources including the Gi-
gaword corpus
7
, WMT and European Parliamen-
tary Proceedings
8
, and web-crawled data from
Wikipedia and the web. Only m-best generated
candidates from the baseline were considered dur-
ing generation, along with labeled neighbors? la-
bels.
7
LDC2011T07
8
http://www.statmt.org/wmt13/
BLEU
Setup Tune Test
Baseline+LargeLM 41.48 39.86
SLP+LargeLM 42.82 41.29
Table 5: Results with the large language model scenario. The
gains are even better than with the smaller language model.
Table 5 presents the results of using this lan-
guage model. We obtained a robust, 1.43-BLEU
point gain, indicating that the addition of the
newly induced phrases provided genuine transla-
tion improvements that cannot be compensated by
the language model effect. Further examination of
the differences between the two systems yielded
that most of the improvements are due to better
bigrams and trigrams, as indicated by the break-
down of the BLEU score precision per n-gram,
and primarily leverages higher quality generated
candidates from the baseline system. We analyze
the output of these systems further in the output
analysis section below (?3.5).
3.4 Urdu-English
In order to evaluate the robustness of these results
beyond one language pair, we looked at Urdu-
English, a low resource pair likely to benefit from
this approach. In this set of experiments, we used
the large language model in ?3.3, and only used
baseline-generated candidates. We experimented
with two extreme setups that differed in the data
assumed parallel, from which we built our base-
line system, and the data treated as monolingual,
from which we built our source and target graphs.
In the first setup, we use the noisy parallel
data for graph construction and augment the non-
comparable corpora with it:
? parallel: ?Ur-En Train?
? Urdu monolingual: ?Ur Noisy Parallel?+?Ur
Non-Comparable?
? English monolingual: ?En II Noisy Paral-
lel?+?En II Non-Comparable?
The results from this setup are presented as ?Base-
line? and ?SLP+Noisy? in Table 6. In the second
setup, we train a baseline system using the data in
Table 2, augmented with the noisy parallel text:
? parallel: ?Ur-En Train?+?Ur Noisy Paral-
lel?+?En II Noisy Parallel?
? Urdu monolingual: ?Ur Non-Comparable?
? English monolingual: ?En II Non-
Comparable?
682
!Ex Source Reference Baseline System 1 (Ar) !???#$#"! %$??" ! sending reinforcements strong reinforcements sending reinforcements (N) 2 (Ar)  !???$??'!+!! with extinction OOV with extinction (N) 3 (Ar) !???#?? ??? ! thwarts address  thwarted (N) 4 (Ar) !?? ???# ! was quoted as saying attributed to was quoted as saying (G) 5 (Ar) ????"! ??! $#??& ! abdalmahmood said he said abdul mahmood  mahmood said (G) 6 (Ar)  ?#"! ????? it deems OOV it deems (G) 7 (Ur) !?"! ?$ ! I am hopeful this hope I am hopeful (N) 8 (Ur) ??! $???$ ! to defend him to defend to defend himself (G) 9 (Ur) !??? ???? ! while speaking In the  in conversation (N) 
Figure 2: Nine example outputs of our system vs. the baseline highlighting the properties of our approach. Each example is
labeled (Ar) for Arabic source or (Ur) for Urdu source, and system candidates are labeled with (N) if the candidate unlabeled
phrase?s labeled neighbor?s label, or (G) if the candidate was generated.
The results from this setup are presented as ?Base-
line+Noisy? and ?SLP? in Table 6. The two setups
allow us to examine how effectively our method
can learn from the noisy parallel data by treating it
as monolingual (i.e., for graph construction), com-
pared to treating this data as parallel, and also ex-
amines the realistic scenario of using completely
non-comparable monolingual text for graph con-
struction as in the second setup.
BLEU
Setup Tune Test
Baseline 21.87 21.17
SLP+Noisy 26.42 25.38
Baseline+Noisy 27.59 27.24
SLP 28.53 28.43
Table 6: Results for the Urdu-English evaluation evaluated
with BLEU. All experiments were conducted with the larger
language model, and generation only considered the m-best
candidates from the baseline system.
In the first setup, we get a huge improvement of
4.2 BLEU points (?SLP+Noisy?) when using the
monolingual data and the noisy parallel data for
graph construction. Our method obtained much
of the gains achieved by the supervised baseline
approach that utilizes the noisy parallel data in
conjunction with the NIST-provided parallel data
(?Baseline+Noisy?), but with fewer assumptions
on the nature of the corpora (monolingual vs.
parallel). Furthermore, despite completely un-
aligned, non-comparable monolingual text on the
Urdu and English sides, and a very large language
model, we can still achieve gains in excess of
1.2 BLEU points (?SLP?) in a difficult evaluation
scenario, which shows that the technique adds a
genuine translation improvement over and above
na??ve memorization of n-gram sequences.
3.5 Analysis of Output
Figure 2 looks at some of the sample hypotheses
produced by our system and the baseline, along
with reference translations. The outputs produced
by our system are additionally annotated with the
origin of the candidate, i.e., labeled neighbor?s la-
bel (N) or generated (G).
The Arabic-English examples are numbered 1
to 5. The first example shows a source bigram un-
known to the baseline system, resulting in a sub-
optimal translation, while our system proposes the
correct translation of ?sending reinforcements?.
The second example shows a word that was an
OOV for the baseline system, while our system
got a perfect translation. The third and fourth ex-
amples represent bigram phrases with much bet-
ter translations compared to backing off to the
lexical translations as in the baseline. The fifth
Arabic-English example demonstrates the pitfalls
of over-reliance on the distributional hypothesis:
the source bigram corresponding to the name ?abd
almahmood? is distributional similar to another
named entity ?mahmood? and the English equiva-
lent is offered as a translation. The distributional
hypothesis can sometimes be misleading. The
sixth example shows how morphological informa-
tion can propose novel candidates: an OOV word
is broken down to its stem via the analyzer and
candidates are generated based on the stem.
The Urdu-English examples are numbered 7
to 9. In example 7, the bigram ?par umeed?
(corresponding to ?hopeful?) is never seen in the
baseline system, which has only seen ?umeed?
(?hope?). By leveraging the monolingual corpus
to understand the context of this unlabeled bigram,
we can utilize the graph structure to propose a syn-
tactically correct form, also resulting in a more flu-
ent and correct sentence as determined by the lan-
guage model. Examples 8 & 9 show cases where
the baseline deletes words or translates them into
more common words e.g., ?conversation? to ?the?,
while our system proposes reasonable candidates.
683
4 Related Work
The idea presented in this paper is similar in spirit
to bilingual lexicon induction (BLI), where a seed
lexicon in two different languages is expanded
with the help of monolingual corpora, primarily by
extracting distributional similarities from the data
using word context. This line of work, initiated
by Rapp (1995) and continued by others (Fung
and Yee, 1998; Koehn and Knight, 2002) (inter
alia) is limited from a downstream perspective, as
translations for only a small number of words are
induced and oftentimes for common or frequently
occurring ones only. Recent improvements to BLI
(Tamura et al, 2012; Irvine and Callison-Burch,
2013b) have contained a graph-based flavor by
presenting label propagation-based approaches us-
ing a seed lexicon, but evaluation is once again
done on top-1 or top-3 accuracy, and the focus is
on unigrams.
Razmara et al (2013) and Irvine and Callison-
Burch (2013a) conduct a more extensive evalua-
tion of their graph-based BLI techniques, where
the emphasis and end-to-end BLEU evaluations
concentrated on OOVs, i.e., unigrams, and not on
enriching the entire translation model. As with
previous BLI work, these approaches only take
into account source-side similarity of words; only
moderate gains (and in the latter work, on a sub-
set of language pairs evaluated) are obtained. Ad-
ditionally, because of our structured propagation
algorithm, our approach is better at handling mul-
tiple translation candidates and does not need to
restrict itself to the top translation.
Klementiev et al (2012) propose a method that
utilizes a pre-existing phrase table and a small
bilingual lexicon, and performs BLI using mono-
lingual corpora. The operational scope of their ap-
proach is limited in that they assume a scenario
where unknown phrase pairs are provided (thereby
sidestepping the issue of translation candidate
generation for completely unknown phrases), and
what remains is the estimation of phrasal proba-
bilities. In our case, we obtain the phrase pairs
from the graph structure (and therefore indirectly
from the monolingual data) and a separate gener-
ation step, which plays an important role in good
performance of the method. Similarly, Zhang and
Zong (2013) present a series of heuristics that are
applicable in a fairly narrow setting.
The notion of translation consensus, wherein
similar sentences on the source side are encour-
aged to have similar target language translations,
has also been explored via a graph-based approach
(Alexandrescu and Kirchhoff, 2009). Liu et al
(2012) extend this method by proposing a novel
structured label propagation algorithm to deal with
the generalization of propagating sets of labels
instead of single labels, and also integrated in-
formation from the graph into the decoder. In
fact, we utilize this algorithm in our propagation
step (?2.4). However, the former work operates
only at the level of sentences, and while the latter
does extend the framework to sub-spans of sen-
tences, they do not discover new translation pairs
or phrasal probabilities for new pairs at all, but
instead re-estimate phrasal probabilities using the
graph structure and add this score as an additional
feature during decoding.
The goal of leveraging non-parallel data in ma-
chine translation has been explored from several
different angles. Paraphrases extracted by ?pivot-
ing? via a third language (Callison-Burch et al,
2006) can be derived solely from monolingual
corpora using distributional similarity (Marton et
al., 2009). Snover et al (2008) use cross-lingual
information retrieval techniques to find potential
sentence-level translation candidates among com-
parable corpora. In this case, the goal is to
try and construct a corpus as close to parallel
as possible from comparable corpora, and is a
fairly different take on the problem we are look-
ing at. Decipherment-based approaches (Ravi and
Knight, 2011; Dou and Knight, 2012) have gen-
erally taken a monolingual view to the problem
and combine phrase tables through the log-linear
model during feature weight training.
5 Conclusion
In this work, we presented an approach that
can expand a translation model extracted from a
sentence-aligned, bilingual corpus using a large
amount of unstructured, monolingual data in both
source and target languages, which leads to im-
provements of 1.4 and 1.2 BLEU points over
strong baselines on evaluation sets, and in some
scenarios gains in excess of 4 BLEU points. In
the future, we plan to estimate the graph structure
through other learned, distributed representations.
Acknowledgments
The authors would like to thank Chris Dyer, Arul
Menezes, and the anonymous reviewers for their
helpful comments and suggestions.
684
References
Andrei Alexandrescu and Katrin Kirchhoff. 2009.
Graph-based learning for statistical machine trans-
lation. In Proceedings of Human Language Tech-
nologies: The 2009 Annual Conference of the North
American Chapter of the Association for Compu-
tational Linguistics, NAACL-HLT ?09, pages 119?
127. Association for Computational Linguistics,
June.
Ond?rej Bojar, Christian Buck, Chris Callison-Burch,
Christian Federmann, Barry Haddow, Philipp
Koehn, Christof Monz, Matt Post, Radu Soricut, and
Lucia Specia. 2013. Findings of the 2013 Work-
shop on Statistical Machine Translation. In Pro-
ceedings of the Eighth Workshop on Statistical Ma-
chine Translation, pages 1?44, Sofia, Bulgaria, Au-
gust. Association for Computational Linguistics.
Chris Callison-Burch, Philipp Koehn, and Miles Os-
borne. 2006. Improved statistical machine trans-
lation using paraphrases. In Proceedings of the
Human Language Technology Conference of the
NAACL, Main Conference, pages 17?24, New York
City, USA, June. Association for Computational
Linguistics.
Victor Chahuneau, Eva Schlinger, Noah A. Smith, and
Chris Dyer. 2013. Translating into morphologically
rich languages with synthetic phrases. In Proc. of
EMNLP.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Computational Linguistics, 33(2):201?228,
June.
Qing Dou and Kevin Knight. 2012. Large scale deci-
pherment for out-of-domain machine translation. In
Proceedings of the 2012 Joint Conference on Empir-
ical Methods in Natural Language Processing and
Computational Natural Language Learning, pages
266?275. Association for Computational Linguis-
tics, July.
Pascale Fung and Lo Yuen Yee. 1998. An ir approach
for translating new words from nonparallel, compa-
rable texts. In Proceedings of the 36th Annual Meet-
ing of the Association for Computational Linguis-
tics and 17th International Conference on Computa-
tional Linguistics - Volume 1, ACL ?98, pages 414?
420, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Michel Galley and Christopher D. Manning. 2008. A
simple and effective hierarchical phrase reordering
model. EMNLP ?08, pages 848?856, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,
and Dan Klein. 2008. Learning bilingual lexicons
from monolingual corpora. In Proceedings of ACL-
08: HLT, pages 771?779, Columbus, Ohio, June.
Association for Computational Linguistics.
Ann Irvine and Chris Callison-Burch. 2013a. Com-
bining bilingual and comparable corpora for low re-
source machine translation. In Proceedings of the
Eighth Workshop on Statistical Machine Transla-
tion, pages 262?270, Sofia, Bulgaria, August. As-
sociation for Computational Linguistics.
Ann Irvine and Chris Callison-Burch. 2013b. Su-
pervised bilingual lexicon induction with multiple
monolingual signals. In Proceedings of the 2013
Conference of the North American Chapter of the
Association for Computational Linguistics: Human
Language Technologies, pages 518?523, Atlanta,
Georgia, June. Association for Computational Lin-
guistics.
Alexandre Klementiev, Ann Irvine, Chris Callison-
Burch, and David Yarowsky. 2012. Toward sta-
tistical machine translation without parallel corpora.
In Proceedings of the 13th Conference of the Euro-
pean Chapter of the Association for Computational
Linguistics, pages 130?140, Avignon, France, April.
Association for Computational Linguistics.
Philipp Koehn and Kevin Knight. 2002. Learning a
translation lexicon from monolingual corpora. In
In Proceedings of ACL Workshop on Unsupervised
Lexical Acquisition, pages 9?16.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the 2003 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics on Human Language Technology - Vol-
ume 1, NAACL ?03, pages 48?54, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Shujie Liu, Chi-Ho Li, Mu Li, and Ming Zhou. 2012.
Learning translation consensus with structured la-
bel propagation. In Proceedings of the 50th Annual
Meeting of the Association for Computational Lin-
guistics: Long Papers - Volume 1, ACL ?12, pages
302?310, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Yuval Marton, Chris Callison-Burch, and Philip
Resnik. 2009. Improved statistical machine trans-
lation using monolingually-derived paraphrases. In
Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing, EMNLP
?09, pages 381?390, Singapore, August. Association
for Computational Linguistics.
David McClosky, Eugene Charniak, and Mark John-
son. 2006. Effective self-training for parsing. In
Proceedings of the Human Language Technology
Conference of the NAACL, Main Conference, pages
152?159, New York City, USA, June. Association
for Computational Linguistics.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics - Volume 1, ACL ?03, pages 160?
167, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
685
Kishore Papineni, Salim Roukos, Todd Ward, and Wei
jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. pages 311?318.
Reinhard Rapp. 1995. Identifying word translations in
non-parallel texts. In Proceedings of the 33rd An-
nual Meeting of the Association for Computational
Linguistics, ACL ?95.
Sujith Ravi and Kevin Knight. 2011. Deciphering for-
eign language. In Proceedings of the 49th Annual
Meeting of the Association for Computational Lin-
guistics: Human Language Technologies, pages 12?
21, Portland, Oregon, USA, June. Association for
Computational Linguistics.
Majid Razmara, Maryam Siahbani, Gholamreza Haf-
fari, and Anoop Sarkar. 2013. Graph propagation
for paraphrasing out-of-vocabulary words in statis-
tical machine translation. In Proceedings of the
51st of the Association for Computational Linguis-
tics, ACL-51, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Matthew Snover, Bonnie Dorr, and Richard Schwartz.
2008. Language and translation model adaptation
using comparable corpora. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP ?08, pages 857?866,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Akihiro Tamura, Taro Watanabe, and Eiichiro Sumita.
2012. Bilingual lexicon extraction from compara-
ble corpora using label propagation. In Proceedings
of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, EMNLP-CoNLL ?12,
pages 24?36.
Kristina Toutanova, Hisami Suzuki, and Achim Ruopp.
2008. Applying morphology generation models to
machine translation. In Proceedings of ACL-08:
HLT, pages 514?522, Columbus, Ohio, June. Asso-
ciation for Computational Linguistics.
Jiajun Zhang and Chengqing Zong. 2013. Learning
a phrase-based translation model from monolingual
data with application to domain adaptation. In Pro-
ceedings of the 51st Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long
Papers), pages 1425?1434, Sofia, Bulgaria, August.
Association for Computational Linguistics.
Xiaojin Zhu, Zoubin Ghahramani, and John D. Laf-
ferty. 2003. Semi-supervised learning using gaus-
sian fields and harmonic functions. In Proceedings
of the Twentieth International Conference on Ma-
chine Learning, ICML ?03, pages 912?919.
686
