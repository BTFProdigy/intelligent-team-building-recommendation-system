Proceedings of the EACL 2009 Workshop on Language Technologies for African Languages ? AfLaT 2009, pages 89?95,
Athens, Greece, 31 March 2009. c?2009 Association for Computational Linguistics
A repository of free lexical resources for African languages:  
the project and the method 
Piotr Ba?ski 
Institute of English Studies 
University of Warsaw 
Warsaw, Poland 
bansp@o2.pl 
Beata W?jtowicz 
Institute of Oriental Studies 
University of Warsaw 
Warsaw, Poland 
b.wojtowicz@uw.edu.pl 
 
Abstract 
We report on a project which we believe to 
have the potential to become home to, among 
others, bilingual dictionaries for African lan-
guages. Kept in a well-structured XML for-
mat with several possible degrees of confor-
mance, the dictionaries will be able to get us-
able even in their early versions, which will 
be then subject to supervised improvement as 
user feedback accumulates. The project is 
FreeDict, part of SourceForge, a well-known 
Internet repository of open source content.  
We demonstrate a possible process of dic-
tionary development on the example of one 
of FreeDict dictionaries, a Swahili-English 
dictionary that we maintain and have been 
developing through subsequent stages of in-
creasing complexity and machine-
processability. The aim of the paper is to 
show that even a small bilingual lexical re-
source can be submitted to this project and 
gradually developed into a machine-
processable form that can then interact with 
other FreeDict resources. We also present the 
immediate benefits of locating bilingual Afri-
can dictionaries in this project. 
We have found FreeDict to be a very promis-
ing project with a lot of potential, and the 
present paper is meant to spread the news 
about it, in the hope to create an active com-
munity of linguists and lexicographers of 
various backgrounds, where common re-
search subprojects can be fruitfully carried 
out. 
1 Introduction 
The FreeDict project was started by Horst Eyer-
mann in 2000 and initially hosted bilingual dic-
tionaries produced by concatenating (crossing) 
the contents of the dictionaries in the Ergane pro-
ject (http://download.travlang.com/Ergane/), 
with Esperanto as the interlanguage. At first, the 
data was kept in the DICT format (Faith and 
Martin, 1997). 
DICT (Dictionary Server Protocol) is by now 
a well-established TCP-based query/response 
protocol that allows a client to access definitions 
from a set of various dictionary databases. It 
provides data in textual form, but it also has the 
potential of providing MIME-encoded content. 
The clients can be free-standing desktop applica-
tions or they can be integrated into editors or 
web browsers. DICT web gateways also exist  
(see e.g. http://dict.org/). 
The DICT format is a plain text format with an 
accompanying index file. The FreeDict-DICT 
interface initially used so-called ?c5 files?.1 A c5 
example of an entry from version 0.0.1 of Swa-
hili-English dictionary is presented below. 
 
abiria 
     passenger(s) 
 
Later on, the project adopted the TEI P4 standard 
(Sperberg-McQueen and Burnard, 2002) as its 
primary format, and with the help of its second 
administrator, Michael Bunk, created tools for 
conversion from the TEI into a variety of other 
dictionary platforms. A simple dictionary editor 
was also created. The change of the primary 
format was a very fortunate move, thanks to 
which we can today recommend the project as 
the possible home for free African language dic-
tionaries big and small. 
We are going to base our discussion on the 
Swahili-English dictionary, the first FreeDict 
dictionary encoded according to the guidelines of 
TEI P5 XML standard (TEI Consortium, 2007). 
The dictionary in its current form is an offshoot 
of a different project of ours that we decided to 
make available on a free license and in version 
0.4 contains over 2600 headwords. We use it to 
demonstrate a possible process of dictionary de-
                                                 
1 C5 is the format used, among others, by the CIA World 
Factbook, where the heading is at the left edge and the con-
tents are indented by 5 spaces. 
89
velopment, from the simplest to the advanced, 
machine-processable form. 
2 From glossaries to rich lexical data-
bases: the possible shapes of FreeDict 
dictionaries 
A dictionary can begin its life at FreeDict as a 
simple glossary, with the simplest format possi-
ble, as shown in the made-up entry below: 
 
<entry> 
<form><orth>alasiri</orth></form> 
<def>afternoon</def> 
</entry> 
 
The next example entry comes from Swahili-
English xFried Freedict Dictionary, version 
0.0.2, compiled by Beata W?jtowicz. That dic-
tionary contained around 1500 entries of varied 
quality. It was based on a dictionary extracted 
from Morris P. Fried?s Swahili-Kiswahili to Eng-
lish Translation Program, to which selected en-
tries from the first FreeDict Swahili-English dic-
tionary (compiled by Horst Eyermann) were 
added. That version also introduced information 
on parts of speech. 
 
<entry> 
<form><orth>alasiri</orth></form> 
<def>   afternoon</def> 
<gramGrp> <pos>n</pos> </gramGrp> 
</entry> 
 
On the way to version 0.3, the entry looked as 
follows: 
 
<entry xml:id="alasiri">      
  <form><orth>alasiri</orth></form>             
  <gramGrp><pos>n</pos></gramGrp> 
  <sense> 
 <def>afternoon (period between 3   
  p.m. and 5 p.m.)</def> 
  </sense> 
</entry> 
 
All the bracketed information was then turned 
into separate <note/> elements, in order to 
make the translation equivalents easily proc-
essable (see Prinsloo and de Schryver, 2002, for 
remarks on processability of translation equiva-
lents). The change was performed by regex 
search-and-replace, roughly from 
\((.+)\)</def> into </def><note 
type="hint">$1</note>2, with a subsequent 
                                                 
2 This is in fact a slight simplification of what has been 
done, made for the purpose of clarity. Naturally, the regexes 
have to be adopted to the circumstances (regularity of mar-
kup, regularity of expressions in brackets, the number of 
review of all the new <note/> elements ex-
tracted by an XPath query. 
Depending on the regularity of expressions in 
brackets, some additional words would be in-
serted into the search string, to be converted into 
<note/> elements of the appropriate type. Ini-
tially, only @type="hint" was used, as the 
most generic. At the moment, there are several 
more specialized type values, including 
@type="editor", which contains editorial re-
marks that will not be shown to the user but will 
remain in the source. @type-less <note/> ele-
ments are used for quick localized communica-
tion between editors and are discarded by XSLT 
scripts when preparing the source version for 
release (they are also clearly marked by the CSS 
stylesheet that accompanies the dictionary, so 
that the editors can easily spot each note when 
reviewing the dictionary in a browser). FreeDict 
advocates the use of some other types of notes: 
recording the last editor of the entry, the date of 
the latest modification, and the degree of cer-
tainty, valuable in this kind of projects (where, 
e.g., some automated changes would set the cer-
tainty level to ?low? and as such requiring edito-
rial approval). 
In the current version, 0.4, the <sense/> 
element looks as follows. 
 
<sense> 
<def>afternoon</def> 
<note type="def">period between 3    
   p.m. and 5 p.m.</note> 
</sense> 
 
This is what we decided to keep in version 0.4, 
exactly for the purpose of illustrating the possi-
ble development stages of dictionaries. In the 
next version, the <sense/> element will eventu-
ally attain the form currently (i.e., after Septem-
ber 2007) recommended by the TEI Guidelines 
for translation equivalents: 
 
<sense> 
<cit type="trans"> 
    <quote>afternoon</quote> 
<def>period between 3 p.m. and 
5 p.m.</def> 
</cit> 
</sense> 
 
The <quote/> element holds the translation 
equivalent that can be an anchor for dictionary 
                                                                          
such expressions per single element content, etc.). Some-
times, an XSL transformation may turn out to do a better 
job, thanks to the many string-handling functions of XPath 
2.0. 
90
reversal or concatenation. The <def/> element 
above is not abused anymore and holds a real 
definition, i.e., an ?explanatory equivalent?, 
which may become a sense-discriminating note 
in the reversed dictionary. 
We stress that each of the XML structures pre-
sented above (some of them admittedly border-
ing on tag abuse) conforms to the general P5 
format and can be easily processed and pub-
lished. In other words, dictionary editors are not 
forced to conform to the final format in order to 
see their work being used and commented on. 
The next section presents another aspect of 
dictionary creation, where what matters is the 
ease of data manipulation and filling in the pre-
dictable information for the developer. 
3 Plural forms: an illustration of auto-
mated creation of entries 
At the stage of development at which brackets 
have been eliminated from translation equiva-
lents, a relatively simple entry might look as 
shown below. This is an entry as created by a 
developer, to be further processed by XSLT.3 
 
<entry> 
<form> 
 <orth>adui</orth> 
 <ref target="#maadui"/> 
</form> 
<gramGrp><pos>n</pos></gramGrp> 
<sense><def>enemy</def></sense> 
<sense> 
 <def>opponent</def> 
 <note type="hint">in games or     
 sports</note> 
</sense> 
</entry> 
 
This entry is then processed and turned into the 
form presented below. 
 
<entry xml:id="adui"> 
<form> 
 <orth>adui</orth> 
</form> 
<xr type="plural-form"><ref tar-
get="#maadui">maadui</ref></xr> 
<gramGrp> 
 <pos>n</pos> 
</gramGrp> 
<sense xml:id="adui.1" n="1"> 
 <def>enemy</def> 
</sense> 
<sense xml:id="adui.2" n="2"> 
                                                 
3 The verbosity of XML markup can be overwhelming, but 
many XML editors feature content completion and can save 
the developer a lot of typing, the more so that TEI schemas 
are often part of the editor package.  
 <def>opponent</def> 
 <note type="hint">in games or     
 sports</note> 
</sense> 
</entry> 
 
If the plural form does not exist in the dictionary, 
the script creates an entry for it: 
 
<entry xml:id="maadui"> 
<form> 
 <orth>maadui</orth> 
</form> 
<gramGrp> 
 <pos>n</pos> 
</gramGrp> 
<sense> 
 <xr type="plural-sense">Plural of    
 <ref target="#adui">adui</ref> 
 </xr> 
 <def>enemy</def> 
 <def>opponent <note type="hint">in   
 games or sports</note></def> 
</sense> 
</entry> 
 
The equivalents in this kind of automatically cre-
ated entries for plurals will remain within 
<def/> elements even after we adopt the 
cit/quote system mentioned above in the discus-
sion of alasiri. This is because <def/> elements 
are not anchors for dictionary reversal, and plural 
entries will be skipped by the reversal tools, 
unless the plural/collective form has its own 
unique meaning, as is the case with e.g. majani, 
which is morphologically the plural form of jani 
?leaf; blade of grass?, but apart from that, it 
should also be glossed as ?grass?, and it is the 
latter form that should become a headword in the 
reversed, English-Swahili dictionary. 
Another area where the XML format and tools 
give excellent results is text normalization. An 
earlier example shows unnecessary spaces in 
version 0.0.2: <def>   afternoon</def>. 
Handling these required only the use of an XPath 
function normalize-space(), which strips all 
the unwanted whitespace characters.4 
All the indexing is also done automatically. 
The indexing system in this particular dictionary 
is based on the shape of the headword, which it 
is easy to convert into form acceptable by the 
XML ID attributes (the XPath translate() 
and replace() functions are handy here). All 
                                                 
4 That version contained more traps for machine-processing, 
such as bracketed parts of words ? sometimes this was 
done in a nontrivial manner, as in the entry for adui: 
<def>   enemy(-ies)</def>. See Prinsloo and de 
Schryver (2002) for remarks on the non-friendliness of such 
space-saving devices. 
91
the entries are first reordered alphabetically, then 
the script checks for homographs and assigns 
them appropriate indexes (e.g. chapa-1 for the 
noun meaning ?brand?, chapa-2 for the verb 
meaning ?beat?) and appropriate attributes used 
later in the creation of superscripts (suppressed 
in the plain-text DICT format). Multiple senses 
are also treated similarly ? each receives its 
own @xml:id attribute and numbering (see the 
example of adui above). 
We emphasize the fact that the encoding for-
mat makes it possible to reduce the developer?s 
workload, with each stage of the dictionary en-
hancement being publishable. This allows one to 
work on the dictionary on and off, in their spare 
time. 
4 Visualization of underlying structure 
Some Swahili words are best lemmatized as 
stems. Version 0.4 of our dictionary does not yet 
display the differences between bound stems and 
free forms, but we will transfer this functionality 
(which we use in another project) to one of the 
future versions. This will make it possible for us 
to, e.g., add hyphens to bound forms and prepend 
?-a ? to adjectives introduced by the ?-a of rela-
tionship?, adding extra structure visible to the 
end user, but ignored in sorting or queries. 
Disjunctively written languages can be han-
dled similarly. Kiango (2000:36) lists the follow-
ing examples from Haya, discussing the prob-
lems surrounding alphabetization of nouns in 
print dictionaries: 
 
a ka yaga ?air? 
e m pambo ?seed? 
o mu twe  ?head? 
The vocalic pre-prefixes should not be used for 
the purpose of arranging headwords, because if 
they are, all nouns end up under one of the three 
letters. Instead, class prefixes (ka, m, mu) should 
form the basis for alphabetization. In the 
XML/TEI format adopted by FreeDict, such 
problems are easily solved, either by using a 
separate element to hold the pre-prefix, or by the 
use of an appropriate attribute. The former solu-
tion is illustrated below. 
 
<entry> 
<form> 
 <orth extent="ppref">a</orth> 
 <orth>ka yaga</orth> 
</form> 
<def>air</def> 
</entry> 
The default value for the @extent attribute is 
?full?, so it only needs to be mentioned where 
the value is different.  
An AfLaT reviewer rightly points out that in 
an electronic dictionary, alphabetization is irrele-
vant. Indeed, the DICT format features separate 
?dictionary? and ?index? files, and searching is 
done on the index file, which addresses the rele-
vant portions of the dictionary file. The issue of 
alphabetization arises, however, in two cases: 
when preparing a print version of the dictionary, 
or when using the dictionary outside of the DICT 
system (this is a ?working view? for the main-
tainers that can also be used as an out-of-the-box 
view for users). 
In connection with the first case ? preparing 
print/PDF versions of dictionaries ? it is worth 
pointing out that conversion from TEI XML into 
various publication formats is made easy thanks 
to the open-source XSLT conversion suite main-
tained by Sebastian Rahtz (http://www.tei-
c.org/Tools/Stylesheets/). 
As for the ?out-of-the-box preview?, FreeDict 
dictionaries, by virtue of being marked up in 
XML, can be equipped with CSS stylesheets that 
make it possible to display the XML source in 
the browser, as if it were an HTML page. Here, 
because the user can search the page for the 
given form, alphabetization is not so relevant, 
but it can be handy, if only for aesthetic reasons. 
Below is a screenshot of a fragment of the CSS 
view of the file swa-eng.tei from version 0.4 dis-
tribution package, opened directly in the Firefox 
browser. 
 
 
Figure 1: A CSS preview of the source XML 
 
The CSS adds some text (e.g. ?[sg=pl]?, ?(pl:?, 
or sense numbering) and imposes visual structure 
onto the source XML. As can be seen in the en-
try for kiungo, we give precedence to formal 
92
properties of headwords over the semantic dis-
tinctions, but other macrostructural decisions are 
obviously possible as well. The figure below 
shows one more CSS view, demonstrating sub-
categorization, treatment of notes (all the brack-
eted strings are contents of separate XML ele-
ments, with parentheses supplied by the CSS), as 
well as the treatment of homographs.  
 
 
Figure 2: Subcategorization and grammar notes (an-
other CSS view of the source XML) 
 
Our use of colours (here: shades of grey) is also a 
function of the CSS, introduced mainly with the 
developer in mind, as a kind of an error-checking 
device. 
5 Other possible enhancements of the 
microstructure 
In section 3, we have illustrated a possible 
method of refining dictionary structure in an 
automatic fashion. Thanks to the many possible 
variations of the format, other features may be 
introduced stage by stage. They include, e.g., 
adding the corresponding plurals (illustrated 
above) or marking forms where the plural is the 
same as the singular, as done below by the use of 
the @type attribute. This example also illustrates 
the addition of cross-references to synonyms, 
where eropleni is linked to the second, inani-
mate, sense of ndege ?bird; airplane?. 
 
<entry xml:id="eropleni"> 
<form type="N"> 
 <orth>eropleni</orth> 
</form> 
<gramGrp> 
 <pos>n</pos> 
</gramGrp> 
<sense> 
 <def>airplane</def> 
 <xr type="syn">(synonym: <ref tar  
 get="#ndege.2">ndege</ref>)</xr> 
</sense> 
</entry> 
 
The <form/> element below illustrates the han-
dling of alternative spellings of the noun af-
isa/ofisa ?officer?. Both headwords are used in 
retrieval. 
 
<entry xml:id="afisa"> 
  <form> 
<orth n="1">afisa</orth> 
<orth type="variant"  
          n="2">ofisa</orth> 
<ref target="#maafisa" n="1"/> 
<ref target="#maofisa" n="2"/> 
  </form> 
The above is the source as prepared by a devel-
oper. This is then processed, the plural forms are 
created if they do not exist in the dictionary, and 
the result is as in figure 3 below. 
 
Figure 3: Illustration of alternative spellings/plurals 
(CSS view) 
 
Other examples of what can be gradually intro-
duced into the dictionary include addition of 
subcategorization information (illustrated by 
pako in Figure 2, where ?pron? is the POS and 
?poss? is the content of the <subc/> element), 
addition of explicit noun-class- and agreement-
marking, introduction of irregularly inflected 
forms, tables with inflection (linked to the ap-
propriate stems), nested entries, and, obviously, 
the continuous improvement of lexicographic 
information (the arrangement and selection of 
senses, selection of headwords, an appropriate 
POS system). 
Crucially, this system allows a developer to 
?publish early, publish often?, and few of the 
enhancements mentioned here depend on others 
? developers are free to choose and to extend 
the dictionary at their own pace. 
In our case, this is a gradual move towards 
structures of finer granularity, suitable for rever-
sal (into English-Swahili) and concatenation with 
other dictionaries (we are going to use English as 
the bridge language for pairing the Swahili-
English dictionary with English-* dictionaries).  
6 Potential for the future 
We wish to stress the potential that the encoding 
format and the entire project have for producing 
lexical resources for ?non-commercial? lan-
guages, where funding and the time that the de-
velopers may spend on the dictionary are not al-
ways guaranteed. FreeDict dictionary develop-
ment can proceed in stages, one can start with a 
93
simple format and get the dictionary published 
on-line practically within days. The project has 
all the SourceForge publishing facilities at its 
disposal, together with bug/patch/etc. trackers 
and community forums. It also has a mailing list 
and a wiki that can serve to document some pos-
sibly difficult aspects of dictionary creation. 
Thanks to the robust build system of FreeDict, 
creating a tarball containing a DICT-formatted 
dictionary and index is only a matter of issuing 
the ?make? command with appropriate argu-
ments, and submitting the resulting archive to the 
SourceForge file release system.5 
FreeDict is the nexus for the following: 
? XML, with its potential for creating 
well-structured documents, 
? TEI P5, a de-facto standard taking ad-
vantage of this potential,  
? the SourceForge repository as well as 
distribution and content-management net-
work, 
? the DICT distribution network: apart 
from being able to query DICT servers 
straight from the desktop, Firefox users 
can also take advantage of an add-on cli-
ent that returns definitions for highlighted 
words on a web page, 
? FreeDict tools (still under development 
for TEI P5) as means to manipulate dic-
tionaries and to create, among others, the 
DICT format (usable directly from DICT 
servers and by other dictionary-providing 
projects, e.g., StarDict or Open Dict).6 
Additionally, lexical resources submitted to 
FreeDict may undergo further transformations: 
reversal or concatenation, which means that 
work put into developing a single resource may 
well be re-used in developing others. Consider-
ing the possible re-use of lexical resources, they 
are expected to be prepared with a view towards 
clean exposure of translation equivalents (in the 
cit/quote system or at least by judicious use of 
separators and brackets). 
The project has its own distribution system, in 
the form of GNU/Linux packages ? for exam-
                                                 
5 This is something that a dictionary creator need not bother 
about ? submitting a TEI source of the dictionary to the 
mailing list is enough. 
6 The FreeDict build process provides targets for platforms 
other than DICT, e.g. the Evolutionary Dictionary 
(http://www.mrhoney.de/y/1/html/evolutio.htm) or zbedic 
(http://bedic.sourceforge.net/). 
ple, K?stutis Bili?nas is the packager for Debian 
Linux and maintains a page tracking the usage of 
Debian-FreeDict packages; 
Apart from the above, the content published 
by FreeDict is guaranteed to be free.  
7 The costs of developing for FreeDict 
An AfLaT reviewer suggested that we provide a 
measure of the effort required to develop a re-
source for FreeDict. We hope to show here that 
this is very much dependent on the quality and 
form of the resource and on how much time the 
dictionary creator is willing to invest into it. Cru-
cially, given the open-source nature of the pro-
ject, even a simple, small list of near-binary pair-
ings of equivalents can be a) quickly made useful 
to e.g. the readers of web pages written in the 
given language, and b) extended by others into a 
more satisfying resource. 
It may be that the effort needed to create lan-
guage resources in e.g. Lexique Pro, an excellent 
free tool by SIL International 
(http://www.lexiquepro.com/) is smaller, but 
there are differences in that Lexique Pro is a 
Windows-only closed-source program whose 
native MDF (Multi-Dictionary-Formatter) format 
is not as flexible as XML and therefore cannot be 
processed by the many tools that handle XML 
and TEI in particular. Consequently, the perspec-
tives for re-use of Lexique Pro dictionaries in 
computational linguistic applications are much 
smaller. To our knowledge, Lexique Pro does not 
make it possible for users to query words straight 
off web pages, which can be done thanks to dict, 
a Firefox add-on (http://dict.mozdev.org/). It ad-
mittedly has other advantages that make it a seri-
ous alternative.7 
The ideal solution would be to have an editing 
front-end such as Lexique Pro coupled with the 
openness and modifiability of the data offered by 
FreeDict. Indeed, there are plans for creating a 
converter from the new LIFT interchange stan-
dard (http://code.google.com/p/lift-standard/) 
that the beta versions of Lexique Pro can read 
                                                 
7 We do not discuss professional commercial dictionary 
writing systems such as TshwaneLex 
(http://tshwanedje.com/tshwanelex/) because, despite the 
academic discounts, they may be out of range for the aver-
age developer. It is worth mentioning that the discounted 
versions of TshwaneLex come with the understandable ?no-
commercial-use? restriction, which is in conflict with either 
the GNU Public License or the nearly equivalent Creative 
Commons BY-SA license that all SourceForge resources 
must be under (cf. http://www.gnu.org/licenses/gpl-
faq.html). 
94
and write, to the version of the TEI format used 
by FreeDict. That would undoubtedly enhance 
the attractiveness of the project. 
To sum up, developing for FreeDict minimally 
requires some basic knowledge of XML. Free 
XML editors exist (e.g. XML Copy Editor, 
http://sourceforge.net/projects/xml-copy-editor/) 
that can make editing easier by autocompleting 
the elements (inserting closing tags, suggesting 
elements and attributes that are allowed at the 
given place in the structure) and signalling en-
coding mistakes. 
8 African Languages and FreeDict 
A reviewer remarked that the link to African lan-
guage technology in this paper appears only to be 
present in the examples. Indeed, FreeDict is not a 
project that focuses on African-languages ? it is 
a project where African language resources can 
be hosted and quickly become useful to users. 
Given an opportunity, we will encourage re-
searches dealing with other languages to join the 
project ? which will hopefully result in the crea-
tion of more cross-language resources, especially 
given that the encoding format is not tied to any 
particular language and is able to easily accom-
modate features characteristic of practically any 
language. 
During the session on ?African Languages in 
Advance? at the 2008 Pozna? Linguistic Meet-
ing, where we presented our Swahili-Polish pro-
ject and also mentioned FreeDict as the place 
where we wanted to donate parts of our test 
Swahili-English dictionary that would otherwise 
remain on our disks, we talked to an organizer of 
that session, Karien Brits, about the advantages 
that this project can have for some of her col-
leagues working on South-African languages. 
This is what encouraged us to move on with the 
FreeDict Swahili-English dictionary and we hope 
that others will also find this project, and the 
possibilities that it offers, attractive. 
The FreeDict project has recently awoken af-
ter a period of lower activity, and at the moment, 
every week brings something new. Currently, as 
far as African languages are concerned, apart 
from Swahili?English dictionaries, the project 
hosts very basic Afrikaans?English dictionaries, 
and an Afrikaans-German dictionary (all of them 
in need of a maintainer).8 We hope that FreeDict 
                                                 
8 Dictionary sources can be accessed from the ?download? 
link on the project page: http://sourceforge.net/projects/ 
freedict/. They can be queried online at e.g. http://dict.org/, 
by setting the appropriate language pair in the database. 
will become home to many African language 
resources, and, thanks to the possibility of dic-
tionary concatenation, facilitate also the creation 
of many African?European dictionary pairs as 
well as all-African bilingual dictionaries. 
Acknowledgements 
We are grateful to three anonymous AfLaT-2009 
reviewers for their helpful comments on the pre-
vious version of this paper, and to Michael Bunk 
for confirming that our version of the history of 
the project is close to reality. We also wish to 
thank Janusz S. Bie? for turning our attention to 
the FreeDict project a few years ago.  
References  
DICT: http://www.dict.org/ 
Faith, Rik and Martin, Brett. 1997. A Dictionary 
Server Protocol. Request for Comments: 2229 
(RFC #2229). Network Working Group. Available 
from ftp://ftp.isi.edu/in-notes/rfc2229.txt (last ac-
cessed on January 19, 2009). 
FreeDict: http://www.freedict.org/, 
http://freedict.sourceforge.net/ 
Kiango, John G. 2000. Bantu lexicography: a criti-
cal survey of the principles and process of 
constructing dictionary entries. Tokyo: Institute 
for the Study of Languages and Cultures of Asia 
and Africa, Tokyo University of Foreign Studies. 
Prinsloo, Danie J. and Gilles-Maurice de Schryver. 
2002. Reversing an African-language lexicon: the 
Northern Sotho Terminology and Orthography No. 
4 as a case in point. South African Journal of Afri-
can Languages, 22/2: 161?185 
Sperberg-McQueen, Michael and Lou Burnard (eds). 
2002. The Text Encoding Initiative Guidelines 
(P4). Text Encoding Initiative, Oxford. 
TEI Consortium, eds. 2007. TEI P5: Guidelines for 
Electronic Text Encoding and Interchange. 
Version 1.2.0. Last updated on October 31st 2008. 
TEI Consortium.   Available from                             
http://www.tei-c.org/Guidelines/P5/ (last accessed 
on January 19, 2009). 
95
Balto-Slavonic Natural Language Processing 2007, June 29, 2007, pages 43?50,
Prague, June 2007. c?2007 Association for Computational Linguistics
Towards the Automatic Extraction of Definitions in Slavic
1Adam Przepio?rkowski
2?ukasz Dego?rski
8Beata Wo?jtowicz
Institute of Computer Science PAS
Ordona 21, Warsaw, Poland
adamp@ipipan.waw.pl
ldegorski@bach.ipipan.waw.pl
beataw@bach.ipipan.waw.pl
4Kiril Simov
5Petya Osenova
Institute for Parallel Processing BAS
Bonchev St. 25A, Sofia, Bulgaria
kivs@bultreebank.org
petya@bultreebank.org
3Miroslav Spousta
7Vladislav Kubon?
Charles University
Malostranske? na?me?st?? 25
Prague, Czech Republic
spousta@ufal.ms.mff.cuni.cz
vk@ufal.ms.mff.cuni.cz
6Lothar Lemnitzer
University of Tu?bingen
Wilhelmstr. 19, Tu?bingen, Germany
lothar@sfs.uni-tuebingen.de
Abstract
This paper presents the results of the prelim-
inary experiments in the automatic extrac-
tion of definitions (for semi-automatic glos-
sary construction) from usually unstructured
or only weakly structured e-learning texts
in Bulgarian, Czech and Polish. The ex-
traction is performed by regular grammars
over XML-encoded morphosyntactically-
annotated documents. The results are less
than satisfying and we claim that the rea-
son for that is the intrinsic difficulty of the
task, as measured by the low interannota-
tor agreement, which calls for more sophis-
ticated deeper linguistic processing, as well
as for the use of machine learning classifica-
tion techniques.
1 Introduction
The aim of this paper is to report on the preliminary
results of a subtask of the European Project Lan-
guage Technology for eLearning (http://www.
lt4el.eu/) consisting in the identification of
term definitions in eLearning materials (Learning
Objects; henceforth: LOs), where definitions are
understood pragmatically, as those text fragments
which may, after perhaps some minor editing, be
put into a glossary. Such automatically extracted
term definitions are to be presented to the author or
the maintainer of the LO and, thus, significantly fa-
cilitate and accelerate the creation of a glossary for
a given LO. From this specification of the task it fol-
lows that good recall is much more important than
good precision, as it is easier to reject wrong glos-
sary candidates than to browse the LO for term def-
initions which were not automatically spotted.
The project involves 9 European languages in-
cluding 3 Slavic (and, regrettably, no Baltic) lan-
guages: one South Slavic, i.e., Bulgarian, and two
West Slavic, i.e., Czech and Polish. For all lan-
guages, shallow grammars identifying definitions
have been constructed; after mentioning some previ-
ous work on Information Extraction (IE) for Slavic
languages and on extraction of definitions in sec-
tion 2, we briefly describe the three Slavic grammars
developed within this project in section 3. Section 4
presents the results of the application of these gram-
mars to LOs in respective languages. These results
are evaluated in section 5, where main problems, as
well as some possible solutions, are discussed. Fi-
nally, section 6 concludes the paper.
43
2 Related Work
Definition extraction is an important NLP task,
most frequently a subtask of terminology extraction
(Pearson, 1996), the automatic creation of glossaries
(Klavans and Muresan, 2000; Klavans and Muresan,
2001), question answering (Miliaraki and Androut-
sopoulos, 2004; Fahmi and Bouma, 2006), learning
lexical semantic relations (Malaise? et al, 2004; Stor-
rer and Wellinghoff, 2006) and automatic construc-
tion of ontologies (Walter and Pinkal, 2006). Tools
for definition extraction are invariably language-
specific and involve shallow or deep processing,
with most work done for English (Pearson, 1996;
Klavans and Muresan, 2000; Klavans and Muresan,
2001) and other Germanic languages (Fahmi and
Bouma, 2006; Storrer and Wellinghoff, 2006; Wal-
ter and Pinkal, 2006), as well as French (Malaise? et
al., 2004). To the best of our knowledge, no previ-
ous attempts at definition extraction have been made
for Slavic, with the exception of some work on Bul-
garian (Tanev, 2004; Simov and Osenova, 2005).
Other work on Slavic information extraction has
been carried out mainly for the last 5 years. Prob-
ably the first forum where such work was compre-
hensively presented was the International Workshop
on Information Extraction for Slavonic and Other
Central and Eastern European Languages (IESL),
RANLP, Borovets, 2003, Bulgaria. One of the pa-
pers presented there, (Droz?dz?yn?ski et al, 2003), dis-
cusses shallow SProUT (Becker et al, 2002) gram-
mars for Czech, Polish and Lithuanian. SProUT has
subsequently been extensively used for the informa-
tion extraction from Polish medical texts (Piskorski
et al, 2004; Marciniak et al, 2005).1
3 Shallow Grammars for Definition
Extraction
The input to the task of definition extraction is
XML-encoded morphosyntactically-annotated text,
possibly with some keywords already marked by an
1SProUT has not been seriously considered for the task at
hand for two reasons: first, it was decided that only open source
tools will be used in the current project, if only available, sec-
ond, the input format to the current task is morphosyntactically-
annotated XML-encoded text, rather than raw text, as normally
expected by SProUT. The second obstacle could be removed by
converting input texts to the SProUT-internal XML representa-
tion.
independent process. For example, the representa-
tion of a Polish sentence starting as Konstruktywizm
k?adzie nacisk na (Eng. ?Constructivism puts em-
phasis on?) may be as follows:2
<s id="s9">
<markedTerm id="mt7" kw="y">
<tok base="konstruktywizm" ctag="subst"
id="t253"
msd="sg:nom:m3">Konstruktywizm</tok>
</markedTerm>
<tok base="klasc" ctag="fin" id="t254"
msd="sg:ter:imperf">kladzie</tok>
<tok base="nacisk" ctag="subst" id="t255"
msd="sg:acc:m3">nacisk</tok>
<tok base="na" ctag="prep" id="t256"
msd="acc">na</tok>
[...]
<tok base="." ctag="interp" id="t273">.
</tok>
</s>
For each language, definitions were manually
marked in two batches of texts: the first batch, con-
sulted during the process of grammar development,
contained at least 300 definitions, and the second
batch, held out for evaluation, contained about 150
definitions. All grammars are regular grammars im-
plemented with the use of the lxtransduce tool
(Tobin, 2005), a component of the LTXML2 toolset
developed at the University of Edinburgh.3 An ex-
ample of a simple rule for prepositional phrases is
given below:
<rule name="PP">
<seq>
<query match="tok[@ctag = ?prep?]"/>
<ref name="NP1">
<with-param name="case" value="??"/>
</ref>
</seq>
</rule>
This rule identifies a sequence whose first element
is a token tagged as a preposition and whose subse-
quent elements are identified by a rule called NP1.
This latter rule (not shown here for brevity) is a pa-
rameterised rule which finds a nominal phrase of a
given case, but the way it is called above ensures that
it will find an NP of any case.
2Part of the representation has been replaced by ?[...]?.
3Among the tools considered here were also CLaRK (Simov
et al, 2001), ultimately rejected because it currently does not
work in batch mode, and GATE / JAPE (Cunningham et al,
2002), not used here because we found GATE?s handling of
previously XML-annotated texts rather cumbersome and ill-
documented. Cf. also fn. 1.
44
Currently the grammars show varying degrees of
sophistication, with a small Bulgarian grammar (8
rules in a 2.5-kilobyte file), a larger Polish grammar
(34 rules in a 11KiB file) and a sophisticated Czech
grammar most developed (147 rules in a 28KiB
file). The patterns defined by these three grammars
are similar, but sufficiently different to defy an at-
tempt to write a single parameterised grammar.4 The
remainder of this section briefly describes the gram-
mars.
3.1 Bulgarian
The Bulgarian grammar is manually constructed af-
ter examination of the manually annotated defini-
tions. Here is a list of the rule schemata, together
with the number and percentage of matching defini-
tions:
Pattern # %
NP is NP 140 34.2
NP verb NP 18 29.8
NP - NP 21 5.0
This is NP 15 3.7
It represents NP 4 1.0
other patterns 107 26.2
Table 1: Bulgarian definition types
In the second schema above, ?verb? is a verb or
a verb phrase (not necessarily a constituent) which
is one of the following: ?????????????? (to repre-
sent), ????????? (to show), ?????????? (to mean),
???????? (to describe), ??? ????????? (to be used),
??????????? (to allow), ????? ?????????? ???
(to give opportunity), ??? ??????? (is called),
??????????? (to improve), ??????????? (to ensure),
?????? ??? (to serve as), ??? ???????? (to be under-
stood as), ???????????? (to denote), ????????? (to
contain), ?????????? (to determine), ?????????
(to include), ??? ???????? ????? (is defined as),
??? ???????? ??? (is based on).
We classify the rules in five types: copula defi-
nitions, copula definitions with anaphoric relation,
copula definitions with ellipsis of the copula, defi-
nitions with a verb phrase, definitions with a verb
4Because of this relative language-dependence of definition
patters, which includes, e.g., idiosyncratic case information,
we have not seriously considered re-using rules for other, non-
Slavic, languages.
phrase and anaphoric relation. Each of these types of
definitions defines an NP (sometimes via anaphoric
relation) by another one. There are some variations
of the models where some parenthetical expressions
are presented in the definition.
The grammar contains several most important
rules for each type. The different verb patterns are
encoded as a lexicon. For some of the rules, variants
with parenthetical phrases are also encoded. The rest
of the grammar is devoted to the recognition of noun
phrases and parenthetical phrases. For parentheti-
cal phrases, we have encoded a list of such possible
phrases, extracted on the basis of a bigger corpus.
The NP grammar in our view is the crucial grammar
for recognition of the definitions. Most work now
has to be invested into developing the more complex
and recursive NPs.
3.2 Czech
The Czech grammar for definition context extraction
is constructed to follow both linguistic intuition and
observation of common patterns in manually anno-
tated data.
We adapted a grammar5 based mainly on the ob-
servation of Czech Wikipedia entries. Encyclopedia
definitions are usually clear and very well structured,
but it is quite difficult to find such well-formed defi-
nitions in common texts, including learning objects.
The rules were extended using part of our manually
annotated texts, evaluated and adjusted in several it-
erations, based on the observation of the annotated
data.
Pattern # %
NP is/are NP 52 21.2
NP verb NP 45 18.4
structural 39 15.9
NP (NP) 30 12.2
NP -/:/= NP 20 8.2
other patterns 59 24.1
Table 2: Czech definition types
There are 21 top level rules, divided into five cate-
gories. Most of the correctly marked definitions fall
into the copula verb (?is/are?) category. The sec-
5The grammar was originally developed by Nguyen Thu
Trang.
45
ond most successful rule is the one using selected
verbs like ?definuje? (defines), ?znamen?? (means),
?vymezuje? (delimits), ?pr?edstavuje? (presents) and
several others. The remaining categories make use
of the typical patterns of characters (dash, colon,
equal sign and brackets) or additional structural in-
formation (e.g., HTML tags).
3.3 Polish
The Polish grammar rules are divided into three lay-
ers. Similarly to the Czech grammar, each layer only
refers to itself or lower layers. This allows for ex-
pressing top level rules in a clear and easily man-
ageable way.
The top level layer consists of rules representing
typical patterns found in Polish documents:
Pattern # %
NP (...) are/is NP-INS 40 15.6
NP -/: NP 39 15.2
NP (are/is) to NP-NOM 27 10.6
NP VP-3PERS 25 9.8
NP - i.e./or WH-question 11 4.3
N ADJ - PPAS 8 3.1
NP, i.e./or NP 7 2.7
NP-ACC one may
describe/define as NP-ACC 5 2.0
other patterns
(not in the grammar) 94 36.7
Table 3: Polish definition types
The middle layer consists of rules catching pat-
terns such as ?simple NP in given case, followed by
a sequence of non-punctuation elements? or ?cop-
ula?.
The bottom layer rules basically only refer to
POS markup in the input files (or other bottom layer
rules).
4 Results
As mentioned above, the testing corpus for each lan-
guage consists of about 150 definitions, unseen dur-
ing the construction of the grammar.6
6Obviously, three different corpora had to be used to eval-
uate the grammars for the three languages, but the corpora are
similar in size and character, so any differences in results stem
mostly from the differences in the three grammars.
The Bulgarian test corpus, containing around
76,800 tokens, consists of the third part of the
Calimera guidelines (http://www.calimera.
org/). We view this document as appropriate for
testing because it reflects the chosen domain and it
combines definitions from otherwise different sub-
domains, such as XML language, Internet usage,
etc. There are 203 manually annotated definitions
in this corpus: 129 definitions contained in one sen-
tence, 69 definitions split across 2 sentences, 4 def-
initions in 3 sentences and one definition in 4 sen-
tences. Note that the real test part is the set of the
129 definitions in one sentence, since the Bulgar-
ian grammar does not consider cross-sentence def-
initions in any way.
Czech data used for evaluation consist of several
chapters of the Calimera guidelines and Microsoft
Excel tutorial. The tutorial is a typical text used
in e-learning, consisting of five chapters describing
sheets, tables, formating, graphs and lists. The cor-
pus consists of over 90,000 tokens and contains 162
definitions, out of which 153 are contained in a sin-
gle sentence, 6 span 2 sentences, and 3 definitions
span 3 sentences.
Polish test corpus consists of over 83,200 tokens
containing 157 definitions: 148 definitions are con-
tained within one sentence, while 9 span 2 sen-
tences. The corpus is made up of 10 chapters of a
popular introduction to and history of computer sci-
ence and computer hardware.
Each grammar was quantitatively evaluated by
comparing manually annotated files with the same
files annotated automatically by the grammar. After
considering various ways of quantitative evaluation,
we decided to do the comparison at token level: pre-
cision was calculated as the ratio of the number of
those tokens which were parts of both a manually
marked definition and an automatically discovered
definition to the number of all tokens in automati-
cally discovered definitions, while recall was taken
to be the ratio of the number of tokens simultane-
ously in both kinds of definitions to the number of
tokens in all manually annotated definitions. Since,
for this task, recall is more important than precision,
we used the F2-measure for the combined result.7
7In general, F? = (1 + ?) ? (precision ? recall)/(? ?
precision+recall). Perhaps? larger than 2 could be used, but it
is currently not clear to us what criteria should be assumed when
46
The results for the three grammars are given in
Table 4. Note that the processing model for Czech
precision recall F2
Bulgarian 20.5% 2.2% 3.1
Czech 18.3% 40.7% 28.9
Polish 14.8% 22.2% 19.0
Table 4: Token-based evaluation of shallow gram-
mars
differs from the other two languages, as the input
text is converted to a flat format, as described in sec-
tion 5.3, and grammar rules are sensitive to sentence
boundaries (and may operate over them).
5 Evaluation and Possible Improvements
5.1 Interannotator Agreement
We calculated Cohen?s kappa statistic (1) for the cur-
rent task, where both the relative observed agree-
ment among raters Pr(a) and the probability that
agreement is due to chance Pr(e) where calculated
at token level.
? =
Pr(a) ? Pr(e)
1 ? Pr(e)
(1)
More specifically, we assumed that two annotators
agree on a token if the token belongs to a definition
either according to both annotations or according to
neither. In order to estimate the probability of agree-
ment due to chance Pr(e), we measured, separately
for each annotator, the proportion of tokens found in
definitions to all tokens in text, which resulted in two
probability estimates p1 and p2, and treated Pr(e) as
the probability that the two annotators agree if they
randomly, with their own probability, classify a to-
ken as belonging to a definition, i.e.:
Pr(e) = p1 ? p2 + (1 ? p1) ? (1 ? p2) (2)
The interannotator agreement (IAA) was mea-
sured this way for Czech and Polish, where ? for
each language ? the respective test corpus was an-
notated by two annotators. The results are 0.44 for
Czech and 0.31 for Polish. Such results are very low
for any classification task, and especially low for a
deciding on the exact value of ?. Note that it would not make
sense to use recall alone, as it is trivial to write all-accepting
grammars with 100% recall.
binary classification task. They show that the task of
identifying definitions in running texts and agreeing
on which parts of text count as a definition is intrin-
sically very difficult. They also call for the recon-
sideration of the evaluation and IAA measurement
methodology based on token classification.8
5.2 Evaluation Methodology
To the best of our knowledge, there is no estab-
lished evaluation methodology for the task of def-
inition extraction, where definitions may span sev-
eral sentences.9 For this reason we evaluated the re-
sults again, in a different way: we treated an auto-
matically discovered definition as correct, if it over-
lapped with a manually annotated definition. We
calculated precision as the number of automatic defi-
nitions overlapping with manual definitions, divided
by the number of automatic definitions, while re-
call ? as the number of manual definitions overlap-
ping automatic definitions, divided by the number of
manual definitions.10
The results for the three grammars, given in Ta-
ble 5, are much higher than those in Table 4 above,
although still less than satisfactory.
precision recall F2
Bulgarian 22.5% 8.9% 11.1
Czech 22.3% 46% 33.9
Polish 23.3% 32% 28.4
Table 5: Definition-based evaluation of shallow
grammars
5.3 Definitions and Sentence Boundaries
Regardless of the inherent difficulties of the task and
difficulties with the evaluation of the results, there
is clear room for improvement; one possible path
8A better approximation would be to measure IAA on the
basis of sentence or (as suggested by an anonymous reviewer)
NP classification; we intend to pursue this idea in future work.
9With the assumption that definitions are no longer than
a sentence, usually the task is treated as a classification task,
where sentences are classified as definitional or not, and ap-
propriate precision and recall measures are applied at sentence
level.
10At this stage definition fragments distributed across a num-
ber of different sentences were treated as different definitions,
which negatively affects the evaluation of the Bulgarian gram-
mar, as the Bulgarian test corpus contains a large number of
multi-sentence definitions.
47
to explore concerns multi-sentence definitions. As
noted above, for all languages considered here, there
were definitions which were spanning 2 or more sen-
tences; this turned out to be a problem especially for
Bulgarian, were 36% of definitions crossed a sen-
tence boundary.11
Such multi-sentence definitions are a problem be-
cause in the DTD adopted in this project definitions
are subelements of sentences rather than the other
way round. In case of a multi-sentence definition,
for each sentence there is a separate element en-
capsulating the part of the definition contained in
this sentence. Although these are linked via spe-
cial attributes and the information that they are part
of the same definition can subsequently be recov-
ered, it is difficult to construct an lxtransduce
grammar which would be able to automatically mark
such multi-sentence definitions: an lxtransduce
grammar expects to find a sequence of elements and
wrap them in a single larger element.
A solution to this technical problem has been im-
plemented in the Czech grammar, where first the in-
put text is flattened (via an XSLT script), so that,
e.g.:
<par id="d1p2">
<s id="d1p2s1">
<tok id="d1p2s1t1" base="Pavel"
ctag="N" msd="NMS1-----A----">
Pavel</tok>
<tok id="d1p2s1t2" base="satrapa"
ctag="N" msd="NMS1-----A----">
Satrapa</tok>
</s>
</par>
becomes:
<par id="Sd1p2"/>
<s id="Sd1p2s1"/>
<tok id="d1p2s1t1" base="Pavel"
ctag="N" msd="NMS1-----A----">
Pavel</tok>
<tok id="d1p2s1t2" base="satrapa"
ctag="N" msd="NMS1-----A----">
Satrapa</tok>
<s id="Ed1p2s1"/>
<par id="Ed1p2"/>
11An example of a Polish manually annotated multi-sentence
definition is: . . . opracowano techniki antyspamowe. Tech-
niki te drastycznie zaniz?aja? wartos?c? strony albo ja? banuja?. . .
(Eng. ?. . . anti-spam techniques were developed. Such tech-
niques drastically lower the value of the page or they ban it. . . ?).
The definition is split into two fragments fully contained in re-
spective sentences: techniki antyspamowe and Techniki te. . . .
No attempt at anaphora resolution is made.
This flattened representation is an input to a gram-
mar which is sensitive to the empty s and par ele-
ments and may discover definitions containing such
elements; in such a case, the postprocessing script,
which restores the hierarchical paragraph and sen-
tence structure, splits such definitions into smaller
elements, fully contained in respective sentences.
5.4 Problems Specific to Slavic
At least in case of the two West Slavic languages
considered here, the task of writing a definition
grammar is intrinsically more difficult than for Ger-
manic or Romance languages, mainly for the follow-
ing two reasons.
First, Czech and Polish have very rich nominal
inflection with a large number of paradigm-internal
syncretisms. These syncretisms are a common cause
of tagger errors, which percolate to further stages of
processing. Moreover, the number of cases makes it
more difficult to encode patterns like ?NP verb NP?,
as different verbs may combine with NPs of different
case. In fact, even two different copulas in Polish
take different cases!
Second, the relatively free word order increases
the number of rules that must be encoded, and makes
the grammar writing task more labour-intensive and
error-prone. The current version of the Polish gram-
mar, with 34 rules, is rather basic, and even the 147
rules of the Czech grammar do not take into consid-
eration all possible patterns of grammar definitions.
As Tables 4 and 5 show, there is a positive corre-
lation between the grammar size and the value of
F2, and the Bulgarian and Polish grammars certainly
have room to grow. Moreover, a path that is well
worth exploring is to drastically increase the num-
ber of rules and, hence, the recall, and then deal with
precision via Machine Learning methods (cf. sec-
tion 5.6).
5.5 Levels of Linguistic Processing
The work reported here has been an excercise in
definition extraction using shallow parsing methods.
However, the poor results suggest that this is one
of the tasks that require a much more sophisticated
and deeper approach to language analysis. In fact,
in turns out that virtually all successful attempts at
definition extraction that we are aware of build on
worked-out deep linguistic approaches (Klavans and
48
Muresan, 2000; Fahmi and Bouma, 2006; Walter
and Pinkal, 2006), some of them combining syn-
tactic and semantic information (Miliaraki and An-
droutsopoulos, 2004; Walter and Pinkal, 2006).
Unfortunately, for most Baltic and Slavic lan-
guages, such deep parsers are unavailable or have
not yet been extensively tested on real texts. One
exception is Czech, where a number of parsers were
already described and evaluated (on the Prague De-
pendency Treebank) in (Zeman, 2004, ? 14.2); the
best of these parsers reach 80?85% accuracy.
For Polish, apart from a number of linguistically
motivated toy parsers, there is a possibly wide cov-
erage deep parser (Wolin?ski, 2004), but it has not yet
been evaluated on naturally occurring texts. The sit-
uation is probably most dire for Bulgarian, although
there have been attempts at the induction of a depen-
dency parser from the BulTreeBank (Marinov and
Nivre, 2005; Chanev et al, 2006).
Nevertheless, if other possible paths of improve-
ment suggested in this section do not bring satisfac-
tory results, we plan to make an attempt at adapting
these parsers to the task at hand.
5.6 Postprocessing: Machine Learning and
Keyword Identification
Various approaches to the machine learning treat-
ment of the task of classifying sentences or snippets
as definitions or non-definitions can be found, e.g.,
in (Miliaraki and Androutsopoulos, 2004; Fahmi
and Bouma, 2006) and references therein. In the
context of the present work, such methods may be
used to postprocess apparent definitions found at
earlier processing stages and decide which of them
are genuine definitions. For example, (Fahmi and
Bouma, 2006) report that a system trained on 2299
sentences, including 1366 definition sentences, may
increase the accuracy of a definition extraction tool
from 59% to around 90%.12
Another possible improvement may consist in,
again, aiming at very high recall and then using
an independent keyword detector to mark keywords
(and key phrases) in text and classifying as genuine
definitions those definitions, whose defined term has
been marked as a keyword.
12The numbers are so high ?probably due to the fact that the
current corpus consists of encyclopedic material only? (Fahmi
and Bouma, 2006, fn. 4).
Whatever postprocessing technique or combina-
tion of techniques proves most efficient, it seems that
the linguistic processing should aim at high recall
rather than high precision, which further justifies the
use of the F2 measure for evaluation.13
6 Conclusion
To the best of our knowledge, this paper is the first
report on the task of definition extraction for a num-
ber of Slavic languages. It shows that the task is
intrinsically very difficult, which partially explains
the relatively low results obtained. It also calls atten-
tion to the fact that there is no established evaluation
methodology where possibly multi-sentence defini-
tions are involved and suggests what such method-
ology could amount to. Finally, the paper suggests
ways of improving the results, which we hope to fol-
low and report in the future.
References
Markus Becker et al 2002. SProUT ? shallow process-
ing with typed feature structures and unification. In
Proceedings of the International Conference on NLP
(ICON 2002), Mumbai, India.
Sharon A. Caraballo. 2001. Automatic Construction of a
Hypernym-Labeled Noun Hierarchy from Text. Ph. D.
dissertation, Brown University.
Atanas Chanev, Kiril Simov, Petya Osenova, and Sve-
toslav Marinov. 2006. Dependency conversion and
parsing of the BulTreeBank. In proceedings of the
LREC workshop Merging and Layering Linguistic In-
formation, Genoa, Italy.
Hamish Cunningham, Diana Maynard, Kalina
Bontcheva, and Valentin Tablan. 2002. GATE:
A framework and graphical development environment
for robust NLP tools and applications. In Proceedings
of the 40th Anniversary Meeting of the Association for
Computational Linguistics.
Witold Droz?dz?yn?ski, Petr Homola, Jakub Piskorski, and
Vytautas Zinkevic?ius. 2003. Adapting SProUT to
processing Baltic and Slavonic languages. In Infor-
mation Extraction for Slavonic and Other Central and
Eastern European Languages, pp. 18?25.
Ismail Fahmi and Gosse Bouma. 2006. Learning to iden-
tify definitions using syntactic features. In Proceed-
ings of the EACL 2006 workshop on Learning Struc-
tured Information in Natural Language Applications.
13Note that the situation here is different than in the task of
acquiring hyponymic relations from texts, where high-precision
manual rules (Hearst, 1992) must be augmented with statistical
clustering methods to increase recall (Caraballo, 2001).
49
Marti Hearst. 1992. Automatic acquisition of hy-
ponyms from large text corpora. In Proceedings of
the Fourteenth International Conference on Computa-
tional Linguistics, Nantes, France.
Judith L. Klavans and Smaranda Muresan. 2000.
DEFINDER: Rule-based methods for the extraction of
medical terminology and their associated definitions
from on-line text. In Proceedings of the Annual Fall
Symposium of the American Medical Informatics As-
sociation.
Judith L. Klavans and Smaranda Muresan. 2001. Eval-
uation of the DEFINDER system for fully automatic
glossary construction. In Proceedings of AMIA Sym-
posium 2001.
V?ronique Malais?, Pierre Zweigenbaum, and Bruno
Bachimont. 2004. Detecting semantic relations be-
tween terms in definitions. In S. Ananadiou and
P. Zweigenbaum, editors, COLING 2004 CompuTerm
2004: 3rd International Workshop on Computational
Terminology, pp. 55?62, Geneva, Switzerland. COL-
ING.
Ma?gorzata Marciniak, Agnieszka Mykowiecka, Anna
Kups?c?, and Jakub Piskorski. 2005. Intelligent con-
tent extraction from Polish medical texts. In L. Bolc
et al, editors, Intelligent Media Technology for Com-
municative Intelligence, Second International Work-
shop, IMTCI 2004, Warsaw, Poland, September 13-14,
2004, Revised Selected Papers, volume 3490 of Lec-
ture Notes in Computer Science, pp. 68?78. Springer-
Verlag.
Svetoslav Marinov and Joakim Nivre. 2005. A data-
driven parser for Bulgarian. In Proceedings of the
Fourth Workshop on Treebanks and Linguistic Theo-
ries, pp. 89?100, Barcelona.
Spyridoula Miliaraki and Ion Androutsopoulos. 2004.
Learning to identify single-snippet answers to defi-
nition questions. In Proceedings of COLING 2004,
pp. 1360?1366, Geneva, Switzerland. COLING.
Jennifer Pearson. 1996. The expression of defini-
tions in specialised texts: a corpus-based analysis.
In M. Gellerstam et al, editors, Proceedings of the
Seventh Euralex International Congress, pp. 817?824,
G?teborg.
Jakub Piskorski et al 2004. Information extraction for
Polish using the SProUT platform. In M. A. K?opotek
et al, editors, Intelligent Information Processing and
Web Mining, pp. 227?236. Springer-Verlag, Berlin.
Kiril Simov and Petya Osenova. 2005. BulQA:
Bulgarian-Bulgarian Question Answering at CLEF
2005. In CLEF, pp. 517?526.
Kiril Simov et al 2001. CLaRK ? an XML-based sys-
tem for corpora development. In P. Rayson et al, edi-
tors, Proceedings of the Corpus Linguistics 2001 Con-
ference, pp. 558?560, Lancaster.
Angelika Storrer and Sandra Wellinghoff. 2006. Auto-
mated detection and annotation of term definitions in
German text corpora. In Proceedings of LREC 2006.
Hristo Tanev. 2004. Socrates: A question answering
prototype for Bulgarian. In Recent Advances in Nat-
ural Language Processing III, Selected Papers from
RANLP 2003, pages 377?386. John Benjamins.
Richard Tobin, 2005. Lxtransduce, a replace-
ment for fsgmatch. University of Edinburgh.
http://www.cogsci.ed.ac.uk/~richard/
ltxml2/lxtransduce-manual.html.
Stephan Walter and Manfred Pinkal. 2006. Automatic
extraction of definitions from German court decisions.
In Proceedings of the Workshop on Information Ex-
traction Beyond The Document, pp. 20?28, Sydney,
Australia. Association for Computational Linguistics.
Marcin Wolin?ski. 2004. Komputerowa weryfikacja gra-
matyki S?widzin?skiego. Ph. D. dissertation, ICS PAS,
Warsaw.
Daniel Zeman. 2004. Parsing with a Statistical Depen-
dency Model. Ph. D. dissertation, Charles University,
Prague.
50
