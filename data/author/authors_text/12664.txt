Unsupervised Relation Extraction from Web Documents
Kathrin Eichler, Holmer Hemsen and Gu?nter Neumann
DFKI GmbH, LT-Lab, Stuhlsatzenhausweg 3 (Building D3 2), D-66123 Saarbru?cken
{FirstName.SecondName}@dfki.de
Abstract
The IDEX system is a prototype of an interactive dynamic Information Extraction (IE) system. A user of the system
expresses an information request in the form of a topic description, which is used for an initial search in order to retrieve
a relevant set of documents. On basis of this set of documents, unsupervised relation extraction and clustering is done by
the system. The results of these operations can then be interactively inspected by the user. In this paper we describe the
relation extraction and clustering components of the IDEX system. Preliminary evaluation results of these components are
presented and an overview is given of possible enhancements to improve the relation extraction and clustering components.
1. Introduction
Information extraction (IE) involves the process of au-
tomatically identifying instances of certain relations of
interest, e.g., produce(<company>, <product>, <lo-
cation>), in some document collection and the con-
struction of a database with information about each
individual instance (e.g., the participants of a meet-
ing, the date and time of the meeting). Currently, IE
systems are usually domain-dependent and adapting
the system to a new domain requires a high amount
of manual labour, such as specifying and implement-
ing relation?specific extraction patterns manually (cf.
Fig. 1) or annotating large amounts of training cor-
pora (cf. Fig. 2). These adaptations have to be made
offline, i.e., before the specific IE system is actually
made. Consequently, current IE technology is highly
statical and inflexible with respect to a timely adapta-
tion to new requirements in the form of new topics.
Figure 1: A hand-coded rule?based IE?system (schemat-
ically): A topic expert implements manually task?specific
extraction rules on the basis of her manual analysis of a
representative corpus.
1.1. Our goal
The goal of our IE research is the conception and im-
plementation of core IE technology to produce a new
Figure 2: A data?oriented IE system (schematically): The
task?specific extraction rules are automatically acquired by
means of Machine Learning algorithms, which are using
a sufficiently large enough corpus of topic?relevant docu-
ments. These documents have to be collected and costly
annotated by a topic?expert.
IE system automatically for a given topic. Here, the
pre?knowledge about the information request is given
by a user online to the IE core system (called IDEX)
in the form of a topic description (cf. Fig. 3). This
initial information source is used to retrieve relevant
documents and extract and cluster relations in an un-
supervised way. In this way, IDEX is able to adapt
much better to the dynamic information space, in par-
ticular because no predefined patterns of relevant re-
lations have to be specified, but relevant patterns are
determined online. Our system consists of a front-end,
which provides the user with a GUI for interactively in-
specting information extracted from topic-related web
documents, and a back-end, which contains the rela-
tion extraction and clustering component. In this pa-
per, we describe the back-end component and present
preliminary evaluation results.
1.2. Application potential
However, before doing so we would like to motivate
the application potential and impact of the IDEX ap-
Figure 3: The dynamic IE system IDEX (schematically):
a user of the IDEX IE system expresses her information
request in the form of a topic description which is used for
an initial search in order to retrieve a relevant set of doc-
uments. From this set of documents, the system extracts
and collects (using the IE core components of IDEX) a set
of tables of instances of possibly relevant relations. These
tables are presented to the user (who is assumed to be the
topic?expert), who will analyse the data further for her in-
formation research. The whole IE process is dynamic, since
no offline data is required, and the IE process is interactive,
since the topic expert is able to specify new topic descrip-
tions, which express her new attention triggered by a novel
relationship she was not aware of beforehand.
proach by an example application. Consider, e.g., the
case of the exploration and the exposure of corruptions
or the risk analysis of mega construction projects. Via
the Internet, a large pool of information resources of
such mega construction projects is available. These
information resources are rich in quantity, but also
in quality, e.g., business reports, company profiles,
blogs, reports by tourists, who visited these construc-
tion projects, but also web documents, which only
mention the project name and nothing else. One of
the challenges for the risk analysis of mega construc-
tion projects is the efficient exploration of the possibly
relevant search space. Developing manually an IE sys-
tem is often not possible because of the timely need
of the information, and, more importantly, is proba-
bly not useful, because the needed (hidden) informa-
tion is actually not known. In contrast, an unsuper-
vised and dynamic IE system like IDEX can be used
to support the expert in the exploration of the search
space through pro?active identification and clustering
of structured entities. Named entities like for example
person names and locations, are often useful indicators
of relevant text passages, in particular, if the names are
in some relationship. Furthermore, because the found
relationships are visualized using an advanced graph-
ical user interface, the user can select specific names
and find associated relationships to other names, the
documents they occur in or she can search for para-
phrases of sentences.
2. System architecture
The back-end component, visualized in Figure 4, con-
sists of three parts, which are described in detail in this
section: preprocessing, relation extraction and relation
clustering.
2.1. Preprocessing
In the first step, for a specific search task, a topic of
interest has to be defined in the form of a query. For
this topic, documents are automatically retrieved from
the web using the Google search engine. HTML and
PDF documents are converted into plain text files. As
the tools used for linguistic processing (NE recogni-
tion, parsing, etc.) are language-specific, we use the
Google language filter option when downloading the
documents. However, this does not prevent some doc-
uments written in a language other than our target
language (English) from entering our corpus. In ad-
dition, some web sites contain text written in several
languages. In order to restrict the processing to sen-
tences written in English, we apply a language guesser
tool, lc4j (Lc4j, 2007) and remove sentences not clas-
sified as written in English. This reduces errors on
the following levels of processing. We also remove sen-
tences that only contain non-alphanumeric characters.
To all remaining sentences, we apply LingPipe (Ling-
Pipe, 2007) for sentence boundary detection, named
entity recognition (NER) and coreference resolution.
As a result of this step database tables are created,
containing references to the original document, sen-
tences and detected named entities (NEs).
2.2. Relation extraction
Relation extraction is done on the basis of parsing po-
tentially relevant sentences. We define a sentence to be
of potential relevance if it at least contains two NEs.
In the first step, so-called skeletons (simplified depen-
dency trees) are extracted. To build the skeletons, the
Stanford parser (Stanford Parser, 2007) is used to gen-
erate dependency trees for the potentially relevant sen-
tences. For each NE pair in a sentence, the common
root element in the corresponding tree is identified and
the elements from each of the NEs to the root are col-
lected. An example of a skeleton is shown in Figure 5.
In the second step, information based on dependency
types is extracted for the potentially relevant sen-
tences. Focusing on verb relations (this can be ex-
tended to other types of relations), we collect for each
verb its subject(s), object(s), preposition(s) with ar-
guments and auxiliary verb(s). We can now extract
verb relations using a simple algorithm: We define a
verb relation to be a verb together with its arguments
(subject(s), object(s) and prepositional phrases) and
consider only those relations to be of interest where at
least the subject or the object is an NE. We filter out
relations with only one argument.
2.3. Relation clustering
Relation clusters are generated by grouping relation
instances based on their similarity.
web documents document
retrieval
topic specific documents plain text documents
sentence/documents+
 NE tables
languagefiltering
syntactic +typed dependencyparsing 
sov?relationsskeletons +
clustering
conversion
Preprocessing
Relation extraction
Relation clustering
sentencesrelevant
filtering of
relationfiltering
table of clustered relations
sentence boundary
resolutioncoreference
detection,NE recognition,
Figure 4: System architecture
Figure 5: Skeleton for the NE pair ?Hohenzollern? and ?Brandenburg? in the sentence ?Subsequent members of
the Hohenzollern family ruled until 1918 in Berlin, first as electors of Brandenburg.?
The comparably large amount of data in the corpus
requires the use of an efficient clustering algorithm.
Standard ML clustering algorithms such as k-means
and EM (as provided by the Weka toolbox (Witten
and Frank, 2005)) have been tested for clustering the
relations at hand but were not able to deal with the
large number of features and instances required for an
adequate representation of our dataset. We thus de-
cided to use a scoring algorithm that compares a re-
lation to other relations based on certain aspects and
calculates a similarity score. If this similarity score ex-
ceeds a predefined threshold, two relations are grouped
together.
Similarity is measured based on the output from the
different preprocessing steps as well as lexical informa-
tion from WordNet (WordNet, 2007):
? WordNet: WordNet information is used to deter-
mine if two verb infinitives match or if they are in
the same synonym set.
? Parsing: The extracted dependency information is
used to measure the token overlap of the two sub-
jects and objects, respectively. We also compare
the subject of the first relation with the object of
the second relation and vice versa. In addition,
we compare the auxiliary verbs, prepositions and
preposition arguments found in the relation.
? NE recognition: The information from this step
is used to count how many of the NEs occurring
in the contexts, i.e., the sentences in which the
two relations are found, match and whether the
NE types of the subjects and objects, respectively,
match.
? Coreference resolution: This type of information
is used to compare the NE subject (or object) of
one relation to strings that appear in the same
coreference set as the subject (or object) of the
second relation.
Manually analyzing a set of extracted relation in-
stances, we defined weights for the different similarity
measures and calculated a similarity score for each re-
lation pair. We then defined a score threshold and clus-
tered relations by putting two relations into the same
cluster if their similarity score exceeded this threshold
value.
3. Experiments and results
For our experiments, we built a test corpus of doc-
uments related to the topic ?Berlin Hauptbahnhof?
by sending queries describing the topic (e.g., ?Berlin
Hauptbahnhof?, ?Berlin central station?) to Google
and downloading the retrieved documents specifying
English as the target language. After preprocessing
these documents as described in 2.1., our corpus con-
sisted of 55,255 sentences from 1,068 web pages, from
which 10773 relations were automatically extracted
and clustered.
3.1. Clustering
From the extracted relations, the system built 306 clus-
ters of two or more instances, which were manually
evaluated by two authors of this paper. 81 of our clus-
ters contain two or more instances of exactly the same
relation, mostly due to the same sentence appearing in
several documents of the corpus. Of the remaining 225
clusters, 121 were marked as consistent, 35 as partly
consistent, 69 as not consistent. We defined consis-
tency based on the potential usefulness of a cluster to
the user and identified three major types of potentially
useful clusters:
? Relation paraphrases, e.g.,
accused (Mr Moore, Disney, In letter)
accused (Michael Moore, Walt Disney
Company)
? Different instances of the same pattern, e.g.,
operates (Delta, flights, from New York)
offers (Lufthansa, flights, from DC)
? Relations about the same topic (NE), e.g.,
rejected (Mr Blair, pressure, from Labour
MPs)
reiterated (Mr Blair, ideas, in speech, on
March)
created (Mr Blair, doctrine)
...
Of our 121 consistent clusters, 76 were classified as be-
ing of the type ?same pattern?, 27 as being of the type
?same topic? and 18 as being of the type ?relation para-
phrases?. As many of our clusters contain two instances
only, we are planning to analyze whether some clusters
should be merged and how this could be achieved.
3.2. Relation extraction
In order to evaluate the performance of the relation ex-
traction component, we manually annotated 550 sen-
tences of the test corpus by tagging all NEs and verbs
and manually extracting potentially interesting verb
relations. We define ?potentially interesting verb rela-
tion? as a verb together with its arguments (i.e., sub-
ject, objects and PP arguments), where at least two
of the arguments are NEs and at least one of them
is the subject or an object. On the basis of this crite-
rion, we found 15 potentially interesting verb relations.
For the same sentences, the IDEX system extracted 27
relations, 11 of them corresponding to the manually
extracted ones. This yields a recall value of 73% and
a precision value of 41%.
There were two types of recall errors: First, errors in
sentence boundary detection, mainly due to noisy in-
put data (e.g., missing periods), which lead to parsing
errors, and second, NER errors, i.e., NEs that were
not recognised as such. Precision errors could mostly
be traced back to the NER component (sequences of
words were wrongly identified as NEs).
In the 550 manually annotated sentences, 1300 NEs
were identified as NEs by the NER component. 402
NEs were recognised correctly by the NER, 588
wrongly and in 310 cases only parts of an NE were
recognised. These 310 cases can be divided into three
groups of errors. First, NEs recognised correctly, but
labeled with the wrong NE type. Second, only parts
of the NE were recognised correctly, e.g., ?Touris-
mus Marketing GmbH? instead of ?Berlin Tourismus
Marketing GmbH?. Third, NEs containing additional
words, such as ?the? in ?the Brandenburg Gate?.
To judge the usefulness of the extracted relations, we
applied the following soft criterion: A relation is con-
sidered useful if it expresses the main information given
by the sentence or clause, in which the relation was
found. According to this criterion, six of the eleven
relations could be considered useful. The remaining
five relations lacked some relevant part of the sen-
tence/clause (e.g., a crucial part of an NE, like the
?ICC? in ?ICC Berlin?).
4. Possible enhancements
With only 15 manually extracted relations out of 550
sentences, we assume that our definition of ?potentially
interesting relation? is too strict, and that more inter-
esting relations could be extracted by loosening the ex-
traction criterion. To investigate on how the criterion
could be loosened, we analysed all those sentences in
the test corpus that contained at least two NEs in order
to find out whether some interesting relations were lost
by the definition and how the definition would have to
be changed in order to detect these relations. The ta-
ble in Figure 6 lists some suggestions of how this could
be achieved, together with example relations and the
number of additional relations that could be extracted
from the 550 test sentences.
In addition, more interesting relations could be
found with an NER component extended by more
types, e.g., DATE and EVENT. Open domain NER
may be useful in order to extract NEs of additional
types. Also, other types of relations could be inter-
esting, such as relations between coordinated NEs,
option example additional relations
extraction of relations,
where the NE is not the
complete subject, object or
PP argument, but only part
of it
Co-operation with <ORG>M.A.X.
2001<\ORG> <V>is<\V> clearly of
benefit to <ORG>BTM<\ORG>.
25
extraction of relations with
a complex VP
<ORG>BTM<\ORG> <V>invited and or
supported<\V> more than 1,000 media rep-
resentatives in <LOC>Berlin<\LOC>.
7
resolution of relative pro-
nouns
The <ORG>Oxford Centre for Maritime
Archaeology<\ORG> [...] which will
<V>conduct<\V> a scientific symposium in
<LOC>Berlin<\LOC>.
2
combination of several of the
options mentioned above
<LOC>Berlin<\LOC> has <V>developed to
become<\V> the entertainment capital of
<LOC>Germany<\LOC>.
7
Figure 6: Table illustrating different options according to which the definition of ?potentially interesting relation?
could be loosened. For each option, an example sentence from the test corpus is given, together with the number
of relations that could be extracted additionally from the test corpus.
e.g., in a sentence like The exhibition [...] shows
<PER>Clemens Brentano<\PER>, <PER>Achim
von Arnim<\PER> and <PER>Heinrich von
Kleist<\PER>, and between NEs occurring in the
same (complex) argument, e.g., <PER>Hanns Peter
Nerger<\PER>, CEO of <ORG>Berlin Tourismus
Marketing GmbH (BTM) <\ORG>, sums it up [...].
5. Related work
Our work is related to previous work on domain-
independent unsupervised relation extraction, in par-
ticular Sekine (2006), Shinyama and Sekine (2006) and
Banko et al (2007).
Sekine (2006) introduces On-demand information ex-
traction, which aims at automatically identifying
salient patterns and extracting relations based on these
patterns. He retrieves relevant documents from a
newspaper corpus based on a query and applies a POS
tagger, a dependency analyzer and an extended NE
tagger. Using the information from the taggers, he ex-
tracts patterns and applies paraphrase recognition to
create sets of semantically similar patterns. Shinyama
and Sekine (2006) apply NER, coreference resolution
and parsing to a corpus of newspaper articles to ex-
tract two-place relations between NEs. The extracted
relations are grouped into pattern tables of NE pairs
expressing the same relation, e.g., hurricanes and their
locations. Clustering is performed in two steps: they
first cluster all documents and use this information to
cluster the relations. However, only relations among
the five most highly-weighted entities in a cluster are
extracted and only the first ten sentences of each arti-
cle are taken into account.
Banko et al (2007) use a much larger corpus, namely
9 million web pages, to extract all relations between
noun phrases. Due to the large amount of data, they
apply POS tagging only. Their output consists of mil-
lions of relations, most of them being abstract asser-
tions such as (executive, hired by, company) rather
than concrete facts.
Our approach can be regarded as a combination of
these approaches: Like Banko et al (2007), we extract
relations from noisy web documents rather than com-
parably homogeneous news articles. However, rather
than extracting relations from millions of pages we re-
duce the size of our corpus beforehand using a query in
order to be able to apply more linguistic preprocessing.
Like Sekine (2006) and Shinyama and Sekine (2006),
we concentrate on relations involving NEs, the assump-
tion being that these relations are the potentially in-
teresting ones. The relation clustering step allows us
to group similar relations, which can, for example, be
useful for the generation of answers in a Question An-
swering system.
6. Future work
Since many errors were due to the noisiness of the ar-
bitrarily downloaded web documents, a more sophisti-
cated filtering step for extracting relevant textual infor-
mation from web sites before applying NE recognition,
parsing, etc. is likely to improve the performance of
the system.
The NER component plays a crucial role for the qual-
ity of the whole system, because the relation extraction
component depends heavily on the NER quality, and
thereby the NER quality influences also the results of
the clustering process. A possible solution to improve
NER in the IDEX System is to integrate a MetaNER
component, combining the results of several NER com-
ponents. Within the framework of the IDEX project
a MetaNER component already has been developed
(Heyl, to appear 2008), but not yet integrated into the
prototype. The MetaNER component developed uses
the results from three different NER systems. The out-
put of each NER component is weighted depending on
the component and if the sum of these values for a pos-
sible NE exceeds a certain threshold it is accepted as
NE otherwise it is rejected.
The clustering step returns many clusters containing
two instances only. A task for future work is to in-
vestigate, whether it is possible to build larger clus-
ters, which are still meaningful. One way of enlarging
cluster size is to extract more relations. This could
be achieved by loosening the extraction criteria as de-
scribed in section 4. Also, it would be interesting to see
whether clusters could be merged. This would require
a manual analysis of the created clusters.
Acknowledgement
The work presented here was partially supported by a
research grant from the?Programm zur Fo?rderung von
Forschung, Innovationen und Technologien (ProFIT)?
(FKZ: 10135984) and the European Regional Develop-
ment Fund (ERDF).
7. References
Michele Banko, Michael J. Cafarella, Stephen Soder-
land, Matthew Broadhead, and Oren Etzioni. 2007.
Open information extraction from the web. In Proc.
of the International Joint Conference on Artificial
Intelligence (IJCAI).
Andrea Heyl. to appear 2008. Unsupervised relation
extraction. Master?s thesis, Saarland University.
Lc4j. 2007. Language categorization library for Java.
http://www.olivo.net/software/lc4j/.
LingPipe. 2007. http://www.alias-i.com/lingpipe/.
Satoshi Sekine. 2006. On-demand information extrac-
tion. In ACL. The Association for Computer Lin-
guistics.
Yusuke Shinyama and Satoshi Sekine. 2006. Preemp-
tive information extraction using unrestricted re-
lation discovery. In Proc. of the main conference
on Human Language Technology Conference of the
North American Chapter of the Association of Com-
putational Linguistics, pages 304?311. Association
for Computational Linguistics.
Stanford Parser. 2007. http://nlp.stanford.edu/
downloads/lex-parser.shtml.
Ian H. Witten and Eibe Frank. 2005. Data Min-
ing: Practical machine learning tools and techniques.
Morgan Kaufmann, San Francisco, 2nd edition.
WordNet. 2007. http://wordnet.princeton.edu/.
Living up to standards  
 
 
Margaret King 
TIM/ISSCO 
ETI 
University of Geneva 
Margaret.King@issco.unige.ch 
  
 
Abstract 
This paper attacks one part of the 
question "Are evaluation methods, 
metrics and resources reusable" by 
arguing that a set of ISO standards 
developed for the evaluation of software 
in general are as applicable to natural 
language processing software as to any 
other. Main features of the ISO proposals 
are presented, and a number of 
applications where they have been 
applied are mentioned, although not 
discussed in any detail. 
Acknowledgements 
The work recorded here is far from being all my 
own. I would like first to record my thanks to 
Nigel Bevan, technical editor of the ISO 
standards discussed for much interesting and 
enlightening discussion. Then many thanks must 
go to all my colleagues in the EAGLES and ISLE 
projects, especially Sandra Manzi and Andrei 
Popescu-Belis. Finally, I must thank all those 
whose work on applying the standards reported 
here provoked reflection and helped to convince  
me of the value of the approach: Marc Blasband, 
Maria Canelli, Dominique Estival, Daniele 
Grasso, V?ronique Sauron, Marianne Starlander 
and Nancy Underwood. 
1 Introduction 
 
 
 
 
 
This paper is constructed around a syllogism:  
1. ISO standards 9126 and 14598 are 
applicable to the evaluation of any type 
of software 
2. Natural language processing software is 
a type of software 
3. ISO standards 9126 and 14598 are 
applicable to the evaluation of natural 
language processing software. 
 
In support of the major premise, I shall set out 
some of the major features of the ISO standards 
in question. The minor premise needs no support: 
indeed, it is almost a tautology. The truth of the 
conclusion will logically depend therefore on 
whether I have managed to convince the reader 
of the truth of the major premise. There will be 
little explicit argument in this direction: simply 
setting out key features of the approach should 
suffice. I will try, however, to reinforce the 
conclusion by briefly reviewing a number of 
natural language processing applications where 
the ISO standards have been followed with 
encouraging results. My hope, of course, is to 
encourage readers to apply the standards 
themselves. 
2 ISO standards work on software 
evaluation 
ISO has been publishing standards on software 
evaluation since 1991. The bibliography gives a 
detailed picture of what standards have already 
been published and of what standards are in 
preparation. ISO/IEC 9126 was the first standard 
to appear. It has subsequently been modified, and 
in its new versions the original content of 1991 
has been refined, modified and distributed over a 
series of separate but inter-related standards.  
The keystone of ISO work is that the basis of 
an evaluation is an explicit and detailed statement 
of what is required of the object to be evaluated. 
This statement is formulated very early in the 
process of defining an evaluation and is called a 
?quality model?. The process of evaluation 
involves defining how measurements can be 
applied to the object to be evaluated in order to 
discover how closely it meets the requirements 
set out in the quality model. 
?The object to be evaluated? is a clumsy 
phrase. It has been used because, in the ISO 
picture, evaluation may take place at any point in 
the lifecycle of a software product, and may have 
as its object not only the final product but 
intermediate products, including specifications 
and code which has not yet been executed. It 
follows from this that a quality model may apply 
to a set of specifications just as much as to a 
piece of finished software. Indeed, one might 
envisage using quality models as a way of 
guiding the whole process of producing a 
software product, from initial research and 
prototyping through to delivering and field 
testing the final product. That this is in line with 
best practice in software engineering constitutes, 
to my mind, an argument in favour of the ISO 
proposals. 
As well as a set of standards relating to the 
definition of quality models (the 9126 series) ISO 
also offers a set of standards relating to the 
process of evaluation (the 14598 series). One 
document sets out a standard for the evaluation 
process seen at its most generic level, further 
proposals relate definition of the process to the 
particular viewpoints of software developers, of 
acquirers of software and of evaluators typically 
working as third party evaluators. Other 
documents in the 14598 series provide supporting 
material for those involved in evaluation, 
offering standards for planning and management 
of evaluations and for documentation of 
evaluation modules. Of the 9126 series, only the 
first document which directly deals with quality 
models has as yet been published. Documents in 
preparation deal with standards for the metrics 
which form a critical accompaniment to any 
quality model. It would be unrealistic in the 
space of a single paper to discuss even the 
documents already published in any detail. In 
what follows, we concentrate on outlining the 
foundations of the ISO proposals, the quality 
model and the process of evaluation.  
3 Quality models (ISO 9126) 
A quality model consists of a set of quality 
characteristics, each of which is decomposed into 
a set of quality sub-characteristics. Metrics 
measure how an object to be evaluated performs 
with respect to the quality characteristics and 
sub-characteristics. The quality characteristics 
and sub-characteristics making up the quality 
model of ISO 9126-1/01 are shown in figure 1, 
on the next page. All that figure 1 shows are 
names: ISO 9126-1/01 gives both definitions and 
discussion. 
The quality characteristics are intended to be 
applicable to any piece of software product or 
intermediate product. They are thus necessarily 
defined at a rather high level of generality, and 
need to be made more specific before they are 
applicable to any particular piece of software. 
They are also defined through natural language 
definitions, and are thus not formal in the 
mathematical or logical sense. This being so, 
they are open to interpretation. Defining a 
specific evaluation implies deciding on an 
appropriate interpretation for that evaluation. 
ISO 9126/01, whilst not barring the 
possibility that a quality model other than that 
contained in the standard might be used, requires 
that if another model is used, it should be clearly 
described. 
?Software quality shall be evaluated using a 
defined quality model. A quality model shall be 
used when setting quality goals for software 
products and intermediate products. This part of 
ISO/IEC 9126 provides a recommended quality 
model which can be used as a checklist of issues 
relating to quality (although other ways of 
categorising quality may be more appropriate in 
particular circumstances). When a quality model 
other than that in this part of ISO/IEC 9126 is 
used it shall be clearly described.? (ISO 9126/01, 
1.5, Quality relationships). 
Work within the EAGLES project on 
defining a general framework for evaluation 
design extended this model by allowing the 
quality sub-characteristics in their turn to be  
 
 
decomposed; the process of decomposition being 
repeated if necessary. 
 
 
 suitability 
accuracy 
interoperability 
security 
 
functionality  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1 
 
 
The structure thus obtained is hierarchical, and, 
theoretically of unlimited depth. ISO 9126-1/01 
does not rigidly specify the relationship between 
quality characteristics and metrics. The EAGLES 
extension requires that each terminal node of the 
structure has at least one metric associated with 
it. The structure then becomes a hierarchy of 
attribute value pairs, where each node is labelled 
with the name of an attribute. The values of the 
attributes at the terminal nodes are directly 
obtained by the application of metrics. The value 
of a higher level node is obtained by combining 
the values of attributes nodes immediately 
dominated by the higher level node: values 
percolate upwards. Exactly how the combination 
of values is done is determined by a combining 
function which reflects the relative importance of 
the attributes in a particular evaluation. This 
formalization provides an operational semantics 
for any particular instantiation of the quality 
model. Once the evaluation designer has decided 
what attributes to include in his quality model 
software 
product 
quality 
reliability 
usability 
maturity 
fault tolerance 
recoverability 
understandability 
learnability 
operability 
attractiveness 
time behaviour 
resource utilisation 
analysability 
changeability 
stability 
testability 
portability 
efficiency 
maintainability 
adaptability 
installability 
co-existence 
replaceability 
and how to organise them, and once he has 
defined and assigned metrics to the terminal 
nodes, what functionality, for example, means 
within that quality model is defined by the 
decomposition of the functionality node and by 
the associated metrics. 
Metrics will be discussed only briefly here. 
The ISO standard distinguishes between 
internal metrics, external metrics and quality in 
use metrics. The difference between them is 
determined by what kind of an evaluation 
object they are applied to. 
Internal metrics apply to static properties of 
software, that is software considered 
independently of its execution. Examples 
might be the number of lines of code or the 
programming language used. As can be seen 
from the inclusion of the programming 
language in this list, metrics are not necessarily 
quantitative in their nature, although they 
should, of course, be as objective as possible. 
(This is one of the points we shall not go into 
further here.) 
External metrics apply to software when it is 
being executed, to the behaviour of the system 
as seen from outside. Thus they may measure 
the accuracy of the results, the response time 
of the software, the learnability of the user 
interface and a host of other attributes that go 
to make up the quality of the software as a 
piece of software.  
Quality in use metrics apply when the 
software is being used to accomplish a 
particular task in a particular environment. 
They are more concerned with the effects of 
using the software than with the software 
itself. Quality in use metrics are therefore very 
dependent on a particular environment and a 
particular task. Quality in use is itself a super-
ordinate aspect of quality, for these same 
reasons. It is clearly influenced by the quality 
characteristics which make up the quality 
model, but is determined by the interaction of 
different quality characteristics in a particular 
task environment. 
The ISO standards published so far say little 
about what makes a metric a good metric. 
Some work elsewhere (Popescu-Belis, 1999, 
Hovy et al 2003) has made some suggestions. 
First, metrics should be coherent, in the 
sense that they should respect the following 
criteria: 
? A metric should reach its highest value 
for perfect quality (with respect to the 
attribute being measured), and, reciprocally, 
only reach its highest level when quality is 
perfect. 
? A metric should reach its lowest level only 
for the worst possible quality (again, with 
respect to the attribute being tested) 
? A metric should be monotonic: that is, if the 
quality of software A is higher than that of 
software B, then the score of A should be 
higher than the score of B. 
We might compare two metrics (or more strictly 
two rating functions: see the section on process 
below) by saying that a metric m1 is more severe 
than a metric m2 if it yields lower scores than m2 for 
every possible quality level. Conversely, one metric 
may be more lenient than another. 
To these rather formal considerations, we might 
add: 
? A metric must be clear and intuitive 
? It must correlate well with human 
judgements under all conditions 
? It must measure what it is supposed to 
measure 
? It must be reliable, exhibiting as little 
variance as possible across evaluators or for 
equivalent inputs 
? It must be cheap to prepare and to apply 
? It should be automated if possible 
4 Evaluation process (ISO 14598) 
A first section of ISO 14598-1/99 is concerned with 
an overview of how all the different 9126 and 
14596 documents concerned with software 
evaluation fit together. This overview can be 
summarized quite briefly. It is fundamental to the 
preparation of any evaluation that a quality model 
reflecting the user?s requirements of the object to be 
evaluated be constructed. The 9126 series of 
documents is intended to support construction of 
the quality model.  
The 14598 series is concerned with the 
process of evaluation, seen from different 
viewpoints. Separate documents in the series 
tackle evaluation from the point of view of 
developers, acquirers and (third party) 
evaluators. All of these make use of the 9126 
series, and are further supported by the second 
half of 14598-1, which sets out a generic 
picture of the process of evaluation, and by 
two further documents, the first concerned 
with planning and management of a software 
evaluation process, the second with guidance 
for documenting evaluation modules. 
Although these other documents in the 
series are clearly important, we limit ourselves 
here to summarizing the process of evaluation, 
as set out in ISO 14598-1. 
The evaluation process is conceived as 
being generic: it applies to component 
evaluation as well as to system evaluation, and 
may be applied at any appropriate phase of the 
product life cycle. 
The evaluation process is broken down into 
four main stages, each of which is considered 
separately below: 
 
Stage I: Establish evaluation requirements. 
 
This step is broken down into a further three 
steps: 
 
a) Establish the purpose of the 
evaluation 
 
The commentary on this point reveals just how 
wide the scope of the standard is intended to 
be. The purpose of evaluating the quality of an 
intermediate product may be to: 
? Decide on the acceptance of an 
intermediate product from a sub-
contractor  
? Decide on the completion of a process 
and when to send products to the next 
process 
? Predict or estimate end product quality 
? Collect information on intermediate 
products in order to control and manage 
the process 
(The reader will remember that intermediate 
product means, for example, specifications or code 
before it is executed). 
The purpose of evaluating an end product may be 
to: 
? Decide on the acceptance of the product 
? Decide when to release the product 
? Compare the product with competitive 
products  
? Select a product from among alternative 
products 
? Assess both positive and negative effects of a 
product when it is used 
? Decide when to enhance or replace the 
product. 
It follows from this very broad range of possibilities 
that the standard is meant to apply not only to any 
kind of intermediate or final software product, but 
to any evaluation scenario, including comparative 
evaluation. 
 
b) Identify types of products to be evaluated 
 
Types of products here does not mean application 
software, but rather is concerned with the stage 
reached in the product?s life cycle, which 
determines whether and what intermediate product 
or final product is to be evaluated.  
 
c) Specify quality model 
 
The quality model is, of course, to be defined using 
ISO 9126-1/01 as a guide. However, a note quoted 
again below adds: 
 
?The actual characteristics and sub-characteristics 
which are relevant in any particular situation will 
depend on the purpose of the evaluation and should 
be identified by a quality requirements study. The 
ISO/IEC 9126-1 characteristics and sub-
characteristics provide a useful checklist of issues 
related to quality, but other ways of categorising 
quality may be more appropriate in particular 
circumstances.? (ISO 14598-1/99) 
 
An important word here is ?checklist?: the basic 
purpose of the ISO quality model is to serve as a 
guide and as a reminder for what should be 
included in evaluating software. Arguing about 
the exact interpretation of the quality 
characteristics is pointless. Their interpretation 
is given by the model in which they are 
incorporated. 
 
Stage II:Specify the evaluation 
 
This too breaks down into three steps: 
a) Select metrics 
b) Establish rating levels for metrics 
c)   Establish criteria for assessment 
Quality characteristics and sub-characteristics 
cannot be directly measured. Metrics must 
therefore be defined which correlate to the 
quality characteristic. Different metrics may be 
used in different environments and at different 
stages of a product?s development. Metrics 
have already been discussed to some extent in 
the section on quality models above. 
A metric typically involves producing a 
score on some scale, reflecting the particular 
system?s performance with respect to the 
quality characteristic in question. This score, 
uninterpreted, says nothing about whether the 
system performs satisfactorily. To illustrate 
this idea, consider the Geneva education 
system, where marks in examinations range 
from 1 to 6. How do you know, without being 
told, that 6 is the best mark and 1 the worst? In 
fact, most people guess that it is so: they may 
then have a difficult time in Zurich where 1 is 
the highest mark. Establishing rating levels for 
metrics involves determining the 
correspondence between the uninterpreted 
score and the degree of satisfaction of the 
requirements. Since quality refers to given 
needs, there can be no general rules for when a 
score is satisfactory. This must be determined 
for each specific evaluation. 
Each measure contributes to the overall 
judgement of the product, but not necessarily 
in a uniform way. It may be, for example, that 
one requirement is critical, whilst another is 
desirable, but not strictly necessary. In this 
case, if a system performs badly with respect 
to the critical characteristic, it will be assessed 
negatively no matter what happens to all the 
other characteristics. If it performs badly with 
respect to the desirable but not necessary 
characteristic, it is its performance with respect to 
all the other characteristics which will determine 
whether the system is acceptable or not. 
This consideration feeds directly into the third 
step, establishing criteria for assessment, which 
involves defining a procedure for summarizing the 
results of the evaluation of the different 
characteristics, using for example decision tables or 
weighting functions of different kinds.  
 
Stage III: Design the evaluation 
 
Designing the evaluation involves producing an 
evaluation plan, which describes the evaluation 
methods and the schedule of the evaluator action. 
The other documents in the 14598 series expand on 
this point, and the plan should be consistent with a 
measurement plan, as described and discussed in 
the document on planning and management. (ISO 
14598-2/00) 
 
Stage IV: Execute the evaluation 
 
This final stage again breaks down into three stages: 
 
a) Measurement 
b) Rating 
       c)   Assessment 
These steps are intuitively straightforward in the 
light of the discussion above. Measurement gives a 
score on a scale appropriate to the metric being 
used. Rating determines the correlation between the 
raw score and the rating levels, in other words, tells 
us whether the score can be considered to be 
satisfactory. Assessment is a summary of the set of 
rated levels and can be seen as a way of putting 
together the individual ratings to give an overall 
picture which also reflects the relative importance 
of different characteristics in the light of the 
particular quality requirements. Final decisions are 
taken on the basis of the assessment.  
5 ISO, EAGLES and natural language   
applications in practice. 
It would be impossible of course to claim 
knowledge of all applications of the ISO standards, 
even within the limited area of work on natural 
language. In this concluding section only those 
applications that came to the author?s 
cognisance through her involvement with work 
in the EAGLES, ISLE and Parmenides projects 
are mentioned.  
The ISO model of 9126/91 as extended and 
formalized by the first EAGLES project has 
been tested by application to a number of 
different language engineering applications. 
Within the TEMAA project it was applied to 
the evaluation of spelling checkers, and initial 
work was done on quality models for grammar 
checkers and translation memory systems. As 
part of the EAGLES project itself, a number of 
projects in the general field of information 
retrieval were asked to apply the framework, 
and produced, in those cases where the project 
included a substantial evaluation component, 
encouraging results. The second EAGLES 
project was, for the evaluation group, 
essentially a consolidation and dissemination 
project, where an attempt was made to 
encourage use of earlier results. During this 
time, the model was also applied in the context 
of the ARISE project, which developed a 
prototype system whereby information on 
railway timetables could be obtained through 
spoken dialogue. Similarly, an Australian 
manufacturer of speech software used the 
framework to evaluate a spoken language 
dialogue system. Case studies undertaken in 
the context of post-graduate work have applied 
the ISO/EAGLES methodology to the 
evaluation of dictation systems, grammar 
checkers and terminology extraction tools. One 
part of the ISLE project, now coming to an 
end, has been applying the methodology to the 
construction of a large scale quality model of 
machine translation systems. Many of the 
results of this work can be consulted by 
looking at the EAGLES and ISLE web sites.  
Recently, work has begun on the 
Parmenides project. This project is concerned 
with ontology based semantic mining of 
information from web based documents, with a 
special interest in keeping track of information 
which changes over time. Evaluation plays an 
important role in the project. Three separate 
user groups are supplying the basis for case 
studies. At the time of writing, user 
requirements are being defined, which will be 
translated into quality requirements for the software 
to be developed within the project and which will 
serve as the basis for the quality models to be used 
in on-going and final evaluation. 
6 Conclusion. 
The workshop for which this paper has been written 
addresses the question of whether there is anything 
that can be shared between evaluations. The answer 
which I hope to have made convincing is that one 
thing which can be shared is a way of thinking 
about how evaluations should be designed and 
carried out. Adhering to an acknowledged standard 
in the construction of quality models and in 
developing the process of a specific evaluation can 
only make it easier to share more detailed aspects of 
evaluation and provides a common framework for 
discussion of such issues as metrics and their 
validity. 
References. 
Blasband, M. 1999. Practice of Validation: the ARISE 
Application of the EAGLES Framework. EELS 
(European Evaluation of Language Systems) 
Conference, Hoevelaken, The Netherlands. 
EAGLES Evaluation Working Group. 1996. EAGLES 
Evaluation of Natural Lnaguage Processing Systems. 
Final Report, Center for Sprogteknologi, Copenhagen, 
Denmark. 
Hovy, E, King, M and Popescu-Belis, A. 2002. 
Computer-aided Specification of Quality Models for 
MT Evaluation. Third International Conference on 
Language Resources and Evaluation (LREC). 
Hovy, E, King, M and Popescu-Belis, A. 2003. 
Principles of Context Based Machine Translation 
Evaluation. ISLE report. 
ISO/IEC 9126-1:2001 Software engineering ? product 
quality ? Part 1: Quality Model. Geneva, International 
Organization for Standardization and International 
Electrotechnical Commission. 
ISO/IEC DTR 9126-2 (in preparation): Software 
engineering ? product quality ? Part 2: External 
metrics. . Geneva, International Organization for 
Standardization and International Electrotechnical 
Commission 
ISO/IEC CD TR 9126-3 (in preparation): Software 
engineering ? product quality ? Part 3: Internal 
metrics. . Geneva, International Organization for 
Standardization and International 
Electrotechnical Commission 
ISO/IEC CD 9126-4 (in preparation): Software 
engineering ? product quality ? Part 4: Quality 
in use metrics. . Geneva, International 
Organization for Standardization and 
International Electrotechnical Commission 
ISO/IEC CD 9126-30 (in preparation): Software 
engineering ? Software product quality 
requirements and evaluation ? Part 30: Quality 
metrics ? Metrics reference model and guide. . 
Geneva, International Organization for 
Standardization and International 
Electrotechnical Commission 
ISO/IEC 14598-1:1999 Information technology ? 
Software product evaluation ? Part 1: General 
Overview. Geneva, International Organization 
for Standardization and International 
Electrotechnical Commission 
ISO/IEC 14598-2:2000? Software engineering - 
product evaluation ? Part 2: Planning and 
Management. Geneva, International 
Organization for Standardization and 
International Electrotechnical Commission 
ISO/IEC 14598-3:2000? Software engineering - 
product evaluation ? Part 3: Process for 
developers. . Geneva, International Organization 
for Standardization and International 
Electrotechnical Commission 
ISO/IEC 14598-5:1998 Information technology ? 
Software product evaluation ? Part 5: Process 
for evaluators Geneva, International 
Organization for Standardization and 
International Electrotechnical Commission 
ISO/IEC 14598-4:1999? Software engineering - 
product evaluation ? Part 4: Process for 
acquirers     Geneva, International Organization 
for Standardization and International 
Electrotechnical Commission 
ISO/IEC 14598-6:2001? Software engineering - 
product evaluation ? Part 6: Documentation of 
evaluation modules Geneva, International 
Organization for Standardization and 
International Electrotechnical Commission 
King, M. 1996. Evaluating Natural Language 
Processing Systems. Communications of the 
Association for Computing Machinery (CACM), 
Vol. 39, Number 1. 
Popescu-Belis, A. 1999. Evaluation of natural 
anguage processing systems: a model for 
coherence verification of quality measures. M. 
Blasband and P. Paroubek, eds, A Blueprint for a 
General Infrastructure for Natural Language 
Processing Systems Evaluation Using Semi-Automatic 
Quantitative Approach Black Box Approach in a 
Multilingual Environment. ELSE project. (Evaluation 
in Speech and Language Engineering). 
Sparck-Jones, K. and Galliers J.R. 1996. Evaluating 
Natural Language Processing Systems:An Analysis 
and Review. Lecture Notes in Artificial Intelligence 
1083. Springer-Verlag. 
 
