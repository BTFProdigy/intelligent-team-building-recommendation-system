Proceedings of the SIGDIAL 2013 Conference, pages 344?348,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
A Semi-supervised Approach for Natural Language Call Routing 
 
 
Tatiana Gasanova 
Institute of Communications Engineer-
ing, Ulm University, Germany 
tatiana.gasanova@uni-
ulm.de 
Eugene Zhukov 
Institute of Computer Science and 
Telecommunications, Siberian State 
Aerospace University, Russia 
zhukov.krsk@gmail.com 
Roman Sergienko 
Institute of Computer Science and 
Telecommunications, Siberian State 
Aerospace University, Russia 
romaserg@list.ru 
Eugene Semenkin 
Institute of Computer Science and 
Telecommunications, Siberian State 
Aerospace University, Russia 
eugenesemenkin@yandex.com 
Wolfgang Minker 
Institute of Communications Engineer-
ing, Ulm University, Germany 
wolfgang.minker@uni-ulm.de 
 
 
  
 
Abstract 
Natural Language call routing remains a com-
plex and challenging research area in machine 
intelligence and language understanding. This 
paper is in the area of classifying user utter-
ances into different categories. The focus is on 
design of algorithm that combines supervised 
and unsupervised learning models in order to 
improve classification quality. We have shown 
that the proposed approach is able to outper-
form existing methods on a large dataset and 
do not require morphological and stop-word 
filtering. In this paper we present a new for-
mula for term relevance estimation, which is a 
modification of fuzzy rules relevance estima-
tion for fuzzy classifier. Using this formula 
and only 300 frequent words for each class, we 
achieve an accuracy rate of 85.55% on the da-
tabase excluding the ?garbage? class (it in-
cludes utterances that cannot be assigned to 
any useful class or that can be assigned to 
more than one class). Dividing the ?garbage? 
class into the set of subclasses by agglomera-
tive hierarchical clustering we achieve about 
9% improvement of accuracy rate on the 
whole database. 
1 Introduction 
Natural language call routing can be treated as an 
instance of topic categorization of documents 
(where the collection of labeled documents is 
used for training and the problem is to classify 
the remaining set of unlabeled test documents) 
but it also has some differences. For instance, in 
document classification there are much more 
terms in one object than in single utterance from 
call routing task, where even one-word utteranc-
es are common. 
A number of works have recently been published 
on natural language call classification. B. Car-
penter, J. Chu-Carroll, C.-H. Lee and H.-K. Kuo 
proposed approaches using a vector-based in-
formation retrieval technique, the algorithms de-
signed by A. L. Gorin, G. Riccardi, and J. H. 
Wright use a probabilistic model with salient 
phrases. R. E. Schapire and Y. Singer focused on 
a boosting-based system for text categorization.  
The most similar work has been done by A. 
Albalate, D. Suendermann, R. Pieraccini, A. 
Suchindranath, S. Rhinow, J. Liscombe, K. 
Dayanidhi, and W. Minker. They have worked 
on the data with the same structure: the focus 
was on the problem of big part of non-labeled 
data and only few labeled utterances for each 
class, methods of matching the obtained clusters 
and the given classes have also been considered; 
they provided the comparison of several classifi-
cation methods that are able to perform on the 
large scale data.  
The information retrieval approach for call rout-
ing is based on the training of the routing matrix, 
which is formed by statistics of appearances of 
344
words and phrases in a training set (usually after 
morphological and stop-word filtering). The new 
caller request is represented as a feature vector 
and is routed to the most similar destination vec-
tor. The most commonly used similarity criterion 
is the cosine similarity. The performance of sys-
tems, based on this approach, often depends on 
the quality of the destination vectors.  
In this paper we propose a new term relevance 
estimation approach based on fuzzy rules rele-
vance for fuzzy classifier (H. Ishibuchi, T. 
Nakashima, and T. Murata., 1999) to improve 
routing accuracy. We have also used a decision 
rule different from the cosine similarity. We as-
sign relevancies to every destination (class), cal-
culate the sums of relevancies of words from the 
current utterance and choose the destination with 
the highest sum.  
The database for training and performance eval-
uation consists of about 300.000 user utterances 
recorded from caller interactions with commer-
cial automated agents. The utterances were man-
ually transcribed and classified into 20 classes 
(call reasons), such as appointments, operator, 
bill, internet, phone or video. Calls that cannot be 
routed certainly to one reason of the list are clas-
sified to class _TE_NOMATCH.  
A significant part of the database (about 27%) 
consists of utterances from the ?garbage? class 
(_TE_NOMATCH). Our proposed approach de-
composes the routing task into two steps. On the 
first step we divide the ?garbage? class into the 
set of subclasses by one of the clustering algo-
rithms and on the second step we define the call 
reason considering the ?garbage? subclasses as 
separate classes. We apply genetic algorithms 
with the whole numbers alphabet, vector quanti-
zation network and hierarchical agglomerative 
clustering in order to divide ?garbage? class into 
subclasses. The reason to perform such a cluster-
ing is due to simplify the detection of the class 
with non-uniform structure.  
Our approach uses the concept of salient phrases: 
for each call reason (class) only 300 words with 
the highest term relevancies are chosen. It allows 
us to eliminate the need for the stop and ignore 
word filtering. The algorithms are implemented 
in C++. 
As a baseline for results comparison we have 
tested some popular classifiers from RapidMiner, 
which we have applied to the whole database and 
the database with decomposition.  
This paper is organized as follows: In Section II, 
we describe the problem and how we perform the 
preprocessing. Section III describes in detail the 
way of the term relevance calculating and the 
possible rules of choosing the call class. In Sec-
tion IV we present the clustering algorithms 
which we apply to simplify the ?garbage? class 
detection. Section V reports on the experimental 
results. Finally, we provide concluding remarks 
in Section VI. 
2 Problem Description and Data Pre-
processing 
The data for testing and evaluation consists of 
about 300.000 user utterances recorded from 
caller interactions with commercial automated 
agents. Utterances from this database are manu-
ally labeled by experts and divided into 20 clas-
ses (_TE_NOMATCH, appointments, operator, 
bill, internet, phone etc). Class _TE_NOMATCH 
includes utterances that cannot be put into anoth-
er class or can be put into more than one class. 
The database is also unbalanced, some classes 
include much more utterances than others (the 
largest class _TE_NOMATCH includes 6790 ut-
terances and the smallest one consists of only 48 
utterances).  
The initial database has been preprocessed to be 
a binary matrix with rows representing utterances 
and columns representing the words from the 
vocabulary. An element from this binary matrix, 
aij, equals to 1 if in utterance i the word j appears 
and equals to 0 if it does not appear.  
Utterance duplicates were removed. The prepro-
cessed database consisting of 24458 utterances 
was divided into train (22020 utterances, 
90,032%) and test set (2438 utterances, 9,968%) 
such that the percentage of classes remained the 
same in both sets. The size of the dictionary of 
the whole database is 3464 words, 3294 words 
appear in training set, 1124 words appear in test 
set, 170 words which appear only in test set and 
do not appear in training set (unknown words), 
33 utterances consisted of only unknown words, 
and 160 utterances included at least one un-
known word. 
3 Term Relevance Estimation  
For each term we assign a real number term rele-
vance that depends on the frequency in utteranc-
es. Term relevance is calculated using a modified 
formula of fuzzy rules relevance estimation for 
fuzzy classifier. Membership function has been 
replaced by word frequency in the current class. 
The details of the procedure are:  
Let L be the number of classes; ni is the number 
of utterances of the ith class; Nij is the number of 
345
jth word occurrence in all utterances of the ith 
class; Tji=Nji/ni is the relative frequency of jth 
word occurrence in the ith class. 
Rj=maxi Tji, Sj=arg(maxi Tji) is the number of 
class which we assign to jth word; 
The term relevance, Cj, is given by 
 
 
 
 
Cj is higher if the word occurs often in few clas-
ses than if it appears in many classes.  
The learning phase consists of counting the C 
values for each term, it means that this algorithm 
uses the statistical information obtained from 
train set. We have tested several different 
decision rules defined in Table 1. 
 
 Decision rules 
RC 
 
For each class i we 
calculate Ai 
 
Then we find the num-
ber of class which 
achieves maximum of 
Ai 
 
RC max 
 
C 
 
C with 
limit 
 
R 
 
Table 1. Decision Rules  
 
The best obtained accuracies is achieved with the 
decision rule C, where the destination is chosen 
that has the highest sum of word relevancies 
from the current utterance. In Table 2 we show 
the obtained results on the whole database and 
database without ?garbage? class. 
 
 Train Test 
With class ?garbage? 0,614 0,551 
Without class ?garbage? 0,887 0,855 
Table 2. Performance of the new TRE approach 
4 Clustering methods 
After the analysis of the performances of stand-
ard classification algorithms on the given data-
base, we can conclude that there exists one spe-
cific class (class _TE_NOMATCH) where all 
standard techniques perform worse. Due to the 
non-uniform structure of the ?garbage? class it is 
difficult to detect the whole class by the pro-
posed procedure. If we apply this procedure di-
rectly we achieve only 55% of accuracy rate on 
the test data (61% on the train data). We suggest 
to divide the ?garbage? class into the set of sub-
classes using one of the clustering methods and 
then recount the values of Cj taking into account 
that there are 19 well defined classes and that the 
set of the ?garbage? subclasses can be consider 
as separate classes.  
In this paper the following clustering methods 
are used: a genetic algorithm with integers, vec-
tor quantization networks trained by a genetic 
algorithm, hierarchical agglomerative clustering 
with different metrics.  
4.1 Genetic Algorithm 
The train set accuracy is used as a fitness func-
tion. Each individual is the sequence of nonnega-
tive integer numbers (each number corresponds 
to the number of ?garbage? subclass). The length 
of this sequence is the number of utterances from 
train set which belong to the ?garbage? class. 
We apply this genetic algorithm to find directly 
the optimal clustering using different numbers of 
clusters and we can conclude that with increasing 
the clusters number (in the ?garbage? class) we 
get better classification accuracy on the whole 
database. We have used the following parameters 
of GA: population size = 50, number of genera-
tion = 50, weak mutation, tournament selection, 
uniform crossover, averaged by 50 runs. Apply-
ing this method we achieve about 7% improve-
ment of accuracy rate on train data and about 5% 
on test data.  
4.2 Vector Quantization Network 
We have also implemented vector quantization 
network. For a given number of subclasses we 
search for the set of code vectors (the number of 
code vectors is equal to the number of sub-
classes). These code vectors are optimized using 
genetic algorithm where as a fitness function we 
use the classification quality on the train set. 
Each code vector corresponds to a certain ?gar-
bage? subclass. The object belongs to the sub-
class if the distance between it and the corre-
sponding code vector is smaller than the distanc-
es between the object and all other code vectors. 
Applying this algorithm to the given database we 
obtain results similar to the results of the genetic 
algorithm.  
4.3 Hierarchical Agglomerative Clustering 
In this work we consider hierarchical agglomera-
tive binary clustering where we set each utter-
ance to one subclass and then we consequently 
group classes into pairs until there is only one 
).
1
1
(
1
1
1
?
? ?=
=
?
?=
L
Si
i
jijL
i
ji
j
j
T
L
R
T
C
?
=
=
iSj
jji
j
CRA
:
)maxarg(
i
iAwinner =
?
=
=
iSj
jji
j
CRA
:
max
?
=
=
iSj
ji
j
CA
:
?
>
=
=
constC
iSj
ji
j
j
CA
:
?
=
=
iSj
ji
j
RA
:
346
class containing all utterances or until we 
achieve a certain number of classes. The perfor-
mance of hierarchical clustering algorithms de-
pends on the metric (the way to calculate the dis-
tance between objects) and the criterion for clus-
ters union. In this work we use Hamming metric 
and Ward criterion (J. Ward. 1963).  
5 Experimental results 
The approach described above has been applied 
on the preprocessed corpus which has been pro-
vided by Speech Cycle company. We propose 
that only terms with highest value of RC (prod-
uct of R and C) are contributed to the total sum. 
We have investigated the dependence of the new 
TRE approach on the frequent words number 
(Figure 1). The best accuracy rate was obtained 
with more than 300 frequent words. By using 
only limited set of words we eliminated the need 
of stop and ignore words filtering. This also 
shows that the method works better if utterance 
includes terms with high C values. This approach 
requires informative well-defined classes and 
enough data for statistical model. 
 
Figure 1. New TRE approach with different numbers 
of frequent words (x-axis: number of frequent words; 
y-axis: accuracy) 
 
Figure 2. Overall accuracy 
Figure 3. Comparison of decision rules (x-axis: deci-
sion rule; y-axis: accuracy) 
 
We have tested standard classification algorithms 
(k-nearest neighbors algorithms, Bayes classifi-
ers, Decision Stump, Rule Induction, perceptron) 
and the proposed approach on the database with 
?garbage? class and on the database without it 
(Figure 2). The proposed algorithm outperforms 
all other methods with has an accuracy rate of 
85.55%. Figure 3 provides accuracies of different 
decision rules. Applying the proposed formula to 
the whole database we obtain 61% and 55% of 
classification quality on train and test data. We 
should also mention that the common tf.idf ap-
proach gives us on the given data 45% and 38% 
of accuracy rate on the train and test data. The 
proposed approach performs significantly better 
on this kind of data.  
Using the agglomerative hierarchical clustering 
we achieve about 9% improvement. The best 
classification quality is obtained with 35 sub-
classes on the train data (68.7%) and 45 sub-
classes on the test data (63.9%). Clustering into 
35 subclasses gives 63.7% of accuracy rate on 
the test data. 
6 Conclusion 
This paper reported on call classification experi-
ments on large corpora using a new term rele-
vance estimation approach. We propose to split 
the classification task into two steps: 1) cluster-
ing of the ?garbage? class in order to simplify its 
detection; 2) further classification into meaning-
ful classes and the set of ?garbage? subclasses. 
The performance of the proposed algorithm is 
compared to several standard classification algo-
rithms on the database without the ?garbage? 
class and found to outperform them with the ac-
curacy rate of 85.55%.  
Dividing the ?garbage? class into the set of sub-
classes by genetic algorithm and vector quantiza-
tion network we obtain about 5% improvement 
of accuracy rate and by agglomerative hierar-
chical clustering we achieve about 9% improve-
ment of accuracy rate on the whole database.  
0,6 
0,65 
0,7 
0,75 
0,8 
0,85 
0,9 
0 20 50 100 150 200 300 
Train set accuracy Test set accuracy 
0 
0,2 
0,4 
0,6 
0,8 
RC RC max C C with 
limit 
R 
Train Set Accuracy Test Set Accuracy 
347
References  
A. Albalate, D. Suendermann, R. Pieraccini, and W. 
Minker. 2009. Mathematical Analysis of Evolution, 
Information, and Complexity, Wiley, Hoboken, 
USA. 
A. Albalate, D. Suendermann D., and W. Minker. 
2011. International Journal on Artificial Intelli-
gence Tools, 20(5). 
A. Albalate, A. Suchindranath, D. Suendermann, and 
W. Minker. 2010. Proc. of the Interspeech 2010, 
11th Annual Conference of the International 
Speech Communication Association, Makuhari, Ja-
pan. 
A. Albalate, S. Rhinow, and D. Suendermann. 2010. 
Proc. of the ICAART 2010, 2nd International Con-
ference on Agents and Artificial Intelligence, Va-
lencia, Spain. 
A.L. Gorin, G. Riccardi, and J. H. Wright. 1997. 
Speech Commun., vol. 23, pp. 113?127. 
B. Carpenter and J. Chu-Carroll. 1998. Proc. ICSLP-
98, pp. 2059?2062. 
C.-H. Lee, B. Carpenter, W. Chou, J. Chu-Carroll, W. 
Reichl, A. Saad, and Q. Zhou. 2000. Speech 
Commun., vol. 31, no. 4, pp. 309?320. 
D. Suendermann, J. Liscombe, K. Dayanidhi, and R. 
Pieraccini. 2009. Proc. of the SIGDIAL 2009, Lon-
don, UK.  
H. Ishibuchi, T. Nakashima, and T. Murata. 1999. 
Trans. on Systems, Man, and Cybernetics, vol. 29, 
pp. 601-618. 
H.-K. Kuo and C.-H. Lee. 2000. Proc. of ICSLP?00. 
J. Chu-Carroll and B. Carpenter. 1999. Comput. Lin-
guist., vol. 25, no. 3, pp. 361- 388. 
J. Ward. 1963. Journal of the American Statistical 
Association, 58 (301): 236-244. 
J. H. Wright, A. L. Gorin, and G. Riccardi. 1997. 
Proc. Eurospeech-97, pp. 1419?1422. 
K. Evanini, D. Suendermann, and R. Pieraccini. 2007. 
Proc. of the ASRU 2007, Kyoto, Japan. 
R. E. Schapire and Y. Singer. 2000. Mach. Learn., 
vol. 39, no. 2/3, pp. 135?168. 
348
Proceedings of the 5th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 84?89,
Baltimore, Maryland, USA. June 27, 2014. c?2014 Association for Computational Linguistics
Opinion Mining and Topic Categorization with Novel Term Weighting 
 
 
Tatiana Gasanova 
Institute of Communications Engineer-
ing, Ulm University, Germany 
tatiana.gasanova@uni-
ulm.de 
Roman Sergienko 
Institute of Communications Engineer-
ing, Ulm University, Germany 
roman.sergienko@uni-ulm.de 
Shakhnaz Akhmedova 
Institute of Computer Science and 
Telecommunications, Siberian State 
Aerospace University, Russia 
shahnaz@inbox.ru  
Eugene Semenkin 
Institute of Computer Science and 
Telecommunications, Siberian State 
Aerospace University, Russia 
eugenesemenkin@yandex.com 
Wolfgang Minker 
Institute of Communications Engineer-
ing, Ulm University, Germany 
wolfgang.minker@uni-ulm.de 
 
  
 
 
 
Abstract 
In this paper we investigate the efficiency of 
the novel term weighting algorithm for opin-
ion mining and topic categorization of arti-
cles from newspapers and Internet. We com-
pare the novel term weighting technique with 
existing approaches such as TF-IDF and 
ConfWeight. The performance on the data 
from the text-mining campaigns DEFT?07 
and DEFT?08 shows that the proposed meth-
od can compete with existing information re-
trieval models in classification quality and 
that it is computationally faster. The pro-
posed text preprocessing method can be ap-
plied in large-scale information retrieval and 
data mining problems and it can be easily 
transported to different domains and different 
languages since it does not require any do-
main-related or linguistic information. 
1 Introduction 
Nowadays, Internet and social media generate a 
huge amount of textual information. It is in-
creasingly important to develop methods of text 
processing such as text classification. Text clas-
sification is very important for such problems 
as automatic opining mining (sentiment analy-
sis) and topic categorization of different articles 
from newspapers and Internet. 
Text classification can be considered to be a 
part of natural language understanding, where 
there is a set of predefined categories and the 
task is to automatically assign new documents 
to one of these categories. The method of text 
preprocessing and text representation influences 
the results that are obtained even with the same 
classification algorithms.  
The most popular model for text classifica-
tion is vector space model. In this case text cat-
egorization may be considered as a machine 
learning problem. Complexity of text categori-
zation with vector space model is compounded 
by the need to extract the numerical data from 
text information before applying machine learn-
ing methods. Therefore text categorization con-
sists of two parts: text preprocessing and classi-
fication using obtained numerical data. 
All text preprocessing methods are based on 
the idea that the category of the document de-
pends on the words or phrases from this docu-
ment. The simplest approach is to take each 
word of the document as a binary coordinate 
and the dimension of the feature space will be 
the number of words in our dictionary.  
There exist more advanced approaches for 
text preprocessing to overcome this problem 
such as TF-IDF (Salton and Buckley, 1988) and 
ConfWeight methods (Soucy and Mineau, 
2005). A novel term weighting method (Gasa-
nova et al., 2013) is also considered, which has 
84
some similarities with the ConfWeight method, 
but has improved computational efficiency. It is 
important to notice that we use no morphologi-
cal or stop-word filtering before text prepro-
cessing. It means that the text preprocessing can 
be performed without expert or linguistic 
knowledge and that the text preprocessing is 
language-independent. 
In this paper we have used k-nearest neigh-
bors algorithm, Bayes Classifier, support vector 
machine (SVM) generated and optimized with 
COBRA (Co-Operation of Biology Related Al-
gorithms) which has been proposed by 
Akhmedova and Semenkin (2013), Rocchio 
Classifier or Nearest Centroid Algorithm (Roc-
chio, 1971) and Neural Network as classifica-
tion methods. RapidMiner and Microsoft Visual 
Studio C++ 2010 have been used as implemen-
tation software. 
For the application of algorithms and com-
parison of the results we have used the DEFT 
(?D?fi Fouille de Texte?) Evaluation Package 
2008 (Proceedings of the 4th DEFT Workshop, 
2008) which has been provided by ELRA and 
publically available corpora from DEFT?07 
(Proceedings of the 3rd DEFT Workshop, 
2007).  
The main aim of this work is to evaluate the 
competitiveness of the novel term weighting 
(Gasanova et al., 2013) in comparison with the 
state-of-the-art techniques for opining mining 
and topic categorization. The criteria using in 
the evaluation are classification quality and 
computational efficiency. 
This paper is organized as follows: in Section 
2, we describe details of the corpora. Section 3 
presents text preprocessing methods. In Section 
4 we describe the classification algorithms 
which we have used to compare different text 
preprocessing techniques. Section 5 reports on 
the experimental results. Finally, we provide 
concluding remarks in Section 6. 
2 Corpora Description 
The focus of DEFT 2007 campaign is the sen-
timent analysis, also called opinion mining. We 
have used 3 publically available corpora: re-
views on books and movies (Books), reviews on 
video games (Games) and political debates 
about energy project (Debates). 
The topic of DEFT 2008 edition is related to 
the text classification by categories and genres. 
The data consists of two corpora (T1 and T2) 
containing articles of two genres: articles ex-
tracted from French daily newspaper Le Monde 
and encyclopedic articles from Wikipedia in 
French language. This paper reports on the re-
sults obtained using both tasks of the campaign 
and focuses on detecting the category. 
 
Corpus Size Classes 
Books Train size = 2074 
Test size = 1386 
Vocabulary = 52507 
0: negative,  
1: neutral,  
2: positive 
Games Train size = 2537 
Test size = 1694 
Vocabulary = 63144 
0: negative,  
1: neutral,  
2: positive 
Debates Train size = 17299 
Test size = 11533 
Vocabulary = 59615 
0: against,  
1: for 
Table 1. Corpora description (DEFT?07) 
 
Corpus Size Classes 
T1 Train size = 15223 
Test size = 10596 
Vocabulary = 202979 
0: Sport,  
1: Economy,  
2: Art, 
3: Television 
T2 Train size = 23550 
Test size = 15693 
Vocabulary = 262400 
0: France,  
1: International,  
2: Literature, 
3: Science, 
4: Society 
Table 2. Corpora description (DEFT?08) 
 
All databases are divided into a training 
(60% of the whole number of articles) and a test 
set (40%). To apply our algorithms we extract-
ed all words which appear in the training set 
regardless of the letter case and we also exclud-
ed dots, commas and other punctual signs. We 
have not used any additional filtering as exclud-
ing the stop or ignore words. 
3 Text Preprocessing Methods 
3.1 Binary preprocessing 
We take each word of the document as a binary 
coordinate and the size of the feature space will 
be the size of our vocabulary (?bag of words?). 
3.2 TF-IDF 
TF-IDF is a well-known approach for text pre-
processing based on multiplication of term fre-
quency tfij (ratio between the number of times 
the ith word occurs in the jth document and the 
document size) and inverse document frequen-
cy idfi. 
 ???? =
???
??
,  (1) 
   
85
where tij is the number of times the i
th word oc-
curs in the jth document. Tj is the document size 
(number of the words in the document). 
There are different ways to calculate the 
weight of each word. In this paper we run clas-
sification algorithms with the following vari-
ants. 
1) TF-IDF 1 
 ???? = ???
|?|
??
, (2) 
where |D| is the number of document in the 
training set and ?? is the number of documents 
that have the ith word. 
2) TF-IDF 2 
The formula is given by equation (2) except 
??  is calculated as the number of times i
th word 
appears in all documents from the training set. 
3) TF-IDF 3 
 ???? = ?
|?|
??
?
?
,? ? (0,1), (3) 
where ?? is calculated as in TF-IDF 1 and ? is 
the parameter (in this paper we have tested ? = 
0.1, 0.5, 0.9). 
4) TF-IDF 4 
The formula is given by equation (3) except 
??  is calculated as in TF-IDF 4. 
3.3 ConfWeight 
Maximum Strength (Maxstr) is an alternative 
method to find the word weights. This approach 
has been proposed by Soucy and Mineau 
(2005). It implicitly does feature selection since 
all frequent words have zero weights. The main 
idea of the method is that the feature f has a 
non-zero weight in class c only if the f frequen-
cy in documents of the c class is greater than 
the f frequency in all other classes. 
The ConfWeight method uses Maxstr as an 
analog of IDF: 
???????????? = ???????? + 1? ? ??????(?). 
Numerical experiments (Soucy and Mineau, 
2005) have shown that the ConfWeight method 
could be more effective than TF-IDF with SVM 
and k-NN as classification methods. The main 
drawback of the ConfWeight method is compu-
tational complexity. This method is more com-
putationally demanding than TF-IDF method 
because the ConfWeight method requires time-
consuming statistical calculations such as Stu-
dent distribution calculation and confidence 
interval definition for each word. 
3.4 Novel Term Weighting (TW) 
The main idea of the method (Gasanova et al., 
2013) is similar to ConfWeight but it is not so 
time-consuming. The idea is that every word 
that appears in the article has to contribute 
some value to the certain class and the class 
with the biggest value we define as a winner for 
this article. 
For each term we assign a real number term 
relevance that depends on the frequency in ut-
terances. Term weight is calculated using a 
modified formula of fuzzy rules relevance esti-
mation for fuzzy classifiers (Ishibuchi et al., 
1999). Membership function has been replaced 
by word frequency in the current class. The de-
tails of the procedure are the following: 
Let L be the number of classes; ni is the 
number of articles which belong to the ith class; 
Nij is the number of the j
th word occurrence in 
all articles from the ith class; Tij = Nij / ni is the 
relative frequency of the jth word occurrence in 
the ith class. 
?? = max? ??? , ?? = arg (max? ???) is the 
number of class which we assign to the jth word; 
The term relevance, Cj, is given by 
 
).
1
1
(
1
1
1
?
? ?=
=
?
?=
L
Si
i
ijjL
i
ji
j
j
T
L
R
T
C  
(4) 
Cj is higher if the word occurs more often in 
one class than if it appears in many classes. We 
use novel TW as an analog of IDF for text pre-
processing. 
The learning phase consists of counting the C 
values for each term; it means that this 
algorithm uses the statistical information 
obtained from the training set.  
4 Classification Methods 
We have considered 11 different text prepro-
cessing methods (4 modifications of TF-IDF, 
two of them with three different values of ? 
parameter, binary representation, ConfWeight 
and the novel TW method) and compared them 
using different classification algorithms. The 
methods have been implemented using 
RapidMiner (Shafait, 2010) and Microsoft Vis-
ual Studio C++ 2010 for Rocchio classifier and 
SVM. The classification methods are: 
- k-nearest neighbors algorithm with dis-
tance weighting (we have varied k from 1 to 
15); 
- kernel Bayes classifier with Laplace cor-
rection; 
- neural network with error back propaga-
tion (standard setting in RapidMiner); 
- Rocchio classifier with different metrics 
and ? parameter; 
86
- support vector machine (SVM) generated 
and optimized with Co-Operation of Biology 
Related Algorithms (COBRA). 
Rocchio classifier (Rocchio, 1971) is a well-
known classifier based on the search of the 
nearest centroid. For each category we calculate 
a weighted centroid: 
?? =
1
|??|
? ? ? ?
1
???,???????
???? ? ?????,? , 
where ?? is a set of documents which belong to 
the class c; ??,?????? are k documents which do not 
belong to the class c and which are close to the 
centroid 
1
|??|
? ?;???? ? is parameter corresponds 
to relative importance of negative precedents. 
The given document is put to the class with the 
nearest centroid. In this work we have applied 
Rocchio classifier with ? ? (0.1; 0.9) and with 
three different metrics: taxicab distance, 
Euclidean metric and cosine similarity. 
COBRA is a new meta-heuristic algorithm 
which has been proposed by Akhmedova and 
Semenkin (2013). It is based on cooperation of 
biology inspired algorithms such as Particle 
Swarm Optimization (Kennedy and Eberhart, 
1995), Wolf Pack Search Algorithm (Yang, 
2007), Firefly Algorithm (Yang, 2008), Cuckoo 
Search Algorithm (Yang and Deb, 2009) and 
Bat Algorithm (Yang, 2010). For generating 
SVM-machine the original COBRA is used: 
each individual in all populations represents a 
set of kernel function?s parameters .,, d??  
Then for each individual constrained modifica-
tion of COBRA is applied for finding vector w 
and shift factor b. And finally individual that 
showed the best classification rate is chosen as 
the designed classifier. 
5 Experimental Results 
The DEFT (?D?fi Fouille de Texte?) Evaluation 
Package 2008 and publically available corpora 
from DEFT?07 (Books, Games and Debates) 
have been used for algorithms application and 
results comparison. In order to evaluate ob-
tained results with the campaign participants we 
have to use the same measure of classification 
quality: precision, recall and F-score. 
Precision for each class i is calculated as the 
number of correctly classified articles for class i 
divided by the number of all articles which al-
gorithm assigned for this class. Recall is the 
number of correctly classified articles for class i 
divided by the number of articles that should 
have been in this class. Overall precision and 
recall are calculated as the arithmetic mean of 
the precisions and recalls for all classes (macro-
average). F-score is calculated as the harmonic 
mean of precision and recall. 
Tables 3-7 present the F-scores obtained on 
the test corpora. The best values for each prob-
lem are shown in bold. Results of the all classi-
fication algorithms are presented with the best 
parameters. We also present for each corpus 
only the best TF-IDF modification.  
 
Classification 
algorithm 
Binary  TF-
IDF 
Conf 
Weight 
Novel 
TW 
Bayes 0.489 0.506 0.238 0.437 
k-NN 0.488  0.517 0.559 0.488 
Rocchio 0.479  0.498 0.557 0.537 
SVM (CO-
BRA) 
0.558  0.580 0.588 0.619 
Neural network 0.475  0.505 0.570 0.493 
Table 3. Classification results for Books 
 
Classification 
algorithm 
Binary  TF-
IDF 
Conf 
Weight 
Novel 
TW 
Bayes 0.653  0.652 0.210 0.675 
k-NN 0.703  0.701 0.720 0.700 
Rocchio 0.659  0.678 0.717 0.712 
SVM (CO-
BRA) 
0.682  0.687 0.645 0.696 
Neural network 0.701  0.679 0.717 0.691 
Table 4. Classification results for Games 
 
Classification 
algorithm 
Binary  TF-
IDF 
Conf 
Weight 
Novel 
TW 
Bayes 0.555  0.645 0.363 0.616 
k-NN 0.645  0.648 0.695 0.695 
Rocchio 0.636  0.646 0.697 0.696 
SVM (CO-
BRA) 
0.673  0.669 0.714 0.700 
Neural network 0.656  0.647 0.705 0.697 
Table 5. Classification results for Debates 
 
Classification 
algorithm 
Binary  TF-
IDF 
Conf 
Weight 
Novel 
TW 
Bayes 0.501  0.690 0.837 0.794 
k-NN 0.800  0.816 0.855 0.837 
Rocchio 0.794  0.825 0.853 0.838 
SVM (CO-
BRA) 
0.788  0.827 0.840 0.856 
Neural network 0.783  0.830 0.853 0.854 
Table 6. Classification results for T1 
 
Classification 
algorithm 
Binary  TF-
IDF 
Conf 
Weight 
Novel 
TW 
Bayes 0.569  0.728 0.712 0.746 
k-NN 0.728  0.786 0.785 0.811 
Rocchio 0.765  0.825 0.803 0.834 
SVM (CO-
BRA) 
0.794  0.837 0.813 0.851 
Neural network 0.799  0.838 0.820 0.843 
Table 7. Classification results for T2 
 
87
We can see from the Tables 3-7 that the best 
F-scores have been obtained with either 
ConfWeight or novel Term Weighting prepro-
cessing. The algorithm performances on the 
Games and Debates corpora achieved the best 
results with ConfWeight; however, we can see 
that the F-scores obtained with novel Term 
Weighting preprocessing are very similar 
(0.712 and 0.720 for Games; 0.700 and 0.714 
for Debates). Almost all best results have been 
obtained with SVM except the Games database 
where we achieved the highest F-score with k-
NN algorithm. 
This paper focuses on the text preprocessing 
methods which do not require language or do-
main-related information; therefore, we have 
not tried to achieve the best possible classifica-
tion quality. However, the result obtained on 
Books corpus with novel TW preprocessing and 
SVM (generated using COBRA) as classifica-
tion algorithm has reached 0.619 F-score which 
is higher than the best known performance 
0.603 (Proceedings of the 3rd DEFT Workshop, 
2007). Performances on other corpora have 
achieved close F-score values to the best sub-
missions of the DEFT?07 and DEFT?08 partici-
pants. 
We have also measured computational effi-
ciency of each text preprocessing technique. 
We have run each method 20 times using the 
Baden-W?rttemberg Grid (bwGRiD) Cluster 
Ulm (Every blade comprehends two 4-Core 
Intel Harpertown CPUs with 2.83 GHz and 16 
GByte RAM). After that we calculated average 
values and checked statistical significance of 
the results. 
Figure 1 and Figure 2 compare average com-
putational time in minutes for different prepro-
cessing methods applied on DEFT?07 and 
DEFT?08 corpora. 
 
Figure 1. Computational efficiency of text pre-
processing methods (DEFT?07) 
 
Figure 2. Computational efficiency of text pre-
processing methods (DEFT?08) 
 
The average value for all TF-IDF modifica-
tions is presented because the time variation for 
the modifications is not significant. 
We can see in Figure 1 and Figure 2 that TF-
IDF and novel TW require almost the same 
computational time. The most time-consuming 
method is ConfWeight (CW). It requires ap-
proximately six times more time than TF-IDF 
and novel TW for DEFT?08 corpora and about 
three-four times more time than TF-IDF and 
novel TW for DEFT?07 databases. 
6 Conclusion 
This paper reported on text classification exper-
iments on 5 different corpora of opinion mining 
and topic categorization using several classifi-
cation methods with different text prepro-
cessing. We have used ?bag of words?, TF-IDF 
modifications, ConfWeight and the novel term 
weighting approach as preprocessing tech-
niques. K-nearest neighbors algorithms, Bayes 
classifier, Rocchio classifier, support vector 
machine trained by COBRA and Neural Net-
work have been applied as classification algo-
rithms.  
The novel term weighting method gives simi-
lar or better classification quality than the 
ConfWeight method but it requires the same 
amount of time as TF-IDF. Almost all best re-
sults have been obtained with SVM generated 
and optimized with Co-Operation of Biology 
Related Algorithms (COBRA). 
We can conclude that numerical experiments 
have shown computational and classification 
efficiency of the proposed method (the novel 
TW) in comparison with existing text prepro-
cessing techniques for opinion mining and topic 
categorization. 
88
References  
Akhmedova Sh. and Semenkin E. 2013. Co-
Operation of Biology Related Algorithms. Pro-
ceedings of the IEEE Congress on Evolutionary 
Computation (CEC 2013):2207-2214. 
Association Fran?aise d?Intelligence Artificielle. 
2007. Proceedings of the 3rd DEFT Workshop. 
DEFT '07. AFIA, Grenoble, France. 
Gasanova T., Sergienko R., Minker W., Semenkin 
E. and Zhukov E. 2013. A Semi-supervised Ap-
proach for Natural Language Call Routing. Pro-
ceedings of the SIGDIAL 2013 Conference:344-
348. 
Ishibuchi H., Nakashima T., and Murata T. 1999. 
Performance evaluation of fuzzy classifier sys-
tems for multidimensional pattern classification 
problems. IEEE Trans. on Systems, Man, and Cy-
bernetics, 29:601-618. 
Kennedy J. and Eberhart R. 1995. Particle Swarm 
Optimization. Proceedings of IEEE International 
Conference on Neural Networks:1942-1948.   
Le traitement automatique du langage naturel ou de 
la langue naturelle. 2008. Proceedings of the 4th 
DEFT Workshop. DEFT '08. TALN, Avignon, 
France. 
Salton G. and Buckley C. 1988. Term-Weighting 
Approaches in Automatic Text Retrieval.  Infor-
mation Processing and Management:513-523. 
Shafait F., Reif M., Kofler C., and Breuel T. M. 
2010. Pattern Recognition Engineering. 
RapidMiner Community Meeting and Conference, 
9. 
Soucy P. and Mineau G.W. 2005. Beyond TFIDF 
Weighting for Text Categorization in the Vector 
Space Model. Proceedings of the 19th Interna-
tional Joint Conference on Artificial Intelligence 
(IJCAI 2005):1130-1135. 
Rocchio J. 1971. Relevance Feedback in Infor-
mation Retrieval. The SMART Retrieval System-
Experiments in Automatic Document Processing, 
Prentice-Hall:313-323. 
Yang Ch. 2007. Algorithm of Marriage in Honey 
Bees Optimization Based on the Wolf Pack 
Search. Proceedings of International Conference 
on Intelligent Pervasive Computing:462-467. 
Yang X.S. 2008. Nature-Inspired Metaheuristic Al-
gorithms. 
Yang X.S. and Deb S. 2009. Cuckoo search via 
Levy flights. Proceedings of World Congress on 
Nature & Biologically Inspired Computing:210-
214. 
Yang X.S. 2010. A New Metaheuristic Bat-Inspired 
Algorithm. Proceedings of Nature Inspired Co-
operative Strategies for Optimization (NISCO 
2010):65-74. 
 
 
 
 
89
