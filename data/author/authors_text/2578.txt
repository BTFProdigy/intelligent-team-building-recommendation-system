Proceedings of the ACL Interactive Poster and Demonstration Sessions,
pages 89?92, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Transonics: A Practical Speech-to-Speech Translator for English-Farsi
Medical Dialogues
Emil Ettelaie, Sudeep Gandhe, Panayiotis Georgiou,
Kevin Knight, Daniel Marcu, Shrikanth Narayanan ,
David Traum
University of Southern California
Los Angeles, CA 90089
ettelaie@isi.edu, gandhe@ict.usc.edu,
georgiou@sipi.usc.edu, knight@isi.edu,
marcu@isi.edu, shri@sipi.usc.edu,
traum@ict.use.edu
Robert Belvin
HRL Laboratories, LLC
3011 Malibu Canyon Rd.
Malibu, CA 90265
rsbelvin@hrl.com
Abstract
We briefly describe a two-way speech-to-
speech English-Farsi translation system
prototype developed for use in doctor-
patient interactions.  The overarching
philosophy of the developers has been to
create a system that enables effective
communication, rather than focusing on
maximizing component-level perform-
ance.  The discussion focuses on the gen-
eral approach and evaluation of the
system by an independent government
evaluation team.
1 Introduction
In this paper we give a brief description of a
two-way speech-to-speech translation system,
which was created under a collaborative effort
between three organizations within USC (the
Speech Analysis and Interpretation Lab of the
Electrical Engineering department, the Information
Sciences Institute, and the Institute for Creative
Technologies) and the Information Sciences Lab of
HRL Laboratories.  The system is intended to pro-
vide a means of enabling communication between
monolingual English speakers and monolingual
Farsi (Persian) speakers.  The system is targeted at
a domain which may be roughly characterized as
"urgent care" medical interactions, where the Eng-
lish speaker is a medical professional and the Farsi
speaker is the patient.  In addition to providing a
brief description of the system (and pointers to pa-
pers which contain more detailed information), we
give an overview of the major system evaluation
activities.
2 General Design of the system
Our system is comprised of seven speech and
language processing components, as shown in Fig.
1. Modules communicate using a centralized mes-
sage-passing system. The individual subsystems
are the Automatic Speech Recognition (ASR) sub-
system, which uses n-gram Language Models
(LM) and produces n-best lists/lattices along with
the decoding confidence scores. The output of the
ASR is sent to the Dialog Manager (DM), which
displays the n-best and passes one hypothesis on to
the translation modules, according to a user-
configurable state. The DM sends translation re-
quests to the Machine Translation (MT) unit. The
MT unit works in two modes: Classifier based MT
and a fully Stochastic MT. Depending on the dia-
logue manager mode, translations can be sent to
the unit selection based Text-To-Speech synthe-
sizer (TTS), to provide the spoken output. The
same basic pipeline works in both directions: Eng-
lish ASR, English-Persian MT, Persian TTS, or
Persian ASR, Persian-English MT, English TTS.
There is, however, an asymmetry in the dia-
logue management and control, given the desire for
the English-speaking doctor to be in control of the
device and the primary "director" of the dialog.
The English ASR used the University of Colo-
rado Sonic recognizer, augmented primarily with
LM data collected from multiple sources, including
89
our own large-scale simulated doctor-patient dia-
logue corpus based on recordings of medical stu-
dents examining standardized patients (details in
Belvin et al 2004).
1
 The Farsi acoustic models r e-
quired an eclectic approach due to the lack of ex-
isting labeled speech corpora.  The approach
included borrowing acoustic data from English by
means of developing a sub-phonetic mapping be-
tween the two languages, as detailed in (Srini-
vasamurthy & Narayanan 2003), as well as use of
a small existing Farsi speech corpus (FARSDAT),
and our own team-internally generated acoustic
data.  Language modeling data was also obtained
from multiple sources.  The Defense Language
Institute translated approximately 600,000 words
of English medical dialogue data (including our
standardized patient data mentioned above), and in
addition, we were able to obtain usable Farsi text
from mining the web for electronic news sources.
Other  smaller  amounts of  training  data  were ob
tained from various sources, as detailed in  (Nara-
yanan et al 2003, 2004).  Additional detail on de-
velopment methods for all of these components,
system integration and evaluation can also be
found in the papers just cited.
The MT components, as noted, consist of both a
Classifier and a stochastic translation engine,  both
                                                           
1
 Standardized Patients are typically actors who have been
trained by doctors or nurses to portray symptoms of particular
illnesses or injuries.  They are used extensively in medical
education so that doctors in training don't have to "practice"
on real patients.
developed by USC-ISI team members.  The Eng-
lish Classifier uses approximately 1400 classes
consisting mostly of standard questions used by
medical care providers in medical interviews.
Each class has a large number of paraphrases asso-
ciated with it, such that if the care provider speaks
one of those phrases, the system will identify it
with the class and translate it to Farsi via table-
lookup.  If the Classifier cannot succeed in finding
a match exceeding a confidence threshold, the sto-
chastic MT engine will be employed.  The sto-
chastic MT engine relies on n-gram
correspondences between the source and target
languages.  As with ASR, the performance of the
component is highly dependent on very large
amounts of training data.  Again, there were multi-
ple sources of training data used, the most signifi-
cant being the data generated by our own team's
English collection effort, supported by translation
into Farsi by DLI. Further details of the MT com-
ponents can be found in Narayanan et al, op.cit.
3 Enabling Effective Communication
The approach taken in the development of Tran-
sonics was what can be referred to as the total
communication pathway.  We are not so concerned
with trying to maximize the performance of a
given component of the system, but rather with the
effectiveness of the system as a whole in facilitat-
ing actual communication.  To this end, our design
and development included the following:
MT
English to Farsi
Farsi to English
ASR
English
Prompts or TTS
Farsi
Prompts or TTS
English
ASR
Farsi
GUI:
prompts,
 confirmations,
 ASR switch
Dialog
Manager
SMT
English to Farsi
Farsi to English
Figure 1: Architecture of the Transonics system.  The Dialogue Manager acts as the hub through which the
individual components interact.
90
i. an "educated guess" capability (system
guessing at the meaning of an utterance) from the
Classifier translation mechanism?this proved very
useful for noisy ASR output, especially for the re-
stricted domain of medical interviews.
ii. a flexible and robust SMT good for filling in
where the more accurate Classifier misses.
iii. exploitation of a partial n-best list as part of
the GUI used by the doctor/medic for the English
ASR component and the Farsi-to-English transla-
tion component.
iv. a dialog manager which in essence occa-
sionally makes  "suggestions" (for next questions
for the doctor to ask) based on query sets which are
topically related to the query the system believes it
recognized the doctor to have spoken.
Overall, the system achieves a respectable level of
performance in terms of allowing users to follow a
conversational thread in a fairly coherent way, de-
spite the presence of frequent ungrammatical or
awkward translations (i.e. despite what we might
call non-catastrophic errors).
4 Testing and Evaluation
In addition to our own laboratory tests, the sys-
tem was evaluated by MITRE as part of the
DARPA program.  There were two parts to the
MITRE evaluations, a "live" part, designed pri-
marily to evaluate the overall task-oriented effec-
tiveness of the systems, and a "canned" part,
designed primarily to evaluate individual compo-
nents of the systems.
The live evaluation consisted of six medical
professionals (doctors, corpsmen and physician?s
assistants from the Naval Medical Center at Quan-
tico, and a nurse from a civilian institution) con-
ducting unrehearsed "focused history and physical
exam" style interactions with Farsi speakers play-
ing the role of patients, where the English-speaking
doctor and the Farsi-speaking patient communi-
cated by means of the Transonics system.  Since
the cases were common enough to be within the
realm of general internal medicine, there was no
attempt to align ailments with medical specializa-
tions among the medical professionals.
MITRE endeavored to find primarily monolin-
gual Farsi speakers to play the role of patient, so as
to provide a true test of the system to enable com-
munication between people who would otherwise
have no way to communicate.  This goal was only
partially realized, since one of the two Farsi patient
role-players was partially competent in English.
2
The Farsi-speaking role-players were trained by a
medical education specialist in how to simulate
symptoms of someone with particular injuries or
illnesses.  Each Farsi-speaking patient role-player
received approximately 30 minutes of training for
any given illness or injury.  The approach was
similar to that used in training standardized pa-
tients, mentioned above (footnote 1) in connection
with generation of the dialogue corpus.
MITRE established a number of their own met-
rics for measuring the success of the systems, as
well as using previously established metrics.  A
full discussion of these metrics and the results ob-
tained for the Transonics system is beyond the
scope of this paper, though we will note that one of
the most important of these was task-completion.
There were 5 significant facts (5 distinct facts for
each of 12 different scenarios) that the medical
professional should have discovered in the process
of interviewing/examining each Farsi patient.  The
USC/HRL system averaged 3 out of the 5 facts,
which was a slightly above-average score among
the 4 systems evaluated.  A "significant fact" con-
sisted of determining a fact which was critical for
diagnosis, such as the fact that the patient had been
injured in a fall down a stairway, the fact that the
patient was experiencing blurred vision, and so on.
Significant facts did not include items such as a
patient's age or marital status.
3
  We report on this
measure in that it is perhaps the single most im-
portant component in the assessment, in our opin-
ion, in that it is an indication of many aspects of
the system, including both directions of the trans-
lation system.  That is, the doctor will very likely
conclude correct findings only if his/her question is
translated correctly to the patient, and also if the
patient's answer is translated correctly for the doc-
tor.  In a true medical exam, the doctor may have
                                                           
2
 There were additional difficulties encountered as well, hav-
ing to do with one of the role-players not adequately grasping
the goal of role-playing.  This experience highlighted the
many challenges inherent in simulating domain-specific
spontaneous dialogue.
3
 Unfortunately, there was no baseline evaluation this could be
compared to,  such as assessing whether any of the critical
facts could be determined without the use of the system at all.
91
other means of determining some critical facts
even in the absence of verbal communication, but
in the role-playing scenario described, this is very
unlikely.  Although this measure is admittedly
coarse-grained, it simultaneously shows, in a crude
sense, that the USC/HRL system compared fa-
vorably against the other 3 systems in the evalua-
tion, and also that there is still significant room for
improvement in the state of the art.
As noted, MITRE devised a component evalua-
tion process also consisting of running 5 scripted
dialogs through the systems and then measuring
ASR and MT performance.  The two primary
component measures were a version of BLEU for
the MT component (modified slightly to handle the
much shorter sentences typical of this kind of dia-
log) and a standard Word-Error Rate for the ASR
output.  These scores are shown below.
Table 1:  Farsi BLEU Scores
IBM BLEU
ASR
IBM BLEU
TEXT
English to Farsi
0.2664 0.3059
Farsi  to English 0.2402 0.2935
The reason for the two different BLEU scores is
that one was calculated based on the ASR compo-
nent output being translated to the other language,
while the other was calculated from human tran-
scribed text being translated to the other language.
Table 2:  HRL/USC WER for Farsi and English
English Farsi
WER 11.5% 13.4%
5 Conclusion
In this paper we have given an overview of the
design, implementation and evaluation of the Tran-
sonics speech-to-speech translation system for nar-
row domain two-way translation.  Although there
are still many significant hurdles to be overcome
before this kind of technology can be called truly
robust, with appropriate training and two coopera-
tive interlocutors, we can now see some degree of
genuine communication being enabled.  And this is
very encouraging indeed.
6 Acknowledgements
This work was supported primarily by the DARPA
CAST/Babylon program, contract N66001-02-C-
6023.
References
R. Belvin, W. May, S. Narayanan, P. Georgiou, S. Gan-
javi.  2004. Creation of a Doctor-Patient Dialogue
Corpus Using Standardized Patients. In Proceedings of
the Language Resources and Evaluation Conference
(LREC), Lisbon, Portugal.
S. Ganjavi, P. G. Georgiou, and S. Narayanan. 2003.
Ascii based transcription schemes for languages with
the Arabic script: The case of Persian. In Proc. IEEE
ASRU,  St. Thomas, U.S. Virgin Islands.
S. Narayanan, S. Ananthakrishnan, R. Belvin, E. Ette-
laie, S. Ganjavi, P. Georgiou, C. Hein, S. Kadambe,
K. Knight, D. Marcu, H. Neely, N. Srinivasamurthy,
D. Traum and D. Wang.  2003. Transonics: A speech
to speech system for English-Persian Interactions,
Proc. IEEE ASRU,  St. Thomas, U.S. Virgin Islands.
S. Narayanan, S. Ananthakrishnan, R. Belvin, E. Ette-
laie, S. Gandhe, S. Ganjavi, P. G. Georgiou, C. M.
Hein, S. Kadambe, K. Knight, D. Marcu, H. E.
Neely, N. Srinivasamurthy, D. Traum, and D. Wang.
2004. The Transonics Spoken Dialogue Translator:
An aid for English-Persian Doctor-Patient interviews,
in Working Notes of the AAAI Fall symposium on
Dialogue Systems for Health Communication, pp 97-
-103.
N. Srinivasamurthy, and S. Narayanan. 2003. Language
adaptive Persian speech recognition. In proceedings
of Eurospeech 2003.
92
Proceedings of SPEECHGRAM 2007, pages 33?40,
Prague, Czech Republic, June 2007. c?2007 Association for Computational Linguistics
Handling Out-of-Grammar Commands in Mobile Speech Interaction  
Using Backoff Filler Models 
Tim Paek1, Sudeep Gandhe2, David Maxwell Chickering1, Yun Cheng Ju1 
1 Microsoft Research, One Microsoft Way, Redmond, WA 98052 USA 
2
 USC Institute for Creative Technologies, 13274 Fiji Way, Marina del Rey, CA 90292, USA 
{timpaek|dmax|yuncj}@microsoft.com, gandhe@usc.edu 
 
 
Abstract 
In command and control (C&C) speech in-
teraction, users interact by speaking com-
mands or asking questions typically speci-
fied in a context-free grammar (CFG). Un-
fortunately, users often produce out-of-
grammar (OOG) commands, which can re-
sult in misunderstanding or non-
understanding.  We explore a simple ap-
proach to handling OOG commands that 
involves generating a backoff grammar 
from any CFG using filler models, and util-
izing that grammar for recognition when-
ever the CFG fails.  Working within the 
memory footprint requirements of a mobile 
C&C product, applying the approach 
yielded a 35% relative reduction in seman-
tic error rate for OOG commands.  It also 
improved partial recognitions for enabling 
clarification dialogue. 
1 Introduction 
In command and control (C&C) speech interaction, 
users interact with a system by speaking com-
mands or asking questions.  By defining a rigid 
syntax of possible phrases, C&C reduces the com-
plexity of having to recognize unconstrained natu-
ral language.  As such, it generally affords higher 
recognition accuracy, though at the cost of requir-
ing users to learn the syntax of the interaction 
(Rosenfeld et al, 2001).  To lessen the burden on 
users, C&C grammars are authored in an iterative 
fashion so as to broaden the coverage of likely ex-
pressions for commands, while remaining rela-
tively simple for faster performance.  Nevertheless, 
users can, and often still do, produce OOG com-
mands.  They may neglect to read the instructions, 
or forget the valid expressions.  They may mistak-
enly believe that recognition is more robust than it 
really is, or take too long to articulate the right 
words.  Whatever the reason, OOG commands can 
engender misunderstanding (i.e., recognition of the 
wrong command) or non-understanding (i.e., no 
recognition), and aggravate users who otherwise 
might not realize that their commands were OOG.  
In this paper, we explore a simple approach to 
handling OOG commands, designed specifically to 
meet the memory footprint requirements of a C&C 
product for mobile devices.  This paper is divided 
into three sections.  First, we provide background 
on the C&C product and discuss the different types 
of OOG commands that occur with personal mo-
bile devices. Second, we explain the details of the 
approach and how we applied it to the product do-
main. Finally, we evaluate the approach on data 
collected from real users, and discuss possible 
drawbacks. 
2 Mobile C&C 
With the introduction of voice dialing on mobile 
devices, C&C speech interaction hit the wider 
consumer market, albeit with rudimentary pattern 
recognition. Although C&C has been 
commonplace in telephony and accessibility for 
many years, only recently have mobile devices 
have the memory and processing capacity to 
support not only automatic speech recognition 
(ASR), but a whole range of multimedia 
functionalities that can be controlled with speech.  
Leveraging this newfound computational capacity 
is Voice Command, a C&C application for high-
end mobile devices that allows users to look up 
contacts, place phone calls, retrieve appointments, 
obtain device status information, control 
multimedia and launch applications.  It uses an 
embedded, speaker-independent recognizer and 
operates on 16 bit, 16 kHz, Mono audio. 
33
OOG commands pose a serious threat to the us-
ability of Voice Command. Many mobile users ex-
pect the product to ?just work? without having to 
read the manual.  So, if they should say ?Dial Bob?, 
when the proper syntax for making a phone call is 
Call {Name}, the utterance will likely be mis-
recognized or dropped as a false recognition.  If 
this happens enough, users may abandon the prod-
uct, concluding that it or ASR in general, does not 
work. 
2.1 OOG frequency 
Given that C&C speech interaction is typically 
geared towards a relatively small number of words 
per utterance, an important question is, how often 
do OOG commands really occur in C&C?  In Pro-
ject54 (Kun & Turner, 2005), a C&C application 
for retrieving police information in patrol cars, 
voice commands failed on average 15% of the time, 
roughly 63% of which were due to human error.  
Of that amount, roughly 54% were from extrane-
ous words not found in the grammar, 12% from 
segmentation errors, and the rest from speaking 
commands that were not active. 
To examine whether OOG commands might be 
as frequent on personal mobile devices, we col-
lected over 9700 commands of roughly 1 to 3 sec-
onds each from 204 real users of Voice Command, 
which were recorded as sound (.wav) files.  We 
also logged all device data such as contact entries 
and media items.  All sound files were transcribed 
by a paid professional transcription service.  We 
ignored all transcriptions that did not have an asso-
ciated command; the majority of such cases came 
from accidental pressing of the push-to-talk button.  
Furthermore, we focused on user-initiated com-
mands, during which time the active grammar had 
the highest perplexity, instead of yes-no responses 
and clarification dialogue.  This left 5061 tran-
scribed utterances. 
2.2 Emulation method 
With the data transcribed, we first needed a me-
thod to distinguish between In-Grammar (ING) 
and OOG utterances.  We developed a simulation 
environment built around a desktop version of the 
embedded recognizer which could load the same 
Voice Command grammars and update them with 
user device data, such as contact entries, for each 
sound file.  It is important to note the desktop ver-
sion was not the engine that is commercially 
shipped and optimized for particular devices, but 
rather one that serves testing and research purposes. 
The environment could not only recognize sound 
files, but also parse string input using the dynami-
cally updated grammars as if that were the recog-
nized result.  We utilized the latter to emulate rec-
ognition of all transcribed utterances for Voice 
Command.  If the parse succeeded, we labeled the 
utterance ING, otherwise it was labeled OOG. 
Overall, we found that slightly more than one 
out of every four (1361 or 26.9%) transcribed ut-
terances were OOG.  We provide a complete 
breakdown of OOG types, including extraneous 
words and segmentation errors similar to Project54, 
in the next section.  It is important to keep in mind 
that being OOG by emulation does not necessarily 
entail that the recognizer will fail on the actual 
sound file.  For example, if a user states ?Call Bob 
at mobile phone? when the word ?phone? is OOG, 
the recognizer will still perform well.  The OOG 
percentage for Voice Command may also reflect 
the high perplexity of the name-dialing task.  Users 
had anywhere from 5 to over 2000 contacts, each 
of which could be expressed in multiple ways (e.g., 
first name, first name + last name, prefix + last 
name, etc.).  In summary, our empirical analysis of 
the data suggests that OOG utterances for mobile 
C&C on personal devices can indeed occur on a 
frequent basis, and as such, are worth handling. 
2.3 OOG type 
In order to explore how we might handle different 
types of OOG commands, we classified them ac-
cording to functional anatomy and basic edit op-
erations.  With respect to the former, a C&C utter-
ance consists of three functional components: 
 
1. Slot: A dynamically adjustable list repre-
senting a semantic argument, such as {Con-
tact} or {Date}, where the value of the ar-
gument is typically one of the list members. 
2. Keyword: A word or phrase that uniquely 
identifies a semantic predicate, such as Call 
or Battery, where the predicate corresponds 
in a one-to-one mapping to a type of com-
mand.  
3. Carrier Text: A word or phrase that is de-
signed to facilitate naturalistic expression of 
commands and carries no attached semantic 
content, such as ?What is? or ?Tell me?. 
 
34
For example, in the command ?Call Bob at mo-
bile?, the word ?Call? is the keyword, ?Bob? and 
?mobile? are slots, and ?at? is a carrier word.  
If we were to convert an ING command to 
match an OOG command, we could perform a se-
ries of edit operations: substitution, deletion, and 
insertion.  For classifying OOG commands, substi-
tution implies the use of an unexpected word, dele-
tion implies the absence of an expected word, and 
insertion implies the addition of a superfluous 
word. 
Starting with both functional anatomy and edit 
operations for classification, Table 1 displays the 
different types of OOG commands we labeled 
along with their relative frequencies.  Because 
more than one label might apply to an utterance, 
we first looked to the slot for an OOG type label, 
then keyword, then everything else.  
The most frequent OOG type, at about 60%, was 
OOG Slot, which referred to slot values that did 
not exist in the grammar.  The majority of these 
cases came from two sources: 1) contact entries 
that users thought existed but did not ? sometimes 
they did exist, but not in any normalized form (e.g., 
?Rich? for ?Richard?), and 2) mislabeling of most-
ly foreign names by transcribers.  Although we 
tried to correct as many names as we could, given 
the large contact lists that many users had, this 
proved to be quite challenging.  
The second most frequent OOG type was Inser-
tion at about 14%.  The majority of these insertions 
were single words.  Note that similar to Project54, 
segmentation errors occurred quite often at about 
9%, when the different segmentation types are 
added together. 
3 Backoff Approach 
Having identified the different types of OOG 
commands, we needed to devise an approach for 
handling them that satisfied the requirements of 
Voice Command for supporting C&C on mobile 
devices.  
3.1 Mobile requirements 
For memory footprint, the Voice Command team 
specified that our approach should operate with 
less than 100KB of ROM and 1MB of RAM.  Fur-
thermore, the approach could not require changes 
OOGType % Total Description Examples 
Insertion 14.2% adding a non-keyword, non-slot word call britney porter on mobile phone [?phone? is 
superfluous] 
Deletion 3.1% deleting a non-keyword, non-slot 
word my next appointments [?what are? missing] 
Substitution 2.5% replacing a non-keyword, non-slot 
word 
where is my next appointment  
[?where? is not supported] 
Segmentation 8.2% incomplete utterance show, call, start 
Keyword 
Substitution 4.6% replacing a keyword 
call 8 8 2 8 0 8 0 [?dial? is keyword] ,  
dial john horton [?call? is keyword] 
Keyword 
Segmentation 0.1% incomplete keyword what are my appoint 
Keyword  
Deletion 2.2% deleting the keyword marsha porter at home [?call? missing] 
Slot  
Substitution 0.4% replacing slot words 
call executive 5 on desk  
[?desk? is not slot value] 
Slot  
Segmentation 0.9% incomplete slot call alexander woods on mob 
Slot Deletion 1.0% deleted slot call tracy morey at 
Disfluencies 1.8% disfluencies - mostly repetitions start expense start microsoft excel 
Order  
Rearrangement 0.6% 
changing the order of words within a 
keyword 
what meeting is next [Should be ?what is my 
next meeting?] 
Noise 0.7% non primary speaker oregon state home coming call brandon jones 
on mobile phone 
OOG Slot 59.8% The slot associated with this utterance is out of domain 
Show Rich Lowry [?Richard? is contact entry] , 
dial 0 2 1 6 [Needs > 7 digits] 
 
Table 1. Different OOG command types and their relative frequencies for the Voice Command product. The brack-
eted text in the ?Examples? column explicates the cause of the error 
35
to the existing embedded Speech API (SAPI).  
Because the team also wanted to extend the func-
tionality of Voice Command to new domains, we 
could not assume that we would have any data for 
training models.  Although statistical language 
models (SLM) offer greater robustness to varia-
tions in phrasing than fixed grammars (Rosenfeld, 
2000), the above requirements essentially prohib-
ited them.  So, we instead focused on extending the 
use of the base grammar, which for Voice Com-
mand was a context-free grammar (CFG): a formal 
specification of rules allowing for embedded recur-
sion that defines the set of possible phrases (Man-
ning & Sch?tze, 1999).  
Despite the manual effort that CFGs often re-
quire, they are widely prevalent in industry (Knight 
et al, 2001) for several reasons. First, they are easy 
for designers to understand and author.  Second, 
they are easy to modify; new phrases can be added 
and immediately recognized with little effort.  And 
third, they produce transparent semantics without 
requiring a separate natural language understand-
ing component; semantic properties can be at-
tached to CFG rules and assigned during recogni-
tion.  By focusing on CFGs, our approach allows 
industry designers who are more accustomed to 
fixed grammars to continue using their skill set, 
while hopefully improving the handling of utter-
ances that fall outside of their grammar. 
3.2 Leveraging a backoff grammar 
As long as utterances remain ING, a CFG affords 
fast and accurate recognition, especially because 
engines are often tuned to optimize C&C recogni-
tion.  For example, in comparing recognition per-
formance in a statistical and a CFG-based recog-
nizer for the same domain, Knight et al (2001) 
found that the CFG outperformed the SLM.  In 
order to exploit the optimization of the engine for 
C&C utterances that are ING, we decided to utilize 
a two-pass approach where each command is ini-
tially submitted to the base CFG.  If the confidence 
score of the top recognition C1 falls below a rejec-
tion threshold RCFG, or if the recognizer declares a 
false recognition (based on internal engine fea-
tures), then the audio stream is passed to a backoff 
grammar which then attempts to recognize the 
command.  If the backoff grammar fails to recog-
nize the command, or the top recognition falls 
again below a rejection threshold RBG, then users 
experience the same outcome as they normally 
would otherwise, except with a longer delay.  Fig-
ure 1(a) summarizes the approach. 
In order to generate the backoff grammar and 
still stay within the required memory bounds of 
Voice Command, we explored the use of the built-
in filler or garbage model, which is a context-
independent, acoustic phone loop.  Expressed in 
the syntax as ?...?, filler models capture phones in 
whatever context they are placed.  The functional 
anatomy of a C&C utterance, as explained in Sec-
tion 2.3, sheds light on where to place them: before 
and/or after keywords and/or slots.  As shown Fig-
ure 1(b), to construct a backoff grammar from a 
CFG during design time, we simply parse each 
CFG rule for keywords and slots, remove all car-
rier phrases, and insert filler models before and/or 
after the keywords and/or slots.  Although it is 
straightforward to automatically identify keywords 
(words that uniquely map to a CFG rule) and slots 
(lists with semantic properties), developers may 
want to edit the generated backoff grammar for any 
keywords and slots they wish to exclude; for ex-
ample, in cases where more than one keyword is 
found for a CFG rule. 
For both slots and keywords, we could employ 
any number of different patterns for placing the 
filler models, if any.  Table 2 displays some of the 
patterns in SAPI 5 format, which is an XML for-
mat where question marks indicate optional use.  
Although the Table is for keywords, the same pat-
terns apply for slots.  As shown in k4, even the 
functional constituent itself can be optional.  Fur-
thermore, alternate lists of patterns can be com-
posed, as in kn.  Depending on the number and type 
 
 
Figure 1. (a) A two-pass approach which leverages a 
base CFG for ING recognition and a backoff grammar 
for failed utterances. (b) Design time procedure for 
generating a backoff grammar 
36
of functional constituents for a CFG rule, backoff 
rules can be constructed by adjoining patterns for 
each constituent.  We address the situation when a 
backoff rule corresponds to multiple CFG rules in 
Section 3.4. 
3.3 Domain feasibility 
Because every C&C utterance can be characterized 
by its functional constituents, the backoff filler ap-
proach generically applies to C&C domains, re-
gardless of the actual keywords and slots.  But the 
question remains, is this generic approach feasible 
for handling the different OOG types for Voice 
Command discussed in Section 2.3? 
The filler model is clearly suited for Insertions, 
which are the second most frequent OOG type, 
because it would capture the additional phones.  
However, the most frequent OOG type, OOG Slot, 
cannot be handled by the backoff approach.  That 
requires the developer to write better code for 
proper name normalization (e.g, ?Rich? from ?Ri-
chard?) as well as breaking down the slot value 
into further components for better partial matching 
of names.  Because new C&C domains may not 
utilize name slots, we decided to treat improving 
name recognition as separate research.  Fortu-
nately, opportunity for applying the backoff filler 
approach to OOG Slot types still exists. 
3.4 Clarification of partial recognitions 
As researchers have observed, OOG words con-
tribute to increased word-error rates (Bazzi & 
Glass, 2000) and degrade the recognition perform-
ance of surrounding ING words (Gorrell, 2003).  
Hence, even if a keyword surrounding an OOG 
slot is recognized, its confidence score and the 
overall phrase confidence score will often be de-
graded.  This is in some ways an unfortunate by-
product of confidence annotation, which might be 
circumvented if SAPI exposed word lattice prob-
abilities.  Because SAPI does not, we can instead 
generate partial backoff rules that comprise only a 
subset of the functional constituents of a CFG rule.  
For example, if a CFG rule contains both a key-
word and slot, then we can generate a partial back-
off rule with just one or the other surrounded by 
filler models.  Using partial backoff rules prevents 
degradation of confidence scores for ING constitu-
ents and improves partial recognitions, as we show 
in Section 4.  Partial backoff rules not only handle 
OOG Slot commands where, for example, the 
name slot is not recognized, but also many types of 
segmentation, deletion and substitution commands 
as well. 
Following prior research (Gorrell et al, 2002; 
Hockey et al, 2003), we sought to improve partial 
recognitions so that the system could provide feed-
back to users on what was recognized, and to en-
courage them to stay within the C&C syntax.  Cla-
rification dialogue with implicit instruction of the 
syntax might proceed as follows: If a partial recog-
nition only corresponded to one CFG rule, then the 
system could assume the semantics of that rule and 
remind the user of the proper syntax.  On the other 
hand, if a partial recognition corresponded to more 
than one rule, then a disambiguation dialogue 
could relate the proper syntax for the choices.  For 
example, suppose a user says ?Telephone Bob?, 
using the OOG word ?Telephone?.  Although the 
original CFG would most likely misrecognize or 
even drop this command, our approach would ob-
tain a partial recognition with higher confidence 
score for the contact slot.  If only one CFG rule 
contained the slot, then the system could engage in 
the confirmation, ?Did you mean to say, call 
Bob?? On the other hand, if more than one CFG 
rule contained the slot, then the system could en-
gage in a disambiguation dialogue, such as ?I 
heard 'Bob'. You can either call or show Bob?.  
Either way, the user is exposed to and implicitly 
taught the proper C&C syntax. 
3.5 Related research 
In related research, several researchers have inves-
tigated using both a CFG and a domain-trained 
SLM simultaneously for recognition (Gorrell et al, 
2002; Hockey et al, 2003).  To finesse the per-
formance of a CFG, Gorrell (2003) advocated a 
two-pass approach where an SLM trained on CFG 
Scheme Keyword Pattern 
k1 <keyword/> 
k2 (?)?  <keyword> 
k3 (?)?  <keyword/>  (?)? 
k4 (?)?  <keyword/>? (?)? 
kn 
<list> 
(?)?  <keyword/>? (?)? 
(?) 
</list> 
 
Table 2. Possible patterns in SAPI 5 XML format for 
placing the filler model, which appears as 
?...?.Question marks indicate optional use. 
37
data (and slightly augmented) is utilized as a back-
off grammar.  However, only the performance of 
the SLM on a binary OOG classification task was 
evaluated and not the two-pass approach itself.  In 
designing a multimodal language acquisition sys-
tem, Dusan & Flanagan (2002) developed a two-
pass approach where they utilized a dictation n-
gram as a backoff grammar and added words rec-
ognized in the second pass into the base CFG.  Un-
fortunately, they only evaluated the general usabil-
ity of their architecture. 
Because of the requirements outlined in Section 
3.1, we have focused our efforts on generating a 
backoff grammar from the original CFG, taking 
advantage of functional anatomy and filler models.  
The approach is agnostic about what the actual fil-
ler model is, and as such, the built-in phone loop 
can easily be replaced by word-level (e.g., Yu et 
al., 2006) and sub-word level filler models (e.g., 
Liu et al, 2005).  In fact, we did explore the word-
level filler model, though so far we have not been 
able to meet the footprint requirements.  We are 
currently investigating phone-based filler models. 
Outside of recognition with a CFG, researchers 
have pursued methods that directly model OOG 
words as sub-word units in the recognition search 
space of a finite state transducer (FST) (Bazzi & 
Glass, 2000).  OOG words can also be dynamically 
incorporated into the FST (Chung et al, 2004).  
Because this line of research depends on entirely 
different engine architecture, we could not apply 
the techniques. 
4 Evaluation 
In C&C speech interaction, what matters most is 
not word-error rate, but semantic accuracy and task 
completion.  Because task completion is difficult to 
evaluate without collecting new data, we evaluated 
the semantic accuracy of the two-pass approach 
against the baseline of using just the CFG on the 
data we collected from real users, as discussed in 
Section 2.1.  Furthermore, because partial 
recognitions can ultimately result in a successful 
dialogue, we carried out separate evaluations for 
the functional constituents of a command (i.e., 
keyword and slot) as well as the complete 
command (keyword + slot).  For Voice Command, 
no command contained more than one slot, and 
because the vast majority of single slot commands 
were commands to either call or show a contact 
entry, we focused on those two commands as a 
proof of concept. 
For any utterance, the recognizer can either ac-
cept or reject it.  If it is accepted, then the seman-
tics of the utterance can either be correct (i.e., it 
matches what the user intended) or incorrect.  The 
following metrics can now be defined: 
 
precision = CA / (CA + IA)   (1) 
recall = CA / (CA + R)    (2) 
accuracy = CA / (CA + IA + R)   (3) 
 
where CA denotes accepted commands that are 
correct, IA denotes accepted commands that are 
incorrect, and R denotes the number of rejected 
commands.  Although R could be decomposed into 
correct and incorrect rejections, for C&C, 
recognition failure is essentially perceived the 
same way by users: that is, as a non-understanding. 
4.1 Results 
For every C&C command in Voice Command, the 
embedded recognizer returns either a false 
recognition (based on internal engine parameters) 
or a recognition event with a confidence score.  As 
described in Section 3.2, if the confidence score 
falls below a rejection threshold RCFG, then the 
audio stream is processed by the backoff grammar 
which also enforces its own threshold RBG.  The 
RCFG for Voice Command was set to 45% by a 
proprietary tuning procedure for optimizing 
acoustic word-error rate.  For utterances that 
exceeded RCFG, 84.2% of them were ING and 
15.8% OOG.  For utterances below RCFG, 48.5% 
 
 
Figure 2. The semantic accuracies comparing the 
baseline CFG against both the BG (backoff grammar 
alone) and the two-pass approach (CFG + Backoff) 
separated into functional constituent groups and fur-
ther separated by ING and OOG commands. 
38
were ING and 51.5% OOG.  Because a 
considerable number of utterances may be ING in 
the second pass, as it was in our case, RBG requires 
tuning as well.  Instead of using a development 
dataset to tune RBG, we decided to evaluate our 
approach on the entire data with RBG set to the 
same proprietary threshold as RCFG.  In post-hoc 
analyses, this policy of setting the two thresholds 
equal and reverting to the CFG recognition if the 
backoff confidence score falls below RBG achieved 
results comparable to optimizing the thresholds. 
Figure 2 displays semantic accuracies separated 
by ING and OOG commands.  Keyword evalua-
tions comprised 3700 ING and 1361 OOG com-
mands.  Slot and keyword + slot evaluations com-
prised 2111 ING and 138 OOG commands.  Over-
all, the two-pass approach was significantly higher 
in semantic accuracy than the baseline CFG, using 
McNemar's test (p<0.001).  Not surprisingly, the 
largest gains were with OOG commands.  Notice 
that for partial recognitions (i.e., keyword or slot 
only), the approach was able to improve accuracy, 
which with further clarification dialogue, could 
result in task completions.  Interestingly, the ap-
proach performed the same for keyword + slot as it 
did for slot, which suggests that getting the slot 
correct is crucial to recognizing surrounding key-
words.  Despite the high percentage of OOG Slots, 
slot accuracy still increased due to better handling 
of other OOG types such as deletions, insertions 
and substitutions.   
Finally, as a comparison, for the keyword + slot 
task, an upper bound of 74.3% ? 1.1% (10-fold 
cross-validated standard error) overall semantic 
accuracy was achieved using a small footprint sta-
tistical language modeling technique that re-ranked 
CFG results (Paek & Chickering, 2007), though 
the comparison is not completely fair given that the 
technique was focused on predictive language 
modeling and not on explicitly handling OOG ut-
terances.  Also note that in all cases, the backoff 
grammar alone performed worse than either the 
CFG or the two-pass approach.   
Table 3 provides a more detailed view of the re-
sults for the just OOG commands as well as the 
relative reductions in semantic error rate (RER).  
Notice that the approach increases recall, which 
signifies less non-understandings. However, this 
comes at the price of a small increase in misunder-
standings, as seen in the decrease in precision.  
Overall, the best reduction in semantic error rate 
achieved by the approach was about 35%. 
Decomposing RER by OOG types, we found 
that for keyword evaluations, the biggest im-
provement (52% RER), came about for Deletion 
types, or commands with missing carrier words.  
This makes sense because the backoff grammar 
only cares about the keyword.  For slot and key-
word + slot evaluations, Insertion types maintained 
the biggest improvement at 38% RER. 
Note that the results presented are those ob-
tained without tuning.  If application developers 
wanted to find an optimal operating point, they 
would need to decide what is more important for 
their application: precision or recall, and adjust the 
thresholds until they reach acceptable levels of per-
formance.  Ideally, these levels should accord with 
what real users of the application would accept. 
4.2 Efficiency 
Given that the approach was aimed at satisfying 
the mobile requirements stated in Section 3.1, 
which it did, we also compared the processing time 
it takes to arrive at a recognition or false 
recognition between the CFG alone and the two-
pass approach.  Because of the filler models, the 
backoff grammar is a more relaxed version of CFG 
with a larger search space, and as such, takes 
slightly more processing time. The average 
processing time for the CFG in our simulation 
environment was about 395 milliseconds, whereas 
the average processing time for the two passes was 
about 986 milliseconds.  Hence, when the backoff 
grammar is used, the total computation time is 
approximately 2.5 times that of a single pass alone.  
In our experiments, a total of 1570 commands (i.e. 
31%) required the two passes, while 3491 of them 
were accepted after a single CFG pass. 
 CFG 2-PASS RER 
Prec 85.0% 79.0% -39.7% 
Recall 36.8% 58.6% 34.5% Keyword 
Acc 34.5% 50.7% 24.7% 
Prec 89.3% 88.2% -10.3% 
Recall 58.1% 77.6% 46.5% Slot 
Acc 54.4% 70.3% 34.9% 
Prec 89.3% 88.2% -10.3% 
Recall 58.1% 77.6% 46.5% Keyword 
+ Slot 
Acc 54.4% 70.3% 34.9% 
 
Table 3. Relative reductions in semantic error rate, or 
Relative Error Reduction (RER) for OOG commands 
grouped by keyword, slot and keyword + slot evalua-
tions. ?2-PASS? denotes the two-pass approach. 
39
4.3 Drawbacks 
In exploring the backoff filler approach, we 
encountered a few drawbacks that are worth 
considering when applying this approach to other 
domains.  The first issue dealt with false positives.  
In the data collection for Voice Command, a total 
of 288 utterances contained no discernable speech.  
If these were included in the data set, they would 
amount to about 5% of all utterances.  As 
mentioned previously, these were mostly cases 
when the push-to-talk button was accidentally 
triggered.  When we evaluated the approach on 
these utterances, we found that the CFG accepted 
36 or roughly 13% of them, while the proposed 
approach accepted 115 or roughly 40% of them.  
For our domain, this problem can be avoided by 
instructing users to lock their devices when not in 
use to prevent spurious initiations.  For other C&C 
domains where unintentional command initiations 
occur frequently, this may be a serious concern, 
though we suspect that users will be more 
forgiving of accidental errors than real errors. 
Another drawback dealt with generating the 
backoff grammar.  As we discussed in Section 3.2, 
various patterns for placing filler models can be 
utilized.  Although we did explore the possibility 
that perhaps certain patterns might generalize 
across domains, we found that it was better to 
hand-craft patterns to the application.  For Voice 
Command, we used the kn pattern specified in Ta-
ble 2 for keywords, and the identical sn pattern for 
slots because they proved to be best suited to the 
product grammars in pre-trial experiments. 
5 Conclusion & Future Direction 
In this paper, we classified the different types of 
OOG commands that might occur in a mobile 
C&C application, and presented a simple two-pass 
approach for handling them that leverages the base 
CFG for ING recognition and a backoff grammar 
OOG recognition.  The backoff grammar is gener-
ated from the original CFG by surrounding key-
words and/or slots with filler models.  Operating 
within the memory footprint requirements of a 
mobile C&C product, the approach yielded a 35% 
relative reduction in semantic error rate for OOG 
commands, and improved partial recognitions, 
which can facilitate clarification dialogue. 
We are now exploring small footprint, phone-
based filler models.  Another avenue for future 
research is to further investigate optimal policies 
for deciding when to pass to the backoff grammar 
and when to use the backoff grammar recognition. 
References 
I. Bazzi & J. Glass. 2000. Modeling out-of-vocabulary 
words for robust speech recognition. In Proc. ICSLP. 
G. Chung, S. Seneff, C.Wang, & I. Hetherington. 2004. 
A dynamic vocabulary spoken dialogue interface. In 
Proc. ICSLP. 
S. Dusan & J. Flanagan. 2002. Adaptive dialog based 
upon multimodal language acquisition. In Proc. IC-
MI. 
G. Gorrell, I. Lewin, & M. Rayner. 2002. Adding intel-
ligent help to mixed initiative spoken dialogue sys-
tems. In Proc. ICSLP. 
G. Gorrell. 2003. Using statistical language modeling to 
identify new vocabulary in a grammar-based speech 
recognition system. In Proc. Eurospeech. 
B. Hockey, O. Lemon, E. Campana, L. Hiatt, G. Aist, J. 
Hieronymus, A. Gruenstein, & J. Dowding. 2003. 
Targeted help for spoken dialogue systems: intelli-
gent feedback improves naive users? performance. In 
Proc. EACL, pp. 147?154. 
S. Knight, G. Gorrell, M. Rayner, D. Milward, R. Koel-
ing, & I. Lewin. 2001. Comparing grammar-based 
and robust approaches to speech understanding: A 
case study. In Proc. Eurospeech. 
A. Kun & L. Turner. 2005. Evaluating the project54 
speech user interface. In Proc. Pervasive. 
P. Liu, Y. Tian, J. Zhou, & F. Soong. 2005. Background 
model based posterior probability for measuring 
confidence. In Proc. Interspeech. 
C.D. Manning & H. Sch?utze. 1999. Foundations of 
Statistical Natural Language Processing. MIT Press, 
Cambridge,Massachusetts. 
Paek, T. & Chickering, D. 2007. Improving command 
and control speech recognition: Using predictive us-
er models for language modeling. UMUAI, 17(1):93-
117. 
Rosenfeld, R. 2000. Two decades of statistical language 
modeling: Where do we go from here? In Proc. of the 
IEEE, 88(8): 1270?1278. 
R. Rosenfeld, D. Olsen, & A. Rudnicky. 2001. Univer-
sal speech interfaces. Interactions, 8(6):34?44. 
D. Yu, Y.C. Ju, Y. Wang, & A. Acero. 2006. N-gram 
based filler model for robust grammar authoring. In 
Proc. ICASSP. 
40
Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 64?67,
Columbus, June 2008. c?2008 Association for Computational Linguistics
Rapidly Deploying Grammar-Based Speech Applications 
with Active Learning and Back-off Grammars 
Tim Paek1, Sudeep Gandhe2, David Maxwel Chickering1 
1 Microsoft Research, One Microsoft Way, Redmond, WA 98052 
2 USC Institute for Creative Technologies, 13274 Fiji Way, Marina del Rey, CA 90292 
{timpaek|dmax}@microsoft.com, gandhe@usc.edu 
                                                          
2 Second author was partly sponsored by the U.S. Army Research, Development, and Engineering Command (RDECOM). Statements and opi-
nions expressed do not necessarily reflect the position or the policy of the U.S. Government, and no official endorsement should be inferred. 
Abstract 
Grammar-based approaches to spoken lan-
guage understanding are utilized to a great ex-
tent in industry, particularly when developers 
are confronted with data sparsity. In order to 
ensure wide grammar coverage, developers 
typically modify their grammars in an itera-
tive process of deploying the application, col-
lecting and transcribing user utterances, and 
adjusting the grammar. In this paper, we ex-
plore enhancing this iterative process by leve-
raging active learning with back-off 
grammars. Because the back-off grammars 
expand coverage of user utterances, develop-
ers have a safety net for deploying applica-
tions earlier. Furthermore, the statistics related 
to the back-off can be used for active learning, 
thus reducing the effort and cost of data tran-
scription. In experiments conducted on a 
commercially deployed application, the ap-
proach achieved levels of semantic accuracy 
comparable to transcribing all failed utter-
ances with 87% less transcriptions. 
1 Introduction 
Although research in spoken language understand-
ing is typically pursued from a statistical perspec-
tive, grammar-based approaches are utilized to a 
great extent in industry (Knight et al, 2001). 
Speech recognition grammars are often manually 
authored and iteratively modified as follows: Typi-
cally, context-free grammars (CFG) are written in 
a format such as Speech Recognition Grammar 
Specification (SRGS) (W3C, 2004) and deployed. 
Once user utterances are collected and transcribed, 
the grammars are then adjusted to improve their 
coverage. This process continues until minimal 
OOG utterances are observed. In this paper, we 
explore enhancing this iterative process of gram-
mar modification by combining back-off gram-
mars, which expand coverage of user utterances, 
with active learning, which reduces ?the number of 
training examples to be labeled by automatically 
processing unlabeled examples, and then selecting 
the most informative ones with respect to a speci-
fied cost function for a human to label? (Hakkani-
Tur et al, 2002). This paper comprises three sec-
tions. In Section 2, we describe our overall ap-
proach to rapid application development (RAD). In 
Section 3, we explain how data transcription can 
be reduced by leveraging active learning based on 
statistics related to the usage of back-off gram-
mars. Finally, in Section 4, we evaluate the active 
learning approach with simulation experiments 
conducted on data collected from a commercial 
grammar-based speech application. 
2 RAD Approach & Related Work 
Working under the assumption that developers in 
industry will continue to use CFGs for rapid appli-
cation development, our approach to grammar 
modification is as follows: 
1. Create a CFG (either manually or automatically). 
1.1 Generate a back-off grammar from the CFG. 
2. Deploy the application. 
2.1 Use the back-off grammar for OOG utterances. 
3. Gather data from users. 
4. Selectively transcribe data by using statistics re-
lated to the back-off for active learning; i.e., transcribe 
only those utterances that satisfy the active learning 
criterion. 
5. Modify CFG either manually or automatically and 
go to step 1.1. 
To begin with, developers start with a CFG in Step 
1. If they had access to a grammatical platform 
64
such as Regulus (Rayner et al, 2006), they could 
in principle construct a CFG automatically for any 
new domain, though most developers will probably 
manually author the grammar. Two steps are added 
to the typical iterative process. In Step 1.1, we 
generate a back-off grammar from the CFG. One 
way to accomplish this is by constructing a back-
off CFG using filler models (Paek et al, 2007), 
which when applied to the same command-and-
control task in Section 4 can result in a 35% rela-
tive reduction in semantic error rate for OOG ut-
terances. However, the back-off grammar could 
also be a SLM trained on artificial data created 
from the CFG (Galescu et al, 1998). Whatever 
back-off mechanism is employed, its coverage 
should be wider than the original CFG so that ut-
terances that fail to be recognized by the CFG, or 
fall below an acceptable confidence threshold, can 
be handled by the back-off in a second or simulta-
neous pass. That is the gist of Step 2.1, the second 
additional step. It is not only important to generate 
a back-off grammar, but it must be utilized for 
handling possible OOG utterances. 
Our approach attempts to reduce the usual cost 
associated with grammar modification after the 
application has been deployed and data collected in 
Step 4. The idea is simple: Exploit the fast and ac-
curate CFG recognition of in-grammar (ING) ut-
terances by making OOG utterances handled by 
the back-off grammar ING. In other words, expand 
CFG coverage to include whatever gets handled by 
the back-off grammar. This idea is very comple-
mentary with a two-pass recognition approach 
where the goal is to get utterances correctly recog-
nized by a CFG on the first pass so as to minimize 
computational expenses (Paek et al, 2007).  
All of this can be accomplished with reduced 
transcription effort by keeping track of and leve-
raging back-off statistics for active learning. If the 
back-off is a CFG, we keep track of statistics re-
lated to which CFG rules were utilized the most, 
whether they allowed the task to be successfully 
completed, etc. If the back-off is a SLM, we keep 
track of similar statistics related to the semantic 
alignment and mapping in spoken language under-
standing. Given an active learning criterion, these 
statistics can be used to selectively transcribe ut-
terances which can then be used to modify the 
CFG in Step 5 so that OOG utterances become 
ING. Section 3 covers this in more detail. 
Finally, in Step 5, the CFG grammar is mod-
ified using the selectively transcribed utterances. 
Although developers will probably want to do this 
manually, it is possible to automate much of this 
step by making grammar changes with minimal 
edit distance or Levenshtein distance. 
Leveraging a wider coverage back-off grammar 
is of course not new. For grammar-based applica-
tions, several researchers have investigated using a 
CFG along with a back-off grammar either simul-
taneously via a domain-trained SLM (Gorrell et 
a1., 2002), or in two-pass recognition using either 
an SLM trained on CFG data (Gorrell, 2003) or a 
dictation n-gram (Dusan & Flanagan, 2002). To 
our knowledge however, no prior research has con-
sidered leveraging statistics related to the back-off 
grammar for active learning, especially as part of a 
RAD approach. 
3 Active Learning 
Our overall approach utilizes back-off grammars to 
provide developers with a safety net for deploying 
applications earlier, and active learning to reduce 
transcription effort and cost. We now elaborate on 
active learning, demonstrate the concept with re-
spect to a CFG back-off. 
Active learning aims at reducing transcription 
of training examples by selecting utterances that 
are most likely to be informative according to a 
specified cost function (Hakkani-Tur et al, 2002). 
In the speech community, active learning has been 
successfully applied to reducing the transcription 
effort for ASR (Hakkani-Tur et al, 2002), SLU 
(Tur et al, 2003b), as well as finding labeling er-
rors (Tur et al, 2003). In our case, the examples 
are user utterances that need to be transcribed, and 
the learning involves modifying a CFG to achieve 
wider coverage of user expressions. Instead of pas-
sively transcribing everything and modifying the 
CFG as such, the grammar can ?actively? partici-
pate in which utterances are transcribed. 
The usual procedure for selecting utterances for 
grammar modification is to transcribe at least all 
failed utterances, such as those that fall below a 
rejection threshold. By leveraging a back-off 
grammar, developers have more information with 
which to select utterances for transcription. For a 
CFG back-off, how frequently a back-off rule fired 
can serve as an active learning criterion because 
that is where OOG utterances are handled. Given 
65
this active learning criterion, the algorithm would 
proceed as follows (where i denotes iteration, St 
denotes the set of transcribed utterances, and Su 
denotes the set of all utterances): 
[1] Modify CFGi using St and generate corresponding 
back-offi from the CFGi. 
[2] Recognize utterances in set Su using CFGi + back-
offi. 
[3] Compute statistics on what back-off rules fired 
when and how frequently. 
[4] Select the k utterances that were handled by the 
most frequently occurring back-off rule and tran-
scribe them. Call the new transcribed set as Si. 
[5] ;t t i u u iS S S S S S? ? ??  
[6] Stop when CFGi achieves a desired level of seman-
tic accuracy, or alternatively when back-off rules 
only handle a desired percentage of Su, otherwise 
go to Step 1. 
Note that the set Su grows with each iteration and 
follows as a result of deploying an application with 
a CFGi + back-offi. Step [1] corresponds to Step 5, 
1.1, and 2.1 of our approach. Steps [2-4] above 
constitute the active learning criterion and can be 
adjusted depending on what developers want to 
optimize. This algorithm currently assumes that 
runtime efficiency is the main objective (e.g., on a 
mobile device); hence, it is critical to move utter-
ances recognized in the second pass to the first 
pass. If developers are more interested in learning 
new semantics, in Step [4] above they could tran-
scribe utterances that failed in the back-off. With 
an active learning criterion in place, Step [6] pro-
vides a stopping criterion. This too can be adjusted, 
and may even target budgetary objectives. 
4 Evaluation 
For evaluation, we used utterances collected from 
204 users of Microsoft Voice Command, a gram-
mar-based command-and-control (C&C) applica-
tion for high-end mobile devices (see Paek et al, 
2007 for details). We partitioned 5061 transcribed 
utterances into five sets, one of which was used 
exclusively for testing. The remaining four were 
used for iterative CFG modification. For the first 
iteration, we started with a CFG which was a de-
graded version of the grammar currently shipped 
with the Voice Command product. It was obtained 
by using the mode, or the most frequent user utter-
ance, for each CFG rule. We compared two ap-
proaches: CFG_Full, where each iterative CFG 
was modified using the full set of transcribed utter-
ances that resulted in a failure state (i.e., when a 
false recognition event occurred or the phrase con-
fidence score fell below 45%, which was set by a 
proprietary tuning procedure for optimizing word-
error rate), and CFG_Active, where each iterative 
CFG was modified using only those transcribed 
utterances corresponding to the most frequently 
occurring CFG back-off rules. For both CFG_Full 
and CFG_Active, CFGi was modified using the 
same set of heuristics akin to minimal edit dis-
tance. In order to assess the value of using the 
back-off grammar as a safety net, we also com-
pared CFG_Full+Back-off, where a derived CFG 
back-off was utilized whenever a failure state oc-
curred with CFG_Full, and CFG_Active+Back-off, 
where again a CFG back-off was utilized, this time 
with the back-off derived from the CFG trained on 
selective utterances. 
As our metric, we evaluated semantic accuracy 
since that is what matters most in C&C settings. 
Furthermore, because recognition of part of an ut-
terance can increase the odds of ultimately achiev-
ing task completion (Paek et al, 2007), we carried 
out separate evaluations for the functional consti-
tuents of a C&C utterance (i.e., keyword and slot) 
as well as the complete phrase (keyword + slot). 
We computed accuracy as follows: For any single 
utterance, the recognizer can either accept or reject 
it. If it is accepted, then the semantics of the utter-
ance can either be correct (i.e., it matches what the 
user intended) or incorrect, hence: 
accuracy = CA / (CA + IA + R)   (1) 
where CA denotes accepted commands that are 
correct, IA denotes accepted commands that are 
incorrect, and R denotes the number of rejections. 
Table 2 displays semantic accuracies for both 
CFG_Full and CFG_Active. Standard errors about 
the mean were computed using the jacknife proce-
dure with 10 re-samples. Notice that both 
CFG_Full and CFG_Active initially have the same 
accuracy levels because they start off with the 
same degraded CFG. The highest accuracies ob-
tained almost always occurred in the second itera-
tion after modifying the CFG with the first batch of 
transcriptions. Thereafter, all accuracies seem to 
decrease. In order to understand why this would be 
case, we computed the coverage of the ith CFG on 
the holdout set. This is reported in the ?OOG%? 
column. Comparing CFG_Full to CFG_Active on 
66
keyword + slot accuracy, CFG_Full decreases in 
accuracy after the second iteration as does 
CFG_Active. However, the OOG% of CFG_Full is 
much lower than CFG_Active. In fact, it seems to 
level off after the second iteration, suggesting that 
perhaps the decrease in accuracies reflects the in-
crease in grammar perplexity; that is, as the gram-
mar covers more of the utterances, it has more 
hypotheses to consider, and as a result, performs 
slightly worse. Interestingly, after the last iteration, 
CFG_Active for keyword + slot and slot accuracies 
was slightly higher (69.06%) than CFG_Full 
(66.88%) (p = .05). Furthermore, this was done 
with 193 utterances as opposed to 1393, or 87% 
less transcriptions. For keyword accuracy, 
CFG_Active (64.09%) was slightly worse than 
CFG_Full (66.10%) (p < .05). 
With respect to the value of having a back-off 
grammar as a safety net, we found that both 
CFG_Full and CFG_Active achieved much higher 
accuracies with the back-off for keyword, slot, and 
keyword + slot accuracies. Notice also that the dif-
ferences between CFG_Full and CFG_Active after 
the last iteration were much closer to each other 
than without the back-off, suggesting applications 
should always be deployed with a back-off. 
5 Conclusion 
In this paper, we explored enhancing the usual 
iterative process of grammar modification by leve-
raging active learning with back-off grammars. 
Because the back-off grammars expand coverage 
of user utterances to handle OOG occurrences, de-
velopers have a safety net for deploying applica-
tions earlier. Furthermore, because statistics related 
to the back-off can be used for active learning, de-
velopers can reduce the effort and cost of data 
transcription. In our simulation experiments, leve-
raging active learning achieved levels of semantic 
accuracy comparable to transcribing all failed ut-
terances with 87% less transcriptions. 
References 
S. Dusan & J. Flanagan. 2002. Adaptive dialog based upon multimod-
al language acquisition. In Proc. of ICMI. 
L. Galescu, E. Ringger, & J. Allen. 1998. Rapid language model de-
velopment for new task domains. In Proc. of LREC. 
G. Gorrell, I. Lewin, & M. Rayner. 2002. Adding intelligent help to 
mixed initiative spoken dialogue systems. In Proc. of ICSLP. 
G.. Gorrell. 2003. Using statistical language modeling to identify new 
vocabulary in a grammar-based speech recognition system. In 
Proc. of Eurospeech. 
D. Hakkani-Tur, G. Riccardi & A. Gorin. 2002. Active learning for 
automatic speech recognition. In Proc. of ICASSP. 
S. Knight, G. Gorrell, M. Rayner, D. Milward, R. Koel-ing, & I. Le-
win. 2001. Comparing grammar-based and robust approaches to 
speech understanding: A case study. In Proc. of Eurospeech. 
T. Paek, S. Gandhe, D. Chickering & Y. Ju. 2007. Handling out-of-
grammar commands in mobile speech interaction using back-off 
filler models. In Proc. of ACL Workshop on Grammar-Based Ap-
proaches to Spoken Language Processing (SPEECHGRAM). 
M. Rayner, B.A. Hockey, & P. Bouillon. 2006. Putting Linguistics 
into Speech Recognition: The Regulus Grammar Compiler. CSLI. 
G. Tur, M. Rahim & D. Hakkani-Tur. 2003. Active labeling for spo-
ken language understanding. In Proc. of Eurospeech. 
G. Tur, R. Schapire, & D. Hakkani-Tur. 2003b. Active learning for 
spoken language understanding. In Proc. of ICASSP. 
W3C. 2004. Speech Recognition Grammar Specification Version 1.0. 
http://www.w3.org/TR/speech-grammar  
Approach i 
Utterances 
Transcribed 
Keyword  
Accuracy 
Slot  
Accuracy 
Keyword + Slot 
Accuracy 
Processing 
Time (ms) 
OOG% 
CFG_Full 
 
1 0 50.25% (0.13%) 46.84% (0.22%) 46.84% (0.22%) 387 (3.9005) 61.10% 
2 590 66.20% (0.12%) 71.02% (0.23%) 70.59% (0.23%) 401 (4.0586) 31.92% 
3 1000 65.80% (0.15%) 69.72% (0.19%) 69.06% (0.19%) 422 (4.5804) 31.30% 
4 1393 66.10% (0.13%) 67.54% (0.22%) 66.88% (0.21%) 433 (4.7061) 30.95% 
CFG_Full + 
Back-off 
1 0 66.70% (0.10%) 66.23% (0.22%) 66.01% (0.22%) 631 (11.1320) 61.10% 
2 590 73.32% (0.11%) 72.11% (0.22%) 71.68% (0.23%) 562 (10.4696) 31.92% 
3 1000 72.52% (0.12%) 72.11% (0.21%) 71.46% (0.22%) 584 (10.4985) 31.30% 
4 1393 73.02% (0.10%) 71.02% (0.23%) 70.37% (0.23%) 592 (10.6805) 30.95% 
CFG_Active 
1 0 50.25% (0.13%) 46.84% (0.22%) 46.84% (0.22%) 387 (3.9005) 61.10% 
2 87 64.09% (0.13%) 74.29% (0.21%) 74.07% (0.22%) 395 (4.1469) 42.09% 
3 138 64.29% (0.15%) 70.15% (0.22%) 69.50% (0.24%) 409 (4.3375) 38.02% 
4 193 64.09% (0.15%) 69.72% (0.23%) 69.06% (0.24%) 413 (4.4015) 37.93% 
CFG_Active 
+ Back-off 
1 0 66.70% (0.10%) 66.23% (0.22%) 66.01% (0.22%) 631 (11.1320) 61.10% 
2 87 72.52% (0.10%) 76.91% (0.19%) 76.47% (0.21%) 568 (10.3494) 42.09% 
3 138 71.72% (0.14%) 71.90% (0.24%) 71.24% (0.27%) 581 (10.6330) 38.02% 
4 193 71.21% (0.15%) 71.90% (0.25%) 71.24% (0.26%) 580 (10.5266) 37.93% 
Table 2. Semantic accuracies for partial (keyword or slot) and full phrase recognitions (keyword + slot) using a CFG trained on either 
?Full? or ?Active? transcriptions (i.e., selective transcriptions based on active learning). Parentheses indicate standard error about the mean.  
The ?i? column represents iteration.  The ?Utterances Transcribed? column is cumulative.  The ?OOG%? column represents coverage of the 
ith CFG on the hold-out set. Rows containing ?Back-off? evaluate 2-pass recognition using both the CFG and a derived CFG back-off. 
 
 
67
Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 172?181,
Columbus, June 2008. c?2008 Association for Computational Linguistics
An Evaluation Understudy for Dialogue Coherence Models
Sudeep Gandhe and David Traum
Institute for Creative Technologies
University of Southern California
13274 Fiji way, Marina del Rey, CA, 90292
{gandhe,traum}@ict.usc.edu
Abstract
Evaluating a dialogue system is seen as a
major challenge within the dialogue research
community. Due to the very nature of the task,
most of the evaluation methods need a sub-
stantial amount of human involvement. Fol-
lowing the tradition in machine translation,
summarization and discourse coherence mod-
eling, we introduce the the idea of evaluation
understudy for dialogue coherence models.
Following (Lapata, 2006), we use the infor-
mation ordering task as a testbed for evaluat-
ing dialogue coherence models. This paper re-
ports findings about the reliability of the infor-
mation ordering task as applied to dialogues.
We find that simple n-gram co-occurrence
statistics similar in spirit to BLEU (Papineni
et al, 2001) correlate very well with human
judgments for dialogue coherence.
1 Introduction
In computer science or any other research field, sim-
ply building a system that accomplishes a certain
goal is not enough. It needs to be thoroughly eval-
uated. One might want to evaluate the system just
to see to what degree the goal is being accomplished
or to compare two or more systems with one another.
Evaluation can also lead to understanding the short-
comings of the system and the reasons for these. Fi-
nally the evaluation results can be used as feedback
in improving the system.
The best way to evaluate a novel algorithm or a
model for a system that is designed to aid humans
in processing natural language would be to employ
it in a real system and allow users to interact with it.
The data collected by this process can then be used
for evaluation. Sometimes this data needs further
analysis - which may include annotations, collect-
ing subjective judgments from humans, etc. Since
human judgments tend to vary, we may need to em-
ploy multiple judges. These are some of the reasons
why evaluation is time consuming, costly and some-
times prohibitively expensive.
Furthermore, if the system being developed con-
tains a machine learning component, the problem of
costly evaluation becomes even more serious. Ma-
chine learning components often optimize certain
free parameters by using evaluation results on held-
out data or by using n-fold cross-validation. Eval-
uation results can also help with feature selection.
This need for repeated evaluation can forbid the use
of data-driven machine learning components.
For these reasons, using an automatic evalua-
tion measure as an understudy is quickly becoming
a common practice in natural language processing
tasks. The general idea is to find an automatic eval-
uation metric that correlates very well with human
judgments. This allows developers to use the auto-
matic metric as a stand-in for human evaluation. Al-
though it cannot replace the finesse of human evalu-
ation, it can provide a crude idea of progress which
can later be validated. e.g. BLEU (Papineni et al,
2001) for machine translation, ROUGE (Lin, 2004)
for summarization.
Recently, the discourse coherence modeling com-
munity has started using the information ordering
task as a testbed to test their discourse coherence
models (Barzilay and Lapata, 2005; Soricut and
Marcu, 2006). Lapata (2006) has proposed an au-
172
tomatic evaluation measure for the information or-
dering task. We propose to use the same task as a
testbed for dialogue coherence modeling. We evalu-
ate the reliability of the information ordering task as
applied to dialogues and propose an evaluation un-
derstudy for dialogue coherence models.
In the next section, we look at related work in
evaluation of dialogue systems. Section 3 sum-
marizes the information ordering task and Lap-
ata?s (2006) findings. It is followed by the details
of the experiments we carried out and our observa-
tions. We conclude with a summary future work di-
rections.
2 Related Work
Most of the work on evaluating dialogue systems fo-
cuses on human-machine communication geared to-
wards a specific task. A variety of evaluation met-
rics can be reported for such task-oriented dialogue
systems. Dialogue systems can be judged based
on the performance of their components like WER
for ASR (Jurafsky and Martin, 2000), concept er-
ror rate or F-scores for NLU, understandability for
speech synthesis etc. Usually the core component,
the dialogue model - which is responsible for keep-
ing track of the dialogue progression and coming
up with an appropriate response, is evaluated indi-
rectly. Different dialogue models can be compared
with each other by keeping the rest of components
fixed and then by comparing the dialogue systems
as a whole. Dialogue systems can report subjective
measures such as user satisfaction scores and per-
ceived task completion. SASSI (Hone and Graham,
2000) prescribes a set of questions used for elicit-
ing such subjective assessments. The objective eval-
uation metrics can include dialogue efficiency and
quality measures.
PARADISE (Walker et al, 2000) was an attempt
at reducing the human involvement in evaluation. It
builds a predictive model for user satisfaction as a
linear combination of some objective measures and
perceived task completion. Even then the system
needs to train on the data gathered from user sur-
veys and objective features retrieved from logs of di-
alogue runs. It still needs to run the actual dialogue
system and collect objective features and perceived
task completeion to predict user satisfaction.
Other efforts in saving human involvement in
evaluation include using simulated users for test-
ing (Eckert et al, 1997). This has become a popu-
lar tool for systems employing reinforcement learn-
ing (Levin et al, 1997; Williams and Young, 2006).
Some of the methods involved in user simulation
are as complex as building dialogue systems them-
selves (Schatzmann et al, 2007). User simulations
also need to be evaluated as how closely they model
human behavior (Georgila et al, 2006) or as how
good a predictor they are of dialogue system perfor-
mance (Williams, 2007).
Some researchers have proposed metrics for eval-
uating a dialogue model in a task-oriented system.
(Henderson et al, 2005) used the number of slots in
a frame filled and/or confirmed. Roque et al (2006)
proposed hand-annotating information-states in a di-
alogue to evaluate the accuracy of information state
updates. Such measures make assumptions about
the underlying dialogue model being used (e.g.,
form-based or information-state based etc.).
We are more interested in evaluating types of di-
alogue systems that do not follow these task-based
assumptions: systems designed to imitate human-
human conversations. Such dialogue systems can
range from chatbots like Alice (Wallace, 2003),
Eliza (Weizenbaum, 1966) to virtual humans used
in simulation training (Traum et al, 2005). For
such systems, the notion of task completion or ef-
ficiency is not well defined and task specific objec-
tive measures are hardly suitable. Most evaluations
report the subjective evaluations for appropriateness
of responses. Traum et. al. (2004) propose a cod-
ing scheme for response appropriateness and scoring
functions for those categories. Gandhe et. al. (2006)
propose a scale for subjective assessment for appro-
priateness.
3 Information Ordering
The information ordering task consists of choos-
ing a presentation sequence for a set of information
bearing elements. This task is well suited for text-
to-text generation like in single or multi-document
summarization (Barzilay et al, 2002). Recently
there has been a lot of work in discourse coher-
ence modeling (Lapata, 2003; Barzilay and Lap-
ata, 2005; Soricut and Marcu, 2006) that has used
173
information ordering to test the coherence mod-
els. The information-bearing elements here are sen-
tences rather than high-level concepts. This frees the
models from having to depend on a hard to get train-
ing corpus which has been hand-authored for con-
cepts.
Most of the dialogue models still work at the
higher abstraction level of dialogue acts and inten-
tions. But with an increasing number of dialogue
systems finding use in non-traditional applications
such as simulation training, games, etc.; there is a
need for dialogue models which do not depend on
hand-authored corpora or rules. Recently Gandhe
and Traum (2007) proposed dialogue models that
do not need annotations for dialogue-acts, seman-
tics and hand-authored rules for information state
updates or finite state machines.
Such dialogue models focus primarily on gener-
ating an appropriate coherent response given the di-
alogue history. In certain cases the generation of
a response can be reduced to selection from a set
of available responses. For such dialogue models,
maintaining the information state can be considered
as a secondary goal. The element that is common
to the information ordering task and the task of se-
lecting next most appropriate response is the ability
to express a preference for one sequence of dialogue
turns over the other. We propose to use the informa-
tion ordering task to test dialogue coherence models.
Here the information bearing units will be dialogue
turns.1
There are certain advantages offered by using in-
formation ordering as a task to evaluate dialogue co-
herence models. First the task does not require a
dialogue model to take part in conversations in an
interactive manner. This obviates the need for hav-
ing real users engaging in the dialogue with the sys-
tem. Secondly, the task is agnostic about the under-
lying dialogue model. It can be a data-driven statis-
tical model or information-state based, form based
or even a reinforcement learning system based on
MDP or POMDP. Third, there are simple objective
measures available to evaluate the success of infor-
mation ordering task.
Recently, Purandare and Litman (2008) have used
1These can also be at the utterance level, but for this paper
we will use dialogue turns.
this task for modeling dialogue coherence. But they
only allow for a binary classification of sequences
as either coherent or incoherent. For comparing dif-
ferent dialogue coherence models, we need the abil-
ity for finer distinction between sequences of infor-
mation being put together. Lapata (2003) proposed
Kendall?s ? , a rank correlation measure, as one such
candidate. In a recent study they show that Kendall?s
? correlates well with human judgment (Lapata,
2006). They show that human judges can reliably
provide coherence ratings for various permutations
of text. (Pearson?s correlation for inter-rater agree-
ment is 0.56) and that Kendall?s ? is a good in-
dicator for human judgment (Pearson?s correlation
for Kendall?s ? with human judgment is 0.45 (p <
0.01)).
Before adapting the information ordering task for
dialogues, certain questions need to be answered.
We need to validate that humans can reliably per-
form the task of information ordering and can judge
the coherence for different sequences of dialogue
turns. We also need to find which objective mea-
sures (like Kendall?s ? ) correlate well with human
judgments.
4 Evaluating Information Ordering
One of the advantages of using information order-
ing as a testbed is that there are objective measures
available to evaluate the performance of information
ordering task. Kendall?s ? (Kendall, 1938), a rank
correlation coefficient, is one such measure. Given
a reference sequence of length n, Kendall?s ? for an
observed sequence can be defined as,
? = # concordant pairs ? # discordant pairs# total pairs
Each pair of elements in the observed sequence
is marked either as concordant - appearing in the
same order as in reference sequence or as discor-
dant otherwise. The total number of pairs is Cn2 =
n(n? 1)/2. ? ranges from -1 to 1.
Another possible measure can be defined as the
fraction of n-grams from reference sequence, that
are preserved in the observed sequence.
bn = # n-grams preserved# total n-grams
In this study we have used, b2, fraction of bigrams
and b3, fraction of trigrams preserved from the ref-
erence sequence. These values range from 0 to 1.
Table 1 gives examples of observed sequences and
174
Observed Sequence b2 b3 ?
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9] 1.00 1.00 1.00
[8, 9, 0, 1, 2, 3, 4, 5, 6, 7] 0.89 0.75 0.29
[4, 1, 0, 3, 2, 5, 8, 7, 6, 9] 0.00 0.00 0.60
[6, 9, 8, 5, 4, 7, 0, 3, 2, 1] 0.00 0.00 -0.64
[2, 3, 0, 1, 4, 5, 8, 9, 6, 7] 0.56 0.00 0.64
Table 1: Examples of observed sequences and their re-
spective b2, b3 & ? values. Here the reference sequence
is [0,1,2,3,4,5,6,7,8,9].
respective b2, b3 and ? values. Notice how ? al-
lows for long-distance relationships whereas b2, b3
are sensitive to local features only. 2
5 Experimental Setup
For our experiments we used segments drawn from 9
dialogues. These dialogues were two-party human-
human dialogues. To ensure applicability of our
results over different types of dialogue, we chose
these 9 dialogues from different sources. Three of
these were excerpts from role-play dialogues involv-
ing negotiations which were originally collected for
a simulation training scenario (Traum et al, 2005).
Three are from SRI?s Amex Travel Agent data which
are task-oriented dialogues about air travel plan-
ning (Bratt et al, 1995). The rest of the dialogues are
scripts from popular television shows. Fig 6 shows
an example from the air-travel domain. Each excerpt
drawn was 10 turns long with turns strictly alternat-
ing between the two speakers.
Following the experimental design of (Lapata,
2006) we created random permutations for these di-
alogue segments. We constrained our permutations
so that the permutations always start with the same
speaker as the original dialogue and turns strictly al-
ternate between the speakers. With these constraints
there are still 5!? 5! = 14400 possible permutations
per dialogue. We selected 3 random permutations
for each of the 9 dialogues. In all, we have a total
of 27 dialogue permutations. They are arranged in 3
sets, each set containing a permutation for all 9 di-
alogues. We ensured that not all permutations in a
given set are particularly very good or very bad. We
used Kendall?s ? to balance the permutations across
2For more on the relationship between b2, b3 and ? see row
3,4 of table 1 and figure 4.
the given set as well as across the given dialogue.
Unlike Lapata (2006) who chose to remove the
pronouns and discourse connectives, we decided not
do any pre-processing on the text like removing
disfluencies or removing cohesive devices such as
anaphora, ellipsis, discourse connectives, etc. One
of the reason is such pre-processing if done manu-
ally defeats the purpose of removing humans from
the evaluation procedure. Moreover it is very diffi-
cult to remove certain cohesive devices such as dis-
course deixis without affecting the coherence level
of the original dialogues.
6 Experiment 1
In our first experiment, we divided a total of 9 hu-
man judges among the 3 sets (3 judges per set). Each
judge was presented with 9 dialogue permutations.
They were asked to assign a single coherence rat-
ing for each dialogue permutation. The ratings were
on a scale of 1 to 7, with 1 being very incoherent
and 7 being perfectly coherent. We did not provide
any additional instructions or examples of scale as
we wanted to capture the intuitive idea of coherence
from our judges. Within each set the dialogue per-
mutations were presented in random order.
We compute the inter-rater agreement by using
Pearson?s correlation analysis. We correlate the rat-
ings given by each judge with the average ratings
given by the judges who were assigned the same set.
For inter-rater agreement we report the average of 9
such correlations which is 0.73 (std dev = 0.07). Art-
stein and Poesio (2008) have argued that Krippen-
dorff?s ? (Krippendorff, 2004) can be used for inter-
rater agreement with interval scales like the one we
have. In our case for the three sets ? values were
0.49, 0.58, 0.64. These moderate values of alpha in-
dicate that the task of judging coherence is indeed a
difficult task, especially when detailed instructions
or examples of scales are not given.
In order to assess whether Kendall?s ? can be used
as an automatic measure of dialogue coherence, we
perform a correlation analysis of ? values against
the average ratings by human judges. The Pearson?s
correlation coefficient is 0.35 and it is statistically
not significant (P=0.07). Fig 1(a) shows the rela-
tionship between coherence judgments and ? val-
ues. This experiment fails to support the suitability
175
(a) Kendall?s ? does not correlate well with human
judgments for dialogue coherence.
(b) Fraction of bigram & trigram counts correlate well
with human judgments for dialogue coherence.
Figure 1: Experiment 1 - single coherence rating per permutation
of Kendall?s ? as an evaluation understudy.
We also analyzed the correlation of human judg-
ments against simple n-gram statistics, specifically
(b2 + b3) /2. Fig 1(b) shows the relationship be-
tween human judgments and the average of fraction
of bigrams and fraction of trigrams that were pre-
served in the permutation. The Pearson?s correlation
coefficient is 0.62 and it is statistically significant
(P<0.01).
7 Experiment 2
Since human judges found it relatively hard to as-
sign a single rating to a dialogue permutation, we
decided to repeat experiment 1 with some modifica-
tions. In our second experiment we asked the judges
to provide coherence ratings at every turn, based on
the dialogue that preceded that turn. The dialogue
permutations were presented to the judges through a
web interface in an incremental fashion turn by turn
as they rated each turn for coherence (see Fig 5 in
the appendix for the screenshot of this interface). We
used a scale from 1 to 5 with 1 being completely in-
coherent and 5 as perfectly coherent. 3 A total of 11
judges participated in this experiment with the first
set being judged by 5 judges and the remaining two
sets by 3 judges each.
3We believe this is a less complex task than experiment 1
and hence a narrower scale is used.
For the rest of the analysis, we use the average
coherence rating from all turns as a coherence rat-
ing for the dialogue permutation. We performed
the inter-rater agreement analysis as in experiment
1. The average of 11 correlations is 0.83 (std dev =
0.09). Although the correlation has improved, Krip-
pendorff?s ? values for the three sets are 0.49, 0.35,
0.63. This shows that coherence rating is still a hard
task even when judged turn by turn.
We assessed the relationship between the aver-
age coherence rating for dialogue permutations with
Kendall?s ? (see Fig 2(a)). The Pearson?s correlation
coefficient is 0.33 and is statistically not significant
(P=0.09).
Fig 2(b) shows high correlation of average coher-
ence ratings with the fraction of bigrams and tri-
grams that were preserved in permutation. The Pear-
son?s correlation coefficient is 0.75 and is statisti-
cally significant (P<0.01).
Results of both experiments suggest that,
(b2 + b3) /2 correlates very well with human judg-
ments and can be used for evaluating information
ordering when applied to dialogues.
8 Experiment 3
We wanted to know whether information ordering as
applied to dialogues is a valid task or not. In this ex-
periment we seek to establish a higher baseline for
176
(a) Kendall?s ? does not correlate well with human
judgments for dialogue coherence.
(b) Fraction of bigram & trigram counts correlate well
with human judgments for dialogue coherence.
Figure 2: Experiment 2 - turn-by-turn coherence rating
the task of information ordering in dialogues. We
presented the dialogue permutations to our human
judges and asked them to reorder the turns so that
the resulting order is as coherent as possible. All 11
judges who participated in experiment 2 also partic-
ipated in this experiment. They were presented with
a drag and drop interface over the web that allowed
them to reorder the dialogue permutations. The re-
ordering was constrained to keep the first speaker
of the reordering same as that of the original di-
alogue and the re-orderings must have strictly al-
ternating turns. We computed the Kendall?s ? and
fraction of bigrams and trigrams (b2 + b3) /2 for
these re-orderings. There were a total of 11 ? 9
= 99 reordered dialogue permutations. Fig 3(a)
and 3(b) shows the frequency distribution of ? and
(b2 + b3) /2 values respectively.
Humans achieve high values for the reordering
task. For Kendall?s ? , the mean of the reordered dia-
logues is 0.82 (std dev = 0.25) and for (b2 + b3) /2,
the mean is 0.71 (std dev = 0.28). These values es-
tablish an upper baseline for the information order-
ing task. These can be compared against the random
baseline. For ? random performance is 0.02 4 and
4Theoretically this should be zero. The slight positive bias
is the result of the constraints imposed on the re-orderings -
like only allowing the permutations that have the correct starting
speaker.
for (b2 + b3) /2 it is 0.11. 5
9 Discussion
Results show that (b2 + b3) /2 correlates well with
human judgments for dialogue coherence better than
Kendall?s ? . ? encodes long distance relationships
in orderings where as (b2 + b3) /2 only looks at lo-
cal context. Fig 4 shows the relationship between
these two measures. Notice that most of the order-
ings have ? values around zero (i.e. in the middle
range for ? ), whereas majority of orderings will have
a low value for (b2 + b3) /2. ? seems to overesti-
mate the coherence even in the absence of immedi-
ate local coherence (See third entry in table 1). It
seems that local context is more important for dia-
logues than for discourse, which may follow from
the fact that dialogues are produced by two speakers
who must react to each other, while discourse can be
planned by one speaker from the beginning. Traum
and Allen (1994) point out that such social obliga-
tions to respond and address the contributions of the
other should be an important factor in building dia-
logue systems.
The information ordering paradigm does not take
into account the content of the information-bearing
items, e.g. the fact that turns like ?yes?, ?I agree?,
5This value is calculated by considering all 14400 permuta-
tions as equally likely.
177
(a) Histogram of Kendall?s ? for reordered se-
quences
(b) Histogram of fraction of bigrams & tri-
grams values for reordered sequences
Figure 3: Experiment 3 - upper baseline for information ordering task (human performance)
?okay? perform the same function and should be
treated as replaceable. This may suggest a need to
modify some of the objective measures to evaluate
the information ordering specially for dialogue sys-
tems that involve more of such utterances.
Human judges can find the optimal sequences
with relatively high frequency, at least for short
dialogues. It remains to be seen how this varies
with longer dialogue lengths which may contain
sub-dialogues that can be arranged independently of
each other.
10 Conclusion & Future Work
Evaluating dialogue systems has always been a ma-
jor challenge in dialogue systems research. The core
component of dialogue systems, the dialogue model,
has usually been only indirectly evaluated. Such
evaluations involve too much human effort and are a
bottleneck for the use of data-driven machine learn-
ing models for dialogue coherence. The information
ordering task, widely used in discourse coherence
modeling, can be adopted as a testbed for evaluating
dialogue coherence models as well. Here we have
shown that simple n-gram statistics that are sensi-
tive to local features correlate well with human judg-
ments for coherence and can be used as an evalua-
tion understudy for dialogue coherence models. As
with any evaluation understudy, one must be careful
while using it as the correlation with human judg-
ments is not perfect and may be inaccurate in some
cases ? it can not completely replace the need for
full evaluation with human judges in all cases (see
(Callison-Burch et al, 2006) for a critique of BLUE
along these lines).
In the future, we would like to perform more ex-
periments with larger data sets and different types
of dialogues. It will also be interesting to see the
role cohesive devices play in coherence ratings. We
would like to see if there are any other measures or
certain modifications to the current ones that corre-
late better with human judgments. We also plan to
employ this evaluation metric as feedback in build-
ing dialogue coherence models as is done in ma-
chine translation (Och, 2003).
Acknowledgments
The effort described here has been sponsored by the U.S. Army
Research, Development, and Engineering Command (RDE-
COM). Statements and opinions expressed do not necessarily
reflect the position or the policy of the United States Govern-
ment, and no official endorsement should be inferred. We would
like to thank Radu Soricut, Ron Artstein, and the anonymous
SIGdial reviewers for helpful comments.
References
Ron Artstein and Massimo Poesio. 2008. Inter-coder
agreement for computational linguistics. In To appear
in Computational Linguistics.
Regina Barzilay and Mirella Lapata. 2005. Modeling
local coherence: An entity-based approach. In Proc.
ACL-05.
178
Regina Barzilay, Noemie Elhadad, and Kathleen McKe-
own. 2002. Inferring strategies for sentence ordering
in multidocument summarization. JAIR, 17:35?55.
Harry Bratt, John Dowding, and Kate Hunicke-Smith.
1995. The sri telephone-based atis system. In Pro-
ceedings of the Spoken Language Systems Technology
Workshop, January.
Chris Callison-Burch, Miles Osborne, and Philipp
Koehn. 2006. In proceedings of EACL-2006.
Wieland Eckert, Esther Levin, and Roberto Pieraccini.
1997. User modeling for spoken dialogue system eval-
uation. In Automatic Speech Recognition and Under-
standing, pages 80?87, Dec.
Sudeep Gandhe and David Traum. 2007. Creating spo-
ken dialogue characters from corpora without annota-
tions. In Proceedings of Interspeech-07.
Sudeep Gandhe, Andrew Gordon, and David Traum.
2006. Improving question-answering with linking di-
alogues. In International Conference on Intelligent
User Interfaces (IUI), January.
Kalliroi Georgila, James Henderson, and Oliver Lemon.
2006. User simulation for spoken dialogue systems:
Learning and evaluation. In proceedings of Inter-
speech.
James Henderson, Oliver Lemon, and Kallirroi Georgila.
2005. Hybrid reinforcement/supervised learning for
dialogue policies from communicator data. In pro-
ceedings of IJCAI workshop.
Kate S. Hone and Robert Graham. 2000. Towards a tool
for the subjective assessment of speech system inter-
faces (SASSI). Natural Language Engineering: Spe-
cial Issue on Best Practice in Spoken Dialogue Sys-
tems.
Daniel Jurafsky and James H. Martin. 2000. SPEECH
and LANGUAGE PROCESSING: An Introduction to
Natural Language Processing, Computational Lin-
guistics, and Speech Recognition. Prentice-Hall.
Maurice G. Kendall. 1938. A new measure of rank cor-
relation. Biometrika, 30:81?93.
Klaus Krippendorff. 2004. Content Analysis, An Intro-
duction to Its Methodology 2nd Edition. Sage Publi-
cations.
Mirella Lapata. 2003. Probabilistic text structuring: Ex-
periments with sentence ordering. In Proceedings of
the 41st Annual Meeting of the Association for Com-
putational Linguistics, Sapporo, Japan.
Mirella Lapata. 2006. Automatic evaluation of informa-
tion ordering. Computational Linguistics, 32(4):471?
484.
Esther Levin, Roberto Pieraccini, and Wieland Eckert.
1997. Learning dialogue strategies within the markov
decision process framework. In Automatic Speech
Recognition and Understanding, pages 72?79, Dec.
Chin-Yew Lin. 2004. ROUGE: a package for automatic
evaluation of summaries. In Proceedings of the Work-
shop on Text Summarization Branches Out.
Franz Josef Och. 2003. Minimum error rate training for
statistical machine translation. In In ACL 2003: Proc.
of the 41st Annual Meeting of the Association for Com-
putational Linguistics, July.
Kishore A. Papineni, Salim Roukos, Todd Ward, and
Wei-Jing Zhu. 2001. BLEU: a method for automatic
evaluation of machine translation. In Technical Re-
port RC22176 (W0109-022), IBM Research Division,
September.
Amruta Purandare and Diane Litman. 2008. Analyz-
ing dialog coherence using transition patterns in lexi-
cal and semantic features. In Proceedings 21st Inter-
national FLAIRS Conference, May.
Antonio Roque, Hua Ai, and David Traum. 2006. Evalu-
ation of an information state-based dialogue manager.
In Brandial 2006: The 10th Workshop on the Seman-
tics and Pragmatics of Dialogue.
Jost Schatzmann, Blaise Thomson, Karl Weilhammer,
Hui Ye, and Steve Young. 2007. Agenda-based user
simulation for bootstrapping a pomdp dialogue sys-
tem. In proceedings of HLT/NAACL, Rochester, NY.
Radu Soricut and Daniel Marcu. 2006. Discourse gener-
ation using utility-trained coherence models. In Proc.
ACL-06.
David R. Traum and James F. Allen. 1994. Discourse
obligations in dialogue processing. In proceedings of
the 32nd Annual Meeting of the Association for Com-
putational Linguistics (ACL-94), pages 1?8.
David R. Traum, Susan Robinson, and Jens Stephan.
2004. Evaluation of multi-party virtual reality dia-
logue interaction. In In Proceedings of Fourth Interna-
tional Conference on Language Resources and Evalu-
ation (LREC), pages 1699?1702.
David Traum, William Swartout, Jonathan Gratch, and
Stacy Marsella. 2005. Virtual humans for non-team
interaction training. In AAMAS-05 Workshop on Cre-
ating Bonds with Humanoids, July.
M. Walker, C. Kamm, and D. Litman. 2000. Towards de-
veloping general models of usability with PARADISE.
Natural Language Engineering: Special Issue on Best
Practice in Spoken Dialogue Systems.
Richard Wallace. 2003. Be Your Own Botmaster, 2nd
Edition. ALICE A. I. Foundation.
Joseph Weizenbaum. 1966. Eliza?a computer program
for the study of natural language communication be-
tween man and machine. Communications of the
ACM, 9(1):36?45, January.
Jason D. Williams and Steve Young. 2006. Partially ob-
servable markov decision processes for spoken dialog
systems. Computer Speech and Language, 21:393?
422.
Jason D. Williams. 2007. A method for evaluating and
comparing user simulations: The cramer-von mises di-
vergence. In IEEE Workshop on Automatic Speech
Recognition and Understanding (ASRU).
179
Appendix
(a) (b) (c)
Figure 4: Distributions for Kendall?s ? , (b2 + b3) /2 and the relationship between them for all possible dialogue
permutations with 10 turns and earlier mentioned constraints.
Figure 5: Screenshot of the interface used for collecting coherence rating for dialogue permutations.
180
Agent AAA at American Express may I help you?
User yeah this is BBB BBB I need to make some travel arrangements
Agent ok and what do you need to do?
User ok on June sixth from San Jose to Denver, United
Agent leaving at what time?
User I believe there?s one leaving at eleven o?clock in the morning
Agent leaves at eleven a.m. and arrives Denver at two twenty p.m. out of San Jose
User ok
Agent yeah that?s United flight four seventy
User that?s the one
Doctor hello i?m doctor perez
how can i help you
Captain uh well i?m with uh the local
i?m i?m the commander of the local company
and uh i?d like to talk to you about some options you have for relocating your clinic
Doctor uh we?re not uh planning to relocate the clinic captain
what uh what is this about
Captain well have you noticed that there?s been an awful lot of fighting in the area recently
Doctor yes yes i have
we?re very busy
we?ve had many more casual+ casualties many more patients than than uh usual in the
last month
but uh what what is this about relocating our clinic
have have uh you been instructed to move us
Captain no
but uh we just have some concerns about the increase in fighting xx
Doctor are you suggesting that we relocate the clinic
because we had no plans
we uh we uh we?re located here and we?ve been uh
we are located where the patients need us
Captain yeah but
yeah actually it is a suggestion that you would be a lot safer if you moved away from
this area
we can put you in an area where there?s n+ no insurgents
and we have the area completely under control with our troops
Doctor i see captain
is this a is this a suggestion from your commander
Captain i?m uh the company commander
Figure 6: Examples of the dialogues used to elicit human judgments for coherence
181
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 245?248,
The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational Linguistics
I?ve said it before, and I?ll say it again: An empirical investigation of the
upper bound of the selection approach to dialogue
Sudeep Gandhe and David Traum
Institute for Creative Technologies
13274 Fiji way, Marina del Rey, CA 90292
{gandhe,traum}@ict.usc.edu
Abstract
We perform a study of existing dialogue
corpora to establish the theoretical max-
imum performance of the selection ap-
proach to simulating human dialogue be-
havior in unseen dialogues. This maxi-
mum is the proportion of test utterances
for which an exact or approximate match
exists in the corresponding training cor-
pus. The results indicate that some do-
mains seem quite suitable for a corpus-
based selection approach, with over half of
the test utterances having been seen before
in the corpus, while other domains show
much more novelty compared to previous
dialogues.
1 Introduction
There are two main approaches toward automat-
ically producing dialogue utterances. One is the
selection approach, in which the task is to pick
the appropriate output from a corpus of possible
outputs. The other is the generation approach, in
which the output is dynamically assembled using
some composition procedure, e.g. grammar rules
used to convert information from semantic repre-
sentations and/or context to text.
The generation approach has the advantage of
a more compact representation for a given gener-
ative capacity. But for any finite set of sentences
produced, the selection approach could perfectly
simulate the generation approach. The generation
approach generally requires more analytical effort
to devise a good set of grammar rules that cover
the range of desired sentences but do not admit un-
desirable or unnatural sentences. Whereas, in the
selection approach, outputs can be limited to those
that have been observed in human speech. This
affords complex and human-like sentences with-
out much detailed analysis. Moreover, when the
output is not just text but presented as speech, the
system may easily use recorded audio clips rather
than speech synthesis. This argument also extends
to multi-modal performances, e.g. using artist an-
imation motion capture or recorded video for an-
imating virtual human dialogue characters. Often
one is willing to sacrifice some generality in or-
der to achieve more human-like behavior than is
currently possible from generation approaches.
The selection approach has been used for a
number of dialogue agents, including question-
answering characters at ICT (Leuski et al, 2006;
Artstein et al, 2009; Kenny et al, 2007), FAQ
bots (Zukerman and Marom, 2006; Sellberg and
Jo?nsson, 2008) and web-site information charac-
ters. It is also possible to use the selection ap-
proach as a part of the process, e.g. from words to
a semantic representation or from a semantic rep-
resentation to words, while using other approaches
for other parts of dialogue processing.
The selection approach presents two challenges
for finding an appropriate utterance:
? Is there a good enough utterance to select?
? How good is the selection algorithm at find-
ing this utterance?
We have previously attempted to address the sec-
ond question, by proposing the information or-
dering task for evaluating dialogue coherence
(Gandhe and Traum, 2008). Here we try to ad-
dress the first question, which would provide a
theoretical upper bound in quality for any selec-
tion approach. We examine a number of different
dialogue corpora as to their suitability for the se-
lection approach.
We make the following assumptions to allow
automatic evaluation across a range of corpora.
Actual human dialogues represent a gold-standard
for computer systems to emulate; i.e. choosing an
actual utterance in the correct place is the best pos-
sible result. Other utterances can be evaluated as
to how close they come to the original utterance,
245
using a similarity metric.
Our methodology is to examine a test corpus of
human dialogue utterances to see how well a se-
lection approach could approximate these, given a
training corpus of utterances in that domain. We
look at exact matches as well as utterances having
their similarity score above a threshold. We in-
vestigate the effect of the size of training corpora,
which lets us know how much data we might need
to achieve a certain level of performance. We also
investigate the effect of domain of training cor-
pora.
2 Dialogue Corpora
We examine human dialogue utterances from a va-
riety of domains. Our initial set contains six dia-
logue corpora from ICT as well as three other pub-
licly available corpora.
SGT Blackwell is a question-answering char-
acter who answers questions about the U.S. Army,
himself, and his technology. The corpus con-
sists of visitors interacting with SGT Blackwell at
an exhibition booth at a museum. SGT Star is
a question-answering character, like SGT Black-
well, who talks about careers in the U.S. Army.
The corpus consists of trained handlers present-
ing the system. Amani is a bargaining character
used as a prototype for training soldiers to perform
tactical questioning. The SASO system is a ne-
gotiation training prototype in which two virtual
characters negotiate with a human ?trainee? about
moving a medical clinic. The Radiobots system is
a training prototype that responds to military calls
for artillery fire. IOTA is an extension of the Ra-
diobots system. The corpus consists of training
sessions between a human trainee and a human in-
structor on a variety of missions. Yao et al (2010)
provides details about the ICT corpora.
Other corpora involved dialogues between
two people playing specific roles in planning,
scheduling problem for railroad transportation,
the Trains-93 corpus (Heeman and Allen, 1994)
and for emergency services, the Monroe corpus
(Stent, 2000). The Switchboard corpus (Godfrey
et al, 1992) consists of telephone conversations
between two people, based on provided topics.
We divided the data from each corpus into a
training set and a test set, as shown in Table 1. The
data consists of utterances from one or more hu-
man speakers who engage in dialogue with either
virtual characters (Radiobots, Blackwell, Amani,
Star, SASO) or other humans (Switchboard, Mon-
roe, IOTA, Trains-93). These corpora differ along
a number of dimensions such as the size of the
corpus, dialogue genre (question-answering, task-
oriented or conversational), types of tasks (ar-
tillery calls, moving and scheduling resources, in-
formation seeking) and motivation of the partici-
pants (exploring a new technology ? SGT Black-
well , presenting a demo ? SGT Star, undergo-
ing training ? Amani, IOTA or simply for collect-
ing the corpus ? Switchboard, Trains-93, Monroe).
While the set of corpora we include does not cover
all points in these dimensions, it does present an
interesting range.
3 Dialogue Utterance Similarity Metrics
To answer the question of whether an adequate
utterance exists in our training corpus that could
be selected and used, we need an appropriate-
ness measure. We assume that an utterance pro-
duced by a human in a dialogue is appropriate,
and thus the problem becomes one of construct-
ing an appropriate similarity function to compare
the human-produced utterance with the utterances
available from the training corpus. Given a train-
ing corpus Utrain and a similarity function f ,
we calculate the score for a test utterance ut as,
maxsimf (ut) = maxi f(ut, ui);ui ? Utrain
There are several choices for the utterance simi-
larity function f . Ideally such a function would
take meaning and context into account rather than
just surface similarity, but these aspects are harder
to automate, so for our initial experiments we look
at several surface metrics, as described below.
Exact measure returns 1 if the utterances are ex-
actly same and 0 otherwise. 1-WER, a similar-
ity measure related to word error rate, is defined
as min (0, 1? levenshtein(ut, ui)/length(ut)).
METEOR (Lavie and Denkowski, 2009), one of
the automatic evaluation metrics used in machine
translation is a good candidate for f . METEOR
finds optimal word-to-word alignment between
test and reference strings based on several modules
that match exact words, stemmed words and syn-
onyms. METEOR is a tunable metric and for our
analysis we used the default parameters tuned for
the Adequacy & Fluency task. All previous mea-
sures take into account the word ordering of test
and reference strings. In contrast, document simi-
larity measures used in information retrieval gen-
erally follow the bag of words assumption, where a
246
Domain
Train Test mean(maxsimf ) % of utterances
MET - METEOR
# utt words # utt words EOR 1-WER Dice Cosine Exact ? 0.9 ? 0.8
Blackwell 17755 84.7k 2500 12.0k 0.913 0.878 0.917 0.921 69.6 75.8 82.1
Radiobots 995 6.8k 155 1.2k 0.905 0.864 0.920 0.924 53.6 67.7 83.2
SGT Star 2974 16.6k 400 2.2k 0.897 0.860 0.906 0.911 65.0 70.5 78.0
SASO 3602 23.3k 510 3.6k 0.821 0.742 0.830 0.837 38.4 48.6 62.6
IOTA 4935 50.4k 650 5.6k 0.768 0.697 0.800 0.808 36.2 42.8 51.4
Trains 93 5554 47.2k 745 6.0k 0.729 0.633 0.758 0.769 34.5 36.9 42.8
SWBD1 19741 138.2k 3173 21.5k 0.716 0.628 0.736 0.753 35.8 37.9 44.2
Amani 1455 15.8k 182 1.9k 0.675 0.562 0.694 0.706 18.7 25.8 30.8
Monroe 5765 43.0k 917 8.8k 0.594 0.491 0.639 0.658 22.3 23.6 26.1
Table 1: Corpus details and within domain results
string is converted to a set of tokens. Here we also
considered Cosine and Dice coefficients using the
standard boolean model. In our experiments, the
surface text was normalized and all punctuation
was removed.
4 Experiments
Results Within a Domain
In our first experiment, we computed maxsimf
scores for all test corpus utterances in a given
domain using the training utterances from the
same domain. For the domains Blackwell, SGT
Star, SASO, Amani & Radiobots which are imple-
mented dialogue systems our corpus consists of
user utterances only. For Trains 93 and Monroe
corpora, we make sure to match the speaker roles
for ut and ui. For Switchboard, where speakers
do not have any special roles and for IOTA, where
the speaker information was not readily accessi-
ble, we ignore the speaker information and select
utterances from either speaker.
Table 1 reports the mean of maxsimf scores.
These can be interpreted as the expectation of
maxsimf score for a new test utterance. The
higher this expectation, the more likely it is that
an utterance similar to the new one has been
seen before and thus the domain will be more
amenable to selection approaches. This table
also shows the percentage of utterances that had
a maxsimMeteor score above a certain thresh-
old. The correlation between maxsimf for dif-
ferent choices of f (except Exact match) is very
high (Pearson?s r > 0.94). The histogram anal-
ysis shows that SGT Star, Blackwell, Radiobots
1Switchboard (SWBD) is a very large corpus and for run-
ning our experiments in a reasonable computing time we only
selected a small portion of it.
Figure 1: maxsimMeteor vs # utterances in train-
ing data for different domains
and SASO domains are better suited for selec-
tion approaches. Domains like Trains-93, Monroe,
Switchboard and Amani have a more diffuse dis-
tribution and are not best suited for selection ap-
proaches, at least with the amount of data we have
available. The IOTA domain falls somewhere in
between these two domain classes.
Effect of Training Data Size
Figure 1 shows the effect of training data size
on the maxsimMeteor score. Radiobots shows
very high scores even for small amounts of train-
ing data. SGT Star and SGT Blackwell also con-
verge fairly early. Switchboard, on the other hand,
does not achieve very high scores even with a
large number of utterances. For all domains, with
around 2500 training utterances maxsimMeteor
reaches 90% of its maximum possible value for
the training set.
Comparing Different Domains
In order to understand the similarities be-
tween different dialogue domains, we computed
maxsimMeteor for a test domain using training
247
Training Domains
IOTA Radio-
bots
SGT
Star
Black-
well
Amani SASO Trains-
93
Monroe SWBD
Te
st
in
g
D
om
ai
ns
IOTA 0.768 0.440 0.247 0.334 0.196 0.242 0.255 0.297 0.334
Radiobots 0.842 0.905 0.216 0.259 0.161 0.183 0.222 0.270 0.284
SGT Star 0.324 0.136 0.897 0.622 0.372 0.438 0.339 0.417 0.527
Blackwell 0.443 0.124 0.671 0.913 0.507 0.614 0.424 0.534 0.696
Amani 0.393 0.134 0.390 0.561 0.675 0.478 0.389 0.420 0.509
SASO 0.390 0.125 0.341 0.516 0.459 0.821 0.443 0.454 0.541
Trains 93 0.434 0.112 0.214 0.468 0.272 0.429 0.753 0.627 0.557
Monroe 0.409 0.119 0.217 0.428 0.276 0.404 0.534 0.630 0.557
SWBD 0.368 0.110 0.280 0.490 0.362 0.383 0.562 0.599 0.716
Table 2: Mean of maxsimMeteor for comparing different dialogue domains. The bold-faced values are
the highest in the corresponding row.
sets from other domains. In this exercise, we ig-
nored the speaker information. Table 2 reports
the mean values of maxsimMeteor for different
training domains. For all the testing domains,
using the training corpus from the same domain
produces the best results. Notice that Radiobots
also has good performance with the IOTA train-
ing data. This is as expected since IOTA is an
extension of Radiobots and should cover a lot of
utterances from the Radiobots domain. Switch-
board and Blackwell training corpora have a over-
all higher score for all testing domains. This may
be due to the breadth and size of these corpora. On
the other extreme, the Radiobots training domain
performs very poorly on all testing domains other
than itself.
5 Discussion
We have examined how well suited a corpus-
based selection approach to dialogue can succeed
at mimicking human dialogue performance across
a range of domains. The results show that such an
approach has the potential of doing quite well for
some domains, but much less well for others. Re-
sults also show that for some domains, quite mod-
est amounts of training data are needed for this
operation. Applying this method across corpora
from different domains can also give us a simi-
larity metric for dialogue domains. Our hope is
that this kind of analysis can help inform the de-
cision of what kind of language processing meth-
ods and dialogue architectures are most appropri-
ate for building a dialogue system for a new do-
main, particularly one in which the system is to
act like a human.
Acknowledgments
This work has been sponsored by the U.S. Army Re-
search, Development, and Engineering Command (RDE-
COM). Statements and opinions expressed do not necessarily
reflect the position or the policy of the United States Gov-
ernment, and no official endorsement should be inferred. We
would like to thank Ron Artstein and others at ICT for com-
piling the ICT Corpora used in this study.
References
R. Artstein, S. Gandhe, J. Gerten, A. Leuski, and D. Traum.
2009. Semi-formal evaluation of conversational charac-
ters. In Languages: From Formal to Natural. Essays Ded-
icated to Nissim Francez on the Occasion of His 65th
Birthday, volume 5533 of LNCS. Springer.
S. Gandhe and D. Traum. 2008. Evaluation understudy for
dialogue coherence models. In Proc. of SIGdial 08.
J. J. Godfrey, E. C. Holliman, and J. McDaniel. 1992.
Switchboard: Telephone speech corpus for research and
development. In Proc. of ICASSP-92, pages 517?520.
P. A. Heeman and J. Allen. 1994. The TRAINS 93 dialogues.
TRAINS Technical Note 94-2, Department of Computer
Science, University of Rochester.
P. Kenny, T. Parsons, J. Gratch, A. Leuski, and A. Rizzo.
2007. Virtual patients for clinical therapist skills training.
In Proc. of IVA 07, Paris, France. Springer.
A. Lavie and M. J. Denkowski. 2009. The meteor metric
for automatic evaluation of machine translation. Machine
Translation, 23:105?115.
A. Leuski, R. Patel, D. Traum, and B. Kennedy. 2006. Build-
ing effective question answering characters. In Proc. of
SIGdial 06, pages 18?27, Sydney, Australia.
L. Sellberg and A. Jo?nsson. 2008. Using random indexing to
improve singular value decomposition for latent semantic
analysis. In Proc. of LREC?08, Morocco.
A. J. Stent. 2000. The monroe corpus. Technical Report
728, Computer Science Dept. University of Rochester.
X. Yao, P. Bhutada, K. Georgila, K. Sagae, R. Artstein, and
D. Traum. 2010. Practical evaluation of speech recogniz-
ers for virtual human dialogue systems. In LREC 2010.
I. Zukerman and Y. Marom. 2006. A corpus-based approach
to help-desk response generation. In CIMCA/IAWTIC ?06.
248
Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 347?349,
Portland, Oregon, June 17-18, 2011. c?2011 Association for Computational Linguistics
Rapid Development of Advanced Question-Answering Characters
by Non-experts
Sudeep Gandhe and Alysa Taylor and Jillian Gerten and David Traum
USC Institute for Creative Technologies
12015 Waterfront Drive, Playa Vista, CA 90094, USA
<lastname>@ict.usc.edu
Abstract
We demonstrate a dialogue system and the ac-
companying authoring tools that are designed
to allow authors with little or no experience
in building dialogue systems to rapidly build
advanced question-answering characters. To
date seven such virtual characters have been
built by non-experts using this architecture
and tools. Here we demonstrate one such char-
acter, PFC Sean Avery, which was developed
by a non-expert in 3 months.
1 Introduction
Our goal is to allow non-experts to build advanced
question-answering Virtual Human characters. By
non-experts, we mean that scenario authors need not
have any background in computational linguistics
or any experience in building dialogue systems; al-
though they can be an expert in the specific domain
of interaction. The advanced question-answering
characters we want to build should have the abil-
ity to decide whether to answer a specific question
truthfully or to lie about it or to simply refuse to an-
swer depending on the dialogue context and scenario
design. Interviewers can practice their questioning
skills by interacting with virtual characters that can
engage in simple bargaining behavior. In order to
reveal certain information marked as sensitive, inter-
viewers may have to employ several different tactics
like building rapport, offering to provide what the
virtual character wants or threatening the character
with consequences for not cooperating. The Amani
character (Artstein et al, 2009) is an example ad-
vanced question-answering character.
Several different architectures have been used for
building Virtual Human dialogue systems (Traum,
2008). Leuski et al (2006; 2010) describe one
such architecture that can be used to build simple
question-answering characters. In this architecture,
scenario designers author a list of questions, a list
of answers and all possible links between these. Al-
though this approach allows non-experts to develop
virtual characters, it falls short of maintaining coher-
ence over sequence of utterances greater in length
than two. Traum et al (2008) describe an archi-
tecture that models such dependencies by using an
information-state based dialogue model (Traum and
Larsson, 2003), but this architecture is not currently
accessible to non-experts for authoring, due to ex-
pertise needed in designing task model plans and
thematic-role based framebanks.
We choose an intermediate approach, using a sim-
ple information-state dialogue manager, making au-
thoring accessible to non-experts through use of
the accompanying integrated authoring tool, Do-
mainEditor (Gandhe et al, 2009).
2 PFC Sean Avery
Private First Class (PFC) Sean Avery is a virtual
character who has witnessed a fellow soldier and
his accomplice smuggling something suspicious on
a U.S. Army base. Avery can be questioned about
what he saw, who the soldier and the accomplice
were. But the soldier involved in the smuggling is
from Avery?s platoon and Avery wants to cover up
the incident. He may choose to lie and will need
more persuasion in the form of threats & offers (see
figure 1).
347
Player ynq Are you Sean Avery?
Avery yes Yes
Avery assert My name is PFC Sean Avery.
Player whq Tell me what you saw.
Avery assert The soldier got out of the car and took a
package from the back seat.
Player whq Do you know who the soldier was?
Avery assert-
lie
I have no clue what the soldier?s name is.
Player threat If you don?t tell me your privileges will be
revoked.
Avery response-
threat
I don?t want to be on restriction
Player whq Who was the soldier?
Avery assert The soldier?s name is Ryan Benton.
Player whq Tell me more about the soldier.
Avery assert The PFC is in my platoon and one of my
closest friends.
Player whq What was inside the package?
Avery assert He was carrying a puppy.
Player whq What was inside the package?
Avery self-
repeat
As I said earlier,
Avery assert The suspicious package contained a
puppy.
Player whq Whose idea was it to smuggle the puppy
on the base.
Avery elicit-
offer
Will you give the soldier a less severe pun-
ishment if I talk?
Player accept yes
Avery response-
offer
That?s comforting, I wouldn?t want him to
get thrown off base or anything.
Avery assert I think the soldier told his girlfriend it
wouldn?t be a big deal to sneak a puppy
on base.
Figure 1: A sample dialogue with PFC Sean Avery char-
acter. Player refers to the human trainee.
3 Authoring Process
The authoring process for such characters has two
phases. The first phase is a top-down process which
begins with defining the character?s domain knowl-
edge using the authoring tool, DomainEditor (See
Figure 2). The basic unit of domain knowledge is
an <object, attribute, value> triple. DomainEditor
automatically generates all relevant Dialogue Acts
(DAs) following a dialogue act schema. The de-
fault schema was authored by experts specifically
Figure 2: DomainEditor: An Integrated Authoring tool
for designing the conversational domain, and specifying
the utterances that map to various dialogue acts.
for tactical questioning, but can be easily tailored
to add different types of DAs for other scenarios.
Each DA has a detailed XML representation and a
pseudo-natural language gloss generated using tem-
plates. E.g. a template like ?Attribute of Object is
Value? for an assert dialogue act type. The growth
in number of DAs represents the growth in charac-
ter?s domain knowledge (See figure 3). Our experi-
ence with several non-expert authors is that the do-
main reaches a stable level relatively early. Most of
the domain authoring occurs during this phase. Sce-
nario designers author one or two utterances for each
of the character?s DAs and substantially more exam-
ples for player?s DAs in order to ensure robust NLU
performance. These utterances are used as training
data for NLU and NLG.
The second phase is a bottom-up phase which in-
volves collecting a dialogue corpus by having vol-
unteers interview the virtual character that has been
built. The utterances from this corpus can then be
annotated with the most appropriate DA. This sec-
ond phase is responsible for a rapid growth in player
utterances. It can also lead to minor domain expan-
sion and small increase in character utterances, as
needed to cover gaps found in the domain knowl-
edge.
4 System Architecture
Figure 4 depicts the architecture for our dialogue
system. CMU pocketsphinx1 is used for speech
1http://cmusphinx.sourceforge.net/
348
Figure 3: Amount of resources collected across time for
the character, PFC Sean Avery
Figure 4: Architecture for the Advanced Question-
Answering Conversational Dialogue System
recognition and CereVoice (Aylett et al, 2006) for
speech synthesis. The information-state based dia-
logue manager (DM) communicates with NLU and
NLG using dialogue acts (DAs). NLU maps rec-
ognized speech to one of the DAs from the set that
is automatically generated by the DomainEditor. If
the confidence for the best candidate DA is below
a certain threshold, NLU generates a special non-
understanding DA ? unknown. The information-
state is in part based on conversational game the-
ory (Lewin, 2000). The main responsibilities of the
DM are to update the information state of the dia-
logue based on the incoming DA and to select the
response DAs. The information state update rules
describe grammars for conversational game struc-
ture and are written as state charts using SCXML2.
These state charts model various subdialogues like
question-answering, offer, threat, greetings, clos-
ings, etc. The DM also implements advanced fea-
tures like topic-tracking and grounding (Roque and
Traum, 2009). The virtual human character de-
2State Chart XML ? http://www.w3.org/TR/scxml/
Apache commons SCXML ? http://commons.apache.org/scxml
livers synthesized speech and corresponding non-
verbal behavior, based on additional components of
the ICT Virtual Human Toolkit3.
Acknowledgments
This work was sponsored by the U.S. Army Research, Devel-
opment, and Engineering Command (RDECOM). The content
does not necessarily reflect the position or the policy of the U.S.
Government, and no official endorsement should be inferred.
We would like to thank other members of the TACQ team who
helped design the architecture.
References
Ron Artstein, Sudeep Gandhe, Michael Rushforth, and
David Traum. 2009. Viability of a simple dialogue act
scheme for a tactical questioning dialogue system. In
proc. of 13th SemDial workshop : DiaHolmia.
M. P. Aylett, C. J. Pidcock, and M. E. Fraser. 2006. The
cerevoice blizzard entry 2006: A prototype database
unit selection engine. In Blizzard Challenge Work-
shop, Pittsburgh.
Sudeep Gandhe, Nicolle Whitman, David Traum, and
Ron Artstein. 2009. An integrated authoring tool for
tactical questioning dialogue systems. In 6th Work-
shop on Knowledge and Reasoning in Practical Dia-
logue Systems, Pasadena, California, July.
Anton Leuski and David R. Traum. 2010. NPCEditor:
A tool for building question-answering characters. In
proc. of LREC? 10.
Anton Leuski, Ronakkumar Patel, David Traum, and
Brandon Kennedy. 2006. Building effective question
answering characters. In Proceedings of the 7th SIG-
dial Workshop on Discourse and Dialogue, Australia.
I. Lewin. 2000. A formal model of conversational game
theory. In 4th SemDial workshop: Gotalog 2000.
Antonio Roque and David Traum. 2009. Improving a
virtual human using a model of degrees of grounding.
In Proceedings of IJCAI-09.
David Traum and Staffan Larsson. 2003. The informa-
tion state approach to dialogue management. In Jan
van Kuppevelt and Ronnie Smith, editors, Current and
New Directions in Discourse and Dialogue. Kluwer.
David Traum, William Swartout, Jonathan Gratch, and
Stacy Marsella, 2008. A Virtual Human Dialogue
Model for Non-Team Interaction, volume 39 of Text,
Speech and Language Technology. Springer.
David Traum. 2008. Talking to virtual humans: Dia-
logue models and methodologies for embodied con-
versational agents. In Ipke Wachsmuth and Gu?nther
Knoblich, editors, Modeling Communication with
Robots and Virtual Humans, pages 296?309. Springer.
3http://vhtoolkit.ict.usc.edu/
349
Proceedings of the SIGDIAL 2013 Conference, pages 251?260,
Metz, France, 22-24 August 2013. c?2013 Association for Computational Linguistics
Surface Text based Dialogue Models for Virtual Humans
Sudeep Gandhe and David Traum
USC Institute for Creative Technologies,
12015 Waterfront Drive, Playa Vista, CA 90094, USA
srgandhe@gmail.com, traum@ict.usc.edu
Abstract
We present virtual human dialogue mod-
els which primarily operate on the surface
text level and can be extended to incor-
porate additional information state annota-
tions such as topics or results from simpler
models. We compare these models with
previously proposed models as well as two
human-level upper baselines. The mod-
els are evaluated by collecting appropri-
ateness judgments from human judges for
responses generated for a set of fixed dia-
logue contexts. Our results show that the
best performing models achieve close to
human-level performance and require only
surface text dialogue transcripts to train.
1 Introduction
Virtual Humans (VH) are autonomous agents who
can play the role of humans in simulations (Rickel
and Johnson, 1999; Traum et al, 2005). For these
simulations to be convincing these agents must
have the ability to communicate with humans and
other agents using natural language. Like other di-
alogue system types, different architectures have
been proposed for virtual human dialogue sys-
tems. These architectures can afford different fea-
tures and require different sets of resources. E.g.,
an information state based architecture such as the
one used in SASO-ST (Traum et al, 2005) can
model detailed understanding of the task at hand
and progression of dialogue, but at the cost of re-
quiring resources such as information state update
rules and an annotated corpus or grammar to be
able to map surface text to dialogue acts.
For some virtual human dialogue genres such
as simple question-answering or some negotiation
domains, a simple model of dialogue progression
would suffice. In such a case we can build dia-
logue models that primarily operate on a surface
text level. These models only require surface text
dialogue transcripts as a resource, and don?t re-
quire expensive manual update rules, grammars,
or even extensive corpus annotation.
In this paper, we describe the construction and
evaluation of several models for engaging in dia-
logue by selecting an utterance that has been seen
previously in a corpus. We include one model that
has been used for this task previously (Gandhe and
Traum, 2007b), an adaptation of a model that has
been used in a similar manner, though on hand-
authored data sets, rather than data sets extracted
automatically from a corpus (Leuski and Traum,
2008), as well as a new set of models, using per-
ceptrons on surface text features as well as more
abstract information state annotations such as top-
ics. We also tackle the question of evaluating such
dialogue models manually as well as automati-
cally, starting with systematically analyzing var-
ious decisions involved in the evaluation process.
We situate our work with respect to previous eval-
uation methods.
2 Related Work
The task of a dialogue model is to formulate an
utterance given a dialogue context. There are
two approaches towards formulating an utterance:
Generation, where a response is compositionally
created from elements of the information state,
including the context of previous utterances, and
Selection, where a response is chosen from pre-
viously seen set of responses. In (Gandhe and
Traum, 2010), we examined the theoretical poten-
tial for the selection approach, looking at a wide
variety of domains, and evaluating based on sim-
ilarity between the actual utterance and the best
match in the previously seen corpus. We saw a
wide variance in scores across domains, both as to
the similarity scores and improvement of scores as
more data is considered. For task-oriented plan-
ning domains, such as Monroe (Stent, 2000) and
251
TRAINS (Heeman and Allen, 1994), as well as
open conversation in Switchboard (Godfrey et al,
1992), the performance was very low. On the other
hand, for more limited domains such as simple
question-answering (Leuski et al, 2006) or role-
play negotiation in a scenario, the performance
was high, with METEOR scores averaging over
0.8.
One possible selection criterion is to assume
that the most appropriate response is the most
probable response according to a model trained
on human-human dialogues. More formally, let
there be a dialogue ?u1, u2, . . . , ut?1, ut, . . . , uT ?,
where utterance ut appears in contextt =
?u1, u2, . . . , ut?1?. If we have a dialogue model
P estimated from the training corpus then the for-
mulated response uq for some unseen contextq is
given by,
ut = argmax
i
P (ui|contextt) ?ui ? Upossible
where Upossible is a set of all possible response ut-
terances. Ideally we would like to estimate a prob-
ability distribution P , but since it?s hard to esti-
mate and we only need argmax for this applica-
tion, we approximate P with a ranking function.
We can compare previous work within this frame-
work.
In our previous work (Gandhe and Traum,
2007a), we used context similarity as the rank-
ing function P (see section 3.1 for details). This
model is trained from in-domain surface text di-
alogue transcripts. Leuski et al (2006) model P
as cross-lingual relevance, where the task of se-
lecting an appropriate response is seen as cross-
lingual information retrieval where the response
utterance ut is the relevant document and the
contextt is treated as a query from different lan-
guage. This model has been applied to simple
question answering where context is the previous
utterance and the training data is manually anno-
tated question-answer pairs. DeVault et al (2011)
have proposed to use a multi-class classification
model (such as maximum entropy) for estimat-
ing P . Their method restricts the set Upossible
to a set of canonical utterances which represent
distinct dialogue acts. This allows for a limited
number of classes (|Upossible|) and also maximizes
the number of distinct contexts seen per utterance.
This model is also trained from manually anno-
tated utterance-context pairs and can additionally
use manually created utterance paraphrases.
Apart from the models discussed above which
have been mainly applied to dialogue domains
situated in a story context, there has been some
work in surface text based dialogue models for
open domains. Ritter et al (2011) use informa-
tion retrieval based and statistical machine trans-
lation (SMT) based approaches towards predicting
the next response in Twitter conversations. Also
Chatbots typically use surface text based process-
ing such as string transformations (e.g., AIML
rules (Wallace, 2003)). Such rules can also be
learned from a dialogue corpus (Abu Shawar and
Atwell, 2005). Systems employing SMT or string
transformation rules are formulating a response
by Generation approach and it can be frequently
ungrammatical or incoherent, unlike the selection
approach which will always pick something that
someone has once said (even though it might be
inappropriate in the current context).
3 Dialogue Models
3.1 Nearest Context
In previous work (Gandhe and Traum, 2007a), we
modeled P as,
P (ui|contextq) ? Sim(contexti, contextq)
where contexti is the context in which utterance
ui was seen in training corpus and Sim is con-
text similarity in a customized vector-space model.
The model restricts the set of possible response
utterances (Upossible) to the set of utterances ob-
served in the training data (Utrain). The context
is approximated using the previous two utterances
(one from each speaker). This model does not use
the contents of the utterance ui itself.
3.2 Cross-lingual Relevance Model
Leuski et al (2006) model P as a cross-lingual rel-
evance model. This model takes into account the
content of the utterance ui as well as the content of
the context. It does not impose any restriction on
Upossible, but in practice it is restricted to the set
of utterances in the training data. The model al-
lows the context to be composed of multiple fields,
each with its own weight. This allows us to ex-
tend the model where the context is approximated
by the previous two utterances. The weights need
to be learned using a held-out development set,
which presents a challenge in the case of multiple
fields (possible if we add more information state
annotations), modest amounts of training data and
252
non-availability of an automatic and reliable esti-
mate of the model?s performance. Here, for the
first time, we apply this model to automatically
extracted pairs of utterance-context and evaluate
it. For our model we used the implementation that
is available as a part of NPCEditor (Leuski and
Traum, 2011) and manually set the field weights
corresponding to the two previous utterances to be
equal (0.5).
3.3 Perceptron
As discussed earlier, the task of selecting the most
appropriate response can be viewed as multi-class
classification. But there are a couple of issues.
First, since we operate at the surface text level,
each unique response utterance will be labeled as
a separate class. The number of classes is the
number of unique utterances seen in the training
set, which is relatively large. As the training data
grows, the number of classes will increase. Sec-
ond, there are very few examples (on average a
single example) per class. We need a classifier that
can overcome these issues.
The perceptron algorithm and its variants ?
voted perceptron and averaged perceptron are
well known classification models (Freund and
Schapire, 1999). They have been extended for use
in various natural language processing tasks such
as part-of-speech tagging (Collins, 2002), pars-
ing (Collins, 2004) and discriminative language
modeling (Roark et al, 2007). Here we use the
averaged perceptron model for mapping from dia-
logue context to an appropriate response utterance.
Collins (2002) outlines the following four com-
ponents of a perceptron model:
? The training data. In our case it is a set of au-
tomatically extracted utterance-context pairs
{. . . , ?ui, contexti?, . . .}
? A function GEN(context) that enumerates a
set of all possible outputs (response utter-
ances) for any possible input (dialogue con-
text)
? A feature extraction function ? :
?u, context? ? Rd that is defined over
all possible pairings of response utterances
and dialogue contexts. d is the total number
of possible features.
? A parameter vector ?? ? Rd
Using such a perceptron model, the most appropri-
ate response utterance (ut) for the given dialogue
context (contextt) is given by,
uq = argmax
ui?GEN(context)
?(ui, contextq) ? ??
Algorithm 1 Perceptron Training Algorithm
Initialize: t? 0 ; ??0 ? 0
for iter = 1 to MAX ITER do
for i = 1 to N do
ri ? argmaxu?GEN(contexti) ?(u, contexti) ? ??t
if ri 6= ui then
??t+1 ? ??t + ?(ui, contexti) ?
?(ri, contexti)
else
??t+1 ? ??t
end if
t? t+ 1
end for
end for
return ??? (?t ??t)/(MAX ITER?N)
The parameter vector ?? is trained using the
training algorithm described in Algorithm 1. The
algorithm goes through the training data one in-
stance at a time. For every training instance, it
computes the best response utterance (ri) for the
context based on its current estimate of the param-
eter vector ??t. The algorithm changes the param-
eter vector only if it makes an error (ri 6= ui). The
update drives the parameter vector away from the
error (ri) and towards the correct output (ui). The
final parameter vector ?? is an average of all the in-
termediate ??t values. The averaging of parameter
vectors avoids overfitting.
The feature extraction function ? can list any
arbitrary features from the pair ?u, context?. We
consider information state annotations (ISt) along
with the surface text corresponding to the previous
two turns. The features could also include scores
computed from other models, such as those pre-
sented in sections 3.1 and 3.2. Figure 1 illustrates
an example context and utterance, and several fea-
tures. We examine several sets of features, Surface
text based features (?S), Retrieval model based
features (?R), and Topic based features (?T ).
Surface text based features (?S) are the fea-
tures extracted from the surface text of the previ-
ous utterances in the dialogue context (contextj)
and the response utterance (ui). ?S(d)(ux, uy) ex-
tracts surface text features from two utterances ? a
response utterance (ux) and an utterance (uy) from
the context that is (d) utterances away. There are
four types of features we extract:
253
? common term(d,w) features indicate the
number of times a wordw appears in both the
utterances. The total number of possible fea-
tures is O(|V |) and we select a small subset
of words (Selected common(d)) from the
vocabulary.
? The common term count(d) feature indi-
cates the number of words that appear in both
utterances.
? The unique common term count(d) fea-
ture indicates the number of unique words
that appear in both utterances.
? cross term(d,wx, wy) features indicate the
number of times the word wx appears in the
utterance ux and the word wy appears in the
utterance uy. The total possible number of
such cross features is very large (O(|V |2)),
where |V | is the utterance vocabulary size.
In order to keep the training tractable and
avoid overfitting, we select a small subset of
cross features (Selected cross(d)) from all
possible features.
In this model, we perform feature selection by
selecting the subsets Selected cross(d) and
Selected common(d). The training algorithm re-
quires evaluating the feature extraction (?S) func-
tion for all possible pairings of response utterances
and contexts. One simple feature selection crite-
rion is to allow the features only appearing in true
pairings of response utterance and context (i.e.
features from ?S(?ui, contextj?) ?i = j). The
subset Selected common(d) for common term
features is selected by extracting features from
only such true pairings.
For selecting cross term(d,wx, wy) features
we use only true pairings but we need to
reduce this subset even further. We im-
pose additional constraints based on the col-
lection frequency of lexical events such as,
cf(wx) > thresholdx, cf(wy) > thresholdy,
cf(?wx, wy?) > thresholdxy. Further reduction
in size of the selected subset of cross term fea-
tures is achieved by ranking the features using a
suitable ranking function and choosing the top n
features. In this model, we rank the cross term
features based on pointwise mutual-information
pmi(?wx, wy?) given by,
log p(?wx, wy?)p(wx)p(wy)
= log
(
#?wx,wy?
#??,??
)
(
#?wx,??
#??,??
)
?
(
#??,wy?
#??,??
)
Summing up, ?S(d)(ux, uy) =
{cross term(d,wx, wy) : wx ? ux?
wy ? uy ? ?wx, wy? ? Selected cross(d)}
? {common term(d,w) : w ? ux ?w ? uy ?
w ? Selected common(d)}
? {common term count(d)}
? {unique common term count(d)}
Retrieval model based features (?R) are
the scores computed in a fashion similar to
the Nearest Context model. Sim(ux, uy) is
a cosine similarity function for tf-idf weighted
vector space representations of utterances and
Sim(contexta, contextb) is the same function
from Nearest Context model. We define three fea-
tures,
? retrieval score =
|L|max
k=1
Sim(contextj , contextk) ? Sim(ui, uk)
? context sim@best utt match =
Sim(contextj , contextb)
where, b = |L|argmax
k=1
Sim(ui, uk)
? utt sim@best context match = Sim(ui, ub)
where, b = |L|argmax
k=1
Sim(contextj , contextk)
?R(?ui, contextj?) = {retrieval score,
context sim@best utt match,
utt sim@best context match}
Topic based feature (?T ) tracks the topic sim-
ilarity between the topic of the dialogue context
and the response utterance. A topic is marked
as mentioned if a set of keywords triggering that
topic have been previously mentioned in the dia-
logue. Each information state (IS) consists of a
topic signature which can be viewed as a boolean
vector representing mentions of topics.
?T (?ui, contextj?) = {topic similarity}
topic similarity = cosine(ISi, ISj)
where, ISi is the topic and is part of contexti
which is the context associated with the utterance
ui.
The perceptron model presented here allows
novel combinations of resources such as combin-
ing surface text transcripts with information state
annotations for tracking topics in the conversa-
tion. As compared to the generative cross-lingual
relevance model approach, the perceptron model
is a discriminative model. It is also a paramet-
ric model and the inference requires linear time
with respect to the size of candidate utterances
(|GEN(context)|) and the number of features (|??|).
Although, computing some of the features them-
selves (e.g., ?R features) requires linear time with
254
...
contextj [uj(?2)] Doctor you are the threat i need protection from you
[uj(?1)] Captain no no
you do you do not need protection from me
i am here to help you
uh what i would like to do is move your your clinic to a safer location
and uh give you money and medicine to help build it
utterance [ui] Doctor i have no way of moving
?S(?ui, contextj?) = { cross term(?2, ?moving?, ?need?) = 1,
common term(?2, ?i?) = 1,
common term count(?2) = 1, unique common term count(?2) = 1,
cross term(?1, ?moving?, ?give?) = 1,
common term(?1, ?i?) = 1, common term(?1, ?no?) = 1,
common term count(?1) = 2, unique common term count(?1) = 2,
retrieval score = 0.198, context sim@best utt match = 0.198,
utt sim@best context match = 0,
topic similarity = 0.667 }
Figure 1: Features extracted from a context (contextj) and a response utterance (ui)
respect to the size of the training data. The per-
ceptron model can rank an arbitrary set of utter-
ances given a dialogue context. But some of the
features (e.g., topic similarity) require that the
utterance ui(ui ? |GEN(context)|) be associated
with a known context (contexti). For all our mod-
els we use GEN(context) = Utrain.
We have implemented three different vari-
ants of the perceptron model based on the
choice of features used. Perceptron(surface)
model uses only surface text features (? =
?S). The other two models are Percep-
tron(surface+retrieval) where ? = ?S ? ?R and
Perceptron(surface+retrieval+topic) where ? =
?S ? ?R ? ?T .
Figure 2 shows a schematic representation of
these models along with the set of resources be-
ing used by each model. The figure also shows the
relationships between these models. The arrows
point from a less informative model to a more in-
formative model and the annotations on these ar-
rows indicate the additional information used.
4 Evaluation
For the experiments reported in this paper, we
used the human-human spoken dialogue corpus
collected for the project SASO-ST (Traum et al,
2005). In this scenario, the trainee acts as an
Army Captain negotiating with a simulated doc-
Figure 2: A schematic representation of imple-
mented unsupervised dialogue models and the re-
lationships between the information used by their
ranking functions.
tor to convince him to move his clinic to another
location. The corpus is a collection of 23 roleplay
dialogues and 13 WoZ dialogues lasting an aver-
age of 40 turns (a total of ? 1400 turns and ? 30k
words).
We perform a Static Context evalua-
tion (Gandhe, 2013). In Static Context evaluation,
all the dialogue models being evaluated receive
the same set of contexts as input. These dialogue
contexts are extracted from actual in-domain
255
human-human dialogues and are not affected by
the dialogue model being evaluated. For every
turn whose role is to be played by the system, we
predict the most appropriate response in place of
that turn given the dialogue context.
Since the goal for virtual humans is to be as
human-like as possible, a suitable evaluation met-
ric is how appropriate or human-like the responses
are for a given dialogue context. The evaluation
reported here employs human judges. We set up a
simple subjective 5-point likert scale for rating ap-
propriateness ? 1 being a very inappropriate non-
sensical response and 5 being a perfectly appropri-
ate response.
We built five dialogue models to play the role
of the doctor in SASO-ST domain, viz.: Near-
est Context (section 3.1), Cross-lingual Relevance
Model (section 3.2) and three perceptron models
(section 3.3) with different feature sets. These
dialogue models are evaluated using 5 in-domain
human-human dialogues from the training data (2
roleplay and 3 WoZ dialogues, referred to as test
dialogues). A dialogue model is trained in a leave-
one-out fashion where the training data consists of
all dialogues except the one test dialogue that is
being evaluated. A dialogue model trained in this
fashion is then used to predict the most appropri-
ate response for every context that appears in the
test dialogue. This process is repeated for each test
dialogue and for each dialogue model being evalu-
ated. In this evaluation setting, the actual response
utterance found in the original human-human dia-
logue may not belong to the set of utterances being
ranked by the dialogue model. We also compare
these five dialogue models with two human-level
upper baselines. Figure 4 in the appendix shows
some examples of utterances returned by a couple
of the models.
4.1 Human-level Upper Baselines
In order to establish an upper baseline for human-
level performance for the evaluation task, we con-
ducted a wizard data collection. We asked human
volunteers (wizards) to perform a similar task to
that performed by the dialogue models being eval-
uated. The wizard is presented with a set of ut-
terances (Utrain) and is asked to select a subset
from these that will be appropriate as a response
for the presented dialogue context. Compared to
this, the task of the dialogue model is to select
a single most appropriate response for the given
context.
DeVault et al (2011) carried out a similar wiz-
ard data collection but at the dialogue act level,
where wizards were asked to select only one re-
sponse dialogue act for each dialogue context.
Their findings suggest that there are several valid
response dialogue acts for a dialogue context. A
specific dialogue act can be realized in several
ways at the surface text level. For these reasons
we believe that for a given dialogue context there
are often several appropriate response utterances
at the surface text level. In our setting the dia-
logue models work at the surface text level and
hence the wizards were asked to select a subset of
surface text utterances that would be appropriate
responses. Each wizard was asked to select sev-
eral (ideally between five and ten, but always at
least one) appropriate responses for each dialogue
context. Four wizards participated in this data col-
lection with each wizard selecting responses for
the contexts from the same five human-human test
dialogues. The set of utterances to chose from
(Utrain) for every test dialogue was built in the
same leave-one-out fashion as used for evaluating
the implemented dialogue models.
There are a total of 89 dialogue contexts where
the next turn belongs to doctor. As expected, wiz-
ards frequently chose multiple utterances as ap-
propriate responses (mean = 7.80, min = 1, max
= 25).
This data collected from wizards is used to build
two human-level upper-baseline models for the
task of selecting a response utterance given a di-
alogue context:
Wizard Max Voted model returns the response
which gets the maximum number of votes
from the four wizards. Ties are broken
randomly.
Wizard Random model returns a random utter-
ance from the list of all utterances marked as
appropriate by one of the wizards.
4.2 Comparative Evaluation of Models
We performed a static context evaluation using
four judges for the above-mentioned two human-
level baselines (Wizard Random and Wizard Max
Voted) and five dialogue models (Nearest Con-
text, Cross-lingual Relevance Model and three
perceptron models), as described in section 3.3.
We tune the parameters used for the perceptron
256
models based on the automatic evaluation met-
ric, Weak Agreement (DeVault et al, 2011). Ac-
cording to this evaluation metric a response utter-
ance is judged as perfectly appropriate (a score
of 5) if any of the wizards chose this response
utterance for given context and inappropriate (a
score of 0) otherwise. The Perceptron(surface)
model was trained using 30 iterations, the Per-
ceptron(surface+retrieval) using 20 iterations,
and the Perceptron(surface+retrieval+topic) was
trained using 25 iterations. For all perceptron
models we used thresholdx = thresholdy =
thresholdxy = 3.
For a comparative evaluation of dialogue mod-
els, we need an evaluation setup where judges
could see the complete dialogue context along
with the response utterances generated by the di-
alogue models to be evaluated. In this setup, we
show all the response utterances next to each other
for easy comparison and we do not show the ac-
tual response utterance that was encountered in
the original human-human dialogue. We built a
web interface for collecting appropriateness rat-
ings that addresses the above requirements. Fig-
ure 3 shows the web interface used by the four
judges to evaluate the appropriateness of response
utterances for given dialogue context. The appro-
priateness was rated on the same scale of 1 to 5.
The original human-human dialogue (roleplay or
WoZ) is shown on the left hand side and the re-
sponse utterances from different dialogue models
are shown on the right hand side. In cases where
different dialogue models produce the same sur-
face text response only one candidate surface text
is shown to judge. Once the judge has rated all the
candidate responses they can proceed to the next
dialogue context. This setting allows for compar-
ative evaluation of different dialogue models. The
presentation order of responses from different di-
alogue models is randomized. Two of the judges
also performed the role of the wizards in our wiz-
ard data collection as outlined in section 4.1, but
the wizard data collection and the evaluation tasks
were separated by a period of over 3 months.
Table 1 shows the results of our compara-
tive evaluation for each judge and averaged over
all judges. We also computed inter-rater agree-
ment for individual ratings for all response ut-
terances using Krippendorff?s ? (Krippendorff,
2004). There were a total of n = 397 distinct
response utterances that were judged by the eval-
uators. The Krippendorff?s ? for all four judges
was 0.425 and it ranges from 0.359 to 0.495 for
different subsets of judges. The value of ? indi-
cates that the inter-rater agreement is substantially
above chance (? > 0), but indicates a fair amount
of disagreement, indicating that judging appropri-
ateness is a hard task even for human judges. Al-
though there is low inter-rater agreement at the
individual response utterance level there is high
agreement at the dialogue model level. Pearson?s
correlation between the average appropriateness
for different dialogue models ranges from 0.928
to 0.995 for different pairs of judges.
We performed a paired Wilcoxon test to check
for statistically significant differences in differ-
ent dialogue models. Wizard Max Voted is sig-
nificantly more appropriate than all other models
(p < 0.001). Wizard Random is significantly more
appropriate than Cross-lingual Relevance Model
(p < 0.05) and significantly more appropriate
than the three perceptron models as well as Near-
est Context model (p < 0.001). Cross-lingual
Relevance Model is significantly more appropri-
ate than Nearest Context (p < 0.01). All other
differences are not statistically significant at the 5
percent level.
We found that adding topic annotations did not
help. This is in contrast with previous observa-
tion (Gandhe and Traum, 2007b), where topic in-
formation helped when evaluation was performed
in Dynamic Context setting. In Dynamic Context
setting, the dialogue model is used in an online
fashion where the response utterances it generates
become part of the dialogue contexts with respect
to which the subsequent responses are predicted
and evaluated. The topic information ensures sys-
tematic progression of dialogue. But for static
context evaluation such help is not required as the
dialogue contexts are extracted from human hu-
man dialogues and are fixed.
5 Conclusion
In this paper we introduced dialogue models that
can be trained simply from in-domain surface
text dialogue transcripts. Some of these models
also allow for incorporating additional informa-
tion state features such as topics or results of sim-
pler models. We have evaluated the appropriate-
ness of responses and have compared these mod-
els with two human-level baselines. Evaluating
response appropriateness is highly subjective as
257
Figure 3: Screenshot of the user interface for static context comparative evaluation of dialogue models
Model #Utts Avg. appropriateness Appropriateness(All judges)
Judge 1 Judge 2 Judge 3 Judge 4 Avg stddev
Nearest Context 89 4.12 3.98 3.40 3.53 3.76 1.491
Perceptron(surface) 89 3.97 4.11 3.51 3.62 3.80 1.445
Perceptron
(surface+retrieval)
89 4.26 4.12 3.51 3.72 3.90 1.414
Perceptron
(surface+retrieval+topic)
89 4.21 4.09 3.51 3.57 3.85 1.433
Cross-lingual Relevance
Model
89 4.28 4.31 3.70 3.91 4.05 1.314
Wizard Random 89 4.55 4.55 4.03 4.16 4.32 1.153
Wizard Max Voted 89 4.76 4.84 4.40 4.52 4.63 0.806
Table 1: Offline comparative evaluation of dialogue models.
can be seen from the fact that utterances which
receive more wizard votes (Wizad Max Voted) re-
ceive significantly higher appropriateness ratings
than those which receive fewer votes (Wizard Ran-
dom). The performance of best performing dia-
logue models are close to human-level baselines.
In future we plan to use larger datasets which
should be easy, since no additional annotations are
required for training these dialogue models.
Acknowledgments
The effort described here has been sponsored by
the U.S. Army. Any opinions, content or informa-
tion presented does not necessarily reflect the posi-
tion or the policy of the United States Government,
and no official endorsement should be inferred.
References
Bayan Abu Shawar and Eric Atwell. 2005. Using cor-
pora in machine-learning chatbot systems. Interna-
tional Journal of Corpus Linguistics, 10:489?516.
258
Michael Collins. 2002. Discriminative training meth-
ods for hidden markov models: theory and experi-
ments with perceptron algorithms. In Proceedings
of the ACL-02 conference on Empirical methods in
natural language processing - Volume 10, EMNLP
?02, pages 1?8, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Michael Collins, 2004. Parameter estimation for sta-
tistical parsing models: theory and practice of
distribution-free methods, pages 19?55. Kluwer
Academic Publishers, Norwell, MA, USA.
David DeVault, Anton Leuski, and Kenji Sagae. 2011.
Toward learning and evaluation of dialogue policies
with text examples. In Proceedings of the SIGDIAL
2011 Conference, pages 39?48, Portland, Oregon,
June. Association for Computational Linguistics.
Yoav Freund and Robert E. Schapire. 1999. Large
margin classification using the perceptron algorithm.
Mach. Learn., 37:277?296, December.
Sudeep Gandhe and David Traum. 2007a. Creating
spoken dialogue characters from corpora without an-
notations. In Proceedings of Interspeech-07.
Sudeep Gandhe and David Traum. 2007b. First steps
towards dialogue modeling from an un-annotated
human-human corpus. In 5th Workshop on knowl-
edge and reasoning in practical dialogue systems,
Hyderabad, India.
Sudeep Gandhe and David Traum. 2010. I?ve said it
before, and i?ll say it again: an empirical investiga-
tion of the upper bound of the selection approach to
dialogue. In Proceedings of the 11th Annual Meet-
ing of the Special Interest Group on Discourse and
Dialogue, SIGDIAL ?10, pages 245?248, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Sudeep Gandhe. 2013. Rapid prototyping and evalu-
ation of dialogue systems for virtual humans. Ph.D.
thesis, University of Southern California.
J. J. Godfrey, E. C. Holliman, and J. McDaniel. 1992.
Switchboard: Telephone speech corpus for research
and development. In Proc. of ICASSP-92, pages
517?520.
P. A. Heeman and J. Allen. 1994. The TRAINS 93 di-
alogues. TRAINS Technical Note 94-2, Department
of Computer Science, University of Rochester.
Klaus Krippendorff. 2004. Content Analysis, An Intro-
duction to Its Methodology 2nd Edition. Sage Pub-
lications.
Anton Leuski and David Traum. 2008. A statistical
approach for text processing in virtual humans. In
Proccedings of 26th Army Science Conference.
Anton Leuski and David Traum. 2011. Npceditor:
Creating virtual human dialogue using information
retrieval techniques. AI Magazine, 32(2):42?56.
Anton Leuski, Ronakkumar Patel, David Traum, and
Brandon Kennedy. 2006. Building effective ques-
tion answering characters. In Proceedings of the
7th SIGdial Workshop on Discourse and Dialogue,
pages 18?27, Sydney, Australia, July. Association
for Computational Linguistics.
Jeff Rickel and W. Lewis Johnson. 1999. Virtual hu-
mans for team training in virtual reality. In Proceed-
ings of the Ninth International Conference on Artifi-
cial Intelligence in Education, pages 578?585. IOS
Press.
Alan Ritter, Colin Cherry, and William B. Dolan. 2011.
Data-driven response generation in social media. In
Proceedings of the Conference on Empirical Meth-
ods in Natural Language Processing, EMNLP ?11,
pages 583?593, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Brian Roark, Murat Saraclar, and Michael Collins.
2007. Discriminative n-gram language modeling.
Comput. Speech Lang., 21:373?392, April.
A. J. Stent. 2000. The monroe corpus. Technical
Report 728, Computer Science Dept. University of
Rochester.
David Traum, William Swartout, Jonathan Gratch, and
Stacy Marsella. 2005. Virtual humans for non-
team interaction training. In AAMAS-05 Workshop
on Creating Bonds with Humanoids, July.
Richard Wallace. 2003. AIML Overview. ALICE A. I.
Foundation.
259
Original Human-Human Dialogue Model response
Captain hello how are you doing sir
Doctor what do you want
i have patients waiting for me
hello
what can i do for you mr
Captain sir i come by to offer you some assistance ah on
behalf of my commander
ah we want to try to support you in this hospital
and ah improving the health and welfare of the
civilians that you have under your care
Doctor well this is your opinion i see captain
is this a is this a suggestion from your
commander
Captain i am sorry sir
could you say again
Doctor well this is your opinion you are the threat
i need protection from you
(a) Perceptron(surface)
Original Human-Human Dialogue Model response
Captain hello doctor perez
Doctor hello hello
what was your name
captain dezois very nice to meet you
i am sorry but i am very busy today
so i only have a limited amount of time
what can i help you with
Captain i am captain xx
Doctor so
do you need help
ok
what do you want i have patients waiting for me
Captain yes
i have a very urgent matter to discuss
with you
Doctor are you injured pl please captain
i i am sorry to cut you off
but i really must uh go
i was in the middle of examining a patient
(b) Cross-lingual Relevance Model
Figure 4: Example interaction for the dialogue models in static context setting. The second column
shows the original human-human dialogue and the third column shows the dialogue model?s response
for the corresponding system turn.
260
Proceedings of the SIGDIAL 2014 Conference, pages 251?253,
Philadelphia, U.S.A., 18-20 June 2014.
c?2014 Association for Computational Linguistics
SAWDUST: a Semi-Automated Wizard Dialogue Utterance Selection Tool
for domain-independent large-domain dialogue
Sudeep Gandhe David Traum
University of Southern California, Institute for Creative Technologies
srgandhe@gmail.com, traum@ict.usc.edu
Abstract
We present a tool that allows human wiz-
ards to select appropriate response utter-
ances for a given dialogue context from
a set of utterances observed in a dia-
logue corpus. Such a tool can be used
in Wizard-of-Oz studies and for collecting
data which can be used for training and/or
evaluating automatic dialogue models. We
also propose to incorporate such automatic
dialogue models back into the tool as an
aid in selecting utterances from a large di-
alogue corpus. The tool allows a user to
rank candidate utterances for selection ac-
cording to these automatic models.
1 Motivation
Dialogue corpora play an increasingly important
role as a resource for dialogue system creation.
In addition to its traditional roles, such as train-
ing language models for speech recognition and
natural language understanding, the dialogue cor-
pora can be directly used for the selection ap-
proach to response formation (Gandhe and Traum,
2010). In the selection approach, the response is
formulated by simply picking the appropriate ut-
terance from a set of previously observed utter-
ances. This approach is used in many wizard of
oz systems, where the wizard presses a button to
select an utterance, as well as in many automated
dialogue systems (Leuski et al., 2006; Zukerman
and Marom, 2006; Sellberg and J?onsson, 2008)
The resources required for the selection ap-
proach are a set of utterances to choose from and
optionally, a set of pairs of ?context, response
utterance? to train automatic dialogue models. A
wizard can generate such resources by performing
two types of tasks. First is the traditional Wizard-
of-Oz dialogue collection, where a wizard inter-
acts with a user of the dialogue system. Here the
wizard selects an appropriate response utterance
for a context that is being updated in a dynamic
fashion as the dialogue proceeds (dynamic context
setting). The second task is geared towards gather-
ing data for training/evaluating automatic dialogue
models, where a wizard is required to select ap-
propriate responses (perhaps more than one) for a
context which is extracted from a human-human
dialogue. The context does not change based on
the wizard?s choices (static context setting).
A wizard tool should help with the challenges
presented by these tasks. A challenge for both
of these tasks is that if the number of utterances
in the corpus is large (e.g., more than the num-
ber of buttons that can be placed on a computer
screen), it may be very difficult for a wizard to lo-
cate appropriate utterances. For the second task of
creating human-verified training/evaluation data,
tools like NPCEditor (Leuski and Traum, 2010)
have been developed which, allow the tagging of
a many to many relationships between contexts
(approximated simply as input utterance) and re-
sponses. In other cases, a corpus of dialogues is
used to acquire the set of selectable utterances, in
which each context is followed by a single next
utterance, and many utterances appear only once.
This sparsity of data makes the selection task hard.
Moreover, it may be the case that there are many
possible continuations of a context or contexts in
which an utterance may be appropriate (DeVault
et al., 2011).
We address these needs with a semi-automated
wizard tool that allows a wizard to engage in dy-
namic or static context utterance selection, select
multiple responses, and use several kinds of search
tools to locate promising utterances from a large
set that can?t all be displayed or remembered. In
the next section we describe the tool and how it
can be used. Then we describe how this tool was
used to create evaluation data in the static context
setting.
251
Figure 1: A screenshot of the interface for the wizard data collection
in static context setting.
Figure 2: A Histogram for the
number of selected appropriate
responses.
Figure 3: Avg. cardinality of the
set for different values of |R|.
2 Wizard Tool
Our wizard tool consists of several different views
(see figure 1), and is similar in some respects to the
IORelator annotation tool (DeVault et al., 2010),
but specialized to act as a wizard interface. The
first view (left pane) is a dialogue context, that
shows the recent history of the dialogue, before
the wizard?s decision point. The second view (top
right pane) shows a list of possible utterances that
can be selected from. This view can be ordered
in several different ways, as described below. Fi-
nally, there is a view of selected utterances (bot-
tom right pane). In the case of dynamic context,
the wizard will probably only select one utterance
and then a dialogue partner will respond with a
new utterance that extends the previous context.
In the case of static evaluation, however, used for
training and/or evaluating automated selection al-
gorithms, it is often helpful to select multiple ut-
terances if more than one is appropriate.
To help wizards explore the set of all possible
utterances, we provide the ability to rank the utter-
ances by various automated scores. Our configu-
ration used in the static context task uses Score1 as
the score calculated using one of the automatic di-
alogue models, specifically Nearest Context model
(Gandhe and Traum, 2007) - this model orders
candidate utterances from the corpus by the sim-
ilarity of their previous two utterances to the cur-
rent dialogue context. Score2 is surface text sim-
ilarity, computed as the METEOR score (Lavie
and Denkowski, 2009) between the candidate ut-
terance and the actual response utterance present
at that location in original human-human dialogue
(which is not available to the wizard). Wizards can
also search the set of utterances for specific key-
words and the third column, Relevance, shows the
score for the search string entered by the wizards.
The last column RF stands for relevance feedback
and ranks the utterances by similarity to the utter-
ances that have already been chosen by the wiz-
ard. This allows wizards to easily find paraphrases
of already selected response utterances. Clicking
the header of any of these columns will reorder the
utterance list by the automated score, by relevance
(assuming a search term has been entered) or by
relevance feedback (assuming one or more utter-
ances have already been chosen).
3 Evaluation
We evaluated the tool by having four human vol-
unteers (wizards) use it in order to establish an up-
per baseline for human-level performance in the
static context evaluation task described in (Gandhe
and Traum, 2013). Wizards were instructed in how
to use the search and relevance feedback features.
In order to not bias the wizards, they were not told
exactly what score1 and score2 indicate, but just
that the scores can be useful in search.
Each wizard is presented with a set of utter-
ances (U
train
) (|U
train
| ? 500) and is asked to
select a subset from these that will be appropri-
ate as a response for the presented dialogue con-
text. Each wizard was requested to select some-
where between 5 to 10 (at-least one) appropriate
responses for each dialogue context extracted from
252
five different human-human dialogues. There are
a total of 89 dialogue contexts for the role that
the wizards were to play. Figure 2 shows the his-
togram for the number of utterances selected as
appropriate responses by the four wizards. As ex-
pected, wizards frequently chose multiple utter-
ances as appropriate responses (mean = 7.80, min
= 1, max = 25).
To get an idea about how much the wizards
agree among themselves for this task, we calcu-
lated the overlap between the utterances selected
by a specific wizard and the utterances selected by
another wizard or a set of wizards. Let U
T
c
be a set
of utterances selected by a wizard T for a dialogue
context c. Let R be a set of wizards (T /? R) and
U
R
c
be the union of sets of utterances selected by
the set of wizards (R) for the same context c. Then
we define the following overlap measures,
Precision
c
=
|U
T
c
? U
R
c
|
|U
T
c
|
Recall
c
=
|U
T
c
? U
R
c
|
|U
R
c
|
Jaccard
c
=
|U
T
c
? U
R
c
|
|U
T
c
? U
R
c
|
Dice
c
=
2|U
T
c
? U
R
c
|
|U
T
c
|+ |U
R
c
|
Meteor
c
=
1
|U
T
c
|
?
u
t
METEOR (u
t
, U
R
c
) ?u
t
? U
T
c
We compute the average values of these over-
lap measures for all contexts and for all possible
settings of test wizards and reference wizards. Ta-
ble 1 shows the results with different values for the
number of wizards used as reference.
#ref Prec. Rec. Jacc. Dice Meteor
1 0.145 0.145 0.077 0.141 0.290
2 0.244 0.134 0.093 0.170 0.412
3 0.311 0.121 0.094 0.171 0.478
Table 1: Inter-wizard agreement
Precision can be interpreted as the probability
that a response utterance selected by a wizard is
also considered appropriate by at least one other
wizard. Precision rapidly increases along with
the number of reference wizards used. This hap-
pens because the size of the set U
R
c
steadily in-
creases with more reference wizards. Figure 3
shows this observed increase and the expected in-
crease if there were no overlap between the wiz-
ards. The near-linear increase in |U
R
c
| suggests
that selecting appropriate responses is a hard task
and may require a lot more than four wizards to
achieve convergence.
Subjectively, the wizards reported no major us-
ability problems with the tool, and were able to
use all four utterance ordering techniques to find
appropriate utterances.
4 Future Work
Future work involves performing some formal
evaluations comparing this tool to other tools (that
are missing some of the features of this tool) in
terms of amount of time taken to make selections
and quality of the selections, using the same eval-
uation techniques as (Gandhe and Traum, 2013).
We also see a promising future for semi-
automated selection, which blurs the line between
a pure algorithmic response and pure wizard se-
lection. Here the wizard can select appropriate re-
sponses, which can be used by algorithms as su-
pervised training data, meanwhile the algorithms
can be used to seed the wizard?s selection.
References
David DeVault, Susan Robinson, and David Traum. 2010.
IORelator: A graphical user interface to enable rapid se-
mantic annotation for data-driven natural language under-
standing. In Proc. of the 5th Joint ISO-ACL/SIGSEM
Workshop on Interoperable Semantic Annotation (ISA-5).
David DeVault, Anton Leuski, and Kenji Sagae. 2011. An
evaluation of alternative strategies for implementing dia-
logue policies using statistical classification and rules. In
Proceedings of the IJCNLP 2011, Nov.
Sudeep Gandhe and David Traum. 2007. Creating spoken
dialogue characters from corpora without annotations. In
Proceedings of Interspeech-07, Antwerp, Belgium.
Sudeep Gandhe and David Traum. 2010. I?ve said it be-
fore, and I?ll say it again: an empirical investigation of
the upper bound of the selection approach to dialogue. In
Proceedings of the SIGDIAL ?10, Tokyo, Japan.
Sudeep Gandhe and David Traum. 2013. Surface text based
dialogue models for virtual humans. In Proceedings of the
SIGDIAL 2013, Metz, France.
A. Lavie and M. J. Denkowski. 2009. The meteor metric
for automatic evaluation of machine translation. Machine
Translation, 23:105?115.
Anton Leuski and David R. Traum. 2010. NPCEditor: A
tool for building question-answering characters. In Pro-
ceedings of LREC 2010, Valletta, Malta.
Anton Leuski, Ronakkumar Patel, David Traum, and Bran-
don Kennedy. 2006. Building effective question answer-
ing characters. In Proc. of SIGDIAL ?06, Australia.
Linus Sellberg and Arne J?onsson. 2008. Using random in-
dexing to improve singular value decomposition for latent
semantic analysis. In Proceedings of LREC?08, Morocco.
Ingrid Zukerman and Yuval Marom. 2006. A corpus-based
approach to help-desk response generation. In Computa-
tional Intelligence for Modelling, Control and Automation
(CIMCA 2006), IAWTIC 2006.
253
