Modelling Grounding 
Co l in  Matheson  
University of Edinburgh 
Edinburgh,  Scotland 
colin, mat  heson @ed. ac. uk 
and Discourse Obligations Using Update 
Rules 
Mass imo Poes io  
University of Edinburgh 
Edinburgh, Scotland 
massimo.poesio@ed.ac.uk 
Dav id  T raum 
University of Mary land 
Maryland,  USA 
t raum@cs.umd.edu 
Abst rac t  
This paper describes an implementation f some key 
aspects of a theory of dialogue processing whose 
main concerns are to provide models of GROUND- 
ING and of the role of DISCOURSE OBLIGATIONS in 
an agent's deliberation processes. Our system uses 
the TrindiKit dialogue move engine toolkit, which 
assumes a model of dialogue in which a participan. 
t's knowledge is characterised in terms of INFORMA- 
TION STATES which are subject o various kinds of 
updating mechanisms. 
1 I n t roduct ion  
In this paper we describe a preliminary implemen- 
tation of a 'middle-level' dialogue management sys- 
tem. The key tasks of a dialogue manager are to 
update the representation f dialogue on the basis of 
processed input (generally, but not exclusively, lan- 
guage utterances), and to decide what (if anything) 
the system should do next. There is a wide range of 
opinions concerning how these tasks should be per- 
formed, and in particular, how the ongoing dialogue 
state should be represented: e.g., as something very 
specific to a particular domain, or according to some 
more general theory of (human or human inspired) 
dialogue processing. At one extreme, some systems 
represent only the (typically very rigid) transitions 
possible in a perceived ialogue for the given task, 
often using finite states in a transition etwork to 
represent the dialogue: examples of this are sys- 
tems built using Nuance's DialogueBuilder or the 
CSLU's Rapid Application Prototyper. The other 
extreme is to build the dialogue processing theory on 
top of a full model of rational agency (e.g., (Bretier 
and Sadek, 1996)). The approach we take here lies 
in between these two extremes: we use rich repre- 
sentations of information states, but simpler, more 
dialogue-specific deliberation methods, rather than 
a deductive reasoner working on the basis of an ax- 
iomatic theory of rational agency. We show in this 
paper that the theory of information states we pro- 
pose can, nevertheless, beused to give a character- 
isation of dialogue acts such as those proposed by 
the Discourse Resource Initiative precise nough to 
formalise the deliberation process of a dialogue man- 
ager in a completely declarative fashion. 
Our implementation is based on the approach to 
dialogue developed in (Traum, 1994; Poesio and 
Traum, 1997; Poesio and Traum, 1998; Traum et al, 
1999). This theory, like other action-based theories 
of dialogue, views dialogue participation i terms of 
agents performing dialogue acts, the effects of which 
are to update the information state of the partici- 
pants in a dialogue. However, our view of dialogue 
act effects is closer in some respects to that of (All- 
wood, 1976; Allwood, 1994) and (Singh, 1998) than 
to the belief and intention model of (Sadek, 1991; 
Grosz and Sidner, 1990; Cohen and Levesque, 1990). 
Particular emphasis i placed on the social commit- 
ments of the dialogue participants (obligations to 
act and commitments to propositions) without mak- 
ing explicit claims about the actual beliefs and in- 
tentions of the participants. Also, heavy empha- 
sis is placed on how dialogue participants socially 
GROUND (Clark and Wilkes-Gibbs, 1986) the infor- 
mation expressed in dialogue: the information state 
assumed in this theory specifies which information is 
assumed to be already part of the common ground at 
a given point, and which part has been introduced, 
but not yet been established. 
The rest of this paper is structured as follows. The 
theory of dialogue underlying the implementation is 
described in more detail in Section 2. Section 3 de- 
scribes the implementation itself. Section 4 shows 
how the system updates its information state while 
participating in a fairly simple dialogue. 
2 Theoret i ca l  Background 
One basic assumption underlying this work is that 
it is useful to analyse dialogues by describing the 
relevant 'information' that is available to each par- 
ticipant. The notion of INFORMATION STATE (IS) is 
therefore mployed in deciding what the next action 
should be, and the effects of utterances are described 
in terms of the changes they bring about in ISs. A 
particular instantiation ofa dialogue manager, from 
this point of view, consists of a definition of the con- 
tents of ISs plus a description of the update processes 
which map from IS to IS. Updates are typically trig- 
gered by 'full' dialogue acts such as assertions or 
directives, 1 of course, but the theory allows parts of 
utterances, including individual words and even sub- 
parts of words, to be the trigger. The update rules 
for dialogue acts that we assume here are a simpli- 
fied version of the formalisations proposed in (Poesio 
and Traum, 1998; Traum et al, 1999) (henceforth, 
PTT). 
The main aspects of PTT which have been im- 
plemented concern the way discourse obligations are 
handled and the manner in which dialogue partic- 
ipants interact o add information to the common 
ground. Obligations are essentially social in nature, 
and directly characterise poken dialogue; a typical 
example of a discourse obligation concerns the rela- 
tionship between questions and answers. Poesio and 
Traum follow (Traum and Allen, 1994) in suggesting 
that the utterance of a question imposes an obliga- 
tion on the hearer to address the question (e.g., by 
providing an answer), irrespective of intentions. 
As for the process by which common ground is es- 
tablished, or GROUNDING (Clark and Schaefer, 1989; 
Traum, 1994), the assumption i PTT is that classi- 
cal speech act theory is inherently too simplistic in 
that it ignores the fact that co-operative interaction 
is essential in discourse; thus, for instance, simply as- 
serting something does not make it become mutually 
'known' (part of the common ground). It is actually 
necessary for the hearer to provide some kind of ac- 
knowledgement that the assertion has been received, 
understood or not understood, accepted or rejected, 
and so on. Poesio and Traum view the public in- 
formation state as including both material that has 
already been grounded, indicated by GND here, and 
material that hasn't been grounded yet. These com- 
ponents of the information state are updated when 
GROUNDING ACTS such as acknowledgement areper- 
formed. Each new contribution results in a new DIS- 
COURSE UNIT (DU) being added to the information 
state (Traum, 1994) and recorded in a list of 'un- 
grounded iscourse units' (UDUS); these DUs can 
then be subsequently grounded as the result, e.g., of 
(implicit or explicit) acknowledgements. 
3 Imp lement ing  PTT  
In this section, we describe the details of the im- 
plementation. First, in Section 3.1, we describe the 
TrindiKit tool for building dialogue managers that 
we used to build our system. In Section 3.2, we de- 
scribe the information states used in the implemen- 
tation, an extension and simplification of the ideas 
from PTT discussed in the previous ection. Then, 
in Section 3.3, we discuss how the information state 
is updated when dialogue acts are observed. Finally, 
1We assume here the DRI classification f dialogue acts 
(Discourse Resource Initiative, 1997). 
/ . ' ? "  .. -,. ' " . . ,  
I.lol'lP.lllit~ ~;l{lle (i$) 
'E 1 
Figure 1: TrindiKit Architecture 
in Section 3.4, we describe the rules used by the sys- 
tem to adopt intentions and perform its own actions. 
An extended example of how these mechanisms are 
used to track and participate in a dialogue is pre- 
sented in Section 4. 
3.1 TrindiKit 
The basis for our implementation is the TrindiKit 
dialogue move engine toolkit implemented as part 
of the TRINDI project (Larsson et al, 1999). The 
toolkit provides upport for developing dialogue sys- 
tems, focusing on the central dialogue management 
components. 
The system architecture assumed by the TrindiKit 
is shown in Figure 1. A prominent feature of this ar- 
chitecture is the information state, which serves as a 
central 'blackboard' that processing modules can ex- 
amine (by means of defined CONDITIONS) or change 
(by means of defined OPERATIONS). The structure 
of the IS for a particular dialogue system is defined' 
by the developer who uses the TrindiKit to build 
that system, on the basis of his/her own theory of 
dialogue processing; no predefined notion of infor- 
mation state is provided. 2 The toolkit provides a 
number of abstract data-types such as lists, stacks, 
and records, along with associated conditions and 
operations, that can be used to implement the user's 
theory of information states; other abstract ypes 
can also be defined. In addition to this customis- 
able notion of information state, TrindiKit provides 
a few system variables that can also used for inter- 
module communication. These include input for the 
raw observed (language) input, latest_moves which 
2In TRINDI we are experimenting with multiple instanti- 
ations of three different theories of information state (Traum 
et al, 1999). 
2 
contains the dialogue moves observed in the most 
recent urn, la tes t_speaker ,  and next_moves, con- 
taining the dialogue moves to be performed by the 
system in the next turn. 
A complete system is assumed to consist of sev- 
eral modules interacting via the IS. (See Figure 1 
again.) The central component is called the DIA- 
LOGUE MOVE ENGINE (DME). The DME performs 
the processing needed to integrate the observed i- 
alogue moves with the IS, and to select new moves 
for the system to perform. These two functions are 
encapsulated in the UPDATE and SELECTION sub- 
modules of the DME. The update and select mod- 
ules are specified by means of typed rules, as well as 
sequencing procedures to determine when to apply 
the rules. We are here mainly concerned with UP- 
DATE RULES (urules) ,  which consist of four parts: a 
name, a type, a list of conditions to check in the in- 
formation state, and a list of operations to perform 
on the information state, u ru les  are described in 
more detail below, in Section 3.3. There are also 
two modules outside the DME proper, but still cru- 
cial to a complete system: INTERPRETATION, which 
consumes the input and produces a list of dialogue 
acts in the latest_moves variable (potentially mak- 
ing reference to the current information state), and 
GENERATION, which produces NL output from the 
dialogue acts in the next_moves variable. Finally, 
there is a CONTROL module, that governs the se- 
quencing (or parallel invocation) of the other mod- 
ules. In this paper we focus on the IS and the DME; 
our current implementation only uses very simple 
interpretation and generation components. 
3.2 Information States in PTT  
In this section we discuss the information state used 
in the current implementation. The main difference 
between the implemented IS and the theoretical pro- 
posal in (Poesio and Traum, 1998) is that in the im- 
plementation the information state is partitioned in 
fields, each containing information of different ypes, 
whereas in the theoretical version the information 
state is a single repository of facts (a DISCOURSE 
REPRESENTATION STRUCTURE). Other differences 
are discussed below. An example IS with some fields 
filled is shown in Figure 2; this is the IS which results 
from the second utterance in the example dialogue 
discussed in Section 4, A route please. 3
The IS in Figure 2 is a record with two main 
parts, W and C. The first of these represents the 
system's (Wizard) view of his own mental state and 
of the (semi-)public information discussed in the di- 
alogue; the second, his view of the user's (Caller) 
information state. This second part is needed to 
3All diagrams in this paper are automatically generated 
from TrindiKit system internal representations and displayed 
using the Thistle dialogue editor (Calder, 1998). Some have 
been subsequently edited for brevity and clarity. 
r \] understandingAct( W,DU3 )\ 1 ~1 
\[OBL: ~lddre~(C,CA2 ) \] / // 
~. .  / . .  /CAS:C2 ,~..,~g.(C.DU2) \ /  I I  
. . . . .  / .... \c^:: ' / /  I I  
/ sc : < " ! H 
\[COND: < > J H 
UDUS: <DU3> H 
\[ \[OBL: <,ddri~z(C.CA2 ) > \ ] \ ]  H 
DH: <CA2: C2. into requi~t( W.?help fore1 ):> 
/ / LCOND: < > J/ If 
: | LID: DU2 J I I  
/ \[ lilt / / / /CA5: C2. dil c,(C ivemule(W)  \ / /H 
/ / /DH: (CAS: C2. a,swer( C.CA2.CA4 ) ) //11 
ko,, //11 
I / IIII / \[ LCOND: <IICIpt(W.CA6)-> obl(W~iveroutc(W))>J III 
/ LID: DU3 JII 
1 /,,,,o_,..,,,.,( w.:,,.-, )\ I I  
/ ; l i~oute(W ) 
t ~,,*~,~,d~(W.bU3)/ JI 
lINT: <letrome(C)>\] J 
Figure 2: Structure of Information States 
model misunderstandings arising from the dialogue 
participants having differing views on what has been 
grounded; as we are not concerned with this problem 
here, we will ignore C in what follows. 
w contains information on the grounded mate- 
rial (GND), on the ungrounded information (UDUS, 
PDU and CDU), and on W's intentions (INT). GND 
contains the information that has already been 
grounded; the other fields contain information about 
the contributions still to be grounded. As noticed 
above, in PTT  it is assumed that for each new ut- 
terance, a new DU is created and added to the IS. 
The current implementation differs from the full the- 
ory in that only two DUs are retained at each point; 
the current DU (CDU) and the previous DU (PDU). 
The CDU contains the information in the latest con- 
tribution, while the PDU contains information from 
the penultimate contribution. Information is moved 
f rom PDU to GND as a result of an ack (acknowl- 
edgement) dialogue act (see below.) 
The DUs and the GND field contain four fields, 
representing obligations (OBL), the dialogue history 
(DH), propositions to which agents are socially com- 
mitted (scP),  and conditional updates (COND). The 
value of OBL is a list of action types: actions that 
agents are obliged to perform. An action type is 
specified by a PREDICATE, a DIALOGUE PARTICI- 
PANT, and a list of ARGUMENTS. The value of see 
is a list of a particular type of mental states, so- 
cial commitments of agents to propositions. 4 These 
are specified by a DIALOGUE PARTICIPANT, and a 
PROPOSITION. Finally, the elements in DH are dia- 
4SCPs play much the same role in PTT as do beliefs in 
many BDI accounts of speech acts. 
3 
logue actions, which are instances of dialogue action 
types. A dialogue action is specified by an action 
type, a dialogue act id, and a confidence level CONF 
(the confidence that an agent has that that dialogue 
act has been observed). 
The situation in Figure 2 is the result of updates to 
the IS caused by utterance \[2\] in the dialogue in (6), 
which is assumed to generate a d i rect  act as well as 
an assert  act and an answer  act. 5 That utterance 
is also assumed to contain an implicit acknowledge- 
ment of the original question; this understanding act 
has resulted in the contents of DU2 being grounded 
(and subsequently merged with GND), as discussed 
below. 
GND.OBL in Figure 2 includes two obligations. 
The first is an obligation on W to perform an under- 
standing act (the predicate is unders tand ingAct ,  
the participant is W, and there is just one argument, 
DU3, which identifies the DU in CDU by referring to 
its ID). The second obligation is an obligation on C 
to address  conversational ct CA2; this ID points 
to the appropriate info_request  in the DH list by 
means of the ID number. Obligations are specified 
in CDU and PDU, as well. Those in PDU are simply 
a subset of those in GND, since at point in the up- 
date process shown in Figure 2 this field contains 
information that has already been grounded (note 
that DU2 is not in UDUS anymore); but CDU con- 
tains obligations that have not been grounded yet - 
in particular, the obligation on W to address  CA6. 
GND.DH in this IS contains a list of dialogue ac- 
tions whose occurrence has already been grounded: 
the info_request  performed by utterance 1, with ar- 
gument a question, 6 and the implicit acknowledge 
performed by utterance 2. 7 The DH field in CDU con- 
tains dialogue acts performed by utterance 2 that do 
need to be grounded: a directive by C to W to per- 
form an action of type g iveroute,  and an assert  
by C of the proposition want(C,  route), by which C 
provides an answer  to the previous info_request 
CA2. 
The COND field in CDU contains a conditional up- 
date resulting from the directive performed by that 
utterance. The idea is that directives do not imme- 
diately lead to obligations to perform the mentioned 
action: instead (in addition to an obligation to ad- 
dress the action with some sort of response), their ef- 
fect is to add to the common ground the information 
that if the directive is accepted  by the addressee, 
SThe fact that the utterance of a route please constitutes 
an answer is explicitly assumed; however, it should be possible 
to derive this information automatically (perhaps along the 
lines suggested by Kreutel (Kreutel, 1998)). 
6We use the notation ?p to indicate a question of the form 
?(\[x\],p(x)). 
7We assume here, as in (Traum, 1994) and (Poesio and 
Traum, 1998), that understanding acts do not have to be 
grounded themselves, which would result in a infinite regress. 
then he or she has the obligation to perform the ac- 
tion type requested. (In this case, to give a route to 
C.) 
3.3 Update  Rules  in PTT  
We are now in a position to examine the update 
mechanisms which are performed when new dia- 
logue acts are recognised. When a dialogue par- 
ticipant takes a turn and produces an utterance, 
the interpretation module sets the system variable 
latest_moves to contain a representation f the di- 
alogue acts performed with the utterance. The up- 
dating procedure then uses update rules to modify 
the IS on the basis of the contents of latest_moves 
and of the previous IS. The basic procedure is de- 
scribed in (1) below, s
(1) 1. Create a new DU and push it on top of 
UDUs. 
2. Perform updates on the basis of backwards 
grounding acts. 
. If any other type of act is observed, record 
it in the dialogue history in CDU and apply 
the update rules for this kind of act 
4. Apply update rules to all parts of the IS 
which contain newly added acts. 
The first step involves moving the contents of CDU 
to PDU (losing direct access to the former PDU con- 
tents) and putting in CDU a new empty DU with 
a new identifier. The second and third steps deal 
explicitly with the contents of la test .moves,  ap- 
plying one urule (of possibly a larger set) for each 
act in latest_moves. The relevant effects for each 
act are summarised in (2), where the variables have 
the following types: 
IDx 
DUx 
DP 
q 
PROP 
Act 
o(DP) 
P(ID) 
Q(ID) 
Dialogue Act Identification Number 
DU Identification Number 
Dialogue Participant (i.e., the speaker) 
A Question 
A Proposition 
An Action 
The other dialogue participant 
The content of the ID, a proposition 
The content of the ID, a question 
SSee (Poesio et al, 1999; Traum et al, 1999) for different 
versions of this update procedure used for slightly different 
versions of the theory. 
4 
(2) act ID:2, accept (DP, ID2) 
effect accomplished via rule resolution 
act ID:2, ack(DP, DU1) 
effect peRec(w.Gnd,w.pdu.tognd) 
effect remove(DU1,UDUS) 
act ID:2, agree(DP, ID2) 
effect push(scP,scp(DP,P(ID2))) 
act ID:2, answer(DP,ID2,ID3) 
effect push(scP,ans(DP, Q(ID2),P(ID2))) 
act ID:2, assert (DP,PROP) 
effect push(scP,sep(DP, PROP)) 
effect push (COND,accept (o(DP),ID)-+ 
scp(o(DP),PROP)) 
act ID:I, assert(DP,PROP) 
effect push (COND,accept (o(DP),ID)-~ 
scp(o(DP),PROP)) 
act ID:2, check(DP,PROP) 
effect push(OSL,address(o(DP),ID)) 
effect push(COND,agree(o(DP),ID) --~ 
scp(DP, PROP)) 
act ID:2, direct (DP, Act) 
effect push(OBL,address(o(DP),ID)) 
effect push(CONI),accept (o(DP),ID) -~ 
obl(o(DP),Act)) 
act ID:2, info_request (DP, Q) 
effect push(osL,address(o(DP),ID)) 
The ack act is the only backward grounding act 
implemented at the moment. The main effect of an 
ack is to merge the information i the acknowledged 
DU (assumed to be PDU) into GND, also removing 
this DU from UDUS. Unlike the other acts described 
below, ack acts are recorded irectly into GND.DH, 
rather than into CDU.TOGND.DH. 
All of the other updates are performed in the third 
step of the procedure in (1). The only effect of ac- 
cept acts is to enable the conditional rules which 
are part of the effect of assert and direct, leading 
to social commitments and obligations, respectively. 
agree acts also trigger conditional rules introduced 
by check; in addition, they result in the agent be- 
ing socially committed to the proposition i troduced 
by the act with which the agent agrees. Perform- 
ing an answer to question ID2 by asserting propo- 
sition P(ID3) commits the dialogue participant to 
the proposition that P(ID3) is indeed an answer to 
Q(ID2). 
The two rules for assert are where the confidence 
levels are actually used, to implement a simple ver- 
ification strategy. The idea is that the system only 
assumes that the user is committed to the asserted 
proposition when a confidence l vel of 2 is observed, 
while some asserts are assumed not to have been 
sufficiently well understood, and are only assigned a 
confidence l vel 1. This leads the system to perform 
a check, as we will see shortly. 
The next three update rules, for check, direct, 
and info_req, all impose an obligation on the other 
dialogue participant to address the dialogue act. In 
addition, the direct rule introduces a conditional 
act: acceptance of the directive will impose an obli- 
gation on the hearer to act on its contents. 
In addition, all FORWARD ACTS 9 in the DRI 
scheme (Discourse Resource Initiative, 1997) impose 
an obligation to perform an understanding act (e.g., 
an acknowledgement): 
(3) 1 act 
effect 
ID:c, forward-looking-act (DP) 
push(OBL,u-act (o(DP),CDU.id)) I 
The internal urules implementing the updates in 
(2) have the format shown in (4), which is the urule 
for info_request. 
(4) =uxe( doZnfoR, q. rulet~.S, 
\[ hearer(DP), 
latest_moves: in(Hove}, 
Move:valEec(pred,inforeq) \],  
\[ incr_set(update_cycles,_), 
incr_set (next.dh_id, HID), 
next _du_name (ID), 
pushRec (w'cdu'tognd'dh, 
record ( \[atype=Move, c level=2, id=HID \ ] ) ) ,  
pushRec (e'cdu~tosnd'obl, 
record ( \[pred~address, dp=DP, 
argsfstackset ( 
\[record ( \[i%em=IIID\] )\] ) \] ) ), 
pushRec (w'gnd" obl, 
record ( \[pred=uact, dp=P, 
args=stackset ( 
\[rocord(\[item=ID\] ) \] ) \] )) \ ] ) .  
As noted above, these rules have four parts; a 
name, a type, a list of conditions, and a list of ef- 
fects. The conditions in (4) state that there must be 
a move in latest_moves whose predicate is inforeq. 
The effects l? state that the move should be recorded 
in the dialogue history in CDU, that an obligation to 
address the request should be pushed into OBL in 
CDU, and that the requirement for an understand- 
ing act by W should be pushed irectly into the list 
in W.GND. 
The fourth and final step of the algorithm cycles 
through the updating process in case recently added 
facts have further implications. For instance, when 
an action has been performed that matches the an- 
tecedent of a rule in COND, the consequent is es- 
tablished. Likewise, when an action is performed 
it releases any obligations to perform that action. 
Thus, accept, answer, and agree are all ways of 
releasing an obligation to address, since these are 
all appropriate backward looking actions. Similarly, 
an agent will drop intentions to perform actions it 
has already (successfully) performed. 
3.4 Deliberation 
We assume, in common with BDI-approaches to 
agency (e.g., (Bratman et al, 1988)) that intentions 
9Forward acts include assert, check, direct, and 
info_request. 
l?The ID and HID values simply contain numbers identifying 
the discourse units and conversational acts. 
5 
are the primary mental attitude leading to an agen- 
t's actions. The main issues to explain then become 
how such intentions are adopted given the rest of 
the information state, and how an agent gets from 
intentions to actual performance. 
For the latter question, we take a fairly simplistic 
approach here: all the intentions to perform dia- 
logue acts are simply transferred to the next_moves 
system variable, with the assumption that the gen- 
eration module can realise all of them as a single ut- 
terance. A more sophisticated approach would be to 
weight the importance of (immediate) realisation of 
sets of intentions and compare this to the likelihood 
that particular utterances will achieve these effects 
at minimal cost, and choose accordingly. We leave 
this for future work (see (Traum and Dillenbourg, 
1998) for some preliminary ideas along these lines), 
concentrating here on the first issue - how the sys- 
tem adopts intentions to perform dialogue acts from 
other aspects of the mental state. 
The current system takes the following factors into 
account: 
? obligations (to perform understanding acts, to 
address previous dialogue acts, to perform other 
actions) 
? potential obligations (that would result if an- 
other act were performed, as represented in the 
COND field) 
? insufficiently understood ialogue acts (with a 
1 confidence level in CDU.DH) 
? intentions to perform complex acts 
The current deliberation process assumes maxi- 
mal cooperativity, in that the system always chooses 
to meet its obligations whenever possible, and also 
chooses to provide a maximally helpful response 
when possible. Thus, when obliged to address  a 
previous dialogue act such as a question or direc- 
tive, it will choose to actually return the answer or 
perform the action, if possible, rather than reject or 
negotiate such a performance, which would also be 
acting in accordance with the obligations (see (Kreu- 
tel, 1998) on how acts might be rejected). 
In the current implementation, the following rules 
are used to adopt new intentions (i.e., to update the 
INT field): 
(5) 1. add an intention to acknowl- 
edge(W,CDU), given an obligation to 
perform a u-act,  if everything in CDU is 
sufficiently understood (i.e., to level 2); 
2. add an intention to accept a directive or an- 
swer a question as the result of an obligation 
to address a dialogue act; 
3. add an intention to perform an action if 
COND contains a conditional that will estab- 
lish an obligation to perform the action, and 
the antecedent of this conditional is another 
action that is already intended. (This an- 
ticipatory planning allows the obligation to 
be discharged at the same time it is invoked, 
e.g., without giving an intermediate accep- 
tance of an directive.) 
4. add an intention to perform a (dialogue) ac- 
tion motivated by the intention to perform 
the current task. In the case of the Au- 
toroute domain, we have two cases: the sys- 
tem may decide 
(a) to check any dialogue acts in CDU at 
confidence level 1, which contain infor- 
mation needed to discharge the intention 
to give a route; or 
(b) to perform a question asking about a new 
piece of information that has not been es- 
tablished (this is decided by inspecting 
GND.SCP and CDU.SCP). For example, 
it may decide to ask about the starting 
point, the time of departure, etc. 
4 Extended Example  
In this section, we discuss more examples of how the 
information state changes as a result of processing 
and performing dialogue acts. It is useful to do this 
by looking briefly at a typical Autoroute dialogue, 
shown in (6). 11 Our implementation can process this 
sort of dialogue using very simple interpretation and 
generation routines that provide the dialogue acts 
in latest_moves from the text strings, and produce 
W's output text from the dialogue acts which the 
system places in next_moves. 
(6) W \[1\]: How can I help? 
C \[2\]: A route please 
W \[3\]: Where would you like to start? 
C \[4\]: Malvern 
W \[5\]: Great Malvern? 
C \[6\]: Yes 
W \[7\]: Where do you want to go? 
C \[8\]: Edwinstowe 
W \[9\]: Edwinstowe in Nottingham? 
C \[10\]: Yes 
W \[11\]: When do you want to leave? 
C \[12\]: Six pm 
W \[13\]: Leaving at 6 p.m.? 
C \[14\]: Yes 
W \[15\]: Do you want the quickest or the 
shortest route? 
C \[16\]: Quickest 
W \[17\]: Please wait while your route is cal- 
culated. 
We assume that before the dialogue starts, W has 
the intention to ask C what kind of help is required, 
liThe interchanges have been cleaned up to some extent 
here, mainly by removing pauses and hesitations. 
6 
W: 
I \[ /gi . . . . .  ,e(W, \ I OBL: ~understandlngAet(W,DU5)) / \address(C,CA8 ) \[ \[ / CA I0: C2, acknowledge(C.DU4 ) \ iGND: /DH: (CAg: C2, accept( W.CA6 ) ~/ / SCP: < ? 
\[COND: < > 
UDUS: <DU5> 
\[ /CA9:C2 . . . .  pt(W,CA6) \ / /  
TOGND: OH: \CAS: C2. info request( W,?start ) I l l  PDU: / /  
LCOND: < > 
LID: DU4 
DH: ~/CA 12: C2, a~wer( C,CA8,CAI 1 ) 
CDU: TOGND: \CA 11: Cl, assert( C.s 'tart(malvem) ) 
SCP: < > 
LCOND: < > 
LID: DU5 
/ check (W.,start(malvern ) ) \
INT: ~acknowledge( W.DU5 ) 
\giveroute( W ) / 
I INT: <getroute( C ) ? l 
Figure 3: Information State Prompting Check in \[5\] 
and that C has the intention to find a route. We also 
assume that W has the turn, and that the presence 
of the how can I help intention triggers an utterance 
directly. Figure 2, presented above, shows the in- 
formation state after utterance \[2\]. The intentions 
in that figure lead directly to the system producing 
utterance \[3\]. 
Looking a little further ahead in the dialogue, Fig- 
ure 3 shows the information state after utterance 
\[4\]. 12 Here we can see in CDU.TOGND.DH (along with 
the ack act CA10, in GND.DH) the dialogue moves 
that this utterance has generated. Note that the as- 
sert, CA l l ,  is only at confidence level 1, indicating 
lack of sufficient certainty in this interpretation as 
the town 'Great Malvern'. This lack of certainty and 
the resulting lack of a relevant SCP in CDU.TOGND 
lead the deliberation routines to produce an inten- 
tion to check this proposition rather than to move 
directly on to another information request. This 
intention leads to utterance \[5\], which, after inter- 
pretation and updating on the new dialogue acts, 
leads to the information state in Figure 4. The in- 
teresting thing here is the condition which appears 
in CDU.TOGND.COND as a result of the check; the 
interpretation of this is that, if C agrees with the 
check, then W will be committed to the proposition 
that the starting place is Malvern (C would also be 
committed to this by way of the direct effects of an 
agree act). 
12The actual information state contains all the previously 
established ialogue acts, SCPs and Obligations in GND~ from 
Figure 2 and intermediate utterances. Here we have deleted 
these aspects from the figures for brevity and clarity. 
OBL: \glveroute(W ) / \[ 
/CAI3: C2. acknowledge( W.DU5 ) \ \ [  
GND: /DH: ~ CA 12: C2, answer(C.CA8 ) }/  / \CA 11 :C 1, assert(C.start(malvem) ) / \ [SCP: < ? 
LCOND: < > J 
UDUS: <DU6> 
IOBL: < ? 
W: TOGND: DFt: \CA i i: Ci, as~rt(C,s~t(malvem) ) i i i  
PDU: SCP: < > 
LID: DU5 
r r?BL: <address(C.CAl4)> 
\]TOGND: \[DH: <CA 14: C2, check (W.,smrt(rnalvern)) > 
CDU: / /SOP: < > 
LCOND: <agree(C,CAl4 \] .> scp(W,start(malvern) }; 
L In: DU6 
INT: ,:giveroute(W )> 
C: lINT: <getroute(C)>l 
Figure 4: Information State Following Check in \[5\] 
I r /..<,.,..,.,,..,~.?,,,,,=.,:>,.,8>\ 1 11 / ?BL: \gi . . . . .  l ,(w) / / / /  
/,--,,-, /c , , . :c2  .,~,.>,,~=<W.DU~>\/ / /  
GND: \[----: \CA 16: C2. igi~e( C,CA 14 ) / /  / /  
Is<:,> ,/,,<,,< c~,.,,,.,<,.,,=,,.<,,.. > > ~, / / /  i : \scp(W,slirl( malvcrn D/  / / /  
l.CO~D: < > J N 
uous: <oul> / /  
I loB, . . . .  11 l/ W: TOGND" DH: <CA 16: C2. agree(C,CA 14 ) > 
iPDU: \[ " \[SCP: < sop( C,start( malvern ) } > H | /  
/ / LCOND: < > J/ l /  
\] LID: DO7 J / /  
/ \[ \[OBL: <Iddress(C.CAI8 )> 11ll 
/ /TOGND" / DH: <CAIS: C2. info_nequesi(W.?dest )>//// 
/ c~U: / / sc~: < > / IN  
/ / L~oNo: < > ill / 
/ uo: ,~  .ill 
C: lINT: <getroute(C)>l J 
Figure 5: Information state following \[7\] 
After C's agreement in \[6\], the deliberation rou- 
tine is able to move past discussion of the start- 
ing point, and add an intention to ask about the 
next piece of information, the destination. This 
leads to producing utterance \[7\], which also implic- 
itly acknowledges \[6\], after which C's agreement is 
grounded, leading to the IS shown in Figure 5. Note 
that the list in W.GND.SCP in Figure 5 indicates that 
both C and W are committed to the proposition that 
the starting place is Malvern. 
5 Conc lus ions  
It has only been possible here to introduce the basic 
concerns of the PTT account of dialogue modelling 
and to pick out one or two illustrative examples to 
highlight the implementational approach which has 
7 
been assumed. Current and future work is directed 
towards measuring the theory against more challeng- 
ing data to test its validity; cases where ground- 
ing is less automatic are an obvious source of such 
tests, and we have identified a few relevant problem 
cases in the Autoroute dialogues. We do claim, how- 
ever, that the implementation as it stands validates 
a number of key aspects of the theory and provides 
a good basis for future work in dialogue modelling. 
Acknowledgments  
The TRINDI (Task Oriented Instructional Dia- 
logue) project is supported by the Telematics Appli- 
cations Programme, Language Engineering Project 
LE4-8314. Massimo Poesio is supported by an EP- 
SRC Advanced Research Fellowship. 
Re ferences  
J. Allwood. 1976. Linguistic Communication as 
Action and Cooperation. Ph.D. thesis, GSteborg 
University, Department of Linguistics. 
J. Allwood. 1994. Obligations and options in dia- 
logue. Think Quarterly, 3:9-18. 
M. E. Bratman, D. J. Israel and M. E. Pollack. 1988. 
Plans and Resource-Bounded Practical Reason- 
ing. Computational Intelligence, 4(4). 
P. Bretier and M. D. Sadek. 1996. A rational agent 
as the kernel of a cooperative spoken dialogue 
system: Implementing a logical theory of inter- 
action. In J. P. Miiller, M. J. Wooldridge, and 
N. R. Jennings, editors, Intelligent Agents III -- 
Proceedings of the Third International Workshop 
on Agent Theories, Architectures, and Languages 
(ATAL-96), Lecture Notes in Artificial Intelli- 
gence. Springer-Verlag, Heidelberg. 
J. Calder. 1998. Thistle: diagram display en- 
gines and editors. Technical Report HCRC/TR- 
97, HCRC, University of Edinburgh, Edinburgh. 
H. H. Clark and E. F. Schaefer. 1989. Contributing 
to discourse. Cognitive Science, 13:259-294. 
H. H. Clark and D. Wilkes-Gibbs. 1986. Referring 
as a collaborative process. Cognition, 22:1-39. 
Also appears as Chapter 4 in (Clark, 1992). 
H. H. Clark. 1992. Arenas of Language Use. Uni- 
versity of Chicago Press. 
P. R. Cohen and H. J. Levesque. 1990. Rational in- 
teraction as the basis for communication. I  P. R. 
Cohen, J. Morgan, and M. E. Pollack, editors, In- 
tentions in Communication. MIT Press. 
Discourse Resource Initiative. 1997. Standards for 
dialogue coding in natural anguage processing. 
Report no. 167, Dagstuhl-Seminar. 
B. J. Grosz and C. L. Sidner. 1990. Plans for dis- 
course. In P. R. Cohen, J. Morgan, and M. E. Pol- 
lack, editors, Intentions in Communication. MIT 
Press. 
J. Kreutel. 1998. An obligation-driven computa- 
tional model for questions and assertions in dia- 
logue. Master's thesis, Department ofLinguistics, 
University of Edinburgh, Edinburgh. 
S. Larsson, P. Bohlin, J. Bos, and D. Traum. 1999. 
Trindikit manual. Technical Report Deliverable 
D2.2 - Manual, Trindi. 
M. Poesio, R. Cooper, S. Larsson, D. Traum, and 
C. Matheson. 1999. Annotating conversations for 
information state update. In Proceedings of Am- 
stelogue 99, 3rd Workshop on the Semantics and 
Pragmatics of Dialogues. 
M. Poesio and D. R. Tranm. 1997. Conversational 
actions and discourse situations. Computational 
Intelligence, 13(3). 
M. Poesio and D. R. Traum. 1998. Towards an ax- 
iomatization of dialogue acts. In Proceedings of 
Twendial'98, 13th Twente Workshop on Language 
Technology, pages 207-222. 
M. D. Sadek. 1991. Dialogue acts are rational plans. 
In Proceedings o\] the ESCA/ETR workshop on 
multi-modal dialogue. 
M. P. Singh. 1998. Agent communication lan- 
guages: Rethinking the principles. IEEE Com- 
puter, 31(12):40-47. 
D. R. Traum and J. F. Allen. 1992. A speech acts 
approach to grounding in conversation. In Pro- 
ceedings 2nd International Conference on Spoken 
Language Processing (ICSLP-92), pages 137-40, 
October. 
D. R. Traum and J. F. Allen. 1994. Discourse obli- 
gations in dialogue processing. In Proceedings of 
the 32nd Annual meeting of the Association for 
Computational Linguistics, pages 1-8, June. 
D. R. Traum, J. Bos, R. Cooper, S. Larsson, I. 
Lewin, C. Matheson, and M. Poesio. 1999. A 
model of dialogue moves and information state re- 
vision. Technical Report Deliverable D2.1, Trindi. 
D. R. Traum and P. Dillenbourg. 1998. Towards a 
Normative Model of Grounding in Collaboration. 
In Proceedings of the ESSLLI98 workshop on Mu- 
tual Knowledge, Common Ground and Public In- 
formation. 
D. R. Traum. 1994. A computational theory 
of grounding in natural language conversation. 
Ph.D. thesis, Computer Science, University of 
Rochester, New York, December. 
8 
Pronomina l i za t ion  rev is i ted*  
Renate  Hensche l  and Hua Cheng ~md Mass imo Poes io  
HCRC,  University of Edinburgh, UK 
{henschel,huac,poesio}@cogsci .ed.ac.uk 
Abst rac t  
Pronolninalization has been related to tile idea 
of a local focus - a set of discourse entities in 
the speaker's centre of attention, for exmnple ill 
Gundel et al (1993)'s givenness hierarchy or 
in centering theory. Both accounts ay that the 
determination of tile tbcus depends on syntac- 
tic as well as pragmatic factors, but have not 
been able to pin those factors down. In this 
paper, we uncover the major factors which de- 
termine the focus set in descriptive texts. This 
new tbcus definition has been ew, luated with re- 
spect to two corporm museum exhibit labels, 
mid newspaper mtieles. It provides an opera- 
tionalizable basis for pronoun production, and 
has been implemented as the reusable module 
gnome-np. The algorithm l)ehind gnome-np is 
conlpared with the most recent pronoun gener- 
ation algorithm of McCoy and Strube (1999). 
1 In t roduct ion  
Besides the well established problem of ln'onoun 
resolution, pronoun generation is now attract- 
ing renewed attention. In the past, generation 
systelns generated pronouns without attaching 
much importance to the problem, one notable 
exception being the classical algorithm of Dale 
(1990), loosely based on centering theory. With 
the emergence of corpus based studies in comtm- 
rational linguistics, the question arises whether 
it is possible to refine known standard algo- 
rithms, or whether an improvement is only to 
be achieved with the hell) of world knowledge 
reasoning - a matter too complex to be dealt 
with reliably at this time. Tile Ibrlner direction 
is represented by tile pioneering work of McCoy 
and Strube (1999). They propose a refined algo- 
rithm for tile choice between definite description 
on the one hand and pronoun on the other for 
* The work reported in this paper has been calmed out 
with the tinancial support of UK ESPllC grant L51126. 
animate referents I , which is based on distancG 
time structure mid ambiguity constraints. 
Here we introduce a more general algoritlnn 
for the pronominalization decision that is valid 
not only for animate but for inanimate referents 
as well. In conformity with McCoy and Strube, 
we group noun phrases with definite determiner 
and proper nalnes together under tile term "det'- 
inite description". The algorithm proposes a 
new pronominalization strategy, which beyond 
McCoy and Strube (1999)'s criteria makes use 
of the discom'se status of the antecedent and 
parallelism effects. 
The algorithm has been implemented as the 
reusable module gnome-np. It has been re-used 
in tile web hypertext generation system ILEX 
(see Oberlander el; al. (1998)). It shows ml 
accuracy over 87% with respect o two corpora 
(each 5000 words) of difl'erent genres. 
2 Accounts  of  p ronomina l i za t ion  
In previous ace(rants pronominalization has 
been related to the idea of a local focus of ~d;- 
tention: a set of discourse referents who/wlfich 
is in the center of attention of the speaker (e.g. 
Sidner (1979), givenness hierarchy (Gun(lel et 
al., 1993), centering theory (Grosz et al, 1995), 
RAFT/RAPId. (Suri, 1993)). Whereas (Gundel 
el; al., 1993) do not atteml)t to make their fo- 
cus notion operationalizable, this has been at- 
tempted by fllrther develolmlents of centering. 
However these have mostly been applied to the 
pronoun resolution problem. In the following 
we discuss three versions of centering and show 
that their application to the pronoun generation 
problem is nevertheless linfited. 
Center ing .  Centering was developed to ex- 
plain local discourse coherence; the extent to 
which it benefits pronoun generation is how- 
ever not immediately clear, hi centering, 
1We llSO the terms "discourse ntity" and "refc'rent" 
synonymously in this paper. 
306 
the discour.~e ntiti(:s (:vok(:d in a (',(n'l;ain ui:- 
terall(;e 'tt i ga'e c~dle(t fi)rward-looldng centers 
(Cfs). It is assumed i;lmt they are 1)ari;ially or- 
der(:d. As a major dei;erminant of the ordering, 
the gramma.tical fun(:t;ion hierarchy (roughly: 
SUI/.I~>OIM>OTIII~;IIS) has 1)een 1)r,')t)o,~ed. 13e- 
CallSe other fa.cl;ors afl'e(:ting th(: ord(:r have no(; 
1)een (',lld)or;~ted in de(;ail, this ranking (as tit(' 
only Ol)erai, ionaliza/)le ha.n(lle) has 1)(:(x)m(: the 
si;:m(lard ra.nking in several comt)utational i)- 
1)\]ic~(,ions of (:entering. The 1)ackward-tooking 
center (hencefl)rth C1)) is a distinguished 1nero- 
bet of (;lie CI:~, which is defined as the most 
highly ranked member of the Ct5 of th(' previous 
lltterallee 'u , i_  \] which is realized in v, i .  The Cb is 
consid(:red as (;h(: \]o('al focus of ai;i;(:ntion. Cen- 
tering sta,t('s two rules. Only the first; rule makes 
~t (:brim at)()u(; l)ronominaliz~tion: ll!any (;lemen(; 
of the uti;eran(:e ui--1 is realiz(,,d in v,i as l)ro- 
nomh th(m l;he C\]) must l)e l)r()l~()minalized in 'it i 
as well. As lloted tw McCoy mid Struhe (\]999), 
this rule apl)lies only in (,h(; ease that (:we sul)se- 
qllent lltterallCeS share Hl()r('~ (;}l~tll o11(~ ref('A'ellt~ 
m:td (;hal; l;h(: non-el) ref(.'r(,.nt is 1)ronominalized 
in (,he se(xmd ui:termme. \]}ut why (;\]fis non-el) 
referent is realized as a, l)ronomt is not given t)y 
(~h(' (:heory. 
Itowev(:r, f()lk)wing mot(: (;he sl>irit of (xuli;er- 
ing tha.n the actual definition, ()he (:au under- 
stand (;he el) as (;he refin'ent which is prefer?d)lv 
l)ronominalize(t. General t)r(mominalizai;ion ()f 
the backward-looking center was in fact a claim 
of (mrly c(mtering, lmi; h~d 1;o l)e al)andone(1 be- 
cause of (:olmter-evidenee from r(,'al discourse. 
Bnt the id(::r (,ha.t t)l'on()nfinaliztti;ion of the Ct) 
could 1)e a, m(:ans of establishing lo(:td discourse 
(x)herence is still 1)revalenl;. \]t has accordingly 
})een use(t l)y seine generation systems to (:on- 
trol 1)ronominalization e.g. in the IIA,;X sys- 
l~em (O1)erlm~dcr et al, 1998), the el) is always 
realized as a pronomL 
Semant ic  center ing .  Centering is a.lso found 
in Dale (1990) as the method of t)ronominaliza- 
tion control. However, Dale's center detinii;ion 
differs from standard centering theory in i;hat it 
is defined semantically and not on the basis of 
a syni;aetie ranking. 2 This apl)roaeh has some 
appeal, espc.cially for generation, ix;cause it sup- 
l)ori, s the natural mo(hfla.rity bel;ween strate- 
2In 1)ari;icular, \])al(: adol)l;s (;he l'estll(; o1' I;he aci;ion 
(Ienol:ed 1)3' ihe previous claus(; of a recil)(: as (;he center. 
~i(: generation -wlfi(:h would determine t;he se- 
mantk: c(:nter for each uttera.llce and tactical 
g(:neration which dec.ides about granmmt:.ical 
fimetions. 
Funct iona l  center ing.  Finally, the cent;ering 
version suggest;ed \])y Stl'ul)e and Hahn (\]!)99) 
al>l)ears to r(:veal an underlying discourse mech- 
anism resl)onsil)le for centering: the information 
si;rH(:ture of  an u t terance  (roughly the given- 
new i)ai;t(:rn) is (;he de('l)er reason for the rank- 
ing ()\[ the %rward-h)oking (:eni:(:rs. This l)er- 
mii;s a generalization of sl;andar(l centering into 
a language-indel)ell(leld; l;heory eovering 1)oth 
trex: and fixed word-order languages. It is how- 
ever then surprising that this result is not made 
maximld use of in the Sltb.sc,,(lttent generaISon- 
orient, cA work of McCoy an(1 Strube (1999). 
Beyond center ing.  The questions wlfi(:h re- 
main ol)en with all (;hree al)l)roat:hes - stan- 
dard (;(;nterillg, S(:lll;~ll(;ic entering and fun(:-- 
tional eentering- are: 
\ ] .~ .~ \~?\]ly are ill real texts  a. lat%e nunl\])(:r of 
C1)'s not t)rononfinalized? 
_P21 \~q~y are non-C1)r(:ferents 1)ronominalized? 
or (:x1)ressc(l indel)(,.1Mently of centering: 
IP~I  \~qty are in real texl;s a large nunfl)(,r of 
(tis(:our,s(: entities with an ani;ecedent in i;h(', 
previous utteran(:e not l)ronomina\]ized? 
FI?2~ \NqW can more tlmn one entity 1)e t)ronom- 
inalize(l in one ui;t(,rance? 
\].h'on~ a corlms-driven view, question \ [~  is the 
larger prol)lem. 
~/\[(;C()y all(t Stl'lt\])e (19,()9) were the first to 
sng;~est all al~ol;il~llln for ~ellel'~ttioll which solves 
t;h(,se problems. It was motivated by the ol)- 
servation that ~t la.rge percentage of NPs which 
would have been realized by 1)ronouns using 
known algorithms, are in fact not, realized as 
pronomls in real text. They suggest that such 
NPs serve to mark ~time changes' in the dis- 
course. Their algorithm aecordingly makes use 
of distance, context ambiguity mid telnl)ora\] 
discourse structure to decide about 1)ronolni- 
nalization. In our work, we have considered a 
corpus of a ditl'erent genre in which I, emt)oral 
cha.nge does nol, 1)bW a determining role: de- 
script, ive (;exts. \?e 1)repose a new algorithm 
307 
that significantly simplifies the problem of pro- 
noun choice. It is based on a new definition of 
the local focus, which views the discourse status 
of the antecedent as the major motivation be- 
hind focusing. The algorithm performs equally 
well when applied to McCoy and Strube's cor- 
pus of newspaper articles. 
3 Corpus  ana lys is  
The algorithm we will present below has been 
developed in close relation to the MUSE corpus -
a corpus of museum exhibit labels a. The corpus 
is a collection of web pages of the Paul Getty 
Museum, pages from an exhibition catalogue, 
and pages froln a jewellery book. Typical char- 
acteristics are tile central role of inanimate ref- 
erents in these texts, and the lack of temporal 
change thus providing an interesting counter- 
t)art to the newst)aper genre investigated by Mc- 
Coy and Strube. 
With an overall set of around 5000 words, tile 
cortms contains 1450 NPs. Each NP has been 
annotated with respect o, among others, gram- 
matical function, discourse status, gender, num- 
ber, countability, and antecedent relationships. 
23% of the NPs form reference chains (i.e. at 
least two mentions of one and the same referent 
in one text), the other 77% are only mentioned 
once. We have 101 different reference chains; 
the chain-fbrming NPs fall into 10\] discourse- 
new and 213 anaphoric NPs. In the following, 
we will only discuss the anaphoric NPs. 50% of 
the anaphoric NPs are realized as definite de- 
scriptions, 50% as pronouns. We distinguish be- 
tween locally bound pronouns, which are deter- 
mined syntactically (Binding Theory, (Chore- 
sky, 1981)), and which we expect the tacti- 
cal generator to handle correctly, and pronouns 
which are not locally bound so-called dis- 
course pronouns. We investigated possible co l  
relations between the discourse 1)ronouns and 
semantic/pragmatic features of their context. 
The basic notions that we found were dis- 
tance, discourse status of the antecedent, and 
grammatical function of the antecedent. All 
three notions need a precise definition. 
D is tance .  ~lb be able to determine the dis- 
tance between a discourse entity and its an- 
tecedent, a precise determination f what counts 
3UI{L: h t tp  ://www. hcrc.  ed. ac. uk/ 'gnome/corpora 
as utterance unit is necessary. Following 
Kameyama (1998), we take as u t te rance  unit 
the finite clause, l{elative clauses and con> 
plenmnt clauses are not counted as utterances 
on their own. This means that we count 
clauses containing complement clauses or rel- 
ative clauses as single utterances. 4,a The pre- 
v ious u t te rance  is the preceding utterance at 
the same level of embedding. 
Note that we allow the treatment of clauses 
with VP coordination (subject ellipsis) as com- 
plex coordinated clauses as done in Kameyalna 
(1998), thus handling subject ellipsis as a dis- 
course pronoun; our algorithm does not; insist 
on this view however. 
The following correlation between pronoun 
use and distance was tbund in our corpus: 97% 
of the pronouns have an antecedent in the same 
or the previous utterance. 
D iscourse s tatus .  The information status of 
a discourse ntity in an utterance is either given 
or new. We use these terms with an identi- 
cal Lneaning as g~vn'nd and focus in Vallduvi 
(1993). Discourse status, as introduced by 
Prince (1992), is a similar but different notion: 
A discourse ntity is d iscourse-o ld,  if it has 
been mentioned before in the same discourse; 
it is d i scourse -new otherwise. All cases of 
givenness by indirect means like part-whole, 
set-member relationships, other bridging rela- 
tions, inferences (Prince's inferrables, anchored 
and situationally evoked entities) are judged as 
discourse-new, thus taking into account only 
tile identity antecedent relationship. We share 
Prince's opinion that pronominalization has to 
do with discourse status, whereas definiteness 
has to do with information status. 
66% of all short-distance discourse pronouns 
in the MUSE corpus refer to an antecedent which 
is in itself discourse-old. 
Sub jec thood .  The third strong correlation is
the relation between pronoun use and the grmn- 
matical function of the antecedent. 63% of dis- 
course pronouns have a subject as antecedent. 
The following table shows the overall distribu- 
tion of antecedent properties for short-distance 
4This deviates from Kameyama, who analyzes re- 
ported speech as separate utterance. 
'SComplement and relative clauses consisting of inore 
than one tinite clause create their own internal evel of 
focusing. 
308 
discourse 1}ronouns and (shown ill 1}rackets) h),' 
short-distance definite descriptions. 
old new 
,~,t)io{:{; a8% (22%) 25% (12%) 
~ot s,,1,.i 28% (18%) .(}% (48%) 
4 Algor i thm 
\] ~ased on these corpus  s{,ndy resnl|;s, we {lefhle a 
new notion of the local focus -- the set of refer- 
ents which arc awfilal)le for prononfilmlization. 
The local focus is ut}dated at each utterance 
boundary, and is defined as the set of referents 
of the 1)revious utterance which are: 
(a) d iscourse-o ld ,  or 
(b) rea l ized as sub jec t .  
This set {:all theoretically {:ontain \]11{}re thnu one 
\ ] . l t  ill \]HOSt cas,, s, O0 a.(1 (\])) are o,,o 
and the same singlel;on sol;, which coul{l be seen 
as the well-known Cb. Thus sta\]ldar(l cent{~r- 
ing app{,,a\]'s as ~t spe{'ial case of {}m a\])l}roa{:h. 
This account means that newly introduced r(;t L 
{;l'C\]l|,S arc ll(}l; immediately l)ron{}mina\]ized i\]1 
the following utterance, Ulfless they have bc{,.n 
introduced as subject- -ml ol)servation made l}y 
Brenlmn (19!}8) and now confirlile(t with respect 
to our data also. 
The l}rOl}osed efinition of the lo{:al focus ~,,ou-.. 
eralizes the t2){:using mechanisms assm~led i\]1 
{x211i,el'\]llg all(t intro(hlces the discourse status ()f 
the antecedent as (}he lnain {:rii;eriol~ })ehi\]\]d th{~ 
l}\]onomi\]laliza.ti{}n decision. It is i\]\]t{;\]'{>ting iT() 
lIOl,e I;ll:tl; \]X/i{:Coy all{1 Sl;rul}{; (1.9991 also nmke 
use of the discourse status of the ant(x:{,Aent; 
without mentioning it exl)lh:itly. For a. certah\] 
sul)sel; of' intrasentealtial nat)horic relal, ions ill 
amMguous cont(,xts they I)rOl}OSe \])ro\]\]on\]ina.1- 
ization in case the antec(;dent would 1)c the 1)re- 
ferred one in Stl'ut)e (1998)'s pr{}lloun resohl- 
lion algoritlnn. Because the set {}i' a.ntecedents 
is l'mlkcd there with resl)eCt to infbrm~tion sta- 
tus, this is identical with {mr proposal. Why 
tlmy do not use the discourse status as a gen- 
eral criBerion is not clear. We believe that the 
discourse status of the antecedent as pr{momi- 
nalization trigger is a general rule, in discourse 
Sell-l&ii/;,ies. 
The central role of discourse sLat;us and sub- 
jecthoo{1 are in our opinion 1101; accidental. The 
Bwo nol;ions retlecl; tw{\] tyt)ical stra{,egies 1;o 
introduce a new referenl; inBo l;he (liscourse. 
Wc will assume here the mnm~rked inf(}rnmt;ion 
structure of an utterance: g iven  - new. The 
subject usually is part of (or identical to) the 
9i'uen. Let X i)e a certain referent; which is newly 
intl'oduced in utterance (ul), and referred to 
again in t;lle following ui~tera.nce 0121. In the 
first strategy, X is introduced in the new non- 
subject )arl; of (u\].). And ill this l)ati;el'n the sec- 
ond lnention of X in (u2) is not pronominalized. 
In exalnple (1) given in Figure 1 tile local focus 
for ,,t ;el'a,,ce one el m0,1  : {t,.4; 
'm..,in 'morns" is new in (\]1\].) and \]1ol; pronominal -  
ized in (112). The other typical strategy is where 
the referent is tirst mentioned ill a subject posi- 
tion. This is typical for a segment onset, or the 
beginning of ~ text,. Ofl;en this referent is given 
})y other lneans -- for example, l)y refhrence to a 
1)icture., or to a r(;lated object. In example (2) 
of Figure \]., the second mention is i)rononfinal - 
ized. ~\]'\]ms 1;11(} sul)jecI; position seems to time- 
lion as creating a givemless allocation for the 
denotexl rcfercnl;. These two strategies roughly 
correspond with two types of thcnlatic develop- 
nlent identified in l)mm,q (197d). 
Para l le l i sm.  Our definition of l;he local tS- 
cus licenses 91% (62 of 68 pronouns) of all 
short-distance discourse pronouns in ore" corpus. 
Looking at tile pronomls violating the prol)osed 
accounl;, we nm.de ,,11 interesting observal;ion: 
n}osl; el l;heln occtlr ill conl;exts of strong t)ar- 
allelism. \'Vc call an anphoric NP ~/~,1~2 paral le l  
if it has ml a.ntccedenl; ' ~q)l in the previous utter- 
~Ill(;{;~ alld 'll,l) I alld '**,i12 \]rove Lhe 5alllC graummt i -  
cal funct ion.  1,k)l" work  wi th  real I;ex{;, il; is useful  
to inchlde cases whel'(~' 7L\]) 2 is a 1)osscssive or gen- 
itive NP inside a certain 'npa, and  'np\] and  'np:~ 
have the same gralnmatical flmction. Depend- 
ing on the concrel;e function, we distinguish sub- 
.ic{:t and object 1)arallelisln. Strong parallelism 
is a simulta.neous subject mid object para.lMism 
in two consecutive clauses. Strong i)arallelism 
always overrides the local focus criterion, mid 
allows tbr pronominalization of referents with 
discourse-new antecedents in nonsut)ject posi- 
tion. 
The local focus definition refilled by the par- 
allelism eff'ect is ml explanation for question P~ 
and a small portion of \ [~\ ] ,  but most cases of 
problem \ [~ r(nnain open.  q_'wo reasons for not  
prononfinalizing a reh~rent which is a nwanber 
of the local focus need to be considered: 
~ alnbiguous context, 
309 
(1) (\[11.) Shortly after irzh, eriti~,.q the building in 1752, he commissioned th, e areh, iteet Pierre Conta, nt 
d'Ivry to renovate the main  rooms. 
(u2) The engravings for these rooms , showing the wall lights in place, were reproduced in Dide~vt's 
Encyclopaedic, one of the principal works of the Age of E'nlightenment. 
(2) (ul) Scottish born, Canadian based jeweller, A l i son  Ba i ley -Smi th ,  constructs elaborate and cer- 
emoniaI jewellers} from irtd,ustrial wire. 
(u2) Her  materials are often gathered from so'arces such as abandoned television .sets ... 
(3) ~ i~ With attachments s'ach as an omtlav micvometer~ the microscope irtcovporates the latest sei- 
entitle technology of the mid-17OOs. 
(u2) The design of its era'ring gilt b~vnze stand was the heigh, t of the Rococo stifle ... 
(4) (u0) the table probably came from, the Tr ianon de Porcela ine , a small house built for th, e King's 
mistress, Madame de }l/\[ontespar~,, on the 9ro'unds of the Palace of Vet.sallies. 
(ul) This table's marquetry of ivory and horn, painted blue underneath, would have followed the 
house's  blue-and-white color sehcrne, imitating blue-and-white Chinese porcelain, a fashionable aTI.d 
highly prized material. 
(u2) Blue-arM-white cevarnie tiles decorated the house, ... 
Figure t: Corlms examples 
discourse structure signalling. 
Ambigu i ty .  Along with McCoy and Strube 
we argue that ambiguity with respect to gen- 
der/number influences the pronominalization 
decision: members of the local fbcus which have 
a competing referent (refbrent with similar gen- 
der/number) in some span to the left of the ref- 
erent to be generated should not be realized as 
t)ronouns o as to minimize the inference load 
for the reader. However, not to allow pronom- 
inalization in all ambiguous context situations 
does not ~I)t)ear to be consistent with real texts 
(McCoy and Strube, 1999). In the MUSE cortms 
one third of all focal NPs occur in ambiguous 
contexts, one half of them is pronominalized, 
the other half is not. Two questions require a 
precise answer to use the ambiguity constraint 
in a generation algorithm: 
? Which set of 1)reviously mentioned refer- 
ents or text st)an is taken into account br 
referents to be in competition? 
? Which referents are pronominalizcd despite 
an alnbiguous context? 
The answer is surprisingly simple: I/.eferents 
of the previous utterance which are not in the lo- 
cal fbcus do not disturb pronominalization, even 
if they have the stone gender/nmnber. Only if 
the actual referent has a competitor in the local 
fbcus, is pronominalization blocked. This is il- 
lustrated in Figure 1 with exmnples (3) and (4), 
respectively. In (3) the microscope is discourse- 
old and the only member in the local focus for 
(u2); the competing referents ocular micrvmc- 
tcr" mid technology are new and hence not local 
fbr utterance (u2). In (4), the local focus for 
(u2) is {the  at, th,  tl, e l,.o'asc} 
A slight improvement of the performance of 
the algorithm can be achieved by regarding 
the role of "heavy" nonrestrictive modification. 
hm\]uding the referents of discom'se-new NPs 
which are amplified by appositions or nonre- 
strictive relative clauses into the set; of' possible 
competitors improves accuracy slightly. 
D iscourse  s t ructure  signall ing. It is now 
known that detinite descriptions (or more gen- 
eral overspecified NPs) signal the start of a new 
discourse segment (Passommau, 1996; Vonk et 
al., 1992). For most generation systems gener- 
ate from an I/ST-like text; plan, discourse seg- 
ments are naturally given. The only question 
fl'om the generation perspective is the degree of 
detail provided by the segmentation. 
Our algorithm gnome-up assumes that the 
discourse segmentation has already been speci- 
fied. At each segment boundary, the local focus 
is set to n i l ,  thereby disallowing pronominal- 
ization for all discourse ntities of the first ut- 
terance in the segment onset. 
It is also well known that plmmed discourse 
with repeated phrases at the begimfing of a 
clause are seen as 'bad style'. Identical repeated 
pronouns at the clause onset are rarely found 
in expository and descriptive texts (2.6% of all 
discourse pronouns in our corpus). Hmnan writ- 
ers usually avoid possibly dull lack of variation 
by employing various aggregation techniques. 
310 
Let X 1)e a refl,'renl; I o 1)e generated in Ill;l;erailCO (112), and focu,.s 1)e the' scl; of rc'h;reni;s of the 1)rcvious 
ul;l;eran(:(; ul) which are 
(a) discoursc-okl, or 
(b) realized as subject. 
(1) X has an antecedent beyond a segment boundary def description 
(2) X has an antecedent two or more ul;i;cranccs distant def (lescril)tion 
(:~) X hits ~Ul alll;(X'(xl(,ll(; i l l  ( l l \ ] ) :  ~lll(l 
(3a) X occurs in strong 1)aralM contc'xt 1)ronoun 
(31)) X ? focu,.s (lcf dcscril)tion 
(3(:) X C .foc~z.s and 
? X has a coral)cling relbrcnt Y c focus dc'f description 
? X has a comp(~ting retba'cnt Y in (ul) amplitic(1 with appo- (lef dcscril)tion 
sition or nonrestrictive r lative clause 
? o lso  pro l lOl l l l  
The repeti|;ion 1)locking rul(; overri(lcs the 1)ronominalization suggesl;ed in (3c) to a definite description. 
Figme 2: The algoritlnn 
rl'hlls pronoun rel)etition 1)hwking seems 1.o Jw~ 
an aggregation trigger rather than ~ motivation 
for definite description generation. We hyl)ot\]> 
(;size l;hat t;he at)l)ar(mt Kcquen(:y of (lelinite (le- 
scriptions ill t)lmnm(l discourse has much to do 
with repetit ion blocking, but is used with re- 
specl; to a very line-grained, 1)tel)ably genre- 
specific discourse, si;rtlCl;lll'e. Olle candidate for 
this is the, t(,mt)oral structure in newst)at)er ar- 
i;ieles proposed by McCoy and Sta'ul)e. 
When evahutting Ollr algorM m l, w(' only used 
tile pa.l'agr~I)h seglnenl;~ti;ion given in the corpus. 
\]{lit for g;etlel'al;ioll systel l ls,  which usually sir(' 
not equil)l)Cd wit.h develol)ed a.ggrcgal.i(m eal- 
tries, we have also made avai\]ablc a t)ronoun rci> 
et,itioli blocking rule: If a discourse ntity in the 
local focus has a nont)ossessiv(~ l)ronomilml an- 
|;ecedelit, in'onolninalizal;ioli will 1)e J)loel?cd at 
this l/line. Figure 2 SUll:lnlarizcs the algorithm. 
The presented pronominalization algorithm 
has been implelnented ill the reusable module 
gnome-np, gnome-np consists of a colnponent 
for discom'se model lnanagement and one for 
NP form determination, it is designed to 1)e 
plugged ill ~~:\['1;(;1' text 1)lanning, coneeI)tualiza- 
ti(m, and sentence plalming, trot 1)etbre tactical 
generation. 
5 Eva luat ion  
A comparison of the t)erforlnance of our algo- 
r ithln with 1,he annotated MUS1.; corlms and Mc- 
Coy and Strube's newspatmr corlms is given in 
Table 1. The e, valuation has been carried out 
for the algorithm gnome-np without cm\])loying 
the rel)etith)n blocking rule and without; a line- 
grained discourse segmentation. Layout scg- 
lllell{;s Wel'e llse(l for the MUSE COl'l)llS. Beeal lse 
l/he munl)er of annotal;e(l seglnent OllSe\[;s Jill' the 
newsl)aper corpus is not easy to r('-estat)lish, wc 
giv(; here two figures fol" this eori)us: tirst with- 
out any segment, ons('t signalling (lower 1)ound), 
and second with the assulnt)l;ion that 15 short- 
distance definite (tcscriptions mark segment on- 
s<%s. The tigures include locally-herald l/re - 
nouns to yield J)(;tter cOlnl)arability wil;h McCoy 
and Sl;rul)e. '.\[lic, figur(,'s in l,hc, (:ohmms 'gnome- 
nil' represc, nl; I;\]lose NPs whose form is l)re(li(',led 
correctly 1)y 1;hi; new algoril;hm when evaluatc(l 
against l;h('~ a\]moi;at,(~(l corpora. 
The figures in T~d)le 1 show that  our al- 
gorithm performs very well in both domains, 
even without using a tiner discourse Seglnen- 
ration such as telnt)ol'al structure. Moreover, 
it; pertBrms better on McCoy and Stl'ul)e's cor- 
pus than their own algorithm, which success- 
fldly predicted the choice between realization by 
pronoml and realization by detinite description 
in 84.7% of all eases. The disagreements oc- 
('ur tirsl; tbr long distance t)rol~ouns (in our ter- 
lilino\]ogy: prollOtll lS lIlore than one clause dis- 
tanI;) and, second, ill hmger tel'trent chains with 
well established focus. For the latter, whereas 
gnome-np wouhl always suggest a tn'OlmUn, the 
real discourse swaps betweeli pronoun mid deft- 
nile description. Thus a finer segmentat ion or a 
repetit ion blocking rule could still improve the 
result fllrther. 
311 
MUSE gnome-r ip agreement  newspaper  gnome-np  agreement  
pronouus 112 101 90.2% 302 267 88.d% 
def descril)tions 101 86 85.1% 225 187 202 83.1% 89.70{o 
tota l  213 t87 87.8% 527 454 469 86.1% 89.0% 
'Dtble 1: Per formance compar ison 
6 Conc lus ions  
This  paper  has presented a new a lgor i thm tbr 
the pronomina l izat ion of third person discourse 
entit ies. The  Mgorithm, first, is imp lemented  
as a reusable module tbr generat ion systenls 
and, second, provides a theoret ical  account of 
pronomina l i zat ion  i general. 
The  proposed a lgor i thm provides a solution 
for quest ion \ [~  above by widening the defini- 
t ion of local Ibcus to be a set with possibly more 
than  one referent. The Mgor i thm also oilers a 
new solution fbr prob lem \ [ ~  above, aml)igu- 
ous pronoun generat ion.  Discom'se s t ructur ing 
(~\ ] )  is assumed as given. A sufficiently fine- 
grained discourse structur ing has been explored, 
for example,  by McCoy and Strube fbr their  do- 
n lmn of newspaper  articles, but  remMns an issue 
fbr future research fbr other domains.  We have 
shown that  next to 1)roxilnity, the discourse sta- 
tus of the mltecedcnt is a main cr iter ion for trig- 
gering pronomilml izat ion.  
The  suggested a lgor i thm general izes known 
fbcusing accounts. Gundel  et al (1993)'s cog- 
nit ive slat, us of being "in fbcus" is now approxi-  
mated  by the set of all discourse-old entit ies and 
the subject  of the previous utterance.  The  new 
focus determinat ion  is also a general izat ion of 
center ing's  Cb. The  focus so defined serves two 
funct ions s imultaneously:  to tr igger 1)ronomi- 
nMization, and to provide the set; of compet i tors  
for pronoun generat ion in ambiguous  contexts.  
A l though our t ra in ing corpus is too small  to jus- 
t i fy general  clMms, the ewduat ion with respect 
to tile newspaper  genre provides evidence that  
this f inding is valid for p lanned discourse ill gen- 
erM, independent  of the concrete genre. 
References 
Susan Brennan. 1998. Centering as a, psychological re- 
source for achieving joint reference in spontaneous 
discourse. In Marilyn A.  Walker, Aravind K..loshi, 
and Ellen F. Prince, editors, Centering Theory in Dis- 
course, pages 227 - 250. Clarendon Press, Oxtbrd. 
Noam Chomsky. 1981. Lectures on government and 
binding. Foris, l)ordrecht. 
Robert Dale. 1990. Generating referring cxpression.s. 
The MIT Press, Cambridge, Massachusetts. 
Fl'anti~ek Dane~. 1974. \]hmctional sentence perspective 
mid the orga.nisa.tion of the text. In Frantigek Dane~, 
editor, Papers on Functional Sentence Perspective, 
pages 106 128. Academia, Prague. 
Barbara J. Grosz, Aravind K. Joshi, and Scott Wein- 
stein. 1995. Centering: A fl:amework for modelling 
the local coherence of discourse. Computational Lin- 
guistics, 21 (2):203 16d.  
Jeanette K. Gundel, Nancy Iledberg, and Ron Zaeharski. 
1993. Cognitive status aim the tbrm of rethr,:ing ex- 
pressions ill discourse. Lang'uagc , 69:27d 3(}7. 
Megulni Kameyama. 19.(t8. lntrasentcntial centering: A 
case study. In Marilyn A. Walker, Aravind K..Joshi, 
and Ellen F. Prince, editors, Centering 7'hcory in Dis- 
course, pages 89 -- 114. Clarendon Press, Oxtbrd. 
Ka.thlcen McCoy a,nd Michael Strube. 1999. Generating 
anaphorie xpressions: Pronoun or delinite descrip- 
tion? In Proceedings of ACL '99 Workshop: Refer- 
ence and discourse structure, pages 63 -- 71. 
J. Obcrlander, M. O'Donnell, A. Knott, and C. Mellish. 
1998. Conversation in the museuln: experiments in
dynamic hypermedia with the intelligent labelling ex- 
I)Iorcr. New l~,(:view of Multimedia and Hypermedia, 
pages 11 32. 
l/,ebecca Passonlmau. 1996. Using centering to re- 
lax gricean constraints on discourse anaphorie noun 
phrases. L(tngu, age wn, d ,qp('.ech, 39(2):229-- 264. 
Ellen F. Prince. 1992. The ZPC letter: Subjects, deli- 
niteness aim inforina.tion status. In W. C. Mam~ and 
S. A. Thompson, editors, Discourse desciption: Di- 
verse linguistic anab.lSCS of a flmd-raisi.ng text..lohn 
Benjamins, Amsterdam. 
Cmldace L. Sidner. 1979. 7bwards a computationally 
theory of definite anaphora comprehension i  English 
disourse. PhD thesis. 
Michael Strube and Udo ItMm. 1999. Functional cen- 
tering - grounding referential coherence in int'orma- 
tion structure. Computational Linguistics, 25(3):309 
- 344 .  
Michael Strube. 1998. Never look back: An alternative 
to centering. In Proceedings of Coling-ACL '98, pages 
1251 - 1257. 
Linda Z. Suri. 1993. Extending focussing frameworks to 
process complex sentences and to correct the written 
English of proficient signers of American Sign Lan- 
guage. PhD thesis. 
Enrico VMlduvi. 1993. lMbrmation packaging- a survey. 
Technical rel)ort, HCRC research Pal)er RP-d4. 
W. Vonk, G. Hustinx, and W. Simons. 1992. The use of 
referential expressions in structuring discourse. Lan- 
guage and Cognitive P~vcesses, 7(3 /4) :301  -333 .  
312 
Corpus-based Development and Evaluation of a System for Processing 
Definite Descriptions 
Renata Vieira 
Universidade do Vale do Rio dos Sines 
Av. Unisinos 950 - Cx. Postal 275 
93022-000 S~o Leopoldo RS Brazil. 
renata@exatas, unisinos, br 
Massimo Poesio 
University of Edinburgh 
HCRC and Informatics 
Edinburgh, Scotland 
Massimo. Poesio@ed. ac.uk 
Abstract 
we present an implelnented system for processing 
definite descriptions. The system is based on the re- 
sults of a corpus analysis previously reported, which 
showed how common discourse-new descriptions 
are in newspaper corpora, and identified several 
problems to be dealt with when developing compu- 
tational methods for interpreting bridging descrip- 
tions. The annotated corpus produced in this ear- 
licr work was used to extensively evaluate the pro- 
posed techniques for matchiug delinite descriptions 
with their antecedents, discourse segmentation, rec- 
ognizing discourse-new descriptions, and suggest- 
ing anchors for bridging descriptions. 
1 Motivation 
In previous work (Poesio and Vieira, 1998) we re- 
ported the results of corpus annotation experiments 
in which the subjects were asked to classify lhe uses 
of delinite descriptions in Wall Stree Journal arti- 
cles according to a scheme derived from work by 
Hawkins (1978) and Prince (1981 ) and including 
three classes: I)II{I'~CT ANAPltORA, I)ISCOURSE- 
NEW, and I~RIDGING DESCI{II'TION (Clark, 1977). 
This study showed that about half of the time, deli- 
nite descriptions are used to introduce a new entity 
in the discourse, rather than to refer to an object al- 
ready mentioned. We also observed that our sub- 
jects didn't always agree on the classification of a 
given delinite; the problem was especially acute for 
bridging descriptions. 
In this paper, we present an implemented system 
for processing delinite descriptions based on the re- 
suits of that earlier study. In our system, techniques 
for recognising discourse-new descriptions play a 
role as ilnportant as techniques for identifying the 
antecedent of anaphoric ones. The system also in- 
corporates robust techniques for processing bridg- 
ing descriptions. 
A fundamental characteristic ofour system is that 
it was developed so that its perfomlance could be 
evaluated using the annotated corpus. In the papm; 
we discuss how we arrived at the optimal version of 
the system by measuring the performance of each 
method in this way. Because of the problems ob- 
served in our previous tudy concerning agreement 
between annotators, we evaluated the system both 
by measnring precision/recall against a 'gold stan- 
dard' and by meastu'ing the agreement between the 
annotation it produces and the mmotators. 
2 General Overview 
At the moment, the only systems engaged in se- 
mantic interpretation whose performance can be 
evaluated on l'aMy unrestricted text such as the 
Wall Street Journal articles are based on a shallow- 
processing approach, i.e., that do not rely on exten- 
sive amounts of hand-coded commonsense knowl- 
edge (Carter, 1987; Appelt, 1995; Humphreys et al, 
1998). I Our system is of this type: it only relies 
on structural information, on the infommtion pro- 
vided by pre-existing lexical sources uch as Word- 
Net (Fellbaum, 1998), on minimal amouuts of gen- 
eral hand-coded iuformation, and on information 
that can be acquired automatically from a corpus. 
Although we believe that quantitative ewduations of 
the performance of a systeln on a large number of 
examples are the only true assessment of its perfor- 
mance, and therefore a shallow processing approach 
is virtually unavoidable for implemented systems 
until better sources of commonsense knowledge be- 
come available, we do know that this approach liln- 
its the performance of a system on those instances 
of definite descriptions which do require common- 
sense knowledge for their resolution. (We grouped 
these in what we call the 'bridging' chtss.) We 
I Most systems participating in the Message Understanding 
Conference (MUC) evaluations are customized tospecific do- 
nmins by adding hand-coded commonsense knowledge. 
899 
nevertheless developed heuristic techniques for pro- 
cessing these types of delinilcs its well, which n/ay 
provide a baseline against which the hains in perfor-- 
nlanco (\]lit to tile llSO oi: COlllnlOllSellse knowlodoe  
can be assessed more clearly. 
Our system attempts to classify each deli- 
nitc description as either I)IRIX:T ANAI' I tORA, 
I)IS('.()UI~,SIT,-NI';W, all(t IgRII)GING I)lv.S(21{II'TION. 
The lirsl chlss includes deihfite descriptions whose 
head is identical to thai o1' their antecedent, as in 
a Iiouse ... lhe house? The second includes del L 
inile descriptions that refer to objects not alma@ 
mentioned in the text and ,lot rclated to any such 
object. (Some of these definite descriptions refer 
to objects whose existence is widely known, such 
as discourse-initial references to lhe i)ot;e; other in- 
stances of discourse-new descriptions refer to o1:> 
jects thai can be assumed to bc unique, even if unfa~ 
miliar, such as lhe.filwl woman lo climb all Scollish 
Mum'os.) lqnally, we classify as bridging descrip- 
tions all dclinitc descriptions whose resolution de-. 
pen(Is on knowledge of relalions between objects, 
such as delinite descriptions thai refer to an object 
rehlted 1o an entity ah'eady introduced in the dis-- 
com'se by a relation other than identity (Prince's 
'inlerrables'), as in the f la t . . ,  the living, room; and 
de/tulle descriptions that refer an object aheady m 
troduced, but using a different predicate, as in Ihe 
car . . . lhe  vehicle, hi addition to this chlssitica- 
lion, the system iries to identify the antecedents 
of anaphoric descriptions and the anchors (Fraurud, 
1990) of bridging ones. Accordingly, we developed 
three types of heuristics: 
, for resolving directly anaphoric descriptions. 
These iuclude heuristics for dealing with see- 
mentation and to handle modificatiou. 
? for identifying discourse-new descriptions. 
Some of these heuristics attempt to recognize 
semantically lunctional definite descriptions 
(Hawkins, 1978; Loebner, 1987), whereas oth- 
ers try to recognize definite descriptions that 
are anchored via their modification (Clark and 
Marshall, 1981; Prince, 1981). 
,, for identifying the anchor of a bridging de- 
scription and the semantic relation between the 
bridging description and its anchor. WordNet 
is accessed, and heuristics for named entity 
recognition were also developed. 
The final configuration of the system was arrived 
at on the basis of an extensive wfluation of the 
hemislics using Ihe corpus annotated in our previ- 
ous work (Poesio and Vieira, 1998). The evaluation 
was used both io detemfine whMl version o1' each 
heuristic worked belier, and to identify the best o1", 
der in which to try them. 
The corl)us we used consists of 3d. texts frolu/he 
Peun Treebank I included in the ACIJl)CI (\]\])-rolu. 
20 of lhese texts were treated its 'training,, corptls'; 
this corpus contains 1040 (lclinite descril)tions, ()l' 
which 312 arc anaphoric, /192 discernso-.new, and 
204 bridging, id more texts were used as 'test 
corpus'; fllesc include d64 delinile description,% of 
which 154 haw: been classified its anaphoric, 218 as 
discourse-new, and 81 as bridging. 
3 Tlhe Hem~istics And  The i r  Per fbrmance  
3.1t /Resolving Anaplhorie Detinites 
We discuss heuristics for two sut)problcms of lhe 
lask o\[ resolving anaphoric dclinites: limitin?; ihe 
accessibility of discourse entities (seomcnlation), 
and |aking into accotln/ 1t4e information given by 
pre- and post-modiliers. See (Vieira, 1998) for a 
discussion of tile other heuristics used by the sy> 
tonl. 
Segmentalion In 
l i IE-spans limited 
MI",NTS t1131 l l lay 
general, discourse entities have 
to pra?matical ly delermined ,Slit;.. 
be nested (see, e.g., (Rcichman, 
1985; Grosz and Sidner, 1986; Fox, 1987)). E.g., 
in our corpus we found that about 10% of direct 
anal~horic detinite descriptions have more than one 
possible antecedent if segmentalion is nol taken into 
account (Vieira and Poesio, 1999). Recognizing 
the hierarchical structure of segments in a text is, 
howevm; still pretty umch an open problem, kS it 
involves reasoning about intentions; 2 better results 
have been achieved on the simpler task of 'chtlnk- 
ing' the text into approximate segments, generally 
by means of lexical density measures (Hearst, 1997) 
In fact, the lnethods Io limit the lifespan of dis- 
course entity we considered for our system were 
even simplel: One type of heuristics we looked 
at are window-based techniques, i.e., considering 
as potential antecedents only the discourse ntities 
within fixed-size windows of previous entences, al- 
lowing however for some discourse ntities to take 
a longer life span: we call this method LOOSE SEG- 
MENTATION. More specifically, a discourse entity 
is considered as potential antecedent for a definite 
2See, howeve,, (Marcu, 1999). 
900 
doscril)iiol~ w\] l ( : l i  lhc mi{oc~:dclii'~; head i~ idcniic~ll 
lo lhc doscril)lioli~s licad, nnd 
o lho poicnlial anlcc:cdoniL <', (li,,4ailcc lroul llic do-. 
,smrip/ioii b; \vidiin Ihc es/ab\]ishod window, or 
c;i sc 
o ihe ix)lcnliai aulcccdcnl is il~;cll-~t stil)~;t:ClllCli{ 
i l lC i l l iO l l ,  o r  c ls( :  
o ill~; (lciilfi ic dcscrif)iitni alid lhc 4iiicc:cxJoiii ;llC 
ktcnllcal Ni's (i i iohidhig ilic ariiclc). 
Wo a lso  coiisido'rcd mi c'vcii shill)lor I',i,;(:i,.NCY 
hc:uristic: fllis involw:s kt:cl)iu ?, a l~ibl,: iiidc:xcd l)y 
ihc" hc, ad.'; o\[  i)()ic-ntial nuli'x:cd(;lli,<;, ';llch dial the cn- 
i r j  for i iO l l i l  \[\I c(;iliahl<~ ihc iilc!cx oi il ic I:isi t)cciil (. 
rciio(', t)\[:ilii 4ui(tcc'.dciii wilh licad N. i:iiulliy~ wc (',oii- 
,<;i<icri'd moiul)hiaii{)ny, <fl~;('l,, c. lllllitiii ;iiid rc~(:cuc3,. 
Tli(~ be',<;1 rl'!;ull,<; wi;rc obi;ihicd wilh a couit)i- 
iia{i()n ()1 ilic: rccmicy mid ,<;c;?,iii('.ul~liion hcuri<~lic,<,: 
.iu,'a o,c  polciiii41 anicccdclli \[or CilCii dJl\]orc, iH lioad 
liOtlil iF, awlilal:)lc Jor r~soluiion> l lw la!d oC('llrfOllCO 
()t  ihai hc41d ilOlili. Tilt; rc,solutic)u ,~lill re.six:oh; lhc 
sc:Oill{:ill;lliOii h?llrislic (I()()SO vcrxioli). ' I ' l l ( ;  rc:(:;ll\] 
(J:,), i)ro(;i,';ioit (P) all(l IL-illt:ilStlrc (l;) ft:stlllt; Io\] l\[lo 
lwo hcurislics ;irc i)rc:sc;ntt:d hi 'l\]iblc I. 
( kmfl>iucd lic/u i:-ilics I l> I i' I I: 
"I ?CIIICIICCS -I rt}ct~llcx/ 1 "/~7-()()(A' KI.77<A ' S i. '!'7!(/< 
{~ SCIIICIICCS I rt'CCilCy I "li.~I4'J S,I\]I)()<,/~ ~gi.'~7'/<, 
'lhl)lc: I: (:OlilI)iilii/~t Ioo~;r ,'-;t;l'JllC:ili~lliOl! ;llltl i'(; 
CCiiCy lict;ri',dics 
The vcr<<doli with liighc:r I,' value: iu 'lhhlo I (4- 
.~;ciitt;liOC window i)his rt:ct:llcy) \vat.; dlO.'4t;ii aud 
u,.4od hi ill(" icsl,<; di,~mus.sod hi /he icst <ff lliis ,<;cc/iou. 
/Vom~ Mod~fie#;~' lll 9,onoial, wlioii niaichhi~,~ a (Ioli- 
liito doscriplion wilh a i)olt;utial aillocodOlit tho in- 
\[orillatiOli provided hy lho pfCllOlllhlaI ;Jild lhc l)OSl - 
noinifial pni( of lJic ll<)tili i)hraso also ll;i<<; lo I)o lakon 
iltto aoco/inl: SO, lor  O?alnplo, ~t i)\[ue cdv Cailll()t 
SClVO ~lS the anlcccdonl for l/re red cas, of lhe hottxe 
on the 1(',./7 for fl#e house on lhe righ#. ' lt iking l)ropor 
care of  the scniantic contril)uiion of these prcnlod- 
iliers would, in ~oiloral, roquiro OOliimOllSt'nso roa -  
lhc  <'-;laiidard dchnihOliS of precision aild recall ffoill ilif()i- 
n la l ion  r t : l r i cva i  \VOfC used: R :: i l l ln lbc,  r of ottIocIS ('f type A 
corrcclly idcnlilicd I)y the system / tolal lltlmbcr of objccls o1" 
lyi)e A, I ) = llunfl~c:r (fl" corrccl ideniilicali(ms of oh.iccls of lypc 
A / l\[)I~11 nund}cf of objects of lyl)c A identilicd by file system, 
1;:~ RI)/1~,+1 ). 
,<;olihiF, ~ Jk)l- ilic; lilonit:lil, WC unly dcvclolxxl hc.uiiv 
iic sohilioii.~> to ihc prol)lcni> hichidhil,.: 
al lowiug all anlcccdcnl to n-ialch with a dcli 
nik; dtJscripl\]oli i I  lho l)lcn\]odificrs ol: tile d('- 
scrip/ion illt: ~1 ,'4ttl),'4t;l t)l lhc l)rc',niodiiiurs (>1 
ihu, aiilcccdciii. 'rhi,<~ heuristic deals wiih &' l  t- 
hiiiot-; whicl i  mmlain io~,s inlornmlion lhan lilt; 
;tillt;t;c;dc;li/~ Silch ~i,<; f i l l  0/(\[  VicloFidn hotlmd... 
/hu /IOlt,~d~ ~lli(t t)rt:vt;il/S Illli/c;ho,~; ,~;uch zis ih? 
/)!/A'iH(tXX COIIHH\[Illi\[Y... lhc fOlllL~(t#; IIIf)F(f (l(;-. 
i ivixl I)l(/(:k l)oliii(:al (:ommus~i/y. 
~, al lowing a n(m-i)rolnodilicd auteccdoilt lo 
lllaich wilh lilly SalllO head definite. Tll is t ;cc  
oud ht;ulixlic (!c'~il~; with dc, iinitc'.~ lhai (:onlilhl 
nddilioii;il i i l \[ornlaiion, such ;l!', (~ check?. ,  llm 
Io:>7 check. 
The ll'MIIi,~; o\[  ~)lu i)roniodiJit;r in,lithium, ~ll~to-. 
ri lhni art: l);cs(:iilod hi Table 2. hi lhai Tal)lc; we: al,~;o 
.<;how lhc: fO.~lilb; oblahlod wilh a niodiliod lnatchhig 
algoril lnn hicludhil,> ~i ihird rulo> thai allows ;i lm; ? 
modified nnlLc;Ccdoni io nialch with a dclhfiio wl~o,~;o 
xcl o l  i)rc-liiodilic;rs i,s a SUl)Oisol of lhc s('i of iuodi- 
lit'r.~ o1: lho aiilccc, donl (Jill claboralion (ff rule 2). \Vc 
Ic~dcd ciicli of lllcso lhrt;o hourislics alollt: mid ihcir 
coiiil)iiialit)l/,<;. (Tlic. Iourih liliC .~hnl)ly lc;i)~',;il,~ lli~: 
lc, sldis showu in ' lhl4c 1.) 
Alllcccdci/\[s clcclion 14 1 1' 
I. Aiil -,';till )csc'- subscl ()9.197</< , 91.21% 
2. Ani-cinply 3_'i. 12% <$8.2()</~ 
3. Ant suhsct/I)csc, sci ().I.'t4{A 
i,i,a 7 0.i~<: ,,.> 7s-'~U 15/.H% 
1 and 3 "15.96% ~1.13% 
None 7g.. ~2% g .03% 
Tal)h" ?,: l:,valualioll of ihc houristics 
cqlIoII (Vt'l'Sit)ll 1 ) 
I I: 
7g. I "%4 
I 0',I.~59; 
'iq.,'11(/< 
~J..,!:,1% I 
,*~ I. I (t/< 
~(. o% 
Ik)r laCJm)dili- 
The I)csl inccisiol\] is achieved by lhc niatchin~,, al. 
gori l lnn Ihm does not  allow for new information in 
tilt; anaphoric expression, but i l ic best results ovc> 
all arc again obtained by combining rule I and rule 
2, alfllough oilhcr 2 or 3 works equally wcll when 
combinod with 1. 
Ovc#wll results .for amqUioric Ue.finite Uescriplions 
'lk) sumnm|'izc, lhc version of the system that 
achiow, s the best rcsulls as far as anal)hOl'iC dcl ini lc 
descriptions arc concerned includes : 
I. combined scgmmHation and roconcy~ 
901 
2. 4-sentence window, 
3. considering indefinites, definites and posses- 
sives as potential antecedents (Vieira, 1998), 
4. the pmmodification of the description must be 
contained in the premodificatiou of the an- 
tecedent when the antecedeut has no premodi- 
tiers. 
3.2 Heuristics for Recognizing Discourse-New 
Descriptions 
As mentioned above, a central characteristic of our 
system is that it also includes heuristics for recog- 
nising discourse-new descriptions (i.e., definite de- 
scriptions that introduce new discourse ntities) on 
the basis of syntactic and lexical features of the 
noun phrase. Our heuristics are based on the dis- 
cussion by Hawkins (1978), who identified a num- 
ber of correlations between certain types of syntac- 
tic structure and discourse-new descriptions, par- 
ticularly those that he called 'unfamiliar' definites 
(i.e., those whose existence cannot be expected to 
be known ou the basis of generally shared knowl- 
edge), including: 
? the presence of 'special predicates':4 
- the occurrence of pre-modifiers uch as 
first or best when accompanied with full 
relatives, e.g., the .\[irs't peJwon to sail 
to America (Hawkius calls these 'un- 
explanatory modifiers'; Loebner (1987) 
showed how these predicates may license 
the use of definite descriptions in an ac- 
count of definite descriptions based on 
functionality); 
- a head noun taking a complement such 
as the fact that there is li\['e on Earth 
(Hawkins calls this subclass 'Nt' comple- 
ments'); 
? the presence of restrictive modification, as in 
the inequities of the current land-ownership 
system. 
Our system attempts to recognize these syntactic 
patterus; in addition, it considers as unfamiliar some 
definites occurring in 
4This list was developed by hand; more recently, Bean and 
Riloff (1999) proposed methods for autolnatically extracting 
fl'om a corpus uch special predicates, i.e., heads that correlate 
well with discourse novelty. 
? appositive coustructions (e.g., Glenn Cox, the 
president of Phillips Petroleum Co.); 
* copular constructions (e.g.,the man most likely 
to gain custody of all this is a career politician 
named David Dinkins). 
In our corpus study (Poesio and Vieira, 1998) 
we found that our subjects did better at ideutify- 
ing discourse-new descriptions all together (K=.68) 
than they did at distinguish 'unfamiliar' from 'larger 
situation' (Hawkins, 1978) cases (K = .63). This 
finding was confirmed by our implementation: al- 
though each of the heuristics is designed, in princi- 
ple, to identify only one of the uses (larger situation 
or unfamiliar), they work better when used all to- 
gether to the class of discourse new descriptions. 
The overall recall and precision results for the 
heuristics for identifying discourse new descriptions 
are shown in Table 3. In this Table we do not distin- 
guish between the two types of discourse-new de- 
scriptions, 'unfamiliar' and 'larger-situation'. The 
column headed by (#) represents the number of 
cases of descriptions classified as discourse new in 
the standard annotation; + indicates the total num- 
ber of discourse-new descriptions correctly identi- 
fied; - the nmnber of errors. These results are for the 
version of the system (version 1) that uses the best 
version of the heuristics for dealing with anaphoric 
descriptions discussed above, and that doesn't at- 
tempt o resolve bridging descriptions. 
I Discourse new l# \] + l -  I P" I P I F I 
Training data 492 368 60 75% 86% 80% 
Test data 218 151 58 69% 72% 70% 
Table 3: Evaluation of the heuristics for identifying 
discourse new descriptions 
3.3 Bridging Descriptions 
Bridging descriptions are tile class of definite de- 
scriptions which a shallow processing system is 
least equipped to handle, and therefore the most cru- 
cial indicator of where commonsense knowledge is 
actually needed. We knew from the start that in 
general, a system can only resolve certain types of 
bridging descriptions when supplied with an ade- 
quate kuowledge base; in fact, the typical way of 
implementing a system for resolving bridging ref- 
erences has been to restrict tim domain and feed 
the system with hand-coded world knowledge (see, 
e.g., (Siduei; 1979) and especially (Carter, 1987)). 
902 
Furthermore, the relation between bridging descrip- 
tions and their anchors may be arbitrarily complex 
(Clark, 1977; Sidnm; 1979; Prince, 1981; Strand, 
1996) and our own results indicate that the stone de- 
scription may relate to different anchors in a text, 
which makes it difficult to decide what the intended 
anchor and the intended link are (Poesio and Vieira, 
1998). Nevertheless, we feel that trying to process 
these definite descriptions is the only way to dis- 
cover which types of commonsense knowledge are 
actually needed.. 
We began by developing a classilication of bridg- 
ing descriptions according to the kind of informa- 
tion needed to resolve them, rather than on the ba- 
sis of the possible relations between descriptions 
and their anchors as usually done in the literature 
(Vieira, 1998). This allowed us to get an idea 
of what types of bridging descriptions our system 
might be able to resolve. We classified definite de- 
scriptions as follows: 
? cases based on well-delined lexical relations, 
such as synonymy, hypernymy and meronymy, 
that can be found in a lexical database such as 
WordNet (Fellbaum, 1998)-as in thef lat. . ,  lhe 
living room; 
? bridging descriptions in which the antecedent 
is a proper name and the description a common 
noun, whose resolution requires some way o1' 
recognizing the type of object denoted by the 
proper name (as in Bach ... the composer); 
? cases in which the anchor is not the head noun 
but a noun modifying an antecedent, as in the 
compaw has been selling discount packages 
... the discounts 
? cases in which the antecedent (anchor) is not 
introduced by an NP but by a vl', as in 
Kadane oil is currently drilling two oil wells. 
The activity...  
? descriptions whose the antecedent is not ex- 
plicitly mentioned in the text, but is implicitly 
available because it is a discourse topic-e.g., 
the industo, in a text referring to oil coml)a- 
nies; 
? cases in which the relation with the anchor is 
based on more general commonsense knowl- 
edge, e.g., about cause-consequence relations. 
We developed heuristics for handling the first 
three of these classes: lexical bridges, bridges based 
on names, and bridges to entities introduced by non- 
head nouns in a compound nominal. We refer the 
reader to (Vieira, 1998) for discussion of the heuris- 
tics for this last class. 
Our system attempts to resolve lexical bridges by 
consulting WordNet to determine if there is a se- 
mantic relation between the head noun of the de- 
scription and the head noun of one of the NI'S in the 
previous five sentences. The results of this search 
for our training corpus, in which 204 descriptions 
are classified as bridging, are shown in Table 4. It is 
interesting to note that the semantic relations found 
in this automatic semch were not always those ob- 
served in our manual analysis. 
Bridging 
Class 
Synonimy 
Hyponimy 
Meronimy 
Sister 
Relations 
Found 
11 
59 
30 
Total 106 30 
Right % 
Anchors Right 
4 36% 
18 30% 
2 33% 
6 20% 
28% 
Table 4: Ewduation of the search for anchors using 
WordNet 
We developed a simple heuristic method for as- 
signing types to named entities. Our method iden- 
titied entity types for 66% (535/814) of all names 
in the corpus (organizations, persons and locations). 
The precision was 95%. We could have had a better 
recall if we had adopted more comprehensive lists 
of cue words, or consulted dictionaries of names 
as done for the systems participating in MUC-6. 
There, recall in the named entity task varies t'rom 
82% to 96%, and precision l'rom 89% to 97%. 5 
4 Overall Evaluation of the System 
The order of application of heuristics is as impof  
tant as the heuristics themselves. The final order of 
application was also arrived at on the basis of an ex- 
tensive evaluation (Vieira, 1998), and is based on 
the l'ollowing strategy: 6 
5A more recent version o1' the system using the named en- 
tity recognition software developed by ItCRC for the MUC-7 
competition (Mikheev el al., 1999) is discussed in (Isbikawa, 
1998). 
aWe also attempted tolearn the best order of application of 
the heuristics automatically b  means of decision tree learn- 
ing algorithms (Quinlan, 1993), without however observing a
signflicant difference in pcrfommnce. See (Vieira, 1998) for 
details. 
903 
An Empirically Based System for 
Processing Definite Descriptions 
Renata Vieira* 
Universidade do Vale do Rio dos Sinos 
Mass imo Poesio t
University of Edinburgh 
We present an implemented system for processing definite descriptions in arbitrary domains. 
The design of the system is based on the results of a corpus analysis previously reported, which 
highlighted the prevalence ofdiscourse-new descriptions in newspaper corpora. The annotated 
corpus was used to extensively evaluate the proposed techniques for matching definite descriptions 
with their antecedents, discourse segmentation, recognizing discourse-new descriptions, and 
suggesting anchors for bridging descriptions. 
1. Introduction 
Most models of definite description processing proposed in the literature tend to 
emphasise the anaphoric role of these elements. 1 (Heim \[1982\] is perhaps the best for- 
malization of this type of theory). This approach is challenged by the results of exper- 
iments we reported previously (Poesio and Vieira 1998), in which subjects were asked 
to classify the uses of definite descriptions in Wall Street Journal articles according 
to schemes derived from proposals by Hawkins (1978) and Prince (1981). The results 
of these experiments indicated that definite descriptions are not primarily anaphoric; 
about half of the time they are used to introduce a new entity in the discourse. In 
this paper, we present an implemented system for processing definite descriptions 
based on the results of that earlier study. In our system, techniques for recognizing 
discourse-new descriptions play a role as important as techniques for identifying the 
antecedent of anaphoric ones. 
A central characteristic of the work described here is that we intended from the 
start to develop a system whose performance could be evaluated using the texts an- 
notated in the experiments mentioned above. Assessing the performance of an NLP 
system on a large number of examples is increasingly seen as a much more thorough 
evaluation of its performance than trying to come up with counterexamples; it is con- 
sidered essential for language ngineering applications. These advantages are thought 
by many to offset some of the obvious disadvantages of this way of developing NLP 
theories-- in particular, the fact that, given the current state of language processing 
technology, many hypotheses of interest cannot be tested yet (see below). As a result, 
quantitative valuation is now commonplace in areas of language engineering such 
as parsing, and quantitative valuation techniques are being proposed for semantic 
* Universidade do Vale do Rio dos Sinos - UNISINOS, Av. Unisinos 950 - Cx. Postal 275, 93022-000 Sao 
Leopoldo RS Brazil. E-mail: renata@exatas.unisinos.br 
t University of Edinburgh, ICCS and Informatics, 2 Buccleuch Place, EH8 9LW Edinburgh UK. E-maih 
Massimo.Poesio@ed.ac.uk 
1 We use the term definite description (Russell 1905) to indicate definite noun phrases with the definite 
article the, such as the car. We are not concerned with other types of definite noun phrases uch as 
pronouns, demonstratives, or possessive descriptions. Anaphoric expressions are those linguistic 
expressions u ed to signal, evoke, or refer to previously mentioned entities. 
(~) 2001 Association for Computational Linguistics 
Computational Linguistics Volume 26, Number 4 
interpretation as well, for example, at the Sixth and Seventh Message Understand- 
ing Conferences (MUC-6 and MUC-7) (Sundheim 1995; Chinchor 1997), which also 
included evaluations of systems on the so-called coreference task, a subtask of which 
is the resolution of definite descriptions. The system we present was developed to be 
evaluated in a quantitative fashion, as well, but because of the problems concerning 
agreement between annotators observed in our previous tudy, we evaluated the sys- 
tem both by measuring precision/recall against a "gold standard," as done in MUC, 
and by measuring agreement between the annotations produced by the system and 
those proposed by the annotators. 
The decision to develop a system that could be quantitatively evaluated on a large 
number of examples resulted in an important constraint: we could not make use of 
inference mechanisms such as those assumed by traditional computational theories 
of definite description resolution (e.g., Sidner 1979; Carter 1987; Alshawi 1990; Poesio 
1993). Too many facts and axioms would have to be encoded by hand for theories 
of this type to be tested even on a medium-sized corpus. Our system, therefore, is 
based on a shallow-processing approach more radical even than that attempted by 
the first advocate of this approach, Carter (1987), or by the systems that participated 
in the MUC evaluations (Appelt et al 1995; Gaizaukas et al 1995; Humphreys et al 
1988), since we made no attempt o fine-tune the system to maximize performance 
on a particular domain. The system relies only on structural information, on the in- 
formation provided by preexisting lexical sources uch as WordNet (Fellbaum 1998), 
on minimal amounts of general hand-coded information, or on information that could 
be acquired automatically from a corpus. As a result, the system does not really have 
the resources to correctly resolve those definite descriptions whose interpretation does 
require complex reasoning (we grouped these in what we call the "bridging" class). 
We nevertheless developed heuristic techniques for processing these types of definites 
as well, the idea being that these heuristics may provide a baseline against which the 
gains in performance due to the use of commonsense knowledge can be assessed more 
clearly. 2
The paper is organized as follows: We first summarize the results of our previous 
corpus study (Poesio and Vieira 1998) (Section 2) and then discuss the model of deft- 
nite description processing that we adopted as a result of that work and the general 
architecture of the system (Section 3). In Section 4 we discuss the heuristics that we 
developed for resolving anaphoric definite descriptions, recognizing discourse-new 
descriptions, and processing bridging descriptions, and, in Section 5, how the per- 
formance of these heuristics was evaluated using the annotated corpus. Finally, we 
present he final configuration of the two versions of the system that we developed 
(Section 6), review other systems that perform similar tasks (Section 7), and present 
our conclusions and indicate future work (Section 8). 
2. Preliminary Empirical Work 
As mentioned above, the architecture of our system is motivated by the results con- 
cerning definite description use in our corpus, discussed in Poesio and Vieira (1998). 
In this section we briefly review the results presented in that paper. 
2 In fact, it is precisely because we are interested in identifying the types of commonsense r asoning 
actually used in language processing that we focused on definite descriptions rather than on other 
types of anaphoric expressions ( uch as pronouns and ellipsis) that can be processed much more 
effectively on the basis of syntactic information alone (Lappin and Leass 1994; Hardt 1997). 
540 
Vieira and Poesio Processing Definite Descriptions 
2.1 The Corpus 
We used a subset of the Penn Treebank I corpus (Marcus, Santorini, and Marcinkiewicz 
1993) from the ACL/DCI  CD-ROM, containing newspaper articles from the Wall Street 
Journal. We divided the corpus into two parts: one, containing about 1,000 definite 
descriptions, was used as a source during the development of the system; we will refer 
to these texts as Corpus 1. 3 The other part, containing about 400 definite descriptions, 
was kept aside during development and used for testing; we will refer to this subset 
as Corpus 2. 4 
2.2 Classifications of Anaphoric Expressions 
The best-known studies of definite description use (Hawkins 1978; Prince 1992; Frau- 
rud 1990; L6bner 1987; Clark 1977; Sidner 1979; Strand 1996) classify definite descrip- 
tions on the basis of their relation with their antecedent. A fundamental distinction 
made in these studies is between descriptions that denote the same discourse ntity 
as their antecedent (which we will call anaphoric or, following Fraurud, subsequent 
mention), descriptions that denote an object hat is in some way "associated" with the 
antecedent-- for example, it is part of it, as in a car. . ,  the wheel (these definite expres- 
sions are called "associative descriptions" by Hawkins and "inferrables" by Prince), 
and descriptions that introduce a new entity into the discourse. 
In the case of semantic identity between definite description and antecedent, a 
further distinction can be made depending on the semantic relation between the pred- 
icate used in the description and that used for the antecedent. The predicate used in 
an anaphoric definite description may be a synonym of the predicate used for the an- 
tecedent (a house ... the home), a general izat ion/hypernym (an oak.., the tree), and even, 
sometimes, a special izat ion/hyponym (a tree.., the oak). In fact, the NP introducing the 
antecedent may not have a head noun at all, e.g., when a proper name is used, as in 
Bill Clinton... the president. We will use the term direct anaphora when both description 
and antecedent have the same head noun, as in a house.., the house. Direct anaphors are 
the easiest definite descriptions for a shallow system to resolve; in all other cases, as 
well as when the antecedent and the definite description are related in a more indirect 
way, lexical knowledge, or more generally encyclopedic knowledge, is needed. 
All of the classifications mentioned above also acknowledge the fact that not all 
definite descriptions depend on the previous discourse for their interpretation. Some 
refer to an entity in the physical environment, others to objects which are assumed to 
be known on the basis of common knowledge (Prince's "discourse-new/hearer-old" 
expressions, uch as the pope), and still others are licensed by virtue of the semantics 
of their head noun and complement (as in the fact that Milan won the Italian football 
championship). 
2.3 A Study of Definite Description Use 
In the experiments discussed in Poesio and Vieira (1998) we asked our subjects to 
classify all definite description uses in our two corpora. These experiments had the 
dual objective of verifying how easy it was for human subjects to agree on the distinc- 
tions between definite descriptions just discussed, and producing data that we could 
use to evaluate the performance of a system, The classification schemes we used were 
simpler than those proposed in the literature just mentioned and were motivated, on 
3 The texts in question are w0203, w0207, w0209, w0301, w0305, w0725, w0760, w0761, w0765, w0766, 
w0767, w0800, w0803, w0804, w0808, w0820, w1108, w1122, w1124, and w1137. 
4 The articles in this second subset are w0766, wsj_0003, wsj_0013, wsj_0015, wsj_0018, wsj_0020, wsj_0021, 
wsj_0022, wsj_0024, wsj_0026, wsj_0029, wsj_0034, wsj_0037, and wsj_0039. 
541 
Computational Linguistics Volume 26, Number 4 
the one hand, by the desire to make the annotation uncomplicated for the subjects 
employed in the empirical analysis and, on the other hand, by our intention to use the 
annotation to get an estimate of how well a system using only limited lexical and en- 
cyclopedic knowledge could do. s We ran two experiments, using two slightly different 
classification schemes. In the first experiment we used the following three classes: 6 
? direct anaphora: subsequent-mention definite descriptions that refer to 
an antecedent with the same head noun as the description; 
? br idging descriptions: definite descriptions that either (i) have an 
antecedent denoting the same discourse ntity, but using a different head 
noun (as in house  . . .  bu i ld ing) ,  or (ii) are related by a relation other than 
identity to an entity already introduced in the discourse; 7 
? discourse-new: first-mention definite descriptions that denote objects not 
related by shared associative knowledge to entities already introduced in 
the discourse. 
In the second experiment we treated all anaphoric definite descriptions as part of 
one class (direct anaphora + bridging (i)), and all inferrables as part of a different class 
(bridging (ii)), without significant changes in the agreement results. 
Agreement among annotators was measured using the K statistic (Siegel and 
Castellan 1988; Carletta 1996). K measures agreement among k annotators over and 
above chance agreement (Siegel and Castellan 1988). The K coefficient of agreement is
defined as: 
K - -  P (a )  - P (E )  
1 - 
where P(A) is the proportion of times the annotators agree, and P(E) the proport ion of 
times that we would expect hem to agree by chance. The interpretation of K figures is 
an open question, but in the field of content analysis, where reliability has long been 
an issue (Krippendorff 1980), K > 0.8 is generally taken to indicate good reliability, 
whereas 0.68 < K < 0.8 allows tentative conclusions to be drawn. Carletta et al (1997) 
observe, however, that in other areas, such as medical research, much lower levels of 
K are considered acceptable (Landis and Koch 1977). 
An interesting overall result of our study was that the most reliable distinction that 
our annotators could make was that between first-mention and subsequent-mention 
(K = 0.76); the measure of agreement for the three-way distinction just discussed was 
K = 0.73. The second interesting result concerned the distribution of definite descrip- 
tions in the three classes above: we found that about half of the definite descriptions 
were discourse-new. The distribution of the definite descriptions in classes in our first 
experiment according to annotators A and B are shown in Tables 1 and 2, respec- 
tively. (Class IV includes cases of idiomatic expressions or doubts expressed by the 
annotators). 
The third main result was that we found very little agreement between our sub- 
jects on identifying briding descriptions: in our second experiment, he agreement on 
5 Previous attempts o annotate anaphoric relations had resulted in very low agreement levels; for 
example, in the coreference annotation experiments for MUC-6 (Sundheim 1995), relations other than 
identity were dropped ue to difficulties in annotating them. 
6 In this experiment, our subjects could also classify adefinite description as "idiomatic" or 
"doubt'--see tables below. 
7 In Poesio and Vieira (1998), Hawkins's term "associative" was used for this class; but in fact, the 
definition we used for the class is closest to the sense of "bridging" used by Clark (1977). 
542 

Computational Linguistics Volume 26, Number 4 
ined, Fraurud (1990, 421) claims that: 
a model where the processing of first-mention definites always in- 
volves a failing search for an already established iscourse referent as 
a first step seems less attractive. A reverse ordering of the procedures 
is, quite obviously, no solution to this problem, but a simultaneous 
processing as proposed by Bosch and Geurts (1989) might be. 
Fraurud proposes, contra Heim (1982), that processing a definite NP may in- 
volve establishing a new discourse entity. 8 This new discourse entity may then be 
linked to one or more anchors in the text or to a background referent. 9 Fraurud dis- 
cusses the example of the description the king, interpreted relationally, encountered in
a text in which no king has been previously mentioned. Lexicoencyclopedic knowl- 
edge would provide the information that a king is related to a period and a country; 
these would constitute the anchors. The selection of the anchors would identify the 
pertinent period and country, and this would make possible the identification of a 
referent: say, for the anchors 1989 and Sweden, the referent identified would be Carl 
Gustav XVI. 1? 
The most interesting aspect of Fraurud's proposal is the hypothesis that first- 
mention definites are not necessarily recognized simply because no suitable antecedent 
has been found; independent strategies for recognizing them may be involved. This 
hypothesis is consistent with L6bner's proposal (L6bner 1987) that the fundamental 
property of a definite description is that it denotes a function (in a logical sense); this 
function can be part of the meaning assigned to the definite description by the gram- 
mar (as in the beginning of X), or can be specified by context (as in the case of anaphoric 
definites). Fraurud's and L6bner's ideas can be translated into a requirement that a 
system have separate methods or rules for recognizing discourse-new descriptions 
(and in particular, L6bner's "semantically functional" definites) in addition to rules 
for resolving anaphoric definite descriptions; these rules may run in parallel with the 
rules for resolving anaphoric definites, rather than after them. 
Rather than deciding a priori on the question of whether the heuristic rules (in 
our case) for identifying discourse-new descriptions hould be run in parallel with 
resolution or after it, we treated this as an empirical question. We made the archi- 
tecture of the system fairly modular, so that we could both try different heuristics 
and try applying them in a different order, using the corpus for evaluation. We dis- 
cuss all the heuristics that we tried in Section 4, and our evaluation of them in Sec- 
tion 5. 
3.2 Architecture of Our System 
The overall architecture of our system is shown in Figure 1. The system attempts to 
classify each definite description as either direct anaphora, discourse-new, or bridging 
description. In addition to this classification, the system tries to identify the antecedents 
of anaphoric descriptions and the anchors (Fraurud 1990) of bridging descriptions. The 
8 Discourse entities are representations in the discourse model of entities explicitly mentioned (Webber 
1979; Heim 1982). 
9 Background referents are entities that have not been mentioned in the discourse--those entities that 
Grosz (1977) would call "elements ofthe implicit focus." 
10 Fraurud oes not explain what it is that justifies the use of definite descriptions, if not familiarity. In
Poesio and Vieira (1998) we suggest that L/3bner's proposal (LSbner 1987) seems to account for the 
most data. 
544 
Vieira and Poesio Processing Definite Descriptions 
Treebank 1 
~xt ract ion~ 
I 
\[NP, a, house\] 
\[NP, the, house\] 
\[S,...\[...\]\] 
-1 List of NPs and Sentences 
potential_antecedent( l,np(...),...). 
potential_antecedent(2,np(...),...). 
definite_description(3,np(...),...). 
definite_description(5,np(...),...). 
/ 
Text p rocess ing~ 
\ 
L i~ngu ist ic ~ 
CZ?  
Discourse r presentation 
dd_class(3,anaphora). 
dd_class(5,bridging). 
dd_class(6,discourse_new). 
corer(3,1 ). 
bridging(5,2). 
coref_chain(\[ 13\]). 
total(anaphora,22). 
total(bridging,9). 
total(discourse_new,28). 
System's results 
Figure 1 
System architecture. 
system processes parsed newswire texts from the Penn Treebank I, constructing a fairly 
simple discourse model that consists of a list of discourse ntities that may serve as 
potential antecedents (which we call simply potential antecedents), according to the 
chosen segmentation algorithm (see below). The system uses the discourse model, 
syntactic information, and a small amount of lexical information to classify definite 
descriptions as discourse-new or to link them to anchors in the text; WordNet is also 
consulted by the version of the system that attempts to resolve bridging descriptions. 
The system is implemented in Sicstus Prolog. 
545 
Computational Linguistics Volume 26, Number 4 
Input. The texts in the Penn Treebank corpus consist of parsed sentences represented 
as Lisp lists. During a preprocessing phase, a representation i Prolog list format 
is produced for each sentence, and the noun phrases it contains are extracted. The 
output of this preprocessing phase is passed to the system proper. For example, the 
sentence in (1) is represented in the Treebank as (2) and the input to the system after the 
preprocessing phase is (3). 11 Note that all nested NPs are extracted, and that embedded 
NPs such as the Organization of Petroleum Exporting Countries are processed before the 
NPs that embed them (in this case, the squabbling within the Organization of Petroleum 
Exporting Countries). 
(1) 
(2) 
(3) 
Mideast politics have calmed down and the squabbling within the 
Organization of Petroleum Exporting Countries eems under control for now. 
( (s (s 
(NP Mideast politics) 
have 
(VP calmed 
down)) 
and 
(S (NP the squabbling 
(PP within 
(NP the Organization 
(PP of 
(NP Petroleum Exporting Countries))))) 
(VP seems 
(PP under 
(NP control))) 
(PP for 
(NP now)))) 
.) 
\[NP,Mideast,politics\]. 
\[NP,Petroleum,Exporting,Countries\]. 
\[NP,the,Organization, 
\[PP,of,\[NP,Petroleum,Exporting,Countries\]\]\]. 
\[NP,the,squabbling,\[PP,within,\[NP,the,0rganization, 
\[PP,of,\[NP,Petroleum,Exporting,Countries\]\]\]\]\]. 
\[NP,control\]. 
\[\[S,\[S,\[NP,Mideast,politics\],have,\[VP,calmed, 
\[PP,down\]\]\],and,\[S,\[NP,the,squabbling,\[PP,within, 
\[NP,the,Drganization,\[PP,of,\[NP,Petroleum,Exporting, 
Countries\]\]\]\]\],\[VP,seems,\[PP,under,\[NP,control\]\], 
\[PP,for,now\]\]\]\],.\]. 
Output. The system outputs the classification it has assigned to each definite descrip- 
tion in the text, together with the coreferential nd bridging links it has identified. 
11 Prolog variables will be indicated in the rest of the paper by the use of .... at the beginning and end of 
the variables; e.g., _X_ for variable X. 
546 

Computational Linguistics Volume 26, Number 4 
is coordination: for example, our algorithm does not recognize that a noun such as 
reporters below is a head noun: 
\[NP, reporters,and, editors,\[PP, of \[NP, The,WSJ\]\]\]. 
4.1.2 Potential Antecedents. The second problem is to determine which NPs should 
be used to resolve definite descriptions, among all those in the text. The system keeps 
track of NP index, NP structure, head noun, and NP type (definite, indefinite, bare 
plural, possessive n) of each potential antecedent, as illustrated by (5) below. 
(5) potential_antecedent(l,np(NP),head(H),type(T)). 
Examples of potential antecedents extracted from (6) are shown in (7): 
(6) 
(7) 
In an interview with reporters of the Wall Street Journal, the candidate 
appears quite confident of victory and of his ability to handle the 
mayoralty. 
a. 
potential_antecedent(l,np(_NPstructure), 
head(reporters), 
type(indef)). 
potential antecedent(2,np( NPstructure ), 
head(interview), 
type(indef)). 
b? 
potential_antecedent(3,np(_NPstructure_), 
head(3ournal), 
type(def)). 
potential antecedent(4,np( NPstructure ), 
head(candidate), 
type(def)). 
potential antecedent(5,np(NPstructure_), 
head(mayoralty), 
type(def)). 
C. 
potential_antecedent(6,np(_NPstructure_), 
head(ability), 
type(possessive)). 
d. 
potential_antecedent(7,np(_NPstructure_), 
head(victory), 
type(other)). 
We found that different recall/precision trade-offs can be achieved epending on the 
choice of potential antecedents--i.e., depending on whether all NPs are considered 
as possible antecedents, or only indefinite NPs, or various other subsets--so we ran 
experiments to identify the best group of potential antecedents. Four different NP 
12 Other NPs not included in any of these categories are identified as type(other). 
548 , 
Vieira and Poesio Processing Definite Descriptions 
subsets were considered: 
. 
. 
. 
. 
indefinite NPs, defined as NPs containing the indefinite articles a, an, 
some and bare/cardinal plurals, as in (7a); 13 
indefinite NPs and definite descriptions (NPs beginning with the definite 
article) ((7a) and (7b)); 
indefinite NPs, definite descriptions, and possessive NPs (NPs with a 
possessive pronoun or possessive mark) ((7a), (7b) and (7c)); 
all NPs ((7a), (7b), (7c)) and (7d)). 
The results obtained by considering each subset of the total number of NPs as potential 
antecedents are discussed in Section 5.2. 
4.1.3 Segmentation. The set of potential antecedents of anaphoric expressions i also 
restricted by the fact that antecedents end to have a limited "life span"--i.e., they 
only serve as antecedents for anaphoric expressions within pragmatically determined 
segments of the whole text (see, for example, Reichman \[1985\], Grosz and Sidner 
\[1986\] and Fox \[1987\]). In our corpus we found that about 10% of direct anaphoric 
definite descriptions have more than one possible antecedent if segmentation is not 
taken into account (Vieira and Poesio 1996). In (8), for example, the antecedent of 
the housej mentioned in sentence 50 is not the house mentioned earlier in sentences 2 
and 19, but another (nonmobile) house implicitly introduced in sentence 49 by the 
reference to the yard. 
(8) 2. A deep trench now runs along its north wall, exposed when the housei 
lurched two feet off its foundation during last week's earthquake? 
? ? ? 
19. Others grab books, records, photo albums, sofas and chairs, working 
frantically in the fear that an aftershock will jolt the housei again. 
20. The owners, William and Margie Hammack, are luckier than many 
others? 
49. When Aetna adjuster Bill Schaeffer visited a retired couple in 
Oakland last Thursday, he found them living in a mobile homek parked in 
front of their yard. 
50. The housej itself, located about 50 yards from the collapsed section of 
double-decker highway Interstate 880, was pushed about four feet off its 
foundation and then collapsed into its basement. 
? . ?  
65. As Ms. Johnson stands outside the Hammack housei after winding up 
her chores there, the house/begins to creak and sway. 
13 Only plural nouns ending in s are handled by the system. 
549 
Computational Linguistics Volume 26, Number 4 
In general, it is not sufficient o look at the most recent antecedents only: this 
is because segments are organized hierarchically, and the antecedents introduced in 
a segment at a lower level are typically not accessible from a segment at a higher 
level (Fox 1987; Grosz 1977; Grosz and Sidner 1986; Reichman 1985), whereas the 
antecedents introduced in a prior segment at the same level may be. Later in (8), 
for example, the housej in sentence 50 becomes inaccessible again, and in sentence 65, 
the text starts referring again to the house introduced in sentence 2. Automatically 
recognizing the hierarchical structure of texts is an unresolved problem, as it involves 
reasoning about intentions; 14 better esults have been achieved on the simpler task of 
"chunking" the text into sequences of segments, generally by means of lexical density 
measures (Hearst 1997; Richmond, Smith, and Amitay 1997). 
The methods for limiting the life span of discourse ntities that we considered 
for our system are even simpler. One type of heuristic we looked at are window- 
based techniques, i.e., considering only the antecedents within fixed-size windows of 
previous entences, although we allow some discourse ntities to have a longer life 
span: we call this method loose segmentation. More specifically, a discourse ntity 
is considered a potential antecedent for a definite description when the antecedent's 
head is identical to the description's head, and 
? the potential antecedent is within the established window, or else 
? the potential antecedent is itself a subsequent mention, or else 
? the definite description and the antecedent are identical NPs (including 
the article). 
We also considered an even simpler ecency heuristic: this involves keeping atable 
indexed by the heads of potential antecedents, such that the entry for noun N contains 
the index of the last occurrence of an antecedent with head N. Finally, we considered 
combinations of segmentation a d recency. (The results of these two heuristics are 
compared in Section 5.2. 
4.1.4 Noun Modifiers. Once the head nouns of the antecedent and of the description 
have been identified, the system attempts to match them. This head-matching strategy 
works correctly in simple cases like (9): 
(9) Grace Energy hauled a rig here ... The rig was built around 1980. 
In general, however, when matching a definite description with a potential an- 
tecedent the information provided by the prenominal nd the postnominal part of the 
noun phrases also has to be taken into account. For example, a blue car cannot serve 
as the antecedent for the red car, or the house on the left for the house on the right. In our 
corpus, cases of antecedents that would incorrectly match by simply matching heads 
without regarding premodification include: 
(10) a. the business community .. .  the younger, more activist black political 
community; 
b. the population.., the voting population. 
14 See, however, Marcu (1999). 
550 
Vieira and Poesio Processing Definite Descriptions 
Again, taking proper account of the semantic ontribution of these premodif iers would, 
in general, require commonsense reasoning. For the moment,  we only developed 
heuristic solutions to the problem, including: 
? al lowing an antecedent to match with a definite description if the 
premodif iers of the description are a subset of the premodif iers of the 
antecedent. This heuristic deals with definites that contain less 
information than the antecedent, such as an old Victorian house .. .  the 
house, and prevents matches uch as the business community .. .  the 
younger, more activist black political community. 
? al lowing a nonpremodif ied antecedent to match with any same head 
definite. This second heuristic deals with definites that contain additional 
information, such as a check.. ,  the lost check. 
The information that two discourse entities are disjoint may come from postmod- 
ification, as well, although same head antecedents with different postmodification are 
not as common as those with differences in premodification. An example from our 
corpus is shown in (11). 
(11) a chance to accomplish several objectives ... the chance 
to demonstrate an entrepreneur 
like himself could run Pinkerton's better than an unfocused conglomerate or
investment banker. 
The heuristic method we developed to deal with postmodification is to compare the 
description and antecedent, preventing resolution in those cases where both are post- 
modif ied and the modifications are not the same. (These results are also discussed in 
Section 5.2.) 
4.2 Discourse-New Descriptions 
As mentioned above, a fundamental  characteristic of our system is that it also includes 
heuristics for recognizing discourse-new descriptions (i.e., definite descriptions that 
introduce new discourse entities) on the basis of syntactic and lexical features of the 
noun phrase. Our heuristics are based on the discussion in Hawkins (1978), who 
identified a number  of correlations between certain types of syntactic structure and 
discourse-new descriptions, particularly those he called "unfamil iar" definites (i.e., 
those whose existence cannot be expected to be known on the basis of generally 
shared knowledge), including: is 
the presence of "special predicates": 
the occurrence of premodif iers uch as first or best when 
accompanied with full relatives, e.g., the first person to sail to 
America (Hawkins calls these "unexplanatory modifiers"; LObner 
15 Hawkins himself proposes a transformation-based account of unfamiliar definites, but the correlations 
he identified proved to be a useful source of heuristics for identifying these uses of definite 
descriptions even though the existence of counterexamples to these heuristics uggests that a 
syntactic-based account cannot be entirely correct. Most of these examples can be accounted for in 
terms of L6bner's theory of definiteness. 
551 
Computational Linguistics Volume 26, Number 4 
\[1987\] showed how these predicates may license the use of 
definite descriptions in an account of definite descriptions based 
on functionality); 
a head noun taking a complement such as the fact that there is life 
on Earth (Hawkins calls this subclass "NP complements"); 
the presence of restrictive modification, as in the inequities of the current 
land-ownership system. 
Our system attempts to recognize these syntactic patterns. We also added heuristics 
classifying as unfamiliar some definites occurring in 
? appositive constructions (e.g., Glenn Cox, the president of Phillips 
Petroleum Co.); 
? copular constructions (e.g., the man most likely to gain custody of all this is a 
career politician named David Dinkins). 
(The reason definite descriptions in appositive and copular constructions tend to be 
discourse-new, in fact unfamiliar, is that the information eeded for the identification 
is given by the NP to which the apposition is attached and the predicative part of the 
copular construction, respectively. 16) 
Finally, we found that three classes of what Hawkins called "larger situation" 
definites (those whose existence can be assumed to be known on the basis of encyclo- 
pedic knowledge, such as the pope) can also be recognized on the basis of heuristics 
exploiting syntactic and lexical features: 
? definites that behave like proper nouns, like the United States; 
? definites that have proper nouns in their premodification, such as the 
Iran-Iraq war; 
? definites referring to time, such as the time or the morning. 
In our corpus study we found that our subjects did much better at identifying 
discourse-new descriptions all together (K = 0.68) than they did at distinguishing 
unfamiliar from larger situation cases (K = 0.63). This finding was confirmed by our 
implementation: although each of the heuristics is designed, in principle, to identify 
only one of the uses (larger situation or unfamiliar), they work better at identifying 
together the whole class of discourse-new descriptions. 
4.2.1 Special Predicates. Some cases of discourse-new definite descriptions can be 
identified by comparing the head noun or modifiers of the definite NP with a list of 
predicates that are either functional or likely to take a complement (L6bner 1987). Our 
list of predicates that, when taking NP complements, are generally used to introduce 
discourse-new entities, was compiled by hand and currently includes the nouns fact, 
result, conclusion, idea, belief, saying, and remark. In these cases, what licenses the use of 
a definite is not anaphoricity, but the fact that the head noun can be interpreted as 
16 In the systems participating inMUC, definite descriptions occurring in appositions are treated as 
anaphoric on the preceding NP; our system considers the NP and the apposition as a unit that 
introduces a new referent to the discourse. 
552 
Vieira and Poesio Processing Definite Descriptions 
semantically functional; the noun complement specifies the argument of the function. 
Functionality is enough to license the use of the definite description (LObner 1987). An 
example of definite description classified as discourse-new on these grounds is given 
in (12). 
(12) Mr. Dinkins also has failed to allay Jewish voters' fears about his 
association with the Rev. Jesse Jackson, despite the fact that few local 
non-Jewish politicians have been as vocal for Jewish causes in the past 20 years as 
Mr. Dinkins has. 
When encountering a definite whose head noun occurs in this list, the system 
checks if a complement is present or if the definite appears in a copular construction 
(e.g., the fact is that...). 
A second list of special predicates consulted by the system includes what Hawkins 
called unexplanatory modifiers: these include adjectives uch as -first, last, best, most, 
maximum, minimum, and only and superlatives in general. 17. All of these adjectives are 
predicate modifiers that turn a head noun into a function, therefore again--according 
to LObner--licensing the use of a definite even when no antecedent is present (see 
examples below). When applying this heuristic, the system verifies the presence of a 
complement for some of the modifiers (first, last), but not for superlatives. 
(13) a. Mr. Ramirez just got the first raise he can remember in eight years, to $8.50 
an hour from $8. 
b. She jumps at the slightest noise. 
Finally, our system uses a list of special predicates that we found to correlate well 
with larger situation uses (i.e., definite descriptions referring to objects whose existence 
is generally known). This list consists mainly of terms indicating time reference, and 
includes the nouns hour, time, morning, afternoon, night, day, week, month, period, quarter, 
year, and their respective plurals. An example from the corpus is: 
(14) Only 14,505 wells were drilled for oil and natural gas in the U.S. in the 
first nine months of the year. 
Other definites typically used with a larger situation interpretation are the moon, 
the sky, the pope, and the weather. 
It should be noted that although these constructions may indicate a discourse- 
new interpretation, these expressions may also be used anaphorically; this is one of 
the cases in which a decision has to be made concerning the relative priority of differ- 
ent heuristics. We discuss this issue further in connection with the evaluation of the 
system's performance in Section 5.18 
4.2.2 Restrictive and Nonrestrictive Modification. A second set of heuristics for iden- 
tifying discourse-new descriptions that we derived from Hawkins's suggestions and 
17 The list should be made more comprehensive; so far it includes the cases observed in the corpus 
analysis and a few other similar modifiers. 
18 More recently, Bean and Riloff (1999) have proposed methods for automatically extracting from a 
corpus heads that correlate well with discourse novelty. 
553 
Computational Linguistics Volume 26, Number 4 
Table 3 
Distribution of prepositional phrases and relative clauses. 
Restrictive Postmodification # % 
Prepositional phrases 152 77% 
Relative clauses 45 23% 
Total 197 100 % 
from our corpus analysis look for restrictive modification. 19We developed patterns 
to recognize restrictive postmodification and nonrestrictive postmodification; we also 
tested the correlation between discourse novelty and premodification. We discuss each 
of these heuristics in turn. 
Restrictive Postmodification. Hawkins (1978) pointed out that unfamiliar definites often 
include referent-establishing relative clauses and associative clauses, while warning 
that not all relative clauses are referent-establishing. Some statistics about this corre- 
lation were reported by Fraurud (1990): she found that in her corpus 75% of complex 
definite NPs (i.e., modified by genitives, postposed PPs, restrictive adjectival modifiers) 
were first-mention. A great number of definite descriptions with restrictive postmod- 
ifiers are unfamiliar in our corpus as well (Poesio and Vieira 1998); in fact, restrictive 
postmodification was found to be the single most frequent feature of first-mention de- 
scriptions. Constructions of this type are good indicators of discourse novelty because 
a restrictive postmodifier may license the use of a definite description either by pro- 
viding a link to the rest of the discourse (as in Prince's "containing inferrables') or by 
making the description into a functional concept. Looking for restrictive postmodifiers 
might therefore be a good way of identifying discourse-new descriptions. 
The distribution of restrictive postmodifiers in our corpus is shown in Table 3; 
examples of each type of postmodifier are given below. 
(15) 
(16) 
Relative clauses: these are finite clauses sometimes (but not always) 
introduced by relative pronouns uch as who, whom, which, where, when, 
why, and that: 
a. The place where he lives .. .  
b. The guy we met .. .  
Nonfinite postmodifiers: these include ing, ed (participle), and infinitival 
clauses. 
a. The man writing the letter is my friend. 
b. The man to consult is Wilson. 
Prepositional phrases and of-clauses: Quirk et al (1985) found that 
prepositional phrases are the most common type of postmodification i  
English---three or four times more frequent han either finite or nonfinite 
clausal postmodification. This was confirmed by our corpus study (see 
19 The term restrictive modification is used when the modifier provides information that is essential to 
identify the discourse ntity referred to by the NP (Quirk et al 1985). The modification is nonrestrictive 
when the head provides ufficient information to identify the discourse ntity, so that the information 
provided by the modification is not essential for identification. 
554 
Vieira and Poesio Processing Definite Descriptions 
Table 4 
Distribution of prepositions (1). 
Prepositional Phrases # % 
Of-phrases 120 79% 
Other prepositions 32 21% 
Total 152 100% 
Table 3). The types of prepositions observed for 188 postmodified 
descriptions are shown in Table 4; of-clauses are the most common. 
Our program uses the following patterns to identify restrictive postmodifiers: 2? 
(17) a. 
\[HP,the,_Premodifiers_,_Head_, \[SBAKQI_\] i_\] ; 
b. 
\[NP, the, _Premodif iers_, _Head_, \[SBAR i _\] \[ _\] ; 
C. 
\[HP ,the, _Premodifiers_, _Head_, \[S i_\] I_\] ; 
d. 
\[NP, the, _Premodif iers_, _Head_, \[VP J _\] l _\] ; 
e. 
\[NP, the, _Premodifiers_, _Head_, \[PP,_ I_\] i_\] ; 
f. 
\[NP, the, _Premodifiers_, _Head_, \[WHPP, _ I _\] I _\] ? 
In the Treebank, sometimes the modified NP is embedded in another NP, so structures 
like (18) are also considered (again for all types of clauses just shown above): 
(18) \[NP,\[NP,the,_Premodifiers_,_Head_\],\[Clause\]\]. 
Nonrestrictive postmodiJi'cation. We found it important to distinguish restrictive from 
nonrestrictive postmodification, since in our corpus, definite descriptions with nonre- 
strictive postmodifiers were generally not discourse-new. Our system recognizes non- 
restrictive postmodifiers by the simple yet effective heuristic of looking for commas. 
This heuristic orrectly recognizes nonrestrictive postmodification in cases like: 
(19) The substance, discovered almost by accident, is very important. 
which are annotated in the Penn Treebank I as follows: 
(20) \[NP,the,proposal,',',\[SBAR,\[WHHP,which\],also,\[S,\[HP,T\],would, 
\[VP,create,\[NP,a,new,type,\[PP,of,\[HP,individual, 
retirement,account\]Ill\]I, ',' \]... 
20 Note that an NP may have zero, one, or more premodifiers. 
555 
Computational Linguistics Volume 26, Number 4 
Restrictive Premodification. Restrictive modification is not as common in prenominal 
position as in posthead position, but it is often used, and was also found to correlate 
well with larger situation and unfamiliar uses of definite descriptions (Poesio and 
Vieira 1998). A restrictive premodifier may be a noun (as in (21)), a proper noun, or 
an adjective, m Sometimes numerical figures (usually referring to dates) are used as 
restrictive premodifiers, as in (22)). 
(21) 
(22) 
A native of the area, he is back now after riding the oil-field boom to the 
top, then surviving the bust running an Oklahoma City convenience 
store. 
the 1987 stock market crash; 
The heuristic we tested was to classify definite descriptions premodified by a proper 
noun as larger situation. 
4.2.3 Appositions. During our corpus analysis we found additional syntactic patterns 
that appeared to correlate well with discourse novelty yet had not been discussed 
by Hawkins, such as definite descriptions occurring in appositive constructions: they 
usually refer to the NP modified by the apposition, therefore there is no need for the 
system to look for an antecedent. Appositive constructions are treated in the Treebank 
as NP modifiers; therefore the system recognizes an apposition by checking whether 
the definite occurs in a complex noun phrase with a structure consisting of a sequence 
of noun phrases (which might be separated by commas, or not) one of which is a 
name or is premodified by a name, as in the examples in (23). 
(23) a. Glenn Cox, the president of Phillips Petroleum 
b. \[NP, \[NP,Glenn,Cox\] , ', ', \[NP,the,president, 
\[PP, of, \[NP, Phil l ips, Petroleum\] \] \] \] ; 
c. the oboist, Heinz Holliger 
d. \[NP, \[NP, the, oboist\] , \[NP, Heinz, Holl iger\] \] . 
In fact a definite description may itself be modified by an apposition, e.g., an indefinite 
NP, as shown by (24). Such cases of appositive constructions are also recognized by 
the system. 
(24) the Sandhills Luncheon Care, a tin building in midtown. 
Other examples of apposition recognized by the system are: 
(25) a. the very countercultural chamber group Tashi; 
b. the new chancellor, John Major; 
c. the Sharpshooter, a freshly drilled oil well two miles deep; 
21 Our system cannot distinguish adjectives orverbs from nouns in premodification because it works 
directly off the parsed version of the Treebank, without looking at part-of-speech tags. 
556 
Vieira and Poesio Processing Definite Descriptions 
4.2.4 Copular Phrases. Copular phrases such as the Prime Minister is Tony Blair also 
often involve discourse-new descriptions. We developed the following heuristic for 
handling copula constructions. If a description occurs in subject position, the system 
looks at the VP. If the head of the VP is the verb to be, to seem, or to become, and the 
complement of the verb is not an adjectival phrase, the system classifies the description 
as discourse-new. Two examples correctly handled by this heuristic are shown in (26); 
the syntactic representation f these cases in the Penn Treebank I is shown in (27). 
(26) a. The bottom line is that he is a very genuine and decent guy. 
b. When the dust and dirt settle in an extra-nasty mayoral race, the man 
most likely to gain custody of all this is a career politician named David 
Dinkins. 
(27) \[S,\[NP,The,bottom,line\],\[VP,is,\[NP,\[SBAR,that... l \ ] l \ ] .  
If the complement of the verb is an adjective, the subject is typically interpreted 
referentially and should not be considered iscourse-new on the basis of its comple- 
ment (e.g., The president of the US is tall). Adjectival complements are represented as 
follows in the Treebank: 
(28) \[S,\[NP,The,missing,watch\],\[VP,is,\[AD3P,emblematic...\]\]\]. 
Definite descriptions in object position of the verb to be, such as the one shown in 
(29), are also considered iscourse-new. 
(29) What the investors object o most is the effect hey say the proposal would 
have on their ability to spot telltale "clusters" of trading activity. 
4.2.5 Proper Names. Proper names preceded by the definite article, such as (30), are 
common in the genre we are dealing with, newspaper articles. 
(30) the Securities and Exchange Commission. 
The first appearance of these definite descriptions in the text is usually a discourse- 
new description; subsequent mentions of proper names are regarded as cases of 
anaphora. To recognize proper names, the system simply checks whether the head 
is capitalized. If the test succeeds, the definite is classified as a larger situation use .  22 
4.3 Bridging Descriptions 
Bridging descriptions are the definite descriptions that a shallow processing system 
is least equipped to handle. Linguistic and computational theories of bridging de- 
scriptions identify two main subtasks involved in their resolution: finding the element 
in the text to which the bridging description is related (anchor) and identifying the 
relation (link) holding between the bridging description and its anchor (Clark 1977; 
Sidner 1979; Heim 1982; Carter 1987; Fraurud 1990; Strand 1996). The speaker is h- 
censed to use a bridging description when he or she can assume that the commonsense 
22 Note that this test is performed just after trying to find an antecedent, so that the second instance of 
the same proper (head) noun will be classified as an anaphoric use. 
557 
Computational Linguistics Volume 26, Number 4 
knowledge required to identify the relation is shared by the listener (Hawkins 1978; 
Clark and Marshall 1981; Prince 1981). This dependence on commonsense knowledge 
means that, in general, a system can only resolve bridging descriptions when supplied 
with an adequate knowledge base; for this reason, the typical way of implementing 
a system for resolving bridging descriptions has been to restrict he domain and feed 
the system with hand-coded world knowledge (see, for example, Sidner \[1979\] and 
especially Carter \[1987\]). A broader view of bridging phenomena (not only bridging 
descriptions) is presented in Hahn, Strube, and Markert (1996). They make use of a 
knowledge base from which they extract conceptual links to feed an adaptation of the 
centering model (Grosz, Joshi, and Weinstein 1995). 
The relation between bridging descriptions and their anchors may be arbitrarily 
complex (Clark 1977; Sidner 1979; Prince 1981; Strand 1996), and the same description 
may relate to different anchors in a text: this makes it difficult to decide what the 
intended anchor and the intended link are (Poesio and Vieira 1998). For all these 
reasons, this class has been the most challenging problem we have dealt with in the 
development of our system, and the results we have obtained so far can only be 
considered very preliminary. Nevertheless, we feel that trying to process these definite 
descriptions i the only way to discover which types of commonsense knowledge are 
actually needed. 
4.4 Types of Bridging Descriptions 
Our work on bridging descriptions began with the development of a classification of
bridging descriptions (Vieira and Teufel 1997) according to the kind of information 
needed to resolve them, rather than on the basis of the possible relations between 
descriptions and their anchors as is typical in the literature. This allowed us to get 
an estimate of what types of bridging descriptions we might expect our system to 
resolve. The classification is as follows: 
? cases based on well-defined lexical relations, such as synonymy, 
hypernymy, and meronymy, that can be found in a lexical database such 
as WordNet (Fellbaum 1998), as in theyqat . . .  the living room; 
? bridging descriptions in which the antecedent is a proper name and the 
description a common oun, whose resolution requires ome way of 
recognizing the type of object denoted by the proper name, as in Bach . . .  
the composer; 
? cases in which the anchor is not the head noun but a noun modifying an 
antecedent, asin the company has been selling discount packages . . .  the 
discounts 
? cases in which the antecedent (anchor) is not introduced by an NP but 
by a VP, as in Kadane oil is currently drilling two oil wells. The activity . . .  
? descriptions whose antecedent is not explicitly mentioned in the text, but 
is implicitly available because it is a discourse topic, e.g., the industry in a 
text referring to oil companies; 
? cases in which the relation with the anchor is based on more general 
commonsense knowledge, e.g., about cause-consequence relations. 
In the rest of this section, we describe the heuristics we developed for handling 
the first three of these classes: lexical bridges, bridges based on names, and bridges 
558 
Vieira and Poesio Processing Definite Descriptions 
to entities introduced by nonhead nouns in a compound nominal (Poesio, Vieira, and 
Teufel 1997). 
4.4.1 Bridging Descriptions and WordNet. In order to get a system that could be eval- 
uated on a corpus containing texts in different domains, we used WordNet (Fellbaum 
1998) as an approximation of a lexical knowledge source. We developed a WordNet 
interface (Vieira and Teufel 1997) that reports a possible semantic link between two 
nouns when one of the following is true: 
? the nouns are in the same synset (i.e., they are synonyms of each other), 
as in suit~lawsuit; 
? the nouns are in a hyponymy/hypernymy relation with each other, as in 
dollar~currency; 
? there is a direct or indirect meronymy/holonymy (part of/has parts) 
relation between them, as in door~house; 
? the nouns are coordinate sisters, i.e. hyponyms of the same hypernym, 
such as home~house, which are hyponyms of housing, lodging. 
Sometimes, finding a relation between two predicates involves complex searches 
through WordNet's hierarchy. For example, there may be no relation between two 
head nouns, but there is a relation between compound nouns in which these nouns 
appear: thus, there is no semantic relation between record~album, but only a synonymy 
relation between record_album~album. We found that extended searches of this type, or 
searches for indirect meronymy relations, yielded extremely ow recall and precision at 
a very high computational cost; both types of search were dropped at the beginning of 
the tests we ran to process the corpus consulting WordNet (Poesio, Vieira, and Teufel 
1997). The results of our tests with WordNet are presented in Section 5.4. 
4.4.2 Bridging Descriptions and Named Entity Recognition. Definite descriptions 
that refer back to entities introduced by proper names (such as Pinkerton Inc ... the 
company) are very common in newspaper articles. Processing such descriptions requires 
determining an entity type for each name in the text, that is, if we recognize Pinkerton 
Inc. as an entity of type company, we can then resolve the subsequent description 
the company, or even a description such as the firm by finding a synonymy relation 
between company and firm using WordNet. 
This so-called named entity recognition task has received considerable attention 
recently (Mani and MacMillan 1996; McDonald 1996; Paik et al 1996; Bikel et al 
1997; Palmer and Day 1997; Wacholder and Ravin 1997; Mikheev, Moens, and Grover 
1999) and was one of the tasks evaluated in the Sixth and Seventh Message Under- 
standing Conferences. In MUC-6, 15 different systems participated in the competition 
(Sundheim 1995). For the version of the system discussed and evaluated here, we im- 
plemented a preliminary algorithm for named entity recognition that we developed 
ourselves; a more recent version of the system (Ishikawa 1998) uses the named en- 
tity recognition software developed by HCRC for the MUC-7 competition (Mikheev, 
Moens, and Grover 1999). 
WordNet contains the types of a few names--typically, of famous people, coun- 
tries, states, cities, and languages. Other entity types can be identified using appositive 
constructions and abbreviations ( uch as Mr., Co., and Inc.) as cues. Our algorithm for 
assigning a type to proper names is based on a mixture of the heuristics just described. 
559 
Computational Linguistics Volume 26, Number 4 
The system first looks for the above-mentioned cues to try to identify the name type. If 
no cue is found, pairs consisting of the proper name and each of the elements from the 
list country, city, state, continent, language, person are consulted in our WordNet interface 
to verify the existence of a semantic relation. 
The recall of this algorithm was increased by including a backtracking mechanism 
that reprocesses a text, filling in the discourse representation with missing name types. 
With this mechanism we can identify later the type for the name Morishita in a textual 
sequence in which the first occurrence of the name does not provide surface indication 
of the entity type: e.g., Morishita . . .  Mr. Morishita. The second mention includes such 
a clue (Mr.); by processing the text twice, we recover such missing types. 
After finding the types for names, the system uses the techniques previously de- 
scribed for same-head matching or WordNet lookup to match the descriptions with 
the types found for previous named entities. 
4.4.3 Compound Nouns. Sometimes, the anchor for a bridging description is a non- 
head noun in a compound noun: 
(31) stock market crash..,  the markets; 
One way to process these definite descriptions would be to update the discourse 
model with discourse referents not only for the NP as a whole, but also for the em- 
bedded nouns. For example, after processing stock market crash, we could introduce a
discourse referent for stock market, and another discourse referent for stock market crash. 23 
The description the markets would be coreferring with the first of these referents (with 
an identical head noun), and then we could simply use our anaphora resolution algo- 
rithms. This solution, however, makes available discourse referents that are generally 
inaccessible for anaphora (Postal 1969). For example, it is generally accepted that in 
(32), a deer is not accessible for anaphoric reference. 24
(32) I saw la deeri hunter\]j. ItT was dead. 
Therefore, we followed a different route. Our algorithm for identifying anchors at- 
tempts to match not only heads with heads, but also: 
. 
(33) 
2. 
(34) 
3. 
(35) 
The head of a description with the premodifiers of a previous NP: 
the stock market crash..,  the markets; 
The premodifiers of a description with the premodifiers of its 
antecedents: 
his art business .. .  the art gallery. 
And finally, the premodifiers of the description with the head of a 
previous NP: 
a 15-acre plot and main home .. .  the home site. 
23 Note that the collection of potential antecedents containing all NPs will just have the NP head crash for 
stock market crash. The system considers the whole NP structure as only one discourse referent, 
according to the structure of the Penn Treebank: \[NP, the,1987,stock,market,crash\]. 
24 These proposed constraints have been challenged by Ward, Sproat, and McKoon (1991). 
560 
Vieira and Poesio Processing Definite Descriptions 
5. Evaluation of the Heuristics 
In this section we discuss the tests we ran to arrive at a final configuration of the 
system. The performance of the heuristics discussed in Section 4 was evaluated by 
comparing the results of the system with the human annotation of the corpus pro- 
duced during the experiments discussed in Poesio and Vieira (1998). Several variants 
of our heuristics were tried using Corpus 1 as training data; after deciding upon an 
optimal version, our algorithms were evaluated using Corpus 2 as test data. Because 
our proposals concerning bridging descriptions are much less developed than those 
concerning anaphoric descriptions and discourse-new descriptions, we ran separate 
evaluations of two versions of the system: Version 1, which does not attempt o re- 
solve bridging descriptions, and Version 2, which does; we will point out below which 
version of the system is considered in each evaluation. 
5.1 Evaluation Methods 
The fact that the annotators working on our corpus did not always agree either on 
the classification of a definite description or on its anchor aises the question of how 
to evaluate the performance of our system. We tried two different approaches: eval- 
uating the performance of the system by measuring its precision and recall against a 
standardized annotation based on majority voting (as done in MUC), and measuring 
the extent of the system's agreement with the rest of the annotators by means of the 
same metric used to measure agreement among the annotators themselves (the kappa 
statistic). We used the first form of evaluation to measure both the performance of
the single heuristics and the performance of the system as a whole; the agreement 
measure was only used to measure the overall performance of the system. We discuss 
each of these in turn. as 
5.1.1 Precision and Recall. Recall and precision are measures commonly used in In- 
formation Retrieval to evaluate a system's performance. Recall is the percentage of 
correct answers reported by the system in relation to the number of cases indicated 
by the annotated corpus: 
R = number of correct responses 
number of cases 
whereas precision is the percentage of correctly reported results in relation to the total 
reported: 
p = number of correct responses 
number of responses 
These two measures may be combined to form one measure of performance, the F 
measure, which is computed as follows: 
F - (W + 1)RP 
(WR) + P 
W represents he relative weight of recall to precision and typically has the value 1. 
A single measure gives us a balance between the two results; 100% of recall may be 
due to a precision of 0% and vice versa. The F measure penalizes both very low recall 
and very low precision. 
25 For a rather thorough discussion ofthe problem of evaluating anaphora esolution algorithms, see 
Mitkov (2000). 
561 
Computational Linguistics Volume 26, Number 4 
5.1.2 Semiautomatic Evaluation against a Standardized Annotation. The precision 
and recall figures for the different variants of the system were obtained by com- 
paring the classification produced by each version with a standardized annotation, 
extracted from the annotations produced by our human annotators by majority judge- 
ment: whenever at least two of the three coders agreed on a class, that class was chosen. 
Details of how the standard annotation was obtained are given in Vieira (1998). 26 
The system's performance as a classifier was automatically evaluated against he 
standard annotation of the corpus as follows. Each NP in a text is given an index: 
(36) A house1?6... The house135... 
When a text is annotated or processed, the coder or system associates each index of a 
definite description with a type of use; both the standard annotation and the system's 
output are represented as Prolog assertions. 
(37) a .  
system: 
b. 
coder: 
dd_class(135,anaphoric). 
dd_class(135,anaphoric). 
To assess the system's performance on the identification of a coreferential an- 
tecedent, it is necessary to compare the links that indicate the antecedent of each de- 
scription classified as anaphora. These links are also represented as Prolog assertions, 
as follows: 
(38) a .  
coder : corer (135,106). 
b. 
system: corer (135,106). 
The system uses these assertions to build an equivalence class of discourse ntities, 
called a coreference chain. When comparing an antecedent indicated by the system 
for a given definite description with that in the annotated corpus, the corresponding 
coreference chain is checked--that is, the system's indexes and the annotated indexes 
do not need to be exactly the same as long as they belong to the same coreference 
chain. In this way, both (40a) and (40b) would be evaluated as correct answers if the 
corpus is annotated with the links shown in (39). 
(39) A house1?6... The house135... The house154... 
coder: corer(135,106). 
coder: corer(154,135). 
(40) a .  
system: corer (154,135). 
b. 
system: corer(154,106). 
26 An alternative method is to give fractional values to a classification depending on the number of 
agreements (Hatzivassiloglou and McKeown (1993). 
562 
Vieira and Poesio Processing Definite Descriptions 
In the end, we still need to check the results manually, because our annotated coref- 
erence chains are not complete: our annotators did not annotate all types of anaphoric 
expressions, o it may happen that the system indicates as antecedent an element out- 
side an annotated coreference chain, such as a bare noun or possessive. In (41), for 
example, suppose that all references to the house are coreferential: 
(41) A house1?6... The house135... His house14?... The house154... 
corer ( 154,140). 
If NP 135 is indicated as the antecedent for NP 154 in the corpus annotation (so that 
140 is not part of the annotated coreference hain), and the system indicates 140 as the 
antecedent for 154, an error is reported by the automatic evaluation, even though all 
of these NPs refer to the same entity. A second consequence of the fact that the coref- 
erence chains in our standard annotation are not complete is that in the evaluation of 
direct anaphora resolution, we only verify if the antecedents indicated are correct; we 
do not evaluate how complete the coreferential chains produced by the system are. By 
contrast, in the evaluation of the MUC coreference task, where all types of referring 
expressions are considered, the resulting co-reference hains are evaluated, rather than 
just the indicated antecedent (Vilain et al 1995). Even our limited notion of corefer- 
ence chain was, nevertheless, very helpful in the automatic evaluation, considerably 
reducing the number of cases to be checked manually. 
5.1.3 Measuring the Agreement of the System with the Annotators. Because the 
agreement between our annotators in Poesio and Vieira (1998) was often only partial, 
in addition to precision and recall measures, we evaluated the system's performance 
by measuring its agreement with the annotators using the K statistic we used in Poesio 
and Vieira (1998) to measure agreement among annotators. Because the proper inter- 
pretation of K figures is still open to debate, we interpret the K figures resulting from 
our tests comparatively, rather than absolutely, (by comparing better and worse levels 
of agreement). 
5.2 Anaphora Resolution 
We now come to the results of the evaluation of alternative versions of the heuristics 
dealing with the resolution of direct anaphora (segmentation, selection of potential 
antecedents, and premodification) discussed in Section 4.1. The optimal version of our 
system is based on the best results we could get for resolving direct anaphora, because 
we wanted to establish the coreferential relations among discourse NPs as precisely 
as possible. 
5.2.1 Life Span of Discourse Entities. In Section 4.1 we discussed two heuristics for 
limiting the life span of discourse ntities. The first segmentation heuristic discussed 
there, loose segmentation, is window based, but the restriction on sentence distance 
is relaxed (i.e., the resolver will consider an antecedent outside the window) when 
either: 
? the antecedent is itself a subsequent-mention; or 
? the antecedent is identical to the definite description being resolved 
(including the article). 
With loose segmentation, it is possible for the system to identify more than one 
coreference link for a definite description: all antecedents satisfying the requirements 
563 
Computational Linguistics Volume 26, Number 4 
Table 5 
Evaluation of loose segmentation a d recency heuristics. 
Heuristics R P F 
Segmentation: 1-sentence window 71.79% 86.48% 78.45% 
Segmentation: 4-sentence window 76.92% 82.75% 79.73% 
Segmentation: 8-sentence window 78.20% 80.26% 79.22% 
Recency: all sentences 80.76% 78.50% 79.62% 
Table 6 
Evaluation of the strict segmentation heuristic. 
Strict Segmentation R P F 
1-sentence window 29.48% 89.32% 44.33% 
4-sentence window 57.69% 88.23% 69.76% 
8-sentence window 67.94% 84.46% 75.31% 
within the current w indow will be indicated as a possible antecedent. Therefore, when 
evaluating the system's results, we may find that all antecedents indicated for the 
resolution of a description were right, or some were right and some wrong, or that all 
were wrong. The recall and precision figures reported here relate to those cases were 
all resolutions indicated were right according to the annotated corpus. 
In Section 4.1 we also discussed a second segmentation heuristic, which we called 
recency: the system does not collect all candidate NPs as potential antecedents, but 
only keeps the last occurrence of an NP from all those having the same head noun, 
and there are no restrictions regarding the antecedent's distance. 
The results of these two methods for different w indow sizes are shown in Ta- 
ble 5. The results in this table were obtained by considering as potential antecedents 
indefinites (i.e., NPs with determiners a,an, and some; bare NPs; and cardinal plurals), 
possessives, and definite descriptions, as in Vieira and Poesio (1996); we also used the 
premodif ication heuristics proposed there. Alternatives to these heuristics were also 
evaluated; the results are discussed later in this section. 
The resulting F measures were almost the same for all heuristics, but there was 
clearly an increase in recall with a loss of precision when enlarging the window size} 7 
The recency heuristic had the best recall, but the lowest precision, although not much 
lower than the others. The best precision was achieved with a one-sentence window, 
and recall was not dramatical ly affected, but this only happened because the window 
size constraint was relaxed. 
To show what happens when a strict version of the window-based segmentation 
approach is used, consider Table 6. (Strict segmentation means that the system only 
considers those antecedents that are inside the sentence window for resolving a de- 
scription, with no exceptions.) As the table shows, this form of segmentation results 
in higher precision, but has a strong negative ffect on recall. The overall F values are 
all worse than for the heuristics in Table 5. 
Finally, we tried a combination of the recency and segmentation heuristics: just one 
potential antecedent for each different head noun is available for resolution, the last 
27 In our experiments small differences in recall, precision, and F measures are frequent. We generally 
assume in this paper that such differences are not significant, but a more formal significance t st along 
the lines of that in Chinchor (1995) will eventually be necessary to verify this. 
564 
Vieira and Poesio Processing Definite Descriptions 
Table 7 
Combining loose segmentation a d recency heuristics. 
Combined Heuristics R P F 
4 sentences + recency 75.96% 87.77% 81.44% 
8 sentences + recency 77.88% 84.96% 81.27% 
Table 8 
Evaluation of the heuristics for choosing potential antecedents. 
Antecedents Selection R P F 
Indefinites, definite descriptions, and possessives 75.96% 87.77% 81.44% 
All NPS 77.88% 86.17% 81.81% 
Indefinites and definite descriptions 73.39% 88.41% 80.21% 
Indefinites only 12.17% 77.55% 21.05% 
occurrence of that head noun. The resolution still respects the segmentation heuristic 
(loose version). The results are presented inTable 7. This table shows that by combining 
the recency and loose segmentation approaches to segmentation we obtain a better 
trade-off between recall and precision than using each heuristic separately. The version 
with higher F value in Table 7 (four-sentence window plus recency) was chosen as 
standard and used in the tests discussed in the rest of this section. 
5.2.2 Potent ia l  Antecedents .  Next, we evaluated the various ways of restricting the set 
of potential antecedents discussed in Section 4.1, using four-sentence-window l ose 
segmentation with recency. In an earlier version of the system (Vieira and Poesio 1996), 
only those definite descriptions that were not resolved with a same-head antecedent 
were considered as potential antecedents; resolved definite descriptions would be 
linked to previous NPs, but would not be made available for subsequent resolution. 
(The idea was that the same antecedent used in one resolution could be used to resolve 
all subsequent mentions cospecifying with that definite description.) An important dif- 
ference between that implementation a d the current one is that in the new version, 
the definltes resolved by the system are also made available as potential antecedents of 
subsequent definites. This is because in our previous prototype, errors in identifying 
an indefinite antecedent were sometimes propagated through a coreference chain, so 
that the right antecedent would be missed. The results are shown in Table 8. 
If we only consider indefinites as potential antecedents, recall is extremely low 
(12%); we also get the worst precision. In other words, considering only indefinites 
for the resolution of definite descriptions i too restrictive; this is because our corpus 
contains alarge number of first-mention definite descriptions that serve as antecedents 
for subsequent references ( imilar results were also reported in Fraurud \[1990\]). The 
version with the highest precision (88%) is the one that only considers indefinites and 
definite descriptions as antecedents, but recall is lower compared to the version that 
considered other NPs. We chose, as the basis for further testing, aversion that combines 
near-optimal values for F and precision, i.e., the version that takes indefinites, definite 
descriptions, and possessives (first row in Table 8). 
5.2.3 Premodifiers. Finally, we tested our heuristics for dealing with premodifiers. We 
tested the matching algorithm from Vieira and Poesio (1996) in the present version 
of the system; the results are presented in Table 9. In that table, we also show the 
565 
Computational Linguistics Volume 26, Number 4 
Table 9 
Evaluation of the heuristics for premodification (Version 1). 
Antecedents Selection R P F 
1. Ant-set/Desc-subset 69.87% 91.21% 79.12% 
2. Ant-empty 55.12% 88.20% 67.85% 
3. Ant-subset/Desc-set 64.74% 88.59% 74.81% 
1 and 2 (basic v.) 75.96% 87.77% 81.44% 
1 and 3 75.96% 87.13% 81.16% 
None 78.52% 81.93% 80.19% 
results obtained with a modified matching algorithm including a third rule, which 
allows a premodified antecedent to match with a definite whose set of premodifiers 
is a superset of the set of modifiers of the antecedent (an elaboration of rule 2). We 
tested each of these three heuristics alone and in combination. (The fourth line simply 
repeats the results hown in Table 7.) 
The main result of this evaluation is that using a modified segmentation heuristic 
(including recency) reduces the overall impact of the heuristics for premodification 
the performance of the algorithm in comparison with the system discussed in Vieira 
and Poesio (1996)? The best precision is still achieved by the matching algorithm that 
does not allow for new information in the anaphoric expression, but the best results 
overall are again obtained by combining rule I and rule 2, although either 2 or 3 works 
equally well when combined with 1. (Note that the combination of heuristics 2 and 3 
is equivalent to heuristic 3alone, since rule 3 subsumes rule 2.) Heuristic 2 and 3 alone 
are counterintuitive and indeed give the poorest results; however, the impact is greater 
on recall than precision, which suggests that the introduction of new information in 
noun modification is not very frequent? 
One of the problems with our premodifier heuristics i that although a difference in 
premodification usually indicates noncoreference, as for the company's abrasive segment 
and the engineering materials egment, there are a few cases in our corpus in which 
coreferent descriptions have totally different premodification from their antecedents, 
as in: 
(42) the pixie-like clarinetist ... the soft-spoken clarinetist? 
These cases would be hard even for a system using real commonsense reasoning, since 
often the information i  the premodifier isnew; we consider these examples one of the 
best arguments for including in the system a focus-tracking mechanism along the lines 
of Sidner (1979). Our heuristic matching algorithm also suggests wrong antecedents 
in cases like the rules in (43), when the last mention refers to a modified concept (the 
new rules are different from the previous ones). 
(43) Currently, the rules force executives ... 
The rule changes would ... 
The rules will eliminate ... 
Finally, the matching algorithm gets the wrong result in cases uch as the population 
? . .  the voting population where the new information i dicates a subset, superset, or part 
of a previously mentioned referent. 
566 
Vieira and Poesio Processing Definite Descriptions 
Table 10 
Evaluation of the heuristics for direct anaphora (Version 1). 
Anaphora Classification # + - R P F 
Training data 312 243 27 78% 90% 83% 
Test data 154 103 12 67% 90% 77% 
Anaphora Resolution # + - R P F 
Training data 312 237 33 76% 88% 81% 
Test data 154 96 19 62% 83% 71% 
5.2.4 Overa l l  Resu l ts  for Anaphor ic  Def in i te  Descr ipt ions .  To summarize, on the 
basis of the tests just discussed, the heuristics that achieve the best results for anaphoric 
definite descriptions are: 
. 
2. 
3. 
. 
combined loose segmentation a d recency, 
four-sentence window, 
considering indefinites, definites, and possessives as potential 
antecedents, 
the premodification f the description must be contained in the 
premodification f the antecedent or when the antecedent has no 
premodifiers. 
In Table 10 we present the overall results on anaphora classification and anaphora 
resolution for the version of the system that does not attempt o resolve bridging 
descriptions, for both training data and test data. The reason there are different figures 
for anaphora resolution and classification is that the system may correctly classify a 
description as anaphoric, but then find the wrong antecedent. We used this set of 
heuristics when evaluating the heuristics for discourse-new and bridging descriptions 
in the rest of the paper. 
The column headed # represents he number of cases of descriptions classified as 
anaphora in the standard annotation; +indicates the total number of anaphora (clas- 
sification and resolution) correctly identified; - indicates the total number of errors. 
5.2.5 Errors in Anaphora  Reso lu t ion .  Before discussing the results of the other heuris- 
tics used by the system, we will discuss in more detail some of the errors in the 
resolution of anaphoric descriptions made by using the heuristics just discussed. 
Some errors are simply caused by misspellings in the Treebank, as in the example 
below, where the antecedent is misspelled as spokewoman. 
(44) A Lorillard spokewoman ... The Lorillard spokeswoman 
The most common problems are due to the heuristics limiting the search for an- 
tecedents. In (45), both sentence 7 and sentence 30 are outside the window considered 
by the system when trying to resolve the adjusters in 53. 
(45) 7. She has been on the move almost incessantly since last Thursday, 
when an army of adjusters, employed by major insurers, invaded the San 
Francisco area. 
567 
Computational Linguistics Volume 26, Number 4 
? ? ?  
30. Aetna, which has nearly 3,000 adjusters, had deployed about 750 of 
them 
53. Many of the adjusters employed by Aetna and other insurers 
Limiting the type of potential antecedents o indefinites, definite descriptions, 
and possessives, while improving precision, also leads to problems, because the an- 
tecedents introduced by other NPs, such as proper names, are missed--e.g., Toni John- 
son in (46). The following definite description is then classified by the system as larger 
situation/unfamiliar. Some of these problems are corrected in Version 2 of the system, 
which also attempts to handle bridging descriptions and therefore uses algorithms for 
assigning a type to such entities? 
(46) Toni Johnson pulls a tape measure across the front of what was once a 
stately Victorian home. 
The petite, 29-year-old Ms. Johnson ... 
The premodification heuristics prevent the system from finding the right an- 
tecedent in the (rare) cases of coreferent descriptions with different premodifiers, as 
in (47). 
(47) The Victorian house that Ms. Johnson is inspecting has been deemed 
unsafe by town officials? 
Once inside, she spends nearly four hours measuring and diagramming 
each room in the 80-year-old house? 
In the following example, it is the lack of a proper treatment of postmodification 
that causes the problem. The system classifies the description the earthquake-related 
claims as anaphoric to claims from that storm, but it is discourse-new according to the 
standardized annotation. 
(48) Most companies till are trying to sort through the wreckage caused by 
Hurricane Hugo in the Carolinas last month? 
Aetna, which has nearly 3,000 adjusters, had deployed about 750 of them 
in Charlotte, Columbia, and Charleston? 
Adjusters who had been working on the East Coast say the insurer will 
still be processing claims from that storm through December. 
It could take six to nine months to handle the earthquake-related claims? 
In (49), the system correctly classifies the definite description the law as anaphoric, 
but suggests as antecedent an income tax law, whereas a majority of our annotators 
568 
Vieira and Poesio Processing Definite Descriptions 
Table 11 
Evaluation of the heuristics for identifying discourse-new descriptions. 
Discourse -new # + - R P F 
Training data 492 368 60 75% 86% 80% 
Test data 218 151 58 69% 72% 70% 
indicated a money lending law as the antecedent. 28 
(49) Nearly 20 years ago, Mr. Morishita, founder and chairman of Aichi 
Corp., a finance company, received a 10-month suspended sentence from 
a Tokyo court for violating a money-lending law and an income tax law. 
He was convicted of charging interest rates much higher than what the 
law permitted, and attempting to evade income taxes by using a double 
accounting system. 
Finally, the system is incapable of resolving plural references to collections of ob- 
jects introduced by singular NPs, even when these collections were introduced by 
coordinated noun phrases. Although it would be relatively easy to add rules for han- 
dling the simplest cases (possibly at the expense of a decrease in precision), many of 
these references can only be resolved by means of nontrivial operations. 
(50) The owners, William and Margie Hammack, are luckier than many others. 
The Hammacks ... 
5.3 Ident i f i ca t ion  o f  D iscourse -New Descr ip t ions  
The overall recall and precision results for the heuristics for identifying discourse- 
new descriptions presented in Section 4.2 are shown in Table 11. In this table we do 
not distinguish between the two types of discourse-new descriptions, unfamiliar and 
larger-situation (Hawkins 1978). As already mentioned in Section 4.2, distinguishing 
between the two types of discourse-new descriptions identified by Hawkins, Prince, 
and others isn't easy even for humans (Fraurud 1990; Poesio and Vieira 1998); and 
indeed, our heuristics for recognizing discourse-new descriptions work better when 
evaluated together. The column headed # represents the number of cases of descrip- 
tions classified as discourse-new in the standard annotation; +indicates the total num- 
ber of discourse-new descriptions correctly identified; - the number of errors. These 
results are for the version of the system that uses the best version of the heuristics 
for dealing with anaphoric descriptions discussed above, and that doesn't attempt o 
resolve bridging descriptions (Version 1). 
The performance of the specific heuristics discussed in Section 4.2 is shown in 
Tables 12 to 15. Table 12 shows the results of the heuristics for larger situation uses 
on the training data, whereas Table 13 reports the performance on the same data of 
28 The law could also be interpreted as referring to "the law system in general," in which case none of the 
antecedents would be correct (or either could be taken as anchor for a bridging interpretation f the 
definite). 
569 
Computational Linguistics Volume 26, Number 4 
Table 12 
Evaluation of heuristics for larger situation uses (training data). 
Larger Situation Total Found Errors Precision 
Names 73 10 86% 
Time references 50 7 86% 
Premodification 41 19 54% 
Total 164 36 78% 
Table 13 
Evaluation of heuristics for unfamiliar uses (training data). 
Unfamiliar Total Found Errors Precision 
NP compl/Unexp mod 32 2 93% 
Apposition 27 2 92% 
Copula 8 2 75% 
Postmodification 197 18 91% 
Total 264 24 91% 
Table 14 
Evaluation of heuristics for larger situation uses (test data). 
Larger Situation Total Found Errors Precision 
Names 44 14 68% 
Time references 21 5 64% 
Premodification 17 9 47% 
Total 82 28 66% 
the heuristics for unfamiliar uses. We report only precision figures because our stan- 
dard annotation only gives us information about the classification of these discourse 
descriptions as discourse-new, not about the reason they were classified in a certain 
way (larger situation or unfamiliar). The most common feature of discourse-new de- 
scriptions is postmodification; the least satisfactory results are those for proper names 
in premodification. As expected, the heuristics for recognizing unfamiliar uses (many 
of which are licensed by linguistic knowledge) achieve better precision than those for 
larger situation uses, which depend more on commonsense knowledge. 
Tables 14 and 15 summarize the results of the heuristics for discourse-new de- 
scriptions on the test data (Corpus 2). Again, the best results were obtained by the 
heuristics for recognizing unfamiliar uses. The biggest difference in performance was 
shown by the heuristic hecking the presence of the definite in a copula construction, 
which performed very well on the training data, but poorly on the test data. The actual 
performance of that heuristic is difficult to evaluate, however, as a very low recall was 
reported for both training and test data. 
In the following sections, we analyze some of the problems encountered by the 
version of the system using these heuristics. 
Apposition. Coordinated NPs with more than two conjuncts are a problem for this 
heuristic, since in the Penn Treebank I, coordinated NPs have a structure that matches 
the pattern used by the system for recognizing appositions. For example, the coordi- 
nated NP in the sentence G-7 consists of the U.S., Japan, Britain, West Germany, Canada, 
570 
Vieira and Poesio Processing Definite Descriptions 
Table 15 
Evaluation of heuristics for unfamiliar uses (test data). 
Unfamiliar Total Found Errors Precision 
NP compl/Unexp mod 16 2 87% 
Apposition 10 2 80% 
Copula 6 4 33% 
Postmodification 95 22 77% 
Total 127 30 76% 
France and Italy has the structure in (51). 
(51) \[NP, \[NP,the,U.S.\] ,,, \[NP, Japan\] ,,, \[NP,Britain\] ,,, \[NP,West, 
Germany\] ,,, \[NP, Canada\],,,  \[NP, France\], and, \[NP, Italy\] \] 
Copula. This heuristic was difficult to evaluate because there few examples, and the 
precision in the two data sets is very different (see Tables 13 and 15 above). One 
problem is that the descriptions in copula constructions might also be interpreted 
as bridging descriptions. For instance, the description the result in (52a) below is the 
result of something mentioned previously, while the copula construction specifies its 
referent. Other ambiguous examples are (52b) and (52c): 
(52) a. The result is that those rich enough to own any real estate at all have 
boosted their holdings ubstantially. 
b. The chief culprits, he says, are big companies and business groups that 
buy huge amounts of land not for their corporate use, but for resale at 
huge profit. 
c. The key man seems to be the campaign manager, Mr. Lynch. 
Restrictive premod~cation. One problem with this heuristic is that although proper 
nouns in premodifier positions are often used with discourse-new definites (e.g., 
the Iran-Iraq war), they may also be used as additional information in associative or 
anaphoric uses: 
(53) Others grab books, records, photo albums, sofas and chairs, working 
frantically in the fear that an aftershock will jolt the house again. 
As Ms. Johnson stands outside the Hammack house after winding up her 
chores there, the house begins to creak and sway. 
Restrictive postmodification. If the system fails to find an antecedent or anchor and the 
description is postmodified, it may wrongly be classified as discourse-new. In (54) the 
filing on the details of the spinoff was classified as bridging on documents filed ... by the 
coders, but the system classified it as discourse-new. 
(54) Documents filed with the Securities and Exchange Commission on the pending 
spinoff disclosed that Cray Research Inc. will withdraw the almost $100 
million in financing it is providing the new firm if Mr. Cray leaves or if 
the product-design project he heads is scrapped. 
571 
Computational Linguistics Volume 26, Number 4 
The filing on the details of the spinoff caused Cray Research stock to jump 
$2.875 yesterday to close at $38 in New York Stock Exchange composite 
trading. 
Proper nouns. As we have already seen--(46), repeated below as (55)--a definite de- 
scription that looks like a proper noun (the petite, 29-year-old Ms. Johnson) may in fact 
be anaphoric. This is not always a problem, as the system does attempt to find an- 
tecedents for these definites, as well, but if the antecedent is not found (as in the 
example below) the description is incorrectly classified as discourse-new. 
(55) Toni Johnson pulls a tape measure across the front of what was once a 
stately Victorian home. 
The petite, 29-year-old Ms. Johnson ... 
Special predicates? In this example the system classified as discourse-new a time refer- 
ence (the same time), which is classified as bridging in the standard annotation. 
(56) Newsweek's circulation for the first six months of 1989 was 3,288,453, flat 
from the same period last year. 
U.S. News' circulation in the same time was 2,303,328, down 2.6%. 
5.4 Bridging Descriptions 
As mentioned in Section 2, our corpus annotation experiments showed bridging de- 
scriptions to be the most difficult class for humans to agree on. Even when our anno- 
tators agreed that a particular expression was a bridging description, different anchors 
would be available in the text for the interpretation f that bridging description? This 
makes the results of the system for this class very difficult to evaluate; furthermore, 
the results must be evaluated by hand? 
We first tested the heuristics individually on the training data (the same data 
used in a previous analysis of the performance ofour system on bridging descriptions 
\[Vieira nd Teufel 1997\]) by adding them to Version I of the system one at a time. These 
separate tests were manually evaluated? We then integrated all of these heuristics into 
a version of the system called Version 2, using both automatic and manual evaluation. 
In this section we discuss only the results of the individual heuristics; the overall 
results of Version 2 are discussed in Section 6. 
Bridging descriptions are much more sensitive than other types of definite de- 
scriptions to the local focus (Sidner 1979); for this reason, Version 2 uses a different 
search strategy for bridging descriptions than for other definite descriptions. Rather 
than considering all definite descriptions in the current window simultaneously, it goes 
back one sentence at a time and stops as soon as a relation with a potential anchor is 
found. 
5.4.1 Using WordNet to Identify Anchors. Our system consults WordNet o determine 
if a definite description may be semantically related to one of the NPs in the previous 
572 
Vieira and Poesio Processing Definite Descriptions 
Table 16 
Evaluation of the search for anchors using WordNet. 
Bridging Class Relations Found Right Anchors % Right 
Synonymy 11 4 36% 
Hyponymy 59 18 30% 
Meronymy 6 2 33% 
Sister 30 6 20% 
Total 106 30 28% 
five sentences. 29 The results of this search over our training corpus, in which 204 
descriptions were classified as bridging, are shown in Table 16. It is interesting to 
note that the semantic relations found in this automatic search were not always those 
observed in our manual analysis. 
The main reason the figures are so low is that the existence of a semantic relation 
in WordNet is not a sufficient condition (nor a strong indication) to establish a link 
between an antecedent and a bridging description. In only about a third of the cases 
was a potential antecedent for which we could find a semantic relation in WordNet an 
appropriate anchor. An example is (57): although there is a semantic relation between 
argument and information in WordNet, the description the argument is related to the 
VP contend rather than to the NP information. Some form of focusing seems to play a 
crucial role in restricting the range of antecedents ( ee also the discussion in Hitzeman 
and Poesio \[1998\]). 
(57) A SEC proposal to ease reporting requirements for some company 
executives would undermine the usefulness of information on insider 
trades as a stock-picking tool, individual investors and professional 
money managers contend. 
They make the argument in letters to the agency about rule changes 
proposed this past summer that, among other things, would exempt 
many middle-management executives from reporting trades in their own 
companies' shares. 
Sense ambiguity is responsible for some of the false positives. For instance, the 
noun company has at least two distinct senses: "visitor" (as in Ihave company) and "busi- 
ness." A relation of hypernymy was found between company and human (its "visitor" 
sense), whereas in the text the noun company was used in the "business" sense. A 
more important problem, however, is the incompleteness of the information encoded 
in WordNet. To have an idea of how complete the information in WordNet is con- 
cerning the relations that are encoded, we selected from our two corpora 70 bridging 
descriptions that we had manually identified as being linked to their anchors by one of 
the semantic relations encoded in WordNet--synonymy, hypernymy (hyponymy), and 
meronymy (holonymy). In Table 17 we show the percentages of such relations actually 
encoded in WordNet. (The fourth column in the table indicates the cases in which the 
expected relation is not encoded, but the two nouns are sisters in the hierarchy.) 
As we can see from the table, the recall figure was quite disappointing, especially 
for synonymy relations. In some cases, the problem was simply that some of the 
29 We found that for bridging descriptions, a five-sentence window worked better than a four-sentence 
one. 
573 
Computational Linguistics Volume 26, Number 4 
Table 17 
Evaluation of the encoding of semantic relations in WordNet. 
Bridging Class Anchor/DD Pairs Found in WN Found Sister % 
Syn 20 5 2 35% 
Hyp 32 17 1 56% 
Mer 18 5 2 38% 
Total 70 27 5 46% 
artifact 
I is_a 
structure 
housing 
lodging 
i s -~"x .~. -a  
house home 
building 
edifice 
part_of 
room 
part_~"x...part_of 
wall floor 
Figure 2 
An example of problematic organization i WordNet. 
words we looked for were not in WordNet: examples include newsweekly (news-weekly), 
crocidolite, countersuit (counter-suit). Other times, the word we looked for was contained 
in WordNet, but not in the same typographic format as it was presented in the text; 
for example we had spinoff in a text, whereas WordNet had only an entry for spin- 
off. A second source of problems was the use in the WSJ articles of domain-specific 
terminology with context-dependent senses, such as slump, crash, and bust, which in 
articles about the economy are all synonyms. Finally, in other cases the relations were 
missing due to the structure of WordNet: for instance, in WordNet the nouns room, 
wall, and floor are encoded as part of building but not of house (see Figure 2). 
In summary, our tests have shown that the knowledge encoded in WordNet is 
not sufficient o interpret al semantic relations between a bridging description and 
its antecedent found in the kind of text we are dealing with: only 46% of the rela- 
tions observed were encoded in WordNet. The possibility of using domain-specific, 
automatically acquired lexical information for this purpose is being explored: see, for 
example, Poesio, Schulte im Walde, and Brew (1998). In addition, we found that just 
looking for the closest semantic relative is not enough to find anchors for bridging 
descriptions; this search has to be constrained by some type of focusing mechanism. 
5.4.2 Evaluating the Results for Bridging Descriptions Based on Proper Names. 
Identifying named entity types is a prerequisite for resolving descriptions based on 
names. The simple heuristics discussed in Section 5.4 identified entity types for 66% 
574 
Vieira and Poesio Processing Definite Descriptions 
(535/814) of all names in the corpus (organizations, persons, and locations); precision 
was 95%.  30 The errors we found were sometimes due to name or sense ambiguity. In 
the same text a name may refer both to a person and a company, as in Cray Comput- 
ers Corp. and Seymour Cray. When looking in WordNet for a type for the name Steve 
Reich we found for the name Reich the type country. These problems have also been 
noted by the authors of systems participating in MUC-6 (Appelt 1995). We also found 
undesirable relations uch as hypernymy for person and company. 
5.4.3 Evaluating the Results for Bridging Descriptions Based on Compound Nouns. 
We had 25 definite descriptions manual ly  identified as based on compound nouns. 
For these 25 cases, our implemented heuristics achieved a recall of 36% (9/25) but, 
in some cases, found valid relations other than the ones we identified. The low recall 
was sometimes due to segmentation. Sometimes the spelling of the premodif ication 
was slightly different from the one of the description, as in a 15-acre plot.. ,  the 15 acres. 
6. Overall Evaluation of the System 
As mentioned above, we implemented two versions of the system. Version 1 only 
resolves direct anaphora nd identifies discourse-new descriptions; Version 2 also deals 
with bridging descriptions. Both versions of the system have at their core a decision 
tree in which the heuristics discussed in the previous sections are tried in a fixed order 
to classify a certain definite description and find its anchor. Determining the optimal 
order of application of the heuristics in the decision tree is crucial to the performance 
of the system. In both versions of the system we used a decision tree developed by 
hand on the basis of extensive valuation; we also attempted to determine the order 
of application automatically, by means of decision tree learning algorithms (Quinlan 
1993). 
In this section we first present he hand-crafted ecision tree and the results ob- 
tained using this decision tree for Version 1 and Version 2; we then present he results 
concerning the agreement between system and annotators, and we briefly discuss the 
results obtained using the decision tree acquired automatically. 
6.1 Integration of the Heuristics 
The hand-crafted order of the heuristics in both versions is the following. For each NP 
of the input, 
. 
2. 
The system assigns an index to it. 
The NPs that may serve as potential antecedents are made available for 
description resolution by means of the optimal selection criterion 
discussed in Section 4.1. 
30 By comparison, the systems participating in MUC-6 had a recall for the named entity task ranging 
from 82% to 96%, and precision from 89% to 97%, but used comprehensive lists of cue words or 
consulted ictionaries of names. The system from Sheffield (Gaizauskas et al 1995), for instance, used a 
list of 2,600 names of organizations, 94company designators (Co., Ltd, PLC, etc.), 160 titles (Dr., Mr., 
etc.), about 500 human ames from the Oxford Advanced Learner's Dictionary, 2,268 place names 
(country, province, and city names), and other trigger words for locations, government institutions and 
organizations (Golf, Mountain, Agency, Ministry, Airline, etc.). In MUC-7, the best combined precision 
score, 93.39%, was achieved by the system from LTG in Edinburgh (Mikheev, Moens, and Grover 1999), 
which doesn't use such knowledge sources. We used this system in a version of our prototype that 
only attempts to resolve bridging descriptions (Ishikawa 1998). 
575 

Vieira and Poesio Processing Definite Descriptions 
Spec-Pred 
Y 
/ 
1 
N 
Y 
/ 
3 
N 
Y 
/ 
3 
N 
Y 
/ 
3 
Spec-Pred = special predicate 
Appos = apposition 
Dir-Ana = same head antecedent 
PropN = proper noun 
RPostm = restrictive postmodification 
RPrem = restrictive premodification 
CopC = copular construction 
N 
Y 
/ 
3 
N 
1 Direct anaphora 
2 Bridging 
3 Discourse new 
~C 
\N  
N 
2 Fail 
Figure 3 
Hand-designed decision tree for Version 1 and Version 2. 
? only then try to interpret he definite description as a bridge (last test). 
The heuristics for recognizing bridging descriptions are only applied when the 
other heuristics fail. This is because the performance of these heuristics is very poor 
and also because some of the heuristics that deal with bridging descriptions are com- 
putationally expensive; the idea was to eliminate those cases less likely to be bridg- 
ing before applying these heuristics. The system does not classify all occurrences of 
definite descriptions: when none of the tests succeeds, the definite description is not 
classified. We observed in our first tests that definite descriptions not resolved as direct 
anaphora nd not identified as discourse-new by our heuristics were mostly classified 
in the standardized annotation as bridging descriptions or discourse-new. Examples 
of discourse-new descriptions not identified by our heuristics are larger situation uses 
such as the world, the nation, the government, the economy, the marketplace, the spring, the 
577 

Vieira and Poesio Processing Definite Descriptions 
TOTAL TYPES IDENTIF IED BY THE SYSTEM 
anaphoric:  270 
larger s it . /unfam: 428 
total: 698 
TOTAL NON CLASSIF IED 
anaphoric:  41 
larger s it . /unfam: 113 
associat ive:  162 
idiom: 20 
doubt: 6 
total: 342 
TOTAL TYPES CLASSIF IED BY HAND 
anaphoric:  312 
larger sit . /unfam: 492 
associat ive:  204 
idiom: 22 
doubt: i0 
total: 1040 
Figure 5 
Summary of the results of Version 1 on training data. 
Table 18 
Global results of Version 1 on training data. 
System's tasks R P F 
Anaphora classification 78% 90% 83% 
Anaphora resolution 76% 88% 81% 
Discourse-new 75% 86% 80% 
Overall 59% 88% 70% 
Table 19 
Evaluation of Version l on the test data. 
System's tasks R P F 
Anaphora classification 67% 90% 77% 
Anaphora resolution 62% 83% 71% 
Discourse-new 69% 72% 70% 
Overall 53% 76% 63% 
The recall and precision figures for the system's performance over the test data are 
presented in Table 19. This corpus consisted of 14 texts, containing 2,990 NPs. Again, 
almost half of the NPs were considered as potential antecedents. The system processed 
464 defir~te descriptions; of these, the system could classify 324:115 as direct anaphora, 
209 as discourse-new. Of the antecedents, 88 were definites themselves. The system 
incorrectly resolved 77 definite descriptions: 19 anaphoric definites and 58 discourse- 
new. As before, there were just a few more errors in anaphora resolution than in 
anaphora classification. The overall recall for the test data was 53% (247/464); precision 
was 76% (247/324). 
One difference between the results on the two data sets is the distribution into 
classes of those descriptions that the system fails to classify. In the first corpus, the 
largest number of cases not classified are bridging descriptions. By contrast, the largest 
number of cases not classified by the system in Corpus 2 are discourse-new. 
579 
Computational Linguistics Volume 26, Number 4 
NR. OF TEXTS:  14 NR. OF NOUN PHRASES:  2990 
NR. OF ANTECEDENTS CONSIDERED:  1226 
Indefinites: 657 
Possessives: 144 
Def in i tes :  425 
NR. OF DEF IN ITE  DESCRIPT IONS:  464 
D IRECT ANAPHORA:  115 ANTECEDENTS FOUND: Indef in i tes :  21 
Possessives: 6 
Def in i tes :  88 
D ISCOURSE NEW DESCRIPT IONS:  209 
LARGER S ITUAT ION USES:  82 UNFAMIL IAR USES : 127 
NAMES : 44 NP  COMP. /UN.MOD. :  16 
T IME REFERENCES : 21 APPOSIT IONS : i0 
REST.PREMOD.  : 17 REST.  POSTMOD.  : 95 
COPULA : 6 
NON- IDENTIF IED:  140 
TOTAL  EST IMATED ERRORS (for anaphora  c lass i f i ca t ion)  : 12 
TOTAL  EST IMATED ERRORS (for anaphora  reso lu t ion)  : 19 
TOTAL  EST IMATED ERRORS (for la rger  s i tuat ion /unfami l ia r ) :  58 
Figure 6 
Global results of Version 1 on test data, 
TOTAL TYPES IDENTIF IED BY  THE SYSTEM 
anaphoric: 115 
la rger  s i t . /un fam:  209 
total :  324 
TOTAL  NON CLASSIF IED 
anaphoric: 29 
la rger  s i t . /un fam:  61 
assoc ia t ive :  46 
doubt :  4 
total :  140 
TOTAL  TYPES CLASS IF IED BY  HAND 
anaphoric: 154 
la rger  s i t . /un fam:  218 
assoc ia t ive :  81 
doubt :  I i 
total :  464 
Figure 7 
Summary of the results for test data. 
6.3 Results for Bridging Descriptions 
As discussed in Section 5.4, the results of the heuristics for bridging descriptions pre- 
sented in Section 4.3 were not very good. We nevertheless included these heuristics in 
Version 2 of the system, which, as discussed above, applied them to those descriptions 
that failed to be recognized as direct anaphora or discourse-new. The heuristics were 
applied in the following order: 
1. proper names, 
580 
Vieira and Poesio Processing Definite Descriptions 
Table 20 
Evaluation of the bridging heuristics all together. 
Bridging Found by System False 
Class Positive 
Names 12 14 
Common ouns 15 10 
WordNet 34 76 
Total 61 100 
Table 21 
Comparative evaluation of the two versions (test data). 
System's versions R P F 
V.1 Overall 53% 76% 62% 
V.2 Overall 57% 70% 62% 
2. compound nouns, 
3. WordNet, 
Training Data. The manual evaluation of the results of Version 2 on the training data 
is presented in Table 20. The table lists the number of acceptable anchors and the 
number of false positives found by each heuristic. Note that the system sometimes 
finds anchors that are not those identified manually, but are nevertheless acceptable. 
We found fewer bridging relations than the number we observed in the corpus 
analysis (204); furthermore, the number of false positives produced by such heuristics 
is almost wice the number of right answers. 
Test Data. Version 2 was tested over the test data using automatic evaluation--i.e., the 
system was only evaluated as a classifier, and the anchors found were not analyzed. 
A total of 57 bridging relations were found, but only 19 of the definite descriptions 
classified as bridges by the system had been classified as bridging descriptions in the 
standard annotation. Compared to Version 1 of the system, which does not resolve 
bridging descriptions, Version 2 has higher recall but lower precision, as shown in 
Table 21. 
6.4 Agreement  among System and Annotators for Version 1 and Version 2 
As a second form of evaluation of the performance of the system, we measured its 
agreement with the annotators on the test data using the K statistic. 
Version 1 of the system finds a classification for 318 out of 464 definite descrip- 
tions in Corpus 2 (the test data). If all the definite descriptions that the system cannot 
classify are treated as discourse-new, the agreement between the system and the three 
subjects that annotated this corpus on the two classes first-mention (= discourse-old) 
and subsequent-mention (= discourse-new or bridges) is K = 0.7; this should be com- 
pared with an agreement of K = 0.77 between the three annotators themselves. If, 
instead of counting these definite descriptions as discourse-new, e simply do not 
include them in our measure of agreement, then the agreement between the system 
and the annotators i  K = 0.78, as opposed to K = 0.81 between the annotators. (Notice 
that the fact that the agreement between annotators goes up, as well, indicates that 
the definite descriptions that the system can't handle are "harder" than the rest.) 
581 
Computational Linguistics Volume 26, Number 4 
Version 2 finds a classification for 355 out of 464 definite descriptions; however, 
its agreement figures are worse. If we count the cases that the system can't classify 
as discourse-new, the agreement between the system and the three annotators for the 
three classes is K = 0.57; if we count hem as bridges, K = 0.63; if we just discard those 
cases, K = 0.63 again. (By comparison, the agreement among annotators on the three 
classes was K -~ 0.68 overall and K = 0.70 on just the cases that the system was able 
to classify.) As mentioned above, the cases that the system can't handle are mainly 
discourse-new descriptions ( ee Figure 7). 
6.5 Deriving the Order of Application of the Heuristics Automatically 
6.5.1 Inducing a Decision Tree. The decision tree discussed in Section 6.1 was derived 
manually, by trial and error. We also tried to derive the order of application of the 
heuristics automatically. To do this, we used a modified version of the system to 
assign Boolean feature values to each definite description in the training corpus (i.e., 
the system checked if the features applied to a definite description instance or not). 
The following features were used: 
. 
. 
. 
4. 
5. 
Special predicates (Spec-Pred): this feature has the value yes if a special 
predicate occurs in the definite description (as specified in Section 4.2), 
and if a complement is there when needed. 
Direct anaphora (Dir-Ana): this feature has the value yes if the system 
can find an antecedent with a same-head noun for that description 
(respecting the constraints discussed in Section 4.1). 
Apposition (Appos): yes when the description is in appositive 
construction. 
Proper noun (PropN): yes when the description has a capitalized initial. 
Restrictive postmodification (RPostm): yes if the definite description is
modified by relative or associative clauses. 
This list of features, together with the classification assigned to each description i  
the standard annotation (DDUse), was used to train an implementation f Quinlan's 
learning algorithm ID3 (Quinlan 1993). We excluded the verification of restrictive pre- 
modification and copula constructions, since these parameters had given the poorest 
results before (see Section 6.2). An example of the samples used to train ID3 is shown 
in (58). 
(58) Spec-Pred Dir-Ana Appos PropN RPostm DDUse 
no no  no  yes  no  3 
no no  no  no  yes  3 
no no no  no  no  2 
no no  no  no  no  2 
no no no  no  no 1 
no yes  no  no  no  1 
The algorithm generates a decision tree on the basis of the samples given. The resulting 
decision tree is presented in Figure 8. 
The main difference between this algorithm and the algorithm we arrived at by 
hand is that the first feature checked by the decision tree generated by ID3 is the 
presence of an antecedent with a same-head noun. The presence of special predicates, 
which we adopted as the first test in our decision tree, is only the fourth test in the 
tree in Figure 8. 
582 
Vieira and Poesio Processing Definite Descriptions 
Dir-Ana Dir-Ana = same head antecedent 
/////NNNNN RPostm = restrictive postmodification 
/ /~stm Appos = apposition 
Spec-Pred =special predicate 
1 /~// ~ &  N PropN = proper noun 
3 ~ec-Pred 
2 Bridging 
3 Discourse new 3 2 
Figure 8 
Generated ecision tree. 
6.5.2 Evaluation of the Automatically Learned Decision Tree. The performance of 
the learned decision tree was compared with that of the algorithm we arrived at by 
trial and error as follows: The first 14 texts of Corpus 1 (845 descriptions) were used 
as training data to generate the decision tree. We then tested the learned algorithm 
over the other 6 texts of that corpus (195 instances of definite descriptions). 
Two different ests were undertaken: 
first, we gave as input to the learning algorithm all cases classified as 
direct anaphora, discourse-new, or bridging, 818 in total (this test 
produces the decision tree presented in the previous section); 
in a second test, the algorithm was trained only with direct anaphora 
and discourse-new descriptions (639 descriptions); all cases classified as 
bridging, idiom, or doubt in the standard annotation were not given as 
input in the learning process. This algorithm was then only able to 
classify descriptions as one of those two classes. The resulting decision 
tree classifies descriptions with a same-head antecedent as anaphoric; all 
the rest as discourse-new. 
Here we present he results evaluated all together, considering the system as a 
classifier only, i.e., without considering the tasks of anaphora resolution and of identi- 
fication of discourse-new descriptions separately. The output produced by the learned 
algorithm is compared to the standard annotation. Since the learned algorithm classi- 
fies all cases, the number of responses i equal to the number of cases, as a consequence, 
recall is the same as precision, and so is the F measure. 
The tests over 6 texts with 195 definite descriptions gave the following results: 
? R = P = F = 69% when the algorithm was trained with three classes; 
? R = P = F = 75%, when training with two classes only. 
583 
Computational Linguistics Volume 26, Number 4 
The best results were achieved by the algorithm trained for two classes only. 
This is not surprising, especially considering how difficult it was for our subjects to 
distinguish between discourse-new and bridging descriptions. 
The hand-crafted decision tree (Version 2) achieved 62% recall and 85% precision 
(F = 71.70%) on those same texts: i.e., a higher precision, but a lower F measure, due 
to a lower recall, since---unlike the learned algorithm--it does not classify all instances 
of definite descriptions. If, however, we take the class discourse-new as a default for 
all cases of definite descriptions not resolved by the system, recall, precision, and F 
value go to 77%, slightly higher than the rates achieved by the decision tree produced 
by ID3. 
As the learned decision tree has the search for a same-head antecedent as the first 
test, we modified our algorithm to work in the same way, and tested it again with the 
two corpora. The results with this configuration were: 
? R = 0.75, P = 0.87, F = 0.80, for the training data (compared with 
R = 0.76, P = 0.88, F = 0.81) ; 
? R = 0.59, P = 0.83, F = 0.69, for the test data (compared with R = 0.62, 
P = 0.83, F = 0.71). 
In other words, the results were about the same, although a slightly better performance 
was obtained when the tests to identify discourse-new descriptions were tried first. 
7. Other Computational Models of Definite Description Processing 
A major difference between our proposal and almost all others (theoretical and im- 
plemented) is that we concentrate on definite descriptions; most of the systems we 
discuss below attempt o resolve all types of anaphoric expressions, often concentrat- 
ing on pronouns. Focusing on definite descriptions allowed us to investigate what 
types of lexical knowledge and commonsense inference are actually used in natural 
language comprehension. 
From an architectural standpoint, he main difference between our work and other 
proposals in the literature is that we paid considerably more attention to the problem 
of identifying discourse-new definite descriptions. 32 
Previous work on computational methods for definite description resolution can 
be divided in two camps: proposals that rely on commonsense r asoning (and are 
therefore ither mainly theoretical or domain dependent), and systems that can be 
quantitatively evaluated, such as those competing on the coreference task in the Sixth 
and Seventh Message Understanding Conference (Sundheim 1995). We discuss these 
two types of work in turn. 
7.1 Models Based On Commonsense Reasoning 
The crucial characteristic of these proposals is that they exploit hand-coded common- 
sense knowledge, and cannot therefore be tested on just any arbitrary text. Some of 
them are simply tested on texts that were especially built for the purpose of testing 
the system (Carter 1987; Carbonell and Brown 1988); systems like the Core Language 
Engine are more robust, but they have to be applied to a domain restricted enough 
that all relevant knowledge can be encoded by hand. 
32 This problem is also a central concern in the work by Bean and Riloff (1999). 
584 
Vieira and Poesio Processing Definite Descriptions 
Sidner's Theory of Definite Anaphora Comprehension. I  her dissertation, Sidner (1979) 
proposed a complete theory of definite NP resolution, including detailed algorithms 
for resolving pronouns, anaphoric definite descriptions, and bridging descriptions. She 
also proposed methods for resolving larger situation uses; the one class her methods 
do not handle are those definite descriptions that, following Hawkins, we have called 
unfamiliar uses. 
The main contribution of Sidner's dissertation is her theory of focus and its role 
in resolving definite NPs; to this day, her focus-tracking algorithms are arguably the 
most detailed account of the phenomenon. The main problem with Sidner's work from 
our perspective is that her algorithms rely heavily on the availability of a semantic 
network and causal reasoner; furthermore, some of the inference mechanisms are left 
relatively underspecified (this latter problem was in part corrected in subsequent work 
by Carter--see below). Lexical and con~nonsense knowledge play three important 
roles in Sidner's system: they are used to track focus, to resolve bridging descriptions 
and larger situation uses, and to evaluate interpretive hypotheses, discarding those 
that seem implausible. Only recently have robust knowledge-based methods for some 
of these tasks begun to appear, and their performance is still not very good, as seen 
above in our discussion of using WordNet as a semantic network; 33 as for checking 
the plausibility of a hypothesis on the basis of causal knowledge about the world, we 
now have a much better theoretical grasp of how such inferences could be made (see, 
for example, Hobbs et al \[1993\] and Lascarides and Asher \[1993\]), but we are still 
quite a long way from a general inference ngine. 
We also found that some of Sidner's resolution rules are too restrictive. For ex- 
ample, her Cospecification rule 1 prescribes that definite description and focus must 
have the same head, and no new information can be introduced by the definite; but 
this rule is violated fairly frequently in our corpus. This criticism is not new: In 1983, 
it was already recognized that an anaphoric full noun phrase may include some new 
and unshared information about a previously mentioned entity (Grosz, Joshi, and We- 
instein 1983), and Carter (1987) weakened some of the restrictions proposed by Sidner 
in his system. 
Carter's Shallow Processing Anaphor Resolver. Carter (1987) implemented a modified ver- 
sion of Sidner's algorithm and integrated it with an implemented version of Wilks' 
theory of commonsense r asoning. This work is interesting for two reasons: first of all, 
because Carter, unlike Sidner, attempted to evaluate the performance ofhis system; and 
because, in doing so, he addressed the commonsense r asoning problem in some detail. 
Carter's ystem, SPAR, is based on the Shallow Processing Hypothesis: that in re- 
solving anaphors, reasoning should be avoided as much as possible. This is, of course, 
the same approach taken in our own work, which could be seen as pushing Carter's ap- 
proach to the extreme. The difference is that when it becomes necessary, SPAR does use 
two commonsense knowledge sources: a semantic network based on Alshawi's theory 
of memory for text interpretation (Alshawi 1987) and a causal reasoner based on Wilks' 
work (Wilks 1975). In both cases, the necessary information was encoded by hand. 
Carter's system was tested over short stories specifically designed for the testing 
of the system: about 40 written by Carter himself, and 23 written by others. These 
latter contain about 80 definite descriptions. SPAR correctly resolved all anaphors in 
the stories written by Carter, and 66 out of 80 of the descriptions in the 23 other stories. 
33 An implementation f a (simplified) version of Sidner's focus-tracking algorithms capable of being 
used by a system like ours was presented in Azzam, Humphreys, and Gaizauskas (1998). 
585 
Computational Linguistics Volume 26, Number 4 
(Carter himself points out that these results are "of limited significance because of the 
simplicity of the texts processed compared to 'real' texts" \[p. 238\].) 
The Core Language Engine. The Core Language Engine (CLE) (Alshawi 1992) is a domain- 
independent system developed at SRI Cambridge, which translates English sentences 
into formal representations. The system was used by SRI for a variety of applications, 
including spoken language translation and airline reservations. The CLE makes use of 
a core lexicon (to which new entries can be added) and uses an abductive common- 
sense reasoner to produce an interpretation a d to verify the plausibility of choice of 
referents from an ordered list; the required world knowledge has to be added by hand 
for each domain, together with whatever lexical knowledge is needed. 
The construction of the formal representation goes through an intermediate stage 
called quasi-logical form (QLF). The QLF may contain unresolved terms correspond- 
ing to anaphoric NPs including, among others, definite descriptions. The resolution 
process that transforms QLFs into resolved logical form representations of entences i  
described in Alshawi (1990). Definite descriptions are represented asquantified terms. 
The referential readings of definite descriptions are handled by proposing referents 
from the external application context (larger situation uses) as well as the CLE context 
model (anaphoric uses). Attributive readings may also be proposed uring QLF reso- 
lution; some of these seem to correspond to our unfamiliar uses. Thus, the CLE seems 
to account for discourse-new descriptions, although they are not explicitly mentioned, 
and the methods used for choosing a referential or an attributive interpretation are 
not discussed. To our knowledge, no analysis of the performance of the system has 
been published. 
7.2 The Systems Involved in the MUC-6 Coreference Task 
The seven systems that participated in the MUC-6 competition can all be quantitatively 
evaluated; they achieved recall scores ranging from 35.69% to 62.78% and precision 
scores ranging from 44.23% to 71.88% on nominal coreference. 
It is important to note that the evaluation in MUC-6 differed from ours in three 
important aspects. First of all, these systems have to parse the texts, which often in- 
troduces errors; furthermore, these systems often cannot get complete parses for the 
sentences they are processing. Secondly, the evaluation i  MUC-6 considers the coref- 
erential chain as a whole, and not only one correct antecedent. The third difference is
that these systems process a wider range of referring expressions, including pronouns 
and bare nouns, while our system only processes definite NPs. On the other hand, not 
all definite descriptions are marked in the MUC-6 coreference task: these systems are 
only required to identify identity relations, and only if the antecedent was introduced 
by a noun phrase (not if it was a clause or a conjoined NP). This leaves out discourse- 
new descriptions and, especially, bridging descriptions, which, as we have seen, are 
by far the most difficult cases. 
Kameyama (1997) analyzes in detail the coreference module of the SRI system 
that participated in MUC-6 (Appelt et al 1995). This system achieved one of the top 
scores for the coreference task: a recall of 59% and a precision of 72%. The SRI system 
uses a sort hierarchy claimed to be sparse and incomplete. For definite descriptions, 
Kameyama reports the results of a test on five articles, containing 61 definite descrip- 
tions in total; recall was 46% (28/61), and for proper names, 69% (22/32). The precision 
figures for these two subclasses are not reported. Some of the errors in definite de- 
scriptions are said to be due to nonidentity referential relations; however, there is no 
mention of differences between discourse-new and bridging descriptions. Other errors 
were said to be related to failure in recognizing synonyms. 
586 
Vieira and Poesio Processing Definite Descriptions 
7.3 Probabilistic Methods in Anaphora Resolution 
Aone and Bennet (1995) propose an automatically trainable anaphora resolution sys- 
tem. They train a decision tree using the C 4.5 algorithm by feeding feature vectors 
for pairs of anaphor and antecedent. They use 66 features, including lexical, syntac- 
tic, semantic, and positional features. Their overall recall and precision figures are 
66.56% and 72.18%. Considering only definite NPs whose referent is an organization 
(that is the only distinction available in their report), recall is 35.19% and precision 
50% (measured on 54 instances). Their training and test texts were newspaper articles 
about joint ventures, and they claim that because ach article always talked about 
more than one organization, finding the antecedents of organizational naphora was 
not straightforward. 
In Burger and Connolly (1992) a Bayesian network is used to resolve anaphora 
by probabilistically combining linguistic evidence. Their sources of evidence are c- 
command (syntactic onstraints), semantic agreement (gender, person, and number 
plus a term subsumption hierarchy), discourse focus, discourse structure, recency, and 
centering. Their methods are described and exemplified but not evaluated. A Bayesian 
framework is also proposed by Cho and Maida (1992) for the identification of definite 
descriptions' referents. 
8. Conclusions and Future Work 
8.1 Contributions 
We have presented a domain-independent system for definite description interpreta- 
tion whose development was based on an empirical study of definite description use 
that included multiannotator experiments. Our system not only attempts to find an 
antecedent for a definite description, it also uses methods for recognizing discourse- 
new descriptions, which our previous studies revealed to be the largest class of def- 
inite descriptions in our corpus. Our algorithms for segmentation, matching, and 
identification of discourse-new descriptions only rely on syntax-based heuristics and 
on on-line lexical sources such as WordNet; the final configuration of these heuris- 
tics, as well as their order of application, was arrived at on the basis of extensive 
experiments using our training corpus. Because our system only relies on "shal- 
low" information, it encounters problems when commonsense r asoning is actually 
needed; on the other hand, it can be tested on any domain without extensive hand- 
coding. 
As far as direct anaphora is concerned, we evaluated heuristic algorithms for 
segmentation and matching. Our system achieved 62% recall and 83% precision for 
direct anaphora resolution on our test data. For identifying discourse-new descriptions, 
we exploited the correlation between certain types of syntactic onstructions and type 
of use noted by Hawkins (1978) and semantically explained by L6bner (1987). Our 
system achieved 69% recall and 72% precision for this class on the test data. Overall, 
the version of the system that only attempts to recognize first-mention and subsequent- 
mention definite descriptions achieved a recall of 53% and a precision of 76% on the 
test corpus if we count the definite descriptions the system can't handle as errors; if 
we count them as discourse-new, both recall and precision are 66%. 
The class of bridging descriptions i  the most difficult to process: this is in part 
because humans themselves do not agree much on which definites count as bridges 
and what their anchors are, in part because lexical knowledge and commonsense 
reasoning are necessary to solve them. Our results for this class are, therefore, still 
very tentative; this did not much affect the performance of the system, however, since 
in the texts we tried, bridging descriptions are a relatively small class. Noncoreferent 
587 
Computational Linguistics Volume 26, Number 4 
bridging descriptions were around 8% of the definite descriptions in the corpus, and 
the class of bridging descriptions including those with a coreferent antecedent with 
a different head noun were about 15% of the total. We tried techniques that do not 
involve heavy axiomatization f commonsense knowledge, and only used an existing 
lexical source, WordNet. 
In other text genres the distribution of definite descriptions into classes might 
change; spoken dialogue, for example, tends to have a higher number of deictic def- 
inite descriptions. However, other researchers (Fraurud 1990) found a similar distri- 
bution of first-mention and subsequent-mention definites in text corpora; we believe 
therefore that the heuristics we propose here, and their ordering, will still be ade- 
quate. Direct anaphora nd discourse-new descriptions can be processed with much 
simpler methods and it seems that the distinguishing features do not usually over- 
lap. 
8.2 What's Needed Next? 
We would like to emphasize again that we are not trying to suggest hat shallow 
methods will be sufficient for processing definite descriptions in the long run. What 
we do believe is that hypotheses about processing should be evaluated; unfortunately, 
only fairly simple techniques can be tested in this way at the moment, but this work 
can serve to motivate more clearly the use of more complex methods. 
We highlighted throughout the paper, and particularly in Section 5, some of the 
points where shallow methods break down, and better lexical sources or commonsense 
knowledge are needed. By far the worse results are obtained for bridging descriptions; 
in this area, the most urgent needs are better sources of lexical knowledge, 34 and some 
robust focusing mechanism. Finding better ways of segmenting the text is perhaps the 
area in which the most progress has been made since we started this project; robust 
methods for text segmentation are now available (Hearst 1997; Richmond, Smith, and 
Amitay 1997). A proper treatment of modification seems harder; as discussed in Sec- 
tion 4.1, it seems necessary to rely heavily on reasoning in some cases. In order to 
improve our treatment of discourse-new descriptions it will be necessary, on the one 
hand, to find ways of automatically acquiring lexical information about the function- 
ality of nouns and adjectives, and on the other hand, to have sources of encyclopedic 
knowledge available. 
8.3 Future Work 
8.3.1 Simple Extensions. In this project we were more interested in clearly identify- 
ing the subtasks of the definite description process that in achieving optimum per- 
formance; as a consequence, there are a number of fairly simple ways in which the 
final version of the system could be improved. The next step in making our system 
truly testable on any type of text would be to make it work off the output of a robust 
parser: we are currently testing Abney's CASS parser (Abney 1991) for this purpose. 
See Ishikawa (1998), for some initial results. We are also experimenting with existing 
software that performs in a more sophisticated way some of the tasks that our system 
currently implements in a fairly crude fashion, including lemmatization, proper name 
recognition, and named entity typing. 
Another aspect of the system that deserves further examination is the construction 
of coreference chains and cases of multiple resolutions. We did not get a clear picture 
34 As mentioned above, we have done some preliminary work on acquiring this information 
automatically (Poesio, Schulte im Walde, and Brew 1998; Ishikawa 1998). 
588 
Vieira and Poesio Processing Definite Descriptions 
of how complete or incomplete, or how broken, the coreferential chains resulting from 
the processing of one text are, nor did we relate them to the chains of the annotated 
texts; to do so, the system and the annotation would have to be extended to cover all 
cases of anaphoric expressions. 
8.3.2 The Role of Focus in Definite Descriptions Processing. Our tests with bridging 
descriptions resulted in a great number of false positives. Our analysis of these data, 
as well as of other corpora (Hitzeman and Poesio 1998), suggests that a local focusing 
mechanism as proposed in Grosz (1977), Sidner (1979), Grosz, Joshi, and Weinstein 
(1983, 1995), and Grosz and Sidner (1986) would improve the results obtained by our 
system. 
There are several reasons why our system does not yet include such a mechanism. 
One problem already mentioned is that Sidner's algorithms as stated, and even as 
implemented by Carter, are difficult to implement, since considerably more lexical 
information is needed than we have available (e.g., about the thematic roles of verbs), 
a rich knowledge base is needed both to resolve bridging descriptions and larger 
situation uses, and commonsense inference is needed to evaluate the plausibility of 
hypotheses. A second problem with Sidner's theory of local focus, as well as others 
such as Centering Theory (Grosz, Joshi, and Weinstein 1995), is the lack of a precise 
characterization f how to deal with complex sentences. Revisions and extensions of 
Sidner's proposal related to these problems have been proposed in Suri and McCoy 
(1994), and include algorithms for updating focus in complex sentences containing 
adjunct clauses uch as before- and after-clauses. 
We plan to incorporate simpler focus-tracking mechanisms in future versions of 
the system, possibly along the lines of Azzam, Humphreys, and Gaizauskas (1998) or 
Tetreault (1999). 
8.3.3 Theoretical Developments. We defended the importance of developing methods 
for identifying discourse-new descriptions, and we believe that there is still need for 
research into the semantics of this class; that is, what, exactly, licenses the use of a 
definite description to refer to a discourse-new entity? The role of premodification 
and postmodification should also be further examined. Postmodification is one of 
the most frequent features of discourse-new descriptions; additional empirical studies 
considering a detailed subclassification f discourse-new descriptions would give us 
a better understanding of the problem. The postmodification of a description often 
acts as an explicit anchor (what LObner \[1987\] calls "disambiguating arguments and 
attributes"); understanding how the head noun of a postmodified escription relates 
"semantically" with its complement is a problem similar to that of identifying the 
semantic relation between a bridging description and its anaphoric anchor, but to date 
there hasn't been much research on this topic (while there has been a lot of work on 
identifying the relations that hold between the premodifiers, especially in noun-noun 
compounds). An NP's head noun may also corefer with its complement, as seen in 
the examples in (59): 
(59) a. the dream of home ownership 
b. the issue of student grants 
We also observed that definite descriptions with premodification were responsible for 
considerable disagreement among the annotators, the reasons for which are still to be 
explained. 
589 
Computational Linguistics Volume 26, Number 4 
We wish to thank Ellen Bard, Rafael 
Bordini, Jean Carletta, Miriam Eckert, Kari 
Fraurud, Rob Gaizauskas, Janet Hitzeman, 
Chris Mellish, and our anonymous 
reviewers for comments, help, and 
suggestions. Renata Vieira was supported in 
part by a fellowship from CNPq, Brazil; 
Massimo Poesio is supported by an EPSRC 
Advanced Research Fellowship. 
References 
Abney, Steve. 1991. Parsing by chunks. In 
R. Berwick, S. Abney, and C. Tenny, 
editors, Principle-based Parsing. Kluwer, 
Dordrecht, pages 257-278. 
Alshawi, Hiyan. 1987. Memory and Context 
for Language Interpretation. Cambridge 
University Press, Cambridge. 
Alshawi, Hiyan. 1990. Resolving 
quasiqogical forms. Computational 
Linguistics, 16(3):133-144. 
Alshawi, Hiyan, editor. 1992. The Core 
Language Engine. MIT Press, Cambridge, 
MA. 
Aone, Chinatsu and Scott W. Bennett. 1995. 
Automated acquisition of anaphora 
resolution strategies. In Proceedings ofthe 
AAAI Spring Symposium on Empirical 
Methods in Discourse Interpretation and 
Generation, pages 1-7, Stanford. 
Appelt, Douglas, Jerry R. Hobbs, John Bear, 
David Israel, Megumi Kameyama, Andy 
Kehler, David Martin, Karen Myers, and 
Mabry Tyson. 1995. SRI International 
FASTUS system MUC-6 test results and 
analysis. In Proc. of the Sixth Message 
Understanding Conference, pages 237-248, 
Columbia, MD, November. 
Azzam, Saliha, Kevin Humphreys, and 
Robert Gaizauskas. 1998. Evaluating a
focus-based approach to anaphora 
resolution. In COLING-ACL "98: 36th 
Annual Meeting of the Association for 
Computational Linguistics and the 17th 
International Conference on Computational 
Linguistics, pages 74-78, Montreal, 
Quebec, Canada. 
Bean, David L. and Ellen Riloff. 1999. 
Corpus-based i entification of 
non-anaphoric noun phrases. In 
Proceedings ofthe 37th Annual Meeting, 
pages 373-380, University of Maryland. 
Association for Computational 
Linguistics. 
Bikel, Daniel, Scott Miller, 
Richard Schwartz, and Ralph Weischedel. 
1997. Nymble: A high-performance 
learning name finder. In Proceedings ofthe 
5th Conference on Applied Natural Language 
Processing, pages 194-201, Washington, 
DC Association for Computational 
Linguistics. 
Bosch, Peter and Bart Geurts. 1989. 
Processing definite NPs. IWBS Report 78, 
IBM Germany, July. 
Burger, John D. and Dennis Connolly. 1992. 
Probabilistic resolution of anaphoric 
reference. In Proceedings ofthe AAAI Fall 
Symposium on Probabilistic Approaches to 
Natural Language, pages 17-24, 
Cambridge, MA. 
Carbonell, Jamie and Ralf D. Brown. 1988. 
Anaphora resolution: A multi-strategy 
approach. In Proceedings ofthe 12th 
International Conference on Computational 
Linguistics (COLING-88), pages 96-101, 
Budapest, Hungary. 
Carletta, Jean. 1996. Assessing agreement on 
classification tasks: The kappa statistic. 
Computational Linguistics, 22(2):249-254. 
Carletta, Jean, Amy Isard, Stephen Isard, 
Jacqueline C. Kowtko, Gwyneth 
Doherty-Sneddon, and Anne H. 
Anderson. 1997. The reliability of a 
dialogue structure coding scheme. 
Computational Linguistics, 23(1):13-32. 
Carter, David M. 1987. Interpreting Anaphors 
in Natural Language Texts. Ellis Horwood, 
Chichester, UK. 
Chinchor, Nancy A. 1995. Statistical 
significance of MUC-6 results. In 
Proceedings ofthe Sixth Message 
Understanding Conference (MUC-6), 
pages 39-44, Columbia, MD, 
November 6-8. 
Chinchor, Nancy A. 1997. Overview of 
MUC-7/MET-2. In Proceedings ofthe 
Seventh Message Understanding Conference 
(MUC-7). Available at http://www.muc. 
saic.com/proceedings/ 
muc_7_proceedings/overview, html. 
Cho, Sehyeong and Anthony S. Maida. 1992. 
Using a Bayesian framework to identify 
the referents of definite descriptions. In
Proceedings ofthe AAAI Fall Symposium on 
Probabilistic Approaches to Natural Language, 
pages 39-46, Cambridge, MA. 
Clark, Herbert H. 1977. Inferences in 
comprehension. I  D. Laberge and S. J. 
Samuels, editors, Basic Process in Reading: 
Perception and Comprehension. Lawrence 
Erlbaum, pages 243-263. 
Clark, Herbert H. and Catherine R. 
Marshall. 1981. Definite reference and 
mutual knowledge. In A. Joshi, 
B. Webber, and I. Sag, editors, Elements of 
Discourse Understanding. Cambridge 
University Press, New York. 
Fellbaum, Christiane, editor. 1998. WordNet: 
An Electronic Lexical Database. MIT Press, 
Cambridge, MA. 
590 
Vieira and Poesio Processing Definite Descriptions 
Fox, Barbara A. 1987. Discourse Structure and 
Anaphora. Cambridge University Press, 
Cambridge, UK. 
Fraurud, Keri. 1990. Definiteness and the 
processing of NPs in natural discourse. 
Journal o/Semantics, 7:395-433. 
Gaizauskas, Robert, Takahiro Wakao, 
Kevin Humphreys, Hamish Cunningham, 
and Yorick Wilks. 1995. University of 
Sheffield: Description of the LaSIE System 
as used for MUC-6. In Proceedings o/the 
Sixth Message Understanding Conference 
(MUC-6), pages 207-220. Morgan 
Kaufmann. 
Grosz, Barbara J. 1977. The Representation a d 
Use of Focus in Dialogue Understanding. 
Ph.D. thesis, Stanford University. 
Grosz, Barbara J., Aravind K. Joshi, and 
Scott Weinstein. 1983. Providing a unified 
account of definite noun phrases in 
discourse. In Proceedings o/the 21st Annual 
Meeting, pages 44-50. Association for 
Computational Linguistics. 
Grosz, Barbara. J , Aravind. K. Joshi, and 
Scott Weinstein. 1995. Centering: A
framework for modeling the local 
coherence of discourse. Computational 
Linguistics, 21(2):202-225. (The paper 
originally appeared as an unpublished 
manuscript in 1986.). 
Grosz, Barbara J. and Candace L. Sidner. 
1986. Attention, intention, and the 
structure of discourse. Computational 
Linguistics, 12(3):175-204. 
Hahn, Udo; Michael Strube, and Katja 
Markert. 1996. Bridging textual ellipsis. In 
COLING '96: Proceedings ofthe 16th 
International Conference on Computational 
Linguistics, pages 496-501, Kopenhagen, 
Aug 5-9 1996. 
Hardt, Daniel. 1997. An empirical approach 
to VP ellipsis. Computational Linguistics, 
23(4):525-541. 
Hatzivassiloglou, Vasileios and 
Kathleen McKeown. 1993. Towards the 
automatic identification of adjectival 
scales: clustering adjectives according to 
meaning. In Proceedings o/the 31st Annual 
Meeting, pages 172-182, Ohio State 
University. Association for Computational 
Linguistics. 
Hawkins, John A. 1978. DeJiniteness and 
Indefiniteness. Croom Helm, London. 
Hearst, Marti A. 1997. TextTiling: 
Segmenting text into multi-paragraph 
subtopic passages. Computational 
Linguistics, 23(1):33-64. 
Helm, Irene. 1982. The Semantics o/Definite 
and Indefinite Noun Phrases. Ph.D. thesis, 
University of Massachusetts atAmherst. 
Hitzeman, Janet and Massimo Poesio. 1998. 
Long-distance pronominalisation a d 
global focus. In COLING/ACL "98: 36th 
Annual Meeting of the Association/or 
Computational Linguistics and 17th 
International Conference on Computational 
Linguistics. Volume 1, pages 550-556, 
Montreal, Quebec, Canada. 
Hobbs, Jerry R., Mark E. Stickel, Douglas A. 
Appelt, and Paul Martin. 1993. 
Interpretation asabduction. Arti~cial 
Intelligence Journal, 63:69-142. 
Humphreys, Kevin, Robert Gaizauskas, 
Saliha Azzam, Chris Huyck, B. Mitchell, 
and Hamish Cunningham. 1998. 
University of Sheffield: Description of the 
LaSIE-II System as used for MUC-7. In 
Proceedings ofthe Seventh Message 
Understanding Conference (MUC-7). 
Available on the Web at 
www.muc.saic.com. 
Ishikawa, Tomonori. 1998. Acquisition of 
associative information and resolution of 
bridging descriptions. Master's thesis, 
University of Edinburgh, Department of
Linguistics, Edinburgh, Scotland. 
Kameyama, Megumi. 1997. Recognizing 
referential links: An information 
extraction perspective. In Proceedings off the 
ACL Workshop on Operational Factors in 
Practical, Robust Anaphora Resolution/or 
Unrestricted Texts, pages 46-53, Madrid, 
Spain, July. Association for 
Computational Linguistics. 
Krippendorff, Klaus. 1980. Content Analysis: 
An Introduction to its Methodology. Sage 
Publications, Beverly Hills, London. 
Landis, J. R., and G. G. Koch. 1977. The 
measurement of observer agreement for 
categorial data. Biometrics, 36:159-174. 
Lappin, Shalom and H. J. Leass. 1994. An 
algorithm for pronominal anaphora 
resolution. Computational Linguistics, 
20(4):535-562. 
Lascarides, Alex and Nicholas Ashen 1993. 
Temporal interpretation, discourse 
relations and commonsense entailment. 
Linguistics and Philosophy, 16(5):437-493. 
LObner, Sebastian. 1987. Natural anguage 
and generalised quantifier theory. In 
P. G/irdenfors, editor, Generalized 
Quantifiers. D. Reidel, Dordrecht, The 
Netherlands, pages 93-108. 
Mani, Inderjeet and T. Richard MacMillan. 
1996. Identifying unknown proper names 
in newswire text. In Bran Boguraev and 
James Pustejovsky, editors, Corpus 
Processing/or Lexical Acquisition. MIT 
Press, Cambridge, MA, pages 41-59. 
Marcu, Daniel. 1999. A decision-based 
approach to rhetorical parsing. In 
Proceedings off the 37th Annual Meeting, 
591 
Computational Linguistics Volume 26, Number 4 
pages 365--372, University of Maryland, 
June. Association for Computational 
Linguistics. 
Marcus, Mitchell P., Beatrice Santorini, and 
Mary Ann Marcinkiewicz. 1993. Building 
a large annotated corpus of English: The 
Penn Treebank. Computational Linguistics, 
19(2):313-330. 
McDonald, David. 1996. Internal and 
external evidence in the identification and 
semantic ategorization f proper names. 
In Bran Boguraev and James Pustejovsky, 
editors, Corpus Processing for Lexical 
Acquisition. MIT Press, Cambridge, MA, 
pages 21-39. 
Mikheev, Andrei, Marc Moens, and 
Claire Grover. 1999. Named entity 
recognition without gazetteers. In
Proceedings ofEACL, pages 1-8, Bergen, 
Norway. EACL. 
Mitkov, Ruslan. 2000. Towards more 
comprehensive evaluation in anaphora 
resolution. In Proceedings ofthe 2nd 
International Conference on Language 
Resources and Evaluation, 
pages 1,309-1,314, Athens. 
Paik, Woojin, Elizabeth D. Liddy, 
Edmund Yu, and Mary McKenna. 1996. 
Categorizing and standardizing proper 
nouns for efficient information retrieval. 
In Bran Boguraev and James Pustejovsky, 
editors, Corpus Processing for Lexical 
Acquisition. MIT Press, Cambridge, MA, 
pages 61-73. 
Palmer, David D. and David S. Day. 1997. A 
statistical profile of the named entity task. 
In Proceedings ofthe 5th Conference on 
Applied Natural Language Processing, 
pages 190-193, Washington, DC, March. 
Association for Computational 
Linguistics. 
Poesio, Massimo. 1993. A situation-theoretic 
formalization of definite description 
interpretation i  plan elaboration 
dialogues. In Peter Aczel, David Israel, 
Yasuhiro Katagiri, and Stanley Peters, 
editors, Situation Theory and its 
Applications, Volume 3. CSLI, Stanford, 
chapter 12, pages 339-374. 
Poesio, Massimo, Sabine Schulte im Walde, 
and Chris Brew. 1998. Lexical clustering 
and definite description interpretation. In 
Proceedings ofthe AAAI Spring Symposium 
on Learning for Discourse, pages 82-89, 
Stanford, CA, March. AAAI. 
Poesio, Massimo and Renata Vieira. 1998. A 
corpus-based investigation of definite 
description use. Computational Linguistics, 
24(2):183-216. 
Poesio, Massimo, Renata Vieira, and 
Simone Teufel. 1997. Resolving bridging 
references in unrestricted text. In 
R. Mitkov, editor, Proceedings ofthe ACL 
Workshop on Operational Factors in Robust 
Anaphora Resolution, pages 1-6, Madrid. 
Also available as HCRC Research Paper 
HCRC/RP-87, University of Edinburgh. 
Postal, Paul M. 1969. Anaphoric islands. In 
R. I. Binnick et al, editor, Papers~om the 
Fifth Regional Meeting of the Chicago 
Linguistic Society, pages 205-235. 
University of Chicago. 
Prince, Ellen F. 1981. Toward a taxonomy of 
given-new information. In Peter Cole, 
editor, Radical Pragmatics. Academic Press, 
New York, pages 223-256. 
Prince, Ellen F. 1992. The ZPG letter: 
Subjects, definiteness, and information 
status. In S. Thompson and W. Mann, 
editors, Discourse Description: Diverse 
Analyses of a Fund-Raising Text. John 
Benjamins, pages 295-325. 
Quinlan, J. Ross. 1993. C4.5: Programs for 
Machine Learning. Morgan Kaufmann, San 
Mateo, CA. 
Quirk, Randolph, Sydney Greenbaum, 
Geoffrey Leech, and Jan Svartvik. 1985. A 
Comprehensive Grammar of the English 
Language. Longman, London. 
Reichman, Rachel. 1985. Getting Computers to 
Talk Like You and Me. MIT Press, 
Cambridge, MA. 
Richmond, Kevin, Andrew Smith, and 
Einat Amitay. 1997. Detecting subject 
boundaries within text: A 
language-independent sta istical 
approach. In Proceedings ofThe Second 
Conference on Empirical Methods in Natural 
Language Processing (EMNLP-2), 
pages 47-54, Brown University. 
Russell, Bertrand. 1905. On denoting. Mind, 
14:479-493. Reprinted in Logic and 
Knowledge, R. C. Marsh, editor, George 
Allen and Unwin, London. 
Sidner, Candace L. 1979. Towards a
computational theory of definite anaphora 
comprehension in English discourse. Ph.D. 
thesis, MIT. 
Siegel, Sydney and N. John Castellan. 1988. 
Nonparametric statistics for the Behavioral 
Sciences. 2nd edition. McGraw-Hill. 
Strand, Kjetil. 1996. A taxonomy of linking 
relations. Manuscript. A preliminary 
version presented at the Workshop on 
Indirect Anaphora, Lancaster University, 
1996. 
Sundheim, Beth M. 1995. Overview of the 
results of the MUC-6 evaluation. In 
Proceedings ofthe Sixth Message 
Understanding Conference (MUC-6), 
pages 13-31, Columbia, MD, 
November 6-8. 
592 
Vieira and Poesio Processing Definite Descriptions 
Suri, Linda Z. and Kathleen F. McCoy. 1994. 
RAFT/RAPR and centering: A
comparison and discussion of problems 
related to processing complex sentences. 
Computational Linguistics, 20(2):301-317. 
Tetreault, Joel R. 1999. Analysis of 
syntax-based pronoun resolution 
methods. In Proceedings ofthe 37th Annual 
Meeting, pages 602-605, University of 
Maryland, June. Association for 
Computational Linguistics. 
Vieira, Renata. 1998. Definite Description 
Resolution i  Unrestricted Texts. Ph.D. 
thesis, University of Edinburgh, Centre 
for Cognitive Science, February. 
Vieira, Renata and Massimo Poesio. 1996. 
Processing definite descriptions in
corpora. Presented at the Discourse 
Anaphora nd Resolution Colloquium 
(DAARC), Lancaster University, 
Lancaster, UK. Also available as Research 
Paper HCRC/RP-86, University of 
Edinburgh, Human Communication 
Research Centre. 
Vieira, Renata and Simone Teufel. 1997. 
Towards resolution of bridging 
descriptions. In Proceedings ofthe 35th Joint 
Meeting of the Association for Computational 
Linguistics, pages 522-524, Madrid. 
Vilain, Marc, John Burger, John Aberdeen, 
Dennis Connolly, and Lynette Hirschman. 
1995. A model-theoretic coreference 
scoring scheme. In Proc. of the Sixth 
Message Understanding Conference, 
pages 45-52. 
Wacholder, Nina and Yael. Ravin. 1997. 
Disambiguation ofproper names in text. 
In Proceedings ofthe 5th Conference on 
Applied Natural Language Processing, 
pages 202-208, Washington, DC, March. 
Association for Computational 
Linguistics. 
Ward, Gregory, Richard Sproat, and 
Gail McKoon. 1991. A pragmatic analysis 
of so-called anaphoric islands. Language, 
67:439-474. 
Webber, Bonnie L. 1979. A Formal Approach to 
Discourse Anaphora. Garland, New York. 
Wilks, Yorick A. 1975. A preferential 
pattern-matching semantics for natural 
language. Artificial Intelligence, 6:53-74. 
593 

 
	
	Specifying the Parameters of Centering Theory: a Corpus-Based
Evaluation using Text from Application-Oriented Domains
M. Poesio, H. Cheng, R. Henschel, J. Hitzeman,y R. Kibble,x and R. Stevenson
University of Edinburgh, ICCS and HCRC,
fpoesio,huac,henschelg@cogsci.ed.ac.uk
y The MITRE Corporation, hitz@linus.mitre.org
xUniversity of Brighton, ITRI, Rodger.Kibble@itri.bton.ac.uk
University of Durham, Psychology and HCRC, Rosemary.Stevenson@durham.ac.uk
Abstract
The definitions of the basic concepts,
rules, and constraints of centering the-
ory involve underspecified notions such
as ?previous utterance?, ?realization?,
and ?ranking?. We attempted to find the
best way of defining each such notion
among those that can be annotated reli-
ably, and using a corpus of texts in two
domains of practical interest. Our main
result is that trying to reduce the num-
ber of utterances without a backward-
looking center (CB) results in an in-
creased number of cases in which some
discourse entity, but not the CB, gets
pronominalized, and viceversa.
1 MOTIVATION
Centering Theory (Grosz et al, 1995; Walker et
al., 1998b) is best characterized as a ?parametric?
theory: its key definitions and claims involve no-
tions such as ?utterance?, ?realization?, and ?rank-
ing? which are not completely specified; their pre-
cise definition is left as a matter for empirical re-
search, and may vary from language to language.
A first goal of the work presented in this paper
was to find which way of specifying these param-
eters, among the many proposed in the literature,
would make the claims of centering theory most
accurate as predictors of coherence and pronomi-
nalization for English. We did this by annotating
a corpus of English texts with the sort of informa-
tion required to implement some of the most pop-
ular variants of centering theory, and using this
corpus to automatically check two central claims
of the theory, the claim that all utterances have a
backward looking center (CB) (Constraint 1), and
the claim that if any discourse entity is pronomi-
nalized, the CB is (Rule 1). In doing this, we tried
to make sure we would only use information that
could be annotated reliably.
Our second goal was to evaluate the predic-
tions of the theory in domains of interest for real
applications?natural language generation, in our
case. For this reason, we used texts in two gen-
res not yet studied, but of interest to developers of
NLG systems: instructional texts and descriptions
of museum objects to be displayed on Web pages.
The paper is organized as follows. We first re-
view the basic notions of the theory. We then dis-
cuss the methods we used: our annotation method
and how the annotation was used. In Section 4 we
present the results of the study. A discussion of
these results follows.
2 FUNDAMENTALS OF CENTERING
THEORY
Centering theory (Grosz et al, 1995; Walker et
al., 1998b) is an ?object-centered? theory of text
coherence: it attempts to characterize the texts
that can be considered coherent on the basis of
the way discourse entities are introduced and dis-
cussed.1 At the same time, it is also meant to
be a theory of salience: i.e., it attempts to pre-
dict which entities will be most salient at any
given time (which should be useful for a natural
language generator, since it is these entities that
are most typically pronominalized (Gundel et al,
1993)).
According to the theory, every UTTERANCE in
a spoken dialogue or written text introduces into
the discourse a number of FORWARD-LOOKING
CENTERS (CFs). CFs correspond more or less
1For a discussion of ?object-centered? vs. ?relation-
centered? notions of coherence, see (Stevenson et al, 2000).
to discourse entities in the sense of (Karttunen,
1976; Webber, 1978; Heim, 1982), and can be
linked to CFs introduced by previous or suc-
cessive utterances. Forward-looking centers are
RANKED, and because of this ranking, some CFs
acquire particular prominence. Among them, the
so-called BACKWARD-LOOKING CENTER (CB),
defined as follows:
Backward Looking Center (CB) CB(U
i+1
), the
BACKWARD-LOOKING CENTER of utter-
ance U
i+1
, is the highest ranked element of
CF(U
i
) that is realized in U
i+1
.
Utterance U
i+1
is classified as a CONTINUE if
CB(U
i+1
) = CB(U
i
) and CB(U
i+1
) is the most
highly ranked CF of U
i+1
; as a RETAIN if the CB
remains the same, but it?s not any longer the most
highly-ranked CF; and as a SHIFT if CB(U
i+1
) 6=
CB(U
i
).
The main claims of the theory are articulated in
terms of constraints and rules on CFs and CB.
Constraint 1: All utterances of a segment except
for the 1st have exactly one CB.
Rule 1: if any CF is pronominalized, the CB is.
Rule 2: (sequences of) continuations are pre-
ferred over (sequences of) retains, which are
preferred over (sequences of) shifts
Constraint 1 and Rule 2 express a preference for
utterances in a text to talk about the same ob-
jects; Rule 1 is the main claim of the theory about
pronominalization. In this paper we concentrate
on Constraint 1 and Rule 1.
One of the most unusual features of centering
theory is that the notions of utterance, previous
utterance, ranking, and realization used in the def-
initions above are left unspecified, to be appropri-
ately defined on the basis of empirical evidence,
and possibly in a different way for each language.
As a result, centering theory is best viewed as a
cluster of theories, each of which specifies the
parameters in a different ways: e.g., ranking has
been claimed to depend on grammatical function
(Kameyama, 1985; Brennan et al, 1987), on the-
matic roles (Cote, 1998), and on the discourse sta-
tus of the CFs (Strube and Hahn, 1999); there are
at least two definitions of what counts as ?previ-
ous utterance? (Kameyama, 1998; Suri and Mc-
Coy, 1994); and ?realization? can be interpreted
either in a strict sense, i.e., by taking a CF to be
realized in an utterance only if an NP in that utter-
ance denotes that CF, or in a looser sense, by also
counting a CF as ?realized? if it is referred to in-
directly by means of a bridging reference (Clark,
1977), i.e., an anaphoric expression that refers to
an object which wasn?t mentioned before but is
somehow related to an object that already has, as
in the vase . . . the handle (see, e.g., the discussion
in (Grosz et al, 1995; Walker et al, 1998b)).
3 METHODS
The fact that so many basic notions of centering
theory do not have a completely specified def-
inition makes empirical verification of the the-
ory rather difficult. Because any attempt at di-
rectly annotating a corpus for ?utterances? and
their CBs is bound to force the annotators to adopt
some specification of the basic notions of the the-
ory, previous studies have tended to study a par-
ticular variant of the theory (Di Eugenio, 1998;
Kameyama, 1998; Passonneau, 1993; Strube and
Hahn, 1999; Walker, 1989). A notable exception
is (Tetreault, 1999), which used an annotated cor-
pus to compare the performance of two variants
of centering theory.
The work discussed here, like Tetreault?s, is an
attempt at using corpora to compare different ver-
sions of centering theory, but considering also pa-
rameters of centering theory not studied in this
earlier work. In particular, we looked at different
ways of defining the notion of utterance, we stud-
ied the definition of realization, and more gener-
ally the role of semantic information. We did this
by annotating a corpus with information that has
been claimed by one or the other version of cen-
tering theory to play a role in the definitions of
its basic notions - e.g., the grammatical function
of an NP, anaphoric relations (including infor-
mation about bridging references) and how sen-
tences break up into clauses and subclausal units?
and then tried to find out the best way of specify-
ing these notions automatically, by trying out dif-
ferent configurations of parameters, and counting
the number of violations of the constraints and
rules that would result by adopting a particular
parameter configuration.
The Data
The aim of our project, which is called
GNOME and whose home page is at
http://www.hcrc.ed.ac.uk/ ~ gnome,
is to develop NP generation algorithms whose
generality is to be verified by incorporating
them in two distinct systems: the ILEX system
developed at the University of Edinburgh, that
generates Web pages describing museum objects
on the basis of the perceived status of its user?s
knowledge and of the objects she previously
looked at (Oberlander et al, 1998); and the
ICONOCLAST system, developed at the Univer-
sity of Brighton, that supports the creation of
patient information leaflets (Scott et al, 1998).
The corpus we collected includes texts from
both the domains we are studying. The texts
in the museum domain consist of descriptions
of museum objects and brief texts about the
artists that produced them; the texts in the
pharmaceutical domain are leaflets providing the
patients with the legally mandatory information
about their medicine. The total size of the corpus
is of about 6,000 NPs. For this study we used
about half of each subset, for a total number of
about 3,000 NPs, of which 103 are third person
pronouns (72 in the museum domain, 31 in the
pharmaceutical domain) and 61 are third-person
possessive pronouns (58 in the museum domain,
3 in the pharmaceutical domain).
Annotation
Previous empirical studies of centering theory
typically involved a single annotator annotat-
ing her corpus according to her own subjective
judgment (Passonneau, 1993; Kameyama, 1998;
Strube and Hahn, 1999). One of our goals was
to use for our study only information that could
be annotated reliably (Passonneau and Litman,
1993; Carletta, 1996), as we believe this will
make our results easier to replicate. The price
we paid to achieve replicability is that we could-
n?t test all hypotheses proposed in the literature,
especially about segmentation and about ranking.
We discuss some of the problems in what follows.
(The latest version of the annotation manual is
available from the GNOME project?s home page.)
We used eight annotators for the reliability study
and the annotation.
Utterances Kameyama (1998) noted that iden-
tifying utterances with sentences is problematic
in the case of multiclausal sentences: e.g., gram-
matical function ranking becomes difficult to
measure, as there may be more than one sub-
ject. She proposed to use all and only tensed
clauses instead of sentences as utterance units,
and then classified finite clauses into (i) utter-
ance units that constitute a ?permanent? update
of the local focus: these include coordinated
clauses and adjuncts) and (ii) utterance units that
result in updates that are then erased, much as
in the way the information provided by subor-
dinated discourse segments is erased when they
are popped. Kameyama called these EMBED-
DED utterance units, and proposed that clauses
that serve as verbal complements behave this way.
Suri and McCoy (1994) did a study that led them
to propose that some types of adjuncts?in particu-
lar, clauses headed by after and before?should be
treated as ?embedded? rather than as ?permanent
updates? as suggested by Kameyama; these re-
sults were subsequently confirmed by more con-
trolled experiments Pearson et al (2000). Nei-
ther Kameyama nor Suri and McCoy discuss par-
entheticals; Kameyama only briefly mentions rel-
ative clauses, but doesn?t analyze them in detail.
In order to evaluate these definitions of ut-
terance (sentences versus finite clauses), as well
as the different ways of defining ?previous utter-
ance?, we marked up in our corpus what we called
(DISCOURSE) UNITS. These include clauses, as
well as other sentence subconstituents which may
be treated as separate utterances, including paren-
theticals, preposed PPs, and (the second element
of) coordinated VPs. The instructions for mark-
ing up units were in part derived from (Marcu,
1999); for each unit, the following attributes were
marked:
 utype: whether the unit is a main clause,
a relative clause, appositive, a parenthetical,
etc.
 verbed: whether the unit contains a verb or
not.
 finite: for verbed units, whether the verb is
finite or not.
 subject: for verbed units, whether they have
a full subject, an empty subject (expletive,
as in there sentences), or no subject (e.g., for
infinitival clauses).
The agreement on identifying the boundaries of
units, using the  statistic discussed in (Carletta,
1996), was  = :9 (for two annotators and 500
units); the agreement on features(2 annotators
and at least 200 units) was follows:
Attribute  Value
utype .76
verbed .9
finite .81
subject .86
NPs Our instructions for identifying NP mark-
ables derive from those proposed in the MATE
project scheme for annotating anaphoric relations
(Poesio et al, 1999). We annotated attributes of
NPs which could be used to define their ranking,
including:
 The NP type, cat (pronoun, proper name,
etc.)
 A few other ?basic? syntactic features, num,
per, and gen, that could be used to identify
contexts in which the antecedent of a pro-
noun could be identified unambiguously;
 The grammatical function, gf;
 ani: whether the object denoted is animate
or inanimate
 deix: whether the object is a deictic refer-
ence or not
The agreement values for these attributes are as
follows:
Attribute  Value
ani .81
cat .9
deix .81
gen .89
gf .85
num .84
per .9
one of the features of NPs claimed to affect rank-
ing (Sidner, 1979; Cote, 1998) that we haven?t
so far been able to annotate because of failure
to reach acceptable agreement is thematic roles
( = :35).
Anaphoric information Finally, in order to
compute whether a CF from an utterance was re-
alized directly or indirectly in the following ut-
terance, we marked up anaphoric relations be-
tween NPs, again using a variant of the MATE
scheme. Theories of focusing such as (Sidner,
1979; Strube and Hahn, 1999), as well as our own
early experiments with centering, suggested that
indirect realization can play quite a crucial role in
maintaining the CB; however, previous work, par-
ticularly in the context of the MUC initiative, sug-
gested that while it?s fairly easy to achieve agree-
ment on identity relations, marking up bridging
references is quite hard; this was confirmed by,
e.g., Poesio and Vieira (1998). As a result we did
annotate this type of relations, but to achieve a
reasonable agreement, and to contain somehow
the annotators? work, we limited the types of re-
lations annotators were supposed to mark up, and
we specified priorities. Thus, besides identity
(IDENT) we only marked up three non-identity
(?bridging? (Clark, 1977)) relations, and only re-
lations between objects. The relations we mark
up are a subset of those proposed in the ?extended
relations? version of the MATE scheme (Poesio et
al., 1999) and include set membership (ELE-
MENT), subset (SUBSET), and ?generalized pos-
session? (POSS), which includes part-of relations
as well as more traditional ownership relations.
As expected, we achieved a rather good agree-
ment on identity relations. In our most recent
analysis (two annotators looking at the anaphoric
relations between 200 NPs) we observed no real
disagreements; 79.4% of these relations were
marked up by both annotators; 12.8% by only
one of them; and in 7.7% of the cases, one of
the annotators marked up a closer antecedent than
the other. Concerning bridges, limiting the re-
lations did limit the disagreements among an-
notators (only 4.8% of the relations are actually
marked differently) but only 22% of bridging ref-
erences were marked in the same way by both an-
notators; 73.17% of relations are marked by only
one or the other annotator. So reaching agreement
on this information involved several discussions
between annotators and more than one pass over
the corpus.
Segmentation Segmenting text in a reliable
fashion is still an open problem, and in addition
the relation between centering (i.e., local focus
shifts) and segmentation (i.e., global focus shifts)
is still not clear: some see them as independent
aspects of attentional structure, whereas other re-
searchers define centering transitions with respect
to segments (see, e.g., the discussion in the intro-
duction to (Walker et al, 1998b)). Our prelim-
inary experiments at annotating discourse struc-
ture didn?t give good results, either. Therefore,
we only used the layout structure of the texts
as a rough indication of discourse structure. In
the museum domain, each object description was
treated as a separate segment; in the pharmaceu-
tical domain, each subsection of a leaflet was
treated as a separate segment. We then identified
by hand those violations of Constraint 1 that ap-
peared to be motivated by too broad a segmenta-
tion of the text.2
Automatic computation of centering
information
The annotation thus produced was used to au-
tomatically compute utterances according to the
particular configuration of parameters chosen,
and then to compute the CFs and the CB (if any)
of each utterance on the basis of the anaphoric
information and according to the notion of rank-
ing specified. This information was the used to
find violations of Constraint 1 and Rule 1. The
behavior of the script that computes this informa-
tion depends on the following parameters:
utterance: whether sentences, finite clauses, or
verbed clauses should be treated as utter-
ances.
previous utterance: whether adjunct clauses
should be treated Kameyama-style or
Suri-style.
rank: whether CFs should be ranked according
to grammatical function or discourse status
in Strube and Hahn?s sense
2(Cristea et al, 2000) showed that it is indeed possible
to achieve good agreement on discourse segmentation, but
that it requires intensive training and repeated iterations; we
intend to take advantage of a corpus already annotated in this
way in future work.
realization: whether only direct realization
should be counted, or also indirect realiza-
tion via bridging references.
4 MAIN RESULTS
The principle we used to evaluate the different
configurations of the theory was that the best def-
inition of the parameters was the one that would
lead to the fewest violations of Constraint 1 and
Rule 1. We discuss the results for each principle.
Constraint 1: All utterances of a segment
except for the 1st have precisely one CB
Our first set of figures concerns Constraint 1:
how many utterances have a CB. This con-
straint can be used to evaluate how well cen-
tering theory predicts coherence, in the follow-
ing sense: assuming that all our texts are co-
herent, if centering were the only factor behind
coherence, all utterances should verify this con-
straint. The first table shows the results obtained
by choosing the configuration that comes clos-
est to the one suggested by Kameyama (1998):
utterance=finite, prev=kameyama, rank=gf, real-
ization=direct. The first column lists the number
of utterances that satisfy Constraint 1; the second
those that do not satisfy it, but are segment-initial;
the third those that do not satisfy it and are not
segment-initial.
CB Segment Initial NO CB Total Number
Museum 132 35 245 412
Pharmacy 158 13 198 369
Total 290 48 443 791
The previous table shows that with this config-
uration of parameters, most utterances do not sat-
isfy Constraint 1 in the strict sense even if we take
into account text segmentation (admittedly, a very
rough one). If we take sentences as utterances,
instead of finite clauses, we get fewer violations,
although about 25% of the total number of utter-
ances are violations:
CB Segment Initial NO CB Total Number
Museum 120 22 85 227
Pharmacy 152 8 51 211
Total 272 30 136 438
Using Suri and McCoy?s definition of previous
utterance, instead of Kameyama?s (i.e., treating
adjuncts as embedded utterances) leads to a slight
improvement over Kameyama?s proposal but still
not as good as using sentences:
CB Segment Initial NO CB Total Number
Museum 140 35 237 412
Pharmacy 167 14 188 369
Total 307 49 425 791
What about the finite clause types not consid-
ered by Kameyama or Suri and McCoy? It turns
out that we get better results if we do not treat as
utterances relative clauses (which anyway always
have a CB, under standard syntactic assumptions
about the presence of traces referring to the modi-
fied noun phrase), parentheticals, clauses that oc-
cur in subject position; and if we treat as a single
utterance matrix clauses with empty subjects and
their complements (as in it is possible that John
will arrive tomorrow).
CB Segment Initial NO CB Total Number
Museum 143 35 153 331
Pharmacy 161 14 159 334
Total 304 49 312 665
But by far the most significant improvement to the
percentage of utterances that satisfy Constraint 1
comes by adopting a looser definition of ?real-
izes?, i.e., by allowing a discourse entity to serve
as CB of an utterance even if it?s only referred to
indirectly in that utterance by means of a bridg-
ing reference, as originally proposed by Sidner
(1979) for her discourse focus. The following se-
quence of utterances explains why this could lead
to fewer violations of Constraint 1:
(1) (u1) These ?egg vases? are of exceptional
quality: (u2) basketwork bases support
egg-shaped bodies (u3) and bundles of straw
form the handles, (u4) while small eggs resting
in straw nests serve as the finial for each lid. (u5)
Each vase is decorated with inlaid decoration:
. . .
In (1), u1 is followed by four utterances. Only
the last of these directly refers to the set of egg
vases introduced in u1, while they all contain im-
plicit references to these objects. If we adopt this
looser notion of realization, the figures improve
dramatically, even with the rather restricted set of
relations on which our annotators agree. Now the
majority of utterances satisfy Constraint 1:
CB Segment Initial NO CB Total Number
Museum 225 35 71 331
Pharmacy 174 14 146 334
Total 399 49 217 665
And of course we get even better results by treat-
ing sentences as utterances:
CB Segment Initial NO CB Total Number
Museum 171 17 39 227
Pharmacy 168 7 36 211
Total 339 24 75 438
It is important, however, to notice that even un-
der the best configuration, at least 17% of utter-
ances violate the constraint. The (possibly, obvi-
ous) explanation is that although coherence is of-
ten achieved by means of links between objects,
this is not the only way to make texts coherent.
So, in the museum domain, we find utterances
that do not refer to any of the previous CFs be-
cause they express generic statements about the
class of objects of which the object under discus-
sion is an instance, or viceversa utterances that
make a generic point that will then be illustrated
by a specific object. In the following example,
the second utterance gives some background con-
cerning the decoration of a particular object.
(2) (u1) On the drawer above the door, gilt-bronze
military trophies flank a medallion portrait of
Louis XIV. (u2) In the Dutch Wars of 1672 -
1678, France fought simultaneously against the
Dutch, Spanish, and Imperial armies, defeating
them all. (u3) This cabinet celebrates the Treaty
of Nijmegen, which concluded the war.
Coherence can also be achieved by explicit
coherence relations, such as EXEMPLIFICA-
TION in the following example:
(3) (u1) Jewelry is often worn to signal membership
of a particular social group. (u2) The Beatles
brooch shown previously is another case in point:
Rule 1: if any NP is pronominalized, the CB is
In the previous section we saw that allowing
bridging references to maintain the CB leads to
fewer violations of Constraint 1. One should
not, however, immediately conclude that it would
be a good idea to replace the strict definition
of ?realizes? with a looser one, because there
is, unfortunately, a side effect: adopting an in-
direct notion of realizes leads to more viola-
tions of Rule 1. Figures are as follows. Us-
ing utterance=s, rank=gf, realizes=direct 22 pro-
nouns violating Rule 1 (9 museum, 13 pharmacy)
(13.4%), whereas with realizes=indirect we have
38 violations (25, 13) (23%); if we choose utter-
ance=finite, prev=suri, we have 23 violations of
rule 1 with realizes=direct (13 + 10) (14%), 32
with realizes=indirect (21 + 11) (19.5%). Using
functional centering (Strube and Hahn, 1999) to
rank the CFs led to no improvements, because of
the almost perfect correlation in our domain be-
tween subjecthood and being discourse-old. One
reason for these problems is illustrated by (4).
(4) (u1) A great refinement among armorial signets
was to reproduce not only the coat-of-arms but
the correct tinctures; (u2) they were repeated in
colour on the reverse side (u3) and the crystal
would then be set in the gold bezel.
They in u2 refers back to the correct tinctures (or,
possibly, the coat-of-arms), which however only
occurs in object position in a (non-finite) com-
plement clause in (u1), and therefore has lower
ranking than armorial signets, which is realized
in (u2) by the bridge the reverse side and there-
fore becomes the CB having higher rank in (u1),
but is not pronominalized.
In the pharmaceutical leaflets we found a num-
ber of violations of Rule 1 towards the end of
texts, when the product is referred to. A possi-
ble explanation is that after the product has been
mentioned sentence after sentence in the text, by
the end of the text it is salient enough that there
is no need to put it again in the local focus by
mentioning it explicitly. E.g., it in the following
example refers to the cream, not mentioned in any
of the previous two utterances.
(5) (u1) A child of 4 years needs about a third of
the adult amount. (u2) A course of treatment for
a child should not normally last more than five
days (u3) unless your doctor has told you to use
it for longer.
5 DISCUSSION
Our main result is that there seems to be a trade-
off between Constraint 1 and Rule 1. Allowing
for a definition of ?realizes? that makes the CB be-
have more like Sidner?s Discourse Focus (Sidner,
1979) leads to a very significant reduction in the
number of violations of Constraint 1.3 We also
noted, however, that interpreting ?realizes? in this
way results in more violations of Rule 1. (No
differences were found when functional center-
ing was used to rank CFs instead of grammati-
3Footnote 2, page 3 of the intro to (Walker et al, 1998b)
suggests a weaker interpretation for the Constraint: ?there is
no more than one CB for utterance?. This weaker form of
the Constraint does hold for most utterances, but it?s almost
vacuous, especially for grammatical function ranking, given
that utterances have at most one subject.
cal function.) The problem raised by these re-
sults is that whereas centering is intended as an
account of both coherence and local salience, dif-
ferent concepts may have to be used in Constraint
1 and Rule 1, as in Sidner?s theory. E.g., we might
have a ?Center of Coherence?, analogous to Sid-
ner?s discourse focus, and that can be realized in-
directly; and a ?Center of Salience?, similar to her
actor focus, and that can only be realized directly.
Constraint 1 would be about the Center of Coher-
ence, whereas Rule 1 would be about the Center
of Salience. Indeed, many versions of centering
theory have elevated the CP to the rank of a sec-
ond center.4
We also saw that texts can be coherent even
when Constraint 1 is violated, as coherence can
be ensured by other means (e.g., by rhetorical re-
lations). This, again, suggests possible revisions
to Constraint 1, requiring every utterance either
to have a center of coherence, or to be linked by a
rhetorical relation to the previous utterance.
Finally, we saw that we get fewer violations of
Constraint 1 by adopting sentences as our notion
of utterance; however, again, this results in more
violations of Rule 1. If finite clauses are used as
utterances, we found that certain types of finite
clauses not previously discussed, including rela-
tive clauses and matrix clauses with empty sub-
jects, are best not treated as utterances. We didn?t
find significant differences between Kameyama
and Suri and McCoy?s definition of ?previous ut-
terance?. We believe however more work is still
needed to identify a completely satisfactory way
of breaking up sentences in utterance units.
ACKNOWLEDGMENTS
We wish to thank Kees van Deemter, Barbara di
Eugenio, Nikiforos Karamanis and Donia Scott
for comments and suggestions. Massimo Poesio
is supported by an EPSRC Advanced Fellowship.
Hua Cheng, Renate Henschel and Rodger Kib-
ble were in part supported by the EPSRC project
GNOME, GR/L51126/01. Janet Hitzeman was in
part supported by the EPSRC project SOLE.
4This separation among a ?center of coherence? and a
?center of salience? is independently motivated by consid-
erations about the division of labor between the text planner
and the sentence planner in a generation system; see, e.g.,
(Kibble, 1999).
References
S.E. Brennan, M.W. Friedman, and C.J. Pollard. 1987.
A centering approach to pronouns. In Proc. of the
25th ACL, pages 155?162, June.
J. Carletta. 1996. Assessing agreement on classifica-
tion tasks: the kappa statistic. Computational Lin-
guistics, 22(2):249?254.
H. H. Clark. 1977. Inferences in comprehension. In
D. Laberge and S. J. Samuels, editors, Basic Pro-
cess in Reading: Perception and Comprehension.
Lawrence Erlbaum.
S. Cote. 1998. Ranking forward-looking centers. In
M. A. Walker, A. K. Joshi, and E. F. Prince, editors,
Centering Theory in Discourse, chapter 4, pages
55?70. Oxford.
D. Cristea, N. Ide, D. Marcu, and V. Tablan. 2000.
Discourse structure and co-reference: An empirical
study. In Proc. of COLING.
B. Di Eugenio. 1998. Centering in italian. In M. A.
Walker, A. K. Joshi, and E. F. Prince, editors, Cen-
tering Theory in Discourse, chapter 7, pages 115?
138. Oxford.
B. J. Grosz, A. K. Joshi, and S. Weinstein. 1995.
Centering: A framework for modeling the local co-
herence of discourse. Computational Linguistics,
21(2):202?225.
J. K. Gundel, N. Hedberg, and R. Zacharski. 1993.
Cognitive status and the form of referring expres-
sions in discourse. Language, 69(2):274?307.
I. Heim. 1982. The Semantics of Definite and In-
definite Noun Phrases. Ph.D. thesis, University of
Massachusetts at Amherst.
M. Kameyama. 1985. Zero Anaphora: The case of
Japanese. Ph.D. thesis, Stanford University.
M. Kameyama. 1998. Intra-sentential centering: A
case study. In M. A. Walker, A. K. Joshi, and
E. F. Prince, editors, Centering Theory in Dis-
course, chapter 6, pages 89?112. Oxford.
L. Karttunen. 1976. Discourse referents. In J. Mc-
Cawley, editor, Syntax and Semantics 7 - Notes from
the Linguistic Underground. Academic Press.
R. Kibble. 1999. Cb or not Cb? centering applied to
NLG. In Proc. of the ACL Workshop on discourse
and reference.
D. Marcu. 1999. Instructions for manually annotat-
ing the discourse structures of texts. Unpublished
manuscript, USC/ISI, May.
J. Oberlander, M. O?Donnell, A. Knott, and C. Mel-
lish. 1998. Conversation in the museum: Exper-
iments in dynamic hypermedia with the intelligent
labelling explorer. New Review of Hypermedia and
Multimedia, 4:11?32.
R. Passonneau and D. Litman. 1993. Feasibility of
automated discourse segmentation. In Proceedings
of 31st Annual Meeting of the ACL.
R. J. Passonneau. 1993. Getting and keeping the cen-
ter of attention. In M. Bates and R. M. Weischedel,
editors, Challenges in Natural Language Process-
ing, chapter 7, pages 179?227. Cambridge.
J. Pearson, R. Stevenson, and M. Poesio. 2000. Pro-
noun resolution in complex sentences. In Proc. of
AMLAP, Leiden.
M. Poesio and R. Vieira. 1998. A corpus-based inves-
tigation of definite description use. Computational
Linguistics, 24(2):183?216, June.
M. Poesio, F. Bruneseaux, and L. Romary. 1999. The
MATE meta-scheme for coreference in dialogues in
multiple languages. In M. Walker, editor, Proc. of
the ACL Workshop on Standards and Tools for Dis-
course Tagging, pages 65?74.
D. Scott, R. Power, and R. Evans. 1998. Generation
as a solution to its own problem. In Proc. of the
9th International Workshop on Natural Language
Generation, Niagara-on-the-Lake, CA.
C. L. Sidner. 1979. Towards a computational theory
of definite anaphora comprehension in English dis-
course. Ph.D. thesis, MIT.
R. Stevenson, A. Knott, J. Oberlander, and S McDon-
ald. 2000. Interpreting pronouns and connectives.
Language and Cognitive Processes, 15.
M. Strube and U. Hahn. 1999. Functional centering?
grounding referential coherence in information
structure. Computational Linguistics, 25(3):309?
344.
L. Z. Suri and K. F. McCoy. 1994. RAFT/RAPR
and centering: A comparison and discussion of
problems related to processing complex sentences.
Computational Linguistics, 20(2):301?317.
J. R. Tetreault. 1999. Analysis of syntax-based pro-
noun resolution methods. In Proc. of the 37th ACL,
pages 602?605, University of Marylan, June. ACL.
M. A. Walker, A. K. Joshi, and E. F. Prince, editors.
1998b. Centering Theory in Discourse. Oxford.
M. A. Walker. 1989. Evaluating discourse process-
ing algorithms. In Proc. ACL-89, pages 251?261,
Vancouver, CA, June.
B. L. Webber. 1978. A formal approach to discourse
anaphora. Report 3761, BBN, Cambridge, MA.
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 1?8, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Improving LSA-based Summarization with Anaphora Resolution
Josef Steinberger
University of West Bohemia
Univerzitni 22, Pilsen 30614,
Czech Republic
jstein@kiv.zcu.cz
Mijail A. Kabadjov
University of Essex
Wivenhoe Park, Colchester CO4 3SQ,
United Kingdom
malexa@essex.ac.uk
Massimo Poesio
University of Essex
Wivenhoe Park, Colchester CO4 3SQ,
United Kingdom
poesio@essex.ac.uk
Olivia Sanchez-Graillet
University of Essex
Wivenhoe Park, Colchester CO4 3SQ,
United Kingdom
osanch@essex.ac.uk
Abstract
We propose an approach to summarization
exploiting both lexical information and
the output of an automatic anaphoric re-
solver, and using Singular Value Decom-
position (SVD) to identify the main terms.
We demonstrate that adding anaphoric
information results in significant perfor-
mance improvements over a previously
developed system, in which only lexical
terms are used as the input to SVD. How-
ever, we also show that how anaphoric in-
formation is used is crucial: whereas using
this information to add new terms does re-
sult in improved performance, simple sub-
stitution makes the performance worse.
1 Introduction
Many approaches to summarization can be very
broadly characterized as TERM-BASED: they at-
tempt to identify the main ?topics,? which gen-
erally are TERMS, and then to extract from the
document the most important information about
these terms (Hovy and Lin, 1997). These ap-
proaches can be divided again very broadly in ?lex-
ical? approaches, among which we would include
LSA-based approaches, and ?coreference-based? ap-
proaches . Lexical approaches to term-based sum-
marization use lexical relations to identify cen-
tral terms (Barzilay and Elhadad, 1997; Gong and
Liu, 2002); coreference- (or anaphora-) based ap-
proaches (Baldwin and Morton, 1998; Boguraev and
Kennedy, 1999; Azzam et al, 1999; Bergler et al,
2003; Stuckardt, 2003) identify these terms by run-
ning a coreference- or anaphoric resolver over the
text.1 We are not aware, however, of any attempt to
use both lexical and anaphoric information to iden-
tify the main terms. In addition, to our knowledge no
authors have convincingly demonstrated that feed-
ing anaphoric information to a summarizer signif-
icantly improves the performance of a summarizer
using a standard evaluation procedure (a reference
corpus and baseline, and widely accepted evaluation
measures).
In this paper we compare two sentence extraction-
based summarizers. Both use Latent Semantic
Analysis (LSA) (Landauer, 1997) to identify the
main terms of a text for summarization; however,
the first system (Steinberger and Jezek, 2004), dis-
cussed in Section 2, only uses lexical information
to identify the main topics, whereas the second sys-
tem exploits both lexical and anaphoric information.
This second system uses an existing anaphora reso-
lution system to resolve anaphoric expressions, GUI-
TAR (Poesio and Kabadjov, 2004); but, crucially,
two different ways of using this information for
summarization were tested. (Section 3.) Both sum-
marizers were tested over the CAST corpus (Orasan
et al, 2003), as discussed in Section 4, and sig-
1The terms ?anaphora resolution? and ?coreference resolu-
tion? have been variously defined (Stuckardt, 2003), but the lat-
ter term is generally used to refer to the coreference task as de-
fined in MUC and ACE. We use the term ?anaphora resolution? to
refer to the task of identifying successive mentions of the same
discourse entity, realized via any type of noun phrase (proper
noun, definite description, or pronoun), and whether such dis-
course entities ?refer? to objects in the world or not.
1
nificant improvements were observed over both the
baseline CAST system and our previous LSA-based
summarizer.
2 An LSA-based Summarizer Using
Lexical Information Only
LSA (Landauer, 1997) is a technique for extracting
the ?hidden? dimensions of the semantic representa-
tion of terms, sentences, or documents, on the basis
of their contextual use. It is a very powerful tech-
nique already used for NLP applications such as in-
formation retrieval (Berry et al, 1995) and text seg-
mentation (Choi et al, 2001) and, more recently,
multi- and single-document summarization.
The approach to using LSA in text summariza-
tion we followed in this paper was proposed in
(Gong and Liu, 2002). Gong and Liu propose to
start by creating a term by sentences matrix A =
[A1, A2, . . . , An], where each column vector Ai rep-
resents the weighted term-frequency vector of sen-
tence i in the document under consideration. If there
are a total of m terms and n sentences in the docu-
ment, then we will have an m ? n matrix A for the
document. The next step is to apply Singular Value
Decomposition (SVD) to matrix A. Given an m? n
matrix A, the SVD of A is defined as:
(1) A = U?V T
where U = [uij ] is an m ? n column-orthonormal
matrix whose columns are called left singular vec-
tors, ? = diag(?1, ?2, . . . , ?n) is an n ? n di-
agonal matrix, whose diagonal elements are non-
negative singular values sorted in descending order,
and V = [vij ] is an n?n orthonormal matrix, whose
columns are called right singular vectors.
From a mathematical point of view, applying
SVD to a matrix derives a mapping between the m-
dimensional space spawned by the weighted term-
frequency vectors and the r-dimensional singular
vector space. From a NLP perspective, what the SVD
does is to derive the latent semantic structure of the
document represented by matrix A: a breakdown
of the original document into r linearly-independent
base vectors (?topics?). Each term and sentence from
the document is jointly indexed by these ?topics?.
A unique SVD feature is that it is capable of cap-
turing and modelling interrelationships among terms
so that it can semantically cluster terms and sen-
tences. Furthermore, as demonstrated in (Berry et
al., 1995), if a word combination pattern is salient
and recurring in document, this pattern will be cap-
tured and represented by one of the singular vec-
tors. The magnitude of the corresponding singular
value indicates the importance degree of this pattern
within the document. Any sentences containing this
word combination pattern will be projected along
this singular vector, and the sentence that best repre-
sents this pattern will have the largest index value
with this vector. As each particular word combi-
nation pattern describes a certain topic in the doc-
ument, each singular vector can be viewed as repre-
senting a salient topic of the document, and the mag-
nitude of its corresponding singular value represents
the degree of importance of the salient topic.
The summarization method proposed by Gong
and Liu (2002) should now be easy to understand.
The matrix V T describes the importance degree of
each ?implicit topic? in each sentence: the summa-
rization process simply chooses the most informa-
tive sentence for each term. In other words, the kth
sentence chosen is the one with the largest index
value in the kth right singular vector in matrix V T .
The summarization method proposed by Gong
and Liu has some disadvantages as well, the main of
which is that it is necessary to use the same number
of dimensions as is the number of sentences we want
to choose for a summary. However, the higher the
number of dimensions of reduced space is, the less
significant topic we take into a summary. In order
to remedy this problem, we (Steinberger and Jezek,
2004) proposed the following modifications to Gong
and Liu?s summarization method. After computing
the SVD of a term by sentences matrix, we compute
the length of each sentence vector in matrix V . This
is to favour the index values in the matrix V that
correspond to the highest singular values (the most
significant topics). Formally:
(2) sk =
?
?r
i=1 v2k,i ? ?2i ,
where sk is the length of the vector of k?th sentence
in the modified latent vector space, and its signif-
icance score for summarization too. The level of
dimensionality reduction (r) is essentially learned
from the data. Finally, we put into the summary the
sentences with the highest values in vector s. We
showed in previous work (Steinberger and Jezek,
2
2004) that this modification results in a significant
improvement over Gong and Liu?s method.
3 Using Anaphora Resolution for
Summarization
3.1 The case for anaphora resolution
Words are the most basic type of ?term? that can
be used to characterize the content of a document.
However, being able to identify the most important
objects mentioned in the document clearly would
lead to an improved analysis of what is important in
a text, as shown by the following news article cited
by Boguraev and Kennedy (1999):
(3) PRIEST IS CHARGED WITH POPE ATTACK
A Spanish priest was charged here today with attempt-
ing to murder the Pope. Juan Fernandez Krohn, aged
32, was arrested after a man armed with a bayonet ap-
proached the Pope while he was saying prayers at Fa-
tima on Wednesday night. According to the police, Fer-
nandez told the investigators today that he trained for
the past six months for the assault. . . . If found guilty,
the Spaniard faces a prison sentence of 15-20 years.
As Boguraev and Kennedy point out, the title of the
article is an excellent summary of the content: an en-
tity (the priest) did something to another entity (the
pope). Intuitively, understanding that Fernandez and
the pope are the central characters is crucial to pro-
vide a good summary of texts like these.2 Among
the clues that help us to identify such ?main charac-
ters?, the fact that an entity is repeatedly mentioned
is clearly important.
Purely lexical methods, including the LSA-based
methods discussed in the previous section, can only
capture part of the information about which enti-
ties are frequently repeated in the text. As exam-
ple (3) shows, stylistic conventions forbid verbatim
repetition, hence the six mentions of Fernandez in
the text above contain only one lexical repetition,
?Fernandez?. The main problem are pronouns, that
tend to share the least lexical similarity with the
form used to express the antecedent (and anyway are
usually removed by stopword lists, therefore do not
2It should be noted that for many newspaper articles, indeed
many non-educational texts, only a ?entity-centered? structure
can be clearly identified, as opposed to a ?relation-centered?
structure of the type hypothesized in Rhetorical Structures The-
ory (Knott et al, 2001; Poesio et al, 2004).
get included in the SVD matrix). The form of defi-
nite descriptions (the Spaniard) doesn?t always over-
lap with that of their antecedent, either, especially
when the antecedent was expressed with a proper
name. The form of mention which more often over-
laps to a degree with previous mentions is proper
nouns, and even then at least some way of dealing
with acronyms is necessary (cfr. European Union
/ E.U.). The motivation for anaphora resolution is
that it should tell us which entities are repeatedly
mentioned.
In this work, we tested a mixed approach to in-
tegrate anaphoric and word information: using the
output of the anaphoric resolver GUITAR to modify
the SVD matrix used to determine the sentences to
extract. In the rest of this section we first briefly in-
troduce GUITAR, then discuss the two methods we
tested to use its output to help summarization.
3.2 GUITAR: A General-Purpose Anaphoric
Resolver
The system we used in these experiments, GUITAR
(Poesio and Kabadjov, 2004), is an anaphora resolu-
tion system designed to be high precision, modular,
and usable as an off-the-shelf component of a NL
processing pipeline. The current version of the sys-
tem includes an implementation of the MARS pro-
noun resolution algorithm (Mitkov, 1998) and a par-
tial implementation of the algorithm for resolving
definite descriptions proposed by Vieira and Poe-
sio (2000). The current version of GUITAR does not
include methods for resolving proper nouns.
3.2.1 Pronoun Resolution
Mitkov (1998) developed a robust approach to
pronoun resolution which only requires input text
to be part-of-speech tagged and noun phrases to be
identified. Mitkov?s algorithm operates on the ba-
sis of antecedent-tracking preferences (referred to
hereafter as ?antecedent indicators?). The approach
works as follows: the system identifies the noun
phrases which precede the anaphor within a distance
of 2 sentences, checks them for gender and number
agreement with the anaphor, and then applies genre-
specific antecedent indicators to the remaining can-
didates (Mitkov, 1998). The noun phrase with the
highest aggregate score is proposed as antecedent.
3
3.2.2 Definite Description Resolution
The Vieira / Poesio algorithm (Vieira and Poesio,
2000) attempts to classify each definite description
as either direct anaphora, discourse-new, or bridg-
ing description. The first class includes definite de-
scriptions whose head is identical to that of their an-
tecedent, as in a house . . . the house. Discourse-
new descriptions are definite descriptions that refer
to objects not already mentioned in the text and not
related to any such object. Bridging descriptions are
all definite descriptions whose resolution depends
on knowledge of relations between objects, such as
definite descriptions that refer to an object related
to an entity already introduced in the discourse by
a relation other than identity, as in the flat . . . the
living room. The Vieira / Poesio algorithm also at-
tempts to identify the antecedents of anaphoric de-
scriptions and the anchors of bridging ones. The
current version of GUITAR incorporates an algorithm
for resolving direct anaphora derived quite directly
from Vieira / Poesio, as well as a statistical version
of the methods for detecting discourse new descrip-
tions (Poesio et al, 2005).
3.3 SVD over Lexical and Anaphoric Terms
SVD can be used to identify the ?implicit topics? or
main terms of a document not only when on the basis
of words, but also of coreference chains, or a mix-
ture of both. We tested two ways of combining these
two types of information.
3.3.1 The Substitution Method
The simplest way of integrating anaphoric in-
formation with the methods used in our earlier
work is to use anaphora resolution simply as a pre-
processing stage of the SVD input matrix creation.
Firstly, all anaphoric relations are identified by the
anaphoric resolver, and anaphoric chains are identi-
fied. Then a second document is produced, in which
all anaphoric nominal expressions are replaced by
the first element of their anaphoric chain. For exam-
ple, suppose we have the text in (4).
(4) S1: Australia?s new conservative government on
Wednesday began selling its tough deficit-slashing bud-
get, which sparked violent protests by Aborigines,
unions, students and welfare groups even before it was
announced.
S2: Two days of anti-budget street protests preceded
spending cuts officially unveiled by Treasurer Peter
Costello.
S3: ?If we don?t do it now, Australia is going to be in
deficit and debt into the next century.?
S4: As the protesters had feared, Costello revealed a
cut to the government?s Aboriginal welfare commission
among the hundreds of measures implemented to claw
back the deficit.
An ideal resolver would find 8 anaphoric chains:
Chain 1 Australia - we - Australia
Chain 2 its new conservative government (Australia?s new
conservative government) - the government
Chain 3 its tough deficit-slashing budget (Australia?s tough
deficit-slashing budget) - it
Chain 4 violent protests by Aborigines, unions, students and
welfare groups - anti-budget street protests
Chain 5 Aborigines, unions, students and welfare groups - the
protesters
Chain 6 spending cuts - it - the hundreds of measures imple-
mented to claw back the deficit
Chain 7 Treasurer Peter Costello - Costello
Chain 8 deficit - the deficit
By replacing each element of the 8 chains above
in the text in (4) with the first element of the chain,
we get the text in (5).
(5) S1: Australia?s new conservative government on
Wednesday began selling Australia?s tough deficit-
slashing budget, which sparked violent protests by Abo-
rigines, unions, students and welfare groups even be-
fore Australia?s tough deficit-slashing budget was an-
nounced.
S2: Two days of violent protests by Aborigines, unions,
students and welfare groups preceded spending cuts of-
ficially unveiled by Treasurer Peter Costello.
S3: ?If Australia doesn?t do spending cuts now, Aus-
tralia is going to be in deficit and debt into the next
century.?
S4: As Aborigines, unions, students and welfare
groups had feared, Treasurer Peter Costello revealed a
cut to Australia?s new conservative government?s Abo-
riginal welfare commission among the spending cuts.
This text is then used to create the SVD input matrix,
as done in the first system.
3.3.2 The Addition Method
An alternative approach is to use SVD to identify
?topics? on the basis of two types of ?terms?: terms in
the lexical sense (i.e., words) and terms in the sense
of objects, which can be represented by anaphoric
4
chains. In other words, our representation of sen-
tences would specify not only if they contain a cer-
tain word, but also if they contain a mention of a
discourse entity (See Figure 1.) This matrix would
then be used as input to SVD.
Figure 1: Addition method.
The chain ?terms? tie together sentences that con-
tain the same anaphoric chain. If the terms are
lexically the same (direct anaphors - like deficit
and the deficit) the basic summarizer works suffi-
ciently. However, Gong and Liu showed that the best
weighting scheme is boolean (i.e., all terms have the
same weight); our own previous results confirmed
this. The advantage of the addition method is the
opportunity to give higher weights to anaphors.
4 Evaluation
4.1 The CAST Corpus
To evaluate our system, we used the corpus of
manually produced summaries created by the CAST
project3 (Orasan et al, 2003). The CAST cor-
pus contains news articles taken from the Reuters
Corpus and a few popular science texts from the
British National Corpus. It contains information
about the importance of the sentences (Hasler et
al., 2003). Sentences are marked as essential or im-
portant. The corpus also contains annotations for
3The goal of this project was to investigate to what extent
Computer-Aided Summarization can help humans to produce
high quality summaries with less effort.
linked sentences, which are not significant enough
to be marked as important/essential, but which have
to be considered as they contain information essen-
tial for the understanding of the content of other sen-
tences marked as essential/important.
Four annotators were used for the annotation,
three graduate students and one postgraduate. Three
of the annotators were native English speakers, and
the fourth had advanced knowledge of English. Un-
fortunately, not all of the documents were annotated
by all of the annotators. To maximize the reliability
of the summaries used for evaluation, we chose the
documents annotated by the greatest number of the
annotators; in total, our evaluation corpus contained
37 documents.
For acquiring manual summaries at specified
lengths and getting the sentence scores (for relative
utility evaluation) we assigned a score 3 to the sen-
tences marked as essential, a score 2 to important
sentences and a score 1 to linked sentences. The
sentences with highest scores are then selected for
ideal summary (at specified lenght).
4.2 Evaluation Measures
Evaluating summarization is a notoriously hard
problem, for which standard measures like Preci-
sion and Recall are not very appropriate. The main
problem with P&R is that human judges often dis-
agree what are the top n% most important sentences
in a document. Using P&R creates the possibility
that two equally good extracts are judged very dif-
ferently. Suppose that a manual summary contains
sentences [1 2] from a document. Suppose also that
two systems, A and B, produce summaries consist-
ing of sentences [1 2] and [1 3], respectively. Us-
ing P&R, system A will be ranked much higher than
system B. It is quite possible that sentences 2 and 3
are equally important, in which case the two systems
should get the same score.
To address the problem with precision and recall
we used a combination of evaluation measures. The
first of these, relative utility (RU) (Radev et al,
2000) allows model summaries to consist of sen-
tences with variable ranking. With RU, the model
summary represents all sentences of the input doc-
ument with confidence values for their inclusion in
the summary. For example, a document with five
sentences [1 2 3 4 5] is represented as [1/5 2/4 3/4
5
Evaluation Lexical LSA Manual Manual
Method Substitution Additition
Relative Utility 0.595 0.573 0.662
F-score 0.420 0.410 0.489
Cosine Similarity 0.774 0.806 0.823
Main Topic Similarity 0.686 0.682 0.747
Table 1: Evaluation of the manual annotation improvement - summarization ratio: 15%.
Evaluation Lexical LSA Manual Manual
Method Substitution Addition
Relative Utility 0.645 0.662 0.688
F-score 0.557 0.549 0.583
Cosine Similarity 0.863 0.878 0.886
Main Topic Similarity 0.836 0.829 0.866
Table 2: Evaluation of the manual annotation improvement - summarization ratio: 30%.
4/1 5/2]. The second number in each pair indicates
the degree to which the given sentence should be
part of the summary according to a human judge.
This number is called the utility of the sentence.
Utility depends on the input document, the summary
length, and the judge. In the example, the system
that selects sentences [1 2] will not get a higher score
than a system that chooses sentences [1 3] given
that both summaries [1 2] and [1 3] carry the same
number of utility points (5+4). Given that no other
combination of two sentences carries a higher util-
ity, both systems [1 2] and [1 3] produce optimal
extracts. To compute relative utility, a number of
judges, (N ? 1) are asked to assign utility scores to
all n sentences in a document. The top e sentences
according to utility score4 are then called a sentence
extract of size e. We can then define the following
system performance metric:
(6) RU =
?n
j=1 ?j
?N
i=1 uij
?n
j=1 ?j
?N
i=1 uij
,
where uij is a utility score of sentence j from anno-
tator i, ?j is 1 for the top e sentences according to the
sum of utility scores from all judges and ?j is equal
to 1 for the top e sentences extracted by the system.
For details see (Radev et al, 2000).
The second measure we used is Cosine Similarity,
according to the standard formula:
(7) cos(X,Y ) =
?
i xi?yi
?
?
i(xi)2?
?
?
i(yi)2
,
4In the case of ties, some arbitrary but consistent mecha-
nism is used to decide which sentences should be included in
the summary.
where X and Y are representations of a system sum-
mary and its reference summary based on the vector
space model. The third measure is Main Topic Sim-
ilarity. This is a content-based evaluation method
based on measuring the cosine of the angle between
first left singular vectors of a system summary?s
and its reference summary?s SVDs. (For details see
(Steinberger and Jezek, 2004).) Finally, we mea-
sured ROUGE scores, with the same settings as in the
Document Understanding Conference (DUC) 2004.
4.3 How Much May Anaphora Resolution
Help? An Upper Bound
We annotated all the anaphoric relations in the 37
documents in our evaluation corpus by hand us-
ing the annotation tool MMAX (Mueller and Strube,
2003).5 Apart from measuring the performance of
GUITAR over the corpus, this allowed us to establish
the upper bound on the performance improvements
that could be obtained by adding an anaphoric re-
solver to our summarizer. We tested both methods
of adding the anaphoric knowledge to the summa-
rizer discussed above. Results for the 15% and 30%
ratios6 are presented in Tables 1 and 2. The baseline
is our own previously developed LSA-based sum-
marizer without anaphoric knowledge. The result
is that the substitution method did not lead to sig-
nificant improvement, but the addition method did:
5We annotated personal pronouns, possessive pronouns, def-
inite descriptions and also proper nouns, who will be handled by
a future GUITAR version.
6We used the same summarization ratios as in CAST.
6
Evaluation Lexical LSA CAST GUITAR GUITAR
Method Substitution Addition
Relative Utility 0.595 0.527 0.530 0.640
F-score 0.420 0.348 0.347 0.441
Cosine Similarity 0.774 0.726 0.804 0.805
Main Topic Similarity 0.686 0.630 0.643 0.699
Table 3: Evaluation of the GUITAR improvement - summarization ratio: 15%.
Evaluation Lexical LSA CAST GUITAR GUITAR
Method Substitution Addittion
Relative Utility 0.645 0.618 0.626 0.678
F-score 0.557 0.522 0.524 0.573
Cosine Similarity 0.863 0.855 0.873 0.879
Main Topic Similarity 0.836 0.810 0.818 0.868
Table 4: Evaluation of the GUITAR improvement - summarization ratio: 30%.
addition could lead to an improvement in Relative
Utility score from .595 to .662 for the 15% ratio, and
from .645 to .688 for the 30% ratio. Both of these
improvements were significant by t-test at 95% con-
fidence.
4.4 Results with GUITAR
To use GUITAR, we first parsed the texts using Char-
niak?s parser (Charniak, 2000). The output of the
parser was then converted into the MAS-XML for-
mat expected by GUITAR by one of the preproces-
sors that come with the system. (This step includes
heuristic methods for guessing agreement features.)
Finally, GUITAR was ran to add anaphoric infor-
mation to the files. The resulting files were then
processed by the summarizer.
GUITAR achieved a precision of 56% and a recall
of 51% over the 37 documents. For definite descrip-
tion resolution, we found a precision of 69% and
a recall of 53%; for possessive pronoun resolution,
the precision was 53%, recall was 53%; for personal
pronouns, the precision was 44%, recall was 46%.
The results with the summarizer are presented
in Tables 3 and 4 (relative utility, f-score, cosine,
and main topic). The contribution of the differ-
ent anaphora resolution components is addressed in
(Kabadjov et al, 2005). All versions of our summa-
rizer (the baseline version without anaphora resolu-
tion and those using substitution and addition) out-
performed the CAST summarizer, but we have to em-
phasize that CAST did not aim at producing a high-
performance generic summarizer; only a system that
could be easily used for didactical purposes. How-
ever, our tables also show that using GUITAR and the
addition method lead to significant improvements
over our baseline LSA summarizer. The improve-
ment in Relative Utility measure was significant by
t-test at 95% confidence. Using the ROUGE mea-
sure we obtained improvement (but not significant).
On the other hand, the substitution method did not
lead to significant improvements, as was to be ex-
pected given that no improvement was obtained with
?perfect? anaphora resolution (see previous section).
5 Conclusion and Further Research
Our main result in this paper is to show that using
anaphora resolution in summarization can lead to
significant improvements, not only when ?perfect?
anaphora information is available, but also when
an automatic resolver is used, provided that the
anaphoric resolver has reasonable performance. As
far as we are aware, this is the first time that such
a result has been obtained using standard evaluation
measures over a reference corpus. We also showed
however that the way in which anaphoric informa-
tion is used matters: with our set of documents at
least, substitution would not result in significant im-
provements even with perfect anaphoric knowledge.
Further work will include, in addition to extend-
ing the set of documents and testing the system with
other collections, evaluating the improvement to be
achieved by adding a proper noun resolution algo-
rithm to GUITAR.
7
References
S. Azzam, K. Humphreys and R. Gaizauskas. 1999. Using
coreference chains for text summarization. In Proceedings
of the ACL Workshop on Coreference. Maryland.
B. Baldwin and T. S. Morton. 1998. Dynamic coreference-
based summarization. In Proceedings of EMNLP. Granada,
Spain.
R. Barzilay and M. Elhadad. 1997. Using lexical chains for text
summarization. In Proceedings of the ACL/EACL Workshop
on Intelligent Scalable Text Summarization. Madrid, Spain.
S. Bergler, R. Witte, M. Khalife, Z. Li, and F. Rudzicz.
2003. Using Knowledge-poor Coreference Resolution for
Text Summarization. In Proceedings of DUC. Edmonton.
M. W. Berry, S. T. Dumais and G. W. O?Brien. 1995. Using
Linear Algebra for Intelligent IR. In SIAM Review, 37(4).
B. Boguraev and C. Kennedy. 1999. Salience-based content
characterization of text documents. In I. Mani and M. T.
Maybury (eds), Advances in Automatic Text Summarization,
MIT Press. Cambridge, MA.
E. Charniak. 2000. A maximum-entropy-inspired parser. In
Proceedings of NAACL. Philadelphia.
F. Y. Y. Choi, P. Wiemer-Hastings and J. D. Moore. 2001. La-
tent Semantic Analysis for Text Segmentation. In Proceed-
ings of EMNLP. Pittsburgh.
Y. Gong and X. Liu. 2002. Generic Text Summarization Us-
ing Relevance Measure and Latent Semantic Analysis. In
Proceedings of ACM SIGIR. New Orleans.
L. Hasler, C. Orasan and R. Mitkov. 2003. Building better
corpora for summarization. In Proceedings of Corpus Lin-
guistics. Lancaster, United Kingdom.
E. Hovy and C. Lin. 1997. Automated text summarization in
SUMMARIST. In ACL/EACL Workshop on Intelligent Scal-
able Text Summarization. Madrid, Spain.
M. A. Kabadjov, M. Poesio and J. Steinberger. 2005. Task-
Based Evaluation of Anaphora Resolution: The Case of
Summarization. In RANLP Workshop ?Crossing Barriers
in Text Summarization Research?. Borovets, Bulgaria.
A. Knott, J. Oberlander, M. O?Donnell, and C. Mellish. 2001.
Beyond elaboration: The interaction of relations and focus in
coherent text. In Sanders, T., Schilperoord, J., and Spooren,
W. (eds), Text representation: linguistic and psycholinguistic
aspects. John Benjamins.
T. K. Landauer and S. T. Dumais. 1997. A solution to Plato?s
problem: The latent semantic analysis theory of the acqui-
sition, induction, and representation of knowledge. In Psy-
chological Review, 104, 211-240.
R. Mitkov. 1998. Robust pronoun resolution with limited
knowledge. In Proceedings of COLING. Montreal.
C. Mueller and M. Strube. 2001. MMAX: A Tool for the Anno-
tation of Multi-modal Corpora. In Proceedings of the IJCAI
Workshop on Knowledge and Reasoning in Practical Dia-
logue Systems. Seattle.
C. Orasan, R. Mitkov and L. Hasler. 2003. CAST: a Computer-
Aided Summarization Tool. In Proceedings of EACL. Bu-
dapest, Hungary.
M. Poesio and M. A. Kabadjov. 2004. A General-Purpose, off-
the-shelf Anaphora Resolution Module: Implementation and
Preliminary Evaluation. In Proceedings of LREC. Lisbon,
Portugal.
M. Poesio, R. Stevenson, B. Di Eugenio, and J. M. Hitzeman.
2004. Centering: A parametric theory and its instantiations.
Computational Linguistics, 30(3).
M. Poesio, M. A. Kabadjov, R. Vieira, R. Goulart, and
O. Uryupina. 2005. Do discourse-new detectors help def-
inite description resolution? In Proceedings of IWCS.
Tilburg, The Netherlands.
D. R. Radev, H. Jing, and M. Budzikowska. 2000.
Centroid-based summarization of multiple documents. In
ANLP/NAACL Workshop on Automatic Summarization.
Seattle.
J. Steinberger and K. Jezek. 2004. Text Summarization and
Singular Value Decomposition. In Proceedings of ADVIS.
Izmir, Turkey.
R. Stuckardt. 2003. Coreference-Based Summarization and
Question Answering: a Case for High Precision Anaphor
Resolution. In International Symposium on Reference Reso-
lution. Venice, Italy.
R. Vieira and M. Poesio. 2000. An empirically-based system
for processing definite descriptions. In Computational Lin-
guistics, 26(4).
8
c? 2004 Association for Computational Linguistics
Centering: A Parametric Theory and Its
Instantiations
Massimo Poesio? Rosemary Stevenson?
University of Essex University of Durham
Barbara Di Eugenio? Janet Hitzeman?
University of Illinois at Chicago MITRE Corporation
Centering theory is the best-known framework for theorizing about local coherence and
salience; however, its claims are articulated in terms of notions which are only partially specified,
such as ?utterance,? ?realization,? or ?ranking.? A great deal of research has attempted to
arrive at more detailed specifications of these parameters of the theory; as a result, the claims of
centering can be instantiated in many different ways. We investigated in a systematic fashion
the effect on the theory?s claims of these different ways of setting the parameters. Doing this
required, first of all, clarifying what the theory?s claims are (one of our conclusions being that
what has become known as ?Constraint 1? is actually a central claim of the theory). Secondly,
we had to clearly identify these parametric aspects: For example, we argue that the notion of
?pronoun? used in Rule 1 should be considered a parameter. Thirdly, we had to find appropriate
methods for evaluating these claims. We found that while the theory?s main claim about salience
and pronominalization, Rule 1?a preference for pronominalizing the backward-looking center
(CB)?is verified with most instantiations, Constraint 1?a claim about (entity) coherence and
CB uniqueness?is much more instantiation-dependent: It is not verified if the parameters are
instantiated according to very mainstream views (?vanilla instantiation?), it holds only if indirect
realization is allowed, and is violated by between 20% and 25% of utterances in our corpus even
with the most favorable instantiations. We also found a trade-off between Rule 1, on the one hand,
and Constraint 1 and Rule 2, on the other: Setting the parameters to minimize the violations of
local coherence leads to increased violations of salience, and vice versa. Our results suggest that
?entity? coherence?continuous reference to the same entities?must be supplemented at least
by an account of relational coherence.
1. Motivations
Centering theory (Joshi and Weinstein 1981; Grosz, Joshi, and Weinstein 1983, 1995;
Walker, Joshi, and Prince 1998b) is the component of Grosz and Sidner?s overall theory
? Department of Computer Science, Wivenhoe Park, Colchester CO4 35Q, U.K. E-mail:
poesio@essex.ac.uk.
? Department of Psychology, University of Durham, U.K.
? Department of Computer Science, University of Illinois at Chicago, Chicago, IL 60607-7053, USA.
E-mail: bdieugen@cs.uic.edu
? MITRE Corporation, 202 Burlington Road, Bedford, MA 01730-1428, USA. E-mail:hitze@mitre.org.
Submission received: 16 April 2002; Revised submission received: 3 September 2003; Accepted for
publication: 11 December 2003
310
Computational Linguistics Volume 30, Number 3
of attention and coherence in discourse (Grosz 1977; Sidner 1979; Grosz and Sidner
1986) concerned with local coherence and salience, that is, coherence and salience
within a discourse segment. A fundamental characteristic of centering is that it is bet-
ter viewed as a linguistic theory than a computational one. By this we mean that its
primary aim is to make cross-linguistically valid claims about which discourses are
easier to process, abstracting away from specific algorithms for anaphora resolution
or anaphora generation (although many such algorithms are based on the theory).
The result is a very different theory from those one usually finds in computational
linguistics. In central papers such as Grosz, Joshi, and Weinstein (1995), no algo-
rithms are provided to compute notions such as ?utterance,? ?previous utterance,?
?ranking,? and ?realization? that play a crucial role in the theory. The researchers
working on centering argue that while these concepts play a central role in any theory
of discourse coherence and salience, their precise characterization is best left for sub-
sequent research, indeed, that some of these concepts (e.g., ranking) might be defined
in a different way for each language (Walker, Iida, and Cote 1994). In other words,
these notions should be viewed as parameters of centering. This feature of the theory
has inspired a great deal of research attempting to specify centering?s parameters for
different languages (Kameyama 1985; Walker, Iida, and Cote 1994; Di Eugenio 1998;
Turan 1998; Strube and Hahn 1999). Competing versions of the central definitions
and claims of the theory have also been proposed: For example, different definitions
of backward-looking center (CB) can be found in Grosz, Joshi, and Weinstein (1983,
1995) and Gordon, Grosz, and Gillion (1993). As a result, a researcher wishing to test
the predictions of centering, or to use it for practical applications, is confronted with
a large number of possible instantiations of the theory.
The main goal of the work reported in this article was to explore the space of
parameter configurations, measuring the impact of different ways of setting the pa-
rameters of centering on the theory?s claims. This required specifying in an explicit
way what centering?s main claims are; clearly identifying the parameters, not all of
which have previously been discussed in the literature; and developing appropriate
methods (and statistical tests) to carry out this evaluation. The comparison between
instantiations was carried out by annotating a corpus of English texts from different
genres with the information needed to test a variety of centering instantiations and
using this corpus to assess the extent to which the theory?s claims are verified once the
parameters are set in a certain way. The proponents of centering have clearly stated
that the aim of the theory is to identify preferences that make discourses easier to pro-
cess; clearly, the best way to test such preferences is through behavioral experiments,
and many aspects of the theory have in fact been tested this way (Hudson, Tanen-
haus, and Dell 1986; Gordon, Grosz, and Gillion 1993; Brennan 1995). But given the
enormous number of possible ways of setting the theory?s parameters, a systematic
comparison can be made only by computational means. A corpus-based evaluation
has other advantages, as well, among which is that it is perhaps the best way to
identify the aspects of the theory that need to be further specified, and the factors
such as temporal coherence or stylistic variation that may interact with the prefer-
ences expressed by centering. (Also, knowing the extent to which real texts conform
to centering preferences is an important goal in its own right.)
In previous corpus-based studies of centering (Walker 1989; Passonneau 1993;
Byron and Stent 1998; Di Eugenio 1998; Kameyama 1998; Strube and Hahn 1999;
Tetreault 2001), only a few instantiations of centering were compared. The present
study is more systematic in that it considers a greater number of parameters, as
well as more parameter instantiations, including ?crossing? instantiations in which
the parameters are set according to proposals due to different researchers. Only re-
311
Poesio et al Centering: A Parametric Theory
liable annotation techniques were used; we produced an annotation manual that
can be used to extend our analysis to other data, as well as a companion Web site
(http://cswww.essex.ac.uk/staff/poesio/cbc/) to allow readers to try out instantia-
tions not discussed in this article. (The Web site also contains the annotation manual
and a technical report with a full discussion of all results.) Last but not least, our
evaluation is arguably more neutral than in most previous studies in that, first of all,
we are not proposing a new instantiation of the theory; and secondly, all parameter
instantiations were tested on the same data.
The article is organized as follows. We first review the basic concepts of the theory,
discussing the three claims on which we focus (Constraint 1, Rule 1, and Rule 2) and the
parameters used in their formulation. We then discuss how our corpus was annotated
and how the annotation was used to compute violations of the three main claims. In
Section 4 we present our main results, which are discussed in Section 5.
2. Centering Theory and Its Parameters
It is not possible to discuss in this article the entire centering literature; we merely
summarize in this section some of this work in enough detail to allow the reader to
follow the discussion in the rest of the article. For more details, we refer the reader
to classic references such as Grosz, Johsi, and Weinstein (1995) and Walker, Joshi, and
Prince (1998b) or the discussion of centering in Poesio and Stevenson (forthcoming).
2.1 Motivations and Main Intuitions
Centering is simultaneously a theory of discourse coherence and of discourse salience.
As a theory of coherence, it attempts to characterize entity-coherent discourses: dis-
courses that are considered coherent because of the way discourse entities are intro-
duced and discussed.1 At the same time, centering is also intended to be a theory of
salience: that is, it attempts to predict which entities will be most salient at any given
time.
The main claim about local coherence made in centering is that discourse segments
in which successive utterances keep mentioning the same discourse entities are ?more
coherent? than discourse segments in which different entities are mentioned. This
hypothesis was formulated by Chafe (1976) and is backed by empirical evidence such
as Kintsch and van Dijk (1978) and Givon (1983). In centering this hypothesis is further
strenghtened by proposing that every utterance has a unique ?main link? with the
previous utterance: the CB. Having a unique CB, it is claimed, considerably simplifies
the complexity of the inferences required to integrate an utterance into the discourse
(Joshi and Kuhn 1979; Joshi and Weinstein 1981).
Centering?s first contention as far as local salience is concerned is that the dis-
course entities realized by an utterance (more on realization below) are ranked: that is,
that in each utterance some discourse entities are more salient than others. This claim,
as well, is a basic tenet of much work on discourse (Sidner 1979; Prince 1981; Givon
1983; Gundel, Hedberg, and Zacharski 1993) and is supported by much psychologi-
cal evidence (Hudson, Tanenhaus, and Dell 1986; Gernsbacher and Hargreaves 1988;
Gordon, Grosz, and Gillion 1993; Stevenson, Crawley, and Kleinman 1994).
1 Entity-based theories of coherence are so-called by contrast with relation-centered theories of
coherence, such as those developed in Hobbs (1979) and Mann and Thompson (1988) and used in Fox
(1987) and Lascarides and Asher (1993). The earliest detailed entity-based theory of coherence we are
aware of is by Kintsch and van Dijk (1978), who also explicitly mention the need to supplement such
theories with a theory of relational coherence (more on this in Section 5)
312
Computational Linguistics Volume 30, Number 3
These claims about coherence and salience are linked by two further hypotheses:
that the identity of the CB is crucially determined by the entities? ranking and that the
CB is most likely to be realized as a pronoun. This assumption that a ?main entity? or
?topic? or ?focus? is the preferred interpretation of pronouns is commonly found in
theories in the psychological (e.g., Sanford and Garrod 1981), computational (Sidner
1979) and linguistic (Gundel, Hedberg, and Zacharski 1993) literature and is motivated
by evidence such as the contrast between examples (1) and (2):
(1) a. Something must be wrong with John.
b. He has been acting quite odd. (He = John)
c. He called up Mike yesterday.
d. John wanted to meet him quite urgently.
(2) a. Something must be wrong with John.
b. He has been acting quite odd. (He = John)
c. He called up Mike yesterday.
d. He wanted to meet him quite urgently.
Discourses (1) and (2) differ only only in their (d) sentences, but according to
Grosz et al, (1d) is not as felicitous as (2d). The reason, they argue, is that after the
(c) utterances, the discourse entity John is more highly ranked than Mike, so it will be
the CB of the next utterance provided that it is realized in it; and given the preference
for pronominalizing the CB, John should be pronominalized if anything else is.
This link between pronominalization and the identity of the CB has been used
by Grosz et al to support the claim discussed above that utterances have a unique
CB (contra, e.g., Sidner [1979] whose theory assumed two foci). Grosz et al note
the contrast between continuations (3c)?(3f) of the discourse initiated by utterances
(3a)?(3b):
(3) a. Susan gave Betsy a pet hamster.
b. She reminded her that such hamsters were quite shy.
c. She asked Betsy whether she liked the gift.
d. Betsy told her that she really liked the gift.
e. Susan asked her whether she liked the gift.
f. She told Susan that she really liked the gift.
Grosz et al argue that continuations (3c)?(3f) are less and less acceptable, whereas if
Susan and Betsy were equally ranked after (3b), all variants should be equally accept-
able.
2.2 Terminology and Definitions
2.2.1 Local Focus, Forward-Looking Centers, and Utterances. A fundamental assump-
tion underlying centering is that processing a discourse involves continuous updates
to the local attentional state, or local focus. The local focus includes a set of forward-
looking centers (CFs), which correspond to Sidner?s (1979) ?potential discourse foci?
and can be viewed as mentions of discourse entities (Karttunen 1976; Webber 1978;
Heim 1982; Kamp and Reyle 1993). The local focus also contains information about the
relative prominence or rank of these CFs. The local focus gets updated after every ut-
terance: In this update, the current CFs are replaced by new ones, and the CB changes,
313
Poesio et al Centering: A Parametric Theory
as well (see below).2 The set of CFs introduced in the local focus by utterance Ui in
a discourse segment (DS) is indicated by CF(Ui,DS), generally abbreviated to CF(Ui).
Brennan, Friedman, and Pollard (1987) formalized the relationship between utterances
and CFs by means of one of their so-called constraints:3
Constraint 2: Every element of CF(U,DS) must be realized in U.
2.2.2 Ranking, Preferred Centers, and Backward-Looking Centers. We already men-
tioned two important claims of the theory: that forward-looking centers are ranked,
and that because of this ranking, some CFs acquire particular prominence. The ranking
function is only required to be partial, but the most highly ranked CF realized by an
utterance (when one exists) is called the Preferred Center (CP). Ranking is also used
to characterize one of the CFs as the CB. The CB is the closest concept in centering
to the traditional notion of ?topic? (Sgall 1967; Chafe 1976; Sanford and Garrod 1981;
Givon 1983; Vallduvi 1990; Gundel, Hedberg, and Zacharski 1993) and plays a cen-
tral role in the theory?s claims about both coherence and salience. Although in Grosz,
Joshi, and Weinstein (1983), the CB was characterized only in intuitive terms, most
subsequent work has been based on the definition below (Grosz, Joshi, and Weinstein
1995), referred to as ?Constraint 3? by Brennan, Friedman, and Pollard (1987):
Constraint 3 CB(Ui), the backward-looking center of utterance Ui, is the highest-
ranked element of CF(Ui?1) that is realized in Ui.
Notice that according to this definition, the computation of the CB depends exclusively
on ranking and ?previous utterance,? making these parameters crucially important for
the framework.4
2.2.3 Transitions. The hypothesis that discourses are easier to process when successive
utterances are perceived as being ?about? a unique discourse entity is formalized in
centering in terms of a classification of utterances according to the type of transition
(update) they induce in the local focus. Many such classifications of transitions have
been proposed; Grosz, Joshi, and Weinstein (1995) distinguish among three types of
transitions, depending on whether the backward-looking center of Ui?1 is maintained
or not in Ui and on whether CB(Ui) is also the most highly ranked entity (CP) of Ui:
Center Continuation (CON): CB(Ui) = CB(Ui?1), and CB(Ui) is the most highly
ranked CF (CP) of Ui (i.e., CP(Ui) = CB(Ui))
Center Retaining (RET): CB(Ui) = CB(Ui?1), but CP(Ui) = CB(Ui)
Center Shifting (SHIFT): CB(Ui?1) = CB(Ui)
We will consider a few alternative classification schemes below, after discussing how
these classifications are used to formulate one of the core claims of centering, Rule 2.
2 The hypothesis that discourse processing involves continuous updates to the discourse model also lies
at the heart of so-called ?dynamic? theories of discourse semantics (Heim 1982; Kamp and Reyle 1993).
3 The order of presentation of constraints and rules followed here differs from that more familiar in the
centering literature. This is because we want to distinguish between definitions and claims, and the
three constraints proposed by Brennan et al do not all have the same status: While Constraint 2 can be
seen as a ?filter? ruling out certain values of CF(Ui), Constraint 3 is a definition, and Constraint 1 an
empirical claim.
4 Other ways of defining the CB have been proposed. We refer the interested reader to the longer report
and to the companion Web site.
314
Computational Linguistics Volume 30, Number 3
2.3 Main Claims
In the words of Grosz et al, the most fundamental claim of centering is that ?to the
extent that discourse adheres to centering constraints, its coherence will increase and
the inference load placed upon the hearer will decrease? (Grosz, Joshi, and Wein-
stein 1995, page 210). Grosz et al list seven such ?costraints,? three of which can be
directly evaluated. Even though we are not following here the distinction between
?constraints? and ?rules? introduced in Brennan, Friedman, and Pollard (1987), we
will use for these three claims the names Brennan et al gave them, by which they are
now best known:
Constraint 1 (Strong): All utterances of a segment except for the first have exactly
one CB.
Rule 1 (GJW95): If any CF is pronominalized, the CB is.
Rule 2 (GJW95): (Sequences of) continuations are preferred over (sequences of)
retains, which are preferred over (sequences of) shifts.
2.3.1 Constraint 1, Topic Uniqueness, and Entity Coherence. If we view the CB as
a formalization of the idea of ?topic? (Vallduvi 1990; Gundel 1998; Hurewitz 1988;
Miltsakaki 1999; Beaver 2004), Constraint 1 expresses, first and foremost, the original
claim from Joshi and Kuhn (1979) and Joshi and Weinstein (1981) that discourses with
exactly one (or no more than one) ?topic? at each point are easier to process. This view
contrasts both with Sidner?s (1979) hypothesis that utterances may have two ?topics?
and with theories such as Givon (1983), Alshawi (1987), Lappin and Leass (1994) and
Arnold (1998), which view ?topichood? as a matter of degree and therefore allow for
an arbitrary number of topics.
In the strong form just presented, Constraint 1 is also a claim about local coher-
ence. It expresses a preference for discourses to be entity coherent: to continue talking
about the same entities. Each utterance in a segment should realize at least one of the
discourse entities realized in the previous utterance. A weaker form of Constraint 1
has also been suggested (e.g., Walker, Joshi, and Prince 1998a, footnote 2, page 3); the
preference for a unique CB is preserved, but not the preference for entity coherence.
Constraint 1 (Weak): All utterances of a segment except for the first have at most
one CB.
2.3.2 Rule 1 and Pronominalization. Rule 1 is the main claim of centering about
pronominalization. In the version presented above, it states a preference for pronomi-
nalizing the CB, if anything is pronominalized at all. We also examined two alternative
formulations. The original form of the claim in Grosz, Joshi, and Weinstein (1983) was
as follows:
Rule 1 (GJW83): If the CB of the current utterance is the same as the CB of the
previous utterance, a pronoun should be used.
Gordon, Grosz, and Gillion (1993) proposed a much stronger form of the claim. They
found that entities realized in certain positions in the sentence were read more slowly
unless pronominalized (repeated-name penalty [RNP]).5 This evidence led them to
5 Gordon et al observed increased reading times when proper names were used instead of pronouns to
realize an entity in subject position referring to an entity realized in first-mentioned or subject position.
315
Poesio et al Centering: A Parametric Theory
propose a more restrictive definition of CB (briefly, that the CB is the entity subject to
the RNP?for discussion, see the longer version of this article available on the Web
site) and a stronger form of Rule 1, requiring the CB (defined in this more restrictive
way) to be always pronominalized:
Rule 1 (Gordon et al): The CB should be pronominalized.
Although we will refer to this version as ?Gordon et al?s? for brevity, readers should
keep in mind that because the definition of CB proposed by Gordon et al is more
restrictive, their version of Rule 1 is properly evaluated only using that definition.
(The results with this instantiation are discussed in the longer version of the article
availabel on the Web site.)
2.3.3 Rule 2 and the Classification of Transitions. Rule 2 is a claim about coherence, as
well: It states a preference for preserving the CB over changing it and for realizing it as
the most salient entity over changing its relative ranking. This aspect of the theory has
received a lot of attention; several variants of this constraint have been proposed, as
well as many ways of classifying transitions. We studied many alternative proposals.
The version of Rule 2 presented in Grosz, Joshi, and Weinstein (1995) expresses
preferences among sequences of transitions (e.g., CON-CON over SHIFT-SHIFT) rather
than preferences for particular transitions. This form of the constraint is in part mo-
tivated by empirical results. Di Eugenio (1998), for example, found that the relative
distribution of null and explicit pronouns in Italian depends on the previous transition
as well: in Center Continuations that follow a CON or a SHIFT, it is much more likely
that a null pronoun will be used, whereas in Center Continuations that follow a RET
transition, both null and explicit pronouns are equally likely. Turan (1998) obtained
similar results for null and explicit pronouns in Turkish.
Other researchers argue instead that the inferential load is evaluated utterance by
utterance (Brennan, Friedman, and Pollard 1987; Walker, Iida, and Cote 1994; Walker,
Joshi, and Prince 1998a). The version of Rule 2 proposed by Brennan et al is as follows:
Rule 2 (Single Transitions): Transition states are ordered. The CON transition is
preferred to the RET transition, which is preferred to the Smooth Shift
transition, which is preferred to the Rough Shift transition.
This formulation of Rule 2 depends on a further distinction between two types of
SHIFT: Smooth Shift (SSH), when CB(Un) = CP(Un), and Rough-Shift (RSH), when
CB(Un) = CP(Un). Transitions can then be classified along two dimensions, as in the
following table:
CB(Un) = CB(Un?1) or CB(Un?1) = NIL CB(Un) = CB(Un?1)
CB(Un) = CP(Un) Continue Smooth Shift
CB(Un) = CP(Un) Retain Rough Shift
Further refinements of these classification schemes have been proposed. Kameyama
(1986) proposed a fourth transition type, Center Establishment (EST), for utterances
E.g., in Bruno was the bully of the neighborhood. Bruno / He often taunted Tommy, the second sentence would
be read more slowly when Bruno was used than when He was used.
316
Computational Linguistics Volume 30, Number 3
that establish a CB after an utterance without one, such as the first utterance of a seg-
ment. Walker, Iida, and Cote (1994) argued that these utterances should be classified
as Center Continuations, the idea being that even the first utterance of a segment does
have a CB, but this CB is initially underspecified and is determined only when the sec-
ond utterance is processed. Notice that according to the strong version of Constraint
1, the first utterance of a discourse segment is the only utterance allowed not to have a
CB in a coherent discourse; hence, none of these classification schemes for transitions
includes classes either for the inverse of Center Establishment, that we might call Ze-
roing (ZERO) transition (a CB less utterance following one which does have a CB) or
for CB-less utterances following other CB-less ones (the NULL transition).
Strube and Hahn (1999), like Grosz, Joshi, and Weinstein (1995), claim that infer-
ential load is evaluated across sequences (pairs, in fact) of transitions but argue for
a different way of evaluating the inferential load of utterances. In their view, classi-
fications of transitions such as those above do not reflect what should be one of the
crucial claims of the theory: that the CP of one utterance predicts the CB of the next.
In order to formalize their view, they propose a different classification scheme, based
on the distinction between cheap and expensive transitions (Strube and Hahn 1999,
page 332):
? A transition pair is cheap if the CB of the current utterance is correctly
predicted by the CP of the previous utterance, that is, if CB(Un) =
CP(Un?1).
? A transition pair is expensive if CB(Un) = CP(Un?1).
Strube and Hahn then propose a new version of Rule 2 based on this distinction:
Rule 2 (Strube and Hahn): Cheap transition pairs are preferred to expensive ones.6
2.4 The Parameters of Centering
Although Grosz et al discussed possible definitions for the concepts used in the claims
above??utterance,? ?previous utterance,? ?ranking,? and ?realization??they didn?t
settle on a specific definition, even for English. Similarly undefined is the notion of
?pronominalization? governed by Rule 1. But without further specification of these
concepts it is impossible to evaluate the claims above, just as it is not possible to
evaluate the predictions of, say, ?government and binding theory? without providing
an explicit definition of ?command? or ?argument?. As a result, a considerable amount
of research has been concerned with establishing the best specification for what are,
essentially, parameters of the theory. We briefly review some of these proposals in this
section.7
6 Kibble (2001) proposed a version of Rule 2 that further develops the ?decompositional? view of Rule 2
introduced by Brennan et al, while simultaneously incorporating Strube and Hahn?s intuition that
?cheap? transitions should be preferred. Kibble formulates his version of Rule 2 as a series of
preferences: for transitions that preserve the CB?that is, those such that CB(Un) = CB(Un?1) (he calls
these transitions cohesive), that identify CB(Un) with CP(Un), and/or that are cheap. Code to test an
earlier version of Kibble?s form of Rule 2 (Kibble 2000) has been incorporated in the scripts discussed
later in the article, and the results can be seen in the longer technical report accompanying this article,
or from the companion Web site. We will, however, omit a discussion of this version here, in part for
reasons of space, in part because the final version of the rule in Kibble (2001) differs substantially from
the original version that we evaluated.
7 For more details, and for a discussion of the motivations behind these proposals, see the extended
version of this article and Poesio and Stevenson (forthcoming).
317
Poesio et al Centering: A Parametric Theory
2.4.1 Utterance and Previous Utterance. In the early centering papers, utterances were
implicitly identified with sentences. Kameyama (1998), however, argued that such
identification makes the number of potential antecedents of anaphoric expressions
much greater than if they were resolved clause by clause. Furthermore, she noted
that this identification leads to problems with multiclausal sentences: for example,
grammatical function ranking becomes difficult to compute, as a sentence may have
more than one subject. Kameyama proposed that the local focus be updated after every
tensed clause, not after every sentence, and classified tensed clauses into (1) utterances
that constitute a ?permanent? update of the local focus, such as coordinated clauses
and adjuncts, and (2) embedded utterances that result in temporary updates that are
then ?popped?, much as the information introduced into discourse by subordinated
discourse segments is popped according to Grosz and Sidner (1986). According to
Kameyama, only a few types of clauses, such as the complements of certain verbs,
are embedded. For example, Kameyama proposes to break up (4) into utterances as
follows, and to treat each of these utterances, including subordinate clauses such as
(U2) or (U5), as an update:
(4) (u1) Her entrance in Scene 2 Act 1 brought some disconcerting applause
(u2) even before she had sung a note. (u3) Thereafter the audience
waxed applause happy (u4) but discriminating operagoers reserved
judgment (u5) as her singing showed signs of strain.
Experiments by Pearson, Stevenson, and Poesio (2000) confirmed that CFs introduced
in main clauses are significantly more likely to be subsequently mentioned than CFs
introduced in complement clauses. However, a semicontrolled study by Suri and Mc-
Coy (1994) suggested that other types of clauses?specifically, adjunct clauses headed
by after and before?are also ?embedded,? not ?permanent updates? as suggested by
Kameyama; these results were subsequently confirmed by Cooreman and Sanford
(1996). The status of other types of clauses is less clear. Kameyama (1998) also pro-
posed a tentative analysis of relative clauses, according to which they are temporarily
treated as utterances and update the local focus but are then merged with the embed-
ding clause; she didn?t, however provide empirical support for this hypothesis. Other
types of subordinate clauses and parentheticals are not discussed in this literature.
Strube (1998) and Miltsakaki (1999) question Kameyama?s identification of utter-
ances with (tensed) clauses. Miltsakaki (1999) argues, on the basis of data from English
and Greek, that the local focus is updated only after every sentence and that only the
CFs in the main clause are considered when establishing the CB.
2.4.2 Realization. Grosz, Joshi, and Weinstein (1995) simply say that what it means for
utterance U to realize center C depends on the particular semantic theory one adopts.
They consider two ways in which a discourse entity may be ?realized? in an utterance
as required by Constraint 2. Direct realization is when a noun phrase in the utterance
refers to that discourse entity. Indirect realization is when one of the noun phrases
in the utterance is an associative reference to that discourse entity in the sense of
Hawkins (1978),8 that is, an anaphoric expression that refers to an object which wasn?t
mentioned before but is somehow related to an object that already has. For example,
in the following discourse:
(5) (u1) John walked toward the house. (u2) The door was open.
8 Associative references are one type of bridging reference (Clark 1997).
318
Computational Linguistics Volume 30, Number 3
John, the house and the door are directly realized in the respective utterances; in addition,
the house can be thought as being indirectly realized in (u2) by virtue of being referred
to by the associative reference the door (see, e.g., the discussion in Grosz, Joshi, and
Weinstein [1995] and Walker, Joshi, and Prince [1998b]). Clearly, the computation of
the CB is affected by which entities are considered to be ?realized? in an utterance: In
(5), for example, (u2) has a CB (the house) only if the house is considered to be realized
in (u2) by virtue of its being associated with the door. To our knowledge, the effect of
these alternative notions of realization on the predictions of the theory has not been
previously studied, even though theories of focusing such as Sidner?s (1979) do allow
the (discourse) focus to be realized in an utterance in these cases, and the issue is often
mentioned in discussions of centering.
A related issue is whether empty realizations, or traces, should count as realiza-
tions of an entity. Many theories of grammar hypothesize that morphologically null
elements occur in the syntactic structure underlying a variety of constructions, includ-
ing control constructions as in (6a), reduced relatives as in (6b), and even coordinated
verb phrases (VP) as in (6c):
(6) a. John wanted (? to buy a house).
b. John bought a house (? abandoned by its previous occupiers).
c. John bought a house and (? promptly demolished it).
If, for example, the coordinated VP in (6c) is considered a separate utterance, whether
or not it contains a realization of John is going to determine whether or not it has a
CB. To our knowledge, morphologically null elements have been considered in the
centering literature only for languages other than English.
An issue that has been raised in the centering literature (e.g., Walker 1993; Di
Eugenio 1998; Byron and Stent 1998) is whether the CF list contains only entities
realized as third-person noun phrases (NPs), or also the entities realized as first- and
second-person NPs. Walker (1993) suggests that deictic entities are beyond the purview
of centering; however, in example (7), neither utterance (u2) nor utterance (u3) would
have a CB if second-person pronoun you is not counted as introducing an entity in
the CF list.9
(7) (u1) You should not use PRODUCT-Z
(u2) if you are pregnant of breast-feeding.
(u3) Whilst you are receiving PRODUCT-Z . . .
2.4.3 Ranking. Perhaps the most discussed parameter of centering, at least in the
versions of the theory that accept the definition of CB specified by Constraint 3, is
the ranking function. Most researchers working on centering, including Grosz et al,
assume that several factors play a role in determining the relative ranking of forward-
looking centers; in fact, Walker, Iida, and Cote (1994) and Walker, Joshi, and Prince
(1998a) claim that the factors affecting ranking may not be the same in all languages.
9 According to Walker, Joshi, and Prince (1998a), in the original version of Grosz, Joshi, and Weinstein
(1995), which appeared in 1986, Grosz et al provided a more explicit definition of realization:
An utterance U realizes a center c if c is an element of the situation described by U, or c is
the semantic interpretation of some subpart of U.
With this definition, all of the cases considered above?the anchors of associative references, traces, and
the entities realized as first- and second-person pronouns?would be considered as realized by an
utterance.
319
Poesio et al Centering: A Parametric Theory
Nevertheless, most versions of the theory developed since Kameyama (1985, 1986) and
Grosz, Joshi, and Weinstein (1986) have assumed that grammatical function plays the
main role in determining the order among forward-looking centers, at least for English.
Specifically, Grosz, Joshi, and Weinstein (1995) claim that subjects are ranked more
highly than objects, which are ranked more highly than other grammatical positions:
SUBJ ? OBJ ? OTHERS (see also Kameyama 1986; Hudson, Tanenhaus, and Dell 1986).
Slightly different ranking functions based on grammatical function were proposed by
Brennan, Friedman, and Pollard (1987) (who made a further distinction between objects
and indirect objects), by Walker, Iida, Cote (1994) for Japanese, and by Turan (1998)
for Turkish. There is quite a lot of psychological support for at least the part of this
claim stating that entities realized as subjects are more salient than entities realized in
other grammatical functions (Hudson, Tanenhaus, and Dell 1986; Gordon, Grosz, and
Gillion 1993; Brennan 1995; Hudson-D?Zmura and Tanenhaus 1998).
Other factors affecting ranking have been considered as well. Rambow (1993)
proposed that a number of facts about scrambling in German could be explained if
ranking in German were to be determined by surface order of realization. The idea
that order of mention affects salience is well supported by psychological evidence;
for example, the results of probe experiments by Gernsbacher and Hargreaves (1998)
suggest that the first-mentioned discourse entity in a sentence is the most salient. The
interaction of order of mention with grammatical function has also been studied. As
mentioned above, Gordon, Grosz and Gillion (1993) observed a repeated name penalty
for CFs in subject position coreferring with an entity previously introduced. This effect
was observed both when the antecedent was in subject position and when it was the
first-mentioned entity in a nonsubject position (as in In Lisa?s opinion, he shouldn?t have
done that), suggesting that first-mentioned CFs are as highly ranked as subjects.
Strube and Hahn (1999) argue that in German, the rank of discourse entities is
determined by the position they hold in Prince?s (1981, 1992) givenness hierarchy.
Specifically, Strube and Hahn argue that hearer-old entities rank higher than mediated
entities, which and in turn rank higher than hearer-new entities: HEARER-OLD ?
MEDIATED ? HEARER-NEW.10 Order of mention also plays a role in their ranking:
Within each category, the entities realized earlier in the sentence are ranked more
highly.
Finally, Sidner?s original (1979) claim that ranking depended on thematic roles,
abandoned in the early versions of centering, was revisited by Cote (1998). This view is
supported by psychological work on ?implicit causality? verbs (Caramazza et al 1997)
as well as work by Stevenson, Crawley, and Kleinman (1994), and Pearson, Stevenson,
and Poesio (2001). In particular, there is evidence that with certain verbs, the normal
preference for subjects to rank higher than their objects is reversed, although these
preferences are modified by other factors such as order of mention, type of connective,
and animacy (Stevenson, Crawley, and Kleinman 1994; Stevenson et al 2000; Pearson,
Stevenson, and Poesio 2001).
2.4.4 R1-Pronouns. Rule 1 states that if any CF is pronominalized, the CB is, but
the theory does not explicitly specify which types of ?pronouns? are covered by this
rule. It seems clear that realization as a third-person singular pronoun does count;
that is, if the choice is between using a third-person singular pronoun to realize a
10 Strube and Hahn?s hearer-old entities include Prince?s evoked (= discourse old) and unused entities,
which are entities such as Margaret Thatcher that are supposed to be part of shared knowledge.
Mediated entities are the entities falling in Prince?s categories inferrable, containing inferrable, and
anchored brand-new.
320
Computational Linguistics Volume 30, Number 3
CB or another CF, the CB should be chosen. We also saw that in languages such as
Italian, Japanese, and Turkish, the preferred realizations of CBs are morphologically
null elements (Kameyama 1986; Walker, Iida, Cote 1994; Turan 1998; Di Eugenio 1998).
But should an utterance in English count as verifying the rule if a CF is realized as a
third-person pronoun, and the CB as a trace? Or if the CB is realized with a full NP,
but a second CF is realized with a demonstrative pronoun? And what about first- and
second-person pronouns? The precise characterization of the (sub)class of pronouns
subject to Rule 1, which we will call R1-pronouns, is clearly an essential aspect of the
theory, yet to the best of our knowledge, no proposals in this regard can be found in
the centering literature.
2.5 Empirical Support for, and Applications of, Centering
Centering has served as the theoretical foundation for a lot of work in linguistics,
natural language processing (NLP), and psychology. This includes annotation stud-
ies testing the claims of the theory for languages including English, German, Hindi,
Italian, Japanese, and Turkish (e.g., Kameyama 1985; Passonneau 1993; Walker, Iida,
and Cote 1994; Di Eugenio 1998; Turan 1998) and several papers in (Walker, Joshi,
and Prince (1998b). The claims about pronominalization made in centering have been
applied to develop algorithms both for anaphora resolution (Brennan, Friedman, and
Pollard 1987; Strube and Hahn 1999; Tetreault 2001) and for sentence planning (Dale
1992; Henschel, Cheng, and Poesio 2000); this work can be viewed as providing an
evaluation of claims such as Rule 1. Ideas from centering, and in particular Rule 2, are
increasingly found useful in text planning (McKeown 1985; Kibble and Power 2000;
Knott et al 2001; Karamanis 2003).
We have already seen that some predictions of the theory have also been tested
with psychological techniques. In many of these experiments, differences in process-
ing pronominal references in a sentence in a sentence to entities with different ranks
(according to a particular instantiation of the theory) were observed: Hudson, for ex-
ample, observed that pronominal references to entities introduced in subject position
in the previous sentence are interpreted more quickly than nonpronominal references
or references to nonsubjects (Hudson, Tanenhaus, and Dell 1986; Hudson-D?Zmura
and Tanenhaus 1998). And we already mentioned that Gordon, Grosz, and Gillion
(1993) identified a processing time slowdown, the RNP, when NPs in subject position
referring to entities introduced in subject or first-mentioned position in the previous
sentence are not pronominalized.
However, the discussion in this section should have made it clear just how many
parameters the theory has and in how many different ways they can be instantiated.
To our knowledge, no previous study has attempted to analyze in a systematic way
how varying the instantiation of more than one of these parameters affects the claims
of the theory, especially for combinations of parameter settings not considered in the
original papers. This analysis is the goal of the work discussed here.
3. A Corpus-Based Comparison of Centering?s Instantiations
Given the many ways in which the parameters of centering can be set, the only feasible
way to make a systematic comparison between the theory?s ?instantiations? is by
computational means. That is, running computer simulations of the process of local
focus update using an annotated corpus and comparing the results obtained under
different instantiations. The evaluation principle we use for this comparison is the
number of ?violations? of the theory?s claims resulting when the parameters are set
in a certain way (e.g., whether pronominalization choices are in accordance with Rule
321
Poesio et al Centering: A Parametric Theory
1). In this section we discuss how we set about performing such comparison, the data
we used, our annotation methods, and how the annotation was used.
3.1 Evaluating the Claims of Centering against a Corpus
A preliminary question we had to address is what are in fact the main claims of the
theory. As discussed in Section 2, of the seven claims mentioned in Grosz, Joshi, and
Weinstein (1995), Constraint 1, Rule 1, and Rule 2 are the ones that can actually be
verified using a corpus; we concentrated on these. Because several variants of these
three claims have been proposed, we evaluated a few of these variants as well.11
The second important question is how these three claims are meant to be inter-
preted and what we can expect a corpus to tell us about them. The proponents of
centering are quite clear that the theory does not state ?hard? facts about language,
that is, the kind of facts whose violation leads to ungrammaticality judgments. Con-
straint 1, Rule 1, and Rule 2 are meant to be preferences which, when followed, lead
to texts that are easier to process.12 The mere presence of a few exceptions to a claim
does not, therefore, count as a falsification. For one thing, we should expect centering
preferences to interact with other constraints (a point not emphasized enough in the
centering literature). And for another, there may be no way of expressing a particular
piece of information without violating some such preferences.13 So at best, we can
expect the three claims to be verified in a statistical sense: that is, that the number
of utterances that verify such claims will be significantly higher than the number of
utterances that violate them?and in fact, we may find that for some claims, even
statistical significance will not be achieved. As a result, we evaluated each claim using
statistical tests; the tests we used are the sign test for Constraint 1 and Rule 1, and the
Page test for Rule 2 (Siegel and Castellan 1988).
It is also important to keep in mind that a corpus cannot tell us whether ?vi-
olations? actually result in processing difficulties: This can be determined only by
behavioral studies such as reading-time experiments. So we should make it absolutely
clear that minimizing violations cannot and should not be the only deciding factor in
theorizing about centering. Nevertheless, the combinatorics of the problem make it im-
possible to compare the parameter instantiations in any other way. Furthermore, this
form of evaluation is also the most systematic way to identify other preferences and
constraints that may interact with centering. We return to these issues in Section 5.
3.2 The Data
The data used in this work are texts from the GNOME corpus, which currently in-
cludes texts from three domains. The museum subcorpus consists of descriptions of
museum objects and brief texts about the artists who produced them.14 The pharma-
ceutical subcorpus is a selection of leaflets providing patients with legally mandatory
11 In this version of the article, we assume that the CB is defined by Constraint 3. For the results with
alternative definitions of CB, see the extended technical report or the companion Web site.
12 Beaver (2004) argues?correctly, in our opinion?that in one of the best-known pronoun resolution
algorithms based on centering, that proposed by Brennan, Friedman, and Pollard (1987), Rule 1 is
effectively used as a hard constraint, a problem fixed by Beaver?s own optimality-theoretic
reformulation of the algorithm. It is nevertheless quite clear that in the theory, Rule 1 has the status of
a preference.
13 This point is especially important from an NLG perspective: see, e.g., Karamanis (2003). We will return
to this issue in Section 5.
14 The museum subcorpus extends the corpus collected to support the ILEX and SOLE projects at the
University of Edinburgh. ILEX generates Web pages describing museum objects on the basis of the
perceived status of its user?s knowledge and of the objects she has previously looked at (Oberlander et
al. 1998) The SOLE project ILEX with concept-to-speech abilities, using linguistic information to control
intonation (Hitzeman et al 1998).
322
Computational Linguistics Volume 30, Number 3
information about their medicines.15 The GNOME corpus also includes tutorial dia-
logues from the Sherlock corpus collected at the University of Pittsburgh (Di Eugenio,
Moore, and Paolucci 1997). Each subcorpus contains about 6,000 NPs. Texts from the
first two domains were used for the main experiments reported here. The third sub-
corpus was used for the segmentation experiments discussed in the extended technical
report available on the Web site.
The data used for this study have two characteristics that make them of particular
interest. First of all, they cover genres not previously considered in studies on centering
and more similar to those that ?real? NLP applications have to contend with. At the
same time, they are strongly entity-centered (see, e.g., Knott et al[2001] for an analysis
of the museum data), so the hypotheses about coherence formulated in centering are
likely to play an important part in the way these texts are constructed.
3.3 Annotation
The previous corpus-based investigations of centering theory we are aware of (Walker
1989; Passonneau 1993, 1998; Byron and Stent 1998; Di Eugenio 1998; Hurewitz 1998;
Kameyama 1998; Strube and Hahn 1999) were all carried out by a single annotator
annotating her or his corpus according to her or his own subjective judgment. One of
our goals was to use for this study only information that could be annotated reliably
(Passonneau and Litman 1993; Carletta 1996), as we believe this will make our results
easier to replicate. The price we paid to achieve replicability is that we couldn?t test
all proposals about the computation of centering parameters proposed in the litera-
ture, especially those about segmentation and about ranking, as discussed below. The
annotation followed a detailed manual, available on the companion Web site. Eight
paid annotators were involved in the reliability studies and the annotation. In the
following we briefly discuss the information that we were able to annotate, what we
didn?t annotate, and the problems we encountered; for more details, we refer readers
to the extended version of the article and the Web site.
A systematic comparison among different ways of setting the parameters would
be prohibitively expensive with traditional psychological methods, but it?s not easy
to do with corpus analysis, either. Obviously, it can?t be done by directly annotating
?utterances? or ?CB? according to one way of fixing the parameters, as has been done
in most previous studies of centering theory (Byron and Stent 1998; Di Eugenio 1998;
Kameyama 1998; Passonneau 1993; Walker 1989). Instead, we annotated our corpus
with the primitive concepts used by different instantiations of the theory, that is, in-
formation that has been claimed by one or the other instantiation of centering to play
a role in the definitions of its basic notions. This includes, for example, how sentences
break up into clauses and subclausal units; grammatical function; and anaphoric re-
lations, including bridging references. An automatic script uses this information to
compute utterances, their CF ranking, and their CB, according to a particular way of
setting the parameters, and to compute statistics relevant to the three claims according
to that instantiation.
3.3.1 Utterances. In order to evaluate the definitions of utterance proposed in the
literature (sentences versus finite clauses), as well as the different proposals concerning
the ?previous utterance? discussed above, we marked all spans of text that might be
claimed to update the local focus. This includes sentences (defined as all units of text
15 The leaflets in the pharmaceutical subcorpus are a subset of the collection of all patient leaflets in the
United Kingdom, which was digitized to support the ICONOCLAST project at the University of
Brighton, developing tools to support multilingual generation (Scott, Power, and Evans 1998).
323
Poesio et al Centering: A Parametric Theory
ending with a period, a question mark, or an exclamation point) as well as what we
called (discourse) units. Units include clauses (defined as sequences of text containing
a verbal complex, all its obligatory arguments, and all postverbal adjuncts) as well as
other sentence subconstituents that might independently update the local focus, such
as parentheticals, preposed prepositional phrases (PPs), and (the second element of)
coordinated VPs.16
Sentences have one attribute, stype, specifying whether the sentence is declarative,
interrogative, imperative, or exclamative. The attributes of units include
? utype: whether the unit is a main clause, a relative clause, an appositive,
a parenthetical, etc.
? verbed: whether the unit contains a verb or not.
? finite: for verbed units, whether the verb is finite or not.
Marking up sentences proved to be quite easy; marking up units, on the other hand,
required extensive annotator training. The agreement on identifying the boundaries
of units, using the kappa statistic discussed in Carletta (1996), was ? = .9 (for two
annotators and 500 units); the agreement on features (two annotators and at least 200
units) was as follows: utype: ? = .76; verbed: ? = .9; finite: ? = .81. In total, the texts
used for the main study contain 505 sentences and more than 1,000 units, including
900 finite clauses
3.3.2 NPs. Our instructions for identifying NP markables derive from those proposed
in the MATE scheme for annotating anaphoric relations (Poesio, Bruneseaux, and Ro-
mary 1999), in turn derived from DRAMA (Passonneau 1997) and MUC-7 (Chinchor
and Sundheim 1995). In the GNOME corpus, NPs are marked by <ne> (?Nominal en-
tity?) elements. In total, the texts used for this study contain 3,345 NPs. These include
586 pronouns, among which are 217 third-person personal and possessive pronouns,
23 demonstratives, and 308 second-person pronouns; 1,290 definite NPs, including 554
the-NPs, 250 possessive NPs, and 391 proper nouns; 1,119 indefinite NPs, including
745 bare NPs and 269 a-NPs; and 350 other NPs, including 117 quantified NPs and
114 coordinated NPs.
We annotated 14 attributes of <ne> elements specifying their syntactic, semantic,
and discourse properties (Poesio 2000). Those relevant to the study discussed here
include the following:
? The np type, cat, with values such as a-np, that-np, the-np, and pers-pro.
? The agreement features num, per, and gen, used to identify contexts in
which the antecedent of a pronoun could be identified unambiguously.
? The grammatical function gf. Our instructions for this feature are derived
from those used in the FRAMENET project (Baker, Fillmore, and Lowe
1998); see also http://www.icsi.berkeley.edu/?framenet/. The values are
subj, obj, predicate (used for postverbal objects in copular sentences, such
as This is (a production watch)), there-obj (for postverbal objects in
there-sentences), comp (for indirect objects), adjunct (for the argument of
PPs modifying VPs), gen (for nps in determiner position in possessive
NPs), np-compl, np-part, np-mod, adj-mod, and no-gf (for nps occurring by
themselves?eg., in titles).
16 Our instructions for marking up such elements benefited from the discussion of clauses in Quirk and
Greenbaum (1973) and from Marcu?s (1999) proposals for discourse units annotation.
324
Computational Linguistics Volume 30, Number 3
The agreement values for these attributes are as follows: cat: .9; gen: .89; gf: .85; num:
.84; per: .9. We encountered problems even with supposedly ?easy? information such
as number and gender, but especially so with semantic attributes (see the annotation
manual available on the Web site). We were, however, able to mark up the attributes
relevant for this study in a reliable fashion. One exception is that we weren?t able
to reach acceptable agreement on a feature of NPs often claimed to affect ranking,
thematic roles (Sidner 1979; Cote 1988; Stevenson, Crawley, and Kleinman 1994); the
agreement value in this case was ? = .35. As a result, we were not able to evaluate
ranking functions based on thematic roles.
3.3.3 Anaphoric Information. In order to determine whether a CF of an utterance is
realized directly or indirectly in the following utterance, it is necessary to annotate
the anaphoric relations that CFs enter into, including both identity relations and, in
order to compute indirect realization, associative relations (Hawkins 1978). This type
of annotation raises, however, a number of difficult (and sometimes unresolved) se-
mantic issues (Poesio 2004). As part of the MATE and GNOME projects, an extensive
analysis of previously existing schemes for so-called ?coreference annotation,? such
as the MUC-7 scheme, was carried out, highlighting a number of problems with such
schemes, ranging from issues with the annotation methodology to semantic issues.
Although some of these schemes, like DRAMA, allow the marking of associative re-
lations, none of them analyze which among such relations can be reliably annotated
(Poesio, Bruneseaux, and Romary 1999; Poesio 2000). The semantic problems with
these schemes include the inappropriate use of the term coreference to cover semantic
relations such as that between an intensional entity like the temperature that may take
different values at different time points and these values (as in the price of aluminum
siding rose from $3.85 to $4.02) or between a quantifier and a variable the quantifier
binds, in which neither may corefer (as in none of the meetings resulted in an agreement
between its participants (van Deemter and Kibble 2000; Poesio 2004). In MATE, a general
scheme was developed which includes a fine-grained repertoire of semantic relations,
such as binding and function value (Poesio, Bruneseaux, and Romary 1999). For the
GNOME corpus, we adopted a simplified version of the MATE scheme, as for our
purposes it?s not essential to mark all semantic relations between entities introduced
by a text, but only those that may establish a ?link? between two utterances. So for
example, it is in general unnecessary in our case to mark a relation between the subject
of a copular sentence and its predicate (e.g., between the price of aluminum siding and
either $3.85 or $4.02 in the example above). Also, our texts do not include any case
of bound anaphora, so it was not necessary to offer this option to our annotators.
In the GNOME corpus, anaphoric information is marked by means of a special
?ante? element; the ?ante? element itself specifies the index of the anaphoric expression
(a ?ne? element) and the type of semantic relation (e.g., identity), whereas one or more
embedded ?anchor? elements indicate possible antecedents.17 (See (8).)
(8) <unit finite=?finite-yes? id=?u227?>
<ne id=?ne546? gf=?subj?> The drawing of
<ne id=?ne547? gf=?np-compl?>the corner cupboard </ne></ne>
<unit finite=?no-finite? id=?u228?>, or more probably
<ne id=?ne548? gf=?no-gf?> an engraving of
<ne id=?ne549? gf=?np-compl?> it </ne></ne>
17 The presence of more than one ?anchor? element indicates that the anaphoric expression is ambiguous.
325
Poesio et al Centering: A Parametric Theory
</unit>,
.
.
.
</unit>
<ante current="ne549" rel="ident"> <anchor ID="ne547"> </ante>
Work such as Sidner (1979) and Strube and Hahn (1999) as well as our own prelimi-
nary analysis, suggested that indirect realization can play a crucial role in maintaining
the CB. However, previous attempts at marking anaphoric information, particularly
in the context of the MUC initiative, suggested that while it?s fairly easy to achieve
agreement on identity relations, marking up associative relations is quite hard; this
was confirmed by studies such as Poesio and Vieira (1998). For these reasons, and to
reduce the annotators? work, we marked only a few types of relations, and we specified
priorities. Besides identity (IDENT), we marked up only three associative relations: set
membership (ELEMENT), subset (SUBSET), and ?generalized possession? (POSS), which
includes part-of relations as well as ownership relations. We marked only relations
between objects realized by noun phrases and not, for example, anaphoric references
to actions, events, or propositions implicitly introduced by clauses or sentences. We
also gave strict instructions to our annotators concerning how much to mark. (See
the annotation manual available on the Web site.) Furthermore, we specified prefer-
ences: For example, in Francois, the Dauphin, the embedding NP would be chosen as
an antecedent, rather than the NP in appositive position.
As expected, we found a reasonable (if not perfect) agreement on identity relations.
In our most recent analysis (two annotators marking the anaphoric relations between
200 NPs) we observed no real disagreements; 79.4% of the relations were marked up
by both annotators; 12.8% by only one of them; and in 7.7% of the cases, one of the
annotators marked up a closer antecedent than the other.18 With associative references,
limiting the relations did curtail the disagreements among annotators (only 4.8% of the
relations are actually marked differently), but only 22% of bridging references were
marked in the same way by both annotators; 73.17% of relations were marked by only
one or the other annotator. So reaching agreement on this information involved several
discussions between annotators and more than one pass over the corpus (Poesio 2000).
3.3.4 Segmentation. According to Grosz and Sidner (1986), centering is meant to cap-
ture preferences only within discourse segments. A proper evaluation of the claims of
the theory would therefore require a corpus in which discourse segments have been
identified. Unfortunately, discourse segments are difficult to identify reliably (Passon-
neau and Litman 1993), and Grosz and Sidner (1986) do not provide a specification
of discourse intentions explicit enough that it can be used to identify the intentional
structure of texts?which, according to Grosz and Sidner, determines their segmenta-
tion. As a result, only preliminary attempts at annotating texts according to Grosz and
Sidner?s theory have been made.
For this reason, most previous corpus-based studies of centering either ignored
segmentation or used heuristics such as those proposed by Walker (1989): Consider
every paragraph as a separate discourse segment, except when its first sentence con-
tains a pronoun in subject position or a pronoun whose agreement features are not
18 In previous work (Poesio and Vieira 1998) we came to the conclusion that kappa, while appropriate
when the number of categories is fixed and relatively small, is problematic for anaphoric reference,
when neither condition applies, and may result in inflated values of agreement.
326
Computational Linguistics Volume 30, Number 3
matched by any other CF in the same sentence. We tested only heuristic methods as
well, using the layout structure of the texts as a rough indicator of discourse structure.
In this article we discuss only the results with the heuristic proposed by Walker. In
the extended technical report available on the Web site, we discuss the results with
other segmentation heuristics, as well as further results with the tutorial dialogues
subdomain of the GNOME corpus, independently annotated according to relational
discourse analysis (Moser and Moore 1996), a technique inspired by Grosz and Sid-
ner?s proposals, from which a Grosz and Sidner?like segmentation was extracted as
proposed in Poesio and Di Eugenio (2001).
3.4 Automatic Computation of Centering Information
The annotated corpus is used by Perl scripts that automatically compute the center-
ing data structures (utterances, CFs, and CBs) according to the particular parameter
instantiation chosen, find violations of Constraint 1, Rule 1, and Rule 2 (according to
several versions of Rule 1 and Rule 2), and evaluate the claims using the statistical
tests. The behavior of the scripts is controlled by a number of parameters, including
CBdef: which definition of CB should be used. (All the results discussed in this
article were computed using the definition in Constraint 3.)
uttdef: identify utterances with sentences, finite clauses, or verbed clauses.
previous utterance: treat adjunct clauses Kameyama-style or Suri and McCoy-
style.
realization: allow only direct realization, or allow indirect realization as well.
CF-filter: treat all NPs as introducing CFs, or exclude certain classes. At the mo-
ment it is possible to omit first- and second-person NPs and/or NPs in
predicative position (e.g., a policeman in John is a policeman).
rank: rank CFs according to grammatical function, linear order, a combination of
the two as in Gordon, Grosz, and Gillion (1993), or information status as
in Strube and Hahn (1999).
prodef: consider as R1-pronouns only third-person personal pronouns (it, they),
or include also demonstrative pronouns (that, these), and/or the second-
person pronoun (you).
segmentation: identify segments using Walker?s heuristics, or with paragraphs,
sections, or whole texts.
Among the many other script parameters whose effect will not be discussed here we
will just mention those that determine whether implicit anaphors in bridging refer-
ences should be treated as CFs, the relative ranking of entities in complex NPs, and
how to handle ?preposed? adjunct clauses. (See the extended technical report on the
Web site.) The algorithm used to compute statistics concerning violations of the claims
is fairly straightforward, and we will therefore omit it here; the interested reader can
find a discussion in the extended technical report. The one additional complication that
we need to mention here is relative pronouns. As it could be argued that the decision
to generate a relative pronoun is primarily controlled by grammatical considerations,
we attempted to ignore them as much as possible, in the following sense. Our scripts
do not count an utterance as a violation/verification of Rule 1 from Grosz, Joshi, and
Weinstein (1995) if the only ?pronoun? realizing a non-CB is a relative pronoun, or
the CB is realized only by a relative pronoun. What this means in practice is that the
327
Poesio et al Centering: A Parametric Theory
Table 1
Number of utterances and CFs with the vanilla instantiation.
Museum Pharmaceutical Total
Number of utterances 430 577 1,007
Number of utterances that are segment boundaries 91 134 225
Number of CFs 1,731 1,308 3,039
number of utterances examined to evaluate Rule 1 is generally less than the number
of utterances with a CB, as we will see shortly.
4. Main Results
Given the number of parameters, it is difficult, if not impossible, to discuss the results
with all instantiations. Instead, we begin by discussing the results with what we call
the ?vanilla instantiation,? based on the settings for the parameters most often used
in discussions of the theory. We then examine the results obtained by varying the
definitions of utterance and realization. After establishing the ?best? values for these
parameters, we consider the effect of alternative ranking functions. Additional results
are discussed in the extended technical report available on the companion Web site.
Readers who want to try out instantiations not discussed here should visit the Web
site.
4.1 The Vanilla Instantiation
What we call the ?vanilla instantiation? is not an instantiation actually proposed in
the literature, but an attempt to come as close as possible to a ?mainstream? instanti-
ation of centering by blending proposals from Grosz, Joshi, and Weinstein (1995) and
Brennan, Friedman, and Pollard (1987) and incorporating additional suggestions from
Kameyama (1998) and Walker, Joshi, and Prince (1998a). The vanilla instantiation is
based on the definition of CB from Grosz, Joshi, and Weinstein (1995) and uses gram-
matical function for ranking, as proposed there and in Brennan, Friedman, Pollard
(1987). Because Grosz et al do not provide a definition of utterance, the vanilla instan-
tiation incorporates the hypothesis from Kameyama (1998) that utterances are finite
clauses and the characterization of ?previous utterance? proposed there.19 Concerning
realization, in the vanilla instantiation only third-person NPs introduce CFs, and a
discourse entity counts as ?realized? in an utterance only if it is explicitly mentioned.
For the purposes of Rule 1, we mainly studied a ?strict? definition of R1-pronoun
including only personal (and possessive) pronouns and relative pronouns and traces
(see Walker, Joshi, and Prince [1998a], page 4), but we also considered a ?broader?
definition including the demonstrative pronouns this, that, these, and those. Relative
clauses are assumed to include a link to the NP they modify, possibly not explicitly
realized. The segmentation heuristic proposed by Walker (1989) is adopted. With the
parameters set in this way, the number of utterances and CFs in our corpus is as
shown in Table 1.
19 We simplified Kameyama?s hypothesis about relative clauses by considering only instantiations in
which they were treated as utterances both ?locally? and ?globally?, and ones in which they weren?t.
328
Computational Linguistics Volume 30, Number 3
Table 2
Utterances and CBs with the vanilla instantiation.
Museum Pharmaceutical Total Percentage
Number of times at least one CF(Un) is realized in Un+1 195 162 357 (35.4%)
Utterances that satisfy Constraint 1 (have exactly one CB) 189 157 346 (34.4%)
Utterances with more than one CB 6 5 11 (1.1%)
Utterances without a CB but are segment boundary 67 96 163 (16.2%)
Utterances without a CB 168 319 487 (48.4%)
4.1.1 Constraint 1. The statistics relevant to Constraint 1 (that utterances have exactly
one/at most one CB) are shown in Table 2. This table clearly indicates that the weak
version of Constraint 1 (Weak C1) is likely to be verified with the vanilla instantiation.
Even without counting segment boundaries, Weak C1 is verified by 833 utterances
out of 1,007 (82.7%) and violated by only 11 (1.1%): The chance that Weak C1 will
not hold with a different sample is p ? 0.001 by the sign test. (We will henceforth
write +833, ?11 to indicate numbers of verifiers and violators.) On the other hand,
the strong version of C1?that every utterance has exactly one CB?is not likely to
hold with this instantiation: In our corpus, only 346 utterances (34.4%) have exactly
one CB, whereas 498 utterances (49.4%) have zero or more than one CB. With +346,
?498, the chance of error in rejecting the null hypothesis that Strong C1 doesn?t hold
is obviously much higher than 10%. The chance of error doesn?t go below 10% even
if we count the 163 utterances that do not contain references to CFs introduced in the
previous utterance but are segment boundaries and therefore are not governed by the
constraint. In other words, if the vanilla instantiation were the ?right? way of setting
the parameters, we would have to conclude that in the genres contained in our corpus,
utterances are very likely to have a unique CB, but entity coherence does not play a
major role in ensuring that a text is coherent: only 35.4% of utterances in our corpus
would be ?entity-coherent,? that is, would contain an explicit mention to an entity
realized in the previous finite clause.
The following example illustrates why there are so many violations of Strong
C1 with the vanilla instantiation. If we identify utterances with finite clauses, the
two sentences in (9) break up into five utterances, and only the last of these can be
considered in any sense to directly refer to the set of egg vases introduced in (u1).20
(9) (u1) These ?egg vases? are of exceptional quality: (u2) basketwork bases
support egg-shaped bodies (u3) and bundles of straw form the handles,
(u4) while small eggs resting in straw nests serve as the finial for
each lid. (u5) Each vase is decorated with inlaid decoration: . . .
Clearly, there are two ways of ?fixing? this problem. One is to claim that utterances
are best identified with sentences, in which case we would have only two utterances
in this example, one for each sentence. The other is to allow for indirect realization:
20 In fact, the anaphoric relation here is not identity; rather, the set of egg vases serves as domain
restriction for the quantifier in (u5). We were not able to mark this distinction reliably.
329
Poesio et al Centering: A Parametric Theory
(u2)?(u4) all contain implicit references to the egg vases, and therefore all will have a
CB if indirect realization is allowed. Both possibilities are considered below.
The fact that 11 utterances (1.1%) in the corpus have more than one CB (i.e., they
violate Weak C1 as well) is also worth noticing. The reason for the violation is that
in ?classic? centering, ranking is only required to be a partial order (see, e.g., Walker,
Joshi, and Prince 1998a page 3),21 so when two CFs with the same rank in Ui are both
realized in Ui+1, both become the CB. This is illustrated in (10), where we show the
XML markup so that the attributes of elements are visible:
(10) <unit finite=?finite-yes? id=?u227?>
<ne id=?ne546? gf=?subj?>The drawing of
<ne id=?ne547? gf=?np-compl?>the corner cupboard</ne></ne>
<unit finite=?no-finite? id=?u228?>, or more probably
<ne id=?ne548? gf=?no-gf?> an engraving of
<ne id=?ne549? gf=?np-compl?> it </ne></ne>
</unit>,
must have caught
<ne id=?ne550? gf=?obj?>
<ne id=?ne551? gf=?gen?>Branicki?s </ne> attention</ne>
</unit>
<unit id="u229" finite="finite-yes">
<ne gf="subj" id="ne552">Dubois</ne> was commissioned through
<ne gf="adjunct" id="ne553"> a Warsaw dealer </ne>
<unit id="u230" finite="finite-no"> to construct
<ne gf="obj" id="ne554"> the cabinet </ne>
for<ne gf="adjunct" id="ne555">the Polish aristocrat</ne>
</unit>
</unit>
<ante current=?ne554? rel=?ident?><anchor antecedent=?ne549?>
</anchor></ante>
<ante current=?ne555? rel=?ident?><anchor antecedent=?ne551?>
</anchor></ante>
In this example, two discourse entities introduced in utterance (u227) are realized in
utterance (u229):22 the corner cupboard (realized in (u227) by (ne547) and (ne549) and in
(u229) by (ne554)) and Branicki (realized in (u227) by (ne551), and in (u229) by (ne555)).
As their grammatical functions are equivalent under the ranking proposed by Grosz
et al, (np-compl, for np-complement, and gen, for genitive?see the annotation manual
available on the Web site), these two CFs have the same rank in (u227), so they are both
CBs of (u229). The same problem occurs with coordinated NPs, both of which have
the same grammatical function. This problem with the vanilla instantiation can also
be ?fixed? by requiring the ranking function to be a total order, which is easily done
by adding a disambiguation factor such as linear order, as done by Strube and Hahn.
On the other hand, the requirement that ranking be total has not been previously
discussed in the centering literature, and one might argue conversely that examples
such as the one above are arguments againts centering?s claim that utterances have
only one CB. We return to this issue in Section 5.
21 It?s not clear to us why ranking is required only to be partial, yet the CB is clearly claimed to be unique.
22 Neither (u228) nor (u230) is treated as an utterance as they are not finite.
330
Computational Linguistics Volume 30, Number 3
Table 3
Transition statistics for the Brennan et al version of Rule 2.
Museum Pharmaceutical Total (Percentages)
Establishment 96 93 189 (18.8%)
Continuation 37 33 70 (7.0%)
Retain 22 16 38 (3.8%)
Smooth Shift 22 15 37 (3.7%)
Rough Shift 18 5 23 (2.3%)
ZERO 87 81 168 (16.7%)
NULL 148 334 482 (47.9%)
Total 430 577 1,007
4.1.2 Rule 2. The statistics relevant for Brennan et al?s version of Rule 2 are shown
in Table 3. The most obvious consideration suggested by this table is that the three
most frequent transitions in our corpus are ones that either have not been previously
discussed in the Centering literature or have been discussed only in a limited way. By
far the most frequent transition (47.9% of the total) is NULL: following up an utterance
without a CB with a second one, also without a CB. (Examples include (u3), (u4), and
(u5) in (9).) We found this transition discussed only in Passonneau (1998). The second
most common transition (18.8%) is Kameyama?s Establishment (the transition between
an utterance without a CB and one with a CB), followed by its reverse, the ZERO
transition (between an utterance with a CB and one without), never mentioned in the
literature. (An example of ZERO is (u2) in (9).) If we ignore NULL, Establishments,
and ZEROs, the preferences are roughly as predicted by Brennan et al: The Page test
for ordered alternatives (Siegal and Castellan 1988, pages 184?188) indicates a chance
of less than .001 that the four transitions are equally likely. But only the differences
between CON and RET/SSH, and between SSH and RSH, are significant; and there
are more Shifts (SSH + RSH) than Retains.23
Grosz et al?s formulation of Rule 2 in terms of sequences also roughly holds,
except that there are too few sequences for the results to be considered significant,
as shown in Table 4. As we?ll see again in Section 5, in our corpus there seems to
be a preference for avoiding repetition; this tendency is confirmed by the numbers in
the table, which indicate a dispreference for maintaining the same CB for too long or
for maintaining it in the most salient position, at least at the level of finite clauses:
EST/CON sequences are twice as common as sequences of Continuations. As for
the claim that Retaining transitions prepare for Shifts, the figures do not lend much
support to the idea: Retains are more frequently followed by Continuations than by
Shifts, and almost as frequently by other Retains.
Of the other formulations of Rule 2, the version based on a preference for cheap
transition pairs over expensive ones proposed by Strube and Hahn is not verified with
the ranking function used in the vanilla instantiation?which is not, we should em-
23 Similar results were obtained by Passonneau (1998).
331
Poesio et al Centering: A Parametric Theory
Table 4
Rule 2 statistics considering sequences of transitions.
Museum Pharmaceutical Total
Continuation Sequences 10 6 16
Continuation/Retain 9 3 12
Establishment/Continuation 17 18 35
Retain Sequences 15 3 8
Retain/Continuation 7 7 14
Retain/Smooth Shift 3 2 5
Retain/Rough Shift 4 1 5
Smooth Shift Sequences 2 1 3
Rough Shift Sequences 2 1 3
Null Sequences 95 228 323
Other 290 312 602
Table 5
Cheap and expensive transitions with the vanilla instantiation.
Museum Pharmaceutical Total
Cheap transitions 76 63 139
Expensive transitions 263 380 643
Cheap transition pairs 21 14 35
Expensive transition pairs 162 234 396
phasize, the one assumed by Strube and Hahn themselves.24 Ignoring the 225 segment
boundary utterances, we find 396 pairs of expensive transitions and 35 pairs of cheap
transitions, as shown in Table 5. These statistics mean that in only 139 cases out of
357 (the total number of entity-coherent utterances with this instantiation; see Table 2),
CB(Ui) is predicted by CP(Ui?1). We do find that 219 utterances, the majority (61.3%)
of entity-coherent ones, are ?salient? in the sense of Kibble (2001)?that is, their CB is
the same as their CP.
4.1.3 Salience and Pronominalization. The statistics for pronominalization are shown
in Table 6. As noted above, our corpus contains 217 uses of third-person pronouns, 23
demonstratives, and 78 complementizers.25 In this instantiation we take R1-pronouns
to include only personal pronouns and complementizers, for a total of 295 R1-pronouns.
If we identify utterances with finite clauses, 61 personal pronouns (28.1%) have their
24 Similar results were found for dialogues by Byron and Stent (1998).
25 We will use the term complementizer to indicate relative pronouns and relative traces.
332
Computational Linguistics Volume 30, Number 3
Table 6
R1-pronouns in the corpus with the vanilla instantiation.
Museum Pharmaceutical Total
Total number of R1-pronouns 200 95 295
Number of personal pronouns 144 73 217
Number of complementizers 56 22 78
Table 7
cbs and pronominalization with the vanilla instantiation.
Museum Pharmaceuticla Total (Percentage)
Total number of realizations of CBs 211 163 374
Total number of CBs realized as R1-pronouns 138 68 206 (55.1%)
CBs realized as personal pronouns 85 48 133 (35.6%)
CBs realized as complementizers 53 20 73 (19.5%)
CBs not realized as R1-pronouns 73 95 168 (44.9%)
Total number of R1-pronouns that do not realize CBs 58 23 81 (27.5%)
Personal pronouns that do not realize CBs 55 21 76 (35.0%)
Complementizers that do not realize CBs 3 2 5 (6.4%)
antecedent in the same utterance, and 28 (13%) are ?long-distance pronouns? (Hitze-
man and Poesio 1998) whose antecedent is in neither the same nor the previous utter-
ance.
Table 7 shows that the relation between pronominalization and CB with the vanilla
instantiation is not straightforward: Only 55.1% of the 374 mentions of CBs26 are
pronominalized. And if relative clause complementizers were not included among
the R1-pronouns (on the grounds that the decision to use a complementizer is primar-
ily dictated by grammatical, rather than discourse, considerations), more CBs would
be realized as non-R1 pronouns (171, 44.9%) than as R1-pronouns (137, 35.9%). On the
other hand, 73% of R1-pronouns do refer to the CB.27
Table 8 analyzes pronominalization in terms of the three versions of Rule 1 we
are considering.28 Given the figures in Table 7, it should already be clear that the
stronger version of Rule 1 we considered, always pronominalize the CB?generalizing
the proposal by Gordon, Grosz, and Gillion (1993) to the less restrictive definition of
CB given by Constraint 3?is not verified: In fact, 55% of utterances violate it. The two
other versions of Rule 1 we are considering, however?Rule 1 (GJW 83), pronominalize
26 Even though only 357 utterances have a CB with this instantiation, a CB may be realized more than
once in an utterance.
27 Earlier versions of these findings led to the development of the pronominalization algorithm in
Henschel, Cheng, and Poesio (2000).
28 As discussed in Section 3, what is counted here are utterances that verify or violate Rule 1. Not all
utterances are considered: of the 346 utterances that have exactly one CB, 72 are ignored by the script
in that the only realization of an R1-pronoun is done via a relative pronoun or trace, so only 274 (27.2%
of the total number of utterances) are considered relevant for Rule 1.
333
Poesio et al Centering: A Parametric Theory
Table 8
Evaluation of the different versions of Rule 1 with the vanilla instantiation.
Museum Pharmaceutical Total (Percentage)
GJW 95?utterances that satisfy 130 135 265 (96.7%)
GJW 95?utterances that violate 7 2 9 (3.3%)
GJW 83?utterances that satisfy 117 105 222 (81%)
GJW 83?utterances that violate 20 32 52 (19.0%)
Gordon?utterances that satisfy 77 45 122 (44.5%)
Gordon?utterances that violate 60 92 152 (55.5%)
the CB if it?s the same as the CB of the previous utterance, and especially Rule 1 (GJW
95), pronominalize the CB if anything else is?are verified by most utterances.
There are two classes of violations of Rule 1 (GJW 95): possessive pronouns and
pronouns referring to ?global topics?. In (11), CB(u3), PRODUCT-X, is realized as a
proper noun, whereas a possessive pronoun is used to refer intrasententially to the
baby.29
(11) (u1) Infants and children must not be treated continuously with
PRODUCT-X for long periods
(u2) because it may reduce the activity of the adrenal glands, and so
lower resistance to disease.
(u3) Similar effects on a baby may occur after extensive use of
PRODUCT-X by its mother during the last weeks of pregnancy
(u4) or when she is breastfeeding the baby.
In the pharmaceutical leaflets, several violations of Rule 1 are found toward the end
of text, when pronouns are sometimes used to realize the product described by the
leaflet. For example, it in the following example refers to the cream discussed by the
leaflet, not mentioned in the previous two utterances.
(12) (u1) A child of 4 years needs about a third of the adult amount. (u2) A
course of treatment for a child should not normally last more than five
days (u3) unless your doctor has told you to use it for longer.
What we seem to observe here is a conflict between the ?global? preference to real-
ize the ?main character? of a discourse as a pronoun, and the ?local? preference to
pronominalize the locally most salient entity, as identified by the CB.30 By the end of
a leaflet, the product discussed in the leaflet has been mentioned a number of times,
so that it is salient enough to justify pronominalization even when it is not in CF list.
We saw in Table 7 that although there are only 9 violations of Rule 1 from Grosz,
Joshi, and Weinstein (1995), 81 R1-pronouns do not realize CBs. The majority of the
29 The problem of intrasentential pronouns in Centering is discussed, e.g., in Walker (1989), Tetreault
(2001), and Poesio and Stevenson (forthcoming).
30 See also Giouli (1996) and Byron and Stent (1998).
334
Computational Linguistics Volume 30, Number 3
72 cases of pronouns that do not refer to the CB but do not violate Rule 1 fall into
two classes: (1) R1-pronouns used in utterances without a CB (the majority) and (2)
R1-pronouns used in utterances in which the CB is pronominalized as well?as in the
following example, in which both the microscope and the amateur scientist are realized
(by a personal pronoun and a relative trace) in the relative clause (u2):
(13) (u1) This microscope belonged to an amateur scientist,
(u2) who would have used it to explore the mysteries of the natural
world.
82.6% of demonstrative pronouns in the corpus do not realize the CB, which is what
one would expect on the basis of, for example, Gundel, Hedberg, and Zacharski (1993),
and Passonneau (1993). This suggests that treating demonstrative pronouns as R1-
pronouns would not lead to improvements with respect to Rule 1. On the other hand,
because there are only 23 demonstrative pronouns in the corpus, such a change would
be unlikely to drastically affect the results. And indeed, with a broader definition of
R1-pronoun that includes demonstrative pronouns, we find a few more violations of
Rule 1 (GJW 95) (11 instead of 9) and a few less violations of Rule 1 (Gordon et al) (148
instead of 152) and of Rule 1 (GJW 83) (50 instead of 52), but none of these differences
is significant. The results reported in the rest of the article are all obtained with the
?narrow? definition of R1-pronoun that does not include demonstratives.31
4.1.4 Differences Among the Domains. The texts in the museum domain seem to be
more in agreement with the predictions of the theory than the texts in the pharmaceu-
tical domain. This is especially the case for Rule 1. There are fewer personal pronouns
in the texts in the pharmaceutical domain (73 of 1,308 CFs, [5%], as opposed to 144
of 1,731 [8%] for the museum domain), and whereas in the museum domain 40.3%
(85/211) of CB realizations are done via personal pronouns (65.4% if we consider all
R1-pronouns), in the pharmaceutical domain only 29.4% (48/163) are (41.7% for R1-
pronouns). The percentage of utterances satisfying the strong version of Constraint
1 is much higher in the museum domain (44.0%, 189/430) than in the pharmaceuti-
cal one (27.2%, 157/577), and the percentage of utterances with no CB that are not
segment boundaries is much higher in this second domain (55.3%, 319/577) than in
the first (39.1%, 168/430). Finally, almost 72% of utterances in the pharmaceutical do-
main are NULL or ZERO transitions (415/577), whereas just 54.6% are in the museum
domain (235/430); the percentage of EST and CON is also higher in the museum
domain (133/430, 31%, versus 126/577 [21.8%]). These differences are in part due to
the large number of second-person pronouns you in the pharmaceutical domain, so
that the statistics for Constraint 1 improve if we treat the entities referred to by these
pronouns as CFs, as we will see below. A second reason for these differences is that
layout plays a much more important role in the pharmaceutical domain, providing a
different way of achieving coherence. (See Section 5.)
4.2 Varying the Utterance Parameters
We now begin to explore alternative parameter settings. As always, space constraints
prevent a full discussion of all the instantiations. In this article, we discuss the results
with most of the variants quite briefly and analyze at some length only the instantiation
31 The relation between demonstrative NPs in general and the CB in our corpus is analyzed in detail in
Poesio and Nygren-Modjeska (2003).
335
Poesio et al Centering: A Parametric Theory
that identifies utterances with sentences; the results are summarized with graphs.
The extended technical report available on the companion Web site contains a more
extensive discussion of some of the variants; interested readers are also encouraged
to try further instantiations on the Web site. In this subsection we consider how the
definition of utterance (parameter uttdef) and the value of the parameter previous
utterance affect the claims.
4.2.1 Treating Coordinated Verb Phrases as Utterances. Many researchers working
on spoken dialogues or NLG assume that each element of a coordinated VP counts
as a separate utterance: that is, that in We should send the engine to Avon and hook it to
the tanker car, the coordinate VP hook it to the tanker car is actually a separate utterance.
Treating coordinated VPs as separate utterances in our corpus would of course result
in more utterances (1,041 vs. 1,007), which of course would lead to worse results unless
these utterances were treated as containing an implicit trace. If we do so, we obtain
slightly (but significantly) better results for Strong C1 (48% violations instead of 49%),
and nonsignificant differences for Rule 1 and Rule 2 (with slightly higher numbers of
Continuations and slightly lower number of Retains).
4.2.2 Using All Verbed Clauses Instead of Just the Finite Ones. A second extension
of the definition of utterance is to treat as utterances all clauses with a verb, including,
for example, the infinitival to-clause in John wants to visit Bill. The results with such an
instantiation, as well, crucially depend on our grammatical assumptions. With such
a definition, we get of course many more utterances (1,267 instead of 1,007), most of
which, like the example infinitival clause just given, do not contain explicit mentions
of the argument in subject position; so again, if we didn?t assume that traces are
present in such clauses, we would find significantly more violations of Strong C1 (685
instead of 498). Using a crude mechanism for tracking traces (adding a trace referring
to the subject of the matrix clause to all nonfinite complement clauses), we still find a
larger number of violations (598) than with the vanilla instantiation, but because the
number of utterances is much greater, these violations represent a significantly lower
percentage of the total (47% instead of 49%). We find no significant differences in the
number of violations of Rule 1. As for Rule 2, this change results in significantly fewer
NULL transitions (45.0% instead of 47.9%) and significantly more EST (22.1% instead
of 18.8%) and SSH (5.6% instead of 3.7%).
4.2.3 Restricting Finite Clauses. In general, the best results for C1 are obtained by
considering larger chunks of text as a single utterance, thus reducing the number of
utterances. In particular, fewer violations are obtained by not considering as utter-
ances finite clauses that occur as parentheticals, as subjects (as in That John could do this
to Mary was a big surprise to me), and as matrix clauses with an empty subject (as in
It is likely that John will arrive tomorrow). This merging only reduces the overall num-
ber of utterances from 1,007 to 972, but the result is a simultaneous reduction in the
violations of Strong C1 from 498 to 469 (48.2%) (which is significant by the binomial-
proportions test, though still not enough for Strong C1 to be verified) and an increase
in the number of utterances that satisfy Rule 1 (GJW 95) to 281. The violations of Rule
1 are also reduced to 8 (2.8%) (not significant). (There are virtually no changes for Rule
2.) Because of these small improvements, in the rest of the article we always exclude
these occurrences when discussing the results with finite clauses as utterances; we
refer to this instantiation as ?vanilla??.
336
Computational Linguistics Volume 30, Number 3
4.2.4 Relative Clauses. Relative clauses turned out to be one of the most complex
problems we had to face. The reader may recall that Kameyama tentatively proposes
(without empirical support) that relative clauses have a ?mixed? status: They should
be locally treated as updating the local focus, but at the global level they should
be merged with the embedding utterance. This proposal, however, seems to assume
that the local focus may be updated with the content of certain utterances some time
after they have been first processed, which is a rather radical change to the basic
assumptions of the framework. In this study, we simply compared an instantiation
in which relative clauses are treated as utterances with one in which they are not. In
addition, we considered treating relative clauses as adjuncts (i.e., as not embedded) and
treating them as complements (embedded).32 The figures reported so far were obtained
by treating relative clauses as utterances and as akin to adjuncts.33 Not treating relative
clauses as separate utterances results in a 6.5% reduction in the number of utterances
with respect to vanilla? (908 instead of 972) and in fewer violations of Strong C1 (452
[439 utterances without a CB, 13 with two CBs] instead of 469 [457 and 12]); however,
the percentage of violations is higher (49.7% vs. 48.2%). The number of violations of
Rule 1 also stays the same (8 [2.7%]). From the point of view of Rule 2, a lot of relative
clauses seem to function as EST, since their number goes down by almost 15%, to
17.3% (from 190 to 157); we also see a 30% reduction in SSH and an increase in NULL,
to 50.6% of the total. Everything else stays the same.
In purely numerical terms, then, not treating relative clauses as separate utterances
would not improve the results. Furthermore, and most important, we feel that not
treating finite relative clauses as separate utterances would make it very difficult to
maintain the principle that utterances are identified with finite clauses. For these
reasons, in the rest of the article we will continue to count relative clauses as finite
clauses.
4.2.5 Suri and McCoy?s Definition of Previous Utterance. As discussed in Section 2,
Suri and McCoy (1994) suggested that after- and before-clauses behave more like em-
bedding elements (i.e., like complements) than like coordinating ones, and Cooreman
and Sanford (1996) found evidence supporting this treatment for when clauses, as well.
The previous utterance parameter of our script can be used to compare this proposal
with Kameyama?s. When this parameter is set ?Kameyama-style,? adjunct clauses are
treated as not embedded, so that, in (14), the previous utterance for (u3) is (u2). (This
was the setting used for the results discussed so far.) When the parameter is set to
(generalized) Suri and McCoy, adjunct clauses are treated as embedded, so that the
previous utterance for (u3) is (u1):
(14) (u1) John woke up (u2) when Bill rang the door.
(u3) He had forgotten the appointment.
Using Suri and McCoy?s definition of previous utterance results in a small but sig-
nificant reduction in the number of violations of Strong C1, in small improvements
concerning R1 (GJW 95), and in small, but not significant, improvements for Rule 2. As
far as Strong C1 is concerned, 20 utterances that violate Strong C1 with Kameyama?s
32 The difference matters when the relative clause occurs at the end of an embedding clause, as in John
wanted a photograph of the man that Bill had seen entering the building at night. He . . .
33 We also remind the reader that our script treats all relative clauses as containing a link referring to the
entity modified by the relative, even when the clause does not contain an explicit relative pronoun or
complementizer, so that they never violate C1.
337
Poesio et al Centering: A Parametric Theory
definition satisfy it under Suri and McCoy?s, but 9 utterances become violations (by
the sign test, +20, ?9, p ? .03). The reduction is not, however, sufficient for Strong
C1 to be verified (+355, ?458). With Rule 1, we find that the number of utterances
that verify the GJW 95 version increases (+287), but the number and percentage of
violations stays the same (8 [about 2%]).
We should note, however, that the differences between Kameyama?s and Suri?s
definition of previous utterance have mostly to do with a type of clause that was
discussed only briefly by Kameyama and not at all by Suri and McCoy: relative clauses,
as in (15):
(15) (u1) This brooch is made of titanium,
(u2) which is one of the refractory metals.
(u3) It was made by Anne-Marie Shillitoe, an Edinburgh jeweller, in 1991.
If the ?generalized Kameyama? definition of previous utterance is adopted, the pre-
vious utterance for (u3) is the relative clause, (u2); this causes a violation of Strong
C1. In the ?generalized Suri and McCoy? instantiation, by contrast, the relative clause
is treated as embedded; this seems to be the better approach. If relative clauses were
not treated as separate utterances or were treated as embedded in both instantiations,
we would find an equal number of violations, although about 20 violations would be
different in each instantiation. One example in which the difference does have to do
with the way adjuncts are handled is (7). PRODUCT-Z is not mentioned in the adjunct
if-clause, and therefore Strong C1 is violated if (u2) is taken as previous utterance for
(u3). In this case, Suri and McCoy?s proposal works better than Kameyama?s.
(7) (u1) You should not use PRODUCT-Z
(u2) if you are pregnant or breast-feeding.
(u3) Whilst you are receiving PRODUCT-Z ....
Conversely, in the following example, the adjunct clause, as you may damage the patch
inside, introduces the entity the patch, which is then referred to in (u3), so treating
the adjunct (u2) as embedded leads to a violation of C1. In this case, Kameyama?s
definition of previous utterance gives the right result:
(16) (u1) Do not use scissors
(u2) as you may damage the patch inside.
(u3) Take out the patch.
Given that these improvements are significant, if small, in the rest of the article, we
will use Suri and McCoy?s definition when uttdef is set to finite clause. However, our
discussion, and especially the contrast between (7) and (16), gives further support to
the idea that utterances may be best identified with sentences. We consider this setting
next.
4.2.6 Sentences. The setting of uttdef with the most dramatic impact on Strong C1
is that which identifies utterances with sentences. The reasons for this have already
been illustrated with (9): If utterances are identified with sentences, there are only two
utterances in that example, both containing references to the egg vases. The reduction
in violations is such that with this instantiation more utterances verify Strong C1
338
Computational Linguistics Volume 30, Number 3
Table 9
Statistics relevant to Constraint 1 when utterances are identified with sentences.
Museum Pharmaceutical Total (Percentage)
Number of times at least one CF(Un) is realized in Un+1 131 147 278 (41.6%)
Utterances that satisfy Constraint 1 (have exactly one CB) 126 138 264 (39.5%)
Utterances with more than one CB 5 9 14 (2.1%)
Utterances without a CB but segment boundary 65 80 145 (21.7%)
Utterances without a CB 75 171 246 (36.8%)
than violate it, although not so many as to ensure verification at the 5% level.34 The
statistics relevant to Constraint 1 with this definition of utterance are shown in Table 9.
Although Strong C1 is still not verified if we consider all 669 segments of text that
contain NPs, the number of utterances that satisfy Strong C1 (264) is slightly larger
than the number of those that don?t (260).
However, identifying utterances with sentences also has several negative (if small)
effects. The main among these is that the number of violations of Rule 1 goes up:
In the case of Rule 1 (GJW 95), by 50%, from 8 to 12. The reason for this increase is
in part simply that more utterances have a CB; but in some cases, the problem could
be viewed as the CB?s not being updated quickly enough. Consider the following
example:
(17) (s1) The engravings for these rooms, showing the wall lights in place,
were reproduced in Diderot?s Encyclopedie, one of the principal works
of the Age of Enlightenment. (s2) An inscription on the Getty Museum?s
drawing for one of these wall lights explains (cl3) that it should hang
above the fireplace.
The pronoun it in sentence (s2) violates Rule 1 if utterances are viewed as sentences,
but not if they are viewed as clauses. This is because in the first case (s2) has a single
CB, the wall lights, whereas with the vanilla? instantiation clause, (cl3) is a separate
utterance, with CB one of these wall lights. Because the number of violations is still quite
small, both Rule 1 (GJW 95) and Rule 1 (GJW 83) are still verified (+252, ?12; and
+209, ?55, respectively, as opposed to +287, ?8 and +243, ?52, with the vanilla?
instantiation, Suri setting),35 although Rule 1 (Gordon et al) still isn?t (+97, ?167).
Note also that with this instantiation, the number of CBs realized by R1-pronouns
(129) is much smaller than the number realized by other types of NPs (209).
34 There is one complication: Many CFs are introduced not in sentences, but in in titles and other layout
elements that do not have a sentential format, such as Chandelier or Side effects. In order not to leave
these CFs ?stranded,? the scripts also treat as an utterance every unit that contains an NP which is not
contained in any sentence, just as we did for the vanilla instantiation. This means, however, that the
number of utterances is much larger than the number of sentences (669 instead of 505), and that Strong
C1 is not verified, even though it would be if only the 505 sentences were considered (the sign test
would then show p ? 0.001 for Strong C1).
35 The number of utterances to be tested of course varies depending on whether utterances are identified
with finite clauses (295) or sentences (264).
339
Poesio et al Centering: A Parametric Theory
Table 10
Rule 2 statistics with sentences as utterances.
Museum Pharmaceutical Total (Percentage)
Establishments 54 68 122 (18.2%)
Continuations 28 33 61 (9.1%)
Retain 22 23 45 (6.7%)
Smooth Shift 7 12 19 (2.8%)
Rough Shift 20 11 31 (4.6%)
ZERO 52 66 118 (17.6%)
NULL 88 185 273 (40.8%)
The results for Rule 2 are not that different from those obtained with finite clauses,
but we do observe more Continuations and fewer NULLs. The statistics are shown
in Table 10. Note the much greater number of Rough Shifts than of Smooth Shifts,
although the ranking suggested by Brennan et al is still verified by the Page test.
There are still too few sequences to truly test the version of Rule 2 proposed
by Grosz et al, but the preferences are roughly verified. As for the version of Rule
2 proposed by Strube and Hahn, there still 10 times as many expensive-expensive
sequences (191) as cheap-cheap ones (18).
4.2.7 Interim Summary. The effect of the changes in the definition of utterance and
previous utterance on Strong C1 and Rule 1 are summarized in Figure 1 and Figure 2,
respectively. As the figures show, most such changes have fairly small effects, even
though the effects are often significant. The one exception is identifying utterances
with sentences; treating all clauses as utterances also has a positive impact, provided
that we assume nonfinite clauses contain an implicit realization of the subject of the
matrix clause.
Even though identifying utterances with sentences leads to much better results for
Strong C1, we will not simply abandon the hypothesis that utterances may coincide
with finite clauses. This is in part for theoretical reasons, such as the fact that in
other theories of discourse in which ?units? are assumed, such as RST, these units
are generally finite clauses. Also, identifying utterances with sentences leads to small,
but significant, increases in the number of violations of Rule 1 (from 8 in the vanilla
instantiation [2.8%] to 12 [4.5%]) and in the number of Rough Shifts (from 2.9% to
4.6%). We will also see in a moment that there are other ways of changing the vanilla
instantiation that satisfy Strong C1, so identifying utterances with sentences is not
strictly necessary.
In the rest of the article we will, therefore, study the effect of changes to other
parameters both on instantiations in which utterances are identified with finite clauses
(henceforth, u = f ) and on instantiations in which they are identified with sentences
(u = s).
4.3 Realization
In this section we discuss the effect of changes in the values of the realization param-
eters: realization and CF-filter.
340
Computational Linguistics Volume 30, Number 3
Figure 1
The effect of utterance parameters on Strong C1: A summary.
Figure 2
The effect of utterance parameters on Rule 1 (GJW 95): A summary.
4.3.1 IF: Indirect Realization + u = f . Examples such as (9) indicate that another
way to reduce the number of violations of Strong Constraint 1 is to allow for indirect
realization: Then the bridging references to the egg vases in (u2), (u3), and (u4) would
make them the CB of these utterances. And indeed, if we modify the ?best? among the
u = f instantiations (vanilla?, using our generalization of Suri and McCoy?s proposal
to determine the previous utterance) to allow for indirect realization, the reduction in
violations to Strong C1 is such that with 525 utterances (54.0%) having exactly one CB
and 325 having zero or more than one (33.4%), Strong C1 is verified by the sign test
(+525, ?325).36
36 The number of utterances is obviously not affected by changes in the realization parameters.
341
Poesio et al Centering: A Parametric Theory
Table 11
Rule 2 statistics with indirect realization, 4u = f .
Museum Pharmaceutical Total (Percentage)
Establishments 75 95 170 (17.5%)
Continuations 49 40 89 (9.2%)
Retain 76 51 127 (13.1%)
Smooth Shift 39 25 64 (6.6%)
Rough Shift 60 37 97 (10%)
ZERO 60 78 138 (14.2%)
NULL 46 241 287 (29.5%)
However, allowing for indirect realization has a negative effect on other claims,
just as the change to u=s does. The first negative effect is that the number of utter-
ances with more than one CB almost doubles, from 13 with the ?generalized Suri and
McCoy? instantiation (1.3%) to 22 (2.3%). This is because by increasing the number
of ?persistent entities,? we increase the chance of their having an equivalent ranking
in the previous utterance. The number of violations of Rule 1 exactly doubles: from
8 with the Suri and McCoy instantiation to 16. But because with indirect realization
more utterances have a CB, the number of utterances that matter for the purposes of
Rule 1 also increases, from 295 to 467, so that the percentage of violations to Rule 1
does not change that much. With indirect realization, 3.4% of utterances violate Rule
1 (GJW 95), as opposed to 2.7% with generalized Suri and direct realization. As a
result, Rule 1 (GJW 95) (+451, ?16) and Rule 1 (GJW 83) (+318, ?149) are still verified,
whereas Rule 1 (Gordon et al) still isn?t (+136, ?331). An example of a pronoun that
becomes a violation of Rule 1 (GJW 95) if we allow CFs to be indirectly realized is
shown in (18). The NP one stand in (u42) realizes a bridging reference to the discourse
entity introduced by the NP the two stands in (u39), which is therefore realized in (u42),
and thus becomes its CB, but it is not pronominalized.
(18) (u39) The two stands are of the same date as the coffers, but were
originally designed to hold rectangular cabinets.
(u42) One stand was adapted in the late 1700s or early 1800s century to
make it the same height as the other.
Finally, the change to indirect realization has a big impact on the statistics for Rule 2,
shown in Table 11. On the positive side, the number of NULL transitions goes down
significantly (to less than 30%), and the percentages of the four ?classic? transitions
go up. However, the greatest increases are in the number of RET (from 3.8% to 13.1%)
and RSH (from 2.6% to 10.0%). The fact that there are many more RET than CON and
many more RSH than SSH means that this is the first instantiation for which Rule 2
(BFP) is not verified by a Page test. The reason for this can be seen in (18): Because
implicit realizations are implicit NP modifiers (i.e., one stand is interpreted as one of the
two stands), they are never CPs of an utterance. (Rule 2 [Strube and Hahn] still isn?t
verified, although the percentage of cheap transitions increases from 154/747, [20.6%]
to 207/747 [27.7%]).
342
Computational Linguistics Volume 30, Number 3
Table 12
Statistics about Strong C1 with u = s and indirect realization
MUSEUM PHARMA TOTAL
Number of times at least one CF(Un) is realized in Un + 1 194 222 416 (62.2%)
Utterances that satisfy Constraint 1 (have exactly one CB) 184 206 390 (58.3%)
Utterances with more than one CB 10 16 26 (3.9%)
Utterances without a CB that are segment boundaries 47 55 102 (15.2%)
Utterances without CB 30 121 151 (22.6%)
Below, we indicate the instantiations with u=f, Suri and McCoy-style treatment of
adjuncts, and direct realization as DF and those with the same settings, but indirect
realization, as IF.
4.3.2 IS: Indirect Realization +u = s. As one might expect, the results for Constraint
1 get even better if indirect realization is combined with the u=s setting. As shown
in Table 12, With this instantiation (henceforth, IS) 390 utterances out of 669 (58.3%)
satisfy Strong C1, and 177 (26.5%) violate it?significantly better than the instantiation
with u=s and direct realization (henceforth, DS). On the other hand, the number of
utterances with more than one CB almost doubles again (and with respect to the DS
instantiation), to 26 (3.9%) from 14 (2.1%).
The number of violations of Rule 1 (GJW 95), as well, doubles again with respect to
the DS instantiation, from 12 (4.5%) to 26 (6.7% of the 390 utterances with a CB and an
R1-pronoun). While this number of violations isn?t enough to invalidate Rule 1 (GJW
95) (+364, ?26), it is three times the number of violations with the vanilla instantiation.
As for what we called Rule 1 (Gordon et al), even with this instantiation more than
75% of utterances violate it: +97, ?293.37
The results with Rule 2 are comparable to those obtained with the IF instantiation.
Just as in that case, Rule 2 (BFP) is not verified according to a Page test, even though
there is a great reduction in the number of NULL transitions (to 23.2%). The percentage
of RET is even greater than with IF (114 [17.0%], almost twice the percentage of CON
[9.4%]) as is that of Rough Shifts (100 [14.9%], almost three times the percentage of
Smooth Shifts [5.2%]). If we ignore segment boundaries, cheap transitions are 136/444,
30.6% of the total (as opposed to 22.0% with DS and 27.7% with IF).
4.3.3 Second-Person CFs. Second-person pronouns (henceforth, PRO2s) are generally
assumed to be used deictically rather than anaphorically (see, e.g., Di Eugenio 1998).
37 Some readers might think that the additional violations of Rule 1 obtained with instantiations IF and IS
(such as the one in example (18)) shouldn?t really count as violations of Rule 1, because bridging
references such as one stand contain an implicit reference to the two stands, that is, are semantically
equivalent to one of the two stands, and it is these implicit anaphors that satisfy Rule 1. One of the
parameters not discussed here, bridges policy, controls whether these implicit anaphoric references are
treated as R1-pronouns. It turns out that doing this actually results in more violations of Rule 1, because
most bridging references do not refer either to the CB of the present utterance or the CB of the
previous one (see Poesio 2003), and every bridging reference not referring to the CB may cause a
violation. In fact, treating these implicit anaphoric references as R1-pronouns (hence, as CFs) also
dramatically increases the number of utterances with more than one CB, as well as the number of
Rough Shifts. For details, see the extended report and the companion Web site.
343
Poesio et al Centering: A Parametric Theory
However, it has been suggested in recent work that especially in dialogue, they may
actually realize CFs (Byron and Stent 1998).38 In our corpus, and especially in the phar-
maceutical domain, PRO2s are very numerous and often seem to play an important
role in maintaining the coherence of the discourse. And in fact, allowing PRO2s to
count as realizations of CFs does reduce the number of violations of Strong C1 with
both the u = f and the u = s instantiations of the theory, both with direct and with
indirect realization. Even with DF (and the Suri-McCoy setting of the previous utter-
ance parameter), allowing PRO2s to count as CFs is sufficient on its own to verify
Strong C1: The museum domain is not affected, but in the pharmaceutical domain,
the number of utterances that satisfy Strong C1 increases from 164 to 273, so that in
total 464 utterances satisfy C1 and 367 violate it, which makes the constraint verified
(by the sign test, p ? .03). With DS, if we treat PRO2s as CFs, 332 utterances verify
Strong C1, and 214 don?t (as opposed to +264, ?260 when PRO2s are not treated as
CFs). Allowing for indirect realization we get even better results: With IF, we get +623,
?242, a significant improvement even over the instantiation with direct realization and
PRO2s; with IS, we get +439, ?145.
The results with Rule 2 are also improved by treating PRO2s as CFs. The percent-
age of NULL transitions is greatly reduced (for DF, down to 35.0% [from 47.6%]; for
DS, to 30.8% [from 40.8%]; for IF, to 18.2% [from 29.5%]; for IS, to 15.1% [from 23.2%]).
As a result, the percentage of ?continuous? transitions (Kibble 2001)?EST, CON, RET,
SSH, and RSH?increases. However, RSH and SSH increase, as well as EST and CON:
In the IF instantiation with PRO2s, EST are the most common transition (20.2%), but
in the IS instantiation, RSH are (18.4%). Because of these increases, treating PRO2s as
realizations of CFs does not fix the problem with rule 2 (BFP) observed above: the
Rule still isn?t verified with IF and IS. There are no significant changes with Rule 2
(Strube and Hahn).
The results with Rule 1 crucially depend on whether we do or do not consider
second-person pronouns as R1-pronouns. In either case, letting PRO2s realize CFs
results in more violations of Rule 1 (GJW 95), both in absolute and in relative terms,
because more utterances have a CB and therefore count as violations or verifications of
the rule. But if we don?t consider PRO2s as R1-pronouns, then the increase in violations
is small: for DF, from 8 (2.7%) to 12 (2.9%); for DS, from 12 (4.5%) to 17 (5.1%); for IF,
from 16 (3.4%) to 20 (3.5%); and for IS, from 26 (6.7%) to 31 (7.1%). If we treat PRO2s as
R1-pronouns, however, the percentage of violations of Rule 1 (GJW 95) almost triples
for the u=f instantiations and doubles for the u=s ones: 31 violations for DF (7.6%), 38
for DS (11.4%), 51 for IF (9.1%), and 67 for is (15.3%). (Of course, in all of these cases,
Rule 1 [GJW 95] remains verified in a statistical sense.) The reason for this increase
in violations is that PRO2s do not seem to be very good indicators of the CB: About
half of PRO2s are not realizations of CBs, in all instantiations. Given these results,
it seems clear to us that it?s not a good idea to treat PRO2s as R1-pronouns; it?s less
clear whether to treat them as realizing cfs. As we find the position that PRO2s play a
deictic function convincing, in the rest of the article, we will not include their referents
among the CFs, but we will indicate where doing so would result in major differences.
The interested reader is advised to try the alternatives on the companion Web site.
4.3.4 Predicative NPs. The two alternative views considered so far about which en-
tities to realize both result in an increase in the number of CFs. What if we were to
38 Walker (personal communication) also observed that in Japanese, zero pronouns?often taken as
referring to the CB?are allowed to refer to second-person entities.
344
Computational Linguistics Volume 30, Number 3
attempt to reduce the number of CFs instead? Prima facie, one would imagine this type
of modification to have a negative impact on C1, but perhaps some of the violations
of Rule 1 might disappear.
Among the NPs that might be thought not to introduce CFs, an obvious candidate
are predicative NPs, that is, NPs like a policeman in John is a policeman that play the role
of predicates in the logical form of an utterance. But in fact, because our annotators
were instructed to mark up John rather than a policeman as antecedent of subsequent
anaphoric expression in these examples, filtering away such NPs does not have any
positive result at all; on the contrary, it does have a significant (if small) negative
impact on Strong C1,39 because in some cases the annotators had been forced to mark
up an NP in predicative position as the antecedent of an anaphoric expression against
the instructions. Two such examples are given below. Especially in the second case, it
is not clear how else the annotators could have marked the antecedent of Bjorg:
(19) a. An important artist in making these links has been Yasuki
Hiramatsu. His knowledge of metalcraft allows him to push and play
against the boundaries of what the material can physically do.
b. Two such jewellers are Toril Bjorg from Norway and Jacqueline Mina
from England. It may be unsurprising that Bjorg, as a Scandinavian,
should choose silver as her material.
In the following we will continue to treat predicative NPs as not introducing CFs.
4.3.5 Interim Summary. The realization parameters have an even greater impact than
the utterance parameters, especially on Strong C1 and Rule 2. Either allowing for
indirect realization or treating second-person pronouns as introducing CFs, is sufficient
for Strong C1 to be verified. When the two settings are combined, a large majority of
utterances verifies the constraint. On the other hand, allowing for indirect realization
also results in significant increases in the number of violations of Rule 1, although
overall the percentage of such violations remains pretty small. In addition, indirect
realization leads to such an increase in the number of RET and RSH that there are
fewer CON than RET and fewer SSH than RSH, and Rule 2 (BFP) is not verified by
any of the instantiations with indirect realization we have discussed. Treating PRO2s
as realizations of CFs, while sufficient to cause Strong C1 to be verified, has less of
an effect on Rule 2, but when we combine this setting with the IS instantiations, we
obtain an instantiation in which RSH is the most common transition. The effects of
the realization choices are summarized in Figures 3 and 4.
4.4 Ranking
4.4.1 Grammatical Function + Linear Disambiguation. We observed above that be-
cause grammatical function does not always specify a unique, most highly ranked CF,
when using that ranking function some utterances end up with more than one CB,
which causes the violations of the weak version of Constraint 1 seen above (up to 5.7%
of the total in the IF instantiation treating PRO2s as CF realizations). We also men-
tioned, however, that this problem can be fixed by requiring the ranking function to be
a total order, which, in turn, is easily done by adding a tie-breaking factor. Given the
results in Gernsbacher and Hargreaves (1988) and Gordon, Grosz, and Gillion (1993),
39 The difference is significantly larger for all the instantiations not treating PRO2s as CFs; larger, but not
significantly so, if PRO2s are treated as CFs.
345
Poesio et al Centering: A Parametric Theory
Figure 3
The effect of the realization parameters on the violations of Strong C1.
Figure 4
The effect of the realization parameters on the percentage of violations of Rule 1.
the most obvious disambiguating factor is linear order: Whenever two CFs are equally
ranked, assign to, say, the leftmost CF a higher rank. And indeed, we saw in Section 2
that linear order is used by Strube and Hahn (1999) to resolve ties, albeit in conjunction
with a ranking function other than grammatical function. In this section we evaluate
the ranking function obtained by adding linear order to grammatical function, which
we call GFTHERELIN.40 The results with this ranking function are summarized in
Table 13.
40 The reason for the ?there? in the function?s name is that the results can be slightly improved by a
further small change: ranking postcopular nps in there-sentences (e.g., someone in There is someone at the
door) as subjects rather than objects. See, e.g., Sidner (1979).
346
Computational Linguistics Volume 30, Number 3
Table 13
Summary of results for Strong C1, Rule 1 (GJW 95), and Rule 2 (BFP) with GFTHERELIN
ranking. ? indicates that a claim is not verified at the .05 level; ? that it?s not verified at the
.01 level.
Instantiation Strong C1 Rule 1 (GJW 95) Rule 2 (BFP) (Page test,
(Percentage violations) (Percentage violations) probability of not
being verified)
DF-predicate +352,?450 (46.3%) ? +291,?11 (3.6%) .001
DF-predicate+pro2 +465,?355 (36.5%) +403,?15 (3.6%) .001
DS-predicate +273,?249 (37.2%) ? +259,?14 (5.1%) .001
DS-predicate+pro2 +347,?197 (29.4%) +325,?22 (6.3%) .001
IF-predicate +529,?310 (31.9%) +463,?18 (3.7%) 1 ?
IF-predicate+pro2 +635,?219 (22.5%) +325,?22 (3.7%) .05?
IS-predicate +408,?157 (23.5%) +378,?30 (7.4%) .05?
IS-predicate+pro2 +469,?113 (16.9%) +432,?37 (7.4%) 1 ?
The table summarizes eight instantiations: DF, DS, IF, and IS, each in two variants
(including PRO2s and without them). For each instantiation, the table lists verifiers
and violations of Strong C1 and Rule 1 (GJW 95) and the percentage of violations, as
well as the results of the Page test for Rule 2.
Adopting GFTHERELIN as a ranking function doesn?t lead to major changes as far
as Strong C1 is concerned. This is because the only change from the results obtained
with simple grammatical function is that the utterances previously classified as having
two CBs get reclassified as having one, and with the instantiations that would benefit
the most from a reduction in Strong C1 violations?those based on DF?the number of
multi-CB sentences is fairly small, typically 1?2%, although this is enough to make the
improvement significant by the sign test with all instantiations. The improvements are
greater with the u=s instantiations, since with sentences it?s more common for more
than one CF to be realized in the same grammatical position; for example, in the IS
instantiations in which PRO2s are considered as realizations of CFs, we find that 5.7%
of utterances (38/669) have more than one CB. However, Strong C1 is already verified
with these instantiations, even with simple grammatical function.
As in all previous cases, better results with Strong C1 are counterbalanced by
worse results for Rule 1?although, again, not so much worse that Rule 1 ends up
not being verified. The results with the DF instantiations aren?t significantly worse:
For example, we find +291, ?11 (3.6%) with the instantiation not including predicative
NPs and second-person pronouns, as opposed to +280, ?9 for the same instantiation
with simple grammatical function ranking. The number of violations of Rule 1 is
significantly greater with the DS instantiation if PRO2s are treated as CF realizations:
+325, -22 (6.3%) vs. +310, -17 (5.2%). In two of the additional five violations of Rule
1, however, the problem is simply that by adding a disambiguation element, we turn
utterances whose CB is undefined (because more than one CF is equally ranked) into
utterances with a CB. One such example is (20):
(20) (s7) Intended to hold jewels or small precious items, the interiors of
this pair of coffers are lined with tortoiseshell and brass or pewter, with
secret compartments in the base.
347
Poesio et al Centering: A Parametric Theory
Table 14
Transition percentages for IS with GFTHERELIN ranking.
Museum Pharmaceutical Total (Percentage)
Establishments 47 60 107 (16.0%)
Continuations 28 44 72 (10.8%)
Retain 56 65 121 (18.1%)
Smooth Shift 8 24 32 (4.8%)
Rough Shift 48 28 76 (11.4%)
ZERO 43 58 101 (15.1%)
NULL 41 119 160 (23.9%)
(s8) The coffers are each decorated using techniques known as premiere
partie marquetry, a pattern of brass and pewter on a tortoiseshell ground,
and its reverse, contrepartie, a tortoiseshell pattern on a background of
pewter and brass.
With simple grammatical function, both the coffers and brass are CBs of (s8), which
is therefore treated by our script as not having a well-defined CB. As a result, the
pronominalization of a non-CB, premiere partie marquetry, is not counted as a violation.
(s8), however, becomes a violation with GFTHERELIN, since the coffers becomes its
only CB.
With the IF instantiation, the percentage of violations of Rule 1 (3.7%) is nonsignif-
icantly greater than the percentage with simple grammatical function (3.5%). The
percentage of violations with the two IS instantiations (7.4%) is significantly worse (at
the .01 level) than with simple grammatical function (6.7% and 7.1%, respectively).
Table 13 shows that using GFTHERELIN has a positive effect on the number
of violations of Rule 2 (BFP). Whereas with simple grammatical function, none of the
instantiations with indirect realization verifies Rule 2 (BFP) by the Page rank test, with
GFTHERELIN the IS instantiation does (although only at the .05 level), as does IF if
PRO2s are treated as CF realizations. (All direct realization instantiations still verify
the rule.) The main reason for this change is a significant reduction in the percentage
of RSH with GFTHERLIN, especially for the IF and IS instantiations: With IF we see
a reduction in RSH from 9.7% to 7.6%; with IS, from 14.6% to 11.4%. With the DS
instantiation the percentage of RET and RSH is about twice what we find with the
DF instantiation, just as we observed with simple grammatical function ranking, but
otherwise the results are pretty similar to those with DF. With the IF and the IS
instantiations, we get small but significant increases in CON and RET and a reduction
in RSH. In Table 14, we report the complete percentages for this instantiation, for
comparison with other ranking functions.
The change to GFTHERELIN hardly affects the relative percentages of cheap and
expensive transitions, so the results concerning Rule 2 (Strube and Hahn) do not
change.
The IS instantiation with GFTHERELIN ranking is the one in which all three claims
are verified without need to treat PRO2s as CF realizations, even though Rule 2 is
348
Computational Linguistics Volume 30, Number 3
verified with this instantiation only at the .05 level. We will therefore concentrate on
this instantiation when making comparisons with the other ranking variants.
4.4.2 Linear Order. Among the ranking functions alternative to grammatical function,
perhaps the simplest is the one that ranks CFs in the order of their occurrence in the
utterance, from left to right. This ranking function was explicitly proposed by Rambow
(1993) to account for facts about scrambling in German, and effects of order of mention
have been observed by, among others, Gernsbacher and Hargreaves (1988), Gordon,
Grosz, and Gillion (1993), and Stevenson, Crawley, and Kleinman (1994).
Using linear order instead of GFTHERELIN has no effect at all on Constraint 1,
as one would expect, since all that matters for the constraint to be verified is whether
discourse entities are mentioned in successive utterances and whether the ranking
function is total. However, no significant differences were observed with Rule 1 (GJW
95), either: with IS, we find +378, ?30, with linear order, as opposed to +377, ?31,
with GFTHERELIN.41 This is because linear order is a very good approximation of
grammatical function in English: Subjects tend to occur in first position, objects in
second position, etc. The one claim for which the differences are significant is Rule 2
(BFP): with IS, enough CON become RET and enough SSH become RSH that Rule 2
is no longer verified even at the .05 level. (The rule is still verified in the DF and the
DS instantiations.)
All in all, these results are not grounds to argue that linear order is a better rank-
ing function than GFTHERELIN;42 however, because the differences are so small, they
also suggest that linear order (which is far easier to compute) might be a good ap-
proximation of grammatical-function ranking for practical applications working with
English.
4.4.3 Information Structure. Replacing GFTHERELIN with the ranking function pro-
posed by Strube and Hahn (1999) (henceforth, STRUBE-HAHN): rank hearer-old en-
tities more highly than inferrables, and these higher than hearer-old entities?cannot
and did not lead to different results for Strong C1, for the reasons already discussed
for linear-order ranking. Less expected was the fact that?again, just as in the case
of linear order?we didn?t find any significant differences with Rule 1 (GJW 95), ei-
ther, although with the IF and IS instantiations, we find one more violation than with
GFTHERELIN.43 This doesn?t mean that the exact same utterances are violations in
both cases, rather, that the differences ?balance out?. We already saw one example in
which STRUBE-HAHN ranking results in a violation of Rule 1, whereas GFTHERELIN
ranking doesn?t: This is the first sentence in (10), illustrating the kind of situations in
which a partial ranking may result in two CBs. We repeat that sentence in (21) and
include the preceding sentence:
(21) (s67) An inventory of Count Branicki?s possessions made at his death
describes both the corner cupboard and the objects displayed on its
shelves: a collection of mounted Chinese porcelain and clocks, some
embellished with porcelain flowers.
41 With IF the difference goes the other way: +463, ?18, for GFTHERELIN, +463, ?19, for linear order.
There are no differences at all with DF and DS.
42 This point is reinforced by a number of results from Gordon and collaborators (e.g., Gordon, Grosz,
and Gillion 1993; Gordon et al 1999) suggesting that hierarchical position in the parse tree is a better
predictor of salience than linear order, as well as by results suggesting that for a range of languages,
linear order is much less effective?see, for example, Prasad and Strube (2000) for Hindi.
43 We discuss the results only with the version of Rule 1 proposed by Grosz, Joshi, and Weinstein (1995).
349
Poesio et al Centering: A Parametric Theory
Table 15
Transition percentages for IS with STRUBE-HAHN ranking.
Museum Pharmaceutical Total (Percentage)
Establishments 47 60 107 (16.0%)
Continuations 39 55 94 (14.1%)
Retain 50 53 103 (15.4%)
Smooth Shift 18 26 44 (6.6%)
Rough Shift 33 27 60 (9.0%)
ZERO 43 58 101 (15.1%)
NULL 41 119 160 (23.9%)
(s68) The drawing of the corner cupboard, or more probably an
engraving of it, must have caught Branicki?s attention.
As the corner cupboard is in object position, it gets higher ranking in (s67) than Count
Branicki, which is in NP-modifier position, which?while not explicitly discussed in
the centering literature?will presumably fall among the ?other? cases. As a result,
the cupboard is the CB of (s68), and its pronominalization is predicted by Rule 1.
With STRUBE-HAHN ranking, Count Branicki is the highest-ranked entity of (s67), and
therefore the CB of (s68); hence the violation. Conversely, (22) is an example in which
GFTHERELIN and ranking results in a violation of Rule 1, while STRUBE-HAHN
ranking doesn?t:
(22) (s88) Christened by his contemporaries as ?the most skillful artisan in
Paris,? Andre`-Charles Boulle?s name is synonymous with the practice of
veneering furniture with marquetry of tortoiseshell, pewter, and brass.
(s89) Although he did not invent the technique, Boulle was its greatest
practitioner and lent his name to its common name: Boulle work.
In this example, Andre`-Charles Boulle?s name, the subject of (s88), is ranked higher than
Andre`-Charles Boulle, and is therefore the CB of (s89), in which, however, it is not
pronominalized, even though both Boulle and the technique he invented are. Notice
that (21) and (22) are almost stereotypical instances of the class of examples that led
Sidner (1979) to argue that two foci are needed, one for animated entities, and one for
the entities acted upon; we return to this issue in Section 5.
The one claim where STRUBE-HAHN ranking makes a clear difference is Rule 2
(BFP). About 20% of RET become CON and about 20% of RSH become SSH. Although
we still find more RET than CON and more RSH than SSH, these changes are sufficient
to cause Rule 2 (BFP) to be verified at the .01 level in all instantiations considered.44 The
transition percentages with IS and STRUBE-HAHN ranking are presented in Table 15.
Even with IS, however?the instantiation closest to the one proposed by Strube
and Hahn?we still find many more expensive transitions (272) than cheap ones (172)
44 With DF and DS the number of RET and RSH goes down drastically, so that we do find more CON
than RET and more SSH than RSH, but we still find more SSH than RET.
350
Computational Linguistics Volume 30, Number 3
and almost three times as many expensive-expensive sequences (137) as cheap-cheap
ones (56), so Rule 2 (Strube and Hahn) is not verified.
4.4.4 Summary. Because Strong C1 is the most problematic claim, it was to be expected
that the most studied parameter of centering, ranking, would have a smaller impact
than the utterance and realization parameters. It is nevertheless interesting that the
results for Rule 1 (GJW 95) are virtually identical under the three versions of ranking
we considered. More differences can be found for Rule 2 (BFP), which is not verified
by any instantiation with linear-order ranking and is verified only by a few instantia-
tions with GFTHERELIN. Adopting STRUBE-HAHN ranking does result in a greater
percentage of utterances being classified into one of the ?continuous? classes and in a
lower probability of Rule 2?s (BFP) being falsified. Finally, not even these last changes
to parameter settings were sufficient to cause either Rule 1 (Gordon et al) or Rule 2
(Strube and Hahn) to be verified.
5. Discussion
In this section we first discuss the effects of different parameter settings on the re-
sults obtained; we then analyze the claims of centering theory, draw a few theoretical
conclusions, and make some suggestions for further work (empirical and theoretical).
5.1 Setting the Parameters
5.1.1 Comparing Instantiations. A central goal of this study was to compare different
ways of instantiating centering?s parameters, and different versions of its claims, on a
single data set, also examining combinations not previously considered (e.g., whether
Brennan et al?s version of Rule 2 would be verified when the parameters were set
as suggested by Strube and Hahn and vice versa. Our first interesting result in this
sense is that if the parameters are set in the most ?mainstream? way (the ?vanilla?
instantiation) only Rule 1 (GJW 95) and Rule 1 (GJW 83) are clearly verified. The results
concerning Constraint 1 are especially negative. As with this instantiation only 35% of
utterances are continuous?that is, CF(Un) ? CF(Un?1) = ? (Kibble 2000; Karamanis
2001)?only the weak version of Constraint 1 is verified. Strong C1, the best-known
formulation, and the one that in our view best captures the idea of ?entity coherence,?
clearly doesn?t hold. Another interesting observation is that if ranking is required
only to be partial, some utterances end up with more than one CB: The percentage
of such utterances is only 1% with the vanilla instantiation but can be as high as 6%
with some instantiations. This is perhaps obvious, but to our knowledge, it had not
been previously discussed.
As for Rule 2, with the vanilla instantiation the version proposed by Brennan et
al. is verified by a Page rank test, but arguably, the most striking fact about transitions
with this instantiation is the prevalence of NULL transitions (47.9%), Establishments
(18.8%), and ZEROs (16.7%). All together, the four types of transitions falling under
the remit of Rule 2 account for only 16% of utterances, and if Smooth Shifts and Rough
Shifts are counted together, with this instantiation there are more Shifts than Retains.
Other classifications and versions of the rule do not correlate much better with the
observed frequencies: For example, only 39% of entity-coherent transitions (139 out
of 357), and 14% of the total, are cheap in the sense of Strube and Hahn (1999) (i.e.,
CP(Un?1) predicts CB(Un)).
These findings concerning the vanilla instantiation should not, however, lead us
to conclude that the theory in general is not verified. Our second major finding is that
parameters do matter: that is, it is possible to set the parameters in such a way as to
351
Poesio et al Centering: A Parametric Theory
make all three claims verified in a statistical sense. However, because Strong C1 is the
claim with the largest percentage of violations, the parameters whose setting matters
the most when one is trying to find an instantiation in which all claims are satisfied
are those controlling utterance definition and CF realization. Considering a center as
realized in an utterance which contains an associative reference to that center is suf-
ficient for Strong C1 to be verified; identifying utterances with sentences instead of
finite clauses also has a strong positive effect. With the resulting instantiations (IF and
IS), Strong C1 is verified, as well as the two ?basic? versions of Rule 1.
We also found, however, that there is a trade-off between Strong C1, on one side,
and Rule 1 and Rule 2, on the other: The changes to the utterance and realization
parameters just mentioned, while reducing the violations of Strong C1, increase those
of Rule 1 and Rule 2 (see, e.g., Table 13). Identifying utterances with sentences, or (to a
lesser extent) allowing indirect realization, results in statistically significant increases
in the number of violations of Rule 1?up to a total of 7.4% in the IS instantiation (see
Figures 2 and 4)?although Rule 1 (GJW 95) and Rule 1 (GJW 83) are so robust that
they are still verified, even in these instantiations.45 These changes to the utterance
and realization parameters have an even greater impact on Rule 2 (BFP), which is
only weakly verified with the vanilla instantiation. With the IF and IS instantiations
and grammatical-function ranking, we find many more RSH than SSH and many
more RET than ?pure? CON (i.e., without counting Establishments); indeed, in the IS
instantiation with GFTHERELIN ranking, RET are the second most common transition.
As a result, Rule 2 (BFP) is verified with is instantiations only at the .05 level, and with
IF instantiations only if second-person pronouns are counted as realizations of CFs.
On the positive side, with these instantiations a much greater percentage of utterances
(45%) are classified as either CON, RET, SSH, or RSH, and a further 16% as EST.
These results can be further strengthened by making one last change to the param-
eters: adopting the ranking function proposed by Strube and Hahn (1999) instead of
GFTHERELIN. With this instantiation, Rule 2 (BFP) is verified at the .01 level, rather
than only at the .05 level. This is because although the STRUBE-HAHN ranking func-
tion has no effect on Strong C1 (obviously) or R1 (more surprisingly), it does result in
some of the RET becoming CON and some of the SSH becoming RSH. Even though
we still find more RET than CON and more RSH than SSH, these changes are enough
to cause Rule 2 (BFP) to be verified at the .01 level with the IS instantiation. Strube
and Hahn?s own version of Rule 2 still isn?t verified, but this version of the rule is not
verified by any of the instantiations we evaluated. In other words, with the IS or IF
instantiation and STRUBE-HAHN ranking, all three claims of the theory are verified
at the .01 level.
The final observation concerning parameter settings is that issues not widely dis-
cussed in the centering literature had a greater impact on the theory?s claims in our
experiments than parameters such as the choice of ranking function or the definition
of previous utterance. Many of these issues, such as the treatment of second-person
pronouns and of empty categories, have to do with the general issue of which entities
should be included in the CF list. Considering second-person pronouns to be realiza-
tions of discourse entities is enough to make Strong C1 satisfied; we also found that
45 Perhaps the most spectacular demonstration of the trade-off between Strong C1 and Rule 1 can be seen
with the versions of the theory that adopt the definitions of CB proposed by Gordon, Grosz, and
Gillion (1993) and Passonneau (1993). (These instantiations are not discussed in this article, but can be
examined on the companion Web site.) By adopting a particularly restrictive definition of CB, these
versions succeed in reducing (indeed, eliminating, in the case of Passonneau) the violations of Rule 1,
but the price is that only a very few utterances have a CB.
352
Computational Linguistics Volume 30, Number 3
a number of extensions to the definition of utterance, such as the inclusion of rela-
tive clauses and nonfinite clauses, led to much worse results unless reduced relative
clauses and nonfinite clauses were taken to include traces linking these clauses to the
clause in which they were embedded.
5.1.2 Minimizing Violations Should Not Be the Overriding Goal. We said in Sec-
tion 3 that we don?t think that minimizing violations should be the only factor taken
into account in deciding how to set parameters. Some violations are best accepted and
explained in terms of the interaction of centering preferences with other preferences.
(See below.)
Special care is needed when alternative definitions are supported by cross-linguistic
evidence or by the results of psychological studies. In the case of ranking, although
we didn?t find any significant differences between grammatical-function ranking and
linear-order ranking for English, one should keep in mind that such differences have
been found for other languages, especially more free-order ones. Prasad and Strube
(2000), for example, found that in Hindi the difference between grammatical-function
and linear-order ranking is significant, and Strube and Hahn (1999) found significant
differences between grammatical function and information structure in German. Con-
versely, before taking the evidence for a slight advantage of STRUBE-HAHN ranking
over grammatical-function ranking as conclusive, one needs to supplement our stud-
ies with psychological experiments reconciling these results with numerous results
indicating the important role played by grammatical function, especially subjecthood
(among others, Hudson, Tanenhaus, and Dell [1986]; Gordon, Grosz, and Gillion [1993];
Brennan [1995]). Information structure has also been found not to be appropriate for
languages including Greek, Hindi, and Turkish (Turan 1998; Prasad and Strube 2000;
Miltsakaki 2002). Similar considerations apply to the definition of previous utterance,
since we saw that a considerable amount of psychological evidence supports treating
adjuncts as embedded, at least when the syntactically embedded clause is at the end
of the sentence (Cooreman and Sanford 1996; Pearson, Stevenson, and Poesio 2000).
In the case of the definition of utterance, our results indicate that identifying utter-
ances with sentences, rather than finite clauses, leads to results much more consistent
with the claimed preference for discourses to be entity-coherent. While this result is
likely to be useful for a number of reasons and for different types of applications (e.g.,
text planners), we believe that further empirical and theoretical work is needed before
conclusions can be reached about when the local focus is updated. For one thing, most
analyses of discourse structure?for example, rhetorical structures theory (Mann and
Thompson 1988)?view clauses as the basic unit of discourse in written text. And in
spoken dialogue one can hardly find any complete sentences; in this case, the update
unit is much more likely to be a prosodic phrase of some sort.
5.2 The Claims of Centering, Revisited
5.2.1 Centering, Pronominalization, and Salience. One clear result of this work is that
centering?s claims about pronominalization?at least, those expressed by the versions
of Rule 1 proposed in Grosz, Joshi, and Weinstein (1995, 1983)?are very robust. Rule 1
(GJW 95) and Rule 1 (GJW 83) are verified with all parameter instantiations, and in a
very convincing way: In the instantiations we considered, the percentage of violations
of Rule 1 (GJW 95) never exceeds 8% of the total number of utterances.
On the other hand, one should keep in mind that these two versions of Rule 1
make very weak claims about pronominalization. All that Rule 1 (GJW 95) says is that
if we decide to pronominalize, then we should pronominalize the CB. This formulation
doesn?t address the real problem for a theory of pronominalization or, more generally,
353
Poesio et al Centering: A Parametric Theory
of NP form decision, which is to decide when a discourse entity should be realized
as a pronoun (Henschel, Cheng, and Poesio 2000). And our results also indicate that
simply strengthening Rule 1 to the form ?pronominalize the CB,? which can be seen
as a generalization of the proposals in Gordon, Grosz, and Gillion (1993), would be
a very bad idea: Between 50% (with u = f ) and 60% (with u = s) of mentions of
the CB are not realized using a pronoun, and conversely, between 30% and 40% of
personal pronouns are not realizations of the CB. Examples like (12) illustrate one
type of situation in which a mismatch between the CB and pronominalization may
occur: By having been mentioned often in a discourse, a discourse entity may become
sufficiently salient (at the global level) to justify pronominalization even when it is
not the CB.46 These observations suggest that the decision to pronominalize does not
depend only on whether a discourse entity is the CB but must involve a number of
further constraints and preferences.47
5.2.2 CT as a Theory of Coherence: Constraint 1. Another result of this work is that
the validity of centering?s claims about local coherence?Constraint 1 and Rule 2?
depends on the choice of the parameters to a much greater extent than is the case for
the claims about pronominalization. Strong C1 does not hold for the vanilla instan-
tiation, although it does hold for any instantiation in which the implicit anaphoric
component of bridging references is treated as an indirect realization and for many
instantiations in which utterances are identified with sentences. But even under the
most favorable parameter instantiations, there are many more exceptions to Strong
C1 (between 20% and 25% of the total number of utterances) than we find even with
the instantiations which are worse for Rule 1 (7?8%). While the weak version of C1,
requiring only that there is at most one most salient entity per utterance, does hold
even with the vanilla instantiation and does capture the claim that utterances with a
unique CB are easier to process, a central aspect of centering since Joshi and Kuhn
(1979) and Joshi and Weinstein (1981), it says nothing about entity coherence?s being
what ensures local coherence.
Further light on entity coherence is shed by recent work on text planning, particu-
larly by Karamanis (2003), which suggests that when all alternative ways of extracting
a text plan from the propositions expressed by texts such as those we are studying
are considered, the actual ordering found in the texts tends to be in greater agreement
with centering?s preferences about entity coherence than with most of its alternatives.
After extracting the propositions48 expressed by texts in the museum subdomain of
our corpus, Karamanis determined that although the sequence actually found in such
texts is not optimal as far as minimizing the violations to entity coherence (with the
instantiation he considers, more than 50% of the utterances violate Strong C1), approx-
imately 70% of the alternative orderings introduce even more violations.
If we accept that the texts in our corpus are coherent, these results suggest that
there must be other ways of achieving local coherence, apart from what we have
been calling here ?entity coherence?. An obvious candidate for an additional, or
46 The role of global focus in the interpretation of pronouns needs further study. A few preliminary
observations can be found in Hitzeman and Poesio (1998).
47 The discrepancy between pronominalization and CB-hood in our corpus is analyzed in more detail by
Henschel, Cheng, and Poesio (2000), who propose an algorithm for pronominalization that takes into
account factors such as the presence of distractors matching the CB?s agreement features that may lead
to the decision not to pronominalize, as well as factors that may result in the pronominalization of a
non-CB. The algorithm achieves an accuracy of 87.8% in the museum domain.
48 More precisely, the lists of CF realized by each utterance with a DF instantiation, representing that
utterance?s arguments.
354
Computational Linguistics Volume 30, Number 3
alternative, coherence-inducing device are rhetorical relations. Indeed, the claim that
?entity? coherence needs to be supplemented by ?relational? coherence can be found
as far back as Kintsch and van Dijk (1978) and Hobbs (1979). This view is supported
by an analysis of our data. With the u=f instantiations, we find in the pharmaceutical
subdomain many examples in which successive utterances do not mention the same
entities, but the connection between clauses is explicitly indicated by connectives, as
in (23):
(23) (u1) This leaflet is a summary of the important information about
Product A.
(u2) If you have any questions or are not sure about anything to do with
your treatment,
(u3) ask your doctor or your pharmacist.
A more complex case are utterances in the museum subdomain that do not refer to
any of the previous CFs because they express generic statements about the class of
objects of which the object under discussion is an instance, or utterances that make
a generic point that will then be illustrated by a specific object. In (24), (u2) gives
background concerning the decoration of a cabinet:
(24) (u1) On the drawer above the door, gilt-bronze military trophies flank a
medallion portrait of Louis XIV. (u2) In the Dutch Wars of 1672?1678,
France fought simultaneously against the Dutch, Spanish, and Imperial
armies, defeating them all. (u3) This cabinet celebrates the Treaty of
Nijmegen, which concluded the war.
While the analysis of such cases in terms of rhetorical relations is more complex, it
seems clear to us that an analysis in terms of underlying semantic connections between
events or propositions is more perspicuous than one in terms of entity coherence.
While it is true that some of these violations could be fixed by adopting a broader
notion of bridging reference?for example, in (24) we might treat France as a bridge to
Louis XIV?this wider notion of bridging reference has proven to be very difficult to
identify in a reliable way.
Now, given that in an RST-style analysis, every discourse unit is connected by at
least one rhetorical link to at least another discourse unit, one might wonder whether
?entity coherence? is still needed once ?relational coherence? is introduced. However,
Knott et al (2001) convincingly argue that in RST, complete connectivity is usually
achieved by introducing relations such as ?Elaboration? that, when looked at closely,
turn out really to be attempts to capture a notion of entity coherence. This work on
rhetorical relations is coming to a position symmetrical to our own: that a purely rela-
tional account is not sufficient, and a separate theory of entity coherence is necessary
(Knott et al 2001).49
5.2.3 Topic Continuity: Rule 2. Rule 2?stating a preference not just to keep talking
about the same objects, but to preserve their relative ranking?also seems much less
robust than Rule 1, irrespective of its formulation and of the instantiation.
49 The respective role of entity coherence, relational coherence, and other forms of coherence in the
examples in our corpus is studied in more detail in Oberlander and Poesio (2002).
355
Poesio et al Centering: A Parametric Theory
As already noted, one of the most interesting observations about this aspect of
the theory concerns the classification of utterances used to formalize it (at least in
the earlier versions of the theory). With pretty much all parameter instantiations that
we tested, two of the most common transitions were the NULL transition (between
two utterances neither of which has a CB), previously considered only in Passonneau
(1998), and the ZERO transition (from an utterance with a CB to one without), which
as far as we can see has never been discussed before. Indeed, with the vanilla in-
stantiation, 84% of all utterances are either NULL, ZERO, or EST and therefore fall
outside the scope of Rule 2 in almost all its formulations. The question raised by this
finding is whether the theory has to be extended to cover such cases or whether they
have to be accounted for by other components of an overall theory of discourse (see
below).
Three versions of Rule 2 were tested in some detail.50 The version of Rule 2 from
Grosz, Joshi, and Weinstein (1995), formulated in terms of sequences and stating a
preference for sequences of CON over sequences of RET over sequences of SHIFT
(which we tested by counting the number of sequence pairs), suffers from the prob-
lem that even with the ?best? instantiations, fewer than one-third of sequence pairs
involve the same transition, and even fewer are sequences of the transitions consid-
ered by Grosz et al Even in the instantiation which yields the best results for Rule 2
(BFP), IS with STRUBE-HAHN ranking, only 13% of sequence pairs are of the form
CON-CON/RET-RET/SH-SH, and altogether only 28% of sequence pairs involve only
transitions considered by Grosz et al Keeping in mind that Rule 2 (GJW 95) applies
only to a minority of sequence pairs, we do find that with IS and STRUBE-HAHN
ranking, the number of CON-CON sequences (37) slightly exceeds the number of RET-
RET (35), which in turn exceeds the number of SH-SH (19, of which 16 are RSH-RSH).
This doesn?t hold with GFTHERELIN ranking, where RET-RET exceeds CON-CON
even if we treat EST as a type of CON; we find no significant difference between the
IF and the IS setting.
Rule 2 (BFP), formulated in terms of single transitions, accounts for larger per-
centages of the data (single utterances) and was found to be verified both with the
vanilla instantiation and with the ?best? instantiations. However, we still observed a
large percentage of NULL transitions with most instantiations; we also found more
RET than CON and more RSH than SSH in most instantiations in which utterances
are identified with sentences or allow for indirect realization.51
Finally, Strube and Hahn?s preference for sequences of cheap transitions over se-
quences of expensive ones isn?t verified by any of the instantiations we tested; in-
deed, in all instantiations we studied we found more expensive transitions than cheap
ones, meaning that the CP of one utterance generally doesn?t predict the CB of the
next.
These mixed results are in line with those of psychological experiments, which
so far haven?t found clear evidence supporting the claim that, say, Continuations
are easier to process than Shifts, let alne Retains (Gordon, Grosz, and Gillion
1993)
50 As noted earlier, an earlier version of Kibble?s proposal was also tested; the results can be viewed on
the companion Web site.
51 CON can be made the most frequent transition by merging EST and CON. We found, however, that
this merging leads to worse results as far as the correlation between the classification of transitions and
two of the linguistic phenomena for which the classification has been used: predicting the form of
subject NPs, and predicting segment boundaries. These results are discussed in the technical
report.
356
Computational Linguistics Volume 30, Number 3
5.3 Theoretical Consequences
While proposing modifications of centering is beyond the scope of this article, we
believe our results do have broad theoretical consequences worthy of further explo-
ration.
5.3.1 Clarification of the Claims and Identification of Further Parameters. Apart from
comparing different ways of setting the parameters already discussed in the literature,
our work had the more fundamental goal of clarifying the claims of centering theory
by identifying aspects that need to be made more precise. Our study raised a number
of questions about the definitions of the concepts used in centering not previously
examined in the literature or only discussed in passing.
Many of these questions have to do with realization, one of the least studied as-
pects of the theory. One such question is the status of entities realized as second-person
pronouns. Our results indicated that if PRO2s are not considered realizations of CF, or
if we treat them as R1-pronouns, we find many more violations of Strong C1 and Rule
1, respectively (although both claims are still verified). We also saw that the results
concerning Constraint 1 and Rule 1 depended on whether reduced relative clauses and
non-finite VPs were assumed to contain traces and whether or not these traces were
assumed to be R1-pronouns. More generally, we identified the need for a clear defi-
nition of ?R1-pronoun?: that is, whether we should include traces in relative clauses,
the implicit anaphoric elements of bridging references, and demonstrative pronouns
among the ?pronouns? to which Rule 1 applies. This question isn?t mentioned in the
literature we know of, yet our results indicate that, for example, treating the implicit
anaphoric elements of bridging references or second-person pronouns as R1-pronouns
is a very bad idea.
Some of the issues raised by this study are relevant only for certain parameter in-
stantiations. One example is the specification of grammatical function ranking beyond
the simplest cases: for example, whether postcopular NPs in there-clauses should be
treated as subjects or objects (our results suggest the former) or how nominal mod-
ifiers should be ranked (we treated them as adjuncts). An issue for instantiations in
which utterances are identified with finite clauses is what the previous utterance is
when an embedded finite clause is in the middle of another finite clause, rather than
at the end, as in the following example, from the Guardian:
(25) But Hutchinson, who appointed Ranieri last season, today said that he
spent 30 minutes with the Italian after the Blackburn match and that
resignation was never an issue.
5.3.2 Separating Entity Coherence from CB Uniqueness. Starting with Brennan, Fried-
man, and Pollard (1987) and, more recently, Beaver (2004) and Kibble (2001) there have
been attempts to ?unpack? some of the original preferences proposed by centering.
We feel this work has greatly helped our understanding of the theory and believe that
it would be similarly useful to unpack Constraint 1 into two separate claims, as well:
one about uniqueness of the CB, and one about entity coherence.
The first function of (both versions of) Constraint 1 is to claim that the CB is
unique. We will call this claim CB uniqueness:
CB Uniqueness: Utterances have at most one CB.
We have argued throughout the article that Strong Constraint 1 has a second function
as well: to express a preference for utterances that do not occur at the beginning of a
357
Poesio et al Centering: A Parametric Theory
segment to mention at least one of the objects included in the previous utterance. Fol-
lowing Kibble (2000) and Karamanis (2001), we will call this second half of Constraint
1 (entity) continuity:
(Entity) Continuity: CF(Ui?1) ? CF(Ui) = ?
Weak C1 is CB uniqueness, whereas Strong C1 is CB uniqueness plus continuity.
5.3.3 A Hybrid View of Coherence. One clear conclusion suggested by our results is
that entity-based accounts of coherence need to be supplemented by accounts of other
factors that induce coherence at the local level. The most direct way to do this would
be to include in continuity a longer list of factors that may link an utterance to its
previous one and claim that in order for an utterance to be ?locally coherent,? at least
one of these factors must be present. The resulting claim would take a form along the
following lines:
Hybrid Continuity For every utterance Ui, at least one of the following must
hold:
1. CF(Ui?1) ? CF(Ui) = ?;
2. There is a rhetorical relation RR such that RR(Ui?1,Ui).52
3. Ui?1 and Ui are temporally coherent in the sense, for
example, of Kameyama, Passonneau, and Poesio (1993).
4. . . . (other factors)
A more sensible approach, especially as we don?t yet know all the factors affecting
coherence, would be to be more explicit about the scope of centering theory, viewing
it not as a comprehensive account of ?local coherence,? but only as an account of the
contribution of entity coherence to local coherence. In other words, we could view
(Entity) Continuity as only one among the preferences holding at the discourse level.
A natural way to formalize this would be to include Entity Continuity among a set of
constraints like those proposed by Beaver, which would also have to include further
constraints specifying preferences for rhetorical and temporal coherence.
5.3.4 CB Uniqueness. We saw in Section 4 that it?s fairly easy to fix the problem of
utterances? violating Weak C1, or CB uniqueness: All that is needed is to strenghten
the requirements on the ranking function and require it to be total, which in turn can
be easily done by adding a disambiguation factor to ranking functions that aren?t total
like grammatical function. Before doing this, however, we should ask whether this is
the conclusion we should draw from the finding that CB uniqueness will be violated
with partial ranking functions?or if instead we should allow for utterances to have
more than one CB.
When multi-CB utterances such as (10) are considered, it is not immediately ob-
vious that one discourse entity (the corner cupboard) is more salient than the other
(Branicki), especially since neither occupies a particularly salient position either in the
previous utterance (u227) or in the current one (u229). Notice also that both entities
have been mentioned before; and furthermore, one of them is animate (Branicki), the
52 This formulation was intentionally designed in such a way as to finesse the issue of whether RR
should be an informational-level relation between the eventualities expressed by the utterances or a
genuine rhetorical relation between the speech acts performed by them.
358
Computational Linguistics Volume 30, Number 3
other inanimate (the cupboard). In these respects, these examples are reminiscent of
the examples that led Sidner (1979) to argue for two foci?sentences with one an-
imate entity (typically in agent position) and an inanimate one (typically in theme
position), like Mortimer sold the book for 10 cents or Mary took a nickel from her toy bank
yesterday. Although the results from studies such as Gordon, Grosz, and Gillion (1993)
suggest that when two animate entities are considered, only one tends to show RNP
effects, we are not aware of any experiment testing materials like those discussed by
Sidner.
The hypothesis that topicality is not restricted to one entity per utterance has been
advanced by a number of researchers, although it is perhaps most clearly associated
with the work of Givon (1983). Within the centering literature, abandoning the claim
that we call ?CB uniqueness? has been suggested by Gundel (1998) and, more radically,
in work such as Strube (1998), Gordon and Hendrick (1999), and Tetreault (2001) in
which the whole notion of CB is abandoned.
As seen in Section 2, the primary motivation for CB uniqueness is complexity-
theoretic arguments: Inference in monadic logics is less expensive than in normal
logics (Joshi and Kuhn 1979; Joshi and Weinstein 1981). Grosz and colleagues?s lin-
guistic evidence for CB uniqueness is contrasts like those in (3), showing that failing to
pronominalize certain entities (Susan, in that example) is a more serious problem than
failing to pronominalize others (Betsy). This claim is further supported by the evidence
concerning the Repeated Name Penalty (Gordon, Grosz, and Gillion 1993). However,
the RNP is observed only in a subset of the cases that would be considered as CB
mentions according to the definition provided by Constraint 3, and in the example we
are discussing (10), neither Branicki nor the cupboard occur in (u229) in a position that
would be subject to RNP effects according to Gordon et al In other words, (some)
evidence used by Grosz et al in support of CB uniqueness cannot be used to argue
that (u229) in (10) has a single CB. This evidence is also consistent with a different
solution to the problem raised by examples like (10): Instead of attempting to preserve
CB uniqueness by requiring the ranking function to be total, one could abandon CB
uniqueness, as suggested in Givon (1983) and Gundel (1998). In both cases, we would
need a separate theoretical account of RNP effects. More empirical evidence is needed
on this issue.53
5.3.5 Variety. The third conclusion suggested by our results is that ensuring variety
seems to be as important a principle in discourse production as maintaining coher-
ence. This is suggested, first of all, by the fact that only slightly over half of CBs are
realized as R1-pronouns. It is also the case that CBs are hardly ever continued for
more than two or three utterances; that the same discourse entity is very unlikely
to be realized using the same type of NP twice in a row (even with pronouns, we
only have 58 pronoun-pronoun sequences?26% of the total); and that two-thirds of
all transition sequences involve two different transitions. In fact, we wonder whether
that the repeated name penalty observed by Gordon et al might not be an instance of
this more general phenomenon.
53 One way to reconcile the different findings would be to use different conceptual tools to characterize
the connection between subsequent utterances. Each utterance satisfying Continuity would have one or
more links to the previous utterance, that we might call centers of coherence; Entity Continuity would
then become a preference for the set of centers of coherence to be nonempty. In particular situations,
may be experimentally identified using the RNP as a test, one of the centers of coherence may acquire
a particular status, leading to a preference for pronominalization. We may call this center the center of
salience, say. It would also be interesting to examine the connection between a solution along these
lines and Sidner?s solution involving two foci.
359
Poesio et al Centering: A Parametric Theory
Acknowledgments
Special thanks to Nikiforos Karamanis,
Alistair Knott, Mark Liberman, Ruslan
Mitkov, Jon Oberlander, Tim Rakow, and
the other members of the GNOME project:
Kees van Deemter, Renate Henschel, Rodger
Kibble, Jamie Pearson, and Donia Scott. We
also wish to thank James Allen, Jennifer
Arnold, Steve Bird, Susan Brennan, Donna
Byron, Herb Clark, George Ferguson,
Jeanette Gundel, Aravind Joshi, Eleni
Miltsakaki, Rashmi Prasad, Ellen Prince,
Len Schubert, Harold Somers, Joel Tetreault,
Lyn Walker, and audiences at the ACL 2000,
the University of Pennsylvania, the
University of Rochester, CLUK, and the
University of Wolverhampton for comments
and suggestions. The corpus presented here
was annotated by Debbie De Jongh, Ben
Donaldson, Marisa Flecha-Garcia, Camilla
Fraser, Michael Green, Shane Montague,
Carol Rennie, and Claire Thomson, together
with the authors. A substantial part of this
work, including the creation of the corpus,
was supported by the EPSRC project
GNOME, GR/L51126/01. Massimo Poesio
was supported during parts of this project
by an EPSRC advanced fellowship. Barbara
Di Eugenio was supported in part by NSF
grant INT 9996195, in part by NATO grant
CRG 9731157. Janet Hitzeman was in part
supported by the EPSRC project SOLE,
GR/L50341.
References
Alshawi, Hiyan. 1987. Memory and Context
for Language Interpretation. Cambridge
University Press, Cambridge.
Arnold, Jennifer E. 1998. Reference Form and
Discourse Patterns. Ph.D. thesis, Stanford
University, Stanford CA.
Baker, Collin F., Charles J. Fillmore, and
John B. Lowe. 1998. The Berkeley
FrameNet project, Montreal, Canada. In
Proceedings of the 36th ACL, pages 86?90.
Beaver, David. 2004. The optimization of
discourse anaphora. Linguistics and
Philosophy, 27(1):3?56.
Brennan, Susan E. 1995. Centering attention
in discourse. Language and Cognitive
Processes, 10:137?167.
Brennan, Susan E., Marilyn W. Friedman,
and Charles J. Pollard. 1987. A centering
approach to pronouns. In Proceedings of the
25th ACL, Stanford, CA, pages 155?162, June.
Byron, Donna and Amanda Stent. 1998. A
preliminary model of centering in dialog.
In Proceedings of the 36th ACL, Montreal,
Canada.
Caramazza, Alfonso, Ellen Grober,
Catherine Garvey, and Jack Yates. 1977.
Comprehension of anaphoric pronouns.
Journal of Verbal Learning and Verbal
Behavior, 16:601?609.
Carletta, Jean. 1996. Assessing agreement on
classification tasks: The kappa statistic.
Computational Linguistics, 22(2):249?254.
Chafe, Wallace. 1976. Givenness,
contrastiveness, definiteness, subjects, and
topics. In C. Li, editor, Subject and Topic.
Academic Press, New York, pages 25?76.
Chinchor, Nancy A. and Beth Sundheim.
1995. Message understanding conference
(MUC) tests of discourse processing. In
Proceedings of the AAAI Spring Symposium
on Empirical Methods in Discourse
Interpretation and Generation, pages 21?26,
Stanford, CA.
Clark, Herbert H. 1977. Bridging. In P. N.
Johnson-Laird and P. C. Wason, editors,
Thinking: Readings in Cognitive Science,
pages 411?420, Cambridge University
Press, London and New York.
Cooreman, Ann and Tony Sanford. 1996.
Focus and syntactic subordination in
discourse. Research Paper no. RP-79,
University of Edinburgh, HCRC.
Cote, Sharon. 1998. Ranking
forward-looking centers. In M. A. Walker,
A. K. Joshi, and E. F. Prince, editors,
Centering Theory in Discourse. Oxford,
University Press, Oxford, chapter 4,
pages 55?70.
Dale, Robert. 1992. Generating Referring
Expressions. MIT Press, Cambridge, MA.
Di Eugenio, Barbara. 1998. Centering in
Italian. In M. A. Walker, A. K. Joshi, and
E. F. Prince, editors, Centering Theory in
Discourse. Oxford University Press,
Oxford, chapter 7, pages 115?138.
Di Eugenio, Barbara, Johanna D. Moore,
and Massimo Paolucci. 1997. Learning
features that predict cue usage. In
Proceedings of the 35th ACL, Madrid.
Fox, Barbara A. 1987. Discourse Structure and
Anaphora. Cambridge University Press,
Cambridge.
Gernsbacher, Morton A. and
David Hargreaves. 1988. Accessing
sentence participants: The advantage of
first mention. Journal of Memory and
Language, 27:699?717.
Giouli, Paraskevi. 1996. Topic chaining and
discourse structure in task-oriented
dialogues. Master?s thesis, Linguistics
Department, University of Edinburgh.
Givon, Talmy, editor. 1983. Topic Continuity
in Discourse: A Quantitative Cross-Language
Study. Benjamins, Amsterdam and
Philadelphia.
360
Computational Linguistics Volume 30, Number 3
Gordon, Peter C., Barbara J. Grosz, and
Laura A. Gillion. 1993. Pronouns, names,
and the centering of attention in
discourse. Cognitive Science, 17:311?348.
Gordon, Peter C. and Randall Hendrick.
1999. The representation and processing
of coreference in discourse. Cognitive
Science, 22:389?424.
Gordon, Peter C., Randall Hendrick,
Kerry Ledoux, and Chin L. Yang. 1999.
Processing of reference and the structure
of language: An analysis of complex noun
phrases. Language and Cognitive Processes,
14(4):353?379.
Grosz, Barbara J. 1977. The Representation and
Use of Focus in Dialogue Understanding.
Ph.D. thesis, Stanford University,
Stanford, CA.
Grosz, Barbara J., Aravind K. Joshi, and
Scott Weinstein. 1983. Providing a unified
account of definite noun phrases in
discourse. In Proceedings of ACL-83,
Cambridge, MA, pages 44?50.
Grosz, Barbara J., Aravind K. Joshi, and
Scott Weinstein. 1986. Towards a
computational theory of discourse
interpretation. Unpublished manuscript.
Grosz, Barbara J., Aravind K. Joshi, and
Scott Weinstein. 1995. Centering: A
framework for modeling the local
coherence of discourse. Computational
Linguistics, 21(2):202?225.
Grosz, Barbara J. and Candace L. Sidner.
1986. Attention, intention, and the
structure of discourse. Computational
Linguistics, 12(3):175?204.
Gundel, Jeanette K. 1998. Centering theory
and the givenness hierarchy: Towards a
synthesis. In M. A. Walker, A. K. Joshi,
and E. F. Prince, editors, Centering Theory
in Discourse. Oxford University Press,
Oxford, chapter 10, pages 183?198.
Gundel, Jeanette K., Nancy Hedberg, and
Ron Zacharski. 1993. Cognitive status and
the form of referring expressions in
discourse. Language, 69(2):274?307.
Hawkins, John A. 1978. Definiteness and
Indefiniteness. Croom Helm, London.
Heim, Irene. 1982. The Semantics of Definite
and Indefinite Noun Phrases. Ph.D. thesis,
University of Massachusetts at Amherst.
Henschel, Renate, Hua Cheng, and
Massimo Poesio. 2000. Pronominalization
revisited. In Proceedings of 18th COLING,
Saarbruecken, August.
Hitzeman, Janet, Alan Black, Paul Taylor,
Chris Mellish, and Jon Oberlander. 1998.
On the use of automatically generated
discourse-level information in a
concept-to-speech synthesis system. In
Proceedings of the International Conference on
Spoken Language Processing (ICSLP98),
Sydney, Australia.
Hitzeman, Janet and Massimo Poesio. 1998.
Long-distance pronominalisation and
global focus. In Proceedings of
ACL/COLING, vol. 1, pages 550?556,
Montreal.
Hobbs, Jerry R. 1979. Coherence and
coreference. Cognitive Science, 3:67?90.
Hudson, Susan B., Michael K. Tanenhaus,
and Gary S. Dell. 1986. The effect of the
discourse center on the local coherence of
a discourse. In Proceedings of the Eight
Annual Meeting of the Cognitive Science
Society, Amherst, MA, pages 96?101.
Hudson-D?Zmura, Susan and Michael K.
Tanenhaus. 1998. Assigning antecedents
to ambiguous pronouns: The role of the
center of attention as the default
assignment. In M. A. Walker, A. K. Joshi,
and E. F. Prince, editors, Centering Theory
in Discourse. Oxford University Press,
Oxford, pages 199?226.
Hurewitz, Felicia. 1998. A quantitative look
at discourse coherence. In M. A. Walker,
A. K. Joshi, and E. F. Prince, editors,
Centering Theory in Discourse. Oxford
University Press, Oxford, pages 273?291.
Joshi, Aravind K. and Steve Kuhn. 1979.
Centered logic: The role of entity centered
sentence representation in natural
language inferencing. In Proceedings of the
IJCAI, Tokyo, pages 435?439.
Joshi, Aravind K. and Scott Weinstein. 1981.
Control of inference: Role of some aspects
of discourse structure?centering. In
Proceedings of the IJCAI, Vancouver, CA,
pages 385?387.
Kameyama, Megumi. 1985. Zero Anaphora:
The Case of Japanese. Ph.D. thesis, Stanford
University, Stanford, CA.
Kameyama, Megumi. 1986. A
property-sharing constraint in centering.
In Proceedings of the ACL-86, New York,
pages 200?206.
Kameyama, Megumi. 1998. Intra-sentential
centering: A case study. In M. A. Walker,
A. K. Joshi, and E. F. Prince, editors,
Centering Theory in Discourse. Oxford
University Press, Oxford, chapter 6,
pages 89?112.
Kameyama, Megumi, Rebecca Passonneau,
and Massimo Poesio. 1993. Temporal
centering. In Proceedings of the 31st ACL,
pages 70?77, Columbus, OH.
Kamp, Hans and Uwe Reyle. 1993. From
Discourse to Logic. Reidel, Dordrecht.
Karamanis, Nikiforos. 2001. Exploring
entity-based coherence. In Proceedings of
the Fourth CLUK. University of Sheffield,
pages 18?26.
361
Poesio et al Centering: A Parametric Theory
Karamanis, Nikiforos. 2003. Entity Coherence
for Descriptive Text Structuring. Ph.D.
thesis, Informatics, University of
Edinburgh.
Karttunen, Lauri. 1976. Discourse referents.
In J. McCawley, editor, Syntax and
Semantics 7?Notes from the Linguistic
Underground. Academic Press,
New York, pages 363?385.
Kibble, Rodger. 2000. A reformulation of
rule 2 of centering theory. Technical
report, ITRI, University of Brighton.
Kibble, Rodger. 2001. A reformulation of
Rule 2 of centering theory. Computational
Linguistics, 27(4):579?587.
Kibble, Rodger and Richard Power. 2000. An
integrated framework for text planning
and pronominalization. In Proceedings of
INLG, Mitzpe Ramon, Israel, June.
Kintsch, Walter and Teun van Dijk. 1978.
Towards a model of discourse
comprehension and production.
Psychological Review, 85:363?394.
Knott, Alistair, Jon Oberlander,
Mick O?Donnell, and Chris Mellish. 2001.
Beyond elaboration: The interaction of
relations and focus in coherent text. In
T. Sanders, J. Schilperoord, and
W. Spooren, editors, Text Representation:
Linguistic and Psycholinguistic Aspects. John
Benjamins, Amsterdam and Philadelphia,
pages 181-196.
Lappin, Shalom and Herbert J. Leass. 1994.
An algorithm for pronominal anaphora
resolution. Computational Linguistics,
20(4):535?562.
Lascarides, Alex and Nick Asher. 1993.
Temporal interpretation, discourse
relations and commonsense entailment.
Linguistics and Philosophy, 16(5):437?493.
Mann, William C. and Sandra A.
Thompson. 1988. Rhetorical structure
theory: Towards a functional theory of
text organization. Text, 8(3):243?281.
Marcu, Daniel. 1999. Instructions for
manually annotating the discourse
structures of texts. Unpublished
manuscript, USC/ISI.
McKeown, Kathy R. 1985. Discourse
strategies for generating natural-language
text. Artificial Intelligence, 27(1):1?41.
Miltsakaki, Eleni. 1999. Locating topics in
text processing. In Proceedings of CLIN,
Utrecht, pages 127?138.
Miltsakaki, Eleni. 2002. Towards an
aposynthesis of topic continuity and
intrasentential anaphora. Computational
Linguistics, 28(3):319?355.
Moser, Megan and Johanna D. Moore. 1996.
Toward a synthesis of two accounts of
discourse structure. Computational
Linguistics, 22(3):409?419.
Oberlander, Jon, Mick O?Donnell,
Alistair Knott, and Chris Mellish. 1998.
Conversation in the museum:
Experiments in dynamic hypermedia with
the intelligent labelling explorer. New
Review of Hypermedia and Multimedia,
4:11?32.
Oberlander, Jon and Massimo Poesio. 2002.
Entity coherence and relational coherence:
a corpus-based investigation. Paper
presented at the Berlin workshop on
Topics in Discourse, September.
Passonneau, Rebecca J. 1993. Getting and
keeping the center of attention. In
M. Bates and R. M. Weischedel, editors,
Challenges in Natural Language Processing.
Cambridge University Press, Cambridge,
chapter 7, pages 179?227.
Passonneau, Rebecca J. 1997. Instructions
for applying discourse reference
annotation for multiple applications
(DRAMA). Unpublished manuscript.
Passonneau, Rebecca J. 1998. Interaction of
discourse structure with explicitness of
discourse anaphoric noun phrases. In
M. A. Walker, A. K. Joshi, and E. F.
Prince, editors, Centering Theory in
Discourse. Oxford University Press,
Oxford, chapter 17, pages 327?358.
Passonneau, Rebecca J. and Diane Litman.
1993. Feasibility of automated discourse
segmentation. In Proceedings of 31st Annual
Meeting of the ACL, Columbus, OH, pages
148?155.
Pearson, Jamie, Rosemary Stevenson, and
Massimo Poesio. 2000. Pronoun resolution
in complex sentences. In Proceedings of
AMLAP, Leiden.
Pearson, Jamie, Rosemary Stevenson, and
Massimo Poesio. 2001. The effects of
animacy, thematic role, and surface
position on the focusing of entities in
discourse. In M. Poesio, editor, Proceedings
of the First SEMPRO, University of
Edinburgh.
Poesio, Massimo. 2000. Annotating a corpus
to develop and evaluate discourse entity
realization algorithms: Issues and
preliminary results. In Proceedings of the
2nd LREC, pages 211?218, Athens, May.
Poesio, Massimo. 2003. Associative
descriptions and salience. In Proceedings of
the EACL Workshop on Computational
Treatments of Anaphora, Budapest.
Poesio, Massimo. 2004. The
MATE/GNOME scheme for anaphoric
annotation, revisited. In Proceedings of
SIGDIAL, Boston, May.
362
Computational Linguistics Volume 30, Number 3
Poesio, Massimo, Florence Bruneseaux, and
Laurent Romary. 1999. The MATE
meta-scheme for coreference in dialogues
in multiple languages. In M. Walker,
editor, Proceedings of the ACL Workshop on
Standards and Tools for Discourse Tagging,
pages 65?74.
Poesio, Massimo and Barbara Di Eugenio.
2001. Discourse structure and anaphoric
accessibility. In Ivana Kruijff-Korbayova?
and Mark Steedman, editors, Proceedings
of the ESSLLI 2001 Workshop on Information
Structure, Discourse Structure and Discourse
Semantics, Helsinki.
Poesio, Massimo and
Natalia Nygren-Modjeska. 2003. The
THIS-NP hypothesis: A corpus-based
investigation. In Proceedings of DAARC,
Lisbon.
Poesio, Massimo and Rosemary Stevenson.
Forthcoming. Salience: Theoretical Models and
Empirical Evidence. Cambridge University
Press, Cambridge and New York.
Poesio, Massimo and Renata Vieira. 1998. A
corpus-based investigation of definite
description use. Computational Linguistics,
24(2):183?216.
Prasad, Rashmi and Michael Strube. 2000.
Discourse salience and pronoun
resolution in Hindi. In Penn Working
Papers in Linguistics, volume 6, University
of Pennsylvania, Philadelphia,
pages 189?208.
Prince, Ellen F. 1981. Toward a taxonomy of
given-new information. In P. Cole, editor,
Radical Pragmatics. Academic Press, New
York, pages 223?256.
Prince, Ellen F. 1992. The ZPG letter:
Subjects, definiteness, and information
status. In S. Thompson and W. Mann,
editors, Discourse Description: Diverse
Analyses of a Fund-Raising Text. Benjamins,
Amsterdam and Philadelphia,
pages 295?325.
Quirk, Randolph and Sidney Greenbaum.
1973. A University Grammar of English.
Longman, Harlow, England.
Rambow, Owen. 1993. Pragmatics aspects
of scrambling and topicalization in
German. In Proceedings of the Workshop
on Centering Theory in Naturally-
Occurring Discourse. Institute for
Research in Cognitive Science,
Philadelphia.
Sanford, Anthony J. and Simon C. Garrod.
1981. Understanding Written Language.
Wiley, Chichester, England.
Scott, Donia, Richard Power, and
Roger Evans. 1998. Generation as a
solution to its own problem. In
Proceedings of the ninth International
Workshop on Natural Language Generation,
Niagara-on-the-Lake, CA.
Sgall, Petr. 1967. Functional sentence
perspective in a generative description.
Prague Studies in Mathematical Linguistics,
2:203?225.
Sidner, Candace L. 1979. Towards a
Computational Theory of Definite Anaphora
Comprehension in English Discourse. Ph.D.
thesis, Massachusetts Institute of
Technology, Cambridge.
Siegel, Sidney and N. John Castellan. 1988.
Nonparametric Statistics for the Behavioral
Sciences, 2nd edition McGraw-Hill, Boston.
Stevenson, Rosemary J., Rosalind A.
Crawley, and David Kleinman. 1994.
Thematic roles, focus, and the
representation of events. Language and
Cognitive Processes, 9:519?548.
Stevenson, Rosemary, Alistair Knott,
Jon Oberlander, and Scott McDonald.
2000. Interpreting pronouns and
connectives: Interactions between
focusing, thematic roles and coherence
relations. Language and Cognitive Processes,
15, pages 225?262.
Strube, Michael. 1998. Never look back: An
alternative to centering. In Proceedings of
COLING-ACL, pages 1251?1257, Montreal.
Strube, Michael and Udo Hahn. 1999.
Functional centering?Grounding
referential coherence in information
structure. Computational Linguistics,
25(3):309?344.
Suri, Linda Z. and Kathleen F. McCoy. 1994.
RAFT/RAPR and centering: A
comparison and discussion of problems
related to processing complex sentences.
Computational Linguistics, 20(2):301?317.
Tetreault, Joel R. 2001. A corpus-based
evaluation of centering and pronoun
resolution. Computational Linguistics,
27(4):507?520.
Turan, Umit. 1998. Ranking forward-looking
centers in Turkish: Universal and
language-specific properties. In M. A.
Walker, A. K. Joshi, and E. F. Prince,
editors, Centering Theory in Discourse.
Oxford University Press, Oxford,
chapter 8, pages 139?160.
Vallduvi, Enric. 1990. The Informational
Component. Ph.D. thesis, University of
Pennsylvania, Philadelphia.
van Deemter, Kees and Rodger Kibble. 2000.
On coreferring: Coreference in MUC and
related annotation schemes. Computational
Linguistics, 26(4):629?637.
Walker, Marilyn A. 1989. Evaluating
discourse processing algorithms. In
Proceedings ACL-89, pages 251?261,
Vancouver, British Columbia, Canad, June.
363
Poesio et al Centering: A Parametric Theory
Walker, Marilyn A. 1993. Initial contexts and
shifting centers. In Proceedings of the
Workshop on Centering, University of
Pennsylvania, Philadelphia.
Walker, Marilyn A., Masayo Iida, and
Sharon Cote. 1994. Japanese discourse
and the process of centering.
Computational Linguistics, 20(2):193?232.
Walker, Marilyn A., Aravind K. Joshi, and
Ellen F. Prince. 1998a. Centering in
naturally occurring discourse: An
overview. In M. A. Walker, A. K. Joshi,
and E. F. Prince, editors, Centering Theory
in Discourse. Oxford University Press,
Oxford, chapter 1, pages 1?28.
Walker, Marilyn A., Aravind K. Joshi, and
Ellen F. Prince, editors. 1998b. Centering
Theory in Discourse. Oxford University
Press, Oxford.
Webber, Bonnie L. 1978. A formal approach
to discourse anaphora. Report no. 3761,
BBN, Cambridge, MA.
Evaluating Centering for Information
Ordering Using Corpora
Nikiforos Karamanis?
University of Cambridge
Chris Mellish??
University of Aberdeen
Massimo Poesio?
University of Essex
Jon Oberlander?
University of Edinburgh
In this article we discuss several metrics of coherence defined using centering theory and
investigate the usefulness of such metrics for information ordering in automatic text generation.
We estimate empirically which is the most promising metric and how useful this metric is using
a general methodology applied on several corpora. Our main result is that the simplest metric
(which relies exclusively on NOCB transitions) sets a robust baseline that cannot be outperformed
by other metrics which make use of additional centering-based features. This baseline can be used
for the development of both text-to-text and concept-to-text generation systems.
1. Introduction
Information ordering (Barzilay and Lee 2004), that is, deciding in which sequence to
present a set of preselected information-bearing items, has received much attention in
recent work in automatic text generation. This is because text generation systems need
to organize the content in a way that makes the output text coherent, that is, easy to read
and understand. The easiest way to exemplify coherence is by arbitrarily reordering the
sentences of a comprehensible text. This process very often gives rise to documents that
do not make sense although the information content is the same before and after the
reordering (Hovy 1988; Marcu 1997; Reiter and Dale 2000).
Entity coherence, which is based on the way the referents of noun phrases (NPs)
relate subsequent clauses in the text, is an important aspect of textual organization.
Since the early 1980s, when it was first introduced, centering theory has been an
influential framework for modelling entity coherence. Seminal papers on centering such
as Brennan, Friedman [Walker], and Pollard (1987, page 160) and Grosz, Joshi, and
Weinstein (1995, page 215) suggest that centering may provide solutions for information
ordering.
Indeed, following the pioneering work of McKeown (1985), recent work on text
generation exploits constraints on entity coherence to organize information (Mellish
et al 1998; Kibble and Power 2000, 2004; O?Donnell et al 2001; Cheng 2002; Lapata
? Computer Laboratory, William Gates Building, Cambridge CB3 0FD, UK.
Nikiforos.Karamanis@cl.cam.ac.uk.
?? Department of Computing Science, King?s College, Aberdeen AB24 3UE, UK.
? Department of Computer Science, Wivenhoe Park, Colchester CO4 3SQ, UK.
? School of Informatics, 2 Buccleuch Place, Edinburgh EH8 9LW, UK.
Submission received: 15 May 2006; revised submission received: 15 December 2007; accepted for publication:
7 January 2008.
? 2008 Association for Computational Linguistics
Computational Linguistics Volume 35, Number 1
2003; Barzilay and Lee 2004; Barzilay and Lapata 2005, among others). Although these
approaches often make use of heuristics related to centering, the features of entity
coherence they employ are usually defined informally. Additionally, centering-related
features are combined with other coherence-inducing factors in ways that are based
mainly on intuition, leaving many equally plausible options unexplored.
Thus, the answers to the following questions remain unclear: (i) How appropriate
is centering for information ordering in text generation? (ii) Which aspects of centering are
most useful for this purpose? These are the issues we investigate in this paper, which
presents the first systematic evaluation of centering for information ordering. To do this,
we define centering-based metrics of coherence which are compatible with several extant
information ordering approaches. An important insight of our work is that centering
can give rise to many such metrics of coherence. Hence, a general methodology
for identifying which of these metrics represent the most promising candidates for
information ordering is required.
We adopt a corpus-based approach to compare the metrics empirically and
demonstrate the portability and generality of our evaluation methods by experimenting
with several corpora. Our main result is that the simplest metric (which relies
exclusively on NOCB transitions) sets a baseline that cannot be outperformed by
other metrics that make use of additional centering-related features. Thus, we provide
substantial insight into the role of centering as an information ordering constraint and
offer researchers working on text generation a simple, yet robust, baseline to use against
their own information ordering approaches during system development.
The article is structured as follows: In Section 2 we discuss our information ordering
approach in relation to other work on text generation. After a brief introduction
to centering in Section 3, Section 4 demonstrates how we derived centering data
structures from existing corpora. Section 5 discusses how centering can be used to
define various metrics of coherence suitable for information ordering. Then, Section 6
outlines a corpus-based methodology for choosing among these metrics. Section 7
reports on the results of our experiments and Section 8 discusses their implications.
We conclude the paper with directions for future work and a summary of our main
contributions.1
2. Information Ordering
Information ordering has been investigated by substantial recent work in text-to-
text generation (Barzilay, Elhadad, and McKeown 2002; Lapata 2003; Barzilay and
Lee 2004; Barzilay and Lapata 2005; Bollegala, Okazaki, and Ishizuka 2006; Ji and
Pulman 2006; Siddharthan 2006; Soricut and Marcu 2006; Madnani et al 2007,
among others) as well as concept-to-text generation (particularly Kan and McKeown
[2002] and Dimitromanolaki and Androutsopoulos 2003).2 We added to this work
by presenting approaches to information ordering based on a genetic algorithm
(Karamanis and Manurung 2002) and linear programming (Althaus, Karamanis, and
Koller 2004) which can be applied to both concept-to-text and text-to-text generation.
These approaches use a metric of coherence defined using features derived from
1 Earlier versions of this work were presented in Karamanis et al (2004) and Karamanis (2006).
2 Concept-to-text generation is concerned with the automatic generation of text from some underlying
non-linguistic representation. By contrast, the input to text-to-text generation applications is text.
30
Karamanis et al Centering for Information Ordering
centering and will serve as the premises of our investigation of centering in this
article.
Metrics of coherence are used in other work on text generation, too (Mellish et al
1998; Kibble and Power 2000, 2004; Cheng 2002). With the exception of Kibble and
Power?s work, the features of entity coherence used in these metrics are informally
defined using heuristics related to centering. Additionally, the metrics are further
specified by combining these features with other coherence-inducing factors such
as rhetorical relations (Mann and Thompson 1987). However, as acknowledged in
most of this work, these are preliminary computational investigations of the complex
interactions between different types of coherence which leave many other equally
plausible combinations unexplored.
Clearly, one would like to know what centering can achieve on its own before
devising more complicated metrics. To address this question, we define metrics which
are purely centering-based, placing any attempt to specify a more elaborate model of
coherence beyond the scope of this article. This strategy is similar to most work on
centering for text interpretation in which additional constraints on coherence are not
taken into account (the papers in Walker, Joshi, and Prince [1998] are characteristic
examples). This simplification makes it possible to assess for the first time how useful
the employed centering features are for information ordering.
Work on text generation which is solely based on rhetorical relations (Hovy 1988;
Marcu 1997, among others) typically masks entity coherence under the ELABORATION
relation. However, ELABORATION has been characterized as ?the weakest of all
rhetorical relations? (Scott and de Souza 1990, page 60). Knott et al (2001) identified
several theoretical problems all related to ELABORATION and suggested that this relation
be replaced by a theory of entity coherence for text generation. Our work builds on this
suggestion by investigating how appropriate centering is as a theory of entity coherence
for information ordering.
McKeown (1985, pages 60?75) also deployed features of entity coherence to
organize information for text generation. McKeown?s ?constraints on immediate focus?
(which are based on the model of entity coherence that was introduced by Sidner
[1979] and precedes centering) are embedded within the schema-driven approach to
generation which is rather domain-specific (Reiter and Dale 2000). By contrast, our
metrics are general and portable across domains and can be applied within information
ordering approaches which are applicable to both concept-to-text and text-to-text
generation.
3. Centering Overview
This section provides an overview of centering, focusing on the aspects which are most
closely related to our work. Poesio et al (2004) and Walker, Joshi, and Prince (1998)
discuss centering and its relation to other theories of coherence in more detail.
According to Grosz, Joshi, and Weinstein (1995), each utterance Un is assigned a
ranked list of forward looking centers (i.e., discourse entities) denoted as CF(Un). The
members of CF(Un) must be realized by the NPs in Un (Brennan, Friedman [Walker],
and Pollard 1987). The first member of CF(Un) is called the preferred center
CP(Un).
The backward looking center CB(Un) links Un to the previous utterance Un?1.
CB(Un) is defined as the most highly ranked member of CF(Un?1) which also belongs
to CF(Un). CF lists prior to CF(Un?1) are not taken into account for the computation
31
Computational Linguistics Volume 35, Number 1
Table 1
Centering transitions are defined according to whether the backward looking center, CB, is
the same in two subsequent utterances, Un?1 and Un, and whether the CB of the current
utterance, CB(Un), is the same as its preferred center, CP(Un). These identity checks are also
known as the principles of COHERENCE and SALIENCE, the violations of which are denoted
with an asterisk.
COHERENCE: COHERENCE?:
CB(Un)=CB(Un?1) CB(Un) =CB(Un?1)
or CB(Un?1) undef.
SALIENCE: CB(Un)=CP(Un) CONTINUE SMOOTH-SHIFT
SALIENCE?: CB(Un) =CP(Un) RETAIN ROUGH-SHIFT
of CB(Un). The original formulations of centering by Brennan, Friedman [Walker], and
Pollard (1987) and Grosz, Joshi, and Weinstein (1995) lay emphasis on the uniqueness
and the locality of the CB and will serve as the foundations of our work.
The CB and the CP are combined to define transitions across pairs of
adjacent utterances (Table 1). This definition of transitions is based on Brennan,
Friedman [Walker], and Pollard (1987) and has been popular with subsequent work.
There exist several variations, however, the most important of which comes from Grosz,
Joshi, and Weinstein (1995), who define only one SHIFT transition.3
Centering makes two major claims about textual coherence, the first of which
is known as Rule 2. Rule 2 states that CONTINUE is preferred to RETAIN, which
is preferred to SMOOTH-SHIFT, which is preferred to ROUGH-SHIFT. Although the
Rule was introduced within an algorithm for anaphora resolution, Brennan, Friedman
[Walker], and Pollard (1987, page 160) consider it to be relevant to text generation
too. Grosz, Joshi, and Weinstein (1995, page 215) also take Rule 2 to suggest that
text generation systems should attempt to avoid unfavorable transitions such as
SHIFTs.
The second claim, which is implied by the definition of the CB (Poesio et al 2004),
is that CF(Un) should contain at least one member of CF(Un?1). This became known
as the principle of CONTINUITY (Karamanis and Manurung 2002). Although Grosz,
Joshi, and Weinstein and Brennan, Friedman [Walker], and Pollard do not discuss
the effect of violating CONTINUITY, Kibble and Power (2000, Figure 1) define the
additional transition NOCB to account for this case. Different types of NOCB transitions
are introduced by Passoneau (1998) and Poesio et al (2004), among others. Other
researchers, however, consider the NOCB transition to be a type of ROUGH-SHIFT
(Miltsakaki and Kukich 2004).
Kibble (2001) and Beaver (2004) introduced the principles of COHERENCE and
SALIENCE, which correspond to the identity checks used to define the transitions
(see Table 1). To improve the way centering resolves pronominal anaphora, Strube
and Hahn (1999) introduced a fourth principle called CHEAPNESS and defined it as
CB(Un)=CP(Un?1). They also redefined Rule 2 to favor transition pairs which satisfy
3 ?CB(Un?1) undef.? in Table 1 stands for the cases where Un?1 does not have a CB. Instead of classifying
the transition of Un as a CONTINUE or a RETAIN in such cases, the additional transition ESTABLISHMENT
is sometimes used (Kameyama 1998; Poesio et al 2004).
32
Karamanis et al Centering for Information Ordering
CHEAPNESS over those which violate it. This means that CHEAPNESS is given priority
over every other centering principle in Strube and Hahn?s model.
In addition to the variability caused by the numerous definitions of transitions and
the introduction of the various principles, parameters such as ?utterance,? ?ranking,?
and ?realization? can also be specified in several ways giving rise to different
instantiations of centering (Poesio et al 2004). The following section discusses how these
parameters were defined in the corpora we deploy.
4. Experimental Data
We made use of the data of Dimitromanolaki and Androutsopoulos (2003), the GNOME
corpus (Poesio et al 2004), and the two corpora that Barzilay and Lapata (2005)
experimented with. In this section, we discuss how the centering representations we
utilize were derived from each corpus.
4.1 The MPIRO-CF Corpus
Dimitromanolaki and Androutsopoulos (2003, henceforth D&A) derived facts from the
database of the MPIRO concept-to-text generation system (Isard et al 2003), realized
them as sentences, and organized them in sets. Each set consisted of six facts which
were ordered by a domain expert. The orderings produced by this expert were shown
to be very close to those produced by two other archeologists (Karamanis and Mellish
2005b).
Our first corpus, MPIRO-CF, consists of 122 orderings that were made available
to us by D&A. We computed a CF list for each fact in each ordering by applying the
instantiation of centering introduced by Kibble and Power (2000, 2004) for concept-to-
text generation. That is, we took each database fact to correspond to an ?utterance?
and specified the ?realization? parameter using the arguments of each fact as the
members of the corresponding CF list. Table 2 shows the CF lists, the CBs, the
centering transitions, and the violations of CHEAPNESS for the following example from
MPIRO-CF:
(1) (a) This exhibit is an amphora.
(b) This exhibit was decorated by the Painter of Kleofrades.
(c) The Painter of Kleofrades used to decorate big vases.
(d) This exhibit depicts a warrior performing splachnoscopy before leaving for the
battle.
(e) This exhibit is currently displayed in the Martin von Wagner Museum.
(f) The Martin von Wagner Museum is in Germany.
MPIRO facts consist of two arguments, the first of which was specified as the CP
following the definition of ?CF ranking? in O?Donnell et al (2001).4 Notice that the
second argument can often be an entity such as en914 that is realized by a canned phrase
of significant syntactic complexity (a warrior performing splachnoscopy before leaving for
the battle). Moreover, the deployed definition of ?realization? is similar to what Grosz,
4 This is the main difference between our approach and that of Kibble and Power, who allow for more than
one potential CP in their CF lists.
33
Computational Linguistics Volume 35, Number 1
Table 2
The CF list, the CB, NOCB, or centering transition (see Table 1) and violations of CHEAPNESS
(denoted with an asterisk) for each fact in Example (1) from the MPIRO-CF corpus.
Fact CF list: next referent} CB Transition CHEAPNESS
{CP, CBn=CPn?1
(1a) {ex1, amphora} n.a. n.a. n.a.
(1b) {ex1, paint-of-kleofr} ex1 CONTINUE ?
(1c) {paint-of-kleofr, en404} paint-of-kleofr SMOOTH-SHIFT ?
(1d) {ex1, en914} ? NOCB n.a.
(1e) {ex1, wagner-mus} ex1 CONTINUE ?
(1f) {wagner-mus, germany} wagner-mus SMOOTH-SHIFT ?
Joshi, and Weinstein (1995) call ?direct realization,? which ignores potential bridging
relations (Clark 1977) between the members of two subsequent CF lists. These relations
are typically not taken into account for information ordering and were not considered
in any of the deployed corpora.
4.2 The GNOME-LAB Corpus
We also made use of the GNOME corpus (Poesio et al 2004), which contains object
descriptions (museum labels) reliably annotated with features relevant to centering.
The motivation for this study was to examine whether the phenomena observed in
MPIRO-CF (which is arguably somewhat artificial) also manifest in texts from the
same genre written by humans without the constraints imposed by a text generation
system.
Based on the definition of museum labels in Cheng (2002, page 65), we identified
20 such texts in GNOME, which were published in a book and a museum Web site (and
were thus taken to be coherent). The following example is a characteristic text from this
subcorpus (referred to here as GNOME-LAB):
(2) (a) Item 144 is a torc.
(b) Its present arrangement, twisted into three rings, may be a modern alteration;
(c) it should probably be a single ring, worn around the neck.
(d) The terminals are in the form of goats? heads.
The GNOME corpus provides us with reliable annotation of discourse units (i.e.,
clauses and sentences) that can be used for the computation of ?utterance? and of
NPs which introduce entities to the CF list. Each feature was marked up by at
least two annotators and agreement was checked using the ? statistic on part of the
corpus.
In order to avoid deviating too much from the MPIRO application domain, we
computed the CF lists from the units that seemed to correspond more closely to MPIRO
facts. So instead of using sentence for the definition of ?utterance,? we followed most
work on centering for English and computed CF lists from GNOME?s finite units.5 The
5 This definition includes titles which do not always have finite verbs, but excludes finite relative clauses,
the second element of coordinated VPs and clause complements which are often taken as not having their
own CF lists in the centering literature.
34
Karamanis et al Centering for Information Ordering
Table 3
First two members of the CF list, the CB, NOCB, or centering transition (see Table 1) and
violations of CHEAPNESS (denoted with an asterisk) for each finite unit in Example (2) from the
GNOME-LAB corpus.
Unit CF list: next referent} CB Transition CHEAPNESS
{CP, CBn=CPn?1
(2a) {de374, de375} n.a. n.a. n.a.
(2b) {de376, de374, ... } de374 RETAIN ?
(2c) {de374, de379, ... } de374 CONTINUE ?
(2d) {de380, de381, ... } ? NOCB n.a.
text spans with the indexes (a) to (d) in Example (2) are examples of such units. Units
such as (2a) are as simple as the MPIRO-generated sentence (1a), whereas others appear
to be of similar syntactic complexity to (1d). On the other hand, the second sentence in
Example (2) consists of two finite units, namely (b) and (c), and appears to correspond
to higher degrees of aggregation than is typically seen in an MPIRO fact. The texts in
GNOME-LAB consist of 8.35 finite units on average.
Table 3 shows the first two members of the CF list, the CB, the transitions, and the
violations of CHEAPNESS for Example (2). Note that the same entity (i.e., de374) is used
to denote the referent of the NP Item 144 in (2a) and its in (2b), which is annotated as
coreferring with Item 144. All annotated NPs introduce referents to the CF list (which
often contains more entities than in MPIRO), but only direct realization is used for the
computation of the list. This means that, similarly to the MPIRO domain, bridging
relations between, for example, it in (2c) and the terminals in (2d), are not taken into
account.
The members of the CF list were ranked by combining grammatical function with
linear order, which is a robust way of estimating ?CF ranking? in English (Poesio et al
2004). In this instantiation, the CP corresponds to the referent of the first NP within the
unit that is annotated as a subject or as the post-copular NP in a there-clause.
4.3 The NEWS and ACCS Corpora
Barzilay and Lapata (2005) presented a probabilistic approach for information ordering
which is particularly suitable for text-to-text generation and is based on a new
representation called the entity grid. A collection of 200 articles from the North American
News Corpus (NEWS) and 200 narratives of accidents from the National Transportation
Safety Board database (ACCS) was used for training and evaluation. Example (3)
presents a characteristic text from the NEWS corpus:
(3) (a) [The Justice Department]S is conducting [an anti-trust trial]O against [Microsoft
Corp.]X with [evidence]X that [the company]S is increasingly attempting to crush
[competitors]O.
(b) [Microsoft]O is accused of trying to forcefully buy into [markets]X where [its
own products]S are not competitive enough to unseat [established brands]O.
(c) [The case]S revolves around [evidence]O of [Microsoft]S aggressively pressuring
[Netscape]O into merging [browser software]O.
(d) [Microsoft]S claims [its tactics]S are commonplace and good economically.
35
Computational Linguistics Volume 35, Number 1
Table 4
Fragment of the entity grid for Example (3). The grammatical function of the referents in each
sentence is reported using S, O, and X (for subject, object, and other). The symbol ??? is used for
referents which do not occur in the sentence.
Referents
Sentences department trial microsoft evidence ... products brands ...
(3a) S O S X ... ? ? ...
(3b) ? ? O ? ... S O ...
(3c) ? ? S O ... ? ? ...
(3d) ? ? S ? ... ? ? ...
(3e) ? ? ? ? ... ? ? ...
(3f) ? X S ? ... ? ? ...
(e) [The government]S may file [a civil suit]O ruling that [conspiracy]S to curb
[competition]O through [collusion]X is [a violation]O of [the Sherman Act]X.
(f) [Microsoft]S continues to show [increased earnings]O despite [the trial]X.
Barzilay and Lapata automatically annotated their corpora for the grammatical function
of the NPs in each sentence (denoted in the example by the subscripts S, O, and
X for subject, object, and other, respectively) as well as their coreferential relations
(which do not include bridging references). More specifically, they used a parser
(Collins 1997) to determine the constituent structure of the sentences from which the
grammatical function for each NP was derived.6 Coreferential NPs such as Microsoft
Corp. and the company in (3a) were identified using the system of Ng and Cardie
(2002).
The entity grid is a two-dimensional array that captures the distribution of NP
referents across sentences in the text using the aforementioned symbols for their
grammatical role and the symbol ??? for a referent that does not occur in a sentence.
Table 4 illustrates a fragment of the grid for the sentences in Example (3).7
Barzilay and Lapata use the grid to compute models of coherence that are
considerably more elaborate than centering. To derive an appropriate instantiation of
centering for our investigation, we compute a CF list for each grid row using the
referents with the symbols S, O, and X. These referents are ranked according to their
grammatical function and their position in the text. This definition of ?CF ranking? is
similar to the one we use in GNOME-LAB. For instance, department is ranked higher
than microsoft in CF(3a) because the Justice Department is mentioned before Microsoft
Corp. in the text. The derived sequence of CF lists is used to compute the additional
centering data structures shown in Table 5.
The average number of sentences per text is 10.4 in NEWS and 11.5 in ACCS.
As we explain in the next section, our centering-based metrics of coherence can be
6 They also used a small set of patterns to recognize passive verbs and annotate arguments involved in
passive constructions with their underlying grammatical function. This is why Microsoft is marked with
the role O in sentence (3b).
7 If a referent such as microsoft is attested by several NPs in the same sentence, for example, Microsoft
Corp. and the company in (3a), the role with the highest priority (in this case S) is used to represent it.
36
Karamanis et al Centering for Information Ordering
Table 5
First two members of the CF list, the CB, NOCB, or centering transitions (see Table 1) and
violations of CHEAPNESS (denoted with an asterisk) for Example (3) from the NEWS corpus.
Sentence CF list: next referent} CB Transition CHEAPNESS
{CP, CBn=CPn?1
(3a) {department, microsoft, ...} n.a. n.a. n.a.
(3b) {products, microsoft, ...} microsoft RETAIN ?
(3c) {microsoft, case, ...} microsoft CONTINUE ?
(3d) {microsoft, tactics} microsoft CONTINUE ?
(3e) {government, conspiracy, ...} ? NOCB n.a.
(3f) {microsoft, earnings, ... } ? NOCB n.a.
deployed directly on unseen texts, so we treated all texts in NEWS and ACCS as test
data.8
5. Computing Centering-Based Metrics of Coherence
Following our previous work (Karamanis and Manurung 2002; Althaus, Karamanis,
and Koller 2004), the input to information ordering is an unordered set of information-
bearing items represented as CF lists. A set of candidate orderings is produced by
creating different permutations of these lists. A metric of coherence uses features from
centering to compute a score for each candidate ordering and select the highest scoring
ordering as the output.9
A wide range of metrics of coherence can be defined in centering?s terms, simply
on the basis of the work we reviewed in Section 3. To exemplify this, let us first assume
that the ordering in Example (3), which is analyzed as a sequence of CF lists in Table 5,
is a candidate ordering. Table 6 summarizes the NOCBs, the violations of COHERENCE,
SALIENCE, and CHEAPNESS, and the centering transitions for this ordering.10
The candidate ordering contains two NOCBs in sentences (3e) and (3f). Its score
according to M.NOCB, the metric used by Karamanis and Manurung (2002) and
Althaus, Karamanis, and Koller (2004), is 2. Another ordering with fewer NOCBs (should
such an ordering exist) will be preferred over this candidate as the selected output of
information ordering if M.NOCB is used to guide this process. M.NOCB relies only on
CONTINUITY. Because satisfying this principle is a prerequisite for the computation of
every other centering feature, M.NOCB is the simplest possible centering-based metric
and will be used as the baseline in our experiments.
According to Strube and Hahn (1999) the principle of CHEAPNESS is the most
important centering feature for anaphora resolution. We are interested in assessing how
suitable M.CHEAP, a metric which utilizes CHEAPNESS, is for information ordering.
CHEAPNESS is violated twice according to Table 6 so the score of the candidate ordering
8 By contrast, Barzilay and Lapata used 100 texts in each domain to train their models and reserved the
other 100 for testing them.
9 If the best coherence score is assigned to several candidate orderings, then the information ordering
algorithm will choose randomly between them.
10 Principles and transitions will be collectively referred to as ?features? from now on.
37
Computational Linguistics Volume 35, Number 1
Table 6
Violations of CONTINUITY (NOCB), COHERENCE, SALIENCE, and CHEAPNESS and centering
transitions for Example (3), based on the analysis in Table 5. The table reports the sentences
marked with each centering feature: That is, sentences (3e) and (3f) are classified as NOCBs, and
so on.
CONTINUITY? COHERENCE? SALIENCE? CHEAPNESS?
NOCB: CBn = CBn?1: CBn = CPn: CBn = CPn?1:
(3e), (3f) ? (3b) (3b), (3c)
CONTINUE: RETAIN: SMOOTH-SHIFT: ROUGH-SHIFT:
(3c), (3d) (3b) ? ?
according to M.CHEAP is 2.11 If another candidate ordering with fewer violations of
CHEAPNESS exists, it will be chosen as a preferred output according to M.CHEAP.
M.BFP employs the transition preferences of Rule 2 as specified by Brennan,
Friedman [Walker], and Pollard (1987). The first score to be computed by M.BFP is
the sum of CONTINUE transitions, which is 2 for the candidate ordering according to
Table 6. If this ordering is found to score higher than every other candidate ordering for
the number of CONTINUEs, it is selected as the output. If another ordering is found to
have the same number of CONTINUEs, the sum of RETAINs is examined, and so forth for
the other two types of centering transitions.12
M.KP, the metric deployed by Kibble and Power (2000) in their text generation
system, sums up the NOCBs as well as the violations of CHEAPNESS, COHERENCE,
and SALIENCE, preferring the ordering with the lowest total cost. In addition to
the violations of CONTINUITY and CHEAPNESS, the candidate ordering also violates
SALIENCE once, so its score according to M.KP is 5. An alternative ordering with a
lower score (if any) will be preferred by this metric. Although Kibble and Power (2004)
introduced a weighted version of M.KP, the exact weighting of centering?s principles
remains an open question, as argued by Kibble (2001). This is why we decided to
experiment with M.KP instead of its weighted variant.
In the remainder of the paper, we take forward the four metrics motivated in this
section as the most appropriate starting point for experimentation. We would like to
emphasize, however, that these are not the only possible options. Indeed, similarly to
the various ways in which centering?s parameters can be specified, there exist many
other ways of using centering to define metrics of entity coherence for information
ordering. These possibilities arise from the numerous other definitions of centering?s
transitions and the various ways in which transitions and principles can be combined.
These are explored in more detail in Karamanis (2003, Chapter 3), which also provides
a formal definition of the metrics discussed previously.
6. Evaluation Methodology
Because using naturally occurring discourse in psycholinguistic studies to investigate
coherence effects is almost infeasible, computational corpus-based experiments are
11 In order to estimate the effect of CHEAPNESS only, NOCBs are not counted as violations of CHEAPNESS.
12 Following Brennan, Friedman [Walker], and Pollard (1987), NOCBs are not taken into account for the
definition of transitions in M.BFP.
38
Karamanis et al Centering for Information Ordering
often the most viable alternative (Poesio et al 2004; Barzilay and Lee 2004). Corpus-
based evaluation can be usefully employed during system development and may
be later supplemented by less extended evaluation based on human judgments as
suggested by Lapata (2006).
The corpus-based methodology of Karamanis (2003) served as our experimental
framework. This methodology is based on the premise that the original sentence order
(OSO, Barzilay and Lee 2004) observed in a corpus text is more coherent than any other
ordering. If a metric takes an alternative ordering to be more coherent than the OSO, it
has to be penalized.
Karamanis (2003) introduced a performance measure called the classification error
rate which is computed according to the formula: Better(M,OSO)+Equal(M,OSO)/2.
Better(M,OSO) stands for the percentage of orderings that score better than the OSO
according to a metric M, and Equal(M,OSO) is the percentage of orderings that score
equal to the OSO.13 This measure provides an indication of how likely a metric is to lead
to an ordering different from the OSO. When comparing several metrics with each other,
the one with the lowest classification error rate is the most appropriate for ordering
the sentences that the OSO consists of. In other words, the smaller the classification
error rate, the better a metric is expected to perform for information ordering. The
average classification error rate is used to summarize the performance of each metric in
a corpus.
To compute the classification error rate we permute the CF lists of the OSO and
classify each alternative ordering as scoring better, equal, or worse than the OSO
according to M. When the number of CF lists in the OSO is fairly small, it is feasible
to search through all possible orderings. For OSOs consisting of more than 10 CF
lists, the classification error rate for the entire population of orderings can be reliably
estimated using a random sample of one million permutations (Karamanis 2003,
Chapter 5).
7. Results
Table 7 shows the average performance of each metric in the corpora employed in our
experiments. The smallest?that is, best?score in each corpus is printed in boldface.
The table indicates that the baseline M.NOCB performs best in three out of four corpora.
The experimental results of the pairwise comparisons of M.NOCB with each of
M.CHEAP, M.KP, and M.BFP in each corpus are reported in Table 8. The exact number
of texts for which the classification error rate of M.NOCB is lower than its competitor for
each comparison is reported in the columns headed by ?lower.? For instance, M.NOCB
has a lower classification error rate than M.CHEAP for 110 (out of 122) texts from
MPIRO-CF. M.CHEAP achieves a lower classification error rate for just 12 texts, and
there do not exist any ties, that is, cases in which the classification error rate of the two
metrics is the same.
The p value returned by the two-tailed Sign Test for the difference in the number
of texts in each corpus, rounded to the third decimal place, is also reported.14 With
13 Weighting Equal(M,OSO) by 0.5 is based on the assumption that, similarly to tossing a coin, the OSO will
on average do better than half of the orderings that score the same as it does when other coherence
constraints are considered.
14 The Sign Test was chosen over its parametric alternatives to test significance because it does not carry
specific assumptions about population distributions and variance.
39
Computational Linguistics Volume 35, Number 1
Table 7
Average classification error rate for the centering-based metrics in each corpus.
Corpus
Metric MPIRO-CF GNOME-LAB NEWS ACCS Mean
M.NOCB 20.42 19.95 30.90 15.51 21.70
M.BFP 19.91 33.01 37.90 21.20 28.01
M.KP 53.15 58.22 57.70 55.60 56.12
M.CHEAP 81.04 57.23 64.60 76.29 69.79
No. of texts 122 20 200 200
Table 8
Comparing M.NOCB with M.CHEAP, M.KP, and M.BFP in each corpus.
MPIRO-CF GNOME-LAB
M.NOCB M.NOCB
lower greater ties p lower greater ties p
M.CHEAP 110 12 0 <0.001 18 2 0 <0.001
M.KP 103 16 3 <0.001 16 2 2 0.002
M.BFP 42 31 49 0.242 12 3 5 0.036
No. of texts 122 20
NEWS ACCS
M.NOCB M.NOCB
lower greater ties p lower greater ties p
M.CHEAP 155 44 1 <0.001 183 17 0 <0.001
M.KP 131 68 1 <0.001 167 33 0 <0.001
M.BFP 121 71 8 <0.001 100 100 0 1.000
No. of texts 200 200
respect to the exemplified comparison of M.NOCB against M.CHEAP in MPIRO-CF,
the p value is lower than 0.001 after rounding. This in turn means that M.NOCB
returns a better classification error rate for significantly more texts in MPIRO-CF
than M.CHEAP. In other words, M.NOCB outperforms M.CHEAP significantly in this
corpus.
Notably, M.NOCB performs significantly better than its competitor in 10 out of
12 cases.15 In the remaining two comparisons, the difference in performance between
M.NOCB and M.BFP is not significant (p > 0.05). However, this does not constitute
evidence against M.NOCB, the simplest of the investigated metrics. In fact, because
M.BFP fails to outperform the baseline, the latter may be considered as the most
promising solution for information ordering in these cases too by applying Occam?s
razor. Thus, M.NOCB is shown to be the best performing metric across all four
corpora.
15 This result is significant too according to the two-tailed Sign Test (p < 0.05).
40
Karamanis et al Centering for Information Ordering
8. Discussion
Our experiments show that M.NOCB is the most suitable metric for information
ordering among the metrics we experimented with. Despite the differences between our
corpora (in genre, average length, syntactic complexity, number of referents in the CF
list, etc.), M.NOCB proves robust across all four of them. It is also the most appropriate
metric to use in both application areas we relate our corpora to, namely concept-to-text
(MPIRO-CF and GNOME-LAB) as well as text-to-text (NEWS and ACCS) generation.
These results indicate that when purely centering-based metrics are used, simply
avoiding NOCBs is more relevant to information ordering than the combinations of
additional centering features that the other metrics make use of.
In this section, we compare our work with other recent evaluation studies,
including the corpus-based investigation of centering by Poesio et al (2004); discuss
the implications of our findings for text generation; and summarize our contributions.
8.1 Recent Evaluation Studies in Information Ordering
There has been significant recent work on the corpus-based evaluation for information
ordering. In this section, we discuss the methodological differences between our work
and the studies which are most closely related to it.
Barzilay and Lee (2004) introduce a stochastic model for information ordering
which computes the probability of generating the OSO and every alternative ordering.
Then, all orderings are ranked according to this probability and the rank given to the
OSO is retrieved. Several evaluation measures are discussed, the most important of
which is the average OSO rank, that is, the average rank of the OSOs in their corpora.
This measure does not take into account that the OSOs differ in length. However, this
information is necessary to estimate reliably the performance of an information ordering
approach, as we discuss in Karamanis and Mellish (2005a) in more detail.
Barzilay and Lapata (2005) overcome this difficulty by introducing a performance
measure called ranking accuracywhich expresses the percentage of alternative orderings
that are ranked lower than the OSO. In Karamanis?s (2003) terms, ranking accuracy
equals 100% ? Better(M,OSO), assuming that no equally ranking orderings exist.16
Barzilay and Lapata (2005) compare the OSO with just 20 alternative orderings,
often sampled out of several millions. On the other hand, Barzilay and Lee (2004)
enumerate exhaustively each possible ordering, which might become impractical as the
search space grows factorially. We overcame these problems by using a large random
sample for the texts which consist of more than 10 sentences as suggested in Karamanis
(2003, Chapter 5). Equally important is the emphasis we placed on the use of statistical
tests, which were not deployed by either Barzilay and Lee or Barzilay and Lapata.
Lapata (2003) presented a methodology for automatically evaluating generated
orderings on the basis of their distance from observed sentence orderings in a corpus.
A measure of rank correlation (called Kendall?s ?), which was subsequently shown to
correlate reliably with human ratings and reading times (Lapata 2006), was used to
estimate the distance between orderings.
16 Neither Barzilay and Lapata (2005) nor Barzilay and Lee (2004) appear to consider the possibility that two
orderings may be equally ranked.
41
Computational Linguistics Volume 35, Number 1
Whereas ? estimates how close the predictions of a metric are to several original
orderings, we measure how likely a metric is to lead to an ordering different than the
OSO. Taking into account more than one OSO for information ordering is the main
strength of Lapata?s method, but to do this one needs to ask several humans to order the
same set of sentences (Madnani et al 2007). Karamanis and Mellish (2005b) conducted
an experiment in the MPIRO domain using Lapata?s methodology which supplements
the work reported in this article. However, such an approach is less practical for much
larger collections of texts such as NEWS and ACCS. This is presumably the reason why
Barzilay and Lapata (2005) use ranking accuracy instead of ? in their evaluation.
8.2 Previous Corpus-Based Evaluations of Centering
Our work investigates how the coherence score of the OSO compares to the scores
of alternative orderings of the sentences that the OSO consists of. As Kibble (2001,
page 582) noticed, this question is crucial from an information ordering viewpoint, but
was not taken into account by any previous corpus-based study of centering. Grosz,
Joshi, and Weinstein (1995, page 215) also suggested that Rule 2 should be tested by
examining ?alternative multi-utterance sequences that differentially realize the same
content.? We are the first to have pursued this research objective in the evaluation of
centering for information ordering.
Poesio et al (2004) observed that there remained a large number of NOCBs under
every instantiation of centering they tested and concluded that centering is inadequate
as a coherence model.17 However, the frequency of NOCBs does not necessarily provide
adequate indication of how appropriate NOCBs (and centering in general) are for
information ordering. Although over 50% of the transitions in GNOME-LAB are NOCBs,
the average classification error rate of approximately 20% for M.NOCB suggests that the
OSO tends to be in greater agreement with the preference to avoid NOCBs than 80% of
the alternative orderings. Thus, it appears that the observed ordering in the corpus does
optimize with respect to the number of potential NOCBs to a great extent.
8.3 A Simple and Robust Baseline for Text Generation
How likely is M.NOCB to come up with the attested ordering in the corpus (the OSO)
if it is actually used to guide an algorithm that orders the CF lists in our corpora?
The average classification error rates (Table 7) estimate exactly this variable. The
performance of M.NOCB varies across the corpora from about 15.5% (ACCS) to 30.9%
(NEWS). We attribute this variation to the aforesaid differences between the corpora.
Notice, however, that these differences affect all metrics in a similar way, not allowing
for another metric to significantly outperform M.NOCB.
Noticeably, even in ACCS, for which M.NOCB achieves its best performance,
approximately one out of six alternative orderings on average are taken to be more
coherent than the OSO. Given the average number of sentences per text in this corpus
17 We viewed the definition of the centering instantiation as being related to the application domain, as we
explained in Section 4. This is why, unlike Poesio et al, we did not experiment with different
instantiations of centering on the same data.
42
Karamanis et al Centering for Information Ordering
(11.5), this means that several millions of alternative orderings are often taken to be
more coherent than the gold standard.
Barzilay and Lapata (2005) report an average ranking accuracy of 87.3% for their
best sentence ordering method in ACCS. This corresponds to an average classification
error rate of 12.7% (assuming that there are no equally scoring orderings in their
evaluation; see Section 8.1). This is equal to an improvement of just 2.8% over
the performance of our baseline metric (15.5%) using a coherence model which is
substantially more elaborate than centering. However, it is in NEWS (for which
M.NOCB returns its worst performance of 30.9%) that this model shows its real strength,
approximating an average classification error rate of 9.6%, which corresponds to an
improvement of 21.3% over our baseline. We believe that the experiments reported in
this article put the studies of our colleagues in better perspective by providing a reliable
baseline to compare their metrics against.
8.4 Moving Beyond Centering-Based Metrics
Following McKeown (1985), Kibble and Power argue in favor of an integrated approach
for concept-to-text generation in which the same centering features are used at different
stages in the generation pipeline. However, our study suggests that features such as
CHEAPNESS and the centering transitions are not particularly relevant to information
ordering. The poor performance of these features can be explained by the fact that they
were originally introduced to account for pronoun resolution rather than information
ordering. CONTINUITY, on the other hand, captures a fundamental intuition about entity
coherence which constitutes part of several other discourse theories.18
CONTINUITY, however, captures just one aspect of coherence. This explains the
relatively high classification error rates for M.NOCB, which needs to be supplemented
with other coherence-inducing factors in order to be used in practice. This verifies the
premises of researchers such as Kibble and Power who a priori use features derived
from centering in combination with other factors in the definition of their metrics. Our
work should be quite helpful for that effort too, suggesting that M.NOCB is a better
starting point for defining such metrics than M.CHEAP or M.KP.
9. Conclusion
In conclusion, our analysis sheds more light on two previously unaddressed questions
in the corpus-based evaluation of centering: (i) which aspects of centering are most
relevant to information ordering and (ii) to what extent centering on its own can be
useful for this purpose. We have shown that the metric which relies exclusively on NOCB
transitions (M.NOCB) sets a baseline that cannot be outperformed by other coherence
metrics which make use of additional centering features. Although this metric does not
perform well enough to be used on its own, it constitutes a simple, yet robust, baseline
against which more elaborate information ordering approaches can be tested during
system development in both text-to-text and concept-to-text generation.
This work can be extended in numerous ways. For instance, given the abundance
of possible centering-based metrics one may investigate whether a different metric can
18 We thank one anonymous reviewer for suggesting this explanation of our results.
43
Computational Linguistics Volume 35, Number 1
outperform M.NOCB in any corpus or application domain. M.NOCB can also serve as
the starting point for the definition of more informed metrics which will incorporate
additional coherence-inducing factors. Finally, given that we used the instantiation
of centering which seemed to correspond more closely to the targeted application
domains, the extent to which computing the CF list in a different way may affect the
performance of the metrics is another question to explore in future work.
Acknowledgments
Many thanks to Aggeliki Dimitromanolaki,
Mirella Lapata, and Regina Barzilay for their
data; to David Schlangen, Ruli Manurung,
James Soutter, and Le An Ha for
programming solutions; and to Ruth Seal
and two anonymous reviewers for their
comments. Nikiforos Karamanis received
support from the Greek State Scholarships
Foundation (IKY) as a PhD student in
Edinburgh as well as the Rapid Item
Generation project and the BBSRC-funded
FlySlip grant (No 38688) as a postdoc in
Wolverhampton and Cambridge,
respectively.
References
Althaus, Ernst, Nikiforos Karamanis, and
Alexander Koller. 2004. Computing locally
coherent discourses. In Proceedings of ACL
2004, pages 399?406, Barcelona.
Barzilay, Regina, Noemie Elhadad, and
Kathleen McKeown. 2002. Inferring
strategies for sentence ordering in
multidocument news summarization.
Journal of Artificial Intelligence Research,
17:35?55.
Barzilay, Regina and Mirella Lapata. 2005.
Modeling local coherence: An entity-based
approach. In Proceedings of ACL 2005,
pages 141?148, Ann Arbor, MI.
Barzilay, Regina and Lillian Lee. 2004.
Catching the drift: Probabilistic content
models with applications to generation
and summarization. In Proceedings of
HLT-NAACL 2004, pages 113?120,
Boston, MA.
Beaver, David. 2004. The optimization of
discourse anaphora. Linguistics and
Philosophy, 27(1):3?56.
Bollegala, Danushka, Naoaki Okazaki, and
Mitsuru Ishizuka. 2006. A bottom-up
approach to sentence ordering for
multi-document summarization. In
Proceedings of ACL-COLING 2006,
pages 385?392, Sydney.
Brennan, Susan E., Marilyn A.
Friedman [Walker], and Carl J. Pollard.
1987. A centering approach to pronouns.
In Proceedings of ACL 1987, pages 155?162,
Stanford, CA.
Cheng, Hua. 2002. Modelling Aggregation
Motivated Interactions in Descriptive Text
Generation. Ph.D. thesis, Division of
Informatics, University of Edinburgh.
Clark, Herbert. H. 1977. Bridging. In P. N.
Johnson-Laird and P. C. Wason, editors,
Thinking: Readings in Cognitive Science.
Cambridge University Press, Cambridge,
pages 9?27.
Collins, Michael. 1997. Three generative,
lexicalised models for statistical parsing.
In Proceedings of ACL-EACL 1997,
pages 16?23, Madrid.
Dimitromanolaki, Aggeliki and Ion
Androutsopoulos. 2003. Learning to order
facts for discourse planning in natural
language generation. In Proceedings of
ENLG 2003, pages 23?30, Budapest.
Grosz, Barbara J., Aravind K. Joshi, and Scott
Weinstein. 1995. Centering: A framework
for modeling the local coherence of
discourse. Computational Linguistics,
21(2):203?225.
Hovy, Eduard. 1988. Planning coherent
multisentential text. In Proceedings of ACL
1988, pages 163?169, Buffalo, NY.
Isard, Amy, Jon Oberlander, Ion
Androutsopoulos, and Colin Matheson.
2003. Speaking the users? languages. IEEE
Intelligent Systems Magazine, 18(1):40?45.
Ji, Paul and Stephen Pulman. 2006. Sentence
ordering with manifold-based
classification in multi-document
summarization. In Proceedings of EMNLP
2006, pages 526?533, Sydney.
Kameyama, Megumi. 1998. Intrasentential
centering: A case study. In Walker, Joshi,
and Prince 1998, pages 89?122.
Kan, Min-Yen and Kathleen McKeown. 2002.
Corpus-trained text generation for
summarization. In Proceedings of INLG
2002, pages 1?8, Harriman, NY.
Karamanis, N. 2006. Evaluating centering for
information ordering in two new domains.
In Proceedings of NAACL 2006, Companion
Volume, pages 65?68, New York.
Karamanis, N., M. Poesio, C. Mellish, and
J. Oberlander. 2004. Evaluating
centering-based metrics of coherence using
44
Karamanis et al Centering for Information Ordering
a reliably annotated corpus. In Proceedings
of ACL 2004, pages 391?398, Barcelona.
Karamanis, Nikiforos. 2003. Entity Coherence
for Descriptive Text Structuring. Ph.D.
thesis, Division of Informatics, University
of Edinburgh.
Karamanis, Nikiforos and Hisar Maruli
Manurung. 2002. Stochastic text
structuring using the principle of
continuity. In Proceedings of INLG 2002,
pages 81?88, Harriman, NY.
Karamanis, Nikiforos and Chris Mellish.
2005a. A review of recent corpus-based
methods for evaluating information
ordering in text production. In Proceedings
of Corpus Linguistics 2005 Workshop on
Using Corpora for NLG, pages 13?18,
Birmingham.
Karamanis, Nikiforos and Chris Mellish.
2005b. Using a corpus of sentence
orderings defined by many experts to
evaluate metrics of coherence for text
structuring. In Proceedings of ENLG 2005,
pages 174?179, Aberdeen.
Kibble, Rodger. 2001. A reformulation of rule
2 of centering theory. Computational
Linguistics, 27(4):579?587.
Kibble, Rodger and Richard Power. 2000.
An integrated framework for text
planning and pronominalisation. In
Proceedings of INLG 2000, pages 77?84,
Mitzpe Ramon.
Kibble, Rodger and Richard Power. 2004.
Optimizing referential coherence in text
generation. Computational Linguistics,
30(4):401?416.
Knott, Alistair, Jon Oberlander, Mick
O?Donnell, and Chris Mellish. 2001.
Beyond elaboration: The interaction of
relations and focus in coherent text. In
T. Sanders, J. Schilperoord, and
W. Spooren, editors, Text Representation:
Linguistic and Psycholinguistic Aspects.
John Benjamins, Amsterdam, chapter 7,
pages 181?196.
Lapata, Mirella. 2003. Probabilistic text
structuring: Experiments with sentence
ordering. In Proceedings of ACL 2003,
pages 545?552, Sapporo.
Lapata, Mirella. 2006. Automatic evaluation
of information ordering: Kendall?s tau.
Computational Linguistics, 32(4):1?14.
Madnani, Nitin, Rebecca Passonneau,
Necip Fazil Ayan, John Conroy, Bonnie
Dorr, Judith Klavans, Dianne O?Leary, and
Judith Schlesinger. 2007. Measuring
variability in sentence ordering for news
summarization. In Proceedings of ENLG
2007, pages 81?88, Schloss Dagstuhl.
Mann, William C. and Sandra A. Thompson.
1987. Rhetorical structure theory: A theory
of text organisation. Technical Report
RR-87-190, University of Southern
California / Information Sciences Institute.
Marcu, Daniel. 1997. The Rhetorical Parsing,
Summarization and Generation of Natural
Language Texts. Ph.D. thesis, University of
Toronto.
McKeown, Kathleen. 1985. Text Generation:
Using Discourse Strategies and Focus
Constraints to Generate Natural Language
Text. Studies in Natural Language
Processing. Cambridge University Press,
Cambridge.
Mellish, Chris, Alistair Knott, Jon
Oberlander, and Mick O?Donnell. 1998.
Experiments using stochastic search for
text planning. In Proceedings of INLG 1998,
pages 98?107, Niagara-on-the-Lake.
Miltsakaki, Eleni and Karen Kukich. 2004.
Evaluation of text coherence for electronic
essay scoring systems. Natural Language
Engineering, 10(1):25?55.
Ng, Vincent and Claire Cardie. 2002.
Improving machine learning approaches
to coreference resolution. In Proceedings of
ACL 2002, pages 104?111, Philadelphia,
PA.
O?Donnell, Mick, Chris Mellish, Jon
Oberlander, and Alistair Knott. 2001. ILEX:
An architecture for a dynamic hypertext
generation system. Natural Language
Engineering, 7(3):225?250.
Passoneau, Rebecca J. 1998. Interaction
of discourse structure with explicitness
of discourse anaphoric phrases. In
Walker, Joshi, and Prince 1998,
pages 327?358.
Poesio, Massimo, Rosemary Stevenson,
Barbara Di Eugenio, and Janet Hitzeman.
2004. Centering: a parametric theory and
its instantiations. Technical Report
CSM-369, Department of Computer
Science, University of Essex. Extended
version of the paper that appeared in
Computational Linguistics 30(3):309?363,
2004.
Reiter, Ehud and Robert Dale. 2000.
Building Natural Language Generation
Systems. Cambridge University Press,
Cambridge.
Scott, Donia and Clarisse Sieckenius
de Souza. 1990. Getting the message across
in RST-based text generation. In Robert
Dale, Chris Mellish, and Michael Zock,
editors, Current Research in Natural
Language Generation. Academic Press, San
Diego, CA, pages 47?74.
45
Computational Linguistics Volume 35, Number 1
Siddharthan, Advaith. 2006. Syntactic
simplification and text cohesion.
Research on Language and Computation,
4(1):77?109.
Sidner, Candace L. 1979. Towards a
Computational Theory of Definite Anaphora
Comprehension in English. Ph.D. thesis, AI
Laboratory/MIT, Cambridge, MA. Also
available as Technical Report No.
AI-TR-537.
Soricut, Radu and Daniel Marcu. 2006.
Discourse generation using utility-trained
coherence models. In Proceedings of
ACL-COLING 2006 Poster Session,
pages 803?810, Sydney.
Strube, Michael and Udo Hahn. 1999.
Functional centering: Grounding
referential coherence in information
structure. Computational Linguistics,
25(3):309?344.
Walker, Marilyn A., Aravind K. Joshi, and
Ellen F. Prince, editors. 1998. Centering
Theory in Discourse. Clarendon Press,
Oxford.
46
LEARNING TO RESOLVE BRIDGING REFERENCES
Massimo Poesio,? Rahul Mehta,? Axel Maroudas,? and Janet Hitzeman?
?Dept. of Comp. Science, University of Essex, UK poesio at essex dot ac dot uk
?MITRE Corporation, USA hitz at mitre dot org
Abstract
We use machine learning techniques to find the
best combination of local focus and lexical distance
features for identifying the anchor of mereological
bridging references. We find that using first men-
tion, utterance distance, and lexical distance com-
puted using either Google or WordNet results in an
accuracy significantly higher than obtained in pre-
vious experiments.
1 Introduction
BRIDGING REFERENCES (BR) (Clark, 1977)?
anaphoric expressions that cannot be resolved
purely on the basis of string matching and thus re-
quire the reader to ?bridge? the gap using common-
sense inferences?are arguably the most interesting
and, at the same time, the most challenging prob-
lem in anaphora resolution. Work such as (Poesio
et al, 1998; Poesio et al, 2002; Poesio, 2003) pro-
vided an experimental confirmation of the hypoth-
esis first put forward by Sidner (1979) that BRIDG-
ING DESCRIPTIONS (BD)1 are more similar to pro-
nouns than to other types of definite descriptions,
in that they are sensitive to the local rather than the
global focus (Grosz and Sidner, 1986). This previ-
uous work also suggested that simply choosing the
entity whose description is lexically closest to that
of the bridging description among those in the cur-
rent focus space gives poor results; in fact, better re-
sults are obtained by always choosing as ANCHOR
of the bridging reference2 the first-mentioned entity
of the previous sentence (Poesio, 2003). But nei-
ther source of information in isolation resulted in an
accuracy over 40%. In short, this earlier work sug-
gested that a combination of salience and lexical /
1We will use the term bridging descriptions to indicate
bridging references realized by definite descriptions, equated
here with noun phrases with determiner the, like the top.
2Following (Poesio and Vieira, 1998), we use the term ?an-
chor? as as a generalization of the term ANTECEDENT, to indi-
cate the discourse entity which an anaphoric expression either
realizes, or is related to by an associative relation; reserving
?antecedent? for the cases of identity.
commonsense information is needed to choose the
most likely anchor; the problem remained of how to
combine this information.
In the work described in this paper, we used ma-
chine learning techniques to find the best combina-
tion of local focus features and lexical distance fea-
tures, focusing on MEREOLOGICAL bridging refer-
ences:3 references referring to parts of an object al-
ready introduced (the cabinet), such as the panels or
the top (underlined) in the following example from
the GNOME corpus (Poesio et al, 2004).
(1) The combination of rare and expensive ma-
terials used on [this cabinet]i indicates that
it was a particularly expensive commission.
The four Japanese lacquer panels date from the
mid- to late 1600s and were created with a technique
known as kijimaki-e.
For this type of lacquer, artisans sanded plain wood
to heighten its strong grain and used it as the back-
ground of each panel. They then added the scenic
elements of landscape, plants, and animals in raised
lacquer. Although this technique was common in
Japan, such large panels were rarely incorporated
into French eighteenth-century furniture.
Heavy Ionic pilasters, whose copper-filled flutes
give an added rich color and contrast to the gilt-
bronze mounts, flank the panels. Yellow jasper, a
semiprecious stone, rather than the usual marble,
forms the top.
2 Two sources of information for bridging
reference resolution
2.1 Lexical information
The use of different sources of lexical knowledge
for resolving bridging references has been inves-
tigated in a series of papers by Poesio et al all
using as dataset the Bridging Descriptions (BDs)
contained in the corpus used by Vieira and Poesio
3We make use of the classification of bridging references
proposed by Vieira and Poesio (2000). ?Mereological? bridging
references are one of the the ?WordNet? bridging classes, which
cover cases where the information required to bridge the gap
may be found in a resource such as WordNet (Fellbaum, 1998):
synonymy, hyponymy, and meronymy.
(2000). In these studies, the lexical distance be-
tween a BD and its antecedent was used to choose
the anchor for the BD among the antecedents in the
previous five sentences. In (Poesio et al, 1997;
Vieira and Poesio, 2000) WordNet 1.6 was used as
a lexical resource, with poor or mediocre results.
These results were due in part to missing entries
and / or relations; in part to the fact that because of
the monotonic organization of information in Word-
Net, complex searches are required even to find ap-
parently close associations (like that between wheel
and car). Similar results using WordNet 1.6 were
reported at around the same time by other groups
- e.g., (Humphreys et al, 1997; Harabagiu and
Moldovan, 1998) and have been confirmed by more
recent studies studying both hyponymy (Markert et
al., 2003) and more specifically mereological BDs.
Poesio (2003) found that none of the 58 mereo-
logical references in the GNOME corpus (discussed
below) had a direct mereological link to their an-
chor: for example, table is not listed as a possi-
ble holonym of drawer, nor is house listed as a
possible holonym for furniture. Garcia-Almanza
(2003) found that only 16 of these 58 mereologi-
cal references could be resolved by means of more
complex searches in WordNet, including following
the hypernymy hierarchy for both the anchor and
the bridging reference, and a ?spreading activation?
search.
Poesio et al (1998) explored the usefulness of
vector-space representations of lexical meaning for
BDs that depended on lexical knowledge about hy-
ponymy and synonymy. The HAL model discussed
in Lund et al (1995) was used to find the anchor
of the BDs in the dataset aleady used by Poesio
et al (1997). However, using vectorial represen-
tations did not improve the results for the ?Word-
Net? BDs: for the synonymy cases the results were
comparable to those obtained with WordNet (4/12,
33%), but for the hyponymy BDs (2/14, as opposed
to 8/14 with WordNet) and especially for mereolog-
ical references (2/12) they were clearly worse. On
the other hand, the post-hoc analysis of results sug-
gested that the poor results were in part due to the
lack of mechanisms for choosing the most salient
(or most recent) BDs.
The poor results for mereological BDs with both
WordNet and vectorial representations indicated
that a different approach was needed to acquire in-
formation about part-of relations. Grefenstette?s
work on semantic similarity (Grefenstette, 1993)
and Hearst?s work on acquiring taxonomic informa-
tion (Hearst, 1998) suggested that certain syntactic
constructions could be usefully viewed as reflect-
ing underlying semantic relations. In (Ishikawa,
1998; Poesio et al, 2002) it was proposed that
syntactic patterns (henceforth: CONSTRUCTIONS)
such as the wheel of the car could indicate that
wheel and car stood in a part-of relation.4 Vector-
based lexical representations whose elements en-
coded the strength of associations identified by
means of constructions like the one discussed were
constructed from the British National Corpus, us-
ing Abney?s CASS chunker. These representations
were then used to choose the anchor of BDs, us-
ing again the same dataset and the same methods
as in the previous two attempts, and using mutual
information to determine the strength of associa-
tion. The results on mereological BDs?recall .67,
precision=.73?were drastically better than those ob-
tained with WordNet or with simple vectorial repre-
sentations. The results with the three types of lex-
ical resources and the different types of BDs in the
Vieira / Poesio dataset are summarized in Table 1.
Finally, a number of researchers recently argued
for using the Web as a way of addressing data
sparseness (Keller and Lapata, 2003). The Web
has proven a useful resource for work in anaphora
resolution as well. Uryupina (2003) used the Web
to estimate ?Definiteness probabilities? used as a
feature to identify discourse-new definites. Mark-
ert et al (2003) used the Web and the construc-
tion method to extract information about hyponymy
used to resolve other-anaphora (achieving an f
value of around 67%) as well as the BDs in the
Vieira-Poesio dataset (their results for these cases
were not better than those obtained by (Vieira and
Poesio, 2000)). Markert et al also found a sharp
difference between using the Web as a a corpus
and using the BNC, the results in the latter case be-
ing significantly worse than when using WordNet.
Poesio (2003) used the Web to choose between the
hypotheses concerning the anchors of mereological
BDs in the GNOME corpus generated on the basis of
Centering information (see below).
2.2 Salience
One of the motivations behind Grosz and Sidner?s
(1986) distinction between two aspects of the atten-
tional state - the LOCAL FOCUS and the GLOBAL
FOCUS?is the difference between the interpretive
preferences of pronouns and definite descriptions.
According to Grosz and Sidner, the interpretation
for pronouns is preferentially found in the local fo-
cus, whereas that of definite descriptions is prefer-
entially found in the global focus.
4A similar approach was pursued in parallel by Berland and
Charniak (1999).
Synonymy Hyponymy Meronymy Total WN Total BDs
BDs in Vieira / Poesio corpus 12 14 12 38 204
Using WordNet 4 (33.3%) 8(57.1%) 3(33.3%) 15 (39%) 34 (16.7%)
Using HAL Lexicon 4 (33.3%) 2(14.3%) 2(16.7%) 8 (22.2%) 46(22.7%)
Using Construction Lexicon 1 (8.3%) 0 8(66.7%) 9 (23.7%) 34(16.7%)
Table 1: BD resolution results using only lexical distance with WordNet, HAL-style vectorial lexicon,
and construction-based lexicon.
However, already Sidner (1979) hypothesized
that BDs are different from other definite descrip-
tions, in that the local focus is preferred for their in-
terpretation. As already mentioned, the error analy-
sis of Poesio et al (1998) supported this finding: the
study found that the strategy found to be optimal for
anaphoric definite descriptions by Vieira and Poesio
(2000), considering as equally likely all antecedents
in the previous five-sentence window (as opposed to
preferring closer antecedents), gave poor results for
bridging references; entities introduced in the last
two sentences and ?main entities? were clearly pre-
ferred. The following example illustrates how the
local focus affects the interpretation of a mereolog-
ical BD, the sides, in the third sentence.
(2) [Cartonnier (Filing Cabinet)]i with Clock
[This piece of mid-eighteenth-century
furniture]i was meant to be used like a modern
filing cabinet; papers were placed in [leather-
fronted cardboard boxes]j (now missing) that
were fitted into the open shelves.
[A large table]k decorated in the same manner
would have been placed in front for working
with those papers.
Access to [the cartonnier]i?s lower half can
only be gained by the doors at the sides, be-
cause the table would have blocked the front.
The three main candidate anchors in this example?
the cabinet, the boxes, and the table?all have sides.
However, the actual anchor, the cabinet, is clearly
the Backward-Looking Center (CB) (Grosz et al,
1995) of the first sentence after the title;5 and if
we assume that entities can be indirectly realized?
see (Poesio et al, 2004)?the cabinet is the CB of
all three sentences, including the one containing the
BR, and therefore a preferred candidate.
In (Poesio, 2003), the impact on associative BD
resolution of both relatively simple salience features
(such as distance and order or mention) and of more
complex ones (such as whether the anchor was a CB
or not) was studied using the GNOME corpus (dis-
cussed below) and the CB-tracking techniques de-
veloped to compare alternative ways of instantiating
5The CB is Centering theory?s (Grosz et al, 1995) imple-
mentation of the notion of ?topic? or ?main entity?.
the parameters of Centering by Poesio et al (2004).
Poesio (2003) analyzed, first of all, the distance be-
tween the BD and the closest mention of the an-
chor, finding that of the 169 associative BDs, 77.5%
had an anchor occurring either in the same sentence
(59) or the previous one (72); and that only 4.2% of
anchors were realized more than 5 sentences back.
These percentages are very similar to those found
with pronouns (Hobbs, 1978).
Next, Poesio analyzed the order of mention of the
anchors of the 72 associative BD whose anchor was
in the previous sentence, finding that 49/72, 68%,
were realized in first position. This finding is con-
sistent with the preference for first-mentioned enti-
ties (as opposed to the most recent ones) repeatedly
observed in the psychological literature on anaphora
(Gernsbacher and Hargreaves, 1988; Gordon et al,
1993). Finally, Poesio examined the hypothesis that
finding the anchor of a BD involves knowing which
entities are the CB and the CP in the sense of Cen-
tering (Grosz et al, 1995). He found that CB(U-1)
is the anchor of 37/72 of the BDs whose anchor is
in the previous utterance (51.3%), and only 33.6%
overall. (CP(U-1) was the anchor for 38.2% asso-
ciative BDs.) Clearly, simply choosing the CB
(or the CP) of the previous sentence as the anchor
doesn?t work very well. However, Poesio also found
that 89% of the anchors of associative BDs had been
CBs or CPs. This suggested that while knowing the
local focus isn?t sufficient to determine the anchor
of a BD, restricting the search for anchors to CBs
and CPs only might increase the precision of the BD
resolution process. This hypothesis was supported
by a preliminary test with 20 associative BDs. The
anchor for a BD with head noun NBD was chosen
among the subset of all potential antecedents (PA)
in the previous five sentences that had been CBs or
CPs by calling Google (by hand) with the query ?the
NBD of the NPA?, where NPA is the head noun of the
potential antecedent, and choosing the PA with the
highest hit count. 14 mereological BDs (70%) were
resolved correctly this way.
3 Methods
The results just discussed suggest that lexical infor-
mation and salience information combine to deter-
mine the anchor of associative BRs. The goal of the
experiments discussed in this paper was to test more
thoroughly this hypothesis using machine learning
techniques to combine the two types of informa-
tion, using a larger dataset than used in this pre-
vious work, and using completely automatic tech-
niques. We concentrated on mereological BDs,
but our methods could be used to study other types
of bridging references, using, e.g., the constructions
used by Markert et al (2003).6
3.1 The corpus
We used for these experiments the GNOME corpus,
already used in (Poesio, 2003). An important prop-
erty of this corpus for the purpose of studying BR
resolution is that fewer types of BDs are annotated
than in the original Vieira / Poesio dataset, but the
annotation is reliable (Poesio et al, 2004).7 The cor-
pus also contains more mereological BDs and BRs
than the original dataset used by Poesio and Vieira.
The GNOME corpus contains about 500 sentences
and 3000 NPs. A variety of semantic and discourse
information has been annotated (the manual is
available from the GNOME project?s home page at
http://www.hcrc.ed.ac.uk/ ? gnome).
Four types of anaphoric relations were annotated:
identity (IDENT), set membership (ELEMENT),
subset (SUBSET), and ?generalized possession?
(POSS), which also includes part-of relations. A
total of 2073 anaphoric relations were annotated;
these include 1164 identity relations (including
those realized with synonyms and hyponyms) and
153 POSS relations.
Bridging references are realized by noun phrases
of different types, including indefinites (as in I
bought a book and a page fell out (Prince, 1981)).
Of the 153 mereological references, 58 mereologi-
cal references are realized by definite descriptions.
6In (Poesio, 2003), bridging descriptions based on set rela-
tions (element, subset) were also considered, but we found that
this class of BDs required completely different methods.
7A serious problem when working with bridging references
is the fact that subjects, when asked for judgments about bridg-
ing references in general, have a great deal of difficulty in
agreeing on which expressions in the corpus are bridging ref-
erences, and what their anchors are (Poesio and Vieira, 1998).
This finding raises a number of interesting theoretical questions
concerning the extent of agreement on semantic judgments, but
also the practical question of whether it is possible to evalu-
ate the performance of a system on this task. Subsequent work
found, however, that restricting the type of bridging inferences
required does make it possible for annotators to agree among
themselves (Poesio et al, 2004). In the GNOME corpus only
a few types of associative relations are marked, but these can
be marked reliably, and do include part-of relations like that
between the top and the cabinet that we are concerned with.
3.2 Features
Our classifiers use two types of input features.
Lexical features Only one lexical feature was
used: lexical distance, but extracted from two dif-
ferent lexical sources.
Google distance was computed as in (Poesio,
2003) (see also Markert et al (2003)): given head
nouns NBD of the BD and NPA of a potential an-
tecedent, Google is called (via the Google API) with
a query of the form ?the NBD of the NPA? (e.g., the
sides of the table) and the number of hits NHits is
computed. Then
Google distance =
{
1 if NHits = 0
1
NHits otherwise
The query ?the NBD of NPA? (e.g., the amount of
cream) is used when NPA is used as a mass noun
(information about mass vs count is annotated in the
GNOME corpus). If the potential antecedent is a pro-
noun, the head of the closest realization of the same
discourse entity is used.
We also reconsidered WordNet (1.7.1) as an al-
ternative way of establishing lexical distance, but
made a crucial change from the studies reported
above. Both earlier studies such as (Poesio et al,
1997) and more recent ones (Poesio, 2003; Garcia-
Almanza, 2003) had shown that mereological infor-
mation in WordNet is extremely sparse. However,
these studies also showed that information about hy-
pernyms is much more extensive. This suggested
trading precision for recall with an alternative way
of using WordNet to compute lexical distance: in-
stead of requiring the path between the head pred-
icate of the associative BD and the head predicate
of the potential antecedent to contain at least one
mereological link (various strategies for performing
a search of this type were considered in (Garcia-
Almanza, 2003)), consider only hypernymy and hy-
ponymy links.
To compute our second measure of lexical dis-
tance between NBD and NPA defined as above,
WordNet distance, the following algorithm was
used. Let distance(s, s?) be the number of hyper-
nim links between concepts s and s?. Then
1. Get from WordNet al the senses of both NBD
and NPA;
2. Get the hypernym tree of each of these senses;
3. For each pair of senses sNBDi and sNPAj , find
the Most Specific Common Subsumer scommij
(this is the closest concept which is an hyper-
nym of both senses).
4. The ShortestWNDistance between NBD and
NPA is then computed as the shortest distance
between any of the senses of NBD and any of
the senses of NPA:
ShtstWNDist(NBD,NPA) =
mini,j(distance(sNBDi , scomij ) + distance(s
com
ij , sNPAj ))
5. Finally, a normalized WordNet distance in the
range 0..1 is then obtained by dividing Shtst-
WNDist by a MaxWNDist factor (30 in our ex-
periments). WordNet distance = 1 if no path
between the concepts was found.
WN distance =
{
1 if no path
ShtstWNDist
MaxWNDist otherwise
Salience features In choosing the salience fea-
tures we took into account the results in (Poesio,
2003), but we only used features that were easy to
compute, hoping that they would approximate the
more complex features used in (Poesio, 2003). The
first of these features was utterance distance, the
distance between the utterance in which the BR oc-
curs and the utterance containing the potential an-
tecedent. (Sentences are used as utterances, as sug-
gested by the results of (Poesio et al, 2004).) As
discussed above, studies such as (Poesio, 2003) sug-
gested that bridging references were sensitive to dis-
tance, in the same way as pronouns (Hobbs, 1978;
Clark and Sengul, 1979). This finding was con-
firmed in our study; all anchors of the 58 mereo-
logical BDs occurred within the previous five sen-
tences, and 47/58 (81%) in the previous two. (It
is interesting to note that no anchor occurred in the
same sentence as the BD.)
The second salience feature was boolean:
whether the potential antecedent had been realized
in first mention position in a sentence (Poesio,
2003; Gernsbacher and Hargreaves, 1988; Gordon
et al, 1993). Two forms of this feature were tried:
local first mention (whether the entity had been re-
alized in first position within the previous five sen-
tences) and global first mention (whether it had
been realized in first position anywhere). 269 en-
tities are realized in first position in the five sen-
tences preceding one of the 58 BDs; 298 entities are
realized in first position anywhere in the preceding
text. For 31/58 of the anchors of mereological BDs,
53.5%, local first mention = 1; global first men-
tion = 1 for 33/58 of anchors, 56.9%.
3.3 Training Methods
Constructing the data set The data set used to
train and test BR resolution consisted of a set of
positive instances (the actual anchors of the mere-
ological BRs) and a set of negative instances (other
entities mentioned in the previous five sentences of
the text). However, preliminary tests showed that
simply including all potential antecedents as nega-
tive instances would make the data set too unbal-
anced, particularly when only bridging descriptions
were considered: in this case we would have had
58 positive instances vs. 1672 negative ones. We
therefore developed a parametric script that could
create datasets with different positive / negative ra-
tios - 1:1, 1:2, 1:3 - by including, with each positive
instance, a varying number of negative instances (1,
2, 3, ...) randomly chosen among the other poten-
tial antecedents, the number of negative instances to
be included for each positive one being a parameter
chosen by the experimenter. We report the results
obtained with 1:1 and 1:3 ratios.
The dataset thus constructed was used for both
training and testing, by means of a 10-fold cross-
validation.
Types of Classifiers Used Multi-layer percep-
trons (MLPs) have been claimed to work well with
small datasets; we tested both our own implemen-
tation of an MLP with back-propagation in Mat-
Lab 6.5, experimenting with different configura-
tions, and an off-the-shelf MLP included in the Weka
Machine Learning Library8, Weka-NN. The best
configuration for our own MLP proved to be one
with a sigle hidden layer and 10 hidden nodes. We
also used the implementation of a Naive Bayes clas-
sifier included in the Weka MLL, as Modjeska et al
(2003) reported good results.
4 Experimental Results
In the first series of experiments only mereological
Bridging Descriptions were considered (i.e., only
bridging references realized by the-NPs). In a
second series of experiments we considered all 153
mereological BRs, including ones realized with in-
definites. Finally, we tested a classifier trained on
balanced data (1:1 and 1:3) to find the anchors of
BDs among all possible anchors.
4.1 Experiment 1: Mereological descriptions
The GNOME corpus contains 58 mereological BDs.
The five sentences preceding these 58 BDs contain
a total of 1511 distinct entities for which a head
could be recovered, possibly by examining their an-
tecedents. This means an average of 26 distinct po-
tential antecedents per BD, and 5.2 entities per sen-
tence. The simplest baselines for the task of finding
8The library is available from
http://www.cs.waikato.ac.nz/ml/weka/.
the anchor are therefore 4% (by randomly choos-
ing one antecedent among those in the previous five
sentences) and 19.2% (by randomly choosing one
antecedent among those in the previous sentence
only). As 4.6 entities on average were realized in
first mention position in the five sentences preced-
ing a BD (269/58), choosing randomly among the
first-mentioned entities gives a slighly higher accu-
racy of 21.3%.
A few further baselines can be established by ex-
amining each feature separately. Google didn?t re-
turn any hits for 1089 out of 1511 distinct PAs, and
no hit for 24/58 anchors; in 8/58 of cases (13.8%)
the entity with the minimum Google distance is the
correct anchor. We saw before that the method for
computing WordNet distance used in (Poesio, 2003)
didn?t find a path for any of the mereological BDs;
however, not trying to follow mereological links
worked much better, achieving the same accuracy
as Google distance (8/58, 13.8%) and finding con-
nections for much higher percentages of concepts:
no path could be found for only 10/58 of actual an-
chors, and for 503/1511 potential antecedents.
Pairwise combinations of these features were also
considered. The best such combination, choosing
the first mentioned entity in the previous sentence,
achieves an accuracy of 18/58, 31%. These baseline
results are summarized in the following table. No-
tice how even the best baselines achieve pretty low
accuracy, and how even simple ?salience? measures
work better than lexical distance measures.
Baseline Accuracy
Random choice between entities in previous 5 4%
Random choice between entities in previous 1 19%
Random choice between First Ment. 21.3%
entities in previous 5
Entity with min Google distance 13.8%
Entity with min WordNet distance 13.8%
FM entity in previous sentence 31%
Min Google distance in previous sentence 17.2%
Min WN distance in previous sentence 25.9%
FM and Min Google distance 12%
FM and Min WN distance 24.1%
Table 2: Baselines for the BD task
The features utterance distance, local first men-
tion, and global f.m. were used in all machine learn-
ing experiments. But since one of our goals was to
compare different lexical resources, only one lexi-
cal distance feature was used in the first two experi-
ment.
The three classifiers were trained to classify a po-
tential antecedent as either ?anchor? or ?not anchor?.
The classification results with Google distance and
WN distance for all three classifiers and the 1:1 data
set (116 instances in total, 58 real anchor, 58 nega-
tive instances), for all elements of the data set, and
averaging across the 10 cross-validations, are shown
in Table 3.
WN Distance Google Distance
(Correct) (Correct)
Our own MLP 92(79.3%) 89(76.7%)
Weka NN 91(78.4%) 86(74.1%)
Weka Naive Bayes 88(75.9%) 85(73.3%)
Table 3: Classification results for BDs
These results are clearly better than those ob-
tained with any of the baseline methods discussed
above. The differences between WN distance and
Google distance, and that between our own MLP
and the Weka implementation of Naive Bayes, are
also significant (by a sign test, p ? .05), whereas
the pairwise differences between our own MLP and
Weka?s NN, and between this and the Naive Bayes
classifier, aren?t. In other words, although we find
little difference between using WordNet and Google
to compute lexical distance, using WordNet leads to
slightly better results for BDs. The next table shows
precision, recall and f-values for the positive data
points, for the feature sets using WN distance and
Google distance, respectively:
Precision Recall F-value
WN features 75.4% 84.5% 79.6%
Google features 70.6% 86.2% 77.6%
Table 4: Precision and recall for positive instances
Using a 1:3 dataset (3 negative data points for
each anchor), overall accuracy increases (to 82% us-
ing Google distance) and accuracy with Google dis-
tance is better than with Wordnet distance (80.6%);
however, the precision and recall figures for the
positive data points get much worse: 56.7% with
Google, 55.7% with Wordnet.
4.2 All mereological references
Clearly, 58 positive instances is a fairly small
dataset. In order to have a larger dataset, we in-
cluded every bridging reference in the corpus, in-
cluding those realized with indefinite NPs, thus
bringing the total to 153 positive instances. We then
ran a second series of experiments using the same
methods as before. The results were slightly lower
than those for BDs only, but in this case there was no
difference between using Google and using WN. F-
measure on positive instances was 76.3% with WN,
75.8% with Google.4.3 A harder test
In a last experiment, we used classifiers trained on
balanced and moderately unbalanced data to deter-
mine the anchor of 6 randomly chosen BDs among
WN Distance Google Distance
(Correct) (Correct)
Weka NN 227(74.2%) 230(75.2%)
Table 5: Classification results for all BDs
all of their 346 possible antecedents in context. For
these experiments, we also tried to use both Google
and WordNet simultaneously. The results for BDs
are shown in Table 6. The first column of the table
specifies the lexical resource used; the second the
degree of balance; the next two columns percentage
correct and F value on a testing set with the same
balance as the training set; the final two columns
perc. correct and F value on the harder test set.
The best results,F=.5, are obtained using both
Google and WN distance, and using a larger (if un-
balanced) training corpus. These results are not as
good as those obtained (by hand) by Poesio (which,
however, used a complete focus tracking mecha-
nism), but the F measure is still 66% higher than
that obtained with the highest baseline (FM only),
and not far off from the results obtained with direct
anaphoric definite descriptions (e.g., by (Poesio and
Alexandrov-Kabadjov, 2004)). It?s also conforting
to note that results with the harder test improve the
more data are used, which suggests that better re-
sults could be obtained with a larger corpus.
5 Related work
In recent years there has been a lot of work to
develop anaphora resolution algorithms using both
symbolic and statistical methods that could be quan-
titatively evaluated (Humphreys et al, 1997; Ng and
Cardie, 2002) but this work focused on identity rela-
tions; bridging references were explicitly excluded
from the MUC coreference task because of the prob-
lems with reliability discussed earlier. Thus, most
work on bridging has been theoretical, like the work
by Asher and Lascarides (1998).
Apart from the work by Poesio et al, the main
other studies attempting quantitative evaluations of
bridging reference resolution are (Markert et al,
1996; Markert et al, 2003). Markert et al (1996)
also argue for the need to use both Centering in-
formation and conceptual knowledge, and attempt
to characterize the ?best? paths on the basis of an
analysis of part-of relations, but use a hand-coded,
domain-dependent knowledge base. Markert et al
(2003) focus on other anaphora, using Hearst? pat-
terns to mine information about hyponymy from the
Web, but do not use focusing knowledge.
6 Discussion and Conclusions
The two main results of this study are, first of
all, that combining ?salience? features with ?lexi-
cal? features leads to much better results than us-
ing either method in isolation; and that these re-
sults are an improvement over those previously re-
ported in the literature. A secondary, but still in-
teresting, result is that using WordNet in a different
way ?taking advantage of its extensive information
about hypernyms to obviate its lack of information
about meronymy?obviates the problems previously
reported in the literature on using WordNet for re-
solving mereological bridging references, leading to
results comparable to those obtained using Google.
(Of course, from a practical perspective Google may
still be preferrable, particularly for languages for
which no WordNet exists.)
The main limitation of the present work is that
the number of BDs and BRs considered, while larger
than in our previous studies, is still fairly small.
Unfortunately, creating a reasonably accurate gold
standard for this type of semantic interpretation pro-
cess is slow work. Our first priority will be therefore
to extend the data set, including also the original
cases studied by Poesio and Vieira.
Current and future work will also include in-
corporating the methods tested here in an actual
anaphora resolution system, the GUITAR system
(Poesio and Alexandrov-Kabadjov, 2004). We are
also working on methods for automatically recog-
nizing bridging descriptions, and dealing with other
types of (non-associative) bridging references based
on synonymy and hyponymy.
Acknowledgments
The creation of the GNOME corpus was supported
by the EPSRC project GNOME, GR/L51126/01.
References
N. Asher and A. Lascarides. 1998. Bridging. Jour-
nal of Semantics, 15(1):83?13.
M. Berland and E. Charniak. 1999. Finding parts in
very large corpora. In Proc. of the 37th ACL.
H. H. Clark and C. J. Sengul. 1979. In search of
referents for nouns and pronouns. Memory and
Cognition, 7(1):35?41.
H. H. Clark. 1977. Bridging. In P. N. Johnson-
Laird and P.C. Wason, editors, Thinking: Read-
ings in Cognitive Science. Cambridge.
C. Fellbaum, editor. 1998. WordNet: An electronic
lexical database. The MIT Press.
A. Garcia-Almanza. 2003. Using WordNet for
mereological anaphora resolution. Master?s the-
sis, University of Essex.
Lex Res Balance Perc on bal F on bal Perc on Hard F on Hard
WN 1:1 70.2% .7 80.2% .2
1:3 75.9% .4 91.7% 0
Google 1:1 64.4% .7 63.6% .1
1.3 79.8% .5 88.4% .3
WN + 1:1 66.3% .6 65.3% .2
Google 1.3 77.9% .4 92.5% .5
Table 6: Results using a classifier trained on balanced data on unbalanced ones.
M. A. Gernsbacher and D. Hargreaves. 1988. Ac-
cessing sentence participants. Journal of Mem-
ory and Language, 27:699?717.
P. C. Gordon, B. J. Grosz, and L. A. Gillion. 1993.
Pronouns, names, and the centering of attention
in discourse. Cognitive Science, 17:311?348.
G. Grefenstette. 1993. SEXTANT: extracting se-
mantics from raw text. Heuristics.
B. J. Grosz and C. L. Sidner. 1986. Attention, in-
tention, and the structure of discourse. Computa-
tional Linguistics, 12(3):175?204.
B. J. Grosz, A. K. Joshi, and S. Weinstein.
1995. Centering. Computational Linguistics,
21(2):202?225.
S. Harabagiu and D. Moldovan. 1998. Knowledge
processing on extended WordNet. In (Fellbaum,
1998), pages 379?405.
M. A. Hearst. 1998. Automated discovery of Word-
net relations. In (Fellbaum, 1998).
J. R. Hobbs. 1978. Resolving pronoun references.
Lingua, 44:311?338.
K. Humphreys, R. Gaizauskas, S. Azzam,
C. Huyck, B. Mitchell, and H. Cunning-
ham Y. Wilks. 1997. Description of the LaSIE-II
System as used for MUC-7. In Proc. of the 7th
Message Understanding Conference (MUC-7).
T. Ishikawa. 1998. Acquisition of associative infor-
mation and resolution of bridging descriptions.
Master?s thesis, University of Edinburgh.
F. Keller and M. Lapata. 2003. Using the Web to
obtain frequencies for unseen bigrams. Compu-
tational Linguistics, 29(3).
K. Lund, C. Burgess, and R. A. Atchley. 1995.
Semantic and associative priming in high-
dimensional semantic space. In Proc. of the 17th
Conf. of the Cogn. Science Soc., pages 660?665.
K. Markert, M. Strube, and U. Hahn. 1996.
Inferential realization constraints on functional
anaphora in the centering model. In Proc. of 18th
Conf. of the Cog. Science Soc., pages 609?614.
K. Markert, M. Nissim, and N.. Modjeska. 2003.
Using the Web for nominal anaphora resolution.
In Proc. of the EACL Workshop on the Computa-
tional Treatment of Anaphora, pages 39?46.
N. Modjeska, K. Markert, and M. Nissim. 2003.
Using the Web in ML for anaphora resolution. In
Proc. of EMNLP-03, pages 176?183.
V. Ng and C. Cardie. 2002. Improving machine
learning approaches to coreference resolution. In
Proceedings of the 40th Meeting of the ACL.
M. Poesio and R. Vieira. 1998. A corpus-based in-
vestigation of definite description use. Computa-
tional Linguistics, 24(2):183?216, June.
M. Poesio, R. Vieira, and S. Teufel. 1997. Resolv-
ing bridging references in unrestricted text. In
R. Mitkov, editor, Proc. of the ACL Workshop on
Robust Anaphora Resolution, pages 1?6, Madrid.
M. Poesio, S. Schulte im Walde, and C. Brew. 1998.
Lexical clustering and definite description inter-
pretation. In Proc. of the AAAI Spring Sympo-
sium on Learning for Discourse, pages 82?89.
M. Poesio, T. Ishikawa, S. Schulte im Walde, and
R. Vieira. 2002. Acquiring lexical knowledge for
anaphora resolution. In Proc. of the 3rd LREC.
M. Poesio and M. Alexandrov-Kabadjov. 2004. A
general-purpose, off the shelf anaphoric resolver.
In Proc. of the 4th LREC, Lisbon.
M. Poesio, R. Stevenson, B. Di Eugenio, and J. M.
Hitzeman. 2004. Centering: A parametric theory
and its instantiations. Comp. Linguistics. 30(3).
M. Poesio. 2003. Associative descriptions and
salience. In Proc. of the EACL Workshop on
Computational Treatments of Anaphora.
E. F. Prince. 1981. Toward a taxonomy of given-
new information. In P. Cole, editor, Radical
Pragmatics, pages 223?256. Academic Press.
C. L. Sidner. 1979. Towards a computational the-
ory of definite anaphora comprehension in En-
glish discourse. Ph.D. thesis, MIT.
O. Uryupina. 2003. High-precision identification
of discourse-new and unique noun phrases. In
Proc. of ACL 2003 Stud. Workshop, pages 80?86.
R. Vieira and M. Poesio. 2000. An empirically-
based system for processing definite descriptions.
Computational Linguistics, 26(4), December.
Evaluating Centering-based metrics of coherence for text
structuring using a reliably annotated corpus
Nikiforos Karamanis,? Massimo Poesio,? Chris Mellish,? and Jon Oberlander?
?School of Informatics, University of Edinburgh, UK, {nikiforo,jon}@ed.ac.uk
?Dept. of Computer Science, University of Essex, UK, poesio at essex dot ac dot uk
?Dept. of Computing Science, University of Aberdeen, UK, cmellish@csd.abdn.ac.uk
Abstract
We use a reliably annotated corpus to compare
metrics of coherence based on Centering The-
ory with respect to their potential usefulness for
text structuring in natural language generation.
Previous corpus-based evaluations of the coher-
ence of text according to Centering did not com-
pare the coherence of the chosen text structure
with that of the possible alternatives. A corpus-
based methodology is presented which distin-
guishes between Centering-based metrics taking
these alternatives into account, and represents
therefore a more appropriate way to evaluate
Centering from a text structuring perspective.
1 Motivation
Our research area is descriptive text generation
(O?Donnell et al, 2001; Isard et al, 2003), i.e.
the generation of descriptions of objects, typi-
cally museum artefacts, depicted in a picture.
Text (1), from the gnome corpus (Poesio et al,
2004), is an example of short human-authored
text from this genre:
(1) (a) 144 is a torc. (b) Its present arrangement,
twisted into three rings, may be a modern al-
teration; (c) it should probably be a single ring,
worn around the neck. (d) The terminals are
in the form of goats? heads.
According to Centering Theory (Grosz et al,
1995; Walker et al, 1998a), an important fac-
tor for the felicity of (1) is its entity coherence:
the way centers (discourse entities), such as
the referent of the NPs ?144? in clause (a) and
?its? in clause (b), are introduced and discussed
in subsequent clauses. It is often claimed in
current work on in natural language generation
that the constraints on felicitous text proposed
by the theory are useful to guide text struc-
turing, in combination with other factors (see
(Karamanis, 2003) for an overview). However,
how successful Centering?s constraints are on
their own in generating a felicitous text struc-
ture is an open question, already raised by the
seminal papers of the theory (Brennan et al,
1987; Grosz et al, 1995). In this work, we ex-
plored this question by developing an approach
to text structuring purely based on Centering,
in which the role of other factors is deliberately
ignored.
In accordance with recent work in the emerg-
ing field of text-to-text generation (Barzilay et
al., 2002; Lapata, 2003), we assume that the in-
put to text structuring is a set of clauses. The
output of text structuring is merely an order-
ing of these clauses, rather than the tree-like
structure of database facts often used in tradi-
tional deep generation (Reiter and Dale, 2000).
Our approach is further characterized by two
key insights. The first distinguishing feature is
that we assume a search-based approach to text
structuring (Mellish et al, 1998; Kibble and
Power, 2000; Karamanis and Manurung, 2002)
in which many candidate orderings of clauses
are evaluated according to scores assigned by
a given metric, and the best-scoring ordering
among the candidate solutions is chosen. The
second novel aspect is that our approach is
based on the position that the most straight-
forward way of using Centering for text struc-
turing is by defining a Centering-based metric
of coherence Karamanis (2003). Together, these
two assumptions lead to a view of text planning
in which the constraints of Centering act not
as filters, but as ranking factors, and the text
planner may be forced to choose a sub-optimal
solution.
However, Karamanis (2003) pointed out that
many metrics of coherence can be derived from
the claims of Centering, all of which could be
used for the type of text structuring assumed in
this paper. Hence, a general methodology for
identifying which of these metrics represent the
most promising candidates for text structuring
is required, so that at least some of them can
be compared empirically. This is the second re-
search question that this paper addresses, build-
ing upon previous work on corpus-based evalu-
ations of Centering, and particularly the meth-
ods used by Poesio et al (2004). We use the
gnome corpus (Poesio et al, 2004) as the do-
main of our experiments because it is reliably
annotated with features relevant to Centering
and contains the genre that we are mainly in-
terested in.
To sum up, in this paper we try to iden-
tify the most promising Centering-based metric
for text structuring, and to evaluate how useful
this metric is for that purpose, using corpus-
based methods instead of generally more expen-
sive psycholinguistic techniques. The paper is
structured as follows. After discussing how the
gnome corpus has been used in previous work
to evaluate the coherence of a text according to
Centering we discuss why such evaluations are
not sufficient for text structuring. We continue
by showing how Centering can be used to define
different metrics of coherence which might be
useful to drive a text planner. We then outline
a corpus-based methodology to choose among
these metrics, estimating how well they are ex-
pected to do when used by a text planner. We
conclude by discussing our experiments in which
this methodology is applied using a subset of the
gnome corpus.
2 Evaluating the coherence of a
corpus text according to Centering
In this section we briefly introduce Centering,
as well as the methodology developed in Poesio
et al (2004) to evaluate the coherence of a text
according to Centering.
2.1 Computing CF lists, CPs and CBs
According to Grosz et al (1995), each ?utter-
ance? in a discourse is assigned a list of for-
ward looking centers (CF list) each of which is
?realised? by at least one NP in the utterance.
The members of the CF list are ?ranked? in or-
der of prominence, the first element being the
preferred center CP.
In this paper, we used what we considered to
be the most common definitions of the central
notions of Centering (its ?parameters?). Poe-
sio et al (2004) point out that there are many
definitions of parameters such as ?utterance?,
?ranking? or ?realisation?, and that the setting
of these parameters greatly affects the predic-
tions of the theory;1 however, they found viola-
tions of the Centering constraints with any way
of setting the parameters (for instance, at least
25% of utterances have no CB under any such
setting), so that the questions addressed by our
work arise for all other settings as well.
Following most mainstream work on Center-
ing for English, we assume that an ?utterance?
corresponds to what is annotated as a finite unit
in the gnome corpus.2 The spans of text with
the indexes (a) to (d) in example (1) are exam-
ples. This definition of utterance is not optimal
from the point of view of minimizing Centering
violations (Poesio et al, 2004), but in this way
most utterances are the realization of a single
proposition; i.e., the impact of aggregation is
greatly reduced. Similarly, we use grammatical
function (gf) combined with linear order within
the unit (what Poesio et al (2004) call gfthere-
lin) for CF ranking. In this configuration, the
CP is the referent of the first NP within the unit
that is annotated as a subject for its gf.3
Example (2) shows the relevant annotation
features of unit u210 which corresponds to
utterance (a) in example (1). According to
gftherelin, the CP of (a) is the referent of ne410
?144?.
(2) <unit finite=?finite-yes? id=?u210?>
<ne id="ne410" gf="subj">144</ne>
is
<ne id="ne411" gf="predicate">
a torc</ne> </unit>.
The ranking of the CFs other than the
CP is defined according to the following pref-
erence on their gf (Brennan et al, 1987):
obj>iobj>other. CFs with the same gf are
ranked according to the linear order of the cor-
responding NPs in the utterance. The second
column of Table 1 shows how the utterances in
example (1) are automatically translated by the
scripts developed by Poesio et al (2004) into a
1For example, one could equate ?utterance? with sen-
tence (Strube and Hahn, 1999; Miltsakaki, 2002), use
indirect realisation for the computation of the CF list
(Grosz et al, 1995), rank the CFs according to their
information status (Strube and Hahn, 1999), etc.
2Our definition includes titles which are not always
finite units, but excludes finite relative clauses, the sec-
ond element of coordinated VPs and clause complements
which are often taken as not having their own CF lists
in the literature.
3Or as a post-copular subject in a there-clause.
CF list: cheapness
U {CP, other CFs} CB Transition CBn=CPn?1
(a) {de374, de375} n.a. n.a. n.a.
(b) {de376, de374, de377} de374 retain +
(c) {de374, de379} de374 continue ?
(d) {de380, de381, de382} - nocb +
Table 1: CP, CFs other than CP, CB, nocb or standard (see Table 2) transition and violations of
cheapness (denoted with an asterisk) for each utterance (U) in example (1)
coherence: coherence?:
CBn=CBn?1 CBn 6=CBn?1
or nocb in CFn?1
salience: CBn=CPn continue smooth-shift
salience?: CBn 6=CPn retain rough-shift
Table 2: coherence, salience and the table of standard transitions
sequence of CF lists, each decomposed into the
CP and the CFs other than the CP, according
to the chosen setting of the Centering param-
eters. Note that the CP of (a) is the center
de374 and that the same center is used as the
referent of the other NPs which are annotated
as coreferring with ne410.
Given two subsequent utterances Un?1 and
Un, with CF lists CFn?1 and CFn respectively,
the backward looking center of Un, CBn, is de-
fined as the highest ranked element of CFn?1
which also appears in CFn (Centering?s Con-
straint 3). For instance, the CB of (b) is de374.
The third column of Table 1 shows the CB for
each utterance in (1).4
2.2 Computing transitions
As the fourth column of Table 1 shows, each
utterance, with the exception of (a), is also
marked with a transition from the previous one.
When CFn and CFn?1 do not have any cen-
ters in common, we compute the nocb transi-
tion (Kibble and Power, 2000) (Poesio et als
null transition) for Un (e.g., utterance (d) in
Table 1).5
4In accordance with Centering, no CB is computed
for (a), the first utterance in the sequence.
5In this study we do not take indirect realisation into
account, i.e., we ignore the bridging reference (anno-
tated in the corpus) between the referent of ?it? de374
in (c) and the referent of ?the terminals? de380 in (d),
by virtue of which de374 might be thought as being a
member of the CF list of (d). Poesio et al (2004) showed
that hypothesizing indirect realization eliminates many
violations of entity continuity, the part of Constraint
1 that rules out nocb transitions. However, in this work
we are treating CF lists as an abstract representation
Following again the terminology in Kibble
and Power (2000), we call the requirement that
CBn be the same as CBn?1 the principle of co-
herence and the requirement that CBn be the
same as CPn the principle of salience. Each
of these principles can be satisfied or violated
while their various combinations give rise to the
standard transitions of Centering shown in Ta-
ble 2; Poesio et als scripts compute these vio-
lations.6 We also make note of the preference
between these transitions, known as Centering?s
Rule 2 (Brennan et al, 1987): continue is pre-
ferred to retain, which is preferred to smooth-
shift, which is preferred to rough-shift.
Finally, the scripts determine whether CBn
is the same as CPn?1, known as the principle
of cheapness (Strube and Hahn, 1999). The
last column of Table 1 shows the violations of
cheapness (denoted with an asterisk) in (1).7
2.3 Evaluating the coherence of a text
and text structuring
The statistics about transitions computed as
just discussed can be used to determine the de-
gree to which a text conforms with, or violates,
Centering?s principles. Poesio et al (2004)
found that nocbs account for more than 50%
of the atomic facts the algorithm has to structure, i.e.,
we are assuming that CFs are arguments of such facts;
including indirectly realized entities in CF lists would
violate this assumption.
6If the second utterance in a sequence U2 has a CB,
then it is taken to be either a continue or a retain,
although U1 is not classified as a nocb.
7As for the other two principles, no violation of
cheapness is computed for (a) or when Un is marked as
a nocb.
of the transitions in the gnome corpus in con-
figurations such as the one used in this pa-
per. More generally, a significant percentage of
nocbs (at least 20%) and other ?dispreferred?
transitions was found with all parameter config-
urations tested by Poesio et al (2004) and in-
deed by all previous corpus-based evaluations of
Centering such as Passoneau (1998), Di Eugenio
(1998), Strube and Hahn (1999) among others.
These results led Poesio et al (2004) to the
conclusion that the entity coherence as formal-
ized in Centering should be supplemented with
an account of other coherence inducing factors
to explain what makes texts coherent.
These studies, however, do not investigate
the question that is most important from the
text structuring perspective adopted in this pa-
per: whether there would be alternative ways of
structuring the text that would result in fewer
violations of Centering?s constraints (Kibble,
2001). Consider the nocb utterance (d) in (1).
Simply observing that this transition is ?dispre-
ferred? ignores the fact that every other ordering
of utterances (b) to (d) would result in more
nocbs than those found in (1). Even a text-
structuring algorithm functioning solely on the
basis of the Centering constraints might there-
fore still choose the particular order in (1). In
other words, a metric of text coherence purely
based on Centering principles?trying to mini-
mize the number of nocbs?may be sufficient to
explain why this order of clauses was chosen,
at least in this particular genre, without need
to involve more complex explanations. In the
rest of the paper, we consider several such met-
rics, and use the texts in the gnome corpus to
choose among them. We return to the issue of
coherence (i.e., whether additional coherence-
inducing factors need to be stipulated in addi-
tion to those assumed in Centering) in the Dis-
cussion.
3 Centering-based metrics of
coherence
As said previously, we assume a text structuring
system taking as input a set of utterances rep-
resented in terms of their CF lists. The system
orders these utterances by applying a bias in
favour of the best scoring ordering among the
candidate solutions for the preferred output.8
In this section, we discuss how the Centering
8Additional assumptions for choosing between the or-
derings that are assigned the best score are presented in
the next section.
concepts just described can be used to define
metrics of coherence which might be useful for
text structuring.
The simplest way to define a metric of coher-
ence using notions from Centering is to classify
each ordering of propositions according to the
number of nocbs it contains, and pick the or-
dering with the fewest nocbs. We call this met-
ric M.NOCB, following (Karamanis and Manu-
rung, 2002). Because of its simplicity, M.NOCB
serves as the baseline metric in our experiments.
We consider three more metrics. M.CHEAP
is biased in favour of the ordering with the
fewest violations of cheapness. M.KP sums
up the nocbs and the violations of cheapness,
coherence and salience, preferring the or-
dering with the lowest total cost (Kibble and
Power, 2000). Finally, M.BFP employs the
preferences between standard transitions as ex-
pressed by Rule 2. More specifically, M.BFP
selects the ordering with the highest number
of continues. If there exist several orderings
which have the most continues, the one which
has the most retains is favoured. The number
of smooth-shifts is used only to distinguish
between the orderings that score best for con-
tinues as well as for retains, etc.
In the next section, we present a general
methodology to compare these metrics, using
the actual ordering of clauses in real texts of
a corpus to identify the metric whose behav-
ior mimics more closely the way these actual
orderings were chosen. This methodology was
implemented in a program called the System for
Evaluating Entity Coherence (seec).
4 Exploring the space of possible
orderings
In section 2, we discussed how an ordering of
utterances in a text like (1) can be translated
into a sequence of CF lists, which is the repre-
sentation that the Centering-based metrics op-
erate on. We use the term Basis for Comparison
(BfC) to indicate this sequence of CF lists. In
this section, we discuss how the BfC is used in
our search-oriented evaluation methodology to
calculate a performance measure for each metric
and compare them with each other. In the next
section, we will see how our corpus was used
to identify the most promising Centering-based
metric for a text classifier.
4.1 Computing the classification rate
The performance measure we employ is called
the classification rate of a metric M on a cer-
tain BfC B. The classification rate estimates
the ability of M to produce B as the output of
text structuring according to a specific genera-
tion scenario.
The first step of seec is to search through
the space of possible orderings defined by the
permutations of the CF lists that B consists of,
and to divide the explored search space into sets
of orderings that score better, equal, or worse
than B according to M.
Then, the classification rate is defined accord-
ing to the following generation scenario. We
assume that an ordering has higher chances of
being selected as the output of text structuring
the better it scores for M. This is turn means
that the fewer the members of the set of better
scoring orderings, the better the chances of B
to be the chosen output.
Moreover, we assume that additional factors
play a role in the selection of one of the order-
ings that score the same for M. On average, B
is expected to sit in the middle of the set of
equally scoring orderings with respect to these
additional factors. Hence, half of the orderings
with the same score will have better chances
than B to be selected by M.
The classification rate ? of a metric M on
B expresses the expected percentage of order-
ings with a higher probability of being gener-
ated than B according to the scores assigned
by M and the additional biases assumed by the
generation scenario as follows:
(3) Classification rate:
?(M,B) = Better(M) + Equal(M)2
Better(M) stands for the percentage of order-
ings that score better than B according to M,
whilst Equal(M) is the percentage of order-
ings that score equal to B according to M. If
?(Mx, B) is the classification rate of Mx on B,
and ?(My, B) is the classification rate of My on
B, My is a more suitable candidate than Mx
for generating B if ?(My, B) is smaller than
?(Mx, B).
4.2 Generalising across many BfCs
In order for the experimental results to be re-
liable and generalisable, Mx and My should be
compared on more than one BfC from a corpus
C. In our standard analysis, the BfCs B1, ..., Bm
from C are treated as the random factor in a
repeated measures design since each BfC con-
tributes a score for each metric. Then, the clas-
sification rates for Mx and My on the BfCs are
compared with each other and significance is
tested using the Sign Test. After calculating the
number of BfCs that return a lower classifica-
tion rate for Mx than for My and vice versa, the
Sign Test reports whether the difference in the
number of BfCs is significant, that is, whether
there are significantly more BfCs with a lower
classification rate for Mx than the BfCs with a
lower classification rate for My (or vice versa).9
Finally, we summarise the performance of M
on m BfCs from C in terms of the average clas-
sification rate Y :
(4) Average classification rate:
Y (M,C) = ?(M,B1)+...+?(M,Bm)m
5 Using the gnome corpus for a
search-based comparison of
metrics
We will now discuss how the methodology
discussed above was used to compare the
Centering-based metrics discussed in Section
3, using the original ordering of texts in the
gnome corpus to compute the average classi-
fication rate of each metric.
The gnome corpus contains texts from differ-
ent genres, not all of which are of interest to us.
In order to restrict the scope of the experiment
to the text-type most relevant to our study, we
selected 20 ?museum labels?, i.e., short texts
that describe a concrete artefact, which served
as the input to seec together with the metrics
in section 3.10
5.1 Permutation and search strategy
In specifying the performance of the metrics we
made use of a simple permutation heuristic ex-
ploiting a piece of domain-specific communica-
tion knowledge (Kittredge et al, 1991). Like
Dimitromanolaki and Androutsopoulos (2003),
we noticed that utterances like (a) in exam-
ple (1), should always appear at the beginning
of a felicitous museum label. Hence, we re-
stricted the orderings considered by the seec
9The Sign Test was chosen over its parametric al-
ternatives to test significance because it does not carry
specific assumptions about population distributions and
variance. It is also more appropriate for small samples
like the one used in this study.
10Note that example (1) is characteristic of the genre,
not the length, of the texts in our subcorpus. The num-
ber of CF lists that the BfCs consist of ranges from 4 to
16 (average cardinality: 8.35 CF lists).
Pair M.NOCB p Winner
lower greater ties
M.NOCB vs M.CHEAP 18 2 0 0.000 M.NOCB
M.NOCB vs M.KP 16 2 2 0.001 M.NOCB
M.NOCB vs M.BFP 12 3 5 0.018 M.NOCB
N 20
Table 3: Comparing M.NOCB with M.CHEAP, M.KP and M.BFP in gnome
to those in which the first CF list of B, CF1,
appears in first position.11
For very short texts like (1), which give rise to
a small BfC, the search space of possible order-
ings can be enumerated exhaustively. However,
when B consists of many more CF lists, it is im-
practical to explore the search space in this way.
Elsewhere we show that even in these cases it
is possible to estimate ?(M,B) reliably for the
whole population of orderings using a large ran-
dom sample. In the experiments reported here,
we had to resort to random sampling only once,
for a BfC with 16 CF lists.
5.2 Comparing M.NOCB with other
metrics
The experimental results of the comparisons of
the metrics from section 3, computed using the
methodology in section 4, are reported in Ta-
ble 3.
In this table, the baseline metric M.NOCB is
compared with each of M.CHEAP, M.KP and
M.BFP. The first column of the Table identifies
the comparison in question, e.g. M.NOCB ver-
sus M.CHEAP. The exact number of BfCs for
which the classification rate of M.NOCB is lower
than its competitor for each comparison is re-
ported in the next column of the Table. For ex-
ample, M.NOCB has a lower classification rate
than M.CHEAP for 18 (out of 20) BfCs from
the gnome corpus. M.CHEAP only achieves a
lower classification rate for 2 BfCs, and there
are no ties, i.e. cases where the classification
rate of the two metrics is the same. The p value
returned by the Sign Test for the difference in
the number of BfCs, rounded to the third deci-
mal place, is reported in the fifth column of the
Table. The last column of the Table 3 shows
M.NOCB as the ?winner? of the comparison
with M.CHEAP since it has a lower classifica-
11Thus, we assume that when the set of CF lists serves
as the input to text structuring, CF1 will be identified
as the initial CF list of the ordering to be generated
using annotation features such as the unit type which
distinguishes (a) from the other utterances in (1).
tion rate than its competitor for significantly
more BfCs in the corpus.12
Overall, the Table shows that M.NOCB does
significantly better than the other three metrics
which employ additional Centering concepts.
This result means that there exist proportion-
ally fewer orderings with a higher probability of
being selected than the BfC when M.NOCB is
used to guide the hypothetical text structuring
algorithm instead of the other metrics.
Hence, M.NOCB is the most suitable among
the investigated metrics for structuring the CF
lists in gnome. This in turn indicates that sim-
ply avoiding nocb transitions is more relevant
to text structuring than the combinations of the
other Centering notions that the more compli-
cated metrics make use of. (However, these no-
tions might still be appropriate for other tasks,
such as anaphora resolution.)
6 Discussion: the performance of
M.NOCB
We already saw that Poesio et al (2004) found
that the majority of the recorded transitions in
the configuration of Centering used in this study
are nocbs. However, we also explained in sec-
tion 2.3 that what really matters when trying
to determine whether a text might have been
generated only paying attention to Centering
constraints is the extent to which it would be
possible to ?improve? upon the ordering chosen
in that text, given the information that the text
structuring algorithm had to convey. The av-
erage classification rate of M.NOCB is an esti-
12No winner is reported for a comparison when the p
value returned by the Sign Test is not significant (ns),
i.e. greater than 0.05. Note also that despite conduct-
ing more than one pairwise comparison simultaneously
we refrain from further adjusting the overall threshold
of significance (e.g. according to the Bonferroni method,
typically used for multiple planned comparisons that em-
ploy parametric statistics) since it is assumed that choos-
ing a conservative statistic such as the Sign Test already
provides substantial protection against the possibility of
a type I error.
Pair M.NOCB p Winner
lower greater ties
M.NOCB vs M.CHEAP 110 12 0 0.000 M.NOCB
M.NOCB vs M.KP 103 16 3 0.000 M.NOCB
M.NOCB vs M.BFP 41 31 49 0.121 ns
N 122
Table 4: Comparing M.NOCB with M.CHEAP, M.KP and M.BFP using the novel methodology
in MPIRO
mate of exactly this variable, indicating whether
M.NOCB is likely to arrive at the BfC during
text structuring.
The average classification rate Y for
M.NOCB on the subcorpus of gnome studied
here, for the parameter configuration of Cen-
tering we have assumed, is 19.95%. This means
that on average the BfC is close to the top 20%
of alternative orderings when these orderings
are ranked according to their probability of
being selected as the output of the algorithm.
On the one hand, this result shows that al-
though the ordering of CF lists in the BfC
might not completely minimise the number of
observed nocb transitions, the BfC tends to
be in greater agreement with the preference to
avoid nocbs than most of the alternative or-
derings. In this sense, it appears that the BfC
optimises with respect to the number of poten-
tial nocbs to a certain extent. On the other
hand, this result indicates that there are quite
a few orderings which would appear more likely
to be selected than the BfC.
We believe this finding can be interpreted in
two ways. One possibility is that M.NOCB
needs to be supplemented by other features in
order to explain why the original text was struc-
tured this way. This is the conclusion arrived at
by Poesio et al (2004) and those text structur-
ing practitioners who use notions derived from
Centering in combination with other coherence
constraints in the definitions of their metrics.
There is also a second possibility, however: we
might want to reconsider the assumption that
human text planners are trying to ensure that
each utterance in a text is locally coherent.
They might do all of their planning just on the
basis of Centering constraints, at least in this
genre ?perhaps because of resource limitations?
and simply accept a certain degree of incoher-
ence. Further research on this issue will require
psycholinguistic methods; our analysis never-
theless sheds more light on two previously un-
addressed questions in the corpus-based evalu-
ation of Centering ? a) which of the Centering
notions are most relevant to the text structur-
ing task, and b) to which extent Centering on
its own can be useful for this purpose.
7 Further results
In related work, we applied the methodology
discussed here to a larger set of existing data
(122 BfCs) derived from the MPIRO system
and ordered by a domain expert (Dimitro-
manolaki and Androutsopoulos, 2003). As Ta-
ble 4 shows, the results from MPIRO verify the
ones reported here, especially with respect to
M.KP and M.CHEAP which are overwhelm-
ingly beaten by the baseline in the new do-
main as well. Also note that since M.BFP fails
to overtake M.NOCB in MPIRO, the baseline
can be considered the most promising solution
among the ones investigated in both domains
by applying Occam?s logical principle.
We also tried to account for some additional
constraints on coherence, namely local rhetor-
ical relations, based on some of the assump-
tions in Knott et al (2001), and what Kara-
manis (2003) calls the ?PageFocus? which cor-
responds to the main entity described in a text,
in our example de374. These results, reported
in (Karamanis, 2003), indicate that these con-
straints conflict with Centering as formulated in
this paper, by increasing - instead of reducing
- the classification rate of the metrics. Hence,
it remains unclear to us how to improve upon
M.NOCB.
In our future work, we would like to experi-
ment with more metrics. Moreover, although we
consider the parameter configuration of Center-
ing used here a plausible choice, we intend to ap-
ply our methodology to study different instan-
tiations of the Centering parameters, e.g. by
investigating whether ?indirect realisation? re-
duces the classification rate for M.NOCB com-
pared to ?direct realisation?, etc.
Acknowledgements
Special thanks to James Soutter for writing the
program which translates the output produced by
gnome?s scripts into a format appropriate for seec.
The first author was able to engage in this research
thanks to a scholarship from the Greek State Schol-
arships Foundation (IKY).
References
Regina Barzilay, Noemie Elhadad, and Kath-
leen McKeown. 2002. Inferring strategies
for sentence ordering in multidocument news
summarization. Journal of Artificial Intelli-
gence Research, 17:35?55.
Susan E. Brennan, Marilyn A. Fried-
man [Walker], and Carl J. Pollard. 1987. A
centering approach to pronouns. In Proceed-
ings of ACL 1987, pages 155?162, Stanford,
California.
Barbara Di Eugenio. 1998. Centering in Italian.
In Walker et al (Walker et al, 1998b), pages
115?137.
Aggeliki Dimitromanolaki and Ion Androut-
sopoulos. 2003. Learning to order facts for
discourse planning in natural language gen-
eration. In Proceedings of the 9th European
Workshop on Natural Language Generation,
Budapest, Hungary.
Barbara J. Grosz, Aravind K. Joshi, and Scott
Weinstein. 1995. Centering: A framework
for modeling the local coherence of discourse.
Computational Linguistics, 21(2):203?225.
Amy Isard, Jon Oberlander, Ion Androutsopou-
los, and Colin Matheson. 2003. Speaking the
users? languages. IEEE Intelligent Systems
Magazine, 18(1):40?45.
Nikiforos Karamanis and Hisar Maruli Manu-
rung. 2002. Stochastic text structuring us-
ing the principle of continuity. In Proceedings
of INLG 2002, pages 81?88, Harriman, NY,
USA, July.
Nikiforos Karamanis. 2003. Entity Coherence
for Descriptive Text Structuring. Ph.D. the-
sis, Division of Informatics, University of Ed-
inburgh.
Rodger Kibble and Richard Power. 2000. An
integrated framework for text planning and
pronominalisation. In Proceedings of INLG
2000, pages 77?84, Israel.
Rodger Kibble. 2001. A reformulation of Rule
2 of Centering Theory. Computational Lin-
guistics, 27(4):579?587.
Richard Kittredge, Tanya Korelsky, and Owen
Rambow. 1991. On the need for domain com-
munication knowledge. Computational Intel-
ligence, 7:305?314.
Alistair Knott, Jon Oberlander, Mick
O?Donnell, and Chris Mellish. 2001. Beyond
elaboration: The interaction of relations
and focus in coherent text. In T. Sanders,
J. Schilperoord, and W. Spooren, edi-
tors, Text Representation: Linguistic and
Psycholinguistic Aspects, chapter 7, pages
181?196. John Benjamins.
Mirella Lapata. 2003. Probabilistic text struc-
turing: Experiments with sentence ordering.
In Proceedings of ACL 2003, Saporo, Japan,
July.
Chris Mellish, Alistair Knott, Jon Oberlander,
and Mick O?Donnell. 1998. Experiments us-
ing stochastic search for text planning. In
Proceedings of the 9th International Work-
shop on NLG, pages 98?107, Niagara-on-the-
Lake, Ontario, Canada.
Eleni Miltsakaki. 2002. Towards an aposyn-
thesis of topic continuity and intrasenten-
tial anaphora. Computational Linguistics,
28(3):319?355.
Mick O?Donnell, Chris Mellish, Jon Oberlan-
der, and Alistair Knott. 2001. ILEX: An ar-
chitecture for a dynamic hypertext genera-
tion system. Natural Language Engineering,
7(3):225?250.
Rebecca J. Passoneau. 1998. Interaction of dis-
course structure with explicitness of discourse
anaphoric phrases. In Walker et al (Walker
et al, 1998b), pages 327?358.
Massimo Poesio, Rosemary Stevenson, Barbara
Di Eugenio, and Janet Hitzeman. 2004. Cen-
tering: a parametric theory and its instantia-
tions. Computational Linguistics, 30(3).
Ehud Reiter and Robert Dale. 2000. Building
Natural Language Generation Systems. Cam-
bridge.
Michael Strube and Udo Hahn. 1999. Func-
tional centering: Grounding referential coher-
ence in information structure. Computational
Linguistics, 25(3):309?344.
Marilyn A. Walker, Aravind K. Joshi, and
Ellen F. Prince. 1998a. Centering in nat-
urally occuring discourse: An overview. In
Walker et al (Walker et al, 1998b), pages
1?30.
Marilyn A. Walker, Aravind K. Joshi, and
Ellen F. Prince, editors. 1998b. Centering
Theory in Discourse. Clarendon Press, Ox-
ford.
Proceedings of the ACL-08: HLT Demo Session (Companion Volume), pages 9?12,
Columbus, June 2008. c?2008 Association for Computational Linguistics
BART: A Modular Toolkit for Coreference Resolution
Yannick Versley
University of Tu?bingen
versley@sfs.uni-tuebingen.de
Simone Paolo Ponzetto
EML Research gGmbH
ponzetto@eml-research.de
Massimo Poesio
University of Essex
poesio@essex.ac.uk
Vladimir Eidelman
Columbia University
vae2101@columbia.edu
Alan Jern
UCLA
ajern@ucla.edu
Jason Smith
Johns Hopkins University
jsmith@jhu.edu
Xiaofeng Yang
Inst. for Infocomm Research
xiaofengy@i2r.a-star.edu.sg
Alessandro Moschitti
University of Trento
moschitti@dit.unitn.it
Abstract
Developing a full coreference system able
to run all the way from raw text to seman-
tic interpretation is a considerable engineer-
ing effort, yet there is very limited avail-
ability of off-the shelf tools for researchers
whose interests are not in coreference, or for
researchers who want to concentrate on a
specific aspect of the problem. We present
BART, a highly modular toolkit for de-
veloping coreference applications. In the
Johns Hopkins workshop on using lexical
and encyclopedic knowledge for entity dis-
ambiguation, the toolkit was used to ex-
tend a reimplementation of the Soon et al
(2001) proposal with a variety of additional
syntactic and knowledge-based features, and
experiment with alternative resolution pro-
cesses, preprocessing tools, and classifiers.
1 Introduction
Coreference resolution refers to the task of identify-
ing noun phrases that refer to the same extralinguis-
tic entity in a text. Using coreference information
has been shown to be beneficial in a number of other
tasks, including information extraction (McCarthy
and Lehnert, 1995), question answering (Morton,
2000) and summarization (Steinberger et al, 2007).
Developing a full coreference system, however, is
a considerable engineering effort, which is why a
large body of research concerned with feature en-
gineering or learning methods (e.g. Culotta et al
2007; Denis and Baldridge 2007) uses a simpler but
non-realistic setting, using pre-identified mentions,
and the use of coreference information in summa-
rization or question answering techniques is not as
widespread as it could be. We believe that the avail-
ability of a modular toolkit for coreference will sig-
nificantly lower the entrance barrier for researchers
interested in coreference resolution, as well as pro-
vide a component that can be easily integrated into
other NLP applications.
A number of systems that perform coreference
resolution are publicly available, such as GUITAR
(Steinberger et al, 2007), which handles the full
coreference task, and JAVARAP (Qiu et al, 2004),
which only resolves pronouns. However, literature
on coreference resolution, if providing a baseline,
usually uses the algorithm and feature set of Soon
et al (2001) for this purpose.
Using the built-in maximum entropy learner
with feature combination, BART reaches 65.8%
F-measure on MUC6 and 62.9% F-measure on
MUC7 using Soon et al?s features, outperforming
JAVARAP on pronoun resolution, as well as the
Soon et al reimplementation of Uryupina (2006).
Using a specialized tagger for ACE mentions and
an extended feature set including syntactic features
(e.g. using tree kernels to represent the syntactic
relation between anaphor and antecedent, cf. Yang
et al 2006), as well as features based on knowledge
extracted from Wikipedia (cf. Ponzetto and Smith, in
preparation), BART reaches state-of-the-art results
on ACE-2. Table 1 compares our results, obtained
using this extended feature set, with results from
Ng (2007). Pronoun resolution using the extended
feature set gives 73.4% recall, coming near special-
ized pronoun resolution systems such as (Denis and
Baldridge, 2007).
9
Figure 1: Results analysis in MMAX2
2 System Architecture
The BART toolkit has been developed as a tool to
explore the integration of knowledge-rich features
into a coreference system at the Johns Hopkins Sum-
mer Workshop 2007. It is based on code and ideas
from the system of Ponzetto and Strube (2006), but
also includes some ideas from GUITAR (Steinberger
et al, 2007) and other coreference systems (Versley,
2006; Yang et al, 2006). 1
The goal of bringing together state-of-the-art ap-
proaches to different aspects of coreference res-
olution, including specialized preprocessing and
syntax-based features has led to a design that is very
modular. This design provides effective separation
of concerns across several several tasks/roles, in-
cluding engineering new features that exploit dif-
ferent sources of knowledge, designing improved or
specialized preprocessing methods, and improving
the way that coreference resolution is mapped to a
machine learning problem.
Preprocessing To store results of preprocessing
components, BART uses the standoff format of the
MMAX2 annotation tool (Mu?ller and Strube, 2006)
with MiniDiscourse, a library that efficiently imple-
ments a subset of MMAX2?s functions. Using a
generic format for standoff annotation allows the use
of the coreference resolution as part of a larger sys-
tem, but also performing qualitative error analysis
using integrated MMAX2 functionality (annotation
1An open source version of BART is available from
http://www.sfs.uni-tuebingen.de/?versley/BART/.
diff, visual display).
Preprocessing consists in marking up noun
chunks and named entities, as well as additional in-
formation such as part-of-speech tags and merging
these information into markables that are the start-
ing point for the mentions used by the coreference
resolution proper.
Starting out with a chunking pipeline, which
uses a classical combination of tagger and chun-
ker, with the Stanford POS tagger (Toutanova et al,
2003), the YamCha chunker (Kudoh and Mat-
sumoto, 2000) and the Stanford Named Entity Rec-
ognizer (Finkel et al, 2005), the desire to use richer
syntactic representations led to the development of
a parsing pipeline, which uses Charniak and John-
son?s reranking parser (Charniak and Johnson, 2005)
to assign POS tags and uses base NPs as chunk
equivalents, while also providing syntactic trees that
can be used by feature extractors. BART also sup-
ports using the Berkeley parser (Petrov et al, 2006),
yielding an easy-to-use Java-only solution.
To provide a better starting point for mention de-
tection on the ACE corpora, the Carafe pipeline
uses an ACE mention tagger provided by MITRE
(Wellner and Vilain, 2006). A specialized merger
then discards any base NP that was not detected to
be an ACE mention.
To perform coreference resolution proper, the
mention-building module uses the markables cre-
ated by the pipeline to create mention objects, which
provide an interface more appropriate for corefer-
ence resolution than the MiniDiscourse markables.
These objects are grouped into equivalence classes
by the resolution process and a coreference layer is
written into the document, which can be used for de-
tailed error analysis.
Feature Extraction BART?s default resolver goes
through all mentions and looks for possible an-
tecedents in previous mentions as described by Soon
et al (2001). Each pair of anaphor and candi-
date is represented as a PairInstance object,
which is enriched with classification features by fea-
ture extractors, and then handed over to a machine
learning-based classifier that decides, given the fea-
tures, whether anaphor and candidate are corefer-
ent or not. Feature extractors are realized as sepa-
rate classes, allowing for their independent develop-
10
Figure 2: Example system configuration
ment. The set of feature extractors that the system
uses is set in an XML description file, which allows
for straightforward prototyping and experimentation
with different feature sets.
Learning BART provides a generic abstraction
layer that maps application-internal representations
to a suitable format for several machine learning
toolkits: One module exposes the functionality of
the the WEKA machine learning toolkit (Witten
and Frank, 2005), while others interface to special-
ized state-of-the art learners. SVMLight (Joachims,
1999), in the SVMLight/TK (Moschitti, 2006) vari-
ant, allows to use tree-valued features. SVM Classi-
fication uses a Java Native Interface-based wrapper
replacing SVMLight/TK?s svm classify pro-
gram to improve the classification speed. Also in-
cluded is a Maximum entropy classifier that is
based upon Robert Dodier?s translation of Liu and
Nocedal?s (1989) L-BFGS optimization code, with
a function for programmatic feature combination.2
Training/Testing The training and testing phases
slightly differ from each other. In the training phase,
the pairs that are to be used as training examples
have to be selected in a process of sample selection,
whereas in the testing phase, it has to be decided
which pairs are to be given to the decision function
and how to group mentions into equivalence rela-
tions given the classifier decisions.
This functionality is factored out into the en-
2see http://riso.sourceforge.net
coder/decoder component, which is separate from
feature extraction and machine learning itself. It
is possible to completely change the basic behav-
ior of the coreference system by providing new
encoders/decoders, and still rely on the surround-
ing infrastructure for feature extraction and machine
learning components.
3 Using BART
Although BART is primarily meant as a platform for
experimentation, it can be used simply as a corefer-
ence resolver, with a performance close to state of
the art. It is possible to import raw text, perform
preprocessing and coreference resolution, and either
work on the MMAX2-format files, or export the re-
sults to arbitrary inline XML formats using XSL
stylesheets.
Adapting BART to a new coreferentially anno-
tated corpus (which may have different rules for
mention extraction ? witness the differences be-
tween the annotation guidelines of MUC and ACE
corpora) usually involves fine-tuning of mention cre-
ation (using pipeline and MentionFactory settings),
as well as the selection and fine-tuning of classi-
fier and features. While it is possible to make rad-
ical changes in the preprocessing by re-engineering
complete pipeline components, it is usually possi-
ble to achieve the bulk of the task by simply mix-
ing and matching existing components for prepro-
cessing and feature extraction, which is possible by
modifying only configuration settings and an XML-
11
BNews NPaper NWire
Recl Prec F Recl Prec F Recl Prec F
basic feature set 0.594 0.522 0.556 0.663 0.526 0.586 0.608 0.474 0.533
extended feature set 0.607 0.654 0.630 0.641 0.677 0.658 0.604 0.652 0.627
Ng 2007? 0.561 0.763 0.647 0.544 0.797 0.646 0.535 0.775 0.633
?: ?expanded feature set? in Ng 2007; Ng trains on the entire ACE training corpus.
Table 1: Performance on ACE-2 corpora, basic vs. extended feature set
based description of the feature set and learner(s)
used.
Several research groups focusing on coreference
resolution, including two not involved in the ini-
tial creation of BART, are using it as a platform
for research including the use of new information
sources (which can be easily incorporated into the
coreference resolution process as features), different
resolution algorithms that aim at enhancing global
coherence of coreference chains, and also adapting
BART to different corpora. Through the availability
of BART as open source, as well as its modularity
and adaptability, we hope to create a larger com-
munity that allows both to push the state of the art
further and to make these improvements available to
users of coreference resolution.
Acknowledgements We thank the CLSP at Johns
Hopkins, NSF and the Department of Defense for
ensuring funding for the workshop and to EML
Research, MITRE, the Center for Excellence in
HLT, and FBK-IRST, that provided partial support.
Yannick Versley was supported by the Deutsche
Forschungsgesellschaft as part of SFB 441 ?Lin-
guistic Data Structures?; Simone Paolo Ponzetto has
been supported by the Klaus Tschira Foundation
(grant 09.003.2004).
References
Charniak, E. and Johnson, M. (2005). Coarse-to-fine n-best
parsing and maxent discriminative reranking. In Proc. ACL
2005.
Culotta, A., Wick, M., and McCallum, A. (2007). First-order
probabilistic models for coreference resolution. In Proc.
HLT/NAACL 2007.
Denis, P. and Baldridge, J. (2007). A ranking approach to pro-
noun resolution. In Proc. IJCAI 2007.
Finkel, J. R., Grenager, T., and Manning, C. (2005). Incorpo-
rating non-local information into information extraction sys-
tems by Gibbs sampling. In Proc. ACL 2005, pages 363?370.
Joachims, T. (1999). Making large-scale SVM learning prac-
tical. In Scho?lkopf, B., Burges, C., and Smola, A., editors,
Advances in Kernel Methods - Support Vector Learning.
Kudoh, T. and Matsumoto, Y. (2000). Use of Support Vector
Machines for chunk identification. In Proc. CoNLL 2000.
Liu, D. C. and Nocedal, J. (1989). On the limited memory
method for large scale optimization. Mathematical Program-
ming B, 45(3):503?528.
McCarthy, J. F. and Lehnert, W. G. (1995). Using decision trees
for coreference resolution. In Proc. IJCAI 1995.
Morton, T. S. (2000). Coreference for NLP applications. In
Proc. ACL 2000.
Moschitti, A. (2006). Making tree kernels practical for natural
language learning. In Proc. EACL 2006.
Mu?ller, C. and Strube, M. (2006). Multi-level annotation of
linguistic data with MMAX2. In Braun, S., Kohn, K., and
Mukherjee, J., editors, Corpus Technology and Language
Pedagogy: New Resources, New Tools, New Methods. Peter
Lang, Frankfurt a.M., Germany.
Ng, V. (2007). Shallow semantics for coreference resolution. In
Proc. IJCAI 2007.
Petrov, S., Barett, L., Thibaux, R., and Klein, D. (2006). Learn-
ing accurate, compact, and interpretable tree annotation. In
COLING-ACL 2006.
Ponzetto, S. P. and Strube, M. (2006). Exploiting semantic role
labeling, WordNet and Wikipedia for coreference resolution.
In Proc. HLT/NAACL 2006.
Qiu, L., Kan, M.-Y., and Chua, T.-S. (2004). A public reference
implementation of the RAP anaphora resolution algorithm.
In Proc. LREC 2004.
Soon, W. M., Ng, H. T., and Lim, D. C. Y. (2001). A machine
learning approach to coreference resolution of noun phrases.
Computational Linguistics, 27(4):521?544.
Steinberger, J., Poesio, M., Kabadjov, M., and Jezek, K. (2007).
Two uses of anaphora resolution in summarization. Informa-
tion Processing and Management, 43:1663?1680. Special
issue on Summarization.
Toutanova, K., Klein, D., Manning, C. D., and Singer, Y.
(2003). Feature-rich part-of-speech tagging with a cyclic de-
pendency network. In Proc. NAACL 2003, pages 252?259.
Uryupina, O. (2006). Coreference resolution with and without
linguistic knowledge. In Proc. LREC 2006.
Versley, Y. (2006). A constraint-based approach to noun phrase
coreference resolution in German newspaper text. In Kon-
ferenz zur Verarbeitung Natu?rlicher Sprache (KONVENS
2006).
Wellner, B. and Vilain, M. (2006). Leveraging machine read-
able dictionaries in discriminative sequence models. In Proc.
LREC 2006.
Witten, I. and Frank, E. (2005). Data Mining: Practical ma-
chine learning tools and techniques. Morgan Kaufmann.
Yang, X., Su, J., and Tan, C. L. (2006). Kernel-based pronoun
resolution with structured syntactic knowledge. In Proc.
CoLing/ACL-2006.
12
Discourse Annotation and Semantic Annotation in the GNOME Corpus
Massimo Poesio
University of Essex,
Department of Computer Science and Centre for Cognitive Science,
United Kingdom
Abstract
The GNOME corpus was created to study the dis-
course and semantic properties of discourse entities
that affect their realization and interpretation, and
particularly salience. We discuss what information
was annotated and the methods we followed.
1 Introduction
The GNOME corpus was created to study the as-
pects of discourse that appear to affect generation,
especially salience (Pearson et al, 2000; Poesio and
Di Eugenio, 2001; Poesio and Nissim, 2001; Poesio
et al, 2004b). Particular attention was paid to the
factors affecting the generation of pronouns (Pear-
son et al, 2000; Henschel et al, 2000), demon-
stratives (Poesio and Nygren-Modjeska, To appear)
possessives (Poesio and Nissim, 2001) and definites
in general (Poesio, 2004a). These results, and the
annotated corpus, were used in the development of
both symbolic and statistical natural language gen-
eration algorithms for sentence planning (Poesio,
2000a; Henschel et al, 2000; Cheng et al, 2001),
aggregation (Cheng, 2001) and text planning (Kara-
manis, 2003). The empirical side of the project in-
volved both psychological experiments and corpus
annotation, based on a scheme based on the MATE
proposals, as well as on a detailed annotation man-
ual (Poesio, 2000b), the reliability of whose instruc-
tions was tested by extensive experiments (Poe-
sio, 2000a). More recently, the corpus has also
been used to develop and evaluate anaphora resolu-
tion systems, with a special focus on the resolution
of bridging references (Poesio, 2003; Poesio and
Alexandrov-Kabadjov, 2004; Poesio et al, 2004a)
Although the results of the studies using the
GNOME corpus mentioned above have been pub-
lished in a number of papers, and although a de-
tailed annotation manual was written and has been
available on the Web for a few years (Poesio,
2000b), none of the previously published papers dis-
cusses in detail the goals of the annotation and the
methodology that was followed, especially for the
non-anaphoric aspects. In this paper we discuss the
methods used to identify possible ?utterances,? the
properties of NPs and discourse entities that were
annotated, and (very briefly) anaphoric information.
.
2 The Data
Texts from three domains were (partially) anno-
tated. The museum subcorpus consists of descrip-
tions of museum objects and brief texts about the
artists that produced them.1 The pharmaceutical
subcorpus is a selection of leaflets providing the
patients with legally mandatory information about
their medicine.2 The GNOME corpus also includes
tutorial dialogues from the Sherlock corpus col-
lected at the University of Pittsburgh. Each sub-
corpus contains about 6,000 NPs, but not all types
of annotation have been completed for all domains.
All sentences, units and NPs have been identified,
and all ?syntactic? properties of NPs (agreement fea-
ture and grammatical function). Anaphoric rela-
tions have been annotated in about half of the texts
in each domain; and the more complex semantic
properties (taxonomic properties, genericity, etc.) in
about 25% of these texts. The total size of the anno-
tated corpus is about 60K.
3 Identifying Utterances
In order to use a corpus to study salience, it is es-
sential to find a way to annotate what in Center-
1The museum subcorpus extends the corpus collected to
support the ILEX and SOLE projects at the University of Ed-
inburgh (Oberlander et al, 1998).
2The leaflets in the pharmaceutical subcorpus are a subset
of the collection of all patient leaflets in the UK which was
digitized to support the ICONOCLAST project at the University
of Brighton (Scott et al, 1998).
ing theory (Grosz et al, 1995) are called UTTER-
ANCES, i.e., the units of text after which the local
focus is updated. In most annotations concerned
with salience, a predefined notion of utterance was
adopted, typically sentences (Miltsakaki, 2002) or
(finite) clauses (Kameyama, 1998). This approach,
however, precludes using the corpus to compare
possible definitions of utterance, one of the goals
of the GNOME annotation (Poesio et al, 2004b).
In order to do this, we marked all spans of text
that might be claimed to update the local focus, in-
cluding sentences (defined as all units of text ending
with a full stop, a question mark, or an exclama-
tion point) as well as what we called (DISCOURSE)
UNITS. Units include clauses (defined as sequences
of text containing a verbal complex, all its oblig-
atory arguments, and all postverbal adjuncts) as
well as other sentence subconstituents that might
be viewed as independently updating the local fo-
cus, such as parentheticals, preposed PPs, and (the
second element of) coordinated VPs. Examples of
clauses, verbal and non-verbal parentheticals, and
preposed PPs marked as units follow; the parenthe-
ses indicate unit boundaries. (Sentence boundaries
are not indicated.)
(1) a. clausal unit with non-verbal parentheti-
cal: (It?s made in the shape of a real object
(? a violin))
b. clausal unit with preposed PP and em-
bedded relative clauses: ((With the de-
velopment of heraldry in the later Middle
Ages in Europe as a means of identifica-
tion), all (who were entitled (to bear arms))
wore signet-rings (engraved with their ar-
morial bearings))
As example (1b) above illustrates, subordinate units
such as clausal complements and relative clauses
were enclosed within the superordinate unit. Sub-
ordinate units also include adjunct clauses headed
by connectives such as before, after, because and
clauses in subject position. In total, the texts used
for the main study contain 505 sentences and more
than 1,000 units, including 900 finite clauses.3
Sentence and Unit Attributes Sentences have
one attribute, STYPE, specifying whether the sen-
tence is declarative, interrogative, imperative, or ex-
clamative. The attributes of units include:
? UTYPE: whether the unit is a main clause,
a relative clause, appositive, a parenthet-
3Our instructions for marking up such elements benefited
from the discussion of clauses in (Quirk and Greenbaum, 1973)
and Marcu?s proposals for discourse units annotation (1999).
ical, etc. The possible values for this
attribute are main, relative, such-as,
appositive, parenthetical,
paren-rel, paren-app, paren-
main, subject, complement, adjunct,
coord-vp,preposed-pp, listitem,
cleft, title, disc-marker.
? VERBED: whether the unit contains a verb.
? FINITE: for verbed units, whether the verb is
finite or not.
? SUBJECT: for verbed units, whether they have
a full subject, an empty subject (expletive, as in
there sentences), or no subject (e.g., for infini-
tival clauses).
Annotation Issues Marking up sentences proved
to be quite easy; marking up units, on the other
hand, required extensive annotator training. The
agreement on identifying the boundaries of units,
using the ? statistic discussed in (Carletta, 1996),
was ? = .9 (for two annotators and 500 units); the
agreement on features (2 annotators and at least 200
units) was as follows: UTYPE: ?=.76; VERBED:
?=.9; FINITE: ?=.81. The main problems
when marking units were to identify complements,
to distinguish clausal adjuncts from prepositional
phrases, and how to mark up coordinated units. The
main problem with complements was to distinguish
non-finite complements of verbs such as want from
the non-finite part of verbal complexes containing
modal auxiliaries such as get, let, make, and have:
(2) a. (I would like (to be able to travel))
b. (I let him do his homework)
One problem that proved fairly difficult to han-
dle (and which, in fact, we didn?t entirely solve)
was clausal coordination. The problem was to pre-
serve enough structure to be able to compute the
previous utterance, while preserving some basic in-
tuitions about what constitutes a clause (roughly,
that by and large clauses were text spans marked
either by the presence of a semantically isolated
verb or by punctuation / layout) which are essen-
tial for annotators and are needed to specify the val-
ues of attributes. This was relatively easy to do
when two main clauses were coordinated; coordi-
nated main clauses were marked as in (3a). How-
ever, it wasn?t completely obvious what to do in the
case of coordination within a subordinate clause, as
in (3b). Because there weren?t many such cases,
rather than using the ?unit? element with a spe-
cial value for UTYPE as we did for coordinated NPs
(which meant specifying all sorts of special val-
ues for attributes) we used a markup element called
?unit-coordination? to maintain the struc-
ture, and then marked up each clause separately,
as shown in (3c) (the ?unit-coordination? is
marked with square brackets).
(3) a. (The Getty museum?s microscope still
works,) (and the case is fitted with a
drawer filled with the necessary attach-
ments).
b. (If you have any questions or are not sure
about anything, ask your doctor or your
pharmacist)
c. ((If [(you have any questions) or (you are
not sure about anything)]), ask your doctor
or your pharmacist)
The elements of text not marked up as units in-
clude: NPs, post-verbal and post-nominal PPs, non-
verbal NP modifiers, coordinated VPs in case the
second conjunct did not have arguments (4a), and
quoted parts of text, when not reported speech (4b).
(4) a. (The oestradiol and norethisterone acetate
are plant derived and synthetically pro-
duced)
b. (The inscription ?CHNETOC
BASHLHKOC CPATHARHC?)
Layout Our genres raised a few issues that, as far
as we know, have not been previously discussed in
the Centering literature. One such problem is what
to do with layout elements such as titles and list ele-
ments, which can clearly serve as the first introduc-
tion of a CF and to move the CB. One example of
title unit is unit (u1) in (5).
(5) (u1) Side effects
Side effects may occur when PRODUCT-
Y is applied to large parts of the body,
We marked these layout elements as units, as in (6),
but using the special value title of the attribute
UTYPE (see above) so that we could test whether it
was better to treat them as utterances or not.
(6) <unit id="u1" utype="title">Side
effects</unit>
<p><s stype="decl"><unit> Side
effects may occur <unit>when PRODUCT-
Y is applied to large parts of the body, ...
</unit> ... </unit> ... </s> ... </p>
Problems with Attributes The most difficult at-
tribute to mark was UTYPE, and our main problem
was to distinguish between relative clauses and par-
entheticals, since it?s not always easy to tell whether
a relative clause is restrictive or non-restrictive (see
also (Cheng et al, 2001)). In the end, we adopted
rules purely based on surface form (the presence or
absence of a comma or other bracketing device).
(See also (Quirk and Greenbaum, 1973).)
Utterances and Propositions The annotation of
units has been shown useful to identify many of the
atomic propositions expressed by a text, and was
therefore used as a basis for studying text planning
(Karamanis, 2003) and aggregation (Cheng, 2001).
4 Properties of Discourse Entities and
their Realization
The main goal of the GNOME annotation was to
study the factors that affect the realization of dis-
course entities, focusing on those entities realized as
NPs. Hence, our main concern was to identify and
to annotate the relevant properties both of discourse
entities themselves and their realizations in a partic-
ular utterance (which we will call FORWARD LOOK-
ING CENTERS, or CFs, following Centering?s termi-
nology). Both types of properties were annotated as
properties of the ?ne? element, used to mark up NPs
in the corpus. Overall, we annotated 14 attributes of
?ne? elements, specifying the syntactic and seman-
tic properties of NPs and the semantic properties of
the discourse entities they realize. We discuss these
attributes in this section. We also annotated seman-
tic relations between discourse entities, particularly
when they express anaphoric relations. Anaphoric
annotation is discussed in the next section.
4.1 Marking up NEs
The ?ne? element is used to mark NPs, as in the
following example (the attributes will be discussed
below):
(7) <unit finite=?finite-yes? id=?u3? utype=?main?
verbed=?verbed-yes?>
<ne id="ne2" cat="poss-np" per="per3" num="sing"
gen="neut" gf="subj" lftype="term"
onto="concrete" ani="inanimate"
deix="deix-no" count="undersp-count"
structure="undersp-structure"
generic="generic-no" loeb="sem-function">
<ne id="ne3" cat="this-np" per="per3" num="sing"
gen="neut" gf="gen" lftype="term"
onto="concrete" ani="inanimate"
deix="deix-yes" count="count-yes"
structure="atom"
generic="generic-no" loeb="pragm-function">
This table?s
</ne>
</ne>
allow
<ne id="ne4" cat="bare-np" per="per3" num="plur"
gen="neut" gf="obj" lftype="term" onto="person"
ani="animate" deix="deix-no" count="count-yes"
structure="set" generic="generic-yes" loeb="sort">
scholars </ne>
<unit finite=?finite-no? id=?u4? utype=?complement?
verbed=?verbed-yes?>
to link
<ne id="ne5" cat="pers-pro" per="per3" num="sing"
gen="neut" gf="obj" lftype="term" onto="concrete"
ani="inanimate" deix="deix-yes" count="count-yes"
structure="atom" generic="generic-no"
loeb="disc-function"> it </ne>
...
The GNOME instructions for identifying NPs derive
from those proposed in MATE (Poesio et al, 1999),
in turn derived from DRAMA (Passonneau, 1997)
and MUC-7 (Hirschman, 1998). An important dif-
ference between the instructions used for GNOME
and those developed for MATE is that instead of at-
tempting to get the annotators to recognize the NP
that realize discourse entities and only mark those,
in GNOME all NPs were marked with ?ne? elements;
the separate LF TYPE attribute was used to distin-
guish between NPs with different types of denota-
tions (see below). This change made the process of
identifying nominal entities easier and potentially
automatic (even though the identification of mark-
ables was still done by hand).
As in the case of units, the main problem with
marking up NPs was coordination. Our approach
was to use a separate ?ne? element to mark up the
coordinated NP, with type (CAT) value coord-np.
We only used a coord-np element if two deter-
miners were present, as in ((your doctor) and (your
pharmacist)). This approach was chosen because it
limited the number of spurious coordinations intro-
duced (in cases such as this is an interesting and
well-known example of early Byzantine jewellery),
but has the limitation that only one ?ne? is marked
in cases such as Your doctor or pharmacist.
4.2 Properties of all NPs
Some of the attributes of ?ne? elements specify
properties of all NPs, whether or not they realize a
discourse entity. We discuss these first.
CAT The CAT attribute is used to mark NP type:
whether the NP is a pronoun, a definite description,
etc.. This attribute is only meant to provide a
very surface-y classification, without attempting to
group NPs in larger classes such as ?definite NP? and
?indefinite NP?. The one attempt to go beyond pure
surface was the introduction of a distinction be-
tween definite descriptions that are really disguised
proper names such as the Beatles, classified as
CAT=the-pn, and all other definite descriptions,
classified as the-np. The complete list of
values for CAT is: a-np, another-np, q-np,
num-np, meas-np, that-np, this-np,
such-np, wh-np, poss-np, bare-np, pn,
the-pn, the-np, pers-pro, poss-pro,
refl-pro, rec-pro, q-pro, wh-pro,
this-pro, that-pro, num-ana (for ?nu-
merical anaphors? such as one in I want one),
null-ana, gerund (for nominalized present
participles such as veneering furniture in the
practice of veneering furniture), coord-np, and
free-rel (for ?free relatives? such as what you
need most in what you need most is a good rest)).
The agreement on this attribute was pretty high,
? = .9; the one problem was the distinction be-
tween the-pn and the-np.
Agreement features: NUM, PER, and GEN These
atributes are used to annotate features that are im-
portant to study pronoun interpretation: gender,
number and person of NPs. Person and number were
generally easy to annotate, but gender was very dif-
ficult because of the presence of many references to
individual of unspecified gender, such as the maker
in the inventory gives neither the name of the maker
nor the location. This problem was solved by intro-
ducing a special undersp-gen value; indeed, un-
derspecified values were provided for all attributes.
The agreement values for these features were: GEN:
? = .89; NUM: ? = .84; PER: ? = .9.
GF This attribute was used to annotate the gram-
matical function of the NP, a property generally
taken to play an important role in determining
the salience of the discourse entity it realizes
(Grosz et al, 1995). Our instructions for this
attribute are derived from those used in the
FRAMENET project ((Baker et al, 1998); see also
http://www.icsi.berkeley.edu/?framenet/).
The values are subj, obj, predicate (used
for post-verbal objects in copular sentences, such
as This is (a production watch)), there-obj
(for post-verbal objects in there-sentences), comp
(for indirect objects), adjunct (for the argument
of PPs modifying VPs), gen (for NPs in deter-
miner position in possessive NPs), np-compl,
np-part, np-mod, adj-mod, and no-gf (for
NPs occurring by themselves - eg., in titles). The
agreement values for GF is ? = .85.
LF TYPE Not all NPs realize discourse entities:
some of them realize quantifiers (e.g., each coffer
in Each coffer has a lid) or predicates (e.g., NPs in
appositive position, such as the oldest son of Louis
XIV in The 1689 inventory of the Grand Dauphin,
the oldest son of Louis XIV, lists a jewel coffer of
similar form and decoration. As said above, in the
GNOME annotation all NPs are treated as markables,
but the LF TYPE attribute is used to indicate the
type of semantic object denoted by an NP: term,
quant or pred. Quantifiers were identified purely
on the basis of the value of the CAT value: all NPs
with CAT=q-np or q-pro should get a value of
quant. A more complex test was used to identify
predicative NPs: three linguistic contexts in which
NP are typically predicative were considered (appo-
sitions, postcopular position in there-sentences, and
become-style sentences) but the annotators were ex-
plicitly asked to check whether the NP was used to
express a property. Agreement was more tentative:
? = .73 (for two annotators, 200 NPs).
Taxonomic information Two semantic attributes
capture information about the type of objects re-
ferred to (or quantifier over) by an NP. The first
attribute, ONTO, was originally introduced to distin-
guish between gerunds (event nominalizations such
as letter-writing) and bare plurals referring to con-
crete objects like scholars, both of which semanti-
cally denote collective objects (Link, 1983; Portner,
1992). Further distinctions were introduced to deal
with ?difficult? objects, such as diseases; particular
types of concrete objects such as medicines and per-
sons were also singled out. Distinctions captured
by the current set of values of ONTO include per-
sons, medicines, other substances, other concrete
objects; events, time intervals, or other abstract enti-
ties; spatial locations; and diseases. The agreement
value for the latest version of ONTO was ? = .8
between two annotators, 200 NPs.
The second ?taxonomic? attribute, ANI, is used
to annotate whether the objects referred to or quan-
tifier over by an NP are animate or inanimate. This
annotation was motivated by a number of studies
suggesting that animacy plays an important role in
salience (Prat-Sala and Branigan, 2000) and our
own experiments suggesting that animacy is much
more important than grammatical function, the-
matic roles, or order of mention in determining
which entities are most likely to be pronominal-
ized (Pearson et al, 2001). We also found that
the discrepancy between the results of Gordon et
al. (1999) and the findings of (Walker and Prince,
1996) can be explained in terms of animacy (Poesio
and Nissim, 2001). Animacy was by far the easiest
semantic attribute for our annotators: ? = .92.
4.3 Semantic properties of Discourse Entities
Semantic properties that may play a role in realiza-
tion but only apply to discourse entities include:4
Structure Two attributes are used to indicate
whether the discourse entity realized by an NP refers
to a mass of certain substance or to countable ob-
jects (attribute COUNT) and, in case of countable
objects, to an atom or a set (attribute STRUCTURE).
These attributes were marked in order to study the
4These attributes were only marked for about 25% of the
corpus.
factors leading to the realization of a discourse en-
tity as a bare NP, in combination with the annotation
of genericity discussed below: the reasoning being
that it should only be possible to use bare singu-
lars to realize a discourse entity described with mass
nouns (as in the ebeniste and his wife lived modestly
in a five-room apartment . . . with simple furniture).5
The main reason for keeping the two at-
tributes separate was that reaching agreement on
STRUCTURE was fairly easy (? = .82 at the second
attempt) whereas COUNT was one of the most dif-
ficult attributes to mark?it took several iterations of
changes to the instructions to achieve a ? = .78, and
substantial revisions would probably still be useful.
Nevertheless, given currently accepted views deriv-
ing from Link?s work (1983), it would make more
sense to merge the two attributes.
GENERIC This attribute is used to indicate
whether the NP should be interpreted generically or
not, which was thought to affect at least two types
of discourse entity realizations: gerunds, that we
took to be event types, and bare NPs, both singular
and referring to substances (e.g., ivory) and plural.
Annotating this information proved to be very diffi-
cult, which was not surprising because genericity is
not yet a completely understood phenomenon. One
complication is that there are two types of ?generic
NPs?: NPs referring to kinds, such as The dodo in
The dodo is extinct (being extinct is not a property
that can be predicated of individual dodos), and NPs
used in generic statements, such as Italians are good
skiers (a property of individual Italians) (Carlson
and Pelletier, 1995). Although some NPs can only
be used to express one or the other interpretation
(e.g., * A dodo is extinct), many can be used in both
ways (Dodos are extinct).
We started trying to make the very basic distinc-
tion between tokens and types one finds, e.g., in
(Lyons, 1977), but even after numerous refinements
we still encountered many problems. One of the
problems our annotators had was whether to treat
references to substances such as ivory and horn in
examples like This table?s marquetry of ivory and
horn ?existentially,? i.e., as referring to the partic-
ular amounts of those substances used in the ta-
ble, or ?generically?, to refer to the kinds. In the
end we decided to follow Carlson (1977) and to
mark all of these examples as references to kinds,
i.e., as generic. A second problem were quantifiers.
Our annotators found it very hard to distinguish
5Apart from the cases in which bare singulars are used to re-
fer to substances, such as the interiors of this pair of coffers are
lined with tortoiseshell and brass, the few discussed exceptions
to this rule are expressions like home in I went home.
between quantified NPs used (non-generically) to
quantify over a specific set of individuals at a partic-
ular spatio-temporal location, as in Many lecturers
went on strike (on March 16th, 2004), and quanti-
fiers used in generic sentences, as in Many lectur-
ers went (habitually) on strike (during those years).
The last version of the instructions (not yet added to
the overall annotation manual) asked annotators to
try to identify generic sentences before attempting
to determine the value of the GENERIC attribute.
With these instructions, we finally reached a reason-
able agreement (? = .82).
LOEB Poesio and Vieira (1998) found that of the
1,400 definite descriptions in their corpus, only
about 50% were subsequent mention or bridging
references, whereas 50% were first mentions. Of
the first mentions, about half (i.e., 25% of the to-
tal) were what Hawkins (1978) would call ?larger
situation? definites, i.e., definite descriptions like
the pope whose referent is supposed to be part of
shared knowledge; whereas the other half includes
what Loebner (1987) calls SEMANTICALLY FUNC-
TIONAL definites, like the first man on the Moon.
Loebner claimed that the paradigmatic case of def-
initeness are not anaphoric NPs, as suggested by
familiarity theories such as Heim?s (1982), but se-
mantically functional ones such as the first person
ever to row across the Pacific on his own. In or-
der to test Loebner?s theory and compare it with one
based on familiarity, we annotated the NPs referring
to discourse entities according to whether they were
functional, relational, or sortal (Poesio, 2004a). We
achieved good reliability on this attribute (? = .82),
and the results do suggest a much greater correlation
between functionality and definiteness than between
familiarity and definiteness (Poesio, 2004a).
5 Anaphora
The one aspect of the GNOME annotation that has
been extensively discussed in previous papers is
anaphoric annotation (Poesio, 2004b; Poesio et al,
2004b); we only discuss this aspect briefly here.
5.1 Annotating Discourse Models
Anaphoric annotation raises a number of difficult
and, sometimes, unresolved semantic issues (Poe-
sio, 2004b). As part of the MATE and GNOME
projects, an extensive analysis of previously exist-
ing schemes for so-called ?coreference annotation,?
such as the MUC-7 scheme, was carried out, high-
lighting a number of problems with such schemes,
ranging from issues with the annotation method-
ology to semantic issues. Proposals for annotat-
ing ?coreference? such as (Hirschman, 1998) have
been motivated by work on Information Extraction,
hence the notion of ?coreference? used is very diffi-
cult to relate to traditional ideas about anaphora (van
Deemter and Kibble, 2000). A distinctive feature
of the GNOME annotation (and the MATE propos-
als from which they derive (Poesio, 2004b)) are ex-
plicitly based on the DISCOURSE MODEL assump-
tion adopted almost universally by linguists (com-
putational and not) working on anaphora resolution
and generation (Webber, 1979; Heim, 1982; Kamp
and Reyle, 1993; Gundel et al, 1993). This is
the hypothesis that interpreting a discourse involves
building a shared discourse model containing DIS-
COURSE ENTITIES that may or may not ?refer? to
specific objects in the world, as well as the relations
between these entities. The annotation for which the
MATE scheme was developed?that we?ll call here
?anaphoric annotation,? is meant as a partial repre-
sentation of the discourse model evoked by a text.
5.2 Anaphoric Annotation in GNOME
For the GNOME corpus, we adopted a simplified ver-
sion of the MATE scheme, as for our purposes it?s
not essential to mark all semantic relations between
entities introduced by a text, but only those that may
establish a ?link? between two utterances. So, for
example, it was not necessary for us to mark a rela-
tion between the subject of a copular sentence and
its predicate - e.g., between the price of aluminum
siding and $3.85 or $4.02 in the example above.
In the GNOME corpus, anaphoric information is
marked by means of a special ?ante? element; the
?ante? element itself specifies the index of the
anaphoric expression (a ?ne? element) and the type
of semantic relation (e.g., identity), whereas one or
more embedded ?anchor? elements indicate pos-
sible antecedents.6 (See (8).)
(8) <unit finite=?finite-yes? id=?u227?>
<ne id=?ne546? gf=?subj?> The drawing of
<ne id=?ne547? gf=?np-compl?>the corner cupboard
</ne></ne>
<unit finite=?no-finite? id=?u228?>,or more probably
<ne id=?ne548? gf=?no-gf?> an engraving of
<ne id=?ne549? gf=?np-compl?>it </ne></ne>
</unit>,
...
</unit>
<ante current="ne549" rel="ident"> <anchor ID="ne547">
</ante>
Work such as (Sidner, 1979; Strube and Hahn,
1999), as well as our own preliminary analysis,
suggested that indirect realization can play a cru-
cial role in maintaining the CB. However, previ-
ous attempts at marking anaphoric information, par-
ticularly in the context of the MUC initiative, sug-
gested that while agreement on identity relations is
6The presence of more than one ?anchor? element indi-
cates that the anaphoric expression is ambiguous.
fairly easy to achieve, marking bridging references
is hard; this was confirmed by Poesio and Vieira
(1998). For these reasons, and to reduce the an-
notators? work, we did not mark all relations. Be-
sides identity (IDENT) we only marked up three
associative relations (Hawkins, 1978): set mem-
bership (ELEMENT), subset (SUBSET), and ?gen-
eralized possession? (POSS), which includes part-
of relations as well as ownership relations. We
only marked relations between objects realized by
noun phrases, excluding anaphoric references to ac-
tions, events or propositions implicitly introduced
by clauses or sentences. We also gave strict in-
structions to our annotators limiting how much to
mark.
As expected, we found a reasonable (if not
perfect) agreement on identity relations. In our
most recent analysis (two annotators looking at the
anaphoric relations between 200 NPs) we observed
no real disagreements; 79.4% of the relations were
marked up by both annotators; 12.8% by only one
of them; and in 7.7% of the cases, one of the an-
notators marked up a closer antecedent than the
other. With associative references, limiting the rela-
tions did limit the disagreements among annotators
(only 4.8% of the relations are actually marked dif-
ferently) but only 22% of bridging references were
marked in the same way by both annotators; 73.17%
of relations are marked by only one or the other
annotator. So reaching agreement on this informa-
tion involved several discussions between annota-
tors and more than one pass over the corpus.
6 Automatically computing the Local
Focus
The reader will have noticed that no attempt was
done to directly mark up properties of the local fo-
cus - e.g., which discourse entity is the CB of a par-
ticular utterance. We found that it is much easier
to annotate the ?building blocks? of a theory of the
local focus, and then use scripts to automatically
compute the CB. There are two advantages to this
approach: first of all, agreement on the ?building
blocks? is much easier to reach than agreement on
the CB?in our preliminary experiments we didn?t go
beyond ? = .6 when trying to directly identify the
CB using the definitions from (Brennan et al, 1987).
And secondly, this approach makes it possible to
compute the CB according to different ways of in-
stantiating what we call the ?parameters of Center-
ing? ?e.g., ranking.
We developed such scripts for the work dis-
cussed in (Poesio et al, 2004b); they can be
tested on the web site associated with that paper,
http://cswww.essex.ac.uk/staff/poesio/
cbc/. These scripts have been subsequently used
to compute the CB in, e.g., (Poesio and Nissim,
2001; Poesio and Nygren-Modjeska, To appear).
7 Discussions and Conclusion
Corpus consistency The main lesson learned
from this effort is that actually using a corpus is the
best way both to ensure its correctness and to learn
which types of information are most useful.
Thematic Roles One attribute on which we
weren?t able to reach acceptable agreement was the
thematic role of an NP, which has been argued to
be a better indicator of salience than grammatical
function (Sidner, 1979; Stevenson et al, 1994); the
agreement value in this case was ? = .35. Other
groups however have shown that this can be done,
e.g., in Framenet (Baker et al, 1998) and more re-
cently in PropBank (Kingsbury and Palmer, 2002).
Planned Revisions of the Scheme A number of
aspects of the annotation scheme used for the cor-
pus could be improved. An obvious improvement
would be to directly annotate predicates with their
WordNet senses instead of annotating ONTO and an-
imacy. We started doing this for the annotation of
modifiers (Cheng et al, 2001), and developed an in-
terface to WordNet, but too late to redo the whole
corpus. Of the attributes, COUNT and GENERIC
were the most difficult to annotate; further tests with
these attributes could be useful.
Automatic annotation A substantial part of the
annotation work required for GNOME now could
(and should) be done automatically, or semi-
automatically. This includes, most obviously, the
identification of sentences and NPs, already done
automatically in the VENEX corpus (Poesio, 2004b);
and at least grammatical function, animacy, and
countability could be automatically annotated in
preliminary form with existing techniques, and then
corrected by hand. We also plan to use the corpus
to bootstrap techniques for automatic identification
of uniqueness and gender.
Acknowledgments
Special thanks to Janet Hitzeman, who collected the
first subset of the museum domain for SOLE; to Re-
nate Henschel, who completed the collection of the
museum subset and wrote the first version of the an-
notation manual; to all our annotators; and to Mi-
jail Alexandrov-Kabadjov and Nikiforos Karama-
nis, who identified a number of annotation prob-
lems. Most of this work was supported by the
EPSRC project GNOME, GR/L51126/01.
References
C. F. Baker, C. J. Fillmore, and J. B Lowe. 1998. The Berkeley
FrameNet project. In Proc. 36th ACL.
S.E. Brennan, M.W. Friedman, and C.J. Pollard. 1987. A cen-
tering approach to pronouns. In Proc. of the 25th ACL.
J. Carletta. 1996. Assessing agreement on classification tasks:
the kappa statistic. Comp. Linguistics, 22(2):249?254.
G. N. Carlson and F. J. Pelletier, editors. 1995. The Generic
Book. University of Chicago Press.
G. N. Carlson. 1977. Reference to Kinds in English. Ph.D.
thesis, University of Massachusetts, Amherst.
H. Cheng, M. Poesio, R. Henschel, and C. Mellish. 2001.
Corpus-based NP modifier generation. In Proc. of the Sec-
ond NAACL, Pittsburgh.
H. Cheng. 2001. Modelling Aggregation Motivated Interac-
tions in Descr. Text Generation. Ph.D. thesis, Edinburgh.
P. C. Gordon, R. Hendrick, K. Ledoux, and C. L. Yang. 1999.
Processing of reference and the structure of language: an
analysis of complex noun phrases. Language and Cognitive
Processes, 14(4):353?379.
B. J. Grosz, A. K. Joshi, and S. Weinstein. 1995. Centering:
A framework for modeling the local coherence of discourse.
Computational Linguistics, 21(2):202?225.
J. K. Gundel, N. Hedberg, and R. Zacharski. 1993. Cognitive
status and the form of referring expressions in discourse.
Language, 69(2):274?307.
J. A. Hawkins. 1978. Definiteness and Indefiniteness. Croom
Helm, London.
I. Heim. 1982. The Semantics of Definite and Indefinite Noun
Phrases. Ph.D. thesis, Univ. of Massachusetts at Amherst.
R. Henschel, H. Cheng, and M. Poesio. 2000. Pronominaliza-
tion revisited. In Proc. of 18th COLING.
L. Hirschman. 1998. MUC-7 coreference task definition, ver-
sion 3.0. In N. Chinchor, editor, In Proc. of the 7th Message
Understanding Conference.
M. Kameyama. 1998. Intra-sentential centering. In M. A.
Walker, A. K. Joshi, and E. F. Prince, editors, Centering
Theory in Discourse, chapter 6, pages 89?112. Oxford.
H. Kamp and U. Reyle. 1993. From Discourse to Logic. D.
Reidel, Dordrecht.
N. Karamanis. 2003. Entity coherence for descriptive text
structuring. Ph.D. thesis, Edinburgh.
P. Kingsbury and M. Palmer. 2002. From Treebank to Prop-
Bank . In Proc. of LREC.
G. Link. 1983. The logical analysis of plurals and mass terms:
A lattice- theoretical approach. In R. Ba?uerle, C. Schwarze,
and A. von Stechow, editors, Meaning, Use and Interpreta-
tion of Language, pages 302?323. Walter de Gruyter.
S. Loebner. 1987. Definites. Journal of Semantics, 4:279?326.
J. Lyons. 1977. Semantics. Cambridge.
D. Marcu. 1999. Instructions for manually annotating the dis-
course structures of texts. Unpublished manuscript.
E. Miltsakaki. 2002. Towards an aposynthesis of topic conti-
nuity and intrasentential anaphora. Computational Linguis-
tics, 28(3):319?355.
J. Oberlander, M. O?Donnell, A. Knott, and C. Mellish. 1998.
Conversation in the museum. New Review of Hypermedia
and Multimedia, 4:11?32.
R. J. Passonneau. 1997. Instructions for applying discourse
reference annotation for multiple applications (DRAMA).
Unpublished manuscript., December.
J. Pearson, R. Stevenson, and M. Poesio. 2000. Pronoun reso-
lution in complex sentences. In Proc. of AMLAP, Leiden.
J. Pearson, R. Stevenson, and M Poesio. 2001. The effects of
animacy, thematic role, and surface position on the focus-
ing of entities in discourse. In M. Poesio, editor, Proc. of
SEMPRO-2001. University of Edinburgh.
M. Poesio and M. Alexandrov-Kabadjov. 2004. A general-
purpose, off the shelf anaphoric resolver. In Proc. of LREC.
M. Poesio and B. Di Eugenio. 2001. Discourse structure
and anaphoric accessibility. In Ivana Kruijff-Korbayova? and
Mark Steedman, editors, Proc. of the ESSLLI 2001 Work-
shop on Inf. Structure, Disc. Structure and Disc. Semantics.
M. Poesio and M. Nissim. 2001. Salience and possessive NPs:
the effect of animacy and pronominalization. In Proc. of
AMLAP (Poster Session).
M. Poesio and N. Nygren-Modjeska. To appear. Focus, activa-
tion, and this-noun phrases. In A. Branco, R. McEnery, and
R. Mitkov, editors, Anaphora Processing. John Benjamins.
M. Poesio and R. Vieira. 1998. A corpus-based investiga-
tion of definite description use. Computational Linguistics,
24(2):183?216, June.
M. Poesio, F. Bruneseaux, and L. Romary. 1999. The MATE
meta-scheme for coreference in dialogues in multiple lan-
guages. In M. Walker, editor, Proc. of the ACL Workshop on
Standards and Tools for Discourse Tagging, pages 65?74.
M. Poesio, R. Mehta, A. Maroudas, and J. Hitzeman. 2004a.
Learning to solve bridging references. In Proc. of the ACL.
M. Poesio, R. Stevenson, B. Di Eugenio, and J. M. Hitzeman.
2004b. Centering: A parametric theory and its instantia-
tions. Computational Linguistics, 30(3).
M. Poesio. 2000a. Annotating a corpus to develop and evaluate
discourse entity realization algorithms. In Proc. of the 2nd
LREC, pages 211?218, Athens, May.
M. Poesio, 2000b. The GNOME Annotation
Manual, Fourth Edition. Available from
http://www.hcrc.ed.ac.uk/ ? gnome.
M. Poesio. 2003. Associative descriptions and salience. In
Proc. of the EACL Workshop on Computational Treatments
of Anaphora, Budapest.
M. Poesio. 2004a. An empirical investigation of definiteness.
In S. Kepser, editor, Proc. of the International Conference
on Linguistic Evidence, Tu?bingen, January.
M. Poesio. 2004b. The MATE/GNOME scheme for anaphoric
annotation, revisited. In Proc. of SIGDIAL, Boston, May.
P. H. Portner. 1992. Situation Theory and the Semantics of
Propositional Expressions. Ph.D. thesis, University of Mas-
sachusetts at Amherst.
M. Prat-Sala and H. Branigan. 2000. Discourse constraints
on syntactic processing in language production. Journal of
Memory and Language, 42(168?182).
R. Quirk and S. Greenbaum. 1973. A University Grammar of
English. Longman.
D. Scott, R. Power, and R. Evans. 1998. Generation as a solu-
tion to its own problem. In Proc. of the 9th INLG.
C. L. Sidner. 1979. Towards a computational theory of defi-
nite anaphora comprehension in English discourse. Ph.D.
thesis, MIT.
R. J. Stevenson, R. A. Crawley, and D. Kleinman. 1994. The-
matic roles, focus, and the representation of events. Lan-
guage and Cognitive Processes, 9:519?548.
M. Strube and U. Hahn. 1999. Functional centering?
grounding referential coherence in information structure.
Computational Linguistics, 25(3):309?344.
K. van Deemter and R. Kibble. 2000. On coreferring: Coref-
erence in MUC and related annotation schemes. Computa-
tional Linguistics, 26(4):629?637. Squib.
M. A. Walker and E. Prince. 1996. A bilateral approach to
givenness. In J. Gundel and T. Fretheim, editors, Reference
Accessibility, pages 291?306. John Benjamins.
B. L. Webber. 1979. A Formal Approach to Discourse
Anaphora. Garland, New York.
DISCOURSE-NEW DETECTORS FOR DEFINITE DESCRIPTION
RESOLUTION: A SURVEY AND A PRELIMINARY PROPOSAL
Massimo Poesio,? Olga Uryupina,? Renata Vieira,?
Mijail Alexandrov-Kabadjov? and Rodrigo Goulart?
?University of Essex, Computer Science and Cognitive Science (UK)
?Universita?t des Saarlandes, Computerlinguistik (Germany)
?Unisinos, Computac?a?o Aplicada (Brazil)
Abstract
Vieira and Poesio (2000) proposed an algorithm for
definite description (DD) resolution that incorpo-
rates a number of heuristics for detecting discourse-
new descriptions. The inclusion of such detec-
tors was motivated by the observation that more
than 50% of definite descriptions (DDs) in an av-
erage corpus are discourse new (Poesio and Vieira,
1998), but whereas the inclusion of detectors for
non-anaphoric pronouns in algorithms such as Lap-
pin and Leass? (1994) leads to clear improvements
in precision, the improvements in anaphoric DD res-
olution (as opposed to classification) brought about
by the detectors were rather small. In fact, Ng and
Cardie (2002a) challenged the motivation for the
inclusion of such detectors, reporting no improve-
ments, or even worse performance. We re-examine
the literature on the topic in detail, and propose a re-
vised algorithm, taking advantage of the improved
discourse-new detection techniques developed by
Uryupina (2003).
1 Introduction
Although many theories of definiteness and many
anaphora resolution algorithms are based on the as-
sumption that definite descriptions are anaphoric,
in fact in most corpora at least half of definite de-
scriptions are DISCOURSE-NEW (Prince, 1992), as
shown by the following examples, both of which are
the first sentences of texts from the Penn Treebank.
(1) a. Toni Johnson pulls a tape measure across
the front of what was once a stately Victorian
home.
b. The Federal Communications Commission
allowed American Telephone & Telegraph
Co. to continue offering discount phone
services for large-business customers
and said it would soon re-examine its
regulation of the long-distance market.
Vieira and Poesio (2000) proposed an algorithm for
definite description resolution that incorporates a
number of heuristics for detecting discourse-new
(henceforth: DN) descriptions. But whereas the
inclusion of detectors for non-anaphoric pronouns
(e.g., It in It?s raining) in algorithms such as Lappin
and Leass? (1994) leads to clear improvements in
precision, the improvements in anaphoric DD reso-
lution (as opposed to classification) brought about
by the detectors were rather small. In fact, Ng
and Cardie (2002a) challenged the motivation for
the inclusion of such detectors, reporting no im-
provements or even worse performance. We re-
examine the literature on the topic in detail, and
propose a revised algorithm, taking advantage of
the improved DN detection techniques developed by
Uryupina (2003).
2 Detecting Discourse-New Definite
Descriptions
2.1 Vieira and Poesio
Poesio and Vieira (1998) carried out corpus stud-
ies indicating that in corpora like the Wall Street
Journal portion of the Penn Treebank (Marcus et
al., 1993), around 52% of DDs are discourse-new
(Prince, 1992), and another 15% or so are bridg-
ing references, for a total of about 66-67% first-
mention. These results led Vieira and Poesio to
propose a definite description resolution algorithm
incorporating independent heuristic strategies for
recognizing DN definite descriptions (Vieira, 1998;
Vieira and Poesio, 2000).
The heuristics proposed by Vieira and Poesio
assumed a parsed input (the Penn Treebank) and
aimed at identifying five categories of DDs licensed
to occur as first mention on semantic or pragmatic
grounds on the basis of work on definiteness includ-
ing Loebner?s account (1987):
1. So-called SEMANTICALLY FUNCTIONAL de-
scriptions (Loebner, 1987). This class included
descriptions with modifiers like first or best
that turned a possibly sortal predicate into a
function (as in the first person to cross the Pa-
cific on a row boat); as well as descriptions
with predicates like fact or belief followed by a
that-clause with the function of specifying the
fact or belief under question. Both types of
definites descriptions were recognized by con-
sulting a hand-coded list of SPECIAL PREDI-
CATES.
2. Descriptions serving as disguised PROPER
NAMES, such as The Federal Communications
Commission or the Iran-Iraq war. The heuris-
tics for recognizing these definite descriptions
were primarily based on capitalization (of the
head or the modifiers).
3. PREDICATIVE descriptions, i.e., descriptions
semantically functioning as predicates rather
than as referring. These include descriptions
occurring in appositive position (as in Glenn
Cox, the president of Phillips Petroleum) and
in certain copular constructions (as in the man
most likely to gain custody of all this is a career
politician named Dinkins). The heuristics used
to recognize these cases examined the syntac-
tic structure of the NP and the clause in which
it appeared.
4. Descriptions ESTABLISHED (i.e., turned
into functions in context) by restric-
tive modification, particularly by es-
tablishing relative clauses (Loebner,
1987) and prepositional phrases, as in
The hotel where we stayed last night was
pretty good. These heuristics, as well,
examined the syntactic structure of the NP.
5. LARGER SITUATION definite descriptions
(Hawkins, 1978), i.e., definite descriptions like
the sun, the pope or the long distance mar-
ket which denote uniquely on the grounds of
shared knowledge about the situation (these are
Loebner?s ?situational functions?). Vieira and
Poesio?s system had a small list of such defi-
nites.
These heuristics were included as tests both of a de-
cision tree concerned only with the task of DN de-
tection, and of decision trees determining the classi-
fication of DDs as anaphoric, bridging or discourse
new. In both cases, the DN detection tests were in-
tertwined with attempts to identify an antecedent for
such DDs. Both hand-coded decision trees and auto-
matically acquired ones (trained using ID3, (Quin-
lan, 1986)) were used for the task of two-way clas-
sification into discourse-new and anaphoric. Vieira
and Poesio found only small differences in the order
of tests in the two decision trees, and small differ-
ences in performance. The hand-coded decision tree
executes in the following order:
1. Try the DN heuristics with the highest accu-
racy (recognition of some types of semanti-
cally functional DDs using special predicates,
and of potentially predicative DDs occurring in
appositions);
2. Otherwise, attempt to resolve the DD as direct
anaphora;
3. Otherwise, attempt the remaining DN heuris-
tics in the order: proper names, descrip-
tions established by relatives and PPs, proper
name modification, predicative DDs occurring
in copular constructions.
If none of these tests succeeds, the algorithm can ei-
ther leave the DD unclassified, or classify it as DN.
The automatically learned decision tree attempts di-
rect anaphora resolution first. The overall results on
the 195 DDs on which the automatically trained de-
cision tree was tested are shown in Table 1. The
baseline is the result achieved by classifying every
DD as discourse-new?with 99 discourse-new DDs
out of 195, this means a precision of 50.8%. Two
results are shown for the hand-coded decision tree:
in one version, the system doesn?t attempt to clas-
sify all DDs; in the other, all unclassified DDs are
classified as discourse-new.
Version of the System P R F
Baseline 50.8 100 67.4
Discourse-new detection only 69 72 70
Hand-coded DT: partial 62 85 71.7
Hand-coded DT: total 77 77 77
ID3 75 75 75
Table 1: Overall results by Vieira and Poesio
2.2 Bean and Riloff
Bean and Riloff (1999) developed a system for iden-
tifying discourse-new DDs1 that incorporates, in ad-
dition to syntax-based heuristics aimed at recogniz-
ing predicative and established DDs using postmod-
ification heuristics similar to those used by Vieira
and Poesio, additional techniques for mining from
corpora unfamiliar DDs including proper names,
larger situation, and semantically functional. Two
1Bean and Riloff use the term EXISTENTIAL for these DDs.
of the techniques proposed by Bean and Riloff are
particularly worth noticing. The SENTENCE-ONE
(S1) EXTRACTION heuristic identifies as discourse-
new every DD found in the first sentence of a text.
More general patterns can then be extracted from
the DDs initially found by S1-extraction, using the
EXISTENTIAL HEAD PATTERN method which, e.g.,
would extract the N+ Government from the
Salvadoran Government and the Guatemalan Gov-
ernment. The DEFINITE ONLY (DO) list contained
NPs like the National Guard or the FBI with a high
DEFINITE PROBABILITY, i.e., whose nominal com-
plex has been encountered at least 5 times with the
definite article, but never with the indefinite. VAC-
CINES were also developed that prevented the use
of patterns identified by S1-extraction or DO-list el-
ements when the definite probability of the definite
was too low. Overall, the algorithm proposed by
Bean and Riloff is as follows:
1. If the head noun of the DD appeared earlier in
the text, classify as anaphoric.
2. Otherwise, if the DD occurs in the S1 list, clas-
sify as discourse-new unless stopped by vac-
cine.
3. Otherwise, classify the DD as DN if one of the
following tests applies:
(a) it occurs in the DO list;
(b) it matches one of the EHP patterns, and is
not stopped by vaccine;
(c) it matches one of the syntactic heuristics
4. Otherwise, classify the DD as anaphoric.
(Note that as in the machine-learned version of the
Vieira and Poesio decision tree, a (simplified) direct
anaphora test is tried first, followed by DN detectors
in decreasing order of accuracy.)
Bean and Riloff trained their system on 1600 ar-
ticles from MUC-4, and tested it on 50 texts. The
S1 extraction methods produced 849 DDs; the DO
list contained 65 head nouns and 321 full NPs. The
overall results are shown in Table 2; the baseline
are the results obtained when classifying all DDs as
discourse-new.
Although the overall precision is not better than
what obtained with the partial hand-coded decision
tree used by Vieira and Poesio, recall is substantially
improved.
2.3 Ng and Cardie
Ng and Cardie (2002a) directly investigate the ques-
tion of whether employing a discourse-new pre-
diction component improves the performance of a
Method R P
Baseline 100 72.2
Syntactic Heuristics 43 93.1
Synt. Heuristics + S1 66.3 84.3
Synt. Heuristics + EHP 60.7 87.3
Synt. Heuristics + DO 69.2 83.9
Synt. Heuristics + S1 + EHP + DO 81.7 82.2
Synt. Heuristics + S1 + EHP + DO + V 79.1 84.5
Table 2: Discourse-new prediction results by Bean
and Riloff
coreference resolution system (specifically, the sys-
tem discussed in (Ng and Cardie, 2002b)). Ng and
Cardie?s work differs from the work discussed so far
in that their system attempts to deal with all types of
NPs, not just definite descriptions.
The discourse-new detectors proposed by Ng and
Cardie are statistical classifiers taking as input 37
features and trained using either C4.5 (Quinlan,
1993) or RIPPER (Cohen, 1995). The 37 features
of a candidate anaphoric expression specify, in ad-
dition to much of the information proposed in pre-
vious work, a few new types of information about
NPs.
? The four boolean so-called LEXICAL features
are actually string-level features: for exam-
ple, str_match is Y if a preceding NP
string-matches the anaphoric expression (ex-
cept for the determiner), and head_match =
Y if a preceding NP?s head string-matches the
anaphoric expression?s. embedded=Y if the
anaphoric expression is a prenominal modifier.
? The second group of 11 (mostly boolean) fea-
tures specifies the type of NP: e.g., pronoun
is Y if the anaphoric expression is a pronoun,
else N.
? The third group of 7 features specifies syn-
tactic properties of the anaphoric expression,
including number, whether NPj is the first of
two NPs in an appositive or predicative con-
struction, whether NPj is pre- or post-modified,
whether it contains a proper noun, and whether
it is modified by a superlative.
? The next group of 8 features are mostly novel,
and capture information not used by previ-
ous DN detectors about the exact composition
of definite descriptions: e.g., the_2n=Y if
the anaphoric expression starts with deter-
miner the followed by exactly two common
nouns, the_num_n=Y if the anaphoric ex-
pression starts with determiner the followed
by a cardinal and a common noun, and
the_sing_n=Y if the anaphoric expression
starts with determiner the followed by a singu-
lar NP not containing a proper noun.
? The next group of features consists of 4 fea-
tures capturing a variety of ?semantic? infor-
mation, including whether a previous NP is an
?alias? of NPj , or whether NPj is the title of a
person (the president).
? Finally, the last three features capture informa-
tion about the position in the text in which NPj
occurs: the header, the first sentence, or the
first paragraph.
Ng and Cardie?s discourse-new predictor was
trained and tested over the MUC-6 and MUC-7 coref-
erence data sets, achieving accuracies of 86.1% and
84%, respectively, against a baseline of 63.8% and
73.2%, respectively. Inspection of the top parts
of the decision tree produced with the MUC-6 sug-
gests that head_match is the most important fea-
ture, followed by the features specifying NP type,
the alias feature, and the features specifying the
structure of definite descriptions.
Ng and Cardie discuss two architectures for the
integration of a DN detector in a coreference sys-
tem. In the first architecture, the DN detector is
run first, and the coreference resolution algorithm
is run only if the DN detector classifies that NP as
anaphoric. In the second architecture, the system
first computes str_match and alias, and runs
the anaphoric resolver if any of them is Y; other-
wise, it proceeds as in the first architecture. The
results obtained on the MUC-6 data with the base-
line anaphoric resolver, the anaphoric resolver aug-
mented by a DN detector as in the first architecture,
and as in the second architecture (using C4.5), are
shown in Table 3. The results for all NPs, pronouns
only, proper names only, and common nouns only
are shown.2
As indicated in the Table, running the DN detector
first leads to worse results?this is because the detec-
tor misclassifies a number of anaphoric NPs as non-
anaphoric. However, looking first for a same-head
antecedent leads to a statistically significant im-
provement over the results of the baseline anaphoric
resolver. This confirms the finding both of Vieira
and Poesio and of Bean and Riloff that the direct
anaphora should be called very early.
2It?s not clear to us why the overall performance of the algo-
rithm is much better than the performance on the three individ-
ual types of anaphoric expressions considered?i.e., which other
anaphoric expressions are handled by the coreference resolver.
MUC-6 MUC-7
R P F R P F
Baseline (no DN detector) 70.3 58.3 63.8 65.5 58.2 61.6
Pronouns 17.9 66.3 28.2 10.2 62.1 17.6
Proper names 29.9 84.2 44.1 27.0 77.7 40.0
Common nouns 25.2 40.1 31.0 26.6 45.2 33.5
DN detector runs first 57.4 71.6 63.7 47.0 77.1 58.4
Pronouns 17.9 67.0 28.2 10.2 62.1 17.6
Proper names 26.6 89.2 41.0 21.5 84.8 34.3
Common nouns 15.4 56.2 24.2 13.8 77.5 23.4
Same head runs first 63.4 68.3 65.8 59.7 69.3 64.2
Pronouns 17.9 67.0 28.2 10.2 62.1 17.6
Proper names 27.4 88.5 41.9 26.1 84.7 40.0
Common nouns 20.5 53.1 29.6 21.7 59.0 31.7
Table 3: Evaluation of the three anaphoric resolvers
discussed by Ng and Cardie.
2.4 Uryupina
Uryupina (2003) trained two separate classifiers (us-
ing RIPPER, (Cohen, 1995)): a DN detector and a
UNIQUENESS DETECTOR, i.e., a classifier that de-
termines whether an NP refers to a unique object.
This is useful to identify proper names (like 1998,
or the United States of America), semantic definites
(like the chairman of Microsoft) and larger situation
definite descriptions (like the pope). Both classi-
fiers use the same set of 32 features. The features of
an NP encode, first, of all, string-level information:
e.g., whether the NP contains capitalized words, dig-
its, or special symbols. A second group of features
specifies syntactic information: whether the NP is
postmodified, and whether it contains an apposition.
Two types of appositions are distinguished, with and
without commas. CONTEXT features specify the
distance between the NP and the previous NP with
the same head, if any. Finally, Uryupina?s system
computes four features specifying the NP?s definite
probability. Unlike the definite probability used by
Bean and Riloff, these features are computed from
the Web, using Altavista. From each NP, its head H
and entire NP without determiner Y are determined,
and four ratios are then computed:
#?the Y?
#Y ,
#?the Y?
#??aY ?? ,
#?the H?
#H ,
#?the H?
#??aH?? .
The classifiers were tested on 20 texts from MUC-
7 (a subset of the second data set used by Ng and
Cardie), parsed by Charniak?s parser. 19 texts were
used for training and for tuning RIPPER?s parame-
ters, one for testing. The results for the discourse
new detection task are shown in Table 4, separat-
ing the results for all NPs and definite NPs only,
and the results without definite probabilities and in-
cluding them. The results for uniqueness detection
are shown in Table 4, in which the results obtained
by prioritizing precision and recall are shown sepa-
rately.
Features P R F
All NPs String+Syn+Context 87.9 86.0 86.9
All 88.5 84.3 86.3
Def NPs String+Syn+Context 82.5 79.3 80.8
All 84.8 82.3 83.5
Table 4: Results of Uryupina?s discourse new clas-
sifier
Features P R F
Best Prec String+Syn+Context 94.0 84.0 88.7
All 95.0 83.5 88.9
Best Rec String+Syn+Context 86.7 96.0 91.1
All 87.2 97.0 91.8
Table 5: Results of Uryupina?s uniqueness classifier
The first result to note is that both of Uryupina?s
classifiers work very well, particularly the unique-
ness classifier. These tables also show that the def-
inite probability helps somewhat the discourse new
detector, but is especially useful for the uniqueness
detector, as one would expect on the basis of Loeb-
ner?s discussion.
2.5 Summary
Quite a lot of consensus on many of the factors play-
ing a role in DN detection for DDs. Most of the al-
gorithms discussed above incorporate methods for:
? recognizing predicative DDs;
? recognizing discourse-new proper names;
? identifying functional DDs;
? recognizing DDs modified by establishing rel-
atives (which may or may not be discourse-
new).
There is also consensus on the fact that DN detection
cannot be isolated from anaphoric resolution (wit-
ness the Ng and Cardie results).
One problem with some of the machine learning
approaches to coreference is that these systems do
not achieve very good results on pronoun and defi-
nite description resolution in comparison with spe-
cialized algorithms: e.g., although Ng and Cardie?s
best version achieves F=65.8 on all anaphoric ex-
pressions, it only achieves F=29.6 for definite de-
scriptions (cfr. Vieira and Poesio?s best result of
F=77), and F=28.2 for pronouns (as opposed to re-
sults as high as F=80 obtained by the pronoun res-
olution algorithms evaluated in (Tetreault, 2001)).
Clearly these systems can only be properly com-
pared by evaluating them all on the same corpora
and the same data, and discussion such as (Mitkov,
2000) suggest caution in interpreting some of the
results discussed in the literature as pre- and post-
processing often plays a crucial role, but we feel that
evaluating DN detectors in conjunction with high-
performing systems would give a better idea of the
improvements that one may hope to achieve.
3 Do Discourse-New Detectors Help?
Preliminary Evaluations
Vieira and Poesio did not test their system with-
out DN-detection, but Ng and Cardie?s results indi-
cate that DN detection does improve results, if not
dramatically, provided that the same_head test is
run first?although their DN detector does not appear
to improve results for pronouns, the one category
for which detection of non-anaphoricity has been
shown to be essential (Lappin and Leass, 1994). In
order to evaluate how much improvement can we
expect by just improving the DN detector, we did
a few preliminary evaluations both with a reimple-
mentation of Vieira and Poesio?s algorithm which
does not include a discourse-new detector, running
over treebank text as the original algorithm, and
with a simple statistical coreference resolver at-
tempting to resolve all anaphoric expressions and
running over unparsed text, using Uryupina?s fea-
tures for discourse-new detection, and over the same
corpus used by Ng and Cardie (MUC-7).
3.1 How much does DN-detection help the
Vieira / Poesio algorithm?
GUITAR (Poesio and Alexandrov-Kabadjov, 2004)
is a general-purpose anaphoric resolver that in-
cludes an implementation of the Vieira / Poesio al-
gorithm for definite descriptions and of Mitkov?s al-
gorithm for pronoun resolution (Mitkov, 1998). It is
implemented in Java, takes its input in XML format
and returns as output its input augmented with the
anaphoric relations it has discovered. GUITAR has
been implemented in such a way as to be fully mod-
ular, making it possible, for example, to replace the
DD resolution method with alternative implementa-
tions. It includes a pre-processor incorporating a
chunker so that it can run over both hand-parsed and
raw text.
A version of GUITAR without the DN detection
aspects of the Vieira / Poesio algorithm was evalu-
ated on the GNOME corpus (Poesio, 2000; Poesio et
al., 2004), which contains 554 definite descriptions,
of which 180 anaphoric, and 305 third-person pro-
nouns, of which 217 anaphoric. The results for defi-
nite descriptions over hand-parsed text are shown in
Table 6.
Total Res Corr NM WM SM R P F
180 182 121 43 16 45 67.2 66.5 66.8
Table 6: Evaluation of the GUITAR system without
DN detection over a hand-annotated treebank
GUITAR without a DN recognizer takes 182 DDs
(Res) as anaphoric, resolving 121 of them cor-
rectly (Corr); of the 182 DDs it attempts to resolve,
only 16 are incorrectly resolved (WM); almost three
times that number (45) are Spurious Matches (SM),
i.e., discourse-new DDs incorrectly interpreted as
anaphoric. (Res=Corr+WM+SM.) The system can?t
find an antecedent for 43 of the 180 anaphoric DDs.
When endowed with a perfect DN detector, GUI-
TAR could achieve a precision P=88.3 which, as-
suming recall stays the same (R=67.2) would mean
a F=76.3.
Of course, these results are obtained assuming
perfect parsing. For a fairer comparison with the
results of Ng and Cardie, we report in Table 7 the
results for both pronouns and definite descriptions
obtained by running GUITAR off raw text.
R P F
Pronouns 65.5 63.0 64.2
DDs 56.7 56.1 56.4
Table 7: Evaluation of the GUITAR system without
DN detection off raw text
Notice that although these results are not partic-
ularly good, they are still better than the results re-
ported by Ng and Cardie for pronouns and definite
NPs.
3.2 How much might DN detection help a
simple statistical coreference resolver?
In order to have an even closer comparison with
the results of Ng and Cardie, we implemented a
simple statistical coreference system, that, like Ng
and Cardie?s system, would resolve all types of
anaphoric expressions, and would run over unparsed
text, but without DN detection. We ran the system
over the MUC-7 data used by Ng and Cardie, and
compared the results with those obtained by using
perfect knowledge about discourse novelty. The re-
sults are shown in Table 8.
R P F
Without DN detection 44.7 54.9 49.3
With DN detection 41.4 80.0 54.6
Table 8: Using an oracle
These results suggest that a DN detector could
lead to substantial improvements for coreference
resolution in general: DN detection might improve
precision by more than 30%, which more than
makes up for the slight deterioration in recall. Of
course, this test alone doesn?t tell us how much im-
provement DN detection would bring to a higher-
performance anaphoric resolver.
4 A New Set of Features for
Discourse-New Detection
Next, we developed a new set of features for dis-
course new detection that takes into account the
findings of the work on DN detection discussed in
the previous sections. This set of features will be
input to an anaphoric resolver for DDs working in
two steps. For each DD,
1. The direct anaphora resolution algorithm from
(Vieira and Poesio, 2000) is run, which at-
tempts to find an head-matching antecedent
within a given window and taking premodifica-
tion into account. The results of the algorithm
(i.e., whether an antecedent was found) is used
as one of the input features of the classifier in
the next step. In addition, a number of features
of the DD that may help recognizing the classes
of DDs discussed above are extracted from the
input. Some of these features are computed ac-
cessing the Web via the Google API.
2. A decision tree classifier is used to classify the
DD as anaphoric (in which case the antecedents
identified at the first step are also returned) or
discourse-new.
The features input to the classifier can be catego-
rized as follows:
Anaphora A single feature,
direct-anaphora, specifying the distance
of the (same-head) antecedent from the DD, if
any (values: none, zero, one, more)
Predicative NPs Two boolean features:
? apposition, if the DD occurs in appos-
itive position;
? copular, if the DD occurs in post-verbal
position in a copular construction.
Proper Names Three boolean features:
? c-head: whether the head is capitalized;
? c-premod: whether one of the premod-
ifiers is capitalized;
? S1: whether the DD occurs in the first sen-
tence of a Web page.
Functionality The four definite probabilities used
by Uryupina (computed accessing the Web),
plus a superlative feature specifying if
one of the premodifiers is a superlative, ex-
tracted from the part of speech tags.
Establishing relative A single feature, specifying
whether NP is postmodified, and by a relative
clause or a prepositional phrase;
Text Position Whether the DD occurs in the title,
the first sentence, or the first paragraph.
We are testing several classifiers in-
cluded in the Weka 3.4 library
(http://www.cs.waikato.ac.nz/?ml/)
including an implementation of C4.5 and a
multi-layer perceptron.
5 Evaluation
Data We are using three corpora for the evalua-
tion, including texts from different genres, in which
all anaphoric relations between (all types of) NPs are
marked. The GNOME corpus includes pharmaceuti-
cal leaflets and museum ?labels? (i.e., descriptions
of museum objects and of the artists that realized
them). As said above, the corpus contains 554 def-
inite descriptions. In addition, we are using the 14
texts from the Penn Treebank included in the cor-
pus used by Vieira and Poesio. We transferred these
texts to XML format, and added anaphoric informa-
tion for all types of NPs according to the GNOME
scheme. Finally, we are testing the system on the
MUC-7 data used by Ng and Cardie
Methods We will compare three versions of the
DD resolution component:
1. The baseline algorithm without DN detection
incorporated in GUITAR described above (i.e.,
only the direct anaphora resolution part of
(Vieira and Poesio, 2000));
2. A complete implementation of the Vieira and
Poesio algorithm, including also the DN detect-
ing heuristics;
3. An algorithm using the statistical classifier dis-
cussed above.
Results Regrettably, the system is still being
tested. We will report the results at the workshop.
6 Discussion and Conclusions
Discussions and conclusions will be based on the
final results.
Acknowledgments
Mijail Alexandrov-Kabadjov is supported by Cona-
cyt. Renata Vieira and Rodrigo Goulart are partially
supported by CNPq.
References
D. L. Bean and E. Riloff. 1999. Corpus-based
identification of non-anaphoric noun phrases. In
Proc. of the 37th ACL, pages 373?380, University
of Maryland. ACL.
W. Cohen. 1995. Fast effective rule induction. In
Proc. of ICML.
J. A. Hawkins. 1978. Definiteness and Indefinite-
ness. Croom Helm, London.
S. Lappin and H. J. Leass. 1994. An algorithm for
pronominal anaphora resolution. Computational
Linguistics, 20(4):535?562.
S. Loebner. 1987. Definites. Journal of Semantics,
4:279?326.
M. P. Marcus, B. Santorini, and M. A.
Marcinkiewicz. 1993. Building a large an-
notated corpus of english: the Penn Treebank.
Computational Linguistics, 19(2):313?330.
R. Mitkov. 1998. Robust pronoun resolution with
limited knowledge. In Proc. of the 18th COL-
ING, pages 869?875, Montreal.
R. Mitkov. 2000. Towards more comprehensive
evaluation in anaphora resolution. In Proc. of
the 2nd International Conference on Language
Resources and Evaluation, pages 1309?1314,
Athens, May.
V. Ng and C. Cardie. 2002a. Identifying anaphoric
and non-anaphoric noun phrases to improve
coreference resolution. In Proc. of 19th COL-
ING.
V. Ng and C. Cardie. 2002b. Improving machine
learning approaches to coreference resolution. In
Proceedings of the 40th Meeting of the ACL.
M. Poesio and M. Alexandrov-Kabadjov. 2004. A
general-purpose, off the shelf anaphoric resolver.
In Proc. of LREC, Lisbon, May.
M. Poesio and R. Vieira. 1998. A corpus-based in-
vestigation of definite description use. Compu-
tational Linguistics, 24(2):183?216, June. Also
available as Research Paper CCS-RP-71, Centre
for Cognitive Science, University of Edinburgh.
M. Poesio, R. Stevenson, B. Di Eugenio, and J. M.
Hitzeman. 2004. Centering: A parametric theory
and its instantiations. Computational Linguistics.
To appear.
M. Poesio. 2000. Annotating a corpus to develop
and evaluate discourse entity realization algo-
rithms: issues and preliminary results. In Proc.
of the 2nd LREC, pages 211?218, Athens, May.
E. F. Prince. 1992. The ZPG letter: subjects, defi-
niteness, and information status. In S. Thompson
and W. Mann, editors, Discourse description: di-
verse analyses of a fund-raising text, pages 295?
325. John Benjamins.
J. R. Quinlan. 1986. Induction of decision trees.
Machine Learning, 1(1):81?106.
J. R. Quinlan. 1993. C4.5: programs for machine
learning. Morgan Kaufmann, San Mateo, CA.
J. R. Tetreault. 2001. A corpus-based evaluation
of centering and pronoun resolution. Computa-
tional Linguistics, 27(4):507?520.
O. Uryupina. 2003. High-precision identification
of discourse-new and unique noun phrases. In
Proc. of the ACL 2003 Student Workshop, pages
80?86.
R. Vieira and M. Poesio. 2000. An empirically-
based system for processing definite descriptions.
Computational Linguistics, 26(4), December.
R. Vieira. 1998. Definite Description Resolution
in Unrestricted Texts. Ph.D. thesis, University of
Edinburgh, Centre for Cognitive Science, Febru-
ary.
The MATE/GNOME Proposals for Anaphoric Annotation, Revisited
Massimo Poesio
University of Essex
Department of Computer Science and Centre for Cognitive Science
United Kingdom
Abstract
In the five years since it was proposed, the
MATE scheme for anaphoric annotation has
been used in a variety of annotation projects,
and the resulting corpora have been used to
study both anaphora resolution and NL gener-
ation. Annotation tools inspired by the propos-
als have been used in some of these projects.
In this paper we discuss these first experiences
with the scheme, some lessons that have been
learned, and suggest a few modifications.
1 Introduction
The MATE ?meta-scheme? for anaphora annotation (Poe-
sio et al, 1999) is one of the annotation schemes devel-
oped as part of the MATE project (McKelvie et al, 2001),
whose goal was to develop annotation tools suitable for
different types of dialogue annotation. The scheme has
served as the basis for a number of annotation projects,
such as the development of the GNOME corpus (Poe-
sio, 2000a) and, more recently, of the VENEX corpus of
anaphora in Italian spoken dialogue and text (Poesio et
al., 2004a). The GNOME corpus has been used to study
salience, particularly as formalized in Centering theory
(Poesio et al, 2004c), to develop statistical models of nat-
ural language generation (e.g., (Poesio, 2000a; Henschel
et al, 2000; Cheng et al, 2001; Cheng, 2001; Karama-
nis, 2003)) and to evaluate anaphora resolution systems,
with a special focus on the resolution of bridging refer-
ences (Poesio, 2003; Poesio and Alexandrov-Kabadjov,
2004; Poesio et al, 2004b). Aspects of the scheme have
been implemented in annotation tools including MMAX
(Mu?ller and Strube, 2003) and the Annotator tool devel-
oped by ILSP. As a result of this work, many aspects
of the proposals concerning anaphoric annotation made
in MATE and GNOME have been subjected to a thorough
test. In this paper we discuss some of the lessons learned
through this work, some issues that have been raised, and
how they have been or could be addressed.
2 The MATE Proposals
The design of an annotation scheme involves a number
of decisions: what has to be annotated, how, and how
the annotation should be recorded (the markup scheme).
One of the most important motivations behind the design
of the MATE proposals for anaphoric annotation is the be-
lief that given the variety of phenomena that go under the
name of anaphora, and the variety of possible applica-
tions, there can be no such thing as a general-purpose
anaphoric annotation instructions. On the other hand,
we also believed that it is possible to design a general
purpose markup scheme (and therefore, general-purpose
tools) that could then be used in different ways for dif-
ferent projects. The approach taken in MATE was then
to design a general markup scheme (the ?meta-scheme?)
and then to show its basic building blocks could be used
to implement different types of anaphoric annotation, in-
cluding some of the most popular schemes for ?coref-
erence annotation,? such as the MUC scheme (MUCCS)
(Hirschman, 1998), Passonneau?s DRAMA scheme (1997)
, and the scheme used for annotation of references to
landmarks in the MapTask corpus. In this section we
summarize the most distinctive features of the proposals
resulting from this basic assumption. The full description
of the MATE scheme is available from the MATE project
pages at http://mate.nis.sdu.dk/.
2.1 Coreference, Anaphora and Discourse
Modeling
The MATE scheme differs from the best-known scheme
for annotating ?coreference,? MUCCS (Hirschman, 1998)
both in the conceptualization underlying the annotation
(i.e., what type of information should be annotated) and
in the way this information is marked up. MUCCS was
designed to encode information deemed useful for a sub-
task of information extraction, and the instructions pro-
vided to annotators were meant to ensure that all infor-
mation provided by a text about a certain entity would be
marked using a single device, the IDENT relation. As
van Deemter and Kibble (2000) point out, however, the
result is rather ad hoc; the IDENT relation as defined by
the instructions doesn?t capture any coherent definition
of ?coreference?. (In fact, the very notion of ?reference?
is rather difficult to formalize precisely.)
The MATE proposals, by contrast, while still labeled as
proposals for ?coreference annotation,? because the name
has become a de facto standard as a result of the MUC
initiative, are explicitly based on the DISCOURSE MODEL
assumption adopted almost universally by linguists (com-
putational and not) working on anaphora resolution and
generation (Webber, 1979; Heim, 1982; Kamp and Reyle,
1993; Gundel et al, 1993). This is the hypothesis that
interpreting a discourse involves building a shared dis-
course model containing DISCOURSE ENTITIES that may
or may not ?refer? to specific objects in the world, as well
as the relations between these entities. The type of an-
notation for which the MATE scheme was developed?and
that we?ll call here ?anaphoric annotation,?.1 is meant as
a partial representation of the discourse model evoked by
a text (hence, for example, the tag used for nominal ex-
pressions denoting discourse entities, ?de?).2
2.2 The Markup Scheme
The design of the MATE workbench was strongly inspired
by the concept of STANDOFF ANNOTATION developed for
the reorganization of the MapTask. The main principle
of standoff annotation is that each level of annotation?
for example, syntactic annotation, dialogue act annota-
tion, and anaphoric annotation?should be stored indepen-
dently; in this way, annotators working on one level need
not be concerned about the other levels of annotation, and
can start immediately without having to wait for other an-
notation tasks to be completed. The separate levels of
annotation are synchronized via a base file, to which the
separate levels point using the HREF mechanism of XML.
The markup scheme for anaphoric relations is the
core aspect of the MATE proposals and its most dis-
tinctive aspect. As in the MUC scheme, it is as-
1van Deemter and Kibble (2000) give a stricly textual def-
inition of ?anaphora? which is very distant from the common
use of the term ?anaphora resolution? in computational linguis-
tics, typically used to indicate the interpretation of (parts of) the
meaning of an expression with respect to the discourse model.
2In fact, the use of the term ?coreference annotation? would
not be completely misguided. van Deemter and Kibble (2000)
assume the definition of ?reference? typically found in formal
semantics, but in functional linguistics, the term ?referring ex-
pression? is used to indicate expressions that introduce new dis-
course entities in a discourse model or that denote an old one
(see, e.g., (Gundel et al, 1993)).
sumed that annotation of anaphoric information involves
identifying MARKABLES (the text constituent that real-
ize semantic objects that may enter in anaphoric rela-
tions), and marking up anaphoric relations between them.
The main difference from MUCCS is that whereas in
MUCCS anaphoric relations are annotated using an at-
tribute of the markables, in the MATE markup scheme?
following the recommendations of the Text Encoding Ini-
tiative (Burnard and Sperberg-McQueen, 2002), and of
Bruneseaux and Romary (1998)?the distinction between
these two steps of annotation is mirrored by a distinc-
tion between two XML elements: ?de?, used to indi-
cate the markables, and ?link?, used to mark informa-
tion about anaphoric relations (or any other semantic re-
lation).3 However, unlike in the TEI proposals, in the
MATE markup scheme ?link? elements are structured
elements, containing one or more ?anchor? element.
The ?link? element specifies the anaphoric expression
(using XML?s HREFmechanism) and the relation between
the anaphoric expression and its antecedent; whereas the
?anchor? element specifies the antecedent, as in (1)
where, for example, the first ?link? elements encodes
the information that the discourse entities realized by the
NPs the engine E3 and it denote the same object.
(1) coref.xml
<de ID="de_01">we</de>?re gonna take
<de ID="de_07"> the engine E3 </de>
and shove <de ID="de_08"> it </de> over
to <de ID="de_02">Corning</de>,
hook <de ID="de_09"> it </de> up to
<de ID="de_03">the tanker car</de>...
<link href="coref.xml#id(de_07)"
type="ident">
<anchor href="coref.xml#id(de_08)"/>
</link>
<link href="coref.xml#id(de_08)"
type="ident">
<anchor href="coref.xml#id(de_09)"/>
</link>
There were two main reasons for having ?link? ele-
ments separated from the elements used to indicate mark-
ables. The first reason is that in this way ?link? ele-
ments can be kept in a separate file from ?de? elements,
in keeping with the idea of standoff annotation. The sec-
ond, and more important, reason is that in this way it is
possible to annotate multiple anaphoric relations involv-
ing the same anaphoric expression without having multi-
ple attributes for each markable.
The reason why ?link? elements may have more than
one ?anchor? element is to allow for the possibility to
annotate ambiguities. For some types of applications, it
may be a good idea not to ask annotators to decide upon
the interpretation of ambiguous anaphoric expressions.
3It was assumed that the tags for ?coreference? annotation
would be part of a special namespace, COREF?i.e., that the ac-
tual name of these tags are ?coref:de?, ?coref:link?, etc.
We omit the namespace indication in this paper.
In these cases, the multiple anchors mechanisms allows
each of the possibilities to be marked by means of a sepa-
rate ?anchor? element. In (2a), for example, the pronun
it in 15.16 could refer equally well to engine E3 or the
tanker car. With the MATE mechanism, both antecedents
can be annotated, as shown in (2b).
(2) a. 15.12 : we?re gonna take the engine E3
15.13 : and shove it over to Corning
15.14 : hook it up to the tanker car
15.15 : _and_
15.16 : and send it back to Elmira
b. coref.xml:
15.12 : we?re gonna take
<de ID="de_15">the engine E3</de>
15.13 : and shove <de ID="de_16"> it </de>
over to Corning
15.14 : hook <de ID="de_17">it</de> up to
<de ID="de_18">the tanker car</de>
15.15 : _and_
15.16 : and send <de ID="de_19">it</de>
back to Elmira
<link href="coref.xml#id(de_16)" type="ident">
<anchor href="coref.xml#id(de_15)"/>
</link>
<link href="coref.xml#id(de_17)" type="ident">
<anchor href="coref.xml#id(de_16)"/>
</link>
<link href="coref.xml#id(de_19)" type="ident">
<anchor href="coref.xml#id(de_17)"/>
<anchor href="coref.xml#id(de_18)"/>
</link>
2.3 Instantiations of the Meta-Scheme
As said above, the markup elements just discussed were
meant to be general enough to support different types of
annotation. Three such examples were considered.
The Core Scheme In the most basic type of corefer-
ence scheme, only anaphoric relations between NPs are
considered, and only identity relations. Schemes of this
type can be implemented by having just one anaphoric
relation, IDENT. The remaining differences between the
schemes have then mostly to do with the instructions to
annotators?for example, which types of anaphoric rela-
tions to be considered as cases of ?identity? (see (van
Deemter and Kibble, 2000) for some problems with the
choices made in MUCCS). In the comments for the de-
signers of a scheme, it was suggested that some of the
cases marked as coreference in MUCCS, such as the rela-
tion between the temperature and 90 degrees in the tem-
perature rose to 90 degrees before dropping to 70 de-
grees, would be best marked as function-value relations
(viewing the temperature as a function from objects and
time points into values, rather than an individual-denoting
term).
Extended Relations In DRAMA, a number of associa-
tive relations are considered, such as SUBSET or PART,
together with instructions how to annotate them. This
types of anaphora can be annotated in the MATE markup
scheme using additional relations, as in (3), where the
discourse entity realized by LES FUSEES QUI ONT
BIEN VOLE? denotes a subset of the set denoted by dis-
course entity DE 88, LES MODELES DE FUSEES.
(3) a. F: Alors donc / vous avez / ici /
LES MODELES DE FUSEES /
M: Oui
F: Et vous allez essayer de vous
mettre d?accord sur un classement
/hein classer
LES FUSEES QUI ONT BIEN VOLE? ou
QUI ONT MOINS BIEN VOLE?
b. F: Alors donc / vous avez / ici /
<de ID="de_88"> les mode?les de fuse?es </de>
M: Oui
F: Et vous allez essayer de vous mettre d?accord
sur un classement /hein classer
<de ID="de_89"> les fuse?es qui ont
bien vole? </de>
ou <de ID="de_90"> qui ont
moins bien vole? </de>
<link href="coref.xml#id(de_89)">
<anchor href="coref.xml#id(de_88)"
type="subset " />
</link>
<link href="coref.xml#id(de_90)"
type="subset " >
<anchor href="coref.xml#id(de_88)"/>
</link>
It was pointed out, however, that the results of
Poesio and Vieira (1998) indicated that this type of an-
notation could be highly unreliable.
References to the Visual Situation A special
?universe? element was suggested for MapTask-
style annotations of references to visible objects. The
?universe? element containing one ?ue? element
for each object in the visual scene; including such
elements in an annotation makes it possible to use
?link? elements to annotate references to such objects.4
Cases in which the participants to a conversation have
different visual situations, as in the MapTask dialogues,
can be handled by having separate universes, one for
each participant to the conversation. In addition, a
WHO-BELIEVES attribute of ?link? elements was
proposed to represent situations in which only one
participant believes that a particular anaphoric relation
holds, as in example (7) (Appendix A), where it?s only
the follower to believe that a gold mine refers to the same
object as diamond mine.
2.4 Instructions for Identifying Markables
Because the goal of the MATE annotation proposals was
to provide a set of tools that could be used to imple-
ment a variety of options, rather than to identify a spe-
cific scheme appropriate for all applications, it didn?t
make sense to specify detailed instructions for annota-
tion. However, a substantial effort was made to pro-
vide an exhaustive inventory of the options for identi-
4The ?universe? mechanism is based on the notion of
?anchor? developed in Discourse Representation Theory (DRT),
although simplified in a number of ways.
fying markables that were available to the designers of
a scheme for anaphoric annotation. These suggestions
were in part derived from MUCCS and from Passonneau?s
DRAMA scheme, but a number of additional problems
were considered as well.
As in MUCCS, it was assumed that annotation of
anaphora is best separated in two steps: first the mark-
ables (the text constituent that realize semantic objects
that may enter in anaphoric relations) are agreed upon,
then anaphoric relations between them are marked.
Concerning markable identification, the main sugges-
tions were to concentrate on anaphoric expressions re-
alized as NPs and their antecedents; and to rely on the
output of a parser as much as possible. But because of
the assumption that only NPs evoking discourse entities
should be considered, it was suggested that not all NP
should be treated as markables: for example, it was rec-
ommended that NPs in post-verbal position in predica-
tive clauses (such as a policeman in John is a policeman)
should be excluded. This recommendation was later re-
considered (see below).
One of the novel aspects of the MATE instructions was
the concern for markable identification in languages other
than English. One such issue was how to deal with incor-
porated clitics and empty subjects; the suggestion was to
use a separate element, ?seg?, to turn verbs into non-
nominal markables, as in the following example:
(4) coref.xml:
A: Dov?e? <de ID="de_157">Gianni?</de>
[Where is Gianni?]
B: <seg type="pred" ID="seg_158 >e?
andato a mangiare </seg>
[_ went to have lunch]
<link href="coref.xml#id(seg_158)"
type="ident">
<anchor href="coref.xml#id(de_157)"/>
</link>
It was also proposed that the ?seg? element could be
used in more ambitious schemes as general mechanism
for specifying non-nominal markables ?e.g., in ellipsis,
to indicate the antecedents of discourse deixis, etc.5
3 Work based on the MATE proposals
Ideas from the MATE ?scheme? have been adopted and
tested both in annotation projects and by the developers
of annotation tools. In this section we review some of
these activities and summarize the conclusions concern-
ing advantages and disadvantages of the MATE scheme
that can be drawn from them.
5A second range of issues considered in the MATE scheme
had to do with dialogue phenomena, such as non-contiguous
elements; we will not consider these issues here.
3.1 Annotation work related to the GNOME project
The most direct application of the ideas discussed above
was found in the annotation work undertaken as part of
the GNOME project. GNOME was concerned with the em-
pirical investigation of the aspects of discourse that ap-
pear to affect generation, especially salience (Pearson et
al., 2000; Poesio et al, 2000; Poesio and Di Eugenio,
2001; Poesio and Nissim, 2001; Poesio et al, 2004c).
Particular attention was paid to the factors affecting the
generation of pronouns (Pearson et al, 2000; Henschel et
al., 2000), demonstratives (Poesio and Nygren-Modjeska,
To appear) possessives (Poesio and Nissim, 2001) and
definites in general (Poesio, 2004). These results, and
the annotated corpus, were applied to the development
of both symbolic and statistical natural language genera-
tion algorithms with the application of these empirical re-
sults to natural language generation, from sentence plan-
ning (Poesio, 2000a; Henschel et al, 2000; Cheng et al,
2001), to aggregation (Cheng, 2001) and text planning
(Kibble and Power, 2000; Karamanis, 2003). The empir-
ical side of the project involved both psychological exper-
iments and corpus annotation, based on a scheme based
on the MATE proposals, as well as on a detailed anno-
tation manual (Poesio, 2000b), the reliability of whose
instructions was tested by extensive experiments (Poesio,
2000a). More recently, the corpus has also been used to
develop and evaluate anaphora resolution systems, with
a special focus on the resolution of bridging references
(Poesio, 2003; Poesio and Alexandrov-Kabadjov, 2004;
Poesio et al, 2004b).
The corpus The GNOME corpus currently includes
texts from three domains, about 3000 NPs were anno-
tated in each domain. The museum subcorpus consists
of descriptions of museum objects, generally with an as-
sociated picture, and brief texts about the artists that pro-
duced them. The pharmaceutical subcorpus is a selection
of leaflets providing the patients with legally mandatory
information about their medicine.
Several layers of information were annotated, includ-
ing layout in the case of text and rhetorical structure in
the case of tutorial dialogues, sentences and potential ut-
terances, noun phrases, a variety of attributes of the ob-
jects denoted by noun phrases,6 and anaphoric relation.
We concentrate here on anaphoric information, and refer
the reader to the manual for the other types of annotation.
Markup scheme The markup scheme for markables
and anaphoric relations adopted in GNOME follows very
6E.g., whether an NP denoted generically or not; whether it
denoted an animate or inanimate entity, as well as other onto-
logical properties; and whether it denoted a discourse entity, a
quantifier, or a predicate. In the case of a discourse entity, we
also annotated whether it denoted an atom, a set, or a mass term;
and whether it denoted uniquely or not.
closely that proposed in MATE, except that the ?de? ele-
ment was renamed ?ne? (since all NPs were marked), and
the ?link? element was renamed ?ante?. More sub-
stantial differences are the decision not to use standoff,
and the introduction of new elements necessary for the
study of salience, such as elements that could be used to
investigate the notion of UTTERANCE used in Centering
(Poesio et al, 2004c).
Although standoff is a clear improvement over includ-
ing all annotation levels in a single file, our own expe-
riences during the creation of the GNOME corpus being
further proof of this, it?s only really possible when tools
are available both to create the annotation and?crucially?
later to ?knit back? the separate levels when needed. As
neither the MATE workbench nor any other tools based
on standoff were available by the time the GNOME an-
notation started,7 in GNOME we didn?t use standoff, but
integrated all levels of annotation in one file; an Emacs
mode was developed for the annotation. This decision
made it very easy to use the annotated corpus for a num-
ber of studies, but did resulted in a number of problems,
the main among which were that the annotators had to be
very careful not to damage other annotations; that annota-
tors working on one level were occasionally confused by
annotations for other levels; and that the annotation work
had to be organized in a careful sequential way even for
levels that could have been annotated independently.
The main new aspect of the markup scheme, espe-
cially as far as our studies of salience were concerned,
are the elements used to annotate potential utterances
in the sense of Centering (Grosz et al, 1995). In or-
der not to prejudge the answer to the question of which
text constituents are best viewed as utterances, we used a
?generic? element called ?unit? to mark up finite and
non-finite clauses, but also parentheticals and apposi-
tions, elements of bulleted lists, etc.
The following example illustrates both the use of
?unit? elements and of the elements ?ne? and ?ante?
replacing ?de? and ?link?:
(5) <unit finite=?finite-yes? id=?u227?>
<ne id=?ne546? gf=?subj?> The drawing of
<ne id=?ne547? gf=?np-compl?>the corner
cupboard </ne>
</ne>
<unit finite=?no-finite? id=?u228?>,
or more probably
<ne id=?ne548? gf=?no-gf?> an engraving of
<ne id=?ne549? gf=?np-compl?> it </ne>
</ne>
</unit>,
...
</unit>
<ante current="ne549" rel="ident">
<anchor ID="ne547">
</ante>
7In the end lack of time prevented the inclusion of a tool for
anaphoric annotation in the released MATE workbench.
Bridging References Apart from the basic anaphoric
relations of identity, in GNOME we were concerned with
bridging references, hence our annotation scheme in-
corporated aspects of the ?Extended Relations? and the
?MapTask? instantiations of the MATE meta-scheme.
One of our aims was to continue the work on bridging
references annotation and interpretation in (Poesio and
Vieira, 1998), which showed that marking up bridging
references is quite hard. In addition, work such as (Sid-
ner, 1979; Strube and Hahn, 1999) suggested that indi-
rect realization can play a crucial role in maintaining the
CB. After testing a few types of associative reference
(Hawkins, 1978), we decided to annotate only three non-
identity relations, as well as identity. These relations are
a subset of those proposed in the ?extended relations? ver-
sion of the MATE scheme: set membership (ELEMENT),
subset (SUBSET), and ?generalized possession? (POSS),
which includes both part-of relations and ownership rela-
tions.
Coder manual Perhaps the most important aspects of
the annotation work in GNOME are the development of
detailed instructions for annotators and the reliability ex-
periments testing several aspects of the scheme, particu-
larly the annotation of bridging references.
The identification of sentences, units and markables
was done entirely by hand, without encountering particu-
lar problems. (The Emacs mode, an extension of SGML-
mode, provides some support for introducing new ele-
ments, marking regions, and attribute editing, as well as
anaphoric annotation.) Unlike in MATE, all NPs were
tagged as ?ne?. The instructions for ?unit?s were
based on Marcu?s proposals for discourse units annota-
tion (Marcu, 1999). All attributes of sentences, ?unit?s
and ?ne?s in the final version of the scheme, including
DEIX, can be annotated reliably.
In order to achieve reliability on anaphoric anno-
tation, the range of anaphoric phenomena considered
was restricted in many ways. Apart from marking a
limited number of associative relations, the annotators
only marked relations between objects realized by noun
phrases and not, for example, anaphoric references to
actions, events or propositions implicitly introduced by
clauses or sentences. We also gave strict instructions
to our annotators concerning how much to mark. They
were told to mark all identity relations, but to mark as-
sociative relations only if either (i) no IDENT relation
could be marked for the anaphoric expression, or (ii)
an IDENT relation with an entity not mentioned in the
previous ?unit?. Furthermore, preferences were speci-
fied, e.g., for appositions: for example, in Francois, the
Dauphin, the embedding NP would be chosen as an an-
tecedent of subsequent anaphoric references, rather than
the NP in appositive position.
We found a reasonable, although by no means perfect,
agreement on identity relations. In a typical analysis (two
annotators looking at the anaphoric relations between 200
NPs) we observed no real disagreements; 79.4% of these
relations were marked up by both annotators; 12.8% by
only one of them; and in 7.7% of the cases, one of the
annotators marked up a closer antecedent than the other.
Limiting the relations did limit the disagreements among
annotators on associative relations (only 4.8% of the re-
lations are actually marked differently) but only 22% of
bridging references were marked in the same way by both
annotators; 73.17% of relations are marked by only one
or the other annotator. Reaching agreement on this infor-
mation involved several discussions between annotators
and more than one pass over the corpus (Poesio, 2000a).
3.2 Annotation tools
Although no annotation tool implementing the MATE or
GNOME schemes as described exists, in the years after
the development of the MATE guidelines tools supporting
XML standoff annotation for coreference have appeared,
including MMAX from EML (Mu?ller and Strube, 2003)
and the Annotator from ILSP. Although the format used
for storing anaphoric information by these tools is not
entirely satisfactory, the files they produce can be easily
converted into MATE format.
MMAX, for example, is based on a simplified stand-
off format, in which three main files are maintained for
each annotated file in the corpus: a base file contain-
ing the words, a file identifying sentences, and a file
identifying markables. Anaphoric information is stored
as attributes of the markables. Two special attributes
are used for this purpose, and recognized by MMAX:
the MEMBER attribute, used to indicate membership in a
coreference chain (a coreference equivalence class), and
the POINTER attribute, used to mark up to one associa-
tive anaphoric relation for each anaphoric expression. We
discuss the use of MMAX in the VENEX project below.
3.3 The VENEX Corpus
The VENEX corpus is an anaphorically annotated corpus
of Italian being created in a joint project between the
Universita? di Venezia and the University of Essex. The
corpus includes both texts (newspaper articles) and dia-
logues (an Italian version of the MapTask corpus). This
project widened our experiences of annotation with the
MATE scheme in a number of respects. First of all, a
number of proposals contained in the MATE guidelines
but not relevant for GNOME, including the suggestions
for dealing with misunderstandings and for incorporated
anaphoric expressions such as clitics, were tested. Sec-
ondly, in this project we are attempting to identify mark-
ables automatically as far a possible, and data are stored
in a standoff format, using a modern annotation tool
(MMAX) for the annotation.
Markup Scheme As MMAX doesn?t support ?link?
elements, and anaphoric information is stored with mark-
ables, it is necessary to use markable attributes to repre-
sent information that would have been encoded as part of
the links. We used a separate attribute to specify the type
of associative relation used by POINTER attribute, and a
SPACE attribute to encode the information stored in the
WHO-BELIEVES attribute of links (see below). In addi-
tion, only one MEMBER and POINTER attributes can be
specified for each markable.
This latter limitation wasn?t much of a problem, given
that the annotation instructions used in VENEX are de-
rived from those developed for GNOME and also attempt
to limit annotators to mark at most one identity and one
bridging relation for each anaphoric expression. The sep-
aration of attributes of links proved, however, a problem,
as annotators often forget to annotate one or the other.
An additional problem is that the version of MMAX we
used (0.92) only allows for one type of markable, mean-
ing that ?unit? elements could not be annotated, and
instead of using separate ?ne? and ?seg? elements for
nominal and non-nominal markables, a single markable
had to be used (see below).8
Misunderstandings The MapTask part of the VENEX
corpus contains numerous examples like (7), where the
differences between Giver and Follower map lead to one
participant believing that two objects are anaphorically
related, while the other participant either is not aware of
this or doesn?t believe this to be the case. We found that
after a few iterations of training, our annotators were able
to handle these cases properly (a more formal evaluation
is underway; we hope to report the results at the meet-
ing). Again, the only problems were caused by the fact
that these attributes had to be added to markables, which
sometimes led to annotators forgetting to set them. (This
was only required in case the default, that an anaphoric
relation was in the common ground of both participants,
didn?t hold.)
4 Discussion
4.1 Aspects of the MATE proposals that have been
proven useful
Our experience with multi-level annotation in GNOME
suggests that standoff is clearly the way to go, allowing
multiple annotators to work on the same files, and sepa-
rating logically independent tasks, but appropriate tools
are required. The annotation tools we have discussed,
such as MMAX, are therefore useful even though they do
8The version of MMAX currently being developed will allow
for multiple markables.
not implement all aspects of the MATE. Knitting back is
also possible with the Discourse API.
Our experience with VENEX suggests that two of the
most beneficial aspects of having a separate ?link? ele-
ment are ones that we had not originally considered: that
they can be used to mark general semantic relations, not
just anaphoric relations (for more complex types of se-
mantic annotation); and that they make it harder for an-
notators to forget to fill in aspects of the annotation. Un-
fortunately, at the moment there is no tool that can be
used to create this type of annotation directly.
4.2 Aspects already reconsidered
Predicative NPs During the GNOME and VENEX anno-
tations we realized that the recommendation not to mark
predicative NPs makes it impossible to do markable iden-
tification automatically. In addition, it?s often difficult
to decide whether an NP is used predicatively or refer-
entially, especially in languages like Italian where sub-
jects in such clauses are often used predicatively (as in
La soluzione e? questa). In GNOME, a new attribute
LF TYPE was introduced to specify the type of seman-
tic object denoted by an NP: term, quant and pred.
The annotators were instructed to concentrate on term-
denoting NPs. The instructions for classifying NPs ac-
cording to their semantic type were based mostly on syn-
tactic information, but the annotation was reliable. In the
instructions for the VENEX annotation, the instructions
for recognizing term-denoting NP are further developed.
Restricting the range of associative relations The
range of associative relations tested in GNOME is much
narrower than those considered in DRAMA, but they can
be annotated reliably, at least in the sense that very few
disagreements are observed. Extending the range of re-
lations to include, for example, attributes (e.g, I am not
going to buy that. The price is too high. or situational
associations (John entered a restaurant. The waiter ap-
proached him immediately) has proven difficult.
Units and Utterances The study of Centering car-
ried in GNOME indicated quite clearly that annotation of
?unit? elements is essential for the study of anaphora.
4.3 Further Revisions
One aspect of the markup scheme that needs revision is
the placement of the semantic relation. One problem we
observed in GNOME is that often the ambiguity is not
simply between two possible antecedents each of which
stands in the same relation to the anaphoric expression,
but between two antecedents which stand in different re-
lations. In the pharmaceutical texts, for example, it is of-
ten unclear whether a particular mention of the medicine
under consideration refers to the generic product, or to the
particular instance that the user has in their hands. In this
case, we would want annotators to mark the anaphoric ex-
pression as IDENT with one object, and ELEMENT of the
other (ELEMENT is also used in GNOME for relations be-
tween instances and types), as follows, but this is not pos-
sible in either the original MATE scheme or in the GNOME
markup scheme:
(6) <ante current="ne1">
<anchor ID="ne2" rel="ident">
<anchor ID="ne3" rel="element">
</ante>
4.4 Open Issues
Ambiguity Offering annotators the opportunity to an-
notate anaphoric ambiguity is essential, especially for an-
notations used to study linguistic phenomena, but raises
serious theoretical and practical problems. A coreference
chain containing such links becomes a coreference (di-
rected) graph, in which each of the paths across the graph
is a potential interpretation. While having multiple paths
is not a problem as far as evaluating the results of an
anaphoric resolver (any path in the graph counts as a valid
solution), it is a serious problems both for scripts attempt-
ing to ensure consistency (e.g., that all references to the
same object are marked as either generic or non-generic?
this is of course impossible when one of the possible an-
tecedents is generic while the other isn?t) as well for an-
notation tools (the problem is of course worsened when
the tool only uses a single attribute to indicate member-
ship in a coreference chain).
Revision A second difficult problem is caused by cases,
common in the MapTask dialogues, in which after a while
a participant realizes that their previous belief that an ob-
ject was identical to another object is mistaken. In these
cases, the participant is arguably revising their previous
beliefs; it is not clear then what should be done with the
annotation of the original anaphoric information.
References
F. Bruneseaux and L. Romary. 1998. Documents
pre?paratoires pour le codage de dialogues multi-
modaux suivant les directives de la TEI. Available at
http://www.loria.fr/?romary/Documents/
index.html.
L. Burnard and C. M. Sperberg-McQueen. 2002. TEI
lite: An introduction to text encoding for interchange.
http://www.tei-c.org/Lite.
H. Cheng, M. Poesio, R. Henschel, and C. Mellish. 2001.
Corpus-based NP modifier generation. In Proc. of the
Second NAACL, Pittsburgh.
Hua Cheng. 2001. Modelling Aggregation Motivated In-
teractions in Descriptive Text Generation. Ph.D. the-
sis, University of Edinburgh.
B. J. Grosz, A. K. Joshi, and S. Weinstein. 1995. Center-
ing: A framework for modeling the local coherence of
discourse. Computational Linguistics, 21(2):202?225.
J. K. Gundel, N. Hedberg, and R. Zacharski. 1993. Cog-
nitive status and the form of referring expressions in
discourse. Language, 69(2):274?307.
J. A. Hawkins. 1978. Definiteness and Indefiniteness.
Croom Helm, London.
I. Heim. 1982. The Semantics of Definite and Indefi-
nite Noun Phrases. Ph.D. thesis, University of Mas-
sachusetts at Amherst.
R. Henschel, H. Cheng, and M. Poesio. 2000. Pronom-
inalization revisited. In Proc. of 18th COLING, Saar-
bruecken, August.
L. Hirschman. 1998. MUC-7 coreference task definition,
version 3.0. In N. Chinchor, editor, In Proc. of the 7th
Message Understanding Conference.
H. Kamp and U. Reyle. 1993. From Discourse to Logic.
D. Reidel, Dordrecht.
N. Karamanis. 2003. Entity coherence for descriptive
text structuring. Ph.D. thesis, University of Edinburgh.
R. Kibble and R. Power. 2000. An integrated frame-
work for text planning and pronominalization. In Proc.
of the International Conference on Natural Language
Generation (INLG), Israel, June.
D. Marcu. 1999. Instructions for manually annotat-
ing the discourse structures of texts. Unpublished
manuscript, USC/ISI, May.
D. McKelvie, A. Isard, A. Mengel, M. B. Moeller,
M. Grosse, and M. Klein. 2001. The MATE work-
bench - an annotation tool for XML corpora. Speech
Communication, 33(1-2):97?112.
C. Mu?ller and M. Strube. 2003. Multi-level annotation in
MMAX. In Proc. of the 4th SIGDIAL, pages 198?207.
R. Passonneau. 1997. Instructions for applying dis-
course reference annotation for multiple applications
(DRAMA). Unpublished manuscript., December.
J. Pearson, R. Stevenson, and M. Poesio. 2000. Pronoun
resolution in complex sentences. In Proc. of AMLAP,
Leiden.
M. Poesio and M. Alexandrov-Kabadjov. 2004. A
general-purpose, off the shelf anaphoric resolver. In
Proc. of LREC, Lisbon, May.
M. Poesio and B. Di Eugenio. 2001. Discourse struc-
ture and anaphoric accessibility. In Ivana Kruijff-
Korbayova? and Mark Steedman, editors, Proc. of the
ESSLLI 2001 Workshop on Information Structure, Dis-
course Structure and Discourse Semantics.
M. Poesio and M. Nissim. 2001. Salience and possessive
NPs: the effect of animacy and pronominalization. In
Proc. of AMLAP (Poster Session).
M. Poesio and N. Nygren-Modjeska. To appear. Focus,
activation, and this-noun phrases: An empirical inves-
tigation. In A. Branco, R. McEnery, and R. Mitkov,
editors, Anaphora Processing. John Benjamins.
M. Poesio and R. Vieira. 1998. A corpus-based investi-
gation of definite description use. Computational Lin-
guistics, 24(2):183?216, June.
M. Poesio, F. Bruneseaux, and L. Romary. 1999. The
MATE meta-scheme for coreference in dialogues in
multiple languages. In M. Walker, editor, Proc. of the
ACL Workshop on Standards and Tools for Discourse
Tagging, pages 65?74.
M. Poesio, H. Cheng, R. Henschel, J. M. Hitzeman,
R. Kibble, and R. Stevenson. 2000. Specifying the
parameters of Centering Theory: a corpus-based eval-
uation using text from application-oriented domains.
In Proc. of the 38th ACL, Hong Kong, October.
M. Poesio, R. Delmonte, A. Bristot, L. Chiran, and
S. Tonelli. 2004a. The VENEX corpus of anaphoric
information in spoken and written Italian. Submitted.
M. Poesio, R. Mehta, A. Maroudas, and J. Hitzeman.
2004b. Learning to solve bridging references. Sub-
mitted.
M. Poesio, R. Stevenson, B. Di Eugenio, and J. M. Hitze-
man. 2004c. Centering: A parametric theory and its
instantiations. Computational Linguistics. To appear.
M. Poesio. 2000a. Annotating a corpus to develop and
evaluate discourse entity realization algorithms: issues
and preliminary results. In Proc. of the 2nd LREC,
pages 211?218, Athens, May.
M. Poesio, 2000b. The GNOME Annotation Scheme
Manual. University of Edinburgh, HCRC and In-
formatics, Scotland, fourth version edition, July.
Available from http://www.hcrc.ed.ac.uk/
? gnome.
M. Poesio. 2003. Associative descriptions and salience.
In Proc. of the EACL Workshop on Computational
Treatments of Anaphora, Budapest.
M. Poesio. 2004. An empirical investigation of defi-
niteness. In S. Kepser, editor, Proc. of the Interna-
tional Conference on Linguistic Evidence, Tu?bingen,
January. University of Tu?bingen, SFB 441.
C. L. Sidner. 1979. Towards a computational theory of
definite anaphora comprehension in English discourse.
Ph.D. thesis, MIT.
M. Strube and U. Hahn. 1999. Functional centering?
grounding referential coherence in information struc-
ture. Computational Linguistics, 25(3):309?344.
K. van Deemter and R. Kibble. 2000. On coreferring:
Coreference in MUC and related annotation schemes.
Computational Linguistics, 26(4):629?637. Squib.
B. L. Webber. 1979. A Formal Approach to Discourse
Anaphora. Garland, New York.
A An example with separate universes and
single-space anaphoric links
(7) a. GIVER: Do_you have diamond_mine.
FOLLOWER: Yes I?ve got a gold_mine.
GIVER: Ah. S--.
FOLLOWER: ....
GIVER: You don?t have diamond_mine though.
FOLLOWER: No. It?s a gold_mine according to
this one.
Presumably that?s the same.
GIVER: Well I?ve got a gold_mine as well
you see. (MT)
b. coref.xml:
<universe ID="common">
<ue ID="ue2"> gold mine </ue>
....
</universe>
<universe ID="GIVER_universe"
modifies="common">
<ue ID="ue1"> diamond mine </ue>
...
</universe>
<universe ID="FOLLOWER_universe"
modifies="common">
....
</universe>
GIVER: Do_you have
<de ID="de_20"> diamond_mine. </de>
FOLLOWER: Yes I?ve got
<de ID="de_21"> a gold_mine. </de>
GIVER: Ah. S--.
FOLLOWER: ....
GIVER: You don?t have
<de ID="de_22"> diamond_mine </de>
though.
FOLLOWER: No.
It?s <de ID="de_23"> a gold_mine</de>
according to this one.
Presumably <de ID="de_24"> that?s </de>
the same.
GIVER: Well I?ve got
<de ID="de_25"> a gold_mine </de>
as well you see.
<link href="coref.xml#id(de_20)" type="ident"
who-believes="G">
<anchor href="coref.xml#id(ue1)"/>
</link>
<link href="coref.xml#id(de_21)" type="ident"
who-believes="F" >
<anchor href="coref.xml#id(ue2)"/>
</link>
<link href="coref.xml#id(de_21)" type="ident"
who-believes="F" >
<anchor href="coref.xml#id(de_20)"/>
</link>
<link href="coref.xml#id(de_22)" type="ident"
who-believes="G">
<anchor href="coref.xml#id(ue1)"/>
</link>
<link href="coref.xml#id(de_22)" type="ident">
<anchor href="coref.xml#id(de_20)"/>
</link>
<link href="coref.xml#id(de_23)" type="ident"
who-believes="F" >
<anchor href="coref.xml#id(ue2)"/>
</link>
<link href="coref.xml#id(de_23)" type="ident"
who-believes="F" >
<anchor href="coref.xml#id(de_22)"/>
</link>
<link href="coref.xml#id(de_24)" type="ident"
who-believes="F" >
<anchor href="coref.xml#id(de_22)" />
</link>
Attribute-Based and Value-Based Clustering: An Evaluation 
Abdulrahman ALMUHAREB and Massimo POESIO 
Department of Computer Science and Centre for Cognitive Science 
University of Essex 
Colchester, United Kingdom, CO4 3SQ 
aalmuh@essex.ac.uk poesio@essex.ac.uk 
 
Abstract 
In most research on concept acquisition from 
corpora, concepts are modeled as vectors of 
relations extracted from syntactic structures.  
In the case of modifiers, these relations often 
specify values of attributes, as in (attr 
red); this is unlike what typically proposed in 
theories of knowledge representation, where 
concepts are typically defined in terms of their 
attributes (e.g., color).  We compared 
models of concepts based on values with 
models based on attributes, using lexical 
clustering as the basis for comparison. We 
find that attribute-based models work better 
than value-based ones, and result in shorter 
descriptions; but that mixed models including 
both the best attributes and the best values 
work best of all. 
1 Introduction 
In most recent research on concept acquisition 
from corpora (e.g., for lexicon construction), 
concepts are viewed as vectors of relations, or 
properties, extracted from syntactic structures 
(Grefenstette, 1993; Lin, 1998; Curran and Moens, 
2002; Kilgarriff, 2003, and many others).  These 
properties often specify values of attributes such as 
color, shape, or size: for example, the vector used 
by Lin (1998) for the concept dog includes the 
property (dog adj-mod brown). (We will 
use the term values here to refer to any modifier.)  
To our knowledge, however, no attempt has been 
made by computational linguists to use the 
attributes themselves in such vectors: i.e., to learn 
that the description of the concept dog includes 
elements such as (dog color) or (dog 
size).  This is surprising when considering that 
most models of concepts in the AI literature are 
based on such attributes (Brachman and Levesque, 
1985). 
Two problems need to be addressed when trying 
to identify concept attributes.  The first problem is 
that values are easier to extract. We found, 
however, that patterns like the X of the dog, 
already used in (Berland and Charniak, 1999; 
Poesio et al 2002) to find part-of relations (using  
techniques derived from those used in (Hearst, 
1998; Caraballo, 1999) to find hyponymy 
relations) are quite effective at finding attributes.  
A second problem might be that instances of such 
patterns are less frequent than those used to extract 
values, even in large corpora such as the British 
National Corpus (BNC).  But this problem, as well, 
is less serious when using the Web as a corpus 
(Kilgarriff and Schuetze, 2003; Keller and Lapata, 
2003; Markert et al submitted).   
We report on two experiments whose goal was 
to test whether identifying attributes leads to better 
lexical descriptions of concepts. We do this by 
comparing the results obtained by using attributes 
or more general modifiers ? that we will simply 
call values ? as elements of concept vectors used 
to identify concept similarities via clustering.  In 
Section 2, we discuss how Web data were used to 
build attribute- and value- based concept vectors, 
and our clustering and evaluation methods.  In 
Section 3, we discuss a first experiment using the 
set of concepts used in (Lund and Burgess, 1996).  
In Section 4, we discuss a second experiment using 
214 concepts from WordNet (Fellbaum, 1998).  In 
Section 5 we return to the notion of attribute.  
2 Methods 
2.1 Using Text Patterns to Build Concept 
Descriptions 
Our techniques for extracting concept 
descriptions are simpler than those used in other 
work in at least two respects.  First of all, we only 
extracted values expressed as nominal modifiers, 
ignoring properties expressed by verbal 
constructions in which the concept occurred as an 
argument (e.g., Lin?s (dog obj-of have)). 
(We originally made this simplification to 
concentrate on the comparison between attributes 
and values (many verbal relations express more 
complex properties), but found that the resulting 
descriptions were still adequate for clustering.) 
Secondly, our data were not parsed or POS-tagged 
prior to extracting concept properties; our patterns 
are word-based.  Full parsing is essential when 
complete descriptions are built (see below) and 
allows the specification of much more general 
patterns (e.g., matching descriptions modified in a 
variety of ways, see below), but is computationally 
much more expensive, particularly when Web data 
are used, as done here. We also found that when 
using the Web, simple text patterns not requiring 
parsing or POS tagging were sufficient to extract 
large numbers of instances of properties with a 
good degree of precision.  
Our methods for extracting 'values' are 
analogous to those used in the previous literature, 
apart from the two simplifications just mentioned:  
i.e., we just consider every nominal modifier as 
expressing a potential property.  The pattern we 
use to extract values is as follows: 
? "[a|an|the] * C [is|was]" 
where C is a concept, and  the wildcard (*) stands 
for  an unspecified value.  The restriction to 
instances containing is or was to ensure that the C 
actually stands for a concept (i.e., avoiding 
modifiers) proved adequate to ensure precision. An 
example of text matching this pattern is: 
? ? an inexpensive car is ? 
The pattern we use for extracting concept 
attributes is based on linguistic tests for attributes 
already discussed, e.g., in (Woods, 1975).  
According to Woods, A is an attribute of C if we 
can say [V is a/the A of C]: e.g., brown is a color 
of dogs.  If no V can be found which is a value of 
A, then A can not be an attribute for the concept C.  
This test only selects attributes that have values, 
and is designed to exclude other functions defined 
over concepts, such as parts.  But some of these 
functions can be (and have been) viewed as 
defining attributes of concepts as well; so for the 
moment we used more general patterns identifying 
all relational nouns taking a particular concept as 
arguments.  (We return on the issue of the 
characterization of attributes below.)  Our pattern 
for attributes is shown below:  
? "the * of the C [is|was]"  
where again C is a concept, but the wildcard 
denotes an unspecified attribute. Again, is/was is 
used to increase precision.  An example of text 
matching this pattern is: 
? ? the price of the car was ? 
Both of the patterns we use satisfy Hearst's 
desiderata for good patterns (Hearst, 1998): they 
are (i) frequent, (ii) precise, and (iii) easy to 
recognize.  Patterns similar to our attribute pattern 
were used by Berland and Charniak (1999) and 
Poesio et al(2002) to find object parts only; after 
collecting their data, Berland and Charniak filtered 
out words ending with "ness", "ing", and "ity", 
because these express qualities of objects, and used 
a ranking method to rank the remaining words.  
(An accuracy of 55% for the top 50 proposed parts 
was reported.)  We found that these patterns can be 
used to collect other sorts of 'attributes', as well. 
2.2 Web Data Collection through Google 
In recent years there has been growing evidence 
that using the Web as a corpus greatly reduces the 
problem of data sparseness, and its size more than 
compensates the lack of balance (e.g., (Keller and 
Lapata, 2003)).  The benefits from using the Web 
over even large corpora like the BNC for 
extracting semantic relations, particularly when 
using simple text patterns, were informally pointed 
out in (Poesio, 2003) and demonstrated more 
systematically by Markert et al(submitted).  These 
findings were confirmed by our experiments.  A 
comparison of numbers of instances of some 
patterns using the Web and the BNC is shown in 
Table 1. 
Pattern Web BNC 
"the * of the *" 23,100,000 208,155
"the * of the * is" 10,900,000 3,627 
"the * of the car is" 26,400 5 
A
tt
ri
bu
te
 
"the * of the hat is" 2,770 1 
"the fast * is" 38,100 3 
"an electronic * is" 120,000 5 
"the * car is" 84,500 24 V
al
ue
 
"the * hat is" 17,100 1 
Table 1:  Comparison of frequencies of some 
patterns in BNC and the Web.  Web frequency is 
based on Google counts 
We collect our data from the Web using the 
Google search engine, accessed via the freely 
available Google Web API1.  The API only allows 
to retrieve the first 1,000 results per search request; 
to overcome this restriction, we use the daterage 
feature of the Google search request.  This feature 
allows the user to fragment the search space into a 
number of periods, hence retrieving only pages that 
have been updated during a specified period. In the 
two experiments presented here, we aimed to 
collect up to 10,000 matches per search request 
using the daterage feature: we divided the search 
space into 100 days starting from January, 1990 
until mid 2004.  (The procedure we used does not 
guarantee collecting all the instances in the 
accessed periods, because if there are more than 
                                                     
1 Google Web API is available on the Web at 
http://www.google.com/apis/ 
1,000 instances in one period, then only the first 
1,000 instances will be collected.) 2 
Our requests to Google take the general form "s1 
* s2" (including the double quotes), where s1 and s2 
are two strings and the wildcard denotes an 
unspecified single word.  For example, the search 
request "a * car is" catches instances such as: [a 
red car is], [a small car is], and [a sport car is]. It 
is worth mentioning that Google does not pay 
attention to punctuation marks; this is one area in 
which parsing would help. 
When receiving results from Google, we do not 
access the actual Web pages, but instead we 
process the snippets that are returned by Google.3 
2.3 Clustering Methods 
The task that we use to compare concept 
descriptions is lexical acquisition via clustering. 
We experimented with clustering systems such as 
COBWEB (Fisher, 1987) and SUBDUE (Cook and 
Holder, 2000) before settling on CLUTO 2.1 
(Karypis, 2002).  CLUTO is a general-purpose 
clustering tool that implements three different 
clustering algorithms: partitional, agglomerative, 
and graph partitioning algorithms.  CLUTO 
produces both flat and hierarchical clusters.  It uses 
a hard clustering technique, where each concept 
can be assigned to only one cluster.  The software 
allows to choose a similarity metric between a set 
including extended Jaccard and cosine.  CLUTO 
was optimized to cluster data of large sizes in a 
reasonable time.  The software also provides 
analysis and visualization tools. 
In this paper, we use extended Jaccard, which 
was found to produce more accurate results than 
the cosine function in similar tasks (Karypis, 2002; 
Curran and Moens, 2003).  In CLUTO, the 
extended Jaccard function works only with the 
graph partitioning algorithm. 
2.4 Evaluation Measures 
We used two types of measures to evaluate the 
clusters produced by CLUTO using the concept 
descriptions discussed above, both of which 
compare the clusters produced by the system to 
model clusters. Accuracy is computed by dividing 
the number of correctly clustered concepts by the 
total number of concepts.  The number of correctly 
clustered concepts is determined by examining 
                                                     
2 Also, registered users of the API can send up to 1,000 
requests per day, but our daily limit was increased by 
Google to 20,000 requests per day. 
3 Snippets are text excerpts captured from the actual 
web pages with embedded HTML tags.  We process the 
snippets by removing the HTML tags and extracting the 
targeted piece of text that was specified in the request. 
each system cluster, finding the class of each 
concept in the model clusters, and determining the 
majority class. The cluster is then labeled with this 
class;   the concepts belonging to it are taken to be 
correctly clustered, whereas the remaining 
concepts are judged to be incorrectly clustered. 
In the contingency table evaluation (Swets, 
1969; Hatzivassiloglou and McKeown, 1993), the 
clusters are converted into two lists (one for the 
system clusters and one for the model clusters) of 
yes-no answers to the question "Does the pair of 
concepts occur in the same cluster?" for each pair 
of concepts.  A contingency table is then built, 
from which recall (R), precision (P), fallout, and F 
measures can be computed.  For example, if the 
model clusters are: (A, B, C) and (D), and the 
system clusters are: (A, B) and (C, D), the yes-no 
lists are as in Table 2, and the contingency table is 
as in Table 3.   
Question Model Answer 
System 
Answer 
Does the pair (A, B) occur in 
the same cluster? Yes Yes 
Does the pair (A, C) occur in 
the same cluster? Yes No 
Does the pair (A, D) occur in 
the same cluster? No No 
Does the pair (B, C) occur in 
the same cluster? Yes No 
Does the pair (B, D) occur in 
the same cluster? No No 
Does the pair (C, D) occur in 
the same cluster? No Yes 
Table 2: Model and the system answers for the 
co-occurrence question 
Model Answer System Answer 
 Yes  No 
a b Yes 
 
1 
 
1 
c d No 
 
2 
 
2 
Table 3: The contingency table 
33.0
ca
aR ?+=  50.0ba
aP =+=  
33.0
db
bFallout ?+=  40.0PR
PR2F ?+
??=  
3 First Experiment: Using a Set of Concepts 
from Lund and Burgess 
One limitation of using Google is that even with 
an increased daily limit of 20,000, it wouldn?t 
really be feasible to attempt to cluster, say, all of 
WordNet 100,000 noun concepts. For this reason, 
we used much smaller sets of concepts in our two 
experiments. The first set alowed us to compare 
our results with those obtained by Lund and 
Burgess (1996); the second set consisted of a larger 
number of concepts from WordNet. 
Lund and Burgess (1996) used a set of 34 
concepts belonging to 3 different classes (animals, 
body parts, and geographical locations) to evaluate 
their method for acquiring lexical representations, 
HAL (Hyperspace Analogue to Language). Lund 
and Burgess were able to correctly cluster all of the 
concepts except for one body part, tooth, which 
was incorrectly clustered with animals.  In this first 
experiment, we used the 34 Lund and Burgess 
concepts plus Italy, horse, and tongue (37 in total) 
to compare value-based and attribute-based 
description when used for clustering, using concept 
descriptions collected using the methods described 
above.   
The input to clustering is a frequency table with 
concepts as rows and values, attributes, or both 
attributes and values as columns.  Each cell in the 
table contains the frequency of co-occurrence 
between the concept and corresponding value or 
attribute.  Before clustering, the frequencies are 
transformed into weighted values using the t test 
(Manning and Schutze, 1999).  (The t test was 
found by Curran and Moens (2002) to be the best 
weighting method.)  The t test formula we used for 
attributes is shown below: 
2
ji
2
jiji
j,i
N
)attribute,concept(C
N
)attribute(C)concept(C
N
)attribute,concept(C
t
???
?
???
? ??
?
 
(1)
where N is the total number of relations, and C is a 
count function.  The values formula is similar. 
We use the CLUTO vcluster command for 
clustering, with parameters: similarity function = 
Extended Jaccard Coefficient, clustering method = 
Graph Partitioning, no. of clusters = 3. 
Vector Size4 
Used Data 
500 1522 3044 4753 4969 
Values  
Only 64.86% 94.59% - - 94.59%
Attributes 
Only 97.30% 97.30% - 97.30% - 
Attributes1522 
and Values1522 
- - 100.00% - - 
Table 4: Clustering accuracy with  values, 
attributes, and their combination, using different 
vector sizes 
                                                     
4 Here, we choose the top k features by their overall 
frequency. 
Table 4 shows the accuracy of the produced 
clusters when using values, attributes, and the 
combination with different vector sizes.  The 
results show that with concept descriptions of 
length 500, attributes (97.30%) are much more 
accurate than values (64.86%).  With vectors of 
size 1522, the accuracy with attributes remains the 
same, while the accuracy with values improves, 
but is still lower than the accuracy with attributes 
(94.59%).  This indicates that attributes have more 
discriminatory power than values: an attribute 
vector of size 500 is sufficient to produce a more 
accurate result than using a value vector of three 
times the size. But perhaps the most interesting 
result is that even though further  increasing the 
size of pure attribute- and value- descriptions (to 
4753 and 4969, respectively) does not improve 
accuracy, perfect accuracy can be obtained by 
using vectors  of length 3044, including the 1522 
best attributes and the 1522 best values.  This 
suggests that while attributes are a good way of 
generalizing across properties, not all properties of 
concepts can be viewed as attribute/value pairs 
(section 5; also (Poesio and Almuhareb, 
submitted)). 
4 Second Experiment: Using a Set of 
Concepts from WordNet 
In order to get a more realistic evaluation and a 
better comparison with work such as (Lin, 1998; 
Curran and Moens, 2002), we also ran a second 
experiment using a larger set of concepts from the 
WordNet noun hierarchy (Fellbaum, 1998).  We 
chose 214 relatively common concepts from 13 
different classes covering a variety of 
subhierarchies (see Appendix A).  Each class 
contains a set of concepts that share a common 
hypernym in the WordNet hierarchy. 
Model Answer Systems Answer 
Yes No 
Yes 1294 503 Boolean 
No 387 20607 
Yes 1117 950 Frequency No 564 20160 
Table 5: The contingency table based on boolean 
and frequency for the combined attributes and 
values 
The frequencies for attributes and values were 
again collected as in the first experiment.  
However, these data were used in a different way.  
In determining the weight, we performed the t test5 
on boolean values instead of the original 
                                                     
5 We consider only positive values of t. 
frequencies6, treating all positive frequencies as 1 
and everything else as 0.  This eliminates the effect 
of variations in frequencies in the original data, the 
intuition being that frequencies do not add to the 
semantics of concepts: what we are interested in is 
the fact that a concept has a given attribute/value, 
regardless of how many times we have 
encountered this fact.  This approach is similar to 
the approach adopted in (Hearst, 1998); see also 
(Curran and Moens, 2002) for a comparison of 
methods dealing with concept vectors based on 
raw frequencies or boolean values.  The 
transformed table is a binary table that contains 
only zeros and ones in its cells.  Table 5 shows the 
contingency table for clusters produced based on 
boolean and frequency for the combined data of 
attributes and values; it shows that boolean data is 
more accurate in the four cases.  
For clustering, as well, we used CLUTO in a 
different way.  Instead of asking CLUTO to 
compute the similarities between the concepts, we 
computed them ourselves, using the version of the 
extended Jaccard similarity function used by 
Curran and Moens, as this version produces better 
results than the one used in CLUTO.  The two 
versions of the extended Jaccard function are 
shown below: 
 
where tm,i and tn,i are the weighted co-occurrence 
values between concept m and concept n with 
attribute/value i, and computed as in equation (1). 
Measures Used 
Data7 Accuracy Recall Precision Fallout F 
Values 
Only 71.96% 58.48% 52.91% 04.14% 55.55%
Attributes 
Only 64.02% 59.90% 53.54% 04.14% 56.54%
Attributes  
And Values 85.51% 76.98% 72.01% 02.38% 74.41%
Table 6: Clustering evaluation based on values, 
attributes, and the combination 
We compute the similarity between each pair of 
concepts, produce a similarity matrix and send it to 
CLUTO for clustering.  We then call the scluster 
                                                     
6 In equation (1), this will effect only C(concepti, 
attributej), other counts will not be effected. 
7 Here, we use full size vectors that contain all the 
features. 
command of CLUTO with the following 
parameters: clustering method = Graph 
Partitioning, no. of clusters = 13. The results of the 
evaluation are shown in Table 6.   
Value-based concept descriptions resulted in 
better clusters than attribute-based when measured 
using Accuracy (71.96% vs. 64.02%), but the other 
measures all indicate that attributes work slightly 
better than values: e.g., F=55.55% for values, 
56.64% for attributes.  The reason for this 
difference is that the Accuracy measure simply 
evaluates if each concept is assigned to its correct 
cluster, while the remaining measures concern 
about the relation between each pair of concepts 
(i.e., if they were assigned to the same cluster or 
not).  But, just as in Experiment 1, the best results 
by any measure are again obtained when using 
concept descriptions containing the best 'attributes' 
and the best 'values'; this time, however, the 
difference is much more significant: Accuracy is 
85.51%, F is 74.41%.  
Model Cluster 
Sy
st
em
 C
lu
st
er
 
B
ui
ld
in
g 
D
is
ea
se
 
V
eh
ic
le
 
Fe
el
in
g 
B
od
y 
Pa
rt
 
Fr
ui
t 
C
re
at
or
 
Pu
bl
ic
at
io
n 
A
ni
m
al
 
Fu
rn
itu
re
 
C
lo
th
 
F.
 R
el
at
io
n 
T
im
e 
1 0 2 0 11 0 0 0 0 0 0 0 0 0 
2 0 0 13 0 0 0 0 0 0 0 0 0 0 
3 0 0 0 0 0 0 0 0 0 0 0 0 17
4 0 0 0 0 0 0 2 0 18 0 0 6 0 
5 2 0 0 1 0 16 0 0 1 0 0 1 0 
6 1 16 0 0 0 0 0 0 0 0 0 0 1 
7 0 0 0 0 0 0 15 0 0 0 0 0 0 
8 0 0 0 0 0 0 0 15 0 0 0 0 0 
9 0 0 0 0 16 0 0 0 0 4 0 0 1 
10 0 0 0 0 0 0 0 0 1 0 0 9 0 
11 1 0 1 0 0 0 0 0 0 6 0 0 0 
12 0 0 0 0 0 0 0 0 0 2 16 0 0 
13 15 0 0 1 0 0 0 1 0 2 0 0 0 
Table 7: The confusion matrix for the clusters 
produced using both attributes and values 
Table 7 shows the confusion matrix for the 
clusters produced using both attributes and values.  
A close inspection of these clusters reveals that 
'furniture' concepts were the less homogeneous 
because they were scattered among four different 
clusters. There are 14 'furniture' concepts; six of 
them (bookcase, cabinet, couch, cradle, desk and 
wardrobe) were grouped in a separate cluster 
which also contains two more concepts (pickup 
and greenhouse).  Four of the concepts (bed, 
lamp, seat, and table) were clustered with 'body 
part' concepts.  Two of the concepts (dresser and 
sofa) were clustered with 'cloth' concepts, and the 
remaining two concepts (chair and lounge) were 
clustered with 'building' concepts. 
?
?
?
?
??+
?
=
+
?
=
i
i,ni,mi,ni,m
i
i,ni,m
nmCLUTO
i
i,ni,m
i
i,ni,m
nmMoens&Curran
))tt(tt(
)tt(
)concept,concept(sim
)tt(
)tt(
)concept,concept(sim
Two points should be noted about the furniture 
concepts.  First, at least two concepts (seat and 
lounge) have more than one sense in WordNet.  
Seat was clustered with body part concepts, which 
is acceptable if we think of seat as "the fleshy part 
of the human body that you sit on" (WordNet, 
sense 2).  The same for lounge, which was 
clustered with buildings, which is consistent with 
its second sense in WordNet: "a public room (as in 
a hotel or airport) with seating where people can 
wait".  This indicates that techniques for 
differentiating between different senses are needed 
? e.g., using a soft clustering technique as in 
(Pereira et al 1993) instead of a hard clustering 
technique.  Second, furniture concepts may not 
have a common prototype that is shared by all of 
the member concepts.  This is a well known 
problem in the prototype theory of concepts 
(Laurence and Margolis, 1999). 
The greater compactness of attribute-based 
representations vs. value-based ones was more 
evident in this second experiment. We collected 
51,045 distinct values and 8,934 distinct attributes; 
the total number of value-concept relations is 
1,026,335, compared to 422,621 attribute-concept 
relations. 
5 Attributes and Values: A discussion 
Although our results suggest that trying to 
identify attributes is beneficial, the notion of 
'attribute' is not completely clear, and has been 
used in widely different ways in Knowledge 
Representation literature.   An attempt of defining 
the notion has been made by Guarino (1992), who 
classifies attributes into relational and non-
relational attributes.  Relational attributes include 
qualities such as color and position, and relational 
roles such as son and spouse.  Non-relational 
attributes include parts such as wheel and engine.  
The Qualia Structure of the Generative Lexicon 
(Pustejovsky, 1991) is another attempt at 
identifying "the essential attributes of an object as 
defined by the lexical item".  Pustejovsky 
identifies four roles: Constitutive Role (Guarino's 
parts), Formal Role (Guarino's qualities), Agentive 
Role (Guarino's relational roles), and Telic Role 
(not included in Guarino's classification). 
Our analysis of the attribute data shows that the 
attributes we found can be mapped in the four roles 
of the Qualia structure.  Table 8 shows how we 
manually mapped the top 50 attributes of the 
concept car to the Qualia roles and the Guarino's 
classes. This mapping is not trivial (e.g., a path is 
not part of a car, and design can be regarded as a 
quality), but a variety of tests may help: 
Morphological and Ontological Tests:  
Dixon (1991) proposed a semantic classification 
for nouns.  According to Dixon, parts are concrete 
concepts and mostly basic noun roots or rarely 
derived from verbs, while qualities are abstract 
concepts and many of them are basic noun roots or 
derived from adjectives, some derived from stems, 
and few derived from verbs.  Our observations also 
suggest that telic attributes are usually derived 
from verbs. 
Attributes Test:  Since attributes can also be 
viewed as concepts (e.g., in WordNet), they 
themselves should have some shared attributes.  
For example: since parts are concrete objects they 
should share attributes such as size, length, and 
geometry.  Also, since qualities usually can be 
assigned values (e.g. age (25)), then they should 
share attributes such as range and average. 
Question Type Test:  Different types of 
attributes tend to occur with different types of 
questions.  For example, relational role attributes 
tend to occur with who-questions like "Who is the 
driver of the car?" and "Who is the manufacturer of 
the car?" 
Guarino 
Class 
Qualia 
Role Car Attributes 
Part Constitutive Role 
front, rear, interior, inside, side, 
body, trunk, exterior, underside, 
hood, back, nose, roof, engine, 
frame, floor, rest, silhouette, 
backseat, wheelbase, battery, 
chassis, path 
Quality Formal Role 
speed, value, weight, price, 
velocity, color, condition, 
momentum, convenience, 
propulsion, look, inertia, state, 
model, history, balance, motion, 
performance 
Relational 
Role 
Agentive 
Role driver, owner 
- Telic Role8 
handling, use, search, design, 
benefit 
Table 8: The classification of the top 50 
attributes of the concept car 
In future work, we plan to use some of these 
tests to classify attributes, and possibly filter some 
of them; this might improve the discrimination 
power of attributes. Also, concepts may share 
certain Qualia, but differ in other respects: for 
example, the chair concept and the man concept 
share some parts (e.g., arm, back, leg, and seat) 
and even some qualities (e.g., color, size, and 
shape) but differ in other levels (i.e., Agentive 
Role, and Telic Role). 
                                                     
8 Telic roles define purposes, functions, and activities 
that are related to the concept. Some valid telic roles for 
the concept car would be: driving, selling, and buying. 
6 Conclusions 
Simple text patterns were used to automatically 
extract both basic value-based and attribute-based 
concept descriptions for clustering purposes. Our 
preliminary results suggest, first of all, that when 
large amounts of data such as the Web are 
accessed, these simple patterns may be sufficient to 
compute descriptions rich enough to discriminate 
quite well, at least with small sets of concepts 
belonging to clearly distinct classes. Secondly, we 
found that even though attributes are fewer than 
values, attribute-based descriptions need not be as 
long as value-based ones to achieve as good or 
better results. Finally, we found that the best 
descriptions included both attributes and more 
general properties. We plan to extend this work 
both by refining our notion of attribute and by 
using more sophisticated patterns working off the 
output of a parser. 
7 Acknowledgement 
Abdulrahman Almuhareb is supported by King 
Abdulaziz City for Science and Technology 
(KACST), Riyadh, Saudi Arabia. We want to 
thank Google for making their Web API available 
to the research community and George Karypis for 
the CLUTO clustering toolkit. 
References 
M. Berland and E. Charniak. 1999. Finding parts in 
very large corpora. In Proc. of the 37th  ACL, 
pages 57?64, University of Maryland. 
R. J. Brachman and H. J. Levesque, editors. 1985. 
Reading in Knowledge Representation. Morgan 
Kaufmann, California. 
S. A. Caraballo. 1999. Automatic construction of a 
hypernym-labeled noun hierarchy from text. In 
Proc. of  the 37th  ACL. 
D. J. Cook and L. B. Holder. 2000. Graph-based 
data mining. IEEE Intelligent Systems, 15(2), 32-
41. 
J. R. Curran and M. Moens. 2002. Improvements 
in automatic thesaurus extraction. In Proc. of the 
ACL Workshop on Unsupervised Lexical 
Acquisition, pages 59?66.  
R. M. W. Dixon. 1991. A New Approach to 
English Grammar, on Semantic Principles. 
Clarendon Press, Oxford. 
C. Fellbaum, editor. 1998. WordNet: An electronic 
lexical database. The MIT Press. 
D. H. Fisher. 1987. Knowledge acquisition via 
incremental conceptual clustering. Machine 
Learning, 2:139?172. 
G. Grefenstette. 1993. SEXTANT: Extracting 
semantics from raw text implementation details. 
Heuristics: The Journal of Knowledge 
Engineering.  
N. Guarino. 1992. Concepts, attributes and 
arbitrary relations: some linguistic and 
ontological criteria for structuring knowledge 
base. Data and Knowledge Engineering, 8, pages 
249?261. 
V. Hatzivassiloglou and K. McKeown. 1993. 
Towards the automatic identification of 
adjectival scales: clustering adjectives according 
to meaning. In Proc. of the 31st ACL, pages 172?
182. 
M. A. Hearst. 1998. Automated discovery of 
WordNet relations. In C. Fellbaum, editor, 
WordNet: An Electronic Lexical Database. MIT 
Press. 
G. Karypis. 2002. CLUTO: A clustering toolkit. 
Technical Report 02-017, University of 
Minnesota. Available at URL: http://www-
users.cs.umn.edu/~karypis/cluto/. 
F. Keller and M. Lapata. 2003. Using the Web to 
obtain frequencies for unseen bigrams. 
Computational Linguistics, 29(3). 
A. Kilgarriff and H. Schuetze. 2003. Introduction 
to the special issue of Computational Linguistics 
on the web as a corpus. Computational 
Linguistics. 
A. Kilgarriff. 2003. Thesauruses for Natural 
Language Processing. In Proc. of the IEEE 2003 
International Conference on Natural Language 
Processing and Knowledge Engineering (NLP-
KE'03), Beijing. 
S. Laurence and E. Margolis. 1999. Concepts and 
Cognitive Science. In E. Margolis and S. 
Laurence, editors, Concepts: Core Readings. 
Cambridge, MA., Bradford Books/MIT Press, 
pages 3-81. 
D. Lin. 1998. Automatic retrieval and clustering of 
similar words. In Proc. of COLING-ACL, 768-
774. 
K. Lund and C. Burgess. 1996. Producing high-
dimensional semantic spaces from lexical co-
occurrence. Behavior Research Methods, 
Instrumentation, and Computers, 28,  203-208.   
C. D. Manning and H. Schuetze. 1999. 
Foundations of Statistical NLP. MIT Press. 
K. Markert, M. Nissim, and N. Modjeska. 2004. 
Comparing Knowledge Sources for Nominal 
Anaphora Resolution. Submitted. 
F. Pereira, N. Tishby, and L. Lee. 1993. 
Distributional clustering of English words. In 
Proc. of the 31st ACL, pages 183-190, 
Columbus, Ohio. 
M. Poesio and A. Almuhareb. 2004. Feature-based 
vs. Property-based KR: An Empirical 
Perspective. Submitted. 
M. Poesio, T. Ishikawa, S. Walde, and R. Vieira. 
2002. Acquiring lexical knowledge for anaphora 
resolution. In Proc. of  LREC, Las Palmas, June. 
M. Poesio. 2003. Associative descriptions and 
salience. In Proc. of the EACL Workshop on 
Computational Treatments of Anaphora, 
Budapest. 
J. Pustejovsky. 1991. The generative lexicon. 
Computational Linguistics, 17(4), pages 409-
441. 
J. A. Swets. 1969. Effectiveness of Information 
Retrieval Methods. American Documentation, 
20, pages 72-89. 
W. A. Woods. 1975. What?s in a link: Foundations 
for semantic networks. In Daniel G. Bobrow and 
Alan M. Collins, editors, Representation and 
Understanding: Studies in Cognitive Science, 
pages 35-82. Academic Press, New York. 
Appendix A. The 214 Concepts from the 13 WordNet Classes Used in Experiment 2 
Class Concepts 
Animal bear, bull, camel, cat, cow, deer, dog, elephant, horse, kitten, lion, monkey, mouse, oyster, puppy, rat, sheep, tiger, turtle, zebra 
Building abattoir, center, clubhouse, dormitory, greenhouse, hall, hospital, hotel, house, inn, library, nursery, restaurant, school, skyscraper, tavern, theater, villa, whorehouse 
Cloth pants, blouse, coat, costume, gloves, hat, jacket, jeans, neckpiece, pajamas, robe, scarf, shirt, suit, trousers, uniform 
Creator architect, artist, builder, constructor, craftsman, designer, developer, farmer, inventor, maker, manufacture, musician, originator, painter, photographer, producer, tailor 
Disease acne, anthrax, arthritis, asthma, cancer, cholera, cirrhosis, diabetes, eczema, flu, glaucoma, hepatitis, leukemia, malnutrition, meningitis, plague, rheumatism, smallpox 
Feeling anger, desire, fear, happiness, joy, love, pain, passion, pleasure, sadness, sensitivity, shame, wonder 
Fruit apple, banana, berry, cherry, grape, kiwi, lemon, mango, melon, olive, orange, peach, pear, pineapple, strawberry, watermelon  
Furniture bed, bookcase, cabinet, chair, couch, cradle, desk, dresser, lamp, lounge, seat, sofa, table, wardrobe 
Body Part ankle, arm, ear, eye, face, finger, foot, hand, head, leg, nose, shoulder, toe, tongue, tooth, wrist 
Publication atlas, book, booklet, brochure, catalog, cookbook, dictionary, encyclopedia, handbook, journal, magazine, manual, phonebook, reference, textbook, workbook  
Family 
Relation  
boy, child, cousin, daughter, father, girl, grandchild, grandfather, grandmother, husband, 
kid, mother, offspring, sibling, son, wife  
Time century, decade, era, evening, fall, hour, month, morning, night, overtime, quarter, season, semester, spring, summer, week, weekend, winter, year 
Vehicle aircraft, airplane, automobile, bicycle, boat, car, cruiser, helicopter, motorcycle, pickup, rocket, ship, truck, van 
 
Identifying Broken Plurals in Unvowelised Arabic Text 
 
Abduelbaset Goweder 
University of Essex 
Dept. of Computer 
Science 
Wivenhoe Park,  
Colchester  CO4 3SQ, 
UK 
agowed@essex.ac.uk 
Massimo Poesio 
University of Essex 
Dept. of Computer 
Science 
Wivenhoe Park, 
Colchester  CO4 3SQ, 
UK 
poesio@essex.ac.uk 
Anne De Roeck 
The Open University 
Dept. of Computing 
Walton Hall, Milton 
Keynes 
Buckinghamshire, MK7 
6AA, UK 
A.DeRoeck@open.ac.uk 
Jeff Reynolds 
University of Essex 
Dept. of Computer 
Science 
Wivenhoe Park, 
Colchester  CO4 3SQ, 
UK 
reynt@essex.ac.uk 
 
Abstract 
Irregular (so-called broken) plural identification 
in modern standard Arabic is a problematic issue 
for information retrieval (IR) and language 
engineering applications, but their effect on the 
performance of IR has never been examined. 
Broken plurals (BPs) are formed by altering the 
singular (as in English: tooth  teeth) through 
an application of interdigitating patterns on 
stems, and singular words cannot be recovered 
by standard affix stripping stemming techniques. 
We developed several methods for BP detection, 
and evaluated them using an unseen test set. We 
incorporated the BP detection component into a 
new light-stemming algorithm that conflates both 
regular and broken plurals with their singular 
forms. We also evaluated the new light-stemming 
algorithm within the context of information 
retrieval, comparing its performance with other 
stemming algorithms. 
1. Introduction 
Broken plurals constitute ~10% of texts in large 
Arabic corpora (Goweder and De Roeck, 2001), and 
~41% of plurals (Boudelaa and Gaskell, 2002). 
Detecting broken plurals is therefore an important 
issue for light-stemming algorithms developed for 
applications such as information retrieval, yet the 
effect of broken plural identification on the 
performance of information retrieval systems has 
not been examined. We present several methods for 
BP detection, and evaluate them using an unseen 
test set containing 187,309 words. We also 
developed a new light-stemming algorithm 
incorporating a BP recognition component, and 
evaluated it within an information retrieval context, 
comparing its performance with other stemming 
algorithms.  
We give a brief overview of Arabic in Section 2. 
Several approaches to BP detection are discussed in 
Section 3, and their evaluation in Section 4. In 
Section 5, we present an improved light stemmer 
and its evaluation. Finally in Section 6, our 
conclusions are summarised.  
2. Arabic Morphology and its Number 
System 
Arabic is a heavily inflected language. Its 
grammatical system is traditionally described in 
terms of a root-and-pattern structure, with about 
10,000 roots (Ali, 1988). Roots such as drs () 
and ktb () are listed alphabetically in standard 
Arabic dictionaries like the Wehr-Cowan (Beesley, 
1996). The root is the most basic verb form. Roots 
are categorized into: triliteral, quadriliteral, or rarely 
pentaliteral. Most words are derived from a finite set 
of roots formed by adding diacritics1 or affixes 
(prefixes, suffixes, and infixes) through an 
application of fixed patterns which are templates to 
help in deriving inflectional and derivational forms 
of a word.  Theoretically, several hundreds of 
Arabic words can be derived from a single root. 
Traditional Arab grammarians describe Arabic 
morphology in terms of patterns associated with the 
basic root f3l (	, ?to do?)- where f, 3, and l are like 
wildcards in regular expressions: the letter f (
 
,?pronounced fa?) represents the first consonant 
(sometimes called a radical), the letter 3 ( , 
?pronounced ain?) represents the second, and the 
letter l ( , ?pronounced lam?) represents the third 
                                                                
1
 Special characters which are superscript or subscript marks 
added to the word. 
respectively. Adding affixes to the basic root f3l 
(	, ?to do?) allows additional such patterns to be 
formed. For instance, adding the letter Alef () as a 
prefix to the basic root f3l (	, ?to do?) we get the 
pattern Af3l (	) which is used to form words such 
as: anhr (, ?rivers?), arjl (, ?legs?), and asqf 
(, ?ceilings?). Some examples of the word 
patterns are Yf3l (), Mf3Wl (), Af3Al (	Proceedings of the Workshop on Frontiers in Corpus Annotation II: Pie in the Sky, pages 76?83,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
The Reliability of Anaphoric Annotation, Reconsidered: Taking Ambiguity
into Account
Massimo Poesio and Ron Artstein
University of Essex,
Language and Computation Group / Department of Computer Science
United Kingdom
Abstract
We report the results of a study of the
reliability of anaphoric annotation which
(i) involved a substantial number of naive
subjects, (ii) used Krippendorff?s ? in-
stead of K to measure agreement, as re-
cently proposed by Passonneau, and (iii)
allowed annotators to mark anaphoric ex-
pressions as ambiguous.
1 INTRODUCTION
We tackle three limitations with the current state of
the art in the annotation of anaphoric relations. The
first problem is the lack of a truly systematic study of
agreement on anaphoric annotation in the literature:
none of the studies we are aware of (Hirschman,
1998; Poesio and Vieira, 1998; Byron, 2003; Poe-
sio, 2004) is completely satisfactory, either because
only a small number of coders was involved, or
because agreement beyond chance couldn?t be as-
sessed for lack of an appropriate statistic, a situation
recently corrected by Passonneau (2004). The sec-
ond limitation, which is particularly serious when
working on dialogue, is our still limited understand-
ing of the degree of agreement on references to ab-
stract objects, as in discourse deixis (Webber, 1991;
Eckert and Strube, 2001).
The third shortcoming is a problem that affects all
types of semantic annotation. In all annotation stud-
ies we are aware of,1 the fact that an expression may
not have a unique interpretation in the context of its
1The one exception is Rosenberg and Binkowski (2004).
occurrence is viewed as a problem with the anno-
tation scheme, to be fixed by, e.g., developing suit-
ably underspecified representations, as done partic-
ularly in work on wordsense annotation (Buitelaar,
1998; Palmer et al, 2005), but also on dialogue act
tagging. Unfortunately, the underspecification solu-
tion only genuinely applies to cases of polysemy, not
homonymy (Poesio, 1996), and anaphoric ambigu-
ity is not a case of polysemy. Consider the dialogue
excerpt in (1):2 it?s not clear to us (nor was to our
annotators, as we?ll see below) whether the demon-
strative that in utterance unit 18.1 refers to the ?bad
wheel? or ?the boxcar?; as a result, annotators? judg-
ments may disagree ? but this doesn?t mean that the
annotation scheme is faulty; only that what is being
said is genuinely ambiguous.
(1) 18.1 S: ....
18.6 it turns out that the boxcar
at Elmira
18.7 has a bad wheel
18.8 and they?re .. gonna start
fixing that at midnight
18.9 but it won?t be ready until 8
19.1 M: oh what a pain in the butt
This problem is encountered with all types of anno-
tation; the view that all types of disagreement indi-
cate a problem with the annotation scheme?i.e., that
somehow the problem would disappear if only we
could find the right annotation scheme, or concen-
trate on the ?right? types of linguistic judgments?
is, in our opinion, misguided. A better approach
2This example, like most of those in the rest of the paper, is
taken from the first edition of the TRAINS corpus collected at
the University of Rochester (Gross et al, 1993). The dialogues
are available at ftp://ftp.cs.rochester.edu/pub/
papers/ai/92.tn1.trains_91_dialogues.txt.
76
is to find when annotators disagree because of in-
trinsic problems with the text, or, even better, to
develop methods to identify genuinely ambiguous
expressions?the ultimate goal of this work.
The paper is organized as follows. We first briefly
review previous work on anaphoric annotation and
on reliability indices. We then discuss our experi-
ment with anaphoric annotation, and its results. Fi-
nally, we discuss the implications of this work.
2 ANNOTATING ANAPHORA
It is not our goal at this stage to propose a new
scheme for annotating anaphora. For this study we
simply developed a coding manual for the purposes
of our experiment, broadly based on the approach
adopted in MATE (Poesio et al, 1999) and GNOME
(Poesio, 2004), but introducing new types of annota-
tion (ambiguous anaphora, and a simple form of dis-
course deixis) while simplifying other aspects (e.g.,
by not annotating bridging references).
The task of ?anaphoric annotation? discussed here
is related, although different from, the task of an-
notating ?coreference? in the sense of the so-called
MUCSS scheme for the MUC-7 initiative (Hirschman,
1998). This scheme, while often criticized, is still
widely used, and has been the basis of coreference
annotation for the ACE initiative in the past two
years. It suffers however from a number of prob-
lems (van Deemter and Kibble, 2000), chief among
which is the fact that the one semantic relation ex-
pressed by the scheme, ident, conflates a number
of relations that semanticists view as distinct: be-
sides COREFERENCE proper, there are IDENTITY
ANAPHORA, BOUND ANAPHORA, and even PRED-
ICATION. (Space prevents a fuller discussion and
exemplification of these relations here.)
The goal of the MATE and GNOME schemes (as
well of other schemes developed by Passonneau
(1997), and Byron (2003)) was to devise instructions
appropriate for the creation of resources suitable for
the theoretical study of anaphora from a linguis-
tic / psychological perspective, and, from a compu-
tational perspective, for the evaluation of anaphora
resolution and referring expressions generation. The
goal is to annotate the discourse model resulting
from the interpretation of a text, in the sense both of
(Webber, 1979) and of dynamic theories of anaphora
(Kamp and Reyle, 1993). In order to do this, annota-
tors must first of all identify the noun phrases which
either introduce new discourse entities (discourse-
new (Prince, 1992)) or are mentions of previously
introduced ones (discourse-old), ignoring those that
are used predicatively. Secondly, annotators have
to specify which discourse entities have the same
interpretation. Given that the characterization of
such discourse models is usually considered part
of the area of the semantics of anaphora, and that
the relations to be annotated include relations other
than Sidner?s (1979) COSPECIFICATION, we will use
the term ANNOTATION OF ANAPHORA for this task
(Poesio, 2004), but the reader should keep in mind
that we are not concerned only with nominal expres-
sions which are lexically anaphoric.
3 MEASURING AGREEMENT ON
ANAPHORIC ANNOTATION
The agreement coefficient which is most widely
used in NLP is the one called K by Siegel and Castel-
lan (1988). Howewer, most authors who attempted
anaphora annotation pointed out that K is not appro-
priate for anaphoric annotation. The only sensible
choice of ?label? in the case of (identity) anaphora
are anaphoric chains (Passonneau, 2004); but ex-
cept when a text is very short, few annotators will
catch all mentions of the same discourse entity?most
forget to mark a few, which means that agreement
as measured with K is always very low. Follow-
ing Passonneau (2004), we used the coefficient ? of
Krippendorff (1980) for this purpose, which allows
for partial agreement among anaphoric chains.3
3.1 Krippendorf?s alpha
The ? coefficient measures agreement among a set
of coders C who assign each of a set of items I to
one of a set of distinct and mutually exclusive cat-
egories K; for anaphora annotation the coders are
the annotators, the items are the markables in the
text, and the categories are the emerging anaphoric
chains. The coefficient measures the observed dis-
agreement between the coders Do, and corrects for
3We also tried a few variants of ?, but these differed from ?
only in the third to fifth significant digit, well below any of the
other variables that affected agreement. In the interest of space
we only report here the results obtained with ?.
77
chance by removing the amount of disagreement ex-
pected by chance De. The result is subtracted from 1
to yield a final value of agreement.
? = 1?
Do
De
As in the case of K, the higher the value of ?,
the more agreement there is between the annotators.
? = 1 means that agreement is complete, and ? = 0
means that agreement is at chance level.
What makes ? particularly appropriate for
anaphora annotation is that the categories are not
required to be disjoint; instead, they must be or-
dered according to a DISTANCE METRIC?a func-
tion d from category pairs to real numbers that spec-
ifies the amount of dissimilarity between the cate-
gories. The distance between a category and itself is
always zero, and the less similar two categories are,
the larger the distance between them. Table 1 gives
the formulas for calculating the observed and ex-
pected disagreement for ?. The amount of disagree-
ment for each item i ? I is the arithmetic mean of the
distances between the pairs of judgments pertaining
to it, and the observed agreement is the mean of all
the item disagreements. The expected disagreement
is the mean of the distances between all the judg-
ment pairs in the data, without regard to items.
Do =
1
ic(c?1) ?i?I ?k?K ?k??K niknik?dkk?
De =
1
ic(ic?1) ?k?K ?k??K nknk?dkk?
c number of coders
i number of items
nik number of times item i is classified in category k
nk number of times any item is classified in category k
dkk? distance between categories k and k?
Table 1: Observed and expected disagreement for ?
3.2 Distance measures
The distance metric is not part of the general defini-
tion of ?, because different metrics are appropriate
for different types of categories. For anaphora anno-
tation, the categories are the ANAPHORIC CHAINS:
the sets of markables which are mentions of the
same discourse entity. Passonneau (2004) proposes
a distance metric between anaphoric chains based on
the following rationale: two sets are minimally dis-
tant when they are identical and maximally distant
when they are disjoint; between these extremes, sets
that stand in a subset relation are closer (less distant)
than ones that merely intersect. This leads to the fol-
lowing distance metric between two sets A and B.
dAB =
?
???
???
0 if A = B
1/3 if A ? B or B ? A
2/3 if A?B 6= /0, but A 6? B and B 6? A
1 if A?B = /0
We also tested distance metrics commonly used
in Information Retrieval that take the size of the
anaphoric chain into account, such as Jaccard and
Dice (Manning and Schuetze, 1999), the ratio-
nale being that the larger the overlap between two
anaphoric chains, the better the agreement. Jac-
card and Dice?s set comparison metrics were sub-
tracted from 1 in order to get measures of distance
that range between zero (minimal distance, identity)
and one (maximal distance, disjointness).
dAB = 1?
|A?B|
|A?B|
(Jaccard)
dAB = 1?
2 |A?B|
|A|+ |B|
(Dice)
The Dice measure always gives a smaller distance
than the Jaccard measure, hence Dice always yields
a higher agreement coefficient than Jaccard when
the other conditions remain constant. The difference
between Dice and Jaccard grows with the size of the
compared sets. Obviously, the Passonneau measure
is not sensitive to the size of these sets.
3.3 Computing the anaphoric chains
Another factor that affects the value of the agree-
ment coefficient?in fact, arguably the most impor-
tant factor?is the method used for constructing from
the raw annotation data the ?labels? used for agree-
ment computation, i.e., the anaphoric chains. We
experimented with a number of methods. How-
ever, since the raw data are highly dependent on
the annotation scheme, we will postpone discussing
our chain construction methods until after we have
described our experimental setup and annotation
scheme. We will also discuss there how compar-
isons are made when an ambiguity is marked.
78
4 THE ANNOTATION STUDY
4.1 The Experimental Setup
Materials. The text annotated in the experiment
was dialogue 3.2 from the TRAINS 91 corpus. Sub-
jects were trained on dialogue 3.1.
Tools. The subjects performed their annotations
on Viglen Genie workstations with LG Flatron mon-
itors running Windows XP, using the MMAX 2 anno-
tation tool (Mu?ller and Strube, 2003).4
Subjects. Eighteen paid subjects participated in
the experiment, all students at the University of Es-
sex, mostly undergraduates from the Departments of
Psychology and Language and Linguistics.
Procedure. The subjects performed the experi-
ment together in one lab, each working on a separate
computer. The experiment was run in two sessions,
each consisting of two hour-long parts separated by
a 30 minute break. The first part of the first session
was devoted to training: subjects were given the an-
notation manual and taught how to use the software,
and then annotated the training text together. After
the break, the subjects annotated the first half of the
dialogue (up to utterance 19.6). The second session
took place five days later. In the first part we quickly
pointed out some problems in the first session (for
instance reminding the subjects to be careful during
the annotation), and then immediately the subjects
annotated the second half of the dialogue, and wrote
up a summary. The second part of the second session
was used for a separate experiment with a different
dialogue and a slightly different annotation scheme.
4.2 The Annotation Scheme
MMAX 2 allows for multiple types of markables;
markables at the phrase, utterance, and turn lev-
els were defined before the experiment. All noun
phrases except temporal ones were treated as phrase
markables (Poesio, 2004). Subjects were instructed
to go through the phrase markables in order (us-
ing MMAX 2?s markable browser) and mark each
of them with one of four attributes: ?phrase? if it
referred to an object which was mentioned earlier
in the dialogue; ?segment? if it referred to a plan,
4Available from http://mmax.eml-research.de/
event, action, or fact discussed earlier in the dia-
logue; ?place? if it was one of the five railway sta-
tions Avon, Bath, Corning, Dansville, and Elmira,
explicitly mentioned by name; or ?none? if it did
not fit any of the above criteria, for instance if it re-
ferred to a novel object or was not a referential noun
phrase. (We included the attribute ?place? in order
to avoid having our subjects mark pointers from ex-
plicit place names. These occur frequently in the
dialogue?49 of the 151 markables?but are rather un-
interesting as far as anaphora goes.) For markables
designated as ?phrase? or ?segment? subjects were
instructed to set a pointer to the antecedent, a mark-
able at the phrase or turn level. Subjects were in-
structed to set more than one pointer in case of am-
biguous reference. Markables which were not given
an attribute or which were marked as ?phrase? or
?segment? but did not have an antecedent specified
were considered to be data errors; data errors oc-
curred in 3 out of the 151 markables in the dialogue,
and these items were excluded from the analysis.
We chose to mark antecedents using MMAX 2?s
pointers, rather than its sets, because pointers allow
us to annotate ambiguity: an ambiguous phrase can
point to two antecedents without creating an asso-
ciation between them. In addition, MMAX 2 makes
it possible to restrict pointers to a particular level.
In our scheme, markables marked as ?phrase? could
only point to phrase-level antecedents while mark-
ables marked as ?segment? could only point to turn-
level antecedents, thus simplifying the annotation.
As in previous studies (Eckert and Strube, 2001;
Byron, 2003), we only allowed a constrained form
of reference to discourse segments: our subjects
could only indicate turn-level markables as an-
tecedents. This resulted in rather coarse-grained
markings, especially when a single turn was long
and included discussion of a number of topics. In
a separate experiment we tested a more compli-
cated annotation scheme which allowed a more fine-
grained marking of reference to discourse segments.
4.3 Computing anaphoric chains
The raw annotation data were processed using
custom-written Perl scripts to generate coreference
chains and calculate reliability statistics.
The core of Passonneau?s proposal (Passonneau,
2004) is her method for generating the set of dis-
79
tinct and mutually exclusive categories required by
? out of the raw data of anaphoric annotation. Con-
sidering as categories the immediate antecedents
would mean a disagreement every time two anno-
tators mark different members of an anaphoric chain
as antecedents, while agreeing that these different
antecedents are part of the same chain. Passonneau
proposes the better solution to view the emerging
anaphoric chains themselves as the categories. And
in a scheme where anaphoric reference is unambigu-
ous, these chains are equivalence classes of mark-
ables. But we have a problem: since our annotation
scheme allows for multiple pointers, these chains
take on various shapes and forms.
Our solution is to associate each markable m with
the set of markables obtained by following the chain
of pointers from m, and then following the pointers
backwards from the resulting set. The rationale for
this method is as follows. Two pointers to a single
markable never signify ambiguity: if B points to A
and C points to A then B and C are cospecificational;
we thus have to follow the links up and then back
down. However, two pointers from a single mark-
able may signify ambiguity, so we should not follow
an up-link from a markable that we arrived at via a
down-link. The net result is that an unambiguous
markable is associated with the set of all markables
that are cospecificational with it on one of their read-
ings; an ambiguous markable is associated with the
set of all markables that are cospecificational with at
least one of its readings. (See figure 1.)
Unambiguous
A
B C
 
 
@
@I
A 7? {A,B,C}
B 7? {A,B,C}
C 7? {A,B,C}
Ambiguous
D E
F
@
@I
 
 
D 7? {D,F}
E 7? {E,F}
F 7? {D,E,F}
Figure 1: Anaphoric chains
This method of chain construction also allows to
resolve apparent discrepancies between reference to
phrase-level and turn-level markables. Take for ex-
ample the snippet below: many annotators marked
a pointer from the demonstrative that in utterance
unit 4.2 to turn 3; as for that in utterance unit 4.3,
some marked a pointer to the previous that, while
others marked a pointer directly to turn 3.
(2) 3.1 M: and while it?s there it
should pick up the tanker
4.1 S: okay
4.2 and that can get
4.3 we can get that done by
three
In this case, not only do the annotators mark differ-
ent direct antecedents for the second that; they even
use different attributes??phrase? when pointing to a
phrase antecedent and ?segment? when pointing to
a turn. Our method of chain construction associates
both of these markings with the same set of three
markables ? the two that phrases and turn 3 ? captur-
ing the fact that the two markings are in agreement.5
4.4 Taking ambiguity into account
The cleanest way to deal with ambiguity would be
to consider each item for which more than one an-
tecedent is marked as denoting a set of interpreta-
tions, i.e., a set of anaphoric chains (Poesio, 1996),
and to develop methods for comparing such sets
of sets of markables. However, while our instruc-
tions to the annotators were to use multiple point-
ers for ambiguity, they only followed these instruc-
tions for phrase references; when indicating the ref-
erents of discourse deixis, they often used multi-
ple pointers to indicate that more than one turn had
contributed to the development of a plan. So, for
this experiment, we simply used as the interpreta-
tion of markables marked as ambiguous the union
of the constituent interpretations. E.g., a markable E
marked as pointing both to antecedent A, belonging
to anaphoric chain {A,B}, and to antecedent C, be-
longing to anaphoric chain {C,D}, would be treated
by our scripts as being interpreted as referring to
anaphoric chain {A,B,C,D}.
5 RESULTS
5.1 Agreement on category labels
The following table reports for each of the four cate-
gories the number of cases (in the first half) in which
5It would be preferable, of course, to get the annotators to
mark such configurations in a uniform way; this however would
require much more extensive training of the subjects, as well as
support which is currently unavailable from the annotation tool
for tracking chains of pointers.
80
a good number (18, 17, 16) annotators agreed on a
particular label?phrase, segment, place, or none?or
no annotators assigned a particular label to a mark-
able. (The figures for the second half are similar.)
Number of judgments 18 17 16 0
phrase 10 3 1 30
segment 1 52
place 16 1 1 54
none 10 5 1 29
Table 2: Cases of good agreement on categories
In other words, in 49 cases out of 72 at least 16
annotators agreed on a label.
5.2 Explicitly annotated ambiguity, and its
impact on agreement
Next, we attempted to get an idea of the amount
of explicit ambiguity?i.e., the cases in which coders
marked multiple antecedents?and the impact on re-
liability resulting by allowing them to do this. In
the first half, 15 markables out of 72 (20.8%) were
marked as explicitly ambiguous by at least one an-
notator, for a total of 55 explicit ambiguity mark-
ings (45 phrase references, 10 segment references);
in the second, 8/76, 10.5% (21 judgments of ambi-
guity in total). The impact of these cases on agree-
ment can be estimated by comparing the values of
K and ? on the antecedents only, before the con-
struction of cospecification chains. Recall that the
difference between the coefficients is that K does
not allow for partial disagreement while ? gives it
some credit. Thus if one subject marks markable A
as antecedent of an expression, while a second sub-
ject marks markables A and B, K will register a dis-
agreement while ? will register partial agreement.
Table 3 compares the values of K and ?, computed
separately for each half of the dialogue, first with
all the markables, then by excluding ?place? mark-
ables (agreement on marking place names was al-
most perfect, contributing substantially to overall
agreement). The value of ? is somewhat higher than
that of K, across all conditions.
5.3 Agreement on anaphora
Finally, we come to the agreement values obtained
by using ? to compare anaphoric chains computed
With place Without place
First Half K 0.62773 0.50066
? 0.65615 0.53875
Second Half K 0.66201 0.44997
? 0.67736 0.47490
The coefficient reported here as K is the one called K by Siegel
and Castellan (1988).
The value of ? is calculated using Passonneau?s distance metric;
for other distance metrics, see table 4.
Table 3: Comparing K and ?
as discussed above. Table 4 gives the value of ? for
the first half (the figures for the second half are sim-
ilar). The calculation of ? was manipulated under
the following three conditions.
Place markables. We calculated the value of ? on
the entire set of markables (with the exception of
three which had data errors), and also on a subset of
markables ? those that were not place names. Agree-
ment on marking place names was almost perfect:
45 of the 48 place name markables were marked cor-
rectly as ?place? by all 18 subjects, two were marked
correctly by all but one subject, and one was marked
correctly by all but two subjects. Place names thus
contributed substantially to the agreement among
the subjects. Dropping these markables from the
analysis resulted in a substantial drop in the value
of ? across all conditions.
Distance measure. We used the three measures
discussed earlier to calculate distance between sets:
Passonneau, Jaccard, and Dice.6
Chain construction. Substantial variation in the
agreement values can be obtained by making
changes to the way we construct anaphoric chains.
We tested the following methods.
NO CHAIN: only the immediate antecedents of an
anaphoric expression were considered, instead
of building an anaphoric chain.
PARTIAL CHAIN: a markable?s chain included only
phrase markables which occurred in the dia-
6For the nominal categories ?place? and ?none? we assign
a distance of zero between the category and itself, and of one
between a nominal category and any other category.
81
With place markables Without place markables
Pass Jacc Dice Pass Jacc Dice
No chain 0.65615 0.64854 0.65558 0.53875 0.52866 0.53808
Partial 0.67164 0.65052 0.67667 0.55747 0.53017 0.56477
Inclusive [?top] 0.65380 0.64194 0.69115 0.53134 0.51693 0.58237
Exclusive [?top] 0.62987 0.60374 0.64450 0.49839 0.46479 0.51830
Inclusive [+top] 0.60193 0.58483 0.64294 0.49907 0.47894 0.55336
Exclusive [+top] 0.57440 0.53838 0.58662 0.46225 0.41766 0.47839
Table 4: Values of ? for the first half of dialogue 3.2
logue before the markable in question (as well
as all discourse markables).
FULL CHAIN: chains were constructed by looking
upward and then back down, including all
phrase markables which occurred in the dia-
logue either before or after the markable in
question (as well as the markable itself, and all
discourse markables).
We used two separate versions of the full chain con-
dition: in the [+top] version we associate the top of
a chain with the chain itself, whereas in the [?top]
version we associate the top of a chain with its orig-
inal category label, ?place? or ?none?.
Passonneau (2004) observed that in the calcula-
tion of observed agreement, two full chains always
intersect because they include the current item. Pas-
sonneau suggests to prevent this by excluding the
current item from the chain for the purpose of cal-
culating the observed agreement. We performed the
calculation both ways ? the inclusive condition in-
cludes the current item, while the exclusive condi-
tion excludes it.
The four ways of calculating ? for full chains,
plus the no chain and partial chain condition, yield
the six chain conditions in Table 4. Other things be-
ing equal, Dice yields a higher agreement than Jac-
card; considering both halves of the dialogue, the
Passonneau measure always yielded a higher agree-
ment that Jaccard, while being higher than Dice in
10 of the 24 conditions, and lower in the remaining
14 conditions.
The exclusive chain conditions always give lower
agreement values than the corresponding inclusive
chain conditions, because excluding the current item
reduces observed agreement without affecting ex-
pected agreement (there is no ?current item? in the
calculation of expected agreement).
The [?top] conditions tended to result in a higher
agreement value than the corresponding [+top] con-
ditions because the tops of the chains retained their
?place? and ?none? labels; not surprisingly, the ef-
fect was less pronounced when place markables
were excluded from the analysis. Inclusive [?top]
was the only full chain condition which gave ? val-
ues comparable to the partial chain and no chain
conditions. For each of the four selections of mark-
ables, the highest ? value was given by the Inclusive
[?top] chain with Dice measure.
5.4 Qualitative Analysis
The difference between annotation of (identity!)
anaphoric relations and other semantic annotation
tasks such as dialogue act or wordsense annotation
is that apart from the occasional example of care-
lessness, such as marking Elmira as antecedent for
the boxcar at Elmira,7 all other cases of disagree-
ment reflect a genuine ambiguity, as opposed to dif-
ferences in the application of subjective categories.8
Lack of space prevents a full discussion of the
data, but some of the main points can already be
made with reference to the part of the dialogue in
(2), repeated with additional context in (3).
7According to our (subjective) calculations, at least one an-
notator made one obvious mistake of this type for 20 items out
of 72 in the first half of the dialogue?for a total of 35 careless
or mistaken judgment out of 1296 total judgments, or 2.7%.
8Things are different for associative anaphora, see (Poesio
and Vieira, 1998).
82
(3) 1.4 M: first thing I?d like you to do
1.5 is send engine E2 off with a boxcar
to Corning to pick up oranges
1.6 uh as soon as possible
2.1 S: okay [6 sec]
3.1 M: and while it?s there it
should pick up the tanker
The two it pronouns in utterance unit 3.1 are exam-
ples of the type of ambiguity already seen in (1).
All of our subjects considered the first pronoun a
?phrase? reference. 9 coders marked the pronoun
as ambiguous between engine E2 and the boxcar, 6
marked it as unambiguous and referring to engine
E2, and 3 as unambiguous and referring to the box-
car. This example shows that when trying to de-
velop methods to identify ambiguous cases it is im-
portant to consider not only the cases of explicit am-
biguity, but also so-called implicit ambiguity?cases
in which subjects do not provide evidence of being
consciously aware of the ambiguity, but the presence
of ambiguity is revealed by the existence of two or
more annotators in disagreement (Poesio, 1996).
6 DISCUSSION
In summary, the main contributions of this work so
far has been (i) to further develop the methodology
for annotating anaphoric relations and measuring the
reliability of this type of annotation, adopting ideas
from Passonneau and taking ambiguity into account;
and (ii) to run the most extensive study of reliabil-
ity on anaphoric annotation todate, showing the im-
pact of such choices. Our future work includes fur-
ther developments of the methodology for measur-
ing agreement with ambiguous annotations and for
annotating discourse deictic references.
ACKNOWLEDGMENTS
This work was in part supported by EPSRC project
GR/S76434/01, ARRAU. We wish to thank Tony
Sanford, Patrick Sturt, Ruth Filik, Harald Clahsen,
Sonja Eisenbeiss, and Claudia Felser.
References
P. Buitelaar. 1998. CoreLex : Systematic Polysemy and
Underspecification. Ph.D. thesis, Brandeis University.
D. Byron. 2003. Annotation of pronouns and their an-
tecedents: A comparison of two domains. Technical
Report 703, University of Rochester.
M. Eckert and M. Strube. 2001. Dialogue acts, synchro-
nising units and anaphora resolution. Journal of Se-
mantics.
D. Gross, J. Allen, and D. Traum. 1993. The TRAINS 91
dialogues. TRAINS Technical Note 92-1, Computer
Science Dept. University of Rochester, June.
L. Hirschman. 1998. MUC-7 coreference task definition,
version 3.0. In N. Chinchor, editor, In Proc. of the 7th
Message Understanding Conference.
H. Kamp and U. Reyle. 1993. From Discourse to Logic.
D. Reidel, Dordrecht.
K. Krippendorff. 1980. Content Analysis: An introduc-
tion to its Methodology. Sage Publications.
C. D. Manning and H. Schuetze. 1999. Foundations of
Statistical Natural Language Processing. MIT Press.
C. Mu?ller and M. Strube. 2003. Multi-level annotation
in MMAX. In Proc. of the 4th SIGDIAL.
M. Palmer, H. Dang, and C. Fellbaum. 2005. Mak-
ing fine-grained and coarse-grained sense distinctions,
both manually and automatically. Journal of Natural
Language Engineering. To appear.
R. J. Passonneau. 1997. Instructions for applying dis-
course reference annotation for multiple applications
(DRAMA). Unpublished manuscript., December.
R. J. Passonneau. 2004. Computing reliability for coref-
erence annotation. In Proc. of LREC, Lisbon.
M. Poesio and R. Vieira. 1998. A corpus-based investi-
gation of definite description use. Computational Lin-
guistics, 24(2):183?216, June.
M. Poesio, F. Bruneseaux, and L. Romary. 1999. The
MATE meta-scheme for coreference in dialogues in
multiple languages. In M. Walker, editor, Proc. of the
ACL Workshop on Standards and Tools for Discourse
Tagging, pages 65?74.
M. Poesio. 1996. Semantic ambiguity and perceived am-
biguity. In K. van Deemter and S. Peters, editors, Se-
mantic Ambiguity and Underspecification, chapter 8,
pages 159?201. CSLI, Stanford, CA.
M. Poesio. 2004. The MATE/GNOME scheme for
anaphoric annotation, revisited. In Proc. of SIGDIAL,
Boston, May.
E. F. Prince. 1992. The ZPG letter: subjects, def-
initeness, and information status. In S. Thompson
and W. Mann, editors, Discourse description: diverse
analyses of a fund-raising text, pages 295?325. John
Benjamins.
A. Rosenberg and E. Binkowski. 2004. Augmenting the
kappa statistic to determine interannotator reliability
for multiply labeled data points. In Proc. of NAACL.
C. L. Sidner. 1979. Towards a computational theory
of definite anaphora comprehension in English dis-
course. Ph.D. thesis, MIT.
S. Siegel and N. J. Castellan. 1988. Nonparametric
statistics for the Behavioral Sciences. McGraw-Hill.
K. van Deemter and R. Kibble. 2000. On coreferring:
Coreference in MUC and related annotation schemes.
Computational Linguistics, 26(4):629?637. Squib.
B. L. Webber. 1979. A Formal Approach to Discourse
Anaphora. Garland, New York.
B. L. Webber. 1991. Structure and ostension in the inter-
pretation of discourse deixis. Language and Cognitive
Processes, 6(2):107?135.
83
Proceedings of the ACL-SIGLEX Workshop on Deep Lexical Acquisition, pages 18?27,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Identifying Concept Attributes Using a Classifier 
Massimo Poesio  
University of Essex 
Computer Science /  
Language and Computation 
poesio at essex.ac.uk 
Abdulrahman Almuhareb 
University of Essex 
Computer Science /  
Language and Computation  
aalmuh at essex.ac.uk 
 
 
Abstract 
We developed a novel classification of 
concept attributes and two supervised 
classifiers using this classification to iden-
tify concept attributes from candidate at-
tributes extracted from the Web. Our 
binary (attribute / non-attribute) classifier 
achieves an accuracy of 81.82% whereas 
our 5-way classifier achieves 80.35%. 
1 Introduction 
The assumption that concept attributes and, more 
in general, features1 are an important aspect of 
conceptual representation is widespread in all dis-
ciplines involved with conceptual representations, 
from Artificial Intelligence / Knowledge Represen-
tation (starting with at least (Woods, 1975) and 
down to (Baader et al 2003)), Linguistics (e.g., in 
the theories of the lexicon based on typed feature 
structures and/or Pustejovsky?s Generative Lexi-
con theory: (Pustejovsky 1995)) and Psychology 
(Murphy 2002, Vinson et al2003).  This being the 
case, it is surprising how little attention has been 
devoted to this aspect of lexical representation in 
work on large-scale lexical semantics in Computa-
tional Linguistics. The most extensive resource at 
                                                          
1 The term attribute is used informally here to indicate the 
type of relational information about concepts that is expressed 
using so-called roles in Description Logics (Baader et al 
2003)?i.e., excluding IS-A style information (that cars are 
vehicles, for instance).  It is meant to be a more restrictive 
term than the term feature, often used to indicate any property 
of concepts, particularly in Psychology. We are carrying out a 
systematic analysis of the sets of features used in work such as 
(Vinson et al 2003) (see Discussion).  
our disposal, WordNet (Fellbaum, 1998) contains 
very little information that would be considered as 
being about ?attributes??only information about  
parts, not about qualities such as height, or even to 
the values of such attributes in the adjective net-
work?and this information is still very sparse. On 
the other hand, the only work on the extraction of 
lexical semantic relations we are aware of has con-
centrated on the type of relations found in Word-
Net: hyponymy (Hearst, 1998; Caraballo, 1999) 
and meronymy (Berland and Charniak, 1999; Poe-
sio et al 2002).2 
The work discussed here could be perhaps best 
described as an example of empirical ontology: 
using linguistics and philosophical ideas to im-
prove the results of empirical work on lexical / on-
tology acquisition, and vice versa, using findings 
from empirical analysis to question some of the 
assumptions of theoretical work on ontology and 
the lexicon. Specifically, we discuss work on the 
acquisition of (nominal) concept attributes whose 
goal is twofold: on the one hand, to clarify the no-
tion of ?attribute? and its role in lexical semantics, 
if any; on the other, to develop methods to acquire 
such information automatically (e.g., to supple-
ment WordNet).  
The structure of the paper is as follows. After a 
short review of relevant literature on extracting 
semantic relations and on attributes in the lexicon, 
we discuss our classification of attributes, followed 
by the features we used to classify them. We then 
discuss our training methods and the results we 
achieved.  
                                                          
2 In work on the acquisition of lexical information about verbs 
there has been some work on the acquisition of thematic roles, 
(e.g., Merlo and Stevenson, 2001). 
18
2 Background 
2.1 Using Patterns to Extract Semantic Rela-
tions 
The work discussed here belongs to a line of re-
search attempting to acquire information about 
lexical and other semantic relations other than 
similarity / synonymy by identifying syntactic 
constructions that are often (but not always!) used 
to express such relations. The earliest work of this 
type we are aware of is the work by Hearst (1998) 
on acquiring information about hyponymy (= IS-A 
links) by searching for instances of patterns such as  
NP {, NP}* or other NP 
(as in, e.g., bruises ?. broken bones and other 
INJURIES).   A similar approach was used by Ber-
land and Charniak (1999) and Poesio et al(2002) 
to extract information about part-of relations using 
patterns such as 
the N of the N is ?. 
(as in the wheel of the CAR is) and by Girju and 
Moldovan (2002) and Sanchez-Graillet and Poesio 
(2004) to extract causal relations. In previous work 
(Almuhareb and Poesio, 2004) we used this same 
approach to extract attributes, using the pattern  
?the * of the C [is|was]? 
(suggested by, e.g., (Woods, 1975) as a test for 
?attributehood?) to search for attributes of concept 
C in the Web, using the Google API. Although the 
information extracted this way proved a useful ad-
dition to our lexical representations from a cluster-
ing perspective, from the point of view of lexicon 
building this approach results in too many false 
positives, as very few syntactic constructions are 
used to express exclusively one type of semantic 
relation. For example, the ?attributes? of deer ex-
tracted using the text pattern above include ?the 
majority of the deer,? ?the lake of the deer,? and 
?the picture of the deer.? Girju and Moldovan 
(2002) addressed the problem of false positives for 
causal relations by developing WordNet-based fil-
ters to remove unlikely candidates. In this work, 
we developed a semantic filter for attributes based 
on a linguistic theory of attributes which does not 
rely on WordNet except as a source of morpho-
logical information (see below). 
2.2 Two Theories of Attributes 
The earliest attempt to classify attributes and other 
properties of substances we are aware of goes back 
to Aristotle, e.g., in Categories,3 but our classifica-
tion of attributes was inspired primarily by the 
work of Pustejovsky (1995) and Guarino (e.g., 
(1992)). According to Pustejovsky?s Generative 
Lexicon theory (1995), an integral part of a lexical 
entry is its Qualia Structure, which consists of 
four ?roles?:4 the Formal Role, specifying what 
type of object it is: e.g., in the case of a book, that 
it has a shape, a color, etc.; the Constitutive Role, 
specifying the stuff and parts that it consists of 
(e.g., in the case of a book,  that it is made of pa-
per, it has chapters and an index, etc.); the Telic 
Role, specifying the purpose of the object (e.g., in 
the case of a book, reading);  and the Agentive 
Role, specifying how the object was created (e.g., 
in the case of a book, by writing).   
Guarino (1992) argues that there are two types 
of attributes: relational and non-relational. Rela-
tional attributes include qualities such as color and 
position, and relational social roles such as son 
and spouse. Non-relational attributes include parts 
such as wheel and engine. Activities are not 
viewed as attributes in Guarino?s classification. 
3 Attribute Extraction and Classification 
The goal of this work is to identify genuine attrib-
utes by classifying candidate attributes collected 
using text patterns as discussed in (Almuhareb and 
Poesio, 2004) according to a scheme inspired by 
those proposed by Guarino and Pustejovsky.  
The scheme we used to classify the training 
data in the experiment discussed below consists of 
six categories:   
? Qualities: Analogous to Guarino?s qualities 
and Pustejovsky?s formal ?role?. (E.g., ?the 
color of the car?.) 
? Parts: Related to Guarino?s non-relational 
attributes and Pustejovsky?s constitutive 
?roles?. (E.g., ?the hood of the car?). 
? Related-Objects: A new category intro-
duced to cover the numerous physical ob-
jects which are ?related? to an object but are 
not part of it?e.g., ?the track of the deer?. 
                                                          
3 E.g., http://plato.stanford.edu/entries/substance. Thanks to 
one of the referees for drawing our attention to this. 
4 ?Facets? would be perhaps a more appropriate term to avoid 
confusions with the use of the term ?role? in Knowledge Rep-
resentation. 
19
? Activities: These include both the types of 
activities which are part of Pustejovsky?s 
telic ?role? and those which would be in-
cluded in his agentive ?role?. (E.g., ?the re-
pairing of the car?.) 
? Related-Agents: For the activities in which 
the concept in question is acted upon, the 
agent of the activity: e.g., ?the writer of the 
book?, ?the driver of the car?. 
? Non-Attributes: This category covers the 
cases in which the construction ?the N of the 
N? expresses other semantic relations, as in: 
?the last of the deer?, ?the majority of the 
deer,? ?the lake of the deer,? and ?in the 
case of the deer?. 
We will quickly add that (i) we do not view this 
classification as definitive?in fact, we already 
collapsed the classes ?part? and ?related objects?  in 
the  experiments discussed below?and (ii) not all 
of these distinctions are very easy even for human 
judges to do.  For example, design, as an attribute 
of a car, can be judged to be a quality if we think 
of it as taking values such as modern and standard; 
on the other hand, design might also be viewed as 
an activity in other contexts discussing the design-
ing process. Another type of difficulty is that a 
given attribute may express different things for 
different objects. For example, introduction is a 
part of a book, and an activity for a product. An 
additional difficulty results from the strong similar-
ity between parts and related-objects. For example, 
?key? is a related-object to a car but it is not part 
of it. We will return to this issue and to agreement 
on this classification scheme when discussing the 
experiment. 
One difference from previous work is that we 
use additional linguistic constructions to extract 
candidate attributes. The construction ?the X of the 
Y is? used in our previous work is only one exam-
ple of genitive construction. Quirk et al(1985) list 
eight types of genitives in English, four of which 
are useful for our purposes:  
? Possessive Genitive: used to express quali-
ties, parts, related-objects, and related-
agents. 
? Genitive of Measure: used to express quali-
ties. 
? Subjective & Objective Genitives: used to 
express activities. 
We used all of these constructions in the work 
discussed here.  
4 Information Used to Classify Attributes 
Our attribute classifier uses four types of informa-
tion: morphological information, an attribute 
model, a question model, and an attributive-usage 
model. In this section we discuss how this informa-
tion is automatically computed.  
4.1 Morphological Information 
Our use of morphological information is based on 
the noun classification scheme proposed by Dixon 
(1991). According to Dixon, derivational morphol-
ogy provides some information about attribute typ-
e. Parts are concrete objects and almost all of them 
are expressed using basic noun roots (i.e., not de-
rived from adjectives or verbs). Most of qualities 
and properties are either basic noun roots or de-
rived from adjectives. Finally, activities are mostly 
nouns derived from verbs. Although these rules 
only have a heuristic value, we found that morpho-
logically based heuristics did provide useful cues 
when used in combination with the other types of 
information discussed below.  
As we are not aware of any publicly available 
software performing automatic derivational mor-
phology, we developed our own (and very basic) 
heuristic methods. The techniques we used involve 
using information from WordNet, suffix-checking, 
and a POS tagger. 
WordNet was used to find nouns that are de-
rived from verbs and to filter out words that are not 
in the noun database. Nouns in WordNet are linked 
to their derivationally related verbs, but there is no 
indication about which is derived from which. We 
use a heuristic based on length to decide this: the 
system checks if the noun contains more letters 
than the most similar related verb. If this is the 
case, then the noun is judged to be derived from 
the verb. If the same word is used both as a noun 
and as a verb, then we check the usage familiarity 
of the word, which can also be found in WordNet. 
If the word is used more as a verb and the verbal 
usage is not rare, then again the system treats the 
noun as derived from the verb. 
20
To find nouns that are derived from adjectives 
we used simple heuristics based on suffix-
checking. (This was also done by Berland and 
Charniak (1999).) All words that end with ?ity? or 
?ness? are considered to be derived from adjec-
tives. A noun not found to be derived from a verb 
or an adjective is assumed to be a basic noun root. 
In addition to derivational morphology, we used 
the Brill tagger (Brill, 1995) to filter out adjectives 
and other types of words that can occasionally be 
used as nouns such as better, first, and whole be-
fore training. Only nouns, base form verbs, and 
gerund form verbs were kept in the candidate at-
tribute list. 
4.2 Clustering Attributes  
Attributes are themselves concepts, at least in the 
sense that they have their own attributes: for ex-
ample, a part of a car, such as a wheel, has its own 
parts (the tyre) its qualities (weight, diameter) etc.  
This observation suggests that it should be possible 
to find similar attributes in an unsupervised fashion 
by looking at their attributes, just as we did earlier 
for concepts (Almuhareb and Poesio, 2004). In 
order to do this, we used our text patterns for find-
ing attributes to collect from the Web up to 500 
pattern instances for each of the candidate attrib-
utes. The collected data were used to build a vecto-
rial representation of attributes as done in 
(Almuhareb and Poesio, 2004).  We then used 
CLUTO (Karypis, 2002) to cluster attributes using 
these vectorial representations. In a first round of 
experiments we found that the classes ?parts? and 
?related objects? were difficult to differentiate, and 
therefore we merged them. The final model clus-
ters candidate attributes into five classes: activities, 
parts & related-objects, qualities, related-agents, 
and non-attributes. This classification was used as 
one of the input features in our supervised classi-
fier for attributes.  
We also developed a measure to identify par-
ticularly distinctive ?attributes of attributes??
attributes which have a strong tendency to occur 
primarily with attributes (or any concept) of a 
given class?which has proven to work pretty well. 
This measure, which we call Uniqueness, actually 
is the product of two factors: the degree of unique-
ness proper, i.e., the probability P(classi | attrib-
utej) that  an attribute (or, in fact, any other noun) 
will belong to class i given than it has attribute j; 
and a measure of ?definitional power? ?the prob-
ability P(attribute j | classi) that a concept belong-
ing to a given class will have a certain attribute. 
Using MLE to estimate these probabilities, the de-
gree of uniqueness of attributesj of classi is com-
puted as follows: 
 
)(
),( 2
,
ji
ji
ji attributeCn
attributeclassC
Uniqueness ?=  
 
where ni is the number of concepts in classi. C is a 
count function that counts concepts that are associ-
ated with the given attribute. Uniqueness ranges 
from 0 to 1. 
Table 1 shows the 10 most distinctive attributes 
for each of the five attribute classes, as determined 
by the Uniqueness measure just introduced, for the 
1,155 candidate attributes in the training data for 
the experiment discussed below. 
 
Class Top 10 Distinctive Attributes 
Related-Agent 
(0.39) 
identity, hands, duty, consent, 
responsibility, part, attention, 
voice, death, job 
Part &  
Related-Object 
(0.40) 
inside, shape, top, outside, sur-
face, bottom, center, front, size, 
interior 
Activity 
(0.29) 
time, result, process, results, 
timing, date, effect, beginning, 
cause, purpose 
Quality 
(0.23) 
measure, basis, determination, 
question, extent, issue, meas-
urement, light, result, increase 
Non-Attribute 
(0.18) 
content, value, rest, nature, 
meaning, format, interpretation, 
essence, size, source 
Table 1: Top 10 distinctive attributes of the five 
classes of candidate attributes. Average distinct-
iveness (uniqueness) for the top 10 attributes is 
shown between parentheses 
 
 Most of the top 10 attributes of related-agents, 
parts & related-objects, and activities are genuinely 
distinctive attributes for such classes. Thus, attrib-
utes of related-agents reflect the ?intentionality? 
aspect typical of members of this class: identity, 
duty, and responsibility. Attributes of parts are 
common attributes of physical objects (e.g., inside, 
shape). Most attributes of activities have to do with 
temporal properties and causal structure: e.g., be-
ginning, cause. The ?distinctive? attributes of the 
21
quality class are less distinctive, but four such at-
tributes (measure, extent, measurement, and in-
crease) are related to values since many of the 
qualities can have different values (e.g., small and 
large for the quality size). There are however sev-
eral attributes in common between these classes of 
attributes, emphasizing yet again how some of 
these distinctions at least are not completely clear 
cut:  e.g., result, in common between activities and 
qualities (two classes which are sometimes diffi-
cult to distinguish). Finally, as one would expect, 
the attributes of the non-attribute class are not 
really distinctive: their average uniqueness score is 
the lowest. This is because ?non-attribute? is a het-
erogeneous class. 
4.3 The Question Model 
Certain types of attributes can only be used when 
asking certain types of questions. For example, it is 
possible to ask ?What is the color of the car?? but 
not ??When is the color of the car??.  
We created a text pattern for each type of ques-
tion and used these patterns to search the Web and 
collect counts of occurrences of particular ques-
tions. An example of such patterns would be: 
? ?what is|are the A  of the? 
where A is the candidate attribute under investiga-
tion. Patterns for who, when, where, and how are 
similar.  
After collecting occurrence frequencies for all 
the candidate attributes, we transform these counts 
into weights using the t-test weighting function as 
done for all of our counts, using the following for-
mula from Manning and Schuetze (1999): 
 
 
2
2
, ),(
N
)()(),(
N
attributequestionC
attributeCquestionC
N
attributequestionC
t
ji
jiji
ji
??
?
 
where N is the total number of relations, and C is a 
count function. 
Table 2 shows the 10 most frequent attributes 
for each question type. This data was collected us-
ing a more restricted form of the question patterns 
and a varying number of instances for each type of 
questions. The restricted form includes a question 
mark at the end of the phrase and was used to im-
prove the precision. For example, the what-pattern 
would be ?what is the * of the *??. 
Question Top 10 Attributes 
what purpose, name, nature, role, cost, func-tion, significance, size, source, status 
who author, owner, head, leader, president, sponsor, god, lord, father, king 
where rest, location, house, fury, word, edge, center, end, ark, voice 
how 
quality, rest, pace, level, length, mo-
rale, performance, content, organiza-
tion, cleanliness 
when end, day, time, beginning, date, onset, running, birthday, fast, opening 
Table 2: Frequent attributes for each question type 
 
Instances of the what-pattern are frequent in the 
Web: the Google count was more than 2,000,000 
for a query issued in mid 2004. The who-pattern is 
next in terms of occurrence, with about 350,000 
instances. The when-pattern is the most infrequent 
pattern, about 5,300 instances. 
The counts broadly reflected our intuitions 
about the use of such questions. What-questions 
are mainly used with qualities, whereas who-
questions are used with related-agents. Attributes 
occurring with when-questions have some tempo-
ral aspects; attributes occurring with how-questions 
are mostly qualities and activities, and attributes in 
where-questions are of different types but some are 
related to locations. Parts usually do not occur with 
these types of questions. 
4.4 Attributive Use  
Finally, we exploited the fact that certain types of 
attributes are used more in language as concepts 
rather than as attributes. For instance, it is more 
common to encounter the phrase ?the size of the 
?? than ?the ?  of the size?. On the other hand, it is 
more common to encounter the phrase ?the * of 
the window? than ?the window of the *?. Gener-
ally speaking, parts, related-objects, and related-
agents are more likely to have more attributes than 
qualities and activities. We used the two patterns 
?the * of the A? and ?the A of the *? to collect 
Google counts for all of the candidate attributes. 
These counts were also weighted using the t-test as 
in the question model. 
Table 3 illustrates the attributive and conceptual 
usage for each attribute class using a training data 
of 1,155 attributes. The usage averages confirm the 
initial assumption.  
22
Average T-Test Score Attribute Class Conceptual Attributive
Parts & 
Related-Objects 18.81 3.00 
Non-Attributes 13.29 11.07 
Related-Agents 12.15 2.54 
Activities 3.22 5.08 
Qualities 0.23 17.09 
Table 3: Conceptual and attributive usage averages 
for each attribute class 
5 The Experiment 
We trained two classifiers: a 2-way classifier that 
simply classifies candidate attributes into attributes 
and non-attributes, and a 5-way classifier that clas-
sifies candidate attributes into activities, parts & 
related-objects, qualities, related-agents, and non-
attributes. These classifiers were trained using de-
cision trees algorithm (J48) from WEKA (Witten 
and Frank, 1999). 
 
Feature 
el
ec
tio
n 
ab
do
m
en
 
ac
id
ity
 
cr
ea
to
r 
pr
ob
le
m
 
Cluster Id 1 2 4 0 3 
What 0.00 0.00 0.00 0.00 3.80 
When 2.62 0.00 0.00 0.00 0.00 
Where 0.78 0.94 0.00 0.00 0.00 
Who 0.00 0.00 0.00 30.28 0.00 
How 2.05 0.00 1.54 0.00 2.61 
Conceptual 38.16 20.15 0.00 0.00 135.40 
Attributive 0.00 0.00 10.22 1.60 0.00 
Morph DV BN DA DV BN 
Attribute 
Class  
(Output) 
Activity Part Quality RelatedAgent 
Non- 
Attribute
Table 4: Five examples of training instances. The 
values for morph are as follows: DV: derived from 
verb; BN: basic noun; DA: derived from adjective 
 
Our training and testing material was acquired 
as follows. We started from the 24,178 candidate 
attributes collected for the concepts in the balanced 
concept dataset we recently developed (Almuhareb 
and Poesio, 2005). We threw out every candidate 
attribute with a Google frequency less than 20; this 
reduced the number of candidate attributes to 
4,728. We then removed words other than nouns 
and gerunds as discussed above, obtaining 4,296 
candidate attributes.   
The four types of input features for this filtered 
set of candidate attributes were computed as dis-
cussed in the previous section. The best results 
were obtained using all of these features. A train-
ing set of 1,155 candidate attributes was selected 
and hand-classified (see below for agreement fig-
ures). We tried to include enough samples for each 
attribute class in the training set. Table 4 shows the 
input features for five different training examples, 
one for each attribute class. 
6  Evaluation 
For a qualitative idea of the behavior of our classi-
fier, the best attributes for some concepts are listed 
in Appendix A. We concentrate here on quantita-
tive analyses. 
6.1 Classifier Evaluation 1: Cross-Validation 
Our two classifiers were evaluated, first of all, us-
ing 10-fold cross-validation. The 2-way classifier 
correctly classified 81.82% of the candidate attrib-
utes (the baseline accuracy is 80.61%). The 5-way 
classifier correctly classified 80.35% of the attrib-
utes (the baseline accuracy is 23.55%). The preci-
sion / recall results are shown in Table 5. 
 
Attribute Class P R F 
2-Way Classifier 
Attribute 0.854 0.934 0.892
Non-Attribute 0.551 0.335 0.417
5-Way Classifier 
Related-Agent 0.930 0.970 0.950
Part & Related-Object 0.842 0.882 0.862
Activity 0.822 0.878 0.849
Quality 0.799 0.821 0.810
Non-Attribute 0.602 0.487 0.538
Table 5: Cross-validation results for the two  
attribute classifiers 
 
As it can be seen from Table 5, both classifiers 
achieve good F values for all classes except for the 
non-attribute class: F-measures range from 81% to 
95%. With the 2-way classifier, the valid attribute 
class has an F-measure of 89.2%. With the 5-way 
classifier, related-agent is the most accurate class 
(F = 95%) followed by part & related-object, 
activity, and quality (86.2%, 84.9%, and 81.0%, 
23
respectively). With non-attribute, however, we 
find an F of 41.7% in the 2-way classification, and 
53.8% in the 5-way classification. This suggests 
that the best strategy for lexicon building would be 
to use these classifiers to ?find? attributes rather 
than ?filter? non-attributes. 
6.2 Classifier Evaluation 2: Human Judges 
Next, we evaluated the accuracy of the attribute 
classifiers against two human judges (the authors). 
We randomly selected a concept from each of the 
21 classes in the balanced dataset.  Next, we used 
the classifiers to classify the 20 best candidate at-
tributes of each concept, as determined by their t-
test scores. Then, the judges decided if the as-
signed classes are correct or not. For the 5-way 
classifier, the judges also assigned the correct class 
if the automatic assigned class is incorrect.  
After a preliminary examination we decided not 
to consider two troublesome concepts: constructor 
and future. The reason for eliminating constructor 
is that we discovered it is ambiguous: in addition 
to the sense of ?a person who builds things?, we 
discovered that constructor is used widely in the 
Web as a name for a fundamental method in object 
oriented programming languages such as Java. 
Most of the best candidate attributes (e.g., call, 
arguments, code, and version) related to the latter 
sense, that doesn?t exist in WordNet. Our system is 
currently not able to do word sense discrimination, 
but we are currently working on this issue. The 
reason for ignoring the concept future was that this 
word is most commonly used as a modifier in 
phrases such as: ?the car of the future?, and ?the 
office of the future?, and that all of the best candi-
date attributes occurred in this type of construction.  
This reduced the number of evaluated concepts to 
19. 
According to the judges, the 2-way classifier 
was on average able to correctly assign attribute 
classes for 82.57% of the candidate attributes. This 
is very close to its performance in evaluation 1. 
The results using the F-measure reveal similar re-
sults too. Table 6 shows the results of the two clas-
sifiers based on the precision and recall measures. 
According to the judges, the 5-way classifier 
correctly classified 68.72% on average. This per-
formance is good but not as good as its perform-
ance in evaluation 1 (80.35%). The decrease in the 
performance was also shown in the F-measure. 
The F-measure ranges from 0.712 to 0.839 exclud-
ing the non-attribute class. 
  
Attribute Class P R F 
2-Way Classifier 
Attribute 0.928 0.872 0.899
Non-Attribute 0.311 0.459 0.369
5-Way Classifier 
Related-Agent 0.813 0.868 0.839
Part & Related-Object 0.814 0.753 0.781
Activity 0.870 0.602 0.712
Quality 0.821 0.658 0.730
Non-Attribute 0.308 0.632 0.414
Table 6: Evaluation against human judges results 
for the two classifiers 
 
An important question when using human 
judges is the degree of agreement among them. 
The K-statistic was used to measure this agree-
ment. The values of K are shown in Table 7. In the 
2-way classification, the judges agreed on 89.84% 
of the cases. On the other hand, the K-statistic for 
this classification task is 0.452. This indicates that 
part of this strong agreement is because that the 
majority of the candidate attributes are valid attrib-
utes. It also shows the difficulty of identifying non-
attributes even for human judges. In the 5-way 
classification, the two judges have a high level of 
agreement; Kappa statistic is 0.749. The judges 
and the 5-way classifier agreed on 63.71% of the 
cases. 
 
Description 2-Way 5-Way
Human Judges 89.84% 80.69%
Human Judges (Kappa) 0.452 0.749 
Human Judges & Classifier 78.36% 63.71%
Table 7: Level of agreement between the human 
judges and the classifiers 
6.3 Re-Clustering the Balanced Dataset 
Finally, we looked at whether using the classifiers 
results in a better lexical description for the pur-
poses of clustering (Almuhareb and Poesio, 2004). 
In Table 8 we show the results obtained using the 
output of the 2-way classifier to re-cluster the 402 
concepts of our balanced dataset, comparing these 
results with those obtained using all attributes (first 
column) and all attributes that remain after fre-
quency cutoff and POS filtering (column 2). The 
results are based on the CLUTO evaluation meas-
24
ures: Purity (which measures the degree of cohe-
sion of the clusters obtained) and Entropy. The 
purity and entropy formulas are shown in Table 9. 
 
Description 
All 
Candidate 
Attributes
Filtered 
Candidate 
Attributes 
2-Way 
Attributes
Purity 0.657 0.672 0.693 
Entropy 0.335 0.319  0.302  
Vector Size 24,178 4,296 3,824 
Table 8: Results of re-clustering concepts using 
different sets of attributes 
 
Clustering the concepts using only filtered can-
didate attributes improved the clustering purity 
from 0.657 to 0.672. This improvement in purity is 
not significant. However, clustering using only the 
attributes sanctioned by  the 2-way classifier im-
proved the purity further to 0.693, and this im-
provement in purity from the initial purity  was 
significant (t = 2.646, df = 801, p < 0.05). 
 
 Entropy Purity 
Single 
Cluster ?=?=
q
i r
i
r
r
i
r
r n
n
n
n
q
SE
1
log
log
1)(  )(max
1)( iri
r
r nn
SP =
Over-
all )(1
r
k
r
r SE
n
nEntropy ?
=
=  )(
1
r
k
r
r SP
n
nPurity ?
=
=
Table 9: Entropy and Purity in CLUTO. 
Sr is a cluster, nr is the size of the cluster, q is the number of 
classes, nir is the number of concepts from the  ith class that 
were assigned to the rth cluster, n is the number of concepts, 
and k is the number of clusters. 
7 Discussion and Conclusions 
The lexicon does not simply contain information 
about synonymy and hyponymy relations; it also 
contains information about the attributes of the 
concepts expressed by senses, as in Qualia struc-
tures. In previous work, we developed techniques 
for mining candidate attributes from the Web; in 
this paper we presented a method for improving 
the quality of attributes thus extracted, based on a 
classification for attributes derived from work in 
linguistics and philosophy, and a classifier that 
automatically tags candidate attributes with such 
classes. Both the 2-way and the 5-way classifiers 
achieve good precision and recall. Our work also 
reveals, however, that the notion of attribute is not 
fully understood. On the one hand, that attribute 
judgments are not always easy for humans even 
given a scheme; on the other hand, the results for 
certain types of attributes, especially activities and 
qualities, could certainly be improved. We also 
found that whereas attributes of physical objects 
are relatively easy to classify, the attributes of 
other types of concepts are harder ?particularly 
with activities. (See the Appendix for examples.) 
Our longer term goal is thus to further clarify the 
notion of attribute, possibly refining our classifica-
tion scheme, in collaboration with linguists, phi-
losophers, and psycholinguists. One comparison 
we are particularly interested in pursuing at the 
moment is that with feature lists used by psycholo-
gist, for whom knowledge representation is en-
tirely concept-based, and virtually every property 
of a concept counts as an attribute, including prop-
erties that would be viewed as IS-A links and what 
would be considered a value. Is it possible to make 
a principled, yet cognitively based distinction? 
Acknowledgments 
Abdulrahman Almuhareb is supported by King 
Abdulaziz City for Science and Technology 
(KACST), Riyadh, Saudi Arabia. We wish to thank 
the anonymous referees for many helpful sugges-
tions.  
 References 
Almuhareb, A. and Poesio, M. (2004). Attribute-Based 
and Value-Based Clustering: An Evaluation. In Proc. 
of EMNLP. Barcelona, July. 
Almuhareb, A. and Poesio, M. (2005). Concept Learn-
ing and Categorization from the Web. In Proc. of 
CogSci. Italy, July. 
Baader, F., Calvanese, D., McGuinness, D., Nardi, D. 
and Patel-Schneider, P. (Editors). (2003). The De-
scription Logic Handbook. Cambridge University 
Press. 
Berland, M. and Charniak, E. (1999). Finding parts in 
very large corpora. In Proc. of the 37th ACL, (pp. 
57?64). University of Maryland. 
Brill, E. (1995). Transformation-Based Error-Driven 
Learning and Natural Language Processing: A Case 
Study in Part of Speech Tagging. Computational 
Linguistics. 
25
Caraballo, S. A. (1999). Automatic construction of a 
hypernym-labeled noun hierarchy from text. In Proc. 
of  the 37th  ACL. 
Dixon, R. M. W. (1991). A New Approach to English 
Grammar, on Semantic Principles. Clarendon Press, 
Oxford. 
Fellbaum, C. (Editor). (1998). WordNet: An electronic 
lexical database. The MIT Press. 
Girju, R. and Moldovan, D. (2002). Mining answers for 
causal questions. In Proc. AAAI.   
Guarino, N. (1992). Concepts, attributes and arbitrary 
relations: some linguistic and ontological criteria for 
structuring knowledge base. Data and Knowledge 
Engineering, 8, (pp. 249?261). 
Hearst, M. A. (1998). Automated discovery of WordNet 
relations. In Fellbaum, C. (Editor). WordNet: An 
Electronic Lexical Database. MIT Press. 
Karypis, G. (2002). CLUTO: A clustering toolkit. Tech-
nical Report 02-017. University of Minnesota. At 
http://www-users.cs.umn.edu/~karypis/cluto/. 
Manning, C. D. and Schuetze H. (1999). Foundations of 
Statistical NLP. MIT Press. 
Merlo, P. and Stevenson, S. (2001). Automatic Verb 
Classification Based on Statistical Distributions of 
Argument Structure. Computational Linguistics. 27: 
3, 373-408. 
Murphy, G. L. (2002). The Big Book of Concepts. The 
MIT Press. 
Poesio, M., Ishikawa, T., Schulte im Walde, S. and 
Vieira, R. (2002).  Acquiring lexical knowledge for 
anaphora resolution. In Proc. Of LREC.     
Pustejovsky, J. (1995). The generative lexicon. MIT 
Press. 
Quirk, R., Greenbaum, S., Leech, G., and Svartvik, J. 
(1985). A comprehensive grammar of the English 
language. London: Longman. 
Sanchez-Graillet, O. and Poesio, M. (2004). Building 
Bayesian Networks from text. In Proc. of LREC, Lis-
bon, May. 
Vinson, D. P., Vigliocco, G., Cappa, S., and Siri, S. 
(2003). The breakdown of semantic knowledge: in-
sights from a statistical model of meaning representa-
tion. Brain and Language, 86(3), 347-365(19). 
Witten, I. H. and Frank, E. (1999). Data Mining: Prac-
tical Machine Learning Tools and Techniques with 
Java Implementations, Morgan Kaufmann. 
Woods, W. A. (1975). What?s in a link: Foundations for 
semantic networks. In Daniel G. Bobrow and Alan 
M. Collins, editors, Representation and Understand-
ing: Studies in Cognitive Science, (pp. 35-82). Aca-
demic Press, New York. 
 
 
 
Appendix A.   5-Way Automatic Classification of the Best Candidate Attributes of 
Some Concepts 
 
Car 
Class Best Attributes 
Activity acceleration, performance, styling, construction, propulsion, insurance, stance, ride, move-ment 
Part & 
Related-
Object  
front, body, mass, underside, hood, roof, nose, graphics, side, trunk, engine, boot, frame, bot-
tom, backseat, chassis, wheelbase, silhouette, floor, battery, windshield, seat, undercarriage, 
tank, window, steering, drive, finish  
Quality  speed, weight, handling, velocity, color, condition, width, look, colour, feel, momentum, heritage, shape, appearance, ownership, make, convenience, age, quality, reliability 
Related-
Agent  driver, owner, buyer, sponsor, occupant, seller 
Non-
Attribute  
rest, price, design, balance, motion, lure, control, use, future, cost, inertia, model, wheel, 
style, position, setup, sale, supply, safety  
 
26
 
Camel 
Class Best Attributes 
Activity introduction, selling, argument, exhaustion 
Part & 
Related-
Object  
nose, hump, furniture, saddle, hair, flesh, neck, milk, head, reins, foot, eye, hooves, humps, 
ass, feet, hoof, flanks, bones, ears, bag, skin, haunches, stomach, legs, urine, meat, penis, 
load, breast, backside, testicles, rope, corpse, house, nostrils, foam, bell, sight, butt, fur, bod-
ies, toe, hoofs, heads, knees, pancreas, mouth, coat, uterus, necks, chin, udders 
Quality  origins, gait, domestication, usefulness, pace, fleetness, smell, existence, appeal, birth, awk-wardness  
Related-
Agent  ghost 
Non-
Attribute  gift, rhythm, physiology, battle, case, example, dance, manner, description 
 
Cancer 
Class Best Attributes 
Activity 
growth, development, removal, treatment, recurrence, diagnosis, pain, spreading, metastasis, 
detection, eradication, elimination, production, discovery, remission, advance, excision, pre-
vention, evolution, disappearance, anxiety 
Part & 
Related-
Object  
location, site, lump, nature, root, cells, margin, formation, margins, roots, world, region 
Quality  
extent, size, seriousness, progression, severity, aggressiveness, cause, progress, symptoms, 
effects, risk, incidence, staging, biology, onset, characteristics, histology, ability, status, ap-
pearance, thickness, sensitivity, causes, prevalence, responsiveness, ravages, frequency, aeti-
ology, circumstances, rarity, outcome, behavior, genetics 
Related-
Agent  club, patient 
Non-
Attribute  
stage, spread, grade, origin, course, power, return, area, response, presence, type, particulars, 
occurrence, prognosis, pathogenesis, source, news, cure, pathology, properties, genesis, 
boundaries, drama, stages, chapter 
 
Family 
Class Best Attributes 
Activity disintegration, protection, decline, destruction, breakup, abolition, participation, reunifica-tion, reconciliation, dissolution, composition, restoration 
Part & 
Related-
Object  
head, institution, support, flower, core, fabric, culture, dimension, food, lineage, cornerstone, 
community 
Quality  
breakdown, importance, honor, structure, sociology, integrity, unity, sanctity, health, privacy, 
survival, definition, influence, honour, involvement, continuity, stability, size, preservation, 
upbringing, centrality, ancestry, solidarity, hallmark, status, functioning, primacy, autonomy  
Related-
Agent  
father, baby, member, mother, members, patriarch, breadwinner, matriarch, man, foundation, 
founder, heir, daughter 
Non-
Attribute  
rest, role, income, history, concept, welfare, pedigree, genealogy, presence, context, origin, 
bond, tradition, taxonomy, system, wealth, lifestyle, surname, crisis, ideology, rights, eco-
nomics, safety 
 
27
Proceedings of the 2009 Workshop on the People?s Web Meets NLP, ACL-IJCNLP 2009, pages 57?62,
Suntec, Singapore, 7 August 2009.
c?2009 ACL and AFNLP
Constructing An Anaphorically Annotated Corpus With Non-Experts:
Assessing The Quality Of Collaborative Annotations.
Jon Chamberlain
University of Essex
School of Computer Science
and Electronic Engineering
jchamb@essex.ac.uk
Udo Kruschwitz
University of Essex
School of Computer Science
and Electronic Engineering
udo@essex.ac.uk
Massimo Poesio
University of Essex
School of Computer Science
and Electronic Engineering
poesio@essex.ac.uk
Abstract
This paper reports on the ongoing work
of Phrase Detectives, an attempt to cre-
ate a very large anaphorically annotated
text corpus. Annotated corpora of the size
needed for modern computational linguis-
tics research cannot be created by small
groups of hand-annotators however the
ESP game and similar games with a pur-
pose have demonstrated how it might be
possible to do this through Web collabora-
tion. We show that this approach could be
used to create large, high-quality natural
language resources.
1 Introduction
The statistical revolution in natural language pro-
cessing (NLP) has resulted in the first NLP
systems and components really usable on a
large scale, from part-of-speech (POS) taggers
to parsers (Jurafsky and Martin, 2008). But it
has also raised the problem of creating the large
amounts of annotated linguistic data needed for
training and evaluating such systems.
This requires trained annotators, which is pro-
hibitively expensive both financially and in terms
of person-hours (given the number of trained an-
notators available) on the scale required.
Recently, however, Web collaboration has
started to emerge as a viable alternative.
Wikipedia and similar initiatives have shown
that a surprising number of individuals are willing
to help with resource creation and scientific
experiments. The goal of the ANAWIKI project
1
is to experiment with Web collaboration as a
solution to the problem of creating large-scale
linguistically annotated corpora. We do this by
developing tools through which members of our
scientific community can participate in corpus
1
http://www.anawiki.org
creation and by engaging non-expert volunteers
with a game-like interface. In this paper we
present ongoing work on Phrase Detectives
2
,
a game designed to collect judgments about
anaphoric annotations, and we report a first
analysis of annotation quality in the game.
2 Related Work
Large-scale annotation of low-level linguistic in-
formation (part-of-speech tags) began with the
Brown Corpus, in which very low-tech and time
consuming methods were used. For the cre-
ation of the British National Corpus (BNC), the
first 100M-word linguistically annotated corpus, a
faster methodology was developed using prelimi-
nary annotation with automatic methods followed
by partial hand-correction (Burnard, 2000).
Medium and large-scale semantic annotation
projects (for wordsense or coreference) are a re-
cent innovation in Computational Linguistics. The
semi-automatic annotation methodology cannot
yet be used for this type of annotation, as the qual-
ity of, for instance, coreference resolvers is not
yet high enough on general text. Nevertheless the
semantic annotation methodology has made great
progress with the development, on the one end,
of effective quality control methods (Hovy et al,
2006) and on the other, of sophisticated annotation
tools such as Serengeti (St?uhrenberg et al, 2007).
These developments have made it possible to
move from the small-scale semantic annotation
projects, the aim of which was to create resources
of around 100K words in size (Poesio, 2004b),
to the efforts made as part of US initiatives such
as Automatic Context Extraction (ACE), Translin-
gual Information Detection, Extraction and Sum-
marization (TIDES), and GALE to create 1 mil-
lion word corpora. Such techniques could not be
expected to annotate data on the scale of the BNC.
2
http://www.phrasedetectives.org
57
2.1 Collaborative Resource Creation
Collaborative resource creation on the Web offers
a different solution to this problem. The motiva-
tion for this is the observation that a group of in-
dividuals can contribute to a collective solution,
which has a better performance and is more ro-
bust than an individual?s solution as demonstrated
in simulations of collective behaviours in self-
organizing systems (Johnson et al, 1998).
Wikipedia is perhaps the best example of col-
laborative resource creation, but it is not an iso-
lated case. The gaming approach to data collec-
tion, termed games with a purpose, has received
increased attention since the success of the ESP
game (von Ahn, 2006).
2.2 Human Computation
Human computation, as a more general concept
than games with a purpose, has become popular
in numerous research areas. The underlying as-
sumption of learning from a vast user population
has been largely the same in each approach. Users
are engaged in different ways to achieve objectives
such as:
? Assigning labels to items
? Learning to rank
? Acquiring structured knowledge
An example of the first category is the ESP
game which was a project to label images with
tags through a competitive game. 13,500 users
played the game, creating 1.3M labels in 3 months
(von Ahn, 2006). Other examples of assigning
lables to items include Phetch and Peekaboom
(von Ahn et al, 2006).
Learning to rank is a very different objective.
For example user judgements are collected in the
Picture This game (Bennett et al, 2009). This is
a two player game where the user has to select
the best matching image for a given query from
a small set of potential candidates. The aim is
to learn a preference ranking from the user votes
to predict the preference of future users. Several
methods for modeling the collected preferences
confirmed the assumption that a consensus rank-
ing from one set of users can be used to model
another.
Phrase Detectives is in the third category, i.e. it
aims to acquire structured knowledge, ultimately
Figure 1: A screenshot of the Annotation Mode.
leading to a linguistically annotated corpus. An-
other example of aiming to acquire large amounts
of structured knowledge is the Open Mind Com-
monsense project, a project to mine commonsense
knowledge to which 14,500 participants con-
tributed nearly 700,000 sentences (Singh, 2002).
Current efforts in attempting to acquire large-
scale world knowledge from Web users include
Freebase
3
and True Knowledge
4
. A slightly dif-
ferent approach to the creation of commonsense
knowledge has been pursued in the Semantic Me-
diaWiki project (Kr?otzsch et al, 2007), an effort to
develop a ?Wikipedia way to the Semantic Web?:
i.e., to make Wikipedia more useful and to support
improved search of web pages via semantic anno-
tation.
3 The Phrase Detectives game
Phrase Detectives offers a simple graphical user
interface for non-expert users to learn how to
annotate text and to make annotation decisions
(Chamberlain et al, 2008).
In order to use Web collaboration to create an-
notated data, a number of issues have to be ad-
dressed. First among these is motivation. For any-
body other than a few truly dedicated people, an-
notation is a very boring task. This is where the
promise of the game approach lies. Provided that
a suitably entertaining format can be found, it may
be possible to get people to tag quite a lot of data
without them even realizing it.
3
http://www.freebase.com/
4
http://www.trueknowledge.com/
58
The second issue is being able to recruit suf-
ficient numbers of useful players to make the re-
sults robust. Both of these issues have been ad-
dressed in the incentive structures of Phrase De-
tectives (Chamberlain et al, 2009).
Other problems still remain, most important of
which is to ensure the quality of the annotated
data. We have identified four aspects that need to
be addressed to control annotation quality:
? Ensuring users understand the task
? Attention slips
? Malicious behaviour
? Genuine ambiguity of data
These issues have been addressed at the design
stage of the project (Kruschwitz et al, 2009).
The goal of the game is to identify relationships
between words and phrases in a short text. An ex-
ample of a task would be to highlight an anaphor-
antecedent relation between the markables (sec-
tions of text) ?This parrot? and ?He? in ?This parrot
is no more! He has ceased to be!? Markables are
identified in the text by automatic pre-processing.
There are two ways to annotate within the game:
by selecting a markable that corefers to another
one (Annotation Mode); or by validating a deci-
sion previously submitted by another player (Vali-
dation Mode).
Annotation Mode (see Figure 1) is the simplest
way of collecting judgments. The player has to lo-
cate the closest antecedent markable of an anaphor
markable, i.e. an earlier mention of the object. By
moving the cursor over the text, markables are re-
vealed in a bordered box. To select it the player
clicks on the bordered box and the markable be-
comes highlighted. They can repeat this process if
there is more than one antecedent markable (e.g.
for plural anaphors such as ?they?). They submit
the annotation by clicking the Done! button.
The player can also indicate that the highlighted
markable has not been mentioned before (i.e. it is
not anaphoric), that it is non-referring (for exam-
ple, ?it? in ?Yeah, well it?s not easy to pad these
Python files out to 150 lines, you know.?) or that
it is the property of another markable (for exam-
ple, ?a lumberjack? being a property of ?I? in ?I
wanted to be a lumberjack!?).
In Validation Mode (see Figure 2) the player
is presented with an annotation from a previous
Figure 2: A screenshot of the Validation Mode.
player. The anaphor markable is shown with the
antecedent markable(s) that the previous player
chose. The player has to decide if he agrees with
this annotation. If not he is shown the Annotation
Mode to enter a new annotation.
In the game groups of players work on the same
task over a period of time as this is likely to lead
to a collectively intelligent decision (Surowiecki,
2005). An initial group of players are asked to an-
notate a markable. If all the players agree with
each other then the markable is considered com-
plete.
However it is likely that the first group of play-
ers will not agree with each other (62% of mark-
ables are given more than one relationship). In this
case each unique relationship for the markable is
validated by another group of players. This type of
validation has also been proposed elsewhere, e.g.
(Krause and Aras, 2009).
When the users register they begin with the
training phase of the game. Their answers are
compared with Gold Standard texts to give them
feedback on their decisions and to get a user rat-
ing, which is used to determine whether they need
more training. Contextual instructions are also
available during the game.
The corpus used in the game is created from
short texts including, for example, Wikipedia arti-
cles selected from the ?Featured Articles? and the
page of ?Unusual Articles?; stories from Project
Gutenberg including Aesop?s Fables, Sherlock
Holmes and Grimm?s Fairy Tales; and dialogue
texts from Textfile.com.
59
Expert 1 vs. Expert 2 Expert 1 vs. Game Expert 2 vs. Game
Overall agreement 94.1% 84.5% 83.9%
DN agreement 93.9% 96.0% 93.1%
DO agreement 93.3% 72.7% 70.0%
NR agreement 100.0% 100.0% 100.0%
PR agreement 100.0% 0.0% 0.0%
Table 1: Agreement figures for overall, discourse-new (DN), discourse-old (DO), non-referring (NR)
and property (PR) attributes.
4 Results
The first public version of Phrase Detectives
went live in December 2008. 1.1 million words
have been converted and made ready for annota-
tion. Over 920 players have submitted more than
380,000 annotations and validations of anaphoric
relations. 46 documents have been fully anno-
tated, meaning that at least 8 players have ex-
pressed their judgment on each markable, and
each distinct anaphoric relation that these players
assigned has been checked by four more players.
To put this in perspective, the GNOME corpus,
produced by traditional methods, included around
3,000 annotations of anaphoric relations (Poesio,
2004a) whereas OntoNotes
5
3.0, with 1 million
words, contains around 140,000 annotations.
4.1 Agreement on annotations
A set of tools were developed to examine the de-
cisions of the players, and address the following
questions:
? How do the collective annotations produced
by the game compare to annotations assigned
by an expert annotator?
? What is the agreement between two experts
annotating the same texts?
The answer to the first question will tell us
whether the game is indeed successful at obtain-
ing anaphoric annotations collaboratively within
the game context. Anaphoric annotations are how-
ever considered much harder than other tasks such
as part-of-speech tagging. Therefore we ask the
second question which will give us an upper bound
of what can be expected from the game in the best
possible case.
We analysed five completed documents from
the Wikipedia corpus containing 154 markables.
5
http://www.ldc.upenn.edu
We first looked at overall agreement and then
broke it down into individual types of anaphoric
relations. The following types of relation can be
assigned by players:
? DN (discourse-new): this markable has no
anaphoric link to any previous markable.
? DO (discourse-old): this markable has an
anaphoric link and the player needs to link
it to the most recent antecedent.
? NR (non-referring): this markable does not
refer to anything e.g. pleonistic ?it?.
? PR (property attribute): this markable repre-
sents a property of a previously mentioned
markable.
DN is the most common relation with 70% of all
markables falling in this category. 20% of mark-
ables are DO and form a coreference chain with
markables previously mentioned. Less than 1% of
markables are non-referring. The remaining mark-
ables have been identified as property attributes.
Each document was also manually annotated in-
dividually by two experts. Overall, we observe
84.5% agreement between Expert 1 and the game
and 83.9% agreement between Expert 2 and the
game. In other words, in about 84% of all cases the
relation obtained from the majority vote of non-
experts was identical to the one assigned by an ex-
pert. Table 1 gives a detailed breakdown of pair-
wise agreement values.
The agreement between the two experts is
higher than between an expert and the game. This
on its own is not surprising. However, an indi-
cation of the difficulty of the annotation task is the
fact that the experts only agree in 94% of all cases.
This can be seen as an upper boundary of what we
might get out of the game.
Furthermore, we see that the figures for DN are
very similar for all three comparisons. This seems
to be the easiest type of relation to be detected.
60
DO relations appear to be more difficult to de-
tect. However if we relax the DO agreement con-
dition and do not check what the antecedent is, we
get agreement figures above 90% in all cases: al-
most 97% between the two experts and between
91% and 93% when comparing an expert with the
game. A number of these cases which are assigned
as DO but with different antecedents are actually
coreference chains which link to the same object.
Extracting coreference chains from the game is
part of the future work.
Although non-referring markables are rare, they
are correctly identified in every case. We additon-
ally checked every completed markable identified
as NR in the corpus and found that there was 100%
precision in 54 cases.
Property (PR) relations are very hard to identify
and not a single one resulted from the game.
4.2 Disagreement on annotations
Disagreements between experts and the game
were examined to understand whether the game
was producing a poor quality annotation or
whether the markable was in fact ambiguous.
These are cases where the gold standard as cre-
ated by an expert is not the interpretation derived
from the game.
? In 60% of all cases where the game proposed
a relation different from the expert annota-
tion, the expert marked this relation to be
a possible interpretation as well. In other
words, the majority of disagreements are not
false annotations but alternatives such as am-
biguous interpretations or references to other
markables in the same coreference chain. If
we counted these cases as correct, we get an
agreement ratio of above 93%, close to pair-
wise expert agreement.
? In cases of disagreement the relation identi-
fied by the expert was typically the second or
third highest ranked relation in the game.
? The cumulative score of the expert relation
(as calculated by the game) in cases of dis-
agreement was 4.5, indicating strong player
support for the expert relation even though it
wasn?t the top answer. A relation with a score
of zero would be interpreted as one that has
as many players supporting it as it has players
disagreeing.
4.3 Discussion
There are very promising results in the agreement
between an expert and the top answer produced
from the game. By ignoring property relations and
the identification of coreference chains, the results
are close to what is expected from an expert. The
particular difficulty uncovered by this analysis is
the correct identification of properties attributes.
The analysis of markables with disagreement
show that some heuristics and filtering should be
applied to extract the highest quality decisions
from the game. In many of the cases the game
recorded plausible interpretations of different re-
lations, which is valuable information when ex-
ploring more difficult and ambiguous markables.
These would also be the markables that automatic
anaphora resolution systems would have difficulty
solving.
The data that was used to generate the results
was not filtered in any way. It would be possible
to ignore annotations from users who have a low
rating (judged when players annotate a gold stan-
dard text). Annotation time could also be a factor
in filtering the results. On average an annotation
takes 9 seconds in Annotation Mode and 11 sec-
onds in Validation Mode. Extreme variation from
this may indicate that a poor quality decision has
been made.
A different approach could be to identify those
users who have shown to provide high quality in-
put. A knowledge source could be created based
on input from these users and ignore everything
else. Related work in this area applies ideas from
citation analysis to identify users of high expertise
and reputation in social networks by, e.g., adopting
Kleinberg?s HITS algorithm (Yeun et al, 2009) or
Google?s PageRank (Luo and Shinaver, 2009).
The influence of document type may have a sig-
nificant impact on both the distribution of mark-
able types as well as agreement between ex-
perts and the game. We have only analysed the
Wikipedia documents, however discourse texts
from Gutenberg may provide different results.
5 Conclusions
This first detailed analysis of the annotations col-
lected from a collaborative game aiming at a large
anaphorically annotated corpus has demonstrated
that high-quality natural language resources can
be collected from non-expert users. A game ap-
proach can therefore be considered as a possible
61
alternative to expert annotations.
We expect that the finally released corpus will
apply certain heuristics to address the cases of dis-
agreement between experts and consensus derived
from the game.
6 Future Work
This paper has focused on percentage agreement
between experts and the game output but this is
a very simplistic approach. Various alternative
agreement coefficients have been proposed that
correct for chance agreement. One such measure
is Cohen?s ? (Cohen, 1960) which we are using to
perform a more indepth analysis of the data.
The main part of our future work remains the
creation of a very large annotated corpus. To
achieve this we are converting source texts to in-
clude them in the game (our aim is a 100M word
corpus). We have already started converting texts
in different languages to be included in the next
version of the game.
Acknowledgments
ANAWIKI is funded by a grant from the Engineer-
ing and Physical Sciences Research Council (EP-
SRC), grant number EP/F00575X/1. Thanks to
Daniela Goecke, Nils Diewald, Maik St?uhrenberg
and Daniel Jettka (University of Bielefeld), Mark
Schellhase (University of Essex) and all the play-
ers who have contributed to the project
References
P. N. Bennett, D. M. Chickering, and A. Mitya-
gin. 2009. Learning consensus opinion: min-
ing data from a labeling game. In Proceedings of
the 18th International World Wide Web Conference
(WWW2009), pages 121?130, Madrid.
L. Burnard. 2000. The British National Corpus Ref-
erence guide. Technical report, Oxford University
Computing Services, Oxford.
J. Chamberlain, M. Poesio, and U. Kruschwitz. 2008.
Phrase Detectives - A Web-based Collaborative An-
notation Game. In Proceedings of I-Semantics,
Graz.
J. Chamberlain, M. Poesio, and U. Kruschwitz. 2009.
A new life for a dead parrot: Incentive structures in
the Phrase Detectives game. In Proceedings of the
Webcentives Workshop at WWW?09, Madrid.
J. Cohen. 1960. A coefficient of agreement for nom-
inal scales. Educational and Psychological Mea-
surement, 20(1):37?46.
E. Hovy, M. Marcus, M. Palmer, L. Ramshaw, and
R. Weischedel. 2006. OntoNotes: The 90% Solu-
tion. In Proceedings of HLT-NAACL06.
N. L. Johnson, S. Rasmussen, C. Joslyn, L. Rocha,
S. Smith, and M. Kantor. 1998. Symbiotic Intel-
ligence: Self-Organizing Knowledge on Distributed
Networks Driven by Human Interaction. In Pro-
ceedings of the Sixth International Conference on
Artificial Life. MIT Press.
D. Jurafsky and J. H. Martin. 2008. Speech and Lan-
guage Processing- 2
nd
edition. Prentice-Hall.
M. Krause and H. Aras. 2009. Playful tagging folkson-
omy generation using online games. In Proceedings
of the 18th International World Wide Web Confer-
ence (WWW2009), pages 1207?1208, Madrid.
M. Kr?otzsch, D. Vrande`ci?c, M. V?olkel, H. Haller, and
R. Studer. 2007. Semantic Wikipedia. Journal of
Web Semantics, 5:251?261.
U. Kruschwitz, J. Chamberlain, and M. Poesio. 2009.
(Linguistic) Science Through Web Collaboration in
the ANAWIKI Project. In Proceedings of Web-
Sci?09, Athens.
X. Luo and J. Shinaver. 2009. MultiRank: Reputation
Ranking for Generic Semantic Social Networks. In
Proceedings of the WWW 2009 Workshop on Web
Incentives (WEBCENTIVES?09), Madrid.
M. Poesio. 2004a. Discourse annotation and semantic
annotation in the gnome corpus. In Proceedings of
the ACL Workshop on Discourse Annotation.
M. Poesio. 2004b. The MATE/GNOME scheme for
anaphoric annotation, revisited. In Proceedings of
SIGDIAL.
P. Singh. 2002. The public acquisition of com-
monsense knowledge. In Proceedings of the AAAI
Spring Symposium on Acquiring (and Using) Lin-
guistic (and World) Knowledge for Information Ac-
cess, Palo Alto, CA.
M. St?uhrenberg, D. Goecke, N. Diewald, A. Mehler,
and I. Cramer. 2007. Web-based annotation of
anaphoric relations and lexical chains. In Proceed-
ings of the ACL Linguistic Annotation Workshop,
pages 140?147.
J. Surowiecki. 2005. The Wisdom of Crowds. Anchor.
L. von Ahn, R. Liu, and M. Blum. 2006. Peekaboom:
a game for locating objects in images. In Proceed-
ings of CHI ?06, pages 55?64.
L. von Ahn. 2006. Games with a purpose. Computer,
39(6):92?94.
C. A. Yeun, M. G. Noll, N. Gibbins, C. Meinel, and
N. Shadbolt. 2009. On Measuring Expertise in Col-
laborative Tagging Systems. In Proceedings of Web-
Sci?09, Athens.
62
Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 961?968
Manchester, August 2008
Coreference Systems based on Kernels Methods
Yannick Versley
SFB 441
University of Tu?bingen
versley@sfs.uni-tuebingen.de
Alessandro Moschitti
DISI
University of Trento
moschitti@disi.unitn.it
Massimo Poesio
DISI
University of Trento
massimo.poesio@unitn.it
Xiaofeng Yang
Data Mining Department
Institute for Infocomm Research
xiaofengy@i2r.a-star.edu.sg
Abstract
Various types of structural information -
e.g., about the type of constructions in
which binding constraints apply, or about
the structure of names - play a central role
in coreference resolution, often in combi-
nation with lexical information (as in ex-
pletive detection). Kernel functions ap-
pear to be a promising candidate to capture
structure-sensitive similarities and com-
plex feature combinations, but care is re-
quired to ensure they are exploited in the
best possible fashion. In this paper we
propose kernel functions for three subtasks
of coreference resolution - binding con-
straint detection, expletive identification,
and aliasing - together with an architec-
ture to integrate them within the standard
framework for coreference resolution.
1 Introduction
Information about coreference relations?i.e.,
which noun phrases are mentions of the same
entity?has been shown to be beneficial in a great
number of NLP tasks, including information
extraction (McCarthy and Lehnert 1995), text
planning (Barzilay and Lapata 2005) and sum-
marization (Steinberger et al 2007). However,
the performance of coreference resolvers on
unrestricted text is still quite low. One reason
for this is that coreference resolution requires a
great deal of information, ranging from string
matching to syntactic constraints to semantic
knowledge to discourse salience information to
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
full common sense reasoning (Sidner 1979; Hobbs
1978, 1979; Grosz et al 1995; Vieira and Poesio
2000; Mitkov 2002). Much of this information
won?t be available to robust coreference resolvers
until better methods are found to represent and
encode common sense knowledge; but part of
the problem is also the need for better methods
to encode information that is in part structural,
in part lexical. Enforcing binding constraints
?e.g., ruling out Peter as antecedent of him in (1a)
requires recognizing that the anaphor occurs in a
particular type of construction (Chomsky 1981;
Lappin and Leass 1994; Yang et al 2006) whose
exact definition however has not yet been agreed
upon by linguists (indeed, it may only be definable
in a graded sense (Sturt 2003; Yang et al 2006)),
witness examples like (1b). Parallelism effects are
a good example of structural information inducing
preferences rather than constraints. Recognizing
that It in examples such as (1c,d) are expletives
requires a combination of structural information
and lexical information (Lappin and Leass 1994;
Evans 2001). But some sort of structure also
underlies our interpretation of other types of
coreference: e.g., knowledge about the structure
of names certainly plays a role in recognizing
that BJ Habibie is a possible antecedent for Mr.
Habibie.
(1) a. John thinks that Peter hates him.
b. John hopes that Jane is speaking only to
himself.
c. It?s lonely here.
d. It had been raining all day.
The need to capture such information suggests
a role for kernel methods (Vapnik 1995) in coref-
erence resolution. Kernel functions make it pos-
sible to capture the similarity between structures
961
without explicitly enumerating all the substruc-
tures, and have therefore been shown to be a vi-
able approach to feature engineering for natural
language processing for any task in which struc-
tural information plays a role, e.g. (Collins and
Duffy 2002; Zelenko et al 2003; Giuglea and Mos-
chitti 2006; Zanzotto and Moschitti 2006; Mos-
chitti et al 2007). Indeed, they have already been
used in NLP to encode the type of structural in-
formation that plays a role in binding constraints
(Yang et al 2006); however, the methods used in
this previous work do not make it possible to ex-
ploit the full power of kernel functions. In this
work, we extend the use of kernel functions for
coreference by designing and testing kernels for
three subtasks of the coreference task:
? Binding constraints
? Expletive detection
? Aliasing
and developing distinct classifiers for each of these
tasks. We show that our developed kernels produce
high accuracy for both distinct classifiers for these
subtasks as well as for the complete coreference
system.
In the remainder: Section 2, briefly describes
the basic kernel functions that we used; Section
3 illustrates our new kernels for expletive, binding
and name alias detection along with a coreference
context kernel; Section 4 reports the experiments
on individual classifiers on expletives, binding and
names whereas Section 5 shows the results on the
complete coreference task; Finally, Section 6 de-
rives the conclusions.
2 Kernel for Structured Data
We used three kernel functions in this work: the
String Kernel (SK) proposed in Shawe-Taylor and
Cristianini (2004) to evaluate the number of sub-
sequences between two sequences, the Syntactic
Tree Kernel (STK; see Collins and Duffy 2002)
which computes the number of syntactic tree frag-
ments and the Partial Tree Kernel (PTK; see Mos-
chitti 2006) which provides a more general repre-
sentation of trees in terms of tree fragments. We
discuss each in turn.
2.1 String Kernels (SK)
The string kernels that we consider count the num-
ber of substrings shared by two sequences contain-
ing gaps, i.e. some of the characters of the original
NP 
D N 
a 
  cat 
NP 
D N 
NP 
D N 
a 
NP 
D N 
NP 
D N 
VP 
V 
brought 
a 
   cat 
  cat 
NP 
D N 
VP 
V 
a 
   cat 
NP 
D N 
VP 
V 
N 
   cat 
D 
a 
V 
brought 
N 
Mary 
? 
Figure 1: A tree with some of its STFs .
NP 
D N 
VP 
V 
brought 
a 
   cat 
NP 
D N 
VP 
V 
a 
   cat 
NP 
D N 
VP 
a 
   cat 
NP 
D N 
VP 
a 
NP 
D 
VP 
a 
NP 
D 
VP 
NP 
N 
VP 
NP 
N 
NP NP 
D N D 
NP 
? 
VP 
Figure 2: A tree with some of its PTFs.
string are skipped. Gaps penalize the weight asso-
ciated with the matched substrings. More in detail,
(a) longer subsequences receive lower weights.
(b) Valid substrings are sequences of the original
string with some characters omitted, i.e. gaps. (c)
Gaps are accounted by weighting functions and (d)
symbols of a string can also be whole words, i.e.
the word sequence kernel Cancedda et al (2003).
2.2 Tree Kernels
The main idea underlying tree kernels is to com-
pute the number of common tree fragments be-
tween two trees without explicitly considering the
whole fragment space. The type of fragments char-
acterize different kernel functions. We consider
syntactic tree fragments (STFs) and partial tree
fragments (PTFs)
2.2.1 Syntactic Tree Kernels (STK)
An STF is a connected subset of the nodes and
edges of the original tree, with the constraint that
any node must have all or none of its children. This
is equivalent to stating that the production rules
contained in the STF cannot be partial. For ex-
ample, Figure 1 shows a tree with its PTFs: [VP [V
NP]] is an STF, [VP [V]] or [VP [NP]] are not STFs.
2.2.2 Partial Tree Kernel (PTK)
If we relax the production rule constraint over
the STFs, we obtain a more general substructure
type, i.e. PTF, generated by the application of par-
tial production rules, e.g. Figure 2 shows that [VP
[NP[D]]] is indeed a valid fragment. Note that
PTK can be seen as a STK applied to all possible
child sequences of the tree nodes, i.e. a string ker-
nel combined with a STK.
2.3 Kernel Engineering
The Kernels of previous section are basic functions
that can be applied to feature vectors, strings and
962
trees. In order to make them effective for a specific
task, e.g. for coreference resolution: (a) we can
combine them with additive or multiplicative op-
erators and (b) we can design specific data objects
(vectors, sequences and tree structures) for the tar-
get tasks.
It is worth noting that a basic kernel applied to
an innovative view of a structure yields a new ker-
nel (e.g. Moschitti and Bejan (2004); Moschitti
et al (2006)), as we show below:
Let K(t
1
, t
2
) = ?(t
1
) ? ?(t
2
) be a basic ker-
nel, where t
1
and t
2
are two trees. If we map t
1
and t
2
into two new structures s
1
and s
2
with a
mapping ?
M
(?), we obtain: K(s
1
, s
2
) = ?(s
1
) ?
?(s
2
) = ?(?
M
(t
1
)) ? ?(?
M
(t
2
)) = ?
?
(t
1
) ?
?
?
(t
2
)=K?(t
1
, t
2
), which is a noticeably different
kernel induced by the mapping ?? = ? ? ?
M
.
3 Kernels for Coreference Resolution
In this paper we follow the standard learning ap-
proach to coreference developed by Soon et al
(2001) and also used the few variants in Ng and
Cardie (2002). In this framework, training and
testing instances consist of a pair (anaphor, an-
tecedent). During training, a positive instance is
created for each anaphor encountered by pairing
the anaphor with its closest antecedent; each of the
non-coreferential mentions between anaphor and
antecedent is used to produce a negative instance.
During resolution, every mention to be resolved is
paired with each preceding antecedent candidate
to form a testing instance. This instance is pre-
sented to the classifier which then returns a class
label with a confidence value indicating the likeli-
hood that the candidate is the antecedent.
The nearest candidate with a positive classifica-
tion will be selected as the antecedent of the pos-
sible anaphor. The crucial point is that in this ap-
proach, the classifier is trained to identify positive
and negative instances of the resolution process. In
previous work on using kernel functions for coref-
erence (Yang et al 2006), structural information
in the form of tree features was included in the
instances. This approach is appropriate for iden-
tifying contexts in which the binding constraints
apply, but not, for instance, to recognize exple-
tives. In this work we adopted therefore a more
general approach, in which separate classifiers are
used to recognize each relevant configuration, and
their output is then used as an input to the coref-
erence classifier. In this section we discuss the
types of structures and kernel functions we used
for three different kinds of classifiers: expletive,
binding and alias classifiers. We then present the
results of these classifiers, and finally the results
with the coreference resolver as a whole.
3.1 Expletive Kernels
In written text, about a third of the occurrences
of the pronoun it are not coreferent to a previ-
ous mention, but either refer to a general discourse
topic (it?s a shame) or do not refer at all, as in the
case of extraposed subjects (it is thought that . . . )
or weather verbs (it?s raining). It is desirable to
minimize the impact that these non-anaphoric pro-
nouns have on the accuracy of a anaphora resolu-
tion: Lappin and Leass (1994), for example, use
several heuristics to filter out expletive pronouns,
including a check for patterns including modal ad-
jectives (it is good/necessary/. . . that . . . ), and cog-
nitive verbs (it is thought/believed/. . . that . . . ).
Newer approaches to the problem use machine-
learning on hand-annotated examples: Evans
(2001) compares a shallow approach based on
surrounding lemmas, part-of-speech tags, and the
presence of certain elements such as modal adjec-
tives and cognitive verbs, trained on 3171 exam-
ples from Susanne and the BNC to a reimplemen-
tation of a pattern-based approach due to Paice and
Husk (1987) and finds that the shallower machine-
learning approach compares favorably to it. Boyd
et al (2005) use an approach that combines some
of Evans? shallow features with hand-crafted pat-
terns in a memory based learning approach and
find that the more informative features are ben-
eficial for the system?s performance (88% accu-
racy against 71% for their reimplementation using
Evans? shallow features).
Evans? study also mentions that incorporating
the expletive classifier as a filter for a pronoun re-
solver gives a gain between 2.86% (for manually
determined weights) and 1% (for automatically op-
timized weights).
Tree kernels are a good fit for expletive classi-
fication since they can naturally represent the lex-
ical and structural context around a word. Our fi-
nal classifier uses the combination of an unmodi-
fied tree (UT) (where the embedding clause or verb
phrase of the pronoun is used as a tree), and a tree
that only preserves the most salient structural fea-
tures (ST).
The reduced representation prunes all nodes that
963
would not be seen as indicative in a pattern ap-
proach, essentially keeping verb argument struc-
ture and important lexical items, such as the gov-
erning verb and, in the case of copula construc-
tions, the predicate. For example, the phrase
(S (NP (PRP It))
(VP (VBZ has)
(NP (NP (DT no) (NN bearing))
(PP (IN on)
(NP (NP (PRP$ our)
(NN work)
(NN force))
(NP (NN today)))))
(. .))
would be reduced to the ST:
(S-I (NP-I (PRP-I It))
(VP (VBX have)
(NP))
(.))
or, in a similar fashion,
(S (NP (PRP it))
(VP (VBZ ?s)
(NP (NP (NN time))
(PP (IN for)
(NP (PRP$ their)
(JJ biannual)
(NN powwow))))))
would just be represented as the ST:
(S-I (NP-I (PRP-I it))
(VP (BE VBZ)
(NP-PRD (NN time))))
3.2 Binding Kernels
The resolution of pronominal anaphora heavily re-
lies on the syntactic information and relationships
between the anaphor and the antecedent candi-
dates, including binding and other constraints, but
also context-induced preferences in sub-clauses.
Some researchers (Lappin and Leass 1994;
Kennedy and Boguraev 1996) use manually de-
signed rules to take into account the grammati-
cal role of the antecedent candidates as well as
the governing relations between the candidate and
the pronoun, while others use features determined
over the parse tree in a machine-learning approach
(Aone and Bennett 1995; Yang et al 2004; Luo
and Zitouni 2005). However, such a solution has
limitations, since the syntactic features have to be
selected and defined manually, and it is still partly
an open question which syntactic properties should
be considered in anaphora resolution.
We follow (Yang et al 2006; Iida et al 2006) in
using a tree kernel to represent structural informa-
tion using the subtree that covers a pronoun and its
antecedent candidate. Given a sentence like ?The
Figure 3: The structure for binding detection for
the instance inst(?the man?, ?him?) in the sentence
?the man in the room saw him?
man in the room saw him.?, we represent the syn-
tactic relation between ?The man? and ?him?, by
the shortest node path connecting the pronoun and
the candidate, along with the first-level of the node
children in the path.
Figure 3 graphically shows such tree highlighted
with dash lines. More in detail we operate the fol-
lowing tree transformation:
(a) To distinguish from other words, we explic-
itly mark up in the structured feature the pronoun
and the antecedent candidate under consideration,
by appending a string tag ?ANA? and ?CANDI?
in their respective nodes, i.e. ?NN-CANDI? for
?man? and ?PRP-ANA? for ?him?.
(b) To reduce the data sparseness, the leaf nodes
representing the words are not incorporated in the
feature, except that the word is the word node of
the ?DET? type (this is to indicate the lexical prop-
erties of an expression, e.g., whether it is a definite,
indefinite or bare NP).
(c) If the pronoun and the candidate are not in the
same sentence, we do not include the nodes denot-
ing the sentences (i.e., ?S? nodes) before the can-
didate or after the pronoun.
The above tree structures will be jointly used
with the basic STK which extracts tree fragments
able to characterize the following information: (a)
the candidate is post-modified by a preposition
phrase, (the node ?PP? for ?in the room? is in-
cluded), (b) the candidate is a definite noun phrase
(the article word ?the? is included), (c) the candi-
date is in a subject position (NP-S-VP structure),
(d) the anaphor is an object of a verb (the node
?VB? for ?saw? is included) and (e) the candidate
is c-commanding the anaphor (the parent of the
NP node for ?the main in the room? is dominat-
ing the anaphor (?him?), which are important for
reference determination in the pronoun resolution.
964
3.3 Encoding Context via Word Sequence
Kernel
The previous structures aim at describing the in-
teraction between one referential and one referent;
if such interaction is observed on another mention
pair, an automatic algorithm can establish if they
corefer or not. This kind of information is the most
useful to characterize the target problem, however,
the context in which such interaction takes place is
also very important. Indeed, natural language pro-
poses many exceptions to linguistic rules and these
can only be detect by looking at the context. To be
able to represent context words or phrases, we use
context word windows around the mentions and
the subsequence kernel function (see section 2.1)
to extract many features from it.
For example, in the context of ?and so Bill
Gates says that?, a string kernel would ex-
tract features including: Bill Gates says that,
says that, Gates, Gates says that, Bill says that,
so Gates says that, and so that and so on.
Name Alias
BJ Habibie Mr. Habibie
Federal Express Fedex
Ju Rong Zhi Ju
Table 1: Examples of coreferent named entities
(aliases) taken from the MUC 6 corpus.
3.4 Kernels for Alias Resolution
Most methods currently employed by coreference
resolution (CR) systems for identifying coreferent
named entities, i.e. aliases, are fairly simplistic in
nature, relying on simple surface features such as
the edit distance between two strings representing
names. We investigate the potential of using the
structure contained within names. This can be very
useful to solve complex cases like those shown in
Table 1, taken from the MUC 6 corpus (Chinchor
and Sundheim 2003). For this purpose, we add
syntactic information to the feature set by tagging
the parts of a name (e.g. first name, last name, etc.)
as illustrated in Figure 4.
To automatically extract such structure we used
the High Accuracy Parsing of Name Internal Struc-
ture (HAPNIS) script1. HAPNIS takes a name as
input and returns a tagged name like what is shown
in Figure 4. It uses a series of heuristics in making
its classifications based on information such as the
1The script is freely available at
http://www.cs.utah.edu/ hal/HAPNIS/.
Figure 4: A proper name labeled with syntactic in-
formation.
serial positions of tokens in a name, the total num-
ber of tokens, the presence of meaningful punctua-
tion such as periods and dashes, as well as a library
of common first names which can be arbitrarily ex-
tended to any size. The tag set consists of the fol-
lowing: surname, forename, middle, link, role, and
suffix2.
Once the structure for a name has been de-
rived, we can apply tree kernels to represent it in
the learning algorithms thus avoiding the manual
feature design. Such structures are not based on
any particular grammar, therefore, any tree sub-
part may be relevant. In this case the most suitable
kernel is PTK, which extracts any tree subpart. It
is worth to note that the name tree structure can
be improved by inserting a separate node for each
name character and exploiting the string matching
approximation carried out by PTK. For example,
Microsoft Inc. will have a large match with Mi-
crosoft Incorporated whereas the standard string
matching would be null.
4 Experiments with Coreference Subtask
Classifiers
In these experiments we test the kernels devised for
expletive (see Section 3.1), binding (see Section
3.2) and alias detection (see Section 3.4), to study
the level of accuracy reachable by our kernel-based
classifiers. The baseline framework is constituted
by SVMs along with a polynomial kernel over the
Soon et al?s features.
4.1 Experiments on Expletive Classification
We used the BBN Pronoun corpus3 as a source of
examples, with the training set consisting of sec-
tions 00-19, yielding more than 5800 instances of
2Daume? reports a 99.1% accuracy rate on his test data set.
We therefore concluded that it was sufficient for our purposes.
3Ralph Weischedel and Ada Brunstein (2005): BBN Pro-
noun Coreference and Entity Type Corpus, LDC2005T33
965
it, with the testing set consisting of sections 20 and
21, using the corresponding parses from the Penn
Treebank for the parse trees. Additionally, we re-
port on the performance of the classifier learnt on
only the first 1000 instances to verify that our ap-
proach also works for small datasets. The results
in Table 2 show that full tree (UT) achieves good
results whereas the salient tree (ST) leads to a bet-
ter ability to generalize, and the combination ap-
proach outperforms both individual trees.
BBN large BBN small
Prec Recl Acc Prec Recl Acc
UT 83.87 61.54 84.35 78.76 52.66 80.85
ST 78.08 67.46 83.98 77.61 61.54 82.50
UT+ST 81.12 68.64 85.27 80.74 64.50 84.16
Table 2: Results for kernel-based expletive detec-
tion (using STK)
Note that the accuracy we get by training on
1000 examples (84% accuracy; see the small col-
umn in Table 2) is better than Boyd?s replication of
Evans (76% accuracy) or their decision tree clas-
sifier (81% accuracy) even though Boyd et al?s
dataset is three times bigger. On the other hand,
Boyd et als full system, which uses substantial
hand-crafted knowledge, gets a still better result
(88% accuracy), which is also higher than the ac-
curacy of our classifier even when trained on the
full 5800 instances.
MUC-6
Prec Recl F
Soon et al 51.25 55.51 53.29
STK 71.93 55.41 62.59
Table 3: Binding classifier: coreference classifica-
tion on same-sentence pronouns
4.2 Experiments with the Binding Classifier
To assess the effect of the binding classifier on
same-sentence pronoun links, we extracted 1398
mention pairs from the MUC-6 training data where
both mentions were in the same sentence and at
least one item of the pair included a pronoun, us-
ing the first 1000 for training and the remaining
398 examples for testing. The results (see Table 3)
show that the syntactic tree kernel (STK) consider-
ably improves the precision of classification of the
Soon et al?s features.
4.3 Experiments on Alias Classification
For our preliminary experiments, we extracted
only pairs in the MUC 6 testing set in which both
mentions were proper names, as determined by
the coreference resolver?s named entity recognizer.
This set of proper names contained about 37,000
pairs of proper names of which about 600 were
positive instances. About 5,500 pairs were ran-
domly selected as test instances and the rest were
used for training.
In the first experiment, we trained a decision
tree classifier to detect if two names are aliases.
For this task, we used either the string kernel score
over the sequence of characters or the edit distance.
The results in Table 4 show that the string kernel
score performs better by 21.6 percentage points in
F-measure.
In the second experiments we used SVMs
trained with the string kernel over the name-
character sequences and with PTK, which takes
into account the structure of names. The re-
sults in Table 5 show that the structure improves
alias detection by almost 5 absolute percent points.
This suggests that an effective coreference sys-
tem should embed PTK and name structures in the
coreference pair representation.
Recall Precision F-measure
String kernel 49.5% 60.8% 54.6%
Edit distance 23.9% 53.1% 33.0%
Table 4: Decision-tree based classification of name
aliases using string kernels and edit distance.
Recall Precision F-measure
String kernel 58.4% 67.5% 62.6%
PTK 64.8% 70.0% 67.3%
Table 5: SVM-based classification of name aliases
using string kernels and tree-based feature.
5 Experiments on Coreference Systems
In this section we evaluate the contribution in the
whole coreference task of the expletive classifier
and the binding kernel. The predictions of the for-
mer are used as a feature of our basic coreference
system whereas the latter is used directly in the
coreference classifier by adding it to the polyno-
mial kernel of the basic system.
Our basic system is based on the standard learn-
ing approach to coreference developed by Soon
et al (2001). It uses the features from Soon et
al?s work, including lexical properties, morpho-
logic type, distance, salience, parallelism, gram-
matical role and so on. The main difference with
966
Soon et al (2001) is the use of SVMs along with a
polynomial kernel.
MUC-6
Prec Recl F
plain 65.2 66.9 66.0
plain+expletive 66.1 66.9 66.5
upper limit 70.0 66.9 68.4
Table 6: Expletive classification: influence on pro-
noun resolution
5.1 Influence of Expletive classification
To see how useful a classifier for expletives can
be, we conducted experiments using the expletive
classifier learned on the BBN pronoun corpus on
the MUC-6 corpus. Preliminary experiments indi-
cated that perfect detection of expletives (i.e. using
gold standard annotation) could raise the precision
of pronoun resolution from 65.2% to 70.0%, yield-
ing a 2.4% improvement in the F-score for pronoun
resolution alone, or 0.6% improvement in the over-
all coreference F-score (see Table 6).
For a more realistic assessment, we used the
classifier learned on the BBN pronoun corpus ex-
amples as an additional feature to gauge the im-
provement that could be achieved using it. While
the gain in precision is small even in comparison
to the achievable error reduction, we need to keep
in mind that our baseline is in fact a well-tuned
system.
MUC-6 ACE02-BNews
R P F R P F
PK 64.3 63.1 63.7 58.9 68.1 63.1
PK+TK 65.2 80.1 71.9 65.6 69.7 67.6
Table 7: Results of the pronoun resolution
5.2 Binding and Context Kernels
In these experiments, we compared our corefer-
ence system based on Polynomial Kernel (PK)
against its combinations with Syntactic Tree Ker-
nels (STK) over the binding structures (Sec. 3.2)
and Word Sequence Kernel (WSK) on context
windows (Sec. 3.3). We experimented with
both the only pronoun and the complete corefer-
ence resolution tasks on the standard MUC-6 and
ACE03-BNews data sets.
On the validation set, the best kernel combina-
tion between PK and STK was STK(T1, T2) ?
PK(~x
1
, ~x
2
)+PK(~x
1
, ~x
2
). Then an improvement
arises when simply summing WSK.
Table 7 lists the results for the pronoun resolu-
tion. We used PK on the Soon et al?s features as
the baseline. On MUC-6, the system achieves a
recall of 64.3% and precision 63.1% and an over-
all F-measure of 63.7%. On ACE02-BNews, the
recall is lower 58.9% but the precision is higher,
i.e. 68.1%, for a resulting F-measure of 63.1%.
In contrast, adding the binding kernel (PK+STK)
leads to a significant improvement in 17% preci-
sion for MUC-6 with a small gain (1%) in recall,
whereas on the ACE data set, it also helps to in-
crease the recall by 7%. Overall, we can see an
increase in F-measure of around 8% for MUC and
4.5% for ACE02-BNews. These results suggest
that the structured feature is very effective for pro-
noun resolution.
MUC-6 ACE02-BNews
R P F R P F
PK 61.5 67.2 64.2 54.8 66.1 59.9
PK+STK 63.4 67.5 65.4 56.6 66.0 60.9
PK+STK+WSK 64.4 67.8 66.0 57.1 65.4 61.0
Table 8: Results of the coreference resolution
Table 8 lists the results on the coreference res-
olution. We note that adding the structured fea-
ture to the polynomial kernel, i.e. using the model
PK+STK, improves the recall of 1.9% for MUC-
6 and 1.8% for ACE-02-BNews and keeps invari-
ant the precision. Compared to pronoun resolu-
tion, the improvement of the overall F-measure is
smaller (about 1%). This occurs since the resolu-
tion of non-pronouns case does not require a mas-
sive use of syntactic knowledge as in the pronoun
resolution problem. WSK further improves the
system?s F1 suggesting that adding structured fea-
tures of different types helps in solving the coref-
erece task.
6 Conclusions
We presented four examples of using kernel-based
methods to take advantage of a structured repre-
sentation for learning problems that arise in coref-
erence systems, presenting high-accuracy classi-
fiers for expletive detection, binding constraints
and same-sentence pronoun resolution, and name
alias matching. We have shown the accuracy
of the individual classifiers for the above tasks
and the impact of expletives and binding classi-
fiers/kernels in the complete coreference resolu-
tion system. The improvement over the individual
and complete tasks suggests that kernel methods
967
are a promising research direction to achieve state-
of-the-art coreference resolution systems.
Future work is devoted on making the use of ker-
nels for coreference more efficient since the size of
the ACE-2 corpora prevented us to directly use the
combination of all kernels that we designed. In this
paper, we have also studied a solution which re-
lates to factoring out decisions into separate clas-
sifiers and using the decisions as binary features.
However, this solution shows some loss in terms of
accuracy. We are currently investigating methods
that allow us to combine the accuracy and flexibil-
ity of the integrated approach with the speed of the
separate classifier approach.
Acknowledgements Y. Versley was funded by the
Deutsche Forschungsgemeinschaft as part of SFB (Collabora-
tive Research Centre) 441. A. Moschitti has been partly sup-
ported by the FP6 IST LUNA project (contract No. 33549).
Part of the work reported in this paper was done at the Johns
Hopkins Summer Workshop in 2007, funded by NSF and
DARPA. We are especially grateful for Alan Jern?s implemen-
tation help for name structure identification.
References
Aone, C. and Bennett, S. W. (1995). Evaluating automated
and manual acquisition of anaphora resolution strategies.
In Proc. ACL 1995, pages 122?129.
Barzilay, R. and Lapata, M. (2005). Modelling local coher-
ence: An entity-based approach. In Proc. of ACL, Ann
Arbor, MI.
Boyd, A., Gegg-Harrison, W., and Byron, D. (2005). Iden-
tifying non-referential it: a machine learning approach in-
corporating linguistically motivated features. In Proc. ACL
WS on Feature Engineering for Machine Learning in Nat-
ural Language Processing.
Cancedda, N., Gaussier, E., Goutte, C., and Renders, J. M.
(2003). Word sequence kernels. JMLR, 3:1059?1082.
Chinchor, N. and Sundheim, B. (2003). Muc 6 corpus. Mes-
sage Understanding Conference (MUC) 6.
Chomsky, N. (1981). Lectures on government and binding.
Foris, Dordrecht, The Netherlands.
Collins, M. and Duffy, N. (2002). New ranking algorithms for
parsing and tagging: kernels over discrete structures and
the voted perceptron. In Proc. ACL 2002, pages 263?270.
Evans, R. (2001). Applying machine learning toward an au-
tomatic classification of it. Literary and Linguistic Com-
puting, 16(1):45?57.
Giuglea, A.-M. and Moschitti, A. (2006). Semantic role la-
beling via framenet, verbnet and propbank. In Proceedings
of Coling-ACL, Sydney, Australia.
Grosz, B., Joshi, A., and Weinstein, S. (1995). Centering: a
framework for modeling the local coherence of discourse.
CL, 21(2):203?225.
Hobbs, J. (1978). Resolving pronoun references. Lingua,
44:339?352.
Hobbs, J. (1979). Resolving pronoun references. Coherence
and Coreference, 3(1):67?90.
Iida, R., Inui, K., and Matsumoto, Y. (2006). Exploiting syn-
tactic patterns as clues in zero-anaphora resolution. In
Proc. Coling/ACL 2006, pages 625?632.
Kennedy, C. and Boguraev, B. (1996). Anaphora for every-
one: pronominal anaphora resolution without a parser. In
Proc. Coling 1996.
Lappin, S. and Leass, H. (1994). An algorithm for pronominal
anaphora resolution. CL, 20(4):525?561.
Luo, X. and Zitouni, I. (2005). Multi-lingual coreference res-
olution with syntactic features. In Proc. HLT/EMNLP 05.
McCarthy, J. and Lehnert, W. (1995). Using decision trees for
coreference resolution. In Proc. IJCAI 1995.
Mitkov, R. (2002). Anaphora resolution. Longman.
Moschitti, A. (2006). Efficient convolution kernels for depen-
dency and constituent syntactic trees. Proc. ECML 2006.
Moschitti, A. and Bejan, C. A. (2004). A semantic kernel for
predicate argument classification. In CoNLL-2004, USA.
Moschitti, A., Pighin, D., and Basili, R. (2006). Semantic
Role Labeling via Tree Kernel Joint Inference. In Pro-
ceedings of CoNLL-X.
Moschitti, A., Quarteroni, S., Basili, R., and Manandhar, S.
(2007). Exploiting syntactic and shallow semantic kernels
for question answer classification. In Proceedings ACL,
Prague, Czech Republic.
Ng, V. and Cardie, C. (2002). Improving machine learning
approaches to coreference resolution. In Proc. ACL 2002.
Paice, C. D. and Husk, G. D. (1987). Towards an automatic
recognition of anaphoric features in english text: The im-
personal pronoun ?it?. Computer Speech and Language,
2:109?132.
Shawe-Taylor, J. and Cristianini, N. (2004). Kernel Methods
for Pattern Analysis. Cambridge University Press.
Sidner, C. (1979). Toward a computational theory of definite
anaphora comprehension in english. Technical report AI-
TR-537, MIT, Cambridge, MA.
Soon, W., Ng, H., and Lim, D. (2001). A machine learning
approach to coreference resolution of noun phrases. CL,
27(4):521?544.
Steinberger, J., Poesio, M., Kabadjov, M., and Jezek, K.
(2007). Two uses of anaphora resolution in summarization.
Information Processing and Management, 43:1663?1680.
Special issue on Summarization.
Sturt, P. (2003). The time-course of the application of binding
constraints in reference resolution. Journal of Memory and
Language.
Vapnik, V. (1995). The Nature of Statistical Learning Theory.
Springer.
Vieira, R. and Poesio, M. (2000). An empirically based sys-
tem for processing definite descriptions. CL, 27(4):539?
592.
Yang, X., Su, J., and Tan, C. (2006). Kernel-based pronoun
resolution with structured syntactic knowledge. In Proc.
COLING-ACL 06.
Yang, X., Su, J., Zhou, G., and Tan, C. (2004). Improving pro-
noun resolution by incorporating coreferential information
of candidates. In Proc. ACL 2004.
Zanzotto, F. M. and Moschitti, A. (2006). Automatic learn-
ing of textual entailments with cross-pair similarities. In
Proceedings of Coling-ACL, Sydney, Australia.
Zelenko, D., Aone, C., and Richardella, A. (2003). Kernel
methods for relation extraction. JMLR, 3(6):1083 ? 1106.
968
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 619?627,
Singapore, 6-7 August 2009.
c?2009 ACL and AFNLP
EEG responds to conceptual stimuli and corpus semantics
Brian Murphy
CIMeC, University of Trento
Rovereto 38068, Italy
brian.murphy@unitn.it
Marco Baroni
CIMeC, University of Trento
Rovereto 38068, Italy
marco.baroni@unitn.it
Massimo Poesio
CIMeC, University of Trento
Rovereto 38068, Italy
massimo.poesio@unitn.it
Abstract
Mitchell et al (2008) demonstrated that
corpus-extracted models of semantic
knowledge can predict neural activation
patterns recorded using fMRI. This
could be a very powerful technique for
evaluating conceptual models extracted
from corpora; however, fMRI is expensive
and imposes strong constraints on data
collection. Following on experiments
that demonstrated that EEG activation
patterns encode enough information to
discriminate broad conceptual categories,
we show that corpus-based semantic rep-
resentations can predict EEG activation
patterns with significant accuracy, and
we evaluate the relative performance of
different corpus-models on this task.
1 Introduction
Models of semantic relatedness induced from cor-
pus data have proven effective in a number of em-
pirical tasks (Sahlgren, 2006) and there is increas-
ing interest in whether distributional information
extracted from corpora correlates with aspects
of speakers? semantic knowledge: see Lund and
Burgess (1996), Landauer and Dumais (1997), Al-
muhareb (2006), Pad?o and Lapata (2007), Schulte
im Walde (2008), among many others. For this
purpose, corpus models have been tested on data-
sets that are based on semantic judgements (met-
alinguistic or meta-cognitive intuitions about syn-
onymy, semantic distance, category-membership)
or behavioural experiments (semantic priming,
property generation, free association). While all
these data are valuable, they are indirect reflec-
tions of semantic knowledge, and when the pre-
dictions they make diverge from those of corpora,
interpretation is problematic: is the corpus model
missing essential aspects of semantics, or are non-
semantic factors biasing the data elicited from in-
formants?
Reading semantic processes and representations
directly from the brain would be an ideal way to
get around these limitations. Until recently, anal-
ysis of linguistic quantities using neural data col-
lected with EEG (measurement at the scalp of volt-
ages induced by neuronal firing) or fMRI (mea-
surement of changes of oxygen concentrations in
the brain tied to cognitive processes) had neither
the advantages of corpora (scale) nor of infor-
mants (finer grained judgements).
However, some clear patterns of differential ac-
tivity have been found for broad semantic classes.
Viewing images of natural (typically animals and
plants) and non-natural (typically artefacts like
tools or vehicles) objects elicits different loci of
activity in fMRI (Martin and Chao, 2001) and
EEG (Kiefer, 2001), that persist across partici-
pants. Differences have also been found in re-
sponse to auditorily or visually presented words of
different lexical classes, such as abstract/concrete,
and verb/noun (Pulverm?uller, 2002). But interpre-
tation of such group results remains somewhat dif-
ficult, as they may be consistent with more than
one distinction: the natural/artefactual division
just mentioned, may rather be between living/non-
living entities, dynamic/static entities, or be based
on embodied experience (e.g. manipulable or not).
More recently, however, machine learning and
other numerical techniques have been successfully
applied to extract semantic information from neu-
ral data in a more discriminative fashion, down
to the level of individual concepts. The work
presented here builds on two strands of previ-
ous work: Murphy et al (2008) use EEG data
to perform semantic categorisation on single stim-
uli; and Mitchell et al (2008) introduce an fMRI-
based method that detects word level distinctions
by learning associations between features of neu-
ral activity and semantic features derived from a
619
corpus. We combine these innovations by intro-
ducing a method that extracts featural represen-
tations from the EEG signal, and uses corpus-
based models to predict word level distinctions in
patterns of EEG activity. The proposed method
achieves a performance level significantly above
chance (also when distinguishing between con-
cepts from the same semantic category, e.g., dog
and cat), and approaching that achieved with
fMRI.
The paper proceeds as follows. The next section
describes a simple behavioural experiment where
Italian-speaking participants had to name photo-
graphic images of mammals and tools while their
EEG activity was being recorded, and continues
to detail how the rich and multidimensional sig-
nals collected were reduced to a small set of op-
timally informative features using a new method.
Section 3 describes a series of corpus-based se-
mantic models derived from both a raw-text web
corpus, and from various parsings of a conven-
tional corpus. In Section 4 we describe the train-
ing of a series of linear models, that each learn
the associations between a set of corpus semantic
features and an individual EEG activity feature.
By combining these models it is possible to pre-
dict the EEG activity pattern for a single unseen
word, and compare this to the observed pattern
for the corresponding concept. Results (Section
5) show that these predictions succeed at a level
significantly above chance, both for coarser dis-
tinctions between words in different superordinate
categories (e.g., differentiating between drill and
gorilla), and, at least for the model based on the
larger web corpus, for those within the same cate-
gory (e.g., drill vs spanner, koala vs gorilla).
2 Neural Activation Data
2.1 Data collection
EEG data was gathered from native speakers of
Italian during a simple behavioural experiment at
the CIMeC/DiSCoF laboratories at Trento Univer-
sity. Seven participants (five male and two fe-
male; age range 25-33; all with college educa-
tion) performed a silent naming task. Each of them
was presented
1
on screen with a series of contrast-
normalised greyscale photographs of tools (gar-
den and work tools) and land mammals (exclud-
ing emotionally valent domesticated animals and
1
Using the E-Prime software package: http://www.
pstnet.com/e-prime/.
~1.5s0.5s 0.5s 2s
Figure 1: Presentation of image stimuli
predators), for which they had to think of the most
appropriate name (see figure 1). They were not ex-
plicitly asked to group the entities into superordi-
nate categories, or to concentrate on their seman-
tic properties, but completing the task involved re-
solving each picture to its corresponding concept.
Images remained on screen until a keyboard re-
sponse was received from the participant to indi-
cate a suitable label had been found, and presenta-
tions were interleaved with three second rest peri-
ods. Thirty stimuli in each of the two classes were
each presented six times, in random order, to give
a total of 360 image presentations in the session.
Response rates were over 95%, and a post-session
questionnaire determined that participants agreed
on image labels in approximately 90% of cases.
English terms for the concepts used are listed be-
low.
Mammals anteater, armadillo, badger, beaver, bi-
son, boar, camel, chamois, chimpanzee, deer,
elephant, fox, giraffe, gorilla, hare, hedge-
hog, hippopotamus, ibex, kangaroo, koala,
llama, mole, monkey, mouse, otter, panda,
rhinoceros, skunk, squirrel, zebra
Tools Allen key, axe, chainsaw, craft knife, crow-
bar, file, garden fork, garden trowel, hack-
saw, hammer, mallet, nail, paint brush, paint
roller, pen knife, pick axe, plaster trowel,
pliers, plunger, pneumatic drill, power drill,
rake, saw, scissors, scraper, screw, screw-
driver, sickle, spanner, tape measure
The EEG signals were recorded at 500Hz from
64 scalp locations based on the 10-20 standard
620
montage.
2
The EEG recording computer and stim-
ulus presentation computer were synchronised by
means of parallel port transmitted triggers. Af-
ter the experiment, pre-processing of the recorded
signals was carried out using the EEGLAB pack-
age (Delorme and Makeig, 2003): signals were
band-pass filtered at 1-50Hz to remove slow drifts
and high-frequency noise, and then down-sampled
to 120Hz. An ICA decomposition was subse-
quently applied (Makeig et al, 1996), and signal
components due to eye-movements were manually
identified and removed.
As a preliminary test to verify that the recorded
signals included category specific patterns, we
applied a discriminative classification technique
based on source-separation, similar to that de-
scribed in Murphy et al (2008). This found that
the categories of mammals and tools could be dis-
tinguished with an accuracy ranging from 57% to
80% (mean of 72% over the seven participants).
2.2 Feature extraction
The features extracted are metrics of signal power
at a particular scalp location, in a particular fre-
quency band, and at a particular time latency rel-
ative to the presentation of each image stimulus.
Termed Event Related Synchronisation (ERS) or
Event Related Spectral Perturbation (ERSP), such
frequency-specific changes in signal amplitude are
known to correlate with a wide range of cogni-
tive functions (Pfurtscheller and Lopes da Silva,
1999), and have specifically been shown to be sen-
sitive to category distinctions during the process-
ing of linguistic and visual stimuli (Murphy et al,
2008; Gilbert et al, 2009).
Feature extraction and selection is performed
individually on a per-participant basis. As a first
step all signal channels are z-score normalised
to control for varying conductivity at each elec-
trode site, and a Laplacian sharpening is applied
to counteract the spatial blurring of signals caused
by the skull, and so minimise redundancy of infor-
mation between channels.
For each stimulus presentation, 14,400 signal
power features are extracted: 64 electrode chan-
nels by 15 frequency bands (of width 3.3Hz, be-
tween 1 and 50Hz) by 15 time intervals (of length
67ms, in the first second after image presentation).
A z-score normalisation is carried out across all
2
Using a Brain Vision BrainAmp system: http://
www.brainproducts.com/.
Figure 2: Mean rank of selected features in the
time/frequency space (left panel) and on the scalp
(right panel) for participant E
stimulus presentations to equalise variance across
frequencies and times: to control both for the low-
pass filtering action of the skull, and for the re-
duced synchronicity of activity at increasing laten-
cies. For each stimulus a mean is then taken over
each of six presentations to arrive at a more reli-
able power estimate for each feature.
3
The feature ranking method used in Mitchell et
al. (2008) evaluates the extent to which the rela-
tionship among stimuli is stable across across pre-
sentations, using a correlational measure,
4
but pre-
liminary analyses with this selection method on
EEG features proved disappointing. Here, two ad-
ditional ranking criteria are used: each feature is
evaluated for its noisiness (the amount of power
variation seen across presentations of the same
stimulus), and for its distinctiveness (the amount
of variation in power estimates across different
stimuli). A combination of these three strategies
is used to rank the features by their informative-
ness, and the top 50 features are then selected for
each participant.
5
A qualitative evaluation of the feature selec-
tion strategy can be carried out by examining
the distribution of features selected. Figure 2
shows the distribution of selected features over the
time/frequency spectrum (left panel), and over the
scalp (right panel - viewed from above, with the
nose pointing upwards). The distribution seen is
3
Stimulus power features are isolated by band-pass filter-
ing for the required frequencies, cropping following the rel-
evant time interval relative to each image presentation, and
then taking the variance of the resulting signal, which is pro-
portional to power.
4
See the associated supplementary materials of Mitchell
et al (2008) for details: http://www.sciencemag.
org/cgi/content/full/320/5880/1191/DC1.
5
Several combinations of these parameters (selection
thresholds of 5, 20, 50, 100, 200 features; ranking criteria in
isolation and in combination) were investigated - the one cho-
sen gave highest overall performance with the web-derived
corpus model: 50 features, combined ranking criteria.
621
Figure 3: First two components of principal com-
ponents analysis of selected features for partici-
pant E (crosses: mammals; circles: tools)
plausible in reference to previous work: lower fre-
quencies (Pfurtscheller and Lopes da Silva, 1999),
latencies principally in the first few hundred mil-
liseconds (Kiefer, 2001), and activity in the visual
centres at the rear of the head, as well as parietal
areas (Pulverm?uller, 2005). A principal compo-
nents analysis can also be performed on the se-
lected features to see if they reflect any plausi-
ble semantic space. As figure 3 shows, the fea-
ture selection stage has captured quite faithfully
the mammal/tool distinction in a totally unsuper-
vised fashion.
3 Corpus-based semantic models
Data from linguistics (Pustejovsky, 1995; Fill-
more, 1982) and neuroscience (Barsalou, 1999;
Barsalou, 2003; Pulverm?uller, 2005) underline
how certain verbs, by emphasising typical ways in
which we interact with entities and how they be-
have, are pivotal in the representation of concrete
nominal concepts. Following these traditions,
Mitchell et al (2008) use 25 manually picked
verbs as their corpus-based features.
Here that approach is replicated by translating
these verbs into Italian. Mitchell et al (2008) se-
lected verbs that denote our interaction with ob-
jects and living things, such as smell and ride.
While the translations are not completely faithful
(because frequent verbs of this sort tend to span
different sets of senses in the two languages), the
aim was to respect the same principle when build-
ing the Italian list. The full list, with our back
translations into English, is presented in Table 1.
We refer to this set as the ?Mitchell? verbs.
alzare ?raise? annusare ?smell/sniff?
aprire ?open? ascoltare ?listen?
assaggiare ?taste? avvicinare ?near?
cavalcare ?ride? correre ?run/flow?
dire ?say/tell? entrare ?enter?
guidare ?drive? indossare ?wear?
maneggiare ?handle? mangiare ?eat?
muovere ?move? pulire ?clean?
puzzare ?stink? riempire ?fill?
rompere ?break? sentire ?feel/hear?
sfregare ?rub? spingere ?push?
temere ?fear? toccare ?touch?
vedere ?see?
Table 1: The ?Mitchell? verbs, with English trans-
lations
As in Mitchell et al (2008), in order to find
a corpus large enough to provide reliable co-
occurrence statistics for our target concepts and
the 25 verbs, we resorted to the Web, queried us-
ing the Yahoo! API.
6
In particular, we represent
each concept by a vector that records how many
times it co-occurred with each target verb within
a span of 5 words left and right, according to Ya-
hoo! counts. We refer to this corpus-based model
as the yahoo-mitchell model below.
While manual verb picking has proved effec-
tive for Mitchell and colleagues (and for us, as we
will see in a moment), ultimately what we are in-
terested in is discovering the most distinctive fea-
tures of each conceptual category. We are there-
fore interested in more systematic approaches to
inducing corpus-based concept descriptions, and
in which of these approaches works best for this
task. The alternative models we consider were
not extracted from the Web, but from an existing
corpus, so that we could rely on pre-existing lin-
guistic annotation (POS tagging, lemmatization,
dependency paths), and perform more flexible,
annotation-aware queries to collect co-occurrence
statistics.
More specifically, we used the la Repub-
blica/SSLMIT corpus
7
, that contains about 400
million tokens of newspaper text. From this, we
extracted four models where nominal concepts are
represented in terms of patterns of co-occurrence
with verbs (we collected statistics for the top
20,000 most common nouns in the corpus, includ-
ing the concepts used as stimuli in the silent nam-
6
http://developer.yahoo.com/search/
7
http:://sslmit.unibo.it/repubblica/
622
ing experiment, and the top 5,000 verbs). We first
re-implemented a ?classic? window-based word
space model (Sahlgren, 2006), referred to below
as repubblica-window, where each noun lemma is
represented by its co-occurrence with verb lem-
mas within the maximum span of a sentence, with
no more than one other intervening noun. The
repubblica-position model is similar, but it also
records the position of the verb with respect to
the noun (so that X-usare ?X-use? and usare-X
?use-X? count as different features), analogously
to the seminal HAL model (Lund and Burgess,
1996). It has been shown that models that take
the syntactic relation between a target word and
a collocate feature into account can outperform
?flat? models in some tasks (Pad?o and Lapata,
2007). The next two models are based on the de-
pendency parse of the la Repubblica corpus docu-
mented by Lenci (2009). We only counted as col-
locates those verbs that were linked to nouns by
a direct path (such as subject and object) or via
preposition-mediated paths (e.g., tagliare con for-
bici ?to cut with scissors?), and where the paths
were among the top 30 most frequent in the cor-
pus. In the repubblica-depfilter model, we record
co-occurrence with verbs that are linked to the
nouns by one of the top 30 paths, but we do
not preserve the paths themselves in the features.
This is analogous to the model proposed by Pad?o
and Lapata (2007). In the repubblica-deppath
model, we preserve the paths as part of the fea-
tures (so that subj-uccidere ?subj-kill? and obj-
uccidere count as different features), analogously
to Lin (1998), Curran and Moens (2002) and oth-
ers. For all models, following standard practice in
computational linguistics (Evert, 2005), we trans-
form raw co-occurrence counts into log-likelihood
ratios.
Following the evaluation paradigm of Mitchell
et al (2008), linear models trained on corpus-
based features are used to predict the pattern of
EEG activity for unseen concepts. This only
works if we have a very limited number of fea-
tures (or else we would have more parameters to
estimate than data-points to estimate them). The
Repubblica-based models have thousands of fea-
tures (one per verb collocate, or verb+path collo-
cate). We adopt two strategies to select a reduced
number of features. In the topfeat versions, we
first pick the 50 features that have the highest asso-
ciation with each of the target concepts. We then
count in how many of these concept-specific top
lists a feature occurs, and we pick the 25 features
that occur in the largest number of them. The intu-
ition is that this should give us a good trade-off be-
tween how characteristic the features are (we only
use features that are highly associated with some
of our concepts), and their generalization capabili-
ties (we pick features that are associated with mul-
tiple concepts). Randomly selected examples of
the features extracted in this way for the various
Repubblica models are reported in Table 2.
repubblica-window repubblica-position
abbattere ?demolish? X-ferire ?X-wound?
afferrare ?seize? X-usare ?X-use?
impugnare ?grasp? dipingere-X ?paint-X?
tagliare ?cut? munire-X ?supply-X?
trovare ?find? tagliare-X ?cut-X?
repubblica-depfilter repubblica-deppath
abbattere ?demolish? con+tagliare ?with+cut?
correre ?run? obj+abbattere ?obj+demolish?
parlare ?speak? obj+uccidere ?obj+kill?
saltare ?jump? intr-subj+vivere ?intr-subj+live?
tagliare ?cut? tr-subj+aprire ?tr-subj+open?
Table 2: Examples of top features from the la Re-
pubblica models
Alternatively, instead of feature selection we
perform feature reduction by means of a Singular
Value Decomposition (SVD) of the noun-by-verb
matrix. We apply the SVD to matrices that include
the top 20,000 most frequent nouns in the cor-
pus (including our target concepts) since the qual-
ity of the resulting reduced model should improve
if we can exploit richer patterns of correlations
among the columns ? verbs ? across rows ? nouns
(Landauer and Dumais, 1997; Sch?utze, 1997). In
the svd versions of our models, we pick as fea-
tures the top 25 left singular vectors, weighted
by the corresponding singular values. These fea-
tures do not have a straightforward interpretation,
but they tend to group verb meanings that belong
to broad semantic domains. For example, among
the original verbs that are most strongly correlated
with one of the top singular vectors of repubblica-
window we find giocare ?play?, vincere ?win? and
perdere ?lose?. Another singular vector is asso-
ciated with ammontare ?amount?, costare ?cost?,
pagare ?pay?, etc. One of the top singular vec-
tors of repubblica-deppath is strongly correlated
with in+scendere ?descend into?, in+mettere ?put
into?, in+entrare ?enter into?, though not all sin-
gular vectors are so clearly characterized by the
verbs they correlate with.
623
None of the la Repubblica models had full cov-
erage of our concept stimulus set (see the second
column of Table 3 below), because our extraction
method missed some multi-word units, and fea-
ture selection led to losing some more items due
to data sparseness (e.g., some target words had no
collocates connected by the dependency paths we
selected). The experiments reported in the next
section used all the target concepts available in
each model, but a replication using the 50 concepts
that were common to all models obtained results
that are comparable. For a direct comparison be-
tween Yahoo! and la Repubblica derived features,
we tried collecting statistics for the Mitchell verbs
from Repubblica as well, but the resulting model
was extremely sparse, and we do not report its per-
formance here.
Finally, it is important to note that any repre-
sentation yielded by a corpus semantic model does
not characterise a concept directly, but is rather an
aggregate of the various senses and usages of the
noun chosen to represent it. This obvious limita-
tion will persist until comprehensive, robust and
computationally efficient word-sense disambigua-
tion techniques become available. However these
models are designed to extract semantic (as op-
posed to syntactic or phonological) properties of
words, and as noted in the introduction, have been
demonstrated to correlate with behavioural effects
of conceptual processing.
4 Predicting EEG patterns using
corpus-based models
In Section 2.2 above we showed how we extracted
features summarizing the spatial, temporal and
frequency distribution of the EEG signal collected
while participants were processing each of the tar-
get concepts. In Section 3, we described various
ways to obtain a compact representation of the
same concepts in terms of corpus-derived features.
We will now discuss the method we employed to
verify whether the corpus-derived features can be
used to predict the EEG patterns ? that is whether
semantics can be used to predict neural activity.
Our hope is that a good corpus-based model will
provide a decomposition of concepts into mean-
ingful properties, corresponding to coherent sub-
patterns of activation in the brain, and thus capture
generalizations across concepts. For example, if
a concept is particularly visually evocative (e.g.,
zebra), we might expect it to be strongly associ-
ated with the verb see, while also causing partic-
ular activation of the vision centres of the brain.
Similarly, concepts with strong associations with
a particular sound (e.g., cuckoo) might be seman-
tically associated with hear while also dispropor-
tionately activating auditory areas of the brain. It
should thus be possible to learn a model of corpus-
to-EEG-pattern correspondences on training data,
and use it to predict the EEG activation patterns of
unseen concepts.
We follow the paradigm proposed by Mitchell et
al. (2008) for fMRI data. For each participant and
selected EEG feature, we train a model where the
level of activation of the latter in response to dif-
ferent concepts is approximated by a linear com-
bination of the corpus features:
~
f = C
~
? + ~
where
~
f is the vector of activations of a specific
EEG feature for different concepts, the matrix C
contains the values of the corpus features for the
same concepts (row-normalised to z-scores),
~
? is
the weight we must learn for each corpus feature,
and~ is a vector of error terms. We use the method
of least squared errors to learn the weights that
maximize the fit of the model. We can then predict
the activation of an EEG feature in response to a
new concept that was not in the training data by a
~
?-weighted sum of the values of each corpus fea-
ture for the new concept. In some cases collinear-
ity in the corpus data (regular linear relationships
among the corpus-feature columns) prevented the
estimation procedure from finding a solution. In
such cases (due to the small number of data, rel-
ative to the number of unknowns), the least in-
formative corpus-features (those that correlated on
average most highly with other features) were iter-
atively removed until a solution was reached. All
models were trained with between 23 and 25 cor-
pus features.
Again following Mitchell and colleagues, we
adopt a leave-2-out paradigm in which a linear
model for each EEG feature is trained in turn on
all concepts minus 2. For each of the 2 left out
concepts, we predict the EEG activation pattern
using the trained linear model and their corpus
features, as just described. We then try to cor-
rectly match the predicted and observed activa-
tions, by measuring the Euclidean distance be-
tween the model-generated EEG activity (a vec-
tor of estimated power levels for the n EEG fea-
624
tures selected) and the corresponding EEG activ-
ity recorded in the experiment (other distance met-
rics gave similar results to the ones reported here).
Given the 2 left-out concepts a and b, we com-
pute 2 matched distances (i.e., distance between
predicted and observed pattern for a, and the same
for b) and 2 mismatched distances (predicted a and
observed b; predicted b and observed a). If the av-
erage of the matched distances is lower than the
average of the mismatched distances, we consider
the prediction successful ? otherwise we count is
as a failed prediction. At chance levels, expected
matching accuracy is 50%.
5 Results
Table 3 shows the comparative results for all the
corpus models introduced in Section 3. The third
column (all) shows the overall accuracy in cor-
rectly matching predicted to observed EEG ac-
tivity patterns, and so successfully distinguishing
word meanings. The significance of the figures is
indicated with the conventional annotation (calcu-
lated using a one-way two-sided t-test across the
individual participant accuracy figures against an
expected population mean of 50%).
8
The second
column shows the coverage of each model of the
60 mammal and tool concepts used, which ranged
from full (for the yahoo-mitchell model) to 51 con-
cepts (for the depfilter-topfeat model). The corre-
sponding number of matching comparisons over
which accuracy was calculated ranged from 1770
down to 1225.
As suggested by previous work (Murphy et al,
2008), and illustrated by figure 3, coarse distinc-
tions between words in different superordinate cat-
egories (e.g., hammer vs armadillo; giraffe vs
nail) may be easier to detect than those among
concepts within the same category (e.g., ham-
mer vs nail; giraffe vs armadillo). The fourth
and fifth columns give these accuracies, and while
between-category discriminations do prove more
reliable, they indicate that, for the top rated model
at least, finer within-category distinctions are also
being captured. Figures from the top two perform-
ing models are given for individual participants in
tables 4 and 5.
8
On average, the difference seen between matched and
mismatched pairs was small, at about 3% of the distance
between observed and predicted representations, and was
marginally bigger for correct than for incorrect predictions
(p < 0.01).
part. overall within between
A 54 53 55
B 54 47 60
C 62 56 67
D 61 56 67
E 68 58 78
F 52 54 51
G 57 51 63
Table 4: Accuracy levels for individual participant
sessions, yahoo-mitchell web corpus
part. overall within between
A 49 52 46
B 59 57 60
C 60 60 59
D 50 45 55
E 56 53 58
F 64 64 65
G 52 49 55
Table 5: Accuracy levels for individual participant
sessions, repubblica-window-svd
6 Discussion
Our results show that corpus-extracted conceptual
models can be used to distinguish between the
EEG activation levels associated with conceptual
categories to a degree that is significantly above
chance. Though category specific patterns are de-
tectable in the EEG signal alone (as illustrated by
the PCA analysis in figure 3), on that basis we can-
not be sure that semantics is being detected. Some
other property of the stimuli that co-varies with the
semantic classes of interest could be responsible,
such as visual complexity, conceptual familiarity,
lexical frequency, or phonological form. Only by
cross-training with individual corpus features and
showing that these hold a predictive relationship to
neural activity have we been able to establish that
EEG patterns encode semantics.
Present evidence indicates that fMRI may pro-
vide richer data for training such models than EEG
(Mitchell and colleagues obtain an average accu-
racy of 77%, and 65% for the within category set-
ting). However, fMRI has several clear disadvan-
tages as a tool for language researchers. First of
all, the fine spatial resolution it provides (down
to 2-3mm), while of great interest to neuroscien-
tists, is not in itself linguistically informative. Its
coarse temporal resolution (of the order of several
seconds), makes it ill-suited to analysing on-line
linguistic processes. EEG on the other hand, de-
spite its low spatial resolution (several centime-
tres), gives millisecond-level temporal resolution,
625
model coverage all within cat between cat
yahoo-mitchell 100 58.3** (5.7) 53.6* (3.7) 63.0** (8.9)
repubblica-window-svd 96.7 55.7* (5.6) 54.3 (6.5) 56.9* (5.9)
repubblica-window-topfeat 93.3 52.1 (4.3) 48.7 (3.6) 55.4 (7.0)
repubblica-deppath-svd 93.3 51.4 (8.7) 49.0 (8.0) 54.0 (10.0)
repubblica-depfilter-topfeat 85.0 51.1 (9.6) 49.3 (9.6) 53.1 (10.0)
repubblica-position-topfeat 93.3 50.0 (5.2) 46.0 (4.7) 53.6 (8.0)
repubblica-deppath-topfeat 86.7 49.9 (9.0) 47.0 (9.3) 52.4 (9.6)
repubblica-position-svd 96.7 49.4 (10.2) 46.6 (9.8) 52.3 (11.3)
repubblica-depfilter-svd 93.3 48.9 (11.1) 47.1 (8.9) 50.6 (12.9)
Table 3: Comparison across corpus models, with percentage concept coverage, mean cross-subject per-
centage prediction accuracy and standard deviation; ?p < 0.05, ? ? p < 0.01
enabling the separate analysis of sequential cogni-
tive processes and states (e.g., auditory process-
ing, word comprehension, semantic representa-
tion). fMRI is also prohibitively expensive for
most researchers (ca. 300 euros per hour at cost
price), compared to EEG (ca. 30 euros per hour).
Finally, there is no prospect of fMRI being minia-
turised, while wearable EEG systems are already
becoming commercially available, making exper-
imentation in more ecological settings a possibil-
ity (e.g., playing with a child, meeting at a desk,
walking around). In short, while EEG can be used
to carry out systematic investigations of categori-
cal distinctions, doing so with fMRI would be pro-
hibitively expensive.
Present results indicate that distinctions be-
tween categories are easier than distinctions be-
tween category elements; and that selecting the
conceptual features by hand gives better results
than discovering them automatically. Both of
these results however may be due to limitations
of the current method. One limitation is that we
have been using the same set of features for all
concepts, which is likely to blur the distinctions
between members of a category more than those
between categories. A second limitation of our
present methodology is that it is constrained to use
very small numbers of semantic features, which
limits its applicability. For example it is hard to
conceive of a small set of verbs, or other parts-of-
speech, whose co-occurrence patterns could suc-
cessfully characterise the full range of meaning
found in the human lexicon. Even the more eco-
nomical corpus-extracted conceptual models tend
to run in the hundreds of features (Almuhareb,
2006). We are currently working on variations in
the method that will address these shortcomings.
The web-based model with manually picked
features outperformed all la Repubblica-based
models. However, the results attained with
repubblica-window-svd are encouraging, espe-
cially considering that we are reporting results for
an EEG feature configuration optimised for the
web data (see footnote 5), and that la Repubblica
is several orders of magnitude smaller than the
web. That data sparseness might be the main is-
sue with la Repubblica models is suggested by
the fact that repubblica-window-svd is the least
sparse of them, since it does not filter data by posi-
tion or dependency path, and compresses informa-
tion from many verbs via SVD. In future research,
we plan to extract richer models from larger cor-
pora. And as the discriminative accuracy of cross-
training techniques improves, further insights into
the relative validity of corpus representations will
be attainable. One research aim is to see if individ-
ual corpus semantic properties are encoded neu-
rally, so providing strong evidence for a particular
model. These techniques may also prove more ob-
jective and reliable in evaluating representations of
abstract concepts, for which it is more difficult to
collect reliable judgements from informants.
References
A. Almuhareb. 2006. Attributes in lexical acquisition.
Dissertation, University of Essex.
L. Barsalou. 1999. Perceptual symbol systems. Be-
havioural and Brain Sciences, 22:577?660.
L. Barsalou. 2003. Situated simulation in the human
conceptual system. Language and Cognitive Pro-
cesses, 18:513?562.
J.R. Curran and M. Moens. 2002. Improvements in
automatic thesaurus extraction. In Proceedings of
SIGLEX, pages 59?66.
A. Delorme and S. Makeig. 2003. Eeglab: an open
source toolbox for analysis of single-trial dynamics
includingindependent component analysis. Journal
of Neuroscience Methods, 134:9?21.
626
S. Evert. 2005. The statistics of word cooccurrences.
Dissertation, Stuttgart University.
Ch. J. Fillmore. 1982. Frame semantics. In Linguis-
tic Society of Korea, editor, Linguistics in the Morn-
ing Calm, pages 111?138. Hanshin, Seoul.
J. Gilbert, L. Shapiro, and G. Barnes. 2009. Processing
of living and nonliving objects diverges in the visual
processing system: evidence from meg. In Proceed-
ings of the Cognitive Neuroscience Society Annual
Meeting.
M. Kiefer. 2001. Perceptual and seman-
tic sources of category-specific effects in object
categorization:event-related potentials during pic-
ture and word categorization. Memory and Cogni-
tion, 29(1):100?116.
T. Landauer and S. Dumais. 1997. A solution to Platos
problem: the latent semantic analysis theory of ac-
quisition, induction, and representation of knowl-
edge. Psychological Review, 104(2):211?240.
A. Lenci. 2009. Argument alternations in italian verbs:
a computational study. In Atti del XLII Congresso
Internazionale di Studi della Societ`a di Linguistica
Italiana.
D. Lin. 1998. Automatic retrieval and clustering
of similar words. In COLING-ACL98, Montreal,
Canada.
K. Lund and C. Burgess. 1996. Producing
high-dimensional semantic spaces from lexical co-
occurrence. Behavior Research Methods, Instru-
ments, and Computers, 28:203?208.
S. Makeig, A.J. Bell, T. Jung, and T.J. Sejnowski.
1996. Independent component analysis of elec-
troencephalographic data. In in Advances in Neu-
ral Information Processing Systems, pages 145?151.
MIT Press.
A. Martin and L. Chao. 2001. Semantic memory and
the brain: structure and processes. Current Opinions
in Neurobiology, 11:194?201.
T. Mitchell, S. Shinkareva, A. Carlson, K. Chang,
V. Malave, R. Mason, and M. Just. 2008. Predicting
human brain activity associated with the meanings
of nouns. Science, 320:1191?1195.
B. Murphy, M. Dalponte, M. Poesio, and L. Bruz-
zone. 2008. Distinguishing concept categories from
single-trial electrophysiological activity. In Pro-
ceedings of the Annual Meeting of the Cognitive Sci-
ence Society.
S. Pad?o and M. Lapata. 2007. Dependency-based con-
struction of semantic space models. Computational
Linguistics, 33(2):161?199.
G. Pfurtscheller and F. Lopes da Silva. 1999. Event-
related EEG/MEG synchronization and desynchro-
nization: Basic principles. Clinical Neurophysiol-
ogy, 110:1842?1857.
F. Pulverm?uller. 2002. The neuroscience of language:
on brain circuits of words and serial order. Cam-
bridge University Press, Cambridge.
F. Pulverm?uller. 2005. Brain mechanisms linking lan-
guage and action. Nature Reviews Neuroscience,
6:576?582.
J. Pustejovsky. 1995. The Generative Lexicon. MIT
Press, Cambridge.
M. Sahlgren. 2006. The Word-Space Model: Us-
ing distributional analysis to represent syntagmatic
and paradigmatic relations between words in high-
dimensional vector spaces. Dissertation, Stockholm
University.
S. Schulte im Walde. 2008. Theoretical adequacy, hu-
man data and classification approaches in modelling
word properties, word relatedness and word classes.
Habilitation, Saarland University.
H. Sch?utze. 1997. Ambiguity Resolution in Natural
Language Learning. CSLI, Stanford.
627
Tutorial Abstracts of ACL-IJCNLP 2009, page 6,
Suntec, Singapore, 2 August 2009. c?2009 ACL and AFNLP
State-of-the-art NLP Approaches to Coreference Resolution:
Theory and Practical Recipes
Simone Paolo Ponzetto
Seminar fu?r Computerlinguistik
University of Heidelberg
ponzetto@cl.uni-heidelberg.de
Massimo Poesio
DISI
University of Trento
massimo.poesio@unitn.it
1 Introduction
The identification of different nominal phrases in
a discourse as used to refer to the same (discourse)
entity is essential for achieving robust natural lan-
guage understanding (NLU). The importance of
this task is directly amplified by the field of Natu-
ral Language Processing (NLP) currently moving
towards high-level linguistic tasks requiring NLU
capabilities such as e.g. recognizing textual entail-
ment. This tutorial aims at providing the NLP
community with a gentle introduction to the task
of coreference resolution from both a theoretical
and an application-oriented perspective. Its main
purposes are: (1) to introduce a general audience
of NLP researchers to the core ideas underlying
state-of-the-art computational models of corefer-
ence; (2) to provide that same audience with an
overview of NLP applications which can benefit
from coreference information.
2 Content Overview
1. Introduction to machine learning approaches
to coreference resolution. We start by focusing
on machine learning based approaches developed
in the seminal works from Soon et al (2001) and
Ng & Cardie (2002). We then analyze the main
limitations of these approaches, i.e. their cluster-
ing of mentions from a local pairwise classifica-
tion of nominal phrases in text. We finally move
on to present more complex models which attempt
to model coreference as a global discourse phe-
nomenon (Yang et al, 2003; Luo et al, 2004;
Daume? III & Marcu, 2005, inter alia).
2. Lexical and encyclopedic knowledge for
coreference resolution. Resolving anaphors to
their correct antecedents requires in many cases
lexical and encyclopedic knowledge. We accord-
ingly introduce approaches which attempt to in-
clude semantic information into the coreference
models from a variety of knowledge sources,
e.g. WordNet (Harabagiu et al, 2001), Wikipedia
(Ponzetto & Strube, 2006) and automatically har-
vested patterns (Poesio et al, 2002; Markert &
Nissim, 2005; Yang & Su, 2007).
3. Applications and future directions. We
present an overview of NLP applications which
have been shown to profit from coreference in-
formation, e.g. question answering and summa-
rization. We conclude with remarks on future
work directions. These include: a) bringing to-
gether approaches to coreference using semantic
information with global discourse modeling tech-
niques; b) exploring novel application scenarios
which could potentially benefit from coreference
resolution, e.g. relation extraction and extracting
events and event chains from text.
References
Daume? III, H. & D. Marcu (2005). A large-scale exploration
of effective global features for a joint entity detection and
tracking model. In Proc. HLT-EMNLP ?05, pp. 97?104.
Harabagiu, S. M., R. C. Bunescu & S. J. Maiorano (2001).
Text and knowledge mining for coreference resolution. In
Proc. of NAACL-01, pp. 55?62.
Luo, X., A. Ittycheriah, H. Jing, N. Kambhatla & S. Roukos
(2004). A mention-synchronous coreference resolution al-
gorithm based on the Bell Tree. In Proc. of ACL-04, pp.
136?143.
Markert, K. & M. Nissim (2005). Comparing knowledge
sources for nominal anaphora resolution. Computational
Linguistics, 31(3):367?401.
Ng, V. & C. Cardie (2002). Improving machine learning ap-
proaches to coreference resolution. In Proc. of ACL-02,
pp. 104?111.
Poesio, M., T. Ishikawa, S. Schulte im Walde & R. Vieira
(2002). Acquiring lexical knowledge for anaphora resolu-
tion. In Proc. of LREC ?02, pp. 1220?1225.
Ponzetto, S. P. & M. Strube (2006). Exploiting semantic role
labeling, WordNet and Wikipedia for coreference resolu-
tion. In Proc. of HLT-NAACL-06, pp. 192?199.
Soon, W. M., H. T. Ng & D. C. Y. Lim (2001). A ma-
chine learning approach to coreference resolution of noun
phrases. Computational Linguistics, 27(4):521?544.
Yang, X. & J. Su (2007). Coreference resolution using se-
mantic relatedness information from automatically dis-
covered patterns. In Proc. of ACL-07, pp. 528?535.
Yang, X., G. Zhou, J. Su & C. L. Tan (2003). Coreference
resolution using competition learning approach. In Proc.
of ACL-03, pp. 176?183.
6
Proceedings of the Linguistic Annotation Workshop, pages 148?155,
Prague, June 2007. c?2007 Association for Computational Linguistics
Standoff Coordination for Multi-Tool Annotation in a Dialogue Corpus
Kepa Joseba Rodr??guez?, Stefanie Dipper?, Michael Go?tze?, Massimo Poesio?,
Giuseppe Riccardi?, Christian Raymond?, Joanna Wisniewska?
?Piedmont Consortium for Information Systems (CSI-Piemonte)
KepaJoseba.Rodriguez@csi.it
?Department of Linguistics. University of Potsdam.
{dipper|goetze}@ling.uni-potsdam.de
?Center for Mind/Brain Sciences. University of Trento.
massimo.poesio@unitn.it
?Department of Information and Communication Technology. University of Trento.
{christian.raymond|riccardi}@dit.unitn.it
?Institute of Computer Science. Polish Academy of Science.
jwisniewska@poczta.uw.edu.pl
Abstract
The LUNA corpus is a multi-lingual, multi-
domain spoken dialogue corpus currently
under development that will be used to de-
velop a robust natural spoken language un-
derstanding toolkit for multilingual dialogue
services. The LUNA corpus will be an-
notated at multiple levels to include an-
notations of syntactic, semantic, and dis-
course information; specialized annotation
tools will be used for the annotation at each
of these levels. In order to synchronize these
multiple layers of annotation, the PAULA
standoff exchange format will be used. In
this paper, we present the corpus and its
PAULA-based architecture.1
1 Introduction
XML standoff markup (Thompson and McKelvie,
1997; Dybkj?r et al, 1998) is emerging as the clean-
est way to organize multi-level annotations of cor-
pora. In many of the current annotation efforts based
on standoff a single multi-purpose tool such as the
NITE XML Toolkit (Carletta et al, 2003) or Word-
Freak (Morton and LaCivita, 2003) is used to anno-
1The members of the LUNA project consortium are: Pied-
mont Consortium for Information Systems (IT), University of
Trento (IT), Loquendo SpA (IT), RWTH-Aachen (DE), Uni-
versity of Avignon (FR), France Telecom R&D Division S.A.
(FR), Polish-Japanese Institute of Information Technology (PL)
and the Institute for Computer Science of the Polish Academy
of Sciences (PL), http://www.ist-luna.eu.
This research was performed in the LUNA project funded by the
EC, DG Infso, Unit E1 and in the Collaborative Research Cen-
ter 632 ?Information Structure?, funded by the German Science
Foundation, http://www.sfb632.uni-potsdam.de.
tate as well as maintain all annotation levels (cf. the
SAMMIE annotation effort (Kruijff-Korbayova? et al,
2006b)).
However, it is often the case that specialized tools
are developed to facilitate the annotation of particu-
lar levels: examples include tools for segmentation
and transcription of the speech signal like PRAAT
(Boersma and Weenink, 2005) and TRANSCRIBER
(Barras et al, 1998), the SALSA tools for FrameNet-
style annotation (Burchardt et al, 2006), and MMAX
(Mu?ller and Strube, 2003) for coreference annota-
tion. Even in these cases, however, it may still be
useful, or even necessary, to be able to visualize
more than one level at once, or to ?knit? together2
multiple levels to create a file that can be used to
train a model for a particular type of annotation.
The Linguistic Annotation Framework by (Ide et al,
2003) was proposed as a unifying markup format to
be used to synchronize heterogeneous markup for-
mats for such purposes.
In this paper, we discuss how the PAULA represen-
tation format, a standoff format inspired by the Lin-
guistic Annotation Framework, is being used to syn-
chronize multiple levels of annotation in the LUNA
corpus, a corpus of spoken dialogues in multiple lan-
guages and multiple domains that is being created to
support the development of robust spoken language
understanding models for multilingual dialogue ser-
vices. The corpus is richly annotated with linguistic
information that is considered relevant for research
on dialogue, including chunks, named entities, argu-
ment structure, coreference, and dialogue acts. We
chose to adopt specialized tools for each level: e.g.,
2In the sense of the knit tool of the LT-XML suite.
148
transcription using TRANSCRIBER, coreference us-
ing MMAX, attributes using SEMANTIZER, etc. To
synchronize the annotation and allow cross-layer op-
erations, the annotations are mapped to a common
representation format, PAULA.
The structure of the paper is as follows. In Sec-
tion 2, we present the LUNA project and the LUNA
corpus with its main annotation levels. In Section 3,
we introduce the PAULA exchange format, focusing
on the representation of time alignment and dialogue
phenomena. Finally we show how PAULA is used in
the LUNA corpus and discuss alternative formats.
2 The LUNA project
The aim of the LUNA project is to advance the state
of the art in understanding conversational speech
in Spoken Dialogue Systems (Gupta et al, 2005),
(Bimbot et al, 2006).
Three aspects of Spoken Language Understand-
ing (SLU) are of particular concern in LUNA: gen-
eration of semantic concept tags, semantic compo-
sition into conceptual structures and context sensi-
tive validation using information provided by the di-
alogue manager. In order to train and evaluate SLU
models, we will create an annotated corpus of spo-
ken dialogues in multiple domains and multiple lan-
guages: French, Italian, and Polish.
2.1 The LUNA corpus
The LUNA corpus is currently being collected, with
a target to collect 8100 human-machine dialogues
and 1000 human-human dialogues in Polish, Italian
and French. The dialogues are collected in the fol-
lowing application domains: stock exchange, hotel
reservation and tourism inquiries, customer support
service/help-desk and public transportation.
2.2 Multilevel annotation
Semantic interpretation involves a number of sub-
tasks, ranging from identifying the meaning of indi-
vidual words to understanding which objects are be-
ing referred to up to recovering the relation between
different semantic objects in the utterance and dis-
course level to, finally, understanding the commu-
nicative force of an utterance.
In some annotation efforts?e.g., in the annotation
of the French MEDIA Corpus (Bonneau-Maynard
and Rosset, 2003)? information about the meaning
of semantic chunks, contextual information about
coreference, and information about dialogue acts are
all kept in a single file. This approach however suf-
fers from a number of problems, including the fact
that errors introduced during the annotation at one
level may make other levels of annotation unusable
as well, and that it is not possible for two anno-
tators to work on different types of annotation for
the same file at the same time. Most current an-
notation efforts, therefore, tend to adopt the ?multi-
level? approach pioneered during the development
of the MAPTASK corpus and then developed as part
of work on the EU-funded MATE project (McKelvie
et al, 2001), in which each aspect of interpreta-
tion is annotated in a separate level, independently
maintained. This approach is being followed, for
instance, in the ONTONOTES project (Hovy et al,
2006) and the SAMMIE project (Kruijff-Korbayova
et al, 2006a).
For the annotation of the LUNA corpus, we de-
cided to follow the multilevel approach as well. That
allows us to achieve more granularity in the anno-
tation of each of the levels and to investigate more
easily dependencies between features that belong to
different levels. Furthermore, we can use different
specialized off-the-shelf annotation tools, splitting
up the annotation task and thus facilitating consis-
tent annotation.
2.3 Annotation levels
The LUNA corpus will contain different types of in-
formation. The first levels are necessary to prepare
the corpus for subsequent semantic annotation, and
include segmentation of the corpus in dialogue turns,
transcription of the speech signal, and syntactic pre-
processing with POS-tagging and shallow parsing.
The next level consists of the annotation of do-
main information using attribute-value pairs. This
annotation will be performed on all dialogues in the
corpus.
The other levels of the annotation scheme are not
mandatory, but at least a part of the dialogues will
be annotated in order to investigate contextual as-
pects of the semantic interpretation. These levels in-
clude the predicate structure, the relations between
referring expressions, and the annotation of dialogue
acts.
149
2.3.1 Segmentation and transcription of the
speech signal
Before transcription and annotation can begin, it
is necessary to segment the speech signal into dia-
logue turns and annotate them with speaker identity
and mark where speaker overlap occurs. The goal
of this segmentation is to be able to perform a tran-
scription and annotation of the dialogue turns with
or without dialogue context. While dialogue context
is preferable for semantic annotation, it slows down
the annotation process.
The tool we will use for the segmentation and
transcription of the speech signal is the open source
tool TRANSCRIBER3 (Barras et al, 1998).
The next step is the transcription of the speech
signal, using conventions for the orthographic tran-
scription and for the annotation of non-linguistic
acoustic events.
2.3.2 Part Of Speech Tagging and Chunking
The transcribed material will be annotated with
POS-tags, morphosyntactic information like agree-
ment features, and segmented based on syntactic
constituency.
For the POS-tags and morphosyntactic features,
we will follow the recommendations made in EA-
GLES (EAGLES, 1996), which allows us to have a
unified representation format for the corpus, inde-
pendently of the tools used for each language.
2.3.3 Domain Attribute Annotation
At this level, semantic segments will be anno-
tated following an approach used for the annotation
for the French MEDIA dialogue corpus (Bonneau-
Maynard and Rosset, 2003).
We specify the domain knowledge in domain on-
tologies. These are used to build domain-specific
dictionaries. Each dictionary contains:
? Concepts corresponding to classes of the ontol-
ogy and attributes of the annotation.
? Values corresponding to the individuals of the
domain.
? Constraints on the admissible values for each
concept.
3http://trans.sourceforge.net
The concept dictionaries are used to annotate se-
mantic segments with attribute-value pairs. The se-
mantic segments are produced by concatenation of
the chunks produced by the shallow parser. A se-
mantic segment is a unit that corresponds unambigu-
ously to a concept of the dictionary.
(1) buongiorno lei [puo` iscriversi]concept1 [agli
esami]concept2 [oppure]concept3 [ottenere
delle informazioni]concept4 come la posso
aiutare4
<concept1 action:inscription>
<concept2 objectDB:examen>
<concept3 conjunctor:alternative>
<concept4 action:obtain info>
2.3.4 Predicate structure
The annotation of predicate structure facilitates
the interpretation of the relation between entities and
events occurring in the dialogue.
There are different approaches to annotate predi-
cate structure. Some of them are based upon syntac-
tic structure, with PropBank (Kingsbury and Palmer,
2003) being one of the most relevant, building the
annotation upon the syntactic representation of the
TreeBank corpus (Marcus et al, 1993). An alter-
native to syntax-driven approaches is the annotation
using semantic roles as in FrameNet (Baker et al,
1998).
For the annotation of predicate structure in the
LUNA corpus, we decided to use a FrameNet-like
approach, rather than a syntax-based approach:
1. Annotation of dialogue interaction has to deal
with disfluencies, non-complete sentences, un-
grammaticality, etc., which complicates the use
of deep syntactic representations.
2. If we start from a syntactic representation, we
have to follow a long way to achieve the seman-
tic interpretation. Syntactic constituents must
be mapped to ?-roles, and then to semantic
roles. FrameNet offers the possibility of anno-
tating using directly semantic criteria.
4Good morning, you can register for the exam or obtain in-
formation. How can I help you?
150
For each domain, we define a set of frames. These
frames are defined based on the domain ontology,
with the named entities providing the frame ele-
ments. For all the frames we introduce the negation
as a default frame element.
For the annotation, first of all we annotate the en-
tities with a frame and a frame element.
Then if the target is overtly realized we make a
pointer from the frame elements to the target. The
next step is putting the frame elements and the target
(if overtly realized) in a set.
(2) buongiorno [lei]fe1 [puo` iscriversi]fe2
[agli esami]fe3 oppure [ottenere delle
informazioni]fe4 come la posso aiutare
set1 = {id1, id2, id3}
frame: inscription
frame-elements:{student, examen, date}
set2 = {id4}
frame = info-request
frame-elements:{student, addressee, topic}
<fe1 frame="inscription"
FE="student" member="set1"
pointer="fe2">
<fe2 frame="inscription"
FE="target" member="set1">
<fe3 frame="inscription"
FE="examen" member="set1"
pointer="fe2">
<fe4 frame="information"
FE="target" member="set2">
2.3.5 Coreference / Anaphoric relations
To annotate anaphoric relations we will use an an-
notation scheme close to the one used in the ARRAU
project (Artstein and Poesio, 2006). This scheme
has been extensively tested with dialogue corpora
and includes instructions for annotating a variety of
anaphoric relations, including bridging relations. A
further reason is the robustness of the scheme that
doesn?t require one single interpretation in the an-
notation.
The first step is the annotation of the information
status of the markables with the tags given and
new. If the markables are annotated with given,
the annotator will select the most recent occurrence
of the object and add a pointer to it. If the mark-
able is annotated with new, we distinguish between
markables that are related to a previously mentioned
object (associative reference) or don?t have such a
relation.
If there are alternative interpretations, which of a
list of candidates can be the antecedent, the annota-
tor can annotate the markable as ambiguous and
add a pointer to each of the possible antecedents.
(3) Wizard: buongiorno [lei]cr1 [puo`
iscriversi]cr2 [agli esami]cr3 oppure ot-
tenere [delle informazioni]cr4 come la posso
aiutare
<cr1 inf status="new" related="no">
<cr2 inf status="new" related="no">
<cr3 inf status="new" related="no">
<cr4 inf status="new" related="no">
Caller: [iscrizione]cr5 [esami]cr65
<cr5 inf status="given"
single phrase antecedent="cr2"
ambiguity="unambiguous">
<cr6 inf status="given"
single phrase antecedent="cr3"
ambiguity="unambiguous">
2.3.6 Dialogue acts
In order to associate the intentions of the speaker
with the propositional content of the utterances, the
segmentation of the dialogue turns in utterances is
based on the annotation of predicate structure. Each
set of frame elements will correspond to an utter-
ance.
Each utterance will be annotated using a multi-
dimensional annotation scheme partially based on
the DAMSL scheme (Allen and Core, 1997) and on
the proposals of ICSI-MRDA (Dhillon et al, 2004).
We have selected nine dialogue acts from the
DAMSL scheme as initial tagset, that can be extended
for the different application domains. Each utter-
ance will be annotated with as many tags as applica-
ble.
(4) Wizard: [buongiorno]utt1 [lei puo` iscriversi
agli esami]utt2 oppure [ottenere delle
5Register for the exam.
151
informzaioni]utt3 [come la posso aiutare]utt4
<utt1 d-act="opening/closing">
<utt2 d-act="statement"
link-frame="set1">
<utt3 d-act="statement"
link-frame="set2">
<utt4 d-act="info-request">
Caller: [iscrizione esami]utt5
<utt5 d-act="answer;statement"
link-frame="set3">
3 PAULA - a Linguistic Standoff Exchange
Format
PAULA stands for Potsdamer Austauschformat fu?r
linguistische Annotation (?Potsdam Interchange
Format for Linguistic Annotation?) and has been de-
veloped for the representation of data annotated at
multiple layers. The application scenario is sketched
in Fig 1: researchers use multiple, specialized off-
the-shelf annotation tools, such as EXMARALDA or
MMAX, to enrich data with linguistic information.
The tools store the data in tool-specific formats and,
hence, it is not straightforward to combine informa-
tion from different sources and, e.g., to search for
correlations across multiple annotation layers.
This is where PAULA comes in: PAULA maps
the tool-specific formats to a common format and
serves as an interchange format between these
tools.6 Moreover, the annotations from the different
sources are merged into one single representation.
PAULA makes this data available for further appli-
cations, such as searching the data by means of the
tool ANNIS7, or to feed statistical applications like
WEKA8.
PAULA is an XML-based standoff format for lin-
guistic annotations, inspired by the ?dump format?
6Currently, we provide PAULA import filters for the follow-
ing tools and formats: Exmaralda, MMAX, RST Tool/URML,
annotate/TIGER XML. Export from PAULA to the tool formats
is at present supported for the original source format only. We
plan to support the export of selected annotations to other tools.
This is, however, not a trivial task since it may involve loss of
information.
7ANNIS: http://www.sfb632.uni-potsdam.de/
annis
8WEKA: http://www.cs.waikato.ac.nz/ml/
weka
Figure 1: PAULA annotation scenario
of the Linguistic Annotation Framework (Ide et al,
2003).9 With PAULA, not only is the primary data
separated from its annotations, but individual anno-
tation layers (such as parts of speech and dialogue
acts) are separated from each other as well. The
standoff approach allows us to mark overlapping
segments in a straightforward way: by distributing
annotations over different files (XML as such does
not easily account for overlapping segments, since
its object model is a hierarchical, tree-like structure).
Moreover, new annotation layers can be added eas-
ily.
PAULA assumes that a representation of the pri-
mary data is stored in a file that optionally spec-
ifies a header with meta information, followed by
a tag <body>, which contains a representation of
the primary data. In Fig. 2, the first box displays
the transcription, with all contributions from the first
speaker coming first, and the contributions from the
other speaker(s) following (put in italics in the Fig-
ure).
The basic type of ?annotation? are markables, en-
coded by the XML element <mark>. Markables
specify ?anchors?, i.e., locations or ranges that can
be annotated by linguistic information. The loca-
tions and ranges are positions or spans in the source
text or timeline, which are referenced by means of
XLinks and XPointer expressions. For instance, the
?Token? markables in Fig. 2 define spans that cor-
9The term ?standoff? describes the situation where primary
data (e.g., the transcription) and annotations of this data are
stored in separate files (Thompson and McKelvie, 1997).
152
 

	
	

Survey Article
Inter-Coder Agreement for
Computational Linguistics
Ron Artstein?
University of Essex
Massimo Poesio??
University of Essex/Universita` di Trento
This article is a survey of methods for measuring agreement among corpus annotators. It exposes
the mathematics and underlying assumptions of agreement coefficients, covering Krippendorff?s
alpha as well as Scott?s pi and Cohen?s kappa; discusses the use of coefficients in several annota-
tion tasks; and argues that weighted, alpha-like coefficients, traditionally less used than kappa-
like measures in computational linguistics, may be more appropriate for many corpus annotation
tasks?but that their use makes the interpretation of the value of the coefficient even harder.
1. Introduction and Motivations
Since the mid 1990s, increasing effort has gone into putting semantics and discourse
research on the same empirical footing as other areas of computational linguistics (CL).
This soon led to worries about the subjectivity of the judgments required to create
annotated resources, much greater for semantics and pragmatics than for the aspects of
language interpretation of concern in the creation of early resources such as the Brown
corpus (Francis and Kucera 1982), the British National Corpus (Leech, Garside, and
Bryant 1994), or the Penn Treebank (Marcus, Marcinkiewicz, and Santorini 1993). Prob-
lems with early proposals for assessing coders? agreement on discourse segmentation
tasks (such as Passonneau and Litman 1993) led Carletta (1996) to suggest the adoption
of the K coefficient of agreement, a variant of Cohen?s ? (Cohen 1960), as this had already
been used for similar purposes in content analysis for a long time.1 Carletta?s proposals
? Now at the Institute for Creative Technologies, University of Southern California, 13274 Fiji Way, Marina
Del Rey, CA 90292.
?? At the University of Essex: Department of Computing and Electronic Systems, University of Essex,
Wivenhoe Park, Colchester, CO4 3SQ, UK. E-mail: poesio@essex.ac.uk. At the University of Trento:
CIMeC, Universita` degli Studi di Trento, Palazzo Fedrigotti, Corso Bettini, 31, 38068 Rovereto (TN), Italy.
E-mail: massimo.poesio@unitn.it.
1 The literature is full of terminological inconsistencies. Carletta calls the coefficient of agreement she
argues for ?kappa,? referring to Krippendorff (1980) and Siegel and Castellan (1988), and using Siegel
and Castellan?s terminology and definitions. However, Siegel and Castellan?s statistic, which they call K,
is actually Fleiss?s generalization to more than two coders of Scott?s ?, not of the original Cohen?s ?; to
confuse matters further, Siegel and Castellan use the Greek letter ? to indicate the parameter which is
estimated by K. In what follows, we use ? to indicate Cohen?s original coefficient and its generalization
to more than two coders, and K for the coefficient discussed by Siegel and Castellan.
Submission received: 26 August 2005; revised submission received: 21 December 2007; accepted for
publication: 28 January 2008.
? 2008 Association for Computational Linguistics
Computational Linguistics Volume 34, Number 4
were enormously influential, and K quickly became the de facto standard for measuring
agreement in computational linguistics not only in work on discourse (Carletta et al
1997; Core and Allen 1997; Hearst 1997; Poesio and Vieira 1998; Di Eugenio 2000; Stolcke
et al 2000; Carlson, Marcu, and Okurowski 2003) but also for other annotation tasks
(e.g., Ve?ronis 1998; Bruce and Wiebe 1998; Stevenson and Gaizauskas 2000; Craggs and
McGee Wood 2004; Mieskes and Strube 2006). During this period, however, a number
of questions have also been raised about K and similar coefficients?some already in
Carletta?s own work (Carletta et al 1997)?ranging from simple questions about the
way the coefficient is computed (e.g., whether it is really applicable when more than
two coders are used), to debates about which levels of agreement can be considered
?acceptable? (Di Eugenio 2000; Craggs and McGee Wood 2005), to the realization that K
is not appropriate for all types of agreement (Poesio and Vieira 1998; Marcu, Romera,
and Amorrortu 1999; Di Eugenio 2000; Stevenson and Gaizauskas 2000). Di Eugenio
raised the issue of the effect of skewed distributions on the value of K and pointed out
that the original ? developed by Cohen is based on very different assumptions about
coder bias from the K of Siegel and Castellan (1988), which is typically used in CL. This
issue of annotator bias was further debated in Di Eugenio and Glass (2004) and Craggs
andMcGeeWood (2005). Di Eugenio andGlass pointed out that the choice of calculating
chance agreement by using individual coder marginals (?) or pooled distributions (K)
can lead to reliability values falling on different sides of the accepted 0.67 threshold,
and recommended reporting both values. Craggs and McGee Wood argued, following
Krippendorff (2004a,b), that measures like Cohen?s ? are inappropriate for measur-
ing agreement. Finally, Passonneau has been advocating the use of Krippendorff?s ?
(Krippendorff 1980, 2004a) for coding tasks in CL which do not involve nominal and
disjoint categories, including anaphoric annotation, wordsense tagging, and summa-
rization (Passonneau 2004, 2006; Nenkova and Passonneau 2004; Passonneau, Habash,
and Rambow 2006).
Now that more than ten years have passed since Carletta?s original presentation
at the workshop on Empirical Methods in Discourse, it is time to reconsider the use
of coefficients of agreement in CL in a systematic way. In this article, a survey of
coefficients of agreement and their use in CL, we have threemain goals. First, we discuss
in some detail the mathematics and underlying assumptions of the coefficients used or
mentioned in the CL and content analysis literatures. Second, we also cover in some
detail Krippendorff?s ?, often mentioned but never really discussed in detail in previous
CL literature other than in the papers by Passonneau just mentioned. Third, we review
the past ten years of experience with coefficients of agreement in CL, reconsidering the
issues that have been raised also from a mathematical perspective.2
2. Coefficients of Agreement
2.1 Agreement, Reliability, and Validity
We begin with a quick recap of the goals of agreement studies, inspired by Krippendorff
(2004a, Section 11.1). Researchers who wish to use hand-coded data?that is, data in
which items are labeled with categories, whether to support an empirical claim or to
develop and test a computational model?need to show that such data are reliable.
2 Only part of our material could fit in this article. An extended version of the survey is available from
http://cswww.essex.ac.uk/Research/nle/arrau/.
556
Artstein and Poesio Inter-Coder Agreement for CL
The fundamental assumption behind the methodologies discussed in this article is that
data are reliable if coders can be shown to agree on the categories assigned to units to
an extent determined by the purposes of the study (Krippendorff 2004a; Craggs and
McGeeWood 2005). If different coders produce consistently similar results, then we can
infer that they have internalized a similar understanding of the annotation guidelines,
and we can expect them to perform consistently under this understanding.
Reliability is thus a prerequisite for demonstrating the validity of the coding
scheme?that is, to show that the coding scheme captures the ?truth? of the phenom-
enon being studied, in case this matters: If the annotators are not consistent then either
some of them are wrong or else the annotation scheme is inappropriate for the data.
(Just as in real life, the fact that witnesses to an event disagree with each other makes it
difficult for third parties to know what actually happened.) However, it is important to
keep in mind that achieving good agreement cannot ensure validity: Two observers of
the same event may well share the same prejudice while still being objectively wrong.
2.2 A Common Notation
It is useful to think of a reliability study as involving a set of items (markables), a
set of categories, and a set of coders (annotators) who assign to each item a unique
category label. The discussions of reliability in the literature often use different notations
to express these concepts. We introduce a uniform notation, which we hope will make
the relations between the different coefficients of agreement clearer.
? The set of items is { i | i ? I } and is of cardinality i.
? The set of categories is { k | k ? K } and is of cardinality k.
? The set of coders is { c | c ? C } and is of cardinality c.
Confusion also arises from the use of the letter P, which is used in the literature with at
least three distinct interpretations, namely ?proportion,? ?percent,? and ?probability.?
We will use the following notation uniformly throughout the article.
? Ao is observed agreement and Do is observed disagreement.
? Ae and De are expected agreement and expected disagreement,
respectively. The relevant coefficient will be indicated with a superscript
when an ambiguity may arise (for example, A?e is the expected agreement
used for calculating ?, and A?e is the expected agreement used for
calculating ?).
? P(?) is reserved for the probability of a variable, and P?(?) is an estimate of
such probability from observed data.
Finally, we use n with a subscript to indicate the number of judgments of a given type:
? nik is the number of coders who assigned item i to category k;
? nck is the number of items assigned by coder c to category k;
? nk is the total number of items assigned by all coders to category k.
557
Computational Linguistics Volume 34, Number 4
2.3 Agreement Without Chance Correction
The simplest measure of agreement between two coders is percentage of agreement or
observed agreement, defined for example by Scott (1955, page 323) as ?the percentage of
judgments on which the two analysts agree when coding the same data independently.?
This is the number of items on which the coders agree divided by the total number
of items. More precisely, and looking ahead to the following discussion, observed
agreement is the arithmetic mean of the agreement value agri for all items i ? I, defined
as follows:
agri =
{
1 if the two coders assign i to the same category
0 if the two coders assign i to different categories
Observed agreement over the values agri for all items i ? I is then:
Ao =
1
i
?
i?I
agri
For example, let us assume a very simple annotation scheme for dialogue acts in
information-seeking dialogues which makes a binary distinction between the categories
statement and info-request, as in the DAMSL dialogue act scheme (Allen and Core
1997). Two coders classify 100 utterances according to this scheme as shown in Table 1.
Percentage agreement for this data set is obtained by summing up the cells on the
diagonal and dividing by the total number of items: Ao = (20+ 50)/100 = 0.7.
Observed agreement enters in the computation of all the measures of agreement we
consider, but on its own it does not yield values that can be compared across studies,
because some agreement is due to chance, and the amount of chance agreement is
affected by two factors that vary from one study to the other. First of all, as Scott (1955,
page 322) points out, ?[percentage agreement] is biased in favor of dimensions with a
small number of categories.? In other words, given two coding schemes for the same
phenomenon, the one with fewer categories will result in higher percentage agreement
just by chance. If two coders randomly classify utterances in a uniform manner using
the scheme of Table 1, we would expect an equal number of items to fall in each of the
four cells in the table, and therefore pure chance will cause the coders to agree on half of
the items (the two cells on the diagonal: 14 +
1
4 ). But suppose wewant to refine the simple
binary coding scheme by introducing a new category, check, as in the MapTask coding
scheme (Carletta et al 1997). If two coders randomly classify utterances in a uniform
manner using the three categories in the second scheme, they would only agree on a
third of the items (19 +
1
9 +
1
9 ).
Table 1
A simple example of agreement on dialogue act tagging.
CODER A
STAT IREQ TOTAL
STAT 20 20 40
CODER B IREQ 10 50 60
TOTAL 30 70 100
558
Artstein and Poesio Inter-Coder Agreement for CL
The second reason percentage agreement cannot be trusted is that it does not
correct for the distribution of items among categories: We expect a higher percentage
agreement when one category is much more common than the other. This problem,
already raised by Hsu and Field (2003, page 207) among others, can be illustrated
using the following example (Di Eugenio and Glass 2004, example 3, pages 98?99).
Suppose 95% of utterances in a particular domain are statement, and only 5% are info-
request. We would then expect by chance that 0.95? 0.95 = 0.9025 of the utterances
would be classified as statement by both coders, and 0.05 ? 0.05 = 0.0025 as info-
request, so the coders would agree on 90.5% of the utterances. Under such circum-
stances, a seemingly high observed agreement of 90% is actually worse than expected by
chance.
The conclusion reached in the literature is that in order to get figures that are compa-
rable across studies, observed agreement has to be adjusted for chance agreement. These
are the measures we will review in the remainder of this article. We will not look at the
variants of percentage agreement used in CL work on discourse before the introduction
of kappa, such as percentage agreement with an expert and percentage agreement with
the majority; see Carletta (1996) for discussion and criticism.3
2.4 Chance-Corrected Coefficients for Measuring Agreement between Two Coders
All of the coefficients of agreement discussed in this article correct for chance on the
basis of the same idea. First we find how much agreement is expected by chance: Let us
call this value Ae. The value 1?Ae will then measure how much agreement over and
above chance is attainable; the value Ao ?Ae will tell us how much agreement beyond
chance was actually found. The ratio between Ao?Ae and 1?Ae will then tell us which
proportion of the possible agreement beyond chance was actually observed. This idea
is expressed by the following formula.
S,?, ? =
Ao ?Ae
1?Ae
The three best-known coefficients, S (Bennett, Alpert, and Goldstein 1954), ? (Scott
1955), and ? (Cohen 1960), and their generalizations, all use this formula; whereas
Krippendorff?s ? is based on a related formula expressed in terms of disagreement
(see Section 2.6). All three coefficients therefore yield values of agreement between
?Ae/1?Ae (no observed agreement) and 1 (observed agreement = 1), with the value 0
signifying chance agreement (observed agreement = expected agreement). Note also
that whenever agreement is less than perfect (Ao < 1), chance-corrected agreement
will be strictly lower than observed agreement, because some amount of agreement
is always expected by chance.
Observed agreement Ao is easy to compute, and is the same for all three
coefficients?the proportion of items on which the two coders agree. But the notion
of chance agreement, or the probability that two coders will classify an arbitrary item
as belonging to the same category by chance, requires a model of what would happen
if coders? behavior was only by chance. All three coefficients assume independence of
the two coders?that is, that the chance of c1 and c2 agreeing on any given category k
3 The extended version of the article also includes a discussion of why ?2 and correlation coefficients are
not appropriate for this task.
559
Computational Linguistics Volume 34, Number 4
Table 2
The value of different coefficients applied to the data from Table 1.
Coefficient Expected agreement Chance-corrected agreement
S 2? ( 12 )
2 = 0.5 (0.7? 0.5)/(1? 0.5) = 0.4
? 0.352 + 0.652 = 0.545 (0.7? 0.545)/(1? 0.545) ? 0.341
? 0.3? 0.4+ 0.6? 0.7 = 0.54 (0.7? 0.54)/(1? 0.54) ? 0.348
Observed agreement for all the coefficients is 0.7.
is the product of the chance of each of them assigning an item to that category:
P(k|c1) ? P(k|c2).4 Expected agreement is then the probability of c1 and c2 agreeing on
any category, that is, the sum of the product over all categories:
ASe = A
?
e = A
?
e = ?
k?K
P(k|c1) ? P(k|c2)
The difference between S, ?, and ? lies in the assumptions leading to the calculation of
P(k|ci), the chance that coder ci will assign an arbitrary item to category k (Zwick 1988;
Hsu and Field 2003).
S: This coefficient is based on the assumption that if coders were operating
by chance alone, we would get a uniform distribution: That is, for any two
coders cm, cn and any two categories kj, kl , P(kj|cm) = P(kl |cn).
?: If coders were operating by chance alone, we would get the same
distribution for each coder: For any two coders cm, cn and any category k,
P(k|cm) = P(k|cn).
?: If coders were operating by chance alone, we would get a separate
distribution for each coder.
Additionally, the lack of independent prior knowledge of the distribution of items
among categories means that the distribution of categories (for ?) and the priors for the
individual coders (for ?) have to be estimated from the observed data. Table 2 demon-
strates the effect of the different chance models on the coefficient values. The remainder
of this section explains how the three coefficients are calculated when the reliability data
come from two coders; we will discuss a variety of proposed generalizations starting in
Section 2.5.
2.4.1 All Categories Are Equally Likely: S. The simplest way of discounting for chance
is the one adopted to compute the coefficient S (Bennett, Alpert, and Goldstein 1954),
also known in the literature as C, ?n, G, and RE (see Zwick 1988; Hsu and Field 2003).
As noted previously, the computation of S is based on an interpretation of chance as
a random choice of category from a uniform distribution?that is, all categories are
equally likely. If coders classify the items into k categories, then the chance P(k|ci) of
4 The independence assumption has been the subject of much criticism, for example by John S. Uebersax.
http://ourworld.compuserve.com/homepages/jsuebersax/agree.htm.
560
Artstein and Poesio Inter-Coder Agreement for CL
any coder assigning an item to category k under the uniformity assumption is 1k ; hence
the total agreement expected by chance is
ASe = ?
k?K
1
k
?
1
k
= k ?
(
1
k
)2
=
1
k
The calculation of the value of S for the figures in Table 1 is shown in Table 2.
The coefficient S is problematic in many respects. The value of the coefficient can
be artificially increased simply by adding spurious categories which the coders would
never use (Scott 1955, pages 322?323). In the case of CL, for example, S would reward
designing extremely fine-grained tagsets, provided that most tags are never actually
encountered in real data. Additional limitations are noted byHsu and Field (2003). It has
been argued that uniformity is the best model for a chance distribution of items among
categories if we have no independent prior knowledge of the distribution (Brennan and
Prediger 1981). However, a lack of prior knowledge does not mean that the distribution
cannot be estimated post hoc, and this is what the other coefficients do.
2.4.2 A Single Distribution: ?. All of the other methods for discounting chance agreement
we discuss in this article attempt to overcome the limitations of S?s strong uniformity
assumption using an idea first proposed by Scott (1955): Use the actual behavior of the
coders to estimate the prior distribution of the categories. As noted earlier, Scott based
his characterization of ? on the assumption that random assignment of categories to
items, by any coder, is governed by the distribution of items among categories in the
actual world. The best estimate of this distribution is P?(k), the observed proportion of
items assigned to category k by both coders.
P(k|c1) = P(k|c2) = P?(k)
P?(k), the observed proportion of items assigned to category k by both coders, is the
total number of assignments to k by both coders nk, divided by the overall number of
assignments, which for the two-coder case is twice the number of items i:
P?(k) =
nk
2i
Given the assumption that coders act independently, expected agreement is computed
as follows.
A?e = ?
k?K
P?(k) ? P?(k) =
?
k?K
(nk
2i
)2
=
1
4i2
?
k?K
n2k
It is easy to show that for any set of coding data, A?e ? A
S
e and therefore ? ? S, with
the limiting case (equality) obtaining when the observed distribution of items among
categories is uniform.
2.4.3 Individual Coder Distributions: ?. The method proposed by Cohen (1960) to calcu-
late expected agreement Ae in his ? coefficient assumes that random assignment of
categories to items is governed by prior distributions that are unique to each coder,
and which reflect individual annotator bias. An individual coder?s prior distribution is
561
Computational Linguistics Volume 34, Number 4
estimated by looking at her actual distribution: P(k|ci), the probability that coder ci will
classify an arbitrary item into category k, is estimated by using P?(k|ci), the proportion
of items actually assigned by coder ci to category k; this is the number of assignments
to k by ci, ncik, divided by the number of items i.
P(k|ci) = P?(k|ci) =
ncik
i
As in the case of S and ?, the probability that the two coders c1 and c2 assign an item to
a particular category k ? K is the joint probability of each coder making this assignment
independently. For ? this joint probability is P?(k|c1) ? P?(k|c2); expected agreement is then
the sum of this joint probability over all the categories k ? K.
A?e = ?
k?K
P?(k|c1) ? P?(k|c2) =
?
k?K
nc1k
i
?
nc2k
i
=
1
i2
?
k?K
nc1knc2k
It is easy to show that for any set of coding data, A?e ? A
?
e and therefore ? ? ?, with the
limiting case (equality) obtaining when the observed distributions of the two coders are
identical. The relationship between ? and S is not fixed.
2.5 More Than Two Coders
In corpus annotation practice, measuring reliability with only two coders is seldom
considered enough, except for small-scale studies. Sometimes researchers run reliability
studies with more than two coders, measure agreement separately for each pair of
coders, and report the average. However, a better practice is to use generalized versions
of the coefficients. A generalization of Scott?s ? is proposed in Fleiss (1971), and a
generalization of Cohen?s ? is given in Davies and Fleiss (1982). We will call these
coefficients multi-? and multi-?, respectively, dropping the multi-prefixes when no
confusion is expected to arise.5
2.5.1 Fleiss?s Multi-?. With more than two coders, the observed agreement Ao can no
longer be defined as the percentage of items on which there is agreement, because
inevitably there will be items on which some coders agree and others disagree. The
solution proposed in the literature is to measure pairwise agreement (Fleiss 1971):
Define the amount of agreement on a particular item as the proportion of agreeing
judgment pairs out of the total number of judgment pairs for that item.
Multiple coders also pose a problem for the visualization of the data. When the
number of coders c is greater than two, judgments cannot be shown in a contingency
table like Table 1, because each coder has to be represented in a separate dimension.
5 Due to historical accident, the terminology in the literature is confusing. Fleiss (1971) proposed a
coefficient of agreement for multiple coders and called it ?, even though it calculates expected agreement
based on the cumulative distribution of judgments by all coders and is thus better thought of as a
generalization of Scott?s ?. This unfortunate choice of name was the cause of much confusion in
subsequent literature: Often, studies which claim to give a generalization of ? to more than two coders
actually report Fleiss?s coefficient (e.g., Bartko and Carpenter 1976; Siegel and Castellan 1988; Di Eugenio
and Glass 2004). Since Carletta (1996) introduced reliability to the CL community based on the definitions
of Siegel and Castellan (1988), the term ?kappa? has been usually associated in this community with
Siegel and Castellan?s K, which is in effect Fleiss?s coefficient, that is, a generalization of Scott?s ?.
562
Artstein and Poesio Inter-Coder Agreement for CL
Fleiss (1971) therefore uses a different type of table which lists each item with the num-
ber of judgments it received for each category; Siegel and Castellan (1988) use a similar
table, which Di Eugenio and Glass (2004) call an agreement table. Table 3 is an example
of an agreement table, in which the same 100 utterances from Table 1 are labeled by
three coders instead of two. Di Eugenio and Glass (page 97) note that compared to
contingency tables like Table 1, agreement tables like Table 3 lose information because
they do not say which coder gave each judgment. This information is not used in the
calculation of ?, but is necessary for determining the individual coders? distributions in
the calculation of ?. (Agreement tables also add information compared to contingency
tables, namely, the identity of the items that make up each contingency class, but this
information is not used in the calculation of either ? or ?.)
Let nik stand for the number of times an item i is classified in category k (i.e., the
number of coders that make such a judgment): For example, given the distribution in
Table 3, nUtt1Stat = 2 and nUtt1IReq = 1. Each category k contributes (
nik
2 ) pairs of agreeing
judgments for item i; the amount of agreement agri for item i is the sum of (
nik
2 ) over all
categories k ? K, divided by (c2), the total number of judgment pairs per item.
agri =
1
(c2)
?
k?K
(
nik
2
)
=
1
c(c? 1) ?
k?K
nik(nik ? 1)
For example, given the results in Table 3, we find the agreement value for Utterance 1
as follows.
agr1 =
1
(32)
((
nUtt1Stat
2
)
+
(
nUtt1IReq
2
))
=
1
3
(1+ 0) ? 0.33
The overall observed agreement is the mean of agri for all items i ? I.
Ao =
1
i
?
i?I
agri =
1
ic(c? 1) ?
i?I
?
k?K
nik(nik ? 1)
(Notice that this definition of observed agreement is equivalent to the mean of the
two-coder observed agreement values from Section 2.4 for all coder pairs.)
If observed agreement is measured on the basis of pairwise agreement (the pro-
portion of agreeing judgment pairs), it makes sense to measure expected agreement in
terms of pairwise comparisons as well, that is, as the probability that any pair of judg-
ments for an item would be in agreement?or, said otherwise, the probability that two
Table 3
Agreement table with three coders.
STAT IREQ
Utt1 2 1
Utt2 0 3
...
Utt100 1 2
TOTAL 90 (0.3) 210 (0.7)
563
Computational Linguistics Volume 34, Number 4
arbitrary coders would make the same judgment for a particular item by chance. This is
the approach taken by Fleiss (1971). Like Scott, Fleiss interprets ?chance agreement? as
the agreement expected on the basis of a single distribution which reflects the combined
judgments of all coders, meaning that expected agreement is calculated using P?(k), the
overall proportion of items assigned to category k, which is the total number of such
assignments by all coders nk divided by the overall number of assignments. The latter,
in turn, is the number of items imultiplied by the number of coders c.
P?(k) =
1
ic
nk
As in the two-coder case, the probability that two arbitrary coders assign an item to a
particular category k ? K is assumed to be the joint probability of each coder making
this assignment independently, that is (P?(k))2. The expected agreement is the sum of
this joint probability over all the categories k ? K.
A?e = ?
k?K
(
P?(k)
)2
=
?
k?K
(
1
ic
nk
)2
=
1
(ic)2
?
k?K
n2k
Multi-? is the coefficient that Siegel and Castellan (1988) call K.
2.5.2 Multi-?. It is fairly straightforward to adapt Fleiss?s proposal to generalize
Cohen?s ? proper to more than two coders, calculating expected agreement based on
individual coder marginals. A detailed proposal can be found in Davies and Fleiss
(1982), or in the extended version of this article.
2.6 Krippendorff?s ? and Other Weighted Agreement Coefficients
A serious limitation of both ? and ? is that all disagreements are treated equally. But
especially for semantic and pragmatic features, disagreements are not all alike. Even for
the relatively simple case of dialogue act tagging, a disagreement between an accept
and a reject interpretation of an utterance is clearly more serious than a disagreement
between an info-request and a check. For tasks such as anaphora resolution, where
reliability is determined by measuring agreement on sets (coreference chains), allowing
for degrees of disagreement becomes essential (see Section 4.4). Under such circum-
stances, ? and ? are not very useful.
In this section we discuss two coefficients that make it possible to differentiate
between types of disagreements: ? (Krippendorff 1980, 2004a), which is a coefficient
defined in a general way that is appropriate for use with multiple coders, different
magnitudes of disagreement, and missing values, and is based on assumptions similar
to those of ?; and weighted kappa ?w (Cohen 1968), a generalization of ?.
2.6.1 Krippendorff?s ?. The coefficient ? (Krippendorff 1980, 2004a) is an extremely ver-
satile agreement coefficient based on assumptions similar to ?, namely, that expected
agreement is calculated by looking at the overall distribution of judgments without
regard to which coders produced these judgments. It applies to multiple coders, and
it allows for different magnitudes of disagreement. When all disagreements are con-
sidered equal it is nearly identical to multi-?, correcting for small sample sizes by
using an unbiased estimator for expected agreement. In this section we will present
564
Artstein and Poesio Inter-Coder Agreement for CL
Krippendorff?s ? and relate it to the other coefficients discussed in this article, but we
will start with ??s origins as a measure of variance, following a long tradition of using
variance to measure reliability (see citations in Rajaratnam 1960; Krippendorff 1970).
A sample?s variance s2 is defined as the sum of square differences from the mean
SS =
?
(x ? x?)2 divided by the degrees of freedom df . Variance is a useful way of
looking at agreement if coders assign numerical values to the items, as in magnitude
estimation tasks. Each item in a reliability study can be considered a separate level
in a single-factor analysis of variance: The smaller the variance around each level, the
higher the reliability. When agreement is perfect, the variance within the levels (s2within)
is zero; when agreement is at chance, the variance within the levels is equal to the
variance between the levels, in which case it is also equal to the overall variance of the
data: s2within = s
2
between = s
2
total. The ratios s
2
within/s
2
between (that is, 1/F) and s
2
within/s
2
total are
therefore 0 when agreement is perfect and 1 when agreement is at chance. Additionally,
the latter ratio is bounded at 2: SSwithin ? SStotal by definition, and df total < 2 df within
because each item has at least two judgments. Subtracting the ratio s2within/s
2
total from 1
yields a coefficient which ranges between?1 and 1, where 1 signifies perfect agreement
and 0 signifies chance agreement.
? = 1?
s2within
s2
total
= 1?
SSwithin/df within
SStotal/df total
We can unpack the formula for ? to bring it to a form which is similar to the other
coefficients we have looked at, and which will allow generalizing ? beyond simple
numerical values. The first step is to get rid of the notion of arithmetic meanwhich lies at
the heart of the measure of variance. We observe that for any set of numbers x1, . . . , xN
with a mean x? = 1N ?
N
n=1 xn, the sum of square differences from the mean SS can be
expressed as the sum of square of differences between all the (ordered) pairs of numbers,
scaled by a factor of 1/2N.
SS =
N
?
n=1
(xn ? x?)2 =
1
2N
N
?
n=1
N
?
m=1
(xn ? xm)2
For calculating ? we considered each item to be a separate level in an analysis of
variance; the number of levels is thus the number of items i, and because each coder
marks each item, the number of observations for each item is the number of coders c.
Within-level variance is the sum of the square differences from the mean of each item,
SSwithin = ?i ?c(xic ? x?i)2, divided by the degrees of freedom df within = i(c ? 1). We
can express this as the sum of the squares of the differences between all of the judgment
pairs for each item, summed over all items and scaled by the appropriate factor. We use
the notation xic for the value given by coder c to item i, and x?i for the mean of all the
values given to item i.
s2within =
SSwithin
df within
=
1
i(c? 1) ?
i?I
?
c?C
(xic ? x?i)
2 =
1
2ic(c? 1) ?
i?I
c
?
m=1
c
?
n=1
(xicm ? xicn)
2
The total variance is the sum of the square differences of all judgments from the grand
mean, SStotal = ?i ?c(xic ? x?)2, divided by the degrees of freedom df total = ic? 1. This
565
Computational Linguistics Volume 34, Number 4
can be expressed as the sum of the squares of the differences between all of the judg-
ments pairs without regard to items, again scaled by the appropriate factor. The notation
x? is the overall mean of all the judgments in the data.
s2total =
SStotal
df total
=
1
ic? 1?
i?I
?
c?C
(xic ? x?)
2 =
1
2ic(ic? 1)
i
?
j=1
c
?
m=1
i
?
l=1
c
?
n=1
(xijcm ? xil cn)
2
Now that we have removed references tomeans from our formulas, we can abstract over
the measure of variance. We define a distance function d which takes two numbers and
returns the square of their difference.
dab = (a ? b)
2
We also simplify the computation by counting all the identical value assignments
together. Each unique value used by the coders will be considered a category k ? K.
We use nik for the number of times item i is given the value k, that is, the number of
coders that make such a judgment. For every (ordered) pair of distinct values ka, kb ? K
there are nikanikb pairs of judgments of item i, whereas for non-distinct values there
are nika(nika ? 1) pairs. We use this notation to rewrite the formula for the within-level
variance. D?o, the observed disagreement for ?, is defined as twice the variance within
the levels in order to get rid of the factor 2 in the denominator; we also simplify the
formula by using the multiplier nikanika for identical categories?this is allowed because
dkk = 0 for all k.
D?o = 2 s
2
within =
1
ic(c? 1) ?
i?I
k
?
j=1
k
?
l=1
nikjnikldkjkl
We perform the same simplification for the total variance, where nk stands for the
total number of times the value k is assigned to any item by any coder. The expected
disagreement for ?, D?e , is twice the total variance.
D?e = 2 s
2
total =
1
ic(ic? 1)
k
?
j=1
k
?
l=1
nkjnkldkjkl
Because both expected and observed disagreement are twice the respective vari-
ances, the coefficient ? retains the same form when expressed with the disagreement
values.
? = 1?
Do
De
Now that ? has been expressed without explicit reference to means, differences, and
squares, it can be generalized to a variety of coding schemes in which the labels cannot
be interpreted as numerical values: All one has to do is to replace the square difference
function d with a different distance function. Krippendorff (1980, 2004a) offers distance
metrics suitable for nominal, interval, ordinal, and ratio scales. Of particular interest is
566
Artstein and Poesio Inter-Coder Agreement for CL
the function for nominal categories, that is, a function which considers all distinct labels
equally distant from one another.
dab =
{
0 if a = b
1 if a = b
It turns out that with this distance function, the observed disagreement D?o is exactly the
complement of the observed agreement of Fleiss?s multi-?, 1? A?o , and the expected
disagreement D?e differs from 1 ? A
?
e by a factor of (ic ? 1)/ic; the difference is due
to the fact that ? uses a biased estimator of the expected agreement in the population
whereas ? uses an unbiased estimator. The following equation shows that given the
correspondence between observed and expected agreement and disagreement, the co-
efficients themselves are nearly equivalent.
? = 1?
D?o
D?e
? 1?
1?A?o
1?A?e
=
1?A?e ? (1?A
?
o )
1?A?e
=
A?o ?A
?
e
1?A?e
= ?
For nominal data, the coefficients ? and ? approach each other as either the number of
items or the number of coders approaches infinity.
Krippendorff?s ? will work with any distance metric, provided that identical cat-
egories always have a distance of zero (dkk = 0 for all k). Another useful constraint is
symmetry (dab = dba for all a, b). This flexibility affords new possibilities for analysis,
which we will illustrate in Section 4. We should also note, however, that the flexibility
also creates new pitfalls, especially in cases where it is not clear what the natural dis-
tance metric is. For example, there are different ways to measure dissimilarity between
sets, and any of these measures can be justifiably used when the category labels are
sets of items (as in the annotation of anaphoric relations). The different distance metrics
yield different values of ? for the same annotation data, making it difficult to interpret
the resulting values. We will return to this problem in Section 4.4.
2.6.2 Cohen?s ?w. A weighted variant of Cohen?s ? is presented in Cohen (1968). The
implementation of weights is similar to that of Krippendorff?s ??each pair of cate-
gories ka, kb ? K is associated with a weight dkakb , where a larger weight indicates more
disagreement (Cohen uses the notation v; he does not place any general constraints on
the weights?not even a requirement that a pair of identical categories have a weight of
zero, or that the weights be symmetric across the diagonal). The coefficient is defined
for two coders: The disagreement for a particular item i is the weight of the pair of
categories assigned to it by the two coders, and the overall observed disagreement is
the (normalized) mean disagreement of all the items. Let k(cn, i) denote the category
assigned by coder cn to item i; then the disagreement for item i is disagri = dk(c1,i)k(c2,i).
The observed disagreement Do is the mean of disagri for all items i, normalized to the
interval [0, 1] through division by the maximal weight dmax.
D?wo =
1
dmax
1
i
?
i?I
disagri =
1
dmax
1
i
?
i?I
dk(c1,i)k(c2,i)
If we take all disagreements to be of equal weight, that is dkaka = 0 for all categories ka
and dkakb = 1 for all ka = kb, then the observed disagreement is exactly the complement
of the observed agreement as calculated in Section 2.4: D?wo = 1?A?o.
567
Computational Linguistics Volume 34, Number 4
Like ?, the coefficient ?w interprets expected disagreement as the amount expected
by chance from a distinct probability distribution for each coder. These individual
distributions are estimated by P?(k|c), the proportion of items assigned by coder c to
category k, that is the number of such assignments nck divided by the number of items i.
P?(k|c) =
1
i
nck
The probability that coder c1 assigns an item to category ka and coder c2 assigns it to
category kb is the joint probability of each coder making this assignment independently,
namely, P?(ka|c1)P?(kb|c2). The expected disagreement is the mean of the weights for
all (ordered) category pairs, weighted by the probabilities of the category pairs and
normalized to the interval [0, 1] through division by the maximal weight.
D?we =
1
dmax
k
?
j=1
k
?
l=1
P?(kj|c1)P?(kl |c2)dkjkl =
1
dmax
1
i2
k
?
j=1
k
?
l=1
nc1kjnc2kldkjkl
If we take all disagreements to be of equal weight then the expected disagreement is
exactly the complement of the expected agreement for ? as calculated in Section 2.4:
D?we = 1?A?e.
Finally, the coefficient ?w itself is the ratio of observed disagreement to expected
disagreement, subtracted from 1 in order to yield a final value in terms of agreement.
?w = 1?
Do
De
2.7 An Integrated Example
We end this section with an example illustrating how all of the agreement coefficients
just discussed are computed. To facilitate comparisons, all computations will be based
on the annotation statistics in Table 4. This confusion matrix reports the results of an
experiment where two coders classify a set of utterances into three categories.
2.7.1 The Unweighted Coefficients. Observed agreement for all of the unweighted coeffi-
cients (S, ?, and ?) is calculated by counting the items on which the coders agree (the
Table 4
An integrated coding example.
CODER A
STAT IREQ CHCK TOTAL
STAT 46 6 0 52
IREQ 0 32 0 32
CODER B CHCK 0 6 10 16
TOTAL 46 44 10 100
568
Artstein and Poesio Inter-Coder Agreement for CL
figures on the diagonal of the confusion matrix in Table 4) and dividing by the total
number of items.
Ao =
46+ 32+ 10
100
= 0.88
The expected agreement values and the resulting values for the coefficients are shown in
Table 5. The values of ? and ? are very similar, which is to be expected when agreement
is high, because this implies similar marginals. Notice that A?e < A
?
e , hence ? > ?; this
reflects a general property of ? and ?, already mentioned in Section 2.4, which will be
elaborated in Section 3.1.
2.7.2 Weighted Coefficients. Suppose we notice that whereas Statement and Info-
Request are clearly distinct classifications, Check is somewhere between the two. We
therefore opt to weigh the distances between the categories as follows (recall that
1 denotes maximal disagreement, and identical categories are in full agreement and
thus have a distance of 0).
Statement Info-Request Check
Statement 0 1 0.5
Info-Request 1 0 0.5
Check 0.5 0.5 0
The observed disagreement is calculated by summing up all the cells in the contingency
table, multiplying each cell by its respective weight, and dividing the total by the
number of items (in the following calculation we ignore cells with zero items).
Do =
46? 0+ 6? 1+ 32? 0+ 6? 0.5+ 10? 0
100
=
6+ 3
100
= 0.09
The only sources of disagreement in the coding example of Table 4 are the six utterances
marked as Info-Requests by coder A and Statements by coder B, which receive the
maximal weight of 1, and the six utterances marked as Info-Requests by coder A and
Checks by coder B, which are given a weight of 0.5.
The calculation of expected disagreement for the weighted coefficients is shown in
Table 6, and is the sum of the expected disagreement for each category pair multiplied
Table 5
Unweighted coefficients for the data from Table 4.
Expected agreement Chance-corrected agreement
S 3? ( 13 )
2 = 13 (0.88?
1
3 )/(1?
1
3 ) = 0.82
?
0.46+0.52
2 +
0.44+0.32
2 +
0.10+0.16
2 = 0.4014 (0.88? 0.4014)/(1? 0.4014) ? 0.7995
? .46? .52+ .44? .32+ .1? .16 = 0.396 (0.88? 0.396)/(1? 0.396) ? 0.8013
569
Computational Linguistics Volume 34, Number 4
Table 6
Expected disagreement of the weighted coefficients for the data from Table 4.
D?e
(46+52)?(46+52)
2?100?(2?100?1) ? 0+
(44+32)?(46+52)
2?100?(2?100?1) ? 1 +
(10+16)?(46+52)
2?100?(2?100?1) ?
1
2
+
(46+52)?(44+32)
2?100?(2?100?1) ? 1 +
(44+32)?(44+32)
2?100?(2?100?1) ? 0 +
(10+16)?(44+32)
2?100?(2?100?1) ?
1
2
+
(46+52)?(10+16)
2?100?(2?100?1) ?
1
2 +
(44+32)?(10+16)
2?100?(2?100?1) ?
1
2 +
(10+16)?(10+16)
2?100?(2?100?1) ? 0
0.4879
D?we
46?52
100?100 ? 0+
44?52
100?100 ? 1 +
10?52
100?100 ?
1
2
+ 46?32100?100 ? 1 +
44?32
100?100 ? 0 +
10?32
100?100 ?
1
2
+ 46?16100?100 ?
1
2 +
44?16
100?100 ?
1
2 +
10?16
100?100 ? 0
0.49
by its weight. The value of the weighted coefficients is given by the formula 1? DoDe , so
? ? 1? 0.090.4879 ? 0.8156, and ?w = 1?
0.09
0.49 ? 0.8163.
3. Bias and Prevalence
Two issues recently raised by Di Eugenio and Glass (2004) concern the behavior of
agreement coefficients when the annotation data are severely skewed. One issue, which
Di Eugenio and Glass call the bias problem, is that ? and ? yield quite different
numerical values when the annotators? marginal distributions are widely divergent;
the other issue, the prevalence problem, is the exceeding difficulty in getting high
agreement values when most of the items fall under one category. Looking at these two
problems in detail is useful for understanding the differences between the coefficients.
3.1 Annotator Bias
The difference between ? and ? on the one hand and ? on the other hand lies in the
interpretation of the notion of chance agreement, whether it is the amount expected
from the the actual distribution of items among categories (?) or from individual coder
priors (?). As mentioned in Section 2.4, this difference has been the subject of much
debate (Fleiss 1975; Krippendorff 1978, 2004b; Byrt, Bishop, andCarlin 1993; Zwick 1988;
Hsu and Field 2003; Di Eugenio and Glass 2004; Craggs and McGee Wood 2005).
A claim often repeated in the literature is that single-distribution coefficients like
? and ? assume that different coders produce similar distributions of items among
categories, with the implication that these coefficients are inapplicable when the anno-
tators show substantially different distributions. Recommendations vary: Zwick (1988)
suggests testing the individual coders? distributions using the modified ?2 test of Stuart
(1955), and discarding the annotation as unreliable if significant systematic discrepan-
cies are observed. In contrast, Hsu and Field (2003, page 214) recommend reporting
the value of ? even when the coders produce different distributions, because it is ?the
only [index] . . . that could legitimately be applied in the presence of marginal hetero-
geneity?; likewise, Di Eugenio andGlass (2004, page 96) recommend using ? in ?the vast
majority . . . of discourse- and dialogue-tagging efforts? where the individual coders?
distributions tend to vary. All of these proposals are based on a misconception: that
570
Artstein and Poesio Inter-Coder Agreement for CL
single-distribution coefficients require similar distributions by the individual annota-
tors in order to work properly. This is not the case. The difference between the coeffi-
cients is only in the interpretation of ?chance agreement?: ?-style coefficients calculate
the chance of agreement among arbitrary coders, whereas ?-style coefficients calcu-
late the chance of agreement among the coders who produced the reliability data. There-
fore, the choice of coefficient should not depend on the magnitude of the divergence
between the coders, but rather on the desired interpretation of chance agreement.
Another common claim is that individual-distribution coefficients like ? ?reward?
annotators for disagreeing on the marginal distributions. For example, Di Eugenio and
Glass (2004, page 99) say that ? suffers from what they call the bias problem, described
as ?the paradox that ?Co [our ?] increases as the coders become less similar.? Similar
reservations about the use of ? have been noted by Brennan and Prediger (1981) and
Zwick (1988). However, the bias problem is less paradoxical than it sounds. Although
it is true that for a fixed observed agreement, a higher difference in coder marginals
implies a lower expected agreement and therefore a higher ? value, the conclusion that
? penalizes coders for having similar distributions is unwarranted. This is because Ao
and Ae are not independent: Both are drawn from the same set of observations. What
? does is discount some of the disagreement resulting from different coder marginals by
incorporating it into Ae. Whether this is desirable depends on the application for which
the coefficient is used.
Themost common application of agreement measures in CL is to infer the reliability
of a large-scale annotation, where typically each piece of data will be marked by just
one coder, by measuring agreement on a small subset of the data which is annotated
by multiple coders. In order to make this generalization, the measure must reflect the
reliability of the annotation procedure, which is independent of the actual annotators
used. Reliability, or reproducibility of the coding, is reduced by all disagreements?both
random and systematic. The most appropriate measures of reliability for this purpose
are therefore single-distribution coefficients like ? and ?, which generalize over the
individual coders and exclude marginal disagreements from the expected agreement.
This argument has been presented recently in much detail by Krippendorff (2004b) and
reiterated by Craggs and McGee Wood (2005).
At the same time, individual-distribution coefficients like ? provide important in-
formation regarding the trustworthiness (validity) of the data on which the annotators
agree. As an intuitive example, think of a person who consults two analysts when
deciding whether to buy or sell certain stocks. If one analyst is an optimist and tends to
recommend buying whereas the other is a pessimist and tends to recommend selling,
they are likely to agree with each other less than two more neutral analysts, so overall
their recommendations are likely to be less reliable?less reproducible?than those that
come from a population of like-minded analysts. This reproducibility is measured by ?.
But whenever the optimistic and pessimistic analysts agree on a recommendation for
a particular stock, whether it is ?buy? or ?sell,? the confidence that this is indeed the
right decision is higher than the same advice from two like-minded analysts. This is
why ? ?rewards? biased annotators: it is not a matter of reproducibility (reliability) but
rather of trustworthiness (validity).
Having said this, we should point out that, first, in practice the difference between
? and ? doesn?t often amount to much (see discussion in Section 4). Moreover, the
difference becomes smaller as agreement increases, because all the points of agreement
contribute toward making the coder marginals similar (it took a lot of experimentation
to create data for Table 4 so that the values of ? and ? would straddle the conventional
cutoff point of 0.80, and even so the difference is very small). Finally, one would expect
571
Computational Linguistics Volume 34, Number 4
the difference between ? and ? to diminish as the number of coders grows; this is shown
subsequently.6
We define B, the overall annotator bias in a particular set of coding data, as the
difference between the expected agreement according to (multi)-? and the expected
agreement according to (multi)-?. Annotator bias is a measure of variance: If we take c to
be a random variable with equal probabilities for all coders, then the annotator bias B
is the sum of the variances of P(k|c) for all categories k ? K, divided by the number of
coders c less one (see Artstein and Poesio [2005] for a proof).
B = A?e ?A
?
e =
1
c? 1 ?
k?K
?
2
P?(k|c)
Annotator bias can be used to express the difference between ? and ?.
? ? ? =
Ao ? (A?e ? B)
1? (A?e ? B)
?
Ao ?A?e
1?A?e
= B ?
(1?Ao)
(1?A?e)(1?A?e )
This allows us to make the following observations about the relationship between
? and ?.
Observation 1. The difference between ? and ? grows as the annotator bias grows: For a
constant Ao and A
?
e , a greater B implies a greater value for ? ? ?.
Observation 2. The greater the number of coders, the lower the annotator bias B, and hence
the lower the difference between ? and ?, because the variance of P?(k|c) does not increase in
proportion to the number of coders.
In other words, provided enough coders are used, it should not matter whether a
single-distribution or individual-distribution coefficient is used. This is not to imply that
multiple coders increase reliability: The variance of the individual coders? distributions
can be just as large with many coders as with few coders, but its effect on the value
of ? decreases as the number of coders grows, and becomes more similar to random
noise.
The same holds for weighted measures too; see the extended version of this article
for definitions and proof. In an annotation study with 18 subjects, we compared ? with
a variant which uses individual coder distributions to calculate expected agreement,
and found that the values never differed beyond the third decimal point (Poesio and
Artstein 2005).
We conclude with a summary of our views concerning the difference between ?-
style and ?-style coefficients. First of all, keep in mind that empirically the difference
is small, and gets smaller as the number of annotators increases. Then instead of
reporting two coefficients, as suggested by Di Eugenio and Glass (2004), the appropriate
coefficient should be chosen based on the task (not on the observed differences between
coder marginals). When the coefficient is used to assess reliability, a single-distribution
coefficient like ? or ? should be used; this is indeed already the practice in CL, because
Siegel and Castellan?s K is identical with (multi-)?. It is also good practice to test
6 Craggs and McGee Wood (2005) also suggest increasing the number of coders in order to overcome
individual annotator bias, but do not provide a mathematical justification.
572
Artstein and Poesio Inter-Coder Agreement for CL
reliability withmore than two coders, in order to reduce the likelihood of coders sharing
a deviant reading of the annotation guidelines.
3.2 Prevalence
We touched upon the matter of skewed data in Section 2.3 when we motivated the need
for chance correction: If a disproportionate amount of the data falls under one category,
then the expected agreement is very high, so in order to demonstrate high reliability
an even higher observed agreement is needed. This leads to the so-called paradox that
chance-corrected agreement may be low even though Ao is high (Cicchetti and Feinstein
1990; Feinstein and Cicchetti 1990; Di Eugenio and Glass 2004). Moreover, when the
data are highly skewed in favor of one category, the high agreement also corresponds
to high accuracy: If, say, 95% of the data fall under one category label, then random
coding would cause two coders to jointly assign this category label to 90.25% of the
items, and on average 95% of these labels would be correct, for an overall accuracy of at
least 85.7%. This leads to the surprising result that when data are highly skewed, coders
may agree on a high proportion of items while producing annotations that are indeed
correct to a high degree, yet the reliability coefficients remain low. (For an illustration,
see the discussion of agreement results on coding discourse segments in Section 4.3.1.)
This surprising result is, however, justified. Reliability implies the ability to dis-
tinguish between categories, but when one category is very common, high accuracy
and high agreement can also result from indiscriminate coding. The test for reliabil-
ity in such cases is the ability to agree on the rare categories (regardless of whether
these are the categories of interest). Indeed, chance-corrected coefficients are sensitive
to agreement on rare categories. This is easiest to see with a simple example of two
coders and two categories, one common and the other one rare; to further simplify the
calculation we also assume that the coder marginals are identical, so that ? and ? yield
the same values. We can thus represent the judgments in a contingency table with just
two parameters:  is half the proportion of items on which there is disagreement, and
? is the proportion of agreement on the Rare category. Both of these proportions are
assumed to be small, so the bulk of the items (a proportion of 1? (? + 2)) are labeled
with the Common category by both coders (Table 7). From this table we can calculate
Ao = 1? 2 and Ae = 1? 2(? + ) + 2(? + )2, as well as ? and ?.
?, ? =
1? 2? (1? 2(? + ) + 2(? + )2)
1? (1? 2(? + ) + 2(? + )2)
=
?
? + 
?

1? (? + )
When  and ? are both small, the fraction after the minus sign is small as well, so ? and ?
are approximately ?/(? + ): the value we get if we take all the items marked by one
Table 7
A simple example of agreement on dialogue act tagging.
CODER A
COMMON RARE TOTAL
COMMON 1? (? + 2)  1? (? + )
CODER B RARE  ? ? + 
TOTAL 1? (? + ) ? +  1
573
Computational Linguistics Volume 34, Number 4
particular coder asRare, and calculate what proportion of those itemswere labeledRare
by the other coder. This is a measure of the coders? ability to agree on the rare category.
4. Using Agreement Measures for CL Annotation Tasks
In this section we review the use of intercoder agreement measures in CL since
Carletta?s original paper in light of the discussion in the previous sections. We begin
with a summary of Krippendorff?s recommendations about measuring reliability
(Krippendorff 2004a, Chapter 11), then discuss how coefficients of agreement have
been used in CL to measure the reliability of annotation schemes, focusing in particular
on the types of annotation where there has been some debate concerning the most
appropriate measures of agreement.
4.1 Methodology and Interpretation of the Results: General Issues
Krippendorff (2004a, Chapter 11) notes with regret the fact that reliability is discussed in
only around 69% of studies in content analysis. In CL as well, not all annotation projects
include a formal test of intercoder agreement. Some of the best known annotation
efforts, such as the creation of the Penn Treebank (Marcus, Marcinkiewicz, and Santorini
1993) and the British National Corpus (Leech, Garside, and Bryant 1994), do not report
reliability results as they predate the Carletta paper; but even among the more recent
efforts, many only report percentage agreement, as for the creation of the PropBank
(Palmer, Dang, and Fellbaum 2007) or the ongoing OntoNotes annotation (Hovy et al
2006). Even more importantly, very few studies apply a methodology as rigorous as
that envisaged by Krippendorff and other content analysts. We therefore begin this
discussion of CL practice with a summary of the main recommendations found in
Chapter 11 of Krippendorff (2004a), even though, as we will see, we think that some
of these recommendations may not be appropriate for CL.
4.1.1 Generating Data to Measure Reproducibility. Krippendorff?s recommendations were
developed for the field of content analysis, where coding is used to draw conclusions
from the texts. A coded corpus is thus akin to the result of a scientific experiment, and
it can only be considered valid if it is reproducible?that is, if the same coded results
can be replicated in an independent coding exercise. Krippendorff therefore argues that
any study using observed agreement as a measure of reproducibility must satisfy the
following requirements:
? It must employ an exhaustively formulated, clear, and usable coding
scheme together with step-by-step instructions on how to use it.
? It must use clearly specified criteria concerning the choice of coders
(so that others may use such criteria to reproduce the data).
? It must ensure that the coders that generate the data used to measure
reproducibility work independently of each other.
Some practices that are common in CL do not satisfy these requirements. The first
requirement is violated by the practice of expanding the written coding instructions
and including new rules as the data are generated. The second requirement is often
574
Artstein and Poesio Inter-Coder Agreement for CL
violated by using experts as coders, particularly long-term collaborators, as such coders
may agree not because they are carefully following written instructions, but because
they know the purpose of the research very well?which makes it virtually impossible
for others to reproduce the results on the basis of the same coding scheme (the prob-
lems arising when using experts were already discussed at length in Carletta [1996]).
Practices which violate the third requirement (independence) include asking coders to
discuss their judgments with each other and reach their decisions by majority vote, or
to consult with each other when problems not foreseen in the coding instructions arise.
Any of these practices make the resulting data unusable for measuring reproducibility.
Krippendorff?s own summary of his recommendations is that to obtain usable
data for measuring reproducibility a researcher must use data generated by three or
more coders, chosen according to some clearly specified criteria, and working indepen-
dently according to a written coding scheme and coding instructions fixed in advance.
Krippendorff also discusses the criteria to be used in the selection of the sample, from
the minimum number of units (obtained using a formula from Bloch and Kraemer
[1989], reported in Krippendorff [2004a, page 239]), to how to make the sample rep-
resentative of the data population (each category should occur in the sample often
enough to yield at least five chance agreements), to how to ensure the reliability of the
instructions (the sample should contain examples of all the values for the categories).
These recommendations are particularly relevant in light of the comments of Craggs
and McGee Wood (2005, page 290), which discourage researchers from testing their
coding instructions on data from more than one domain. Given that the reliability of
the coding instructions depends to a great extent on how complications are dealt with,
and that every domain displays different complications, the sample should contain
sufficient examples from all domains which have to be annotated according to the
instructions.
4.1.2 Establishing Significance. In hypothesis testing, it is common to test for the sig-
nificance of a result against a null hypothesis of chance behavior; for an agreement
coefficient this would mean rejecting the possibility that a positive value of agreement
is nevertheless due to random coding. We can rely on the statement by Siegel and
Castellan (1988, Section 9.8.2) that when sample sizes are large, the sampling distribu-
tion of K (Fleiss?s multi-?) is approximately normal and centered around zero?this
allows testing the obtained value of K against the null hypothesis of chance agreement
by using the z statistic. It is also easy to test Krippendorff?s ? with the interval distance
metric against the null hypothesis of chance agreement, because the hypothesis ? = 0 is
identical to the hypothesis F = 1 in an analysis of variance.
However, a null hypothesis of chance agreement is not very interesting, and demon-
strating that agreement is significantly better than chance is not enough to establish
reliability. This has already been pointed out by Cohen (1960, page 44): ?to knowmerely
that ? is beyond chance is trivial since one usually expects much more than this in the
way of reliability in psychological measurement.? The same point has been repeated
and stressed in many subsequent works (e.g., Posner et al 1990; Di Eugenio 2000;
Krippendorff 2004a): The reason for measuring reliability is not to test whether coders
perform better than chance, but to ensure that the coders do not deviate too much from
perfect agreement (Krippendorff 2004a, page 237).
The relevant notion of significance for agreement coefficients is therefore a confi-
dence interval. Cohen (1960, pages 43?44) implies that when sample sizes are large,
the sampling distribution of ? is approximately normal for any true population value
of ?, and therefore confidence intervals for the observed value of ? can be determined
575
Computational Linguistics Volume 34, Number 4
using the usual multiples of the standard error. Donner and Eliasziw (1987) propose
a more general form of significance test for arbitrary levels of agreement. In contrast,
Krippendorff (2004a, Section 11.4.2) states that the distribution of ? is unknown, so
confidence intervals must be obtained by bootstrapping; a software package for doing
this is described in Hayes and Krippendorff (2007).
4.1.3 Interpreting the Value of Kappa-Like Coefficients. Even after testing significance and
establishing confidence intervals for agreement coefficients, we are still faced with the
problem of interpreting the meaning of the resulting values. Suppose, for example, we
establish that for a particular task, K = 0.78? 0.05. Is this good or bad? Unfortunately,
deciding what counts as an adequate level of agreement for a specific purpose is still
little more than a black art: As we will see, different levels of agreement may be
appropriate for resource building and for more linguistic purposes.
The problem is not unlike that of interpreting the values of correlation coefficients,
and in the area of medical diagnosis, the best known conventions concerning the value
of kappa-like coefficients, those proposed by Landis and Koch (1977) and reported in
Figure 1, are indeed similar to those used for correlation coefficients, where values
above 0.4 are also generally considered adequate (Marion 2004). Many medical re-
searchers feel that these conventions are appropriate, and in language studies, a similar
interpretation of the values has been proposed by Rietveld and van Hout (1993). In
CL, however, most researchers follow the more stringent conventions from content
analysis proposed by Krippendorff (1980, page 147), as reported by Carletta (1996,
page 252): ?content analysis researchers generally think of K > .8 as good reliability,
with .67 < K < .8 allowing tentative conclusions to be drawn? (Krippendorff was dis-
cussing values of ? rather than K, but the coefficients are nearly equivalent for cate-
gorical labels). As a result, ever since Carletta?s influential paper, CL researchers have
attempted to achieve a value of K (more seldom, of ?) above the 0.8 threshold, or, failing
that, the 0.67 level allowing for ?tentative conclusions.? However, the description of
the 0.67 boundary in Krippendorff (1980) was actually ?highly tentative and cautious,?
and in later work Krippendorff clearly considers 0.8 the absolute minimum value of
? to accept for any serious purpose: ?Even a cutoff point of ? = .800 . . . is a pretty
low standard? (Krippendorff 2004a, page 242). Recent content analysis practice seems
to have settled for even more stringent requirements: A recent textbook, Neuendorf
(2002, page 3), analyzing several proposals concerning ?acceptable? reliability, con-
cludes that ?reliability coefficients of .90 or greater would be acceptable to all, .80
or greater would be acceptable in most situations, and below that, there exists great
disagreement.?
This is clearly a fundamental issue. Ideally we would want to establish thresholds
which are appropriate for the field of CL, but as we will see in the rest of this section, a
decade of practical experience hasn?t helped in settling the matter. In fact, weighted
coefficients, while arguably more appropriate for many annotation tasks, make the
issue of deciding when the value of a coefficient indicates sufficient agreement even
K = 0.0 0.2 0.4 0.6 0.8 1.0
Poor Slight Fair Moderate Substantial Perfect
Figure 1
Kappa values and strength of agreement according to Landis and Koch (1977).
576
Artstein and Poesio Inter-Coder Agreement for CL
more complicated because of the problem of determining appropriate weights (see
Section 4.4). We will return to the issue of interpreting the value of the coefficients at
the end of this article.
4.1.4 Agreement and Machine Learning. In a recent article, Reidsma and Carletta (2008)
point out that the goals of annotation in CL differ from those of content analysis, where
agreement coefficients originate. A common use of an annotated corpus in CL is not
to confirm or reject a hypothesis, but to generalize the patterns using machine-learning
algorithms. Through a series of simulations, Reidsma and Carletta demonstrate that
agreement coefficients are poor predictors of machine-learning success: Even highly
reproducible annotations are difficult to generalize when the disagreements contain pat-
terns that can be learned, whereas highly noisy and unreliable data can be generalized
successfully when the disagreements do not contain learnable patterns. These results
show that agreement coefficients should not be used as indicators of the suitability of
annotated data for machine learning.
However, the purpose of reliability studies is not to find out whether annotations
can be generalized, but whether they capture some kind of observable reality. Even if
the pattern of disagreement allows generalization, we need evidence that this general-
ization would be meaningful. The decision whether a set of annotation guidelines are
appropriate or meaningful is ultimately a qualitative one, but a baseline requirement is
an acceptable level of agreement among the annotators, who serve as the instruments
of measurement. Reliability studies test the soundness of an annotation scheme and
guidelines, which is not to be equated with the machine-learnability of data produced
by such guidelines.
4.2 Labeling Units with a Common and Predefined Set of Categories: The Case
of Dialogue Act Tagging
The simplest and most common coding in CL involves labeling segments of text with
a limited number of linguistic categories: Examples include part-of-speech tagging,
dialogue act tagging, and named entity tagging. The practices used to test reliability
for this type of annotation tend to be based on the assumption that the categories used
in the annotation are mutually exclusive and equally distinct from one another; this
assumption seems to have worked out well in practice, but questions about it have been
raised even for the annotation of parts of speech (Babarczy, Carroll, and Sampson 2006),
let alne for discourse coding tasks such as dialogue act coding. We concentrate here on
this latter type of coding, but a discussion of issues raised for POS, named entity, and
prosodic coding can be found in the extended version of the article.
Dialogue act tagging is a type of linguistic annotation with which by now the CL
community has had extensive experience: Several dialogue-act-annotated spoken lan-
guage corpora now exist, such as MapTask (Carletta et al 1997), Switchboard (Stolcke
et al 2000), Verbmobil (Jekat et al 1995), and Communicator (e.g., Doran et al 2001),
among others. Historically, dialogue act annotation was also one of the types of annota-
tion that motivated the introduction in CL of chance-corrected coefficients of agreement
(Carletta et al 1997) and, as we will see, it has been the type of annotation that has
generated the most discussion concerning annotation methodology and measuring
agreement.
A number of coding schemes for dialogue acts have achieved values of K over
0.8 and have therefore been assumed to be reliable: For example, K = 0.83 for the
577
Computational Linguistics Volume 34, Number 4
13-tagMapTask coding scheme (Carletta et al 1997), K = 0.8 for the 42-tag Switchboard-
DAMSL scheme (Stolcke et al 2000), K = 0.90 for the smaller 20-tag subset of the CSTAR
scheme used by Doran et al (2001). All of these tests were based on the same two
assumptions: that every unit (utterance) is assigned to exactly one category (dialogue
act), and that these categories are distinct. Therefore, again, unweighted measures, and
in particular K, tend to be used for measuring inter-coder agreement.
However, these assumptions have been challenged based on the observation that
utterances tend to have more than one function at the dialogue act level (Traum and
Hinkelman 1992; Allen and Core 1997; Bunt 2000); for a useful survey, see Popescu-Belis
(2005). An assertion performed in answer to a question, for instance, typically performs
at least two functions at different levels: asserting some information?the dialogue act
that we called Statement in Section 2.3, operating at what Traum and Hinkelman called
the ?core speech act? level?and confirming that the question has been understood, a di-
alogue act operating at the ?grounding? level and usually known as Acknowledgment
(Ack). In older dialogue act tagsets, acknowledgments and statements were treated as
alternative labels at the same ?level?, forcing coders to choose one or the other when an
utterance performed a dual function, according to a well-specified set of instructions. By
contrast, in the annotation schemes inspired from these newer theories such as DAMSL
(Allen and Core 1997), coders are allowed to assign tags along distinct ?dimensions? or
?levels?.
Two annotation experiments testing this solution to the ?multi-tag? problem with
the DAMSL scheme were reported in Core and Allen (1997) and Di Eugenio et al
(1998). In both studies, coders were allowed to mark each communicative function
independently: That is, they were allowed to choose for each utterance one of the
Statement tags (or possibly none), one of the Influencing-Addressee-Future-Action
tags, and so forth?and agreement was evaluated separately for each dimension using
(unweighted) K. Core and Allen found values of K ranging from 0.76 for answer
to 0.42 for agreement to 0.15 for Committing-Speaker-Future-Action. Using differ-
ent coding instructions and on a different corpus, Di Eugenio et al observed higher
agreement, ranging from K = 0.93 (for other-forward-function) to 0.54 (for the tag
agreement).
These relatively low levels of agreement led many researchers to return to ?flat?
tagsets for dialogue acts, incorporating however in their schemes some of the in-
sights motivating the work on schemes such as DAMSL. The best known example
of this type of approach is the development of the SWITCHBOARD-DAMSL tagset
by Jurafsky, Shriberg, and Biasca (1997), which incorporates many ideas from the
?multi-dimensional? theories of dialogue acts, but does not allow marking an utterance
as both an acknowledgment and a statement; a choice has to be made. This tagset
results in overall agreement of K = 0.80. Interestingly, subsequent developments of
SWITCHBOARD-DAMSL backtracked on some of these decisions. For instance, the
ICSI-MRDA tagset developed for the annotation of the ICSI Meeting Recorder corpus
reintroduces some of the DAMSL ideas, in that annotators are allowed to assign multi-
ple SWITCHBOARD-DAMSL labels to utterances (Shriberg et al 2004). Shriberg et al
achieved a comparable reliability to that obtained with SWITCHBOARD-DAMSL, but
only when using a tagset of just five ?class-maps?.
Shriberg et al (2004) also introduced a hierarchical organization of tags to improve
reliability. The dimensions of the DAMSL scheme can be viewed as ?superclasses? of
dialogue acts which share some aspect of their meaning. For instance, the dimension
of Influencing-Addressee-Future-Action (IAFA) includes the two dialogue acts
Open-option (used to mark suggestions) and Directive, both of which bring into
578
Artstein and Poesio Inter-Coder Agreement for CL
consideration a future action to be performed by the addressee. At least in principle,
an organization of this type opens up the possibility for coders to mark an utterance
with the superclass (IAFA) in case they do not feel confident that the utterance satisfies
the additional requirements for Open-option or Directive. This, in turn, would do
away with the need to make a choice between these two options. This possibility
wasn?t pursued in the studies using the original DAMSL that we are aware of (Core
and Allen 1997; Di Eugenio 2000; Stent 2001), but was tested by Shriberg et al (2004)
and subsequent work, in particular Geertzen and Bunt (2006), who were specifically
interested in the idea of using hierarchical schemes to measure partial agreement, and
in addition experimented with weighted coefficients of agreement for their hierarchical
tagging scheme, specifically ?w.
Geertzen and Bunt tested intercoder agreement with Bunt?s DIT++ (Bunt 2005),
a scheme with 11 dimensions that builds on ideas from DAMSL and from Dynamic
Interpretation Theory (Bunt 2000). In DIT++, tags can be hierarchically related: For
example, the class information-seeking is viewed as consisting of two classes, yes-
no question (ynq) and wh-question (whq). The hierarchy is explicitly introduced in order
to allow coders to leave some aspects of the coding undecided. For example, check is
treated as a subclass of ynq in which, in addition, the speaker has a weak belief that the
proposition that forms the belief is true. A coder who is not certain about the dialogue
act performed using an utterance may simply choose to tag it as ynq.
The distance metric d proposed by Geertzen and Bunt is based on the crite-
rion that two communicative functions are related (d(c1, c2) < 1) if they stand in an
ancestor?offspring relation within a hierarchy. Furthermore, they argue, the magnitude
of d(c1, c2) should be proportional to the distance between the functions in the hierar-
chy. A level-dependent correction factor is also proposed so as to leave open the option
tomake disagreements at higher levels of the hierarchymatter more than disagreements
at the deeper level (for example, the distance between information-seeking and ynq
might be considered greater than the distance between check and positive-check).
The results of an agreement test with two annotators run by Geertzen and Bunt
show that taking into account partial agreement leads to values of ?w that are higher
than the values of ? for the same categories, particularly for feedback, a class for which
Core andAllen (1997) got low agreement. Of course, even assuming that the values of ?w
and ? were directly comparable?we remark on the difficulty of interpreting the values
of weighted coefficients of agreement in Section 4.4?it remains to be seenwhether these
higher values are a better indication of the extent of agreement between coders than the
values of unweighted ?.
This discussion of coding schemes for dialogue acts introduced issues to which
we will return for other CL annotation tasks as well. There are a number of well-
established schemes for large-scale dialogue act annotation based on the assumption
of mutual exclusivity between dialogue act tags, whose reliability is also well known; if
one of these schemes is appropriate for modeling the communicative intentions found
in a task, we recommend to our readers to use it. They should also realize, however,
that the mutual exclusivity assumption is somewhat dubious. If a multi-dimensional or
hierarchical tagset is used, readers should also be aware that weighted coefficients do
capture partial agreement, and need not automatically result in lower reliability or in
an explosion in the number of labels. However, a hierarchical scheme may not reflect
genuine annotation difficulties: For example, in the case of DIT++, one might argue that
it is more difficult to confuse yes-no questions with wh-questions than with statements.
We will also see in a moment that interpreting the results with weighted coefficients is
difficult. We will return to both of these problems in what follows.
579
Computational Linguistics Volume 34, Number 4
4.3 Marking Boundaries and Unitizing
Before labeling can take place, the units of annotation, or markables, need to be
identified?a process Krippendorff (1995, 2004a) calls unitizing. The practice in CL for
the forms of annotation discussed in the previous section is to assume that the units are
linguistic constituents which can be easily identified, such as words, utterances, or noun
phrases, and therefore there is no need to check the reliability of this process. We are
aware of few exceptions to this assumption, such as Carletta et al (1997) on unitization
for move coding and our own work on the GNOME corpus (Poesio 2004b). In cases
such as text segmentation, however, the identification of units is as important as their
labeling, if not more important, and therefore checking agreement on unit identification
is essential. In this section we discuss current CL practice with reliability testing of these
types of annotation, before briefly summarizing Krippendorff?s proposals concerning
measuring reliability for unitizing.
4.3.1 Segmentation and Topic Marking. Discourse segments are portions of text that con-
stitute a unit either because they are about the same ?topic? (Hearst 1997; Reynar
1998) or because they have to do with achieving the same intention (Grosz and Sidner
1986) or performing the same ?dialogue game? (Carletta et al 1997).7 The analysis
of discourse structure?and especially the identification of discourse segments?is the
type of annotation that, more than any other, led CL researchers to look for ways of
measuring reliability and agreement, as it made them aware of the extent of disagree-
ment on even quite simple judgments (Kowtko, Isard, and Doherty 1992; Passonneau
and Litman 1993; Carletta et al 1997; Hearst 1997). Subsequent research identified a
number of issues with discourse structure annotation, above all the fact that segmen-
tation, though problematic, is still much easier than marking more complex aspects of
discourse structure, such as identifying the most important segments or the ?rhetorical?
relations between segments of different granularity. As a result, many efforts to annotate
discourse structure concentrate only on segmentation.
The agreement results for segment coding tend to be on the lower end of the
scale proposed by Krippendorff and recommended by Carletta. Hearst (1997), for
instance, found K = 0.647 for the boundary/not boundary distinction; Reynar (1998),
measuring agreement between his own annotation and the TREC segmentation of
broadcast news, reports K = 0.764 for the same task; Ries (2002) reports even lower
agreement of K = 0.36. Teufel, Carletta, and Moens (1999), who studied agreement on
the identification of argumentative zones, found high reliability (K = 0.81) for their
three main zones (own, other, background), although lower for the whole scheme
(K = 0.71). For intention-based segmentation, Passonneau and Litman (1993) in the
pre-K days reported an overall percentage agreement with majority opinion of 89%, but
the agreement on boundaries was only 70%. For conversational games segmentation,
Carletta et al (1997) reported ?promising but not entirely reassuring agreement on
where games began (70%),? whereas the agreement on transaction boundaries was
K = 0.59. Exceptions are two segmentation efforts carried out as part of annotations
of rhetorical structure. Moser, Moore, and Glendening (1996) achieved an agreement
7 The notion of ?topic? is notoriously difficult to define and many competing theoretical proposals exist
(Reinhart 1981; Vallduv?? 1993). As it is often the case with annotation, fairly simple definitions tend to
be used in discourse annotation work: For example, in TDT topic is defined for annotation purposes
as ?an event or activity, along with all directly related events and activities? (TDT-2 Annotation Guide,
http://projects.ldc.upenn.edu/TDT2/Guide/label-instr.html).
580
Artstein and Poesio Inter-Coder Agreement for CL
of K = 0.9 for the highest level of segmentation of their RDA annotation (Poesio,
Patel, and Di Eugenio 2006). Carlson, Marcu, and Okurowski (2003) reported very
high agreement over the identification of the boundaries of discourse units, the build-
ing blocks of their annotation of rhetorical structure. (Agreement was measured sev-
eral times; initially, they obtained K = 0.87, and in the final analysis K = 0.97.) This,
however, was achieved by employing experienced annotators, and with considerable
training.
One important reason why most agreement results on segmentation are on the
lower end of the reliability scale is the fact, known to researchers in discourse analysis
from as early as Levin and Moore (1978), that although analysts generally agree on the
?bulk? of segments, they tend to disagree on their exact boundaries. This phenomenon
was also observed in more recent studies: See for example the discussion in Passonneau
and Litman (1997), the comparison of the annotations produced by seven coders of
the same text in Figure 5 of Hearst (1997, page 55), or the discussion by Carlson,
Marcu, and Okurowski (2003), who point out that the boundaries between elementary
discourse units tend to be ?very blurry.? See also Pevzner and Hearst (2002) for similar
comments made in the context of topic segmentation algorithms, and Klavans, Popper,
and Passonneau (2003) for selecting definition phrases.
This ?blurriness? of boundaries, combined with the prevalence effects discussed
in Section 3.2, also explains the fact that topic annotation efforts which were only
concerned with roughly dividing a text into segments (Passonneau and Litman 1993;
Carletta et al 1997; Hearst 1997; Reynar 1998; Ries 2002) generally report lower agree-
ment than the studies whose goal is to identify smaller discourse units. When disagree-
ment is mostly concentrated in one class (?boundary? in this case), if the total number of
units to annotate remains the same, then expected agreement on this class is lower when
a greater proportion of the units to annotate belongs to this class. When in addition this
class is much less numerous than the other classes, overall agreement tends to depend
mostly on agreement on this class.
For instance, suppose we are testing the reliability of two different segmentation
schemes?into broad ?discourse segments? and into finer ?discourse units??on a text
of 50 utterances, and that we obtain the results in Table 8. Case 1 would be a situation
in which Coder A and Coder B agree that the text consists of two segments, obviously
agree on its initial and final boundaries, but disagree by one position on the intermediate
boundary?say, one of them places it at utterance 25, the other at utterance 26. Never-
theless, because expected agreement is so high?the coders agree on the classification
of 98% of the utterances?the value of K is fairly low. In case 2, the coders disagree on
three times as many utterances, but K is higher than in the first case because expected
agreement is substantially lower (Ae = 0.53).
The fact that coders mostly agree on the ?bulk? of discourse segments, but tend
to disagree on their boundaries, also makes it likely that an all-or-nothing coefficient
like K calculated on individual boundaries would underestimate the degree of agree-
ment, suggesting low agreement even among coders whose segmentations are mostly
similar. A weighted coefficient of agreement like ? might produce values more in
keeping with intuition, but we are not aware of any attempts at measuring agreement
on segmentation using weighted coefficients. We see two main options. We suspect that
the methods proposed by Krippendorff (1995) for measuring agreement on unitizing
(see Section 4.3.2, subsequently) may be appropriate for the purpose of measuring
agreement on discourse segmentation. A second optionwould be tomeasure agreement
not on individual boundaries but on windows spanning several units, as done in the
methods proposed to evaluate the performance of topic detection algorithms such as
581
Computational Linguistics Volume 34, Number 4
Table 8
Fewer boundaries, higher expected agreement.
Case 1: Broad segments
Ao = 0.96,Ae = 0.89, K = 0.65
CODER A
BOUNDARY NO BOUNDARY TOTAL
BOUNDARY 2 1 3
CODER B NO BOUNDARY 1 46 47
TOTAL 3 47 50
Case 2: Fine discourse units
Ao = 0.88,Ae = 0.53, K = 0.75
CODER A
BOUNDARY NO BOUNDARY TOTAL
BOUNDARY 16 3 19
CODER B NO BOUNDARY 3 28 31
TOTAL 19 31 50
Pk (Beeferman, Berger, and Lafferty 1999) or WINDOWDIFF (Pevzner and Hearst 2002)
(which are, however, raw agreement scores not corrected for chance).
4.3.2 Unitizing (Or, Agreement on Markable Identification). It is often assumed in CL anno-
tation practice that the units of analysis are ?natural? linguistic objects, and therefore
there is no need to check agreement on their identification. As a result, agreement is
usually measured on the labeling of units rather than on the process of identifying them
(unitizing, Krippendorff 1995). We have just seen, however, two coding tasks for which
the reliability of unit identification is a crucial part of the overall reliability, and the
problem of markable identification is more pervasive than is generally acknowledged.
For example, when the units to be labeled are syntactic constituents, it is common
practice to use a parser or chunker to identify themarkables and then to allow the coders
to correct the parser?s output. In such cases one would want to know how reliable the
coders? corrections are. We thus need a general method of testing relibility on markable
identification.
The one proposal for measuring agreement onmarkable identification we are aware
of is the ?U coefficient, a non-trivial variant of ? proposed by Krippendorff (1995). A
full presentation of the proposal would require too much space, so we will just present
the core idea. Unitizing is conceived of as consisting of two separate steps: identifying
boundaries between units, and selecting the units of interest. If a unit identified by one
coder overlaps a unit identified by the other coder, the amount of disagreement is the
square of the lengths of the non-overlapping segments (see Figure 2); if a unit identified
by one coder does not overlap any unit of interest identified by the other coder, the
amount of disagreement is the square of the length of the whole unit. This distance
metric is used in calculating observed and expected disagreement, and ?U itself. We
refer the reader to Krippendorff (1995) for details.
Krippendorff?s ?U is not applicable to all CL tasks. For example, it assumes that
units may not overlap in a single coder?s output, yet in practice there are many
582
Artstein and Poesio Inter-Coder Agreement for CL
Coder A
Coder B
s?? ? s? ? s+? ?
Figure 2
The difference between overlapping units is d(A, B) = s2? + s
2
+ (adapted from Krippendorff
1995, Figure 4, page 61).
annotation schemes which require coders to label nested syntactic constituents. For
continuous segmentation tasks, ?U may be inappropriate because when a segment
identified by one annotator overlaps with two segments identified by another annotator,
the distance is smallest when the one segment is centered over the two rather than
aligned with one of them. Nevertheless, we feel that when the non-overlap assumption
holds, and the units do not cover the text exhaustively, testing the reliabilty of unit
identification may prove beneficial. To our knowledge, this has never been tested in CL.
4.4 Anaphora
The annotation tasks discussed so far involve assigning a specific label to each category,
which allows the various agreement measures to be applied in a straightforward way.
Anaphoric annotation differs from the previous tasks because annotators do not assign
labels, but rather create links between anaphors and their antecedents. It is therefore
not clear what the ?labels? should be for the purpose of calculating agreement. One
possibility would be to consider the intended referent (real-world object) as the label,
as in named entity tagging, but it wouldn?t make sense to predefine a set of ?labels?
applicable to all texts, because different objects are mentioned in different texts. An
alternative is to use the marked antecedents as ?labels?. However, we do not want to
count as a disagreement every time two coders agree on the discourse entity realized
by a particular noun phrase but just happen to mark different words as antecedents.
Consider the reference of the underlined pronoun it in the following dialogue excerpt
(TRAINS 1991 [Gross, Allen, and Traum 1993], dialogue d91-3.2).8
1.1 M: ....
1.4 first thing I?d like you to do
1.5 is send engine E2 off with a boxcar to Corning to
pick up oranges
1.6 as soon as possible
2.1 S: okay
3.1 M: and while it?s there it should pick up the tanker
Some of the coders in a study we carried out (Poesio and Artstein 2005) indicated the
noun phrase engine E2 as antecedent for the second it in utterance 3.1, whereas others
indicated the immediately preceding pronoun, which they had previously marked as
having engine E2 as antecedent. Clearly, we do not want to consider these coders to be in
disagreement. A solution to this dilemma has been proposed by Passonneau (2004): Use
the emerging coreference sets as the ?labels? for the purpose of calculating agreement.
This requires using weighted measures for calculating agreement on such sets, and
8 ftp://ftp.cs.rochester.edu/pub/papers/ai/92.tn1.trains 91 dialogues.txt.
583
Computational Linguistics Volume 34, Number 4
consequently it raises serious questions about weighted measures?in particular, about
the interpretability of the results, as we will see shortly.
4.4.1 Passonneau?s Proposal. Passonneau (2004) recommends measuring agreement on
anaphoric annotation by using sets of mentions of discourse entities as labels, that is,
the emerging anaphoric/coreference chains. This proposal is in line with the meth-
ods developed to evaluate anaphora resolution systems (Vilain et al 1995). But using
anaphoric chains as labels would not make unweighted measures such as K a good
measure for agreement. Practical experience suggests that, except when a text is very
short, few annotators will catch all mentions of a discourse entity: Most will forget to
mark a few, with the result that the chains (that is, category labels) differ from coder
to coder and agreement as measured with K is always very low. What is needed is
a coefficient that also allows for partial disagreement between judgments, when two
annotators agree on part of the coreference chain but not on all of it.
Passonneau (2004) suggests solving the problem by using ? with a distance metric
that allows for partial agreement among anaphoric chains. Passonneau proposes a dis-
tance metric based on the following rationale: Two sets are minimally distant when they
are identical andmaximally distant when they are disjoint; between these extremes, sets
that stand in a subset relation are closer (less distant) than ones that merely intersect.
This leads to the following distance metric between two sets A and B.
dP =
?
?
?
?
?
?
?
0 if A = B
1/3 if A ? B or B ? A
2/3 if A ? B = ?, but A ? B and B ? A
1 if A ? B = ?
Alternative distance metrics take the size of the anaphoric chain into account, based
on measures used to compare sets in Information Retrieval, such as the coefficient of
community of Jaccard (1912) and the coincidence index of Dice (1945) (Manning and
Schu?tze 1999).
Jaccard: dJ = 1?
|A ? B|
|A ? B|
Dice: dD = 1?
2 |A ? B|
|A|+ |B|
In later work, Passonneau (2006) offers a refined distance metric which she called MASI
(Measuring Agreement on Set-valued Items), obtained by multiplying Passonneau?s
original metric dP by the metric derived from Jaccard dJ .
dM = dP ? dJ
4.4.2 Experience with ? for Anaphoric Annotation. In the experiment mentioned previously
(Poesio and Artstein 2005) we used 18 coders to test ? and K under a variety of condi-
tions.We found that even though our coders by and large agreed on the interpretation of
anaphoric expressions, virtually no coder ever identified all the mentions of a discourse
entity. As a result, even though the values of ? and K obtained by using the ID of
the antecedent as label were pretty similar, the values obtained when using anaphoric
chains as labels were drastically different. The value of ? increased, because examples
where coders linked a markable to different antecedents in the same chain were no
584
Artstein and Poesio Inter-Coder Agreement for CL
longer considered as disagreements. However, the value of K was drastically reduced,
because hardly any coder identified all the mentions of discourse entities (Figure 3).
The study also looked at the matter of individual annotator bias, and as mentioned
in Section 3.1, we did not find differences between ? and a ?-style version of ? beyond
the third decimal point. This similarity is what one would expect, given the result about
annotator bias from Section 3.1 and given that in this experiment we used 18 annotators.
These very small differences should be contrasted with the differences resulting from
the choice of distance metrics, where values for the full-chain condition ranged from
? = 0.642 using Jaccard as distance metric, to ? = 0.654 using Passonneau?s metric, to
the value for Dice reported in Figure 3, ? = 0.691. These differences raise an important
issue concerning the application of ?-like measures for CL tasks: Using ? makes it diffi-
cult to compare the results of different annotation experiments, in that a ?poor? value or
a ?high? valuemight result from ?too strict? or ?too generous? distancemetrics, making
it even more important to develop a methodology to identify appropriate values for
these coefficients. This issue is further emphasized by the study reported next.
4.4.3 Discourse Deixis. A second annotation study we carried out (Artstein and Poesio
2006) shows even more clearly the possible side effects of using weighted coefficients.
This study was concerned with the annotation of the antecedents of references to
abstract objects, such as the example of the pronoun that in utterance 7.6 (TRAINS 1991,
dialogue d91-2.2).
7.3 : so we ship one
7.4 : boxcar
7.5 : of oranges to Elmira
7.6 : and that takes another 2 hours
Previous studies of discourse deixis annotation showed that these are extremely diffi-
cult judgments to make (Eckert and Strube 2000; Navarretta 2000; Byron 2002), except
perhaps for identifying the type of object (Poesio and Modjeska 2005), so we simplified
the task by only requiring our participants to identify the boundaries of the area of
text in which the antecedent was introduced. Even so, we found a great variety in
how these boundaries were marked: Exactly as in the case of discourse segmentation
discussed earlier, our participants broadly agreed on the area of text, but disagreed on
Chain K ?
None 0.628 0.656
Partial 0.563 0.677
Full 0.480 0.691
0.4
0.5
0.6
0.7
?
?
?
K
K
K
no partial full
chain chain chain
Figure 3
A comparison of the values of ? and K for anaphoric annotation (Poesio and Artstein 2005).
585
Computational Linguistics Volume 34, Number 4
its exact boundary. For instance, in this example, nine out of ten annotators marked the
antecedent of that as a text segment ending with the word Elmira, but some started with
the word so, some started with we, some with ship, and some with one.
We tested a number of ways tomeasure partial agreement on this task, and obtained
widely different results. First of all, we tested three set-based distance metrics inspired
by the Passonneau proposals that we just discussed: We considered discourse segments
to be sets of words, and computed the distance between them using Passonneau?s
metric, Jaccard, and Dice. Using these three metrics, we obtained ? values of 0.55 (with
Passonneau?s metric), 0.45 (with Jaccard), and 0.55 (with Dice). We should note that
because antecedents of different expressions rarely overlapped, the expected disagree-
ment was close to 1 (maximal), so the value of ? turned out to be very close to the com-
plement of the observed disagreement as calculated by the different distance metrics.
Next, we considered methods based on the position of words in the text. The first
method computed differences between absolute boundary positions: Each antecedent
was associated with the position of its first or last word in the dialogue, and agreement
was calculated using ? with the interval distance metric. This gave us ? values of
0.998 for the beginnings of the antecedent-evoking area and 0.999 for the ends. This is
because expected disagreement is exceptionally low: Coders tend to mark discourse an-
tecedents close to the referring expression, so the average distance between antecedents
of the same expression is smaller than the size of the dialogue by a few orders of
magnitude. The second method associated each antecedent with the position of its
first or last word relative to the beginning of the anaphoric expression. This time we found
extremely low values of ? = 0.167 for beginnings of antecedents and 0.122 for ends?
barely in the positive side. This shows that agreement among coders is not dramatically
better than what would be expected if they just marked discourse antecedents at a fixed
distance from the referring expression.
The three ranges of ? that we observed (middle, high, and low) show agreement on
the identity of discourse antecedents, their position in the dialogue, and their position
relative to referring expressions, respectively. The middle range shows variability of up
to 10 percentage points, depending on the distance metric chosen. The lesson is that
once we start using weighted measures we cannot anymore interpret the value of ?
using traditional rules of thumb such as those proposed by Krippendorff or by Landis
and Koch. This is because depending on the way we measure agreement, we can report
? values ranging from 0.122 to 0.998 for the very same experiment! New interpretation
methods have to be developed, which will be task- and distance-metric specific. We?ll
return to this issue in the conclusions.
4.5 Word Senses
Word sense tagging is one of the hardest annotation tasks. Whereas in the case of part-
of-speech and dialogue act tagging the same categories are used to classify all units, in
the case of word sense tagging different categories must be used for each word, which
makes writing a single codingmanual specifying examples for all categories impossible:
The only option is to rely on a dictionary. Unfortunately, different dictionaries make
different distinctions, and often coders can?t make the fine-grained distinctions that
trained lexicographers can make. The problem is particularly serious for verbs, which
tend to be polysemous rather than homonymous (Palmer, Dang, and Fellbaum 2007).
These difficulties, and in particular the difficulty of tagging senses with a fine-
grained repertoire of senses such as that provided by dictionaries or by WordNet
(Fellbaum 1998), have been highlighted by the three SENSEVAL initiatives. Already
586
Artstein and Poesio Inter-Coder Agreement for CL
during the first SENSEVAL, Ve?ronis (1998) carried out two studies of intercoder
agreement on word sense tagging in the so-called ROMANSEVAL task. One study was
concerned with agreement on polysemy?that is, the extent to which coders agreed
that a word was polysemous in a given context. Six naive coders were asked to make
this judgment about 600 French words (200 nouns, 200 verbs, 200 adjectives) using the
repertoire of senses in the Petit Larousse. On this task, a (pairwise) percentage agreement
of 0.68 for nouns, 0.74 for verbs, and 0.78 for adjectives was observed, corresponding
to K values of 0.36, 0.37, and 0.67, respectively. The 20 words from each category
perceived by the coders in this first experiment to be most polysemous were then used
in a second study, of intercoder agreement on the sense tagging task, which involved
six different naive coders. Interestingly, the coders in this second experiment were
allowed to assign multiple tags to words, although they did not make much use of this
possibility; so ?w was used to measure agreement. In this experiment, Ve?ronis observed
(weighted) pairwise agreement of 0.63 for verbs, 0.71 for adjectives, and 0.73 for nouns,
corresponding to ?w values of 0.41, 0.41, and 0.46, but with a wide variety of values
when measured per word?ranging from 0.007 for the adjective correct to 0.92 for the
noun de?tention. Similarly mediocre results for intercoder agreement between naive
coders were reported in the subsequent editions of SENSEVAL. Agreement studies
for SENSEVAL-2, where WordNet senses were used as tags, reported a percentage
agreement for verb senses of around 70%, whereas for SENSEVAL-3 (English Lexical
Sample Task), Mihalcea, Chklovski, and Kilgarriff (2004) report a percentage agreement
of 67.3% and average K of 0.58.
Two types of solutions have been proposed for the problem of low agreement on
sense tagging. The solution proposed by Kilgarriff (1999) is to use professional lexicog-
raphers and arbitration. The study carried out by Kilgarriff does not therefore qualify
as a true study of replicability in the sense of the terms used by Krippendorff, but it did
show that this approach makes it possible to achieve percentage agreement of around
95.5%. An alternative approach has been to address the problem of the inability of naive
coders to make fine-grained distinctions by introducing coarser-grained classification
schemes which group together dictionary senses (Bruce and Wiebe, 1998; Buitelaar
1998; Ve?ronis 1998; Palmer, Dang, and Fellbaum 2007). Hierarchical tagsets were also
developed, such as HECTOR (Atkins 1992) or, indeed, WordNet itself (where senses are
related by hyponymy links). In the case of Buitelaar and Palmer, Dang, and Fellbaum,
the ?supersenses? were identified by hand, whereas Bruce andWiebe and Ve?ronis used
clustering methods such as those from Bruce and Wiebe (1999) to collapse some of the
initial sense distinctions.9 Palmer, Dang, and Fellbaum (2007) illustrate this practice
with the example of the verb call, which has 28 fine-grained senses in WordNet 1.7:
They conflate these senses into a small number of groups using various criteria?for
example, four senses can be grouped in a group they call Group 1 on the basis of
subcategorization frame similarities (Table 9).
Palmer, Dang, and Fellbaum (2007) achieved for the English Verb Lexical Sense task
of SENSEVAL-2 a percentage agreement among coders of 82% with grouped senses, as
opposed to 71% with the original WordNet senses. Bruce and Wiebe (1998) found that
collapsing the senses of their test word (interest) on the basis of their use by coders and
merging the two classes found to be harder to distinguish resulted in an increase of
9 The methodology proposed in Bruce and Wiebe (1999) is in our view the most advanced technique to
?make sense? of the results of agreement studies available in the literature. The extended version of this
article contains a fuller introduction to these methods.
587
Computational Linguistics Volume 34, Number 4
Table 9
Group 1 of senses of call in Palmer, Dang, and Fellbaum (2007, page 149).
SENSE DESCRIPTION EXAMPLE HYPERNYM
WN1 name, call ?They nameda their son David? LABEL
WN3 call, give a quality ?She called her children lazy LABEL
and ungrateful?
WN19 call, consider ?I would not call her beautiful? SEE
WN22 address, call ?Call me mister? ADDRESS
aThe verb named appears in the original WordNet example for the verb call.
the value of K from 0.874 to 0.898. Using a related technique, Ve?ronis (1998) found that
agreement on noun word sense tagging went up from a K of around 0.45 to a K of 0.86.
We should note, however, that the post hoc merging of categories is not equivalent to
running a study with fewer categories to begin with.
Attempts were also made to develop techniques to measure partial agreement with
hierarchical tagsets. A first proposal in this direction was advanced by Melamed and
Resnik (2000), who developed a coefficient for hierarchical tagsets that could be used
in SENSEVAL for measuring agreement with tagsets such as HECTOR. Melamed and
Resnik proposed to ?normalize? the computation of observed and expected agreement
by taking each label which is not a leaf in the tag hierarchy and distributing it down
to the leaves in a uniform way, and then only computing agreement on the leaves. For
example, with a tagset like the one in Table 9, the cases in which the coders used the
label ?Group 1? would be uniformly ?distributed down? and added in equal measure
to the number of cases in which the coders assigned each of the four WordNet labels.
The method proposed in the paper has, however, problematic properties when used
to measure intercoder agreement. For example, suppose tag A dominates two sub-tags
A1 and A2, and that two coders mark a particular item as A. Intuitively, we would want
to consider this a case of perfect agreement, but this is not what the method proposed
by Melamed and Resnik yields. The annotators? marks are distributed over the two
sub-tags, each with probability 0.5, and then the agreement is computed by summing
the joint probabilities over the two subtags (Equation (4) of Melamed and Resnik 2000),
with the result that the agreement over the item turns out to be 0.52 + 0.52 = 0.5 instead
of 1. To correct this, Dan Melamed (personal communication) suggested replacing the
product in Equation (4) with a minimum operator. However, the calculation of expected
agreement (Equation (5) of Melamed and Resnik 2000) still gives the amount of agree-
ment which is expected if coders are forced to choose among leaf nodes, which makes
this method inappropriate for coding schemes that do not force coders to do this.
One way to use Melamed and Resnik?s proposal while avoiding the discrepancy
between observed and expected agreement is to treat the proposal not as a new co-
efficient, but rather as a distance metric to be plugged into a weighted coefficient
like ?. Let A and B be two nodes in a hierarchical tagset, let L be the set of all leaf
nodes in the tagset, and let P(l|T) be the probability of selecting a leaf node l given
an arbitrary node T when the probability mass of T is distributed uniformly to all the
nodes dominated by T. We can reinterpret Melamed?s modification of Equation (4) in
Melamed and Resnik (2000) as a metric measuring the distance between nodes A and B.
dM+R = 1?
?
l?L
min(P(l|A), P(l|B))
588
Artstein and Poesio Inter-Coder Agreement for CL
This metric has the desirable properties?it is 0 when tags A and B are identical,
1 when the tags do not overlap, and somewhere in between in all other cases. If we
use this metric for Krippendorff?s ? we find that observed agreement is exactly the
same as inMelamed and Resnik (2000) with the product operator replaced byminimum
(Melamed?s modification).
We can also use other distance metrics with ?. For example, we could associate
with each sense an extended sense?a set es(s) including the sense itself and its
grouped sense?and then use set-based distance metrics from Section 4.4, for ex-
ample Passonneau?s dP. To illustrate how this approach could be used to measure
(dis)agreement on word sense annotation, suppose that two coders have to annotate the
use of call in the following sentence (from theWSJ part of the Penn Treebank, section 02,
text w0209):
This gene, called ?gametocide,? is carried into the plant by a virus that
remains active for a few days.
The standard guidelines (in SENSEVAL, say) require coders to assign a WN sense to
words. Under such guidelines, if coder A classifies the use of called in the above example
as an instance of WN1, whereas coder B annotates it as an instance of WN3, we would
find total disagreement (dkakb = 1) which seems excessively harsh as the two senses are
clearly related. However, by using the broader senses proposed by Palmer, Dang, and
Fellbaum (2007) in combination with a distance metric such as the one just proposed,
it is possible to get more flexible and, we believe, more realistic assessments of the
degree of agreement in situations such as this. For instance, in case the reliability study
had already been carried out under the standard SENSEVAL guidelines, the distance
metric proposed above could be used to identify post hoc cases of partial agreement
by adding to each WN sense its hypernyms according to the groupings proposed by
Palmer, Dang, and Fellbaum. For example, A?s annotation could be turned into a new
set label {WN1,LABEL} and B?s mark into the set table {WN3,LABEL}, which would
give a distance d = 2/3, indicating a degree of overlap. The method for computing
agreement proposed here could could also be used to allow coders to choose either a
more specific label or one of Palmer, Dang, and Fellbaum?s superlabels. For example,
suppose A sticks to WN1, but B decides to mark the use above using Palmer, Dang, and
Fellbaum?s LABEL category, then we would still find a distance d = 1/3.
An alternative way of using ? for word sense annotation was developed and tested
by Passonneau, Habash, and Rambow (2006). Their approach is to allow coders to
assign multiple labels (WordNet synsets) for wordsenses, as done by Ve?ronis (1998) and
more recently by Rosenberg and Binkowski (2004) for text classification labels and by
Poesio and Artstein (2005) for anaphora. These multi-label sets can then be compared
using the MASI distance metric for ? (Passonneau 2006).
5. Conclusions
The purpose of this article has been to expose the reader to the mathematics of chance-
corrected coefficients of agreement as well as the current state of the art of using these
coefficients in CL. Our hope is that readers come to view agreement studies not as an
additional chore or hurdle for publication, but as a tool for analysis which offers new
insights into the annotation process. We conclude by summarizing what in our view are
the main recommendations emerging from ten years of experience with coefficients of
agreement. These can be grouped under three main headings: methodology, choice of
coefficients, and interpretation of coefficients.
589
Computational Linguistics Volume 34, Number 4
5.1 Methodology
Our first recommendation is that annotation efforts should perform and report rigorous
reliability testing. The last decade has already seen considerable improvement, from
the absence of any tests for the Penn Treebank (Marcus, Marcinkiewicz, and Santorini
1993) or the British National Corpus (Leech, Garside, and Bryant 1994) to the central
role played by reliability testing in the Penn Discourse Treebank (Miltsakaki et al 2004)
and OntoNotes (Hovy et al 2006). But even the latter efforts only measure and report
percent agreement. We believe that part of the reluctance to report chance-corrected
measures is the difficulty in interpreting them. However, our experience is that chance-
corrected coefficients of agreement do provide a better indication of the quality of the
resulting annotation than simple percent agreement, and moreover, the detailed calcu-
lations leading to the coefficients can be very revealing as to where the disagreements
are located and what their sources may be.
A rigorous methodology for reliability testing does not, in our opinion, exclude the
use of expert coders, and here we feel there may be a motivated difference between the
fields of content analysis and CL. There is a clear tradeoff between the complexity of
the judgments that coders are required to make and the reliability of such judgments,
and we should strive to devise annotation schemes that are not only reliable enough
to be replicated, but also sophisticated enough to be useful (cf. Krippendorff 2004a,
pages 213?214). In content analysis, conclusions are drawn directly from annotated
corpora, so the emphasis is more on replicability; whereas in CL, corpora constitute a
resource which is used by other processes, so the emphasis is more towards usefulness.
There is also a tradeoff between the sophistication of judgments and the availability of
coders who can make such judgments. Consequently, annotation by experts is often
the only practical way to get useful corpora for CL. Current practice achieves high
reliability either by using professionals (Kilgarriff 1999) or through intensive training
(Hovy et al 2006; Carlson, Marcu, and Okurowski 2003); this means that results are not
replicable across sites, and are therefore less reliable than annotation by naive coders
adhering to written instructions. We feel that inter-annotator agreement studies should
still be carried out, as they serve as an assurance that the results are replicable when
the annotators are chosen from the same population as the original annotators. An
important additional assurance should be provided in the form of an independent
evaluation of the task for which the corpus is used (cf. Passonneau 2006).
5.2 Choosing a Coefficient
One of the goals of this article is to help authors make an informed choice regarding
the coefficients they use for measuring agreement. While coefficients other than K,
specifically Cohen?s ? and Krippendorff?s ?, have appeared in the CL literature as early
as Carletta (1996) and Passonneau and Litman (1996), they hadn?t sprung into general
awareness until the publication of Di Eugenio and Glass (2004) and Passonneau (2004).
Regarding the question of annotator bias, there is an overwhelming consensus in CL
practice: K and ? are used in the vast majority of the studies we reported. We agree with
the view that K and ? are more appropriate, as they abstract away from the bias of spe-
cific coders. But we also believe that ultimately this issue of annotator bias is of little con-
sequence because the differences get smaller and smaller as the number of annotators
grows (Artstein and Poesio 2005). We believe that increasing the number of annotators
is the best strategy, because it reduces the chances of accidental personal biases.
590
Artstein and Poesio Inter-Coder Agreement for CL
However, Krippendorff?s ? is indispensable when the category labels are not
equally distinct from one another. We think there are at least two types of coding
schemes in which this is the case: (i) hierarchical tagsets and (ii) set-valued interpre-
tations such as those proposed for anaphora. At least in the second case, weighted
coefficients are almost unavoidable. We therefore recommend using ?, noting however
that the specific choice of weights will affect the overall numerical result.
5.3 Interpreting the Values
We view the lack of consensus on how to interpret the values of agreement coefficients
as a serious problem with current practice in reliability testing, and as one of the
main reasons for the reluctance of many in CL to embark on reliability studies. Unlike
significance values which report a probability (that an observed effect is due to chance),
agreement coefficients report a magnitude, and it is less clear how to interpret such
magnitudes. Our own experience is consistent with that of Krippendorff: Both in our
earlier work (Poesio and Vieira 1998; Poesio 2004a) and in the more recent efforts
(Poesio and Artstein 2005) we found that only values above 0.8 ensured an annotation
of reasonable quality (Poesio 2004a). We therefore feel that if a threshold needs to be set,
0.8 is a good value.
That said, we doubt that a single cutoff point is appropriate for all purposes.
For some CL studies, particularly on discourse, useful corpora have been obtained
while attaining reliability only at the 0.7 level. We agree therefore with Craggs and
McGee Wood (2005) that setting a specific agreement threshold should not be a pre-
requisite for publication. Instead, as recommended by Di Eugenio and Glass (2004) and
others, researchers should report in detail on the methodology that was followed in
collecting the reliability data (number of coders, whether they coded independently,
whether they relied exclusively on an annotation manual), whether agreement was sta-
tistically significant, and provide a confusion matrix or agreement table so that readers
can find out whether overall figures of agreement hide disagreements on less common
categories. For an example of good practice in this respect, see Teufel and Moens (2002).
The decision whether a corpus is good enough for publication should be based on more
than the agreement score?specifically, an important consideration is an independent
evaluation of the results that are based on the corpus.
Acknowledgments
This work was supported in part by EPSRC
grant GR/S76434/01, ARRAU. We wish to
thank four anonymous reviewers and Jean
Carletta, Mark Core, Barbara Di Eugenio,
Ruth Filik, Michael Glass, George Hripcsak,
Adam Kilgarriff, Dan Melamed, Becky
Passonneau, Phil Resnik, Tony Sanford,
Patrick Sturt, and David Traum for helpful
comments and discussion. Special thanks to
Klaus Krippendorff for an extremely detailed
review of an earlier version of this article. We
are also extremely grateful to the British
Library in London, which made accessible to
us virtually every paper we needed for this
research.
References
Allen, James and Mark Core. 1997. DAMSL:
Dialogue act markup in several layers.
Draft contribution for the Discourse
Resource Initiative, University of
Rochester. Available at
http://www.cs.rochester.edu/
research/cisd/resources/damsl/.
Artstein, Ron and Massimo Poesio. 2005.
Bias decreases in proportion to the number
of annotators. In Proceedings of FG-MoL
2005, pages 141?150, Edinburgh.
Artstein, Ron and Massimo Poesio. 2006.
Identifying reference to abstract objects
in dialogue. In brandial 2006: Proceedings
of the 10th Workshop on the Semantics and
591
Computational Linguistics Volume 34, Number 4
Pragmatics of Dialogue, pages 56?63,
Potsdam.
Atkins, Sue. 1992. Tools for computer-aided
corpus lexicography: The Hector project.
Acta Linguistica Hungarica, 41:5?71.
Babarczy, Anna, John Carroll, and Geoffrey
Sampson. 2006. Definitional, personal,
and mechanical constraints on part
of speech annotation performance.
Natural Language Engineering, 12(1):77?90.
Bartko, John J. and William T. Carpenter, Jr.
1976. On the methods and theory of
reliability. Journal of Nervous and Mental
Disease, 163(5):307?317.
Beeferman, Doug, Adam Berger, and
John Lafferty. 1999. Statistical models
for text segmentation. Machine Learning,
34(1?3):177?210.
Bennett, E. M., R. Alpert, and A. C.
Goldstein. 1954. Communications
through limited questioning. Public
Opinion Quarterly, 18(3):303?308.
Bloch, Daniel A. and Helena Chmura
Kraemer. 1989. 2 ? 2 kappa coefficients:
Measures of agreement or association.
Biometrics, 45(1):269?287.
Brennan, Robert L. and Dale J. Prediger.
1981. Coefficient kappa: Some uses,
misuses, and alternatives. Educational
and Psychological Measurement,
41(3):687?699.
Bruce, Rebecca and Janyce Wiebe. 1998.
Word-sense distinguishability and
inter-coder agreement. In Proceedings
of EMNLP, pages 53?60, Granada.
Bruce, Rebecca F. and Janyce M. Wiebe. 1999.
Recognizing subjectivity: A case study in
manual tagging. Natural Language
Engineering, 5(2):187?205.
Buitelaar, Paul. 1998. CoreLex : Systematic
Polysemy and Underspecification. Ph.D.
thesis, Brandeis University, Waltham, MA.
Bunt, Harry C. 2000. Dynamic interpretation
and dialogue theory. In Martin M. Taylor,
Franc?oise Ne?el, and Don G. Bouwhuis,
editors, The Structure of Multimodal
Dialogue II. John Benjamins, Amsterdam,
pages 139?166.
Bunt, Harry C. 2005. A framework for
dialogue act specification. In Proceedings
of the Joint ISO-ACL Workshop on the
Representation and Annotation of Semantic
Information, Tilburg. Available at:
http://let.uvt.nl/research/ti/
sigsem/wg/discussionnotes4.htm.
Byron, Donna K. 2002. Resolving
pronominal reference to abstract entities.
In Proceedings of the 40th Annual Meeting of
the ACL, pages 80?87, Philadelphia, PA.
Byrt, Ted, Janet Bishop, and John B. Carlin.
1993. Bias, prevalence and kappa. Journal
of Clinical Epidemiology, 46(5):423?429.
Carletta, Jean. 1996. Assessing agreement on
classification tasks: The kappa statistic.
Computational Linguistics, 22(2):249?254.
Carletta, Jean, Amy Isard, Stephen Isard,
Jacqueline C. Kowtko, Gwyneth
Doherty-Sneddon, and Anne H.
Anderson. 1997. The reliability of a
dialogue structure coding scheme.
Computational Linguistics, 23(1):13?32.
Carlson, Lynn, Daniel Marcu, and
Mary Ellen Okurowski. 2003. Building a
discourse-tagged corpus in the framework
of rhetorical structure theory. In Jan C. J.
van Kuppevelt and Ronnie W. Smith,
editors, Current and New Directions in
Discourse and Dialogue. Kluwer, Dordrecht,
pages 85?112.
Cicchetti, Domenic V. and Alvan R.
Feinstein. 1990. High agreement but low
kappa: II. Resolving the paradoxes. Journal
of Clinical Epidemiology, 43(6):551?558.
Cohen, Jacob. 1960. A coefficient of
agreement for nominal scales. Educational
and Psychological Measurement, 20(1):37?46.
Cohen, Jacob. 1968. Weighted kappa:
Nominal scale agreement with provision
for scaled disagreement or partial credit.
Psychological Bulletin, 70(4):213?220.
Core, Mark G. and James F. Allen. 1997.
Coding dialogs with the DAMSL
annotation scheme. In Working Notes of the
AAAI Fall Symposium on Communicative
Action in Humans and Machines, AAAI,
Cambridge, MA. Available at: http://www.
cs.umd.edu/?traum/CA/fpapers.html.
Craggs, Richard and Mary McGee Wood.
2004. A two-dimensional annotation
scheme for emotion in dialogue. In Papers
from the 2004 AAAI Spring Symposium on
Exploring Attitude and Affect in Text:
Theories and Applications, Stanford,
pages 44?49.
Craggs, Richard and Mary McGee Wood.
2005. Evaluating discourse and dialogue
coding schemes. Computational Linguistics,
31(3):289?295.
Davies, Mark and Joseph L. Fleiss. 1982.
Measuring agreement for multinomial
data. Biometrics, 38(4):1047?1051.
Di Eugenio, Barbara. 2000. On the usage of
Kappa to evaluate agreement on coding
tasks. In Proceedings of LREC, volume 1,
pages 441?444, Athens.
Di Eugenio, Barbara and Michael Glass.
2004. The kappa statistic: A second look.
Computational Linguistics, 30(1):95?101.
592
Artstein and Poesio Inter-Coder Agreement for CL
Di Eugenio, Barbara, Pamela W. Jordan,
Johanna D. Moore, and Richmond H.
Thomason. 1998. An empirical
investigation of proposals in collaborative
dialogues. In Proceedings of 36th Annual
Meeting of the ACL, pages 325?329,
Montreal.
Dice, Lee R. 1945. Measures of the amount
of ecologic association between species.
Ecology, 26(3):297?302.
Donner, Allan and Michael Eliasziw.
1987. Sample size requirements for
reliability studies. Statistics in Medicine,
6:441?448.
Doran, Christine, John Aberdeen, Laurie
Damianos, and Lynette Hirschman.
2001. Comparing several aspects of
human-computer and human-human
dialogues. In Proceedings of the 2nd
SIGdial Workshop on Discourse and
Dialogue, Aalborg, Denmark. Available at:
http://www.sigdial.org/workshops/
workshop2/proceedings.
Eckert, Miriam and Michael Strube. 2000.
Dialogue acts, synchronizing units,
and anaphora resolution. Journal of
Semantics, 17(1):51?89.
Feinstein, Alvan R. and Domenic V. Cicchetti.
1990. High agreement but low kappa:
I. The problems of two paradoxes. Journal
of Clinical Epidemiology, 43(6):543?549.
Fellbaum, Christiane, editor. 1998. WordNet:
An Electronic Lexical Database. MIT Press,
Cambridge, MA.
Fleiss, Joseph L. 1971. Measuring nominal
scale agreement among many raters.
Psychological Bulletin, 76(5):378?382.
Fleiss, Joseph L. 1975. Measuring agreement
between two judges on the presence or
absence of a trait. Biometrics, 31(3):651?659.
Francis, W. Nelson and Henry Kucera.
1982. Frequency Analysis of English Usage:
lexicon and grammar. Houghton Mifflin,
Boston, MA.
Geertzen, Jeroen and Harry Bunt. 2006.
Measuring annotator agreement in a
complex hierarchical dialogue act
annotation scheme. In Proceedings of the
7th SIGdial Workshop on Discourse and
Dialogue, pages 126?133, Sydney.
Gross, Derek, James F. Allen, and David R.
Traum. 1993. The Trains 91 dialogues.
TRAINS Technical Note 92-1, University
of Rochester Computer Science
Department, Rochester, NY.
Grosz, Barbara J. and Candace L. Sidner.
1986. Attention, intentions, and the
structure of discourse. Computational
Linguistics, 12(3):175?204.
Hayes, Andrew F. and Klaus Krippendorff.
2007. Answering the call for a standard
reliability measure for coding data.
Communication Methods and Measures,
1(1):77?89.
Hearst, Marti A. 1997. TextTiling:
Segmenting text into multi-paragraph
subtopic passages. Computational
Linguistics, 23(1):33?64.
Hovy, Eduard, Mitchell Marcus, Martha
Palmer, Lance Ramshaw, and Ralph
Weischedel. 2006. OntoNotes: The 90%
solution. In Proceedings of HLT?NAACL,
Companion Volume: Short Papers,
pages 57?60, New York.
Hsu, Louis M. and Ronald Field. 2003.
Interrater agreement measures: Comments
on kappan, Cohen?s kappa, Scott?s ?,
and Aickin?s ?. Understanding Statistics,
2(3):205?219.
Jaccard, Paul. 1912. The distribution of the
flora in the Alpine zone. New Phytologist,
11(2):37?50.
Jekat, Susanne, Alexandra Klein, Elisabeth
Maier, Ilona Maleck, Marion Mast, and
J. Joachim Quantz. 1995. Dialogue acts in
VERBMOBIL. VM-Report 65, Universita?t
Hamburg, DFKI GmbH, Universita?t
Erlangen, and TU Berlin.
Jurafsky, Daniel, Elizabeth Shriberg, and
Debra Biasca. 1997. Switchboard
SWBD-DAMSL shallow-discourse-
function annotation coders manual,
draft 13. Technical Report 97-02,
University of Colorado at Boulder,
Institute for Cognitive Science.
Kilgarriff, Adam. 1999. 95% replicability
for manual word sense tagging. In
Proceedings of the Ninth Conference
of the European Chapter of the Association
for Computational Linguistics,
pages 277?278, Bergen, Norway.
Klavans, Judith L., Samuel Popper, and
Rebecca Passonneau. 2003. Tackling
the internet glossary glut: Automatic
extraction and evaluation of genus
phrases. In Proceedings of the
SIGIR-2003 Workshop on the Semantic Web,
Toronto.
Kowtko, Jacqueline C., Stephen D. Isard,
and Gwyneth M. Doherty. 1992.
Conversational games within dialogue.
Research Paper HCRC/RP-31, Human
Communication Research Centre,
University of Edinburgh.
Krippendorff, Klaus. 1970. Estimating the
reliability, systematic error and random
error of interval data. Educational and
Psychological Measurement, 30(1):61?70.
593
Computational Linguistics Volume 34, Number 4
Krippendorff, Klaus. 1978. Reliability of
binary attribute data. Biometrics,
34(1):142?144. Letter to the editor,
with a reply by Joseph L. Fleiss.
Krippendorff, Klaus. 1980. Content Analysis:
An Introduction to Its Methodology,
chapter 12. Sage, Beverly Hills, CA.
Krippendorff, Klaus. 1995. On the reliability
of unitizing contiguous data. Sociological
Methodology, 25:47?76.
Krippendorff, Klaus. 2004a. Content Analysis:
An Introduction to Its Methodology,
second edition, chapter 11. Sage,
Thousand Oaks, CA.
Krippendorff, Klaus. 2004b. Reliability
in content analysis: Some common
misconceptions and recommendations.
Human Communication Research,
30(3):411?433.
Landis, J. Richard and Gary G. Koch. 1977.
The measurement of observer agreement
for categorical data. Biometrics,
33(1):159?174.
Leech, Geoffrey, Roger Garside, and Michael
Bryant. 1994. CLAWS4: The tagging of the
British National Corpus. In Proceedings of
COLING 1994: The 15th International
Conference on Computational Linguistics,
Volume 1, pages 622?628, Kyoto.
Levin, James A. and James A. Moore. 1978.
Dialogue-games: Metacommunication
structures for natural language
interaction. Cognitive Science, 1(4):395?420.
Manning, Christopher D. and Hinrich
Schuetze. 1999. Foundations of Statistical
Natural Language Processing. MIT Press,
Cambridge, MA.
Marcu, Daniel, Magdalena Romera, and
Estibaliz Amorrortu. 1999. Experiments in
constructing a corpus of discourse trees:
Problems, annotation choices, issues. In
Workshop on Levels of Representation in
Discourse, pages 71?78, University of
Edinburgh.
Marcus, Mitchell P., Mary Ann
Marcinkiewicz, and Beatrice Santorini.
1993. Building a large annotated corpus of
English: the Penn Treebank. Computational
Linguistics, 19(2):313?330.
Marion, Rodger. 2004. The whole art of
deduction. Unpublished manuscript.
Melamed, I. Dan and Philip Resnik. 2000.
Tagger evaluation given hierarchical
tagsets. Computers and the Humanities,
34(1?2):79?84. Available at:
http://www.sahs/utmb.edu/PELLINORE/
Intro to research/wad/wad/ home.htm.
Mieskes, Margot and Michael Strube. 2006.
Part-of-speech tagging of transcribed
speech. In Proceedings of LREC,
pages 935?938, Genoa.
Mihalcea, Rada, Timothy Chklovski, and
Adam Kilgarriff. 2004. The SENSEVAL-3
English lexical sample task. In Proceedings
of SENSEVAL-3, pages 25?28, Barcelona.
Miltsakaki, Eleni, Rashmi Prasad, Aravind
Joshi, and Bonnie Webber. 2004.
Annotating discourse connectives
and their arguments. In Proceedings
of the HLT-NAACL Workshop on
Frontiers in Corpus Annotation, pages 9?16,
Boston, MA.
Moser, Megan G., Johanna D. Moore, and
Erin Glendening. 1996. Instructions
for Coding Explanations: Identifying
Segments, Relations and Minimal Units.
Technical Report 96-17, University of
Pittsburgh, Department of Computer
Science.
Navarretta, Costanza. 2000. Abstract
anaphora resolution in Danish. In
Proceedings of the 1st SIGdial Workshop on
Discourse and Dialogue, Hong Kong,
pages 56?65.
Nenkova, Ani and Rebecca Passonneau.
2004. Evaluating content selection in
summarization: The pyramid method.
In Proceedings of HLT-NAACL 2004,
pages 145?152, Boston, MA.
Neuendorf, Kimberly A. 2002. The
Content Analysis Guidebook. Sage,
Thousand Oaks, CA.
Palmer, Martha, Hoa Trang Dang, and
Christiane Fellbaum. 2007. Making
fine-grained and coarse-grained sense
distinctions, both manually and
automatically. Natural Language
Engineering, 13(2):137?163.
Passonneau, Rebecca J. 2004. Computing
reliability for coreference annotation.
In Proceedings of LREC, volume 4,
pages 1503?1506, Lisbon.
Passonneau, Rebecca J. 2006. Measuring
agreement on set-valued items (MASI)
for semantic and pragmatic annotation.
In Proceedings of LREC, Genoa,
pages 831?836.
Passonneau, Rebecca J., Nizar Habash, and
Owen Rambow. 2006. Inter-annotator
agreement on a multilingual semantic
annotation task. In Proceedings of LREC,
Genoa, pages 1951?1956.
Passonneau, Rebecca J. and Diane J. Litman.
1993. Intention-based segmentation:
Human reliability and correlation with
linguistic cues. In Proceedings of 31st
Annual Meeting of the ACL, pages 148?155,
Columbus, OH.
594
Artstein and Poesio Inter-Coder Agreement for CL
Passonneau, Rebecca J. and Diane J. Litman.
1996. Empirical analysis of three
dimensions of spoken discourse:
Segmentation, coherence and linguistic
devices. In Eduard H. Hovy and Donia
R. Scott, editors, Computational and
Conversational Discourse: Burning Issues ?
An Interdisciplinary Account, volume 151
of NATO ASI Series F: Computer and
Systems Sciences. Springer, Berlin,
chapter 7, pages 161?194.
Passonneau, Rebecca J. and Diane J. Litman.
1997. Discourse segmentation by human
and automated means. Computational
Linguistics, 23(1):103?139.
Pevzner, Lev and Marti A. Hearst. 2002.
A critique and improvement of an
evaluation metric for text segmentation.
Computational Linguistics, 28(1):19?36.
Poesio, Massimo. 2004a. Discourse
annotation and semantic annotation in the
GNOME corpus. In Proceedings of the 2004
ACL Workshop on Discourse Annotation,
pages 72?79, Barcelona.
Poesio, Massimo. 2004b. The
MATE/GNOME proposals for anaphoric
annotation, revisited. In Proceedings of the
5th SIGdial Workshop on Discourse and
Dialogue, pages 154?162, Cambridge, MA.
Poesio, Massimo and Ron Artstein. 2005.
The reliability of anaphoric annotation,
reconsidered: Taking ambiguity into
account. In Proceedings of the Workshop on
Frontiers in Corpus Annotation II: Pie in the
Sky, pages 76?83, Ann Arbor, MI.
Poesio, Massimo and Natalia N. Modjeska.
2005. Focus, activation, and this-noun
phrases: An empirical study. In
Anto?nio Branco, Tony McEnery, and
Ruslan Mitkov, editors, Anaphora
Processing, volume 263 of Current Issues
in Linguistic Theory. John Benjamins,
pages 429?442, Amsterdam and
Philadelphia.
Poesio, Massimo, A. Patel, and Barbara
Di Eugenio. 2006. Discourse structure
and anaphora in tutorial dialogues: An
empirical analysis of two theories of the
global focus. Research in Language and
Computation, 4(2?3):229?257.
Poesio, Massimo and Renata Vieira. 1998.
A corpus-based investigation of definite
description use. Computational Linguistics,
24(2):183?216.
Popescu-Belis, Andrei. 2005. Dialogue acts:
One or more dimensions? Working
Paper 62, ISSCO, University of Geneva.
Posner, Karen L., Paul D. Sampson, Robert A.
Caplan, Richard J. Ward, and Frederick W.
Cheney. 1990. Measuring interrater
reliability among multiple raters: An
example of methods for nominal data.
Statistics in Medicine, 9:1103?1115.
Rajaratnam, Nageswari. 1960. Reliability
formulas for independent decision data
when reliability data are matched.
Psychometrika, 25(3):261?271.
Reidsma, Dennis and Jean Carletta. 2008.
Reliability measurement without limits.
Computational Linguistics, 34(3):319?326.
Reinhart, T. 1981. Pragmatics and linguistics:
An analysis of sentence topics.
Philosophica, 27(1):53?93.
Reynar, Jeffrey C. 1998. Topic Segmentation:
Algorithms and Applications. Ph.D. thesis,
University of Pennsylvania, Philadelphia.
Ries, Klaus. 2002. Segmenting conversations
by topic, initiative and style. In Anni R.
Coden, Eric W. Brown, and Savitha
Srinivasan, editors, Information Retrieval
Techniques for Speech Applications,
volume 2273 of Lecture Notes in Computer
Science. Springer, Berlin, pages 51?66.
Rietveld, Toni and Roeland van Hout.
1993. Statistical Techniques for the Study
of Language and Language Behaviour.
Mouton de Gruyter, Berlin.
Rosenberg, Andrew and Ed Binkowski.
2004. Augmenting the kappa statistic
to determine interannotator reliability
for multiply labeled data points. In
Proceedings of HLT-NAACL 2004: Short
Papers, pages 77?80, Boston, MA.
Scott, William A. 1955. Reliability of content
analysis: The case of nominal scale
coding. Public Opinion Quarterly,
19(3):321?325.
Shriberg, Elizabeth, Raj Dhillon, Sonali
Bhagat, Jeremy Ang, and Hannah Carvey.
2004. The ICSI meeting recorder dialog act
(MRDA) corpus. In Proceedings of the 5th
SIGdial Workshop on Discourse and Dialogue,
pages 97?100, Cambridge, MA.
Siegel, Sidney and N. John Castellan, Jr. 1988.
Nonparametric Statistics for the Behavioral
Sciences, 2nd edition, chapter 9.8.
McGraw-Hill, New York.
Stent, Amanda J. 2001. Dialogue Systems
as Conversational Partners: Applying
Conversation Acts Theory to Natural
Language Generation for Task-Oriented
Mixed-Initiative Spoken Dialogue. Ph.D.
thesis, Department of Computer Science,
University of Rochester.
Stevenson, Mark and Robert Gaizauskas.
2000. Experiments on sentence boundary
detection. In Proceedings of 6th ANLP,
pages 84?89, Seattle, WA.
595
Computational Linguistics Volume 34, Number 4
Stolcke, Andreas, Noah Coccaro, Rebecca
Bates, Paul Taylor, Carol Van Ess-Dykema,
Klaus Ries, Elizabeth Shriberg, Daniel
Jurafsky, Rachel Martin, and Marie Meteer.
2000. Dialogue act modeling for automatic
tagging and recognition of conversational
speech. Computational Linguistics,
26(3):339?373.
Stuart, Alan. 1955. A test for homogeneity of
the marginal distributions in a two-way
classification. Biometrika, 42(3/4):412?416.
Teufel, Simone, Jean Carletta, and Marc
Moens. 1999. An annotation scheme
for discourse-level argumentation in
research articles. In Proceedings of Ninth
Conference of the EACL, pages 110?117,
Bergen.
Teufel, Simone and Marc Moens. 2002.
Summarizing scientific articles:
Experiments with relevance and
rhetorical status. Computational
Linguistics, 28(4):409?445.
Traum, David R. and Elizabeth A.
Hinkelman. 1992. Conversation
acts in task-oriented spoken dialogue.
Computational Intelligence, 8(3):575?599.
Vallduv??, Enric. 1993. Information
packaging: A survey. Research Paper
RP-44, University of Edinburgh, HCRC.
Ve?ronis, Jean. 1998. A study of polysemy
judgments and inter-annotator agreement.
In Proceedings of SENSEVAL-1,
Herstmonceux Castle, England. Available
at: http://www.itri.brighton.ac.uk/
events/senseval/ARCHIVE/PROCEEDINGS/.
Vilain, Marc, John Burger, John Aberdeen,
Dennis Connolly, and Lynette Hirschman.
1995. A model-theoretic coreference
scoring scheme. In Proceedings of the Sixth
Message Understanding Conference,
pages 45?52, Columbia, MD.
Zwick, Rebecca. 1988. Another look at
interrater agreement. Psychological
Bulletin, 103(3):374?378.
596
Unsupervised Relation Extraction from Web Documents
Kathrin Eichler, Holmer Hemsen and Gu?nter Neumann
DFKI GmbH, LT-Lab, Stuhlsatzenhausweg 3 (Building D3 2), D-66123 Saarbru?cken
{FirstName.SecondName}@dfki.de
Abstract
The IDEX system is a prototype of an interactive dynamic Information Extraction (IE) system. A user of the system
expresses an information request in the form of a topic description, which is used for an initial search in order to retrieve
a relevant set of documents. On basis of this set of documents, unsupervised relation extraction and clustering is done by
the system. The results of these operations can then be interactively inspected by the user. In this paper we describe the
relation extraction and clustering components of the IDEX system. Preliminary evaluation results of these components are
presented and an overview is given of possible enhancements to improve the relation extraction and clustering components.
1. Introduction
Information extraction (IE) involves the process of au-
tomatically identifying instances of certain relations of
interest, e.g., produce(<company>, <product>, <lo-
cation>), in some document collection and the con-
struction of a database with information about each
individual instance (e.g., the participants of a meet-
ing, the date and time of the meeting). Currently, IE
systems are usually domain-dependent and adapting
the system to a new domain requires a high amount
of manual labour, such as specifying and implement-
ing relation?specific extraction patterns manually (cf.
Fig. 1) or annotating large amounts of training cor-
pora (cf. Fig. 2). These adaptations have to be made
offline, i.e., before the specific IE system is actually
made. Consequently, current IE technology is highly
statical and inflexible with respect to a timely adapta-
tion to new requirements in the form of new topics.
Figure 1: A hand-coded rule?based IE?system (schemat-
ically): A topic expert implements manually task?specific
extraction rules on the basis of her manual analysis of a
representative corpus.
1.1. Our goal
The goal of our IE research is the conception and im-
plementation of core IE technology to produce a new
Figure 2: A data?oriented IE system (schematically): The
task?specific extraction rules are automatically acquired by
means of Machine Learning algorithms, which are using
a sufficiently large enough corpus of topic?relevant docu-
ments. These documents have to be collected and costly
annotated by a topic?expert.
IE system automatically for a given topic. Here, the
pre?knowledge about the information request is given
by a user online to the IE core system (called IDEX)
in the form of a topic description (cf. Fig. 3). This
initial information source is used to retrieve relevant
documents and extract and cluster relations in an un-
supervised way. In this way, IDEX is able to adapt
much better to the dynamic information space, in par-
ticular because no predefined patterns of relevant re-
lations have to be specified, but relevant patterns are
determined online. Our system consists of a front-end,
which provides the user with a GUI for interactively in-
specting information extracted from topic-related web
documents, and a back-end, which contains the rela-
tion extraction and clustering component. In this pa-
per, we describe the back-end component and present
preliminary evaluation results.
1.2. Application potential
However, before doing so we would like to motivate
the application potential and impact of the IDEX ap-
Figure 3: The dynamic IE system IDEX (schematically):
a user of the IDEX IE system expresses her information
request in the form of a topic description which is used for
an initial search in order to retrieve a relevant set of doc-
uments. From this set of documents, the system extracts
and collects (using the IE core components of IDEX) a set
of tables of instances of possibly relevant relations. These
tables are presented to the user (who is assumed to be the
topic?expert), who will analyse the data further for her in-
formation research. The whole IE process is dynamic, since
no offline data is required, and the IE process is interactive,
since the topic expert is able to specify new topic descrip-
tions, which express her new attention triggered by a novel
relationship she was not aware of beforehand.
proach by an example application. Consider, e.g., the
case of the exploration and the exposure of corruptions
or the risk analysis of mega construction projects. Via
the Internet, a large pool of information resources of
such mega construction projects is available. These
information resources are rich in quantity, but also
in quality, e.g., business reports, company profiles,
blogs, reports by tourists, who visited these construc-
tion projects, but also web documents, which only
mention the project name and nothing else. One of
the challenges for the risk analysis of mega construc-
tion projects is the efficient exploration of the possibly
relevant search space. Developing manually an IE sys-
tem is often not possible because of the timely need
of the information, and, more importantly, is proba-
bly not useful, because the needed (hidden) informa-
tion is actually not known. In contrast, an unsuper-
vised and dynamic IE system like IDEX can be used
to support the expert in the exploration of the search
space through pro?active identification and clustering
of structured entities. Named entities like for example
person names and locations, are often useful indicators
of relevant text passages, in particular, if the names are
in some relationship. Furthermore, because the found
relationships are visualized using an advanced graph-
ical user interface, the user can select specific names
and find associated relationships to other names, the
documents they occur in or she can search for para-
phrases of sentences.
2. System architecture
The back-end component, visualized in Figure 4, con-
sists of three parts, which are described in detail in this
section: preprocessing, relation extraction and relation
clustering.
2.1. Preprocessing
In the first step, for a specific search task, a topic of
interest has to be defined in the form of a query. For
this topic, documents are automatically retrieved from
the web using the Google search engine. HTML and
PDF documents are converted into plain text files. As
the tools used for linguistic processing (NE recogni-
tion, parsing, etc.) are language-specific, we use the
Google language filter option when downloading the
documents. However, this does not prevent some doc-
uments written in a language other than our target
language (English) from entering our corpus. In ad-
dition, some web sites contain text written in several
languages. In order to restrict the processing to sen-
tences written in English, we apply a language guesser
tool, lc4j (Lc4j, 2007) and remove sentences not clas-
sified as written in English. This reduces errors on
the following levels of processing. We also remove sen-
tences that only contain non-alphanumeric characters.
To all remaining sentences, we apply LingPipe (Ling-
Pipe, 2007) for sentence boundary detection, named
entity recognition (NER) and coreference resolution.
As a result of this step database tables are created,
containing references to the original document, sen-
tences and detected named entities (NEs).
2.2. Relation extraction
Relation extraction is done on the basis of parsing po-
tentially relevant sentences. We define a sentence to be
of potential relevance if it at least contains two NEs.
In the first step, so-called skeletons (simplified depen-
dency trees) are extracted. To build the skeletons, the
Stanford parser (Stanford Parser, 2007) is used to gen-
erate dependency trees for the potentially relevant sen-
tences. For each NE pair in a sentence, the common
root element in the corresponding tree is identified and
the elements from each of the NEs to the root are col-
lected. An example of a skeleton is shown in Figure 5.
In the second step, information based on dependency
types is extracted for the potentially relevant sen-
tences. Focusing on verb relations (this can be ex-
tended to other types of relations), we collect for each
verb its subject(s), object(s), preposition(s) with ar-
guments and auxiliary verb(s). We can now extract
verb relations using a simple algorithm: We define a
verb relation to be a verb together with its arguments
(subject(s), object(s) and prepositional phrases) and
consider only those relations to be of interest where at
least the subject or the object is an NE. We filter out
relations with only one argument.
2.3. Relation clustering
Relation clusters are generated by grouping relation
instances based on their similarity.
web documents document
retrieval
topic specific documents plain text documents
sentence/documents+
 NE tables
languagefiltering
syntactic +typed dependencyparsing 
sov?relationsskeletons +
clustering
conversion
Preprocessing
Relation extraction
Relation clustering
sentencesrelevant
filtering of
relationfiltering
table of clustered relations
sentence boundary
resolutioncoreference
detection,NE recognition,
Figure 4: System architecture
Figure 5: Skeleton for the NE pair ?Hohenzollern? and ?Brandenburg? in the sentence ?Subsequent members of
the Hohenzollern family ruled until 1918 in Berlin, first as electors of Brandenburg.?
The comparably large amount of data in the corpus
requires the use of an efficient clustering algorithm.
Standard ML clustering algorithms such as k-means
and EM (as provided by the Weka toolbox (Witten
and Frank, 2005)) have been tested for clustering the
relations at hand but were not able to deal with the
large number of features and instances required for an
adequate representation of our dataset. We thus de-
cided to use a scoring algorithm that compares a re-
lation to other relations based on certain aspects and
calculates a similarity score. If this similarity score ex-
ceeds a predefined threshold, two relations are grouped
together.
Similarity is measured based on the output from the
different preprocessing steps as well as lexical informa-
tion from WordNet (WordNet, 2007):
? WordNet: WordNet information is used to deter-
mine if two verb infinitives match or if they are in
the same synonym set.
? Parsing: The extracted dependency information is
used to measure the token overlap of the two sub-
jects and objects, respectively. We also compare
the subject of the first relation with the object of
the second relation and vice versa. In addition,
we compare the auxiliary verbs, prepositions and
preposition arguments found in the relation.
? NE recognition: The information from this step
is used to count how many of the NEs occurring
in the contexts, i.e., the sentences in which the
two relations are found, match and whether the
NE types of the subjects and objects, respectively,
match.
? Coreference resolution: This type of information
is used to compare the NE subject (or object) of
one relation to strings that appear in the same
coreference set as the subject (or object) of the
second relation.
Manually analyzing a set of extracted relation in-
stances, we defined weights for the different similarity
measures and calculated a similarity score for each re-
lation pair. We then defined a score threshold and clus-
tered relations by putting two relations into the same
cluster if their similarity score exceeded this threshold
value.
3. Experiments and results
For our experiments, we built a test corpus of doc-
uments related to the topic ?Berlin Hauptbahnhof?
by sending queries describing the topic (e.g., ?Berlin
Hauptbahnhof?, ?Berlin central station?) to Google
and downloading the retrieved documents specifying
English as the target language. After preprocessing
these documents as described in 2.1., our corpus con-
sisted of 55,255 sentences from 1,068 web pages, from
which 10773 relations were automatically extracted
and clustered.
3.1. Clustering
From the extracted relations, the system built 306 clus-
ters of two or more instances, which were manually
evaluated by two authors of this paper. 81 of our clus-
ters contain two or more instances of exactly the same
relation, mostly due to the same sentence appearing in
several documents of the corpus. Of the remaining 225
clusters, 121 were marked as consistent, 35 as partly
consistent, 69 as not consistent. We defined consis-
tency based on the potential usefulness of a cluster to
the user and identified three major types of potentially
useful clusters:
? Relation paraphrases, e.g.,
accused (Mr Moore, Disney, In letter)
accused (Michael Moore, Walt Disney
Company)
? Different instances of the same pattern, e.g.,
operates (Delta, flights, from New York)
offers (Lufthansa, flights, from DC)
? Relations about the same topic (NE), e.g.,
rejected (Mr Blair, pressure, from Labour
MPs)
reiterated (Mr Blair, ideas, in speech, on
March)
created (Mr Blair, doctrine)
...
Of our 121 consistent clusters, 76 were classified as be-
ing of the type ?same pattern?, 27 as being of the type
?same topic? and 18 as being of the type ?relation para-
phrases?. As many of our clusters contain two instances
only, we are planning to analyze whether some clusters
should be merged and how this could be achieved.
3.2. Relation extraction
In order to evaluate the performance of the relation ex-
traction component, we manually annotated 550 sen-
tences of the test corpus by tagging all NEs and verbs
and manually extracting potentially interesting verb
relations. We define ?potentially interesting verb rela-
tion? as a verb together with its arguments (i.e., sub-
ject, objects and PP arguments), where at least two
of the arguments are NEs and at least one of them
is the subject or an object. On the basis of this crite-
rion, we found 15 potentially interesting verb relations.
For the same sentences, the IDEX system extracted 27
relations, 11 of them corresponding to the manually
extracted ones. This yields a recall value of 73% and
a precision value of 41%.
There were two types of recall errors: First, errors in
sentence boundary detection, mainly due to noisy in-
put data (e.g., missing periods), which lead to parsing
errors, and second, NER errors, i.e., NEs that were
not recognised as such. Precision errors could mostly
be traced back to the NER component (sequences of
words were wrongly identified as NEs).
In the 550 manually annotated sentences, 1300 NEs
were identified as NEs by the NER component. 402
NEs were recognised correctly by the NER, 588
wrongly and in 310 cases only parts of an NE were
recognised. These 310 cases can be divided into three
groups of errors. First, NEs recognised correctly, but
labeled with the wrong NE type. Second, only parts
of the NE were recognised correctly, e.g., ?Touris-
mus Marketing GmbH? instead of ?Berlin Tourismus
Marketing GmbH?. Third, NEs containing additional
words, such as ?the? in ?the Brandenburg Gate?.
To judge the usefulness of the extracted relations, we
applied the following soft criterion: A relation is con-
sidered useful if it expresses the main information given
by the sentence or clause, in which the relation was
found. According to this criterion, six of the eleven
relations could be considered useful. The remaining
five relations lacked some relevant part of the sen-
tence/clause (e.g., a crucial part of an NE, like the
?ICC? in ?ICC Berlin?).
4. Possible enhancements
With only 15 manually extracted relations out of 550
sentences, we assume that our definition of ?potentially
interesting relation? is too strict, and that more inter-
esting relations could be extracted by loosening the ex-
traction criterion. To investigate on how the criterion
could be loosened, we analysed all those sentences in
the test corpus that contained at least two NEs in order
to find out whether some interesting relations were lost
by the definition and how the definition would have to
be changed in order to detect these relations. The ta-
ble in Figure 6 lists some suggestions of how this could
be achieved, together with example relations and the
number of additional relations that could be extracted
from the 550 test sentences.
In addition, more interesting relations could be
found with an NER component extended by more
types, e.g., DATE and EVENT. Open domain NER
may be useful in order to extract NEs of additional
types. Also, other types of relations could be inter-
esting, such as relations between coordinated NEs,
option example additional relations
extraction of relations,
where the NE is not the
complete subject, object or
PP argument, but only part
of it
Co-operation with <ORG>M.A.X.
2001<\ORG> <V>is<\V> clearly of
benefit to <ORG>BTM<\ORG>.
25
extraction of relations with
a complex VP
<ORG>BTM<\ORG> <V>invited and or
supported<\V> more than 1,000 media rep-
resentatives in <LOC>Berlin<\LOC>.
7
resolution of relative pro-
nouns
The <ORG>Oxford Centre for Maritime
Archaeology<\ORG> [...] which will
<V>conduct<\V> a scientific symposium in
<LOC>Berlin<\LOC>.
2
combination of several of the
options mentioned above
<LOC>Berlin<\LOC> has <V>developed to
become<\V> the entertainment capital of
<LOC>Germany<\LOC>.
7
Figure 6: Table illustrating different options according to which the definition of ?potentially interesting relation?
could be loosened. For each option, an example sentence from the test corpus is given, together with the number
of relations that could be extracted additionally from the test corpus.
e.g., in a sentence like The exhibition [...] shows
<PER>Clemens Brentano<\PER>, <PER>Achim
von Arnim<\PER> and <PER>Heinrich von
Kleist<\PER>, and between NEs occurring in the
same (complex) argument, e.g., <PER>Hanns Peter
Nerger<\PER>, CEO of <ORG>Berlin Tourismus
Marketing GmbH (BTM) <\ORG>, sums it up [...].
5. Related work
Our work is related to previous work on domain-
independent unsupervised relation extraction, in par-
ticular Sekine (2006), Shinyama and Sekine (2006) and
Banko et al (2007).
Sekine (2006) introduces On-demand information ex-
traction, which aims at automatically identifying
salient patterns and extracting relations based on these
patterns. He retrieves relevant documents from a
newspaper corpus based on a query and applies a POS
tagger, a dependency analyzer and an extended NE
tagger. Using the information from the taggers, he ex-
tracts patterns and applies paraphrase recognition to
create sets of semantically similar patterns. Shinyama
and Sekine (2006) apply NER, coreference resolution
and parsing to a corpus of newspaper articles to ex-
tract two-place relations between NEs. The extracted
relations are grouped into pattern tables of NE pairs
expressing the same relation, e.g., hurricanes and their
locations. Clustering is performed in two steps: they
first cluster all documents and use this information to
cluster the relations. However, only relations among
the five most highly-weighted entities in a cluster are
extracted and only the first ten sentences of each arti-
cle are taken into account.
Banko et al (2007) use a much larger corpus, namely
9 million web pages, to extract all relations between
noun phrases. Due to the large amount of data, they
apply POS tagging only. Their output consists of mil-
lions of relations, most of them being abstract asser-
tions such as (executive, hired by, company) rather
than concrete facts.
Our approach can be regarded as a combination of
these approaches: Like Banko et al (2007), we extract
relations from noisy web documents rather than com-
parably homogeneous news articles. However, rather
than extracting relations from millions of pages we re-
duce the size of our corpus beforehand using a query in
order to be able to apply more linguistic preprocessing.
Like Sekine (2006) and Shinyama and Sekine (2006),
we concentrate on relations involving NEs, the assump-
tion being that these relations are the potentially in-
teresting ones. The relation clustering step allows us
to group similar relations, which can, for example, be
useful for the generation of answers in a Question An-
swering system.
6. Future work
Since many errors were due to the noisiness of the ar-
bitrarily downloaded web documents, a more sophisti-
cated filtering step for extracting relevant textual infor-
mation from web sites before applying NE recognition,
parsing, etc. is likely to improve the performance of
the system.
The NER component plays a crucial role for the qual-
ity of the whole system, because the relation extraction
component depends heavily on the NER quality, and
thereby the NER quality influences also the results of
the clustering process. A possible solution to improve
NER in the IDEX System is to integrate a MetaNER
component, combining the results of several NER com-
ponents. Within the framework of the IDEX project
a MetaNER component already has been developed
(Heyl, to appear 2008), but not yet integrated into the
prototype. The MetaNER component developed uses
the results from three different NER systems. The out-
put of each NER component is weighted depending on
the component and if the sum of these values for a pos-
sible NE exceeds a certain threshold it is accepted as
NE otherwise it is rejected.
The clustering step returns many clusters containing
two instances only. A task for future work is to in-
vestigate, whether it is possible to build larger clus-
ters, which are still meaningful. One way of enlarging
cluster size is to extract more relations. This could
be achieved by loosening the extraction criteria as de-
scribed in section 4. Also, it would be interesting to see
whether clusters could be merged. This would require
a manual analysis of the created clusters.
Acknowledgement
The work presented here was partially supported by a
research grant from the?Programm zur Fo?rderung von
Forschung, Innovationen und Technologien (ProFIT)?
(FKZ: 10135984) and the European Regional Develop-
ment Fund (ERDF).
7. References
Michele Banko, Michael J. Cafarella, Stephen Soder-
land, Matthew Broadhead, and Oren Etzioni. 2007.
Open information extraction from the web. In Proc.
of the International Joint Conference on Artificial
Intelligence (IJCAI).
Andrea Heyl. to appear 2008. Unsupervised relation
extraction. Master?s thesis, Saarland University.
Lc4j. 2007. Language categorization library for Java.
http://www.olivo.net/software/lc4j/.
LingPipe. 2007. http://www.alias-i.com/lingpipe/.
Satoshi Sekine. 2006. On-demand information extrac-
tion. In ACL. The Association for Computer Lin-
guistics.
Yusuke Shinyama and Satoshi Sekine. 2006. Preemp-
tive information extraction using unrestricted re-
lation discovery. In Proc. of the main conference
on Human Language Technology Conference of the
North American Chapter of the Association of Com-
putational Linguistics, pages 304?311. Association
for Computational Linguistics.
Stanford Parser. 2007. http://nlp.stanford.edu/
downloads/lex-parser.shtml.
Ian H. Witten and Eibe Frank. 2005. Data Min-
ing: Practical machine learning tools and techniques.
Morgan Kaufmann, San Francisco, 2nd edition.
WordNet. 2007. http://wordnet.princeton.edu/.
BioNLP 2007: Biological, translational, and clinical language processing, pages 195?196,
Prague, June 2007. c?2007 Association for Computational Linguistics
Discovering contradicting protein-protein interactions in text 
Olivia Sanchez-Graillet 
Univ. of Essex, Wivenhoe Park, Colches-
ter CO4 3SQ, U.K. 
osanch@essex.ac.uk 
Massimo Poesio 
Univ. of Essex, Wivenhoe Park, Col-
chester CO4 3SQ, U.K. 
 DIT and Center for Mind/Brain Sci-
ences, Univ. of Trento, Via Sommarive 
14 I-38050 POVO (TN) - Italy 
poesio@essex.ac.uk 
1 Introduction 
In biomedical texts, contradictions about protein-
protein interactions (PPIs) occur when an author 
reports observing a given PPI whereas another au-
thor argues that very same interaction does not take 
place: e.g., when author X argues that ?protein A 
interacts with protein B? whereas author Y claims 
that ?protein A does not interact with B?. Of 
course, merely discovering a potential contradic-
tion does not mean the argument is closed as other 
factors may have caused the proteins to behave in 
different ways. We present preliminary work to-
wards the automatic detection of potential contra-
dictions between PPIs from text and an agreement 
experimental evaluation of our method. 
2 Method 
Our method consists of the following steps: i) ex-
tract positive and negative cases of PPIs and map 
them to a semantic structure; ii) compare the pairs 
of PPIs structures that contain similar canonical 
protein names iii) apply an inference method to the 
selected pair of PPIs.  
We extract positive and negative cases of PPIs 
by applying our system (Sanchez & Poesio, sub-
mitted). Our system considers proteins only as well 
as events where only one protein participates (e.g. 
?PI-3K activity?). The system produces the seman-
tic interpretation shown in Table 1. We manually 
corrected some of the information extracted in or-
der to compare exclusively our inference method 
with human annotators. 
The decision to determine if a C-PPI holds is 
given by the context. This context is formed by the 
combination of semantic components such as PPI 
polarity, verb direction, and manner polarity. 
 
P1 Canonical name of the first participant protein 
P2 Canonical name of the second participant protein. 
Cue-word Word (verbs or their nominalizations) expressing a PPI 
(e.g. interact, interaction, activate, activation, etc.). 
Semantic 
Relation 
Categories in which cue-words are grouped according 
to their similar effect in an interaction. (See Table 2). 
Polarity Whether the PPI is positive or negative 
Direction Direction of a relation according to the effect that a 
protein causes on other molecules in the interaction. 
(See Table 3) 
Manner Modality expressed by adverbs or adjectives (e.g. 
directly, weakly, strong, etc.) 
Manner 
Polarity 
Polarity assigned to manner according to the influence 
they have on the cue-word (see Table 4) 
Table 1. Semantic structure of a PPI 
 
Semantic Rela-
tion 
Verbs/nouns examples 
Activate Activat (e, ed,es,or,ion), transactivat (e,ed,es,ion) 
Inactivate decreas (e,ed,es), down-regulat(e,ed,es,ion) 
Table 2. Example of semantic verb relations 
  
+ - Neutral 
Activate, Attach Inactivate Substitute, React 
Create bond Break bond Modify, Cause 
Generate Release Signal, Associate 
Table 3. Directions of semantic relations 
 
Polarity Word 
(+) 1 strong(ly), direct(ly), potential(y), rapid(ly) 
(-)  0 hardly, indirect(ly), negative(e,ly) 
Table 4. Example of manner polarity 
 
Manner polarity is neutral (2) if the manner word 
is not included in the manner polarity table or if no 
manner word affects the cue-word. 
The method first obtains what we call ?PPI 
state? of each PPI. The PPI state is obtained in two 
steps that follow decision tables1: a) the values for 
                                                 
1
 Some decision tables are omitted due to space reasons. 
195
the combination of the verb direction and the man-
ner polarity (DM) of each PPI; b) then, the DM 
value and the polarity of the corresponding PPI are 
evaluated. 
Second, the method compares the PPI states of 
both PPIs as shown in Table 5.  
 
State1 Sstate2 Result State1 State2 Result 
0 0 NC 3 3 U 
0 1 C 0 4 C 
0 3 U 1 4 C 
1 1 NC 3 4 C 
1 3 U    
Table 5. Decision table for results2 
 
The following example illustrates our method. The 
table below shows two sentences taken from dif-
ferent documents. 
 
Document 1 Document 2 
Cells treated with hyperosmolar stress, 
UV-C, IR, or a cell-permeable form of 
ceramide, C2 ceramide, rapidly down-
regulated PI(3)K activity to 10%-30% of 
the activity found in serum-stimulated 
control cells? 
And fourth, C2-
ceramide did not 
affect the amount of 
PI 3-kinase activity in 
anti-IRS-1 precipi-
tates. 
 
The semantic structures corresponding to these 
sentences are shown in the next table. 
 
 DocA DocB 
P1 C2-ceramide C2-ceramide 
P2 PI-3K PI-3K 
Cue down-regulate affect 
Semantic relation Inactivate Cause 
Polarity positive negative 
Direction negative neutral 
Manner rapidly -- 
Manner polarity positive neutral 
 
The decision tables produced for this example are 
the following3. 
 
PPI Direction Manner DM 
A -  (0) + (1) - (0) 
B N (2) N (2) U (3) 
 
PPI Polarity DM State 
A + (1) - (0) - (0) 
B - (0) U(3) NN (4) 
 
                                                 
2
 Result values: contradiction (C), no contradiction (NC) and 
unsure (U). 
3
 The values included in the tables are: positive=1, nega-
tive=0, neutral=2, unsure=3, and negative-neutral=4. 
PPIA state PPIB state Result 
-(0) NN (4) Contradiction 
 
The result obtained is ?Contradiction?.  
3 Agreement experiment 
As a way of evaluation, we compared agreement 
between our method and human annotators by us-
ing the kappa measure (Siegel and Castellan, 
1998). We elaborated a test containing only of 31 
pairs of sentences (JBC articles) since this task can 
be tiring for human annotators. 
The test consisted on classifying the pairs of 
sentences into three categories: contradiction (C), 
no contradiction (NC) and unsure (U). The values 
of kappa obtained are presented in the following 
table. 
 
Groups Kappa 
Biologists only 0.37 
Biologists and our method 0.37 
Non-biologists only 0.22 
Non-biologists and our method 0.19 
Table 6 Agreement values 
 
Biologists mainly justified their answers based on 
biological knowledge (e.g. methodology, organ-
isms, etc.) while non-biologists based their answers 
on syntax. 
4 Conclusions 
We have presented a simple method to detect po-
tential contradictions of PPIs by using context ex-
pressed by semantics and linguistics constituents 
(e.g. modals, verbs, adverbs, etc). Our method 
showed to perform similarly to biologists and bet-
ter than non-biologists. Interestingly, biologists 
concluded that C-PPIs are rarely found; neverthe-
less, the cases found may be highly significant. 
Continuing with our work, we will try our sys-
tem in a larger set of data. 
References 
Sanchez,O and Poesio,M. (Submitted). Negation of pro-
tein-protein interactions: analysis and extraction. 
Siegel, S. and Castellan, N.J. (1998). Nonparametric 
statistics for the behavioral sciences. 2nd. edition, 
McGraw-Hill. 
196
Unsupervised Relation Extraction from Web Documents
Kathrin Eichler, Holmer Hemsen and Gu?nter Neumann
DFKI GmbH, LT-Lab, Stuhlsatzenhausweg 3 (Building D3 2), D-66123 Saarbru?cken
{FirstName.SecondName}@dfki.de
Abstract
The IDEX system is a prototype of an interactive dynamic Information Extraction (IE) system. A user of the system
expresses an information request in the form of a topic description, which is used for an initial search in order to retrieve
a relevant set of documents. On basis of this set of documents, unsupervised relation extraction and clustering is done by
the system. The results of these operations can then be interactively inspected by the user. In this paper we describe the
relation extraction and clustering components of the IDEX system. Preliminary evaluation results of these components are
presented and an overview is given of possible enhancements to improve the relation extraction and clustering components.
1. Introduction
Information extraction (IE) involves the process of au-
tomatically identifying instances of certain relations of
interest, e.g., produce(<company>, <product>, <lo-
cation>), in some document collection and the con-
struction of a database with information about each
individual instance (e.g., the participants of a meet-
ing, the date and time of the meeting). Currently, IE
systems are usually domain-dependent and adapting
the system to a new domain requires a high amount
of manual labour, such as specifying and implement-
ing relation?specific extraction patterns manually (cf.
Fig. 1) or annotating large amounts of training cor-
pora (cf. Fig. 2). These adaptations have to be made
offline, i.e., before the specific IE system is actually
made. Consequently, current IE technology is highly
statical and inflexible with respect to a timely adapta-
tion to new requirements in the form of new topics.
Figure 1: A hand-coded rule?based IE?system (schemat-
ically): A topic expert implements manually task?specific
extraction rules on the basis of her manual analysis of a
representative corpus.
1.1. Our goal
The goal of our IE research is the conception and im-
plementation of core IE technology to produce a new
Figure 2: A data?oriented IE system (schematically): The
task?specific extraction rules are automatically acquired by
means of Machine Learning algorithms, which are using
a sufficiently large enough corpus of topic?relevant docu-
ments. These documents have to be collected and costly
annotated by a topic?expert.
IE system automatically for a given topic. Here, the
pre?knowledge about the information request is given
by a user online to the IE core system (called IDEX)
in the form of a topic description (cf. Fig. 3). This
initial information source is used to retrieve relevant
documents and extract and cluster relations in an un-
supervised way. In this way, IDEX is able to adapt
much better to the dynamic information space, in par-
ticular because no predefined patterns of relevant re-
lations have to be specified, but relevant patterns are
determined online. Our system consists of a front-end,
which provides the user with a GUI for interactively in-
specting information extracted from topic-related web
documents, and a back-end, which contains the rela-
tion extraction and clustering component. In this pa-
per, we describe the back-end component and present
preliminary evaluation results.
1.2. Application potential
However, before doing so we would like to motivate
the application potential and impact of the IDEX ap-
Figure 3: The dynamic IE system IDEX (schematically):
a user of the IDEX IE system expresses her information
request in the form of a topic description which is used for
an initial search in order to retrieve a relevant set of doc-
uments. From this set of documents, the system extracts
and collects (using the IE core components of IDEX) a set
of tables of instances of possibly relevant relations. These
tables are presented to the user (who is assumed to be the
topic?expert), who will analyse the data further for her in-
formation research. The whole IE process is dynamic, since
no offline data is required, and the IE process is interactive,
since the topic expert is able to specify new topic descrip-
tions, which express her new attention triggered by a novel
relationship she was not aware of beforehand.
proach by an example application. Consider, e.g., the
case of the exploration and the exposure of corruptions
or the risk analysis of mega construction projects. Via
the Internet, a large pool of information resources of
such mega construction projects is available. These
information resources are rich in quantity, but also
in quality, e.g., business reports, company profiles,
blogs, reports by tourists, who visited these construc-
tion projects, but also web documents, which only
mention the project name and nothing else. One of
the challenges for the risk analysis of mega construc-
tion projects is the efficient exploration of the possibly
relevant search space. Developing manually an IE sys-
tem is often not possible because of the timely need
of the information, and, more importantly, is proba-
bly not useful, because the needed (hidden) informa-
tion is actually not known. In contrast, an unsuper-
vised and dynamic IE system like IDEX can be used
to support the expert in the exploration of the search
space through pro?active identification and clustering
of structured entities. Named entities like for example
person names and locations, are often useful indicators
of relevant text passages, in particular, if the names are
in some relationship. Furthermore, because the found
relationships are visualized using an advanced graph-
ical user interface, the user can select specific names
and find associated relationships to other names, the
documents they occur in or she can search for para-
phrases of sentences.
2. System architecture
The back-end component, visualized in Figure 4, con-
sists of three parts, which are described in detail in this
section: preprocessing, relation extraction and relation
clustering.
2.1. Preprocessing
In the first step, for a specific search task, a topic of
interest has to be defined in the form of a query. For
this topic, documents are automatically retrieved from
the web using the Google search engine. HTML and
PDF documents are converted into plain text files. As
the tools used for linguistic processing (NE recogni-
tion, parsing, etc.) are language-specific, we use the
Google language filter option when downloading the
documents. However, this does not prevent some doc-
uments written in a language other than our target
language (English) from entering our corpus. In ad-
dition, some web sites contain text written in several
languages. In order to restrict the processing to sen-
tences written in English, we apply a language guesser
tool, lc4j (Lc4j, 2007) and remove sentences not clas-
sified as written in English. This reduces errors on
the following levels of processing. We also remove sen-
tences that only contain non-alphanumeric characters.
To all remaining sentences, we apply LingPipe (Ling-
Pipe, 2007) for sentence boundary detection, named
entity recognition (NER) and coreference resolution.
As a result of this step database tables are created,
containing references to the original document, sen-
tences and detected named entities (NEs).
2.2. Relation extraction
Relation extraction is done on the basis of parsing po-
tentially relevant sentences. We define a sentence to be
of potential relevance if it at least contains two NEs.
In the first step, so-called skeletons (simplified depen-
dency trees) are extracted. To build the skeletons, the
Stanford parser (Stanford Parser, 2007) is used to gen-
erate dependency trees for the potentially relevant sen-
tences. For each NE pair in a sentence, the common
root element in the corresponding tree is identified and
the elements from each of the NEs to the root are col-
lected. An example of a skeleton is shown in Figure 5.
In the second step, information based on dependency
types is extracted for the potentially relevant sen-
tences. Focusing on verb relations (this can be ex-
tended to other types of relations), we collect for each
verb its subject(s), object(s), preposition(s) with ar-
guments and auxiliary verb(s). We can now extract
verb relations using a simple algorithm: We define a
verb relation to be a verb together with its arguments
(subject(s), object(s) and prepositional phrases) and
consider only those relations to be of interest where at
least the subject or the object is an NE. We filter out
relations with only one argument.
2.3. Relation clustering
Relation clusters are generated by grouping relation
instances based on their similarity.
web documents document
retrieval
topic specific documents plain text documents
sentence/documents+
 NE tables
languagefiltering
syntactic +typed dependencyparsing 
sov?relationsskeletons +
clustering
conversion
Preprocessing
Relation extraction
Relation clustering
sentencesrelevant
filtering of
relationfiltering
table of clustered relations
sentence boundary
resolutioncoreference
detection,NE recognition,
Figure 4: System architecture
Figure 5: Skeleton for the NE pair ?Hohenzollern? and ?Brandenburg? in the sentence ?Subsequent members of
the Hohenzollern family ruled until 1918 in Berlin, first as electors of Brandenburg.?
The comparably large amount of data in the corpus
requires the use of an efficient clustering algorithm.
Standard ML clustering algorithms such as k-means
and EM (as provided by the Weka toolbox (Witten
and Frank, 2005)) have been tested for clustering the
relations at hand but were not able to deal with the
large number of features and instances required for an
adequate representation of our dataset. We thus de-
cided to use a scoring algorithm that compares a re-
lation to other relations based on certain aspects and
calculates a similarity score. If this similarity score ex-
ceeds a predefined threshold, two relations are grouped
together.
Similarity is measured based on the output from the
different preprocessing steps as well as lexical informa-
tion from WordNet (WordNet, 2007):
? WordNet: WordNet information is used to deter-
mine if two verb infinitives match or if they are in
the same synonym set.
? Parsing: The extracted dependency information is
used to measure the token overlap of the two sub-
jects and objects, respectively. We also compare
the subject of the first relation with the object of
the second relation and vice versa. In addition,
we compare the auxiliary verbs, prepositions and
preposition arguments found in the relation.
? NE recognition: The information from this step
is used to count how many of the NEs occurring
in the contexts, i.e., the sentences in which the
two relations are found, match and whether the
NE types of the subjects and objects, respectively,
match.
? Coreference resolution: This type of information
is used to compare the NE subject (or object) of
one relation to strings that appear in the same
coreference set as the subject (or object) of the
second relation.
Manually analyzing a set of extracted relation in-
stances, we defined weights for the different similarity
measures and calculated a similarity score for each re-
lation pair. We then defined a score threshold and clus-
tered relations by putting two relations into the same
cluster if their similarity score exceeded this threshold
value.
3. Experiments and results
For our experiments, we built a test corpus of doc-
uments related to the topic ?Berlin Hauptbahnhof?
by sending queries describing the topic (e.g., ?Berlin
Hauptbahnhof?, ?Berlin central station?) to Google
and downloading the retrieved documents specifying
English as the target language. After preprocessing
these documents as described in 2.1., our corpus con-
sisted of 55,255 sentences from 1,068 web pages, from
which 10773 relations were automatically extracted
and clustered.
3.1. Clustering
From the extracted relations, the system built 306 clus-
ters of two or more instances, which were manually
evaluated by two authors of this paper. 81 of our clus-
ters contain two or more instances of exactly the same
relation, mostly due to the same sentence appearing in
several documents of the corpus. Of the remaining 225
clusters, 121 were marked as consistent, 35 as partly
consistent, 69 as not consistent. We defined consis-
tency based on the potential usefulness of a cluster to
the user and identified three major types of potentially
useful clusters:
? Relation paraphrases, e.g.,
accused (Mr Moore, Disney, In letter)
accused (Michael Moore, Walt Disney
Company)
? Different instances of the same pattern, e.g.,
operates (Delta, flights, from New York)
offers (Lufthansa, flights, from DC)
? Relations about the same topic (NE), e.g.,
rejected (Mr Blair, pressure, from Labour
MPs)
reiterated (Mr Blair, ideas, in speech, on
March)
created (Mr Blair, doctrine)
...
Of our 121 consistent clusters, 76 were classified as be-
ing of the type ?same pattern?, 27 as being of the type
?same topic? and 18 as being of the type ?relation para-
phrases?. As many of our clusters contain two instances
only, we are planning to analyze whether some clusters
should be merged and how this could be achieved.
3.2. Relation extraction
In order to evaluate the performance of the relation ex-
traction component, we manually annotated 550 sen-
tences of the test corpus by tagging all NEs and verbs
and manually extracting potentially interesting verb
relations. We define ?potentially interesting verb rela-
tion? as a verb together with its arguments (i.e., sub-
ject, objects and PP arguments), where at least two
of the arguments are NEs and at least one of them
is the subject or an object. On the basis of this crite-
rion, we found 15 potentially interesting verb relations.
For the same sentences, the IDEX system extracted 27
relations, 11 of them corresponding to the manually
extracted ones. This yields a recall value of 73% and
a precision value of 41%.
There were two types of recall errors: First, errors in
sentence boundary detection, mainly due to noisy in-
put data (e.g., missing periods), which lead to parsing
errors, and second, NER errors, i.e., NEs that were
not recognised as such. Precision errors could mostly
be traced back to the NER component (sequences of
words were wrongly identified as NEs).
In the 550 manually annotated sentences, 1300 NEs
were identified as NEs by the NER component. 402
NEs were recognised correctly by the NER, 588
wrongly and in 310 cases only parts of an NE were
recognised. These 310 cases can be divided into three
groups of errors. First, NEs recognised correctly, but
labeled with the wrong NE type. Second, only parts
of the NE were recognised correctly, e.g., ?Touris-
mus Marketing GmbH? instead of ?Berlin Tourismus
Marketing GmbH?. Third, NEs containing additional
words, such as ?the? in ?the Brandenburg Gate?.
To judge the usefulness of the extracted relations, we
applied the following soft criterion: A relation is con-
sidered useful if it expresses the main information given
by the sentence or clause, in which the relation was
found. According to this criterion, six of the eleven
relations could be considered useful. The remaining
five relations lacked some relevant part of the sen-
tence/clause (e.g., a crucial part of an NE, like the
?ICC? in ?ICC Berlin?).
4. Possible enhancements
With only 15 manually extracted relations out of 550
sentences, we assume that our definition of ?potentially
interesting relation? is too strict, and that more inter-
esting relations could be extracted by loosening the ex-
traction criterion. To investigate on how the criterion
could be loosened, we analysed all those sentences in
the test corpus that contained at least two NEs in order
to find out whether some interesting relations were lost
by the definition and how the definition would have to
be changed in order to detect these relations. The ta-
ble in Figure 6 lists some suggestions of how this could
be achieved, together with example relations and the
number of additional relations that could be extracted
from the 550 test sentences.
In addition, more interesting relations could be
found with an NER component extended by more
types, e.g., DATE and EVENT. Open domain NER
may be useful in order to extract NEs of additional
types. Also, other types of relations could be inter-
esting, such as relations between coordinated NEs,
option example additional relations
extraction of relations,
where the NE is not the
complete subject, object or
PP argument, but only part
of it
Co-operation with <ORG>M.A.X.
2001<\ORG> <V>is<\V> clearly of
benefit to <ORG>BTM<\ORG>.
25
extraction of relations with
a complex VP
<ORG>BTM<\ORG> <V>invited and or
supported<\V> more than 1,000 media rep-
resentatives in <LOC>Berlin<\LOC>.
7
resolution of relative pro-
nouns
The <ORG>Oxford Centre for Maritime
Archaeology<\ORG> [...] which will
<V>conduct<\V> a scientific symposium in
<LOC>Berlin<\LOC>.
2
combination of several of the
options mentioned above
<LOC>Berlin<\LOC> has <V>developed to
become<\V> the entertainment capital of
<LOC>Germany<\LOC>.
7
Figure 6: Table illustrating different options according to which the definition of ?potentially interesting relation?
could be loosened. For each option, an example sentence from the test corpus is given, together with the number
of relations that could be extracted additionally from the test corpus.
e.g., in a sentence like The exhibition [...] shows
<PER>Clemens Brentano<\PER>, <PER>Achim
von Arnim<\PER> and <PER>Heinrich von
Kleist<\PER>, and between NEs occurring in the
same (complex) argument, e.g., <PER>Hanns Peter
Nerger<\PER>, CEO of <ORG>Berlin Tourismus
Marketing GmbH (BTM) <\ORG>, sums it up [...].
5. Related work
Our work is related to previous work on domain-
independent unsupervised relation extraction, in par-
ticular Sekine (2006), Shinyama and Sekine (2006) and
Banko et al (2007).
Sekine (2006) introduces On-demand information ex-
traction, which aims at automatically identifying
salient patterns and extracting relations based on these
patterns. He retrieves relevant documents from a
newspaper corpus based on a query and applies a POS
tagger, a dependency analyzer and an extended NE
tagger. Using the information from the taggers, he ex-
tracts patterns and applies paraphrase recognition to
create sets of semantically similar patterns. Shinyama
and Sekine (2006) apply NER, coreference resolution
and parsing to a corpus of newspaper articles to ex-
tract two-place relations between NEs. The extracted
relations are grouped into pattern tables of NE pairs
expressing the same relation, e.g., hurricanes and their
locations. Clustering is performed in two steps: they
first cluster all documents and use this information to
cluster the relations. However, only relations among
the five most highly-weighted entities in a cluster are
extracted and only the first ten sentences of each arti-
cle are taken into account.
Banko et al (2007) use a much larger corpus, namely
9 million web pages, to extract all relations between
noun phrases. Due to the large amount of data, they
apply POS tagging only. Their output consists of mil-
lions of relations, most of them being abstract asser-
tions such as (executive, hired by, company) rather
than concrete facts.
Our approach can be regarded as a combination of
these approaches: Like Banko et al (2007), we extract
relations from noisy web documents rather than com-
parably homogeneous news articles. However, rather
than extracting relations from millions of pages we re-
duce the size of our corpus beforehand using a query in
order to be able to apply more linguistic preprocessing.
Like Sekine (2006) and Shinyama and Sekine (2006),
we concentrate on relations involving NEs, the assump-
tion being that these relations are the potentially in-
teresting ones. The relation clustering step allows us
to group similar relations, which can, for example, be
useful for the generation of answers in a Question An-
swering system.
6. Future work
Since many errors were due to the noisiness of the ar-
bitrarily downloaded web documents, a more sophisti-
cated filtering step for extracting relevant textual infor-
mation from web sites before applying NE recognition,
parsing, etc. is likely to improve the performance of
the system.
The NER component plays a crucial role for the qual-
ity of the whole system, because the relation extraction
component depends heavily on the NER quality, and
thereby the NER quality influences also the results of
the clustering process. A possible solution to improve
NER in the IDEX System is to integrate a MetaNER
component, combining the results of several NER com-
ponents. Within the framework of the IDEX project
a MetaNER component already has been developed
(Heyl, to appear 2008), but not yet integrated into the
prototype. The MetaNER component developed uses
the results from three different NER systems. The out-
put of each NER component is weighted depending on
the component and if the sum of these values for a pos-
sible NE exceeds a certain threshold it is accepted as
NE otherwise it is rejected.
The clustering step returns many clusters containing
two instances only. A task for future work is to in-
vestigate, whether it is possible to build larger clus-
ters, which are still meaningful. One way of enlarging
cluster size is to extract more relations. This could
be achieved by loosening the extraction criteria as de-
scribed in section 4. Also, it would be interesting to see
whether clusters could be merged. This would require
a manual analysis of the created clusters.
Acknowledgement
The work presented here was partially supported by a
research grant from the?Programm zur Fo?rderung von
Forschung, Innovationen und Technologien (ProFIT)?
(FKZ: 10135984) and the European Regional Develop-
ment Fund (ERDF).
7. References
Michele Banko, Michael J. Cafarella, Stephen Soder-
land, Matthew Broadhead, and Oren Etzioni. 2007.
Open information extraction from the web. In Proc.
of the International Joint Conference on Artificial
Intelligence (IJCAI).
Andrea Heyl. to appear 2008. Unsupervised relation
extraction. Master?s thesis, Saarland University.
Lc4j. 2007. Language categorization library for Java.
http://www.olivo.net/software/lc4j/.
LingPipe. 2007. http://www.alias-i.com/lingpipe/.
Satoshi Sekine. 2006. On-demand information extrac-
tion. In ACL. The Association for Computer Lin-
guistics.
Yusuke Shinyama and Satoshi Sekine. 2006. Preemp-
tive information extraction using unrestricted re-
lation discovery. In Proc. of the main conference
on Human Language Technology Conference of the
North American Chapter of the Association of Com-
putational Linguistics, pages 304?311. Association
for Computational Linguistics.
Stanford Parser. 2007. http://nlp.stanford.edu/
downloads/lex-parser.shtml.
Ian H. Witten and Eibe Frank. 2005. Data Min-
ing: Practical machine learning tools and techniques.
Morgan Kaufmann, San Francisco, 2nd edition.
WordNet. 2007. http://wordnet.princeton.edu/.
Proceedings of the 8th International Conference on Computational Semantics, page 3,
Tilburg, January 2009. c?2009 International Conference on Computational Semantics
Invited Talk
Play your way to an annotated corpus:
Games with a purpose and anaphoric annotation
Massimo Poesio
Universita` di Trento
Language Interaction & Computation Lab
Center for Mind/Brain Sciences
and
University of Essex
Language and Computation Group
School of Computer and Engineering
massimo.poesio@unitn.it
Abstract
The lack of large-scale corpora annotated with semantic information has
been a serious bottleneck for computational semantics, slowing down not
only the development of more advanced statistical methods, but also our
empirical understanding of the phenomena. The creation of the Ontonotes
corpus will finally bring computational semantics to the point where com-
putational syntax was in 1993 - but in the meantime, we have come to
appreciate the limitations of that methodology both theoretically and as
a way of gathering judgments. In this talk, I will discuss an ongoing ef-
fort to use the ?Games with a Purpose? methodology to create a large-scale
anaphorically annotated corpus in which multiple judgments are maintained
about the interpretation of each anaphoric expression - and in particular,
the Phrase Detectives game:
http://www.phrasedetectives.org
Joint work with Jon Chamberlain and Udo Kruschwitz (Uni Essex)
3
Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 87?96,
Queen Mary University of London, September 2009. c?2009 Association for Computational Linguistics
Interactive Gesture in Dialogue: a PTT Model
Hannes Rieser
Bielefeld University
Hannes.Rieser@uni-Bielefeld.de
Massimo Poesio
Universit? di Trento/University of Essex
poesio@essex.ac.uk
Abstract
Gestures are usually looked at in isola-
tion or from an intra-propositional per-
spective essentially tied to one speaker.
The Bielefeld multi-modal Speech-And-
Gesture-Alignment (SAGA) corpus has
many interactive gestures relevant for the
structure of dialogue (Rieser 2008, 2009).
To describe them, a dialogue theory is
needed which can serve as a speech-
gesture interface. PTT (Poesio and Traum
1997, Poesio and Rieser submitted a) can
do this job in principle, how this can be
achieved is the main topic of this paper.
As a precondition, the empirical research
procedure from systematic corpus annota-
tion via gesture typology to a partial on-
tology for gestures is described. It is then
explained how PTT is extended to provide
an incremental modelling of speech plus
gesture in an assertion-acknowledgement
adjacency pair where grounding between
dialogue participants is obtained through
gesture.
1 Introduction and Overview
We present work combining experimental meth-
ods, body-movement tracking techniques, corpus
linguistics and theoretical modelling in order to in-
vestigate the role of iconic gesture in dialogue. We
propose to map speech meaning and gesture mean-
ing into a single compositional meaning which is
then used in grounding and up-dating of infor-
mation states in discourse, using PTT (Poesio &
Traum 1997, Poesio & Rieser submitted 2009a)
to account for the speech-gesture interface. We
argue that several design features of PTT are es-
sential for this purpose, such as accepting sub-
propositional inputs, extracting information from
linguistic surface, using dynamic semantics, bas-
ing the dialogue engine on a theory of grounded-
ness and grounding, and allowing for the resolu-
tion of anaphora across turns.
The structure of the paper is as follows. Sec-
tion 2 looks at the Bielefeld Speech-and-Gesture-
Alignment corpus SAGA from which the data
comes. Section 3 then deals with multi-modal acts
using one example from SAGA (Dial 1 p.??). In
section 4 a short introduction into PTT is provided.
Sections 5 and 6 explain how a gesture typology
and a partial ontology can be extracted from the
annotated data. Both (see Appendix) serve as the
basis for the integration of gesture meaning and
verbal meaning. In section 7 PTT is developed
as an interface for verbal and gestural meaning.
First a PTT description of Dial 1 is provided using
(Poesio and Rieser submitted b, Poesio to appear)
dealing inter alia with anaphora resolution (7.1).
Secondly, PTTs interface properties are detailed
(7.2), the semantic defaults for combining speech
and gesture meaning are set up (7.3), and a gestu-
ral dialogue act is described (7.4). Section 8 con-
tains some preliminary insights into the grounding
of multi-modal content.
2 The Multi-modal SAGA Corpus
The SAGA corpus contains 25 route-description
dialogues taken from three camera perspectives
using body tracking technologies.1 The setting
comes with a Router ?riding on a car? through
a virtual landscape passing five landmarks. The
landmarks are connected by streets. Fig. 1a in Ap-
pendix B shows the Router, Fig. 1b the site, Fig.
1cf. Bergmann, K. et al (2007, 2008)
87
1c the town hall. After the ride the Router reports
his trip in detail to a Follower. We collected audio
and body movement data as well as eye-tracking
data from the Router. The dialogues have all been
annotated, use of functional predicates like IN-
DEXING, MODELLING, SHAPING2 etc. was
rated.
3 An Example from the SAGA corpus
In the dialogue passage (Dial 1) the Router uses
gestures to explain the looks of the town-hall.
We?ll focus on the numbered utterances in this pa-
per; utterances omitted in the reconstruction are
reported in italics, omitted phrases in brackets.
DIAL 1 [ROUTER:] [. . . ]
[. . . ]
und
and
[du]
[you]
folgst
follow
dann
then
dem
the
Stra?enverlauf
street
einfach
simply
nur bis
until
du
you
ah
ah
vor
before
nem
a
gr??eren
larger
Geb?ude
building
stehst.
stand.
(2.1) Das
That
ist
is
dann
then
das
the
Rathaus.
townhall.
(2.2) [Ahm]
[Ahm]
das
that
ist
is
ein
a
U-f?rmiges
U-shaped
Geb?ude.
building.
(2.3) Du
You
blickst
look
[praktisch]
[practically]
da
into
rein.
it.
(2.4) [Das
[That
hei?t]
is]
es
it
hat
has
vorne
to the front
zwei
two
Buchtungen
bulges
und
and
geht
closes
hinten
in the rear
zusammen dann.
then.
[FOLLOWER:] OK.
In (Dial 1) Router?s gestures first come with
two BEATS.3 Shortly after, the BEATs extend
into an ICONIC gesture overlapping town hall
in (2.1)(stills in Apendix B), cf. the still Two-
Handed-Prism-Segment-1. Then the Router?s
DRAWN U-shaped gesture (still One-Handed-
U-Shape) intersects the word U-shaped. Next
his SHAPING the sides of a prism (still Two-
Handed-U-Shaped-Prism-Segment) aligns with
[look pactically] into it. The gesture following
is two-handed: one hand SHAPES the U?s left
branch and the other both the U?s right branch
and its rear bend linking up to the left branch
(stills Two-Handed-Prism-Segment-2A and 2B).
The STROKE overlaps with the words and closes
in the rear. The Follower copies the two-handed
2Annotation PREDICATES are written in capital letters.
Cf. also fn. 5.
3BEATS largely rest on supra-segmentals and would de-
mand a paper of their own.
town hall gesture of the Router in his acknowl-
edgement (still Two-Handed-Prism-Segment-3).
In other words: the Follower?s gesture is aligned
to the Router?s. Being copies of each other, the se-
mantics of the Router?s and the Follower?s gesture
can enter the common ground (cf. 7.4 and 8). In
the reconstruction we will use the translation with
the English word order standardised.4
4 A Short Introduction to PTT
Explanation of dialogue rests on three things:
making clear how the succession of speakers? con-
tributions emerges, stating what the impact of con-
tributions on speakers? minds is and specifying
how information is extracted incrementally from
the contributions. Turning to emerging struc-
ture, PTT assumes that participants perform (often
fragmentary) contributions, discourse units (DUs),
which are dynamic propositions (DRSs in the
sense of (Muskens, 1996)). They contain locu-
tionary acts, conversational events/dialogue acts
plus their propositional contents/DRSs. DUs may
be sub-propositional micro-conversational events.
Dialogue acts are either core speech acts or
grounding acts. Core speech acts can be related to
the present like assert, towards the past like accept
or towards the future like commit. Grounding acts
are acknowledge or repair (Traum 2009). Putting
the distinctions above to work, we obviously can
already model adjacency pairs. For the problems
at issue we do not need more, cf. (Dial 1).
Which attitudes are assumed in current PTT
and which changes of participants? minds are ac-
counted for? Agents can have individual and pri-
vate or common and public intentions. All sorts
of actions, verbal or domain ones, are as a rule
intended, at the outset of changes we have indi-
vidual intentions. Common intentions are for ex-
ample needed in order to explain completions and
repairs (Poesio and Rieser submitted a). Most of
the cooperation facts investigated in Clark (1996)
need common intentions, most prominently, the
intention to carry out a communicative task felic-
itously. Frequently, the vehicle for these types of
intentions are (partial) plans. Plans can also be in-
dividual or shared. In (Dial 1) for example, the
Router has an individual plan how to best map out
his ride and the intention to communicate it to the
4We will end up with a mixture of German gesture
and English wording here. However, for didactic purposes
(sketch the main ideas) this seems acceptable. Sometimes we
will simulate German constructions in English.
88
Follower. The Follower in turn intends to let the
Router control her beliefs. Both have the collec-
tive intention to enable the Follower to follow the
Router?s route. Information presupposed or gener-
ated is contained in the discourse situation which,
in PTT, is just a normal situation with objects and
events, i.e., a DRS.
Conversational participants have command over
information states. An information state is up-
dated whenever a new event is perceived, includ-
ing events such as sub-sentential utterances, and
non-verbal events such as gestures or nods. Hence
the possibility is already implemented in PTT to
model accumulation of information due to ges-
ture. Information common to the dialogue par-
ticipants can be considered as grounded by de-
fault. This assumption connects PTT with other
dialogue theories, for example Clark?s (cf. Clark
and Marshall, 1981, Clark and Schaefer, 1989)
and Traum?s (Traum 2009). Acknowledged in-
formation is at the heart of the grounding pro-
cess. What is grounded is mutually believed ce-
teris paribus. Therefore, grounded information is
part of the pragmatic machinery driving a dialogue
forward (Rieser 2009). Grounding acts are taken
as meta-discoursive devices and not included in
discourse units proper. Besides beliefs and inten-
tions we have obligations as mental attitudes. In
PTT every conversational action induces an obli-
gation on the participant indicated to address that
action.
Information states raise the question of how
changes of information are brought about on the
basic grammatical level, viz. the interpretation
of incrementally produced locutionary acts. The
grammar in which syntactic and semantic interpre-
tation is implemented is LTAG (Abeille? & Ram-
bow (eds), 2000). LTAG is a tree-grammar en-
coding syntactic projections which do the duty
of, say, HPSGs rules, principles and constraints.
Nodes and projecting leaves are decorated with se-
mantic information based on Compositional DRT
as developed in (Muskens, 1996, 2001). A spe-
cific trait of PTT is working with semantic non-
monotonicity at all compositional levels: PTT hy-
pothesizes that semantic computation is the result
of defeasible inferences over DRSs obtained con-
catenating updates of single contributions. These
default inference rules have the effect of seman-
tic composition rules. Due to the impact of inter-
preted LTAG one can say that PTT is well founded
in a bottom up fashion. Especially the default
mechanism of PTT is used to make it a workable
interface for speech and gesture (cf. 7.2 - 7.4).
5 Setting up the Speech-gesture
Interface: Typology and Partial
Ontology
As mentioned, this paper is based on the sys-
tematic annotation of SAGA carried out over the
years 2007-2009 (Rieser 2009). Like many ges-
ture researchers we assume that the semantic and
pragmatic centre of a gesture is its stroke. The
stroke overlaps as a rule with part of a com-
plex constituent, for example the head or the log-
ical subject. The range of speech-gesture over-
lap usually marks the functional position where
the gestures meaning has to be merged into the
speech content. Technically, the annotation is
an ELAN-grid. From the annotation, a set of
gesture types has been factored out in the fol-
lowing way (Rieser 2009). AGENCY5 is in-
stalled as a root feature dominating the role fea-
tures ROUTER and FOLLOWER. Next come the
Router?s and the Follower?s LEFT and RIGHT
HAND and BOTH their HANDS. HANDEDNESS
in turn is mapped onto single annotation fea-
tures like HANDSHAPE, WRISTMOVEMENT,
PATHOFWRISTMOVEMENT etc. Bundles of
features make feature CLUSTERs which yield
classes of objects like curved, straight etc. en-
tities. These build up SHAPES of different di-
mensions:6 ABSTRACT OBJECTs of 0 DIMEN-
SION and LINEs, one-dimensional entities of dif-
ferent curvature. Among the two dimensional
entities are LOCATIONs, RECTANGLEs, CIR-
CLEs7 etc. Then three dimensional sorts come
up: CUBOIDSs CYLINDERs, PRISMs and so on.
In the end we get COMPOSITEs of SHAPES,
for example a BENT LINE in a SPHERE, and
SEQUENCES OF COMPOSITEs.8 The central is-
sue of ?How does a gesture acquire meaning?? is
answered in the following way: A gesture type is
mapped onto a partial ontology description, a stip-
ulation encoding the content attributed to a gesture
by raters. As a rule, gesture content is underspec-
5Gesture types, organised in an inheritance hierarchy
working with defaults (cf. Rieser 2009), are written in CAP-
ITAL ITALICS.
6In the following geometry terms are used mnemonically.
7SHAPEs can in general be fully developed or come in
SEGMENTs. We do not deal with SEGMENTs here.
8SEQUENCES encode evolution of SHAPEs in time.
89
ified and will be completed to some extent when
interfacing with verbal meaning. As an example
of a gesture type and its partial ontology, see e.g.
TwoHandedPrismSegment1 and ?Partial Ontolo-
gyTwoHandedPrismSegment1? in Appendix A.
6 Setting up the Speech-gesture
Interface: Levels of Interaction
Our starting point is the hypothesis detailed in
(Rieser 2008) that a genuine understanding of di-
alogues like (Dial 1) requires integration of multi-
modal meaning at different levels of discourse,
from fine grained lexical definitions up to rhetor-
ical relations. In the rest of the paper, we will
specify how information from spoken utterances
merges with information from gestures, using
(Dial 1) as an example. Omitting the two BEATS
on that is [then], we have the following gestures
on the Router?s side (see stills in Appendix B):
6.1 the PRISM SEGMENT covering the town hall; cf. still
Two-Handed-Prism-Segment
6.2 the DRAWN U-shape overlapping the adjective U-
shaped; still One-Handed-U-Shape
6.3 the PRISM SEGMENT affiliated to [practically] look
into it; still Two-Handed-U-Shaped-Prism-Segment
6.4 the two-handed U-shaped PRISM SEGMENT going
with and closes in the rear; stills Two-Handed-Prism-
Segment-2A and 2B.
The Follower uses a variant of
6.5 the Router?s PRISM SEGMENT in (6.4) followed by
OK; still Two-Handed-Prism-Segment-3.
The key observation from Rieser (2009) is that
gestures interact with verbal contributions at dif-
ferent levels. (6.1) to (6.4) must be integrated at
the level of the semantic interpretation of LTAG.
(6.3) is involved since the stroke covers three con-
stituents in the German wording, the modal ad-
verb [practically], the pronoun it, and the separa-
ble prefix da rein/into of the verb blickst/you look.
We will develop a simplified solution here using
the ?verb? look-into. Similarly, in (6.4) the ges-
ture contains information relevant for closes in the
rear, i.e. for the whole VP. The gesture informa-
tion has to be integrated into the Router?s dialogue
acts at the interface points mentioned. Therefrom
several side issues arise, for example the treatment
of anaphora across Router?s or Follower?s contri-
butions. In (Dial 1) the Follower uses gestural
information only to acknowledge. It is a multi-
modal example of acknowledging by imitating the
Router?s multi-modal acts. Her gesture and the
OK form a kind of ?complex acknowledgement?.
This way the Router?s contributions (6.2) to (6.4)
and the Follower?s contribution (6.5) show the in-
teractive role of gesture, more specifically, gesture
content in its use for grounding. We will briefly
comment upon that in section 8.
7 Using PTT as an Interface for Verbal
Meaning and Gestural Meaning
7.1 The verbal part of (Dial 1)
According to PTT, the discourse situation after the
verbal updates brought about by (Dial 1) would be
as follows.9 (We only represent one aspect of the
content of the initial utterances of (Dial 1).):
[DU0, DU1, DU2, DU3, DU4, DU5 |
DU0 is [. . . K1, . . . |
K1 is [b1 | building(b1), large(b1)],
. . . ]
DU1 is [u2.1, K2, ce2.1 |
u2.1: utter(Router,?Das ist das Rathaus?),
sem(u2.1) is K2,
K2 is [th1, tnhl |
th1 is ?y1. K1; [ | y1 is b1],
tnhl is ?u. [ | town hall (u)],
th1 is tnhl,
ce2.1: assert(Router, Follower, K2),
generate(u2.1, ce2.1)],
DU2 is [u2.2, K4, ce2.2 |
u2.2: utter(Router, ?das ist ein
U-f?rmiges Geb?ude.?),
sem(u2.2) is K4,
K4 is [th2 | th2 is ?y2. K5; [ |s: y2 is b1],
building(th2), U-shaped(th2),
K5 is K1],
ce2.2: assert(Router, Follower, K4),
generate(u2.2, ce2.2)],
DU3 is [u2.3, K7, ce2.3|
u2.3: utter(Router, ?Du blickst da rein?),
sem(u2.3) is K7,
K7 is [th3, s1 | th3 is ?y3. K8; [|s: y3 is b1],
s1: look-into(Follower, th3),10
K8 is K4],
ce2.3: assert(Router, Follower, K7),
generate(u2.3, ce2.3)],
DU4 is [u2.4, K9, ce2.4|
u4: utter(Router, ?es hat vorne
zwei Buchtungen und geht hinten zus. dann?),
sem(u2.4) is K9,
K9 is [th4,bu1,bu2,s2,s3,s4,s5,s6,
re1,re2 |
th4 is ?y4. K10;
[ | y4 is th3], K10 is
K7,
bulge(bu1), bulge(bu2),
s2: has(th4, bu1),
9Abbreviations used in the PTT-fragment: The prefixes
are usually followed by a number n ? 0. DU = discourse
unit, ce = conversational event, K = DRS, u = utterance, sem
= semantic function, x, y, z . . . = DRs, e: event, s: = situation.
In the DRSs ?,? stands for conjunction an ?;? between DRSs
for composition of DRSs.
90
s3: has(th4, bu2),
to-front-of(bu1, th4),
to-front-of(bu2, th4),
rear(re1), s4: has(bu1,
re1), rear(re2)11,
s5:has(bu2, re2),
s6: meet(re1, re2)],
ce2.44: assert(Router, Follower, K9),
generate(u2.4, ce2.4)].
The model of anaphora resolution accounting for
the anaphoric cases is developed in (Poesio and
Rieser, submitted 2009 b). The anaphoric Das/this
in DU1 depends on the discourse entity a larger
building introduced at the beginning of the con-
versation in DRS K1: K1 is the resource situation
for the anaphoric definite. The second das/this
still depends on the same resource situation. The
pronouns, however, behave differently: Pronoun
da/there in DU3 takes up the antecedent a U-
shaped building, whereas the es/it in DU4 in turn
refers to the it in DU3. Observe that the verbal part
of (Dial 1) alone would already specify the inter-
pretation completely: nothing essential is missing.
As it will become clear below, what gestures do in
this example is to add details to the verbally deter-
mined models and restrict the model set.
7.2 Tying in Gestures with Utterances
What we have got so far is a PTT-representation
of the verbal part of (Dial 1). We now move on
to how the information coming from the Router?s
gestures gets integrated with the verbal informa-
tion ? in particular, how this integration can take
place below the sentential level. Our account
builds on two key ideas from PTT. First of all,
gestures are part of the discourse situation ? i.e.,
the occurrence of gestures is recorded in the infor-
mation state?s representation of the discourse sit-
uation. Second, every occurrence of a sentence
constituent counts as a conversational event ? a
MICRO CONVERSATIONAL EVENT (MCE).
With these assumptions in place, the interaction
of speech meaning and gesture meaning ? how
the two types of meanings combine to specify the
overall meaning of a contribution ? can be spec-
ified using the same mechanisms that specify the
meaning of MCEs: i.e., with (prioritized) defaults
in the sense of (Reiter, 1980, Brewka 1989). One
10Observe that the town-hall and the U-shaped building are
the same.
11Observe that the gesture dynamically shapes two rears
which meet.
example of a default specifying semantic com-
position is the BINARY SEMANTIC COMPO-
SITION (BSC) developed in (Poesio to appear,
Poesio and Rieser submitted a) to specify the de-
fault way in which MCEs meanings can be derived
from the meanings of their constituents. (We use
the notation > to indicate defeasible inference, ?
to indicate ?dominated by?.)
BSC: u1?u, u2?u, sem(u1) is ????? sem(u2) is
?? , complete(u,u1,u2)
> sem(u) is ?(? )
BSC can however be overridden in a number
of circumstances: most notably, when anaphora
interpretation processes identify a referent for a
definite description like uNP1: utter(?the build-
ing?), in which case sem(uNP1) will be the refer-
ent as opposed to a set of properties; or in cases
of metonymy such as those studied by Nunberg
(2004), in which the meaning of a MCE may be
derived even more indirectly. We hypothesize that
the integration of utterance meaning and gesture
meaning is specified by interface defaults that
may override the general meaning in a similar
way by enriching the normal meaning of MCEs.
We provide several examples of interface defaults
below. For reasons of space, we only specify
the results of default inference, without provid-
ing full derivations of the multi-modal meanings.
For the gestures only the semantics12 is speci-
fied, abstracted from the description of the par-
tial ontology (cf. Appendix A for details). Utter-
ance meaning then operates on the partial ontol-
ogy information. MM abbreviates ?multi-modal?;
?lex-entry? means the word-form at stake, ?lex-
definition? means an explict dictionary definition
for the word, for example in the style of the OED,
cast into PL1.
7.3 The Interface Defaults
The general heuristic strategy for setting up inter-
face defaults designed to combine verbal mean-
ing and gesture meaning is to probe into the PTT
structure as deep as you need in order to fit in the
gestural content properly. Gestures may be rele-
vant at any level of discourse, as shown in (Rieser,
2008) and demonstrated below; this means that
sometimes gestural content has to be stored ?deep
12This is due to the fact that we do not integrate gestures
into the discourse situation here. If these are integrated one
will use their type description as syntax in AVM format. Ges-
tures do not have the normal category syntax.
91
in? the lexical definition of a word, at other times
one has to remain on the top level of semantic
composition or even follow up the contributions
produced so far. The interface defaults mostly fol-
low the general schema:
? -prefix mentioning the open parameters + lex-
icon definition + open parameters applied to iconic
meaning = ? -abstracted partial ontology descrip-
tion where the ? -bound parameters secure bind-
ing.
An exception to this is (7.3.5.1) which uses the
notion of satisfaction (see stills in Appendix B).
7.3.1 The PRISM SEGMENT aligned with
[the] town hall (6.1). To begin with, gestural
meaning can enrich the meaning of a nominal
utterance. The interface default allowing this is
called Noun meaning extended (NMExt)13
NMExt: Noun(u), sem(u) is ?x lex-
definition(x), u?u?, N?(u?), u overlaps g,
gesture(g), iconic-meaning(g) is ?p partial
ontology(p)
>sem(u?) is ?x (lex-definition(x)) iconic-
meaning(g)(x)
For instance in the dialogue under consideration
lex-definition is the predicate ?large building used
for the administration of local government? abbre-
viated as ??P?x [[ |s: large building(x), used for
the administration of local government(x)]; P(x)]?
and the Partial Ontology TwoHandedPrismSeg-
ment1 from the Appendix A, resulting in the fol-
lowing meaning for the utterance of ?town hall?
accompanied by the gesture:
(7.3.1.1) ?x [ ls, rs, loc|s: large building(x), used
for the administration of local government(x),
side(ls, x), left(ls, Router), side(rs, x), right(rs,
Router), location(loc, x)]
Observe that the fine-grained local information is
provided by the gesture.
7.3.2 The DRAWN U-shape overlapping the ad-
jective U-shaped is an example of gesture enrich-
ing an adjectival meaning through the interface de-
fault Adjective meaning extended (AdjMExt)
AdjMExt: Adjective(u), sem(u) is ?P?x [|lex-
entry(x), P(x)], u?u?, N?(u?), u overlaps g, ges-
ture(g), iconic-meaning(g) is ?p partial ontol-
ogy(p)
> sem(u?) is ?P?Q?x([|lex-entry(x), P(x)];
Q(x)) iconic- meaning(g)(x).
13??p partial ontology (p)? in NMExt and the following
defaults is used in the following way: The expression ?partial
ontology? refers to information from the partial ontology list
in the Appendix A. What has to be chosen can be seen from
the application of the default below.
Using AdjMExt and the meaning of the gesture
OneHanded-U-shape in the Partial Ontology we
obtain (7.3.2.1) as an enriched meaning for ?U-
shaped?, ??? denoting mereological composition:
(7.3.2.1) ?Q?x([|U-shaped(x), ?us(strai- ght-
line(lr, us), arc(lb, us), straight-line(ll, us), us =
lr ? lb ? ll )(x)]; Q(x))
After fitting in the noun modified by the multi-
modal content into position ?Q?, the DRs will have
to be correctly bound.
Observe that we could apply (NMext) and (Ad-
jMext) iteratively to arrive at a complex MM
Nom-meaning.
7.3.3 The PRISM SEGMENT affiliated to
[practically] look into it is computed using
the interface default Verb meaning extended
(VMExt).
VMExt: VP(u), V(u1), NP(u2), u1? u, u2? u,
sem(u1) is ?P?x([|s: lex-definition(x), P(x)], u
overlaps g, gesture(g), iconic-meaning(g) is ?p
partial ontology(p)
> sem(u) is ?P?x([|s: lex-definition(x), P(x)])
iconic-meaning(g)(x)
VMExt gives us, again using the informa-
tion from the Partial Ontology TwoHanded-U-
shapedPrism from the Appendix:
(7.3.3.1) ?x([|s: focus(agent, x), space(x),
bounded(x), empty(x), ?p[hl, ls, lel, fs, hr, rs,
ler, d| prism(p), height(hl, ls), left-side(ls, p),
front-side(fs, p), left(ls, Router), height(hr, rs),
right-side(rs, p), length(ler, rs), right(rs, Router),
length(lel, ls), distance(d, ls, lr), lel = ler](x)])
Again we see that fine-grained information is
provided by the gesture, especially the prag-
matic anchoring of the space looked into from the
Router?s position.
7.3.4 Finally, the two-handed U-shaped PRISM
SEGMENT going with and closes in the rear
needs a default VP meaning extended (VPMex-
tended). The gesture information is distributed
among the verb ?closes? and the PP ?in the rear?,
the assumption being that the object closing does
so at a particular location which is part of the ob-
ject itself. So we have:
VPMExt: V(u) ? VP(uph1), P(u) ? PP(uph2),
Det(u) ? NP(uph3), Nom(uph4) ? NP(uph3),
PP(uph2) ? VP(uph1), sem(u) is ?P?x([|lex-
definition(x)]; P(x)), u overlaps g, gesture(g),
iconic-meaning(g) is ?p partial ontology(p)
> sem(uph1) = ?P?x([|lex-definition(x), P(x)];
iconic-meaning(g)(x)
The default using Appendix A, Partial On-
tology TwoHanded-U-shapedPrism, generates the
following MM meaning:
92
(7.3.4.1) ?x([ |s: close(x), at(s, loc), prism(leftp),
prism(rightp), part(leftp, x), part(rightp, x), sec-
tion(sectl, leftp), leftside(lefts, leftp), length(ll,
lefts), left(lefts, Router), section(sectr, rightp),
rightside(rights, rightp), frontside(fronts, rightp),
bent(rightp), meet(lefts, rights, loc), right(rights,
Router), parallel(lefts, rights), distance(d, lfts,
rhts)]).
7.3.5 The Follower?s U-shaped gesture: So far,
gesture meaning constrained word meanings or
constituent meanings. In contrast, the Follower?s
U-shaped gesture invades dialogue structure. The
Follower?s reply has two steps. Her iconic ges-
ture yields a predicate U-shaped in much the same
way as the Router?s contribution in DU2 and DU4
does. This is combined with a DR anaphorically
linked to the Router?s preceding its and thats. The
gesture in turn takes up the Router?s U-shapes
from DU2 and DU4. So we get an anaphora
related to antecedent multi-modal information.14
Her ?OK? then simply acknowledges her own
DU5 filled up. Acknowledgement of the Router?s
contributions is achieved indirectly. In order to
model all that, we have to Hook up the Gesture?s
Content with a DR. This is simply
(7.3.5.1) ?p(iconic-meaning(p))DR for some
preceding discourse referent DR satisfying
iconic-meaning.
The relevant iconic meaning is taken from Par-
tial Ontology TwoHandedPrismSegment3: sec-
tion(sect, p), leftpart(lftp, p), lengthl(lftp),
left(leftp, Follower), rightpart(rtp, p), right(rightp,
Follower), lengthr(rtp), lftp = rtp, p = lftp ? rtp.
7.4 A Gestural Dialogue Act of Assertion
Concerning dialogue structure, we have concen-
trated on the verbal part of (Dial 1) in 7.1. In the
SAGA corpus there are many data showing how
dialogue structure interfaces with gesture mean-
ing. In 7.3.5 a default for the follower?s U-shaped
gesture was given. Its embedding into the PTT-
description of (Dial 1) is shown in DU5 below:
(7.4) DU5 is [g1, K10|
g1: gesticulate(Follower, Router, U-shape),
sem(g1) is K10,
K10 is [ |s: th5 is th4, ?p(section(sect, p),
leftpart(lftp, p), lengthl(lftp),
left(leftp, Follower), rightpart(rtp, p),
right(rightp, Follower), lengthr(rtp),
lftp is rtp, p is lftp ? rtp)(th5))]
ce5: assert(Follower, Router, K10),
generate(g1, ce5)],],
14These anaphorical relations are not reconstructed here
but delegated to a follow-up paper.
[ce6, u6|
u6: utter(Follower,?OK?),
ce6: ack(Follower, DU5),
textbfgenerate(u6, ce6)]]
In the multi-modal dialogue passage we have
?gesticulate? instead of ?utter?. The semantics, us-
ing the default (7.3.5.1) ?Hook up the Gesture?s
Content with a DR? and material from Appendix
A is provided in the standard way by K10. It is as-
sumed that gestural content can be generated and
asserted. The Follower?s acknowledgement is a
sort of self-acknowledgement that percolates up
through anaphora.
8 Grounding by Gesture: a Genuine
Case of Gestural Alignment
The different defaults, Noun-meaning ex-
tended (NMextended), Adjective meaning
extended (AdjMextended), Verb meaning ex-
tended (VMextended), VP meaning extended
(VPMextended) and Hook up the Gesture?s
Content with a DR, clearly indicate that integra-
tion of gesture meaning has to operate on levels of
different grain. Gesture can operate on a sub-word
level if one has to attach its meaning to parts of a
lexical definition, on the word level, on the level
of constituents, and, as a consequence of all that,
on specific dialogue acts. Furthermore, we have
seen gesture at two inter-propositional levels at
work, at the interface among the contributions of
one agent (see Router?s contributions which are
all ?united? by communicating the appearance of
the town hall) and at the interface among contri-
butions of different agents (Router-Follower).The
Follower acknowledges by imitating gestures of
the Router; this is a genuine case of gestural align-
ment. Alternatively, she could also acknowledge
verbally, uttering ?U-shaped? but she chooses a
gestural content. Obviously, speakers think that
this works. Her ?OK? furthermore shows that
verbal and gestural means can work in tandem.
So, in the end, the U-shape of the town hall is
rooted in the common ground by default and the
Router can continue with describing the route
leading to the next landmark.
Acknowledgements
Support by the SFB 674, Bielefeld University,
is gratefully acknowledged. We also want to
than three anonymous reviewers for carful reading
93
and suggestions for improvement. Hannes Rieser
wants to thank Florian Hahn for common work on
gesture typology starting in 2007.
References
Abeill?, A & Rambow, O. (eds) 2004. Tree Adjoining
Grammars. CSLI Publ. Stanford, CA.
Bergmann, K., Fr?hlich, C., Hahn, F., Kopp, St., L?ck-
ing, A. and Rieser, H. June 2007. Wegbeschreibung-
sexperiment: Grobannotationsschema. Univ. Biele-
feld, MS.
Bergmann, K., Damm, O., Fr?hlich, C., Hahn, F.,
Kopp, St., L?cking, A., Rieser, H. and Thomas,
N. June 2008. Annotationsmanual zur Gestenmor-
phologie. Univ. Bielefeld, MS.
Brewka, G. 1989. Nonmonotonic reasoning: from the-
oretical foundation towards efficient computation.
Hamburg, Univ., Diss.
Clark, H. H. 1996. Using Language. Cambridge Uni-
versity Press.
Clark, H. H. and Marshall, C. R. 1981. Definite Ref-
erence and Mutual Knowledge. In A. K. Joshi, B.
Webber, and I. A. Sag (eds.),Elements of Discourse
Understanding. CUP
Clark, H. H. and Schaefer, E. F. 1989. Contributing to
Discourse. Cognitive Science, 13, 259-294.
Muskens, R. 1996. Combining Montague Semantics
and Discourse Representation. Linguistics and Phi-
losophy 19, pp. 143-186.
Muskens, R. 2001. Talking about Trees and Truth-
conditions. Journal of Logic, Language and Infor-
mation, 10(4), pp. 417-455.
Nunberg, G. 2004. The Pragmatics of Deferred Inter-
pretation. In: Horn, L.R. and Ward, G.: The Hand-
book of Pragmatics. Blackwell Publishing Ltd.
Poesio, M. to appear. Incrementality and underspec-
ification in semantic interpretation. CSLI Publica-
tions.
Poesio, M. February 2009. Grounding in PTT. Talk
given at Bielefeld Univ.
Poesio, M. and Rieser, H. submitted a. Completions,
Coordination, and Alignment in Dialogue.
Poesio, M. and Rieser, H. submitted b. Anaphora and
Direct Reference: Empirical Evidence from Point-
ing.
Poesio, M. and Traum, D. 1997. ?Conversational Ac-
tions and Discourse Situations?, Computational In-
telligence, v. 13, n.3, 1997, pp.1- 45.
Rieser, H. 2008. Aligned Iconic Gesture in Different
Strata of MM Route-description Dialogue. In Pro-
ceedings of LONdial 2008, pp. 167-174
Rieser, H. 2009. On Factoring out a Gesture Typology
from the Bielefeld Speech-And- Gesture-Alignment
Corpus. Talk given at the GW 2009, ZiF Bielefeld,
to appear in the Proceedings of GW 2009.
Traum, D. 2009. Computational Models of Ground-
ing for Human-Computer Dialogue. Talk given at
Bielefeld Univ., February 2009
94
Appendices
Appendix A: Gesture Types and Description of Partial Ontology
Due to limited space gesture types and ontology descriptions are only partially characterised.
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
TwoHandedPrismSegment 1
R.G.Left.HandShapeShape loose B spread
R.G.Left.HandPalmDirection PDN/PTR
R.G.Left.BackOfHandDirection BAB
R.G.Left.Practice grasping-indexing
R.G.Left.Perspective speaker
R.G.Right.HandShapeShape loose B spread
R.G.Right.HandPalmDirection PDN/PTL
R.G.Right.BackOfHandDirection BAB
R.G.Right.Practice grasping-indexing
R.G.Right.Perspective speaker
R.Two-handed-configuration TT
R.Movement-relative-to-other-hand 0
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Partial Ontology TwoHandedPrismSegment 1
R.G.Left.HandShapeShape-loose B spread side(ls, p)
R.G.Left.HandPalmDirection-PDN/PTR left(ls, Router)
R.G.Right.HandShapeShape-loose B spread side(rs, p)
R.G.Right.HandPalmDirection-PDN/PTL right(rs, Router)
R.Two-handed-configuration-TT location(loc, p)
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
OneHanded-U-shape
R.G.Right.HandShapeShape G
R.G.Right.PalmDirection PDN/PTL>PDN/PTB>PDN
R.G.Right.BackOfHandDirection BAB/BTL>BAB/BDN>BAB/BDN/BTL
R.G.Right.PathOfWristLocation ARC
R.G.Right.WristLocationMovementDirectio MR>MF>ML
R.G.Right.Extent MEDIUM
R.G.Right.Practice drawing
R.G.Right.Pespective speaker
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Partial Ontology OneHanded-U-shape
R.G.Right.PathOfWristLocation-ARC U-shape(us)
R.G.Right.WristLocation straight-line(lr, us) ?
MovementDirection-MR>MF>ML bent-line(lb, us) ?
straight-line(ll, us)
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
TwoHandedPrismSegment 2
R.G.Left.HandShapeShape B spread
R.G.Left.HandPalmDirection PTR
R.G.Left.BackOfHandDirection BAB/BUP > BAB
R.G.Left.PathOfWristLocation LINE
R.G.Left.WristLocation MF
MovementDirection
R.G.Left.Practice shaping-modelling
R.G.Left.Perspective speaker
R.G.Right.HandShapeShape B spread
R.G.Right.HandPalmDirection PTL
R.G.Right.BackOfHandDirection BAB/BUP > BAB
R.G.Right.PathOfWristLocation LINE
R.G.Right.WristLocation MF
MovementDirection
R.G.Right.Practice shaping-modelling
R.G.Right.Perspective speaker
R.Two-handed-configuration PF
R.Movement-relative-to-other-hand SYNC
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Partial Ontology TwoHandedPrismSegment 2
R.G.Left.HandShapeShape-B spread hight(hl, ls)
R.G.Left.HandPalmDirection-PTR leftside(ls, p)
? prism(p)
R.G.Left.PathOfWristLocation-LINE length(lel, ls)
R.G.Left.WristLocation frontside(fs, p)
MovementDirection-MF
R.G.Left.Perspective-speaker left(ls, speaker)
R.G.Right.HandShapeShape-B spread hight(hr, rs)
R.G.Right.HandPalmDirection-PTL rightside(rs, p)
? prism(p)
R.G.Right.PathOfWristLocation-LINE length(ler, rs)
R.G.Right.WristLocation frontside(fs, p)
MovementDirection-MF
R.G.Right.Perspective-speaker right(rs, speaker)
R.Two-handed-configuration-PF distance(d, ls, lr)
R.Movement-relative-to-other-hand-SYNC lel = ler
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
TwoHanded-U-shapedPrism
R.G.Left.HandShapeShape small C
R.G.Left.HandPalmDirection PAB
R.G.Left.BackOfHandDirection BAB/BTR
R.G.Left.PathOfWristLocation LINE
R.G.Left.WristLocation MF
MovementDirection
R.G.Left.Practice shaping
R.G.Left.Perspective speaker
R.G.Right.HandShapeShape small C
R.G.Right.HandPalmDirection PAB/PTL>
PTL>PTB/PTL
R.G.Right.BackOfHandDirection BAB/BTR>
BAB>BAB/BTL
R.G.Right.PathOfWristLocation LINE>LINE
R.G.Right.WristLocation MF>ML
MovementDirection
R.G.Right.Practice shaping
R.G.Right.Perspective speaker
R.Two-handed-configuration BHA
R.Movement-relative-to-other-hand SYNC
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Partial Ontology TwoHanded-U-shapedPrism
R.G.Left.HandShapeShape-small C section(sectl, leftp)
R.G.Left.PathOfWristLocation-LINE leftside(lefts, leftp)
R.G.Left.WristLocation length(ll, lefts)
MovementDirection -MF
R.G.Left.Perspective-speaker left(lefts, speaker)
R.G.Right.HandShapeShape-small section(sectr, rightp)
R.G.Right.PathOfWristLocation-LINE>LINE rightside(rights, rightp) ?
frontside(fronts, rightp)
R.G.Right.WristLocation>ML bent(rightp) ?
MovementDirection-MF meet(lefts, rights)
R.G.Right.Perspective-speaker right(rights, speaker)
R.Movement-relative-to-other-hand-SYNC parallel(lefts, rights) ?
distance(d, lefts, rights)
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
95
??
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
TwoHandedPrismSegment 3
R.G.Left.HandShapeShape C
R.G.Right.HandPalmDirection PDN/PTR>
PAB/PUP
R.G.Left.BackOfHandDirection BAB>
BTL/BUP
R.G.Left.PathOfWristLocation ARC
R.G.Left.WristLocationMovementDirection ML>MB
R.G.Left.Practice shaping
R.G.Left.Perspective speaker
R.G.Right.HandShapeShape C
R.G.Right.HandPalmDirection PDN/PTL>
PAB/PUP
R.G.Right.BackOfHandDirection BAB>
BTR/BUP
R.G.Right.PathOfWristLocation ARC
R.G.Right.WristLocationMovementDirection MR>MB
R.G.Right.Practice shaping
R.G.Right.Perspective speaker
R.Two-handed-configuration BHA
R.Movement-relative-to-other-hand Mirror-Sagittal
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Partial Ontology TwoHandedPrismSegment 3
R.G.Left.HandShapeShape section(sect, p)
R.G.Left.PathOfWristLocation leftpart(lftp, p)
R.G.Left.WristLocationMovementDirection lengthl(lftp)
R.G.Left.Perspective left(leftp, speaker)
R.G.Right.HandShapeShape section(sect, p)
R.G.Right.PathOfWristLocation rightpart(rtp, p)
R.G.Right.WristLocationMovementDirection lengthr(rtp)
R.G.Right.Perspective speaker
R.Two-handed-configuration lftp = rtp
R.Movement-relative-to-other-hand p = lftp ? rtp
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
Appendix B: Figure 1
(a) The Router on his trip. (b) The site traversed by the
Router. The U-shaped building
is the town hall
(c) Fig. 1c shows the town hall
as described and gestured by
the Router.
(d) Two-Handed-Prism-
Segment-1
(e) One-Handed-U-Shape (f) Two-Handed-U-Shaped-
Prism-Segment
(g) Two-Handed-Prism-
Segment-2A
(h) Two-Handed-Prism-
Segment-2B
(i) Two-Handed-Prism-
Segment-3
Figure 1: The SAGA Setting
96
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1960?1970,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Of words, eyes and brains:
Correlating image-based distributional semantic models
with neural representations of concepts
Andrew J. Anderson, Elia Bruni, Ulisse Bordignon, Massimo Poesio and Marco Baroni
Center for Mind/Brain Sciences (University of Trento, C.so Bettini 31, Rovereto, Italy)
first.last@unitn.it
Abstract
Traditional distributional semantic models ex-
tract word meaning representations from co-
occurrence patterns of words in text cor-
pora. Recently, the distributional approach has
been extended to models that record the co-
occurrence of words with visual features in
image collections. These image-based models
should be complementary to text-based ones,
providing a more cognitively plausible view
of meaning grounded in visual perception. In
this study, we test whether image-based mod-
els capture the semantic patterns that emerge
from fMRI recordings of the neural signal.
Our results indicate that, indeed, there is a
significant correlation between image-based
and brain-based semantic similarities, and that
image-based models complement text-based
ones, so that the best correlations are achieved
when the two modalities are combined. De-
spite some unsatisfactory, but explained out-
comes (in particular, failure to detect differ-
ential association of models with brain areas),
the results show, on the one hand, that image-
based distributional semantic models can be a
precious new tool to explore semantic repre-
sentation in the brain, and, on the other, that
neural data can be used as the ultimate test set
to validate artificial semantic models in terms
of their cognitive plausibility.
1 Introduction
Many recent neuroscientific studies have brought
support to the view that concepts are represented
in terms of patterns of neural activation over broad
areas, naturally encoded as vectors in a neural se-
mantic space (Haxby et al, 2001; Huth et al, 2012).
Similar representations are also widely used in com-
putational linguistics, and in particular in distribu-
tional semantics (Clark, 2012; Erk, 2012; Turney
and Pantel, 2010), that captures meaning in terms
of vectors recording the patterns of co-occurrence
of words in large corpora, under the hypothesis that
words that occur in similar contexts are similar in
meaning.
Since the seminal work of Mitchell et al (2008),
there has thus being interest in investigating whether
corpus-harvested semantic representations can con-
tribute to the study of concepts in the brain. The
relation is mutually beneficial: From the point of
view of brain activity decoding, a strong correlation
between corpus-based and brain-derived conceptual
representations would mean that we could use the
former (much easier to construct on a very large
scale) to make inferences about the second: e.g., us-
ing corpus-based representations to reconstruct the
likely neural signal associated to words we have no
direct brain data for. From the point of view of com-
putational linguistics, neural data provide the ulti-
mate testing ground for models that strive to cap-
ture important aspects of human semantic mem-
ory (much more so than the commonly used ex-
plicit semantic rating benchmarks). If we found that
a corpus-based model of meaning can make non-
trivial predictions about the structure of the semantic
space in the brain, that would make a pretty strong
case for the intriguing idea that the model is approx-
imating, in interesting ways, the way in which hu-
mans acquire and represent semantic knowledge.
1960
We take as our starting point the extensive experi-
ments reported in Murphy et al (2012), who showed
that purely corpus-based distributional models are at
least as good at brain signal prediction tasks as ear-
lier models that made use of manually-generated or
controlled knowledge sources (Chang et al, 2011;
Palatucci et al, 2009; Pereira et al, 2011), and we
evaluate a very recent type of distributional model,
namely one that is not extracted from textual data
but from image collections through automated vi-
sual feature extraction techniques. It has been ar-
gued that this new generation of image-based dis-
tributional models (Bruni et al, 2011; Bruni et al,
2012b; Feng and Lapata, 2010; Leong and Mihal-
cea, 2011) provides a more realistic view of mean-
ing, since humans obviously acquire a large propor-
tion of their semantic knowledge from perceptual
data. The first question that we ask, thus, is whether
the more ?grounded? image-based models can help
us in interpreting conceptual representations in the
brain. More specifically, we will compare the per-
formance of different image-based representations,
and we will test whether text- and image-based rep-
resentations are complementary, so that when used
together they can better account for patterns in neu-
ral data. Finally, we will check for differences be-
tween anatomical regions in the degree to which text
and/or image models are effective, as one might ex-
pect given the well-known functional specializations
of different anatomical regions.
2 Brain data
We use the data that were recorded and preprocessed
by Mitchell et al (2008), available for download in
their supporting online material.1 Full details of the
experimental protocol, data acquisition and prepro-
cessing can be found in Mitchell et al (2008) and
the supporting material. Key points are that there
were nine right-handed adult participants (5 female,
age between 18 and 32). The experimental task was
to actively think about the properties of sixty objects
that were presented visually, each as a line drawing
in combination with a text label. The entire set of
objects was presented in a random order in six ses-
sions, each object remained on screen for 3 seconds
with a seven second fixation gap between presenta-
1http://www.cs.cmu.edu/?tom/science2008/
tions.
Mitchell and colleagues examined 12 categories,
five objects per category, for a total of 60 concepts
(words). Due to coverage limitations, we use 51/60
words representing 11/12 categories. Table 1 con-
tains the full list of 51 words organized by category.
fMRI acquisition and preprocessing Mitchell et
al. (2008) acquired functional images on a Siemens
Allegra 3.0T scanner using a gradient echo EPI
pulse sequence with TR=1000 ms, TE=30 ms and
a 60? angle. Seventeen 5-mm thick oblique-axial
slices were imaged with a gap of 1-mm between
slices. The acquisition matrix was 64?64 with
3.125?3.125?5-mm voxels. They subsequently
corrected data for slice timing, motion, linear trend,
and performed temporal smoothing with a high-pass
filter at 190s cutoff. The data were normalized to
the MNI template brain image, spatially normalized
into MNI space and resampled to 3?3?6 mm3 vox-
els. The voxel-wise percent signal change relative to
the fixation condition was computed for each object
presentation. The mean of the four images acquired
4s post stimulus presentation was used for analysis.
To create a single representation per object per
participant, we took the voxel-wise mean of the six
presentations of each word. Likewise to create a sin-
gle representation per category per participant, we
took the voxel-wise mean of all word models per
category, per participant.
Anatomical parcellation Analysis was conducted
on the whole brain, and to address the question of
whether there are differences in models? effective-
ness between anatomical regions, brains were fur-
ther partitioned into frontal, parietal, temporal and
occipital lobes. This partitioning is coarse (each lobe
is large and serves many diverse functions), but, for
an initial test, appropriate, given that each lobe has
specialisms that on face value are amenable to inter-
pretation by our different distributional models and
the exact nature of specialist processing in localised
areas is often subject to debate (so being overly re-
strictive may be risky). Formulation of the distribu-
tional models is described in detail in the Section 3,
but for now it is sufficient to know that the Object
model is derived from image statistics of the object
depicted in images, Context from image statistics of
the background scene, Object&Context is a com-
1961
Animals Bear, Cat, Cow, Dog Horse
Building Apartment, Barn, Church, House
Building parts Arch, Chimney, Closet, Door, Window
Clothing Coat, Dress, Pants, Shirt, Skirt
Furniture Bed, Chair, Desk, Dresser, Table
Insect Ant, Bee, Beetle, Butterfly, Fly
Kitchen utensils Bottle, Cup, Glass, Knife, Spoon
Man made objects Bell, Key, Refrigerator, Telephone, Watch
Tool Chisel, Hammer, Screwdriver
Vegetable Celery, Corn, Lettuce, Tomato
Vehicle Airplane, Bicycle, Car, Train, Truck
Table 1: The 51 words represented by the brain and the distributional models, organized by category.
bination of the two, and Window2 is a text-based
model.
The occipital lobe houses the primary visual pro-
cessing system and consequently it is reasonable
to expect some bias toward image-based semantic
models. Furthermore, given that experimental stim-
uli incorporated line drawings of the object,and the
visual cortex has a well-established role in process-
ing low-level visual statistics including edge detec-
tion (Bruce et al, 2003), we naturally expected a
good performance from Object (formulated from
edge orientation histograms of similar objects).
Following Goodale and Milner (1992)?s influ-
ential perception-action model (see McIntosh and
Schenk (2009) for recent discussion), visual infor-
mation is channeled from the occipital lobe in two
streams: a perceptual stream, serving object identi-
fication and recognition; and an action stream, spe-
cialist in processing egocentric spatial relationships
and ultimately supporting interaction with the world.
The perceptual stream leads to the temporal lobe.
Here the fusiform gyrus (shared with the occipital
lobe) plays a general role in object categorisation
(e.g., animals and tools (Chao et al, 1999), faces
(Kanwisher and Yovel, 2006), body parts (Peelen
and Downing, 2005) and even word form percep-
tion (McCandliss et al, 2003)). As the parahip-
pocampus is strongly associated with scene repre-
sentation (Epstein, 2008), we expect both the Object
and Context models to capture variability in the tem-
poral lobe. Of wider relevance to semantic process-
ing, the medial temporal gyrus, inferior temporal
gyrus and ventral temporal lobe have generally been
implicated to have roles in supramodal integration
and concept retrieval (Binder et al, 2009). Given
this, we expected that incorporating text would also
be valuable and that the Window2&Object&Context
combination would be a good model.
The visual action stream leads from the occipi-
tal lobe to the parietal lobe to support spatial cog-
nition tasks and action control (Sack, 2009). In
that there seems to be an egocentric frame of ref-
erence, placing actor in environment, it is tempt-
ing to speculate that the Context model is more ap-
propriate than the Object model here. As the pari-
etal lobe also contains the angular gyrus, thought
to be involved in complex, supra-modal information
integration and knowledge retrieval (Binder et al,
2009), we might again forecast that integrating text
and image information would boost performance, so
Window2&Context was earmarked as a strong can-
didate.
The frontal lobe, is traditionally associated with
high-level processing and manipulation of abstract
knowledge and rules and controlled behaviour
(Miller et al, 2002). Regarding semantics, the dor-
somedial prefrontal cortex has been implicated in
self-guided retrieval of semantic information (e.g.,
uncued speech production), the ventromedial pre-
frontal cortex in motivation and emotional process-
ing, the inferior frontal gyrus in phonological and
syntactic processing, (Binder et al, 2009) and in-
tegration of lexical information (Hagoort, 2005).
Given the association with linguistic processing we
anticipated a bias in favour of Window2.
The four lobes were identified and partitioned
using Tzourio-Mazoyer et al (2002)?s automatic
anatomical labelling scheme.
1962
Voxel selection The set of 500 most stable voxels,
both within the whole brain and from within each
region of interest were identified for analysis. The
most stable voxels were those showing consistent
variation across the different stimuli between scan-
ning sessions. Specifically, and following a similar
strategy to Mitchell et al (2008), for each voxel, the
set of 51 words from each unique pair of scanning
sessions were correlated using Pearson?s correlation
(6 sessions and therefore 15 unique pairs), and the
mean of the 15 resulting correlation coefficients was
taken as the measure of stability. The 500 voxels
with highest mean correlations were selected.
3 Distributional models
Distributional semantic models approximate word
meaning by keeping track of word co-occurrence
statistics from large textual input, relying on the dis-
tributional hypothesis: The meaning of a word can
be induced by the context in which it occurs (Turney
and Pantel, 2010). Despite their great success, these
models still rely on verbal input only, while humans
base their meaning representation also on perceptual
information (Louwerse, 2011).
Thanks to recent developments in computer vi-
sion, it is nowadays possible to take the visual per-
ceptual channel into account, and build new com-
putational models of semantics enhanced with vi-
sual information (Feng and Lapata, 2010; Bruni et
al., 2011; Leong and Mihalcea, 2011; Bergsma and
Goebel, 2011; Bruni et al, 2012a). Given a set of
target concepts and a collection of images depicting
those concepts, it is indeed possible to first encode
the image content into low-level features, and subse-
quently convert it into a higher-level representation
based on the bag-of-visual-words method (Grauman
and Leibe, 2011). Recently, Bruni et al (2012b)
have shown that better semantic representations can
be extracted if we first localize the concept in the
image, and then extract distinct higher-level features
(visual words) from the box containing the concept
and from the surrounding context. We also follow
this strategy here.
In our experiments we utilize both traditional text-
based models and experimental image-based mod-
els, as well as their combination.
3.1 Textual models
Verb We experiment with the original text-based
semantic model used to predict fMRI patterns by
Mitchell et al (2008). Each object stimulus word
is represented as a 25-dimensional vector, with each
value corresponding to the normalized sentence-
wide co-occurrence of that word with one of 25
manually-picked sensorimotor verbs (such as see,
hear, eat, . . . ) in a trillion word text corpus.
Window2 To create this model, we collect text
co-occurrence statistics from the freely available
ukWaC and Wackypedia corpora combined (about 3
billion words in total).2 As collocates of our distri-
butional model we select a set of 30K words, namely
the top 20K most frequent nouns, 5K most frequent
adjectives and 5K most frequent verbs.
In the tradition of HAL (Lund and Burgess, 1996),
the model is based on co-occurrence statistics with
collocates within a fixed-size window of 2 to the left
and right of each target word. Despite their sim-
plicity, narrow-window-based models have shown
to achieve state-of-the-art results in various stan-
dard semantic tasks (Bullinaria and Levy, 2007)
and to outperform both document-based and syntax-
based models trained on the same corpus (Bruni et
al., 2012a). Moreover, in Murphy et al (2012) a
window-based model very similar to ours was not
significantly worse than their best model for brain
decoding. We tried also a few variations, e.g., us-
ing a larger window or different transformations on
the raw co-occurrences from those presented below,
but with little, insignificant changes in performance.
Given that our focus here is on visual information,
we only report results for Window2 and its combi-
nation with visual models.
3.2 Visual models
Our visual models are inspired by Bruni et al
(2012b), that have explored to what extent extract-
ing features from images where objects are local-
ized results in better semantic representations. They
found that extracting visual features separately from
the object and its surrounding context leads to bet-
ter performance than not using localization, and us-
ing only object- and, more surprisingly, context-
extracted features also results in performant models
2http://wacky.sslmit.unibo.it/
1963
(especially when evaluating inter-object similarity,
the context in which an object is located can signif-
icantly contribute to semantic representation, in cer-
tain cases carrying even more information than the
depicted object itself).
More in detail, with localization the visual fea-
tures (visual words) can be extracted from the ob-
ject bounding box (in our experiments, the Object
model) or from only outside the object box (the
Context model). A combined model is obtained
by concatenating the two feature vectors (the Ob-
ject&Context model).
Visual model construction pipeline To extract
visual co-occurrence statistics, we use images from
ImageNet (Deng et al, 2009),3 a very large im-
age database organized on top of the WordNet hi-
erarchy (Fellbaum, 1998). ImageNet has more than
14 million images, covering 21K WordNet nominal
synsets. ImageNet stands out for the high quality of
its images, both in terms of resolution and concept
annotations. Moreover, for around 3K concepts, an-
notations of object bounding boxes is provided. This
last feature allows us to exploit object localization
within our experiments.
To build visual distributional models, we utilize
the bag-of-visual-words (BoVW) representation of
images (Sivic and Zisserman, 2003; Csurka et al,
2004). Inspired by NLP, BoVW discretizes the im-
age content in terms of a histogram of visual word
counts. Differently from NLP, in vision there is not a
natural notion of visual words, hence a visual vocab-
ulary has to be built from scratch. The process works
as follows. First, a large set of low-level features is
extracted from a corpus of images. The low-level
feature vectors are subsequently clustered into dif-
ferent regions (visual words). Given then a new im-
age, each of the low-level feature vectors extracted
from the patches that compose it is mapped to the
nearest visual word (e.g., in terms of Euclidean dis-
tance from the cluster centroid) such that the image
can be represented with a histogram counting the in-
stances of each visual word in the image.
As low-level features we use SIFT, the Scale In-
variant Feature Transform (Lowe, 2004). SIFT fea-
tures are good at capturing parts of objects and are
designed to be invariant to image transformations
3http://www.image-net.org/
such as change in scale, rotation and illumination.
To construct the visual vocabulary, we cluster the
SIFT features into 25K different clusters.4 We add
also spatial information by dividing the image into
several subregions, representing each of them in
terms of BoVW and then stacking the resulting his-
tograms (Lazebnik et al, 2006). We use in total 8
different regions, obtaining a final vector of 200K
dimensions (25K visual words ? 8 regions). Since
each concept in our dataset is represented by mul-
tiple images, we pool the visual word occurrences
across images by summing them up into a single
vector.
To perform the entire visual pipeline we use
VSEM, an open library for visual semantics (Bruni
et al, 2013).5
3.3 Model transformations and combination
Once both the textual and the visual models are built,
we perform two different transformations on the raw
co-occurrence counts. First, we transform them into
nonnegative Pointwise Mutual Information (PMI)
association scores (Church and Hanks, 1990). As a
second transformation, we apply dimensionality re-
duction to the two matrices. In particular, we adopt
the Singular Value Decomposition (SVD), one of the
most effective methods to approximate the original
data in lower dimensionality space (Schu?tze, 1997),
and reduce the vectors to 50 dimensions.
To combine text- and image-based semantic mod-
els in a joint representation, we separately normalize
their vectors to unit length, and concatenate them,
along the lines of Bruni et al (2011). More sophis-
ticated combination models have been proposed in
the recent literature on multimodal semantics. For
example, Bruni et al (2012a) use SVD as a mix-
ing strategy, given its ability to smooth the matrices
and uncover latent dimensions. Another example is
Silberer and Lapata (2013), where Canonical Corre-
lation Analysis is used. We reserve the exploration
of more advanced combination methods for further
studies.
Finally, to represent the 11 categories we experi-
ment with (see Table 1), we average the vectors of
the concepts they include.
4We use k-means, the most commonly employed clustering
algorithm for this task.
5http://clic.cimec.unitn.it/vsem/
1964
4 Experiments
A question is posed over how to evaluate the rela-
tionship between the different distributional models
and brain data. Comparing each model?s predictive
performance using the same strategy as Mitchell et
al. (2008) (also followed by Murphy et al (2012))
is one possibility: they used multiple regression to
relate distributional codes to individual voxel activa-
tions, thus allowing brain states to be estimated from
previously unseen distributional codes. Regression
models were trained on 58/60 words and in testing
the regression models estimated the brain state as-
sociated with the 2 unseen distributional codes. The
predicted brain states were compared with the actual
fMRI data, and the process repeated for each per-
mutation of left-out words, to build a metric of pre-
diction accuracy. For our purposes, a fair compari-
son of models using this strategy is complicated by
differences in dimensionality between both seman-
tic models and lobes (which we compare to other
lobes) in association with the comparatively small
number of words in the fMRI data set. Large dimen-
sionality models risk overfitting the data, and it is a
nuisance to try to reliably correct for the effects of
overfitting in performance comparisons. Not least,
to thoroughly evaluate all possible cross-validation
permutations is demanding in processing time, and
we have many models to compare.
An alternative approach, and that which we
have adopted, is representational similarity analy-
sis (Kriegeskorte et al, 2008). Representational
similarity analysis circumvents the previous prob-
lems by abstracting each fMRI/distributional data
source to a common structure capturing the inter-
relationships between each pair of data items (e.g.,
words). Specifically, for each model/participant?s
fMRI data/anatomical region, the similarity struc-
ture was evaluated by taking the pairwise correla-
tion (Pearson?s correlation coefficient) between all
unique category or word combinations. This pro-
duced a list of 55 category pair correlations and 121
word pair correlations for each data source. For all
brain data, correlation lists were averaged across the
nine participants to produce a single list of mean
word pair correlations and a single list of mean cat-
egory pair correlations for each anatomical region
and the whole brain. Then to provide a measure of
similarity between models and brain data, the cor-
relation lists for respective data sources were them-
selves correlated using Spearman?s rank correlation.
Statistical significance was tested using a permuta-
tion test: The word-pair (or category-pair) labels
were randomly shuffled 10,000 times to estimate a
null distribution when the two similarity lists are
not correlated. The p-value is calculated as the pro-
portion of random correlation coefficients that are
greater than or equal to the observed coefficient.
5 Results
5.1 Category-level analyses
Do image models correlate with brain data? Ta-
ble 2 displays results of Spearman?s correlations be-
tween the per-category similarity structure of dis-
tributional models and brain data. There is a sig-
nificant correlation between every purely image-
based model and the occipital, parietal and tempo-
ral lobes, and also the whole brain (.38? ? ?.51,
all p?.01). The frontal lobe is less well described.
Still, whilst not significant, correlations are only
marginally above the conventional p = .05 cutoff
(all are less than p = .064). This strongly suggests
that the answer to our first question is yes: distri-
butional models derived from images can be used
to explain concept fMRI data. Otherwise Window2
significantly correlates with the whole brain and all
anatomical regions except for the frontal lobe where
?=.34, p = .07. In contrast Verb (the original, par-
tially hand-crafted model used by Mitchell and col-
leagues) captures inter-relationships poorly and nei-
ther correlates with the whole brain or any lobe.
Do different models correlate with different
anatomical regions? 2-way ANOVA without
replication was used to test for differences in cor-
relation coefficients between the five pure-modality
models (Verb, Window2, Object, Context and Ob-
ject&Context), and the four brain lobes. This re-
vealed a highly significant difference between mod-
els F(4,12)=45.2, p<.001. Post-hoc 2-tailed t-tests
comparing model pairs found that Verb differed sig-
nificantly from all other models (correlations were
lower). There was a clear difference even when Verb
(mean?sd over lobes = .1?.1) was compared to the
second weakest model, Object (mean?sd=.4?.09),
where t =-7.7, p <.01, df=4. There were no
1965
Frontal Parietal Occipital Temporal Whole-Brain
Verb 0.00 (0.51) 0.06 (0.37) 0.24 (0.10) 0.07 (0.35) 0.17 (0.17)
Window2 0.34 (0.06) 0.49 (0.00) 0.47 (0.01) 0.47 (0.00) 0.44 (0.00)
Object 0.27 (0.07) 0.38 (0.02) 0.45 (0.00) 0.47 (0.00) 0.43 (0.01)
Context 0.33 (0.06) 0.50 (0.00) 0.44 (0.00) 0.44 (0.01) 0.44 (0.01)
Object&Context 0.32 (0.05) 0.48 (0.00) 0.51 (0.00) 0.49 (0.00) 0.49 (0.00)
Window2&Object 0.32 (0.06) 0.45 (0.00) 0.52 (0.00) 0.53 (0.00) 0.49 (0.00)
Window2&Context 0.39 (0.04) 0.57 (0.00) 0.53 (0.00) 0.55 (0.00) 0.51 (0.00)
Window2&Object&Context 0.37 (0.04) 0.52 (0.00) 0.55 (0.00) 0.55 (0.00) 0.53 (0.00)
Table 2: Matrix of correlations between each pairwise combination of distributional semantic models and brain data.
Correlations correspond to the pairwise similarity between the 11 categories. In each column the first value corre-
sponds to Spearman?s rank correlation coefficient and the value in parenthesis is the p-value.
Frontal Parietal Occipital Temporal Whole-Brain
Verb -0.04 (0.72) 0.09 (0.06) 0.07 (0.20) 0.03 (0.31) 0.07 (0.18)
Window2 0.07 (0.13) 0.19 (0.00) 0.12 (0.06) 0.21 (0.00) 0.13 (0.04)
Object 0.01 (0.40) 0.08 (0.07) 0.17 (0.01) 0.18 (0.00) 0.17 (0.01)
Context 0.04 (0.24) 0.14 (0.01) 0.01 (0.44) 0.12 (0.02) 0.02 (0.38)
Object&Context 0.03 (0.31) 0.13 (0.01) 0.10 (0.07) 0.17 (0.00) 0.11 (0.06)
Window2&Object 0.04 (0.24) 0.16 (0.00) 0.16 (0.01) 0.23 (0.00) 0.17 (0.00)
Window2&Context 0.07 (0.12) 0.20 (0.00) 0.09 (0.11) 0.22 (0.00) 0.11 (0.07)
Window2&Object&Context 0.05 (0.18) 0.18 (0.00) 0.12 (0.05) 0.23 (0.00) 0.13 (0.02)
Table 3: Matrix of correlations between each pairwise combination of distributional semantic models and brain data.
Correlations correspond to the pairwise similarity between the 51 words. In each column the first value corresponds
to Spearman?s rank correlation coefficient and the value in parenthesis is the p-value.
other significant differences between models. How-
ever there was a highly significant difference be-
tween lobes F(3,12)=13.77, p <.001. Post-hoc 2-
tailed t-tests comparing lobe pairs found that the
frontal lobe yielded significantly different correla-
tions (lower) than each other lobe. When the frontal
lobe (mean?sd over models = .25?.14) was com-
pared to the second weakest anatomical region, the
parietal lobe (mean?sd=.38?.19), the difference
was highly significant, t =-8, df=3, p <.01. This
introduces the question of whether this difference in
correlations is the result of differences in neural cat-
egory organisation and representation, or differences
in the quality of the signal, which we address next.
Category-level inter-correlations between lobes
were all relatively strong and highly significant. The
occipital lobe was found to be the most distinct, be-
ing similar to the temporal lobe (?=.71, p <.001),
but less so to the parietal and frontal lobes (?=.53,
p <.001 and ?=.57, p <.001 respectively). The
temporal lobe shows roughly similar levels of cor-
relation to each other lobe (all .71? ? ?.73, all
p <.001). The frontal and parietal lobes are related
most strongly to each other (?=.77, p <.001), to a
slightly lesser extent to the temporal lobe (in both
cases ?=.73, p <.001) and least so to the occipital
lobe. These strong relationships are consistent with
there being a broadly similar category organisation
across lobes.
To appraise this assertion in the context of the
previously detected difference between the frontal
lobe and all other lobes, we examine the raw cat-
egory pair similarity matrices derived from the oc-
cipital lobe and the frontal lobe (Figure 1). All the
below observations are qualitative. Although it is
difficult to have intuitions about the relative differ-
ences between all category pairs (e.g., whether tools
or furniture should be more similar to animals), we
might reasonably expect some obvious similarities.
For instance, for animals to be visually similar to in-
1966
sects and clothing, because all have legs and arms
and curves (of course we would not expect a strong
relationship between insects and clothes in function
or other modalities such as sound), buildings to be
similar to building parts and vehicles (hard edges
and windows), building parts to be similar to furni-
ture (e.g., from Table 1 we see there is some overlap
in category membership between these categories,
such as closet and door) and tools to be similar to
kitchen utensils. All of these relationships are main-
tained in the occipital lobe, and many are visible in
the frontal lobe (including the similarity between in-
sects and clothes), however there are exceptions that
are difficult to explain e.g., within the frontal lobe,
building parts are not similar to furniture, kitchen
utensils are closer to clothing than to tools and ve-
hicles are more similar to clothing than anything
else. As such we conclude that category-level rep-
resentations were similar across lobes with differ-
ences likely due to variation in signal quality be-
tween lobes.
Are text- and image-based semantic models com-
plementary? Turning to the question of whether
text- and image-derived semantic information can
be complementary, we observe from Table 2 that
there is not a single instance of a joint model with
a weaker correlation than its pure-image counter-
part. The Window2 model showed a stronger cor-
relation than the Window2&Object model for the
frontal and parietal lobes, but was weaker than Win-
dow2&Object&Context and Window2&Context in
all tests and was also weaker than any joint model
in whole-brain comparisons. The mean?sd correla-
tions for all purely image-based results pooled over
lobes (3 models * 4 lobes) was .42?.08 in com-
parison to .49?.08 for the joint models. The rel-
ative performance of Object vs. Context vs. Ob-
ject&Context on the four different lobes is preserved
between image-based and joint models: correlating
the 12 combinations using Spearman?s correlation
gives ?=.85, p <.001. Differences can be statis-
tically quantified by pooling all image related cor-
relation coefficients for each anatomical region (3
models * 4 regions), as for the respective joint mod-
els, and comparing with a 2-tailed Wilcoxon signed
rank test. Differences were highly significant (W=0,
p <.001,n=12). This evidence accumulates to sug-
Figure 1: Similarity (Pearson correlation) between each
category pair in (top) occipital and (bottom) frontal lobes.
gest that text and image-derived semantic informa-
tion can be complementary in interpreting concept
fMRI data.
5.2 Word-level analyses
Do image models capture word pair similari-
ties? Per-word results generally corroborate the
relationships observed in the previous section in
the sense that Spearman?s correlation between per-
word and per-category results for the 40 combina-
tions of models and lobes was ?=.78, p <.001.
There were differences, most obviously a dramatic
drop in the strength of correlation coefficients for
the per-word results, visible in Table 3. Subsets
1967
of per-word image-based models correlated with
three lobes and the whole brain. Correlations corre-
sponding to significance values of p <.05 were ob-
served in the temporal and parietal lobes, for Con-
text, Object&Context and Window2 whereas Ob-
ject was correlated with the occipital and temporal
lobes (p <.05). 2-way ANOVA without replica-
tion was used to test for differences between mod-
els and lobes. This revealed a significant differ-
ence between models (F(4,12)=4.05, p=.027). Post-
hoc t-tests showed that the Window2 model signifi-
cantly differed from (was stronger than) the Context
(t=3.8, p =.03, df=3) and Object&Context models
(t =4.5, p =.02, df=3). There were no other signifi-
cant differences between models. There was again a
significant difference between lobes (F(3,12)=7.89,
p < .01), with the frontal lobe showing the weak-
est correlations. Post-hoc 2-tailed t-tests comparing
lobe-pairs found that the frontal lobe differed signif-
icantly (correlations were weaker) from the parietal
(t =-9, p <.001, df=4) and temporal lobes (t =-6.4,
p <.01, df=4) but not from the occipital lobe (t =-
2.18, p =.09, df=4). No other significant differences
between lobes were observed.
Are there differences between models/lobes?
Word-level inter-correlations between lobes were all
significant and the pattern of differences in correla-
tion strength largely resembled that of the category-
level analyses. The occipital lobe was again most
similar to the temporal lobe (?=.57, p <.001), but
less so to the parietal and frontal lobes (?=.47,
p <.001 and ?=.34, p <.001 respectively). The
temporal lobe this time showed stronger correlation
to the parietal (?=.68, p <.001) and frontal lobes
(?=.61, p <.001) than the occipital lobe. The frontal
and parietal lobes were again strongly related to one
another (?=.67, p <.001). These results echo the
category-level findings, that word-level brain activ-
ity is also organised in a similar way across lobes.
Consequently this diminishes our chances of uncov-
ering neat interactions between models and brain ar-
eas (where for instance the Window2 model corre-
lates with the frontal lobe and Object model matches
the occipital lobe). It is however noteworthy that
we can observe some interpretable selectivity in
lobe*model combinations. In particular the Con-
text model better matches the parietal lobe than the
Object model, which in turn better captures the oc-
cipital and temporal lobes (Observations are quali-
tative). Also as we see next, adding text informa-
tion boosts performance in both parietal and tempo-
ral lobes (see Section 2 on our expectations about
information encoded in the lobes).
Does joining text and image models help word-
level interpretation? As concerns the benefits of
joining Text and Image information, per-word joint
models were generally stronger than the respective
image-based models. There was one exception:
adding text to the Object model weakened corre-
lation with the occipital lobe. Joint models were
exclusively stronger than Window2 for the tempo-
ral and occipital lobes, and were stronger in 1/3 of
cases for the frontal and parietal lobes. In an anal-
ogous comparison to the per-category analysis, a
Wilcoxon signed rank test was used to examine the
difference made by adding text information to image
models (pooling 3 models over 4 anatomical areas
for both image and joint models). The mean?sd of
image models was .1?.06 whereas for Joint models
it was .15?.07. The difference was highly signifi-
cant (W=1, p <.001, n=12).
6 Conclusion
This study brought together, for the first time, two
recent research lines: The exploration of ?seman-
tic spaces? in the brain using distributional semantic
models extracted from corpora, and the extension
of the latter to image-based features. We showed
that image-based distributional semantic measures
significantly correlate with fMRI-based neural sim-
ilarity patterns pertaining to categories of concrete
concepts as well as concrete basic-level concepts ex-
pressed by specific words (although correlations, es-
pecially at the basic-concept level, are rather low,
which might signify the need to develop still more
performant distributional models and/or noise inher-
ent to neural data). Moreover, image-based mod-
els complement a state-of-the-art text-based model,
with the best performance achieved when the two
modalities are combined. This not only presents an
optimistic outlook for the future use of image-based
models as an interpretative tool to explore issues of
cognitive grounding, but also demonstrates that they
are capturing useful additional aspects of meaning to
1968
the text models, which are likely relevant for com-
putational semantic tasks.
The weak comparative performance of the origi-
nal Mitchell et al?s Verb model is perhaps surprising
given its previous success in prediction (Mitchell et
al., 2008), but a useful reminder that a good predic-
tor does not necessarily have to capture the internal
structure of the data it predicts.
The lack of finding organisational differences be-
tween anatomical regions differentially described by
the various models is perhaps disappointing, but not
uncontroversial, given that the dataset was not origi-
nally designed to tease apart visual information from
linguistic context. It is however interesting that
in the more challenging word-level analysis some
meaningful trend was visible. In future experiments
it may prove valuable to configure a fMRI stimulus
set where text-based and image-based interrelation-
ships are maximally different. Collecting our own
fMRI data will also allow us to move beyond ex-
ploratory analysis, to test sharper predictions about
distributional models and their brain area correlates.
There are also many opportunities for focusing anal-
yses on different subsets of brain regions, with the
semantic system identified by Binder et al (2009) in
particular presenting one interesting avenue for in-
vestigation.
Acknowledgments
This research was partially funded by a Google Re-
search Award to the fifth author.
References
Shane Bergsma and Randy Goebel. 2011. Using visual
information to predict lexical preference. In Proceed-
ings of RANLP, pages 399?405, Hissar, Bulgaria.
Jeffrey R. Binder, Rutvik H. Desai, William W. Graves,
and Lisa L. Conant. 2009. Where is the semantic
system? a critical review and meta-analysis of 120
functional neuroimaging studies. Cerebral Cortexl,
12:2767?2796.
Vicki Bruce, Patrick R Green, and Georgeson Mark A.
2003. Visual perception: Physiology, psychology, and
ecology. Psychology Pr.
Elia Bruni, Giang Binh Tran, and Marco Baroni. 2011.
Distributional semantics from text and images. In Pro-
ceedings of the EMNLP GEMS Workshop, pages 22?
32, Edinburgh, UK.
Elia Bruni, Gemma Boleda, Marco Baroni, and
Nam Khanh Tran. 2012a. Distributional semantics in
Technicolor. In Proceedings of ACL, pages 136?145,
Jeju Island, Korea.
Elia Bruni, Jasper Uijlings, Marco Baroni, and Nicu
Sebe. 2012b. Distributional semantics with eyes: Us-
ing image analysis to improve computational represen-
tations of word meaning. In Proceedings of ACM Mul-
timedia, pages 1219?1228, Nara, Japan.
Elia Bruni, Ulisse Bordignon, Adam Liska, Jasper Ui-
jlings, and Irina Sergienya. 2013. Vsem: An open li-
brary for visual semantics representation. In Proceed-
ings of ACL, Sofia, Bulgaria.
John Bullinaria and Joseph Levy. 2007. Extract-
ing semantic representations from word co-occurrence
statistics: A computational study. Behavior Research
Methods, 39:510?526.
Kai-min Chang, Tom Mitchell, and Marcel Just. 2011.
Quantitative modeling of the neural representation of
objects: How semantic feature norms can account for
fMRI activation. NeuroImage, 56:716?727.
Linda L Chao, James V Haxby, and Alex Martin. 1999.
Attribute-based neural substrates in temporal cortex
for perceiving and knowing about objects. Nature neu-
roscience, 2(10):913?919.
Kenneth Church and Peter Hanks. 1990. Word asso-
ciation norms, mutual information, and lexicography.
Computational Linguistics, 16(1):22?29.
Stephen Clark. 2012. Vector space models of lexical
meaning. In Shalom Lappin and Chris Fox, editors,
Handbook of Contemporary Semantics, 2nd edition.
Blackwell, Malden, MA. In press.
Gabriella Csurka, Christopher Dance, Lixin Fan, Jutta
Willamowski, and Ce?dric Bray. 2004. Visual cate-
gorization with bags of keypoints. In In Workshop on
Statistical Learning in Computer Vision, ECCV, pages
1?22, Prague, Czech Republic.
Jia Deng, Wei Dong, Richard Socher, Lia-Ji Li, and
Li Fei-Fei. 2009. Imagenet: A large-scale hierarchi-
cal image database. In Proceedings of CVPR, pages
248?255, Miami Beach, FL.
Russell A Epstein. 2008. Parahippocampal and ret-
rosplenial contributions to human spatial navigation.
Trends in cognitive sciences, 12(10):388?396.
Katrin Erk. 2012. Vector space models of word meaning
and phrase meaning: A survey. Language and Lin-
guistics Compass, 6(10):635?653.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge, MA.
Yansong Feng and Mirella Lapata. 2010. Visual infor-
mation in semantic representation. In Proceedings of
HLT-NAACL, pages 91?99, Los Angeles, CA.
1969
Melvyn A. Goodale and David Milner. 1992. Separate
visual pathways for perception and action. Trends in
Neurosciences, 15:20?25.
Kristen Grauman and Bastian Leibe. 2011. Visual Object
Recognition. Morgan & Claypool, San Francisco.
Peter Hagoort. 2005. On Broca, brain, and bind-
ing: a new framework. Trends in cognitive sciences,
9(9):416?423.
James Haxby, Ida Gobbini, Maura Furey, Alumit Ishai,
Jennifer Schouten, and Pietro Pietrini. 2001. Dis-
tributed and overlapping representations of faces and
objects in ventral temporal cortex. Science, 293:2425?
2430.
Alexander Huth, Shinji Nishimoto, An Vu, and Jack Gal-
lant. 2012. A continuous semantic space describes the
representation of thousands of object and action cate-
gories across the human brain. Neuron, 76(6):1210?
1224.
Nancy Kanwisher and Galit Yovel. 2006. The fusiform
face area: a cortical region specialized for the percep-
tion of faces. Philosophical Transactions of the Royal
Society B: Biological Sciences, 361(1476):2109?2128.
Nikolaus Kriegeskorte, Marieke Mur, and Peter Ban-
dettini. 2008. Representational similarity analysis?
connecting the branches of systems neuroscience.
Frontiers in systems neuroscience, 2.
Svetlana Lazebnik, Cordelia Schmid, and Jean Ponce.
2006. Beyond bags of features: Spatial pyramid
matching for recognizing natural scene categories. In
Proceedings of CVPR, pages 2169?2178, Washington,
DC.
Chee Wee Leong and Rada Mihalcea. 2011. Going
beyond text: A hybrid image-text approach for mea-
suring word relatedness. In Proceedings of IJCNLP,
pages 1403?1407.
Max Louwerse. 2011. Symbol interdependency in sym-
bolic and embodied cognition. Topics in Cognitive
Science, 3:273?302.
David G Lowe. 2004. Distinctive Image Features from
Scale-Invariant Keypoints. International Journal of
Computer Vision, 60:91?110.
Kevin Lund and Curt Burgess. 1996. Producing
high-dimensional semantic spaces from lexical co-
occurrence. Behavior Research Methods, 28:203?208.
Bruce D McCandliss, Laurent Cohen, and Stanislas De-
haene. 2003. The visual word form area: expertise
for reading in the fusiform gyrus. Trends in cognitive
sciences, 7(7):293?299.
Robert D McIntosh and Thomas Schenk. 2009. Two vi-
sual streams for perception and action: Current trends.
Neuropsychologia, 47(6):1391?1396.
Earl K Miller, David J Freedman, and Jonathan D Wal-
lis. 2002. The prefrontal cortex: categories, con-
cepts and cognition. Philosophical Transactions of
the Royal Society of London. Series B: Biological Sci-
ences, 357(1424):1123?1136.
Tom Mitchell, Svetlana Shinkareva, Andrew Carlson,
Kai-Min Chang, Vincente Malave, Robert Mason, and
Marcel Just. 2008. Predicting human brain activ-
ity associated with the meanings of nouns. Science,
320:1191?1195.
Brian Murphy, Partha Talukdar, and Tom Mitchell. 2012.
Selecting corpus-semantic models for neurolinguistic
decoding. In Proceedings of *SEM, pages 114?123,
Montreal, Canada.
Mark Palatucci, Dean Pomerleau, Geoffrey Hinton, and
Tom Mitchell. 2009. Zero-shot learning with seman-
tic output codes. In Proceedings of NIPS, pages 1410?
1418, Vancouver, Canada.
Marius V Peelen and Paul E Downing. 2005. Selectivity
for the human body in the fusiform gyrus. Journal of
Neurophysiology, 93(1):603?608.
Francisco Pereira, Greg Detre, and Matthew Botvinick.
2011. Generating text from functional brain images.
Frontiers in Human Neuroscience, 5(72). Published
online: http://www.frontiersin.org/
human_neuroscience/10.3389/fnhum.
2011.00072/abstract.
Alexander T Sack. 2009. Parietal cortex and spatial cog-
nition. Behavioural brain research, 202(2):153?161.
Hinrich Schu?tze. 1997. Ambiguity Resolution in Natural
Language Learning. CSLI, Stanford, CA.
Carina Silberer and Mirella Lapata. 2013. Models of
semantic representation with visual attributes. In Pro-
ceedings of ACL, Sofia, Bulgaria.
Josef Sivic and Andrew Zisserman. 2003. Video Google:
A text retrieval approach to object matching in videos.
In Proceedings of ICCV, pages 1470?1477, Nice,
France.
Peter Turney and Patrick Pantel. 2010. From frequency
to meaning: Vector space models of semantics. Jour-
nal of Artificial Intelligence Research, 37:141?188.
N Tzourio-Mazoyer, B Landeau, D Papathanassiou,
F Crivello, O Etard, N Delcroix, B Mazoyer, and M Jo-
liot. 2002. Automated anatomical labeling of activa-
tions in SPM using a macroscopic anatomical parcel-
lation of the MNI MRI single-subject brain. Neuroim-
age, 15(1):273?289.
1970
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 279?287,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
Identifying fake Amazon reviews as learning from crowds
Tommaso Fornaciari
Ministero dell?Interno
Dipartimento della Pubblica Sicurezza
Segreteria del Dipartimento
ComISSIT
tommaso.fornaciari@interno.it
Massimo Poesio
University of Essex
CSEE
University of Trento
CIMeC
poesio@essex.ac.uk
Abstract
Customers who buy products such as
books online often rely on other customers
reviews more than on reviews found on
specialist magazines. Unfortunately the
confidence in such reviews is often mis-
placed due to the explosion of so-called
sock puppetry?authors writing glowing
reviews of their own books. Identifying
such deceptive reviews is not easy. The
first contribution of our work is the cre-
ation of a collection including a number
of genuinely deceptive Amazon book re-
views in collaboration with crime writer
Jeremy Duns, who has devoted a great
deal of effort in unmasking sock puppet-
ing among his colleagues. But there can
be no certainty concerning the other re-
views in the collection: all we have is a
number of cues, also developed in collab-
oration with Duns, suggesting that a re-
view may be genuine or deceptive. Thus
this corpus is an example of a collection
where it is not possible to acquire the
actual label for all instances, and where
clues of deception were treated as anno-
tators who assign them heuristic labels. A
number of approaches have been proposed
for such cases; we adopt here the ?learn-
ing from crowds? approach proposed by
Raykar et al. (2010). Thanks to Duns? cer-
tainly fake reviews, the second contribu-
tion of this work consists in the evaluation
of the effectiveness of different methods of
annotation, according to the performance
of models trained to detect deceptive re-
views.
1 Introduction
Customer reviews of books, hotels and other prod-
ucts are widely perceived as an important rea-
son for the success of e-commerce sites such as
amazon.com or tripadvisor.com. How-
ever, customer confidence in such reviews is often
misplaced, due to the growth of the so-called sock
puppetry phenomenon: authors / hoteliers writing
glowing reviews of their own works / hotels (and
occasionally also negative reviews of the competi-
tors).
1
The prevalence of this phenomenon has
been revealed by campaigners such as crime writer
Jeremy Duns, who exposed a number of fellow au-
thors involved in such practices.
2
A number of
sites have also emerged offering Amazon reviews
to authors for a fee.
3
Several automatic techniques for exposing such
deceptive reviews have been proposed in recent
years (Feng et al., 2012; Ott et al., 2001). But like
all work on deceptive language (computational or
otherwise) (Newman et al., 2003; Strapparava and
Mihalcea, 2009), such works suffer from a seri-
ous problem: the lack of a gold standard contain-
ing ?real life? examples of deceptive uses of lan-
guage. This is because it is very difficult to find
definite proof that an Amazon review is either de-
ceptive or genuine. Thus most researchers recre-
ate deceptive behavior in the lab, as done by New-
man et al. (2003). For instance, Ott et al. (2001),
Feng et al. (2012) and Strapparava and Mihalcea
(2009) used crowdsourcing, asking turkers to pro-
duce instances of deceptive behavior. Finally, Li
et al. (2011) classify reviews as deceptive or truth-
ful by hand on the basis of a series of heuristics:
they start by excluding anonymous reviews, then
use their helpfulness and other criteria to decide
1
The phenomenon predates Internet - see e.g., Amy Har-
mon, ?Amazon Glitch Unmasks War Of Reviewers?, New
York Times, February 14, 2004.
2
See Andrew Hough, ?RJ Ellory: fake book reviews
are rife on internet, authors warn?, telegraph.co.uk,
September 3, 2012
3
See Alison Flood, ?Sock puppetry and fake reviews:
publish and be damned?, guardian.co.uk, September 4,
2012 and David Streitfeld, ?Buy Reviews on Yelp, Get Black
Mark?, nytimes.com, October 18, 2012.
279
whether they are deceptive or not. Clearly a more
rigorous approach to establishing the truth or oth-
erwise of reviews on the basis of such heuristic
criteria would be useful.
In this work we develop a system for identify-
ing deceptive reviews in Amazon. Our proposal
makes two main contributions:
1. we identified in collaboration with Jeremy
Duns a series of criteria used by Duns and
other ?sock puppet hunters? to find suspicious
reviews / reviewers, and collected a dataset of
reviews some of which are certainly false as
the authors admitted so, whereas others may
be genuine or deceptive.
2. we developed an approach to the truthful-
ness of reviews based on the notion that the
truthfulness of a review is a latent variable
whose value cannot be known, but can be es-
timated using some criteria as potential indi-
cators of such value?as annotators?and then
we used the learning from crowds algorithm
proposed by Raykar et al. (2010) to assign a
class to each review in the dataset.
The structure of the paper is as follows. In Sec-
tion 2 we describe how we collected our dataset;
in Section 3 we show the experiments we carried
out and in Section 4 we discuss the results.
2 Deception clues and dataset
2.1 Examples of Unmasked Sock Puppetry
After reading an article by Alison Flood on The
Guardian of September 4th, 2012
4
, discussing
how crime writer Jeremy Duns had unmasked a
number of ?sock puppeteers,? we contacted him.
Duns was extremely helpful; he pointed us to the
other articles on the topic, mostly on The New York
Times, and helped us create a set of deception
clues and the dataset used in this work.
On July 25
th
, 2011, an article appeared on
www.moneytalksnews.com, entitled ?3 Tips
for Spotting Fake Product Reviews - From Some-
one Who Wrote Them?.
5
Sandra Parker, author
of the text, in that page described her experience
as ?professional review writer?. According to her
4
Sock puppetry and fake reviews: publish and be damned,
http://www.guardian.co.uk/books/2012/
sep/04/sock-puppetry-publish-be-damned
5
http://www.moneytalksnews.com/2011/07/25
/3-tips-for-spotting-fake-product-reviews--
-from-someone-who-wrote-them/
statements, advertising agencies were used to pay
her $10-20 for writing reviews on sites like Ama-
zon.com. She was not asked to lie, but ?if the re-
view wasn?t five star, they didn?t pay?. In an arti-
cle of August 19
th
, written by David Streitfeld on
www.nytimes.com,
6
she actually denied that
point: ?We were not asked to provide a five-star
review, but would be asked to turn down an as-
signment if we could not give one?.
In any case, in her article Sandra Parker gave
the readers some common sense-based advices, in
order to help them to recognize possible fake re-
views. One of these suggestions were also useful
for this study, as discussed in Section 2.3. From
our point of view, however, the most interesting
aspect of the article relied in the fact that, letting
know the name of an author of fake reviews, it
made possible to identify them in Amazon.com,
with an high degree of confidence.
A further article written on August 25
th
by
David Streitfeld gave us another similar opportu-
nity.
7
In fact, thanks to his survey, it was possible
to come to know the titles of four books, whose the
authors paid an agency in order to receive reviews.
2.2 The corpus
Using the suggestions of Jeremy Duns and the in-
formation in these articles we built a corpus we
called DEREV (DEception in REViews), consist-
ing of clearly fake, possibly fake, and possibly
genuine book reviews posted on www.amazon.
com. The corpus, which will be freely available
on demand, consists of 6819 reviews downloaded
from www.amazon.com, concerning 68 books
and written by 4811 different reviewers. The 68
books were chosen trying to balance the number
of reviews (our units of analysis) related to sus-
pect books which probably or surely received fake
reviews, with the number of reviews hypothesized
to be genuine in that we expected the authors of
the books not to have bought reviews. In partic-
ular, we put into the group of the suspect books -
henceforth SB - the reviews of the four books in-
dicated by David Streitfeld. To this first nucleus,
we also added other four books, written by three
of the authors of the previous group. We also in-
6
http://www.nytimes.com/2011/08/20/
technology/finding-fake-reviews-online.
html?_r=1&
7
http://www.nytimes.com/2012/08/26/busin
ess/book-reviewers-for-hire-meet-a-demand-
for-online-raves.html?pagewanted=all
280
cluded in the SB group the 22 books for which
Sandra Parker wrote a review. Lastly, we noticed
that some reviewers of the books pointed out by
David Streitfeld tended to write reviews of the
same books: we identified 16 of them, and consid-
ered suspect as well. In total, on November 17
th
,
2011 we downloaded the reviews of 46 books con-
sidered as suspect, which received 2707 reviews.
8
We also collected the reviews of 22 so called ?in-
nocent books?, for a total of 4112 reviews. These
books were mainly chosen among classic authors,
such as Conan Doyle or Kipling, or among liv-
ing writers who are so renowned that any reviews?
purchase would be pointless: this is the case, for
example, of Ken Follett and Stephen King. As
shown by the number of the reviews, the books
of these authors are so famous that they receive a
great amount of readers? opinions.
The size of DEREV is 1175410 tokens, con-
sidering punctuation blocks as single token. The
mean size of the reviews is 172.37 tokens. The ti-
tles of the reviews were neither included in these
statistics nor in the following analyses.
2.3 Deception clues
Once created the corpus, we identified a set of
clues, whose presence suggested the deceptiveness
of the reviews. These clues are:
Suspect Book - SB The first clue of deceptive-
ness was the reference of the reviews to a sus-
pect book, identified as described above. This
is the only clue which is constant for all the
reviews of the same book.
Cluster - Cl The second clue comes from the
suggestions given by Sandra Parker in her
mentioned article. As she pointed out, the
agencies she worked for were used to give her
48 hours to write a review. Being likely that
the same deadline was given to other review-
ers, Sandra Parker warns to pay attention if
the books receive many reviews in a short pe-
riod of time. Following her advice, we con-
sidered as positive this clue of deceptiveness
if the review belonged to a group of at least
two reviews posted within 3 days.
Nickname - NN A service provided by Amazon
is the possibility for the reviewers to register
8
We specify the date of the download because, obviously,
if the data collection would be repeated today, the overall
number of reviews would be greater.
in the website and to post comments using
their real name. Since the real identity of the
reviewers involves issues related to their rep-
utation, we supposed it is less probable that
the writers of fake reviews post their texts us-
ing their true name. Moreover, a similar as-
sumption was probably accepted by Li et al.
(2011), who considered the profile features of
the reviewers, and among them the use or not
of their real name.
Unknown Purchase - UP Lastly, the probably
most interesting information provided by
Amazon is whether the reviewer bought the
reviewed book through Amazon itself. It
is reasonable to think that, if the reviewer
bought the book, he also read it. Therefore,
the absence of information about the certified
purchase was considered a clue of deceptive-
ness.
2.4 Gold and silver standard
The clues of deception discussed above give us
a heuristic estimate of the truthfulness of the re-
views. Such estimation represents a silver stan-
dard of our classes, as these are not determined
through certain knowledge of the ground truth, but
simply thanks to hints of deceptiveness. The meth-
ods we used in order to assign the heuristic classes
to the reviews are described in the next Section;
however for our purposes we needed a gold stan-
dard, that is at least a subset of reviews whose
ground truth was known with a high degree of con-
fidence. This subset was identified as follows.
First, we considered as false the 22 reviews
published by Sandra Parker, even though not all
her reviews are characterized by the presence of
all the deception clues. Even though we cannot
really say whether her reviews reflect her opin-
ion of the books in question or not, she explic-
itly claimed to have been paid for writing them;
and she only bought on Amazon three of these
22 books. This is the most accurate knowledge
about fake reviews not artificially produced we
have found in literature. Then we focused on the
four books whose authors admitted to have bought
the reviews.
9
Three of them received many re-
views, which made it difficult to understand if
they were truthful or not. However, one of these
9
http://www.nytimes.com/2012/08/26/busin
ess/book-reviewers-for-hire-meet-a-demand-
for-online-raves.html?pagewanted=all
281
Table 1: The distribution of deception clues in the
reviews
Nr. clues Reviews Tot. %
False 4 903
rev. 3 1913 2816 41.30%
True 2 2528
rev. 1 1210
0 265 4003 58.70%
books (?Write your first book?, by Peter Biadasz)
received only 20 reviews, which therefore could
be considered as fake with high degree of proba-
bility. Even though we have no clear evidence that
a small number of reviews correlates with a greater
likelihood of deception, since we know this book
received fake reviews, and there are only few re-
views for it, we felt it is pretty likely that those
are fake. Therefore we examined the reviews writ-
ten by these twenty authors, and considered as
false only those showing the presence of all the
deception clues described above. In this way, we
found 96 reviews published by 14 reviewers, and
we added them to the 22 of Sandra Parker, for a
total of 118 reviews written by 15 authors.
Once identified this subset of fake reviews, we
selected other 118 reviews which did not show
the presence of any deception clue, that is chosen
from books above any suspicion, written by au-
thors who published the review having made use
of their real name and having bought the book
through Amazon and so on.
In the end, we identified a subset of DEREV
constituted by 236 reviews, whose class was
known with high degree of confidence and con-
sidered them as our gold standard.
3 Experiments
We carried out two experiments, in which the
classes assigned to the reviews of DEREV were
found adopting two different strategies. In the first
experiment the classes of the reviews were de-
termined using majority voting of our deception
clues. This experiment is thus conceptually simi-
lar to those of Li et al. (2011), who trained models
using supervised methods with the aim of identi-
fying fake reviews. We discuss this experiment in
the next Section. In the second experiment, learn-
ing from crowds was used (Raykar et al., 2010).
This approach is discussed in Section 3.2.1.
In both experiments we carried out a 10-fold
cross-validation where in each iteration feature se-
lection and training were carried out using 90% of
the part of the corpus with only silver standard an-
notation and 90% of the subset with gold. The test
set used in each iteration consisted of the remain-
ing tenth of reviews with gold standard classes,
which were employed in order to evaluate the pre-
dictions of the models. This allowed to estimate
the efficiency of the strategies we used to deter-
mine our silver standard classes.
3.1 Majority Voting
3.1.1 Determining the class of reviews by
majority voting
The deception clues discussed in Section 2.3 were
used in our first experiment to identify the class of
each review using majority voting. In other words,
those clues were considered as independent pre-
dictors of the class; the class predicted by the ma-
jority of the annotators/clues was assigned to the
review. Specifically, if 0, 1 or 2 deception clues
were found, the review was classified as true; if
there were 3 or 4, the review was considered false.
Table 1 shows the distribution of the number of
deception clues in the reviews in DEREV.
3.1.2 Feature selection
In both experiments each review was represented
as feature vector. The features were just of uni-
grams, bigrams and trigrams of lemmas and part-
of-speech (POS), as collected from the reviews
through TreeTagger
10
(Schmid, 1994).
Since in each experiment we applied a 10-fold
cross-validation, in every fold the features were
extracted from the nine-tenths of DEREV em-
ployed as training set. Once identified the train-
ing set, we computed the frequency lists of the
n-grams of lemmas and POS. The lists were col-
lected separately from the reviews belonging to
the class ?true? and to the class ?false?. Such sep-
aration was aimed to take into consideration the
most highly frequent n-grams of both genuine and
fake reviews. However, for the following steps of
the feature selection, only the n-grams which ap-
peared more than 300 times in every frequency list
were considered: a threshold empirically chosen
for ease of calculations. In fact, among the most
10
http://www.ims.uni-stuttgart.
de/projekte/corplex/TreeTagger/
DecisionTreeTagger.html
282
Table 2: The most frequent n-grams collected
N-grams Lemmas POS Total
Unigrams 34 21
Bigrams 21 13
Trigrams 13 8
Total 68 42 110
frequents, in order to identify the features most
effective in discriminating the two classes of re-
views, the Information Gain (IG) of the selected n-
grams was computed (Kullback and Leibler, 1951;
Yang and Pedersen, 1997).
Then, after having found the Information Gain
of the n-grams of lemmas and part-of-speech, a
further reduction of the features was realized. In
fact, we selected a relatively small amount of fea-
tures, in order to facilitate the computation of the
Raykar et al.?s algorithm (discussed in Sub-section
3.2.1), and only the n-grams with the highest IG
values were selected to be taken as features of the
vectors which represented the reviews. In par-
ticular, the n-grams were collected according to
the scheme shown in Table 2. By the way, 8,
13, 21 and 34 are numbers belonging to the Fi-
bonacci series (Sigler, 2003). They were chosen
because they grow exponentially and are used, in
our case, to give wider representation to the short-
est n-grams.
Lastly, two more features were added to the fea-
ture set, that is the length of the review, considered
with and without punctuation. Therefore, in each
fold of the experiment, the vectors of the reviews
were constituted by 112 values: 2 corresponding
to the length of the review, and 110 representing
the (not normalized) frequency, into the review it-
self, of the selected n-grams of lemmas and POS.
3.1.3 Baselines
The best way to assess the improvement coming
from the algorithm would have been with respect
to a supervised baseline. However this was not
possible as we could only be certain regarding the
classification of a fraction of the reviews (our gold
standard: 236 reviews, for a total of about 23,000
tokens). We felt such a small dataset could not be
used for training, but only for evaluation; therefore
we used instead two simple heuristic baselines.
Majority baseline. The simplest metric for per-
formance evaluation is the majority baseline: al-
ways assign to a review the class most represented
in the dataset. Since in the subset of DEREV with
gold standard we had 50% of true and false re-
views, simply 50% is our majority baseline.
Random baseline. Furthermore, we estimated a
random baseline through a Monte Carlo simula-
tion. This kind of simulation allows to estimate the
performance of a classifier which performs several
times a task over random outputs whose distribu-
tion reflects that of real data.
In particular, for this experiment, since we had
236 reviews whose 50% were labeled as false,
100000 times we produced 236 random binomial
predictions, having p = .5. In each simulation,
the random prediction was compared with our real
data. It turned out that in less than .01% of tri-
als the level of 62.29% of correct predictions was
exceeded. The thresholds for precision and recall
in detecting deceptive reviews were 62.26% and
66.95% respectively.
3.1.4 Models
We tested a number of supervised learning meth-
ods to learn a classifier using the classes deter-
mined by majority voting, but the best results
were obtained using Support Vector Machines
(SVMs) (Cortes and Vapnik, 1995), already em-
ployed in many applications involving text classi-
fication (Yang and Liu, 1999).
3.1.5 Results
The results obtained by training a supervised clas-
sifier over the dataset with classes identified with
majority voting are shown in the Table 3. The
highest results are in bold. The methodological
approach and performance achieved in this exper-
iment seems to be comparable to that of Strappar-
ava and Mihalcea (2009) and, more recently, of Li
et al. (2011). However Li et al. (2011) evaluate the
effectiveness of different kind of features with the
aim of annotating unlabeled data, while we try to
evaluate the reliability of heuristic classes in train-
ing.
3.2 Learning from Crowds
3.2.1 The Learning from Crowds algorithm
As pointed out by Raykar et al. (2010), major-
ity voting is not necessarily the most effective
way to determine the real classes in problems like
283
Table 3: The experiment with the majority voting classes
Correctly Incorrectly Precision Recall F-measure
classified reviews classified reviews
False reviews 75 43 83.33% 63.56% 72.12%
True reviews 103 15
Total 178 58
Total accuracy 75.42%
Random baseline 62.29% 62.26% 66.95%
those of reviews where there is no gold standard.
This is because annotators are not equally reli-
able, and the reviews are not equally challenging.
Hence the output of the majority voting may be af-
fected by unevaluated biases. To address this prob-
lem, Raykar et al. (2010) presented a maximum-
likelihood estimator that jointly learns the classi-
fier/regressor, the annotator accuracy, and the ac-
tual true label.
For ease of exposition, Raykar et al. (2010) use
as classifier the logistic regression, even though
they specify their algorithm would work with any
classifier. In case of logistic regression, the prob-
ability for an entity x ? X of belonging to a class
y ? Y with Y = {1, 0} is a sigmoid function
of the weight vector w of the features of each in-
stance x
i
, that is p[y = 1|x,w] = ?(w
>
x), where,
given a threshold ?, the class y = 1 if w
>
x ? ?.
Annotators? performance, then, is evaluated ?in
terms of the sensitivity and specificity with respect
to the unknown gold standard?: in particular, in a
binary classification problem, for the annotator j
the sensitivity ?
j
is the rate of positive cases iden-
tified by the annotator ?i.e., the recall of positive
cases? while the specificity ?
j
is the annotator?s
recall of negative cases.
Given a dataset D constituted of indepen-
dently sampled entities, a number of annotators
R, and the relative parameters ? = {w,?, ?},
the likelihood function which needs to be maxi-
mized, according to Raykar et al. (2010), would
be p[D|?] =
?
N
i=1
p[y
1
i
, ...y
R
i
|x
i
, ?], and the
maximum-likelihood estimator is obtained by
maximizing the log-likelihood, that is
?
?
ML
= {??,
?
?, w?} = argmax
?
{ln p[D|?]}. (1)
Raykar et al. (2010) propose to solve this max-
imization problem (Bickel and Doksum, 2000)
through the technique of Expectation Maximiza-
tion (EM) (Dempster et al., 1977). The EM al-
gorithm can be used to recover the parameters of
the hidden distributions accounting for the distri-
bution of data. It consists of two steps, an Expecta-
tion step (E-step) followed by a Maximization step
(M-step), which are iterated until convergence.
During the E-step the expectation of the term y
i
is
computed starting from the current estimate of the
parameters. In the M-step the parameters ? are up-
dated by maximizing the conditional expectation.
Regarding the third parameter, w, Raykar et al.
(2010) admit there is not a closed form solution
and suggest to use the Newton-Raphson method.
3.2.2 Determining the class of reviews using
Learning from Crowds
In order to apply Raykar?s algorithm, we pro-
ceeded as follows. First, we applied the procedure
for feature selection described in Subsection 3.1.2
to create a single dataset: that is, the corpus was
not divided in folds, but the feature selection in-
volved all of DEREV. This dataset was built using
the classes resulting from the majority voting ap-
proach and included these columns:
? The class assignments of the four clues dis-
cussed in Sub-section 2.3 ? SB, Cl, NN, UP;
? The majority voting class;
? The 112 features identified according to the
procedure presented in Sub-section 3.1.2.
Then, we implemented the algorithm proposed
by Raykar et al. (2010) in R.
11
We computed a Lo-
gistic Regression (Gelman and Hill, 2007) on the
dataset to compute the weight vectorw, used to es-
timate for each instance the probability p
i
for the
review of belonging to the class ?true?. For the lo-
gistic regression we used the 112 surface features
11
http://www.r-project.org/
284
Table 4: The experiment with Raykar et al.?s algorithm classes
Correctly Incorrectly Precision Recall F-measure
classified reviews classified reviews
False reviews 85 33 78.70% 72.03% 75.22%
True reviews 95 23
Total 180 56
Total accuracy 76.27%
Random baseline 62.29% 62.26% 66.95%
mentioned above, adopting as class the majority
voting, as suggested by Raykar et al. (2010).
The parameters ? and ? were estimated regard-
ing the three clues Cl - Cluster, NN - Nickname
and UP - Unknown Purchase. The attribute SB -
Suspect Book was not used, in order to carry out
the EM algorithm exclusively on heuristic data, re-
moving the information obtained through sources
external to the dataset. The parameters ? and ?
of the three clues were obtained not from ran-
dom classes, as the EM algorithm would allow, but
again comparing the clues? labels with the major-
ity voting class. In fact, aware of the local maxi-
mum problem of EM, in this way we tried to en-
hance the reliability of the results posing a config-
uration which could be, at least theoretically, bet-
ter than a completely random one.
Knowing these values for each instance of the
dataset, we computed the E-step and we updated
our parameters in M-step.
The E-step and the M-step were iterated 100
times, in which the log-likelihood increases mono-
tonically, indicating a convergence to a local max-
imum.
The final value of p
i
determined the new class of
each instance: if p
i
> .5 the review was labeled as
true, otherwise as false. In the end, the EM clus-
terization allowed to label 3267 reviews as false
and 3552 as true, that is 47.91% and 52.09% of
DEREV respectively.
3.2.3 Feature selection
The feature selection for this experiment was ex-
actly the same presented for the previous one in
Sub-section 3.1.2; the only, fundamental differ-
ence was that in the first experiment the classes
derived from the majority voting rule, while in
the second experiment the classes were identified
through the Raykar et al.?s strategy.
3.2.4 Baselines
As in the first experiment, we compared the per-
formance of the models with the same majority
and random baselines discussed in Sub-section
3.1.3.
3.2.5 Models
We used the classes determined through the Learn-
ing by Crowds algorithm to train SVMs models,
with the same settings employed in the first exper-
iments.
3.2.6 Results
Table 4 shows the results of the classifier trained
over the dataset whose the classes were identified
through the Raykar et al.?s algorithm.
4 Discussion
4.1 Deceptive language in reviews
Of the 4811 reviewers who wrote reviews included
in our corpus, about 900 were anonymous, and
only 16 wrote 10 or more reviews. If, in one hand,
this prevented us from verifying the performance
of the models with respect to particular reviewers,
on the other hand we had the opportunity of evalu-
ating the style in writing reviews across many sub-
jects.
In our experiments, we extracted simple surface
features constituted by short n-grams of lemmas
and part-of-speech. In literature there is evidence
that also other kinds of features are effective in de-
tecting deception in reviews: for example, infor-
mation about the syntactic structures of the texts
(Feng et al., 2012). In our pilot studies we did not
obtain improvements using syntactic features. But
even the frequency of n-grams can provide some
insight regarding deceptive language in reviews;
and with this aim we focused on the unigrams ap-
pearing more than 50 times in the 236 reviews
285
constituting the gold standard of DEREV, whose
un/truthfulness is known. The use of self-referred
pronouns and adjectives is remarkably different in
true and fake reviews: in the genuine ones, the pro-
nouns ?I?, ?my? and ?me? are found 371, 74 and 51
times respectively, while in the fake ones the pro-
noun ?I? is present only 149 and ?me? and ?my?
less than 50 times. This reduced number of self-
references is coherent with the findings of other
well-known studies regarding deception detection
(Newman et al., 2003); however, while in truthful
reviews the pronoun ?you? appears only 84 times,
in the fake ones the frequency of ?you? and ?your?
is 151 and 75. It seems that while the truth-tellers
simple state their opinions, the deceivers address
directly the reader. Probably they tend to give ad-
vice: after all, this is what they are paid for. The
frequency of the word ?read? - that is the activ-
ity simulated in fake reviews - is also quite imbal-
anced: 137 in true reviews and 97 in the fake ones.
Lastly, it is maybe surprising that in the false re-
views terms related to positive feelings/judgments
do not have the highest frequency; instead in truth-
ful reviews we found 52 times the term ?good?
(and 56 times the ambiguous term ?like?): also this
outcome is similar to that of the mentioned study
of Newman et al. (2003).
4.2 Estimating the gold standard
The estimation of the gold standard is a recur-
rent problem in many tasks of text classification
and in particular with deceptive review identifica-
tion, that is an application where the deceptiveness
of the reviews cannot be properly determined but
only heuristically assessed.
In this paper we introduced a new dataset for
studying deceptive reviews, constituted by 6819
instances whose 236 (that is about 3.5% of the cor-
pus) were labeled with the highest degree of confi-
dence ever seen before. We used this subset to test
the models that we trained on the other reviews of
DEREV, whose the class was heuristically deter-
mined.
With this purpose, we adopted two techniques.
First, we simply considered the value of our clues
of deception as outputs of just as many annotators,
and we assigned the classes to each review accord-
ing to majority voting. Then we clustered our in-
stances using the Learning from Crowd algorithm
proposed by Raykar et al. (2010). Lastly we car-
ried out the two experiments of text classification
described above.
The results suggest that both methods achieve
accuracy well above the baseline. However, the
models trained using Learning from Crowd classes
not only achieved the highest accuracy, but also
outperformed the thresholds for precision and re-
call in detecting deceptive reviews (Table 4), while
the models trained with the majority voting classes
showed a very high precision, but at the expense of
the recall, which was lower than the baseline (Ta-
ble 3).
Since the results even with simple majority vot-
ing classes were positive, we carried out two more
experiments, identical to those described above
except that we included in the feature set the three
deception clues Cluster - Cl, Nickname - NN and
Unknown Purchase - UP. Both with majority vot-
ing and with learning from Crowds classes, the ac-
curacy of the models exceeded 97%. This might
seem to suggest that those clues are very effective;
but given that the deception clues were used to de-
rive the silver standard, their use as features could
be considered to some extent circular (Subsection
2.4). Moreover, not all of our non-linguistic cues
may be found in all review scenarios, and therefore
the applicability of our methods to all review sce-
narios will have to be investigated. Specifically,
Cluster is likely to be applicable to most review
domains, Nickname and Unknown Purchase are
Amazon features that may or may not be adopted
by other services allowing users to provide re-
views. However, our main concern was not to
evaluate the effectiveness of these specific clues of
deception, but to investigate whether better strate-
gies for labeling instances than simple majority
voting could be found.
In this perspective, the performance of our
second experiment, in which the Learning from
Crowds algorithm was employed, stands out. In
fact in that case we tried to identify the classes of
the instances abstaining from making use of any
external information regarding the reviews: in par-
ticular, we ignored the Suspect Book - SB clue of
deception which, by contrast, took part in the cre-
ation of the majority voting classes.
This outcome suggests that, even in scenarios
where the gold standard is unknown, the Learning
from Crowds algorithm is a reliable tool for label-
ing the reviews, so that effective models can be
trained in order to classify them as truthful or not.
286
References
Bickel, P. and Doksum, K. (2000). Mathemati-
cal statistics: basic ideas and selected topics.
Number v. 1 in Mathematical Statistics: Basic
Ideas and Selected Topics. Prentice Hall.
Cortes, C. and Vapnik, V. (1995). Support-vector
networks. Machine Learning, 20.
Dempster, A. P., Laird, N. M., and Rubin, D. B.
(1977). Maximum Likelihood from Incomplete
Data via the EM Algorithm. Journal of the
Royal Statistical Society. Series B (Methodolog-
ical), 39(1):1?38.
Feng, S., Banerjee, R., and Choi, Y. (2012). Syn-
tactic stylometry for deception detection. In
Proceedings of the 50th Annual Meeting of the
Association for Computational Linguistics (Vol-
ume 2: Short Papers), pages 171?175, Jeju
Island, Korea. Association for Computational
Linguistics.
Gelman, A. and Hill, J. (2007). Data Analysis
Using Regression and Multilevel/Hierarchical
Models. Analytical Methods for Social Re-
search. Cambridge University Press.
Kullback, S. and Leibler, R. A. (1951). On in-
formation and sufficiency. Ann. Math. Statist.,
22(1):79?86.
Li, F., Huang, M., Yang, Y., and Zhu, X. (2011).
Learning to identify review spam. In Proceed-
ings of the Twenty-Second international joint
conference on Artificial Intelligence-Volume
Volume Three, pages 2488?2493. AAAI Press.
Newman, M. L., Pennebaker, J. W., Berry, D. S.,
and Richards, J. M. (2003). Lying Words:
Predicting Deception From Linguistic Styles.
Personality and Social Psychology Bulletin,
29(5):665?675.
Ott, M., Choi, Y., Cardie, C., and Hancock, J.
(2001). Finding deceptive opinion spam by any
stretch of the imagination. In Proceedings of
the 49th Annual Meeting of the Association for
Computational Linguistics: Human Language
Technologies, pages 309?319, Portland, Ore-
gon, USA. Association for Computational Lin-
guistics.
Raykar, V. C., Yu, S., Zhao, L. H., Valadez, G. H.,
Florin, C., Bogoni, L., and Moy, L. (2010).
Learning from crowds. Journal of Machine
Learning Research, 11:1297?1322.
Schmid, H. (1994). Probabilistic part-of-speech
tagging using decision trees. In Proceedings
of International Conference on New Methods in
Language Processing.
Sigler, L., editor (2003). Fibonacci?s Liber Abaci:
A Translation Into Modern English of Leonardo
Pisano?s Book of Calculation. Sources and
Studies in the History of Mathematics and Phys-
ical Sciences. Springer Verlag.
Strapparava, C. and Mihalcea, R. (2009). The Lie
Detector: Explorations in the Automatic Recog-
nition of Deceptive Language. In Proceed-
ing ACLShort ?09 - Proceedings of the ACL-
IJCNLP 2009 Conference Short Papers.
Yang, Y. and Liu, X. (1999). A re-examination of
text categorization methods. In Proceedings of
the 22nd annual international ACM SIGIR con-
ference on Research and development in infor-
mation retrieval, SIGIR ?99, pages 42?49, New
York, NY, USA. ACM.
Yang, Y. and Pedersen, J. O. (1997). A
comparative study on feature selection in
text categorization. CiteSeerX - Scientific
Literature Digital Library and Search En-
gine [http://citeseerx.ist.psu.edu/oai2] (United
States).
287
Proceedings of the Student Research Workshop at the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 106?115,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
Automatic Creation of Arabic Named Entity Annotated Corpus Using
Wikipedia
Maha Althobaiti, Udo Kruschwitz, and Massimo Poesio
School of Computer Science and Electronic Engineering
University of Essex
Colchester, UK
{mjaltha, udo, poesio}@essex.ac.uk
Abstract
In this paper we propose a new methodology to ex-
ploit Wikipedia features and structure to automati-
cally develop an Arabic NE annotated corpus. Each
Wikipedia link is transformed into an NE type of
the target article in order to produce the NE an-
notation. Other Wikipedia features - namely redi-
rects, anchor texts, and inter-language links - are
used to tag additional NEs, which appear without
links in Wikipedia texts. Furthermore, we have de-
veloped a filtering algorithm to eliminate ambiguity
when tagging candidate NEs. Herein we also in-
troduce a mechanism based on the high coverage of
Wikipedia in order to address two challenges partic-
ular to tagging NEs in Arabic text: rich morphology
and the absence of capitalisation. The corpus cre-
ated with our new method (WDC) has been used to
train an NE tagger which has been tested on differ-
ent domains. Judging by the results, an NE tagger
trained on WDC can compete with those trained on
manually annotated corpora.
1 Introduction
Supervised learning techniques are well known
for their effectiveness to develop Named Entity
Recognition (NER) taggers (Bikel et al., 1997;
Sekine and others, 1998; McCallum and Li, 2003;
Benajiba et al., 2008). The main disadvantage of
supervised learning is that it requires a large an-
notated corpus. Although a substantial amount
of annotated data is available for some languages,
for other languages, including Arabic, more work
is needed to enrich their linguistic resources. In
fact, changing the domain or just expanding the
set of classes always requires domain-specific ex-
perts and new annotated data, both of which cost
time and effort. Therefore, current research fo-
cuses on approaches that require minimal human
intervention to facilitate the process of moving the
NE classifiers to new domains and to expand NE
classes.
Semi-supervised and unsupervised learning ap-
proaches, along with the automatic creation of
tagged corpora, are alternatives that avoid manu-
ally annotated data (Richman and Schone, 2008;
Althobaiti et al., 2013). The high coverage and
rich informational structure of online encyclope-
dias can be exploited for the automatic creation of
datasets. For example, many researchers have in-
vestigated the use of Wikipedia?s structure to clas-
sify Wikipedia articles and to transform links into
NE annotations according to the link target type
(Nothman et al., 2008; Ringland et al., 2009).
In this paper we present our approach to au-
tomatically derive a large NE annotated corpora
from Arabic Wikipedia. The key to our method
lies in the exploitation of Wikipedia?s concepts,
specifically anchor texts
1
and redirects, to handle
the rich morphology in Arabic, and thereby elim-
inate the need to perform any deep morphologi-
cal analysis. In addition, a capitalisation probabil-
ity measure has been introduced and incorporated
into the approach in order to replace the capitalisa-
tion feature that does not exist in the Arabic script.
This capitalisation measure has been utilised in or-
der to filter ambiguous Arabic NE phrases during
annotation process.
The remainder of this paper is structured as fol-
lows: Section 2 illustrates structural information
about Wikipedia. Section 3 includes background
information on NER, including recent work. Sec-
tion 4 summarises the proposed methodology.
Sections 5, 6, and 7 describe the proposed algo-
rithm in detail. The experimental setup and the
evaluation results are reported and discussed in
Section 8. Finally, the conclusion features com-
ments regarding our future work.
1
The terms ?anchor texts? and ?link labels? are used inter-
changeably in this paper.
106
2 The Structure of Wikipedia
Wikipedia is a free online encyclopedia project
written collaboratively by thousands of volunteers,
using MediaWiki
2
. Each article in Wikipedia is
uniquely identified by its title. The title is usually
the most common name for the entity explained
in the article.
2.1 Types of Wikipedia Pages
2.1.1 Content Pages
Content pages (aka Wikipedia articles) contain the
majority of Wikipedia?s informative content. Each
content page describes a single topic and has a
unique title. In addition to the text describing the
topic of the article, content pages may contain ta-
bles, images, links and templates.
2.1.2 Redirect Pages
A redirect page is used if there are two or more
alternative names that can refer to one entity
in Wikipedia. Thus, each alternative name is
changed into a title whose article contains a redi-
rect link to the actual article for that entity. For ex-
ample, ?UK? is an alternative name for the ?United
Kingdom?, and consequently, the article with the
title ?UK? is just a pointer to the article with the
title ?United Kingdom?.
2.1.3 List of Pages
Wikipedia offers several ways to group articles.
One method is to group articles by lists. The items
on these lists include links to articles in a particu-
lar subject area, and may include additional infor-
mation about the listed items. For example, ?list
of scientists? contains links to articles of scientists
and also links to more specific lists of scientists.
2.2 The Structure of Wikipedia Articles
2.2.1 Categories
Every article in the Wikipedia collection should
have at least one category. Categories should be
on vital topics that are useful to the reader. For
example, the Wikipedia article about the United
Kingdom in Wikipedia is associated with a set of
categories that includes ?Countries bordering the
Atlantic Ocean?, and ?Countries in Europe?.
2
An open source wiki package written in PHP
2.2.2 Infobox
An infobox is a fixed-format table added to the
top right-hand or left-hand corner of articles to
provide a summary of some unifying parameters
shared by the articles. For instance, every scientist
has a name, date of birth, birthplace, nationality,
and field of study.
2.3 Links
A link is a method used by Wikipedia to link pages
within wiki environments. Links are enclosed in
doubled square brackets. A vertical bar, the ?pipe?
symbol, is used to create a link while labelling it
with a different name on the current page. Look at
the following two examples,
1 - [[a]] is labelled ?a? on the current page and
links to taget page ?a?.
2 - [[a|b]] is labelled ?b? on the current page, but
links to target page ?a?.
In the second example, the anchor text (aka link
label) is ?a?, while ?b?, a link target, refers to the
title of the target article. In the first example, the
anchor text shown on the page and the title of the
target article are the same.
3 Related Work
Current NE research seeks out adequate alter-
natives to traditional techniques such that they
require minimal human intervention and solve
deficiencies of traditional methods. Specific
deficiencies include the limited number of NE
classes resulting from the high cost of setting up
corpora, and the difficulty of adapting the system
to new domains.
One of these trends is distant learning, which
depends on the recruitment of external knowledge
to increase the performance of the classifier, or
to automatically create new resources used in the
learning stage.
Kazama and Torisawa (2007) exploited
Wikipedia-based features to improve their NE
machine learning recogniser?s F-score by three
percent. Their method retrieved the corresponding
Wikipedia entry for each candidate word sequence
in the CoNLL 2003 dataset and extracted a cate-
gory label from the first sentence of the entry.
The automatic creation of training data has
also been investigated using external knowledge.
An et al. (2003) extracted sentences containing
listed entities from the web, and produced a
1.8 million Korean word dataset. Their corpus
107
performed as well as manually annotated training
data. Nothman et al. (2008) exploited Wikipedia
to create a massive corpus of named entity
annotated text. They transformed Wikipedia?s
links into named entity annotations by classifying
the target articles into standard entity types
3
.
Compared to MUC, CoNLL, and BBN corpora,
their Wikipedia-derived corpora tend to perform
better than other cross-corpus train/test pairs.
Nothman et al. (2013) automatically created
massive, multilingual training annotations for
named entity recognition by exploiting the text
and internal structure of Wikipedia. They first
categorised each Wikipedia article into named
entity types, training and evaluating on 7,200
manually-labelled Wikipedia articles across nine
languages: English, German, French, Italian,
Polish, Spanish, Dutch, Portuguese, and Russian.
Their cross-lingual approach achieved up to 95%
accuracy. They transformed Wikipedia?s links
into named entity annotations by classifying the
target articles into standard entity types. This
technique produced reasonable annotations, but
was not immediately able to compete with exist-
ing gold-standard data. They better aligned their
automatic annotations to the gold standard corpus
by deducing additional links and heuristically
tweaking the Wikipedia corpora. Following this
approach, millions of words in nine languages
were annotated. Wikipedia-trained models were
evaluated against CONLL shared task data and
other gold-standard corpora. Their method out-
performed Richman and Schone (2008) and Mika
et al. (2008), and achieved scores 10% higher
than models trained on newswire when tested on
manually annotated Wikipedia text.
Alotaibi and Lee (2013) automatically de-
veloped two NE-annotated sets from Arabic
Wikipedia. The corpora were built using the
mechanism that transforms links into NE an-
notations, by classifying the target articles into
named entity types. They used POS-tagging,
morphological analysis, and linked NE phrases to
detect other mentions of NEs that appear without
links in text. By contrast, our method does not
require POS-tagging or morphological analysis
and just identifies unlinked NEs by matching
phrases from an automatically constructed and
filtered alternative names with identical terms in
3
The terms ?type?, ?class? and ?category? are used inter-
changeably in this paper.
the articles texts, see Section 6. The first dataset
created by Alotaibi and Lee (2013) is called
WikiFANE(whole) and contains all sentences
retrieved from the articles. The second set, which
is called WikiFANE(selective), is constructed by
selecting only the sentences that have at least one
named entity phrase.
4 Summary of the Approach
All of our experiments were conducted on the
26 March 2013 Arabic version of the Wikipedia
dump
4
. A parser was created to handle the medi-
awiki markup and to extract structural information
from the Wikipedia dump such as a list of redirect
pages along with their target articles, a list of pairs
containing link labels and their target articles in
the form ?anchor text, target article?, and essential
information for each article (e.g., title, body text,
categories, and templates).
Many of Wikipedia?s concepts such as links, an-
chor texts, redirects, and inter-language links have
been exploited to transform Wikipedia into a NE
annotated corpus. More details can be found in
the next sections. Generally, the following steps
are necessary to develop the dataset:
1. Classify Wikipedia articles into a specific set
of NE types.
2. Identify matching text in the title and the first
sentence of each article and label the match-
ing phrases according to the article type.
3. Label linked phrases in the text according to
the NE type of the target article.
4. Compile a list of alternative titles for articles
and filter out ambiguous ones.
5. Identify matching phrases in the list and the
Wikipedia text.
6. Filter sentences to prevent noisy sentences
being included in the corpus.
We explain each step in turn in the following sec-
tions.
5 Classifying Wikipedia Articles into NE
Categories
Categorising Wikipedia articles is the initial step
in producing NE training data. Therefore, all
Wikipedia articles need to be classified into a
specific set of named entity types.
4
http://dumps.wikimedia.org/arwiki/
108
5.1 The Dataset and Annotation
In order to develop a Wikipedia document clas-
sifier, we used a set of 4,000 manually classi-
fied Wikipedia articles that are available free on-
line
5
. The set was manually classified using the
ACE (2008) taxonomy and a new class (Product).
Therefore, there were eight coarse-grained cate-
gories in total: Facility, Geo-Political, Location,
Organisation, Person, Vehicle, Weapon, and Prod-
uct. As our work adheres to the CoNLL definition,
we mapped these classified Wikipedia articles into
CoNLL NE types ? namely person, location, or-
ganisation, miscellaneous, or other ? based on the
CoNLL 2003 annotation guidelines (Chinchor et
al., 1999).
5.2 The Classification of Wikipedia Articles
Many researchers have already addressed the task
of classifying Wikipedia articles into named entity
types (Dakka and Cucerzan, 2008; Tardif et al.,
2009). Alotaibi and Lee (2012) is the only study
that has experimented with classifying the Arabic
version of Wikipedia into NE classes. They have
explored the use of Naive Bayes, Multinomial
Naive Bayes, and SVM for classifying Wikipedia
articles, and achieved a F-score ranging from 78%
and 90% using different language-dependent and
independent features.
We conducted three experiments that used a
simple bag-of-words features extracted from dif-
ferent portions of the Wikipedia document and
metadata. We summarise the portions of the doc-
ument included in each experiment below:
Exp1: Experiment 1 involved tokens from the
article title and the entire article body.
Exp2: Rich metadata in Wikipedia proved ef-
fective for the classification of articles (Tardif et
al., 2009; Alotaibi and Lee, 2012). Therefore, in
Experiment 2 we included tokens from categories,
templates ? specifically ?Infobox? ? as well as to-
kens from the article title and first sentence of the
document.
Exp3: Experiment 3 involved the same set of
tokens as experiment 2 except that categories and
infobox features were marked with suffixes to dif-
ferentiate them from tokens extracted from the ar-
ticle body text. This step of distinguishing tokens
based on their location in the document improved
the accuracy of document?s classification (Tardif
et al., 2009; Alotaibi and Lee, 2012).
5
www.cs.bham.ac.uk/?fsa081/
In order to optimise features, we implemented a
filtered version of the bag-of-words article repre-
sentation (e.g., removing punctuation marks and
symbols) to classify the Arabic Wikipedia doc-
uments instead of using a raw dataset (Alotaibi
and Lee, 2012). In addition, the same study
shows the high impact of applying tokenisation
6
as opposed to the neutral effect of using stem-
ming. We used the filtered features proposed in
the study of Alotaibi and Lee (2012), which in-
cluded removing punctuation marks, symbols, fil-
tering stop words, and normalising digits. We ex-
tended the features, however, by utilising the to-
kenisation scheme that involves separating con-
junctions, prepositions, and pronouns from each
word.
The feature set has been represented using Term
Frequency-Inverse Document Frequency (TF ?
IDF ). This representation method is a numeri-
cal statistic that reflects how important a token is
to a document.
5.3 The Results of Classifying the Wikipedia
Articles
As for the learning process, our Wikipedia doc-
uments classifier was trained using Liblinear
7
.
80% of the 4,000 hand-classified Wikipedia
articles were dedicated to the training stage, while
20% were specified to test the classifier. Table
1 is a comparison of the precision, recall, and
F-measure of the classifiers that resulted from the
three experiments. The Exp3 classifier performed
better than the other classifiers. Therefore, it was
selected to classify all of the Wikipedia articles.
At the end of this stage, we obtained a list of
pairs containing each Wikipedia article and its
NE Type. We stored this list in a database in
preparation for the next stage: developing the
NE-tagged training corpus.
Table 1: The results of the three Wikipedia docu-
ment classifiers.
6
It is also called decliticization or segmentation.
7
www.csie.ntu.edu.tw/?cjlin/liblinear/
109
6 The Annotation Process
6.1 Utilising the Titles of Articles and Link
Targets
Identifying corresponding words in the article ti-
tle and the entire body of text and then tagging the
matching phrases with the NE-type can be a risky
process, especially for terms with more than one
meaning. For example, the title of the article de-
scribing the city (
	
?A?, ?Cannes?)
8
can also, in Ara-
bic, refer to the past verb (
	
?A?, ?was?). The portion
of the Wikipedia article unlikely to produce errors
during the matching process is the first sentence,
which usually contains the definition of the term
the Wikipedia article is written about (Zesch et al.,
2007).
When identifying matching terms in the arti-
cle title and the first sentence, we found that ar-
ticle titles often contain abbreviations, while the
first sentence spells out entire words. This pat-
tern makes it difficult to identify matching terms
in the title and first sentence, and frequently ap-
pears in biographical Wikipedia articles. For ex-
ample, one article is entitled (?


	
P@Q? @ Q?K
.
?K
.
@, ?Abu
Bakr Al-Razi?), but the first sentence states the full
name of the person: (?


	
P@Q? @ AK


Q?
	
P
	
?K
.
??


m
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 804?813,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
A Cross-Lingual ILP Solution to Zero Anaphora Resolution
Ryu Iida
Tokyo Institute of Technology
2-12-1, ?Ookayama, Meguro,
Tokyo 152-8552, Japan
ryu-i@cl.cs.titech.ac.jp
Massimo Poesio
Universita` di Trento,
Center for Mind / Brain Sciences
University of Essex,
Language and Computation Group
massimo.poesio@unitn.it
Abstract
We present an ILP-based model of zero
anaphora detection and resolution that builds
on the joint determination of anaphoricity and
coreference model proposed by Denis and
Baldridge (2007), but revises it and extends it
into a three-way ILP problem also incorporat-
ing subject detection. We show that this new
model outperforms several baselines and com-
peting models, as well as a direct translation of
the Denis / Baldridge model, for both Italian
and Japanese zero anaphora. We incorporate
our model in complete anaphoric resolvers for
both Italian and Japanese, showing that our
approach leads to improved performance also
when not used in isolation, provided that sep-
arate classifiers are used for zeros and for ex-
plicitly realized anaphors.
1 Introduction
In so-called ?pro-drop? languages such as Japanese
and many romance languages including Italian,
phonetic realization is not required for anaphoric
references in contexts in which in English non-
contrastive pronouns are used: e.g., the subjects of
Italian and Japanese translations of buy in (1b) and
(1c) are not explicitly realized. We call these non-
realized mandatory arguments zero anaphors.
(1) a. [EN] [John]
i
went to visit some friends. On
the way, [he]
i
bought some wine.
b. [IT] [Giovanni]
i
ando` a far visita a degli am-
ici. Per via, ?
i
compro` del vino.
c. [JA] [John]
i
-wa yujin-o houmon-sita.
Tochu-de ?
i
wain-o ka-tta.
The felicitousness of zero anaphoric reference
depends on the referred entity being sufficiently
salient, hence this type of data?particularly in
Japanese and Italian?played a key role in early
work in coreference resolution, e.g., in the devel-
opment of Centering (Kameyama, 1985; Walker et
al., 1994; Di Eugenio, 1998). This research high-
lighted both commonalities and differences between
the phenomenon in such languages. Zero anaphora
resolution has remained a very active area of study
for researchers working on Japanese, because of the
prevalence of zeros in such languages1 (Seki et al,
2002; Isozaki and Hirao, 2003; Iida et al, 2007a;
Taira et al, 2008; Imamura et al, 2009; Sasano et
al., 2009; Taira et al, 2010). But now the availabil-
ity of corpora annotated to study anaphora, includ-
ing zero anaphora, in languages such as Italian (e.g.,
Rodriguez et al (2010)), and their use in competi-
tions such as SEMEVAL 2010 Task 1 on Multilin-
gual Coreference (Recasens et al, 2010), is lead-
ing to a renewed interest in zero anaphora resolu-
tion, particularly at the light of the mediocre results
obtained on zero anaphors by most systems partici-
pating in SEMEVAL.
Resolving zero anaphora requires the simulta-
neous decision that one of the arguments of a
verb is phonetically unrealized (and which argu-
ment exactly?in this paper, we will only be con-
cerned with subject zeros as these are the only
type to occur in Italian) and that a particular en-
tity is its antecedent. It is therefore natural to
view zero anaphora resolution as a joint inference
1As shown in Table 1, 64.3% of anaphors in the NAIST Text
Corpus of Anaphora are zeros.
804
task, for which Integer Linear Programming (ILP)?
introduced to NLP by Roth and Yih (2004) and suc-
cessfully applied by Denis and Baldridge (2007) to
the task of jointly inferring anaphoricity and deter-
mining the antecedent?would be appropriate.
In this work we developed, starting from the ILP
system proposed by Denis and Baldridge, an ILP
approach to zero anaphora detection and resolu-
tion that integrates (revised) versions of Denis and
Baldridge?s constraints with additional constraints
between the values of three distinct classifiers, one
of which is a novel one for subject prediction. We
demonstrate that treating zero anaphora resolution
as a three-way inference problem is successful for
both Italian and Japanese. We integrate the zero
anaphora resolver with a coreference resolver and
demonstrate that the approach leads to improved re-
sults for both Italian and Japanese.
The rest of the paper is organized as follows.
Section 2 briefly summarizes the approach proposed
by Denis and Baldridge (2007). We next present our
new ILP formulation in Section 3. In Section 4 we
show the experimental results with zero anaphora
only. In Section 5 we discuss experiments testing
that adding our zero anaphora detector and resolver
to a full coreference resolver would result in overall
increase in performance. We conclude and discuss
future work in Section 7.
2 Using ILP for joint anaphoricity and
coreference determination
Integer Linear Programming (ILP) is a method for
constraint-based inference aimed at finding the val-
ues for a set of variables that maximize a (linear) ob-
jective function while satisfying a number of con-
straints. Roth and Yih (2004) advocated ILP as a
general solution for a number of NLP tasks that re-
quire combining multiple classifiers and which the
traditional pipeline architecture is not appropriate,
such as entity disambiguation and relation extrac-
tion.
Denis and Baldridge (2007) defined the following
object function for the joint anaphoricity and coref-
erence determination problem.
min
?
?i,j??P
cC
?i,j?
? x
?i,j?
+ c?C
?i,j?
? (1 ? x
?i,j?
)
+
?
j?M
cA
j
? y
j
+ c?A
j
? (1 ? y
j
) (2)
subject to
x
?i,j?
? {0, 1} ??i, j? ? P
y
j
? {0, 1} ?j ? M
M stands for the set of mentions in the document,
and P the set of possible coreference links over these
mentions. x
?i,j?
is an indicator variable that is set to
1 if mentions i and j are coreferent, and 0 otherwise.
y
j
is an indicator variable that is set to 1 if mention
j is anaphoric, and 0 otherwise. The costs cC
?i,j?
=
?log(P (COREF|i, j)) are (logs of) probabilities
produced by an antecedent identification classifier
with ?log, whereas cA
j
= ?log(P (ANAPH|j)),
are the probabilities produced by an anaphoricity de-
termination classifier with ?log. In the Denis &
Baldridge model, the search for a solution to an-
tecedent identification and anaphoricity determina-
tion is guided by the following three constraints.
Resolve only anaphors: if a pair of mentions ?i, j?
is coreferent (x
?i,j?
= 1), then mention j must be
anaphoric (y
j
= 1).
x
?i,j?
? y
j
??i, j? ? P (3)
Resolve anaphors: if a mention is anaphoric (y
j
=
1), it must be coreferent with at least one antecedent.
y
j
?
?
i?M
j
x
?i,j?
?j ? M (4)
Do not resolve non-anaphors: if a mention is non-
anaphoric (y
j
= 0), it should have no antecedents.
y
j
?
1
|M
j
|
?
i?M
j
x
?i,j?
?j ? M (5)
3 An ILP-based account of zero anaphora
detection and resolution
In the corpora used in our experiments, zero
anaphora is annotated using as markable the first
verbal form (not necessarily the head) following the
position where the argument would have been real-
ized, as in the following example.
805
(6) [Pahor]
i
e` nato a Trieste, allora porto princi-
pale dell?Impero Austro-Ungarico.
A sette anni [vide]
i
l?incendio del Narodni
dom,
The proposal of Denis and Baldridge (2007) can be
easily turned into a proposal for the task of detecting
and resolving zero anaphora in this type of data by
reinterpreting the indicator variables as follows:
? y
j
is 1 if markable j (a verbal form) initiates a
verbal complex whose subject is unrealized, 0
otherwise;
? x
?i,j?
is 1 if the empty mention realizing the
subject argument of markable j and markable
i are mentions of the same entity, 0 otherwise.
There are however a number of ways in which this
direct adaptation can be modified and extended. We
discuss them in turn.
3.1 Best First
In the context of zero anaphora resolution, the ?Do
not resolve non-anaphors? constraint (5) is too weak,
as it allows the redundant choice of more than one
candidate antecedent. We developed therefore the
following alternative, that blocks selection of more
than one antecedent.
Best First (BF):
y
j
?
?
i?M
j
x
?i,j?
?j ? M (7)
3.2 A subject detection model
The greatest difficulty in zero anaphora resolution
in comparison to, say, pronoun resolution, is zero
anaphora detection. Simply relying for this on the
parser is not enough: most dependency parsers are
not very accurate at identifying cases in which the
verb does not have a subject on syntactic grounds
only. Again, it seems reasonable to suppose this
is because zero anaphora detection requires a com-
bination of syntactic information and information
about the current context. Within the ILP frame-
work, this hypothesis can be implemented by turn-
ing the zero anaphora resolution optimization prob-
lem into one with three indicator variables, with the
objective function in (8). The third variable, z
j
, en-
codes the information provided by the parser: it is
1 with cost cS
j
= ?log(P (SUBJ |j)) if the parser
thinks that verb j has an explicit subject with proba-
bility P (SUBJ |j), otherwise it is 0.
min
?
?i,j??P
cC
?i,j?
? x
?i,j?
+ c?C
?i,j?
? (1 ? x
?i,j?
)
+
?
j?M
cA
j
? y
j
+ c?A
j
? (1 ? y
j
)
+
?
j?M
cS
j
? z
j
+ c?S
j
? (1 ? z
j
) (8)
subject to
x
?i,j?
? {0, 1} ??i, j? ? P
y
j
? {0, 1} ?j ? M
z
j
? {0, 1} ?j ? M
The crucial fact about the relation between z
j
and
y
j
is that a verb has either a syntactically realized NP
or a zero pronoun as a subject, but not both. This is
encoded by the following constraint.
Resolve only non-subjects: if a predicate j syntac-
tically depends on a subject (z
j
= 1), then the predi-
cate j should have no antecedents of its subject zero
pronoun.
y
j
+ z
j
? 1 ?j ? M (9)
4 Experiment 1: zero anaphora resolution
In a first round of experiments, we evaluated the per-
formance of the model proposed in Section 3 on zero
anaphora only (i.e., not attempting to resolve other
types of anaphoric expressions).
4.1 Data sets
We use the two data sets summarized in Table 1.
The table shows that NP anaphora occurs more fre-
quently than zero-anaphora in Italian, whereas in
Japanese the frequency of anaphoric zero-anaphors2
is almost double the frequency of the remaining
anaphoric expressions.
Italian For Italian coreference, we used the anno-
tated data set presented in Rodriguez et al (2010)
and developed for the Semeval 2010 task ?Corefer-
ence Resolution in Multiple Languages? (Recasens
et al, 2010), where both zero-anaphora and NP
2In Japanese, like in Italian, zero anaphors are often used
non-anaphorically, to refer to situationally introduced entities,
as in I went to John?s office, but they told me that he had left.
806
#instances (anaphoric/total)
language type #docs #sentences #words zero-anaphors others all
Italian train 97 3,294 98,304 1,093 / 1,160 6,747 / 27,187 7,840 / 28,347
test 46 1,478 41,587 792 / 837 3,058 / 11,880 3,850 / 12,717
Japanese train 1,753 24,263 651,986 18,526 / 29,544 10,206 / 161,124 28,732 / 190,668
test 696 9,287 250,901 7,877 / 11,205 4,396 / 61,652 12,273 / 72,857
In the 6th column we use the term ?anaphoric? to indicate the number of zero anaphors that have an antecedent in
the text, whereas the total figure is the sum of anaphoric and exophoric zero-anaphors - zeros with a vague / generic
reference.
Table 1: Italian and Japanese Data Sets
coreference are annotated. This dataset consists
of articles from Italian Wikipedia, tokenized, POS-
tagged and morphologically analyzed using TextPro,
a freely available Italian pipeline (Pianta et al,
2008). We parsed the corpus using the Italian ver-
sion of the DESR dependency parser (Attardi et al,
2007).
In Italian, zero pronouns may only occur as omit-
ted subjects of verbs. Therefore, in the task of
zero-anaphora resolution all verbs appearing in a
text are considered candidates for zero pronouns,
and all gold mentions or system mentions preced-
ing a candidate zero pronoun are considered as can-
didate antecedents. (In contrast, in the experiments
on coreference resolution discussed in the following
section, all mentions are considered as both candi-
date anaphors and candidate antecedents. To com-
pare the results with gold mentions and with system
detected mentions, we carried out an evaluation us-
ing the mentions automatically detected by the Ital-
ian version of the BART system (I-BART) (Poesio
et al, 2010), which is freely downloadable.3
Japanese For Japanese coreference we used the
NAIST Text Corpus (Iida et al, 2007b) version
1.4?, which contains the annotated data about NP
coreference and zero-anaphoric relations. We also
used the Kyoto University Text Corpus4 that pro-
vides dependency relations information for the same
articles as the NAIST Text Corpus. In addition, we
also used a Japanese named entity tagger, CaboCha5
for automatically tagging named entity labels. In
the NAIST Text Corpus mention boundaries are not
annotated, only the heads. Thus, we considered
3http://www.bart-coref.org/
4http://www-lab25.kuee.kyoto-u.ac.jp/nl-
resource/corpus.html
5http://chasen.org?taku/software/cabocha/
as pseudo-mentions all bunsetsu chunks (i.e. base
phrases in Japanese) whose head part-of-speech was
automatically tagged by the Japanese morphologi-
cal analyser Chasen6 as either ?noun? or ?unknown
word? according to the NAIST-jdic dictionary.7
For evaluation, articles published from January
1st to January 11th and the editorials from January
to August were used for training and articles dated
January 14th to 17th and editorials dated October
to December are used for testing as done by Taira
et al (2008) and Imamura et al (2009). Further-
more, in the experiments we only considered subject
zero pronouns for a fair comparison to Italian zero-
anaphora.
4.2 Models
In these first experiments we compared the three
ILP-based models discussed in Section 3: the direct
reimplementation of the Denis and Baldridge pro-
posal (i.e., using the same constrains), a version re-
placing Do-Not-Resolve-Not-Anaphors with Best-
First, and a version with Subject Detection as well.
As discussed by Iida et al (2007a) and Imamura
et al (2009), useful features in intra-sentential zero-
anaphora are different from ones in inter-sentential
zero-anaphora because in the former problem syn-
tactic information between a zero pronoun and its
candidate antecedent is essential, while the lat-
ter needs to capture the significance of saliency
based on Centering Theory (Grosz et al, 1995).
To directly reflect this difference, we created two
antecedent identification models; one for intra-
sentential zero-anaphora, induced using the training
instances which a zero pronoun and its candidate an-
tecedent appear in the same sentences, the other for
6http://chasen-legacy.sourceforge.jp/
7http://sourceforge.jp/projects/naist-jdic/
807
inter-sentential cases, induced from the remaining
training instances.
To estimate the feature weights of each classifier,
we used MEGAM8, an implementation of the Max-
imum Entropy model, with default parameter set-
tings. The ILP-based models were compared with
the following baselines.
PAIRWISE: as in the work by Soon et al (2001),
antecedent identification and anaphoricity determi-
nation are simultaneously executed by a single clas-
sifier.
DS-CASCADE: the model first filters out non-
anaphoric candidate anaphors using an anaphoric-
ity determination model, then selects an antecedent
from a set of candidate antecedents of anaphoric
candidate anaphors using an antecedent identifica-
tion model.
4.3 Features
The feature sets for antecedent identification and
anaphoricity determination are briefly summarized
in Table 2 and Table 3, respectively. The agreement
features such as NUM AGREE and GEN AGREE are
automatically derived using TextPro. Such agree-
ment features are not available in Japanese because
Japanese words do not contain such information.
4.4 Creating subject detection models
To create a subject detection model for Italian, we
used the TUT corpus9 (Bosco et al, 2010), which
contains manually annotated dependency relations
and their labels, consisting of 80,878 tokens in
CoNLL format. We induced an maximum entropy
classifier by using as items all arcs of dependency
relations, each of which is used as a positive instance
if its label is subject; otherwise it is used as a nega-
tive instance.
To train the Japanese subject detection model we
used 1,753 articles contained both in the NAIST
Text Corpus and the Kyoto University Text Corpus.
By merging these two corpora, we can obtain the an-
notated data including which dependency arc is sub-
ject10. To create the training instances, any pair of
a predicate and its dependent are extracted, each of
8http://www.cs.utah.edu/?hal/megam/
9http://www.di.unito.it/?tutreeb/
10Note that Iida et al (2007b) referred to this relation as
?nominative?.
feature description
SUBJ PRE 1 if subject is included in the preceding
words of ZERO in a sentence; otherwise 0.
TOPIC PRE* 1 if topic case marker appears in the preced-
ing words of ZERO in a sentence; otherwise
0.
NUM PRE
(GEN PRE)
1 if a candidate which agrees with ZERO
with regards to number (gender) is included
in the set of NP; otherwise 0.
FIRST SENT 1 if ZERO appears in the first sentence of a
text; otherwise 0.
FIRST WORD 1 if the predicate which has ZERO is the
first word in a sentence; otherwise 0.
POS / LEMMA
/ DEP LABEL
part-of-speech / dependency label / lemma
of the predicate which has ZERO.
D POS /
D LEMMA /
D DEP LABEL
part-of-speech / dependency label / lemma
of the dependents of the predicate which has
ZERO.
PATH* dependency labels (functional words) of
words intervening between a ZERO and the
sentence head
The features marked with ?*? used only in Japanese.
Table 3: Features for anaphoricity determination
which is judged as positive if its relation is subject;
as negative otherwise.
As features for Italian, we used lemmas, PoS tag
of a predicate and its dependents as well as their
morphological information (i.e. gender and num-
ber) automatically computed by TextPro (Pianta et
al., 2008). For Japanese, the head lemmas of predi-
cate and dependent chunks as well as the functional
words involved with these two chunks were used as
features. One case specially treated is when a de-
pendent is placed as an adnominal constituent of a
predicate, as in this case relation estimation of de-
pendency arcs is difficult. In such case we instead
use the features shown in Table 2 for accurate esti-
mation.
4.5 Results with zero anaphora only
In zero anaphora resolution, we need to find all pred-
icates that have anaphoric unrealized subjects (i.e.
zero pronouns which have an antecedent in a text),
and then identify an antecedent for each such argu-
ment.
The Italian and Japanese test data sets contain
4,065 and 25,467 verbal predicates respectively. The
performance of each model at zero-anaphora detec-
tion and resolution is shown in Table 4, using recall
808
feature description
HEAD LEMMA characters of the head lemma in NP.
POS part-of-speech of NP.
DEFINITE 1 if NP contains the article corresponding to DEFINITE ?the?; otherwise 0.
DEMONSTRATIVE 1 if NP contains the article corresponding to DEMONSTRATIVE such as ?that? and ?this?; otherwise 0.
POSSESSIVE 1 if NP contains the article corresponding to POSSESSIVE such as ?his? and ?their?; otherwise 0.
CASE MARKER** case marker followed by NP, such as ?wa (topic)?, ?ga (subject)?, ?o (object)?.
DEP LABEL* dependency label of NP.
COOC MI** the score of well-formedness model estimated from a large number of triplets ? NP, Case, Predicate?.
FIRST SENT 1 if NP appears in the first sentence of a text; otherwise 0.
FIRST MENTION 1 if NP first appears in the set of candidate antecedents; otherwise 0.
CL RANK** a rank of NP in forward looking-center list based on Centering Theory (Grosz et al, 1995)
CL ORDER** a order of NP in forward looking-center list based on Centering Theory (Grosz et al, 1995)
PATH dependency labels (functional words) of words intervening between a ZERO and NP
NUM (DIS)AGREE 1 if NP (dis)agrees with ZERO with regards to number; otherwise 0.
GEN (DIS)AGREE 1 if NP (dis)agrees with ZERO with regards to gender; otherwise 0.
HEAD MATCH 1 if ANA and NP have the same head lemma; otherwise 0.
REGEX MATCH 1 if the string of NP subsumes the string of ANA; otherwise 0.
COMP MATCH 1 if ANA and NP have the same string; otherwise 0.
NP, ANA and ZERO stand for a candidate antecedent, a candidate anaphor and a candidate zero pronoun respectively. The features
marked with ?*? are only used in Italian, while the features marked with ?**? are only used in Japanese.
Table 2: Features used for antecedent identification
Italian Japanese
system mentions gold mentions
model R P F R P F R P F
PAIRWISE 0.864 0.172 0.287 0.864 0.172 0.287 0.286 0.308 0.296
DS-CASCADE 0.396 0.684 0.502 0.404 0.697 0.511 0.345 0.194 0.248
ILP 0.905 0.034 0.065 0.929 0.028 0.055 0.379 0.238 0.293
ILP +BF 0.803 0.375 0.511 0.834 0.369 0.511 0.353 0.256 0.297
ILP +SUBJ 0.900 0.034 0.066 0.927 0.028 0.055 0.371 0.315 0.341
ILP +BF +SUBJ 0.777 0.398 0.526 0.815 0.398 0.534 0.345 0.348 0.346
Table 4: Results on zero pronouns
/ precision / F over link detection as a metric (model
theoretic metrics do not apply for this task as only
subsets of coreference chains are considered). As
can be seen from Table 4, the ILP version with Do-
Not-Resolve-Non-Anaphors performs no better than
the baselines for either languages, but in both lan-
guages replacing that constraint with Best-First re-
sults in a performance above the baselines; adding
Subject Detection results in further improvement for
both languages. Notice also that the performance of
the models on Italian is quite a bit higher than for
Japanese although the dataset is much smaller, pos-
sibly meaning that the task is easier in Italian.
5 Experiment 2: coreference resolution for
all anaphors
In a second series of experiments we evaluated the
performance of our models together with a full
coreference system resolving all anaphors, not just
zeros.
5.1 Separating vs combining classifiers
Different types of nominal expressions display very
different anaphoric behavior: e.g., pronoun res-
olution involves very different types of informa-
tion from nominal expression resolution, depend-
ing more on syntactic information and on the local
context and less on commonsense knowledge. But
the most common approach to coreference resolu-
809
tion (Soon et al, 2001; Ng and Cardie, 2002, etc.)
is to use a single classifier to identify antecedents of
all anaphoric expressions, relying on the ability of
the machine learning algorithm to learn these differ-
ences. These models, however, often fail to capture
the differences in anaphoric behavior between dif-
ferent types of expressions?one of the reasons be-
ing that the amount of training instances is often too
small to learn such differences.11 Using different
models would appear to be key in the case of zero-
anaphora resolution, which differs even more from
the rest of anaphora resolution, e.g., in being partic-
ularly sensitive to local salience, as amply discussed
in the literature on Centering discussed earlier.
To test the hypothesis that using what we will
call separated models for zero anaphora and every-
thing else would work better than combined mod-
els induced from all the learning instances, we man-
ually split the training instances in terms of these
two anaphora types and then created two classifiers
for antecedent identification: one for zero-anaphora,
the other for NP-anaphora, separately induced from
the corresponding training instances. Likewise,
anaphoricity determination models were separately
induced with regards to these two anaphora types.
5.2 Results with all anaphors
In Table 5 and Table 6 we show the (MUC scorer)
results obtained by adding the zero anaphoric reso-
lution models proposed in this paper to both a com-
bined and a separated classifier. For the separated
classifier, we use the ILP+BF model for explicitly
realized NPs, and different ILP models for zeros.
The results show that the separated classi-
fier works systematically better than a combined
classifier. For both Italian and Japanese the
ILP+BF+SUBJ model works clearly better than the
baselines, whereas simply applying the original De-
nis and Baldridge model unchanged to this case we
obtain worse results than the baselines. For Italian
we could also compare our results with those ob-
tained on the same dataset by one of the two sys-
tems that participated to the Italian section of SE-
MEVAL, I-BART. I-BART?s results are clearly bet-
ter than those with both baselines, but also clearly in-
11E.g., the entire MUC-6 corpus contains a grand total of 3
reflexive pronouns.
Japanese
combined separated
model R P F R P F
PAIRWISE 0.345 0.236 0.280 0.427 0.240 0.308
DS-CASCADE 0.207 0.592 0.307 0.291 0.488 0.365
ILP 0.381 0.330 0.353 0.490 0.304 0.375
ILP +BF 0.349 0.390 0.368 0.446 0.340 0.386
ILP +SUBJ 0.376 0.366 0.371 0.484 0.353 0.408
ILP +BF +SUBJ 0.344 0.450 0.390 0.441 0.415 0.427
Table 6: Results for overall coreference: Japanese (MUC
score)
ferior to the results obtained with our models. In par-
ticular, the effect of introducing the separated model
with ILP+BF+SUBJ is more significant when us-
ing the system detected mentions; it obtained perfor-
mance more than 13 points better than I-BART when
the model referred to the system detected mentions.
6 Related work
We are not aware of any previous machine learn-
ing model for zero anaphora in Italian, but there
has been quite a lot of work on Japanese zero-
anaphora (Iida et al, 2007a; Taira et al, 2008; Ima-
mura et al, 2009; Taira et al, 2010; Sasano et al,
2009). In work such as Taira et al (2008) and Ima-
mura et al (2009), zero-anaphora resolution is con-
sidered as a sub-task of predicate argument structure
analysis, taking the NAIST text corpus as a target
data set. Taira et al (2008) and Taira et al (2010) ap-
plied decision lists and transformation-based learn-
ing respectively in order to manually analyze which
clues are important for each argument assignment.
Imamura et al (2009) also tackled to the same prob-
lem setting by applying a pairwise classifier for each
argument. In their approach, a ?null? argument is ex-
plicitly added into the set of candidate argument to
learn the situation where an argument of a predicate
is ?exophoric?. They reported their model achieved
better performance than the work by Taira et al
(2008).
Iida et al (2007a) also used the NAIST text
corpus. They adopted the BACT learning algo-
rithm (Kudo and Matsumoto, 2004) to effectively
learn subtrees useful for both antecedent identifica-
tion and zero pronoun detection. Their model drasti-
cally outperformed a simple pairwise model, but it is
still performed as a cascaded process. Incorporating
810
Italian
system mentions gold mentions
combined separated combined separated
model R P F R P F R P F R P F
PAIRWISE 0.508 0.208 0.295 0.472 0.241 0.319 0.582 0.261 0.361 0.566 0.314 0.404
DS-CASCADE 0.225 0.553 0.320 0.217 0.574 0.315 0.245 0.609 0.349 0.246 0.686 0.362
I-BART 0.324 0.294 0.308 ? ? ? 0.532 0.441 0.482 ? ? ?
ILP 0.539 0.321 0.403 0.535 0.316 0.397 0.614 0.369 0.461 0.607 0.384 0.470
ILP +BF 0.471 0.404 0.435 0.483 0.409 0.443 0.545 0.517 0.530 0.563 0.519 0.540
ILP +SUBJ 0.537 0.325 0.405 0.534 0.318 0.399 0.611 0.372 0.463 0.606 0.387 0.473
ILP +BF +SUBJ 0.464 0.410 0.435 0.478 0.418 0.446 0.538 0.527 0.533 0.559 0.536 0.547
R: Recall, P: Precision, F: f -score, BF: best first constraint, SUBJ: subject detection model.
Table 5: Results for overall coreference: Italian (MUC score)
their model into the ILP formulation proposed here
looks like a promising further extension.
Sasano et al (2009) obtained interesting experi-
mental results about the relationship between zero-
anaphora resolution and the scale of automatically
acquired case frames. In their work, their case
frames were acquired from a very large corpus con-
sisting of 100 billion words. They also proposed
a probabilistic model to Japanese zero-anaphora
in which an argument assignment score is esti-
mated based on the automatically acquired case
frames. They concluded that case frames acquired
from larger corpora lead to better f -score on zero-
anaphora resolution.
In contrast to these approaches in Japanese, the
participants to Semeval 2010 task 1 (especially the
Italian coreference task) simply solved the prob-
lems using one coreference classifier, not distin-
guishing zero-anaphora from the other types of
anaphora (Kobdani and Schu?tze, 2010; Poesio et al,
2010). On the other hand, our approach shows sep-
arating problems contributes to improving perfor-
mance in Italian zero-anaphora. Although we used
gold mentions in our evaluations, mention detection
is also essential. As a next step, we also need to take
into account ways of incorporating a mention detec-
tion model into the ILP formulation.
7 Conclusion
In this paper, we developed a new ILP-based model
of zero anaphora detection and resolution that ex-
tends the coreference resolution model proposed by
Denis and Baldridge (2007) by introducing modi-
fied constraints and a subject detection model. We
evaluated this model both individually and as part
of the overall coreference task for both Italian and
Japanese zero anaphora, obtaining clear improve-
ments in performance.
One avenue for future research is motivated by the
observation that whereas introducing the subject de-
tection model and the best-first constraint results in
higher precision maintaining the recall compared to
the baselines, that precision is still low. One of the
major source of the errors is that zero pronouns are
frequently used in Italian and Japanese in contexts in
which in English as so-called generic they would be
used: ?I walked into the hotel and (they) said ..?. In
such case, the zero pronoun detection model is often
incorrect. We are considering adding a generic they
detection component.
We also intend to experiment with introducing
more sophisticated antecedent identification models
in the ILP framework. In this paper, we used a very
basic pairwise classifier; however Yang et al (2008)
and Iida et al (2003) showed that the relative com-
parison of two candidate antecedents leads to obtain-
ing better accuracy than the pairwise model. How-
ever, these approaches do not output absolute prob-
abilities, but relative significance between two can-
didates, and therefore cannot be directly integrated
with the ILP-framework. We plan to examine ways
of appropriately estimating an absolute score from a
set of relative scores for further refinement.
Finally, we would like to test our model with
English constructions which closely resemble zero
anaphora. One example were studied in the Semeval
2010 ?Linking Events and their Participants in Dis-
course? task, which provides data about null instan-
811
tiation, omitted arguments of predicates like ?We
arrived ?goal at 8pm.?. (Unfortunately the dataset
available for SEMEVAL was very small.) Another
interesting area of application of these techniques
would be VP ellipsis.
Acknowledgments
Ryu Iida?s stay in Trento was supported by the Ex-
cellent Young Researcher Overseas Visit Program
of the Japan Society for the Promotion of Science
(JSPS). Massimo Poesio was supported in part by
the Provincia di Trento Grande Progetto LiveMem-
ories, which also funded the creation of the Italian
corpus used in this study. We also wish to thank
Francesca Delogu, Kepa Rodriguez, Olga Uryupina
and Yannick Versley for much help with the corpus
and BART.
References
G. Attardi, F. Dell?Orletta, M. Simi, A. Chanev, and
M. Ciaramita. 2007. Multilingual dependency pars-
ing and domain adaptation using desr. In Proc. of the
CoNLL Shared Task Session of EMNLP-CoNLL 2007,
Prague.
C. Bosco, S. Montemagni, A. Mazzei, V. Lombardo,
F. Dell?Orletta, A. Lenci, L. Lesmo, G. Attardi,
M. Simi, A. Lavelli, J. Hall, J. Nilsson, and J. Nivre.
2010. Comparing the influence of different treebank
annotations on dependency parsing. In Proceedings of
LREC, pages 1794?1801.
P. Denis and J. Baldridge. 2007. Joint determination of
anaphoricity and coreference resolution using integer
programming. In Proc. of HLT/NAACL, pages 236?
243.
B. Di Eugenio. 1998. Centering in Italian. In M. A.
Walker, A. K. Joshi, and E. F. Prince, editors, Cen-
tering Theory in Discourse, chapter 7, pages 115?138.
Oxford.
B. J. Grosz, A. K. Joshi, and S. Weinstein. 1995. Center-
ing: A framework for modeling the local coherence of
discourse. Computational Linguistics, 21(2):203?226.
R. Iida, K. Inui, H. Takamura, and Y. Matsumoto. 2003.
Incorporating contextual cues in trainable models for
coreference resolution. In Proceedings of the 10th
EACL Workshop on The Computational Treatment of
Anaphora, pages 23?30.
R. Iida, K. Inui, and Y. Matsumoto. 2007a. Zero-
anaphora resolution by learning rich syntactic pattern
features. ACM Transactions on Asian Language Infor-
mation Processing (TALIP), 6(4).
R. Iida, M. Komachi, K. Inui, and Y. Matsumoto. 2007b.
Annotating a Japanese text corpus with predicate-
argument and coreference relations. In Proceeding of
the ACL Workshop ?Linguistic Annotation Workshop?,
pages 132?139.
K. Imamura, K. Saito, and T. Izumi. 2009. Discrimi-
native approach to predicate-argument structure anal-
ysis with zero-anaphora resolution. In Proceedings of
ACL-IJCNLP, Short Papers, pages 85?88.
H. Isozaki and T. Hirao. 2003. Japanese zero pronoun
resolution based on ranking rules and machine learn-
ing. In Proceedings of EMNLP, pages 184?191.
M. Kameyama. 1985. Zero Anaphora: The case of
Japanese. Ph.D. thesis, Stanford University.
H. Kobdani and H. Schu?tze. 2010. Sucre: A modular
system for coreference resolution. In Proceedings of
the 5th International Workshop on Semantic Evalua-
tion, pages 92?95.
T. Kudo and Y. Matsumoto. 2004. A boosting algorithm
for classification of semi-structured text. In Proceed-
ings of EMNLP, pages 301?308.
V. Ng and C. Cardie. 2002. Improving machine learning
approaches to coreference resolution. In Proceedings
of the 40th ACL, pages 104?111.
E. Pianta, C. Girardi, and R. Zanoli. 2008. The TextPro
tool suite. In In Proceedings of LREC, pages 28?30.
M. Poesio, O. Uryupina, and Y. Versley. 2010. Creating a
coreference resolution system for Italian. In Proceed-
ings of LREC.
M. Recasens, L. Ma`rquez, E. Sapena, M. A. Mart??,
M. Taule?, V. Hoste, M. Poesio, and Y. Versley. 2010.
Semeval-2010 task 1: Coreference resolution in multi-
ple languages. In Proceedings of the 5th International
Workshop on Semantic Evaluation, pages 1?8.
K-J. Rodriguez, F. Delogu, Y. Versley, E. Stemle, and
M. Poesio. 2010. Anaphoric annotation of wikipedia
and blogs in the live memories corpus. In Proc. LREC.
D. Roth and W.-T. Yih. 2004. A linear programming
formulation for global inference in natural language
tasks. In Proc. of CONLL.
R. Sasano, D. Kawahara, and S. Kurohashi. 2009. The
effect of corpus size on case frame acquisition for dis-
course analysis. In Proceedings of HLT/NAACL, pages
521?529.
K. Seki, A. Fujii, and T. Ishikawa. 2002. A probabilistic
method for analyzing Japanese anaphora integrating
zero pronoun detection and resolution. In Proceedings
of the 19th COLING, pages 911?917.
W. M. Soon, H. T. Ng, and D. C. Y. Lim. 2001. A ma-
chine learning approach to coreference resolution of
noun phrases. Computational Linguistics, 27(4):521?
544.
812
H. Taira, S. Fujita, and M. Nagata. 2008. A Japanese
predicate argument structure analysis using decision
lists. In Proceedings of EMNLP, pages 523?532.
H. Taira, S. Fujita, and M. Nagata. 2010. Predicate ar-
gument structure analysis using transformation based
learning. In Proceedings of the ACL 2010 Conference
Short Papers, pages 162?167.
M. A. Walker, M. Iida, and S. Cote. 1994. Japanese
discourse and the process of centering. Computational
Linguistics, 20(2):193?232.
X. Yang, J. Su, and C. L. Tan. 2008. Twin-candidate
model for learning-based anaphora resolution. Com-
putational Linguistics, 34(3):327?356.
813
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 1?8,
Uppsala, Sweden, 15-16 July 2010.
c
?2010 Association for Computational Linguistics
SemEval-2010 Task 1: Coreference Resolution in Multiple Languages
Marta Recasens
?
Llu??s M
`
arquez
?
Emili Sapena
?
M. Ant
`
onia Mart??
?
Mariona Taul
?
e
?
V
?
eronique Hoste
?
Massimo Poesio

Yannick Versley
??
?: CLiC, University of Barcelona, {mrecasens,amarti,mtaule}@ub.edu
?: TALP, Technical University of Catalonia, {lluism,esapena}@lsi.upc.edu
?: University College Ghent, veronique.hoste@hogent.be
: University of Essex/University of Trento, poesio@essex.ac.uk
??: University of T?ubingen, versley@sfs.uni-tuebingen.de
Abstract
This paper presents the SemEval-2010
task on Coreference Resolution in Multi-
ple Languages. The goal was to evaluate
and compare automatic coreference reso-
lution systems for six different languages
(Catalan, Dutch, English, German, Italian,
and Spanish) in four evaluation settings
and using four different metrics. Such a
rich scenario had the potential to provide
insight into key issues concerning corefer-
ence resolution: (i) the portability of sys-
tems across languages, (ii) the relevance of
different levels of linguistic information,
and (iii) the behavior of scoring metrics.
1 Introduction
The task of coreference resolution, defined as the
identification of the expressions in a text that re-
fer to the same discourse entity (1), has attracted
considerable attention within the NLP community.
(1) Major League Baseball sent its head of se-
curity to Chicago to review the second in-
cident of an on-field fan attack in the last
seven months. The league is reviewing se-
curity at all ballparks to crack down on
spectator violence.
Using coreference information has been shown to
be beneficial in a number of NLP applications
including Information Extraction (McCarthy and
Lehnert, 1995), Text Summarization (Steinberger
et al, 2007), Question Answering (Morton, 1999),
and Machine Translation. There have been a few
evaluation campaigns on coreference resolution in
the past, namely MUC (Hirschman and Chinchor,
1997), ACE (Doddington et al, 2004), and ARE
(Orasan et al, 2008), yet many questions remain
open:
? To what extent is it possible to imple-
ment a general coreference resolution system
portable to different languages? How much
language-specific tuning is necessary?
? How helpful are morphology, syntax and se-
mantics for solving coreference relations?
How much preprocessing is needed? Does its
quality (perfect linguistic input versus noisy
automatic input) really matter?
? How (dis)similar are different coreference
evaluation metrics?MUC, B-CUBED,
CEAF and BLANC? Do they all provide the
same ranking? Are they correlated?
Our goal was to address these questions in a
shared task. Given six datasets in Catalan, Dutch,
English, German, Italian, and Spanish, the task
we present involved automatically detecting full
coreference chains?composed of named entities
(NEs), pronouns, and full noun phrases?in four
different scenarios. For more information, the
reader is referred to the task website.
1
The rest of the paper is organized as follows.
Section 2 presents the corpora from which the task
datasets were extracted, and the automatic tools
used to preprocess them. In Section 3, we describe
the task by providing information about the data
format, evaluation settings, and evaluation met-
rics. Participating systems are described in Sec-
tion 4, and their results are analyzed and compared
in Section 5. Finally, Section 6 concludes.
2 Linguistic Resources
In this section, we first present the sources of the
data used in the task. We then describe the auto-
matic tools that predicted input annotations for the
coreference resolution systems.
1
http://stel.ub.edu/semeval2010-coref
1
Training Development Test
#docs #sents #tokens #docs #sents #tokens #docs #sents #tokens
Catalan 829 8,709 253,513 142 1,445 42,072 167 1,698 49,260
Dutch 145 2,544 46,894 23 496 9,165 72 2,410 48,007
English 229 3,648 79,060 39 741 17,044 85 1,141 24,206
German 900 19,233 331,614 199 4,129 73,145 136 2,736 50,287
Italian 80 2,951 81,400 17 551 16,904 46 1,494 41,586
Spanish 875 9,022 284,179 140 1,419 44,460 168 1,705 51,040
Table 1: Size of the task datasets.
2.1 Source Corpora
Catalan and Spanish The AnCora corpora (Re-
casens and Mart??, 2009) consist of a Catalan and
a Spanish treebank of 500k words each, mainly
from newspapers and news agencies (El Peri?odico,
EFE, ACN). Manual annotation exists for ar-
guments and thematic roles, predicate semantic
classes, NEs, WordNet nominal senses, and coref-
erence relations. AnCora are freely available for
research purposes.
Dutch The KNACK-2002 corpus (Hoste and De
Pauw, 2006) contains 267 documents from the
Flemish weekly magazine Knack. They were
manually annotated with coreference information
on top of semi-automatically annotated PoS tags,
phrase chunks, and NEs.
English The OntoNotes Release 2.0 corpus
(Pradhan et al, 2007) covers newswire and broad-
cast news data: 300k words from The Wall Street
Journal, and 200k words from the TDT-4 col-
lection, respectively. OntoNotes builds on the
Penn Treebank for syntactic annotation and on the
Penn PropBank for predicate argument structures.
Semantic annotations include NEs, words senses
(linked to an ontology), and coreference informa-
tion. The OntoNotes corpus is distributed by the
Linguistic Data Consortium.
2
German The T?uBa-D/Z corpus (Hinrichs et al,
2005) is a newspaper treebank based on data taken
from the daily issues of ?die tageszeitung? (taz). It
currently comprises 794k words manually anno-
tated with semantic and coreference information.
Due to licensing restrictions of the original texts, a
taz-DVD must be purchased to obtain a license.
2
Italian The LiveMemories corpus (Rodr??guez
et al, 2010) will include texts from the Italian
Wikipedia, blogs, news articles, and dialogues
2
Free user license agreements for the English and German
task datasets were issued to the task participants.
(MapTask). They are being annotated according
to the ARRAU annotation scheme with coref-
erence, agreement, and NE information on top
of automatically parsed data. The task dataset
included Wikipedia texts already annotated.
The datasets that were used in the task were ex-
tracted from the above-mentioned corpora. Ta-
ble 1 summarizes the number of documents
(docs), sentences (sents), and tokens in the train-
ing, development and test sets.
3
2.2 Preprocessing Systems
Catalan, Spanish, English Predicted lemmas
and PoS were generated using FreeLing
4
for
Catalan/Spanish and SVMTagger
5
for English.
Dependency information and predicate semantic
roles were generated with JointParser, a syntactic-
semantic parser.
6
Dutch Lemmas, PoS and NEs were automat-
ically provided by the memory-based shallow
parser for Dutch (Daelemans et al, 1999), and de-
pendency information by the Alpino parser (van
Noord et al, 2006).
German Lemmas were predicted by TreeTagger
(Schmid, 1995), PoS and morphology by RFTag-
ger (Schmid and Laws, 2008), and dependency in-
formation by MaltParser (Hall and Nivre, 2008).
Italian Lemmas and PoS were provided by
TextPro,
7
and dependency information by Malt-
Parser.
8
3
The German and Dutch training datasets were not com-
pletely stable during the competition period due to a few er-
rors. Revised versions were released on March 2 and 20, re-
spectively. As to the test datasets, the Dutch and Italian doc-
uments with formatting errors were corrected after the eval-
uation period, with no variations in the ranking order of sys-
tems.
4
http://www.lsi.upc.es/ nlp/freeling
5
http://www.lsi.upc.edu/ nlp/SVMTool
6
http://www.lsi.upc.edu// xlluis/?x=cat:5
7
http://textpro.fbk.eu
8
http://maltparser.org
2
3 Task Description
Participants were asked to develop an automatic
system capable of assigning a discourse entity to
every mention,
9
thus identifying all the NP men-
tions of every discourse entity. As there is no
standard annotation scheme for coreference and
the source corpora differed in certain aspects, the
coreference information of the task datasets was
produced according to three criteria:
? Only NP constituents and possessive deter-
miners can be mentions.
? Mentions must be referential expressions,
thus ruling out nominal predicates, appos-
itives, expletive NPs, attributive NPs, NPs
within idioms, etc.
? Singletons are also considered as entities
(i.e., entities with a single mention).
To help participants build their systems, the
task datasets also contained both gold-standard
and automatically predicted linguistic annotations
at the morphological, syntactic and semantic lev-
els. Considerable effort was devoted to provide
participants with a common and relatively simple
data representation for the six languages.
3.1 Data Format
The task datasets as well as the participants?
answers were displayed in a uniform column-
based format, similar to the style used in previous
CoNLL shared tasks on syntactic and semantic de-
pendencies (2008/2009).
10
Each dataset was pro-
vided as a single file per language. Since corefer-
ence is a linguistic relation at the discourse level,
documents constitute the basic unit, and are de-
limited by ?#begin document ID? and ?#end doc-
ument ID? comment lines. Within a document, the
information of each sentence is organized verti-
cally with one token per line, and a blank line after
the last token of each sentence. The information
associated with each token is described in several
columns (separated by ?\t? characters) represent-
ing the following layers of linguistic annotation.
ID (column 1). Token identifiers in the sentence.
Token (column 2). Word forms.
9
Following the terminology of the ACE program, a men-
tion is defined as an instance of reference to an object, and
an entity is the collection of mentions referring to the same
object in a document.
10
http://www.cnts.ua.ac.be/conll2008
ID Token Intermediate columns Coref
1 Major . . . (1
2 League . . .
3 Baseball . . . 1)
4 sent . . .
5 its . . . (1)|(2
6 head . . .
7 of . . .
8 security . . . (3)|2)
9 to . . .
. . . . . . . . . . . .
27 The . . . (1
28 league . . . 1)
29 is . . .
Table 2: Format of the coreference annotations
(corresponding to example (1) in Section 1).
Lemma (column 3). Token lemmas.
PoS (column 5). Coarse PoS.
Feat (column 7). Morphological features (PoS
type, number, gender, case, tense, aspect,
etc.) separated by a pipe character.
Head (column 9). ID of the syntactic head (?0? if
the token is the tree root).
DepRel (column 11). Dependency relations cor-
responding to the dependencies described in
the Head column (?sentence? if the token is
the tree root).
NE (column 13). NE types in open-close notation.
Pred (column 15). Predicate semantic class.
APreds (column 17 and subsequent ones). For
each predicate in the Pred column, its seman-
tic roles/dependencies.
Coref (last column). Coreference relations in
open-close notation.
The above-mentioned columns are ?gold-
standard columns,? whereas columns 4, 6, 8, 10,
12, 14, 16 and the penultimate contain the same
information as the respective previous column but
automatically predicted?using the preprocessing
systems listed in Section 2.2. Neither all layers
of linguistic annotation nor all gold-standard and
predicted columns were available for all six lan-
guages (underscore characters indicate missing in-
formation).
The coreference column follows an open-close
notation with an entity number in parentheses (see
Table 2). Every entity has an ID number, and ev-
ery mention is marked with the ID of the entity
it refers to: an opening parenthesis shows the be-
ginning of the mention (first token), while a clos-
ing parenthesis shows the end of the mention (last
3
token). For tokens belonging to more than one
mention, a pipe character is used to separate mul-
tiple entity IDs. The resulting annotation is a well-
formed nested structure (CF language).
3.2 Evaluation Settings
In order to address our goal of studying the effect
of different levels of linguistic information (pre-
processing) on solving coreference relations, the
test was divided into four evaluation settings that
differed along two dimensions.
Gold-standard versus Regular setting. Only
in the gold-standard setting were participants al-
lowed to use the gold-standard columns, includ-
ing the last one (of the test dataset) with true
mention boundaries. In the regular setting, they
were allowed to use only the automatically pre-
dicted columns. Obtaining better results in the
gold setting would provide evidence for the rel-
evance of using high-quality preprocessing infor-
mation. Since not all columns were available for
all six languages, the gold setting was only possi-
ble for Catalan, English, German, and Spanish.
Closed versus Open setting. In the closed set-
ting, systems had to be built strictly with the in-
formation provided in the task datasets. In con-
trast, there was no restriction on the resources that
participants could utilize in the open setting: sys-
tems could be developed using any external tools
and resources to predict the preprocessing infor-
mation, e.g., WordNet, Wikipedia, etc. The only
requirement was to use tools that had not been de-
veloped with the annotations of the test set. This
setting provided an open door into tools or re-
sources that improve performance.
3.3 Evaluation Metrics
Since there is no agreement at present on a stan-
dard measure for coreference resolution evalua-
tion, one of our goals was to compare the rank-
ings produced by four different measures. The
task scorer provides results in the two mention-
based metrics B
3
(Bagga and Baldwin, 1998) and
CEAF-?
3
(Luo, 2005), and the two link-based
metrics MUC (Vilain et al, 1995) and BLANC
(Recasens and Hovy, in prep). The first three mea-
sures have been widely used, while BLANC is a
proposal of a new measure interesting to test.
The mention detection subtask is measured with
recall, precision, and F
1
. Mentions are rewarded
with 1 point if their boundaries coincide with those
of the gold NP, with 0.5 points if their boundaries
are within the gold NP including its head, and
with 0 otherwise.
4 Participating Systems
A total of twenty-two participants registered for
the task and downloaded the training materials.
From these, sixteen downloaded the test set but
only six (out of which two task organizers) sub-
mitted valid results (corresponding to nine system
runs or variants). These numbers show that the
task raised considerable interest but that the final
participation rate was comparatively low (slightly
below 30%).
The participating systems differed in terms of
architecture, machine learning method, etc. Ta-
ble 3 summarizes their main properties. Systems
like BART and Corry support several machine
learners, but Table 3 indicates the one used for the
SemEval run. The last column indicates the exter-
nal resources that were employed in the open set-
ting, thus it is empty for systems that participated
only in the closed setting. For more specific details
we address the reader to the system description pa-
pers in Erk and Strapparava (2010).
5 Results and Evaluation
Table 4 shows the results obtained by two naive
baseline systems: (i) SINGLETONS considers each
mention as a separate entity, and (ii) ALL-IN-ONE
groups all the mentions in a document into a sin-
gle entity. These simple baselines reveal limita-
tions of the evaluation metrics, like the high scores
of CEAF and B
3
for SINGLETONS. Interestingly
enough, the naive baseline scores turn out to be
hard to beat by the participating systems, as Ta-
ble 5 shows. Similarly, ALL-IN-ONE obtains high
scores in terms of MUC. Table 4 also reveals dif-
ferences between the distribution of entities in the
datasets. Dutch is clearly the most divergent cor-
pus mainly due to the fact that it only contains sin-
gletons for NEs.
Table 5 displays the results of all systems for all
languages and settings in the four evaluation met-
rics (the best scores in each setting are highlighted
in bold). Results are presented sequentially by lan-
guage and setting, and participating systems are
ordered alphabetically. The participation of sys-
tems across languages and settings is rather irreg-
ular,
11
thus making it difficult to draw firm conclu-
11
Only 45 entries in Table 5 from 192 potential cases.
4
System Architecture ML Methods External Resources
BART
(Broscheit et al, 2010) Closest-first with entity-
mention model (English),
Closest-first model (German,
Italian)
MaxEnt (English, Ger-
man), Decision trees
(Italian)
GermaNet & gazetteers (Ger-
man), I-Cab gazetteers (Italian),
Berkeley parser, Stanford NER,
WordNet, Wikipedia name list,
U.S. census data (English)
Corry
(Uryupina, 2010) ILP, Pairwise model SVM Stanford parser & NER, Word-
Net, U.S. census data
RelaxCor
(Sapena et al, 2010) Graph partitioning (solved by
relaxation labeling)
Decision trees, Rules WordNet
SUCRE
(Kobdani and Sch?utze, 2010) Best-first clustering, Rela-
tional database model, Regular
feature definition language
Decision trees, Naive
Bayes, SVM, MaxEnt
?
TANL-1
(Attardi et al, 2010) Highest entity-mention simi-
larity
MaxEnt PoS tagger (Italian)
UBIU
(Zhekova and K?ubler, 2010) Pairwise model MBL ?
Table 3: Main characteristics of the participating systems.
sions about the aims initially pursued by the task.
In the following, we summarize the most relevant
outcomes of the evaluation.
Regarding languages, English concentrates the
most participants (fifteen entries), followed by
German (eight), Catalan and Spanish (seven each),
Italian (five), and Dutch (three). The number of
languages addressed by each system ranges from
one (Corry) to six (UBIU and SUCRE); BART and
RelaxCor addressed three languages, and TANL-1
five. The best overall results are obtained for En-
glish followed by German, then Catalan, Spanish
and Italian, and finally Dutch. Apart from differ-
ences between corpora, there are other factors that
might explain this ranking: (i) the fact that most of
the systems were originally developed for English,
and (ii) differences in corpus size (German having
the largest corpus, and Dutch the smallest).
Regarding systems, there are no clear ?win-
ners.? Note that no language-setting was ad-
dressed by all six systems. The BART system,
for instance, is either on its own or competing
against a single system. It emerges from par-
tial comparisons that SUCRE performs the best in
closed?regular for English, German, and Italian,
although it never outperforms the CEAF or B
3
sin-
gleton baseline. While SUCRE always obtains the
best scores according to MUC and BLANC, Re-
laxCor and TANL-1 usually win based on CEAF
and B
3
. The Corry system presents three variants
optimized for CEAF (Corry-C), MUC (Corry-M),
and BLANC (Corry-B). Their results are consis-
tent with the bias introduced in the optimization
(see English:open?gold).
Depending on the evaluation metric then, the
rankings of systems vary with considerable score
differences. There is a significant positive corre-
lation between CEAF and B
3
(Pearson?s r = 0.91,
p< 0.01), and a significant lack of correlation be-
tween CEAF and MUC in terms of recall (Pear-
son?s r = 0.44, p< 0.01). This fact stresses the
importance of defining appropriate metrics (or a
combination of them) for coreference evaluation.
Finally, regarding evaluation settings, the re-
sults in the gold setting are significantly better than
those in the regular. However, this might be a di-
rect effect of the mention recognition task. Men-
tion recognition in the regular setting falls more
than 20 F
1
points with respect to the gold setting
(where correct mention boundaries were given).
As for the open versus closed setting, there is only
one system, RelaxCor for English, that addressed
the two. As expected, results show a slight im-
provement from closed?gold to open?gold.
6 Conclusions
This paper has introduced the main features of
the SemEval-2010 task on coreference resolution.
5
CEAF MUC B
3
BLANC
R P F
1
R P F
1
R P F
1
R P Blanc
SINGLETONS: Each mention forms a separate entity.
Catalan 61.2 61.2 61.2 0.0 0.0 0.0 61.2 100 75.9 50.0 48.7 49.3
Dutch 34.5 34.5 34.5 0.0 0.0 0.0 34.5 100 51.3 50.0 46.7 48.3
English 71.2 71.2 71.2 0.0 0.0 0.0 71.2 100 83.2 50.0 49.2 49.6
German 75.5 75.5 75.5 0.0 0.0 0.0 75.5 100 86.0 50.0 49.4 49.7
Italian 71.1 71.1 71.1 0.0 0.0 0.0 71.1 100 83.1 50.0 49.2 49.6
Spanish 62.2 62.2 62.2 0.0 0.0 0.0 62.2 100 76.7 50.0 48.8 49.4
ALL-IN-ONE: All mentions are grouped into a single entity.
Catalan 11.8 11.8 11.8 100 39.3 56.4 100 4.0 7.7 50.0 1.3 2.6
Dutch 19.7 19.7 19.7 100 66.3 79.8 100 8.0 14.9 50.0 3.2 6.2
English 10.5 10.5 10.5 100 29.2 45.2 100 3.5 6.7 50.0 0.8 1.6
German 8.2 8.2 8.2 100 24.8 39.7 100 2.4 4.7 50.0 0.6 1.1
Italian 11.4 11.4 11.4 100 29.0 45.0 100 2.1 4.1 50.0 0.8 1.5
Spanish 11.9 11.9 11.9 100 38.3 55.4 100 3.9 7.6 50.0 1.2 2.4
Table 4: Baseline scores.
The goal of the task was to evaluate and compare
automatic coreference resolution systems for six
different languages in four evaluation settings and
using four different metrics. This complex sce-
nario aimed at providing insight into several as-
pects of coreference resolution, including portabil-
ity across languages, relevance of linguistic infor-
mation at different levels, and behavior of alterna-
tive scoring metrics.
The task attracted considerable attention from a
number of researchers, but only six teams submit-
ted their final results. Participating systems did not
run their systems for all the languages and evalu-
ation settings, thus making direct comparisons be-
tween them very difficult. Nonetheless, we were
able to observe some interesting aspects from the
empirical evaluation.
An important conclusion was the confirmation
that different evaluation metrics provide different
system rankings and the scores are not commen-
surate. Attention thus needs to be paid to corefer-
ence evaluation. The behavior and applicability of
the scoring metrics requires further investigation
in order to guarantee a fair evaluation when com-
paring systems in the future. We hope to have the
opportunity to thoroughly discuss this and the rest
of interesting questions raised by the task during
the SemEval workshop at ACL 2010.
An additional valuable benefit is the set of re-
sources developed throughout the task. As task
organizers, we intend to facilitate the sharing of
datasets, scorers, and documentation by keeping
them available for future research use. We believe
that these resources will help to set future bench-
marks for the research community and will con-
tribute positively to the progress of the state of the
art in coreference resolution. We will maintain and
update the task website with post-SemEval contri-
butions.
Acknowledgments
We would like to thank the following peo-
ple who contributed to the preparation of the
task datasets: Manuel Bertran (UB), Oriol
Borrega (UB), Orph?ee De Clercq (U. Ghent),
Francesca Delogu (U. Trento), Jes?us Gim?enez
(UPC), Eduard Hovy (ISI-USC), Richard Johans-
son (U. Trento), Xavier Llu??s (UPC), Montse
Nofre (UB), Llu??s Padr?o (UPC), Kepa Joseba
Rodr??guez (U. Trento), Mihai Surdeanu (Stan-
ford), Olga Uryupina (U. Trento), Lente Van Leu-
ven (UB), and Rita Zaragoza (UB). We would also
like to thank LDC and die tageszeitung for dis-
tributing freely the English and German datasets.
This work was funded in part by the Span-
ish Ministry of Science and Innovation through
the projects TEXT-MESS 2.0 (TIN2009-13391-
C04-04), OpenMT-2 (TIN2009-14675-C03), and
KNOW2 (TIN2009-14715-C04-04), and an FPU
doctoral scholarship (AP2006-00994) held by
M. Recasens. It also received financial sup-
port from the Seventh Framework Programme
of the EU (FP7/2007-2013) under GA 247762
(FAUST), from the STEVIN program of the Ned-
erlandse Taalunie through the COREA and SoNaR
projects, and from the Provincia Autonoma di
Trento through the LiveMemories project.
6
Mention detection CEAF MUC B
3
BLANC
R P F
1
R P F
1
R P F
1
R P F
1
R P Blanc
Catalan
closed?gold
RelaxCor 100 100 100 70.5 70.5 70.5 29.3 77.3 42.5 68.6 95.8 79.9 56.0 81.8 59.7
SUCRE 100 100 100 68.7 68.7 68.7 54.1 58.4 56.2 76.6 77.4 77.0 72.4 60.2 63.6
TANL-1 100 96.8 98.4 66.0 63.9 64.9 17.2 57.7 26.5 64.4 93.3 76.2 52.8 79.8 54.4
UBIU 75.1 96.3 84.4 46.6 59.6 52.3 8.8 17.1 11.7 47.8 76.3 58.8 51.6 57.9 52.2
closed?regular
SUCRE 75.9 64.5 69.7 51.3 43.6 47.2 44.1 32.3 37.3 59.6 44.7 51.1 53.9 55.2 54.2
TANL-1 83.3 82.0 82.7 57.5 56.6 57.1 15.2 46.9 22.9 55.8 76.6 64.6 51.3 76.2 51.0
UBIU 51.4 70.9 59.6 33.2 45.7 38.4 6.5 12.6 8.6 32.4 55.7 40.9 50.2 53.7 47.8
open?gold
open?regular
Dutch
closed?gold
SUCRE 100 100 100 58.8 58.8 58.8 65.7 74.4 69.8 65.0 69.2 67.0 69.5 62.9 65.3
closed?regular
SUCRE 78.0 29.0 42.3 29.4 10.9 15.9 62.0 19.5 29.7 59.1 6.5 11.7 46.9 46.9 46.9
UBIU 41.5 29.9 34.7 20.5 14.6 17.0 6.7 11.0 8.3 13.3 23.4 17.0 50.0 52.4 32.3
open?gold
open?regular
English
closed?gold
RelaxCor 100 100 100 75.6 75.6 75.6 21.9 72.4 33.7 74.8 97.0 84.5 57.0 83.4 61.3
SUCRE 100 100 100 74.3 74.3 74.3 68.1 54.9 60.8 86.7 78.5 82.4 77.3 67.0 70.8
TANL-1 99.8 81.7 89.8 75.0 61.4 67.6 23.7 24.4 24.0 74.6 72.1 73.4 51.8 68.8 52.1
UBIU 92.5 99.5 95.9 63.4 68.2 65.7 17.2 25.5 20.5 67.8 83.5 74.8 52.6 60.8 54.0
closed?regular
SUCRE 78.4 83.0 80.7 61.0 64.5 62.7 57.7 48.1 52.5 68.3 65.9 67.1 58.9 65.7 61.2
TANL-1 79.6 68.9 73.9 61.7 53.4 57.3 23.8 25.5 24.6 62.1 60.5 61.3 50.9 68.0 49.3
UBIU 66.7 83.6 74.2 48.2 60.4 53.6 11.6 18.4 14.2 50.9 69.2 58.7 50.9 56.3 51.0
open?gold
Corry-B 100 100 100 77.5 77.5 77.5 56.1 57.5 56.8 82.6 85.7 84.1 69.3 75.3 71.8
Corry-C 100 100 100 77.7 77.7 77.7 57.4 58.3 57.9 83.1 84.7 83.9 71.3 71.6 71.5
Corry-M 100 100 100 73.8 73.8 73.8 62.5 56.2 59.2 85.5 78.6 81.9 76.2 58.8 62.7
RelaxCor 100 100 100 75.8 75.8 75.8 22.6 70.5 34.2 75.2 96.7 84.6 58.0 83.8 62.7
open?regular
BART 76.1 69.8 72.8 70.1 64.3 67.1 62.8 52.4 57.1 74.9 67.7 71.1 55.3 73.2 57.7
Corry-B 79.8 76.4 78.1 70.4 67.4 68.9 55.0 54.2 54.6 73.7 74.1 73.9 57.1 75.7 60.6
Corry-C 79.8 76.4 78.1 70.9 67.9 69.4 54.7 55.5 55.1 73.8 73.1 73.5 57.4 63.8 59.4
Corry-M 79.8 76.4 78.1 66.3 63.5 64.8 61.5 53.4 57.2 76.8 66.5 71.3 58.5 56.2 57.1
German
closed?gold
SUCRE 100 100 100 72.9 72.9 72.9 74.4 48.1 58.4 90.4 73.6 81.1 78.2 61.8 66.4
TANL-1 100 100 100 77.7 77.7 77.7 16.4 60.6 25.9 77.2 96.7 85.9 54.4 75.1 57.4
UBIU 92.6 95.5 94.0 67.4 68.9 68.2 22.1 21.7 21.9 73.7 77.9 75.7 60.0 77.2 64.5
closed?regular
SUCRE 79.3 77.5 78.4 60.6 59.2 59.9 49.3 35.0 40.9 69.1 60.1 64.3 52.7 59.3 53.6
TANL-1 60.9 57.7 59.2 50.9 48.2 49.5 10.2 31.5 15.4 47.2 54.9 50.7 50.2 63.0 44.7
UBIU 50.6 66.8 57.6 39.4 51.9 44.8 9.5 11.4 10.4 41.2 53.7 46.6 50.2 54.4 48.0
open?gold
BART 94.3 93.7 94.0 67.1 66.7 66.9 70.5 40.1 51.1 85.3 64.4 73.4 65.5 61.0 62.8
open?regular
BART 82.5 82.3 82.4 61.4 61.2 61.3 61.4 36.1 45.5 75.3 58.3 65.7 55.9 60.3 57.3
Italian
closed?gold
SUCRE 98.4 98.4 98.4 66.0 66.0 66.0 48.1 42.3 45.0 76.7 76.9 76.8 54.8 63.5 56.9
closed?regular
SUCRE 84.6 98.1 90.8 57.1 66.2 61.3 50.1 50.7 50.4 63.6 79.2 70.6 55.2 68.3 57.7
UBIU 46.8 35.9 40.6 37.9 29.0 32.9 2.9 4.6 3.6 38.4 31.9 34.8 50.0 46.6 37.2
open?gold
open?regular
BART 42.8 80.7 55.9 35.0 66.1 45.8 35.3 54.0 42.7 34.6 70.6 46.4 57.1 68.1 59.6
TANL-1 90.5 73.8 81.3 62.2 50.7 55.9 37.2 28.3 32.1 66.8 56.5 61.2 50.7 69.3 48.5
Spanish
closed?gold
RelaxCor 100 100 100 66.6 66.6 66.6 14.8 73.8 24.7 65.3 97.5 78.2 53.4 81.8 55.6
SUCRE 100 100 100 69.8 69.8 69.8 52.7 58.3 55.3 75.8 79.0 77.4 67.3 62.5 64.5
TANL-1 100 96.8 98.4 66.9 64.7 65.8 16.6 56.5 25.7 65.2 93.4 76.8 52.5 79.0 54.1
UBIU 73.8 96.4 83.6 45.7 59.6 51.7 9.6 18.8 12.7 46.8 77.1 58.3 52.9 63.9 54.3
closed?regular
SUCRE 74.9 66.3 70.3 56.3 49.9 52.9 35.8 36.8 36.3 56.6 54.6 55.6 52.1 61.2 51.4
TANL-1 82.2 84.1 83.1 58.6 60.0 59.3 14.0 48.4 21.7 56.6 79.0 66.0 51.4 74.7 51.4
UBIU 51.1 72.7 60.0 33.6 47.6 39.4 7.6 14.4 10.0 32.8 57.1 41.6 50.4 54.6 48.4
open?gold
open?regular
Table 5: Official results of the participating systems for all languages, settings, and metrics.
7
References
Giuseppe Attardi, Stefano Dei Rossi, and Maria Simi.
2010. TANL-1: coreference resolution by parse
analysis and similarity clustering. In Proceedings
of SemEval-2.
Amit Bagga and Breck Baldwin. 1998. Algorithms for
scoring coreference chains. In Proceedings of the
LREC Workshop on Linguistic Coreference, pages
563?566.
Samuel Broscheit, Massimo Poesio, Simone Paolo
Ponzetto, Kepa Joseba Rodr??guez, Lorenza Ro-
mano, Olga Uryupina, Yannick Versley, and Roberto
Zanoli. 2010. BART: A multilingual anaphora res-
olution system. In Proceedings of SemEval-2.
Walter Daelemans, Sabine Buchholz, and Jorn Veen-
stra. 1999. Memory-based shallow parsing. In Pro-
ceedings of CoNLL 1999.
George Doddington, Alexis Mitchell, Mark Przybocki,
Lance Ramshaw, Stephanie Strassel, and Ralph
Weischedel. 2004. The Automatic Content Extrac-
tion (ACE) program ? Tasks, data, and evaluation.
In Proceedings of LREC 2004, pages 837?840.
Katrin Erk and Carlo Strapparava, editors. 2010. Pro-
ceedings of SemEval-2.
Johan Hall and Joakim Nivre. 2008. A dependency-
driven parser for German dependency and con-
stituency representations. In Proceedings of the ACL
Workshop on Parsing German (PaGe 2008), pages
47?54.
Erhard W. Hinrichs, Sandra K?ubler, and Karin Nau-
mann. 2005. A unified representation for morpho-
logical, syntactic, semantic, and referential annota-
tions. In Proceedings of the ACL Workshop on Fron-
tiers in Corpus Annotation II: Pie in the Sky, pages
13?20.
Lynette Hirschman and Nancy Chinchor. 1997.
MUC-7 Coreference Task Definition ? Version 3.0.
In Proceedings of MUC-7.
V?eronique Hoste and Guy De Pauw. 2006. KNACK-
2002: A richly annotated corpus of Dutch written
text. In Proceedings of LREC 2006, pages 1432?
1437.
Hamidreza Kobdani and Hinrich Sch?utze. 2010. SU-
CRE: A modular system for coreference resolution.
In Proceedings of SemEval-2.
Xiaoqiang Luo. 2005. On coreference resolution
performance metrics. In Proceedings of HLT-
EMNLP 2005, pages 25?32.
Joseph F. McCarthy and Wendy G. Lehnert. 1995. Us-
ing decision trees for coreference resolution. In Pro-
ceedings of IJCAI 1995, pages 1050?1055.
Thomas S. Morton. 1999. Using coreference in ques-
tion answering. In Proceedings of TREC-8, pages
85?89.
Constantin Orasan, Dan Cristea, Ruslan Mitkov, and
Ant?onio Branco. 2008. Anaphora Resolution Exer-
cise: An overview. In Proceedings of LREC 2008.
Sameer S. Pradhan, Eduard Hovy, Mitch Mar-
cus, Martha Palmer, Lance Ramshaw, and Ralph
Weischedel. 2007. Ontonotes: A unified rela-
tional semantic representation. In Proceedings of
the International Conference on Semantic Comput-
ing (ICSC 2007), pages 517?526.
Marta Recasens and Eduard Hovy. in prep. BLANC:
Implementing the Rand Index for Coreference Eval-
uation.
Marta Recasens and M. Ant`onia Mart??. 2009. AnCora-
CO: Coreferentially annotated corpora for Spanish
and Catalan. Language Resources and Evaluation,
DOI:10.1007/s10579-009-9108-x.
Kepa Joseba Rodr??guez, Francesca Delogu, Yannick
Versley, Egon Stemle, and Massimo Poesio. 2010.
Anaphoric annotation of Wikipedia and blogs in
the Live Memories Corpus. In Proceedings of
LREC 2010, pages 157?163.
Emili Sapena, Llu??s Padr?o, and Jordi Turmo. 2010.
RelaxCor: A global relaxation labeling approach to
coreference resolution for the SemEval-2 Corefer-
ence Task. In Proceedings of SemEval-2.
Helmut Schmid and Florian Laws. 2008. Estimation
of conditional probabilities with decision trees and
an application to fine-grained POS tagging. In Pro-
ceedings of COLING 2008, pages 777?784.
Helmut Schmid. 1995. Improvements in part-of-
speech tagging with an application to German. In
Proceedings of the ACL SIGDAT Workshop, pages
47?50.
Josef Steinberger, Massimo Poesio, Mijail A. Kabad-
jov, and Karel Jeek. 2007. Two uses of anaphora
resolution in summarization. Information Process-
ing and Management: an International Journal,
43(6):1663?1680.
Olga Uryupina. 2010. Corry: A system for corefer-
ence resolution. In Proceedings of SemEval-2.
Gertjan van Noord, Ineke Schuurman, and Vincent
Vandeghinste. 2006. Syntactic annotation of large
corpora in STEVIN. In Proceedings of LREC 2006.
Marc Vilain, John Burger, John Aberdeen, Dennis Con-
nolly, and Lynette Hirschman. 1995. A model-
theoretic coreference scoring scheme. In Proceed-
ings of MUC-6, pages 45?52.
Desislava Zhekova and Sandra K?ubler. 2010. UBIU:
A language-independent system for coreference res-
olution. In Proceedings of SemEval-2.
8
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 104?107,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
BART: A Multilingual Anaphora Resolution System
Samuel Broscheit?, Massimo Poesio?, Simone Paolo Ponzetto?, Kepa Joseba Rodriguez?,
Lorenza Romano?, Olga Uryupina?, Yannick Versley?, Roberto Zanoli?
?Seminar fu?r Computerlinguistik, University of Heidelberg
?CiMeC, University of Trento
?Fondazione Bruno Kessler
?SFB 833, University of Tu?bingen
broscheit@cl.uni-heidelberg.de, massimo.poesio@unitn.it,
ponzetto@cl.uni-heidelberg.de, kepa.rodriguez@unitn.it,
romano@fbk.eu, uryupina@gmail.com,
versley@sfs.uni-tuebingen.de, zanoli@fbk.eu
Abstract
BART (Versley et al, 2008) is a highly mod-
ular toolkit for coreference resolution that
supports state-of-the-art statistical approaches
and enables efficient feature engineering. For
the SemEval task 1 on Coreference Resolu-
tion, BART runs have been submitted for Ger-
man, English, and Italian.
BART relies on a maximum entropy-based
classifier for pairs of mentions. A novel entity-
mention approach based on Semantic Trees is
at the moment only supported for English.
1 Introduction
This paper presents a multilingual coreference reso-
lution system based on BART (Versley et al, 2008).
BART is a modular toolkit for coreference resolution
that supports state-of-the-art statistical approaches
to the task and enables efficient feature engineer-
ing. BART has originally been created and tested
for English, but its flexible modular architecture en-
sures its portability to other languages and domains.
In SemEval-2010 task 1 on Coreference Resolution,
BART has shown reliable performance for English,
German and Italian.
In our SemEval experiments, we mainly focus on
extending BART to cover multiple languages. Given
a corpus in a new language, one can re-train BART
to obtain baseline results. Such a language-agnostic
system, however, is only used as a starting point:
substantial improvements can be achieved by incor-
porating language-specific information with the help
of the Language Plugin. This design provides ef-
fective separation between linguistic and machine
learning aspects of the problem.
2 BART Architecture
The BART toolkit has five main components: pre-
processing pipeline, mention factory, feature extrac-
tion module, decoder and encoder. In addition, an
independent LanguagePlugin module handles all the
language specific information and is accessible from
any component. The architecture is shown on Figure
1. Each module can be accessed independently and
thus adjusted to leverage the system?s performance
on a particular language or domain.
The preprocessing pipeline converts an input doc-
ument into a set of lingustic layers, represented
as separate XML files. The mention factory uses
these layers to extract mentions and assign their
basic properties (number, gender etc). The fea-
ture extraction module describes pairs of mentions
{M
i
,M
j
}, i < j as a set of features.
The decoder generates training examples through
a process of sample selection and learns a pairwise
classifier. Finally, the encoder generates testing ex-
amples through a (possibly distinct) process of sam-
ple selection, runs the classifier and partitions the
mentions into coreference chains.
3 Language-specific issues
Below we briefly describe our language-specific ex-
tensions to BART. These issues are addressed in
more details in our recent papers (Broscheit et al,
2010; Poesio et al, 2010).
3.1 Mention Detection
Robust mention detection is an essential component
of any coreference resolution system. BART sup-
ports different pipelines for mention detection. The
104
Parser
Dep-to-Const
Converter
Morphology
Preprocessing
Mention
Factory
Decoder
Basic features
Syntactic features
Knowledge-based
features
MaxEnt
Classifier
Mention
(with basic
 properties):
- Number
- Gender
- Mention Type
- Modifiers
Unannotated
Text
Coreference
Chains
LanguagePlugin
Figure 1: BART architecture
choice of a pipeline depends crucially on the avail-
ability of linguistic resources for a given language.
For English and German, we use the Parsing
Pipeline and Mention Factory to extract mentions.
The parse trees are used to identify minimal and
maximal noun projections, as well as additional fea-
tures such as number, gender, and semantic class.
For English, we use parses from a state-of-the-art
constituent parser (Petrov et al, 2006) and extract
all base noun phrases as mentions. For German,
the SemEval dependency tree is transformed to a
constituent representation and minimal and maxi-
mal phrases are extracted for all nominal elements
(pronouns, common nouns, names), except when the
noun phrase is in a non-referring syntactic position
(for example, expletive ?es?, predicates in copula
constructions).
For Italian, we use the EMD Pipeline and Men-
tion Factory. The Typhoon (Zanoli et al, 2009)
and DEMention (Biggio et al, 2009) systems were
used to recognize mentions in the test set. For each
mention, its head and extension were considered.
The extension was learned by using the mention an-
notation provided in the training set (13th column)
whereas the head annotation was learned by exploit-
ing the information produced by MaltParser (Nivre
et al, 2007). In addition to the features extracted
from the training set, such as prefixes and suffixes
(1-4 characters) and orthographic information (capi-
talization and hyphenation), a number of features ex-
tracted by using external resources were used: men-
tions recognized by TextPro (http://textpro.fbk.eu),
gazetteers of generic proper nouns extracted from
the Italian phone-book and Wikipedia, and other fea-
tures derived from WordNet. Each of these features
was extracted in a local context of ?2 words.
3.2 Features
We view coreference resolution as a binary classifi-
cation problem. Each classification instance consists
of two markables, i.e. an anaphor and potential an-
tecedent. Instances are modeled as feature vectors
(cf. Table 1) and are handed over to a binary clas-
sifier that decides, given the features, whether the
anaphor and the candidate are coreferent or not. All
the feature values are computed automatically, with-
out any manual intervention.
Basic feature set. We use the same set of rela-
tively language-independent features as a backbone
of our system, extending it with a few language-
specific features for each subtask. Most of them are
used by virtually all the state-of-the-art coreference
resolution systems. A detailed description can be
found, for example, in (Soon et al, 2001).
English. Our English system is based on a novel
model of coreference. The key concept of our model
is a Semantic Tree ? a filecard associated with each
discourse entity containing the following fields:
? Types: the list of types for mentions of a given
entity. For example, if an entity contains the
mention ?software from India?, the shallow
predicate ?software? is added to the types.
? Attributes: this field collects the premodifiers.
For instance, if one of the mentions is ?the ex-
pensive software? the shallow attribute ?expen-
sive? is added to the list of attributes.
? Relations: this field collects the prepositional
postmodifiers. If an entity contains the men-
tion ?software from India?, the shallow relation
?from(India)? is added to the list of relations.
105
For each mention BART creates such a filecard
using syntactic information. If the classifier decides
that both mentions are corefering, the filecard of
the anaphora is merged into the filecard of the an-
tecedent (cf. Section 3.3 below).
The SemanticTreeCompatibility feature
extractor checks whether individual slots of the
anaphor?s filecard are compatible with those of the
antecedent?s.
The StrudelRelatedness feature relies on
Strudel ? a distributional semantic model (Baroni et
al., 2010). We compute Strudel vectors for the sets
of types of the anaphor and the antecedent. The re-
latedness value is determined as the cosine between
the two.
German. We have tested extra features for Ger-
man in our previous study (Broscheit et al, 2010).
The NodeDistance feature measures the num-
ber of clause nodes (SIMPX, R-SIMPX) and preposi-
tional phrase nodes (PX) along the path between M
j
and M
i
in the parse tree.
The PartialMorphMatch feature is a sub-
string match with a morphological extension for
common nouns. In German the frequent use of
noun composition makes a simple string match for
common nouns unfeasible. The feature checks for
a match between the noun stems of M
i
and M
j
.
We extract the morphology with SMOR/Morphisto
(Schmid et al, 2004).
The GermanetRelatedness feature uses the
Pathfinder library for GermaNet (Finthammer and
Cramer, 2008) that computes and discretizes raw
scores into three categories of semantic relatedness.
In our experiments we use the measure from Wu and
Palmer (1994), which has been found to be the best
performing on our development data.
Italian. We have designed a feature to cover Ital-
ian aliasing patterns. A list of company/person des-
ignators (e.g., ?S.p.a? or ?D.ssa?) has been manually
crafted. We have collected patterns of name variants
for locations. Finally, we have relaxed abbreviation
constraints, allowing for lower-case characters in the
abbreviations. Our pilot experiments suggest that,
although a universal aliasing algorithm is able to re-
solve some coreference links between NEs, creating
a language-specific module boosts the system?s per-
formance for Italian substantially.
Basic feature set
MentionType(M
i
),MentionType(M
j
)
SemanticClass(M
i
), SemanticClass(M
j
)
GenderAgreement(M
i
,M
j
)
NumberAgreement(M
i
,M
j
)
AnimacyAgreement(M
i
,M
j
)
StringMatch(M
i
,M
j
)
Distance(M
i
,M
j
)
Basic features used for English and Italian
Alias(M
i
,M
j
)
Apposition(M
i
,M
j
)
FirstMention(M
i
)
English
IsSubject(M
i
)
SemanticTreeCompatibility(M
i
,M
j
)
StrudelRelatedness(M
i
,M
j
)
German
InQuotedSpeech(M
i
), InQuotedSpeech(M
j
)
NodeDistance(M
i
,M
j
)
PartialMorphMatch(M
i
,M
j
)
GermanetRelatedness(M
i
,M
j
)
Italian
AliasItalian(M
i
,M
j
)
Table 1: Features used by BART: each feature describes
a pair of mentions {M
i
,M
j
}, i < j, where M
i
is a can-
didate antecedent and M
j
is a candidate anaphor
3.3 Resolution Algorithm
The BART toolkit supports several models of coref-
erence (pairwise modeling, rankers, semantic trees),
as well as different machine learning algorithms.
Our final setting relies on a pairwise maximum en-
tropy classifier for Italian and German.
Our English system is based on an entity-mention
model of coreference. The key concept of our model
is a Semantic Tree - a filecard associated to each dis-
course entity (cf. Section 3.2). Semantic trees are
used for both computing feature values and guiding
the resolution process.
We start by creating a Semantic Tree for each
mention. We process the document from left to
right, trying to find an antecedent for each men-
tion (candidate anaphor). When the antecedent is
found, we extend its Semantic Tree with the types,
attributes and relations of the anaphor, provided
they are mutually compatible. Consider, for ex-
106
ample, a list of mentions, containing, among oth-
ers, ?software from India?, ?the software? and ?soft-
ware from China?. Initially, BART creates the fol-
lowing semantic trees: ?(type: software) (relation:
from(India))?, ?(type: software)? and ?(type: soft-
ware) (relation: from(China))?. When the second
mention gets resolved to the first one, their seman-
tic trees are merged to ?(type: software) (relation:
from(India)?. Therefore, when we attempt to resolve
the third mention, both candidate antecedents are re-
jected, as their relation attributes are incompatible
with ?from(China)?. This approach helps us avoid
erroneous links (such as the link between the second
and the third mentions in our example) by leveraging
entity-level information.
4 Evaluation
The system was evaluated on the SemEval task 1
corpus by using the SemEval scorer.
First, we have evaluated our mention detection
modules: the system?s ability to recognize both the
mention extensions and the heads in the regular set-
ting. BART has achieved the best score for men-
tion detection in German and has shown reliable
figures for English. For Italian, the moderate per-
formance level is due to the different algorithms
for identifying the heads: the MaltParser (trained
on TUT: http://www.di.unito.it/?tutreeb) produces a
more semantic representation, while the SemEval
scorer seems to adopt a more syntactic approach.
Second, we have evaluated the quality of our
coreference resolution modules. For German, BART
has shown better performance than all the other sys-
tems on the regular track.
For English, the only language targeted by all sys-
tems, BART shows good performance over all met-
rics in the regular setting, usually only outperformed
by systems that were tuned to a particular metric.
Finally, the Italian version of BART shows re-
liable figures for coreference resolution, given the
mention alignment problem discussed above.
5 Conclusion
We have presented BART ? a multilingual toolkit
for coreference resolution. Due to its highly modu-
lar architecture, BART allows for efficient language-
specific feature engineering. Our effort represents
the first steps towards building a freely available
coreference resolution system for many languages.
References
Marco Baroni, Brian Murphy, Eduard Barbu, and Mas-
simo Poesio. 2010. Strudel: A corpus-based semantic
model based on properties and types. Cognitive Sci-
ence, 34(2):222?254.
Silvana Marianela Bernaola Biggio, Claudio Giuliano,
Massimo Poesio, Yannick Versley, Olga Uryupina, and
Roberto Zanoli. 2009. Local entity detection and
recognition task. In Proc. of Evalita-09.
Samuel Broscheit, Simone Paolo Ponzetto, Yannick Ver-
sley, and Massimo Poesio. 2010. Extending BART to
provide a coreference resolution system for German.
In Proc. of LREC ?10.
Marc Finthammer and Irene Cramer. 2008. Explor-
ing and navigating: Tools for GermaNet. In Proc. of
LREC ?08.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev,
Gulsen Eryigit, Sandra Ku?bler, Svetoslav Marinov,
and Erwin Marsi. 2007. Maltparser: A language-
independent system for data-driven dependency pars-
ing. Natural Language Engineering, 13(2):95?135.
Slav Petrov, Leon Barett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proc. of COLING-ACL-06.
Massimo Poesio, Olga Uryupina, and Yannick Versley.
2010. Creating a coreference resolution system for
Italian. In Proc. of LREC ?10.
Helmut Schmid, Arne Fitschen, and Ulrich Heid. 2004.
SMOR: A German computational morphology cover-
ing derivation, composition and inflection. In Proc. of
LREC ?04.
Wee Meng Soon, Hwee Tou Ng, and Daniel Chung Yong
Lim. 2001. A machine learning approach to corefer-
ence resolution of noun phrases. Computational Lin-
guistics (Special Issue on Computational Anaphora
Resolution), 27(4):521?544.
Yannick Versley, Simone Paolo Ponzetto, Massimo Poe-
sio, Vladimir Eidelman, Alan Jern, Jason Smith,
Xiaofeng Yang, and Alessandro Moschitti. 2008.
BART: A modular toolkit for coreference resolution.
In Proceedings of the Linguistic Coreference Work-
shop at the International Conference on Language Re-
sources and Evaluation (LREC-2008).
Zhibiao Wu and Martha Palmer. 1994. Verb semantics
and lexical selection. In Proc. of ACL-94, pages 133?
138.
Roberto Zanoli, Emiliano Pianta, and Claudio Giuliano.
2009. Named entity recognition through redundancy
driven classifier. In Proc. of Evalita-09.
107
Proceedings of the Workshop on Frontiers in Corpus Annotation II: Pie in the Sky, pages 5?12,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
  
Merging PropBank, NomBank, TimeBank, Penn Discourse Treebank and Coreference 
James Pustejovsky, Adam Meyers, Martha Palmer, Massimo Poesio 
 
Abstract 
Many recent annotation efforts for English 
have focused on pieces of the larger problem 
of semantic annotation, rather than initially 
producing a single unified representation. 
This paper discusses the issues involved in 
merging four of these efforts into a unified 
linguistic structure: PropBank, NomBank, the 
Discourse Treebank and Coreference 
Annotation undertaken at the University of 
Essex. We discuss resolving overlapping and 
conflicting annotation as well as how the 
various annotation schemes can reinforce 
each other to produce a representation that is 
greater than the sum of its parts. 
 
1. Introduction 
 
The creation of the Penn Treebank (Marcus et al 
1993) and the word sense-annotated SEMCOR 
(Fellbaum, 1997) have shown how even limited 
amounts of annotated data can result in major 
improvements in complex natural language 
understanding systems. These annotated corpora 
have led to high-level improvements for parsing 
and word sense disambiguation (WSD), on the 
same scale as previously occurred for Part of 
Speech tagging by the annotation of the Brown 
corpus and, more recently, the British National 
Corpus (BNC) (Burnard, 2000). However, the 
creation of semantically annotated corpora has 
lagged dramatically behind the creation of other 
linguistic resources: in part due to the perceived 
cost, in part due to an assumed lack of theoretical 
agreement on basic semantic judgments, in part, 
finally, due to the understandable unwillingness 
of  research groups to get involved in such an 
undertaking. As a result, the need for such 
resources has become urgent.   
 
Many recent annotation efforts for English have 
focused on pieces of the larger problem of 
semantic annotation, rather than producing a 
single unified representation like Head-driven 
Phrase Structure Grammar (Pollard and Sag 
1994) or the Prague Dependency Tecto-
gramatical Representation (Hajicova & Kucer-
ova, 2002). PropBank (Palmer et al 2005) 
annotates predicate argument structure anchored 
by verbs. NomBank (Meyers, et. al., 2004a) 
annotates predicate argument structure anchored 
by nouns.  TimeBank (Pustejovsky et al 2003) 
annotates the temporal features of propositions 
and the temporal relations between propositions. 
The Penn Discourse Treebank (Miltsakaki et al
2004a/b) treats discourse connectives as 
predicates and the sentences being joined as 
arguments. Researchers at Essex were 
responsible for the coreference markup scheme 
developed in MATE (Poesio et al 1999; Poesio, 
2004a) and have annotated corpora using this 
scheme including a subset of the Penn Treebank 
(Poesio and Vieira, 1998), and the GNOME 
corpus (Poesio, 2004a).  This paper discusses the 
issues involved in creating a Unified Linguistic 
Annotation (ULA) by merging annotation of 
examples using the schemata from these efforts. 
Crucially, all individual annotations can be kept 
separate in order to make it easy to produce 
alternative annotations of a specific type of 
semantic information without need to modify the 
annotation at the other levels. Embarking on 
separate annotation efforts has the advantage of 
allowing researchers to focus on the difficult 
issues in each area of semantic annotation and 
the disadvantage of inducing a certain amount of 
tunnel vision or task-centricity ? annotators 
working on a narrow task tend to see all 
phenomena in light of the task they are working 
on, ignoring other factors. However, merging 
these annotation efforts allows these biases to be 
dealt with. The result, we believe, could be a 
more detailed semantic account than possible if 
the ULA had been the initial annotation effort 
rather than the result of merging. 
 
There is a growing community consensus that 
general annotation, relying on linguistic cues, 
and in particular lexical cues, will produce an 
enduring resource that is useful, replicable and 
portable.  We provide the beginnings of one such 
level derived from several distinct annotation 
efforts. This level could provide the foundation 
for a major advance in our ability to 
automatically extract salient relationships from 
text. This will in turn facilitate breakthroughs in 
message understanding, machine translation, fact 
retrieval, and information retrieval. 
 
2. The Component Annotation Schemata 
 
We describe below existing independent 
annotation efforts, each one of which is focused 
on a specific aspect of the semantic 
representation task: semantic role labeling, 
5
  
coreference, discourse relations, temporal 
relations, etc.  They have reached a level of 
maturity that warrants a concerted attempt to 
merge them into a single, unified representation, 
ULA.  There are several technical and theoretical 
issues that will need to be resolved in order to 
bring these different layers together seamlessly.  
Most of these approaches have annotated the 
same type of data, Wall Street Journal text, so it 
is also important to demonstrate that the 
annotation can be extended to other genres such 
as spoken language.  The demonstration of 
success for the extensions would be the training 
of accurate statistical semantic taggers. 
 
PropBank: The Penn Proposition Bank focuses 
on the argument structure of verbs, and provides 
a corpus annotated with semantic roles, 
including participants traditionally viewed as 
arguments and adjuncts.  An important goal is to 
provide consistent semantic role labels across 
different syntactic realizations of the same verb, 
as in the window in [ARG0 John] broke [ARG1 
the window] and [ARG1 The window] broke. 
Arg0 and Arg1 are used rather than the more 
traditional Agent and Patient to keep the 
annotation as theory-neutral as possible, and to 
facilitate mapping to richer representations.  The 
1M word Penn Treebank II Wall Street Journal 
corpus has been successfully annotated with 
semantic argument structures for verbs and is 
now available via the Penn Linguistic Data 
Consortium as PropBank I (Palmer, et. al., 2005).   
Coarse-grained sense tags, based on groupings of 
WordNet senses, are being added, as well as 
links from the argument labels in the Frames 
Files to FrameNet frame elements.  There are 
close parallels to other semantic role labeling 
projects, such as FrameNet (Baker, et. al., 1998; 
Fillmore & Atkins, 1998; Fillmore & Baker, 
2001), Salsa (Ellsworth, et.al, 2004), Prague 
Tectogrammatics (Hajicova & Kucerova, 2002) 
and IAMTC, (Helmreich, et. al., 2004) 
 
NomBank: The NYU NomBank project can be 
considered part of the larger PropBank effort and 
is designed to provide argument structure for 
instances of about 5000 common nouns in the 
Penn Treebank II corpus (Meyers, et. al., 2004a).  
PropBank argument types and related verb 
Frames Files are used to provide a commonality 
of annotation.  This enables the development of 
systems that can recognize regularizations of 
lexically and syntactically related sentence 
structures, whether they occur as verb phrases or 
noun phrases. For example, given an IE system 
tuned to a hiring scenario (MUC-6, 1995), 
NomBank and PropBank annotation facilitate  
generalization over patterns. PropBank and 
NomBank would both support a single IE pattern 
stating that the object (ARG1) of appoint is John 
and the subject (ARG0) is IBM, allowing a 
system to detect that IBM hired John from each 
of the following strings: IBM appointed John, 
John was appointed by IBM, IBM's appointment 
of John, the appointment of John by IBM and 
John is the current IBM appointee.  
 
Coreference: Coreference involves the detection 
of subsequent mentions of invoked entities, as in 
George Bush,? he?.  Researchers at Essex (UK) 
were responsible for the coreference markup 
scheme developed in MATE (Poesio et al 1999; 
Poesio, 2004a), partially implemented in the 
annotation tool MMAX and now proposed as an 
ISO standard; and have been responsible for the 
creation of two small, but commonly used 
anaphorically annotated corpora ? the Vieira / 
Poesio subset of the Penn Treebank (Poesio and 
Vieira, 1998), and the GNOME corpus (Poesio, 
2004a).   Parallel coreference annotation efforts 
funded by ACE have resulted in similar 
guidelines, exemplified by BBN?s recent 
annotation of Named Entities, common nouns 
and pronouns.   These two approaches provide a 
suitable springboard for an attempt at achieving a 
community consensus on coreference. 
 
Discourse Treebank:  The Penn Discourse 
Treebank (PDTB) (Miltsakaki et al2004a/b) is 
based on the idea that discourse connectives are 
predicates with associated argument structure 
(for details see (Miltsakaki et al2004a, 
Miltsakaki et al2004b). The long-range goal is 
to develop a large scale and reliably annotated 
corpus that will encode coherence relations 
associated with discourse connectives, including 
their argument structure and anaphoric links, 
thus exposing a clearly defined level of discourse 
structure and supporting the extraction of a range 
of inferences associated with discourse 
connectives. This annotation references the Penn 
Treebank annotations as well as PropBank, and 
currently only considers Wall Street Journal text. 
 
TimeBank: The Brandeis TimeBank corpus, 
funded by ARDA, focuses on the annotation of 
all major aspects in natural language text 
associated with temporal and event information 
(Day, et al 2003, Pustejovsky, et al 2004). 
Specifically, this involves three areas of the 
annotation: temporal expressions, event-denoting 
6
  
expressions, and the links that express either an 
anchoring of an event to a time or an ordering of 
one event relative to another. Identifying events 
and their temporal anchorings is a critical aspect  
of reasoning, and without a robust ability to 
identify and extract events and their temporal 
anchoring from a text, the real aboutness of the 
article can be missed.  The core of TimeBank is a 
set of 200 news reports documents, consisting of 
WSJ, DUC, and ACE articles, each annotated to 
TimeML 1.2 specification. It is currently being 
extended to AQUAINT articles. The corpus is 
available from the timeml.org website. 
 
3. Unifying Linguistic Annotations 
  
Since September, 2004, researchers representing 
several different sites and annotation projects 
have begun collaborating to produce a detailed 
semantic annotation of two difficult sentences. 
These researchers aim to produce a single unified 
representation with some consensus from the 
NLP community. This effort has given rise to 
both a listserv email list and this workshop: 
http://nlp.cs.nyu.edu/meyers/pie-in-the-sky.html, 
http://nlp.cs.nyu.edu/meyers/frontiers/2005.html 
The merging operations discussed here would 
seem crucial to the furthering of this effort. 
 
3.1 The Initial Pie in the Sky Example 
 
The following two consecutive sentences have 
been annotated for Pie in the Sky.  
 
Two Sentences From ACE Corpus File 
NBC20001019.1830.0181 
 
? but Yemen's president says the FBI has told 
him the explosive material could only have 
come from the U.S., Israel or two Arab 
countries. 
? and to a former federal bomb investigator, 
that description suggests a powerful 
military-style plastic explosive c-4 that can 
be cut or molded into different shapes. 
 
Although the full Pie-in-the-Sky analysis 
includes information from many different 
annotation projects, the Dependency Structure in 
Figure 1 includes only those components that 
relate to PropBank, NomBank, Discourse 
annotation, coreference and TimeBank. Several 
parts of this representation require further 
explanation. Most of these are signified by the 
special arcs, arc labels, and nodes. Dashed lines 
represent transparent arcs, such as the transparent 
dependency between the argument (ARG1) of 
modal can and the or. Or is transparent in that it 
allows this dependency to pass through it to cut 
and mold. There are two small arc loops -- 
investigator is its own ARG0 and description is 
its own ARG1. Investigator is a relational noun 
in NomBank. There is assumed to be an 
underlying relation between the Investigator 
(ARG0), the beneficiary or employer (the ARG2) 
and the item investigated (ARG1). Similarly, 
description acts as its own ARG1 (the thing 
described). There are four special coreference arc 
labels: ARG0-CF, ARG-ANAPH, EVENT-
ANAPH and ARG1-SBJ-CF. At the target of 
these arcs are pointers referring to phrases from 
the previous sentence or previous discourse. The 
first three of these labels are on arcs with the 
noun description as their source. The ARG0-CF 
label indicates that the phrase Yemen's president 
(**1**) is the ARG0, the one who is doing the 
describing. The EVENT-ANAPH label points to 
a previous mention of the describing event, 
namely the clause: The FBI told him the 
explosive material? (**3**). However, as noted 
above, the NP headed by description represents 
the thing described in addition to the action. The 
ARG-ANAPH label points to the thing that the 
FBI told him the explosive material can only 
come from ? (**2**). The ARG1-SBJ-CF label 
links the NP from the discourse what the bomb 
was made from as the subject with the NP 
headed by explosive as its predicate, much the 
same as it would in a copular construction such 
as: What the bomb was made from is the 
explosive C-4. Similarly, the arc ARG1-APP 
marks C-4 as an apposite, also predicated to the 
NP headed by explosive. Finally, the thick arcs 
labeled SLINK-MOD represent TimeML SLINK 
relations between eventuality variables, i.e.,  the 
cut and molded events are modally subordinate 
to the suggests proposition. The merged 
representation aims to be compatible with the 
projects from which it derives, each of which 
analyzes a different aspect of linguistic analysis. 
Indeed most of the dependency labels are based 
on the annotation schemes of those projects. 
 
We have also provided the individual PropBank, 
NomBank and TimeBank annotations below in 
textual form, in order to highlight potential 
points of interaction. 
 
PropBank:  and [Arg2 to a former federal bomb 
investigator], [Arg0 that description]  
[Rel_suggest.01 suggests]  [Arg1 [Arg1 a powerful 
military-style plastic explosive c-4] that 
7
  
 [ArgM-MOD can] be [Rel_cut.01 cut] or  [Rel_mold.01 
molded] [ArgM-RESULT into different shapes]]. 
 
NomBank: and to a former [Arg2 federal] [Arg1 
bomb] [Rel investigator], that description 
suggests a powerful [Arg2 military] - [Rel style] 
plastic [Arg1 explosive] c-4 that can be cut 
or molded into different shapes. 
 
TimeML: and to a former federal bomb 
investigator, that description [Event = ei1 
suggests]  a powerful military-style plastic 
explosive c-4 that  can be [Event = ei2 modal=?can? cut] 
or  [Event = ei3 modal=?can? molded]  into different 
shapes. <SLINK eventInstanceID = ei1 
subordinatedEventID = ei2 relType = ?Modal?/> 
<SLINK eventInstanceID = ei1 
subordinatedEventID = ei3 relType = ?Modal?/> 
 
  
Figure 1. Dependency Analysis of Sentence 2  
 
Note that the subordinating Events indicated by 
the TimeML SLINKS refer to the predicate 
argument structures labeled by PropBank, and 
that the ArgM-MODal also labeled by PropBank 
contains modality information also crucial to the 
SLINKS. While the grammatical modal on cut 
and mold is captured as an attribute value on the 
event tag, the governing event predicate suggest 
introduces a modal subordination to its internal 
argument, along with its relative clause. While 
this markup is possible in TimeML, it is difficult 
to standardize (or automate, algorithmically) 
since arguments are not marked up unless they 
are event denoting.  
 
3.2 A More Complex Example 
 
To better illustrate the interaction between 
annotation levels, and the importance of merging 
information resident in one level but not 
necessarily in another, consider the sentence 
below which has more complex temporal 
properties than the Pie-in-the-Sky sentences and 
its dependency analysis (Figure 2). 
 
 According to reports, sea trials for a patrol boat 
developed by Kazakhstan are being conducted 
and the formal launch is planned for the 
beginning of April this year.  
 
 
Figure 2.  Dependency Analysis of a Sentence 
with Interesting Temporal Properties 
 
The graph above incorporates these distinct 
annotations into a merged representation, much 
like the previous analysis. This sentence has 
more TimeML annotation than the previous 
sentence.  Note the loops of arcs which show that 
According to plays two roles in the sentence: (1) 
it heads a constituent that is the ARGM-ADV of 
the verbs conducted and planned; (2) it indicates 
that the information in this entire sentence is 
attributed to the reports. This loop is problematic 
in some sense because the adverbial appears to 
modify a constituent that includes itself. In 
actuality, however, one would expect that the 
ARGM-ADV role modifies the sentence minus 
the adverbial, the constituent that you would get 
if you ignore the transparent arc from ARGM-
8
  
ADV to the rest of the sentence.  Alternatively, a 
merging decision may elect to delete the ARGM-
ADV arcs, once the more specific predicate 
argument structure of the sentence adverbial 
annotation is available. 
 
The PropBank annotation for this sentence 
would label arguments for develop, conduct and 
plan, as given below. 
 
 [ArgM-ADV According to reports], [Arg1sea trials for  
[Arg1 a patrol boat] [Rel_develop.02 developed] [Arg0 
by Kazakhstan]] are being  
[Rel_conduct.01 conducted]  and [Arg1 the formal 
launch] is [Rel_plan.01 planned]  
[ArgM-TMP for the beginning of April this year].  
 
NomBank would add arguments for report, trial, 
launch and beginning as follows: 
 
 According to [Rel_report.01 reports], [Arg1 [ArgM-LOC 
sea [Rel_trial.01 trials] [Arg1 for [Arg1-CF_launch.01 a 
patrol boat] developed by Kazakhstan] are being 
conducted and the [ArgM-MNR formal] [Rel_launch.01 
launch] is planned for the [[REL_beginning.01 
beginning] [ARG1 of April this year]].  
 
TimeML, however, focuses on the anchoring of 
events to explicit temporal expressions (or 
document creation dates) through TLINKs, as 
well as subordinating relations, such as those 
introduced by modals, intensional predicates, 
and other event-selecting predicates, through 
SLINKs. For discussion, only part of the 
complete annotation is shown below.  
  
According to [Event = ei1  reports], sea [Event = ei3  
trials] for a boat [Event = ei4  developed]  by 
Kazakhstan are being [Event = ei5  conducted] and 
the formal [Event = ei6  launch] 
 is  [Event = ei7  planned] for the [Timex3= t1  beginning 
of April] [Timex3= t2 this year]. 
<SLINK eventID=?ei1? subordinatedEvent=?ei5, 
ei7? relType=EVIDENTIAL/> 
<TLINK eventID=?ei4? relatedToEvent =?ei3? 
relType=BEFORE/> 
<TLINK eventID=?ei6? relatedToTime=?t1? 
relType=IS_INCLUDED /> 
<SLINK eventID=?ei7? 
subordinatedEvent=?ei6? relType=?MODAL?/> 
<TLINK eventID=?ei5? relatedToEvent=?ei3? 
relType=IDENTITY/> 
 
Predicates such as plan and nominals such as 
report are lexically encoded to introduce 
SLINKs with a specific semantic relation, in this 
case, a ?MODAL? relType,. This effectively 
introduces an intensional context over the 
subordinated events. 
 
These examples illustrate the type of semantic 
representation we are trying to achieve.  It is 
clear that our various layers already capture 
many of the intended relationships, but they do 
not do so in a unified, coherent fashion.  Our 
goal is to develop both a framework and a 
process for annotation that allows the individual 
pieces to be automatically assembled into a 
coherent whole.   
 
4.0 Merging Annotations  
 
4.1 First Order Merging of Annotation 
We begin by discussing issues that arise in 
defining a single format for a merged 
representation of PropBank, NomBank and 
Coreference, the core predicate argument 
structures and  referents for the arguments.   One 
possible representation format would be to 
convert each annotation into features and values 
to be added to a larger feature structure. 1 The 
resulting feature structure would combine stand 
alone and offset annotation ? it would include 
actual words and features from the text as well as 
special features that point to the actual text 
(character offsets) and, perhaps, syntactic trees 
(offsets along the lines of PropBank/NomBank). 
Alternative global annotation schemes include 
annotation graphs (Cieri & Bird, 2001), and 
MATE (Carletta, et. al., 1999).  There are many 
areas in which the boundaries between these 
annotations have not been clearly defined, such 
as the treatment of support constructions and 
light verbs, as discussed below.  Determining the 
most suitable format for the merged 
representation should be a top priority. 
 
4.2 Resolving Annotation Overlap 
There are many possible interactions between 
different types of annotation: aspectual verbs 
have argument labels in PropBank, but are also 
important roles for temporal relations.  Support 
                                                 
 
1 The Feature Structure has many advantages as a target 
representation including: (1) it is easy to add lots of detailed 
features; and (2) the mathematical properties of Feature 
Structures are well understood, i.e., there are well-defined 
rule-writing languages, subsumption and unification 
relations, etc. defined for Feature Structures (Carpenter, 
1992) The downside is that a very informative Feature 
Structure is difficult for a human to read.  
 
9
  
constructions also have argument labels, and the 
question arises as to whether these should be 
associated with the support verb or the 
predicative nominal.  Given the sentence They 
gave the chefs a standing ovation, a PropBank 
component will assign role labels to arguments 
of give; a NomBank component will assign 
argument structure to ovation that labels the 
same participants. If the representations are 
equivalent, the question arises as to which of 
them (or both) should be included in the merged 
representation. The following graph  (Figure 3) 
is a combined PropBank and NomBank analysis 
of this sentence. "They" is the ARG0 of both 
"give" and "ovation"; "the chefs" is the ARG2 of 
"give", but the "ARG1" of ovation; "ovation" is 
the ARG1 of "give" and "give" is a support verb 
for "ovation". For this case, a reasonable choice 
might be to preserve the argument structure from 
both NomBank and PropBank, and to do the 
same for other predicative nominals that have 
give (or receive, obtain, request?) as a support 
verb, e.g., (give a kiss/hug/squeeze, give a 
lecture/speech, give a promotion, etc.).   For 
other support constructions, such as take a walk, 
have a headache and make a mistake, the noun is 
really the main predicate and it is questionable 
whether the verbal argument structure carries  
gave
chefsthe
They
a ovationstanding
NP
NP
S
ARG0
REL
ARG2
ARG1
NP
ARG1 REL
ARG0SUPPORT
 
Figure 3. Merged PropBank/NomBank representation 
of They gave the chefs a standing ovation. 
much information, e.g., there are no selection 
restrictions between light verbs and their subject 
(ARG0) -- these are inherited from the noun. 
Thus make a mistake selects a different type of 
subject than make a gain, e.g., people and 
organizations make mistakes, but stock prices 
make gains. For these constructions, the merged 
representation might not need to include the 
(ARG0) relation between the subject of the 
sentence and make, and future propbanking 
efforts might do well to ignore the shared 
arguments of such instances and leave them for 
NomBank. However, the merged representation 
would inherit PropBank?s annotation of some 
other light verb features including: negation, e.g., 
They did not take a walk; modality, e.g., They 
might take a walk; and sentence adverbials, e.g., 
They probably will take a walk. 
 
4.3 Resolving Annotation Conflicts 
Interactions between linguistic phenomena can 
aid in quality control, and conflicts found during 
the deliberate merging of different annotations 
provides an opportunity to correct and fine-tune 
the original layers. For example, predicate 
argument structure (PropBank and NomBank) 
annotation sometimes assumes different 
constituent structure than the Penn Treebank. We 
have noticed some tendencies that help resolve 
these conflicts, e.g., prenominal noun 
constituents as in Indianapolis 500, which forms 
a single argument in NomBank, is correctly 
predicted to be a constituent, even though the 
Penn Treebank II assumes a flatter structure.  
 
Similarly, idioms and multiword expressions 
often cause problems for both PropBank and 
NomBank. PropBank annotators tend to view 
argument structure in terms of verbs and 
NomBank annotators tend to view argument 
structure in terms of nouns. Thus many examples 
that, perhaps, should be viewed as idioms are 
viewed as special senses of either verbs or nouns. 
Having idioms detected and marked before 
propbanking and nombanking could greatly 
improve efficiency.   
 
Annotation accuracy is often evaluated in terms 
of inter-annotation consistency. Task definitions 
may need to err on the side of being more 
inclusive in order to simplify the annotators task. 
For example, the NomBank project assumes the 
following definition of a support verb (Meyers, 
et.al., 2004b):  ?? a verb which takes at least 
two arguments NP1 and XP2 such that XP2 is an 
argument of the head of NP1. For example, in 
John took a walk, a support verb (took) shares 
one of its arguments (John) with the head of its 
other argument (walk).? The easiest way to 
apply this definition is without exception, so it 
will include idiomatic expressions such as keep 
tabs on, take place, pull strings. Indeed, the 
dividing line between support constructions and 
idioms is difficult to draw (Meyers 2004b).   
PropBank annotators are also quite comfortable 
with associating general meanings to the main 
verbs of idiomatic expressions and labeling their 
10
  
argument roles, as in cases like bring home the 
bacon and mince words with. Since idioms often 
have interpretations that are metaphorical 
extensions of their literal meaning, this is not 
necessarily incorrect.  It may be helpful to have 
the literal dependencies and the idiomatic 
reading both represented. The fact that both 
types of meaning are available is evidenced by 
jokes, irony, and puns.  
 
With respect to idioms and light verbs, TimeML 
can be viewed as a mediator between PropBank 
and NomBank. In TimeML, light verbs and the 
nominalizations accompanying them are marked 
with two separate EVENT tags. This guarantees 
an annotation independent of textual linearity 
and therefore ensures a parallel treatment for 
different textual configurations. In (a) the light 
verb construction "make an allusion" is 
constituted of a verb and an NP headed by an 
event-denoting noun, whereas in (b) the nominal 
precedes a VP, which in addition contains a 
second N:  
(a) Max [made an allusion] to the crime.  
(b) Several anti-war [demonstrations have taken 
place] around the globe. 
Both verbal and nominal heads are tagged 
because they both contribute relevant 
information to characterizing the nature of the 
event. The nominal element plays a role in the 
more semantically based task of event 
classification. On the other hand, the information 
in the verbal component is important at two 
different levels: it provides the grammatical 
features typically associated with verbal 
morphology, such as tense and aspect, and at the 
same time it may help in disambiguating cases 
like take/give a class, make/take a phone call. 
The two tagged events are marked as identical by 
a TLINK introduced for that purpose. The 
TimeML annotation for the example in (a) is 
provided below.  
Max [Event = ei1  made] an [Event = ei2  allusion] to 
the crime.  
<TLINK eventID="ei1"relatedToEvent="ei2" 
relType=IDENTITY> 
Some cases of support in NomBank could also 
be annotated as "bridging" anaphora. Consider 
the sentence: The pieces make up the whole. 
It is unclear whether make up is a support verb 
linking whole as the ARG1 of pieces or if pieces 
is linked to whole by bridging anaphora.  
There are also clearer cases. In Nastase, a rival 
player defeated Jimmy Connors in the third 
round, the word rival and Jimmy Connors are 
clearly linked by bridging. However, a wayward 
NomBank annotator might construct a support 
chain (player + defeated) to link rival with its 
ARG1 Jimmy Connors.  In such a case, a 
merging of annotation could reveal annotation 
errors. In contrast, a NomBank annotator would 
be correct in linking John as an argument of walk 
in John took a series of walks (the support chain 
took + series consists of a support verb and a 
transparent noun), but this may not be obvious to 
the non-NomBanker. Thus the merging of 
annotation may result in the more consistent 
specifications for all.  
 
In our view, this process of annotating all layers 
of information and then merging them in a 
supervised manner, taking note of the conflicts, 
is a necessary prerequisite to defining more 
clearly the boundaries between the different 
types of annotation and determining how they 
should fit together.  Other areas of annotation 
interaction include: (1) NomBank  and 
Coreference, e.g. deriving that John teaches 
Mary from John is Mary's teacher involves: (a) 
recognizing that teacher is an argument 
nominalization such that the teacher is the ARG0 
of teach (the one who teaches); and (b) marking 
John and teacher as being linked by predication 
(in this case, an instance of type coreference); 
and (2) Time and Modality -  when a fact used to 
be true, there are two time components: one in 
which the fact is true and one in which it is false. 
Clearly more areas of interaction will emerge as 
more annotation becomes available and as the 
merging of annotation proceeds.  
 
5. Summary 
 
We proposed a way of taking advantage of the 
current practice of separating aspects of semantic 
analysis of text into small manageable pieces. 
We propose merging these pieces, initially in a 
careful, supervised way, and hypothesize that the 
result could be a more detailed semantic analysis 
than was previously available. This paper 
discusses some of the reasons that the merging 
process should be supervised. We primarily gave 
examples involving the interaction of PropBank, 
NomBank and TimeML. However, as the 
merging process continues, we anticipate other 
conflicts that will require resolution. 
 
References 
 
C. F. Baker, F. Collin, C. J. Fillmore, and J. B.  
Lowe (1998), The Berkeley FrameNet 
project. In Proc. of COLING/ACL-98,  86--90 
11
  
O. Babko-Malaya, M. Palmer, X. Nianwen, S.  
Kulick, A. Joshi (2004), Propbank II, 
Delving Deeper, In Proc.  of HLT-NAACL 
Workshop: Frontiers in Corpus Annotation. 
R. Carpenter (1992), The Logic of Typed  
Feature Structures. Cambridge Univ. Press. 
J. Carletta and A. Isard (1999), The MATE  
Annotation Workbench: User Requirements. 
In Proc. of the ACL Workshop: Towards 
Standards and Tools for Discourse Tagging. 
Univ. of Maryland, 11-17 
C. Cieri and S. Bird (2001), Annotation Graphs  
and Servers and Multi-Modal Resources: 
Infrastructure for  Interdisciplinary Education, 
Research and Development Proc. of the ACL 
Workshop on Sharing Tools and Resources 
for Research  and Education, 23-30 
D. Day,  L. Ferro, R. Gaizauskas, P. Hanks, M.  
Lazo, J. Pustejovsky, R. Saur?, A. See, A. 
Setzer, and B. Sundheim (2003), The 
TIMEBANK Corpus. Corpus Linguistics. 
M. Ellsworth, K. Erk, P. Kingsbury and S. Pado  
(2004), PropBank, SALSA, and FrameNet: 
How Design Determines Product, in Proc. of  
LREC 2004 Workshop: Building Lexical 
Resources from Semantically Annotated 
Corpora.  
C. Fellbaum (1997), WordNet: An Electronic  
Lexical Database, MIT Press.. 
C. J. Fillmore and B. T. S. Atkins (1998), 
FrameNet and lexicographic relevance. In the 
Proc. of the First International Conference 
on Language Resources and Evaluation.  
C. J. Fillmore and C. F. Baker (2001), Frame  
semantics for text understanding. In Proc. of 
NAACL WordNet and Other Lexical 
Resources Workshop. 
E. Hajivcova and I. Kuvcerov'a (2002).  
Argument/Valency Structure in PropBank, 
LCS Database and Prague Dependency 
Treebank: A Comparative Pilot Study. In the 
Proc. of the Third International Conference 
on Language Resources and Evaluation 
(LREC 2002),  846--851. 
S. Helmreich, D. Farwell, B. Dorr, N. Habash, L. 
    Levin, T. Mitamura, F. Reeder, K. Miller, E. 
     Hovy, O. Rambow and A. Siddharthan,(2004), 
     Interlingual Annotation of Multilingual Text 
     Corpora, Proc. of the HLT-EACL Workshop 
     on Frontiers in Corpus Annotation. 
A, Meyers, R. Reeves, C. Macleod, R, Szekely,  
V. Zielinska, B. Young, and R. Grishman  
(2004a), The NomBank Project: An Interim 
Report, Proc. of HLT-EACL Workshop: 
Frontiers in Corpus Annotation. 
A. Meyers, R. Reeves, and C. Macleod (2004b),  
NP-External Arguments: A Study of 
Argument Sharing in English. In The ACL 
2004 Workshop on Multiword Expressions: 
Integrating Processing. 
E. Miltsakaki, R. Prasad, A. Joshi and B. Webber. 
 (2004a), The Penn Discourse Treebank. In 
Proc. 4th International Conference on 
Language Resources and Evaluation (LREC 
2004). 
E. Miltsakaki, R. Prasad, A. Joshi and B. Webber  
(2004b), Annotation of Discourse 
Connectives and their Arguments, in Proc. of 
HLT-NAACL Workshop: Frontiers in Corpus 
Annotation 
M.  Marcus, B. Santorini, and M. Marcinkiewicz  
(1993), Building a large annotated corpus of 
english: The penn treebank. Computational 
Linguistics, 19:313--330. 
M. Palmer, D. Gildea, P. Kingsbury (2005), The  
Proposition Bank: A Corpus Annotated with 
Semantic Roles, Computational Linguistics 
Journal, 31:1. 
M. Poesio (2004a), The MATE/GNOME  
Scheme for Anaphoric Annotation, Revisited, 
Proc. of SIGDIAL 
M. Poesio (2004b), Discourse Annotation and  
Semantic Annotation in the GNOME Corpus, 
Proc. of ACL Workshop on Discourse 
Annotation. 
M. Poesio and M. Alexandrov-Kabadjov (2004), 
A general-purpose, off-the-shelf system for 
anaphora resol.. Proc. of LREC. 
M. Poesio, F. Bruneseaux, and L. Romary  
(1999), The MATE meta-scheme for 
coreference in dialogues in multiple language, 
Proc. of the ACL Workshop on Standards for 
Discourse Tagging.  
M. Poesio and R. Vieira (1998), A corpus-based  
investigation of definite description use. 
Computational Linguistics, 24(2). 
C. Pollard and I. A. Sag (1994), Head-driven  
phrase structure grammar. Univ. of Chicago 
Press. 
J. Pustejovsky, R. Saur?, J. Casta?o, D. R. 
 Radev, R. Gaizauskas, A. Setzer, B. 
Sundheim and G. Katz (2004), Representing 
Temporal and Event Knowledge for QA 
Systems. In Mark T. Maybury (ed.), New 
Directions in Question Answering, MIT Press. 
J. Pustejovsky,  B. Ingria, R. Saur?, J. Casta?o, J.  
Littman, R. Gaizauskas, A. Setzer, G. Katz, 
and I. Mani (2003), The Specification 
Language TimeML. In I. Mani, J. 
Pustejovsky, and R. Gaizauskas, editors, The 
Language of Time: A Reader. Oxford Univ. 
Press. 
12
Addressing the Resource
Bottleneck to Create
Large-Scale Annotated Texts
Jon Chamberlain
University of Essex (UK)
email: jchamb@essex.ac.uk
Massimo Poesio
University of Essex (UK) & Universit? di Trento (Italy)
email: poesio@essex.ac.uk
Udo Kruschwitz
University of Essex (UK)
email: udo@essex.ac.uk
Abstract
Large-scale linguistically annotated resources have become available in
recent years. This is partly due to sophisticated automatic and semi-
automatic approaches that work well on specific tasks such as part-of-
speech tagging. For more complex linguistic phenomena like anaphora
resolution there are no tools that result in high-quality annotations with-
out massive user intervention. Annotated corpora of the size needed for
modern computational linguistics research cannot however be created by
small groups of hand annotators. The ANAWIKI project strikes a balance
between collecting high-quality annotations from experts and applying a
game-like approach to collecting linguistic annotation from the general
Web population. More generally, ANAWIKI is a project that explores to
what extend expert annotations can be substituted by a critical mass of
non-expert judgements.
375
376 Chamberlain, Poesio, and Kruschwitz
1 Introduction
Syntactically annotated language resources have long been around, but the greatest
obstacle to progress towards systems able to extract semantic information from text
is the lack of semantically annotated corpora large enough to be used to train and
evaluate semantic interpretation methods. Recent efforts to create resources to sup-
port large evaluation initiatives in the USA such as Automatic Context Extraction
(ACE), Translingual Information Detection, Extraction and Summarization (TIDES),
and GALE are beginning to change this, but just at a point when the community is
beginning to realize that even the 1M word annotated corpora created in substantial
efforts such as Prop-Bank (Palmer et al, 2005) and the OntoNotes initiative (Hovy
et al, 2006) are likely to be too small.
Unfortunately, the creation of 100M-plus corpora via hand annotation is likely to
be prohibitively expensive. Such a large hand-annotation effort would be even less
sensible in the case of semantic annotation tasks such as coreference or wordsense
disambiguation, given on the one side the greater difficulty of agreeing on a ?neutral?
theoretical framework, on the other the difficulty of achieving more than moderate
agreement on semantic judgments (Poesio and Artstein, 2005).
The ANAWIKI project1 presents an effort to create high-quality, large-scale anaphor-
ically annotated resources (Poesio et al, 2008) by taking advantage of the collabora-
tion of the Web community, both through co-operative annotation efforts using tra-
ditional annotation tools and through the use of game-like interfaces. This makes
ANAWIKI a very ambitious project. It is not clear to what extend expert annotations
can in fact be substituted by those judgements submitted by the general public as part
of a game. If successful, ANAWIKI will actually be more than just an anaphora anno-
tation tool. We see it as a framework aimed at creating large-scale annotated corpora
in general.
2 Creating Resources through Web Collaboration
Large-scale annotation of low-level linguistic information (part-of-speech tags) be-
gan with the Brown Corpus, in which very low-tech and time consuming methods
were used; but already for the creation of the British National Corpus (BNC), the first
100M-word linguistically annotated corpus, a faster methodology was developed con-
sisting of preliminary annotation with automatic methods followed by partial hand-
correction (Burnard, 2000). Medium and large-scale semantic annotation projects
(coreference, wordsense) are a fairly recent innovation in Computational Linguistics
(CL). The semi-automatic annotation methodology cannot yet be used for this type of
annotation, as the quality of, for instance, coreference resolvers is not yet high enough
on general text.
Collective resource creation on the Web offers a different way to the solution of
this problem. Wikipedia is perhaps the best example of collective resource creation,
but it is not an isolated case. The willingness of Web users to volunteer on the Web
extends to projects to create resources for Artificial Intelligence. One example is the
OpenMind Commonsense project, a project to mine commonsense knowledge (Singh,
2002) to which 14,500 participants contributed nearly 700,000 sentences. A more
1http://www.anawiki.org
Addressing the Resource Bottleneck to Create Annotated Texts 377
recent, and perhaps more intriguing, development is the use of interactive game-style
interfaces to collect knowledge such as von Ahn et al (2006). Perhaps the best known
example of this approach is the ESP game, a project to label images with tags through
a competitive game (von Ahn, 2006); 13,500 users played the game, creating 1.3M
labels in 3 months. If we managed to attract 15,000 volunteers, and each of them were
to annotate 10 texts of 700 words, we would get a corpus of the size of the BNC.
ANAWIKI builds on the proposals for marking anaphoric information allowing for
ambiguity developed in ARRAU (Poesio and Artstein, 2005) and previous projects.
The ARRAU project found that (i) using numerous annotators (up to 20 in some ex-
periments) leads to a much more robust identification of the major interpretation al-
ternatives (although outliers are also frequent); and (ii) the identification of alternative
interpretations is much more frequently a case of implicit ambiguity (each annotator
identifies only one interpretation, but these are different) than of explicit ambiguity
(annotators identifying multiple interpretations). The ARRAU project also developed
methods to analyze collections of such alternative interpretations and to identify out-
liers via clustering that will be exploited in this project.
Figure 1: A screenshot of the Serengeti expert annotation tool.
3 Annotation Tools
Attempts to create hand annotated corpora face the dilemma of either going for the
traditional CL approach of high-quality annotation (of limited size) by experts or to
involve a large population of non-experts which could result in large-scale corpora
of inferior quality. The ANAWIKI project bridges this gap by combining both ap-
proaches to annotate the data: an expert annotation tool and a game interface. Both
378 Chamberlain, Poesio, and Kruschwitz
Figure 2: A screenshot of the Game Interface (Annotation Mode).
tools are essential parts of ANAWIKI.We briefly describe both, with a particular focus
on the game interface.
3.1 Expert Annotation Tool
An expert annotation tool is used to obtain Gold Standard annotations from computa-
tional linguists. In the case of anaphora annotationwe use the Serengeti tool developed
at the University of Bielefeld (St?hrenberg et al, 2007). The anaphoric annotation of
markables within this environment will be very detailed and will serve as a training
corpus as well as quality check for the second tool (see below). Figure 1 is a screen-
shot of this interface.
3.2 Game Interface
A game interface is used to collect annotations from the general Web population. The
game interface integrates with the database of the expert annotation tool but aims to
collect large-scale (rather than detailed) anaphoric relations. Users are simply asked
to assign an anaphoric link but are not asked to specify what type (or what features)
are present.
Phrase Detectives2 is a game offering a simple user interface for non-expert users
to learn how to annotate text and to make annotation decisions. The goal of the game
is to identify relationships between words and phrases in a short text. Markables are
2http://www.phrasedetectives.org
Addressing the Resource Bottleneck to Create Annotated Texts 379
identified in the text by automatic pre-processing. There are 2 ways to annotate within
the game: by selecting the markable that is the antecedent of the anaphor (Annotation
Mode ? see Figure 2); or by validating a decision previously submitted by another
user (Validation Mode). One motivation for Validation Mode is that we anticipate it
to be twice as fast as Annotation Mode (Chklovski and Gil, 2005).
Users begin the game at the training level and are given a set of annotation tasks
created from the Gold Standard. They are given feedback and guidance when they
select an incorrect answer and points when they select the correct answer. When
the user gives enough correct answers they graduate to annotating texts that will be
included in the corpus. Occasionally, a graduated user will be covertly given a Gold
Standard text to annotate. This is the foundation of the user rating system used to
judge the quality of the user?s annotations.
The game is designed to motivate users to annotate the text correctly by using com-
parative scoring (awarding points for agreeing with the Gold Standard), and retroac-
tive scoring (awarding points to the previous user if they are agreed with by the current
user). Using leader boards and assigning levels for points has been proven to be an
effective motivator, with users often using these as targets (von Ahn, 2006). The game
interface is described in more detail elsewhere (Chamberlain et al, 2008).
4 Challenges
We are aiming at a balanced corpus, similar to the BNC, that includes texts from
Project Gutenberg, the Open American National Corpus, the Enron corpus and other
freely available sources. The chosen texts are stripped of all presentation formatting,
HTML and links to create the raw text. This is automatically parsed to extract mark-
ables consisting of noun phrases. The resulting XML format is stored in a relational
database that can be used in both the expert annotation tool and the game.
There are a number of challenges remaining in the project. First of all, the fully
automated processing of a substantial (i.e. multi-million) word corpus comprising
more than just news articles turned out to be non-trivial both in terms of robustness of
the processing tools as well as in terms of linguistic quality.
A second challenge is to recruit enough volunteers to annotate a 100 million word
corpus within the timescale of the project. It is our intention to use social networking
sites (including Facebook, Bebo, and MySpace) to attract volunteers to the game and
motivate participation by providing widgets (code segments that display the user?s
score and links to the game) to add to their profile pages.
Finally, the project?s aim is to generate a sufficiently large collection of annotations
from which semantically annotated corpora can be constructed. The usefulness of the
created resources can only be proven, for example, by training anaphora resolution
algorithms on the resulting annotations. This will be future work.
5 Next Steps
We are currently in the process of building up a critical mass of source texts. Our aim
is to have a corpus size of 1M words by September 2008. By this time we also intend
having a multilingual user interface (initially English, Italian and German) with the
capacity to annotate texts in different languages although this is not the main focus.
380 Chamberlain, Poesio, and Kruschwitz
In the future we will be considering extending the interface to include different anno-
tation tasks, for example marking coreference chains or Semantic Web mark-up. We
would like to present the game interface to gain feedback from the linguistic commu-
nity.
Acknowledgements
ANAWIKI is funded by EPSRC (EP/F00575X/1). Thanks to Daniela Goecke, Maik
St?hrenberg, Nils Diewald and Dieter Metzing. We also want to thank all volunteers
who have already contributed to the project and the reviewers for valuable feedback.
References
Burnard, L. (2000). The British National Corpus Reference guide. Technical report,
Oxford University Computing Services, Oxford.
Chamberlain, J., M. Poesio, and U. Kruschwitz (2008). Phrase Detectives: A Web-
based Collaborative Annotation Game. In Proceedings of the International Con-
ference on Semantic Systems (I-Semantics?08), Graz. Forthcoming.
Chklovski, T. and Y. Gil (2005). Improving the design of intelligent acquisition in-
terfaces for collecting world knowledge from web contributors. In Proceedings of
K-CAP ?05, pp. 35?42.
Hovy, E., M.Marcus, M. Palmer, L. Ramshaw, and R. Weischedel (2006). OntoNotes:
The 90% Solution. In Proceedings of HLT-NAACL06.
Palmer, M., D. Gildea, and P. Kingsbury (2005). The proposition bank: An annotated
corpus of semantic roles. Computational Linguistics 31(1), 71?106.
Poesio, M. and R. Artstein (2005). The reliability of anaphoric annotation, recon-
sidered: Taking ambiguity into account. In Proceedings of the ACL Workshop on
Frontiers in Corpus Annotation, pp. 76?83.
Poesio, M., U. Kruschwitz, and J. Chamberlain (2008). ANAWIKI: Creating anaphor-
ically annotated resources through Web cooperation. In Proceedings of LREC?08,
Marrakech.
Singh, P. (2002). The public acquisition of commonsense knowledge. In Proceedings
of the AAAI Spring Symposium on Acquiring (and Using) Linguistic (and World)
Knowledge for Information Access, Palo Alto, CA.
St?hrenberg, M., D. Goecke, N. Diewald, A. Mehler, and I. Cramer (2007). Web-
based annotation of anaphoric relations and lexical chains. In Proceedings of the
ACL Linguistic Annotation Workshop, pp. 140?147.
von Ahn, L. (2006). Games with a purpose. Computer 39(6), 92?94.
von Ahn, L., R. Liu, and M. Blum (2006). Peekaboom: a game for locating objects in
images. In Proceedings of CHI ?06, pp. 55?64.
Proceedings of the NAACL HLT 2010 First Workshop on Computational Neurolinguistics, pages 36?44,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Detecting Semantic Category in
Simultaneous EEG/MEG Recordings
Brian Murphy
Centre for Mind/Brain Sciences
University of Trento
corso Bettini 31,
38068 Rovereto, Italy
brian.murphy@unitn.it
Massimo Poesio
Centre for Mind/Brain Sciences
University of Trento
corso Bettini 31,
38068 Rovereto, Italy
massimo.poesio@unitn.it
Abstract
Electroencephalography (EEG) and magne-
toencephalography (MEG) are closely related
neuroimaging technologies that both measure
summed electrical activity of synchronous
sources of neural activity. However they dif-
fer in the portions of the brain to which they
are more sensitive, in the frequency bands they
can detect, and to the amount of noise to which
they are subject. Since semantic representa-
tions are thought to be widely distributed in
the brain, this preliminary study considered
if the broader coverage offered by simulta-
neous EEG/MEG recordings would increase
sensitivity to these cognitive states. The re-
sults showed that MEG data allowed stim-
uli in two semantic categories (mammals and
tools) to be distinguished more accurately, de-
spite some experimental settings that were op-
timised for EEG. The addition of EEG data
did not prove informative, indicating that it
may be redundant relative to MEG, even when
using dimensionality reduction techniques to
combat overfitting.
1 Introduction
Electroencephalography (EEG) and magnetoen-
cephalography (MEG) are similar methods for
recording activity in the brain. Both detect signals
that are produced by the mixing of neural sources,
where each source represents macro-scale synchro-
nisation between the firing of individual neurons.
The sum of these activities induce voltages at the
scalp that are recorded with EEG, and magnetic
fields that are detected with MEG. But the signals
yielded by each technique are not identical for sev-
eral reasons. EEG signals are heavily attenuated
and filtered (both in time in space) by the passage
through skull and tissue. As a result, MEG signals
are less noisy, have finer spatial resolution, capture
a wider range of frequencies, and so have the po-
tential to be more informative. Further, the signal
footprint of MEG and EEG signals on the brain is
not the same: EEG sensors are more sensitive to
currents that are radial to the scalp and so predomi-
nantly detect activity in the at the top of gyri and the
bottom of sulci (the top and bottom of folds in the
surface of the brain); while MEG is more sensitive
to currents that are tangential to the scalp, and so
detects more activity in the side walls of sulci. The
high spatial resolution of MEG means that it cannot
see as deeply into the brain as EEG can. Finally,
MEG sensors of different types (in this case mag-
netometers and planar gradiometers) are sensitive to
magnetic fields of different orientations (see Figure
1): planar gradiometers are most sensitive to current
generators of a particular orientation directly under
the sensor position; magnetometers record genera-
tors that are tangential and peripheral to the sensor
area.
The distribution of sensor coverage may be im-
portant for the decoding of semantic categories in
particular. Neuroimaging evidence suggests that se-
mantic representations may be widely distributed in
the brain. For example there are well-established
differences in neural activity in the fusiform gyrus
that correspond to higher level categories (natural
vs non-natural kinds; people vs places - see e.g.
Chao et al, 2002); there is also evidence that the
36
Figure 1: Schematic from above of selective sensitivity
of three co-located MEG sensors
Left and centre panels show perpendicular planar gradiometers;
right panel shows magnetometer. A co-located EEG electrode
would be most sensitive to currents perpendicular to the scalp.
Image courtesy of Elekta AB.
meaning of bodily actions is encoded in the motor-
cortex (Pulvermu?ller, 2005); and concepts associ-
ated with eating (e.g. foodstuffs) seem to be repre-
sented at least in part by activations in gustatory cor-
tex (Mitchell et al, 2008; Just et al, 2010). Hence
a wide coverage of sensors that are sensitive to dif-
ferent but overlapping portions of brain tissue may
provide a fuller description of semantic memories.
Given the fact that it has been possible to decode
conceptual categories and language semantics from
EEG signals (Murphy et al, 2008, 2009), the ques-
tion is if MEG signals can be shown to be more in-
formative. Similar studies on lower-level tasks typi-
cally used in brain-computer interfaces suggests that
it may be: Hill et al (2006) find that there is a
modest increase in the decoding accuracy on imag-
ined motor activity with MEG, relative to EEG, and
Waldert et al (2008) have similar findings detecting
the direction of hand movements.
A related question is whether the information sup-
plied by EEG and MEG is complementary, and if
so how best it should be combined. This depends
critically on the number of signals used: raising the
number of input signals increases the information
supplied to the machine learning methods, but in-
teracts with their tendency to overfit, if the number
of descriptive dimensions (recorded signals) is of a
similar order of magnitude to the number of training
cases (experimental trials in which a stimulus is pre-
sented). This is often the case with data from neu-
roimaging experiments, as there are practical limita-
tions on the number of data points that can be col-
lected: individual stimuli must usually be separated
by several seconds so that neural signals can return
to baseline between each, and participants can usu-
ally only be expected to perform a task at full at-
tention for 60 minutes or so, in such experimental
environments.
To investigate this question, we replicated an ex-
isting EEG experiment (Murphy et al, 2010). In that
experiment participants had been presented with im-
ages of animals and tools, while EEG activity was
recorded at 64 standard 10-10 locations, and sin-
gle trials (stimulus presentations) could be classi-
fied as representing the category of animal or tool
with an average accuracy of 72% over all seven
participants. The classification methods used were
an adaptive time/frequency window optimisation
(Dalponte et al, 2007), a supervised spatial com-
ponent signal decomposition (Common Spatial Pat-
terns, Koles et al, 1990) that yielded measures of
neural activity based on signal power, and a support-
vector machine (Boser et al, 1992).
The replication experiment reported here was car-
ried out with two participants, and used the same
task and materials, while simultaneously recording
with a 306-channel MEG system (204 gradiometers,
102 magnetometers) and a high-density 124-channel
EEG system. This data was then analysed using the
same machine learning methods as previously, but
varying the number and type of input signals, and
using dimensionality reduction to address increased
dimensionality.
2 Methods
2.1 Experiment and Materials
Two male native speakers of Italian took part in
the study, aged 30 and 47. Both were right-handed
with corrected or normal vision. Participants in this
study receive compensation of 7 euros per hour. The
experiment is conducted under the approval of the
ethics committee at the University of Trento, and
participants gave informed consent.
The participants were asked to perform a silent
naming task on grey-scale images of 30 land-
mammals and 30 work tools. Each stimulus was
presented between four and six times, in randomised
order.1 The participants sat in a relaxed upright posi-
1Participant 1 saw 264 stimulus trials (144 mammal and 120
tool trials); participant 2 saw 360 (180 in each class).
37
tion 1.5m from a projector screen in moderate light-
ing conditions. Images were presented on a medium
grey background and fell within a 6 degree viewing
angle. The task duration was split into five blocks
and the participants were given the choice to pause
between each. The cumulative task time did not ex-
ceed 45 minutes.
Each trial began with the presentation of a fixa-
tion cross for 0.25s, followed by the stimulus image,
a further fixation cross for 0.75s and a blank screen
for 1s. Participants were instructed to silently name
the object represented in their native tongue (Ital-
ian), using the first appropriate label that came to
mind, and to press the keyboard space-bar with the
left-hand to indicate they had found an appropriate
word. If the participant could not think of a suitable
label, they were asked not to make a response. The
image remained on the screen until the participant
responded, or until a time-out of three seconds was
reached. The participants were asked to keep still
during the task, and to avoid eye-movements and fa-
cial muscle activity in particular, except during the
blank period.
The materials were chosen to represent well-
defined semantic categories and to minimise non-
semantic, associative confounds. The set of 30 land
mammals were chosen to be both non-domesticated
and non-threatening, to avoid emotional valence
whether positive (e.g. pets) or negative (e.g. preda-
tors). Thirty hardware and garden implements were
chosen as genuine work tools. Appropriate pho-
tographs were sourced from the internet, and nor-
malised visually: each image file measured 300 pix-
els square; the image proper was converted to grey-
scale, superimposed on a homogeneous light-grey
background and had maximal horizontal and vertical
dimensions of 250 pixels; image contrast was nor-
malised. The concepts represented are listed below.
Land Mammals ant-eater, armadillo, badger,
beaver, bison, boar, camel, chamois, chim-
panzee, deer, elephant, fox, giraffe, gorilla,
hare, hedgehog, hippopotamus, ibex, kan-
garoo, koala, llama, mole, monkey, mouse,
otter, panda, rhinoceros, skunk, squirrel, zebra
(Italian formichiere, armadillo, tasso, castoro,
bisonte, cinghiale, cammello, camoscio, scim-
panz, cervo, elefante, volpe, giraffa, gorilla,
coniglio, riccio, ippopotamo, stambecco,
canguro, koala, lama, talpa, scimmia, topo,
lontra, panda, rinoceronte, puzzola, scoiattolo,
zebra)
Work Tools Allen key, axe, chainsaw, craft-knife,
crowbar, file, garden fork, garden trowel, hack-
saw, hammer, mallet, nail, paint brush, paint
roller, penknife, pick-axe, plaster trowel, pliers,
plunger, pneumatic drill, power-drill, rake, saw,
scissors, scraper, screw, screwdriver, sickle,
spanner, tape-measure (Italian brugola, ascia,
motosega, taglierino, piede di porco, lima,
forcone, paletta, seghetto, martello, mazza,
chiodo, pennello, rullo, coltellino svizzero, pic-
cone, cazzuola, pinza, stura lavandini, martello
pneumatico, trapano, rastrello, sega, forbici,
spatola, vite, cacciavite, falce, chiave inglese,
metro)
2.2 Neural Recordings
The experiment was conducted at the LNiF imag-
ing laboratories at the University of Trento, using a
306-sensor Elekta Neuromag system (2 planar gra-
diometers and 1 magnetometer at each of 102 sensor
locations). A dense-coverage 124-electrode EEG
cap was used also, using a right mastoid reference
and forehead ground. Both sets of signals were
recorded simultaneously at 1000Hz in a magneti-
cally shielded room. At the start of the session the
relative positions of the MEG and EEG sensors were
determined using a Polyhemus 3-D digitisation sys-
tem.
Data preprocessing was conducted using the
MNE, FieldTrip and EEGLAB packages.2 The data
was band-pass filtered at 1-50Hz to remove slow
drifts in the signal and high-frequency noise, and
then down-sampled to 125Hz. Eye and muscle arte-
facts were not removed, but these lie outside the
range of frequencies that were considered in the
analysis described below.
2Martinos Centre for Biomedical Imaging
(http://www.nmr.mgh.harvard.edu/martinos/); Don-
ders Institute for Brain, Cognition and Behaviour
(http://www.ru.nl/neuroimaging/fieldtrip); and Schwartz Center
for Computational Neuroscience (http://sccn.ucsd.edu/eeglab/)
respectively.
38
2.3 Analysis
The analysis method first applies a time/frequency
filter to select an information-rich band and inter-
val for the distinction of interest; a supervised de-
composition to extract components of whole-scalp
synchronous activity that are sensitive to this class
distinction (Common Spatial Patterns, or CSP ? see
Parra et al, 2005; Model and Zibulevsky, 2006;
Philiastides et al, 2006 for examples of other ap-
plications to cognitive neuroscience); and a gen-
eral purpose machine learning algorithm (Support-
Vector Machine or SVM) that uses the resulting
measures of signal power to predict the semantic
class of each trial. Individual trial epochs are arbi-
trarily allocated to one of k interlaced partitions of
equal size in a k-fold training/evaluation procedure.
The time/frequency filter applied here was
adopted from the earlier experiment, as it had been
found to provide optimal separation between trials
of the two classes over the participants of that study.
Using this common window (4-18Hz, 95-360ms af-
ter image onset) allows direct comparison between
the informativity of each type of sensor, or combina-
tion of sensor types. However this may disadvantage
MEG, since it is more sensitive to higher frequency
activity (> 50Hz), which at least one study has
found to vary systematically with semantic classes
(Tanji et al, 2005).
The decomposition method used, CSP (Koles
et al, 1990), extracts spatial components of elec-
trophysiological activity (linear combinations of raw
signals) that correspond to synchronous neural sub-
assemblies. It is a supervised technique that yields
signals whose level of activity (measured as signal
power) is modulated by the binary class distinction
of interest ? that is signals that show high power
when processing mammal concepts, and low power
when processing tool concepts, or vice-versa. CSP
identifies C components (where C is the number of
input channels) that are ranked by their sensitivity
to the class-separation of interest, in terms of op-
timal variance for the two populations of signals
(i.e., high variance between classes and low vari-
ance within classes). In this case we selected the
first and the last rows of this matrix (Ramoser et al,
2000) as the components that are most representa-
tive for the classes mammals and tools, respectively.
This procedure can be interpreted as extracting the
event-related spectral activity (i.e. the relative event-
related synchronisation) of two synchronous neural
structures which have been found to have an op-
timally differential response to the semantic cate-
gories of interest.
The final categorisation step is based on a
Support-Vector Machine (SVM) classifier (Boser
et al, 1992; Vapnik, 1998). The input for each
trial consisted of two measures of neural activity
extracted from the category-sensitive signal compo-
nents: the variance of the waveform, which is pro-
portional to signal power. The features were further
normalised by taking the log, and scaling to a range
of -1 to +1 across all trials. The SVM implementa-
tion used was LIBSVM (Chang and Lin, 2001), and
default parameters were used (radial basis function
kernel, cost parameter of 1, and a gamma value of
the inverse of the number of data-points).3 Test and
training data were kept strictly separate at all stages
of analysis.
In the results that follow here, these techniques
were first applied as before to replicate the previ-
ous experiment, but then also with an additional
step of dimensionality reduction to address the over-
fitting we expected given the dramatically larger
number of input channels (up to 430 if all EEG
and MEG channels were used, compared to 64
channels in the previous experiment). The signal
recorded in any individual channel will be com-
prised of a mix of genuine neural activity (both
relevant and irrelevant to our classification task),
systematic noise sources (e.g. 50Hz electrical line
noise, eye-movement artefacts, heart-beat artefacts),
and additional random noise. And as EEG and MEG
channels record activity from partially overlapping
portions of brain tissue, there is considerable re-
dundancy between neighbouring channels. Princi-
ple Components Analysis (PCA) is a dimensionality
reduction technique that addresses both these issues,
grouping redundant activity into the first (strongest)
3No optimisation of SVM parameters was attempted, as ex-
tensive parameter testing in the earlier experiment did not yield
any improvements in classification performance. We believe
that this is because CSP is in itself a powerful data-mining tech-
nique, that here typically yields two simple clusters of data cor-
responding to each semantic category. We expect a simple lin-
ear classifier would have similar performance on this task.
39
components, and relegating random noise to the last
components. Where PCA was used, it was ap-
plied directly before the CSP-based extraction of
category-specific sources.
3 Results
In the previous EEG experiment, the classification
accuracy averaged 72%, but varied substantially
from one participant to the next, ranging from 56%
to 80%. First we wanted to establish how repre-
sentative these two new simultaneous MEG/EEG
sessions had been, by replicating the EEG-based
analysis. To do this, an arbitrary subset of the 60
EEG channels were selected (taking roughly every
second channel among the total of 124), the stan-
dard time/frequency filter window was applied, and
the resulting data was classified using a 5-fold test-
training procedure.4 The first participant?s data was
typical of the previous cohort, classifying with accu-
racy of 70% (in this session, accuracy over 61% is
significant at p < 0.05, using a one-sided binomial
test, n = 264, p = 0.54), while the second partici-
pant?s data only achieved 52% accuracy (accuracy
over 56% significant at p< 0.05, n= 360, p= 0.5).
To get a first impression of the relative informa-
tivity of each signal type, the same procedure was
performed with subsets of 60 MEG channels: mag-
netometers alone yielded markedly higher results
(78% and 61% for participants 1 and 2 respectively),
while planar gradiometers alone gave marginally
lower results (67% and 48% respectively).
Next, to examine the effect of increasing the
amount of input data, we performed these analyses
using all available channels of each type. In one case
(participant 1, magnetometers) there was a drop in
5% points, and another (participant 2, magnetome-
ters) an increase of 3% points, but generally this had
little effect on results, indicating that in most cases
any increase in available information was offset by
overfitting.
These results are summarised in the first two
columns of in Tables 1 and 2. The tables also show
4In each test/training partition, the labelled training data
alone was used to derive two category specific scalp-maps.
These scalp-maps were used to extract signal components and
resulting signal power measures for all trials. The data was then
partitioned again along the same folds for SVM training and
prediction.
Table 1: Classification accuracy, participant 1
Type (available signals) 60 ch. all ch. 60 cp.
EEG (124) 70% 69% 76%
Magnetometers (102) 78% 73% 78%
Gradiometers (204) 67% 66% 71%
Mag.+Grad. (306) 72% 63% 77%
EEG+Mag. (224) 68% 67% 77%
EEG+Grad. (328) 69% 54% 73%
EEG+Mag.+Grad. (430) 72% 55% 77%
ch: raw channel input; cp: PCA component input
significance: 61% at p < 0.05; 65% at p < 0.001
Table 2: Classification accuracy, participant 2
Type (available signals) 60 ch. all ch. 60 cp.
EEG (124) 52% 50% 52%
Magnetometers (102) 61% 64% 68%
Gradiometers (204) 48% 51% 60%
Mag.+Grad. (306) 63% 50% 56%
EEG+Mag. (224) 56% 53% 58%
EEG+Grad. (328) 52% 53% 62%
EEG+Mag.+Grad. (430) 58% 51% 55%
ch: raw channel input; cp: PCA component input
significance: 56% at p < 0.05; 59% at p < 0.001
the results for all possible combinations of the three
signal types, and it is apparent that the effect of over-
fitting is more pronounced for these larger signal
sets. And though the base level of classification ac-
curacy is very different for these two participants,
both show a similar pattern with respect to signal
type and dimensionality: magnetometers are most
informative for these semantic distinctions, and all
signal types are vulnerable to overfitting effects.
To combat overfitting, we repeated these analy-
ses with dimensionality reduction. Since PCA is
an unsupervised technique, the components were
derived and extracted in one step over the whole
data set. The first (strongest) 60 components were
then taken as input to the same analysis procedure
as before (CSP-derived signal power estimates fed
to the SVM), to give a global description of whole
scalp neural activity, presumably with reduced re-
dundancy and noise. As can be seen in the final
columns of Tables 1 and 2, this resulted in optimal
classification accuracy in almost all cases, both rel-
ative to the full collections of signals, and the 60
40
channel subsets.
A serious limitation of these results however is the
arbitrary selection of signal subsets. While much of
the information recorded between signals is likely
redundant, it could be that the random inclusion or
exclusion of one channel or component could dra-
matically affect accuracy, if that signal was particu-
larly informative, or particularly subject to spurious
noise. So to have a more comprehensive view, we
conducted an exhaustive parameter search through
possible subsets of each combination of signal type,
increasing set size in steps of five, and calculating
average classification accuracy with a moving win-
dow of nine points. The results are illustrated in Fig-
ures 2 (using the raw signals as input) and 3 (using
PCA components of each signal set), and show the
average prediction performance across both experi-
mental participants.
Several things stand out when considering the dif-
ference between the classification performance us-
ing raw signals directly, and dimensionality reduced
sets. In the PCA case, the classification accuracy
levels start higher, rise faster, and peak earlier in
almost all cases. In absolute terms optimum per-
formance is little changed for magnetometer and
EEG signals alone (peaking just above 70% and
60% respectively), while gradiometers seem to ben-
efit somewhat (by about 3% points). But the PCA
lines are also smoother, reflecting more stability in
classification, and so more independence from par-
ticular parameter settings.
Common to both plots is that magnetometers are
the most informative type, followed consecutively
by gradiometers and EEG channels. In terms of mu-
tual redundancy, the information encoded in EEG
channels seems to largely be a subset of that en-
coded by gradiometers (gradiometer performance is
not improved by the addition of EEG channels). The
interaction of magnetometer data and these signal
types is more complex ? magnetometer performance
is reduced by the addition of either or both EEG and
gradiometer channels.
4 Conclusion
This paper reports only two sessions of simultane-
ous MEG/EEG recording, and there were some clear
differences in the results for each participant, so the
conclusions must be considered tentative. Neverthe-
less they suggest that EEG data are to a large ex-
tent redundant with respect to MEG signals. MEG
magnetometers in particular can lead to substantially
higher classification accuracy, with smaller numbers
of channels, than EEG alone. In the case of the sec-
ond participant, prediction with EEG signals did not
approach significance, while MEG signals allowed
highly significant (p 0.001) performance. We be-
lieve that this advantage is due to the lack of attenu-
ation and higher spatial resolution inherent in MEG,
allowing it to pick out individual neural sources with
more precision.
Regardless of the signal types chosen, the high
dimensionality of the data posed challenges. Any
arbitrary subset of channels may leave informative
aspects of brain-activity undetected and this led to
fluctuating results; but including large numbers of
channels invariably leads to overfitting, and conse-
quent falls in classification accuracy. In light of
this, a reduction in dimensions that kept most of the
global signal intact (in this case a principle com-
ponents analysis) proved very effective in prevent-
ing overfitting, giving reliably superior performance
with lower numbers of channels.
While MEG signals proved more informative,
there was not always a dramatic difference in perfor-
mance (peak performance in participant 1 was sim-
ilar for MEG or EEG; for participant 2 there was
a ca. 15% point difference). However this study
used a time interval and frequency band in the sig-
nal that had been optimised for EEG, so it may be
that considering a wider range of frequencies, higher
in the spectrum, could allow MEG to achieve bet-
ter results. Also, though steps were taken to avoid
it, slight movements by the participants relative to
the MEG apparatus will have compromised the reli-
ability of its signals (EEG does not suffer from the
same problem as electrodes are placed directly on
the scalp). This could be addressed in future studies
with continuous head tracking and correction.
Finally, several variations could be tried to im-
prove the overall classification performance of the
system. The spatial decomposition used (CSP) is
particularly prone to overfitting (Parra et al, 2005),
and could be replaced with less aggressive tech-
niques like Linear Discriminant Analysis. Princi-
ple component analysis is a rather brittle technique
41
Figure 2: Classification accuracy taking subsets of raw signals from sensors of different types, 9-point smoothed
Figure 3: Classification accuracy taking subsets of PCA components derived from raw signals from sensors of different
types, 9-point smoothed
42
which is heavily biased towards the few strongest
sources in a system, and so independent component
analysis (ICA) may be a more effective choice for
dimensionality reduction (Makeig et al, 1996). And
data from the various sensor types could be com-
bined in other ways, using an ensemble of classi-
fiers, each based on different subsets of signals, or
by taking more than one class-sensitive component
per category.
Acknowledgements
We are very grateful to Elena Betta, Gianpaolo De-
marchi and Gianpiero Monittola at the LNiF labs
for assistance in MEG data collection. The work
described here was funded by CIMeC, the Au-
tonomous Province of Trento, and the Fondazione
Cassa Risparmio Trento e Rovereto.
References
Boser, B. E.; I. M. Guyon; and V. N. Vapnik (1992):
A training algorithm for optimal margin classi-
fiers. In: 5th Annual ACM Workshop on COLT,
ed. D. Haussler. ACM Press, Pittsburgh, pp. 144?
152.
Chang, Chih-Chung and Chih-Jen Lin (2001): LIB-
SVM: a library for support vector machines.
Chao, Linda L.; Jill Weisberg; and Alex Martin
(2002): Experience-dependent modulation of cat-
egory related cortical activity. Cerebral Cortex,
12:545?551.
Dalponte, Michele; Francesca Bovolo; and Lorenzo
Bruzzone (2007): Automatic selection of fre-
quency and time intervals for classification of
EEG signals. Electronics Letters, 43:1406?1408.
Hill, N.J.; T.N. Lal; M. Schroder; T. Hinterberger;
G. Widman; C.E. Elger; B. Scholkopf; and N. Bir-
baumer (2006): Classifying event-related desyn-
chronization in EEG, ECoG and MEG signals.
Lecture Notes in Computer Science, 4174:404.
Just, M.A.; V.L. Cherkassky; S. Aryal; and T.M.
Mitchell (2010): A neurosemantic theory of con-
crete noun representation based on the underlying
brain codes. PLoS ONE, 5.
Koles, Zoltan J.; Michael S. Lazar; and Steven Z.
Zhou (1990): Spatial patterns underlying popula-
tion differences in the background EEG. Brain
Topography, 2(4):275?284.
Makeig, Scott; Anthony J. Bell; Tzyy-ping Jung;
and Terrence J. Sejnowski (1996): Independent
Component Analysis of Electroencephalographic
Data. In: Advances in Neural Information Pro-
cessing Systems. MIT Press, vol. 8, pp. 145?151.
Mitchell, Tom M.; Svetlana V. Shinkareva; Andrew
Carlson; Kai-Min Chang; Vicente L. Malave;
Robert A. Mason; and Marcel Adam Just (2008):
Predicting Human Brain Activity Associated with
the Meanings of Nouns. Science, 320:1191?1195.
Model, Dmitri and Michael Zibulevsky (2006):
Learning Subject-Specific Spatial and Temporal
Filters for Single-Trial EEG Classification. Neu-
roImage, 32(4):1631?1641.
Murphy, Brian; Marco Baroni; and Massimo Poesio
(2009): EEG responds to conceptual stimuli and
corpus semantics. In: Proceedings of the Confer-
ence on Empirical Methods in Natural Language
Processing. The Association for Computational
Linguistics, pp. 619?627.
Murphy, Brian; Michele Dalponte; Massimo Poe-
sio; and Lorenzo Bruzzone (2008): Distinguish-
ing Concept Categories from Single-Trial Elec-
trophysiological Activity. In: Proceedings of the
Annual Meeting of the Cognitive Science Society.
Murphy, Brian; Massimo Poesio; Francesca Bovolo;
Michele Dalponte; Lorenzo Bruzzone; and Heba
Lakany (2010): EEG decoding of semantic cate-
gory reveals distributed representations for single
concepts. Brain and Language, under review.
Parra, Lucas C.; Clay D. Spence; Adam D. Ger-
son; and Paul Sajda (2005): Recipes for the linear
analysis of EEG. NeuroImage, 28:326?341.
Philiastides, M.G.; R. Ratcliff; and P. Sajda (2006):
Neural representation of task difficulty and de-
cision making during perceptual categorization:
a timing diagram. Journal of Neuroscience,
26(35):8965.
Pulvermu?ller, Friedemann (2005): Brain mecha-
nisms linking language and action. Nature Re-
views Neuroscience, 6:576?582.
Ramoser, H.; J. M. Gerking; and Gert Pfurtscheller
(2000): Optimal spatial filtering of single
43
trial EEG during imagined hand movement.
IEEE Transactions on Rehabilitation Engineer-
ing, 8(4):441?446.
Tanji, Kazuyo; Kyoko Suzuki; Arnaud Delorme;
and Nobukazu Shamoto, Hiroshiand Nakasato
(2005): High-Frequency gamma-Band Activity
in the Basal Temporal Cortex during Picture-
Naming and Lexical-Decision Tasks. Journal of
Neuroscience, 25(13):3287?3293.
Vapnik, V. N. (1998): Statistical Learning Theory.
Wiley.
Waldert, S.; H. Preissl; E. Demandt; C. Braun;
N. Birbaumer; A. Aertsen; and C. Mehring
(2008): Hand movement direction decoded from
MEG and EEG. Journal of Neuroscience,
28(4):1000.
44
Proceedings of the 5th ACL-HLT Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities, pages 54?62,
Portland, OR, USA, 24 June 2011. c?2011 Association for Computational Linguistics
Structure-Preserving Pipelines for Digital Libraries
Massimo Poesio
University of Essex, UK and
Universit? di Trento, Italy
Eduard Barbu
Egon W. Stemle
Universit? di Trento, Italy
{massimo.poesio,eduard.barbu,egon.stemle}
@unitn.it
Christian Girardi
FBK-irst, Trento, Italy
cgirardi@fbk.eu
Abstract
Most existing HLT pipelines assume the input
is pure text or, at most, HTML and either ig-
nore (logical) document structure or remove
it. We argue that identifying the structure of
documents is essential in digital library and
other types of applications, and show that it
is relatively straightforward to extend existing
pipelines to achieve ones in which the struc-
ture of a document is preserved.
1 Introduction
Many off-the-shelf Human Language Technology
(HLT) pipelines are now freely available (examples
include LingPipe,1 OpenNLP,2 GATE3 (Cunning-
ham et al, 2002), TextPro4 (Pianta et al, 2008)),
and although they support a variety of document for-
mats as input, actual processing (mostly) takes no
advantage of structural information, i.e. structural
information is not used, or stripped off during pre-
processing. Such processing can be considered safe,
e.g. in case of news wire snippets, when processing
does not need to be aware of sentence or paragraph
boundaries, or of text being part of a table or a fig-
ure caption. However, when processing large doc-
uments, section or chapter boundaries may be con-
sidered an important segmentation to use, and when
working with the type of data typically found in dig-
ital libraries or historical archives, such as whole
1
http://alias-i.com/lingpipe/
2
http://incubator.apache.org/opennlp/
3
http://http://gate.ac.uk/
4
http://textpro.fbk.eu/
books, exhibition catalogs, scientific articles, con-
tracts we should keep the structure. At least three
types of problems can be observed when trying to
use a standard HLT pipeline for documents whose
structure cannot be easily ignored:
? techniques for extracting content from plain
text do not work on, say, bibliographic refer-
ences, or lists;
? simply removing the parts of a document that
do not contain plain text may not be the right
thing to do for all applications, as sometimes
the information contained in them may also be
useful (e.g., keywords are often useful for clas-
sification, bibliographic references are useful in
a variety of applications) or even the most im-
portant parts of a text (e.g., in topic classifica-
tion information provided by titles and other
types of document structure is often the most
important part of a document);
? even for parts of a document that still can be
considered as containing basically text?e.g.,
titles?knowing that we are dealing with what
we will call here non-paragraph text can be
useful to achieve good - or improve - perfor-
mance as e.g., the syntactic conventions used
in those type of document elements may be dif-
ferent - e.g., the syntax of NPs in titles can be
pretty different from that in other sections of
text.
In this paper we summarize several years of work
on developing structure-preserving pipelines for dif-
ferent applications. We discuss the incorporation of
54
document structure parsers both in pipelines which
the information is passed in BOI format (Ramshaw
and Marcus, 1995), such as the TEXTPRO pipeline
(Pianta et al, 2008), and in pipelines based on a
standoff XML (Ide, 1998). We also present sev-
eral distinct applications that require preserving doc-
ument structure.
The structure of the paper is as follows. We first
discuss the notion of document structure and previ-
ous work in extracting it. We then introduce our ar-
chitecture for a structure-preserving pipeline. Next,
we discuss two pipelines based on this general archi-
tecture. A discussion follows.
2 The Logical Structure of a Document
Documents have at least two types of structure5.
The term geometrical, or layout, structure, refers
to the structuring of a document according to its vi-
sual appearance, its graphical representation (pages,
columns, etc). The logical structure (Luong et al,
2011) refers instead to the content?s organization to
fulfill an intended overall communicative purpose
(title, author list, chapter, section, bibliography, etc).
Both of these structures can be represented as trees;
however, these two tree structures may not be mu-
tually compatible (i.e. representable within a single
tree structure with non-overlapping structural ele-
ments): e.g. a single page may contain the end of
one section and the beginning of the next, or a para-
graph may just span part of a page or column. In this
paper we will be exclusively concerned with logical
structure.
2.1 Proposals concerning logical structure
Early on the separation of presentation and content,
i.e. of layout and logical structure, was promoted by
the early adopters of computers within the typeset-
ting community; well-known, still widely used, sys-
tems include the LATEXmeta-package for electronic
typesetting. The importance of separating document
logical structure from document content for elec-
tronic document processing and for the document
creators lead to the ISO 8613-1:1989(E) specifica-
tion where logical structure is defined as the result
of dividing and subdividing the content of a docu-
5other structure types include e.g. (hyper)links, cross-
references, citations, temporal and spatial relationships
ment into increasingly smaller parts, on the basis of
the human-perceptible meaning of the content, for
example, into chapters, sections, subsections, and
paragraphs. The influential ISO 8879:1986 Stan-
dard Generalized Markup Language (SGML) spec-
ification fostered document format definitions like
the Open Document Architecture (ODA) and inter-
change format, CCITT T.411-T.424 / ISO 8613.
Even though the latter format never gained
wide-spread support, its technological ideas influ-
enced many of today?s formats, like HTML and
CSS as well as, the Extensible Markup Language
(XML), today?s successor of SGML. Today, the ISO
26300:2006 Open Document Format for Office Ap-
plications (ODF), and the ISO 29500:2008 Office
Open XML (OOXML) format are the important
XML-based document file formats.
For the work on digital libraries the Text Encod-
ing Initiative (TEI)6,most notably, developed guide-
lines specifying encoding methods for machine-
readable texts. They have been widely used, e.g. by
libraries, museums, and publishers.
The most common logical elements in such
proposals?chapters, sections, paragraphs, foot-
notes, etc.?can all be found in HTML, LATEX, or
any other modern text processor. It should be
pointed out however that many modern types of doc-
uments found on the Web do not fit this pattern:
e.g. blog posts with comments, and the structure of
reply threads and inner-linkings to other comments
cannot be captured; or much of wikipedia?s non-
paragraph text. (For an in depth comparison and
discussion of logical formats, and formal characteri-
zations thereof we suggest (Power et al, 2003; Sum-
mers, 1998).)
2.2 Extracting logical structure
Two families of methods have been developed to ex-
tract document structure. Older systems tend to fol-
low the template-matching paradigm. In this ap-
proach the assignment of the categories to parts of
the string is done by matching a sequence of hand
crafted templates against the input string S. An
instance of this kind of systems is DeLos (Deriva-
tion of Logical Structure) (Niyogi and Srihari, 1995)
which uses control rules, strategy rules and knowl-
6
http://www.tei-c.org
55
edge rules to derive the logical document structure
from a scanned image of the document. A more elab-
orate procedure for the same task is employed by
Ishitani (Ishitani, 1999). He uses rules to classify the
text lines derived from scanned document image and
then employs a set of heuristics to assign the classi-
fied lines to logical document components. The tem-
plate based approach is also used by the ParaTools,
a set of Perl modules for parsing reference strings
(Jewell, 2000). The drawback of the template based
approaches is that they are usually not portable to
new domains and are not flexible enough to accom-
modate errors. Domain adaptation requires the de-
vising of new rules many of them from scratch. Fur-
ther the scanned documents or the text content ex-
tracted from PDF have errors which are not easily
dealt with by template based systems.
Newer systems use supervised machine learning
techniques which are much more flexible but re-
quire training data. Extracting document structure
is an instance of (hierarchical) sequence labeling,
a well known problem which naturally arises in di-
verse fields like speech recognition, digital signal
processing or bioinformatics. Two kinds of machine
learning techniques are most commonly used for this
problem: Hidden Markov Models (HMM) and Con-
ditional Random Fields (CRF). A system for pars-
ing reference strings based on HMMs was developed
in (Hetzner, 2008) for the California Digital Library.
The system implements a first order HMM where the
set of states of the model are represented by the cat-
egories in C; the alphabet is hand built and tailored
for the task and the probabilities in the probability
matrix are derived empirically. The system obtains
an average F1 measure of 93 for the Cora dataset.
A better performance for sequence labeling is ob-
tained if CRF replaces the traditional HMM. The
reason for this is that CRF systems better tolerate
errors and they have good performance even when
richer features are not available. A system which
uses CRF and a series of post-processing rules for
both document logical structure identification and
reference string parsing is ParsCit (Councill et al,
2008). ParsCit comprises three sub-modules: Sect-
Label and ParseHead for document logical structure
identification and ParsCit for reference string pars-
ing. The system is built on top of the well known
CRF++ package.
The linguistic surface level, i.e. the linear order
of words, sentences, and paragraphs, and the hi-
erarchical, tree-like, logical structure also lends it-
self to parsing-like methods for the structure analy-
sis. However, the complexity of fostering, maintain-
ing, and augmenting document structure grammars
is challenging, and the notorious uncertainty of the
input demands for the whole set of stochastic tech-
niques the field has to offer ? this comes at a high
computing price; cf. e.g.,(Lee et al, 2003; Mao et
al., 2003). It is therefore not surprising that high-
throughput internet sites like CiteSeerX7 use a flat
text classifier (Day et al, 2007).8
3 Digital Libraries and Document
Structure Preservation
Our first example of application in which document
structure preservation is essential are digital libraries
(Witten et al, 2003). In a digital library setting, HLT
techniques can be used for a variety of purposes,
ranging from indexing the documents in the library
for search to classifying them to automatically ex-
tracting metadata. It is therefore becoming more and
more common for HLT techniques to be incorporated
in document management platforms and used to sup-
port a librarian when he / she enters a new document
in the library. Clearly, it would be beneficial if such
a pipeline could identify the logical structure of the
documents being entered, and preserve it: this infor-
mation could be used by the document management
platform to, for instance, suggest the librarian the
most important keywords, find the text to be indexed
or even summarized, and produce citations lists, pos-
sibly to be compared with the digital library?s list of
citations to decide whether to add them.
We are in the process of developing a Portal
for Research in the Humanities (Portale Ricerca
Umanistica-PRU). This digital library will eventu-
ally include research articles about the Trentino re-
gion from Archeology, History, and History of Art.
So far, the pipeline to be discussed next has been
used to include in the library texts from the Italian
archeology journal Preistoria Alpina. One of our
goals was to develop a pipeline that could be used
7
http://citeseerx.ist.psu.edu/
8Still, especially multimedia documents with their possible
temporal and spatial relationships might need more sophisti-
cated methods.
56
whenever a librarian uploads an article in this digital
library, to identify title, authors, abstract, keywords,
content, and bibliographic references from the arti-
cle. The implemented portal already incorporates in-
formation extraction techniques that are used to iden-
tify in the ?content? part of the output of the pipeline
temporal expressions, locations, and entities such
as archeological sites, cultures, and artifacts. This
information is used to allow spatial, temporal, and
entity-based access to articles.
We are in the process of enriching the portal so
that title and author information are also used to au-
tomatically produce a bibliographical card for the ar-
ticle that will be entered in the PRU Library Catalog,
and bibliographical references are processed in or-
der to link the article to related articles and to the
catalog as well. The next step will be to modify the
pipeline (in particular, to modify the Named Entity
Recognition component) to include in the library ar-
ticles from other areas of research in the Humanities,
starting with History. There are also plans to make
it possible for authors themselves to insert their re-
search articles and books in the Portal, as done e.g.,
in the Semantics Archive.9.
We believe the functionalities offered by this por-
tal are or will become pretty standard in digital li-
braries, and therefore that the proposals discussed in
this paper could find an application beyond the use
in our Portal. We will also see below that a docu-
ment structure-sensitive pipeline can find other ap-
plications.
4 Turning an Existing Pipeline into One
that Extracts and Preserves Document
Structure
Most freely available HLT pipelines simply elimi-
nate markup during the initial phases of processing
in order to eliminate parts of the document struc-
ture that cannot be easily processed by their mod-
ules (e.g., bibliographic references), but this is not
appropriate for the Portal described in the previous
section, where different parts of the output of the
pipeline need to be processed in different ways. On
the other end, it was not really feasible to develop
a completely new pipeline from scratch. The ap-
proach we pursued in this work was to take an exist-
9
http://semanticsarchive.net/
ing pipeline and turn it into one which extracts and
outputs document structure. In this Section we dis-
cuss the approach we followed. In the next Section
we discuss the first pipeline we developed according
to this approach; then we discuss how the approach
was adopted for other purposes, as well.
Incorporating a document structure extractor in a
pipeline requires the solution of two basic problems:
where to insert the module, and how to pass on doc-
ument structure information. Concerning the first
issue, we decided to insert the document structure
parser after tokenization but before sentence process-
ing. In regards to the second issue, there are at
present three main formats for exchanging informa-
tion between elements of an HLT pipeline:
? inline, where each module inserts information
in a pre-defined fashion into the file received as
input;
? tabular format as done in CONLL, where to-
kens occupy the first column and each new
layer of information is annotated in a separate
new column, using the so-called IOB format
to represent bracketing (Ramshaw and Marcus,
1995);
? standoff format, where new layers of informa-
tion are stored in separate files.
The two main formats used by modern HLT pipelines
are tabular format, and inline or standoff XML for-
mat. Even though we will illustrate the problem of
preserving document structure in a pipeline of the
former type the PRU pipeline itself supports tabular
format and inline XML (TEI compliant).
The solution we adopted, illustrated in Figure 1,
involves using sentence headers to preserve docu-
ment structure information. In most pipelines using
a tabular interchange information, the output of a
module consists of a number of sentences each of
which consists of
? a header: a series of lines with a hash character
# at the beginning;
? a set of tab-delimited lines representing tokens
and token annotations;
? an empty EOF line.
57
 
# FILE: 11
# PART: id1
# SECTION: title
# FIELDS: token tokenstart sentence pos lemma entity nerType
Spondylus 0 - SPN Spondylus O B-SITE
gaederopus 10 - YF gaederopus O O
, 20 - XPW , O O
gioiello 22 - SS gioiello O O
dell ' 31 - E dell ' O O
Europa 36 - SPN europa B-GPE B-SITE
preistorica 43 - AS preistorico O O
. 55 <eos > XPS full_stop O O
# FILE: 11
# PART: id2
# SECTION: author
# FIELDS: token tokenstart sentence pos lemma entity nerType
MARIA 0 - SPN maria B-PER O
A 6 - E a I-PER O
BORRELLO 8 - SPN BORRELLO I-PER O
& 17 - XPO & O O
. 19 <eos > XPS full_stop O O
(TEI compliant inline XML snippet :)
<text >
<body >
<div type=" section" xml:lang="it">
[...]
<p id="p2" type=" author">
<s id="p2s1"><name key="PER1" type=" person">MARIA A BORRELLO </name >&.</s>
</p>
</div >
</body >
</text >
 
Figure 1: Using sentence headers to preserve document structure information. For illustration, the TEI compliant
inline XML snippet of the second sentence has been added.
58
The header in such pipelines normally specifies only
the file id (constant through the file), the number of
the sentence within the file, and the columns (see
Figure 1). This format however can also be used
to pass on document structure information provided
that the pipeline modules ignore all lines beginning
with a hash, as these lines can then be used to pro-
vide additional meta information. We introduce an
additional tag, SECTION, with the following mean-
ing: a line beginning with # SECTION: specifies the
position in the document structure of the following
sentence. Thus for instance, in Figure 1, the line
# SECTION: title
specifies that the following sentence is a title.
5 An Pipeline for Research Articles in
Archeology
The pipeline currently in use in the PRU Portal
we are developing is based on the strategy just dis-
cussed. In this Section We discuss the pipeline in
more detail.
5.1 Modules
The pipeline for processing archaeological articles
integrates three main modules: a module for recov-
ering the logical structure of the documents, a mod-
ule for Italian and English POS tagging and a gen-
eral Name Entity Recognizer and finally, a Gazetteer
Based Name Entity Recognizer. The architecture of
the system is presented in figure 2. Each module
except the first one takes as input the output of the
previous module in the sequence.
1. Text Extraction. This module extracts the text
from PDF documents. Text extraction from
PDF is a notoriously challenging task. We ex-
perimented with many software packages and
obtained the best results with pdftotext. This is
a component of XPDF, an open source viewer
for PDF documents. pdftotext allows the extrac-
tion of the text content of PDF documents in a
variety of encodings. The main drawback of the
text extractor is that it does not always preserve
the original text order.
2. Language Identification. The archeology
repository contains articles written in one of
the two languages: Italian or English. This
module uses the TextCat language guesser10 for
guessing the language of sentences. The lan-
guage identification task is complicated by the
fact that some articles contain text in both lan-
guages: for example, an article written in En-
glish may have an Italian abstract and/or an Ital-
ian conclusion.
3. Logical Structure Identification. This mod-
ule extracts the logical structure of a document.
For example, it identifies important parts like
the title, the authors, the main headers, tables
or figures. For this task we train the SectLa-
bel component of ParsCit on the articles in the
archeology repository. Details on the training
process, the tag set and the performance of the
module are provided in section 5.2.
4. Linguistic Processing. A set of modules in the
pipeline then perform linguistic processing on
specific parts of the document (the Bibliogra-
phy Section is excluded for example). First En-
glish or Italian POS is carried out as appropri-
ate, followed by English or Italian NER. NER
adaptation techniques have been developed to
identify non-standard types entities that are im-
portant in the domain, such as Archeological
Sites and Archeological Cultures. (This work
is discussed elsewhere.)
5. Reference Parsing. This module relies on
the output of ParsCit software to update the
Archeology Database Bibliography table with
the parsed references for each article. First,
each parsed reference is corrected in an auto-
matic post processing step. Then, the module
checks, using a simple heuristic, if the entry al-
ready exists in the table and updates the table,
if appropriate, with the new record.
Finally, the documents processed by the pipeline
are indexed using the Lucene search engine.
5.2 Training the Logical Document Structure
Identifier
As mentioned in Section 5, we use ParsCit to find the
logical structure of the documents in the archeology
10
http://odur.let.rug.nl/~vannoord/TextCat/
59
Figure 2: The pipeline of the system for PDF article processing in the Archeology Domain
domain. ParsCit comes with general CRF trained
models; unfortunately, they do not perform well on
archeology documents. There are some particulari-
ties of archeology repository articles that require the
retraining of the models. First, as said before, the
text extracted from PDF is not perfect. Second, the
archeology articles contain many figures with bilin-
gual captions. Third, the articles have portions of
the texts in both languages: Italian and English. To
improve the parsing performance two models are
trained: the first model should capture the logical
documents structure for those documents that have
Italian as main language but might contain portions
in English (like the abstract or summary). The sec-
ond model is trained with documents that have En-
glish as main language but might contain fragments
in Italian (like abstract or summary).
The document structure annotation was per-
formed by a student in the archeology department,
and was checked by one of the authors. In total 55
documents have been annotated (35 with Italian as
main language, 20 with English as main Language).
The tagset used for the annotation was specifically
devised for archeology articles. However, as it can
be seen below most of the devised tags can also be
found in general scientific articles. In Table 1 we
present the tag set used for annotation. The column
"Tag Count" gives the number of each tag in the an-
notated documents.
In general the meaning of the tags is self-
explanatory with the possible exception of the
tag VolumeInfo, which reports information for vol-
ume the article is part of. An annotation exam-
ple using this tag is: "<VolumeInfo> Preistoria
Alpina v. 38 (2002) Trento 2002 ISSN 0393-0157
</VolumeInfo>". The volume information can be
further processed by extracting the volume number,
the year of the issue and the International Standard
Serial Number (ISSN). To asses the performance of
the trained models we performed a five fold cross-
validation. The results are reported in the table 2
and are obtained for each tag using the F1 measure
(1):
F1 =
2?P?R
P+R
(1)
The results obtained for the Archeology articles
are in line with those obtained by the authors of
ParsCit and reported in (Luong et al, 2011). The
tag categories for which the performance of the sys-
tem is bad are the multilingual tags (e.g. ItalianAb-
stract or Italian Summary in articles where the main
language is English). We will address this issue in
the future by adapting the language identifier to label
multilingual documents. We also noticed that many
mis-tagged titles, notes or section headers are split
on multiple lines after the text extraction stage. The
system performance might be further improved if a
pre-processing step immediately after the text extrac-
tion is introduced.
60
Tag Tag Count
ItalianFigureCaption 456
ItalianBodyText 347
EnglishFigureCaption 313
SectionHeader 248
EnglishTableCaption 58
ItalianTableCaption 58
Author 71
AuthorEmail 71
AuthorAddress 65
SubsectionHeader 50
VolumeInfo 57
Bibliography 55
English Summary 31
ItalianKeywords 35
EnglishKeywords 35
Title 55
ItalianSummary 29
ItalianAbstract 10
Table 25
EnglishAbstract 13
Note 18
Table 1: The tag set used for Archeology Article Annota-
tion.
6 Additional Applications for
Structure-Sensitive Pipelines
The pipeline discussed above can be used for a va-
riety of other types of documents?archeology doc-
uments from other collections, or documents from
other domains?by simply replacing the document
structure extractor. We also found however that the
pipeline is useful for a variety of other text-analysis
tasks. We briefly discuss these in turn.
6.1 Blogs and Microblogging platforms
Content creation platforms like blogs, microblogs,
community QA sites, forums, etc., contain user gen-
erated data. This data may be emotional, opin-
ionated, personal, and sentimental, and as such,
makes it interesting for sentiment analysis, opinion
retrieval, and mood detection. In their survey on
opinion mining and sentiment analysis Pang and Lee
(2008) report that logical structure can be used to uti-
lize the relationships between different units of con-
tent, in order to achieve a more accurate labeling;
Tag F1
ItalianFigureCaption 70
ItalianBodyText 90
EnglishFigureCaption 71
SectionHeader 90
EnglishTableCaption 70
ItalianTableCaption 75
Author 72
AuthorEmail 75
AuthorAddress 73
SubsectionHeader 65
VolumeInfo 85
Bibliography 98
English Summary 40
ItalianKeywords 55
EnglishKeywords 56
Title 73
ItalianSummary 40
ItalianAbstract 50
Table 67
EnglishAbstract 50
Note 70
Table 2: The Precision and Recall for the trained models.
e.g. the relationships between discourse participants
in discussions on controversial topics when respond-
ing are more likely to be antagonistic than to be re-
inforcing, or the way of quoting?a user can refer to
another post by quoting part of it or by addressing
the other user by name or user ID?in posts on politi-
cal debates hints at the perceived opposite end of the
political spectrum of the quoted user.
We are in the process of creating an annotated cor-
pus of blogs; the pipeline discussed in this paper
was easily adapted to pre-process this type of data
as well.
6.2 HTML pages
In the IR literature it has often been observed that
certain parts of document structure contain infor-
mation that is particularly useful for document re-
trieval. For instance, Kruschwitz (2003) automati-
cally builds domain models ? simple trees of related
terms ? from documents marked up in HTML to
assist users during search tasks by performing auto-
matic query refinements, and improves users? experi-
61
ence for browsing the document collection. He uses
term counts in different markup contexts like non-
paragraph text and running text, and markups like
bold, italic, underline to identify concepts and the
corresponding shallow trees. However, this domain-
independent method is suited for all types of data
with logical structure annotation and similar data
sources can be found in many places, e.g. corporate
intranets, electronic archives, etc.
6.3 Processing Wikipedia pages
Wikipedia, as a publicly available web knowledge
base, has been leveraged for semantic information
in much work, including from our lab. Wikipedia
articles consist mostly of free text, but also con-
tain different types of structured information, e.g. in-
foboxes, categorization and geo information, links
to other articles, to other wiki projects, and to exter-
nal Web pages. Preserving this information is there-
fore useful for a variety of projects.
7 Discussion and Conclusions
The main point of this paper is to argue that the field
should switch to structure-sensitive pipelines. These
are particularly crucial in digital library applications,
but novel type of documents require them as well.
We showed that such extension can be achieved
rather painlessly even in tabular-based pipelines pro-
vided they allow for meta-lines.
References
Isaac G. Councill, C. Lee Giles, and Min-Yen Kan. 2008.
Parscit: An open-source crf reference string parsing
package. In Proceedings of the Language Resources
and Evaluation Conference (LREC 08), May.
Hamish Cunningham, Diana Maynard, Kalina Bontcheva,
and Valentin Tablan. 2002. GATE: A framework and
graphical development environment for robust NLP
tools and applications. In Proceedings of the 40th
Anniversary Meeting of the Association for Computa-
tional Linguistics.
Min-Yuh Day, Richard Tzong-Han Tsai, Cheng-Lung
Sung, Chiu-Chen Hsieh, Cheng-Wei Lee, Shih-Hung
Wu, Kun-Pin Wu, Chorng-Shyong Ong, and Wen-Lian
Hsu. 2007. Reference metadata extraction using a hi-
erarchical knowledge representation framework. Deci-
sion Support Systems, 43(1):152?167, February.
Erik Hetzner. 2008. A simple method for citation meta-
data extraction using hidden markov models. In Pro-
ceedings of the 8th ACM/IEEE-CS joint conference
on Digital libraries, JCDL ?08, pages 280?284, New
York, NY, USA. ACM.
Nancy Ide. 1998. Corpus encoding standard: SGML
guidelines for encoding linguistic corpora. In Proceed-
ings of LREC, pages 463?70, Granada.
Yasuto Ishitani. 1999. Logical structure analysis of doc-
ument images based on emergent computation. In
Proceedings of International Conference on Document
Analysis and Recognition.
Michael Jewell. 2000. Paracite: An overview.
Udo Kruschwitz. 2003. An Adaptable Search System for
Collections of Partially Structured Documents. IEEE
Intelligent Systems, 18(4):44?52, July.
Kyong-Ho Lee, Yoon-Chul Choy, and Sung-Bae Cho.
2003. Logical structure analysis and generation for
structured documents: a syntactic approach. IEEE
transactions on knowledge and data engineering,
15(5):1277?1294, September.
Minh-Thang Luong, Thuy Dung Nguyen, and Min-Yen
Kan. 2011. Logical structure recovery in scholarly
articles with rich document feature. Journal of Digital
Library Systems. Forthcoming.
Song Mao, Azriel Rosenfeld, and Tapas Kanungo. 2003.
Document Structure Analysis Algorithms: A Litera-
ture Survey.
Debashish Niyogi and Sargur N. Srihari. 1995.
Knowledge-based derivation of document logical
structure. In Proceedings of International Conference
on Document Analysis and Recognition, pages 472?
475.
Bo Pang and Lillian Lee. 2008. Opinion Mining and
Sentiment Analysis. Foundations and Trends in Infor-
mation Retrieval, 2(1-2):1?135, January.
Emanuele Pianta, Christian Girardi, and Roberto Zanoli.
2008. The TextPro tool suite. In LREC, 6th edition of
the Language Resources and Evaluation Conference,
Marrakech (Marocco).
Richard Power, Donia Scott, and Nadjet Bouayad-Agha.
2003. Document Structure. Computational Linguis-
tics, 29(2):211?260, June.
Lance A. Ramshaw and Mitchell P. Marcus. 1995. Text
chunking using tranformation-based learning. In Pro-
ceedings of Third ACL Workshop on Very Large Cor-
pora, pages 82?94.
Kristen M. Summers. 1998. Automatic discovery of log-
ical document structure. Ph.D. thesis, Cornell Univer-
sity.
Ian H. Witten, David Bainbridge, and David M. Nichols.
2003. How to build a digital library. Morgan Kauf-
mann.
62
Proceedings of the 15th Conference on Computational Natural Language Learning: Shared Task, pages 61?65,
Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational Linguistics
Multi-metric optimization for coreference: The UniTN / IITP / Essex
submission to the 2011 CONLL Shared Task
Olga Uryupina? Sriparna Saha? Asif Ekbal? Massimo Poesio??
?University of Trento
?Indian Institute of Technology Patna
? University of Essex
uryupina@gmail.com, sriparna@iitp.ac.in,
asif@iitp.ac.in, massimo.poesio@unitn.it
Abstract
Because there is no generally accepted met-
ric for measuring the performance of anaphora
resolution systems, a combination of met-
rics was proposed to evaluate submissions to
the 2011 CONLL Shared Task (Pradhan et
al., 2011). We investigate therefore Multi-
objective function Optimization (MOO) tech-
niques based on Genetic Algorithms to opti-
mize models according to multiple metrics si-
multaneously.
1 Introduction
Many evaluation metrics have been proposed for
anaphora resolution (Vilain et al, 1995; Bagga and
Baldwin, 1998; Doddington et al, 2000; Luo, 2005;
Recasens and Hovy, 2011). Each of these metrics
seems to capture some genuine intuition about the
the task, so that, unlike in other areas of HLT, none
has really taken over. This makes it difficult to com-
pare systems, as dramatically demonstrated by the
results of the Coreference Task at SEMEVAL 2010
(Recasens et al, 2010). It was therefore wise of the
CONLL organizers to use a basket of metrics to as-
sess performance instead of a single one.
This situation suggests using methods to opti-
mize systems according to more than one metric
at once. And as it happens, techniques for doing
just that have been developed in the area of Ge-
netic Algorithms?so-called multi-objective opti-
mization techniques (MOO) (Deb, 2001). The key
idea of our submission is to use MOO techniques
to optimize our anaphora resolution system accord-
ing to three metrics simultaneously: the MUC scorer
(a member of what one might call the ?link-based?
cluster of metrics) and the two CEAF metrics (rep-
resentative of the ?entity-based? cluster). In a pre-
vious study (Saha et al, 2011), we show that our
MOO-based approach yields more robust results than
single-objective optimization.
We test two types of optimization: feature se-
lection and architecture?whether to learn a single
model for all types of anaphors, or to learn sepa-
rate models for pronouns and for other nominals.
We also discuss how the default mention extraction
techniques of the system we used for this submis-
sion, BART (Versley et al, 2008), were modified to
handle the all-mention annotation in the OntoNotes
corpus.
In this paper, we first briefly provide some back-
ground on optimization for anaphora resolution, on
genetic algorithms, and on the method for multi-
objective optimization we used, Non-Dominated
Sorting Genetic Algorithm II (Deb et al, 2002). Af-
ter that we discuss our experiments, and present our
results.
2 Background
2.1 Optimization for Anaphora Resolution
There have only been few attempts at optimization
for anaphora resolution, and with a few exceptions,
this was done by hand.
The first systematic attempt at automatic opti-
mization of anaphora resolution we are aware of was
carried out by Hoste (2005), who used genetic algo-
rithms for automatic optimization of both feature se-
lection and of learning parameters, also considering
61
two different machine learners, TimBL and Ripper.
Her results suggest that such techniques yield im-
provements on the MUC-6/7 datasets. Recasens and
Hovy (2009) carried out an investigation of feature
selection for Spanish using the ANCORA corpus.
A form of multi-objective optimization was ap-
plied to coreference by Munson et al (2005). Mun-
son et al (2005) did not propose to train models so
as to simultaneously optimize according to multi-
ple metrics; instead, they used ensemble selection to
learn to choose among previously trained models the
best model for each example. Their general conclu-
sion was negative, stating that ?ensemble selection
seems too unreliable for use in NLP?, but they did
see some improvements for coreference.
2.2 Genetic Algorithms
Genetic algorithms (GAs) (Goldberg, 1989) are ran-
domized search and optimization techniques guided
by the principles of evolution and natural genetics.
In GAs the parameters of the search space are en-
coded in the form of strings called chromosomes. A
collection of such strings is called a population. An
objective or fitness function is associated with each
chromosome that represents the degree of goodness
of that chromosome. A few of the chromosomes are
selected on the basis of the principle of survival of
the fittest, and assigned a number of copies that go
into the mating pool. Biologically inspired opera-
tors like crossover and mutation are applied on these
chromosomes to yield a new generation of strings.
The processes of selection, crossover and mutation
continues for a fixed number of generations or till a
termination condition is satisfied.
2.3 Multi-objective Optimization
Multi-objective optimization (MOO) can be formally
stated as follows (Deb, 2001). Find the vectors
x? = [x?1, x?2, . . . , x?n]T of decision variables that si-
multaneously optimize the M objective values
{f1(x), f2(x), . . . , fM (x)}
while satisfying the constraints, if any.
An important concept in MOO is that of dom-
ination. In the context of a maximization prob-
lem, a solution xi is said to dominate xj if
?k ? 1, 2, . . . ,M, fk(xi) ? fk(xj) and ?k ?
1, 2, . . . ,M, such that fk(xi) > fk(xj).
Genetic algorithms are known to be more effec-
tive for solving MOO than classical methods such as
weighted metrics, goal programming (Deb, 2001),
because of their population-based nature. A particu-
larly popular genetic algorithm of this type is NSGA-
II (Deb et al, 2002), which we used for our runs.
3 Using MOO for Optimization in
Anaphora Resolution
We used multi-objective optimization techniques for
feature selection and for identifying the optimal ar-
chitecture for the CONLL data. In this section we
briefly discuss each aspect of the methodology.
3.1 The BART System
For our experiments, we use BART (Versley et al,
2008), a modular toolkit for anaphora resolution that
supports state-of-the-art statistical approaches to the
task and enables efficient feature engineering. BART
comes with a set of already implemented features,
along with the possibility to design new ones. It
also implements different models of anaphora reso-
lution, allowing the choice between single and split
classifiers that we explore in our runs, as well as
between mention-pair and entity-mention, and be-
tween best-first and ranking. It also has interfaces
to different machine learners (MaxEnt, SVM, de-
cision trees). It is thus ideally suited for experi-
menting with feature selection and other aspects of
optimization. However, considering all the param-
eters, it was unfeasible to run an optimization on
the amount of data available on CONLL; we fo-
cused therefore on feature selection and the choice
between single and split classifiers. We considered
42 features, including 7 classifying mention type, 8
for string matching of different subparts and differ-
ent levels of exactness, 2 for aliasing, 4 for agree-
ment, 12 for syntactic information including also
binding constraints, 3 encoding salience, 1 encod-
ing patterns extracted from the Web, 3 for proximity,
and 2 for 1st and 2nd person pronouns. Again be-
cause of time considerations, we used decision trees
as implemented in Weka as our classification model
instead of maximum-entropy or SVMs. Finally, we
used a simple mention-pair model without ranking
as in (Soon et al, 2001).
62
3.2 Mention detection
BART supports several solutions to the mention
detection (MD) task. The users can input pre-
computed mentions, thus, experimenting with gold
boundaries or system boundaries computed by ex-
ternal modules (e.g., CARAFE). BART also has
a built-in mention extraction module, computing
boundaries heuristically from the output of a parser.
For the CoNLL shared task, we use the BART
internal MD module, as it corresponds better to
the mention detection guidelines of the OntoNotes
dataset. We have further adjusted this module to im-
prove the MD accuracy. The process of mention de-
tection involves two steps.
First, we create a list of candidate mentions by
merging basic NP chunks with named entities. NP
chunks are computed from the parse trees provided
in the CoNLL distribution, Named entities are ex-
tracted with the Stanford NER tool (Finkel et al,
2005). For each candidate mention, we store it mini-
mal and maximal span. The former is used for com-
puting feature values (e.g., for string matching); it
corresponds to either the basic NP chunk or the NE,
depending on the mention type. The latter is used
for alignment with CoNLL mentions; it is computed
by climbing up the parse tree.
This procedure, combined with the perfect (gold)
coreference resolution, gives us an F-score of
91.56% for the mention detection task on the
CoNLL development set1.
At the second step, we aim at discarding men-
tions that are unlikely to participate in corefer-
ence chains. We have identified several groups of
such mentions: erroneous (?[uh]?), (parts of) multi-
word expressions (?for [example]?), web addresses,
emails (?[http://conll.bbn.com]?), time/date expres-
sions (?two times [a year]?), non-referring pronouns
(?[there]?,?[nobody]?), pronouns that are unlikely
to participate in a chain (?[somebody]?, ?[that]?),
time/date expressions that are unlikely to participate
in a chain (?[this time]?), and expletive ?it?.
Our experiments on the development data show
that the first five groups can be reliably identified
and safely discarded from the processing: even with
1Note that, due to the fact that OntoNotes guidelines exclude
singleton mentions, it is impossible to evaluate the MD compo-
nent independently from coreference resolution.
the perfect resolution, we observe virtually no per-
formance loss (the F-score for our MD module with
the gold coreference resolution remains at 91.45%
once we discard mentions from groups 1-5).
The remaining groups are more problematic:
when we eliminate such mentions, we see perfor-
mance drops with the gold resolution. The exact im-
pact of discarding those mentions can only be as-
sessed once we have trained the classifier.
In practice, we have performed our optimization
experiments, selected the best classifier and then
have done additional runs to fine-tune the mention
detection module.
3.3 Using NSGA-II
Chromosome Representation of Feature and Ar-
chitecture Parameters We used chromosomes of
length 43, each binary gene encoding whether or not
to use a particular feature in constructing the classi-
fier, plus one gene set to 1 to use a split classifier, 0
to use a single classifier for all types of anaphors.
Fitness Computation and Mutations For fitness
computation, the following procedure is executed.
1. Suppose there are N number of features
present in a particular chromosome (i.e., there
are total N number of 1?s in that chromosome).
2. Construct the coreference resolution system
(i.e., BART) with only these N features.
3. This coreference system is evaluated on the de-
velopment data. The recall, precision and F-
measure values of three metrics are calculated.
For MOO, the objective functions corresponding to
a particular chromosome are F1 = F-measureMUC
(for the MUC metric), F2 = F-measure?3 (for CEAF
using the ?3 entity alignment function (Luo, 2005))
and F3 = F-measure?4 (for CEAF using the ?3
entity alignment function). The objective is to:
max[F1, F2, F3]: i.e., these three objective func-
tions are simultaneously optimized using the search
capability of NSGA-II.
We use crowded binary tournament selection as
in NSGA-II, followed by conventional crossover and
mutation for the MOO based optimization. The
most characteristic part of NSGA-II is its elitism op-
eration, where the non-dominated solutions (Deb,
63
2001) among the parent and child populations are
propagated to the next generation. The near-Pareto-
optimal strings of the last generation provide the dif-
ferent solutions to the feature selection problem.
Genetic Algorithms Parameters Using the
CONLL development set, we set the following pa-
rameter values for MOO (i.e., NSGA-II): population
size=20, number of generations=20, probability of
mutation=0.1 and probability of crossover=0.9.
3.4 Running the Optimization
Considering the size of the OntoNotes corpus, it
would be very time-consuming to run an optimiza-
tion experiment on the whole dataset. We have
therefore split the data into 3 sub-samples and per-
formed separate MOO experiments on each one.
The MOO approach provides a set of non-
dominated solutions on the final Pareto optimal
front. All the solutions are equally important from
the algorithmic point of view. We have collected sets
of chromosomes for each sub-sample and evaluated
them on the whole train/development set, picking
the solution with the highest FINAL2 score for our
CoNLL submission.
4 Results
4.1 Development set
Table 1 compares the performance level obtained
using all the features with that of loose re-
implementations of the systems proposed by Soon
et al (2001) and Ng and Cardie (2002), commonly
used as baselines. Our reimplementation of the Ng
& Cardie model uses only a subset of features.
The results in Table 1 show that our system with
a rich feature set does not outperform simpler base-
lines (and, in fact, yields poorer results). A similar
trend has been observed by Ng and Cardie (2002),
where the improvement was only possible after man-
ual feature selection.
The last line of Table 1 shows the performance
level of the best chromosome found through the
MOO technique. As it can be seen, it outperforms
all the baselines according to all the measures, lead-
ing to an improvement of 2-5 percentage points in
the FINAL score.
2The FINAL score is an average of FMUC , FB3 and
FCEAF E .
This suggests that automatic feature selection is
essential to improve performance ? i.e., that an effi-
cient coreference resolution system should combine
rich linguistic feature sets with automatic feature se-
lection mechanisms.
4.2 Test set
We have re-trained our best solution on the com-
bined train and development set, running it on the
test data. This system has showed the following per-
formance in the official evaluation (open track): the
FINAL score of 54.32, FMUC = 57.53%, FB3 =
65.18%, FCEAFE = 40.16%.
5 Conclusion
Our results on the development set suggest that a
linguistically-rich system for coreference resolution
might benefit a lot from feature selection. In partic-
ular, we have investigated Non-Dominated Sorting
Genetic Algorithm II (Deb et al, 2002) for multi-
objective optimization.
In subsequent work, we plan to expand the opti-
mization technique to consider also learning param-
eters optimization, classifier selection, and learning
model selection.
Acknowledgments
This work was in part supported by the Provincia
di Trento Grande Progetto LiveMemories, in part by
an Erasmus Mundus scholarship for Asif Ekbal and
Sriparna Saha.
64
Features FMUC FCEAFE FB3 FINAL
following Soon et al (2001) 54.12 41.08 66.67 53.42
-*-, with splitting 53.81 41.03 66.70 53.31
following Ng & Cardie (2002) 52.97 42.40 66.18 53.31
-*-, with splitting 53.28 40.46 66.03 52.72
All features 50.18 38.54 63.79 50.33
-*-, with splitting 50.19 39.47 65.38 51.16
Optimized feature set (splitting) 57.05 42.61 67.46 55.15
Table 1: Performance on the development set
References
A. Bagga and B. Baldwin. 1998. Algorithms for scoring
coreference chains. In Proc. of the LREC workshop on
Linguistic Coreference, pages 563?566, Granada.
Kalyanmoy Deb, Amrit Pratap, Sameer Agarwal, and
T. Meyarivan. 2002. A fast and elitist multiobjective
genetic algorithm: NSGA-II. IEEE Transactions on
Evolutionary Computation, 6(2):181?197.
Kalyanmoy Deb. 2001. Multi-objective Optimization
Using Evolutionary Algorithms. John Wiley and Sons,
Ltd, England.
G. Doddington, A. Mitchell, M. Przybocki, L. Ramshaw,
S. Strassell, and R. Weischedel. 2000. The auto-
matic content extraction (ACE) program?tasks, data,
and evaluation. In Proc. of LREC.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating non-local informa-
tion into information extraction systems by Gibbs sam-
pling. In Proceedings of the 43rd Annual Meeting of
the Association for Computational Linguistics, pages
363?370.
D. E. Goldberg. 1989. Genetic Algorithms in Search,
Optimization and Machine Learning. Addison-
Wesley, New York.
Veronique Hoste. 2005. Optimization Issues in Ma-
chine Learning of Coreference Resolution. Ph.D. the-
sis, Antwerp University.
X. Luo. 2005. On coreference resolution performance
metrics. In Proc. NAACL / EMNLP, Vancouver.
Art Munson, Claire Cardie, and Rich Caruana. 2005.
Optimizing to arbitrary NLP metrics using ensem-
ble selection. In Proceedings of Human Lan-
guage Technology Conference and Conference on
Empirical Methods in Natural Language Processing
(HLT/EMNLP), pages 539?546.
Vincent Ng and Claire Cardie. 2002. Improving machine
learning approaches to coreference resolution. In Pro-
ceedings of the 40th Annual Meeting on Association
for C omputational Linguistics, pages 104?111.
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,
Martha Palmer, Ralph Weischedel, and Nianwen Xue.
2011. Conll-2011 shared task: Modeling unrestricted
coreference in ontonotes. In Proceedings of the Fif-
teenth Conference on Computational Natural Lan-
guage Learning (CoNLL 2011), Portland, Oregon,
June.
M. Recasens and E. Hovy. 2009. A deeper look into fea-
tures for coreference resolution. In S. Lalitha Devi,
A. Branco, and R. Mitkov, editors, Anaphora Pro-
cessing and Applications (DAARC 2009, number 5847
in LNAI, pages 29?42, Berlin / Heidelberg. Springer-
Verlag.
M. Recasens and E. Hovy. 2011. Blanc: Implement-
ing the rand index for coreference evaluation. Natural
Language Engineering.
M. Recasens, L. Ma`rquez, E. Sapena, M. A. Mart??,
M. Taule?, V. Hoste, M. Poesio, and Y. Versley. 2010.
Semeval-2010 task 1: Coreference resolution in multi-
ple languages. In Proc. SEMEVAL 2010, Uppsala.
Sriparna Saha, Massimo Poesio, Asif Ekbal, and Olga
Uryupina. 2011. Single and multi-objective optimiza-
tion for feature selection in anaphora resolution. Sub-
mitted.
Wee Meng Soon, Hwee Tou Ng, and Daniel Chung Yong
Lim. 2001. A machine learning approach to corefer-
ence resolution of noun phrases. Computational Lin-
guistic, 27(4):521?544.
Yannick Versley, Simone Paolo Ponzetto, Massimo Poe-
sio, Vladimir Eidelman, Alan Jern, Jason Smith,
Xiaofeng Yang, and Alessandro Moschitti. 2008.
BART: a modular toolkit for coreference resolution. In
Proceedings of the 46th Annual Meeting of the Asso-
ciation for Computational Linguistics on Human Lan-
guage Technologies, pages 9?12.
M. Vilain, J. Burger, J. Aberdeen, D. Connolly, and
L. Hirschman. 1995. A model-theoretic coreference
scoring scheme. In Proc. of the Sixth Message Under-
standing Conference, pages 45?52.
65
Proceedings of the EACL 2012 Workshop on Computational Approaches to Deception Detection, pages 39?47,
Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics
On the Use of Homogenous Sets of Subjects in Deceptive Language
Analysis
Tommaso Fornaciari
Center for Mind/Brain Sciences
University of Trento
tommaso.fornaciari@unitn.it
Massimo Poesio
Language and Computation Group
University of Essex
Center for Mind/Brain Sciences
University of Trento
massimo.poesio@unitn.it
Abstract
Recent studies on deceptive language sug-
gest that machine learning algorithms can
be employed with good results for classi-
fication of texts as truthful or untruthful.
However, the models presented so far do
not attempt to take advantage of the dif-
ferences between subjects. In this paper,
models have been trained in order to clas-
sify statements issued in Court as false or
not-false, not only taking into considera-
tion the whole corpus, but also by identify-
ing more homogenous subsets of producers
of deceptive language. The results suggest
that the models are effective in recogniz-
ing false statements, and their performance
can be improved if subsets of homogeneous
data are provided.
1 Introduction
Detecting deceptive communication is a challeng-
ing task, but one that could have a number of use-
ful applications. A wide variety of approaches to
the discovery of deceptive statements have been
attempted, ranging from using physiological sen-
sors such as lie detectors to using neuroscience
methods (Davatzikos et al, 2005; Ganis et al,
2003). More recently, a number of techniques
have been developed for recognizing deception
on the basis of the communicative behavior of
subjects. Given the difficulty of the task, many
such methods rely on both verbal and non-verbal
behavior, to increase accuracy. So for instance
De Paulo et al (2003) considered more than 150
cues, verbal and non-verbal, directly observed
through experimental subjects. But finding clues
indicating deception through manual inspection is
not easy. De Paulo et al asserted that ?behaviors
that are indicative of deception can be indicative
of other states and processes as well?.
The same point is made in more recent liter-
ature: thus Frank et al (2008) write ?We find
that there is no clue or clue pattern that is spe-
cific to deception, although there are clues spe-
cific to emotion and cognition?, and they wish
for ?real-world databases, identifying base rates
for malfeasant behavior in security settings, opti-
mizing training, and identifying preexisting excel-
lence within security organizations?. Jensen et al
(2010) exploited cues coming from audio, video
and textual data.
One solution is to let statistical and machine
learning methods discover the clues. Work such
as Fornaciari and Poesio (2011a,b); Newman et al
(2003); Strapparava and Mihalcea (2009) sug-
gests that these techniques can perform reason-
ably well at the task of discovering deception
even just from linguistic data, provided that cor-
pora containing examples of deceptive and truth-
ful texts are available. The availability of such
corpora is not a trivial problem, and indeed, the
creation of a realistic such corpus is one of the
problems in which we invested substantial effort
in our own previous work, as discussed in Section
3.
In the work discussed in this paper, we tackle
an issue which to our knowledge has not been
addressed before, due to the limitations of the
datasets previously available: this is whether the
individual difference between experimental sub-
jects affect deception detection. In previous work,
lexical (Fornaciari and Poesio, 2011a) and surface
(Fornaciari and Poesio, 2011b) features were em-
ployed to classify deceptive statements issued in
Italian Courts. In this study, we report the results
39
of experiments in which our methods were trained
either over the whole corpus or over smaller sub-
sets consisting of the utterances produced by more
homogenous subsets of subjects. These subsets
were identified either automatically, by cluster-
ing subjects according to their language profile,
or by using meta-information about the subjects
included in the corpus, such as their gender.
The structure of the paper is as follows. In Sec-
tion 2 some background knowledge is introduced.
In Section 3 the data set is described. In Section 4
we discuss our machine learning and experimen-
tal methods. Finally, the results are presented in
Section 5 and discussed in Section 6.
2 Background
2.1 Deceptive language analysis
From a methodological point of view, to investi-
gate deceptive language gives rise to some tricky
issues: first of all, the strategy chosen to collect
data. The literature can be divided in two main
families of studies:
? Field studies;
? Laboratory studies.
The first ones are usually interesting in forensic
applications but in such studies verifying the sin-
cerity of the statements is often complicated (Vrij,
2005). Laboratory studies, instead, are character-
ized by the artificiality of participants? psycholog-
ical conditions: therefore their findings may not
be generalized to deception encountered in real
life.
Due to practical difficulties in collection and
annotation of suitable data, in literature finding
papers in which real life linguistic data are em-
ployed, where truthfulness is surely known, is
less common and Zhou et al (2008) complain
about the lack of ?data set for evaluating decep-
tion detection models?. Just recently some studies
tried to fill this gap, concerning both the English
(Bachenko et al, 2008; Fitzpatrick and Bachenko,
2009) and Italian language (Fornaciari and Poe-
sio, 2011a,b). Just the studies on Italian language
come from data which have constituted the first
nucleus of the corpus analysed here.
2.2 Stylometry
Our own work and that of other authors that re-
cently employed machine learning techniques to
detect deception in text employs techniques very
similar to that of stylometry. Stylometry is a dis-
cipline which studies texts on the basis of their
stylistic features, usually in order to attribute them
to an author - giving rise to the branch of author
attribution - or to get information about the author
himself - this is the field of author profiling.
Stylometric analyses, which relies mainly on
machine learning algorithms, turned out to be ef-
fective in several forensic tasks: not only the clas-
sical field of author profiling (Coulthard, 2004;
Koppel et al, 2006; Peersman et al, 2011; Solan
and Tiersma, 2004) and author attribution (Luy-
ckx and Daelemans, 2008; Mosteller and Wallace,
1964), but also emotion detection (Vaassen and
Daelemans, 2011) and plagiarism analysis (Stein
et al, 2007). Therefore, from a methodological
point of view, Deceptive Language Analysis is a
particular application of stylometry, exactly like
other branches of Forensic Linguistics.
3 Data set
3.1 False testimonies in Court
In order to study deceptive language, we created
the DECOUR - DEception in COURt - corpus,
better described in Fornaciari and Poesio (2012).
DECOUR is a corpus constituted by the tran-
scripts of 35 hearings held in four Italian Courts:
Bologna, Bolzano, Prato and Trento. These tran-
scripts report verbatim the statements issued by a
total of 31 different subjects - four of which have
been heard twice. All the hearings come from
criminal proceedings for calumny and false tes-
timony (artt. 368 and 372 of the Italian Criminal
Code).
In particular, the hearings of DECOUR come
mainly from two situations:
? the defendant for any criminal proceeding
tries to use calumny against someone;
? a witness in any criminal proceeding lies for
some reason.
In both cases, a new criminal proceeding arises,
in which the subjects can issue new statements or
not, and having as a body of evidence the tran-
script of the hearing held in the previous proceed-
ing.
The crucial point is that DECOUR only in-
cludes text from individuals who in the end have
been found guilty. Hence the proceeding ends
40
with a judgment of the Court which summarize
the facts, pointing out precisely the lies told by
the speaker in order to establish his punishment.
Thanks to the transcripts of the hearing and to the
final judgment of the Court, it is possible to anno-
tate the statements of the speakers on the basis of
their truthfulness or untruthfulness, as follows.
3.2 Annotation and agreement
The hearings are dialogs, in which the judge, the
public prosecutor and the lawyer pose questions
to the witness/defendant who in turn has to give
them answers. These answers are the object of
investigation of this study. Each answer is con-
sidered a turn, delimited by the end of the pre-
vious and the beginning of the following inter-
vention of another individual. Each turn is con-
stituted by one or more utterances, delimited by
punctuation marks: period, triple-dots, question
and exclamation marks. Utterances are the anal-
ysis unit of DECOUR and have been annotated as
false, true or uncertain. In order to verify the
agreement in the judgments about truthfulness or
untruthfulness of the utterances, three annotators
separately annotated about 600 utterances. The
agreement study concerning the three classes of
utterances, described in detail in (Fornaciari and
Poesio, 2012), showed that the agreement value
was k=.57. Instead, if the problem is reduced to
a binary task - that is, if true and uncertain utter-
ances are collapsed into a single category of not-
false utterances, opposed to the category of false
ones - the agreement value is k=.64.
3.3 Corpus statistics
The whole corpus has been tokenized and sensi-
tive data have been made anonymous, according
to the previous agreement with the Courts. Then
DECOUR has been lemmatized and POS-tagged
using a version of TreeTagger1 (Schmid, 1994)
trained for Italian.
DECOUR is made up of 3015 utterances, which
come from 2094 turns. 945 utterances have been
annotated as false, 1202 as true and 868 as un-
certain. The size of DECOUR is 41819 tokens,
including punctuation blocks.
1http://www.ims.uni-stuttgart.
de/projekte/corplex/TreeTagger/
DecisionTreeTagger.html
4 Methods
In this Section we first summarize our classifica-
tion methods from previous work, then discuss the
three experiments we carried out.
4.1 Classification methods
Each utterance is described by a feature vector.
As in our previous studies (Fornaciari and Poesio,
2011a,b) three kinds of features were used.
First of all, the feature vectors include very ba-
sic linguistic information such as the length of ut-
terances (with and without punctuation) and the
number of words longer than six letters.
The second type of information are lexical fea-
tures. These features have been collected mak-
ing use of LIWC - Linguistic Inquiry and Word
Count, a linguistic tool realized by Pennebaker
et al (2001) and widely employed in deception
detection (Newman et al, 2003; Strapparava and
Mihalcea, 2009). LIWC is based on a dictionary
in which each term is associated with an appro-
priate set of syntactical, semantical and/or psy-
chological categories. When a text is analysed
with LIWC, the tokens of the text are compared
with the LIWC dictionary. Every time a word
present in the dictionary is found, the count of
the corresponding categories grows. The output
is a profile of the text which relies on the rate of
incidence of the different categories in the text it-
self. LIWC also includes different dictionaries for
several languages, amongst which Italian (Agosti
and Rellini, 2007). Therefore it has been possi-
ble to apply LIWC to Italian deceptive texts, and
the approximate 80 linguistic dimensions which
constitute the Italian LIWC dictionary have been
included as features of the vectors.
Lastly, frequencies of lemmas and part-of-
speech n-grams were used. Five kinds of n-
grams of lemmas and part-of-speech were taken
into consideration: from unigrams to pentagrams.
These frequency lists come from the part of DE-
COUR employed as training set. More precisely,
they come from the utterances held as true or false
of the training set, while the uncertain utterances
have not been considered. In order to empha-
size the collection of features effective in clas-
sifying true and false statements, frequency lists
of n-grams have been built considering true and
false utterances separately. This means that, in
the training set, homologous frequency lists of n-
41
Table 1: The most frequent n-grams collected
N-grams Lemmas POS Total
Unigrams 50 15
Bigrams 40 12
Trigrams 30 9
Tetragrams 20 6
Pentagrams 10 3
Total 150 45 195
grams - unigrams, bigrams and so on - have been
collected from the subset of true utterances and
form the subset of false ones. From these lists,
the most frequent n-grams have been collected, in
a decreasing amount according to the length of the
n-grams. Table 1 shows in detail the number of
the most frequent lemmas and part-of-speech col-
lected for the different n-grams. Then the couples
of frequency lists were merged into one.
This procedure implies that the number of sur-
face features is not determined a priori. In fact
the 195 features indicated in Table 1, which are
collected from true and false utterances, are uni-
fied in a list where each feature has to appear
only once. Therefore, theoretically in the case of
perfect identity of features in true and false ut-
terances, a final list with the same 195 features
would be obtained. In the opposite case, if the
n-grams from true and false utterances would be
completely different, a list of 195 + 195, then 390
n-grams would result. The aim of this procedure
is to get a list of n-grams which could be as much
as possible representative of the features of true
and false utterances. Obviously, the smaller the
overlap of the features of the two subsets, the
greater the difference in the appearance of true
and false utterances, and greater the hope to reach
a good performance in the classification task.
We used the Support Vector Machine imple-
mentation in R (Dimitriadou et al, 2011). As
specified above, the classes of the utterances are
false vs. not-false, where the category of not-false
utterances results from the union of the true and
uncertain ones.
4.2 Corpus division
With the aim of training models able to classify
the utterances of DECOUR as false or not-false,
the corpus has been divided as follows:
Training set The 20 hearings coming from the
Courts of Bologna and Bolzano have been
employed as training set. In terms of anal-
ysis units, this means 2279 utterances, that
is 75.59% of DECOUR. The features of the
vectors come from this set of data.
Test set The 9 hearings of the Court of Trento
have been employed as test set, in order to
evaluate the effectiveness of the trained mod-
els. This test set was made up by 426 utter-
ances, which are 14.13% of DECOUR.
Development set The 6 hearings of the Court of
Prato have been employed as development
set during the phase of choice and calibration
of vector features, therefore this set of utter-
ances is not directly involved in the results of
the following experiments. The develpment
set was constituted by 310 utterances, that is
10.28% of DECOUR.
In the various experimental conditions, some sub-
sets of DECOUR have been taken into consider-
ation. Hence, different hearings have been re-
moved from the test and/or training set in order
to carry out different experiments. Since the test
sets vary in the different experiments, in relation
to each of them different chance levels have been
determined, in order to evaluate the effectiveness
of the models? performance.
4.3 Experiments
Three experiments were carried out. In the first
experiment, the entire corpus was used to train
and test our algorithms. In the second and third
experiment, sub-corpora were identified.
4.3.1 Experiment 1: whole test set
In the first experiment, the classification task
has been carried out simply employing the train-
ing set and the test set as described above, in order
to have a control as reference point in relation to
the following experiments.
4.3.2 Experiment 2: no outliers
In the second experiment, a more homogeneous
subset of DECOUR was obtained by automati-
cally identifying and removing outliers. This was
done in an unsupervised way by building vector
descriptions of the hearings and clustering them.
The features of these vectors were the same n-
grams described above, collected from the whole
42
Figure 1: Multi-Dimensional Scaling of DE-
COURE?ach entity corresponds to a hearing; the letters
represent the sex of the speakers.
corpus (not from the only test set); their values
were the mean values of the frequencies of the ut-
terances belonging to the hearing.
This data set has been transformed into a ma-
trix of between-hearing distances and a Multi-
Dimensional Scaling - MDS function has been
applied to this matrix (Baayen, 2008). Figure 1
shows the plot of MDS function. Each entity cor-
responds to a hearing, and is represented by a let-
ter indicating the sex of the speaker. Getting a
glimpse at Figure 1, it is possible to notice that,
in general, almost all the hearings are quite close
- that is, similar - to each other. Only three hear-
ings seem to be clearly more peripheral than all
the others, particularly the three most to the left in
Figure 1. These hearings have been considered as
outliers and shut out from the experiment. They
are two hearings from Trento and one from Prato.
In practice, it means that the training set, com-
ing from the hearings of Bologna and Bolzano,
remained the same as the previous experiment,
while two hearings have been removed from the
test set, which was constituted only by the hear-
ings of Trento.
4.3.3 Experiment 3: only male speakers
Different from the previous one, the third ex-
periment does not rely on a subset of data au-
tomatically identified. Instead, the subset comes
from personal information concerning the sub-
jects involved in the hearings. In fact, their sex,
place of birth and age at the moment of the hear-
ing are known. In this paper, places of birth
and age have not been taken into consideration,
since grouping them together in reliable cate-
gories raises issues that do not have a straightfor-
ward solution, and the size of the subsets of cor-
pus which would be obtained must be taken into
account.
Therefore this experiment has been carried out
taking into consideration only the sex of the sub-
jects, and in particular it concerned only the hear-
ings involving men. This meant reducing the
training set consistently, where seven hearings of
women were present and thence removed. Instead
from the test set just three hearings have been
taken off, one involving a woman and two involv-
ing a transsexual.
4.4 Baselines
The chance levels for the various test sets have
been calculated through Monte Carlo simula-
tions, each one specific to every experiment. In
each simulation, 100000 times a number of ran-
dom predictions has been produced, in the same
amount and with the same rate of false utterances
of the test set employed in the single experiment.
Then this random output was compared to the real
sequence of false and not-false utterances of the
test set, in order to count the amount of correct
predictions. The rate of correct answers reached
by less than 0.01% of the random predictions has
been accepted as chance threshold for every ex-
periment.
As a baseline, a simple majority baseline was
computed: to classify each utterance as belonging
to the most numerous class in the test set (not-
false).
5 Results
The test set of the first experiemnt, carried out
on the whole test set, was made up of 426 utter-
ances, of which 190 were false, that is 44.60%.
While the majority baseline is 55.40% of accu-
racy, a Monte Carlo simulation applied to the test
set showed that the chance level was 59.60% of
correct predictions. The results are shown in Ta-
ble 2. The overall accuracy - almost 66% - is
clearly above the chance level, being more than
six points greater than the baseline.
43
Table 2: Whole training and test set
Correctly Incorrectly
classified entities classified entities Precision Recall F-measure
False utterances 59 131 80.82% 31.05% 44.86%
True utterances 222 14 62.89% 94.07% 75.38%
Total 281 145
Total percent 65.96% 34.04%
Monte Carlo simulation 59.60%
Majority baseline 55.40%
Table 3: Test set without outliers
Correctly Incorrectly
classified entities classified entities Precision Recall F-measure
False utterances 51 90 80.95% 36.17% 50.00%
True utterances 180 12 66.67% 93.75% 77.92%
Total 231 102
Total percent 69.37% 30.63%
Monte Carlo simulation 61.26%
Majority baseline 57.66%
Table 4: Training and test set with only male speakers
Correctly Incorrectly
classified entities classified entities Precision Recall F-measure
False utterances 32 85 74.42% 27.35% 40.00%
True utterances 179 11 67.80% 94.21% 78.85%
Total 211 96
Total percent 68.73% 31.27%
Monte Carlo simulation 63.19%
Majority baseline 61.89%
In the second experiment, the test set without
outliers was made up of 333 utterances; 141 were
false, which means 42.34% of the test set. The
majority baseline was then at 57.66%, while the
chance threshold determined with a Monte Carlo
simulation had an accuracy rate of 61.26%. Ta-
ble 3 shows the results of the analyses. Taking the
outliers out of the test set alows tthe best perfor-
mance of the three experiments to be reached. In
fact the accuracy is more than 69%, which is more
than eight points above the highest chance level of
61.26%.
In the third experimental condition, where only
male speakers were considered, the training set
was made up of 13 hearings and the test set of
6 hearings. The utterances in the test set were
307, of which 117 were false, meaning 38.11%
of the test set. In this last case, the majority base-
line is at 61.89% of accuracy, while according to
a Monte Carlo simulation the chance level was
63.19%. The overall accuracy reached in this ex-
periment, shown in Table 4, was more than 68%:
higher than the first experiment, but in this case
the lower amount of false utterances in the test
set led to higher chance thresholds. Therefore the
difference between performance and the chance
44
level of 63.19% is now the smallest of all the ex-
periments: just five points and half.
From the point of view of detection of false
utterances, although with internal differences, all
the experiments are placed in the same reference
frame. In particular, the weak point in perfor-
mance is always the recall of false utterances,
which remains more or less at 30%. Instead the
good news comes from the precision in recogniz-
ing them, which is close to 80%. Regarding true
utterances, the recall is always good, being never
lower than 93%, while the precision is close to
65%.
6 Discussion
The goal of this paper was to verify if restricting
the analysis to more homogeneous subsets could
improve the accuracy of our models. The results
are mixed. On the one end, taking the outliers out
of the corpus results in a remarkable improvement
of accuracy in the classification task, in relation
to the performance of the models tested on the
whole test set. On the other end, in other cases
- most clearly, considering only speakers of the
male gender - we find no difference; our hypoth-
esis is that any potential advantage derived from
the increased homogeneity is offset by the reduc-
tion in training material (seven hearings are re-
moved in this case). So the conclusion may be
that increasing homogeneity is effective provided
that the remaining set is still sufficiently large.
Regarding the models? capacity to detect false
rather than true utterances, the difference between
the respective recalls is noteworthy. In fact, while
the recall of not-false utterances is very high, that
of false ones is poor. In other words, the results
indicate that an amount of false utterances is ef-
fectively so similar to the not-false ones, that the
models are not able to detect them. One challenge
for future studies is surely to find a way to detect
some aspect currently neglected of deceptive lan-
guage, which could be employed to widen the size
of false utterances which can be recognized.
On the other hand, in the two more reliable ex-
periments the precision in detecting false utter-
ances was about 80%. This could suggest that an
amount of false utterances exists, whose features
are in some way peculiar and different from not-
false ones. The data seem to show that this subset
could be more or less one third of all the false ut-
terances.
However, this study was not aimed to estimate
the possible performance of the models in an hy-
pothetic practical application. The experimental
conditions taken into consideration, in fact, are
considerably different from those that would be
present in a real life analysis.
The main reason of this difference is that in a
real case to classify every utterance of a hearing
would not be requested. A lot of statements are ir-
relevant or perfectly known as true. Furthermore
it would not make sense to classify all the utter-
ances which have not propositional value, such as
questions or meta-communicative acts. In the per-
spective of deception detection in a real life sce-
nario, to classify this last kind of utterances is use-
less. Only a subset of the propositional statements
should be classified. In a previous study, carried
out on a selection of utterances with propositional
value of a part of DECOUR, machine learning
models reached an accuracy of 75% in classifica-
tion task (Fornaciari and Poesio, 2011b). In that
study, precision and recall of false utterances are
also quite similar to those of this study, the first
being about 90% and the second about 50%.
From a theoretical point of view, the present
study suggests that it is possible to be relatively
confident in the effectiveness of the models in the
analysis of any kind of utterance. This means
that deceptive language is at least in part differ-
ent from the truthful one and stylometric analyses
can detect it. If this is true, the rate of precision
with which false statements are correctly classi-
fied should clearly exceed the chance level.
Also in this case, Monte Carlo simulation is
taken as reference point. Out of the 100000 ran-
dom trials carried out to determine the baseline for
the first experiment, less than 0.01% had a preci-
sion greater than 57.90% in classifying false ut-
terances, in front of a precision of the models at
80.82%. Regarding the second experiment, the
threshold for precision related to false utterances
was 58.15% against a precision of the models at
80.95%. In the third experiment, the baseline
for precision was 55.55% and the performance of
models was 74.42%. In every experiment the gap
is about twenty points per cent. The same cannot
be said about the recall of false utterances: the
baselines of Monte Carlo simulations in the three
experiments were about 51-54%, while the best
models? performance (of the second experiment)
did not exceed 36%.
45
The precision reached in recognizing false
statements shows that the models were reliable
in detection of deceptive language. On the other
hand a remarkable amount of false utterances was
not identified. The challenge for the future is to
understand to which extent it will be possible to
improve the recall in detecting false utterances,
not losing and hopefully improving the relative
precision. At that point, although in specific con-
texts, a computational linguistics? approach could
be really employed to detect deception in real life
scenarios.
7 Acknowledgements
To create DECOUR has been very complex, and
it would not have been possible without the kind
collaboration of a lot of people. Many thanks to
Dr. Francesco Scutellari, President of the Court
of Bologna, to Dr. Heinrich Zanon, President of
the Court of Bolzano, to Dr. Francesco Antonio
Genovese, President of the Court of Prato and to
Dr. Sabino Giarrusso, President of the Court of
Trento.
References
Agosti, A. and Rellini, A. (2007). The Italian
LIWC Dictionary. Technical report, LIWC.net,
Austin, TX.
Baayen, R. (2008). Analyzing linguistic data:
a practical introduction to statistics using R.
Cambridge University Press.
Bachenko, J., Fitzpatrick, E., and Schonwetter,
M. (2008). Verification and implementation
of language-based deception indicators in civil
and criminal narratives. In Proceedings of the
22nd International Conference on Computa-
tional Linguistics - Volume 1, COLING ?08,
pages 41?48, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Coulthard, M. (2004). Author identification, idi-
olect, and linguistic uniqueness. Applied Lin-
guistics, 25(4):431?447.
Davatzikos, C., Ruparel, K., Fan, Y., Shen, D.,
Acharyya, M., Loughead, J., Gur, R., and Lan-
gleben, D. (2005). Classifying spatial patterns
of brain activity with machine learning meth-
ods: Application to lie detection. NeuroImage,
28(3):663 ? 668.
De Paulo, B. M., Lindsay, J. J., Malone, B. E.,
Muhlenbruck, L., Charlton, K., and Cooper, H.
(2003). Cues to deception. Psychological Bul-
letin, 129(1):74?118.
Dimitriadou, E., Hornik, K., Leisch, F., Meyer,
D., and Weingessel, A. (2011). r-cran-
e1071. http://mloss.org/software/
view/94/.
Fitzpatrick, E. and Bachenko, J. (2009). Building
a forensic corpus to test language-based indi-
cators of deception. Language and Computers,
71(1):183?196.
Fornaciari, T. and Poesio, M. (2011a). Lexical
vs. surface features in deceptive language anal-
ysis. In Proceedings of the ICAIL 2011 Work-
shop Applying Human Language Technology to
the Law, AHLTL 2011, pages 2?8, Pittsburgh,
USA.
Fornaciari, T. and Poesio, M. (2011b). Sin-
cere and deceptive statements in italian crimi-
nal proceedings. In Proceedings of the Interna-
tional Association of Forensic Linguists Tenth
Biennial Conference, IAFL 2011, Cardiff,
Wales, UK.
Fornaciari, T. and Poesio, M. (2012). Decour: a
corpus of deceptive statements in italian courts.
In Proceedings of the eighth International Con-
ference on Language Resources and Evalua-
tion, LREC 2012. In press.
Frank, M. G., Menasco, M. A., and O?Sullivan,
M. (2008). Human behavior and deception de-
tection. In Voeller, J. G., editor, Wiley Hand-
book of Science and Technology for Homeland
Security. John Wiley & Sons, Inc.
Ganis, G., Kosslyn, S., Stose, S., Thompson, W.,
and Yurgelun-Todd, D. (2003). Neural corre-
lates of different types of deception: An fmri
investigation. Cerebral Cortex, 13(8):830?836.
Jensen, M. L., Meservy, T. O., Burgoon, J. K., and
Nunamaker, J. F. (2010). Automatic, Multi-
modal Evaluation of Human Interaction. Group
Decision and Negotiation, 19(4):367?389.
Koppel, M., Schler, J., Argamon, S., and Pen-
nebaker, J. (2006). Effects of age and gender on
blogging. In AAAI 2006 Spring Symposium on
Computational Approaches to Analysing We-
blogs.
46
Luyckx, K. and Daelemans, W. (2008). Author-
ship attribution and verification with many au-
thors and limited data. In Proceedings of the
22nd International Conference on Computa-
tional Linguistics - Volume 1, COLING ?08,
pages 513?520, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.
Mosteller, F. and Wallace, D. (1964). Infer-
ence and Disputed Authorship: The Federalist.
Addison-Wesley.
Newman, M. L., Pennebaker, J. W., Berry, D. S.,
and Richards, J. M. (2003). Lying Words:
Predicting Deception From Linguistic Styles.
Personality and Social Psychology Bulletin,
29(5):665?675.
Peersman, C., Daelemans, W., and Van Vaeren-
bergh, L. (2011). Age and gender prediction on
netlog data. Presented at the 21st Meeting of
Computational Linguistics in the Netherlands
(CLIN21), Ghent, Belgium.
Pennebaker, J. W., Francis, M. E., and Booth, R. J.
(2001). Linguistic Inquiry and Word Count
(LIWC): LIWC2001. Lawrence Erlbaum As-
sociates, Mahwah.
Schmid, H. (1994). Probabilistic part-of-speech
tagging using decision trees. In Proceedings of
International Conference on New Methods in
Language Processing.
Solan, L. M. and Tiersma, P. M. (2004). Author
identification in american courts. Applied Lin-
guistics, 25(4):448?465.
Stein, B., Koppel, M., and Stamatatos, E. (2007).
Plagiarism analysis, authorship identification,
and near-duplicate detection pan?07. SIGIR Fo-
rum, 41:68?71.
Strapparava, C. and Mihalcea, R. (2009). The
Lie Detector: Explorations in the Automatic
Recognition of Deceptive Language. In Pro-
ceeding ACLShort ?09 - Proceedings of the
ACL-IJCNLP 2009 Conference Short Papers.
Vaassen, F. and Daelemans, W. (2011). Auto-
matic emotion classification for interpersonal
communication. In 2nd Workshop on Compu-
tational Approaches to Subjectivity and Senti-
ment Analysis (WASSA 2.011).
Vrij, A. (2005). Criteria-based content analysis
- A Qualitative Review of the First 37 Studies.
Psychology, Public Policy, and Law, 11(1):3?
41.
Zhou, L., Shi, Y., and Zhang, D. (2008). A
Statistical Language Modeling Approach to
Online Deception Detection. IEEE Transac-
tions on Knowledge and Data Engineering,
20(8):1077?1081.
47
Proceedings of the Joint Conference on EMNLP and CoNLL: Shared Task, pages 122?128,
Jeju Island, Korea, July 13, 2012. c?2012 Association for Computational Linguistics
BART goes multilingual: The UniTN / Essex submission to the CoNLL-2012
Shared Task
Olga Uryupina? Alessandro Moschitti? Massimo Poesio??
?University of Trento
? University of Essex
uryupina@gmail.com, moschitti@disi.unitn.it, massimo.poesio@unitn.it
Abstract
This paper describes the UniTN/Essex sub-
mission to the CoNLL-2012 Shared Task on
the Multilingual Coreference Resolution. We
have extended our CoNLL-2011 submission,
based on BART, to cover two additional lan-
guages, Arabic and Chinese. This paper fo-
cuses on adapting BART to new languages,
discussing the problems we have encountered
and the solutions adopted. In particular, we
propose a novel entity-mention detection algo-
rithm that might help identify nominal men-
tions in an unknown language. We also dis-
cuss the impact of basic linguistic information
on the overall performance level of our coref-
erence resolution system.
1 Introduction
A number of high-performance coreference resolu-
tion (CR) systems have been created for English in
the past decades, implementing both rule-based and
statistical approaches. For other languages, how-
ever, the situation is far less optimistic. For Ro-
mance and German languages, several systems have
been developed and evaluated, in particular, at the
SemEval-2010 track 1 on Multilingual Coreference
Resolution (Recasens et al, 2010). For other lan-
guages, individual approaches have been proposed,
covering specific subparts of the task, most com-
monly pronominal anaphors (cf., for example, (Iida
and Poesio, 2011; Arregi et al, 2010) and many oth-
ers).
Two new languages, Arabic and Chinese, have
been proposed for the CoNLL-2012 shared task
(Pradhan et al, 2012). They present a challeng-
ing problem: the systems are required to pro-
vide entity mention detection (EMD) and design a
proper coreference resolver for both languages. At
UniTN/Essex, we have focused on these parts of the
task, relying on a modified version of our last-year
submission for English.
Most state-of-the-art full-scale coreference reso-
lution systems rely on hand-written rules for the
mention detection subtask.1 For English, such rules
may vary from corpus to corpus, reflecting specifics
of particular guidelines (e.g. whether nominal pre-
modifiers can be mentions, as in MUC, or not, as in
most other corpora). However, for each corpus, such
heuristics can be adjusted in a straightforward way.
Creating a robust rule-based EMD module for a new
language, on the contrary, is a challenging issue that
requires substantial linguistic knowledge.
In this paper, we advocate a novel approach, re-
casting parse-based EMD as a statistical problem.
We consider a node-filtering model that does not rely
on any linguistic expertise in a given language. In-
stead, we use tree kernels (Moschitti, 2008; Mos-
chitti, 2006) to induce a classifier for mention NP-
nodes automatically from the data.
Another issue to be solved when designing a
coreference resolution system for a new language
is a possible lack of relevant linguistic information.
Most state-of-the-art CR algorithms rely on rela-
tively advanced linguistic representations of men-
tions. This can be seen as a remarkable shift
1Statistical EMD approaches have been proved useful for
ACE-style coreference resolution, where mentions are basic
units belonging to a restricted set of semantic types.
122
from knowledge-lean approaches of the late nineties
(Harabagiu and Maiorano, 1999). In fact, modern
systems try to account for complex coreference links
by incorporating lexicographic and world knowl-
edge, for example, using WordNet (Harabagiu et al,
2001; Huang et al, 2009) or Wikipedia (Ponzetto
and Strube, 2006). For languages other than English,
however, even the most basic properties of mentions
can be intrinsically difficult to extract. For example,
Baran and Xue (2011) have shown that a complex al-
gorithm is needed to identify the number property
of Chinese nouns.
Both Arabic and Chinese have long linguistic tra-
ditions and therefore most grammar studies rely on
terminology that can be very confusing for an out-
sider. For example, several works on Arabic (Hoyt,
2008) mention that nouns can be made definite with
the suffix ?Al-?, but this is not a semantic, but syn-
tactic definiteness. Without any experience in Ara-
bic, one can hardly decide how such ?syntactic defi-
niteness? might affect coreference.
In the present study, we have used the informa-
tion provided by the CoNLL organizers to try and
extract at least some linguistic properties of men-
tions for Arabic and Chinese. We have run several
experiments, evaluating the impact of such very ba-
sic knowledge on the performance level of a coref-
erence resolution system.
The rest of the paper is organized as follows. In
the next section we briefly describe the general ar-
chitecture and the system for English, focusing on
the adjustments made after the last year competition.
Section 3 is devoted to new languages: we first dis-
cuss our EMD module and then describe the proce-
dures for extracting linguistic knowledge. Section 4
discusses the impact of our solutions to the perfor-
mance level of a coreference resolver. The official
evaluation results are presented in Section 5.
2 BART
Our CoNLL submission is based on BART (Versley
et al, 2008). BART is a modular toolkit for corefer-
ence resolution that supports state-of-the-art statisti-
cal approaches to the task and enables efficient fea-
ture engineering. BART has originally been created
and tested for English, but its flexible modular archi-
tecture ensures its portability to other languages and
domains.
The BART toolkit has five main components: pre-
processing pipeline, mention factory, feature extrac-
tion module, decoder and encoder. In addition, an
independent LanguagePlugin module handles all the
language specific information and is accessible from
any component.
The architecture is shown in Figure 1. Each mod-
ule can be accessed independently and thus adjusted
to leverage the system?s performance on a particular
language or domain.
The preprocessing pipeline converts an input doc-
ument into a set of linguistic layers, represented
as separate XML files. The mention factory uses
these layers to extract mentions and assign their
basic properties (number, gender etc). The fea-
ture extraction module describes pairs of mentions
{Mi,Mj}, i < j as a set of features. At the
moment we have around 45 different feature ex-
tractors, encoding surface similarity, morpholog-
ical, syntactic, semantic and discourse informa-
tion. Note that no language-specific information
is encoded in the extractors explicitly: a language-
independent representation, provided by the Lan-
guage Plugin, is used to compute feature val-
ues. For CoNLL-2012, we have created two addi-
tional features: lemmata-match (similar to string
match, but uses lemmata instead of tokens) and
number-agreement-du (similar to commonly
used number agreement features, but supports dual
number).
The encoder generates training examples through
a process of sample selection and learns a pairwise
classifier. Finally, the decoder generates testing ex-
amples through a (possibly distinct) process of sam-
ple selection, runs the classifier and partitions the
mentions into coreference chains.
2.1 Coreference resolution in English
The English track at CoNLL-2012 can be considered
an extension of the last year?s CoNLL task. New
data have been added to the corpus, including two
additional domains, but the annotation guidelines re-
main the same.
We have therefore mainly relied on the CoNLL-
2011 version of our system (Uryupina et al, 2011)
for the current submission, providing only minor ad-
justments. Thus, we have modified our preprocess-
123
POS Tagger
Merger
Mention Tagger
Parser
Preprocessing
Mention
Factory
Coreference
chains(entities)
Language Plugin
UnannotatedText
FeatureExtractor
LearnerMachine
Encoder/Decoder
Figure 1: BART architecture
ing pipeline to operate on the OntoNotes NE-types,
mapping them into MUC types required by BART.
This allows us to participate in the closed track, as
no external material is used any longer.
Since last year, we have continued with our exper-
iments on multi-objective optimization, proposed in
our CoNLL-2011 paper (Uryupina et al, 2011). We
have extended the scope of our work to cover differ-
ent machine learning algorithms and their parame-
ters (Saha et al, 2011). For CoNLL-2012, we have
re-tested all the solutions of our optimization exper-
iments, picking the one with the highest score on the
current development set.
Finally, our recent experiments on domain se-
lection (Uryupina and Poesio, 2012) suggest that,
at least for some subparts of OntoNotes, a sys-
tem might benefit from training a domain-specific
model. We have tested this hypothesis on the
CoNLL-2012 data and have consequently trained
domain-specific classifiers for the nw and bc do-
mains.
3 Coreference resolution in Arabic and
Chinese
We have addressed two main issues when develop-
ing our coreference resolvers for Arabic and Chi-
nese: mention detection and extraction of relevant
linguistic properties of our mentions.
3.1 Mention detection
Mention detection is rarely considered to be a sepa-
rate task. Only very few studies on coreference reso-
lution report on their EMD techniques. Existing cor-
pora of coreference follow different approaches to
mention annotation: this includes defining mention
boundaries (basic vs. maximal NPs), alignment pro-
cedures (strict vs. relaxed with manually annotated
minimal spans vs. relaxed with automatically ex-
tracted heads), the position on singleton and/or non-
referential mentions (annotated vs. not).
The CoNLL-2011/2012 guidelines take a very
strict view on mention boundaries: only the maxi-
mal spans are annotated and no approximate match-
ing is allowed. Moreover, the singleton mentions
(i.e. not participating in coreference relations) are
not marked. This makes the mention detection task
for OntoNotes extremely challenging, especially for
the two new languages: on the one hand, one has
to provide exact boundaries; on the other hand, it is
hard to learn such information explicitly, as not all
the candidate mentions are annotated.
Most CoNLL-2011 systems relied on hand-
written rules for the mention detection subtask. This
was mainly possible due to the existence of well-
studied and thoroughly documented head-detection
rules for English, available as a description for reim-
plementing (Collins, 1999) or as a downloadable
package. Consider the following example:
(1) ..((the rising price)NP2 of (food)NP3)NP1 ..
124
In this fragment, three nominal phrases can be iden-
tified, with the first one (?the rising price of food?)
spanning over the two others (?the rising price?) and
(?food?). According to the OntoNotes annotation
guidelines, the second noun phrase cannot be a men-
tion, because it is embedded in an upper NP and they
share the same head noun. The third noun phrase, on
the contrary, could be a mention?even though it?s
embedded in another NP, their heads are different.
Most CoNLL-2011 participants used as a backbone
a heuristic discarding embedded noun phrases.
For less-known languages, however, this heuris-
tic is only applicable as long as we can compute an
NP?s head reliably. Otherwise it?s hard to distinguish
between candidate mentions similar to NP1 and to
NP2 in the example above.
A set of more refined heuristics is typically ap-
plied to discard or add some specific types of men-
tions. For example, several studies (Bergsma and
Yarowsky, 2011) have addressed the issue of detect-
ing expletive pronouns in English. Again, in the ab-
sence of linguistic expertise, one can hardly engi-
neer such heuristics for a new language manually.
We have investigated the possibility of learn-
ing mention boundaries automatically from the
OntoNotes data. We recast the problem as an NP-
node filtering task: we analyze automatically com-
puted parse trees and consider all the NP-nodes to be
candidate instances to learn a classifier of correct vs.
incorrect mention nodes. Clearly, this approach can-
not account for mentions that do not correspond to
NP-nodes. However, as Table 1 shows, around 85-
89% of all the mentions, both for Arabic and Chi-
nese, are NP-nodes.
train development
NP-nodes % NP-nodes %
Arabic 24068 87.23 2916 87.91
Chinese 88523 85.96 12572 88.52
Table 1: NP-nodes in OntoNotes for Arabic and Chinese:
total numbers and percentage of mentions.
We use tree kernels (Moschitti, 2008; Moschitti,
2006) to induce a classifier that labels an NP node
and a part of the parse tree that surrounds it as
?mention. Two integer parameters control the se-
lection of the relevant part of the parse tree, allowing
for pruning the nodes that are far above or far below
the node of interest.
Our classifier is supposed to decide whether an
NP-node is a mention of a real-world object. Such
mentions, however, are annotated in OntoNotes as
positive instances only when they corefer with some
other mentions. The classifier works as a preproces-
sor for a CR system and therefore has no information
that would allow it to discriminate between single-
ton vs. non-singleton mentions. One can investigate
possibilities for joint EMD and CR to alleviate the
problem. We have adopted a simpler solution: we
tune a parameter (cost factor) that controls the pre-
cision/recall trade-off to bias the classifier strongly
towards recall.
We use a small subset (1-5%) of the training data
to train the EMD classifier. We tune the EMD pa-
rameters to optimize the overall performance: we
run the classifier to extract mentions for the whole
training and development sets, run the coreference
resolver and record the obtained result (CoNLL
score). The whole set of parameters to be tuned
comprise: the size of the training set for EMD, the
precision-recall trade-off, and two pruning thresh-
olds.
3.2 Extracting linguistic properties
All the features implemented in BART use some
kind of linguistic information from the mentions.
For example, the number-agreement feature
first extracts the number properties of individual
mentions. For a language supported by BART, such
properties are computed by the MentionFactory. For
a new language, they should be provided as a part of
the mention representation computed by some ex-
ternal preprocessing facilities. The only obligatory
mention property is its span? the sequence of rel-
evant token ids?all the properties discussed below
are optional.
The following properties have been extracted for
new languages directly from the CoNLL table:
? sentence id
? sequence of lemmata
? speaker (Chinese only)
Coordinations have been determined by analyz-
ing the sequence of PoS tags: any span containing
125
a coordinate conjunction is a coordination. They are
always considered plural and unspecified for gender,
their heads correspond to their entire spans.
For non-coordinate NPs, we extract the head
nouns using simple heuristics. In Arabic, the first
noun in a sequence is a head. In Chinese, the last
one is a head. If no head can be found through this
heuristic, we try the same method, but allow for pro-
nouns to be heads, and, as a default, consider the
whole span to be the head.
Depending on the PoS tag of the head noun, we
classify a mention as an NE, a pronoun or a nomi-
nal (default). For named entities, no further mention
properties have been extracted.
We have compiled lists of pronouns for both Ara-
bic and Chinese from the training and development
data. For Arabic, we use gold PoS tags to classify
pronouns into subtypes, person, number and gender.
For Chinese, no such information is available, so we
have consulted several grammar sketches and lists of
pronouns on the web. We do not encode clusivity2
and honorifics.3
For Arabic, we extract the gender and number
properties of nominals in the following way. First,
we have processed the gold PoS tags to create a list
of number and gender affixes. We compute the prop-
erties of our mentions by analyzing the affixes of
their heads. In a number of constructions, however,
the gender is not marked explicitly, so we have com-
piled a gender dictionary for Arabic lemmata on the
training and development data. If the gender can-
not be computed from affixes, we look it up in the
dictionary.
Finally, we have made an attempt at computing
the definiteness of nominal expressions. For Arabic,
we consider as definites all mentions with definite
head nouns (prefixed with ?Al?) and all the idafa
constructs with a definite modifier.4 We could not
compute definiteness for Chinese reliably.
2In some dialects of Chinese, a distinction is made between
the first person plural inclusive (?you and me?) and the first
person exclusive (?me and somebody else?) pronouns.
3In Chinese, different pronouns should be used address-
ing different persons, reflecting the relative social status of the
speaker and the listener.
4Idafa-constructs are syntactic structures, conveying, very
roughly speaking, genitive semantics, commonly used in Ara-
bic. Their accurate analysis requires some language-specific
processing.
4 Evaluating the impact of kernel-based
mention detection and basic linguistic
knowledge
To adopt our system to new languages, we have fo-
cused on two main issues: EMD and extraction of
linguistic properties. In this section we discuss the
impact of each factor on the overall performance.
Table 2 summarizes our evaluation experiments. All
the figures reported in this section are CoNLL scores
(averages of MUC, B3 and CEAFe) obtained on the
development data.
To evaluate the impact of our kernel-based EMD
(TKEMD), we compare its performance against two
baselines. The lower bound, ?allnp?, considers all
the NP-nodes in a parse tree to be candidate men-
tions. The upper bound, ?goldnp? only considers
gold NP-nodes to be candidate mentions. Note that
the upper bound does not include mentions that do
not correspond to NP-nodes at all (around 12% of
all the mentions in the development data, cf. Table 1
above).
We have created three versions of our corefer-
ence resolver, using different amounts of linguistic
knowledge. The baseline system (Table 2, first col-
umn) relies only on mention spans. The system it-
self is a reimplementation of Soon et al (2001), but,
clearly, only the string-matching feature can be com-
puted without specifying mention properties.
A more advanced version of the system (second
column) uses the same model and the same feature
set, but relies on mention properties, extracted as de-
scribed in Section 3.2 above. The final version (third
column) makes use of all the features implemented
in BART. We run a greedy feature selection algo-
rithm, starting from the string matching and adding
features one by one, until the performance stops in-
creasing.
For Chinese, our EMD approach has proved to be
useful, bringing around 1.5-2% improvement over
the ?allnp? baseline for all the versions of the coref-
erence resolver. The module for extracting mention
properties has only brought a moderate improve-
ment. This is not surprising, as we have not been
able to extract many relevant linguistic properties,
especially for nominals. We believe that an improve-
ment can be achieved on the Chinese data by incor-
porating more linguistic information.
126
baseline +linguistics +linguistics
+features
Arabic
allnp 45.47 46.15 46.32
TKEMD 46.98 47.44 49.07
goldnp 51.08 63.27 64.55
Chinese
allnp 50.72 51.04 51.40
TKEMD 53.10 53.33 53.53
goldnp 57.78 57.30 57.98
Table 2: Evaluating the impact of EMD and linguistic
knowledge: CoNLL F-score.
For Arabic, the linguistic properties could poten-
tially be very helpful: on gold NPs, our linguistically
rich system outperforms its knowledge-lean coun-
terpart by 13 percentage points. Unfortunately, this
improvement is mirrored only partially on the fully
automatically acquired mentions.
5 Official results
Table 3 shows the official results obtained by our
system at the CoNLL-2012 competition.
Metric Recall Precision F-score
English
MUC 61.00 60.78 60.89
BCUBED 63.59 68.48 65.95
CEAF (M) 52.44 52.44 52.44
CEAF (E) 41.42 41.64 41.53
BLANC 67.40 72.83 69.65
Arabic
MUC 41.33 41.66 41.49
BCUBED 65.77 69.23 67.46
CEAF (M) 50.82 50.82 50.82
CEAF (E) 42.43 42.13 42.28
BLANC 65.58 70.56 67.69
Chinese
MUC 45.62 63.13 52.97
BCUBED 59.17 80.78 68.31
CEAF (M) 52.40 52.40 52.40
CEAF (E) 48.47 34.52 40.32
BLANC 68.72 80.76 73.11
Table 3: BART performance at CoNLL-2012: official re-
sults on the test set.
6 Conclusion
In this paper we have discussed our experiments
on adapting BART to two new languages, Chinese
and Arabic, for the CoNLL-2012 Shared Task on
the Multilingual Coreference Resolution. Our team
has some previous experience with extending BART
to cover languages other than English, in particular,
Italian and German. For those languages, however,
most of our team members had at least an advanced
knowledge, allowing for more straightforward engi-
neering and error analysis. Both Arabic and Chi-
nese present a challenge: they require new mention
detection algorithms, as well as special language-
dependent techniques for extracting mention prop-
erties.
For Arabic, we have proposed several simple ad-
justments to extract basic morphological informa-
tion. As our experiments show, this can potentially
lead to a substantial improvement. The progress,
however, is hindered by the mention detection qual-
ity: even though our TKEMD module outperforms
the lower bound baseline, there is still a lot of
room for improvement, that can be achieved after
a language-aware error analysis.
For Chinese, the subtask of extracting relevant lin-
guistic information has turned out to be very chal-
lenging. We believe that, by elaborating on the
methods for assigning linguistic properties to nomi-
nal mentions and combining them with the TKEMD
module, one can boost the performance level of a
coreference resolver.
7 Acknowledgments
The research described in this paper has been par-
tially supported by the European Community?s Sev-
enth Framework Programme (FP7/2007-2013) un-
der the grants #247758: ETERNALS ? Trustworthy
Eternal Systems via Evolving Software, Data and
Knowledge, and #288024: LIMOSINE ? Linguis-
tically Motivated Semantic aggregation engiNes.
127
References
Olatz Arregi, Klara Ceberio, Arantza D??az De Illar-
raza, Iakes Goenaga, Basilio Sierra, and Ana Zelaia.
2010. A first machine learning approach to pronom-
inal anaphora resolution in Basque. In Proceedings
of the 12th Ibero-American conference on Advances in
artificial intelligence, IBERAMIA?10, pages 234?243,
Berlin, Heidelberg. Springer-Verlag.
Elizabeth Baran and Nianwen Xue. 2011. Singular or
plural? Exploiting parallel corpora for Chinese num-
ber prediction. In Proceedings of the Machine Trans-
lation Summit XIII.
Shane Bergsma and David Yarowsky. 2011. NADA:
A robust system for non-referential pronoun detec-
tion. In Proceedings of the Discourse Anaphora and
Anaphor Resolution Colloquium, Faro, Portugal, Oc-
tober.
Michael Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Univer-
sity of Pennsylvania.
Sanda Harabagiu and Steven Maiorano. 1999.
Knowledge-lean coreference resolution and its rela-
tion to textual cohesion and coherence. In Proceed-
ings of the ACL Workshop On The Relation Of Dis-
course/Dialogue Structure And Reference.
Sanda Harabagiu, Ra?zvan Bunescu, and Steven Maio-
rano. 2001. Text and knowledge mining for coref-
erence resolution. In Proceedings of the 2nd Meeting
of the North American Chapter of the Association for
Computational Linguistics, pages 55?62.
Frederick Hoyt. 2008. The Arabic noun phrase. In
The Encyclopedia of Arabic Language and Linguis-
tics. Leiden:Brill.
Zhiheng Huang, Guangping Zeng, Weiqun Xu, and Asli
Celikyilmaz. 2009. Effectively exploiting WordNet
in semantic class classification for coreference resolu-
tion. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing.
Ryu Iida and Massimo Poesio. 2011. A cross-lingual
ILP solution to zero anaphora resolution. In Proceed-
ings of the 49th Annual Meeting of the Association for
Computational Linguistics, pages 804?813.
Alessandro Moschitti. 2006. Efficient convolution ker-
nels for dependency and constituent syntactic trees.
In Proceedings of European Conference on Machine
Learning, pages 318?329.
Alessandro Moschitti. 2008. Kernel methods, syntax and
semantics for relational text categorization. In Pro-
ceeding of the International Conference on Informa-
tion and Knowledge Management, NY, USA.
Simone Paolo Ponzetto and Michael Strube. 2006.
Exploiting semantic role labeling, WordNet and
Wikipedia for coreference resolution. In Human Lan-
guage Technology Conference of the North American
Chapter of the Association of Computational Linguis-
tics, pages 192?199.
Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,
Olga Uryupina, and Yuchen Zhang. 2012. CoNLL-
2012 shared task: Modeling multilingual unrestricted
coreference in OntoNotes. In Proceedings of the
Sixteenth Conference on Computational Natural Lan-
guage Learning (CoNLL 2012), Jeju, Korea.
Marta Recasens, Llu??s Ma`rquez, Emili Sapena,
M.Anto`nia Mart??, Mariona Taule?, Ve?ronique Hoste,
Massimo Poesio, and Yannick Versley. 2010.
SemEval-2010 Task 1: Coreference resolution in
multiple languages. In Proceedings of the 5th
International Workshop on Semantic Evaluations
(SemEval-2010), Uppsala, Sweden.
Sriparna Saha, Asif Ekbal, Olga Uryupina, and Massimo
Poesio. 2011. Single and multi-objective optimiza-
tion for feature selection in anaphora resolution. In
Proceedings of the International Joint Conference on
Natural Language Processing.
Wee Meng Soon, Hwee Tou Ng, and Daniel Chung Yong
Lim. 2001. A machine learning approach to corefer-
ence resolution of noun phrases. Computational Lin-
guistic, 27(4):521?544.
Olga Uryupina and Massimo Poesio. 2012. Domain-
specific vs. uniform modeling for coreference resolu-
tion. In Proceedings of the Language Resources and
Evaluation Conference.
Olga Uryupina, Sriparna Saha, Asif Ekbal, and Mas-
simo Poesio. 2011. Multi-metric optimization for
coreference: The UniTN / IITP / Essex submission to
the 2011 CONLL shared task. In Proceedings of the
Fifteenth Conference on Computational Natural Lan-
guage Learning (CoNLL 2011).
Yannick Versley, Simone Paolo Ponzetto, Massimo Poe-
sio, Vladimir Eidelman, Alan Jern, Jason Smith,
Xiaofeng Yang, and Alessandro Moschitti. 2008.
BART: a modular toolkit for coreference resolution. In
Proceedings of the 46th Annual Meeting of the Asso-
ciation for Computational Linguistics on Human Lan-
guage Technologies, pages 9?12.
128
