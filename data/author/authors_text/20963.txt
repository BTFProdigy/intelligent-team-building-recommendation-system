Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 273?277,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Building Japanese Textual Entailment Specialized Data Sets
for Inference of Basic Sentence Relations
Kimi Kaneko ? Yusuke Miyao ? Daisuke Bekki ?
? Ochanomizu University, Tokyo, Japan
? National Institute of Informatics, Tokyo, Japan
? {kaneko.kimi | bekki}@is.ocha.ac.jp
? yusuke@nii.ac.jp
Abstract
This paper proposes a methodology for
generating specialized Japanese data sets
for textual entailment, which consists of
pairs decomposed into basic sentence rela-
tions. We experimented with our method-
ology over a number of pairs taken from
the RITE-2 data set. We compared
our methodology with existing studies
in terms of agreement, frequencies and
times, and we evaluated its validity by in-
vestigating recognition accuracy.
1 Introduction
In recognizing textual entailment (RTE), auto-
mated systems assess whether a human reader
would consider that, given a snippet of text t1 and
some unspecified (but restricted) world knowl-
edge, a second snippet of text t2 is true. An ex-
ample is given below.
Ex. 1) Example of a sentence pair for RTE
? Label: Y
? t1: Shakespeare wrote Hamlet and Macbeth.
? t2: Shakespeare is the author of Hamlet.
?Label? on line 1 shows whether textual entail-
ment (TE) holds between t1 and t2. The pair is
labeled ?Y? if the pair exhibits TE and ?N? other-
wise.
It is difficult for computers to make such as-
sessments because pairs have multiple interrelated
basic sentence relations (BSRs, for detailed in-
formation on BSRs, see section 3). Recognizing
each BSRs in pairs exactly is difficult for com-
puters. Therefore, we should generate special-
ized data sets consisting of t1-t2 pairs decomposed
into BSRs and a methodology for generating such
data sets since such data and methodologies for
Japanese are unavailable at present.
This paper proposes a methodology for gener-
ating specialized Japanese data sets for TE that
consist of monothematic t1-t2 pairs (i.e., pairs in
which only one BSR relevant to the entailment
relation is highlighted and isolated). In addition,
we compare our methodology with existing stud-
ies and analyze its validity.
2 Existing Studies
Sammons et al(2010) point out that it is necessary
to establish a methodology for decomposing pairs
into chains of BSRs, and that establishing such
methodology will enable understanding of how
other existing studies can be combined to solve
problems in natural language processing and iden-
tification of currently unsolvable problems. Sam-
mons et al experimented with their methodology
over the RTE-5 data set and showed that the recog-
nition accuracy of a system trained with their spe-
cialized data set was higher than that of the system
trained with the original data set. In addition, Ben-
tivogli et al(2010) proposed a methodology for
classifying more details than was possible in the
study by Sammons et al.
However, these studies were based on only En-
glish data sets. In this regard, the word-order
rules and the grammar of many languages (such
as Japanese) are different from those of English.
We thus cannot assess the validity of methodolo-
gies for any Japanese data set because each lan-
guage has different usages. Therefore, it is neces-
sary to assess the validity of such methodologies
with specialized Japanese data sets.
Kotani et al (2008) generated specialized
Japanese data sets for RTE that were designed
such that each pair included only one BSR. How-
ever, in that approach the data set is generated ar-
tificially, and BSRs between pairs of real world
texts cannot be analyzed.
We develop our methodology by generating
specialized data sets from a collection of pairs
from RITE-21 binary class (BC) subtask data sets
containing sentences from Wikipedia. RITE-2 is
273
an evaluation-based workshop focusing on RTE.
Four subtasks are available in RITE-2, one of
which is the BC subtask whereby systems assess
whether there is TE between t1 and t2. The rea-
son why we apply our methodology to part of the
RITE-2 BC subtask data set is that we can con-
sider the validity of the methodology in view of
the recognition accuracy by using the data sets
generated in RITE-2 tasks, and that we can an-
alyze BSRs in real texts by using sentence pairs
extracted from Wikipedia.
3 Methodology
In this study, we extended and refined the method-
ology defined in Bentivogli et al(2010) and devel-
oped a methodology for generating Japanese data
sets broken down into BSRs and non-BSRs as de-
fined below.
Basic sentence relations (BSRs):
? Lexical: Synonymy, Hypernymy, Entailment,
Meronymy;
? Phrasal: Synonymy, Hypernymy, Entailment,
Meronymy, Nominalization, Corference;
? Syntactic: Scrambling, Case alteration, Modi-
fier, Transparent head, Clause, List, Apposi-
tion, Relative clause;
? Reasoning: Temporal, Spatial, Quantity, Im-
plicit relation, Inference;
Non-basic sentence relations (non-BSRs)?
? Disagreement: Lexical, Phrasal, Modal, Mod-
ifier, Temporal, Spatial, Quantity;
Mainly, we used relations defined in Bentivogli
et al(2010) and divided Synonymy, Hypernymy,
Entailment and Meronymy into Lexical and
Phrasal. The differences between our study and
Bentivogli et al(2010) are as follows. Demonymy
and Statements in Bentivogli et al(2010) were
not considered in our study because they were
not necessary for Japanese data sets. In addi-
tion, Scrambling, Entailment, Disagreement:
temporal, Disagreement: spatial and Disagree-
ment: quantity were newly added in our study.
Scrambling is a rule for changing the order of
phrases and clauses. Entailment is a rule whereby
the latter sentence is true whenever the former is
true (e.g., ?divorce?? ?marry?). Entailment is a
rule different from Synonymy, Hypernymy and
Meronymy.
The rules for decomposition are schematized as
follows:
1http://www.cl.ecei.tohoku.ac.jp/rite2/doku.php
? Break down pairs into BSRs in order to bring
t1 close to t2 gradually, as the interpretation
of the converted sentence becomes wider
? Label each pair of BSRs or non-BSRs
such that each pair is decomposed to ensure
that there are not multiple BSRs
An example is shown below, where the underlined
parts represent the revised points.
t1? ???????? ????? ? ????? ????
Shakespearenom Hamlet com Macbethacc writepast?Shakespeare wrote Hamlet and Macbeth.?[List] ???????? ?????? ????
Shakespearenom Hamletacc writepast?Shakespeare wrote Hamlet.?
t2?[Synonymy] ???????? ?????? ?? ????
?phrasal Shakespearenom Hamletgen authorcomp becop?Shakespeare is the author of Hamlet.?
Table 1: Example of a pair with TE
An example of a pair without TE is shown below.
t1? ?????? ???????? ???
Bulgarianom Eurasia.continentdat becop?Bulgaria is on the Eurasian continent.?
[Entailment] ?????? ???? ????
? phrasal Bulgarianom continental.statecomp becop?Bulgaria is a continental state.?
t2?[Disagreement] ?????? ?? ????
?lexical Bulgarianom island.countrycomp becop?Bulgaria is an island country.?
Table 2: Example of a pair without TE (Part 1)
To facilitate TE assessments like Table 3, non-
BSR labels were used in decomposing pairs. In
addition, we allowed labels to be used several
times when some BSRs in a pair are related to ?N?
assessments.
t1? ?????? ???????? ???
Bulgarianom Eurasia.continentdat becop?Bulgaria is on the Eurasian continent.?
[Disagreement] ?????? ???????? ???
?modal Bulgarianom Eurasia.continentdat becop?neg?Bulgaria is not on the Eurasian continent.?
t2?[Synonymy] ?????? ?????? ?????
?lexical Bulgarianom Europedat belongcop?neg?Bulgaria does not belong to Europe.?
Table 3: Example of a pair without TE (Part 2)
As mentioned above, the idea here is to decom-
pose pairs in order to bring t1 closer to t2, the
latter of which in principle has a wider semantic
scope. We prohibited the conversion of t2 because
it was possible to decompose the pairs such that
they could be true even if there was no TE. Never-
theless, since it is sometimes easier to convert t2,
274
we allowed the conversion of t2 in only the case
that t1 contradicted t2 and the scope of t2 did not
overlap with that of t1 even if t2 was converted and
TE would be unchanged. An example in case that
we allowed to convert t2 is shown below. Bold-
faced types in Table 4 shows that it becomes easy
to compare t1 with t2 by converting to t2.
t1? ??? ?????? ???????
Tomnom today breakfastacc eatpast?neg?Tom didn?t eat breakfast today.?
[Scrambling] ??? ??? ??? ???????
today Tomnom breakfastacc eatpast?neg?Today, Tom didn?t eat breakfast.?
t2? ??? ??? ??? ????
this.morning Tomnom breadacc eatpast?This morning, Tom ate bread and salad.?
[Entailment] ??? ??? ??? ????
?phrasal today Tomnom breakfastacc eatpast?Today, Tom ate breakfast.?
[Disagreement] ?????????????
?modal ?Today, Tom ate breakfast.?
Table 4: Example of conversion of t2
4 Results
4.1 Comparison with Existing Studies
We applied our methodology to 173 pairs from the
RITE-2 BC subtask data set. The pairs were de-
composed by one annotator, and the decomposed
pairs were assigned labels by two annotators. Dur-
ing labeling, we used the labels presented in Sec-
tion 3 and ?unknown? in cases where pairs could
not be labeled. Our methodology was developed
based on 112 pairs, and by using the other 61 pairs,
we evaluated the inter-annotator agreement as well
as the frequencies and times of decomposition.
The agreement for 241 monothematic pairs gen-
erated from 61 pairs amounted to 0.83 and was
computed as follows. The kappa coefficient for
them amounted 0.81.
Agreement = ?Agreed?? labels/Total 2
Bentivogli et al (2010) reported an agreement
rate of 0.78, although they computed the agree-
ment by using the Dice coefficient (Dice, 1945),
and therefore the results are not directly compara-
ble to ours. Nevertheless, the close values suggest
2Because the ?Agreed? pairs were clear to be classi-
fied as ?Agreed?, where ?Total? is the number of pairs la-
beled ?Agreed? subtracted from the number of labeled pairs.
?Agreed? labels is the number of pairs labeled ?Agreed? sub-
tract from the number of pairs with the same label assigned
by the two annotators.
that our methodology is comparable to that in Ben-
tivogli?s study in terms of agreement.
Table 5 shows the distribution of monothematic
pairs with respect to original Y/N pairs.
Or
igin
alp
air
s Monothematic pairs
Y N Total
Y (32) 116 ? 116
N (29) 96 29 125
Total (61) 212 29 241
Table 5: Distribution of monothematic pairs with
respect to original Y/N pairs
When the methodology was applied to 61 pairs,
a total of 241 and an average of 3.95 monothe-
matic pairs were derived. The average was slightly
greater than the 2.98 reported in (Bentivogli et al,
2010). For pairs originally labeled ?Y? and ?N?, an
average of 3.62 and 3.31 monothematic pairs were
derived, respectively. Both average values were
slightly higher than the values of 3.03 and 2.80 re-
ported in (Bentivogli et al, 2010). On the basis of
the small differences between the average values
in our study and those in (Bentivogli et al, 2010),
we are justified in saying that our methodology is
valid.
Table 6 3 shows the distribution of BSRs in t1-
t2 pairs in an existing study and the present study.
We can see from Table 6 thatCorferencewas seen
more frequently in Bentivogli?s study than in our
study, while Entailment and Scrambling were
seen more frequently in our study. This demon-
strates that differences between languages are rele-
vant to the distribution and classification of BSRs.
An average of 5 and 4 original pairs were de-
composed per hour in our study and Bentivogli?s
study, respectively. This indicates that the com-
plexity of our methodology is not much different
from that in Bentivogli et al(2010).
4.2 Evaluation of Accuracy in BSR
In the RITE-2 formal run4, 15 teams used our spe-
cialized data set for the evaluation of their systems.
Table 7 shows the average of F1 scores5 for each
BSR.
Scrambling and Modifier yielded high scores
(close to 90%). The score of List was also
3Because ?lexical? and ?phrasal? are classified together
in Bentivogli et al(2010), they are not shown separately in
Table 6.
4In RITE-2, data generated by our methodology were re-
leased as ?unit test data?.
5The traditional F1 score is the harmonic mean of preci-
sion and recall.
275
BSR Monothematic pairsBentivogli et al Present studyTotal Y N Total Y NSynonymy 25 22 3 45 45 0Hypernymy 5 3 2 5 5 0Entailment - - - 44 44 0Meronymy 7 4 3 1 1 0Nominalization 9 9 0 1 1 0Corference 49 48 1 3 3 0Scrambling - - - 15 15 0Case alteration 7 5 2 7 7 0Modifier 25 15 10 42 42 0Transparent head 6 6 0 1 1 0Clause 5 4 1 14 14 0List 1 1 0 3 3 0Apposition 3 2 1 1 1 0Relative clause 1 1 0 8 8 0Temporal 2 1 1 1 1 0Spatial 1 1 0 1 1 0Quantity 6 0 6 0 0 0Implicit relation 7 7 0 18 18 0Inference 40 26 14 2 2 0Disagreement: lexical/phrasal 3 0 3 27 0 27Disagreement: modal 1 0 1 1 0 1Disagreement: temporal - - - 1 0 1Disagreement: spatial - - - 0 0 0Disagreement: quantity - - - 0 0 0Demonymy 1 1 0 - - -Statements 1 1 0 - - -total 205 157 48 241 212 29
Table 6: Distribution of BSRs in t1-t2 pairs in an
existing study and in the present study using our
methodology
BSR F1(%) Monothematic MissPairsScrambling 89.6 15 4Modifier 88.8 42 0List 88.6 3 0Temporal 85.7 1 1Relative clause 85.4 8 2Clause 85.0 14 2Hypernymy: lexical 85.0 5 1Disagreement: phrasal 80.1 25 0Case alteration 79.9 7 2Synonymy: lexical 79.7 9 6Transparent head 78.6 1 2Implicit relation 75.7 18 2Synonymy: phrasal 73.6 36 9Corference 70.9 3 1Entailment: phrasal 70.2 44 7Disagreement: lexical 69.0 2 0Meronymy: lexical 64.3 1 1Nominalization 64.3 1 0Apposition 50.0 1 1Spatial 50.0 1 1Inference 40.5 2 2Disagreement: modal 35.7 1 0Disagreement: temporal 28.6 1 1Total - 241 41
Table 7: Average F1 scores in BSR and frequen-
cies of misclassifications by annotators
nearly 90%, although the data sets included only
3 instances. These scores were high because
pairs with these BSRs are easily recognized in
terms of syntactic structure. By contrast, Dis-
agreement: temporal, Disagreement: modal,
Inference, Spatial and Apposition yielded low
scores (less than 50%). The scores of Disagree-
ment: lexical, Nominalization and Disagree-
ment: Meronymy were about 50-70%. BSRs
that yielded scores of less than 70% occurred less
than 3 times, and those that yielded scores of not
more than 70% occurred 3 times or more, except
for Temporal and Transparent head. Therefore,
the frequencies of BSRs are related to F1 scores,
and we should consider how to build systems that
recognize infrequent BSRs accurately. In addi-
tion, F1 scores in Synonymy: phrasal and En-
tailment: phrasal are low, although these are la-
beled frequently. This is one possible direction of
future work.
Table 7 also shows the number of pairs in BSR
to which the two annotators assigned different la-
bels. For example, one annotator labeled t2 [Ap-
position] while the other labeled t2 [Spatial] in
the following pair:
Ex. 2) Example of a pair for RTE
? t1: Tokyo, the capital of Japan, is in Asia.
? t2: The capital of Japan is in Asia.
We can see from Table 7 that the F1 scores for
BSRs, which are often assessed as different by dif-
ferent people, are generally low, except for several
labels, such as Synonymy: lexical and Scram-
bling. For this reason, we can conjecture that
cases in which computers experience difficulty de-
termining the correct labels are correlated with
cases in which humans also experience such dif-
ficulty.
5 Conclusions
This paper presented a methodology for generat-
ing Japanese data sets broken down into BSRs
and Non-BSRs, and we conducted experiments in
which we applied our methodology to 61 pairs
extracted from the RITE-2 BC subtask data set.
We compared our method with that of Bentivogli
et al(2010) in terms of agreement as well as
frequencies and times of decomposition, and we
obtained similar results. This demonstrated that
our methodology is as feasible as Bentivogli et
al.(2010) and that differences between languages
emerge only as the different sets of labels and the
different distributions of BSRs. In addition, 241
monothematic pairs were recognized by comput-
ers, and we showed that both the frequencies of
BSRs and the rate of misclassification by humans
are relevant to F1 scores.
Decomposition patterns were not empirically
compared in the present study and will be investi-
gated in future work. We will also develop an RTE
inference system by using our specialized data set.
276
References
Bentivogli, L., Cabrio, E., Dagan, I, Giampiccolo, D.,
Leggio, M. L., Magnini,B. 2010. Building Textual
Entailment Specialized Data Sets: a Methodology
for Isolating Linguistic Phenomena Relevant to In-
ference. In Proceedings of LREC 2010, Valletta,
Malta.
Dagan, I, Glickman, O., Magnini, B. 2005. Recog-
nizing Textual Entailment Challenge. In Proc. of
the First PASCAL Challenges Workshop on RTE.
Southampton, U.K.
Kotani, M., Shibata, T., Nakata, T, Kurohashi, S. 2008.
Building Textual Entailment Japanese Data Sets and
Recognizing Reasoning Relations Based on Syn-
onymy Acquired Automatically. In Proceedings of
the 14th Annual Meeting of the Association for Nat-
ural Language Processing, Tokyo, Japan.
Magnini, B., Cabrio, E. 2009. Combining Special-
izedd Entailment Engines. In Proceedings of LTC
?09. Poznan, Poland.
Dice, L. R. 1945. Measures of the amount of ecologic
association between species. Ecology, 26(3):297-
302.
Mark Sammons, V.G.Vinod Vydiswaran, Dan Roth.
2010. ?Ask not what textual entailment can do for
you...?. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguis-
tics, Uppsala, Sweden, pp. 1199-1208.
277
Proceedings of the EACL 2014 Workshop on Computational Approaches to Causality in Language, pages 33?39,
Gothenburg, Sweden, April 26, 2014.
c
?2014 Association for Computational Linguistics
Building a Japanese Corpus of Temporal-Causal-Discourse Structures
Based on SDRT for Extracting Causal Relations
Kimi Kaneko
1
Daisuke Bekki
1,2,3
1
Ochanomizu University, Tokyo, Japan
2
National Institute of Informatics, Tokyo, Japan
3
CREST, Japan Science and Technology Agency, Saitama, Japan
{kaneko.kimi | bekki}@is.ocha.ac.jp
Abstract
This paper proposes a methodology for
generating specialized Japanese data sets
for the extraction of causal relations, in
which temporal, causal and discourse re-
lations at both the fact level and the epis-
temic level, are annotated. We applied
our methodology to a number of text frag-
ments taken from the Balanced Corpus of
Contemporary Written Japanese. We eval-
uated the feasibility of our methodology in
terms of agreement and frequencies, and
discussed the results of the analysis.
1 Introduction
In recent years, considerable attention has been
paid to deep semantic processing. Many studies
(Betherd et al., 2008), (Inui et al., 2007), (Inui
et al., 2003), (Riaz and Girju, 2013) have been
recently conducted on deep semantic processing,
and causal relation extraction (CRE) is one of the
specific tasks in deep semantic processing. Re-
search on CRE is still developing and there are
many obstacles that must be overcome.
Inui et al. (2003) acquired cause and effect
pairs from text, where the antecedent events were
taken as causes and consequent events were taken
as effects based on Japanese keywords such as
kara and node. In (1), for example, the an-
tecedent ame-ga hutta (?it rained?) and the conse-
quent mizutamari-ga dekita (?puddles emerged?)
are acquired as a pair of cause and effect.
(1) Ame-ga
rain-NOM
hutta-node
fall-past-because
mizutamari-ga
puddles-NOM
dekita.
emerge-past
?Because it rained, puddles emerged.?
However, antecedents are not always causes or
reasons for consequents in Japanese, as illustrated
by the following example.
(2) Zinsinziko-ga
injury.accident-NOM
okita-kara
happen-past-because
densya-ga
trains-NOM
tiensita
delay-past
to-iu-wake-dewanai.
it.is.not.the.case.that
?It is not the case that the trains were
delayed because an injury accident hap-
pened.?
In example (2), the antecedent zinsinziko-ga okita
(?an injury accident happened?) is not the cause
of the consequent densya-ga tiensita (?the trains
were delayed?). Though in such sentences that
contain causal expressions there are no causal re-
lations between antecedents and consequents, in
existing studies each sentence containing a causal
expression was extracted as knowledge represent-
ing cause and effect, such as in (Inui et al., 2003).
It is difficult for computers to auto-recognize and
exclude such cases.
In this paper, we report on the analysis of nec-
essary information for acquiring more accurate
cause-effect knowledge and propose a methodol-
ogy for creating a Japanese corpus for CRE. First,
we introduce previous studies and describe infor-
mation that should be used to annotate data sets.
Next, we describe our methodology based on Seg-
mented Discourse Representation Theory (SDRT)
(Asher et al., 2003). Finally, we evaluate the va-
lidity of our methodology in terms of agreement
and frequency, and analyze the results.
2 Previous Studies
In this section, we introduce previous studies on
annotation of temporal, causal and other types of
relations and present a linguistic analysis of tem-
poral and causal relations.
Betherd et al. (2008) generated English data
sets annotated with temporal and causal relations
and analyzed interactions between the two types of
33
relations. In addition, these specialized data sets
were evaluated in terms of agreement and accu-
racy. Relations were classified into two causal cat-
egories (CAUSAL, NO-REL) and three temporal
categories (BEFORE, AFTER, NO-REL). In re-
gard to the evaluation, Betherd et al. pointed out
that the classification was coarse-grained, and that
reanalysis would have to be performed with more
fine-grained relations.
Inui et al. (2005) characterized causal expres-
sions in Japanese text and built Japanese corpus
with tagged causal relations. However, usages
such as that illustrated in (2) and interactions be-
tween temporal relations and causal relations were
not analyzed.
Tamura (2012) linguistically analyzed temporal
and causal relations and pointed out that in rea-
son/purpose constructions in Japanese, the event
time indicated by the tense sometimes contradicts
the actual event time, and that the information nec-
essary to recognize the order between events lies
in the choice of the fact and the epistemic levels
(we will come back to these notions in the sec-
tion 3.4), and the explicit or implicit meaning of
a sentence in the causal expressions in Japanese.
Furthermore, some causal expressions in Japanese
are free from the absolute and relative tense sys-
tems, and both the past and non-past forms can be
freely used in main and subordinate clauses (Chin,
1984) (an example is given in the next section). In
other words, temporal relations are not always re-
solved earlier than causal relations, and therefore
we should resolve temporal relations and causal
relations simultaneously.
Asher et al. (2003) proposed SDRT in order to
account for cases where discourse relations affect
the truth condition of sentences. Because tempo-
ral relations constrain causal relations, the explicit
or implicit meaning of a sentences and the epis-
temic level information affects preceding and fol-
lowing temporal relations in causal expressions in
Japanese, recognition also affects causal relations.
Therefore, the annotation of both causal relations
and discourse relations in corpora is expected to be
useful for CRE. Moreover, which characteristics
(such as tense, actual event time, time when the
event is recognized, meaning and structure of the
sentence and causal relations) will serve as input
and which of them will serve as output depends on
the time and place. Therefore, we should also take
into account discourse relations together with tem-
poral and causal relations. We can create special-
ized data sets for evaluating these types of infor-
mation together by annotating text with discourse,
temporal and causal relations.
However, discourse relations of SDRT are not
distributed into discourse relations and temporal
relations, and as a result the classification of labels
becomes unnecessarily complex. Therefore, it is
necessary to rearrange discourse relations as in the
following example.
(3) Inu-wa
dog-NOM
niwa-o
garden-ACC
kakemawatta.
run-past
Neko-wa
cat-NOM
kotatu-de
kotatsu.heater-LOC
marukunatte-ita.
be.curled.up-past
?The dog ran in the garden. The cat was
curled up in the kotatsu heater.?
This pair of sentences is an antithesis, so we an-
notate it with the ?Contrast? label in SDRT. On the
other hand, the situation described in the first sen-
tence overlaps with that of the second sentence, so
we annotate this pair of sentences with the ?Back-
ground? label as well. Though there are many
cases in which we can annotate a sentence with
discourse relations in this way, dividing temporal
relations from discourse relations as in this study
allows us to avoid overlapping discourse relations.
This study was performed with the aim to rear-
range SDRT according to discourse relations, tem-
poral relations and causal relations separately, and
we generated specialized data sets according to
our methodology. In addition, occasionally it is
necessary to handle the actual event time and the
time when the event was recognized individually.
An example is given below.
(4) Asu
tomorrow
tesuto-ga
exam-NOM
aru-node,
take.place-nonpast-because,
kyoo-wa
today-TOP
benkyoo-suru-koto-ni
to.study-DAT
sita.
decide-past
?Because there will be an exam tomorrow,
I decided to study today.
Before we evaluate the consequent kyoo-wa
benkyoo-suru-koto-ni sita (?I decided to study to-
day?), we should recognize the fact of the an-
tecedent Asu tesuto-ga aru (?there will be an exam
tomorrow?). Whether we deal with the actual
34
Label Description
Precedence(A,B) End time (A) < start time (B)
In other words, event A temporally precedes event B.
Overlap(A,B) Start time (A) < end time (B) ? end time (B) < end time (A)?
In other words, event A temporally overlaps with event B.
Subsumption(A,B) Start time (A) ? end time (B) & End time (A) ? end time (B)?
In other words, event A temporally subsumes event B.
Table 1: Temporal relations list
Level Description
Cause(A,B) The event in A and the event in B are in a causal relation.
Table 2: Causal relation
event time or the time when the event was recog-
nized depends on the circumstances. Therefore,
we decided to annotate text at the fact and epis-
temic levels in parallel to account for such a dis-
tinction.
3 Methodology
We extended and refined SDRT and developed our
own methodology for annotating main and subor-
dinate clauses, phrases located between main and
subordinate clauses (e.g., continuative conjuncts
in Japanese), two consecutive sentences and two
adjoining nodes with a discourse relation. We also
defined our own method for annotating proposi-
tions with causal and temporal relations. The re-
sult of tagging example (5a) is shown in (5b).
(5) a. Kaze-ga
wind-NOM
huita.
blow-past
Harigami-ga
poster-NOM
hagare,
come.off-past
tonda.
flow-past
?The wind blew. A poster came off and
flew away.?
b. [Precedence(pi1,pi3),Explanation(pi1,pi3),
Cause(pi1,pi3)],
[Precedence(pi2,pi4), Explanation(pi2,pi4),
Cause(pi2,pi4)]
pi2pi1
Kaze-ga huita.
pi4pi3
Harigami-ga hagare, tonda.
The remainder of this section is structured as fol-
lows. Sections 3.1 and 3.2 deal with temporal and
causal relations, respectively. Section 3.3 covers
discourse relations, and Section 3.4 describes the
fact level and the epistemic level.
3.1 Temporal Relations
We consider the following three temporal relations
(Table 1). We assume that they represent the rela-
tions between two events in propositions and indi-
cate a start time and an end time. In addition, we
also assume that (start time of e)? (end time of e)
for all events. Based on this, the temporal place-
ment of each two events is limited to the three re-
lations in Table 1.
In this regard, Japanese non-past predicates
occasionally express habitually repeating events,
which have to be distinguished from events occur-
ring later than the reference point. In this paper, in
annotating the scope of the repetition, habitually
repeating events are described as in the following
example.
(6) a. Taiin-go,
After.retirement
{kouen-o
park-ACC
hasiru}
repeat
to.run
yoo-ni-site-iru.
have.a.custom
?After retiring, I have a custom to {run
in the park}
repeat
.?
b. {supootu-inryo-o
Sports.drink-ACC
nonda-ato,
drink-past-after
kouen-o
park-ACC
hasiru}
repeat
run
yoo-ni-site-iru.
have.a.custom
?I have a custom that {I run in the park
after having a sports drink}
repeat
.?
3.2 Causal Relations
We tag pairs of clauses with the following relation
(Table 2) only if there is a causal relation between
events in the proposition. By annotating text with
discourse relations, a fact and epistemic level and
temporal relations, we can describe the presence
35
Label Description
Alternation(A,B) ?A or B?, where the pair of A and B corresponds to logical disjunction (?).
Consequence(A,B) ?If A then B?, where the pair of A and B corresponds to logical implication (?).
Elaboration(A,B) B explains A in detail in the discourse relation.
B of the event is part of A of the event.
Narration(A,B) A and B are in the same situation, and
the pair of A and B corresponds to logical conjunction (?).
Explanation(A,B) The discourse relation indicates A as a cause and B as an effect.
Contrast(A,B) ?A but B?, where A and B are paradoxical.
Commentary(A,B) The content of A is summarized or complemented in B.
Table 3: Discourse relations list
SDRT Our methodology Rules
Alternation(A,B) Alternation(A,B) NA
Consequence(A,B) Consequence(A,B) NA
Elaboration(A,B) Elaboration(A,B) ? A,B (Elaboration(A,B)? Subsumption (A,B))
Narration(A,B) Precedence(A,B) ? Narration(A,B) NA
Background(A,B) Subsumption(A,B) ? Narration(A,B) NA
Result(A,B) Explanation(A,B)
Explanation(A,B) Cause(A,B) ? A,B (Cause(A,B)? Temp rel(A,B)) 1
Contrast(A,B) Contrast(A,B) NA
Commentary(A,B) Commentary(A,B) NA
Table 4: Correspondence between SDRT and our methodology
of causation in finer detail than (Betherd et al.,
2008).
3.3 Discourse Relations
We consider the following discourse relations
based on SDRT (Table 3). There are also relations
that impose limitations on temporal and causal re-
lations (Table 4). The way temporal, causal and
discourse relations affect each other is described
below together with their correspondence to the
relations in SDRT. Bold-faced entries represent
relations integrated in SDRT in our study.
Such limitations on temporal relations provides in-
formation for making a decision in terms of tem-
poral order and cause/effect in the ?de-tensed?
sentence structure
2
(Chin, 1984) in Japanese. An
example is given below.
(7) Kinoo
yesterday
anna-ni
that.much
taberu-kara,
eat-past-because
kyoo
today
onaka-ga
stomach-NOM
itaku
ache-cont
natta-nda.
become-noda
2
Temp rel(A,B) ?
Precedence(A,B)? Overlap(A,B)? Subsumption(A,B)
3
According to (Chin, 1984), ?de-tensed? is a relation
whereby the phrase has lost the meaning contributed by tense,
namely, the logical aspect of the semantic relation between an
antecedent and a consequent has eliminated the aspect tem-
poral relation between them.
?Because you ate that much yesterday, you
have a stomachache today.?
(7) [Precedence(pi1,pi3),Explanation(pi1,pi3),
Cause(pi1,pi3)],
[Precedence(pi2,pi4),Explanation(pi2,pi4),
Cause(pi2,pi4)]
pi2pi1
Kinoo anna-ni taberu-kara,
pi4pi3
kyoo onaka-ga itaku natta-nda.
This is a sentence where the subordinate clause is
in non-past tense and the main clause is in past
tense. Then, we may mistakenly interpret the
event in the subordinate clause as occurring after
the event of the main clause. However, we can de-
termine that in fact it occurred before the event in
the main clause based on the rule imposed by the
?Cause? relation.
3.4 Fact Level and Epistemic Level
A fact level proposition refers to an event and
its states, while an epistemic level proposition
refers to speaker?s recognizing event of a described
event. In Japanese, the latter form is often marked
by the suffix noda that attaches to all kinds of
predicates (which may also be omitted). Both
overt and covert noda introduce embedded struc-
tures, and we annotate them in such a way that a
fact level proposition is embedded in an epistemic
level proposition.
Semantically, the most notable difference be-
tween the two levels is that the tense in the former
36
represents the time that an event takes place, while
the tense in the latter represents the time that the
speaker recognizes the event.
This distinction between the two types of propo-
sitions is carried over to the distinction between
the fact level and the epistemic level causal rela-
tions. We annotate the former by the tag ?Cause?
and the latter by the tag ?Explanation?.
In Japanese, a causal marker such as node (a
continuation form of noda) and kara are both used
in the fact level and the epistemic level. The fact
level causality is a causal relation between the
two events, while the epistemic level causality is a
causal relation between the two recognizing events
of the two events mentioned. Therefore, in the
causal construction, it happens that the precedence
relations between the subordinate and the matrix
clauses in the fact level and the epistemic level do
not coincide, as in the following example.
(8) Kesa
this.morning
nani-mo
nothing-NOM
hoodoo-sare-nakatta-node,
report-passive-NEG.past-because,
kinoo-wa
yesterday-TOP
mebosii
notable
ziken-wa
events-NOM
nakatta-noda.
be-NEG-noda
?Because nothing was reported this morn-
ing, there were no notable event yester-
day.?
[Precedence(pi3,pi1),Explanation(pi3,pi1),
Cause(pi3,pi1)],
[Precedence(pi2,pi4), Explanation(pi2,pi4),
Cause(pi2,pi4)]
pi2pi1
Kesa nani-mo hoodoo-sare-nakatta-
node,
pi4pi3
kinoo-wa mebosii
ziken-wa nakatta-noda.
The temporal relation at the fact level is that pi3
precedes pi1. By contrast, that at the epistemic
level is that pi2 precedes pi4. By describing the
relation between pi1 and pi3 and that between pi2
and pi4 separately, we can reproduce the relation-
ship at both levels.
3.5 Merits
We defined our methodology for annotating text
fragments at both the fact and epistemic levels in
parallel with temporal, causal and discourse re-
lations. Therefore, we can generate specialized
data sets that enable estimating the causality in the
fact and epistemic levels by various cues (such as
known causal relations, truth condition, conjunc-
tions and temporal relations between sentences or
clauses).
In addition, we can say that causal expressions
without causation are not in a causal relation (and
vice versa) by annotating text with both discourse
and causal relations.
4 Results
We applied our methodology to 66 sentences from
the Balanced Corpus of Contemporary Written
Japanese (BCCWJ) (Maekawa, 2008). The sen-
tences were decomposed by one annotator, and la-
bels were assigned to the decomposed segments
by two annotators. During labeling, we used the
labels presented in Section 3. Our methodology
was developed based on 96 segments (38 sen-
tences), and by using the other 100 segments (28
sentences), we evaluated the inter-annotator agree-
ment as well as the frequencies of decomposition
and times of annotation. The agreement for 196
segments generated from 28 sentences amounted
to 0.68 and was computed as follows (the kappa
coefficient for them amounted to 0.79).
Agreement = Agreed labels/Total labels
Analyzing more segments in actual text and im-
proving our methodology can lead to further im-
provement in terms of agreement.
Table 5 shows the distribution of labels into seg-
ments in our study.
label segments
Total fact epistemic
Precedence 25 14 11
Overlap 7 4 3
Subsumption 61 29 32
total 94 47 47
Cause 14 8 6
total 14 8 6
Alternation ? ? ?
Consequence 6 3 3
Elaboration 4 2 2
Narration 66 33 33
Explanation 14 7 7
Contrast 2 1 1
Commentary 94 47 47
Table 5: Distribution of labels in segments in our
study
37
We can see from Table 5 that ?Narration? was
the most frequent one, while ?Alternation? never
appeared. As s result, we can assume that frequent
relations will be separated from non-frequent rela-
tions. So far, all the relations are either frequent or
non-frequent. We should re-analyze the data with
more samples again.
When the methodology was applied to 28 sen-
tences, a total of 100 and an average of 3.57 seg-
ments were derived. This is the number of seg-
ments at both the fact and epistemic levels. With-
out dividing the fact and epistemic levels, an aver-
age of 1.79 segments were derived.
On average, 11 segments per hour were tagged
in our study. Although we should evaluate the va-
lidity after having computed the average decom-
position times, it is assumed that our methodology
is valid when focusing only on labeling.
5 Discussion
We analyzed errors in this annotation exercise.
The annotators often found difficulties in judging
temporal relations in the following two cases: (1)
the case where it was difficult to determine the
scope of the segments pairing and (2) the case
where formalization of lexical meaning is difficult.
In regard to the first case, how to divide seg-
ments sometimes affects temporal relations. In the
following example, consider the temporal relation
between the first and the second sentences.
(9) Marason-ni
marathon-DAT
syutuzyoo-sita.
participate-past.
sonohi-wa
that.day-TOP
6zi-ni
6:00-at
kisyoo-si,
get.up-past,
10zi-ni
10:00-at
totyoo-kara
Metropolitan.Government-from
syuppatu-site,
leave-past,
12zi-ni
12:00-at
kansoo-sita.
finish.running-past.
?I participated in marathon. I got up at
6:00 on that day and left the Metropolitan
Government at 10:00 and finished running
at 12:00.?
When we focus on the first segment of the sec-
ond sentnce (?I got up at 6:00?), its relation to the
first sentence appears to be ?Precedence?. How-
ever, if we consider the second and the third seg-
ments as the same segment, their relation to the
first sentence appears to be ?Subsumption?.
Therefore, we should establish clear criteria for
the segmentation. Although we currently adopts a
criterion that we chose smaller segment in unclear
cases, there still remain 9 unclear cases (tempo-
ral:5, discourse:4).
One of the reason why Kappa coefficient marks
relatively high score is that we only compare the
labels and ignore the difference in the segmenta-
tions. Criteria for deciding the segment scope in
paring segments will improve our methodology.
The second case is exemplified by the tempo-
ral relation between the subordinate clause and the
main clause in the following sentence.
(10) Migawari-no
scapegoat-GEN
tomo-o
friend-ACC
sukuu-tame-ni
to.save
hasiru-noda.
run-noda.
?I run to save my friend who is my scape-
goat.?
If we consider that the saving event only spans
over the very moment of saving, the relation be-
tween the clauses appears to be ?Precedence?.
However, if we consider that running event is a
part of the saving event, the relation between the
clauses is ?Subsumption?.
Thus, judging lexical meaning with respect to
when events start and end involves some difficul-
ties and they yield delicate cases in judging tem-
poral relations.
These problems are mutually related, and the
first problem arises when the components of a lex-
ical meaning are displayed explicitly in the sen-
tence, and the second problem arises when they
are implicit.
6 Conclusions
We analyzed and proposed our methodology based
on SDRT for building a more precise Japanese
corpus for CRE. In addition, we annotated 196
segments (66 sentences) in BCCWJ with tempo-
ral relations, discourse relations, causal relations
and fact level and epistemic level propositions and
evaluated the annotations of 100 segments (28 sen-
tences) in terms of agreement, frequencies and
times for decompositions. We reported and an-
alyzed the result and discussed problems of our
methodology.
The discrepancies of decomposition patterns
were not yet empirically compared in the present
study and will be investigated in future work.
38
References
Asher N. and Lascaridas A. 2003. Logics of Con-
versation: Studies in Natural Language Processing.
Cambridge University Press, Cambridge, UK.
Bethard S., Corvey W. and Kilingenstein S. 2008.
Building a Corpus of Temporal Causal Structure.
LREC 2008, Marrakech, Morocco.
Chin M. 1984. Tense of the predicates for clauses
of compound statement binded by conjunctive parti-
cle -?Suru-Ga? and ?Shita-Ga?, ?Suru-Node? and
?Shita-Node? etc.-. Language Teaching Research
Article.
Inui T., Inui K. and Matsumoto Y. 2005. Acquir-
ing Causal Knowledge from Text Using the Con-
nective Marker Tame. ACM Transactions on Asian
Language Information Processing (ACM-TALIP),
Vol.4, Issue 4, Special Issue on Recent Advances
in Information Processing and Access for Japanese,
435?474.
Inui T., Inui K. and Matsumoto Y. 2003. What Kinds
and Amounts of Causal Knowledge Can Be Aquired
from Text by Using Connective Markers as Clues.
The 6th International Conference on Discovery Sci-
ence (DS-2003), 180?193.
Inui T., Takamura H. and Okumura M. 2007. Latent
Variable Models for Causal Knowledge Acquisition.
Alexander Gelbukh(Ed.), Computational Linguistics
and Intelligent Text Processing, Lecture Notes in
Computer Science, 4393:85?96.
Maekawa K. 2008. Balanced Corpus of Contemporary
Written Japanese. In Proceedings of the 6th Work-
shop on Asian Language Resources (ALR), 101?
102.
Riaz M. and Girju R. 2013. Toward a Bet-
ter Understanding of Causality between Verbal
Events:Extraction and Analysis of the Causal Power
of Verb-Verb Associations. In Proceedings of the
SIGDIAL 2013 Conference, Metz, France 21?30.
Tamura S. 2012. Causal relations and epistemic per-
spectives: Studies on Japanese causal and purposive
constructions. Doctoral thesis, Kyoto University.
39
