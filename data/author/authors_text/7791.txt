Proceedings of the Linguistic Annotation Workshop, pages 77?84,
Prague, June 2007. c?2007 Association for Computational Linguistics
Adding semantic role annotation to a corpus of written Dutch
Paola Monachesi, Gerwert Stevens and Jantine Trapman
Utrecht University, Uil-OTS, Trans 10, 3512 JK Utrecht, The Netherlands
{Paola.Monachesi, Gerwert.Stevens, Jantine.Trapman}@let.uu.nl
Abstract
We present an approach to automatic se-
mantic role labeling (SRL) carried out in
the context of the Dutch Language Corpus
Initiative (D-Coi) project. Adapting ear-
lier research which has mainly focused on
English to the Dutch situation poses an in-
teresting challenge especially because there
is no semantically annotated Dutch corpus
available that can be used as training data.
Our automatic SRL approach consists of
three steps: bootstrapping from a syntacti-
cally annotated corpus by means of a rule-
based tagger developed for this purpose,
manual correction on the basis of the Prop-
Bank guidelines which have been adapted to
Dutch and training a machine learning sys-
tem on the manually corrected data.
1 Introduction
The creation of semantically annotated corpora has
lagged dramatically behind. As a result, the need for
such resources has now become urgent. Several ini-
tiatives have been launched at the international level
in the last years, however, they have focused almost
entirely on English and not much attention has been
dedicated to the creation of semantically annotated
Dutch corpora.
Within the Dutch Language Corpus Initiative (D-
Coi)1, a recently completed Dutch project, guide-
lines have been developed for the annotation of a
Dutch written corpus. In particular, a pilot corpus
1http://lands.let.ru.nl/projects/d-coi/
has been compiled, parts of which have been en-
riched with (verified) linguistic annotations.
One of the innovative aspects of the D-Coi project
with respect to previous initiatives, such as the Spo-
ken Dutch Corpus (CGN - Corpus Gesproken Ned-
erlands) (Oostdijk, 2002), was the development of a
protocol for a semantic annotation layer. In particu-
lar, two types of semantic annotation have been ad-
dressed, that is semantic role assignment and tempo-
ral and spatial semantics (Schuurman and Monach-
esi, 2006). The reason for this choice lies in the fact
that semantic role assignment (i.e. the semantic rela-
tionships identified between items in the text such as
the agents or patients of particular actions), is one of
the most attested and feasible types of semantic an-
notation within corpora. On the other hand, tempo-
ral and spatial annotation was chosen because there
is a clear need for such a layer of annotation in ap-
plications like information retrieval or question an-
swering.
The focus of this paper is on semantic role an-
notation. We analyze the choices we have made
in selecting an appropriate annotation protocol by
taking into consideration existing initiatives such
as FrameNet (Johnson et al, 2002) and PropBank
(Kingsbury et al, 2002) (cf. also the Chinese and
Arabic PropBank). We motivate our choice for the
PropBank annotation scheme on the basis of the
promising results with respect to automatic seman-
tic role labeling (SRL) which have been obtained for
English. Furthermore, we discuss how the SRL re-
search could be adapted to the Dutch situation given
that no semantically annotated corpus was available
that could be used as training data.
77
2 Existing projects
During the last few years, corpora enriched with se-
mantic role information have received much atten-
tion, since they offer rich data both for empirical in-
vestigations in lexical semantics and large-scale lex-
ical acquisition for NLP and Semantic Web applica-
tions. Several initiatives are emerging at the inter-
national level to develop annotation systems of ar-
gument structure. Within our project we have tried
to exploit existing results as much as possible and
to set the basis for a common standard. We want
to profit from earlier experiences and contribute to
existing work by making it more complete with our
own (language specific) contribution given that most
resources have been developed for English.
The PropBank and FrameNet projects have been
evaluated in order to assess whether the approach
and the methodology they have developed for the
annotation of semantic roles could be adopted for
our purposes. Given the results they have achieved,
we have taken their insights and experiences as our
starting point.
FrameNet reaches a level of granularity in the
specification of the semantic roles which might
be desirable for certain applications (i.e. Ques-
tion Answering). Moreover, the predicates are
linked to an underlying frame ontology that clas-
sifies the verbs within a semantic hierarchy. On
the other hand, despite the relevant work of
Gildea and Jurafsky (2002), it is still an open
issue whether FrameNet classes and frame ele-
ments can be obtained and used automatically be-
cause of the richness of the semantic structures em-
ployed (Dzikovska et al, 2004). Furthermore, the
FrameNet approach might raise problems with re-
spect to uniformity of role labeling even if human
annotators are involved. Incompleteness, however,
constitutes the biggest problem, i.e. several frames
and relations among frames are missing mainly be-
cause FrameNet is still under development. Adopt-
ing the FrameNet lexicon for semantic annotation
means contributing to its development with the ad-
dition of (language specific) and missing frames.
In our study, we have assumed that the FrameNet
classification even though it is based on English
could be applicable to Dutch as well. This as-
sumption is supported by the fact that the German
project Saarbru?cken Lexical Semantics Annotation
and analysis (SALSA) (K. Erk and Pinkal, 2003)
has adopted FrameNet with good results. Although
Dutch and English are quite similar, there are differ-
ences on both sides. For example, in the case of the
Spanish FrameNet it turned out that frames may dif-
fer in their number of elements across languages (cf.
(Subirats and Sato, 2004)).
The other alternative was to employ the Prop-
Bank approach which has the advantage of provid-
ing clear role labels and thus a transparent anno-
tation for both annotators and users. Furthermore,
there are promising results with respect to automatic
semantic role labeling for English thus the annota-
tion process could be at least semi-automatic. A dis-
advantage of this approach is that we would have to
give up the classification of frames in an ontology,
as is the case in FrameNet, which could be very use-
ful for certain applications, especially those related
to the Semantic Web. However, in (Monachesi and
Trapman, 2006) suggestions are given on how the
two approaches could be reconciled.
The prospect of semi-automatic annotation was
the decisive factor in the decision to adopt the Prop-
Bank approach for the annotation of semantic roles
within the D-Coi project.
3 Automatic SRL: bootstrapping a corpus
with semantic roles
Ever since the pioneering article of Gildea and Ju-
rafsky (2002), there has been an increasing inter-
est in automatic semantic role labeling (SRL). How-
ever, previous research has focused mainly on En-
glish. Adapting earlier research to the Dutch situ-
ation poses an interesting challenge especially be-
cause there is no semantically annotated Dutch cor-
pus available that can be used as training data. Fur-
thermore, no PropBank frame files for Dutch exist.
To solve the problem of the unavailability of train-
ing data, we have developed a rule-based tagger to
bootstrap a syntactically annotated corpus with se-
mantic roles. After manual correction, this corpus
was used as training data for a machine learning
SRL system. The input data for our SRL approach
consists of Dutch sentences, syntactically annotated
by the Dutch dependency parser Alpino (Bouma et
al., 2000).
78
Syntactic annotation of our corpus is based on the
Spoken Dutch Corpus (CGN) dependency graphs
(Moortgat et al, 2000). A CGN dependency graph
is a tree-structured directed acyclic graph in which
nodes and edges are labeled with respectively c-
labels (category-labels) and d-labels (dependency
labels). C-labels of nodes denote phrasal categories,
such as NP (noun phrase) and PP, c-labels of leafs
denote POS tags. D-Labels describe the grammati-
cal (dependency) relation between the node and its
head. Examples of such relations are SU (subject),
OBJ (direct object) and MOD (modifier).
Intuitively, dependency structures are a great re-
source for a rule-based semantic tagger, for they di-
rectly encode the argument structure of lexical units,
e.g. the relation between constituents. Our goal was
to make optimal use of this information in an au-
tomatic SRL system. In order to achieve this, we
first defined a basic mapping between nodes in a
dependency graph and PropBank roles. This map-
ping forms the basis of our rule-based SRL system
(Stevens, 2006).
Mapping subject and object complements to
PropBank arguments is straightforward: subjects are
mapped to ARG0 (proto-typical agent), direct ob-
jects to ARG1 (proto-typical patient) and indirect ob-
jects to ARG2. An exception is made for ergatives
and passives, for which the subject is labeled with
ARG1.
Devising a consistent mapping for higher num-
bered arguments is more difficult, since their label-
ing depends in general on the frame entry of the
corresponding predicate. Since we could not use
frame information, we used a heuristic method. This
heuristic strategy entails that after numbering sub-
ject/object complements with the rules stated above,
other complements are labeled in a left-to-right or-
der, starting with the first available argument num-
ber. For example, if the subject is labeled with
ARG0 and there are no object complements, the first
available argument number is ARG1.
Finally, a mapping for several types of modifiers
was defined. We refrained from the disambiguation
task, and concentrated on those modifiers that can be
mapped consistently. These modifiers are:
? ArgM-NEG - Negation markers.
? ArgM-REC - Reflexives and reciprocals.
? ArgM-PRD - Markers of secondary predi-
cation: modifiers with the dependency label
PREDM
? ArgM-PNC - Purpose clauses: modifiers that
start with om te . These modifiers are marked
by Alpino with the c-label OTI.
? ArgM-LOC - Locative modifiers: modifiers
with the dependency label LD, the LD label is
used by Alpino to mark modifiers that indicate
a location of direction.
4 XARA: a rule based SRL tagger
With the help of the mappings discussed above, we
developed a rule-based semantic role tagger, which
is able to bootstrap an unannotated corpus with se-
mantic roles. We used this rule-based tagger to re-
duce the manual annotation effort. After all, starting
manual annotation from scratch is time consuming
and therefore expensive. A possible solution is to
start from a (partially) automatically annotated cor-
pus.
The system we developed for this purpose is
called XARA (XML-based Automatic Role-labeler
for Alpino-trees) (Stevens, 2006). 2 XARA is
written in Java, the cornerstone of its rule-based
approach is formed by XPath expressions; XPath
(Clark and DeRose, 1999) is a powerful query lan-
guage for the XML format.
The corpus format we used in our experiments is
the Alpino XML format. This format is designed to
support a range of linguistic queries on the depen-
dency trees in XPath directly (Bouma and Kloost-
erman, 2002). The structure of Alpino XML doc-
uments directly corresponds to the structure of the
dependency tree: dependency nodes are represented
by NODE elements, attributes of the node elements
are the properties of the corresponding dependency
node, e.g. c-label, d-label, pos-tag, lemma, etc.
A rule in XARA consist of an XPath expression
that addresses a node in the dependency tree, and a
target label for that node, i.e. a rule is a (path,label)
pair. For example, a rule that selects direct object
nodes and labels them with ARG1 can be formulated
as:
(//node[@rel=?obj1?], 1)
2The system is available at:
http://www.let.uu.nl/?Paola.Monachesi/personal/dcoi/index.html
79
In this example, a numeric label is used to label a
numbered argument. For ARGMs, string value can
be used as target label.
After their definition, rules can be applied to local
dependency domains, i.e. subtrees of a dependency
tree. The local dependency domain to which a rule
is applied, is called the rule?s context. A context is
defined by an XPath expression that selects a group
of nodes. Contexts for which we defined rules in
XARA are verbal domains, that is, local dependency
structures with a verb as head.
Table 1 shows the performance of XARA on our
treebank.
Table 1: Results of SRL with XARA
Label Precision Recall F?=1
Overall 65,11% 45,83% 53,80
Arg0 98.97% 94.95% 96.92
Arg1 70.08% 64.83% 67.35
Arg2 47.41% 36.07% 40.97
Arg3 13.89% 6.85% 9.17
Arg4 1.56% 1.35% 1.45
ArgM-LOC 83.49% 13.75% 23.61
ArgM-NEG 72.79% 58.79% 65.05
ArgM-PNC 91.94% 39.31% 55.07
ArgM-PRD 63.64% 26.25% 37.17
ArgM-REC 85.19% 69.70% 76.67
Notice XARA?s performance on highered num-
bered arguments, especially ARG4. Manual inspec-
tion of the manual labeling reveals that ARG4 argu-
ments often occur in propositions without ARG2 and
ARG3 arguments. Since our current heuristic label-
ing method always chooses the first available argu-
ment number, this method will have to be modified
in order achieve a better score for ARG4 arguments.
5 Manual correction
The annotation by XARA of our tree bank, was
manually corrected by one human annotator, how-
ever, in order to deal with a Dutch corpus, the Prop-
Bank annotation guidelines needed to be revised.
Notice that both PropBank and D-Coi share the
assumption that consistent argument labels should
be provided across different realizations of the same
verb and that modifiers of the verb should be as-
signed functional tags. However, they adopt a dif-
ferent approach with respect to the treatment of
traces since PropBank creates co-reference chains
for empty categories while within D-coi, empty cat-
egories are almost non existent and in those few
cases in which they are attested, a coindexation has
been established already at the syntactic level. Fur-
thermore, D-coi assumes dependency structures for
the syntactic representation of its sentences while
PropBank employs phrase structure trees. In addi-
tion, Dutch behaves differently from English with
respect to certain constructions and these differences
should be spelled out.
In order to annotate our corpus, the PropBank
guidelines needed a revision because they have been
developed for English and to add a semantic layer
to the Penn TreeBank. Besides the adaption (and
extension) of the guidelines to Dutch (Trapman and
Monachesi, 2006), we also have to consider a Dutch
version of the PropBank frameindex. In PropBank,
frame files provide a verb specific description of all
possible semantic roles and illustrate these roles by
examples. The lack of example sentences makes
consistent annotation difficult. Since defining a set
of frame files from scratch is very time consuming,
we decided to attempt an alternative approach, in
which we annotated Dutch verbs with the same ar-
gument structure as their English counterparts, thus
use English frame files instead of creating Dutch
ones. Although this causes some problems, for ex-
ample, not all Dutch verbs can be translated to a
100% equivalent English counterpart, such prob-
lems proved to be relatively rare. In most cases ap-
plying the PropBank argument structure to Dutch
verbs was straightforward. If translation was not
possible, an ad hoc decision was made on how to
label the verb.
In order to verify the correctness of the annota-
tion carried out automatically by XARA, we have
proceeded in the following way:
1. localize the verb and translate it to English;
only the argument structure of verbs is consid-
ered in our annotation while that of NPs, PPs
and other constituents has been neglected for
the moment.
2. check the verb?s frames file in Prop-
Bank; the appropriate roles for each
80
verb could be identified in PropBank
(http://verbs.colorado.edu/framesets/).
3. localize the arguments of the verb; arguments
are usually NPs, PPs and sentential comple-
ments.
4. localize the modifiers; in addition to the argu-
ments of a verb, modifiers of place, time, man-
ner etc. are marked as well.
An appropriate tool has been selected to carry out
the manual correction. We have made an investiga-
tion to evaluate three different tools for this purpose:
CLaRK3, Salto4 and TrEd5. On the basis of our main
requirements, that is whether the tool is able to han-
dle the xml-structure we have adopted and whether
it provides a user-friendly graphical interface and we
have come to the conclusion that the TrEd tool was
the most appropriate for our needs.
During the manual correction process, some prob-
lems have emerged, as for example the fact that
we have encountered some phenomena, such as the
interpretation of modifiers, for which linguistic re-
search doesn?t provide a standard solution yet, we
have discarded these cases for the moment but it
would be desirable to address them in the future.
Furthermore, the interaction among levels should
be taken more into consideration. Even though the
Alpino parser has an accuracy on our corpus of
81%?90% (van Noord, 2006) and the syntactic cor-
pus which has been employed for the annotation of
the semantic roles had been manually corrected, we
have encountered examples in which the annotation
provided by the syntactic parser was not appropri-
ate. This is the case of a PP which was labeled as
modifier by the syntactic parser but which should
be labeled as argument according to the PropBank
guidelines. There should thus be an agreement in
these cases so that the syntactic structure can be cor-
rected. Furthermore, we have encountered problems
with respect to PP attachment, that is the syntactic
representation gives us correct and incorrect struc-
tures and at the semantic level we are able to disam-
biguate. More research is necessary about how to
deal with the incorrect representations.
3http://www.bultreebank.org/clark/index.html
4http://www.coli.uni-saarland.de/projects/salsa/
5http://ufal.mff.cuni.cz/ pajas/tred/
6 The TiMBL classification system
The manually corrected sentences have been used
as training and test data for an SRL classification
system. For this learning system we have em-
ployed a Memory Based Learning (MBL) approach,
implemented in the Tilburg Memory based learner
(TiMBL) (Daelemans et al, 2004).
TiMBL assigns class labels to training instances
on the basis of features and the feature set plays
an important role in the performance of a classi-
fier. In choosing the feature set for our system, we
mainly looked at previous research, especially sys-
tems that participated in the 2004 and 2005 CoNLL
shared tasks on semantic role labeling (Carreras and
Ma`rquez, 2005).
It is worth noting that none of the systems in the
CoNLL shared tasks used features extracted from
dependency structures. However, we encountered
one system (Hacioglu, 2004) that did not participate
in the CoNLL-shared task but did use the same data
and was based on dependency structures. The main
difference with our system is that Hacioglu did not
use a dependency parser to create the dependency
trees, instead existing constituent trees were con-
verted to dependency structures. Furthermore, the
system was trained and tested on English sentences.
From features used in previous systems and some
experimentation with TiMBL, we derived the fol-
lowing feature set. The first group of features de-
scribes the predicate (verb):
(1) Predicate stem - The verb stem, provided by
Alpino.
(2) Predicate voice - A binary feature indicating
the voice of the predicate (passive/active).
The second group of features describes the candi-
date argument:
(3) Argument c-label - The category label (phrasal
tag) of the node, e.g. NP or PP.
(4) Argument d-label - The dependency label of
the node, e.g. MOD or SU.
(5) Argument POS-tag - POS tag of the node if the
node is a leaf node, null otherwise.
(6) Argument head-word - The head word of the
relation if the node is an internal node or the
lexical item (word) if it is a leaf.
81
(7) Argument head-word - The head word of the
relation if the node is an internal node or the
lexical item (word) if it is a leaf.
(8) Head-word POS tag - The POS tag of the head
word.
(9) c-label pattern of argument - The left to right
chain of c-labels of the argument and its sib-
lings.
(10) d-label pattern - The left to right chain of d-
labels of the argument and its siblings.
(11) c-label & d-label of argument combined -
The c-label of the argument concatenated with
its d-label.
The training set consists of predicate/argument
pairs encoded in training instances. Each instance
contains features of a predicate and its candidate
argument. Candidate arguments are nodes (con-
stituents) in the dependency tree. This pair-wise
approach is analogous to earlier work by van den
Bosch et al (2004) and Tjong Kim Sang et al (2005)
in which instances were build from verb/phrase pairs
from which the phrase parent is an ancestor of the
verb. We adopted their approach to dependency
trees: only siblings of the verb (predicate) are con-
sidered as candidate arguments.
In comparison to experiments in earlier work, we
had relatively few training data available: our train-
ing corpus consisted of 2,395 sentences which com-
prise 3066 verbs, 5271 arguments and 3810 modi-
fiers.6 To overcome our data sparsity problem, we
trained the classifier using the leave one out (LOO)
method (-t leave_one_out option in TiMBL).
With this option set, every data item in turn is se-
lected once as a test item, and the classifier is trained
on all remaining items. Except for the LOO op-
tion, we only used the default TiMBL settings dur-
ing training, to prevent overfitting because of data
sparsity.
7 Results & Evaluation
Table 2 shows the performance of the TiMBL clas-
sifier on our annotated dependency treebank. From
these sentences, 12,113 instances were extracted. To
6We refer to (Oostdijk and Boves, 2006) for general infor-
mation about the domain of the D-Coi corpus and its design.
measure the performance of the systems, the auto-
matically assigned labels were compared to the la-
bels assigned by a human annotator.
Table 2: Results of TiMBL classification
Label Precision Recall F?=1
Overall 70.27% 70.59% 70.43
Arg0 90.44% 86.82% 88.59
Arg1 87.80% 84.63% 86.18
Arg2 63.34% 59.10% 61.15
Arg3 21.21% 19.18% 20.14
Arg4 54.05% 54.05% 54.05
ArgM-ADV 54.98% 51.85% 53.37
ArgM-CAU 47.24% 43.26% 45.16
ArgM-DIR 36.36% 33.33% 34.78
ArgM-DIS 74.27% 70.71% 72.45
ArgM-EXT 29.89% 28.57% 29.21
ArgM-LOC 57.95% 54.53% 56.19
ArgM-MNR 52.07% 47.57% 49.72
ArgM-NEG 68.00% 65.38% 66.67
ArgM-PNC 68.61% 64.83% 66.67
ArgM-PRD 45.45% 40.63% 42.90
ArgM-REC 86.15% 84.85% 85.50
ArgM-TMP 55.95% 53.29% 54.58
It is difficult to compare our results with those
obtained with other existing systems, since our sys-
tem is the first one to be applied to Dutch sentences.
Moreover, our data format, data size and evalua-
tion methods (separate test/train/develop sets ver-
sus LOO) are different from earlier research. How-
ever, to put our results somewhat in perspective, we
looked mainly at systems that participated in the
CoNLL shared tasks on semantic role labeling.
The best performing system that participated in
CoNLL 2005 reached an F1 of 80. There were seven
systems with an F1 performance in the 75-78 range,
seven more with performances in the 70-75 range
and five with a performance between 65 and 70 (Car-
reras and Ma`rquez, 2005).
A system that did not participate in the CoNLL
task, but still provides interesting material for com-
parison since it is also based on dependency struc-
tures, is the system by Hacioglu (2004). This system
scored 85,6% precision, 83,6% recall and 84,6 F1 on
the CoNLL data set, which is even higher than the
best results published so far on the PropBank data
82
sets (Pradhan et al, 2005): 84% precision, 75% re-
call and 79 F1. These results support our claim that
dependency structures can be very useful in the SRL
task.
As one would expect, the overall precision and
recall scores of the classifier are higher than those
of the XARA rule-based system. Yet, we expected
a better performance of the classifier on the lower
numbered arguments (ARG0 and ARG1). Our hy-
pothesis is that performance on these arguments can
be improved by by adding semantic features to our
feature set.
Examples of such features are the subcategoriza-
tion frame of the predicate and the semantic category
(e.g. WordNet synset) of the candidate argument.
We expect that such semantic features will improve
the performance of the classifier for certain types of
verbs and arguments, especially the lower numbered
arguments ARG0 and ARG1 and temporal and spa-
tial modifiers. For example, the Dutch preposition
over can either head a phrase indicating a location
or a time-span. The semantic category of the neigh-
boring noun phrase might be helpful in such cases to
choose the right PropBank label. Thanks to new lex-
ical resources, such as Cornetto (Vossen, 2006), and
clustering techniques based on dependency struc-
tures (van de Cruys, 2005), we might be able to add
lexical semantic information about noun phrases in
future research.
Performance of the classifier can also be im-
proved by automatically optimizing the feature set.
The optimal set of features for a classifier can
be found by employing bi-directional hill climbing
(van den Bosch et al, 2004). There is a wrapper
script (Paramsearch) available that can be used with
TiMBL and several other learning systems that im-
plements this approach7. In addition, iterative deep-
ening (ID) can be used as a heuristic way of finding
the optimal algorithm parameters for TiMBL.
8 Conclusions & Future work
We have presented an approach to automatic seman-
tic role labeling based on three steps: bootstrapping
from a syntactically annotated Dutch corpus with a
rule-based tagger developed for this purpose, man-
ual correction and training a machine learning sys-
7URL: http://ilk.uvt.nl/software.html#paramsearch
tem on the manually corrected data.
The promising results in this area obtained for En-
glish on the basis of PropBank role labels was a de-
cisive factor for our choice to adopt the PropBank
annotation scheme which has been adapted for the
annotation of the Dutch corpus. However, we would
like to adopt the conceptual structure of FrameNet,
even though not necessarily the granularity of its
role assignment approach, to this end we are link-
ing manually the predicates annotated with the Prop-
Bank semantic roles to the FrameNet ontology.
Only a small part of the D-Coi corpus has been an-
notated with semantic information, in order to yield
information with respect to its feasibility. We be-
lieve that a more substantial annotation task will be
carried out in the framework of a follow-up project
aiming at the construction of a 500 million word cor-
pus, in which one million words will be annotated
with semantic information. Hopefully, in the follow-
up project, it will be possible to carry out experi-
ments and measure inter-annotator agreement since
due to financial constraints only one annotator has
annotated the current corpus.
Finally, it would be interesting to see how the
classifier would perform on larger collections and
new genres of data. The follow-up of the D-Coi
project will provide new semantically annotated data
to facilitate research in this area.
References
G. Bouma and G. Kloosterman. 2002. Querying depen-
dency treebanks in xml. In Proceedings of the Third
international conference on Language Resources and
Evaluation (LREC). Gran Canaria.
G. Bouma, G. van Noord, and R. Malouf. 2000. Alpino:
wide-coverage computational analysis of dutch.
X. Carreras and L. Ma`rquez. 2005. Introduction to
the conll-2005 shared task: Semantic role labeling.
In Proceedings of the Eighth Conference on Compu-
tational Natural Language Learning (CoNLL-2005).
Boston, MA, USA.
J. Clark and S. DeRose. 1999. Xml path language
(xpath). W3C Recommendation 16 November 1999.
URL: http://www.w3.org/TR/xpath.
D. Daelemans, D. Zavrel, K. van der Sloot, and
A. van den Bosch. 2004. Timbl: Tilburg memory
based learner, version 5.1, reference guide. ILK Tech-
nical Report Series 04-02, Tilburg University.
83
M. Dzikovska, M. Swift, and J. Allen. 2004. Building
a computational lexicon and ontology with framenet.
In Proceedings of the workshop Building Lexi-
cal Resources with Semantically Annotated Corpora
(LREC) 2004. Lisbon.
D. Gildea and D. Jurafsky. 2002. Automatic labeling of
semantic roles. Comput. Linguist., 28(3):245?288.
K. Hacioglu. 2004. Semantic role labeling using de-
pendency trees. In COLING ?04: Proceedings of the
20th international conference on Computational Lin-
guistics, page 1273. August 2004.
C. R. Johnson, C. J. Fillmore, M. R. L. Petruck, C. F.
Baker, M. J. Ellsworth, J. Ruppenhofer, and E. J.
Wood. 2002. FrameNet:Theory and Practice.
S. Pado K. Erk, A. Kowalski and M. Pinkal. 2003. To-
wards a resource for lexical semantics: A large ger-
man corpus with extensive semantic annotation. In
Proceedings of ACL 2003. Sapporo.
P. Kingsbury, M. Palmer, and M. Marcus. 2002. Adding
semantic annotation to the penn treebank. In Proceed-
ings of the Human Language Technology Conference
(HLT?02).
P. Monachesi and J. Trapman. 2006. Merging framenet
and propbank in a corpus of written dutch. In Proceed-
ings of (LREC) 2006. Genoa.
M. Moortgat, I. Schuurman, and T. van der Wouden.
2000. CGN syntactische annotatie. Internal report
Corpus Gesproken Nederlands.
N. Oostdijk and L. Boves. 2006. User requirements
analysis for the design of a reference corpus of writ-
ten dutch. In Proceedings of (LREC) 2006. Genoa.
N. Oostdijk. 2002. The design of the spoken dutch cor-
pus. In P. Peters, P. Collins, and A. Smith, editors, New
Frontiers of Corpus Research, pages 105?112. Ams-
terdam: Rodopi.
S. Pradhan, K., V. Krugler, W. Ward, J. H. Martin, and
D. Jurafsky. 2005. Support vector learning for seman-
tic argument classification. Machine Learning Jour-
nal, 1-3(60):11?39.
E. Tjong Kim Sang, S. Canisius, A. van den Bosch, and
T. Bogers. 2005. Applying spelling error correction
techniques for improving semantic role labeling. In
Proceedings of the Ninth Conference on Natural Lan-
guage Learning (CoNLL-2005). Ann Arbor, MI, USA.
I. Schuurman and P. Monachesi. 2006. The contours of a
semantic annotation scheme for dutch. In Proceedings
of Computational Linguistics in the Netherlands 2005.
University of Amsterdam. Amsterdam.
G. Stevens. 2006. Automatic semantic role labeling in a
dutch corpus. Master?s thesis, Universiteit Utrecht.
C. Subirats and H. Sato. 2004. Spanish framenet and
framesql. In 4th International Conference on Lan-
guage Resources and Evaluation. Workshop on Build-
ing Lexical Resources from Semantically Annotated
Corpora. Lisbon (Portugal), May 2004.
J. Trapman and P. Monachesi. 2006. Manual for the
annotation of semantic roles in d-coi. Technical report,
University of Utecht.
Tim van de Cruys. 2005. Semantic clustering in dutch.
In Proceedings of CLIN 2005.
A. van den Bosch, S. Canisius, W. Daelemans, I. Hen-
drickx, and E. Tjong Kim Sang. 2004. Memory-
based semantic role labeling: Optimizing features, al-
gorithm, and output. In H.T. Ng and E. Riloff, edi-
tors, Proceedings of the Eighth Conference on Compu-
tational Natural Language Learning (CoNLL-2004).
Boston, MA, USA.
G. van Noord. 2006. At last parsing is now operational.
In Proceedings of TALN 06. Leuven.
P. Vossen. 2006. Cornetto: Een lexicaal-semantische
database voor taaltechnologie. Dixit Special Issue.
Stevin.
84
Proceedings of the Linguistic Annotation Workshop, pages 113?116,
Prague, June 2007. c?2007 Association for Computational Linguistics
XARA: An XML- and rule-based semantic role labeler
Gerwert Stevens
University of Utrecht, the Netherlands
gerwert.stevens@let.uu.nl
Abstract
XARA is a rule-based PropBank labeler for
Alpino XML files, written in Java. I used
XARA in my research on semantic role la-
beling in a Dutch corpus to bootstrap a
dependency treebank with semantic roles.
Rules in XARA are based on XPath expres-
sions, which makes it a versatile tool that is
applicable to other treebanks as well.
In addition to automatic role annotation,
XARA is able to extract training instances
(sets of features) from an XML based tree-
bank. Such an instance base can be used to
train machine learning algorithms for auto-
matic semantic role labeling (SRL). In my
semantic role labeling research, I used the
Tilburg Memory Learner (TiMBL) for this
purpose.
1 Introduction
Ever since the pioneering article of Gildea and Ju-
rafsky (2002), there has been an increasing interest
in automatic semantic role labeling (SRL). In gen-
eral, classification algorithms (a supervised machine
learning strategy) are used for this purpose. Manual
annotated corpora provide a gold standard for such
classifiers.
Starting manual annotation from scratch is very
time consuming and therefore expensive. A possible
solution is to start from a (partially) automatically
annotated corpus. In fact, this reduces the manual
annotation task to a manual correction task. Initial
automatic annotation of a corpus is often referred to
as bootstrapping or unsupervised SRL.
In recent years relatively little effort has gone into
the development of unsupervised SRL systems. This
is partly because semantically annotated English
corpora, such as PropBank (Kingsbury et al, 2002)
and FrameNet (Johnson et al, 2002), currently con-
tain enough data to develop and test SRL systems
based on machine learning. Therefore, bootstrap-
ping large collections of English texts has no prior-
ity anymore. For languages other than English how-
ever, annotated corpora are rare and still very much
needed. Therefore, the development of bootstrap-
ping techniques is very relevant.
One of the languages for which the creation of
semantically annotated corpora has lagged dramat-
ically behind, is Dutch. Within the project Dutch
Language Corpus Initiative (D-Coi)1, the first steps
have been taken towards the development of a large
semantically annotated Dutch corpus. The D-Coi
project is a preparatory project which will deliver
a blueprint and the tools needed for the construc-
tion of a 500-million-word reference corpus of con-
temporary written Dutch. The corpus will be an-
notated with several layers of annotation, amongst
others with semantic roles.
In the context of this project, I developed XARA:
(XML-based Automatic Role-labeler for Alpino-
trees). In my research, XARA was used for two pur-
poses:
? Bootstrap a dependency treebank with seman-
tic roles
1http://lands.let.ru.nl/projects/d-coi/
113
? Extract an instance base for the training of a
semantic role classifier.
2 Rule-based role labeling
2.1 The Alpino XML-format
The input for the semantic role tagger is a set of
sentences annotated by the Dutch dependency parser
Alpino (Bouma et al, 2000) 2. Alpino is based on
a hand-crafted Head-driven Phrase Structure Gram-
mar (HPSG).
The annotation scheme of Alpino dependency
trees is based on the Spoken Dutch Corpus (CGN)
(Oostdijk, 2002) annotation format. In Alpino trees
the same labels are used as in their CGN counter-
parts and nodes are structured in the same way. The
XML-format used to store dependency trees how-
ever differs. In the CGN, sentences are stored in
the TIGER-XML format (Lezius, 2002) 3, Alpino
uses its own XML format to store parsed sentences
(Bouma and Kloosterman, 2002). In our treebank,
every sentence was encoded in a separate XML file.
An example of an Alpino dependency tree annotated
with semantic roles is shown in figure 1. Below, the
corresponding XML output is shown:
<node rel="top">
<node cat="top" rel="top">
<node cat="smain" rel="--">
<node cat="np" rel="su">
<node pos="det" rel="det" word="de"/>
<node pos="noun" rel="hd" word="jongen"/>
</node>
<node pos="verb" rel="hd" word="aait"/>
<node cat="np" rel="obj1">
<node pos="det" rel="det" word="de"/>
<node pos="adj" rel="mod" word="zwarte"/>
<node pos="noun" rel="hd" word="hond"/>
</node>
</node>
</node>
The structure of Alpino XML documents directly
corresponds to the structure of the dependency tree:
dependency nodes are represented by NODE ele-
ments, attributes of the node elements are the c-
label, d-label, pos-tag, etc. The format is designed
to support a range of linguistic queries on the depen-
dency trees in XPath directly (Bouma and Klooster-
2A demonstration of the Alpino parser can be found
on the following website: http://ziu.let.rug.nl/
vannoord_bin/alpino
3see also http://www.ims.uni-stuttgart.de/
projekte/TIGER/TIGERSearch/index.shtml
Figure 1: Example CGN dependency graph (?The
boy pets the black dog?)
SMAIN
SU
np
de jongen
HD
verb
aait
OBJ1
DET
det
de
MOD
adj
zwarte
HD
noun
hond
man, 2002). XPath (Clark and DeRose, 1999) is a
powerful query language for the XML format and it
is the cornerstone of XARA?s rule-based approach.
I would like to stress that although our SRL re-
search focused on Alpino structures, XARA can be
used with any XML-based treebank, thanks to the
fact that XPath and XML are widely accepted stan-
dards. This property satisfies one of the major de-
sign criteria of the system: reusability.
2.2 The annotation process
The input for the tagger is set of directories con-
taining Alpino XML files, called a treebank. Each
sentence is annotated separately by applying a set
of rules. Rules are applied to local dependency do-
mains (subtrees of the complete dependency tree).
The local dependency domain to which a rule is ap-
plied, is called the rule?s context. A context is sim-
ply defined by an XPath expression which selects a
group of nodes.
Suppose for example that we want to apply a cer-
tain rule to nodes that are part of a passive partici-
ple, i.e the context of our rule are passive participles.
Passive participles in Alpino trees are local depen-
dency domains with a root node with c-label PPART.
An example is shown in figure 2.
The dark colored nodes are the ones we are inter-
ested in. To select these nodes, the following XPath
expression can be used:
114
Figure 2: Example PropBank annotation on a De-
pendency tree (?She is never seen?)
SMAIN
SU
1:pron
Arg1
ze
HD
verb
wordt
VC
ppart
OBJ1
1?
MOD
adv
nooit
HD
verb
REL
gezien
//node[@cat=?ppart?]
[preceding-sibling::
node[@rel=?hd? and (@root=?word?)]]
which says that we are looking for nodes with the c-
label PPART and the auxiliary verb indicating passive
tense (word) as preceding sibling.
Once a context is defined, rules can be applied
to nodes in this context. Rules consist of an XPath
expression which specifies a relative path from the
context?s root node to the target node and an output
label. Upon application of the rule, the target node
will be labeled with output label.
The output label can have three kinds of values:
? A positive number n, to label a node with
ARGn.
? The value -1, to label the node with the first
available numbered argument.
? A string value, to label the node with an arbi-
trary label, for example an ARGM.
Notice that because the label can be specified as
a string value, the set of possible labels is not re-
stricted. In my work, I used PropBank labels, but
other labels - such as generic thematic roles - can be
used just as well.
Formally, a rule in XARA can be defined as a
(path, label) pair. Suppose for example that we
want to select direct object nodes in the previously
defined context and assign them the label ARG1.
This can be formulated as:
(./node[@rel=?obj1?],1)
The first element of this pair is an XPath expres-
sion that selects direct object daughters, the second
element is a number that specifies which label we
want to assign to these target nodes. In this case the
label is a positive integer 1, which means the target
node will receive the label ARG1. Upon application
of a rule, an attribute (?pb?) is added to the target
node element in the XML file. This attribute con-
tains the PropBank label.
3 Feature extraction
Besides bootstrapping an unannotated corpus, train-
ing a SRL classifier was another important part of
my automatic SRL strategy. The learning tool I
used for this purpose was TiMBL (Tilburg Memory
Based Learner) (Daelemans et al, 2004).
In order to be able to train a TiMBL classifier, a
file with training data is needed. Training data is
represented as a text file containing instances. Each
line in the text file represents a single instance. An
instance consists of a set of features separated by
commas and a target class. XARA is able to create
such an instance base from a set of XML files auto-
matically.
3.1 The automatic feature extraction process
The target instance base consists of predi-
cate/argument pairs encoded in training instances.
Each instance contains features of a predicate and
its candidate argument. Candidate arguments are
nodes (constituents) in the dependency tree. This
pair-wise approach is analogous to earlier work by
van den Bosch et al (2004) and Tjong Kim Sang
et al (2005) in which instances were built from
verb/phrase pairs from which the phrase parent is an
ancestor of the verb.
Once it is clear how instances will be encoded, an
instance base can be extracted from the annotated
corpus. For example, the following instances can be
extracted from the tree in figure 2:
115
zie,passive,mod,adv,#
zie,passive,su,pron,ARG1
These two example instances consist of 4 features
and a target class each. In this example, the predi-
cate lemma (stem) and voice, and the candidate ar-
gument c-label, d-label are used. For null values
the hash symbol (#) is specified. The first instance
represents the predicate/argument pair (zie, nooit)
(?see,never?), the second instance represents the pair
(zie, ze) (?see, she?).
The extraction of instances from the annotated
corpus can be done fully automatically by XARA
from the command line. The resulting feature base
can be directly used in training a TiMBL classifier.
4 Performance
In order to evaluate the labeling of XARA, the out-
put of XARA?s semantic role tagger was compared
with the manual corrected annotation of 2,395 sen-
tences. The results are shown in table 1.
Table 1: Overall performance
Precision Recall F?=1
65,11% 45,83% 53,80
Since current rules in XARA cover only a sub-
set of PropBank labels, recall is notably lower than
precision. However, current overall performance of
XARA is encouraging. Our expectation is that, es-
pecially if the current rule set is improved and/or ex-
tended, XARA can be a very useful tool in current
and future SRL research.
References
G. Bouma and G. Kloosterman. 2002. Querying depen-
dency treebanks in xml. In Proceedings of the Third
international conference on Language Resources and
Evaluation (LREC). Gran Canaria.
G. Bouma, G. van Noord, and R. Malouf. 2000. Alpino:
wide-coverage computational analysis of dutch.
J. Clark and S. DeRose. 1999. Xml path language
(xpath). W3C Recommendation 16 November 1999.
URL: http://www.w3.org/TR/xpath.
D. Daelemans, D. Zavrel, K. van der Sloot, and
A. van den Bosch. 2004. Timbl: Tilburg memory
based learner, version 5.1, reference guide. ILK Tech-
nical Report Series 04-02, Tilburg University.
D. Gildea and D. Jurafsky. 2002. Automatic labeling of
semantic roles. Comput. Linguist., 28(3):245?288.
C. R. Johnson, C. J. Fillmore, M. R. L. Petruck, C. F.
Baker, M. J. Ellsworth, J. Ruppenhofer, and E. J.
Wood. 2002. FrameNet:Theory and Practice.
P. Kingsbury, M. Palmer, and M. Marcus. 2002. Adding
semantic annotation to the penn treebank. In Proceed-
ings of the Human Language Technology Conference
(HLT?02).
W Lezius. 2002. Ein Suchwerkzeug fu?r syntaktisch
annotierte Textkorpora. Ph.D. thesis, University of
Stuttgart.
N. Oostdijk. 2002. The design of the spoken dutch cor-
pus. In P. Peters, P. Collins, and A. Smith, editors, New
Frontiers of Corpus Research, pages 105?112. Ams-
terdam: Rodopi.
E. Tjong Kim Sang, S. Canisius, A. van den Bosch, and
T. Bogers. 2005. Applying spelling error correction
techniques for improving semantic role labeling. In
Proceedings of the Ninth Conference on Natural Lan-
guage Learning (CoNLL-2005). AnnArbor, MI, USA.
A. van den Bosch, S. Canisius, W. Daelemans, I. Hen-
drickx, and E. Tjong Kim Sang. 2004. Memory-
based semantic role labeling: Optimizing features, al-
gorithm, and output. In H.T. Ng and E. Riloff, edi-
tors, Proceedings of the Eighth Conference on Compu-
tational Natural Language Learning (CoNLL-2004).
Boston, MA, USA.
116
