Building a Japanese-Chinese Dictionary Using
Kanji/Hanzi Conversion
Chooi-Ling Goh, Masayuki Asahara, and Yuji Matsumoto
Graduate School of Information Science, Nara Institute of Science and Technology,
8916-5 Takayama, Ikoma, Nara 630-0192, Japan
{ling-g, masayu-a, matsu}@is.naist.jp
Abstract. A new bilingual dictionary can be built using two existing
bilingual dictionaries, such as Japanese-English and English-Chinese to
build Japanese-Chinese dictionary. However, Japanese and Chinese are
nearer languages than English, there should be a more direct way of
doing this. Since a lot of Japanese words are composed of kanji, which
are similar to hanzi in Chinese, we attempt to build a dictionary for kanji
words by simple conversion from kanji to hanzi. Our survey shows that
around 2/3 of the nouns and verbal nouns in Japanese are kanji words,
and more than 1/3 of them can be translated into Chinese directly. The
accuracy of conversion is 97%. Besides, we obtain translation candidates
for 24% of the Japanese words using English as a pivot language with
77% accuracy. By adding the kanji/hanzi conversion method, we increase
the candidates by 9%, to 33%, with better quality candidates.
1 Introduction
R. Dale et al (Eds.): IJCNLP 2005, LNAI 3651, pp. 670?681, 2005.
c
? Springer-Verlag Berlin Heidelberg 2005
Bilingual dictionaries have unlimited usage. In order for one to learn a new lan-
guage, a bilingual dictionary can never be absent. In natural language process-
ing community, bilingual dictionaries are useful in many areas, such as machine
translation and cross language information retrieval.
In this research, we attempt to build a Japanese-Chinese dictionary using
public available resources. There are already some existing Japanese-Chinese
dictionaries, such as Shogakukan?s Ri-Zhong Cidian [1], but they are not publicly
available in electronic form. Our purpose is to build an electronic dictionary from
public resources and make it public available.
The first dictionary that we use is IPADIC [2], a Japanese dictionary used by
ChaSen [3], a Japanese morphological analyzer. We extract only nouns, verbal
nouns and verbs from this dictionary, and try to search for their translation
equivalents in Chinese.
One can build a new bilingual dictionary for a new pair of languages using two
bilingual lexicons [4?8]. Since it is always easier to get bilingual dictionaries that
involve English as one of the languages, using English as a pivot language is pos-
sible. In this case, we first look for the English translations of one language, and
then try to find the possible candidates in the other language through English.
Then we rank the candidates according to the similarities between the two words
Building a Japanese-Chinese Dictionary Using Kanji/Hanzi Conversion 671
using some linguistic knowledge and statistical information. In our research, we
make use of two public resources, EDICT [9] - a Japanaese-English dictionary
and CEDICT [10] - a Chinese-English dictionary, to create the new language
pair Japanese-Chinese dictionary using English as a pivot language. We obtain
77% accuracy. However, this method extracts only translations for about 24% of
the Japanese words in IPADIC because the EDICT and CEDICT dictionaries
are smaller compared with IPADIC. Therefore, we also look into the possibility
to get the translation words using kanji/hanzi conversion. In Japanese, there are
three types of characters, namely hiragana, katakana and kanji. Kanji characters
are similar to Chinese ideographs. In Chinese, all characters are written in hanzi.
Since most of the kanji characters are originally from China, the usage should
remain unchangeable in certain contexts. The kanji/hanzi conversion method
works only on Japanese words that consist only kanji characters. We obtain a
high accuracy of 97% using this conversion. By combining the two methods, we
increase the number of translation candidates by 9%, from 24% to 33%.
2 Previous Work
Tanaka and Umemura [4] used English as an intermediate language to link
Japanese and French. They are the first who proposed the inverse consultation.
The concept behind is that a translation sometimes may have wider or narrower
meaning than the source word. They first look up the English translations of a
given Japanese word, then the French translations of these English translations.
This step gives a set of French candidates equivalent to the Japanese word. For
each French candidate, its translations in English is collected. The similarity
between the Japanese word and the French word is measured by the number of
matches in their English translation. The more matches show the better candi-
date. This is referred to as ?one time inverse consultation?. The extension can
be furthered by looking up all the Japanese translations of all the English trans-
lation of a given French word and seeing how many times the Japanese word
appears; this is referred to as ?two times inverse consultation?.
Bond at al. [6] applied the ?one time inverse consultation? in constructing
a Japanese-Malay dictionary using a Japanese-English dictionary and a Malay-
English dictionary. They also applied the semantic matching using part of speech
and second-language matching. Matching only compatible parts of speech could
cut down a lot of false matches. The second-language matching score used
Chinese as a second intermediate language. If a word pair could be matched
through two different languages, it is considered a very good match. Their re-
search showed that about 80% of the translations are good if only highest rank
pairs are considered, and 77% for all pairs.
Shirai and Yamamoto [7] used English as an intermediate language to link
Korean and Japanese. They tried on 1,000 Korean words and were able to obtain
the translations for 365 of them. They achieved an accuracy of 72% when the
degree of similarity calculated by one time inverse consultation is higher than a
predefined threshold.
672 C.-L. Goh, M. Asahara, and Y. Matsumoto
Zhang et al [8] used the same approach, that is using English as a pivot
language, for constructing Japanese-Chinese pairs. They used the one time in-
verse consultation method and also the part of speech information for ranking.
Since there is similarity between Japanese kanji and Chinese hanzi, they have
further improved on the method by using the kanji information [11]. First they
searched for the Chinese translations of single character words in Japanese into
using one time inverse consultation. If the Unicode of the two characters are
the same, then the ranking is higher. After getting this list of character pairs,
the similarity between the Japanese word and the Chinese word is calculated
using the edit distance algorithm [12]. Finally, the score obtained from the kanji
information is added to the final score function. Their ranking method was im-
proved and the precision increased from 66.67% to 81.43%. Since only about 50%
of their Japanese words can be translated into Chinese, they also searched for
other approaches to translate the remaining words [13] using web information
and machines translation method.
Our work is quite similar to Zhang et al [11] in the way they constructed
the kanji/hanzi conversion table. The difference is that instead of calculating the
similarity between kanji and hanzi using Unicode and one time inverse consulta-
tion, we make a direct conversion from kanji to hanzi based on the ideographs.
Our method sounds more intuitive and direct because kanji and hanzi are of
the same origin. Later on, they made use of this conversion table to calcutate
the similarity between a Japanese word and a Chinese word from the output of
using English as the pivot language. Their method can make the similar Chinese
words to have higher ranking but cannot generate new translation candidates.
On the other hand, our methods works for both.
3 The Proposed Methods
We propose to combine two methods to find the translations of Japanese entries
in IPADIC version 2.7.0 [2]. IPADIC is a monolingual dictionary and consists of
239,631 entries. We only extract nouns, verbal nouns and verbs (a total of 85,553
entries) in our survey. First, we use English as the pivot languege. Second, we
make direct conversion from kanji to hanzi for kanji word translation. We now
describe in detail the both methods.
3.1 Using Third Language: English
First, we use English as the pivot language to find the translations from Japanese
to English, and then from English to Chinese. Since IPADIC is a monolingual dic-
tionary, we use EDICT as the Japanese-English dictionary. The EDICT version
(V05-001) consists of 110,424 entries. There exist some words that are polyse-
mous with multiple entries. After combining the multiple entry words, we have
106,925 unique entries in the dictionary. For English to Chinese, we use the CE-
DICT dictionary. It consists of 24,665 entries. A word can be polysemous in both
Building a Japanese-Chinese Dictionary Using Kanji/Hanzi Conversion 673
dictionary, meaning that for each word there is only one entry but with multi-
ple translations. All the English translations of different senses are in the same
record. We assume that a bilingual dictionary should be bi-directional, therefore
we reverse the CEDICT dictionary to obtain an English-Chinese dictionary.
The ranking method is the one time inverse consultation [4, 6?8]. Since a word
can be polysemous in both dictionaries, if a source word shares more English
translations with the target translation word, then they can be considered nearer
in meaning. The score is calculated as in equation (1): Let SE(J,Ci) denotes
the similarity between the Japanese word J and the Chinese translation word
candidate Ci, where E(J) and E(Ci) are the sets of English translations for J
and Ci, respectively:
SE(J,Ci) =
2? (|E(J) ? E(Ci)|)
|E(J)|+ |E(Ci)|
(1)
Currently we do not apply the part of speech information in the scoring be-
cause this method requires linguistic experts to decide on the similarity between
two part of speech tags for different languages1. However, this will become part
of our future work.
Table 1 shows the results of using English as the pivot language and one
time inverse consultation as the scoring function. Using the EDICT and CE-
DICT only, 32,380 Japanese words obtain their Chinese translation candidates.
In total, we obtain 149,841 pairs of translation. We get maximum 90 candidates
for a Japanese word, and 4.6 candidates per word by average. Then we check
the Japanese words in IPADIC to get their part of speech tags. We only inves-
tigate on three categories of part of speech tags from the IPADIC, which are
nouns, verbal nouns and verbs. We randomly selected 200 Japanese words from
each category for evaluation. The results are judged using 4 categories: Correct
means that the first rank word is correct (if there are multiple words in the
first rank, it is considered correct if any one of the words is correct), Not-first
means that the correct word exists but not at the first rank, Acceptable means
that the first rank word is acceptable, and Wrong means that all candidates
are wrong. All the categories are exclusive of each other.
POS Total Translated Correct Not-first Acceptable Wrong
Nouns 58,793 14,275 (24.3%) 152 4 20 24
Verbal nouns 12,041 3,770 (31.3%) 90 12 37 61
Verbs 14,719 2,509 (17.0%) 101 18 27 54
Table 1. Ranking results
There are about 24.3% of nouns, 31.3% of verbal nouns and 17.0% of verbs in
IPADIC that give us some translation candidates in Chinese. For the evaluation
1 There are 120 part of speech tags (13 categories) in IPADIC, and 45 in Peking
University dictionary. Both define some quite specialized part of speech tags which
only exist within the dictionary itself.
674 C.-L. Goh, M. Asahara, and Y. Matsumoto
using 200 randomly selected words, we obtain 88%, 69.5% and 73% accuracy,
respectively. The accuracy is 76%, 45% and 50.5%, respectively, if we considered
only the first rank. The accuracy is a bit lower compared with previous work as
we did not apply other linguistic resources such as parts of speech for scoring.
Although improving scoring function can make the rank of the correct words
higher, it cannot further increase the number of candidates. Since both EDICT
and CEDICT are prepared by different people, the way they translate the words
also varies. Furthermore, there is no standization on the format. For example, to
represent a verb in English, sometimes it is written in base form (e.g. ?discuss?),
and sometimes in infinitive form (e.g. ?to discuss?). In Chinese, and sometimes in
Japanese too, a word shown in the dictionary can be a noun and a verb without
inflection. The part of speech category can only be decided based on the usage in
contexts. Therefore the same word may be translated into a noun in English too
(e.g. ?discussion?). It happened too that we cannot find the matches just because
of singular form or plural form (e.g. ?discussions?) of the English translation.
With these non-standardization of the English translation, we cannot match the
exact words unless we do a morphological analysis in English. Therefore, we
also look for other ways to increase to number of candidates. Since Japanese
and Chinese share some common characters (kanji in Japanese and hanzi in
Chinese), we are looking into the possibility of direct conversion to create the
translations. We discuss this method in the following section.
3.2 Direct Conversion of Kanji/Hanzi
Using English as the pivot language is a good starting point to construct a new
language pair. However, there remain a lot of words for which the translations
cannot be obtained. In Chinese, all the characters are hanzi, but in Japanese,
there are hiragana, katakana and kanji. The kanji characters are originated from
ancient China. This group of characters, used in China, Japan and Korea, are
referred to as Han characters. The Han characters capture some semantic infor-
mation which should be common in those languages. One can create a new word
by combining the existing characters but it is hardly that one can create a new
character. Therefore, these characters are stable in their meaning. Due to the
common sharing on these Han characters, there might be a more straightforward
way to translate a word in Japanese into Chinese if all the characters in the word
are made up from kanji only. We refer to this kind of words as kanji words.
A Chinese word can be a noun or a verb without changes of morphological
forms. There is no inflection to differenciate them. EDICT and CEDICT make
no difference on the parts of speech and therefore the translations in English can
be in any form. For example, the following Japanese words and Chinese words
exist for the translations of ?discussion/discussions/to discuss/discuss?.
Japanese: ??, {, ?{, ?, ??, 	?, ?{, ?, ?{, {, ??, O?,
?{, ?, ?{, H, 0, , , , 1d, 3d
Chinese: &, ??, X, ?, ?X, ??, ??, ??, ?, ?X, ?X, F, FX
Building a Japanese-Chinese Dictionary Using Kanji/Hanzi Conversion 675
If we were to match each Japanese word to each of the Chinese words (in fact,
we can say that some of them are acceptable translations), then we will get a
redundency of 286 (22?13) pairs. Although these words have similar translation,
but in fact they have slight differences in meaning. For example, ???? means
the conference amongst the ministers, ???? means negotiations. However, ?dis-
cussion? is one of the translations in English as provided by EDICT. Since the
Japanese kanji characters are originated from China, translating Japanese kanji
words directly to Chinese can be more accurate than going through a third lan-
guage like English. If we look from the Japanese side, 12 out of 22 words (??,
{, ?{, ?, ??, ?, ??, O?, ?, ?{, H, ) could get their exact
translations by just simple conversion of kanji/hanzi (??,FX,NF,?X,?
?, ?X, b?, ,?, ?X, ?F, &X, X), in which some of them cannot get
the translations using English. On the other hand, there also exist some words
that are not translated into the semantic meaning ?discuss? in Japanese but in
Chinese, such as ??X? which should be the same as ??? in Japanese2. For
the single character words in Chinese (&, ?, ?, F), they are seldom used in
Japanese but they do exist with the same meaning (H, ?, ?, {).
There exist equivalent characters between Japanese kanji and Chinese hanzi.
Both type of characters (Han characters in general) capture significant seman-
tic information. Although the pronunciation varies across languages, the visual
form of the characters retains certain level of similarity. Furthermore, Chinese
characters can be divided into the characters used by mainland China (referred
to as Simplified Chinese) and Taiwan (including Hong Kong and Macao, referred
to as Traditional Chinese). Although the ideographs may be different, they are
originally the same characters. Most of the Japanese characters are similar to
Traditional characters.
English love garden rice fly kill talk fill up post excellent sun
Japanese ?  ? ? ?  Q
@
,
b
Traditional Chinese ? h 9 ? l ? ?

8
&
Simplified Chinese ? ? , < ? ? V
?
?
?
Table 2. Successful Traditional-Simplified examples
Our original Japanese characters are coded in EUC and Chinese characters
are coded in GB-2312 codes. To convert a kanji to a hanzi is not a trivial task.
Of course most of the characters share the same ideographs. In this case, we can
use the Unicode for the conversion as these characters share the same Unicode.
However, there exist also quite a number of characters in Japanese that are
written in Traditional Chinese ideographs. We have to convert these characters
from Traditional Chinese to Simplified Chinese (see Table 2). Finally, there are
2 The meaning of ???? (a business talk) in Japanese is different from the meaning
of ????? (to discuss verbally) in Chinese.
676 C.-L. Goh, M. Asahara, and Y. Matsumoto
English gas hair deliver check home pass by burn bad money whole
Japanese [ b ` r ? C ? ? ? 
Traditional Chinese ? ? s l  B t ? ? ?
Simplified Chinese ?   5 * ? ?   
Table 3. Unsuccessful Traditional-Simplified examples
English sardine hackberry maple kite inclusive
Japanese ?  ? y -
English crossroad field/patch rice bowl carpentry chimera
Japanese  [ H  '
Table 4. Japanese-GBK examples
also some characters in Japanese having similar ideographs, but they are neither
Traditional Chinese nor Simplified Chinese (see Table 3). We manually convert
these characters by hand. The following shows the steps to convert the characters
from Japanese to Chinese.
1. Convert from EUC to Unicode using iconv.
2. Convert from Unicode to Unicode-simplified using a Chinese encoding con-
verter3. This step converts possible Traditional characters to Simplified char-
acters.
3. Convert from Unicode-simplified to GB-2312.
4. Those failed to be converted are edited manually by hand.
5. Those characters that do not exist in GB-2312 are converted into GBK using
the Chinese encoding converter.
From IPADIC, we extract 36,069 and 8,016 kanji words from noun and verbal
noun categories4, respectively. From these words, we get 4,454 distinct kanji char-
acters. Out of these characters, only 2,547 characters can be directly converted
using Unicode without changes of ideographs. 1,281 characters are converted
from Traditional Chinese to Simplified Chinese using the Chinese encoding con-
verter. Finally 626 characters are manually checked and 339 characters can be
converted to Simplified Chinese. 287 remain in Japanese ideographs but are con-
verted into GBK codes 5. Most of these words are the names of plants, fish, and
things invented by Japanese (see Table 4). While these GBK coded words may
not be used in Chinese, we just leave them in the conversion table for the sake
of completeness.
3 http://www.madarintools.com/zhcode.html
4 These two categories consist of most of the kanji words in Japanese. However, verbs
are normally hiragana only or a mixture of kanji plus hiragana. Therefore, we omit
verbs in this survey.
5 GBK codes consist of all simplified and traditional characters, including their vari-
ants. Therefore, Japanese characters can also be coded in GBK. However, they are
rarely used in Chinese.
Building a Japanese-Chinese Dictionary Using Kanji/Hanzi Conversion 677
About 61% of nouns and 67% of verbal nouns in Japanese are kanji words
as shown in Table 5. Using the conversion table described above, we convert the
kanji words into Chinese words. Then, we consult these words using a Chinese
dictionary provided by Peking University [14]. There are about 80,000 entries in
this dictionary. About 33% of the nouns and 44% of the verbal nouns are valid
words in Chinese. We randomly select 200 words for evaluation. We evaluate the
results by 3 categories: Correct means that the translation is good, Part-of
means that either the Japanese word or the Chinese word has a wider meaning,
and Wrong means that the meanings are not the same though they have the
same characters. The accuracies obtained are 97% for nouns and 97.5% for verbal
nouns. The pairs that have part-of meaning and different meaning are listed in
Table 6 and Table 7 for references.
POS Total Kanji words Translated Correct Part-of Wrong
Nouns 58,793 36,069 (61%) 11,743 (33%) 189 5 6
Verbal nouns 12,041 8,016 (67%) 3,519 (44%) 190 5 5
Table 5. Kanji/Hanzi conversion results
Japanese Chinese
?? (damage; casualty; victim) ?3 (be murdered; victimization)
? (samurai; warrior; servant) 9 (servant)
?? (a corner; competent) n (a corner; a unit used for money;)
d (charcoal iron; noshi - greeting paper) ?? (charcoal iron)
?? (character formation type) ?? (character formation type; knowing;
understanding)
? (work one?s way) C? (hardship study)
?? (set in place - Buddha statue or
corpse)
?? (set in place - for anything)
{? (artificiality; deliberateness; aggres-
sive action)
*? (action; acomplishment; regard as)
Yu (fall; drop) a (fall; drop; whereabouts; find a place
for; reprove)
?a (attend on the Emperor in his travels;
accompany in the imperial trains)
?a (offer sacrifice to; people gave com-
mend performances in an imperial palace)
Table 6. Part-of translation examples
The advantage of this method is that we can get exact translation for those
borrowed words from Chinese, especially idioms. We all know that it is always
difficult to translate idiomatic phrases from one language to another due to the
different cultural background. If we were to use English as the pivot language
to translate from Japanese to Chinese, it is difficult to have two different bilin-
678 C.-L. Goh, M. Asahara, and Y. Matsumoto
Japanese Chinese
?? (true; really) ?h (ought; should)
?? (nonmember) ?i (ministry councillor; landlord)
?} (deportation; banishment; exile) 5 (flee hither and thither)
s
 (light indigo) ?? (variety; designs and colors)
g (vegetables) h (potherb)
? (picture scroll) ?l (size of a picture)
?o (divide into halves) R? (reduce by half)
?? (self-surrender) ? (private prosecution)
D? (selfish; calculating) K? (plan; intend; calculate)
?? (search for cube root) ? (draw; issue; open)
?? (take a person into custody) ?Z (seduce; tempt)
Table 7. Wrong translation examples
gual dictionaries from two different publishers that translate them in the same
wordings. Since a lot of the idioms in Japanese are originally from China, the
conversion of kanji/hanzi will make the translation process faster and more ac-
curate. Some examples are given below.
???? (same bed different dream - cohabiting but living in different worlds)
O??? (better to be the beak of a rooster than the rump of a bull - better to
be the leader of a small group than a subordinate in a large organization)
#wk? (appearing in unexpected places and at unexpected moments)
The difficulty of this method is the translation of single character words.
Single character words normally have wider meaning (multiple senses) and the
usage is usually based on the context. It is fair enough if we translate the single
character words using the conversion table. However, these characters should
have more translations of other multi-character words. There are 2,049 single
character nouns in Japanese and 1,873 of them exist in Chinese after the con-
version. For verbal nouns, there are 128 Japanese words and 127 words exist in
Chinese (only ? (gossip, rumor) does not exist in Chinese).
3.3 Intergration
We combine both using English as the pivot language and kanji/hanzi conversion
method to get the final list of translation candidates. Table 8 shows the results
in details. We obtain 20,630 for nouns and 5,356 for verbal nouns. In total, we
obtain 28,495 words, in which 7,941 words are new translations. Furthermore,
we add in high quality translation candidates into the new bilingual dictionary.
2,428 of the candidates obtained using kanji/hanzi conversion method already
exist in the translation candidates using English as the pivot language. This can
help to double check on the list of translation candidates and make them rank
higher. 4,893 candidates are served as extra and better quality candidates on
top of the translation candidates obtained using English as the pivot language.
Building a Japanese-Chinese Dictionary Using Kanji/Hanzi Conversion 679
POS Kanji/ Acc. Est. Using Acc. Est. Total In Extra New
hanzi English
Nouns 11,743 97% 11,391 14,275 88% 12, 562 20,630 2,008 3,380 6,355
Verbal nouns 3,519 97.5% 3,431 3,770 69.5% 2,620 5,356 420 1,513 1,586
Verbs - - - 2,509 73% 1,832 2,509 - - -
Total 15,262 14,822 20,554 17,014 28,495 2,428 4,893 7,941
Table 8. Integration results
As an estimation, we will get about 17,014 Japanese words with correct
translations in Chinese using English as the pivot language. By using kanji/hanzi
conversion method, we could get about 14,822 words with correct translation.
4 Discussion and Future Work
In our survey, only 33% of nouns and 44% of verbal nouns created by kanji/hanzi
conversion method exist in the Peking University dictionary. However, this may
be due to the incompleteness of the Chinese dictionary that we used. We also
found some words after the conversion which are acceptable in Chinese though
they do not exist in the dictionary. Some of the examples are as follows: ?
w(autism), ?(the defense of Constituition or religion), ??(sixth sense),
?(the preceding emperor),??(deep sense),?f(misbelief)?. Therefore, we
can further verify the validity of the Chinese words using other resources such
as the information from the web.
The current work consider only kanji/hanzi conversion for Japanese words
that consists on kanji only. There are a lot of words in Japanese that are mixture
of kanji and hiragana. This happens normally with verbs and adjectives. For
example, ?Rd(eat), ?+d(escape), yXw2(produce), ?0(difficult), K0
(happy), ?$(quite)?. We should be able to get some acceptable translations
of these words after removing the hiragana parts, but most of the cases we
cannot obtain the best or good translations. From the 200 verbs that we used
for the evaluation, 139 words exist in Chinese but only 35 are good and 43 are
acceptable. The single characters used in these words are normally used only in
ancient Chinese but not in contemporary Chinese. For example,Rd =? (eat)
and ?d = ?? (throw away), but  (eat) and ? (throw away) in Chinese
are also possible translation in certain contexts. Furthermore, the contemporary
Chinese uses two character words more often than single character words even
they have the same meaning. This is to reduce the semantic ambiguity as single
character words tend to be polysemous. Therefore, direct kanji/hanzi conversion
is not so appropriate and we need another approach to handle this type of words.
We can apply the kanji/hanzi conversion method directly to most of the
Japanese proper nouns, such as person names, organization names and place
names because these names are normally written in kanji characters. Therefore,
we do not need any effort to translate these words from Japanese to Chinese if
680 C.-L. Goh, M. Asahara, and Y. Matsumoto
we have the character conversion table. This will ease a lot in the processing of
machine translation and cross language information retrieval.
The Unicode Consortium encoded the Han characters in Unicode6. Till date,
all the languages that use Han characters have their own encoding systems.
For example, Japanese is encoded in EUC-JP or JIS, Simplified Chinese is in
GB-2312, Traditional Chinese is in Big 5 etc. The same character that is used
in different languages is assigned with different codes. Therefore it is difficult
to convert from one code to another without a conversion table. The Unicode
Consortium solved the problem by unifying the encoding. The same character
with the same ideograph has only one code no matter in which language it is
used. With this unification, it eased a lot on the CJK research, especially in the
area of cross language information retrieval. Currently, they have increased the
number of Han characters from 27,496 characters (version 3.0) to 70,207 char-
acters (version 4.0). Such a huge increment is done by the addition of a large
amount of unusual characters that only have been used in either person names
or place names. With this new version, it covers almost all possible characters
used in hanzi (Chinese), kanji (Japanese) and hanja (Korean). The Unihan (uni-
code for Han characters) provides a lot of information such as the origin, the
specific language using that character, conversion to other encodings etc. The
most useful information in Unihan to our research is the relationship between
the characters. It embeds the links for the variants of characters which are useful
for the conversion from one encoding to the others (Japanese, Traditional Chi-
nese, Simplified Chinese or Korean). If we can make use of this table, then we
can build a complete conversion table that includes all Han characters.
Zhang et al [11] proposed to use kanji information to find the similarity be-
tween a Japanese word and a Chinese word. They matched on the Unicode and
calculated the similarity using the one time inverse consultation. Since they did
not make any conversion such as traditional characters to simplified characters,
some of the characters have the same meaning but different Unicodes. There-
fore, they could not be matched. If they could use the conversion table that we
proposed, then it would help to increase the score of the kanji words.
To convert from Japanese kanji to Simplified characters is easier than the
reverse. It is because some characters in Traditional characters are simplified into
the same characters in Simplified Chinese. For example,b (hair) and` (deliver)
are simplified to. Therefore, it has to depend on the contexts to decide which
Japanese character to use if we were to convert the Chinese Simplified characters
back to Japanese kanji.
5 Conclusion
As a conclusion, we proposed a method to compile a Japanese-Chinese dictionary
using English as the pivot language as a starting point. We made use of the public
available resources such as EDICT, CEDICT and IPADIC for the construction
6 http://www.unicode.org/chart/unihan.html
Building a Japanese-Chinese Dictionary Using Kanji/Hanzi Conversion 681
of the new language pair. The accuracy obtained is 77%. Since Japanese and
Chinese share common Han characters which are semantically heavy loaded,
the same characters used should carry the same meaning. Therefore, we also
proposed a kanji/hanzi conversion method to increase the translation candidates.
The accuracy obtained is 97%. The increment of translation candidates is 9%,
from 24% to 33%. The conversion table created can also be used in other fields
like machine translation and cross language information retrieval.
Acknowledgements
This research uses EDICT file which is the property of the Electronic Dictio-
nary Research and Development Group at Monash University. Thanks go to
http://www.mandarintools.com/zhcode.html for their Chinese Encoding Con-
verter.
References
1. Shogakukan and Peking Shomoinshokan, editors: Ri-Zhong Cidian [Japanese-
Chinese Dictionary] (1987)
2. Asahara, M., Matsumoto, Y.: IPADIC version 2.7.0. Users Manual. Nara Institute
of Science and Technology, Nara, Japan. (2003) http://chasen.naist.jp/.
3. Matsumoto, Y., Kitauchi, A., Yamashita, T., Hirano, Y., Matsuda, H., Takaoka, K.,
Asahara, M.: Morphological Analysis System ChaSen version 2.2.9 Manual. Nara
Institute of Science and Technology, Nara, Japan. (2002) http://chasen.naist.jp/.
4. Tanaka, K., Umemura, K.: Construction of a bilingual dictionary intermediated
by a third language. In: Proc. of COLING. (1994) 297?303
5. Lafourcade, M.: Multilingual dictionary construction and services - case study with
the fe* projects. In: Proc. of PACLING. (1997) 289?306
6. Bond, F., Sulong, R.B., Yamazaki, T., Ogura, K.: Design and construction of a
machine-tractable japanese-malay dictionary. In: Proc. of MT Summit VIII. (2001)
53?58
7. Shirai, S., Yamamoto, K.: Linking english words in two bilingual dictionaries to
generate another language pair dictionary. In: Proc. of ICCPOL. (2001) 174?179
8. Zhang, Y., Ma, Q., Isahara, H.: Automatic acquisition of a japanese-chinese bilin-
gual lexicon using english as an intermediary. In: Proc. of NLPKE. (2003) 471?476
9. Jim Breem: EDICT, Japanese-English Dictionary (2005)
http://www.csse.monash.edu.au/?jwb/edict.html.
10. Paul Denisowski: CEDICT, Chinese-English Dictionary (2005)
http://www.mandarintools.com/cedict.html.
11. Zhang, Y., Ma, Q., Isahara, H.: Use of kanji information in constructing a japanese-
chinese bilingual lexicon. In: Proc. of ALR Workshop. (2004) 42?49
12. Levenshtein, V.: Binary codes capable of correcting deletions, insertions and re-
versals. Doklady Akademii Nauk SSSR 163 (1965) 845?848
13. Zhang, Y., Isahara, H.: Acquiring compound word translation both automatically
and dynamically. In: Proc. of PACLIC 18. (2004) 181?185
14. Peking University: (Peking University Dictionary) http://www.icl.pku.edu.cn/.
Combination of Machine Learning Methods
for Optimum Chinese Word Segmentation
Masayuki Asahara
Chooi-Ling Goh
Kenta Fukuoka
Yotaro Watanabe
Nara Institute of Science and Technology, Japan
E-mail: cje@is.naist.jp
Ai Azuma
Yuji Matsumoto
Takashi Tsuzuki
Matsushita Electric
Industrial Co., Ltd.
Abstract
This article presents our recent work for par-
ticipation in the Second International Chi-
nese Word Segmentation Bakeoff. Our
system performs two procedures: Out-of-
vocabulary extraction and word segmenta-
tion. We compose three out-of-vocabulary
extraction modules: Character-based tag-
ging with different classifiers ? maximum
entropy, support vector machines, and con-
ditional random fields. We also com-
pose three word segmentation modules ?
character-based tagging by maximum en-
tropy classifier, maximum entropy markov
model, and conditional random fields. All
modules are based on previously proposed
methods. We submitted three systems which
are different combination of the modules.
1 Overview
We compose three systems: Models a, b and c for the
closed test tracks on all four data sets.
For Models a and c, three out-of-vocabulary (OOV)
word extraction modules are composed: 1. Maximum
Entropy (MaxEnt) classifier-based tagging; 2. Max-
imum Entropy Markov Model (MEMM)-based word
segmenter with Conditional Random Fields (CRF)-
based chunking; 3. MEMM-based word segmenter
with Support Vector Machines (SVM)-based chunk-
ing. Two lists of OOV word candidates are constructed
either by voting or merging the three OOV word ex-
traction modules. Finally, a CRFs-based word seg-
menter produces the final results using either of the
voted list (Model a) or the merged list (Model c).
Most of the classifiers use surrounding words and
characters as the contextual features. Since word and
character features may cause data sparse problem, we
utilize a hard clustering algorithm (K-means) to define
word classes and character classes in order to over-
come the data sparse problem. The word classes are
used as the hidden states in MEMM and CRF-based
word segmenters. The character classes are used as the
features in character-based tagging, character-based
chunking and word segmentation.
Model b is our previous method proposed in (Goh
et al, 2004b): First, a MaxEnt classifier is used to per-
form character-based tagging to identify OOV words
in the test data. In-vocabulary (IV) word list together
with the extracted OOV word candidates is used in
Maximum Matching algorithm. Overlapping ambi-
guity is denoted by the different outputs from For-
ward and Backward Maximum Matching algorithm.
Finally, character-based tagging by MaxEnt classifier
resolves the ambiguity.
Section 2 describes Models a and c. Section 3 de-
scribes Model b. Section 4 discusses the differences
among the three models.
2 Models a and c
Models a and c use several modules. First, a hard
clustering algorithm is used to define word classes and
character classes. Second, three OOV extraction mod-
ules are trained with the training data. These modules,
then, extract the OOV words in the test data. Third,
the OOV word candidates produced by the three OOV
extraction modules are refined by voting (Model a) or
merging (Model c) them. The final word list is com-
posed by appending the OOV word candidates to the
IV word list. Finally, a CRF-based word segmenter
analyzes the sentence based on the new word list.
2.1 Clustering for word/character classes
We perform hard clustering for all words
and characters in the training data. K-
means algorithm is utilized. We use R 2.2.1
(http://www.r-project.org/) to perform
k-means clustering.
134
Since the word types are too large, we cannot run k-
means clustering on the whole data. Therefore, we di-
vide the word types into 4 groups randomly. K-means
clustering is performed for each group. Words in each
group are divided into 5 disjoint classes, producing 20
classes in total. Preceding and succeeding words in the
top 2000 rank are used as the features for the cluster-
ing. We define the set of the OOV words as the 21st
class. We also define two other classes for the begin-
of-sentence (BOS) and end-of-sentence (EOS). So, we
define 23 classes in total.
20 classes are defined for characters. K-means clus-
tering is performed for all characters in the training
data. Preceding and succeeding characters and BIES
position tags are used as features for the clustering:
?B? stands for ?the first character of a word?; ?I? stands
for ?an intermediate character of a word?; ?E? stands
for ?the last character of a word?; ?S? stands for ?the
single character word?. Characters only in the test data
are not assigned with any character class.
2.2 Three OOV extraction modules
In Models a and c, we use three OOV extraction mod-
ules.
First and second OOV extraction modules use
the output of a Maximam Entropy Markov Model
(MEMM)-based word segmenter (McCallum et al,
2000) (Uchimoto et al, 2001). Word list is composed
by the words appeared in 80% of the training data.
The words occured only in the remaining 20% of the
training data are regarded as OOV words. All word
candidates in a sentence are extracted to form a trel-
lis. Each word is assigned with a word class. The
word classes are used as the hidden states in the trellis.
In encoding, MaxEnt estimates state transition proba-
bilities based on the preceding word class (state) and
observed features such as the first character, last char-
acter, first character class, last character class of the
current word. In decoding, a simple Viterbi algorithm
is used.
The output of the MEMM-based word segmenter is
splitted character by character. Next, character-based
chunking is performed to extract OOV words. We use
two chunkers: based on SVM (Kudo and Matsumoto,
2001) and CRF (Lafferty et al, 2001). The chunker
annotates BIO position tags: ?B? stands for ?the first
character of an OOV word?; ?I? stands for ?other char-
acters in an OOV word?; ?O? stands for ?a character
outside an OOV word?.
The features used in the two chunkers are the char-
acters, the character classes and the information of
other characters in five-character window size. The
word sequence output by the MEMM-based word seg-
menter is converted into character sequence with BIES
position tags and the word classes. The position tags
with the word classes are also introduced as the fea-
tures.
The third one is a variation of the OOV module in
section 3 which is character-based tagging by MaxEnt
classifier. The difference is that we newly introduce
character classes in section 2.1 as the features.
In summary, we introduce three OOV word extrac-
tion modules: ?MEMM+SVM?, ?MEMM+CRF? and
?MaxEnt classifier?.
2.3 Voting/Merging the OOV words
The word list for the final word segmenter are com-
posed by voting or merging. Voting means the OOV
words which are extracted by two or more OOV word
extraction modules. Merging means the OOV words
which are extracted by any of the OOV word extrac-
tion modules. The model with the former (voting)
OOV word list is used in Model a, and the model with
the latter (merging) OOV word list is used in Model c.
2.4 CRF-based word segmenter
Final word segmentation is carried out by a CRF-based
word segmenter (Kudo and Matsumoto, 2004) (Peng
and McCallum, 2004). The word trellis is composed
by the similar method with MEMM-based word seg-
menter. Though state transition probabilities are esti-
mated in the case of MaxEnt framework, the proba-
bilities are normalized in the whole sentence in CRF-
based method. CRF-based word segmenter is robust to
length-bias problem (Kudo and Matsumoto, 2004) by
the global normalization. We will discuss the length-
bias problem in section 4.
2.5 Note on MSR data
Unfortunately, we could not complete Models a and
c for the MSR data due to time constraints. There-
fore, we submitted the following 2 fragmented mod-
els: Model a for MSR data is MEMM-based word
segmenter with OOV word list by voting; Model c for
MSR data is CRF-based word segmenter with no OOV
word candidate.
3 Model b
Model b uses a different approach. First, we extract the
OOV words using a MaxEnt classifier with only the
character as the features. We did not use the character
classes as the features. Each character is assigned with
BIES position tags. Word segmentation by character-
based tagging is firstly introduced by (Xue and Con-
verse, 2002). In encoding, we extract characters within
five-character window size for each character position
in the training data as the features for the classifier.
In decoding, the BIES position tag is deterministically
annotated character by character in the test data. The
135
words that appear only in the test data are treated as
OOV word candidates.
We can obtain quite high unknown word recall with
this model but the precision is a bit low. However,
the following segmentation model will try to elimi-
nate some false unknown words. In the next step, we
append OOV word candidates into the IV word list
extracted from the training data. The segmentation
model is similar to the OOV extraction method, except
that the features include the output from the Maximum
Matching (MaxMatch) algorithm. The algorithm runs
in both forward (FMaxMatch) and backward (BMax-
Match) directions using the final word list as the ref-
erences. The outputs of FMaxMatch and BMaxMatch
are also assigned with BIES tags. The differences be-
tween the FMaxMatch and BMaxMatch outputs indi-
cate the positions where the overlapping ambiguities
occur. The final word segmentation is carried out by
MaxEnt classifier again.
Note, both procedures in Model b use whole train-
ing data in the training phase. The dictionary used in
the MaxMatch algorithm is extracted from the training
data only during the training phase. So, the training of
segmentation model does not explicitly consider OOV
words. We did not use the word and character classes
as features in Model b unlike in the case of Models a
and c. The details of the model can be found in (Goh
et al, 2004b). The difference is that we do not pro-
vide character types here because it is forbidden in
this round. Besides, we also did not prune the OOV
words because this step involve the intervention of hu-
man knowledge.
4 Discussions and Conclusions
Table 1 summarizes the results of the three models.
The proposed systems employ purely corpus-based
statistical/machine learning method. Now, we discuss
what we observe in the three models. We remark two
problems in word segmentation: OOV word problem
and length-bias problem.
OOV word problem is that simple word-based
Markov Model family cannot analyze the words not
included in the word list. One of the solutions is
character-based tagging (Xue and Converse, 2002)
(Goh et al, 2004a). The simple character-based tag-
ging (Model b) achieved high ROOV but the precision
is low. We tried to refine OOV extraction by voting
and merging (Model a and c). However, the ROOV
of Models a and c are not as good as that of Model
b. Figure 1 shows type-precision and type-recall of
each OOV extraction modules. While voting helps to
make the precision higher, voting deteriorates the re-
call. Defining some hand written rules to prune false
OOV words will help to improve the IV word segmen-
tation (Goh et al, 2004b), because the precision of
OOV word extraction becomes higher. Other types of
OOV word extraction methods should be introduced.
For example, (Uchimoto et al, 2001) embeded OOV
models in MEMM-based word segmenter (with POS
tagging). Less than six-character substrings are ex-
tracted as the OOV word candidates in the word trel-
lis. (Peng and McCallum, 2004) proposed OOV word
extraction methods based on CRF-based word seg-
menter. Their CRF-based word segmenter can com-
pute a confidence in each segment. The high confi-
dent segments that are not in the IV word list are re-
garded as OOV word candidates. (Nakagawa, 2004)
proposed integration of word and OOV word position
tag in a trellis. These three OOV extraction method are
different from our methods ? character-based tagging.
Future work will include implementation of these dif-
ferent sorts of OOV word extraction modules.
Length bias problem means the tendency that the lo-
cally normalized Markov Model family prefers longer
words. Since choosing the longer words reduces the
number of words in a sentence, the state-transitions are
reduced. The less the state-transitions, the larger the
likelihood of the whole sentence. Actually, the length-
bias reflects the real distribution in the corpus. Still,
the length-bias problem is nonnegligible to achieve
high accuracy due to small exceptional cases. We used
CRF-based word segmenter which relaxes the prob-
lem (Kudo and Matsumoto, 2004). Actually, the CRF-
based word segmenter achieved high RIV .
We could not complete Model a and c for MSR.
After the deadline, we managed to complete Model
a (CRF + Voted Unk.) and c (CRF + Merged Unk.)
The result of Model a was precesion 0.976, recall
0.966, F-measure 0.971, OOV recall 0.570 and IV re-
call 0.988. The result of Model c was precesion 0.969,
recall 0.963, F-measure 0.966, OOV recall 0.571 and
IV recall 0.974. While the results are quite good, un-
fortunately, we could not submit the outputs in time.
While our results for the three data sets (AS,
CITYU, MSR) are fairly good, the result for the PKU
data is not as good. There is no correlation between
scores and OOV word rates. We investigate unseen
character distributions in the data set. There is no cor-
relation between scores and unseen character distribu-
tions.
We expected Model c (merging) to achieve higher
recall for OOV words than Model a (voting). How-
ever, the result was opposite. The noises in OOV
word candidates should have deteriorated the F-value
of overall word segmentation. One reason might be
that our CRF-based segmenter could not encode the
occurence of OOV words. We defined the 21st word
class for OOV words. However, the training data for
CRF-based segmenter did not contain the 21st class.
We should include the 21st class in the training data
136
Table 1: Our Three Models and Results: F-value/ROOV /RIV (Rank of F-value)
AS CITYU MSR PKU
Model a CRF + Voted Unk. CRF + Voted Unk. MEMM + Voted Unk. CRF + Voted Unk.
0.947/0.606/0.971 0.942/0.629/0.967 0.949/0.378/0.971 0.934/0.521/0.955
(2/11) (2/15) (16/29) (10/23)
Model b Char.-based tagging Char.-based tagging Char.-based tagging Char.-based tagging
0.952/0.696/0.963 0.941/0.736/0.953 0.958/0.718/0.958 0.941/0.760/0.941
(1/11) (3/15) (6/29) (7/23)
Model c CRF + Merged Unk. CRF + Merged Unk. CRF + No Unk. CRF + Merged Unk.
0.939/0.445/0.967 0.928/0.598/0.940 0.943/0.025/0.990 0.917/0.325/0.940
(7/11) (8/15) (21/29) (14/23)
150/764
MEMM+SVM
MEMM+CRF MaxEnt
Voted Precision = 1727/2504=0.689Voted Recall = 1727/3226=0.535Merged Precision = 2532/6003=0.421Merged Recall = 2532/3226=0.784
69/599 586/2136
165/480 420/579
184/304
958/1141
AS
51/406
MEMM+SVM
MEMM+CRF MaxEnt
Voted Precision = 1068/1714=0.623Voted Recall = 1068/1670=0.639Merged Precision = 1367/3531=0.387Merged Recall = 1367/1670=0.818
42/352 206/1059
87/439 114/188
109/196
758/891
CITYU
correctly extracted types(left side)
extracted types(right side)
57/555
MEMM+SVM
MEMM+CRF MaxEnt
Voted Precision = 1196/1659=0.720Voted Recall = 1196/1991=0.600Merged Precision = 1628/4454=0.365Merged Recall = 1628/1991=0.817
40/330 335/1910
93/293 149/243
245/333
709/790
MSR
67/882
MEMM+SVM
MEMM+CRF MaxEnt
Voted Precision = 1528/2827=0.540Voted Recall = 1528/2863=0.533Merged Precision = 2184/7064=0.309Merged Recall = 2184/2863=0.762
87/720 502/2635
181/727 217/424
201/407
929/1269
PKU
Figure 1: OOV Extraction Precision and Recall by Type
by regarding some words as pseudo OOV words.
We also found a bug in the CRF-based OOV word
extration module. The accuracy of the module might
be slightly better than the reported results. However,
the effect of the bug on overall F-value might be lim-
ited, since the module was only part of the OOV ex-
traction module combination ? voting and merging.
Acknowledgement
We would like to express our appreciation to Dr. Taku
Kudo who developed SVM-based chunker and gave us
several fruitful comments.
References
Chooi-Ling Goh, Masayuki Asahara, and Yuji Mat-
sumoto. 2004a. Chinese Word Segmentation by
Classification of Characters. In Proc. of Third
SIGHAN Workshop, pages 57?64.
Chooi-Ling Goh, Masayuki Asahara, and Yuji Mat-
sumoto. 2004b. Pruning False Unknown Words to
Improve Chinese Word Segmentation. In Proc. of
PACLIC-18, pages 139?149.
Taku Kudo and Yuji Matsumoto. 2001. Chunking
with Support Vector Machines. In Proc. of NAACL-
2001, pages 192?199.
Taku Kudo and Yuji Matsumoto. 2004. Applying
Conditional Random Fields to Japanese Morpho-
logical Analysis. In Proc. of EMNLP-2004, pages
230?237.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional Random Fields: Prob-
abilistic Models for Segmenting and Labeling Se-
quence Data. In Proc. of ICML-2001, pages 282?
289.
Andrew McCallum, Dayne Freitag, and Fernando
Pereira. 2000. Maximum Entropy Markov Mod-
els for Information Extraction and Segmentation. In
Proc. of ICML-2000, pages 591?598.
Tetsuji Nakagawa. 2004. Chinese and Japanese Word
Segmentation Using Word-Level and Character-
Level Information. In Proc. of COLING-2004,
pages 466?472.
Fuchun Peng and Andrew McCallum. 2004. Chinese
Segmentation and New Word Detection using Con-
ditional Random Fields. In Proc. of COLING-2004,
pages 562?568.
Kiyotaka Uchimoto, Satoshi Sekine, and Hitoshi Isa-
hara. 2001. The Unknown Word Problem: a
Morphological Analysis of Japanese Using Maxi-
mum Entropy Aided by a Dictionary. In Proc. of
EMNLP-2001, pages 91?99.
Nianwen Xue and Susan P. Converse. 2002. Combin-
ing Classifiers for Chinese Word Segmentation. In
Proc. of First SIGHAN Workshop, pages 63?70.
137
Chinese Unknown Word Identification Using Character-based Tagging and
Chunking
GOH Chooi Ling, Masayuki ASAHARA, Yuji MATSUMOTO
Graduate School of Information Science
Nara Institute of Science and Technology
 
ling-g,masayu-a,matsu  @is.aist-nara.ac.jp
Abstract
Since written Chinese has no space to de-
limit words, segmenting Chinese texts be-
comes an essential task. During this task,
the problem of unknown word occurs. It is
impossible to register all words in a dictio-
nary as new words can always be created
by combining characters. We propose a
unified solution to detect unknown words
in Chinese texts. First, a morphological
analysis is done to obtain initial segmen-
tation and POS tags and then a chunker is
used to detect unknown words.
1 Introduction
Like many other Asian languages (Thai, Japanese,
etc), written Chinese does not delimit words by
spaces and there is no clue to tell where the word
boundaries are. Therefore, it is usually required to
segment Chinese texts prior to further processing.
Previous research has been done for segmentation,
however, the results obtained are not quite satisfac-
tory when unknown words occur in the texts. An
unknown word is defined as a word that is not found
in the dictionary. As for any other language, all pos-
sibilities of derivational morphology cannot be fore-
seen in the form of a dictionary with a fixed number
of entries. Therefore, proper solutions are necessary
for the detection of unknown words.
Along traditional methods, unknown word detec-
tion has been done using rules for guessing their
location. This can ensure a high precision for the
detection of unknown words, but unfortunately the
recall is not quite satisfactory. It is mainly due to
the Chinese language, as new patterns can always
be created, that one can hardly efficiently maintain
the rules by hand. Since the introduction of statis-
tical techniques in NLP, research has been done on
Chinese unknown word detection using such tech-
niques, and the results showed that statistical based
model could be a better solution. The only resource
needed is a large corpus. Fortunately, to date, more
and more Chinese tagged corpora have been created
for research purpose.
We propose an ?all-purpose? unknown word de-
tection method which will extract person names, or-
ganization names and low frequency words in the
corpus. We will treat low frequency words as gen-
eral unknown words in our experiments. First, we
segment and assign POS tags to words in the text
using a morphological analyzer. Second, we break
segmented words into characters, and assign each
character its features. At last, we use a SVM-based
chunker to extract the unknown words.
2 Proposed Method
We shall now describe the 3 steps successively.
2.1 Morphological Analysis
ChaSen is a widely used morphological analyzer for
Japanese texts (Matsumoto et al, 2002). It achieves
over 97% precision for newspaper articles. We as-
sume that Chinese language has similar characteris-
tics with Japanese language to a certain extent, as
both languages share semantically heavily loaded
characters, i.e. kanji for Japanese, hanzi for Chinese.
Based on this assumption, a model for Japanese may
do well enough on Chinese. This morphological an-
alyzer is based on Hidden Markov Models. The tar-
get is to find the word and POS sequence that max-
imize the probability. The details can be found in
(Matsumoto et al, 2002).
2.2 Character Based Features
Character based features allow the chunker to detect
unknown words more efficiently. It is especially the
case when unknown words overlap known words.
For example, ChaSen will segment the phrase ? 

. . . ? (Deng Yingchao before death) into
?  /  / 	 /  /. . . ? (Deng Ying before next life). If
we use word based features, it is impossible to detect
the unknown person name ?  
 ? because it will
not break up the word ?  ? (next life). Breaking
words into characters enables the chunker to look at
characters individually and to identify the unknown
person name above.
The POS tag from the output of morphological
analysis is subcategorized to include the position of
the character in the word. The list of positions is
shown in Table 1. For example, if a word contains
three characters, then the first character is  POS  -B,
the second is  POS  -I and the third is  POS  -E. A
single character word is tagged as  POS  -S.
Table 1: Position tags in a word
Tag Description
S one-character word
B first character in a multi-character word
I intermediate character in a multi-
character word (for words longer than
two characters)
E last character in a multi-character word
Character types can also be used as features for
chunking. However, the only information at our dis-
posal is the possibility for a character to be a fam-
ily name. The set of characters used for translitera-
tion may also be useful for retrieving transliterated
names.
2.3 Chunking with Support Vector Machine
We use a Support Vector Machines-based chunker,
YamCha (Kudo and Matsumoto, 2001), to extract
unknown words from the output of the morphologi-
cal analysis. The chunker uses a polynomial kernel
of degree 2. Please refer to the paper cited for de-
tails.
Basically we would like to classify the characters
into 3 categories, B (beginning of a chunk), I (inside
a chunk) and O (outside a chunk). A chunk is con-
sidered as an unknown word in this case. We can
either parse a sentence forwardly, from the begin-
ning of a sentence, or backwardly, from the end of
a sentence. There are always some relationships be-
tween the unknown words and the their contexts in
the sentence. We will use two characters on each left
and right side as the context window for chunking.
Figure 1 illustrates a snapshot of the chunking
process. During forward parsing, to infer the un-
known word tag ?I? at position i, the chunker uses
the features appearing in the solid box. Reverse is
done in backward parsing.
3 Experiments
We conducted an open test experiment. A one-
month news of year 1998 from the People?s Daily
was used as the corpus. It contains about 300,000
words (about 1,000,000 characters) with 39 POS
tags. The corpus was divided into 2 parts randomly
with a size ratio for training/testing of 4/1.
All person names and organization names were
deleted from the dictionary for extraction. There
were 4,690 person names and 2,871 organization
names in the corpus. For general unknown word,
all words that occurred only once in the corpus were
deleted from the dictionary, and were treated as un-
known words. 12,730 unknown words were created
under this condition.
4 Results
We now present the results of our experiments in re-
call, precision and F-measure, as usual in such ex-
periments.
4.1 Person Name Extraction
Table 2 shows the results of person name extraction.
The accuracy for retrieving person names was quite
satisfiable. We could also extract names overlap-
ping with the next known word. For example, for
the sequence ?  /Ng  /Ag  /v  /f  /v  /v
Position Char. POS(best) Family Name Chunk
i - 2  n-S Y B
i - 1  Ag-S N I
i  Ng-S N I
i + 1  n-B N O
i + 2  n-E Y O
Figure 1: An illustration of chunking process ?President Jiang Zemin?
 /u  /n? (The things that Deng Yingchao used
before death), the system was able to correctly re-
trieve the name ?  Combining Segmenter and Chunker for Chinese Word Segmentation
Masayuki Asahara, Chooi Ling Goh, Xiaojie Wang, Yuji Matsumoto
Graduate School of Information Science
Nara Institute of Science and Technology, Japan
{masayu-a,ling-g,xiaoji-w,matsu}@is.aist-nara.ac.jp
Abstract
Our proposed method is to use a Hidden
Markov Model-based word segmenter and a
Support Vector Machine-based chunker for
Chinese word segmentation. Firstly, input sen-
tences are analyzed by the Hidden Markov
Model-based word segmenter. The word seg-
menter produces n-best word candidates to-
gether with some class information and confi-
dence measures. Secondly, the extracted words
are broken into character units and each char-
acter is annotated with the possible word class
and the position in the word, which are then
used as the features for the chunker. Finally, the
Support Vector Machine-based chunker brings
character units together into words so as to de-
termine the word boundaries.
1 Methods
We participate in the closed test for all four sets of data
in Chinese Word Segmentation Bakeoff. Our method is
based on the following two steps:
1. The input sentence is segmented into a word se-
quence by Hidden Markov Model-based word seg-
menter. The segmenter assigns a word class with
a confidence measure for each word at the hidden
states. The model is trained by Baum-Welch algo-
rithm.
2. Each character in the sentence is annotated with the
word class tag and the position in the word. The
n-best word candidates derived from the word seg-
menter are also extracted as the features. A sup-
port vector machine-based chunker corrects the er-
rors made by the segmenter using the extracted fea-
tures.
We will describe each of these steps in more details.
1.1 Hidden Markov Model-based Word Segmenter
Our word segmenter is based on Hidden Markov Model
(HMM). We first decide the number of hidden states
(classes) and assume that the each word can belong to
all the classes with some probability. The problem is de-
fined as a search for the sequence of word classes C =
c1, . . . , cn given a word sequence W = w1, . . . , wn. The
target is to find W and C for a given input S that maxi-
mizes the following probability:
argmax
W,C
P (W |C)P (C)
We assume that the word probability P (W |C) is con-
strained only by its word class, and that the class prob-
ability P (C) is constrained only by the class of the pre-
ceding word. These probabilities are estimated by the
Baum-Welch algorithm using the training material (See
(Manning and Schu?tze., 1999)). The learning process is
based on the Baum-Welch algorithm and is the same as
the well-known use of HMM for part-of-speech tagging
problem, except that the number of states are arbitrarily
determined and the initial probabilities are randomly as-
signed in our model.
1.2 Correction by Support Vector Machine-based
Chunker
While the HMM-based word segmenter achieves good
accuracy for known words, it cannot identify compound
words and out-of-vocabulary words. Therefore, we in-
troduce a Support Vector Machine(below SVM)-based
chunker (Kudo and Matsumoto, 2001) to cover the er-
rors made by the segmenter. The SVM-based chunker
re-assigns new word boundaries to the output of the seg-
menter.
An SVM (Vapnik, 1998) is a binary classifier. Sup-
pose we have a set of training data for a binary class
problem: (x1, y1), . . . , (xN , yN ), where xi ? Rn is a
feature vector of the i th sample in the training data and
yi ? {+1,?1} is the label of the sample. The goal is to
find a decision function which accurately predicts y for
an unseen x. An SVM classifier gives a decision function
f(x) for an input vector x where
f(x) = sign(
?
zi?SV
?iyiK(x, zi) + b).
f(x) = +1 means that x is a positive member, and
f(x) = ?1 means that x is a negative member. The vec-
tors zi are called support vectors, which receive a non-
zero weight ?i. Support vectors and the parameters are
determined by solving a quadratic programming prob-
lem. K(x, z) is a kernel function which maps vectors
into a higher dimensional space. We use a polynomial
kernel of degree 2 given by K(x, z) = (1 + x ? z)2.
The SVM classifier determines the position tag for
each character. We introduce the word class tag as the
feature, which is generated by the word segmenter. Since
we perform chunking by character units, the feature used
by the classifier will be the information for the character
unit.
The training data for our SVM-based chunker is con-
structed from the output of the HMM-based word seg-
menter defined in the previous section. In the current
setting, the HMM produces all the possible tags (class
labels) for each of the word within a predefined probabil-
ity bound. All the words in the output are then segmented
into characters, and each of the characters is tagged with
pairs of a word class and a position tag. For example,
in the paired tag ?0-B?, ?0? is a class label of the word
which the character belongs to and ?B? indicates the char-
acter?s position in the word. The number of classes is
determined in advance of the HMM learning. The po-
sition tag consists of the following four tags (S/B/E/I):
S means a single-character word; B is the first charac-
ter in a multi-character word; E is the last character in a
multi-character word; I is the intermediate character in a
multi-character word longer than 2 characters. As shown
in Figure 1, we set the HMM-based word segmenter to
produce the classes of n-best word candidates to take into
account multiple possibility of word boundaries.
The correct word boundary can be defined by assigning
either of two kinds of tags to each of the characters. Look
at the rightmost column of Figure 1 named as ?Chunker
Outputs.? The label ?B? in this column shows that the
character is the first character of a correct word, and ?I?
shows that the character is the other part of a word. This
means that the preceding positions of ?B? tags are the
word boundaries.
Those two tags correspond to the two classes of the
SVM chunker: In the training (and test) phrase, we use
window size of two characters to the left and right direc-
tion to learn (and estimate) the class for a character. For
example, the shadowed parts in Figure 1 are used as the
Figure 1: The Extracted Features for the Chunker
features to learn (or estimate) the word boundary tag ?I?
for the character ? ?.
2 Model Validation
To find out the best setting of learning, we would like to
determine ?the number of word classes? and ?the depth of
n-best word candidates? by using some sort of confidence
measure. We perform validation experiments for these
two types of parameters by using the training material
provided.
2.1 Validation Tests for HMM-based Word
Segmenter
The first validation experiment is to determine ?the num-
ber of word classes? of the HMM. 80% of the material is
used for the HMM training, and the other 20% is used as
the validation set. We test two settings for the number of
classes ? 5 & 10. The results are shown in Table 1.
Table 1: Validation Results for HMM
Data # of classes Rec. Prec. F
AS 5 0.845 0.768 0.804
AS 10 0.900 0.857 0.878
CTB 5 0.909 0.844 0.875
CTB 10 0.912 0.848 0.879
HK 5 0.867 0.742 0.799
HK 10 0.867 0.741 0.799
PK 5 0.942 0.902 0.921
PK 10 0.944 0.905 0.924
In most cases, models perform slightly better with the
increasing of the number of classes. When the corpus
size is large like the Academia Sinica data, this tendency
becomes more significant.
Whereas it is known that the Baum Welch algorithm is
very sensitive to the initialization of the classes, we ran-
domly assigned the initial classes without making much
effort. There are two reasons: (1) Since the word seg-
menter outputs are used as the clues to the chunker in our
method, we only need some consistent class annotations.
(2) The initial classes did not affect on the word segmen-
tation accuracy in our pilot experiments.
2.2 Validation Tests for SVM-based Chunker
The second validation test is for the chunking model to
determine both ?the number of word classes? and ?the
depth of the n-best candidates?. 80% of the material used
for the HMM training, another 10% is used for the chunk-
ing model training and the last 10% is used for the val-
idation test. The results are shown in Table 2, 3 and 4.
Since the training of this model is time- and resource-
consuming, the Academia Sinica data being very large
could not get enough time to finish the validation test.
Table 2: Validation Results (CTB) for Chunking
# of classes n-best Rec. Prec. F
5 1 0.957 0.930 0.943
5 2 0.957 0.931 0.944
5 3 0.957 0.930 0.943
5 4 0.957 0.930 0.943
10 1 0.956 0.929 0.943
10 2 0.957 0.928 0.942
10 3 0.956 0.929 0.942
10 4 0.955 0.928 0.941
Table 3: Validation Results (HK) for Chunking
# of classes n-best Rec. Prec. F
5 1 0.853 0.793 0.822
5 2 0.859 0.799 0.828
5 3 0.859 0.799 0.828
5 4 0.859 0.800 0.828
10 1 0.856 0.793 0.823
10 2 0.858 0.797 0.826
10 3 0.857 0.796 0.826
10 4 0.858 0.797 0.826
The results show that the chunker actually improves
the word segmentation accuracy compared with the out-
put of the HMM word segmenter for these three data sets.
The segmentation errors made by the word segmenter for
compound words and unknown words are corrected. The
Table 4: Validation Results (PK) for Chunking
# of classes n-best Rec. Prec. F
5 1 0.960 0.934 0.947
5 2 0.961 0.935 0.948
5 3 0.962 0.936 0.949
5 4 0.962 0.935 0.948
10 1 0.961 0.932 0.946
10 2 0.962 0.935 0.948
10 3 0.961 0.934 0.947
10 4 0.961 0.934 0.947
improvement in Chinese Treebank (CTB) data set is sig-
nificant, because the data set contains many compound
words.
There is no significant difference in the results between
the different depths of n-best answers. Still, we choose
the best model for the test materials among them. If we
need to have a faster analyzer, we should employ only the
best answer of the word segmentation.
For the HMM, the larger number of classes tends to
get better accuracy than smaller ones. However, for the
chunking model, the result is the other way round. The
model with the smaller number of classes gets slightly
better accuracy. So, there should be trade-off between
smaller and larger number of classes.
3 Final Models for Test Material
For the final models, 80% of the training material is used
for HMM training and 100% of the material is used for
the chunking model training. The parameters, namely
?the number of word classes? and ?the depth of n-best
word candidates?, are determined by the validation tests
described in Section 2. While there is no significant dif-
ference between the depths of n-best answers, we choose
the best model among them for the testing. The parame-
ters are shown in Table 7.
We cannot create the model using all the original
Academia Sinica data because of its large size. Therefore,
we use 80% of the data for HMM training (5 classes) and
only 10% for chunking model training (with only the best
candidates).
Table 7: The Models for the Test Material
? with respect to F-Measure in Our Validation Test
Data # of classes n-best F
AS 5 1 N/A
CTB 5 2 0.943
HK 5 4 0.828
PK 5 3 0.948
Table 5: Throughput Speeds (characters per second)
Data Word Seg. (# of words) Fea. Ext. (n-best) Chunker (# of SV) Total Speed
AS 57000 (462750) 7640 (Only Best) 279 (96452) 241
CTB 54400 (77324) 4040 (to 2nd Best) 894 (16736) 671
HK 38900 (93231) 3870 (to 4th Best) 649 (14904) 524
PK 57400 (215865) 6209 (to 3rd Best) 254 (49736) 200
Table 6: Results for the Test Materials
Data T. Rec. T. Prec. F OOV Rec. IV Rec. Ranking
AS 0.944 0.945 0.945 0.574 0.952 3rd/6
CTB 0.852 0.807 0.829 0.412 0.949 8th/10
HK 0.940 0.908 0.924 0.415 0.980 5th/6
PK 0.933 0.916 0.924 0.357 0.975 2nd/4
4 Throughput Speeds
As described, our system is based on three modules:
HMM-based word segmenter, Feature extractor and
SVM-based chunker. The word segmenter is composed
by ChaSen (written in C/C++) (Matsumoto et. al., 2003)
which is adopted for GB/Big5 encoding. The feature
extractor is written in Perl. The SVM-based chunker is
composed by YamCha (written in C++) (Kudo and Mat-
sumoto, 2001).
Table 5 shows the speeds 1 of the three modules indi-
vidually and of the total system. ?# of words? means the
size of the word segmenter lexicon. Note that, if a word
belongs to more than one class, we regard them as differ-
ent words in our definition. ?# of SV? means the number
of support vectors in the chunker. The total system speed
depends highly on that of the chunker. It is known that
the speed of SVM classifiers depends on the number of
support vectors and the number of features.
5 Conclusion
We presented our method for Chinese Word Segmenta-
tion Bakeoff in 2nd SIGHAN Workshop. The results for
the test materials are shown in Table 6. The proposed
method is purely corpus-based statistical/machine learn-
ing method. Although we did not incorporate any heuris-
tic rules (e.g. part-of-speeches, functional words and
concatenation for numbers) into the model, the method
achieved considerable accuracy for the word segmenta-
tion task.
Acknowledgments
We thank Mr. Taku Kudo of NAIST for his development
of the SVM-based chunker YamCha.
1The throughput speeds are measured on a machine: In-
tel(R) Xeon(TM) CPU 2.80GHz ? 2, Memory 4GB, RedHat
Linux 9.
References
T. Kudo and Y. Matsumoto. 2001. Chunking with Sup-
port Vector Machines. In Proc. of NAACL 2001, pages
192?199.
C. D. Manning and H. Schu?tze. 1999. Foundation of
Statistical Natural Language Processing. Chapter 9.
Markov Models, pages 317?340.
Y. Matsumoto, A. Kitauchi, T. Yamashita, Y. Hirano, K.
Takaoka and M. Asahara 2003. Morphological Ana-
lyzer ChaSen-2.3.0 Users Manual Tech. Report. Nara
Institute of Science and Technology, Japan.
L. A. Ramshaw and M. P. Marcus. 1995 Text chunking
using transformation-bases learning In Proc. of the 3rd
Workshop on Very Large Corpora, pages 83?94.
V. N. Vapnik. 1998. Statistical Learning Theory. A
Wiley-Interscience Publication.
 
	 
