A Unified Approach in Speech-to-Speech Translation: Integrating
Features of Speech Recognition and Machine Translation
Ruiqiang Zhang and Genichiro Kikui and Hirofumi Yamamoto
Taro Watanabe and Frank Soong and Wai Kit Lo
ATR Spoken Language Translation Research Laboratories
2-2 Hikaridai, Seiika-cho, Soraku-gun, Kyoto, 619-0288, Japan
{ruiqiang.zhang, genichiro.kikui}@atr.jp
Abstract
Based upon a statistically trained speech
translation system, in this study, we try
to combine distinctive features derived from
the two modules: speech recognition and
statistical machine translation, in a log-
linear model. The translation hypotheses
are then rescored and translation perfor-
mance is improved. The standard trans-
lation evaluation metrics, including BLEU,
NIST, multiple reference word error rate
and its position independent counterpart,
were optimized to solve the weights of the
features in the log-linear model. The exper-
imental results have shown significant im-
provement over the baseline IBM model 4
in all automatic translation evaluation met-
rics. The largest was for BLEU, by 7.9%
absolute.
1 Introduction
Current translation systems are typically of a
cascaded structure: speech recognition followed
by machine translation. This structure, while
explicit, lacks some joint optimality in per-
formance since the speech recognition module
and translation module are running rather in-
dependently. Moreover, the translation module
of a speech translation system, a natural off-
spring of text-input based translation system,
usually takes a single-best recognition hypoth-
esis transcribed in text and performs standard
text-based translation. Lots of supplementary
information available from speech recognition,
such as N -best recognition recognition hypothe-
ses, likelihoods of acoustic and language models,
is not well utilized in the translation process.
The information can be effective for improving
translation quality if employed properly.
The supplementary information can be ex-
ploited by a tight coupling of speech recognition
and machine translation (Ney, 1999) or keeping
the cascaded structure unchanged but using an
integration model, log-linear model, to rescore
the translation hypotheses. In this study the
last approach was used due to its explicitness.
In this paper we intended to improve speech
translation by exploiting these information.
Moreover, a number of advanced features from
the machine translation module were also added
in the models. All the features from the speech
recognition and machine translation module
were combined by the log-linear models seam-
lessly.
In order to test our results broadly, we used
four automatic translation evaluation metrics:
BLEU, NIST, multiple word error rate and po-
sition independent word error rate, to measure
the translation improvement.
In the following, in section 2 we introduce the
speech translation system. In section 3, we de-
scribe the optimization algorithm used to find
the weight parameters in the log-linear model.
In section 4 we demonstrate the effectiveness
of our technique in speech translation experi-
ments. In the final two sections we discuss the
results and present our conclusions.
2 Feature-based Log-linear Models
in Speech Translation
The speech translation experimental system
used in this study illustrated in Fig. 1 is a typi-
cal, statistics-based one. It consists of two ma-
jor cascaded components: an automatic speech
recognition (ASR) module and a statistical ma-
chine translation (SMT) module. Additionally,
a third module, ?Rescore?, has been added to the
system and it forms a key component in the sys-
tem. Features derived from ASR and SMT are
combined in this module to rescore translation
candidates.
Without loss of its generality, in this paper
we use Japanese-to-English translation to ex-
plain the generic speech translation process. Let
X denote acoustic observations of a Japanese
X
utterance
recognized
text
target
translation
E
best
translation
J N1
ASR SMT
E
NxK
1
Rescore
Figure 1: Current framework of speech transla-
tion
utterance, typically a sequence of short-time
spectral vectors received at a frame rate of ev-
ery centi-second. It is first recognized as a
Japanese sentence, J . The recognized sentence
is then translated into a corresponding English
sentence, E.
The conversion from X to J is performed in
the ASR module. Based on Bayes? rule, P (J |X)
can be written as
P (J |X) = Pam(X|J)Plm(J)/P (X)
where Pam(X|J) is the acoustic model likeli-
hood of the observations given the recognized
sentence J ; Plm(J), the source language model
probability; and P (X), the probability of all
acoustic observations.
In the experiment we generated a set of N -
best hypotheses, JN1 = {J1, J2, ? ? ? , JN} 1 and
each Ji is determined by
Ji = arg maxJ??i
Pam(X|J)Plm(J)
where ?i is the set of all possible source sen-
tences excluding all higher ranked Jk?s, 1 ? k ?
i ? 1.
The conversion from J to E in Fig. 1 is
the machine translation process. According
to the statistical machine translation formal-
ism (Brown et al, 1993), the translation process
is to search for the best sentence E? such that
E? = arg max
E
P (E|J) = arg max
E
P (J |E)P (E)
where P (J |E) is a translation model charac-
terizing the correspondence between E and J ;
P (E), the English language model probability.
In the IBM model 4, the translation model
P (J |E) is further decomposed into four sub-
models:
? Lexicon Model ? t(j|e): probability of a
word j in the Japanese language being
translated into a word e in the English lan-
guage.
1Hereafter, J1 is called the single-best hypothesis of
speech recognition; JN1 , the N -best hypotheses.
? Fertility model ? n(?|e): probability of
a English language word e generating ?
words.
? Distortion model ? d: probability of distor-
tion, which is decomposed into the distor-
tion probabilities of head words and non-
head words.
? NULL translation model ? p1: a fixed prob-
ability of inserting a NULL word after de-
termining each English word.
In the above we listed seven features: two
from ASR (Pam(X|J), Plm(J)) and five from
SMT (P (E), t(j|e), n(?|e), d, p1).
The third module in Fig. 1 is to rescore trans-
lation hypotheses from SMT by using a feature-
based log-linear model. All translation can-
didates output through the speech recognition
and translation modules are re-evaluated by us-
ing all relevant features and searching for the
best translation candidate of the highest score.
The log-linear model used in our speech trans-
lation process, P (E|X), is
P?(E|X) =
exp(?Mi=1 ?ifi(X,E))?
E? exp(
?M
i=1 ?ifi(X,E?))
? = {?M1 }
(1)
In the Eq. 1, fi(X,E) is the logarithm value
of the i-th feature; ?i is the weight of the i-
th feature. Integrating different features in the
equation results in different models. In the ex-
periments performed in section 4, four different
models will be trained by increasing the number
of features successively to investigate the effect
of different features for improving speech trans-
lation.
In addition to the above seven features, the
following features are also incorporated.
? Part-of-speech language models: English
part-of-speech language models were used.
POS dependence of a translated English
sentence is an effective constraint in prun-
ing English sentence candidates. In our ex-
periments 81 part-of-speech tags and a 5-
gram POS language model were used.
? Length model P (l|E, J): l is the length
(number of words) of a translated English
sentence.
? Jump weight: Jump width for adjacent
cepts in Model 4 (Marcu and Wong, 2002).
? Example matching score: The translated
English sentence is matched with phrase
translation examples. A score is derived
based on the count of matches (Watanabe
and Sumita, 2003).
? Dynamic example matching score: Similar
to the example matching score but phrases
were extracted dynamically from sentence
examples (Watanabe and Sumita, 2003).
Altogether, we used M(=12) different fea-
tures. In section 3, we review Powell?s algo-
rithm (Press et al, 2000) as our tool to opti-
mize model parameters, ?M1 , based on different
objective translation metrics.
3 Parameter Optimization Based
on Translation Metrics
The denominator in Eq. 1 can be ignored since
the normalization is applied equally to every hy-
pothesis. Hence, the choice of the best transla-
tion, E?, out of all possible translations, E, is
independent of the denominator,
E? = arg max
E
M?
i=1
?ilogPi(X,E) (2)
where we write features, fi(X,E), explicitly in
logarithm, logPi(X,E).
The effectiveness of the model in Eq. 2 de-
pends upon the parameter optimization of the
parameter set ?M1 , with respect to some objec-
tively measurable but subjectively relevant met-
rics.
Suppose we have L speech utterances and
for each utterance, we generate N best speech
recognition hypotheses. For each recogni-
tion hypothesis, K English language transla-
tion hypotheses are generated. For the l-th
input speech utterance, there are then Cl =
{El1 , ? ? ? , ElN?K} translations. All L speech ut-
terances generate L?N?K translations in to-
tal.
Our goal is to minimize the translation ?dis-
tortion? between the reference translations, R,
and the translated sentences, E? .
?M1 = optimize D(E? ,R) (3)
where E? = {E?1, ? ? ? , E?L} is a set of translations
of all utterances. The translation E?l of the l-
th utterance is produced by the (Eq. 2), where
E ? Cl.
Let R = {R1, ? ? ? , RL} be the set of transla-
tion references for all utterances. Human trans-
lators paraphrased 16 reference sentences for
each utterance, i.e., Rl contains 16 reference
candidates for the l-th utterance.
D(E? ,R) is a translation ?distortion? or an
objective translation assessment. The following
four metrics were used specifically in this study:
? BLEU (Papineni et al, 2002): A weighted
geometric mean of the n-gram matches be-
tween test and reference sentences multi-
plied by a brevity penalty that penalizes
short translation sentences.
? NIST : An arithmetic mean of the n-gram
matches between test and reference sen-
tences multiplied by a length factor which
again penalizes short translation sentences.
? mWER (Niessen et al, 2000): Multiple ref-
erence word error rate, which computes the
edit distance (minimum number of inser-
tions, deletions, and substitutions) between
test and reference sentences.
? mPER: Multiple reference position inde-
pendent word error rate, which computes
the edit distance without considering the
word order.
The BLEU score and NIST score are calcu-
lated by the tool downloadable 2.
Because the objective function in the model
(Eq. 3) is not smoothed function, we used Pow-
ell?s search method to find a solution. The Pow-
ell?s algorithm used in this work is similar as the
one from (Press et al, 2000) but we modified the
line optimization codes, a subroutine of Powell?s
algorithm, with reference to (Och, 2003).
Finding a global optimum is usually difficult
in a high dimensional vector space. To make
sure that we had found a good local optimum,
we restarted the algorithm by using various ini-
tializations and used the best local optimum as
the final solution.
4 Experiments
4.1 Corpus & System
The data used in this study was the Basic
Travel Expression Corpus (BTEC) (Kikui et al,
2003), consisting of commonly used sentences
listed in travel guidebooks and tour conversa-
tions. The corpus were designed for developing
multiple language speech-to-speech translation
systems. It contains four different languages:
Chinese, Japanese, Korean and English. Only
Japanese-English parallel data was used in this
2http://www.nist.gov/speech/tests/mt/
Table 1: Training, development and test data
from Basic Travel Expression Corpus(BTEC)
Japanese English
Train Sentences 162,318
Words 1,288,767 949,377
Dev. Sentences 510
Words 4015 2983
Test Sentences 508
Words 4112 2951
study. The speech data was recorded by multi-
ple speakers and was used to train the acoustic
models, while the text database was used for
training the language and translation models.
The standard BTEC training corpus, the first
file and the second file from BTEC standard test
corpus #01 were used for training, development
and test respectively. The statistics of corpus is
shown in table 1.
The speech recognition engine used in the ex-
periments was an HMM-based, large vocabu-
lary continuous speech recognizer. The acoustic
HMMs were triphone models with 2,100 states
in total, using 25 dimensional, short-time spec-
trum features. In the first and second pass of
decoding, a multiclass word bigram of a lexicon
of 37,000 words plus 10,000 compound words
was used. A word trigram was used in rescor-
ing the results.
The machine translation system is a graph-
based decoder (Ueffing et al, 2002). The first
pass of the decoder generates a word-graph, a
compact representation of alternative transla-
tion candidates, using a beam search based on
the scores of the lexicon and language mod-
els. In the second pass an A* search traverses
the graph. The edges of the word-graph, or
the phrase translation candidates, are gener-
ated by the list of word translations obtained
from the inverted lexicon model. The phrase
translations extracted from the Viterbi align-
ments of the training corpus also constitute the
edges. Similarly, the edges are also created from
dynamically extracted phrase translations from
the bilingual sentences (Watanabe and Sumita,
2003). The decoder used the IBM Model 4
with a trigram language model and a 5-gram
part-of-speech language model. The training of
IBM model 4 was implemented by the GIZA++
package (Och and Ney, 2003).
4.2 Model Training
In order to quantify translation improvement by
features from speech recognition and machine
translation respectively, we built four log-linear
models by adding features successively. The
four models are:
? Standard translation model(stm): Only
features from the IBM model 4 (M=5) de-
scribed in section 2 were used in the log-
linear models. We did not perform parame-
ter optimization on this model. It is equiv-
alent to setting all the ?M1 to 1. This model
was the standard model used in most sta-
tistical machine translation system. It is
referred to as the baseline model.
? Optimized standard translation models
(ostm): This model consists of the same
features as the previous model ?stm? but
the parameters were optimized by Powell?s
algorithm. We intended to exhibit the ef-
fect of parameter optimization by compar-
ing this model with the baseline ?stm?.
? Optimized enhanced translation models
(oetm): We incorporated additional trans-
lation features described in section 2 to
enrich the model ?ostm?. In this model
the number of the total features, M , is 10.
Model parameters were optimized. We in-
tended to show how much the enhanced
features can improve translation quality.
? Optimized enhanced speech translation
models (oestm): Features from speech
recognition, likelihood scores of acoustic
and language models, were incorporated
additionally into the model ?oetm?. All
the 12 features described in section 2 were
used. Model parameters were optimized.
To optimize ? parameters of the log-linear
models, we used the development data of 510
speech utterances. We adopted an N -best
hypothesis approach (Och, 2003) to train ?.
For each input speech utterance, N?K candi-
date translations were generated, where N is
the number of generated recognition hypothe-
ses and K is the number of translation hypothe-
ses. A vector of dimension M , corresponding to
multiple features used in the translation model,
was generated for each translation candidate.
The Powell?s algorithm was used to optimize
these parameters. We used a large K to ensure
that promising translation candidates were not
Table 2: Comparisons of single-best and N -best
hypotheses of speech recognition performance
in terms of word accuracy, sentence accuracy,
insertion, deletion and substitution error rates
word sent ins del sub
acc(%) acc(%) (%) (%) (%)
single-best 93.5 78.7 2.0 0.8 3.6
N -best 96.1 87.0 1.2 0.3 2.2
pruned out. In the training, we set N=100 and
K=1, 000.
By using different objective translation eval-
uation metrics described in section 3, for each
model we obtained four sets of optimized pa-
rameters with respect to BLEU, NIST, mWER
and mPER metrics, respectively.
4.3 Translation Improvement by
Additional Features
All 508 utterances in the test data were used to
evaluate the models. Similar to processing the
development data, the speech recognizer gen-
erated N -best (N=100) recognition hypothe-
ses for each test speech utterance. Table 2
shows speech recognition results of the test data
set in single-best and N -best hypotheses. We
observed that over 8% sentence accuracy im-
provement was obtained from the single-best to
the N -best recognition hypotheses. The recog-
nized sentences were then translated into corre-
sponding English sentences. 1,000 such trans-
lation candidates were produced for each recog-
nition hypothesis. These candidates were then
rescored by each of the four models with four
sets of optimized parameters obtained in the
training respectively. The candidates with the
best score were chosen.
The best translations generated by a model
were evaluated by the translation assessment
metrics used to optimize the model parameters
in the development. The experimental results
are shown in Table 3.
In the experiments we changed the number
of speech recognition hypotheses, N , to see how
translation performance is changed as N . We
found that the best translation was achieved
when a relatively smaller set of hypotheses,
N=5, was used. Hence, the values in Table 3
were obtained when N was set to 5.
We test each model by employing the single-
best recognition hypothesis translations and
the N -best recognition hypothesis translations.
Table 3: Translation improvement from the
baseline model(stm) to the optimized enhanced
speech translation model(oestm): Models are
optimized using the same metric as shown in
the columns. Numbers are in percentage except
NIST score.
BLEU NIST mWER mPER
Single-best recognition hypothesis translation
stm 54.2 7.5 39.8 34.8
ostm 59.0 8.9 36.2 34.0
oetm 59.2 9.9 34.3 31.5
N -best recognition hypothesis translation
stm 55.5 7.3 39.8 35.4
ostm 61.1 8.8 36.4 33.9
oetm 61.1 10.0 34.0 31.1
oestm 62.1 10.2 33.7 29.4
The single-best translation was from the trans-
lation of the single best hypotheses of the speech
recognition and the N -best hypothesis trans-
lation was from the translations of all the hy-
potheses produced by speech recognition.
In Table 3, we observe that a large improve-
ment is achieved from the baseline model ?stm?
to the final model ?oestm?. The BLEU, NIST,
mWER, mPER scores are improved by 7.9%,
2.7, 6.1%, 5.4% respectively. Note that a high
value of BLEU and NIST score means a good
translation while a worse translation for mWER
and mPER. Consistent performance improve-
ment was achieved in the single-best and N -
best recognition hypotheses translations. We
observed that the improvement were due to the
following reasons:
? Optimization. Models with optimized pa-
rameters yielded a better translation than
the models with unoptimized parameters.
It can be seen by comparing the model
?stm? with the model ?ostm? for both the
single-best and the N -best results.
? N -best recognition hypotheses. In major-
ity of the cells in Table 3, translation per-
formance of the N -best recognition is bet-
ter than of the corresponding single-best
recognition. N -best BLEU score of ?ostm?
improved over the single-best of ?ostm? by
2.1%. However, NIST score is indifferent
to the change. It appears that NIST score
is insensitive to detect slight translation
changes.
Table 4: Translation improvement of incorrectly
recognized utterances from single-best(oetm) to
N -best(oestm)
BLEU NIST mWER mPER
single-best 29.0 6.1 59.7 51.8
N -best 36.3 7.2 54.4 47.9
? Enhanced features. Translation perfor-
mance is improved steadily when more fea-
tures are incorporated into the log-linear
models. Translation performance of model
?oetm? is better than model ?ostm? be-
cause more effective translation features
are used. Model ?oestm? is better than
model ?oetm? due to its enhanced speech
recognition features. It confirms that our
approach to integrate features from speech
recognition and translation features works
very well.
4.4 Recognition Improvement of
Incorrectly Recognized Sentences
In previous experiments we demonstrated that
speech translation performance was improved
by the proposed enhanced speech translation
model ?oestm?. In this section we want to show
that this improvement is because of the signifi-
cant improvement of incorrectly recognized sen-
tences when N -best recognition hypotheses are
used.
We carried out the following experiments.
Only incorrectly recognized sentences were ex-
tracted for translation and re-scored by the
model ?oetm? for the single-best case and the
model ?oestm? for the N -best case. The trans-
lation results are shown in Table 4. Translation
of incorrectly recognized sentences are improved
significantly as shown in the table.
Because we used N -best recognition hypothe-
ses, the log-linear model chose the recogni-
tion hypothesis among the N hypotheses which
yielded the best translation. As a result, speech
recognition could be improved if the higher ac-
curate recognition hypotheses was chosen for
translation. This effect can be observed clearly
if we extracted the chosen recognition hypothe-
ses of incorrectly recognized sentences. Table 5
shows the word accuracy and sentence accuracy
of the recognition hypotheses selected by the
translation module. The sentence accuracy of
incorrectly recognized sentences was improved
by 7.5%. The word accuracy was also improved.
Table 5: Recognition accuracy of incorrectly
recognized utterance improved by N -best hy-
pothesis translation.
word acc. (%) sent. acc. (%)
single-best 74.6 0
N -best BLEU 76.4 7.5
mWER 75.9 6.5
5 Discussions
As regards to integrating speech recognition
with translation, a coupling structure (Ney,
1999) was proposed as a speech translation in-
frastructure that multiplies acoustic probabili-
ties with translation probabilities in a one-step
decoding procedure. But no experimental re-
sults have been given on whether and how this
coupling structure improved speech translation.
(Casacuberta et al, 2002) used a finite-state
transducer where scores from acoustic infor-
mation sources and lexicon translation models
were integrated together. Word pairs of source
and target languages were tied in the decoding
graph. However, this method was only tested
for a pair of similar languages, i.e., Spanish to
English. For translating between languages of
different families where the syntactic structures
can be quite different, like Japanese and En-
glish, rigid tying of word pair still remains to be
shown its effectiveness for translation.
Our approach is rather general, easy to imple-
ment and flexible to expand. In the experiments
we incorporated features from acoustic models
and language models. But this framework is
flexible to include more effective features. In-
deed, the proposed speech translation paradigm
of log-linear models have been shown effective in
many applications (Beyerlein, 1998) (Vergyri,
2000) (Och, 2003).
In order to use speech recognition features,
the N -best speech recognition hypotheses were
needed. Using N -best could bear computing
burden. However, our experiments have shown
a smaller N seems to be adequate to achieve
most of the translation improvement without
significant increasing of computations.
6 Conclusion
In this paper we presented our approach of in-
corporating both speech recognition and ma-
chine translation features into a log-linear
speech translation model to improve speech
translation.
Under this new approach, translation perfor-
mance was significantly improved. The perfor-
mance improvement was confirmed by consis-
tent experimental results and measured by us-
ing various objective translation metrics. In
particular, BLEU score was improved by 7.9%
absolute.
We show that features derived from speech
recognition: likelihood of acoustic and language
models, helped improve speech translation. The
N -best recognition hypotheses are better than
the single-best ones when they are used in trans-
lation. We also show that N -best recogni-
tion hypothesis translation can improve speech
recognition accuracy of incorrectly recognized
sentences.
The success of the experiments owes to the
use of statistical machine translation and log-
linear models so that various of effective fea-
tures can be jointed and balanced to output the
optimal translation results.
Acknowledgments
We would like to thank for assistance from Ei-
ichiro Sumita, Yoshinori Sagisaka, Seiichi Ya-
mamoto and the anonymous reviewers.
The research reported here was supported in
part by a contract with the National Institute
of Information and Communications Technol-
ogy of Japan entitled ?A study of speech dia-
logue translation technology based on a large
corpus?.
References
Peter Beyerlein. 1998. Discriminative model
combination. In Proc.of ICASSP?1998, vol-
ume 1, pages 481?484.
Peter F. Brown, Vincent J. Della Pietra,
Stephen A. Della Pietra, and Robert L. Mer-
cer. 1993. The mathematics of statistical
machine translation: Parameter estimation.
Computational Linguistics, 19(2):263?311.
Francisco Casacuberta, Enrique Vidal, and
Juan M. Vilar. 2002. Architectures for
speech-to-speech translation using finite-state
models. In Proc. of speech-to-speech trans-
lation workshop, pages 39?44, Philadelphia,
PA, July.
Genichiro Kikui, Eiichiro Sumita, Toshiyuki
Takezawa, and Seiichi Yamamoto. 2003. Cre-
ating corpora for speech-to-speech transla-
tion. In Proc.of EUROSPEECH?2003, pages
381?384, Geneva.
Daniel Marcu and William Wong. 2002. A
phrase-based, joint probability model for sta-
tistical machine translation. In Proc. of
EMNLP-2002, Philadelphia, PA, July.
Hermann Ney. 1999. Speech translation: Cou-
pling of recognition and translation. In Proc.
of ICASSP?1999, volume 1, pages 517?520,
Phoenix, AR, March.
Sonja Niessen, Franz J. Och, Gregor Leusch,
and Hermann Ney. 2000. An evaluation tool
for machine translation: Fast evaluation for
machine translation research. In Proc.of the
LREC (2000), pages 39?45, Athens, Greece,
May.
Franz Josef Och and Hermann Ney. 2003. A
systematic comparison of various statistical
alignment models. Computational Linguis-
tics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate
training in statistical machine translation. In
Proc. of ACL?2003, pages 160?167.
Kishore A. Papineni, Salim Roukos, Todd
Ward, and Wei-Jing Zhu. 2002. Bleu: A
method for automatic evaluation of machine
translation. In Proc. of ACL?2002, pages
311?318, Philadelphia, PA, July.
William H. Press, Saul A. Teukolsky,
William T. Vetterling, and Brian P. Flan-
nery. 2000. Numerical Recipes in C++.
Cambridge University Press, Cambridge,
UK.
Nicola Ueffing, Franz Josef Och, and Hermann
Ney. 2002. Generation of word graphs in sta-
tistical machine translation. In Proc. of the
Conference on Empirical Methods for Natu-
ral Language Processing (EMNLP02), pages
156?163, Philadelphia, PA, July.
Dimitra Vergyri. 2000. Use of word level side
information to improve speech recognition. In
Proc. of the IEEE International Conference
on Acoustics, Speech and Signal Processing,
2000.
Taro Watanabe and Eiichiro Sumita. 2003.
Example-based decoding for statistical ma-
chine translation. In Machine Translation
Summit IX, pages 410?417, New Orleans,
Louisiana.
 
	Automatic Measuring of English Language Proficiency using MT
Evaluation Technology
Keiji Yasuda
ATR Spoken Language Translation
Research Laboratories
Department of SLR
2-2-2 Hikaridai,
?Keihanna Science City?
Kyoto 619-0288 Japan
keiji.yasuda@atr.jp
Fumiaki Sugaya
KDDI R&D Laboratories
2-1-15, Ohara, Kamifukuoka-city,
Saitama, 356-8502, Japan
fsugaya@kddilabs.jp
Eiichiro Sumita
ATR Spoken Language Translation
Research Laboratories
Department of NLR
2-2-2 Hikaridai,
?Keihanna Science City?
Kyoto 619-0288 Japan
eiichiro.sumita@atr.jp
Toshiyuki Takezawa
ATR Spoken Language Translation
Research Laboratories
Department of SLR
2-2-2 Hikaridai,
?Keihanna Science City?
Kyoto 619-0288 Japan
toshiyuki.takezawa@atr.jp
Genichiro Kikui
ATR Spoken Language Translation
Research Laboratories
Department of SLR
2-2-2 Hikaridai,
?Keihanna Science City?
Kyoto 619-0288 Japan
genichiro.kikui@atr.jp
Seiichi Yamamoto
ATR Spoken Language Translation
Research Laboratories
2-2-2 Hikaridai,
?Keihanna Science City?
Kyoto 619-0288 Japan
seiichi.yamamoto@atr.jp
Abstract
Assisting in foreign language learning is one of
the major areas in which natural language pro-
cessing technology can contribute. This paper
proposes a computerized method of measuring
communicative skill in English as a foreign lan-
guage. The proposed method consists of two
parts. The first part involves a test sentence
selection part to achieve precise measurement
with a small test set. The second part is the ac-
tual measurement, which has three steps. Step
one asks proficiency-known human subjects to
translate Japanese sentences into English. Step
two gauges the match between the translations
of the subjects and correct translations based
on the n-gram overlap or the edit distance be-
tween translations. Step three learns the rela-
tionship between proficiency and match. By re-
gression it finds a straight-line fitting for the
scatter plot representing the proficiency and
matches of the subjects. Then, it estimates pro-
ficiency of proficiency-unknown users by using
the line and the match. Based on this approach,
we conducted experiments on estimating the
Test of English for International Communica-
tion (TOEIC) score. We collected two sets of
data consisting of English sentences translated
from Japanese. The first set consists of 330 sen-
tences, each translated to English by 29 subjects
with varied English proficiency. The second set
consists of 510 sentences translated in a similar
manner by a separate group of 18 subjects. We
found that the estimated scores correlated with
the actual scores.
1 Introduction
For effective second language learning, it is ab-
solutely necessary to test proficiency in the sec-
ond language. This testing can help in selecting
educational materials before learning, checking
learners? understanding after learning, and so
on.
To make learning efficient, it is important to
achieve testing with a short turnaround time.
Computer-based testing is one solution for this,
and several kinds of tests have been developed,
including CASEC (CASEC, 2004) and TOEFL-
CBT (TOEFL, 2004). However, these tests are
mainly based on cloze testing or multiple-choice
questions. Consequently, they require labour
costs for expert examination designers to make
the questions and the alternative ?detractor?
answers.
In this paper, we propose a method for the au-
tomatic measurement of English language pro-
ficiency by applying automatic evaluation tech-
niques. The proposed method selects adequate
test sentences from an existing corpus. Then,
it automatically evaluates the translations of
test sentences done by users. The core tech-
nology of the proposed method, i.e., the auto-
matic evaluation of translations, was developed
in research aiming at the efficient development
of Machine Translation (MT) technology (Su et
al., 1992; Papineni et al, 2002; NIST, 2002).
In the proposed method, we apply these MT
evaluation technologies to the measurement of
human English language proficiency. The pro-
posed method focuses on measuring the commu-
nicative skill of structuring sentences, which is
indispensable for writing and speaking. It does
not measure elementary capabilities including
vocabulary or grammar. This method also pro-
poses a test sentence selection scheme to enable
efficient testing.
Section 2 describes several automatic evalua-
tion methods applied to the proposed method.
Section 3 introduces the proposed evaluation
scheme. Section 4 shows the evaluation results
obtained by the proposed method. Section 5
concludes the paper.
2 MT Evaluation Technologies
In this section, we briefly describe automatic
evaluation methods of translation. These meth-
ods were proposed to evaluate MT output, but
they are applicable to translation by humans.
All of these methods are based on the same
idea, that is, to compare the target transla-
tion for evaluation with high-quality reference
translations that are usually done by skilled
translators. Therefore, these methods require a
corpus of high-quality human reference transla-
tions. We call these translations as ?references?.
2.1 DP-based Method
The DP score between a translation output and
references can be calculated by DP matching
(Su et al, 1992; Takezawa et al, 1999). First,
we define the DP score between sentence (i.e.,
word array) Wa and sentence Wb by the follow-
ing formula.
SDP (Wa,Wb) = T ? S ? I ?DT (1)
where T is the total number of words in Wa, S is
the number of substitution words for comparing
Wa to Wb, I is the number of inserted words for
comparing Wa to Wb, and D is the number of
deleted words for comparing Wa to Wb.
Using Equation 1, (Si(j)), that is, the test
sentence unit DP-score of the translation of test
sentence j done by subject i, can be calculated
by the following formula.
SDPi(j) =
max
k=1 to Nref
{
SDP (Wref(k)(j),Wsub(i)(j)), 0
}
(2)
where Nref is the number of references,
Wref(k)(j) is the k-th reference of the test sen-
tence j, and Wsub(i)(j) is the translation of the
test sentence j done by subject i.
Finally, SDPi , which is the test set unit DP-
score of subject i, can be calculated by the fol-
lowing formula.
SDPi =
1
Nsent
Nsent?
j=1
SDPi(j) (3)
where Nsent is the number of test sentences.
2.2 N-gram-based Method
Papineni et al (2002) proposed BLEU, which is
an automatic method for evaluating MT qual-
ity using N -gram matching. The National Insti-
tute of Standards and Technology also proposed
an automatic evaluation method called NIST
(2002), which is a modified method of BLEU.
In this research we use two kinds of units to
apply BLEU and NIST. One is a test sentence
unit and the other is a test set unit. The unit of
utterance corresponds to the unit of ?segment?
in the original BLEU and NIST studies (Pap-
ineni et al, 2002; NIST, 2002).
Equation 4 is the test sentence unit BLEU
score formulation of the translation of test sen-
tence j done by subject i.
SBLEUi (j) =
exp
{ N?
n=1
wn log(pn)?max
(L?ref
Lsys ? 1, 0
)}
(4)
where
pn =?
C?{Candidates}
?
n?gram?{C} Countclip (n?gram)?
C?{Candidates}
?
n?gram?{C} Count(n?gram)
wn = N?1
and
L?ref = the number of words in the reference
translation that is closest in length to the
translation being scored
Lsys = the number of words in the transla-
tion being scored
Equation 5 is the test sentence unit NIST
score formulation of the translation of test sen-
tence j done by subject i.
SNISTi(j) =
?N
n=1
{?
all w1...wn in sys output
info(w1...wn)?
all w1...wn in sys output
(1)
}
?exp
{
? log2
[
min
(
Lsys
Lref , 1
)]}
(5)
where
info(w1 . . . wn) =
log2
( the number of occurence of w1...wn?1
the number of occurence of w1...wn
)
Lref = the average number of words in a ref-
erence translation, averaged over all refer-
ence translations
Lsys = the number of words in the transla-
tion being scored
and ? is chosen to make the brevity penalty fac-
tor=0.5 when the number of words in the sys-
tem translation is 2/3 of the average number
of words in the reference translation. For Equa-
tions 4 and 7, N indicates the maximum n-gram
length. In this research we set N to 4 for BLEU
and to 5 for NIST.
We may consider the unit of the test set cor-
responding to the unit of ?document? or ?sys-
tem? in BLEU and NIST. However, we use for-
mulations for the test set unit scores that are
different from those of the original BLEU and
NIST.
Calculate correlation 
between TOEIC score and 
sentence unit automatic score
References translated
by bilinguals
English writing by 
proficiency-known
human subjects
English sentences
by proficiency
Japanese test set
Automatic evaluation
(sentence unit evaluation)
Corpus
Select test sentences
based on correlation
Figure 1: Flow of Test Set Selection
The test set unit scores of BLEU and NIST
are calculated by Equations 6 and 7.
SBLEUi =
1
Nsent
Nsent?
j=1
SBLEUi(j) (6)
SNISTi =
1
Nsent
Nsent?
j=1
SNISTi(j) (7)
3 The Proposed Method
The proposed method described in this paper
consists of two parts. One is the test set selec-
tion part and the other is the actual measure-
ment part. The measurement part is divided
into two phases: a parameter-estimation phase
and a testing phase. Here, we use the term ?sub-
jects? to refer to the human subjects in the test
set selection part and the parameter-estimation
phase of the measurement part; we use ?users?
to refer to the humans in the testing phase of
the measurement part.
Regression analysis using
proficiency and automatic
scores
References translated
by bilinguals
English writing by 
proficiency-known 
human subjects
English sentences
by proficiency
Japanese test set
Regression 
coefficient
Automatic evaluation
(Test set unit evaluation)
English writing by a user
Automatic evaluation
Estimation of English
proficiency
English sentences
Automatic score
English
proficiency
?Testing phase?
Corpus
?Parameter-estimation phase?
Figure 2: Flow of English Proficiency Measurment
We employ the Test of English for Interna-
tional Communication (TOEIC, 2004) as an ob-
jective measure of English proficiency.
3.1 Test Sentence Selection Method
Figure 1 shows the flow of the test sentence se-
lection. We first calculate the test sentence
unit automatic score by using Equation 2, 4 or
5 for each test sentence and subject. Second,
for each test sentence, we calculate the correla-
tion between the automatic scores and subjects?
TOEIC scores. Finally, using the above results,
we choose the test sentences that give high cor-
relation.
3.2 Method of Measuring English
Proficiency
Figure 2 shows the flow of measuring English
proficiency. In the parameter-estimation phase,
for each subject, we first calculate the test set
unit automatic score by using Equation 3, 6 or
7. Next, we apply regression analysis using the
automatic scores and subjects? TOEIC scores.
In the testing phase, we calculate a user?s
TOEIC score using the automatic score of the
user and the regression line calculated in the
parameter-estimation phase.
4 Experiments
4.1 Experimental Conditions
4.1.1 Test sets
For the experiments, we employ two differ-
ent test sets. One is BTEC (Basic Travel
Expression Corpus) (Takezawa et al, 2002)
and the other is SLTA1 (Takezawa, 1999).
Both BTEC and SLTA1 are parts of bilingual
corpora that have been collected for research
on speech translation systems. However, they
have different features. A detailed analysis
of these corpora was done by Kikui et al
(2003). Here, we briefly explain these test sets.
In this study, we use the Japanese side as a
test set and the English side as a reference for
automatic evaluation.
BTEC
BTEC was designed to cover expressions for
every potential subject in travel conversation.
This test set was collected by investigating
?phrasebooks? that contain Japanese/English
sentence pairs that experts consider useful for
tourists traveling abroad. One sentence con-
tains 8 words on average. The test set for this
experiment consists of 510 sentences from the
BTEC corpus.
The total number of examinees is 18, and
the range of their TOEIC scores is between the
400s and 900s. Every hundred-point range has
3 examinees.
SLTA1
SLTA1 consists of 330 sentences in 23 conver-
sations from the ATR bilingual travel conver-
sation database (Takezawa, 1999). One sen-
tence contains 13 words on average. This corpus
was collected by simulated dialogues between
Japanese and English speakers through a pro-
fessional interpreter. The topics of the conver-
sations are mainly hotel conversations, such as
reservations, enquiries and so on.
The total number of examinees is 29, and the
range of their TOEIC score is between the 300s
and 800s. Excluding the 600s, every hundred-
point range has 5 examinees.
4.1.2 Reference
For the automatic evaluation, we collected 16
references for each test sentence. One of them
is from the English side of the test set, and the
remaining 15 were translated by 5 bilinguals (3
references by 1 bilingual).
4.2 Experimental Results
4.2.1 Experimental Results of Test Set
Selection
Figures 3 and 4 show the correlation between
the test sentence unit automatic score and the
subjects? TOEIC score. Here, the automatic
score is calculated using Equation 2, 4 or 5. Fig-
ure 3 shows the results on BTEC, and Fig. 4
shows the results on SLTA1. In these fig-
ures, the ordinate represents the correlation.
The filled circles indicate the results using the
DP-based automatic evaluation method. The
gray circles indicate the results using BLEU.
The empty circles indicate the results using
NIST. Looking at these figures, we find that
the three automatic evaluation methods show
a similar tendency. Comparing BTEC and
SLTA1, BTEC contains more cumbersome test
sentences. In BTEC, about 20% of the test sen-
tences give a correlation of less than 0. Mean-
while, in the SLTA1, this percentage is about
10%.
-1
-0.8
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
0.8
1
0 30 60 90 120 150 180 210 240 270 300 330 360 390 420 450 480 510
Test sentence (sorted by correlation)
C
o
r
r
e
l
a
t
i
o
n
DP
BLEU
NIST
Figure 3: Correlation between test sentence unit
automatic scores and subjects? TOEIC scores
(BTEC)
Table 1 shows examples of low-correlated test
sentences. As shown in the table, BTEC con-
tains more short and frequently used expres-
sions than does SLTA1. This kind of expres-
sion is thought to be too easy for testing, so
this low-correlation phenomenon is thought to
occur. SLTA1 still contains a few sentences of
this kind (?Example 1? of SLTA1 in the ta-
ble). Additionally, there is another contributing
factor explaining the low correlation in SLTA1.
Looking at ?Example 2? of SLTA1 in the ta-
ble, this expression is not very easy to translate.
For this test sentence, several expressions can
be produced as an English translation. Thus,
automatic evaluation methods cannot evaluate
correctly due to the insufficient variety of ref-
erences. Considering these results, this method
can remove inadequate test sentences due not
only to the easiness of the test sentence but
also to the difficulty of the automatic evalua-
tion. Figures 5 and 6 show the relationship
between the number of test sentences and cor-
relation. This correlation is calculated between
the test set unit automatic scores and the sub-
jects? TOEIC scores. Here, the automatic score
is calculated using Equation 3, 6 or 7. Figure
5 shows the results on BTEC, and Fig. 6 shows
the results on SLTA1.
In these figures, the abscissa represents the
number of test sentences, i.e., Nsent in Equa-
tions 3, 6 and 7, and the ordinate represents
the correlation. Definitions of the circles are
the same as those in the previous figure. Here,
the test sentence selection is based on the cor-
relation shown in Figs. 3 and 4.
Comparing Fig. 5 to Fig. 6, in the case of
Table 1: Example of low-correlated test sentences
Japanese English
Example 1
???????
Good night.
Example 2
????????????
Can I see a menu, please?
Example 1
??????????????????
Yes, with my Mastercard please
Example 2
???????????????????????
????????????????????????
I wish I could take that but we have a limited budget so
how much will that cost?
S
L
T
A
1
B
T
E
C
-1
-0.8
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
0.8
1
0 30 60 90 120 150 180 210 240 270 300 330
Test sentence (sorted by correlation)
C
o
r
r
e
l
a
t
i
o
n
DP
BLEU
NIST
Figure 4: Correlation between test sentence unit
automatic scores and subjects? TOEIC scores
(SLTA1)
0.6
0.65
0.7
0.75
0.8
0.85
0.9
0.95
1
0 30 60 90 120 150 180 210 240 270 300 330 360 390 420 450 480 510
Number of test sentences
C
o
r
r
e
l
a
t
i
o
n
DP
BLEU
NIST
Figure 5: Correlation between test set unit
automatic scores and subjects? TOEIC scores
(BTEC)
using the full test set (510 test sentences for
BTEC, 330 test sentences for SLTA1), the cor-
relation of BTEC is lower than that of SLTA1.
As we mentioned above, the ratio of the low-
correlated test sentences in BTEC is higher than
that of SLTA1 (See Figs. 3 and 4). This issue
is thought to cause a decrease in the correlation
shown in Fig. 5. However, by applying the se-
0.6
0.65
0.7
0.75
0.8
0.85
0.9
0.95
1
0 30 60 90 120 150 180 210 240 270 300 330
Number of test sentences
C
o
r
r
e
l
a
t
i
o
n
DP
BLEU
NIST
Figure 6: Correlation between test set unit
automatic scores and subjects? TOEIC scores
(SLTA1)
50
100
150
200
250
300
350
0 30 60 90 120 150 180 210 240 270 300 330 360 390 420 450 480 510
Number of test sentences
S
t
a
n
d
a
r
d
 
e
r
r
o
r
DP
BLEU
NIST
Figure 7: Standard error (BTEC)
lection based on sentence unit correlation, these
obstructive test sentences can be removed. This
permits the selection of high-correlated small-
sized test sets. In these figures, the highest cor-
relations are around 0.95.
4.2.2 Experimental Results of English
Proficiency Measurement
For the experiments on English proficiency mea-
surement, we carried out a leave-one-out cross
validation test. The leave-one-out cross valida-
50
100
150
200
250
300
350
0 30 60 90 120 150 180 210 240 270 300 330
Number of test sentences
S
t
a
n
d
a
r
d
 
e
r
r
o
r
DP
BLEU
NIST
Figure 8: Standard error (SLTA1)
tion test is conducted not only for the measure-
ment of the English proficiency but also for the
test set selection.
To evaluate the proficiency measurement by
the proposed method, we calculate the standard
error of the results of a leave-one-out cross val-
idation test. The following formula is the defi-
nition of the standard error.
?E =
???? 1
Nuser
Nuser?
i=1
(Ti ?Ai)2 (8)
where Nuser is the number of users, Ti is the
actual TOEIC score of user i, and Ai is user i?s
estimated TOEIC score by using the proposed
method.
Figures 7 and 8 show the relationship between
the number of test sentences and the standard
error.
In these figures, the abscissa represents the
number of test sentences, and the ordinate rep-
resents the standard error. Definitions of the
circles are the same as in the previous figure.
Here, the test sentence selection is based on the
correlation shown in Figs. 3 and 4.
Looking at Figs. 7 and 8, we can observe dif-
ferences between the standard errors of BTEC
and SLTA1. This is thought to be due to the
difference of the number of subjects in the ex-
periments (for the leave-one-out cross valida-
tion test, 17 subjects with BTEC and 28 sub-
jects with SLTA1). Even though these were
closed experiments, the results in Figs. 5 and
6 show an even higher correlation with BTEC
than with SLTA1 at the highest point. There-
fore, there is room for improvement by increas-
ing the number of subjects with BTEC.
In the test using 30 to 60 test sentences in
Figs. 7 and 8, the standard errors are much
smaller than in the test using the full test set
(510 test sentences for BTEC, 330 test sentences
for SLTA1). These results imply that the test
set selection works very well and that it enables
precise testing using a smaller size test set.
5 Conclusion
We proposed an automatic measurement
method for English language proficiency. The
proposed method applies automatic MT evalu-
ation to measure human English language pro-
ficiency. This method focuses on measuring the
communicative skill of structuring sentences,
which is indispensable in writing and speaking.
However, it does not measure elementary capa-
bilities such as vocabulary and grammar. The
method also involves a new test sentence selec-
tion scheme to enable efficient testing.
In the experiments, we used TOEIC as an ob-
jective measure of English language proficiency.
We then applied some currently available auto-
matic evaluation methods: BLEU, NIST and a
DP-based method. We carried out experiments
on two test sets: BTEC and SLTA1. Accord-
ing to the experimental results, the proposed
method gave a good measurement result on a
small-sized test set. The standard error of mea-
surement is around 120 points on the TOEIC
score with BTEC and less than 100 TOEIC
points score with SLTA1. In both cases, the
optimum size of the test set is 30 to 60 test sen-
tences.
The proposed method still needs human
labour to make the references. To obtain higher
portability, we will apply an automatic para-
phrase scheme (Finch et al, 2002; Shimohata
and Sumita, 2002) to make the references auto-
matically.
6 Acknowledgements
The research reported here was supported in
part by a contract with the National Institute
of Information and Communications Technol-
ogy entitled ?A study of speech dialogue trans-
lation technology based on a large corpus?.
References
CASEC. 2004. Computer Assessment
System for English Communication.
http://www.ets.org/toefl/.
A. Finch, T. Watanabe, and E. Sumita. 2002.
?Paraphrasing by Statistical Machine Trans-
lation?. In Proceedings of the 1st Forum on
Information Technology (FIT2002), volume
E-53, pages 187?188.
G. Kikui, E. Sumita, T. Takezawa, and
S. Yamamoto. 2003. ?Creating Corpora for
Speech-to-Speech Translation?. In Proceed-
ings of EUROSPEECH, pages 381?384.
NIST. 2002. Automatic Evaluation
of Machine Translation Quality Us-
ing N-gram Co-Occurence Statistics.
http://www.nist.gov/speech/tests/mt
/mt2001/resource/.
K. Papineni, S. Roukos, T. Ward, and W.-
J. Zhu. 2002. Bleu: a method for auto-
matic evaluation of machine translation. In
Proceedings of the 40th Annual Meeting of
the Association for Computational Linguis-
tics (ACL), pages 311?318.
M. Shimohata and E. Sumita. 2002. ?Auto-
matic Paraphrasing Based on Parallel Corpus
for Normalization?. In Proceedings of Inter-
national Conference on Language Resources
and Evaluation (LREC), pages 453?457.
K.-Y. Su, M.-W. Wu, and J.-S. Chang. 1992.
A new quantitative quality measure for ma-
chine translation systems. In Proceedings of
the 14th International Conference on Com-
putational Linguistics(COLING), pages 433?
439.
T. Takezawa, F. Sugaya, A. Yokoo, and S. Ya-
mamoto. 1999. A new evaluation method for
speech translation systems and a case study
on ATR-MATRIX from Japanese to English.
In Proceeding of Machine Translation Summit
(MT Summit), pages 299?307.
T. Takezawa, E. Sumita, F. Sugaya, H. Ya-
mamoto, and S. Yamamoto. 2002. ?Toward a
Broad-Coverage Bilingual Corpus for Speech
Translation of Travel Conversations in the
Real World?. In Proceedings of International
Conference on Language Resources and Eval-
uation (LREC), pages 147?152.
T. Takezawa. 1999. Building a bilingual travel
conversation database for speech translation
research. In Proceedings of the 2nd Inter-
national Workshop on East-Asian Language
Resources and Evaluation ? Oriental CO-
COSDA Workshop ?99 ?, pages 17?20.
TOEFL. 2004. Test of English as a Foreign
Language. http://www.ets.org/toefl/.
TOEIC. 2004. Test of English
for International Communication.
http://www.ets.org/toeic/.
Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 193?196,
New York, June 2006. c?2006 Association for Computational Linguistics
Subword-based Tagging by Conditional Random Fields for Chinese Word
Segmentation
Ruiqiang Zhang1,2 and Genichiro Kikui? and Eiichiro Sumita1,2
1National Institute of Information and Communications Technology
2ATR Spoken Language Communication Research Laboratories
2-2-2 Hikaridai, Seiika-cho, Soraku-gun, Kyoto, 619-0288, Japan
{ruiqiang.zhang,eiichiro.sumita}@atr.jp
Abstract
We proposed two approaches to improve Chi-
nese word segmentation: a subword-based tag-
ging and a confidence measure approach. We
found the former achieved better performance
than the existing character-based tagging, and
the latter improved segmentation further by
combining the former with a dictionary-based
segmentation. In addition, the latter can be
used to balance out-of-vocabulary rates and
in-vocabulary rates. By these techniques we
achieved higher F-scores in CITYU, PKU and
MSR corpora than the best results from Sighan
Bakeoff 2005.
1 Introduction
The character-based ?IOB? tagging approach has been
widely used in Chinese word segmentation recently (Xue
and Shen, 2003; Peng and McCallum, 2004; Tseng
et al, 2005). Under the scheme, each character of a
word is labeled as ?B? if it is the first character of a
multiple-character word, or ?O? if the character func-
tions as an independent word, or ?I? otherwise.? For ex-
ample, ? (whole) (Beijing city)? is labeled as
? (whole)/O (north)/B (capital)/I (city)/I?.
We found that so far all the existing implementations
were using character-based IOB tagging. In this work
we propose a subword-based IOB tagging, which as-
signs tags to a pre-defined lexicon subset consisting of
the most frequent multiple-character words in addition to
single Chinese characters. If only Chinese characters are
used, the subword-based IOB tagging is downgraded into
a character-based one. Taking the same example men-
tioned above, ? (whole) (Beijing city)? is la-
beled as ? (whole)/O (Beijing)/B (city)/I? in the
subword-based tagging, where ? (Beijing)/B? is la-
beled as one unit. We will give a detailed description of
this approach in Section 2.
? Now the second author is affiliated with NTT.
In addition, we found a clear weakness with the IOB
tagging approach: It yields a very low in-vocabulary (IV)
rate (R-iv) in return for a higher out-of-vocabulary (OOV)
rate (R-oov). In the results of the closed test in Bakeoff
2005 (Emerson, 2005), the work of (Tseng et al, 2005),
using conditional random fields (CRF) for the IOB tag-
ging, yielded very high R-oovs in all of the four corpora
used, but the R-iv rates were lower. While OOV recog-
nition is very important in word segmentation, a higher
IV rate is also desired. In this work we propose a confi-
dence measure approach to lessen the weakness. By this
approach we can change R-oovs and R-ivs and find an
optimal tradeoff. This approach will be described in Sec-
tion 2.2.
In the followings, we illustrate our word segmentation
process in Section 2, where the subword-based tagging is
implemented by the CRFs method. Section 3 presents our
experimental results. Section 4 describes current state-
of-the-art methods for Chinese word segmentation, with
which our results were compared. Section 5 provides the
concluding remarks.
2 Our Chinese word segmentation process
Our word segmentation process is illustrated in Fig. 1. It
is composed of three parts: a dictionary-based N-gram
word segmentation for segmenting IV words, a subword-
based tagging by the CRF for recognizing OOVs, and a
confidence-dependent word segmentation used for merg-
ing the results of both the dictionary-based and the IOB
tagging. An example exhibiting each step?s results is also
given in the figure.
Since the dictionary-based approach is a well-known
method, we skip its technical descriptions. However,
keep in mind that the dictionary-based approach can pro-
duce a higher R-iv rate. We will use this advantage in the
confidence measure approach.
2.1 Subword-based IOB tagging using CRFs
There are several steps to train a subword-based IOB tag-
ger. First, we extracted a word list from the training data
sorted in decreasing order by their counts in the training
193
????????
+XDQJ<LQJ&KXQOLYHVLQ%HLMLQJFLW\
input
????????
+XDQJ<LQJ&KXQOLYHVLQ%HLMLQJFLW\
Dictionary-based word segmentation
?%?,?,?2?2??%?,
+XDQJ%<LQJ,&KXQ,OLYHV2LQ2%HLMLQJ%FLW\,
Subword-based IOB tagging
?%?,?,?2?2??%?,
+XDQJ%<LQJ,&KXQ,OLYHV2LQ2%HLMLQJ%FLW\,
Confidence-based segmentation
????????
+XDQJ<LQJ&KXQOLYHVLQ%HLMLQJFLW\
output
Figure 1: Outline of word segmentation process
data. We chose all the single characters and the top multi-
character words as a lexicon subset for the IOB tagging.
If the subset consists of Chinese characters only, it is a
character-based IOB tagger. We regard the words in the
subset as the subwords for the IOB tagging.
Second, we re-segmented the words in the training
data into subwords belonging to the subset, and assigned
IOB tags to them. For a character-based IOB tagger,
there is only one possibility of re-segmentation. How-
ever, there are multiple choices for a subword-based
IOB tagger. For example, ? (Beijing-city)? can
be segmented as ? (Beijing-city)/O,? or ?
(Beijing)/B (city)/I,? or ? (north)/B (capital)/I
(city)/I.? In this work we used forward maximal match
(FMM) for disambiguation. Of course, backward max-
imal match (BMM) or other approaches are also appli-
cable. We did not conduct comparative experiments be-
cause trivial differences of these approaches may not re-
sult in significant consequences to the subword-based ap-
proach.
In the third step, we used the CRFs approach to train
the IOB tagger (Lafferty et al, 2001) on the training data.
We downloaded and used the package ?CRF++? from the
site ?http://www.chasen.org/t?aku/software.? According to
the CRFs, the probability of an IOB tag sequence, T =
t0t1 ? ? ? tM , given the word sequence, W = w0w1 ? ? ?wM , is
defined by
p(T |W) =
exp
?
?
?
?
?
?
?
M
?
i=1
?
?
?
?
?
?
?
?
k
?k fk(ti?1, ti,W) +
?
k
?kgk(ti,W)
?
?
?
?
?
?
?
?
?
?
?
?
?
?
/Z,
Z =
?
T=t0t1???tM
p(T |W)
(1)
where we call fk(ti?1, ti,W) bigram feature functions be-
cause the features trigger the previous observation ti?1
and current observation ti simultaneously; gk(ti,W), the
unigram feature functions because they trigger only cur-
rent observation ti. ?k and ?k are the model parameters
corresponding to feature functions fk and gk respectively.
The model parameters were trained by maximizing the
log-likelihood of the training data using L-BFGS gradi-
ent descent optimization method. In order to overcome
overfitting, a gaussian prior was imposed in the training.
The types of unigram features used in our experiments
included the following types:
w0,w?1,w1,w?2,w2,w0w?1,w0w1,w?1w1,w?2w?1,w2w0
where w stands for word. The subscripts are position in-
dicators. 0 means the current word; ?1, ?2, the first or
second word to the left; 1, 2, the first or second word to
the right.
For the bigram features, we only used the previous and
the current observations, t?1t0.
As to feature selection, we simply used absolute counts
for each feature in the training data. We defined a cutoff
value for each feature type and selected the features with
occurrence counts over the cutoff.
A forward-backward algorithm was used in the train-
ing and viterbi algorithm was used in the decoding.
2.2 Confidence-dependent word segmentation
Before moving to this step in Figure 1, we produced two
segmentation results: the one by the dictionary-based ap-
proach and the one by the IOB tagging. However, nei-
ther was perfect. The dictionary-based segmentation pro-
duced results with higher R-ivs but lower R-oovs while
the IOB tagging yielded the contrary results. In this sec-
tion we introduce a confidence measure approach to com-
bine the two results. We define a confidence measure,
CM(tiob|w), to measure the confidence of the results pro-
duced by the IOB tagging by using the results from the
dictionary-based segmentation. The confidence measure
comes from two sources: IOB tagging and dictionary-
based word segmentation. Its calculation is defined as:
CM(tiob|w) = ?CMiob(tiob|w) + (1 ? ?)?(tw, tiob)ng (2)
where tiob is the word w?s IOB tag assigned by the IOB
tagging; tw, a prior IOB tag determined by the results of
the dictionary-based segmentation. After the dictionary-
based word segmentation, the words are re-segmented
into subwords by FMM before being fed to IOB tagging.
Each subword is given a prior IOB tag, tw. CMiob(t|w), a
confidence probability derived in the process of IOB tag-
ging, is defined as
CMiob(t|wi) =
?
T=t0t1???tM ,ti=t P(T |W,wi)
?
T=t0t1???tM P(T |W)
where the numerator is a sum of all the observation se-
quences with word wi labeled as t.
194
?(tw, tiob)ng denotes the contribution of the dictionary-
based segmentation. It is a Kronecker delta function de-
fined as
?(tw, tiob)ng = {
1 if tw = tiob
0 otherwise
In Eq. 2, ? is a weighting between the IOB tagging
and the dictionary-based word segmentation. We found
the value 0.7 for ?, empirically.
By Eq. 2 the results of IOB tagging were re-evaluated.
A confidence measure threshold, t, was defined for mak-
ing a decision based on the value. If the value was lower
than t, the IOB tag was rejected and the dictionary-based
segmentation was used; otherwise, the IOB tagging seg-
mentation was used. A new OOV was thus created. For
the two extreme cases, t = 0 is the case of the IOB tag-
ging while t = 1 is that of the dictionary-based approach.
In a real application, a satisfactory tradeoff between R-
ivs and R-oovs could find through tuning the confidence
threshold. In Section 3.2 we will present the experimental
segmentation results of the confidence measure approach.
3 Experiments
We used the data provided by Sighan Bakeoff 2005 to
test our approaches described in the previous sections.
The data contain four corpora from different sources:
Academia Sinica (AS), City University of Hong Kong
(CITYU), Peking University (PKU) and Microsoft Re-
search in Beijing (MSR). Since this work was to evaluate
the proposed subword-based IOB tagging, we carried out
the closed test only. Five metrics were used to evaluate
segmentation results: recall(R), precision(P), F-score(F),
OOV rate(R-oov) and IV rate(R-iv). For detailed info. of
the corpora and these scores, refer to (Emerson, 2005).
For the dictionary-based approach, we extracted a
word list from the training data as the vocabulary. Tri-
gram LMs were generated using the SRI LM toolkit for
disambiguation. Table 1 shows the performance of the
dictionary-based segmentation. Since there were some
single-character words present in the test data but not in
the training data, the R-oov rates were not zero in this
experiment. In fact, there were no OOV recognition.
Hence, this approach produced lower F-scores. However,
the R-ivs were very high.
3.1 Effects of the Character-based and the
subword-based tagger
The main difference between the character-based and the
word-based is the contents of the lexicon subset used
for re-segmentation. For the character-based tagging, we
used all the Chinese characters. For the subword-based
tagging, we added another 2000 most frequent multiple-
character words to the lexicons for tagging. The segmen-
tation results of the dictionary-based were re-segmented
R P F R-oov R-iv
AS 0.941 0.881 0.910 0.038 0.982
CITYU 0.928 0.851 0.888 0.164 0.989
PKU 0.948 0.912 0.930 0.408 0.981
MSR 0.968 0.927 0.947 0.048 0.993
Table 1: Our segmentation results by the dictionary-
based approach for the closed test of Bakeoff 2005, very
low R-oov rates due to no OOV recognition applied.
R P F R-oov R-iv
AS 0.951 0.942 0.947 0.678 0.964
0.953 0.940 0.947 0.647 0.967
CITYU 0.939 0.943 0.941 0.700 0.958
0.950 0.942 0.946 0.736 0.967
PKU 0.940 0.950 0.945 0.783 0.949
0.943 0.946 0.945 0.754 0.955
MSR 0.957 0.960 0.959 0.710 0.964
0.965 0.963 0.964 0.716 0.972
Table 2: Segmentation results by a pure subword-based
IOB tagging. The upper numbers are of the character-
based and the lower ones, the subword-based.
using the FMM, and then labeled with ?IOB? tags by the
CRFs. The segmentation results using CRF tagging are
shown in Table 2, where the upper numbers of each slot
were produced by the character-based approach while the
lower numbers were of the subword-based. We found
that the proposed subword-based approaches were effec-
tive in CITYU and MSR corpora, raising the F-scores
from 0.941 to 0.946 for CITYU corpus, 0.959 to 0.964 for
MSR corpus. There were no F-score changes for AS and
PKU corpora, but the recall rates were improved. Com-
paring Table 1 and 2, we found the CRF-modeled IOB
tagging yielded better segmentation than the dictionary-
based approach. However, the R-iv rates were getting
worse in return for higher R-oov rates. We will tackle
this problem by the confidence measure approach.
3.2 Effect of the confidence measure
In section 2.2, we proposed a confidence measure ap-
proach to re-evaluate the results of IOB tagging by com-
binations of the results of the dictionary-based segmen-
tation. The effect of the confidence measure is shown in
Table 3, where we used ? = 0.7 and confidence threshold
t = 0.8. In each slot, the numbers on the top were of the
character-based approach while the numbers on the bot-
tom were the subword-based. We found the results in Ta-
ble 3 were better than those in Table 2 and Table 1, which
prove that using confidence measure approach achieved
the best performance over the dictionary-based segmen-
tation and the IOB tagging approach. The act of con-
fidence measure made a tradeoff between R-ivs and R-
oovs, yielding higher R-oovs than Table 1 and higher R-
195
R P F R-oov R-iv
AS 0.953 0.944 0.948 0.607 0.969
0.956 0.947 0.951 0.649 0.969
CITYU 0.943 0.948 0.946 0.682 0.964
0.952 0.949 0.951 0.741 0.969
PKU 0.942 0.957 0.949 0.775 0.952
0.947 0.955 0.951 0.748 0.959
MSR 0.960 0.966 0.963 0.674 0.967
0.972 0.969 0.971 0.712 0.976
Table 3: Effects of combination using the confidence
measure. The upper numbers and the lower numbers are
of the character-based and the subword-based, respec-
tively
AS CITYU MSR PKU
Bakeoff-best 0.952 0.943 0.964 0.950
Ours 0.951 0.951 0.971 0.951
Table 4: Comparison our results with the best ones from
Sighan Bakeoff 2005 in terms of F-score
ivs than Table 2.
Even with the use of confidence measure, the word-
based IOB tagging still outperformed the character-based
IOB tagging. It proves the proposed word-based IOB tag-
ging was very effective.
4 Discussion and Related works
The IOB tagging approach adopted in this work is not a
new idea. It was first used in Chinese word segmentation
by (Xue and Shen, 2003), where maximum entropy meth-
ods were used. Later, this approach was implemented
by the CRF-based method (Peng and McCallum, 2004),
which was proved to achieve better results than the maxi-
mum entropy approach because it can solve the label bias
problem (Lafferty et al, 2001).
Our main contribution is to extend the IOB tagging ap-
proach from being a character-based to a subword-based.
We proved the new approach enhanced the word segmen-
tation significantly. Our results are listed together with
the best results from Bakeoff 2005 in Table 4 in terms
of F-scores. We achieved the highest F-scores in CITYU,
PKU and MSR corpora. We think our proposed subword-
based tagging played an important role for the good re-
sults. Since it was a closed test, some information such
as Arabic and Chinese number and alphabetical letters
cannot be used. We could yield a better results than those
shown in Table 4 using such information. For example,
inconsistent errors of foreign names can be fixed if al-
phabetical characters are known. For AS corpus, ?Adam
Smith? are two words in the training but become a one-
word in the test, ?AdamSmith?. Our approaches pro-
duced wrong segmentations for labeling inconsistency.
Another advantage of the word-based IOB tagging
over the character-based is its speed. The subword-based
approach is faster because fewer words than characters
were labeled. We found a speed up both in training and
test.
The idea of using the confidence measure has appeared
in (Peng and McCallum, 2004), where it was used to rec-
ognize the OOVs. In this work we used it more delicately.
By way of the confidence measure we combined results
from the dictionary-based and the IOB-tagging-based and
as a result, we could achieve the optimal performance.
5 Conclusions
In this work, we proposed a subword-based IOB tagging
method for Chinese word segmentation. Using the CRFs
approaches, we prove that it outperformed the character-
based method using the CRF approaches. We also suc-
cessfully employed the confidence measure to make a
confidence-dependent word segmentation. This approach
is effective for performing desired segmentation based on
users? requirements to R-oov and R-iv.
Acknowledgements
The authors appreciate the reviewers? effort and good ad-
vice for improving the paper.
References
Thomas Emerson. 2005. The second international chi-
nese word segmentation bakeoff. In Proceedings of
the Fourth SIGHAN Workshop on Chinese Language
Processing, Jeju, Korea.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: probabilistic models
for segmenting and labeling sequence data. In Proc. of
ICML-2001, pages 591?598.
Fuchun Peng and Andrew McCallum. 2004. Chinese
segmentation and new word detection using condi-
tional random fields. In Proc. of Coling-2004, pages
562?568, Geneva, Switzerland.
Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel
Jurafsky, and Christopher Manning. 2005. A condi-
tional random field word segmenter for Sighan bakeoff
2005. In Proceedings of the Fourth SIGHANWorkshop
on Chinese Language Processing, Jeju, Korea.
Nianwen Xue and Libin Shen. 2003. Chinese word
segmentation as LMR tagging. In Proceedings of the
Second SIGHAN Workshop on Chinese Language Pro-
cessing.
196
Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 961?968,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Subword-based Tagging for Confidence-dependent Chinese Word
Segmentation
Ruiqiang Zhang1,2 and Genichiro Kikui? and Eiichiro Sumita1,2
1National Institute of Information and Communications Technology
2ATR Spoken Language Communication Research Laboratories
2-2-2 Hikaridai, Seiika-cho, Soraku-gun, Kyoto, 619-0288, Japan
{ruiqiang.zhang,eiichiro.sumita}@atr.jp
Abstract
We proposed a subword-based tagging for
Chinese word segmentation to improve
the existing character-based tagging. The
subword-based tagging was implemented
using the maximum entropy (MaxEnt)
and the conditional random fields (CRF)
methods. We found that the proposed
subword-based tagging outperformed the
character-based tagging in all compara-
tive experiments. In addition, we pro-
posed a confidence measure approach to
combine the results of a dictionary-based
and a subword-tagging-based segmenta-
tion. This approach can produce an
ideal tradeoff between the in-vocaulary
rate and out-of-vocabulary rate. Our tech-
niques were evaluated using the test data
from Sighan Bakeoff 2005. We achieved
higher F-scores than the best results in
three of the four corpora: PKU(0.951),
CITYU(0.950) and MSR(0.971).
1 Introduction
Many approaches have been proposed in Chinese
word segmentation in the past decades. Segmen-
tation performance has been improved significantly,
from the earliest maximal match (dictionary-based)
approaches to HMM-based (Zhang et al, 2003) ap-
proaches and recent state-of-the-art machine learn-
ing approaches such as maximum entropy (Max-
Ent) (Xue and Shen, 2003), support vector machine
?Now the second author is affiliated with NTT.
(SVM) (Kudo and Matsumoto, 2001), conditional
random fields (CRF) (Peng and McCallum, 2004),
and minimum error rate training (Gao et al, 2004).
By analyzing the top results in the first and second
Bakeoffs, (Sproat and Emerson, 2003) and (Emer-
son, 2005), we found the top results were produced
by direct or indirect use of so-called ?IOB? tagging,
which converts the problem of word segmentation
into one of character tagging so that part-of-speech
tagging approaches can be used for word segmen-
tation. This approach was also called ?LMR? (Xue
and Shen, 2003) or ?BIES? (Asahara et al, 2005)
tagging. Under the scheme, each character of a
word is labeled as ?B? if it is the first character of a
multiple-character word, or ?I? otherwise, and ?O?
if the character functioned as an independent word.
For example, ??(whole) ???(Beijing city)? is
labeled as ??/O?/B?/I?/I?. Thus, the training
data in word sequences are turned into IOB-labeled
data in character sequences, which are then used as
the training data for tagging. For new test data, word
boundaries are determined based on the results of
tagging.
While the IOB tagging approach has been widely
used in Chinese word segmentation, we found that
so far all the existing implementations were using
character-based IOB tagging. In this work we pro-
pose a subword-based IOB tagging, which assigns
tags to a pre-defined lexicon subset consisting of the
most frequent multiple-character words in addition
to single Chinese characters. If only Chinese char-
acters are used, the subword-based IOB tagging is
downgraded to a character-based one. Taking the
same example mentioned above, ?????? is la-
961
beled as ??/O ??/B ?/I? in the subword-based
tagging, where ???/B? is labeled as one unit. We
will give a detailed description of this approach in
Section 2.
There exists a clear weakness with the IOB tag-
ging approach: It yields a very low in-vocabulary
rate (R-iv) in return for a higher out-of-vocabulary
(OOV) rate (R-oov). In the results of the closed
test in Bakeoff 2005 (Emerson, 2005), the work
of (Tseng et al, 2005), using CRFs for the IOB tag-
ging, yielded a very high R-oov in all of the four
corpora used, but the R-iv rates were lower. While
OOV recognition is very important in word segmen-
tation, a higher IV rate is also desired. In this work
we propose a confidence measure approach to lessen
this weakness. By this approach we can change the
R-oov and R-iv and find an optimal tradeoff. This
approach will be described in Section 2.3.
In addition, we illustrate our word segmentation
process in Section 2, where the subword-based tag-
ging is described by the MaxEnt method. Section 3
presents our experimental results. The effects using
the MaxEnts and CRFs are shown in this section.
Section 4 describes current state-of-the-art methods
with Chinese word segmentation, with which our re-
sults were compared. Section 5 provides the con-
cluding remarks and outlines future goals.
2 Chinese word segmentation framework
Our word segmentation process is illustrated in
Fig. 1. It is composed of three parts: a dictionary-
based N-gram word segmentation for segmenting IV
words, a maximum entropy subword-based tagger
for recognizing OOVs, and a confidence-dependent
word disambiguation used for merging the results
of both the dictionary-based and the IOB-tagging-
based. An example exhibiting each step?s results is
also given in the figure.
2.1 Dictionary-based N-gram word
segmentation
This approach can achieve a very high R-iv, but no
OOV detection. We combined with it the N-gram
language model (LM) to solve segmentation ambi-
guities. For a given Chinese character sequence,
C = c0c1c2 . . . cN , the problem of word segmenta-
tion can be formalized as finding a word sequence,
????????+XDQJ<LQJ&KXQOLYHVLQ%HLMLQJFLW\
input
????????+XDQJ<LQJ&KXQOLYHVLQ%HLMLQJFLW\
Dictionary-based word segmentation
?%?,?,?2?2??%?,+XDQJ%<LQJ,&KXQ,OLYHV2LQ2%HLMLQJ%FLW\,
Subword-based IOB tagging
?%Proceedings of the ACL 2007 Demo and Poster Sessions, pages 157?160,
Prague, June 2007. c?2007 Association for Computational Linguistics
Detecting Semantic Relations between Named Entities in Text
Using Contextual Features
Toru Hirano, Yoshihiro Matsuo, Genichiro Kikui
NTT Cyber Space Laboratories, NTT Corporation
1-1 Hikarinooka, Yokosuka-Shi, Kanagawa, 239-0847, Japan
{hirano.tohru, matsuo.yoshihiro, kikui.genichiro}@lab.ntt.co.jp
Abstract
This paper proposes a supervised learn-
ing method for detecting a semantic rela-
tion between a given pair of named enti-
ties, which may be located in different sen-
tences. The method employs newly intro-
duced contextual features based on center-
ing theory as well as conventional syntac-
tic and word-based features. These features
are organized as a tree structure and are
fed into a boosting-based classification al-
gorithm. Experimental results show the pro-
posed method outperformed prior methods,
and increased precision and recall by 4.4%
and 6.7%.
1 Introduction
Statistical and machine learning NLP techniques are
now so advanced that named entity (NE) taggers are
in practical use. Researchers are now focusing on
extracting semantic relations between NEs, such as
?George Bush (person)? is ?president (relation)? of
?the United States (location)?, because they provide
important information used in information retrieval,
question answering, and summarization.
We represent a semantic relation between two
NEs with a tuple [NE1, NE2, Relation Label]. Our
final goal is to extract tuples from a text. For exam-
ple, the tuple [George Bush (person), the U.S. (loca-
tion), president (Relation Label)] would be extracted
from the sentence ?George Bush is the president of
the U.S.?. There are two tasks in extracting tuples
from text. One is detecting whether or not a given
pair of NEs are semantically related (relation detec-
tion), and the other is determining the relation label
(relation characterization).
In this paper, we address the task of relation de-
tection. So far, various supervised learning ap-
proaches have been explored in this field (Culotta
and Sorensen, 2004; Zelenko et al, 2003). They
use two kinds of features: syntactic ones and word-
based ones, for example, the path of the given pair of
NEs in the parse tree and the word n-gram between
NEs (Kambhatla, 2004).
These methods have two problems which we con-
sider in this paper. One is that they target only intra-
sentential relation detection in which NE pairs are
located in the same sentence, in spite of the fact that
about 35% of NE pairs with semantic relations are
inter-sentential (See Section 3.1). The other is that
the methods can not detect semantic relations cor-
rectly when NE pairs located in a parallel sentence
arise from a predication ellipsis. In the following
Japanese example1, the syntactic feature, which is
the path of two NEs in the dependency structure,
of the pair with a semantic relation (?Ken11? and
?Tokyo12?) is the same as the feature of the pair with
no semantic relation (?Ken11? and ?New York14?).
(S-1) Ken11-wa Tokyo12-de, Tom13-wa
New York14-de umareta15.
(Ken11 was born15 in Tokyo12, Tom13 in
New York14.)
To solve the above problems, we propose a super-
vised learning method using contextual features.
The rest of this paper is organized as follows. Sec-
tion 2 describes the proposed method. We report the
results of our experiments in Section 3 and conclude
the paper in Section 4.
2 Relation Detection
The proposed method employs contextual features
based on centering theory (Grosz et al, 1983) as
well as conventional syntactic and word-based fea-
tures. These features are organized as a tree struc-
ture and are fed into a boosting-based classification
algorithm. The method consists of three parts: pre-
processing (POS tagging, NE tagging, and parsing),
1The numbers show correspondences of words between
Japanese and English.
157
feature extraction (contextual, syntactic, and word-
based features), and classification.
In this section, we describe the underlying idea of
contextual features and how contextual features are
used for detecting semantic relations.
2.1 Contextual Features
When a pair of NEs with a semantic relation appears
in different sentences, the antecedent NE must be
contextually easily referred to in the sentence with
the following NE. In the following Japanese exam-
ple, the pair ?Ken22? and ?amerika32 (the U.S.)?
have a semantic relation ?wataru33 (go)?, because
?Ken22? is contextually referred to in the sentence
with ?amerika32? (In fact, the zero pronoun ?i
refers to ?Ken22?). Meanwhile, the pair ?Naomi25?
and ?amerika32? has no semantic relation, because
the sentence with ?amerika32? does not refer to
?Naomi25?.
(S-2) asu21, Ken22-wa Osaka23-o otozure24
Naomi25-to au26.
(Ken22 is going to visit24 Osaka23 to see26
Naomi25, tomorrow21.)
(S-3) sonogo31, (?i-ga) amerika32-ni watari33
Tom34-to ryoko35 suru.
(Then31, (hei) will go33 to the U.S.32 to travel35
with Tom34.)
Furthermore, when a pair of NEs with a seman-
tic relation appears in a parallel sentence arise from
predication ellipsis, the antecedent NE is contextu-
ally easily referred to in the phrase with the follow-
ing NE. In the example of ?(S-1)?, the pair ?Ken11?
and ?Tokyo12? have a semantic relation ?umareta15
(was born)?. Meanwhile, the pair ?Ken11? and
?New York14? has no semantic relation.
Therefore, using whether the antecedent NE is re-
ferred to in the context with the following NE as fea-
tures of a given pair of NEs would improve relation
detection performance. In this paper, we use cen-
tering theory (Kameyama, 1986) to determine how
easily a noun phrase can be referred to in the follow-
ing context.
2.2 Centering Theory
Centering theory is an empirical sorting rule used to
identify the antecedents of (zero) pronouns. When
there is a (zero) pronoun in the text, noun phrases
that are in the previous context of the pronoun are
sorted in order of likelihood of being the antecedent.
The sorting algorithm has two steps. First, from the
beginning of the text until the pronoun appears, noun
Osaka
23
o asu
21
, Naomi
25
othersni
ga Ken22wa
Priority
Figure 1: Information Stacked According to Center-
ing Theory
phrases are stacked depending on case markers such
as particles. In the above example, noun phrases,
?asu21?, ?Ken22?, ?Osaka23? and ?Naomi25?, which
are in the previous context of the zero pronoun ?i,
are stacked and then the information shown in Fig-
ure 1 is acquired. Second, the stacked information is
sorted by the following rules.
1. The priority of case markers is as follows: ?wa
> ga > ni > o > others?
2. The priority of stack structure is as follows:
last-in first-out, in the same case marker
For example, Figure 1 is sorted by the above rules
and then the order, 1: ?Ken22?, 2: ?Osaka23?, 3:
?Naomi25?, 4: ?asu21?, is assigned. In this way, us-
ing centering theory would show that the antecedent
of the zero pronoun ?i is ?Ken22?.
2.3 Applying Centering Theory
When detecting a semantic relation between a given
pair of NEs, we use centering theory to determine
how easily the antecedent NE can be referred to in
the context with the following NE. Note that we do
not explicitly execute anaphora resolutions here.
Applied centering theory to relation detection is
as follows. First, from the beginning of the text until
the following NE appears, noun phrases are stacked
depending on case markers, and the stacked infor-
mation is sorted by the above rules (Section 2.2).
Then, if the top noun phrase in the sorted order is
identical to the antecedent NE, the antecedent NE is
?positive? when being referred to in the context with
the following NE.
When the pair of NEs, ?Ken22? and ?amerika32?,
is given in the above example, the noun phrases,
?asu21?, ?Ken22?, ?Osaka23? and ?Naomi25?, which
are in the previous context of the following NE
?amerika32?, are stacked (Figure 1). Then they are
sorted by the above sorting rules and the order, 1:
?Ken22?, 2: ?Osaka23?, 3: ?Naomi25?, 4: ?asu21?,
is acquired. Here, because the top noun phrase in
the sorted order is identical to the antecedent NE,
the antecedent NE ?Ken22? is ?positive? when be-
158
amerika
32wa: Ken
22
o: Osaka
23
others: Naomi
25others: asu
21
Figure 2: Centering Structure
ing referred to in the context with the following NE
?amerika32?. Whether or not the antecedent NE is
referred to in the context with the following NE is
used as a feature. We call this feature Centering Top
(CT).
2.4 Using Stack Structure
The sorting algorithm using centering theory tends
to rank highly thoes words that easily become sub-
jects. However, for relation detection, it is necessary
to consider both NEs that easily become subjects,
such as person and organization, and NEs that do not
easily become subjects, such as location and time.
We use the stack described in Section 2.3 as a
structural feature for relation detection. We call this
feature Centering Structure (CS). For example, the
stacked information shown in Figure 1 is assumed
to be structure information, as shown in Figure 2.
The method of converting from a stack (Figure 1)
into a structure (Figure 2) is described as follows.
First, the following NE, ?amerika32?, becomes the
root node because Figure 1 is stacked information
until the following NE appears. Then, the stacked
information is converted to Figure 2 depending on
the case markers. We use the path of the given pair
of NEs in the structure as a feature. For example,
?amerika32 ? wa:Ken22?2 is used as the feature of
the given pair ?Ken22? and ?amerika32?.
2.5 Classification Algorithm
There are several structure-based learning algo-
rithms proposed so far (Collins and Duffy, 2001;
Suzuki et al, 2003; Kudo and Matsumoto, 2004).
The experiments tested Kudo and Matsumoto?s
boosting-based algorithm using sub trees as features,
which is implemented as the BACT system.
In relation detection, given a set of training exam-
ples each of which represents contextual, syntactic,
and word-based features of a pair of NEs as a tree
labeled as either having semantic relations or not,
the BACT system learns that a set of rules are ef-
fective in classifying. Then, given a test instance,
which represents contextual, syntactic, and word-
2
?A? B? means A has a dependency relation to B.
Type % of pairs with semantic relations
(A) Intra-sentential 31.4% (3333 / 10626)
(B) Inter-sentential 0.8% (1777 / 225516)
(A)+(B) Total 2.2% (5110 / 236142)
Table 1: Percent of pairs with semantic relations in
annotated text
based features of a pair of NEs as a tree, the BACT
system classifies using a set of learned rules.
3 Experiments
We experimented with texts from Japanese newspa-
pers and weblogs to test the proposed method. The
following four models were compared:
1. WD : Pairs of NEs within n words are detected
as pairs with semantic relation.
2. STR : Supervised learning method using syn-
tactic3 and word-based features, the path of the
pairs of NEs in the parse tree and the word n-
gram between pairs of NEs (Kambhatla, 2004)
3. STR-CT : STR with the centering top feature
explained in Section 2.3.
4. STR-CS : STR with the centering structure fea-
ture explained in Section 2.4.
3.1 Setting
We used 1451 texts from Japanese newspapers and
weblogs, whose semantic relations between person
and location had been annotated by humans for the
experiments4. There were 5110 pairs with seman-
tic relations out of 236,142 pairs in the annotated
text. We conducted ten-fold cross-validation over
236,142 pairs of NEs so that sets of pairs from a
single text were not divided into the training and test
sets.
We also divided pairs of NEs into two types: (A)
intra-sentential and (B) inter-sentential. The reason
for dividing them is so that syntactic structure fea-
tures would be effective in type (A) and contextual
features would be effective in type (B). Another rea-
son is that the percentage of pairs with semantic rela-
tions out of the total pairs in the annotated text differ
significantly between types, as shown in Table 1.
In the experiments, all features were automati-
cally acquired using a Japanese morphological and
dependency structure analyzer.
3There is no syntactic feature in inter-sentential.
4We are planning to evaluate the other pairs of NEs.
159
(A)+(B) Total (A) Intra-sentential (B) Inter-sentential
Precision Recall Precision Recall Precsion Recall
WD10 43.0(2501/5819) 48.9(2501/5110) 48.1(2441/5075) 73.2(2441/3333) 8.0(60/744) 3.4(60/1777)
STR 69.3(2562/3696) 50.1(2562/5110) 75.6(2374/3141) 71.2(2374/3333) 33.9(188/555) 10.6(188/1777)
STR-CT 71.4(2764/3870) 54.1(2764/5110) 78.4(2519/3212) 75.6(2519/3333) 37.2(245/658) 13.8(245/1777)
STR-CS 73.7(2902/3935) 56.8(2902/5110) 80.1(2554/3187) 76.6(2554/3333) 46.5(348/748) 27.6(348/1777)
WD10: NE pairs that appear within 10 words are detected.
Table 2: Results for Relation Detection
0
0.2
0.4
0.6
0.8
1
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Recall
P
r
e
c
i
s
i
o
n
WD
STR
STR-CT
STR-CS
STR-CS
STR
WD
STR-CT
Figure 3: Recall-precision Curves: (A)+(B) total
3.2 Results
To improve relation detection performance, we in-
vestigated the effect of the proposed method using
contextual features. Table 2 shows results for Type
(A), Type (B), and (A)+(B). We also plotted recall-
precision curves5, altering threshold parameters, as
shown in Figure 3.
The comparison between STR and STR-CT and
between STR and STR-CS in Figure 3 indicates that
the proposed method effectively contributed to rela-
tion detection. In addition, the results for Type (A):
intra-sentential, and (B): inter-sentential, in Table
2 indicate that the proposed method contributed to
both Type (A), improving precision by about 4.5%
and recall by about 5.4% and Type (B), improving
precision by about 12.6% and recall by about 17.0%.
3.3 Error Analysis
Over 70% of the errors are covered by two major
problems left in relation detection.
Parallel sentence: The proposed method solves
problems, which result from when a parallel
sentence arises from predication ellipsis. How-
ever, there are several types of parallel sentence
that differ from the one we explained. (For ex-
ample, Ken and Tom was born in Osaka and
New York, respectively.)
5Precision = # of correctly detected pairs / # of detected pairs
Recall = # of correctly detected pairs / # of pairs with semantic
relations
Definite anaphora: Definite noun phrase, such as
?Shusho (the Prime Minister)? and ?Shacho
(the President)?, can be anaphors. We should
consider them in centering theory, but it is dif-
ficult to find them in Japanese .
4 Conclusion
In this paper, we propose a supervised learning
method using words, syntactic structures, and con-
textual features based on centering theory, to im-
prove both inter-sentential and inter-sentential rela-
tion detection. The experiments demonstrated that
the proposed method increased precision by 4.4%,
up to 73.7%, and increased recall by 6.7%, up to
56.8%, and thus contributed to relation detection.
In future work, we plan to solve the problems re-
lating to parallel sentence and definite anaphora, and
address the task of relation characterization.
References
M. Collins and N. Duffy. 2001. Convolution Kernels for
Natural Language. Proceedings of the Neural Information
Processing Systems, pages 625?632.
A. Culotta and J. Sorensen. 2004. Dependency Tree Kernels
for Relation Extraction. Annual Meeting of Association of
Computational Linguistics, pages 423?429.
B. J. Grosz, A. K. Joshi, and S. Weistein. 1983. Providing a
unified account of definite nounphrases in discourse. Annual
Meeting of Association of Computational Linguistics, pages
44?50.
N. Kambhatla. 2004. Combining Lexical, Syntactic, and Se-
mantic Features with Maximum Entropy Models for Infor-
mation Extraction. Annual Meeting of Association of Com-
putational Linguistics, pages 178?181.
M. Kameyama. 1986. A property-sharing constraint in center-
ing. Annual Meeting of Association of Computational Lin-
guistics, pages 200?206.
T. Kudo and Y. Matsumoto. 2004. A boosting algorithm for
classification of semi-structured text. In Proceedings of the
2004 EMNLP, pages 301?308.
J. Suzuki, T. Hirao, Y. Sasaki, and E. Maeda. 2003. Hier-
archical directed acyclic graph kernel : Methods for struc-
tured natural language data. Annual Meeting of Association
of Computational Linguistics, pages 32?39.
D. Zelenko, C. Aone, and A. Richardella. 2003. Kernel Meth-
ods for Relation Extraction. Journal of Machine Learning
Research, pages 3:1083?1106.
160
Proceedings of the ACL 2007 Demo and Poster Sessions, pages 225?228,
Prague, June 2007. c?2007 Association for Computational Linguistics
Japanese Dependency Parsing Using Sequential Labeling
for Semi-spoken Language
Kenji Imamura and Genichiro Kikui
NTT Cyber Space Laboratories, NTT Corporation
1-1 Hikarinooka, Yokosuka-shi, Kanagawa, 239-0847, Japan
{imamura.kenji, kikui.genichiro}@lab.ntt.co.jp
Norihito Yasuda
NTT Communication Science Laboratories, NTT Corporation
2-4 Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0237, Japan
n-yasuda@cslab.kecl.ntt.co.jp
Abstract
The amount of documents directly published
by end users is increasing along with the
growth of Web 2.0. Such documents of-
ten contain spoken-style expressions, which
are difficult to analyze using conventional
parsers. This paper presents dependency
parsing whose goal is to analyze Japanese
semi-spoken expressions. One characteris-
tic of our method is that it can parse self-
dependent (independent) segments using se-
quential labeling.
1 Introduction
Dependency parsing is a way of structurally ana-
lyzing a sentence from the viewpoint of modifica-
tion. In Japanese, relationships of modification be-
tween phrasal units called bunsetsu segments are an-
alyzed. A number of studies have focused on parsing
of Japanese as well as of other languages. Popular
parsers are CaboCha (Kudo and Matsumoto, 2002)
and KNP (Kurohashi and Nagao, 1994), which were
developed to analyze formal written language ex-
pressions such as that in newspaper articles.
Generally, the syntactic structure of a sentence
is represented as a tree, and parsing is carried out
by maximizing the likelihood of the tree (Charniak,
2000; Uchimoto et al, 1999). Units that do not
modify any other units, such as fillers, are difficult
to place in the tree structure. Conventional parsers
have forced such independent units to modify other
units.
Documents published by end users (e.g., blogs)
are increasing on the Internet alng with the growth
of Web 2.0. Such documents do not use controlled
written language and contain fillers and emoticons.
This implies that analyzing such documents is diffi-
cult for conventional parsers.
This paper presents a new method of Japanese
dependency parsing that utilizes sequential labeling
based on conditional random fields (CRFs) in or-
der to analyze semi-spoken language. Concretely,
sequential labeling assigns each segment a depen-
dency label that indicates its relative position of de-
pendency. If the label set includes self-dependency,
the fillers and emoticons would be analyzed as seg-
ments depending on themselves. Therefore, since it
is not necessary for the parsing result to be a tree,
our method is suitable for semi-spoken language.
2 Methods
Japanese dependency parsing for written language
is based on the following principles. Our method re-
laxes the first principle to allow self-dependent seg-
ments (c.f. Section 2.3).
1. Dependency moves from left to right.
2. Dependencies do not cross each other.
3. Each segment, except for the top of the parsed
tree, modifies at most one other segment.
2.1 Dependency Parsing Using Cascaded
Chunking (CaboCha)
Our method is based on the cascaded chunking
method (Kudo and Matsumoto, 2002) proposed as
the CaboCha parser 1. CaboCha is a sort of shift-
reduce parser and determines whether or not a seg-
ment depends on the next segment by using an
1http://www.chasen.org/?taku/software/cabocha/
225
SVM-based classifier. To analyze long-distance de-
pendencies, CaboCha shortens the sentence by re-
moving segments for which dependencies are al-
ready determined and which no other segments de-
pend on. CaboCha constructs a tree structure by re-
peating the above process.
2.2 Sequential Labeling
Sequential labeling is a process that assigns each
unit of an input sequence an appropriate label (or
tag). In natural language processing, it is applied
to, for example, English part-of-speech tagging and
named entity recognition. Hidden Markov models
or conditional random fields (Lafferty et al, 2001)
are used for labeling. In this paper, we use linear-
chain CRFs.
In sequential labeling, training data developers
can design labels with no restrictions.
2.3 Cascaded Chunking Using Sequential
Labeling
The method proposed in this paper is a generaliza-
tion of CaboCha. Our method considers not only
the next segment, but also the followingN segments
to determine dependencies. This area, including the
considered segment, is called the window, and N is
called the window size. The parser assigns each seg-
ment a dependency label that indicates where the
segment depends on the segments in the window.
The flow is summarized as follows:
1. Extract features from segments such as the
part-of-speech of the headword in a segment
(c.f. Section 3.1).
2. Carry out sequential labeling using the above
features.
3. Determine the actual dependency by interpret-
ing the labels.
4. Shorten the sentence by deleting segments for
which the dependency is already determined
and that other segments have never depended
on.
5. If only one segment remains, then finish the
process. If not, return to Step 1.
An example of dependency parsing for written
language is shown in Figure 1 (a).
In Steps 1 and 2, dependency labels are supplied
to each segment in a way similar to that used by
Label Description
? Segment depends on a segment outside of win-
dow.
0Q Self-dependency
1D Segment depends on next segment.
2D Segment depends on segment after next.
-1O Segment is top of parsed tree.
Table 1: Label List Used by Sequential Labeling
(Window Size: 2)
other sequential labeling methods. However, our
sequential labeling has the following characteristics
since this task is dependency parsing.
? The labels indicate relative positions of the de-
pendent segment from the current segment (Ta-
ble 1). Therefore, the number of labels changes
according to the window size. Long-distance de-
pendencies can be parsed by one labeling process
if we set a large window size. However, growth
of label variety causes data sparseness problems.
? One possible label is that of self-dependency
(noted as ?0Q? in this paper). This is assigned
to independent segments in a tree.
? Also possible are two special labels. Label ?-1O?
denotes a segment that is the top of the parsed
tree. Label ??? denotes a segment that depends
on a segment outside of the window. When the
window size is two, the segment depends on a
segment that is over two segments ahead.
? The label for the current segment is determined
based on all features in the window and on the
label of the previous segment.
In Step 4, segments, which no other segments de-
pend on, are removed in a way similar to that used
by CaboCha. The principle that dependencies do
not cross each other is applied in this step. For ex-
ample, if a segment depends on a segment after the
next, the next segment cannot be modified by other
segments. Therefore, it can be removed. Similarly,
since the ??? label indicates that the segment de-
pends on a segment after N segments, all interme-
diate segments can be removed if they do not have
??? labels.
The sentence is shortened by iteration of the
above steps. The parsing finishes when only one
segment remains in the sentence (this is the segment
226
(a) Written Language
--- 2D 1D 1D -1O
2D 1D -1O
Output
Input
Label
Label
kare wa
(he)
kanojo no
(her)
atatakai
(warm)
magokoro ni
(heart)
kando-shita.
(be moved)
(He was moved by her warm heart.)
Seg. No. 1 2 3 4 5
kare wa
(he)
kanojo no
(her)
atatakai
(warm)
magokoro ni
(heart)
kando-shita.
(be moved)
(b) Semi-spoken Language
Input Uuuum, kyo wa
(today)
...... choshi
(condition)
yokatta desu.
(be good)
0Q --- 0Q 1D -1O
1D -1O
(Uuuum, my condition .... was good today.)
Seg. No. 1 2 3 4 5
Label
Label
Uuuum, kyo wa
(today)
...... choshi
(condition)
yokatta desu.
(be good)Output
1st
Labeling
2nd
Labeling
Figure 1: Examples of Dependency Parsing (Window Size: 2)
Corpus Type # of Sentences # of Segments
Kyoto Training 24,283 234,685
Test 9,284 89,874
Blog Training 18,163 106,177
Test 8,950 53,228
Table 2: Corpus Size
at the top of the parsed tree). In the example in Fig-
ure 1 (a), the process finishes in two iterations.
In a sentence containing fillers, the self-
dependency labels are assigned by sequential label-
ing, as shown in Figure 1 (b), and are parsed as in-
dependent segments. Therefore, our method is suit-
able for parsing semi-spoken language that contains
independent segments.
3 Experiments
3.1 Experimental Settings
Corpora In our experiments, we used two cor-
pora. One is the Kyoto Text Corpus 4.0 2, which is
a collection of newspaper articles with segment and
dependency annotations. The other is a blog cor-
pus, which is a collection of blog articles taken as
semi-spoken language. The blog corpus is manually
annotated in a way similar to that used for the Kyoto
text corpus. The sizes of the corpora are shown in
Table 2.
Training We used CRF++ 3, a linear-chain CRF
training tool, with eleven features per segment. All
2http://nlp.kuee.kyoto-u.ac.jp/nl-resource/corpus.html
3http://www.chasen.org/?taku/software/CRF++/
of these are static features (proper to each segment)
such as surface forms, parts-of-speech, inflections
of a content headword and a functional headword
in a segment. These are parts of a feature set that
many papers have referenced (Uchimoto et al, 1999;
Kudo and Matsumoto, 2002).
Evaluation Metrics Dependency accuracy and
sentence accuracy were used as evaluation metrics.
Sentence accuracy is the proportion of total sen-
tences in which all dependencies in the sentence
are accurately labeled. In Japanese, the last seg-
ment of most sentences is the top of the parsed trees,
and many papers exclude this last segment from the
accuracy calculation. We, in contrast, include the
last one because some of the last segments are self-
dependent.
3.2 Accuracy of Dependency Parsing
Dependency parsing was carried out by combining
training and test corpora. We used a window size
of three. We also used CaboCha as a reference for
the set of sentences trained only with the Kyoto cor-
pus because it is designed for written language. The
results are shown in Table 3.
CaboCha had better accuracies for the Kyoto test
corpus. One reason might be that our method man-
ually combined features and used parts of com-
binations, while CaboCha automatically finds the
best combinations by using second-order polyno-
mial kernels.
For the blog test corpus, the proposed method
using the Kyoto+Blog model had the best depen-
227
Test Corpus Method Training Corpus Dependency Accuracy Sentence Accuracy
(Model)
Kyoto Proposed Method Kyoto 89.87% (80766 / 89874) 48.12% (4467 / 9284)
(Written Language) (Window Size: 3) Kyoto + Blog 89.76% (80670 / 89874) 47.63% (4422 / 9284)
CaboCha Kyoto 92.03% (82714 / 89874) 55.36% (5140 / 9284)
Blog Proposed Method Kyoto 77.19% (41083 / 53226) 41.41% (3706 / 8950)
(Semi-spoken Language) (Window Size: 3) Kyoto + Blog 84.59% (45022 / 53226) 52.72% (4718 / 8950)
CaboCha Kyoto 77.44% (41220 / 53226) 43.45% (3889 / 8950)
Table 3: Dependency and Sentence Accuracies among Methods/Corpora
 88
 88.5
 89
 89.5
 90
 90.5
 91
 1  2  3  4  5
 0
 2e+06
 4e+06
 6e+06
 8e+06
 1e+07
D
ep
en
de
nc
y 
Ac
cu
ra
cy
 (%
)
# 
of
 F
ea
tu
re
s
Window Size
Dependency Accuracy
# of Features
Figure 2: Dependency Accuracy and Number of
Features According to Window Size (The Kyoto
Text Corpus was used for training and testing.)
dency accuracy result at 84.59%. This result was
influenced not only by the training corpus that con-
tains the blog corpus but also by the effect of self-
dependent segments. The blog test corpus contains
3,089 self-dependent segments, and 2,326 of them
(75.30%) were accurately parsed. This represents
a dependency accuracy improvement of over 60%
compared with the Kyoto model.
Our method is effective in parsing blogs be-
cause fillers and emoticons can be parsed as self-
dependent segments.
3.3 Accuracy According to Window Size
Another characteristic of our method is that all de-
pendencies, including long-distance ones, can be
parsed by one labeling process if the window cov-
ers the entire sentence. To analyze this characteris-
tic, we evaluated dependency accuracies in various
window sizes. The results are shown in Figure 2.
The number of features used for labeling in-
creases exponentially as window size increases.
However, dependency accuracy was saturated after a
window size of two, and the best accuracy was when
the window size was four. This phenomenon implies
a data sparseness problem.
4 Conclusion
We presented a new dependency parsing method us-
ing sequential labeling for the semi-spoken language
that frequently appears in Web documents. Sequen-
tial labeling can supply segments with flexible la-
bels, so our method can parse independent words
as self-dependent segments. This characteristic af-
fects robust parsing when sentences contain fillers
and emoticons.
The other characteristics of our method are us-
ing CRFs and that long dependencies are parsed in
one labeling process. SVM-based parsers that have
the same characteristics can be constructed if we in-
troduce multi-class classifiers. Further comparisons
with SVM-based parsers are future work.
References
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proc. of NAACL-2000, pages 132?139.
Taku Kudo and Yuji Matsumoto. 2002. Japanese depen-
dency analyisis using cascaded chunking. In Proc. of
CoNLL-2002, Taipei.
Sadao Kurohashi and Makoto Nagao. 1994. A syntactic
analysis method of long Japanese sentences based on
the detection of conjunctive structures. Computational
Linguistics, 20(4):507?534.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic models
for segmenting and labeling sequence data. In Proc. of
ICML-2001, pages 282?289.
Kiyotaka Uchimoto, Satoshi Sekine, and Hitoshi Isahara.
1999. Japanese dependency structure analysis based
on maximum entropy models. In Proc. of EACL?99,
pages 196?203, Bergen, Norway.
228
Learning Nonstructural Distance Metric
by Minimum Cluster Distortions
Daichi Mochihashi, Genichiro Kikui
ATR Spoken Language Translation
research laboratories
Hikaridai 2-2-2, Keihanna Science City
Kyoto 619-0288, Japan
daichi.mochihashi@atr.jp
genichiro.kikui@atr.jp
Kenji Kita
Center for Advanced Information
Technology, Tokushima University
Minamijosanjima 2-1
Tokushima 770-8506, Japan
kita@is.tokushima-u.ac.jp
Abstract
Much natural language processing still depends on
the Euclidean (cosine) distance function between
two feature vectors, but this has severe problems
with regard to feature weightings and feature cor-
relations. To answer these problems, we propose an
optimal metric distance that can be used as an alter-
native to the cosine distance, thus accommodating
the two problems at the same time. This metric is
optimal in the sense of global quadratic minimiza-
tion, and can be obtained from the clusters in the
training data in a supervised fashion.
We confirmed the effect of the proposed metric
distance by a synonymous sentence retrieval task,
document retrieval task and the K-means clustering
of general vectorial data. The results showed con-
stant improvement over the baseline method of Eu-
clid and tf.idf, and were especially prominent for
the sentence retrieval task, showing a 33% increase
in the 11-point average precision.
1 Introduction
Natural language processing involves many kinds of
linguistic expressions, such as sentences, phrases,
documents and the collection of documents. Com-
paring these expressions based on semantic proxim-
ity is a fundamental task and has many applications.
Generally, two basic approaches exist to compare
two expressions: (a) structural and (b) nonstruc-
tural. Structural approaches make use of syntactic
parsing or dependency analysis to make a rigorous
comparison; nonstructural approaches use vector
representation and provide a rough but fast compar-
ison that is required for search/retrieval from a vast
amount of corpora. While structural approaches
have recently become available in a kernel-based
sophisticated treatment (Collins and Duffy, 2001;
Suzuki et al, 2003), here we concentrate on non-
structural comparison. This is not only because non-
structural comparison constitutes an integral part
in structural methods (that is, even in hierarchi-
cal methods the leaf comparison is still atomic),
but because it is frequently embedded in many ap-
plications where structural parsings are not avail-
able or computationally too expensive. For exam-
ple, information retrieval has long used the ?bag
of words? approach (Baeza-Yates and Ribeiro-Neto,
1999; Schu?tze, 1992) mainly due to a lack of scal-
able segmentation algorithms and the huge amount
of data involved. While segmentation algorithms,
such as TEXTTILING (Hearst, 1994) and its recent
successors using the inter-paragraph similarity ma-
trix (Choi, 2000), all themselves use nonstructural
cosine similarity as a measure of semantic proxim-
ity between paragraphs.
However, the distance function so far has been
largely defined and used ad hoc, usually by a tf.idf
weighting scheme (Salton and Yang, 1973) and a
simple cosine similarity, equivalently, an Euclidean
dot product. In this paper, we propose an optimal
distance function that is parameterized by a global
metric matrix. This metric is optimal in the sense of
global quadratic minimization, and can be learned
from the given clusters in the training data. These
clusters are often attributable with many forms, such
as paragraphs, documents or document collections,
as long as the items in the training data are not com-
pletely independent.
This paper is organized as follows. In section 2
we describe the issue of traditional Euclidean dis-
tances, and section 3 places it into general perspec-
tive with related works in machine learning. Section
4 introduces the proposed metric, and section 5 vali-
dates its effect on the task of sentence retrieval, doc-
ument retrieval and the K-means clustering. Sec-
tions 6 and 7 present discussions and the conclusion.
2 Issues with Euclidean distances
When we address nonstructural matching, linguis-
tic expressions are often modeled by a feature vec-
tor ~x ? Rn, with its elements x1 . . . xn correspond-
ing to the number of occurrences of i?th feature. If
features are simply words, this is called a ?bag of
words?; but in general, features are not restricted to
this kind, and we will use the general term ?feature?
in the rest of the paper.
To measure the distance between two vectors
~u,~v, a dot product or Euclidean distance
d(~u,~v)2 = (~u ? ~v)T (~u ? ~v) (1)
= ?ni=1(ui ? vi)2
(where T denotes a transposition) has been em-
ployed so far 1, with a heuristic feature weighting
such as tf.idf in a preprocessing stage.
However, there are two main problems with this
distance:
(1) The correlation between features is ignored.
(2) Feature weighting is inevitably arbitrary.
Problem (1) is especially important in languages,
because linguistic features (e.g., words) generally
have strong correlations between them, such as col-
locations or typical constructions. But this correla-
tion cannot be considered in a simple dot product.
While it is possible to address this with a specific
kernel function, such as polynomials (Mu?ller et al,
2001), this is not available for many problems, such
as information retrieval or question answering, that
do not fit classifications or cannot be easily ?kernel-
ized?. Problem (2) is a more subtle but inherent one:
while tf.idf often works properly in practice, there
are several options, especially in tf such as logs or
square roots, but we have no principle with which
to choose from. Further, it has no theoretical basis
that gives any optimality as a distance function.
3 Related Works
The issues above of feature correlations and fea-
ture weightings can be summarized as a problem of
defining an appropriate metric in the feature space,
based on the distribution of data. This problem has
recently been highlighted in the field of machine
learning research. (Xing et al, 2002) has an ob-
jective that is quite similar to that of this paper, and
gives a metric matrix that resembles ours based on
sample pairs of ?similar points? as training data.
(Bach and Jordan, 2004) and (Schultz and Joachims,
2004) seek to answer the same problem with an ad-
ditional scenario of spectral clustering and relative
comparisons in Support Vector Machines, respec-
tively. In this aspect, our work is a straight succes-
sor of (Xing et al, 2002) where its general usage
in vector space is preserved. We offer a discussion
on the similarity to our method and our advantages
1When we normalize the length of the vectors |~u| = |~v| = 1
as commonly adopted, (~u ? ~v)T (~u ? ~v) = |~u|2 + |~v|2 ? 2~u ?
~v ? ?~u ? ~v = ? cos(~u,~v) ; therefore, this includes a cosine
similarity (Manning and Sch u?tze, 1999).
in section 6. Finally, we note that the Fisher ker-
nel of (Jaakkola and Haussler, 1999) has the same
concept that gives an appropriate similarity of two
data through the Fisher information matrix obtained
from the empirical distribution of data. However, it
is often approximated by a unit matrix because of
its heavy computational demand.
In the field of information retrieval, (Jiang and
Berry, 1998) proposes a Riemannian SVD (R-SVD)
from the viewpoint of relevance feedback. This
work is close in spirit to our work, but is not aimed
at defining a permanent distance function and does
not utilize cluster structures existent in the training
data.
4 Defining an Optimal Metric
To solve the problems in section 2, we note the func-
tion that synonymous clusters play. There are many
levels of (more or less) synonymous clusters in lin-
guistic data: phrases, sentences, paragraphs, docu-
ments, and, in a web environment, the site that con-
tains the document. These kinds of clusters can of-
ten be attributed to linguistic expressions because
they nest in general so that each expression has a
parent cluster.
Since these clusters are synonymous, we can ex-
pect the vectors in each cluster to concentrate in the
ideal feature space. Based on this property, we can
introduce an optimal weighting and correlation in a
supervised fashion. We will describe this method
below.
4.1 The Basic Idea
As stated above, vectors in the same cluster must
have a small distance between each other in the ideal
geometry. When we measure an L2-distance be-
tween ~u and ~v by a Mahalanobis distance param-
eterized by M :
dM (~u,~v)2 = (~u ? ~v)T M(~u ? ~v) (2)
= ?ni=1
?n
j=1 mij(ui ? vi)(uj ? vj),
where symmetric metric matrix M gives both cor-
responding feature weights and feature correlations.
When we take M = I (unit matrix), we recover the
original Euclidean distance (1).
Equation (2) can be rewritten as (3) because M is
symmetric:
dM (~u,~v)2 = (M1/2(~u?~v))T (M1/2(~u?~v)). (3)
Therefore, this distance amounts to a Euclidean dis-
tance in M 1/2-mapped space (Xing et al, 2002).
Note that this distance is global, and different
from the ordinary Mahalanobis distance in pattern
recognition (for example, (Duda et al, 2000)) that is
defined for each cluster one by one, using a cluster-
specific covariance matrix. That type of distance
cannot be generalized to new kinds of data; there-
fore, it has been used for local classifications. What
we want is a global distance metric that is generally
useful, not a measure for classification to predefined
clusters. In this respect, (Xing et al, 2002) shares
the same objective as ours.
Therefore, we require an optimization over all the
clusters in the training data. Generally, data in the
clusters are distributed as in figure 1(a), comprising
ellipsoidal forms that have high (co)variances for
some dimensions and low (co)variances for other di-
mensions. Further, the cluster is not usually aligned
to the axes of coordinates. When we find a global
metric matrix M that minimizes the cluster distor-
tions, namely, one that reduces high variances and
expands low variances for the data to make a spher-
ical form as good as possible in the M 1/2-mapped
space (figure 1(b)), we can expect it to capture nec-
essary and unnecessary variations and correlations
on the features, combining information from many
clusters to produce a more reliable metric that is not
locally optimal. We will find this optimal M below.
xn
x1
x2
High
variance
High
covariance
Low
variance
(a) Original space
x1
x2
xn
(b) Mapped space
Figure 1: Geometry of feature space.
4.2 Global optimization over clusters
Suppose that each data (for example, sentences or
documents) is a vector ~s ? Rn, and the whole cor-
pus can be divided into N clusters, X1 . . . XN . That
is, each vector has a dimension n, and the number of
clusters is N . For each cluster Xi, cluster centroid
ci is calculated as ~ci = 1/|Xi|
?
~s?Xi ~s , where |X|
denotes the number of data in X . When necessary,
each element in ~sj or ~ci is referenced as sjk or cik
(k = 1 . . . n).
The basic idea above is formulated as follows.
We seek the metric matrix M that minimizes the
distance between each data ~sj and the cluster cen-
troid ~ci, dM (~sj ,~ci) for all clusters X1 . . . XN .
Mathematically, this is formulated as a quadratic
minimization problem
M = arg min
M
N
?
i=1
?
~sj?Xi
dM (~sj,~ci)2
= arg min
M
N
?
i=1
?
~sj?Xi
(~sj ? ~ci)T M(~sj ? ~ci) (4)
under a scale constraint (| ? | means determinant)
|M | = 1. (5)
Scale constraint (5) is necessary for excluding a
degenerate solution M = O. 1 is an arbitrary con-
stant: when we replace 1 by c, c2M becomes a new
solution. This minimization problem is an exten-
sion to the method of MindReader (Ishikawa et al,
1998) to multiple clusters, and has a unique solution
below.
Theorem The matrix that solves the minimization
problem (4,5) is
M = |A|1/nA?1, (6)
where A = [akl] is defined by
akl =
N
?
i=1
?
sj?Xi
(sjl ? cil)(sjk ? cik) . (7)
Proof: See Appendix A.
When A is singular, we can use as A?1 a Moore-
Penrose matrix pseudoinverse A+. Generally, A
consists of linguistic features and is very sparse, and
often singular. Therefore, A+ is nearly always nec-
essary for the above computation. For details, see
Appendix B.
4.3 Generalization
While we assumed through the above construction
that each cluster is equally important, this is not
the case in general. For example, clusters with a
small number of data may be considered weak, and
in the hierarchical clustering situation, a ?grand-
mother? cluster may be weaker. If we have con-
fidences ?1 . . . ?N for the strength of clustering for
each cluster X1 . . . XN , this information can be in-
corporated into (4) by a set of normalized cluster
weights ??i :
M = arg min
M
N
?
i=1
??i
?
~sj?Xi
(~sj ? ~ci)T M(~sj ? ~ci),
where ??i = ?i/
?N
j=1 ?j , and we obtain a respec-
tively weighted solution in (7). Further, we note that
when N = 1, this metric recovers the ordinary Ma-
halanobis distance in pattern recognition. However,
we used equal weights for the experiments below
because the number of data in each cluster was ap-
proximately equal.
5 Experiments
We evaluated our metric distance on the three tasks
of synonymous sentence retrieval, document re-
trieval, and the K-means clustering of general vec-
torial data. After calculating M on the training data
of clusters, we applied it to the test data to see how
well its clusters could be recovered. As a measure of
cluster recovery, we use 11-point average precision
and R-precision for the distribution of items of the
same cluster in each retrieval result. Here, R equals
the cardinality of the cluster; therefore, R-precision
shows the precision of cluster recovery.
5.1 Synonymous sentence retrieval
5.1.1 Sentence cluster corpus
We used a paraphrasing corpus of travel conversa-
tions (Sugaya et al, 2002) for sentence retrieval.
This corpus consists of 33,723,164 Japanese trans-
lations, each of which corresponds to one of the
original English sentences. By way of this cor-
respondence, Japanese sentences are divided into
10,610 clusters. Therefore, each cluster consists
of Japanese sentences that are possible translations
from the same English seed sentence that the clus-
ter has. From this corpus, we constructed 10 sets
of data. Each set contains random selection of 200
training clusters and 50 test clusters, and each clus-
ter contains a maximum of 100 sentences 2. Ex-
periments were conducted on these 10 datasets for
each level of dimensionality reduction (see below)
to produce average statistics.
5.1.2 Features and dimensionality reduction
As a feature of a sentence, we adopted unigrams of
all words and bigrams of functional words from the
part-of-speech tags, because the sequence of func-
tional words is important in the conversational cor-
pus.
While the lexicon is limited for travel conversa-
tions, the number of features exceeds several thou-
sand or more. This may be prohibitive for the calcu-
lation of the metric matrix, therefore, we addition-
ally compressed the features with SVD, the same
method used in Latent Semantic Indexing (Deer-
wester et al, 1990).
5.1.3 Sentence retrieval results
Qualitative result Figure 5 (last page) shows a sam-
ple retrieval result. A sentence with (*) mark at
the end is the correct answer, that is, a sentence
from the same original cluster as the query. We can
see that the results with the metric distance contain
2When the number of data in the cluster exceeds this limit,
100 sentences are randomly sampled. All sampling are made
without replacement.
less noise than a standard Euclid baseline with tf.idf
weighting, achieving a high-precision retrieval. Al-
though the high rate of dimensionality reduction in
figure 6 shows degradation due to the dimension
contamination, the effect of metric distance is still
apparent despite bad conditions.
Quantitative result Figure 2 shows the averaged
precision-recall curves of retrieval and figure 3
shows 11-point average precisions, for each rate
of dimensionality reduction. Clearly, our method
achieves higher precision than the standard method,
and does not degrade much with feature compres-
sions unless we reduce the dimension too much, i.e.,
to < 5%.
0
0.2
0.4
0.6
0.8
1
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Pr
ec
is
io
n
Recall
1%
5%
10%
20%
50%
(a) Metric distance +
idf
0
0.2
0.4
0.6
0.8
1
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
Pr
ec
is
io
n
Recall
1%
5%
10%
20%
50%
(b) Euclidean + idf
Figure 2: Precision-recall of sentence retrieval.
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0 5 10 15 20 25 30 35 40 45 50
Pr
ec
is
io
n
Dimension Reduction(%)
Metric distance +idf
Euclidean distance +idf
Figure 3: 11-point average precision.
5.2 Document retrieval
As a method of tackling clusters of texts, the text
classification task has recently made great advances
with a Na??ve Bayes or SVM classifiers (for exam-
ple, (Joachims, 1998)). However, they all aim at
classifying texts into a few predefined clusters, and
cannot deal with a document that fits neither of the
clusters. For example, when we regard a website as
a cluster of documents, the possible clusters are nu-
merous and constantly increasing, which precludes
classificatory approaches. For these circumstances,
document clustering or retrieval will benefit from a
global distance metric that exploits the multitude of
cluster structures themselves.
5.2.1 Newsgroup text dataset
For this purpose, we used the 20-Newsgroup dataset
(Lang, 1995). This is a standard text classification
dataset that has a relatively large number of classes,
20. Among the 20 newsgroups, we selected 16 clus-
ters of training data and 4 clusters of test data, and
performed 5-fold cross validation. The maximum
number of documents per cluster is 100, and when
it exceeds this limit, we made a random sampling of
100 documents as the sentence retrieval experiment.
Because our proposed metric is calculated from
the distribution of vectors in high-dimensional fea-
ture space, it becomes inappropriate if the norm
of the vectors (largely proportional to document
length) differs much from document to document.
3 Therefore, we used subsampling/oversampling to
form a median length (130 words) on training docu-
ments. Further, we preprocessed them with tf.idf as
a baseline method.
5.2.2 Results
Table 1 shows R-precision and 11-point average
precision. Since the test data contains 4 clusters,
the baselines of precision are 0.25. We can see from
both results that metric distance produces a better
retrieval over the tf.idf and dot product. However,
refinements in precision are certain (average p =
0.0243) but subtle.
This can be thought of as the effect of the dimen-
sionality reduction performed. We first decompose
data matrix X by SVD: X = USV ?1 and build
a k-dimensional compressed representation Xk =
VkX; where Vk denotes a k-largest submatrix of V .
From the equation (3), this means a Euclidean dis-
tance of M 1/2Xk = M1/2VkX . Therefore, Vk may
subsume the effect of M in a preprocessing stage.
Close inspection of table 1 shows this effect as a
tradeoff between M and Vk. To make the most of
metric distance, we should consider metric induc-
tion and dimensionality reduction simultaneously,
or reconsider the problem in kernel Hilbert space.
Dim. R-precision 11-pt Avr. Prec.
Red. Metric Euclid Metric Euclid
0.5% 0.421 0.399 0.476 0.455
1% 0.388 0.368 0.450 0.430
2% 0.359 0.343 0.425 0.409
3% 0.344 0.330 0.411 0.399
4% 0.335 0.323 0.402 0.392
5% 0.329 0.318 0.397 0.388
10% 0.316 0.307 0.379 0.376
20% 0.343 0.297 0.397 0.365
Table 1: Newsgroup text retrieval results.
3Normalizing documents to unit length effectively maps
them to a high-dimensional hypersphere; this proved to pro-
duce an unsatisfactory result. Defining metrics that work on
a hypersphere like spherical K-means (Dhillon and Modha,
2001) requires further research.
5.3 K-means clustering and general vectorial
data
Metric distance can also be used for clustering or
general vectorial data. Figure 4 shows the K-means
clustering result of applying our metric distance to
some of the UCI Machine Learning datasets (Blake
and Merz, 1998). K-means clustering was con-
ducted 100 times with a random start, where K
equals the known number of classes in the data 4.
Clustering precision was measured as an average
probability that a randomly picked pair of data will
conform to the true clustering (Xing et al, 2002).
We also conducted the same clustering for doc-
uments of the 20-Newsgroup dataset to get a small
increase in precision like the document retrieval ex-
periment in section 5.2.
0.6
0.7
0.8
0.9
1
1 2 5 10 13
Dimension
Pre
cisi
on
(a) ?wine? dataset
0.6
0.7
0.8
0.9
1 2 5 10 15 20
Dimension
Pre
cisi
on
(b) ?protein? dataset
0.7
0.8
0.9
1
1 2 3 4
Dimension
Pre
cisi
on
(c) ?iris? dataset
0.6
0.7
0.8
0.9
1
1 2 5 10 20 3 5
Dimension
Pre
cisi
on
(d) ?soybean? dataset
Figure 4: K-means clustering of UCI Machine
Learning dataset results. The horizontal axis shows
compressed dimensions (rightmost is original). The
right bar shows clustering precision using Metric
distance, and the left bar shows that using Euclidean
distance.
6 Discussion
In this paper, we proposed an optimal distance met-
ric based on the idea of minimum cluster distortion
in training data. Although vector distances have fre-
quently been used in natural language processing,
this is a rather neglected but recently highlighted
problem. Unlike recently proposed methods with
spectral methods or SVMs, our method assumes no
such additional scenarios and can be considered as
4Because of the small size of the dataset, we did not apply
cross-validation as in other experiments.
a straight successor to (Xing et al, 2002)?s work.
Their work has the same perspective as ours, and
they calculate a metric matrix A that is similar to
ours based on a set S of vector pairs (~xi, ~xj) that can
be regarded as similar. They report that the effec-
tiveness of A increases as the number of the training
pairs S increases; this requires O(n2) sample points
from n training data, and must be optimized by a
computationally expensive Newton-Raphson itera-
tion. On the other hand, our method uses only linear
algebra, and can induce an ideal metric using all the
training data at the same time. We believe this met-
ric can be useful for many vector-based language
processing methods that have used cosine similar-
ity.
There remains some future directions for re-
search. First, as we stated in section 4.3, the effect
of a cluster weighted generalized metric must be in-
vestigated and optimal weighting must be induced.
Second, as noted in section 5.2.1, the dimensional-
ity reduction required for linguistic data may con-
strain the performance of the metric distance. To
alleviate this problem, simultaneous dimensionality
reduction and metric induction may be necessary, or
the same idea in a kernel-based approach is worth
considering. The latter obviates the problem of di-
mensionality, while it restricts the usage to a situa-
tion where the kernel-based approach is available.
7 Conclusion
We proposed a global metric distance that is use-
ful for clustering or retrieval where Euclidean dis-
tance has been used. This distance is optimal in the
sense of quadratic minimization over all the clus-
ters in the training data. Experiments on sentence
retrieval, document retrieval and K-means cluster-
ing all showed improvements over Euclidean dis-
tance, with a significant refinement with tight train-
ing clusters in sentence retrieval.
Acknowledgement
The research reported here was supported in part by
a contract with the National Institute of Information
and Communications Technology entitled ?A study
of speech dialogue translation technology based on
a large corpus?.
References
Francis R. Bach and Michael I. Jordan. 2004.
Learning Spectral Clustering. In Advances in
Neural Information Processing Systems 16. MIT
Press.
Ricardo A. Baeza-Yates and Berthier A. Ribeiro-
Neto. 1999. Modern Information Retrieval.
ACM Press / Addison-Wesley.
C. L. Blake and C. J. Merz. 1998. UCI
Repository of machine learning databases.
http://www.ics.uci.edu/?mlearn/MLRepository.html.
Freddy Y. Y. Choi. 2000. Advances in domain inde-
pendent linear text segmentation. In Proceedings
of NAACL-00.
Michael Collins and Nigel Duffy. 2001. Convo-
lution Kernels for Natural Language. In NIPS
2001.
S. Deerwester, Susan T. Dumais, and George W.
Furnas. 1990. Indexing by Latent Semantic
Analysis. Journal of the American Society of In-
formation Science, 41(6):391?407.
Inderjit S. Dhillon and Dharmendra S. Modha.
2001. Concept Decompositions for Large Sparse
Text Data Using Clustering. Machine Learning,
42(1/2):143?175.
Richard O. Duda, Peter E. Hart, and David G. Stork.
2000. Pattern Classification *Second Edition.
John Wiley & Sons.
Marti Hearst. 1994. Multi-paragraph segmentation
of expository text. In 32nd. Annual Meeting of
the Association for Computational Linguistics,
pages 9?16.
Yoshiharu Ishikawa, Ravishankar Subramanya, and
Christos Faloutsos. 1998. MindReader: Query-
ing Databases Through Multiple Examples. In
Proc. 24th Int. Conf. Very Large Data Bases,
pages 218?227.
Tommi S. Jaakkola and David Haussler. 1999. Ex-
ploiting generative models in discriminative clas-
sifiers. In Proc. of the 1998 Conference on Ad-
vances in Neural Information Processing Sys-
tems, pages 487?493.
Eric P. Jiang and Michael W. Berry. 1998. Infor-
mation Filtering Using the Riemannian SVD (R-
SVD). In Proc. of IRREGULAR ?98, pages 386?
395.
Thorsten Joachims. 1998. Text categorization with
support vector machines: learning with many
relevant features. In Proceedings of ECML-98,
number 1398, pages 137?142.
Ken Lang. 1995. Newsweeder: Learning to filter
netnews. In Proceedings of the Twelfth Interna-
tional Conference on Machine Learning, pages
331?339.
Christopher D. Manning and Hinrich Schu?tze.
1999. Foundations of Statistical Natural Lan-
guage Processing. MIT Press.
K. R. Mu?ller, S. Mika, G. Ratsch, and K. Tsuda.
2001. An introduction to kernel-based learning
algorithms. IEEE Neural Networks, 12(2):181?
201.
G. Salton and C. S. Yang. 1973. On the specifica-
tion of term values in automatic indexing. Jour-
nal of Documentation, 29:351?372.
Matthew Schultz and Thorsten Joachims. 2004.
Learning a Distance Metric from Relative Com-
parisons. In Advances in Neural Information
Processing Systems 16. MIT Press.
Hinrich Schu?tze. 1992. Dimensions of Mean-
ing. In Proceedings of Supercomputing?92, pages
787?796.
F. Sugaya, T. Takezawa, G. Kikui, and S. Ya-
mamoto. 2002. Proposal for a very-large-corpus
acquisition method by cell-formed registration.
In Proc. LREC-2002, volume I, pages 326?328.
Jun Suzuki, Tsutomu Hirao, Yutaka Sasaki, and
Eisaku Maeda. 2003. Hierarchical Directed
Acyclic Graph Kernel: Methods for Structured
Natural Language Data. In Proc. of the 41th An-
nual Meeting of Association for Computational
Linguistics (ACL2003), pages 32?39.
Eric W. Weisstein. 2004. Moore-Penrose Matrix
Inverse. http://mathworld.wolfram.com/Moore-
PenroseMatrixInverse.html.
Eric P. Xing, Andrew Y. Ng, Michael I. Jordan,
and Stuart Russell. 2002. Distance metric learn-
ing, with application to clustering with side-
information. In NIPS 2002.
Appendix A.
Derivation of the metric matrix
Here we prove theorem 1, namely deriving M that
satisfies the condition
min
M
n
?
i=1
?
~sj?Xi
(~sj ? ~ci)T M(~sj ? ~ci) , (8)
under the constraint
|M | = 1. (9)
Expanding (8), we get
?
i
?
~sj
[ n
?
k=1
n
?
l=1
(sjk ? cik)mkl(sjl ? cil)
]
, (10)
and from (9), for all k
n
?
l=1
(?1)k+lmkl|Mkl| = 1 .
Therefore
n
?
k=1
n
?
l=1
(?1)k+lmkl|Mkl| = n, (11)
where Mkl denotes an adjugate matrix of mkl.
Therefore, we come to minimize (10) under the
constraint (11).
By introducing the Lagrange multiplier ?, we de-
fine
L =
N
?
i=1
?
~sj
[
?
k
?
l
(sjk ? cik)mkl(sjl ? cil)
]
??
[
?
k
?
l
(?1)k+lmkl|Mkl| ? n
]
.
Differentiating by mkl and setting to zero, we obtain
?L
?mkl
=
?
i
?
~sj
(sjk ? cik)(sjl ? cil)
? ?(?1)k+l|Mkl| = 0
? |Mkl| =
?
i
?
~sj (sjk ? cik)(sjl ? cil)
?(?1)k+l . (12)
Let us define M?1 = [m?1kl ]. Then,
m?1kl =
(?1)k+l|Mkl|
|M |
= (?1)k+l|Mkl| (... (9))
=
?
i
?
~sj (sjk ? cik)(sjl ? cil)
? (13)
(... (12))
Therefore, when we define
A = [akl] (14)
as
akl =
N
?
i=1
?
~sj?Xi
(sjl ? cil)(sjk ? cik) , (15)
from (13),
A = ?M?1
... |A| = ?n|M?1| = ?n
... ? = |A|1/n ,
where A is defined by (14), (15).
Appendix B.
Moore-Penrose Matrix Pseudoinverse
The Moore-Penrose matrix pseudoinverse A+ of A
is a unique matrix that has a property of normal in-
verse in that x = A+y is a shortest length least
squares solution to Ax = y even if A is singular
(Weisstein, 2004).
A+ can be calculated simply by a MATLAB
function pinv. Or alternatively (Ishikawa et al,
1998), we can decompose A as
A = U?UT ,
where U is an orthonormal n ? n matrix and ? =
diag(?1, . . . , ?R, 0, . . . , 0) (R = rank(A)). Then,
A+ is calculated as
A+ = U?+UT ,
where ?+ = diag(1/?1, . . . , 1/?R, 0, . . . , 0).
Therefore,
M = (?1?2 ? ? ? ?R)1/RA+.
Query: ?  
	 ?
(?How much is the total??)
Metric distance:
distance synonymous sentence
0.2712 Coling 2010: Poster Volume, pages 400?408,
Beijing, August 2010
Learning to Model Domain-Specific Utterance Sequences for Extractive
Summarization of Contact Center Dialogues
Ryuichiro Higashinaka?, Yasuhiro Minami?, Hitoshi Nishikawa?,
Kohji Dohsaka?, Toyomi Meguro?, Satoshi Takahashi?, Genichiro Kikui?
? NTT Cyber Space Laboratories, NTT Corporation
? NTT Communication Science Laboratories, NTT Corporation
higashinaka.ryuichiro@lab.ntt.co.jp, minami@cslab.kecl.ntt.co.jp
nishikawa.hitoshi@lab.ntt.co.jp, {dohsaka,meguro}@cslab.kecl.ntt.co.jp
{takahashi.satoshi,kikui.genichiro}@lab.ntt.co.jp
Abstract
This paper proposes a novel extractive
summarization method for contact cen-
ter dialogues. We use a particular
type of hidden Markov model (HMM)
called Class Speaker HMM (CSHMM),
which processes operator/caller utterance
sequences of multiple domains simulta-
neously to model domain-specific utter-
ance sequences and common (domain-
wide) sequences at the same time. We
applied the CSHMM to call summariza-
tion of transcripts in six different con-
tact center domains and found that our
method significantly outperforms compet-
itive baselines based on the maximum
coverage of important words using integer
linear programming.
1 Introduction
In modern business, contact centers are becom-
ing more and more important for improving cus-
tomer satisfaction. Such contact centers typically
have quality analysts who mine calls to gain in-
sight into how to improve business productivity
(Takeuchi et al, 2007; Subramaniam et al, 2009).
To enable them to handle the massive number of
calls, automatic summarization has been utilized
and shown to successfully reduce costs (Byrd et
al., 2008). However, one of the problems in cur-
rent call summarization is that a domain ontology
is required for understanding operator/caller utter-
ances, which makes it difficult to port one summa-
rization system from domain to domain.
This paper describes a novel automatic sum-
marization method for contact center dialogues
without the costly process of creating domain on-
tologies. More specifically, given contact center
dialogues categorized into multiple domains, we
create a particular type of hidden Markov model
(HMM) called Class Speaker HMM (CSHMM)
to model operator/caller utterance sequences. The
CSHMM learns to distinguish sequences of indi-
vidual domains and common sequences in all do-
mains at the same time. This approach makes it
possible to accurately distinguish utterances spe-
cific to a certain domain and thereby has the po-
tential to generate accurate extractive summaries.
In Section 2, we review recent work on auto-
matic summarization, including its application to
contact center dialogues. In Section 3, we de-
scribe the CSHMM. In Section 4, we describe
our automatic summarization method in detail. In
Section 5, we describe the experiment we per-
formed to verify our method and present the re-
sults. In Section 6, we summarize and mention
future work.
2 Related Work
There is an abundance of research in automatic
summarization. It has been successfully applied to
single documents (Mani, 2001) as well as to mul-
tiple documents (Radev et al, 2004), and various
summarization methods, such as the conventional
LEAD method, machine-learning based sentence
selection (Kupiec et al, 1995; Osborne, 2002),
and integer linear programming (ILP) based sen-
tence extraction (Gillick and Favre, 2009), have
been proposed. Recent years have seen work on
summarizing broadcast news speech (Hori and
Furui, 2003), multi-party meetings (Murray et al,
2005), and contact center dialogues (Byrd et al,
2008). However, despite the large amount of pre-
vious work, little work has tackled the automatic
summarization of multi-domain data.
400
In the past few decades, contact center dia-
logues have been an active research focus (Gorin
et al, 1997; Chu-Carroll and Carpenter, 1999).
Initially, the primary aim of such research was
to transfer calls from answering agents to oper-
ators as quickly as possible in the case of prob-
lematic situations. However, real-time processing
of calls requires a tremendous engineering effort,
especially when customer satisfaction is at stake,
which led to recent work on the offline process-
ing of calls, such as call mining (Takeuchi et al,
2007) and call summarization (Byrd et al, 2008).
The work most related to ours is (Byrd et al,
2008), which maps operator/caller utterances to
an ontology in the automotive domain by using
support vector machines (SVMs) and creates a
structured summary by heuristic rules that assign
the mapped utterances to appropriate summary
sections. Our work shares the same motivation
as theirs in that we want to make it easier for
quality analysts to analyze the massive number of
calls. However, we tackle the problem differently
in that we propose a newmodeling of utterance se-
quences for extractive summarization that makes
it unnecessary to create heuristics rules by hand
and facilitates the porting of a summarization sys-
tem.
HMMs have been successfully applied to au-
tomatic summarization (Barzilay and Lee, 2004).
In their work, an HMM was used to model the
transition of content topics. The Viterbi decod-
ing (Rabiner, 1990) was performed to find con-
tent topics that should be incorporated into a sum-
mary. Their approach is similar to ours in that
HMMs are utilized to model topic sequences, but
they did not use data of multiple domains in creat-
ing their model. In addition, their method requires
training data (original articles with their reference
summaries) in order to find which content top-
ics should be included in a summary, whereas our
method requires only the raw sequences with their
domain labels.
3 Class Speaker HMM
A Class Speaker HMM (CSHMM) is an exten-
sion of Speaker HMM (SHMM), which has been
utilized to model two-party conversations (Me-
guro et al, 2009). In an SHMM, there are two
states, and each state emits utterances of one of
the two conversational participants. The states are
1:speaker1 2:speaker2
Speaker HMM for Class 1
3:speaker1 4:speaker2
Speaker HMM for Class 2
Figure 1: Topology of an ergodic CSHMM. Num-
bers before ?speaker1? and ?speaker2? denote state
IDs.
connected ergodically and the emission/transition
probabilities are learned from training data by
using the EM-algorithm. Although Meguro et
al., (2009) used SHMMs to analyze the flow of
listening-oriented dialogue, we extend their idea
to make it applicable to classification tasks, such
as dialogue segmentation.
A CSHMM is simply a concatenation of
SHMMs, each of which is trained by using ut-
terance sequences of a particular dialogue class.
After such SHMMs are concatenated, the Viterbi
algorithm is used to decode an input utterance
sequence into class labels by estimating from
which class each utterance has most likely to have
been generated. Figure 1 illustrates the basic
topology of a CSHMM where two SHMMs are
concatenated ergodically. When the most likely
state sequence for an input utterance sequence is
<1,3,4,2>, we can convert these state IDs into
their corresponding classes; that is, <1,2,2,1>,
which becomes the result of utterance classifica-
tion.
We have conceived three variations of CSHMM
as we describe below. They differ in how we treat
utterance sequences that appear commonly in all
classes and how we train the transition probabili-
ties between independently trained SHMMs.
3.1 Ergodic CSHMM
The most basic CSHMM is the ergodic CSHMM,
which is a simple concatenation of SHMMs in
an ergodic manner as shown in Fig. 1. For K
classes, K SHMMs are combined with the initial
and transition probabilities all set to equal. In this
CSHMM, the assignment of class labels solely de-
pends on the output distributions of each class.
3.2 Ergodic CSHMM with Common States
This type of CSHMM is the same as the ergodic
CSHMM except that it additionally has a SHMM
trained from all dialogues of all classes. There-
401
3:speaker1 4:speaker2
1:speaker1 2:speaker2
5:speaker1 6:speaker2
Speaker HMM for Class 1
Speaker HMM for Class 2
Speaker HMM for All Classes (Class 0)
Figure 2: CSHMM with common states.
Copy
Class
1
M1
M1M0
Retrain
Train
Class
k
M0
Mk
Mk
Retrain
Train
Class
K
M0
MK
MK
Retrain
Train
All 
Classes
M0
Train
+
M0
M1 Mk MK
AVG
Concatenate
M1+0 Mk+0 MK+0
M1M0 M0 Mk
M0 MK
M1+0 Mk+0 MK+0
Step 1
Step 2
Step 3
Step 2?
END
Mconcat
If the fitting has 
converged for 
all Mk+0
Split Mconcat into 
pairs again and 
retrain Mk+0
M1?MK become 
less likely to
output common 
sequences
Transition probabilities
of M0 are redistributed
between M0 and Mk
Figure 3: Three steps to create a CSHMM using
concatenated training.
fore, for K classes, this CSHMM has K + 1
SHMMs. Figure 2 shows the model topology.
This newly added SHMM works in a manner sim-
ilar to the background model (Reynolds et al,
2000) representing sequences that are common
to all classes. By having these common states,
common utterance sequences can be classified as
?common?, making it possible to avoid forcefully
classifying common utterance sequences into one
of the given classes.
Detecting common sequences is especially
helpfulwhen several classes overlap in nature. For
example, most dialogues commonly start and end
with greetings, and many calls at contact centers
commonly contain exchanges in which the opera-
tor requests personal information about the caller
for confirmation. Regarding the model topology
in Fig. 2, if the most likely state sequence by
the Viterbi decoding is <1,4,5,6,3,2>, we obtain
a class label sequence <1,2,0,0,2,1> where the
third and fourth utterances are classified as ?zero?,
meaning that they do not belong to any class.
3.3 CSHMM using Concatenated Training
The CSHMMs presented so far have two prob-
lems: one is that the order of utterances of differ-
ent classes cannot be taken into account because
of the equal transition probabilities. As a result,
the very merit of HMMs, their ability to model
time series data, is lost. The other is that the out-
put distributions of common states may be overly
broad because they are the averaged distributions
over all classes; that is, the best path determined
by the Viterbi decoding may not go through the
common states at all.
Our solution to these problems is to apply con-
catenated training (Lee, 1989), which has been
successfully used in speech recognition to model
phoneme sequences in an unsupervised manner.
The procedure for concatenated training is illus-
trated in Fig. 3 and has three steps.
step 1 Let Mk (Mk ? M, 1 ? k ? K) be the
SHMM trained using dialogues Dk where
Dk = {?dj|c(dj) = k}, and M0 be the
SHMM trained using all dialogues; i.e., D.
Here, K means the total number of classes
and c(dj) the class assigned to a dialogue dj .
step 2 Connect each Mk ? M with a copy of
M0 using equal initial and transition proba-
bilities (we call this connected model Mk+0)
and retrain Mk+0 with ?dj ? Dk where
c(dj) = k.
step 3 Merge all models Mk+0 (1 ? k ? K) to
produce one concatenated HMM (Mconcat).
Here, the output probabilities of the copies
of M0 are averaged over K when all models
are merged to create a combined model. If
the fitting of all Mk+0 models has converged
against the training data, exit this procedure;
otherwise, go to step 2 by connecting a copy
of M0 and Mk for all k. Here, the transi-
tion probabilities from M0 to Ml(l 6= k) are
summed and equally distributed between the
copied M0?s self-loop and transitions to the
states in Mk.
In concatenated training, the transition and output
probabilities can be optimized between M0 and
402
Contact
Center
Dialogues
Domain 1
Domain K
?
HMM for Domain 1 HMM for Domain K
HMM for All Domains
Model topic label 
sequences
INPUT: A dialogue in Domain k
Topic Model
Topic label sequence
L
S
A
/
L
D
A
A
s
s
i
g
n
 
t
o
p
i
c
l
a
b
e
l
s
Domain label sequence OUTPUT: summary
Viterbi decoding
Assign
topic labels
Select utterances labeled with Domain k
Class Speaker
HMM
?..
Utterance sequence
Feature sequence
Extract content words
as utterance features
Figure 4: Overview of our summarization
method.
Mk, meaning that the output probabilities of utter-
ance sequences that are common and also found
in Mk can be moved from Mk to M0. This makes
the distribution of Mk sharp (not broad/uniform),
making it likely to output only the utterances rep-
resentative of a class k. As regards M0, its distri-
bution of output probabilities can also be sharp-
ened for utterances that occur commonly in all
classes. This sharpening of distributions is likely
to be helpful for class discrimination.
4 Summarization Method
We apply CSHMMs to extractive summarization
of contact center dialogues because such dia-
logues are two-party, can be categorized into mul-
tiple classes by their call domains (e.g., inquiry
types), and are likely contain many overlapping
exchanges between an operator and a caller across
domains, such as greetings, the confirmation of
personal information, and other cliches in busi-
ness (e.g., name exchanges, thanking/apologizing
phrases, etc.), making them the ideal target for
CSHMMs.
In our method, summarization is performed by
decoding a sequence of utterances of a domain
DMk into domain labels and selecting those ut-
terances that have domain labels DMk. This
makes it possible to extract utterances that are
characteristic of DMk in relation to other do-
mains. Our assumption is that extracting charac-
teristic sequences of a given domain provides a
good summary for that domain because such se-
quences should contain important information ne-
cessitated by the domain.
Figure 4 outlines our extractive summarization
process. The process consists of a training phase
and a decoding phase as described below.
Training phase: Let D (d1 . . . dN ) be the entire
set of contact center dialogues, DMk (DMk ?
DM, 1 ? k ? K) the domain assigned to do-
main k, and Udi,1 . . .Udi,H the utterances in di.
Here, H is the number of utterances in di. From
D, we create two models: a topic model (TM )
and a CSHMM.
The topic model is used to assign a single topic
to each utterance so as to facilitate the training
of the CSHMM by reducing the dimensions of
the feature space. The same approach has been
taken in (Barzilay and Lee, 2004). The topic
model can be created by such techniques as prob-
abilistic latent semantic analysis (PLSA) (S?ingliar
and Hauskrecht, 2006) and latent Dirichlet alo-
cation (LDA) (Tam and Schultz, 2005). PLSA
models the latent topics of the documents and its
Baysian extension is LDA, which also models the
co-occurrence of topics using the Dirichlet prior.
We first derive features Fd1 . . . FdN for the dia-logues. Here, we assume a bag-of-words repre-
sentation for the features; therefore, Fdi is repre-sented as {< w1, c1 > . . . < wV , cV >}, where
V means the total number of content words in the
vocabulary and < wi, ci > denotes that a content
word wi appears ci times in a dialogue. Note that
we derive the features for dialogues, not for utter-
ances, because utterances in dialogue can be very
short, often consisting of only one or two words
and thus making it hard to calculate the word co-
occurrence required for creating a topic model.
From the features, we build a topic model that in-
cludes P(z|w), where w is a word and z is a topic.
Using the topic model, we can assign a single
topic label to every utterance in D by finding its
likely topic; i.e., argmax
z
?
w?words(Udi) P(z|w).
After labeling all utterances in D with topic la-
bels, we train a CSHMM that learns characteristic
topic label sequences in each domain as well as
common topic label sequences across domains.
Decoding phase: Let dj be the input dialogue,
DM(dj) (? DM ) the table for obtaining the do-
main label of dj , and Udj ,1 . . .Udj ,Hdj the utter-ances in dj, where Hdj is the number of the utter-ances. We use TM to map the utterances to topic
403
Domain # Tasks Sentences Characters
FIN 15 8.93 289.93
ISP 15 7.20 259.53
LGU 20 9.85 328.55
MO 15 10.07 326.20
PC 15 9.40 354.07
TEL 18 8.44 322.22
ALL 98 9.01 314.46
Table 1: Scenario statistics: the number of tasks
and averaged number of sentences/characters in a
task scenario in the six domains.
labels Tdj ,1 . . .Tdj ,Hdj and convert them into do-main label sequences DMdj ,1 . . .DMdj ,Hdj us-ing the trained CSHMM by the Viterbi decoding.
Then, we select Udj ,h (1 ? h ? Hdj ) whose cor-responding domain labelDMdj ,h equalsDM(dj)and output the selected utterances in the order of
appearance in the original dialogue as a summary.
5 Experiment
We performed an experiment to verify our sum-
marization method. We first collected simulated
contact center dialogues using human subjects.
Then, we compared our method with baseline sys-
tems. Finally, we analyzed the created summaries
to investigate what had been learned by our CSH-
MMs.
5.1 Dialogue Data
Since we do not have access to actual contact cen-
ter data, we recruited human subjects to collect
simulated contact center dialogues. A total of 90
participants (49 males and 41 females) took the
roles of operator or a caller and talked over tele-
phones in separate rooms. The callers were given
realistic scenarios that included their motivation
for a call as well as detailed instructions about
what to ask. The operators, who had experience
of working at contact centers, were given manuals
containing the knowledge of the domain and ex-
plaining how to answer questions in specific sce-
narios.
The dialogues took place in six different do-
mains: Finance (FIN), Internet Service Provider
(ISP), Local Government Unit (LGU), Mail Or-
der (MO), PC support (PC), and Telecommuni-
cation (TEL). In each domain, there were 15?20
tasks. Table 1 shows the statistics of the task sce-
narios used by the callers. We cannot describe the
details of each domain for lack of space, but ex-
MO task No. 3: It is becoming a good season for the
Japanese Nabe (pan) cuisine. You own a Nabe restau-
rant and it is going well. When you were searching on
the Internet, thinking of creating a new dish, you saw
that drop-shipped Shimonoseki puffer fish was on sale.
Since you thought the puffer fish cuisine would become
hot in the coming season, you decided to order it as a
trial. . . . You ordered a puffer fish set on the Internet,
but you have not received the confirmation email that
you were supposed to receive. . . . You decided to call
the contact center to make an inquiry, ask them whether
the order has been successful, and request them to send
you the confirmation email.
Figure 5: Task scenario in the MO domain. The
scenario was originally in Japanese and was trans-
lated by the authors.
amples of the tasks for FIN are inquiries about in-
surance, notifications of the loss of credit cards,
and applications for finance loans, and those for
ISP are inquiries about fees for Internet access, re-
quests to forward emails, and reissuance of pass-
words. Figure 5 shows one of the task scenarios
in the MO domain.
We collected data on two separate occasions us-
ing identical scenarios but different participants,
which gave us two sets of dialogue data. We used
the former for training our summarization sys-
tem and the latter for testing. We only use the
transcriptions in this paper so as to avoid partic-
ular problems of speech. All dialogues were in
Japanese. Tables 2 and 3 show the statistics of the
training data and the test data, respectively. As
can be seen from the tables, each dialogue is quite
long, which attests to the complexity of the tasks.
5.2 Training our Summarization System
For training our system, we first created a topic
model using LDA.We performed a morphological
analysis using ChaSen1 to extract content words
from each dialogue and made its bag-of-words
features. We defined content words as nouns,
verbs, adjectives, unknown words, and interjec-
tions (e.g., ?yes?, ?no?, ?thank you?, and ?sorry?).
We included interjections because they occur very
frequently in dialogues and often possess impor-
tant content, such as agreement and refusal, in
transactional communication. We use this defini-
tion of content words throughout the paper.
Then, using an LDA software package2, we
built a topic model. We tentatively set the number
1http://chasen-legacy.sourceforge.jp/
2http://chasen.org/?daiti-m/dist/lda/
404
Utterances/Dial. Characters/Utt.
Domain # dial. OPE CAL Both OPE CAL Both
FIN 59 75.73 72.69 148.42 17.44 7.54 12.59
ISP 64 55.09 53.17 108.27 20.11 8.03 14.18
LGU 76 58.28 50.55 108.83 12.83 8.55 10.84
MO 70 66.39 58.74 125.13 15.09 7.43 11.49
PC 56 89.34 77.80 167.14 15.48 6.53 11.31
TEL 66 75.58 63.97 139.55 12.74 8.24 10.67
ALL 391 69.21 61.96 131.17 15.40 7.69 11.76
Table 2: Training data statistics: Averaged num-
ber of utterances per dialogue and characters per
utterance for each domain. OPE and CAL denote
operator and caller, respectively. See Section 5.1
for the full domain names.
Utterances/Dial. Characters/Utt.
Domain # dial. OPE CAL Both OPE CAL Both
FIN 60 73.97 61.05 135.02 14.53 7.50 11.35
ISP 59 76.08 61.24 137.32 15.43 6.94 11.65
LGU 56 66.55 51.59 118.14 14.54 7.53 11.48
MO 47 75.53 64.87 140.40 10.53 6.79 8.80
PC 44 124.02 94.16 218.18 14.23 7.79 11.45
TEL 41 93.71 68.54 162.24 13.94 7.85 11.37
ALL 307 83.07 65.69 148.76 13.98 7.41 11.08
Table 3: Test data statistics.
of topics to 100. Using this topic model, we la-
beled all utterances in the training data using these
100 topic labels.
We trained seven different CSHMMs in all: one
ergodic CSHMM (ergodic0), three variants of er-
godic CSHMMs with common states (ergodic1,
ergodic2, ergodic3), and three variants of CSH-
MMs with concatenated training (concat1, con-
cat2, concat3). The difference within the variants
is in the number of common states. The numbers
0?3 after ?ergodic? and ?concat? indicate the num-
ber of SHMMs containing common states. For
example, ergodic3 has nine SHMMs (six SHMMs
for the six domains plus three SHMMs contain-
ing common states). Since more states would
enable more minute modeling of sequences, we
made such variants in the hope that common se-
quences could be more accurately modeled. We
also wanted to examine the possibility of creat-
ing sharp output distributions in common states
without the concatenated training by such minute
modeling. These seven CSHMMs make seven dif-
ferent summarization systems.
5.3 Baselines
Baseline-1: BL-TF We prepared two baseline
systems for comparison. One is a simple sum-
marizer based on the maximum coverage of high
term frequency (TF) content words. We call
this baseline BL-TF. This baseline summarizes a
dialogue by maximizing the following objective
function:
max
?
zi?Z
weight(wi) ? zi
where ?weight? returns the importance of a con-
tent word wi and zi is a binary value indicating
whether to include wi in the summary. Here,
?weight? returns the count of wi in a given dia-
logue. The maximization is done using ILP (we
used an off-the-shelf solver lp solve3) with the
following three constraints:
xi, zi ? {0, 1}
?
xi?X
lixi ? K
?
i
mijxi ? zj (?zj ? Z)
where xi is a binary value that indicates whether
to include the i-th utterance in the summary, li is
the length of the i-th utterance,K is the maximum
number of characters to include in a summary, and
mij is a binary value that indicates whether wi is
included in the j-th utterance. The last constraint
means that if a certain utterance is included in the
summary, all words in that utterance have to be
included in the summary.
Baseline-2: BL-DD Although BL-TF should be
a very competitive baseline because it uses the
state-of-the-art formulation as noted in (Gillick
and Favre, 2009), having only this baseline is
rather unfair because it does not make use of the
training data, whereas our proposed method uses
them. Therefore, we made another baseline that
learns domain-specific dictionaries (DDs) from
the training data and incorporates them into the
weights of content words of the objective function
of BL-TF. We call this baseline BL-DD. In this
baseline, the weight of a content word wi in a do-
main DMk is
weight(wi,DMk) =
log(P(wi|DMk))
log(P(wi|DM\DMk))
3http://lpsolve.sourceforge.net/5.5/
405
Metric ergodic0 ergodic1 ergodic2 ergodic3 concat1 concat2 concat3
PROPOSED
F 0.177 0.177 0.177 0.177 0.187?e0e1e2e3 0.198?+e0e1e2e3c1 0.199?+e0e1e2e3c1precision 0.145 0.145 0.145 0.145 0.161? 0.191?+ 0.195?+
recall 0.294 0.294 0.294 0.294 0.280? 0.259?+ 0.259?+
(Same-length) BL-TF
F 0.171 0.171 0.171 0.171 0.168 0.164 0.163
precision 0.132 0.132 0.132 0.132 0.135 0.140 0.140
recall 0.294 0.294 0.294 0.294 0.270 0.241 0.240
(Same-length) BL-DD
F 0.189 0.189 0.189 0.189 0.189 0.187 0.187
precision 0.155 0.155 0.155 0.155 0.162 0.170 0.172
recall 0.287 0.287 0.287 0.287 0.273 0.250 0.248
Compression Rate 0.42 0.42 0.42 0.42 0.37 0.30 0.30
Table 4: F-measure, precision, and recall averaged over all 307 dialogues (cf. Table 3) in the test
set for the proposed methods and baselines BL-TF and BL-DD configured to output the same-length
summaries as the proposed systems. The averaged compression rate for each proposed system is shown
at the bottom. The columns (ergodic0?concat3) indicate our methods as well as the character lengths
used by the baselines. Asterisks, ?+?, e0?e3, and c1?c3 indicate our systems? statistical significance by
the Wilcoxon signed-rank test (p<0.01) over BL-TF, BL-DD, ergodic0?3, and concat1?3, respectively.
Statistical tests for the precision and recall were only performed between the proposed systems and
their same-length baseline counterparts. Bold font indicates the best score in each row.
where P(wi|DMk) denotes the occurrence prob-
ability of wi in the dialogues of DMk , and
P(wi|DM\DMk) the occurrence probability of
wi in all domains except for DMk. This log like-
lihood ratio estimates how much a word is char-
acteristic of a given domain. Incorporating such
weights would make a very competitive baseline.
5.4 Evaluation Procedure
We made our seven proposed systems and two
baselines (BL-TF and BL-DD) output extractive
summaries for the test data. Since one of the
shortcomings of our proposedmethod is its inabil-
ity to set the compression rate, we made our sys-
tems output summaries first and made the baseline
systems output their summaries within the charac-
ter lengths of our systems? summaries.
We used scenario texts (See Fig. 5) as reference
data; that is, a dialogue dealing with a certain task
is evaluated using the scenario text for that task.
As an evaluation criterion, we used the F-measure
(F1) to evaluate the retrieval accuracy on the ba-
sis of the recall and precision of retrieved content
words. We used the scenarios as references be-
cause they contain the basic content exchanged
between an operator and a caller, the retrieval ac-
curacy of which should be important for quality
analysts.
We could have used ROUGE (Lin and Hovy,
2003), but we did not because ROUGE does not
correlate well with human judgments in conversa-
tional data (Liu and Liu, 2008). Another benefit of
using the F-measure is that summaries of varying
lengths can be compared.
5.5 Results
Table 4 shows the evaluation results for the pro-
posed systems and the baselines. It can be seen
that concat3 shows the best performance in F-
measure among all systems, having a statistically
better performance over all systems except for
concat2. The CSHMMs with concatenated train-
ing were all better than ergodic0?3. Here, the per-
formance (and output) of ergodic0?3 was exactly
the same. This happened because of the broad dis-
tributions in their common states; no paths went
through the common states and all paths went
through the SHMMs of the six domains instead.
The evaluation results in Table 4 may be rather
in favor of our systems because the summarization
lengths were set by the proposed systems. There-
fore, we performed another experiment to inves-
tigate the performance of the baselines with vary-
ing compression rates and compared their perfor-
mance with the proposed systems in F-measure.
We found that the best performance was achieved
by BL-DD when the compression rate was 0.4
with the F-measure of 0.191, which concat3 sig-
nificantly outperformed by the Wilcoxon signed-
rank test (p<0.01). Note that the performance
shown in Table 4 may seem low. However, we
found that the maximum recall is 0.355 (cal-
406
CAL1 When I order a product from you, I get a confir-
mation email
CAL2 Puffer fish
CAL3 Sets I have ordered, but I haven?t received
the confirmation email
OPE1 Order
OPE2 I will make a confirmation whether you have
ordered
OPE3 Ten sets of Shimonoseki puffer fish by drop-
ship
OPE4 ?Yoriai? (name of the product)
OPE5 Two kilos of bony parts of tiger puffer fish
OPE6 Baked fins for fin sake
OPE7 600 milliliter of puffer fish soy sauce
OPE8 And, grated radish and red pepper
OPE9 Your desired delivery date is the 13th of Febru-
ary
CAL4 Yes, all in small cases
CAL5 This is q in alphabet right?
CAL6 Hyphen g
CAL7 You mean that the order was successful
OPE10 Yes, it was Nomura at JDS call center
Figure 6: Example output of concat3 for MO task
No. 3 (cf Fig. 5). The utterances were translated
by the authors. The compression rate for this dia-
logue was 0.24.
culated by using summaries with no compres-
sion). This means that the maximum F-measure
we could attain is lower than 0.524 (when the pre-
cision is ideal with 1). This is because of the dif-
ferences between the scenarios and the actual di-
alogues. We want to pursue ways to improve our
evaluation methodology in the future.
Despite such issues in evaluation, from the re-
sults, we conclude that our extractive summa-
rization method is effective and that having the
common states and training CSHMMs with con-
catenated training are useful in modeling domain-
specific sequences of contact center dialogues.
5.6 Example of System Output
Figure 6 shows an example output of concat3 for
the scenario MO task No. 3 (cf. Fig. 5). Bold font
indicates utterances that were NOT included in the
summary of concat3?s same-length-BF-DD coun-
terpart. It is clear that sequences related to the
MO domain were successfully extracted. When
we look at the summary of BF-DD, we see such
utterances as ?Can I have your address from the
postcode? and ?Finally, can I have your email ad-
dress?, which are obvious cliches in contact center
dialogues. This indicates the usefulness of com-
mon states for ignoring such common exchanges.
6 Summary and Future Work
This paper proposed a novel extractive sum-
marization method for contact center dialogues.
We devised a particular type of HMM called
CSHMM, which processes operator/caller utter-
ance sequences of multiple domains simulta-
neously to model domain-specific utterance se-
quences and common sequences at the same time.
We trained a CSHMM using the transcripts of
simulated contact center dialogues and verified its
effectiveness for the summarization of calls.
There still remain several limitations in our ap-
proach. One is its inability to change the com-
pression rate, which we aim to solve in the next
step using the forward-backward algorithm (Ra-
biner and Juang, 1986). This algorithm can cal-
culate the posterior probability of each state at
each time frame given an input dialogue sequence,
enabling us to extract top-N domain-specific se-
quences. We also need to find the appropriate
topic number for the topic model. In our imple-
mentation, we used a tentative value of 100, which
may not be appropriate. In addition, we believe
the topic model and the CSHMM can be unified
because these models are fundamentally similar,
especially when LDA is employed. Model topolo-
gies may also have to be reconsidered. In our
CSHMM with concatenated training, the states in
domain-specific SHMMs are only connected to
the common states, which may be inappropriate
because there could be a case where a domain
changes from one to another without having a
common sequence. ApplyingCSHMMs to speech
and other NLP tasks is another challenge. As a
near-term goal, we aim to apply our method to the
summarization of meetings, where we will need to
extend our CSHMMs to deal with more than two
participants. Finally, we also want to build a con-
tact center dialogue agent by extending the CSH-
MMs to partially observableMarkov decision pro-
cesses (POMDPs) (Williams and Young, 2007) by
following the recent work on building POMDPs
from dialogue data in the dynamic Bayesian net-
work (DBN) framework (Minami et al, 2009).
Acknowledgments
We thank the members of the Spoken Dialog
System Group, especially Noboru Miyazaki and
Satoshi Kobashikawa, for their effort in dialogue
data collection.
407
References
Barzilay, Regina and Lillian Lee. 2004. Catching the drift:
Probabilistic content models, with applications to gener-
ation and summarization. In Proceedings of the Human
Language Technology Conference of the North American
Chapter of the Association for Computational Linguistics
(HLT-NAACL), pages 113?120.
Byrd, Roy J., Mary S. Neff, Wilfried Teiken, Youngja
Park, Keh-Shin F. Cheng, Stephen C. Gates, and Karthik
Visweswariah. 2008. Semi-automated logging of contact
center telephone calls. In Proceeding of the 17th ACM
conference on Information and knowledge management
(CIKM), pages 133?142.
Chu-Carroll, Jennifer and Bob Carpenter. 1999. Vector-
based natural language call routing. Computational Lin-
guistics, 25(3):361?388.
Gillick, Dan and Benoit Favre. 2009. A scalable global
model for summarization. In Proceedings of the Work-
shop on Integer Linear Programming for Natural Lan-
guage Processing, pages 10?18.
Gorin, Allen L., Giuseppe Riccardi, and Jerry H. Wright.
1997. How may I help you? Speech Communication,
23(1-2):113?127.
Hori, Chiori and Sadaoki Furui. 2003. A new approach to
automatic speech summarization. IEEE Transactions on
Multimedia, 5(3):368?378.
Kupiec, Julian, Jan Pedersen, and Francine Chen. 1995. A
trainable document summarizer. In Proceedings of the
18th annual international ACM SIGIR conference on Re-
search and development in information retrieval (SIGIR),
pages 68?73.
Lee, Kai-Fu. 1989. Automatic speech recognition: the de-
velopment of the SPHINX system. Kluwer Academic Pub-
lishers.
Lin, Chin-Yew and Eduard Hovy. 2003. Automatic evalua-
tion of summaries using n-gram co-occurrence statistics.
In Proceedings of the 2003Conference of the North Amer-
ican Chapter of the Association for Computational Lin-
guistics on Human Language Technology (NAACL-HLT),
pages 71?78.
Liu, Feifan and Yang Liu. 2008. Correlation between
ROUGE and human evaluation of extractive meeting sum-
maries. In Proceedings of the 46th Annual Meeting of
the Association for Computational Linguistics on Human
Language Technologies (HLT), pages 201?204.
Mani, Inderjeet. 2001. Automatic summarization. John
Benjamins Publishing Company.
Meguro, Toyomi, Ryuichiro Higashinaka, Kohji Dohsaka,
Yasuhiro Minami, and Hideki Isozaki. 2009. Analysis of
listening-oriented dialogue for building listening agents.
In Proceedings of the SIGDIAL 2009 conference, pages
124?127.
Minami, Yasuhiro, Akira Mori, Toyomi Meguro, Ryuichiro
Higashinaka, Kohji Dohsaka, and Eisaku Maeda. 2009.
Dialogue control algorithm for ambient intelligence based
on partially observable Markov decision processes. In
Proceedings of the 1st international workshop on spoken
dialogue systems technology (IWSDS), pages 254?263.
Murray, Gabriel, Steve Renals, and Jean Carletta. 2005. Ex-
tractive summarization of meeting recordings. In Pro-
ceedings of the 9th European Conference on Speech
Communication and Technology (EUROSPEECH), pages
593?596.
Osborne, Miles. 2002. Using maximum entropy for sen-
tence extraction. In Proceedings of the ACL-02 Workshop
on Automatic Summarization, pages 1?8.
Rabiner, Lawrence R. and Biing-Hwang Juang. 1986. An
introduction to hiddenMarkov models. IEEE ASSP Mag-
azine, 3(1):4?16.
Rabiner, Lawrence R. 1990. A tutorial on hidden Markov
models and selected applications in speech recognition.
Readings in speech recognition, 53(3):267?296.
Radev, Dragomir R., Hongyan Jing, Ma?gorzata Stys?, and
Daniel Tam. 2004. Centroid-based summarization of
multiple documents. Information Processing & Manage-
ment, 40(6):919?938.
Reynolds, Douglas A., Thomas F. Quatieri, and Robert B.
Dunn. 2000. Speaker verification using adaptedGaussian
mixture models. Digital Signal Processing, 10(1-3):19 ?
41.
Subramaniam, L. Venkata, Tanveer A. Faruquie, Shajith Ik-
bal, Shantanu Godbole, and Mukesh K. Mohania. 2009.
Business intelligence from voice of customer. In Pro-
ceedings of the 2009 IEEE International Conference on
Data Engineering (ICDE), pages 1391?1402.
Takeuchi, Hironori, L Venkata Subramaniam, Tetsuya Na-
sukawa, Shourya Roy, and Sreeram Balakrishnan. 2007.
A conversation-mining system for gathering insights to
improve agent productivity. In Proceedings of the IEEE
International Conference on E-Commerce Technology
and the IEEE International Conference on Enterprise
Computing, E-Commerce, and E-Services, pages 465?
468.
Tam, Yik-Cheung and Tanja Schultz. 2005. Dynamic
language model adaptation using variational Bayes in-
ference. In Proceedings of the 9th European Confer-
ence on Speech Communication and Technology (EU-
ROSPEECH), pages 5?8.
S?ingliar, Tomas and Milos Hauskrecht. 2006. Noisy-OR
component analysis and its application to link analy-
sis. The Journal of Machine Learning Research, 7:2189?
2213.
Williams, JasonD. and Steve Young. 2007. Partially observ-
able Markov decision processes for spoken dialog sys-
tems. Computer Speech & Language, 21(2):393?422.
408
Coling 2010: Poster Volume, pages 409?417,
Beijing, August 2010
Recognizing Relation Expression between Named Entities based on
Inherent and Context-dependent Features of Relational words
Toru Hirano?, Hisako Asano?, Yoshihiro Matsuo?, Genichiro Kikui?
?NTT Cyber Space Laboratories, NTT Corporation
?Innovative IP Architecture Center, NTT Communications Corporation
hirano.tohru@lab.ntt.co.jp
hisako.asano@ntt.com
{matsuo.yoshihiro,kikui.genichiro}@lab.ntt.co.jp
Abstract
This paper proposes a supervised learn-
ing method to recognize expressions that
show a relation between two named en-
tities, e.g., person, location, or organiza-
tion. The method uses two novel fea-
tures, 1) whether the candidate words in-
herently express relations and 2) how the
candidate words are influenced by the past
relations of two entities. These features
together with conventional syntactic and
contextual features are organized as a tree
structure and are fed into a boosting-based
classification algorithm. Experimental re-
sults show that the proposed method out-
performs conventional methods.
1 Introduction
Much attention has recently been devoted to us-
ing enormous amount of web text covering an ex-
ceedingly wide range of domains as a huge knowl-
edge resource with computers. To use web texts as
knowledge resources, we need to extract informa-
tion from texts that are merely sequences of words
and convert them into a structured form. Although
extracting information from texts as a structured
form is difficult, relation extraction is a way that
makes it possible to use web texts as knowledge
resources.
The aim of relation extraction is to extract se-
mantically related named entity pairs, X and Y ,
and their relation, R, from a text as a struc-
tured form [X , Y , R]. For example, the triple
[Yukio Hatoyama, Japan, prime minister] would
be extracted from the text ?Yukio Hatoyama is the
prime minister of Japan?. This extracted triple
provides important information used in informa-
tion retrieval (Zhu et al, 2009) and building an
ontology (Wong et al, 2010).
It is possible to say that all named entity pairs
that co-occur within a text are semantically related
in some way. However, we define that named en-
tity pairs are semantically related if they satisfy
either of the following rules:
? One entity is an attribute value of the other.
? Both entities are arguments of a predicate.
Following the above definition, explicit and im-
plicit relations should be extracted. An explicit re-
lation means that there is an expression that shows
the relation between a named entity pair in a given
text, while an implicit relation means that there is
no such expression. For example, the triple [Yukio
Hatoyama, Kunio Hatoyama, brother] extracted
from the text ?Yukio Hatoyama, the Democratic
Party, is Kunio Hatoyama?s brother? is an explicit
relation. In contrast, the triple [Yukio Hatoyama,
the Democratic Party, member] extracted from the
same text is an implicit relation because there is
no expression showing the relation (e.g. member)
between ?Yukio Hatoyama? and ?the Democratic
Party? in the text.
Extracting triples [X , Y , R] from a text in-
volves two tasks. One is detecting semantically
related pairs from named entity pairs that co-occur
in a text and the other is determining the rela-
tion between a detected pair. For the former task,
various supervised learning methods (Culotta and
Sorensen, 2004; Zelenko et al, 2003; Hirano et
al., 2007) and bootstrapping methods (Brin, 1998;
Pantel and Pennacchiotti, 2006) have been ex-
plored to date. In contrast, for the latter task,
409
only a few methods have been proposed so far
(Hasegawa et al, 2004; Banko and Etzioni, 2008;
Zhu et al, 2009). We therefore addressed the
problem of how to determine relations between a
given pair.
We used a three-step approach to address this
problem. The first step is to recognize an expres-
sion that shows explicit relations between a given
named entity pair in a text. If no such expression
is recognized, the second step is to estimate the
relationship that exists between a given named en-
tity pair that has an implicit relation. The last step
is to identify synonyms of the relations that are
recognized or estimated in the above steps. In this
paper, we focus on the first step. The task is se-
lecting a phrase from the text that contains a re-
lation expression linking a given entity pair and
outputting the expression as one showing the rela-
tionship between the pair.
In our preliminary experiment, it was found
that using only structural features of a text, such
as syntactic or contextual features, is not good
enough for a number of examples. For instance,
the two Japanese sentences shown in Figure 1
have the same syntactic structure but (a) contains a
relation expression and (b) does not. We therefore
assume there are clues for recognizing relation
expressions other than conventional syntactic and
contextual information. In this paper, we propose
a supervised learning method that includes two
novel features of relational words as well as con-
ventional syntactic and contextual features. The
novel features of our method are:
Inherent Feature: Some words are able to ex-
press the relations between named entities
and some are not. Thus, it would be useful to
know the words that inherently express these
relations.
Context-dependent Feature: There are a num-
ber of typical relationships that change as
time passes, such as ?dating? ? ?engage-
ment? ? ?marriage? between persons. Fur-
thermore, present relations are influenced by
the past relations of a given named entity
pair. Thus, it would be useful to know the
past relations between a given pair and how
the relations change as time passes.
In the rest of this paper, Section 2 references re-
lated work, Section 3 outlines our method?s main
features and related topics, Section 4 describes our
experiments and experimental results, and Section
5 briefly summarizes key points and future work
to be done.
2 Related Work
The ?Message Understanding Conference? and
?Automatic Content Extraction? programs have
promoted relational extraction. The task was stud-
ied so as to extract predefined semantic relations
of entity pairs in a text. Examples include the
supervised learning method cited in (Kambhatla,
2004; Culotta and Sorensen, 2004; Zelenko et al,
2003) and the bootstrapping method cited in (Pan-
tel and Pennacchiotti, 2006; Agichtein and Gra-
vano, 2000). Recently, open information extrac-
tion (Open IE), a novel domain-independent ex-
traction paradigm, has been suggested (Banko and
Etzioni, 2008; Hasegawa et al, 2004). The task is
to detect semantically related named entity pairs
and to recognize the relation between them with-
out using predefined relations.
Our work is a kind of open IE, but our approach
differs from that of previous studies. Banko
(2008) proposed a supervised learning method us-
ing conditional random fields to recognize the re-
lation expressions from words located between a
given pair. Hasegawa (2004) also proposed a rule-
based method that selects all words located be-
tween a given pair as a relation expression if a
given named entities appear within ten words. The
point of these work is that they selected relation
expressions only from the words located between
Osaka Fucho
01
-nosaka ucho
01
-noKacho02-noacho02-no
Yumei
04
-desu.u ei
04
-desu.Suzuki
03
-san-wauzuki
03
-san- a
D
DD
Osaka Fucho
05
-nosaka ucho
05
-noSoumukyoku06-noou ukyoku06-no
Yumei
08
-desu.u ei
08
-desu.Suzuki
07
-san-wauzuki
07
-san- a
D
DD
(a)Mr.Suzuki
03
, a manager
02
of Osaka Prefectural Government
01
, is famous
04
.(b)Mr.Suzuki
07
, administration office
06
in Osaka PrefecturalGovernment
05
, is famous
08
.
(a) (b)
Figure 1: Same syntactic examples
410
given entities in the text, because as far as English
texts are concerned, 86% of the relation expres-
sions of named entity pairs appear between the
pair (Banko and Etzioni, 2008). However, our tar-
get is Japanese texts, in which only 26% of entity
pair relation expressions appear between the pair.
Thus, it is hard to incorporate previous approaches
into a Japanese text.
To solve the problem, our task was to select a
phrase from the entire text that would include a
relation expression for connecting a given pair.
3 Recognizing Relation Expressions
between Named Entities
To recognize the relation expression for a given
pair, we need to select a phrase that includes an
expression that shows the relation between a given
entity pair from among all noun and verb phrases
in a text. Actually, there are two types of candi-
date phrases in this case. One is from a sentence
that contains a given pair (intra-sentential), and
the other is from a sentence that does not (inter-
sentential). For example, the triple [Miyaji21,
Ishii22, taiketsu12] extracted from the following
text is inter-sentential.
(S-1) Chumokoku11-no taiketsu12-ga
mamonaku13 hajimaru14.
(The showcase11 match12 will start14 soon13.)
(S-2) Ano Miyaji21-to Ishii22-toiu
kanemochi23-niyoru yume24-no
kikaku25.
(The dream24 event25 between the rich mens23,
Miyaji21 and Ishii22.)
According to our annotated data shown in Ta-
ble 2, 53% of the semantically-related named en-
tity pairs are intra-sentential and 12% are inter-
sentential. Thus, we first select a phrase from
those in a sentence that contains a given pair, and
if no phrase is selected, select one from the rest of
the sentences in a text.
We propose a supervised learning method that
uses two novel features of relational words as
well as conventional syntactic and contextual fea-
tures. These features are organized as a tree struc-
ture and are fed into a boosting-based classifica-
tion algorithm (Kudo and Matsumoto, 2004). The
highest-scoring phrase is then selected if the score
exceeds a given threshold. Finally, the head of the
selected phrase is output as the relation expression
of a given entity pair.
The method consists of four parts: preprocess-
ing (POS tagging and parsing), feature extraction,
classification, and selection. In this section, we
describe the idea behind using our two novel fea-
tures and how they are implemented to recognize
the relation expressions of given pairs. Before
that, we will describe our proposed method?s con-
ventional features.
3.1 Conventional Features
Syntactic feature
To recognize the intra-sentential relation ex-
pressions for a given pair, we assume that there
is a discriminative syntactic structure that consists
of given entities and their relation expression. For
example, there is a structure for which the com-
mon parent phrase of the given pair, X = ?Ha-
toyama Yukio32? and Y = ?Hatoyama Kunio33?,
has the relation expression, R = ?ani34? in the
Japanese sentence S-3. Figure 2 shows the depen-
dency tree of sentence S-3.
(S-3) Minshuto31-no Hatoyama Yukio32-wa
Hatoyama Kunio33-no ani34-desu.
(Yukio Hatoyama32, the Democratic Party31,
is Kunio Hatoyama33?s brother34.)
To use a discriminative structure for each can-
didate, we make a minimum tree that consists of
given entities and the candidate where each phrase
is represented by a case marker ?CM?, a depen-
dency type ?DT?, an entity class, and the string
and POS of the candidate (See Figure 3).
Minshuto
31
-noinshuto
31
-no
Hatoyama Yukio
32
-waatoya a ukio
32
- a
Ani
34
-desu.ni
34
-desu.
Hatoyama Kunio
33
-noatoya a unio
33
-noD
D D
Figure 2: Dependency tree of sentence S-3
411
X:person:person
Phrasehrase
PhrasehraseCandidateandidatePhrasehrase
Y:person:person
CM:wa: a DT:D:
STR:Ani
34
: ni
34
POS:Noun: ounCM:?: DT:O:CM:no:no DT:D: Inh:1Inh:1C
rank
:1
rank
:1C
prob
:0.23
prob
:0.23
Figure 3: Intra-sentential feature tree
Contextual Feature
To recognize the inter-sentential relation ex-
pressions for a given pair, we assume that there
is a discriminative contextual structure that con-
sists of given entities and their relation expression.
Here, we use a Salient Referent List (SRL) to ob-
tain contextual structure. The SRL is an empirical
sorting rule proposed to identify the antecedent
of (zero) pronouns (Nariyama, 2002), and Hirano
(2007) proposed a way of applying SRL to rela-
tion detection. In this work, we use this way to
apply SRL to recognize inter-sentential relation
expressions.
We applied SRL to each candidate as follows.
First, from among given entities and the candi-
date, we choose the one appearing last in the text
as the root of the tree. We then append noun
phrases, from the chosen one to the beginning of
the text, to the tree depending on case markers,
?wa? (topicalised subject), ?ga? (subject), ?ni?
(indirect object),?wo? (object), and ?others?, with
the following rules. If there are nodes of the same
case marker already in the tree, the noun phrase
is appended as a child of the leaf node of them.
In other cases, the noun phrase is appended as a
child of the root node. For example, we get the
SRL tree shown in Figure 4 with the given entity
pair, X = ?Miyaji21? and Y = ?Ishii22?, and the
candidate, ?taiketsu12?, with the text (S-1, S-2).
To use a discriminative SRL structure, we make
a minimum tree that consists of given entities and
the candidate where each phrase is represented by
an entity class, and the string and POS of the can-
didate (See Figure 5). In this way, there is a prob-
lem when the candidate is a verb phrase, because
ga: Taiketsu
12
ga: aiketsu
12
Ishii
22
Ishii
22
others: Miyaji
21
others: iyaji
21
others: Chumoku
11
others: hu oku
11
Figure 4: Salient referent list tree
only noun phrases are appended to the SRL tree.
If the candidate is a verb phrase, we cannot make
a minimum tree that consists of given entities and
the candidate.
To solve this problem, a candidate verb phrase
is appended to the feature tree using a syntactic
structure. In a dependency tree, almost all verb
phrases have some parent or child noun phrases
that are in the SRL tree. Thus, candidate verb
phrases are appended as offspring of these noun
phrases represented syntactically as ?parent? or
?child?. For example, when given the entity pair,
X = ?Miyaji21? and Y = ?Ishii22?, and the can-
didate, ?hajimaru14? from the text (S-1, S-2), a
feature tree cannot be made because the candi-
date is not in an SRL tree. By extending the way
the syntactic structure is used, ?hajimaru14? has a
child node ?taiketsu12?, which is in an SRL tree,
and this makes it possible to make the feature tree
shown in Figure 6.
3.2 Proposed Features
To recognize intra-sentential or inter-sentential re-
lation expressions for given pairs, we assume
there are clues other than syntactic and contex-
tual information. Thus, we propose inherent and
SRL:gaL:ga Candidateandidate
Y:person:person
X:person:personSRL:othersL:othersSTR:Taiketsu
12
: aiketsu
12
POS:Noun: ounInh:1Inh:1 C
rank
:1
rank
:1 C
prob
:0.23
prob
:0.23
Figure 5: Inter-sentential feature tree
412
SRL:gaL:ga
Dep:Childep: hild Candidateandidate
Y:person:person X:person:personSRL:othersL:othersSTR:Hajimaru
14
: aji aru
14
POS:Verb: erbInh:0Inh:0 C
rank
:2
rank
:2 C
prob
:0.00
prob
:0.00
Figure 6: Extended inter-sentential feature tree
context-dependent features of relational words.
Inherent Feature of Relational words
Some words are able to express the relations be-
tween named entities and some are not. For exam-
ple, the word ?mother? can express a relation, but
the word ?car? cannot. If there were a list of words
that could express relations between named enti-
ties, it would be useful to recognize the relation
expression of a given pair. As far as we know,
however, no such list exists in Japanese. Thus,
we estimate which words are able to express rela-
tions between entities. Here, we assume that al-
most all verbs are able to express relations, and
accordingly we focus on nouns.
When the relation expression, R, of an entity
pair, X and Y , is a noun, it is possible to say ?Y is
R of X? or ?Y is X?s R?. Here, we can say noun
R takes an argument X . In linguistics, this kind
of noun is called a relational noun. Grammatically
speaking, a relational noun is a simple noun, but
because its meaning describes a ?relation? rather
than a ?thing?, it is used to describe relations just
as prepositions do. To estimate which nouns are
able to express the relations between named enti-
ties, we use the characteristics of relational nouns.
In linguistics, many researchers describe the rela-
tionship between possessives and relational nouns
(Chris, 2008). Thus, we use the knowledge that
in the patterns ?B of A? or ?A?s B?, if word B is
a relational noun, the corresponding word A be-
longs to a certain semantic category. In contrast,
if word B is not a relational noun, the correspond-
ing word A belongs to many semantic categories
(Tanaka et al, 1999). Figure 7 shows scattering
of the semantic categories of ?mother? and ?car?
Semantic categoriesRelative
 Frequency
Semantic categoriesRelative
 Frequency
Figure 7: Scattering of semantic category of
?mother? (left) and ?car? (right).
acquired by the following way.
First, we acquired A and B using the patterns
?A no B?1 from a large Japanese corpus, then
mapped words A into semantic categories C= {
c1, c2, ? ? ? , cm } using a Japanese lexicon (Ikehara
et al, 1999). Next, for each word B, we calcu-
lated a scattering score Hc(B) using the semantic
category of corresponding words A. Finally, we
estimated whether a word is a relational noun by
using k-NN estimation with positive and negative
examples. As estimated results, ?Inh:1? shows
that it is a relational noun and ?Inh:0? shows that
it is not. In both cases, the result is appended to
the feature tree as a child of the candidate node
(See Figure 3, 5, or 6).
Hc(B) = ?
?
c?C
P (c|B)logmP (c|B)
P (c|B) = freq(c,B)freq(B)
In our experiments, we acquired 55,412,811
pairs of A and B from 1,698,798 newspaper ar-
ticles and 10,499,468 weblog texts. As training
data, we used the words of relation expressions as
positive examples and other words as negative ex-
amples.
Context-dependent Feature of Relational
words
There are a number of typical relationships that
change as time passes, such as ?dating? ? ?en-
gagement? ? ?marriage? between persons. Fur-
thermore, present relations are affected by the past
relations of a given named entity pair. For in-
stance, if the past relations of a given pair are ?dat-
ing? and ?engagement? and one of the candidates
is ?marriage?, ?marriage? would be selected as the
relation expression of the given pair. Therefore, if
1
?B of A? or ?A?s B? in English.
413
Pair of entity class rm rn PT (rn|rm) Count(rm, rn)
dating 0.050 102
?person,person? dating marriage 0.050 101
engagement 0.040 82
marriage 0.157 786
?person,person? engagement engagement 0.065 325
wedding 0.055 276
president 0.337 17,081
?person,organization? vice president vice president 0.316 16,056
CEO 0.095 4,798
fellow 0.526 61
?person,organization? researcher manager 0.103 12
member 0.078 9
alliance 0.058 8,358
?organization,organization? alliance accommodated 0.027 3,958
acquisition 0.027 3,863
mutual consultation 0.022 2,670
?location,location? neighbour support 0.015 1,792
visit 0.012 1,492
war 0.077 78,170
?location,location? war mutual consultation 0.015 15,337
support 0.010 10,226
Table 1: Examples of calculated relation trigger model between entity classes defined by IREX
we know the past relations of the given pair and
the typical relational change that occurs as time
passes, it would be useful to recognize the rela-
tion expression of a given pair.
In this paper, we represent typical relational
changes that occur as time passes by a simple re-
lation trigger model PT (rn|rm). Note that rm
is a past relation and rn is a relation affected by
rm. This model disregards the span between rn
and rm. To make the trigger model, we automat-
ically extract triples [X , Y , R] from newspaper
articles and weblog texts, which have time stamps
of the document creation. Using these triples with
time stamps for each entity pair, we sort rela-
tions in order of time and count pairs of present
and previous relations. For example, if we ex-
tract ?dating? occurring for an entity pair on Jan-
uary 10, 1998, ?engagement? occurring on Febru-
ary 15, 2001, and ?marriage? occurring on De-
cember 24, 2001, the pairs ?dating, engagement?,
?dating, marriage?, and ?engagement, marriage?
are counted. The counted score is then summed
up by the pair of entity class and the trigger model
is calculated by the following formula.
PT (rn|rm) =
Count(rm, rn)?
rn Count(rm, rn)
For the evaluation, we extracted triples by
named entity recognition (Suzuki et al, 2006), re-
lation detection (Hirano et al, 2007), and the pro-
posed method using the inherent features of rela-
tional words described in Section 3.2. A total of
10,463,232 triples were extracted from 8,320,042
newspaper articles and weblog texts with time
stamps made between January 1, 1991 and June
30, 2006. As examples of the calculated relation
trigger model, Table 1 shows the top three proba-
bility relations rn of several relations rm between
Japanese standard named entity classes defined
in the IREX workshop2. For instance, the rela-
tion ?fellow? has the highest probability of being
changed from the relation ?researcher? between
person and organization as time passes.
2http://nlp.cs.nyu.edu/irex/
414
To obtain the past relations of a given pair in
the input text, we again used the triples with time
stamps extracted as above. The only relations we
use as past relations, Rm = {rm1 , rm2 , ? ? ? , rmk},
are those of a given pair whose time stamps are
older than the input text. Finally, we calcu-
lated probabilities with the following formula us-
ing the past relations Rm and the trigger model
PT (rn|rm).
PT (rn|Rm) = max{PT (rn|rm1),
PT (rn|rm2), ? ? ? , PT (rn|rmk)}
Using this calculated probability, we ranked
candidates and appended the rank ?Crank? and
the probability score ?Cprob? to the feature tree
as a child of the candidate node (See Figure 3,
5, or 6). For example, if the past relations Rm
were ?dating? and ?engagement? and candidates
were ?marriage?, ?meeting?, ?eating?, or ?drink-
ing?, the candidates probabilities were calculated
and ranked as ?marriage? (Cprob:0.15, Crank:1),
?meeting? (Cprob:0.08, Crank:2), etc.
3.3 Classification Algorithms
Several structure-based learning algorithms have
been proposed so far (Collins and Duffy, 2002;
Suzuki et al, 2003; Kudo and Matsumoto, 2004).
The experiments tested Kudo and Matsumoto?s
boosting-based algorithm using sub-trees as fea-
tures, which is implemented as a BACT system.
Given a set of training examples each of which
is represented as a tree labeling whether the can-
didate is the relation expression of a given pair or
not, the BACT system learns that a set of rules
is effective in classifying. Then, given a test in-
stance, the BACT system classifies using a set of
learned rules.
4 Experiments
We conducted experiments using texts from
Japanese newspaper articles and weblog texts to
test the proposed method for both intra- and inter-
sentential tasks. In the experiments, we compared
the following methods:
Conventional Features: trained by conventional
syntactic features for intra-sentential tasks as
Relation Types #
Explicit Intra-sentential 9,178Inter-sentential 2,058
Implicit 5,992
Total 17,228
Table 2: Details of the annotated data
described in Section 3.1, and contextual fea-
tures for inter-sentential tasks as described in
Section 3.1.
+Inherent Features: trained by conventional
features plus inherent features of relational
words described in Section 3.2.
++Context-dependent FeaturesTM: trained
by conventional and inherent features plus
context-dependent features of relational
words with the trigger model described in
Section 3.2.
++Context-dependent FeaturesCM: trained
by conventional and inherent features
plus context-dependent features of rela-
tional words with a cache model. We
evaluated this method to compare it with
Context-dependent FeaturesTM to show the
effectiveness of the proposed trigger model.
The cache model is a simple way to use past
relations in which the probability PC(rcand)
calculated by the following formula and the
rank based on the probability is appended to
every candidate feature tree.
PC(rcand) =
|rcand in past relations|
|past relations|
4.1 Settings
We used 6,200 texts from Japanese newspapers
and weblogs dated from January 1, 2004 to June
30, 2006, manually annotating the semantic rela-
tions between named entities for experiment pur-
poses. There were 17,228 semantically-related
entity pairs as shown in Table 2. In an intra-
sentential experiment, 17,228 entity pairs were
given, but only 9,178 of them had relation expres-
sions. In contrast, in an inter-sentential experi-
ment, 8,050 entity pairs excepted intra-sentential
415
Precision Recall F
Conventional Features 63.5? (3,436/5,411) 37.4? (3,436/9,178) 0.471
+Inherent Features 67.2? (4,036/6,001) 43.9? (4,036/9,178) 0.531
++Context-dependent FeaturesTM 70.7? (4,460/6,312) 48.6? (4,460/9,178) 0.576
++Context-dependent FeaturesCM 67.5? (4,042/5,987) 44.0? (4,042/9,178) 0.533
Table 3: Experimental results of intra-sentential
Precision Recall F
Conventional Features 70.1? (579/825) 28.1? (579/2,058) 0.401
+Inherent Features 77.1? (719/932) 34.9? (719/2,058) 0.480
++Context-dependent FeaturesTM 75.2? (794/1,055) 38.5? (794/2,058) 0.510
++Context-dependent FeaturesCM 74.3? (732/985) 35.5? (732/2,058) 0.481
Table 4: Experimental result of inter-sentential
were given, but only 2,058 of them had relation
expressions.
We conducted five-fold cross-validation over
17,228 entity pairs so that sets of pairs from a sin-
gle text were not divided into the training and test
sets. In the experiments, all features were auto-
matically acquired using a Japanese POS tagger
(Fuchi and Takagi, 1998) and dependency parser
(Imamura et al, 2007).
4.2 Results
Tables 3 and 4 show the performance of several
methods for intra-sentential and inter-sentential.
Precision is defined as the percentage of cor-
rect relation expressions out of recognized ones.
Recall is the percentage of correct relation ex-
pressions from among the manually annotated
ones. The F measure is the harmonic mean of
precision and recall.
A comparison with the Conventional Fea-
tures and Inherent Features method for intra-
/inter-sentential tasks indicates that the proposed
method using inherent features of relational words
improved intra-sentential tasks F by 0.06 points
and inter-sentential tasks F by 0.08 points. Us-
ing a statistical test (McNemar Test) demonstrably
showed the proposed method?s effectiveness.
A comparison with the Inherent Features and
Context-dependent FeaturesTM method showed
that the proposed method using context-dependent
features of relational words improved intra-/inter-
sentential task performance by 0.045 and 0.03
points, respectively. McNemar test results also
showed the method?s effectiveness.
To further compare the usage of context-
dependent features, trigger models, and cache
models, we also used Context-dependent
FeaturesCM method for comparison. Tables
3 and 4 show that our proposed trigger model
performed better than the cache model, and
McNemar test results showed that there was a
significant difference between the models. The
reason the trigger model performed better than
the cache model is that the trigger model correctly
recognized the relation expressions that did not
appear in the past relations of a given pair. Thus,
we can conclude that using typical relationships
that change as time passes helps to recognize
relation expressions between named entities.
5 Conclusion
We proposed a supervised learning method that
employs inherent and context-dependent features
of relational words and uses conventional syntac-
tic or contextual features to improve both intra-
and inter-sentential relation expression recogni-
tion. Our experiments demonstrated that the
method improves the F measure and thus helps
to recognize relation expressions between named
entities.
In future work, we plan to estimate implicit re-
lations between named entities and to identify re-
lational synonyms.
416
References
Agichtein, Eugene and Luis Gravano. 2000. Snow-
ball: Extracting relations from large plain-text col-
lections. In Proceedings of the 5th ACM conference
on Digital libraries, pages 85?94.
Banko, Michele and Oren Etzioni. 2008. The tradeoffs
between open and traditional relation extraction. In
Proceedings of the 46th Annual Meeting on Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies, pages 28?36.
Brin, Sergey. 1998. Extracting patterns and rela-
tions from the world wide web. In WebDB Work-
shop at 6th International Conference on Extending
Database Technology, pages 172?183.
Chris, Barker, 2008. Semantics: An international
handbook of natural language meaning, chap-
ter Possessives and relational nouns. Walter De
Gruyter Inc.
Collins, Michael and Nigel Duffy. 2002. Convolution
kernels for natural language. Advances in Neural
Information Processing Systems, 14:625?632.
Culotta, Aron and Jeffrey Sorensen. 2004. Depen-
dency tree kernels for relation extraction. In Pro-
ceedings of the 42nd Annual Meeting on Association
for Computational Linguistics, pages 423?429.
Fuchi, Takeshi and Shinichiro Takagi. 1998. Japanese
morphological analyzer using word co-occurrence
- jtag. In Proceedings of the 36th Annual Meet-
ing of the Association for Computational Linguis-
tics and 17th International Conference on Compu-
tational Linguistics, volume 1, pages 409?413.
Hasegawa, Takaaki, Satoshi Sekine, and Ralph Grish-
man. 2004. Discovering relations among named
entities from large corpora. In Proceedings of the
42nd Annual Meeting on Association for Computa-
tional Linguistics, pages 415?422.
Hirano, Toru, Yoshihiro Matsuo, and Genichiro Kikui.
2007. Detecting semantic relations between named
entities in text using contextual features. In Pro-
ceedings of the 45th Annual Meeting on Association
for Computational Linguistics, pages 157?160.
Ikehara, Satoru, Masahiro Miyazaki, Satoru Shirai,
Akio Yoko, Hiromi Nakaiwa, Kentaro Ogura, Masa-
fumi Oyama, and Yoshihiko Hayashi. 1999. Ni-
hongo Goi Taikei (in Japanese). Iwanami Shoten.
Imamura, Kenji, Genichiro Kikui, and Norihito Ya-
suda. 2007. Japanese dependency parsing using se-
quential labeling for semi-spoken language. In Pro-
ceedings of the 45th Annual Meeting on Association
for Computational Linguistics, pages 225?228.
Kambhatla, Nanda. 2004. Combining lexical, syntac-
tic, and semantic features with maximum entropy
models for extracting relations. In Proceedings of
the 42nd Annual Meeting on Association for Com-
putational Linguistics, pages 178?181.
Kudo, Taku and Yuji Matsumoto. 2004. A boosting
algorithm for classification of semi-structured text.
In Proceedings of the 2004 Conference on Empiri-
cal Methods in Natural Language Processing, pages
301?308.
Nariyama, Shigeko. 2002. Grammar for ellipsis res-
olution in japanese. In Proceedings of the 9th In-
ternational Conference on Theoretical and Method-
ological Issues in Machine Translation, pages 135?
145.
Pantel, Patrick and Marco Pennacchiotti. 2006.
Espresso: Leveraging generic patterns for automat-
ically harvesting semantic relations. In Proceed-
ings of the 21st International Conference on Com-
putational Linguistics and the 44th annual meeting
of the Association for Computational Linguistics,
pages 113?120.
Suzuki, Jun, Tsutomu Hirao, Yutaka Sasaki, and
Eisaku Maeda. 2003. Hierarchical directed acyclic
graph kernel: Methods for structured natural lan-
guage data. In Proceedings of the 41st Annual
Meeting on Association for Computational Linguis-
tics, pages 32?39.
Suzuki, Jun, Erik McDermott, and HIdeki Isozaki.
2006. Training conditional random fields with mul-
tivariate evaluation measures. In Proceedings of the
43th Annual Meeting on Association for Computa-
tional Linguistics.
Tanaka, Shosaku, Yoichi Tomiura, and Toru Hitaka.
1999. Classification of syntactic categories of
nouns by the scattering of semantic categories (in
japanese). Transactions of Information Processing
Society of Japan, 40(9):3387?3396.
Wong, Wilson, Wei Liu, and Mohammed Bennamoun.
2010. Acquiring semantic relations using the web
for constructing lightweight ontologies. In Proceed-
ings of the 13th Pacific-Asia Conference on Knowl-
edge Discovery and Data Mining.
Zelenko, Dmitry, Chinatsu Aone, and Anthony
Richardella. 2003. Kernel methods for relation ex-
traction. Journal of Machine Learning Research,
3:1083?1106.
Zhu, Jun, Zaiqing Nie, Xiaojing Liu, Bo Zhang, and
Ji-Rong Wen. 2009. Statsnowball: a statistical ap-
proach to extracting entity relationships. In Pro-
ceedings of the 18th international conference on
World Wide Web, pages 101?110.
417
Coling 2010: Poster Volume, pages 910?918,
Beijing, August 2010
Opinion Summarization with Integer Linear Programming Formulation
for Sentence Extraction and Ordering
Hitoshi Nishikawa, Takaaki Hasegawa, Yoshihiro Matsuo and Genichiro Kikui
NTT Cyber Space Laboratories, NTT Corporation{ nishikawa.hitoshi, hasegawa.takaaki
matsuo.yoshihiro, kikui.genichiro
}
@lab.ntt.co.jp
Abstract
In this paper we propose a novel algorithm
for opinion summarization that takes ac-
count of content and coherence, simulta-
neously. We consider a summary as a se-
quence of sentences and directly acquire
the optimum sequence from multiple re-
view documents by extracting and order-
ing the sentences. We achieve this with a
novel Integer Linear Programming (ILP)
formulation. Our proposed formulation is
a powerful mixture of the Maximum Cov-
erage Problem and the Traveling Sales-
man Problem, and is widely applicable to
text generation and summarization tasks.
We score each candidate sequence accord-
ing to its content and coherence. Since
our research goal is to summarize reviews,
the content score is defined by opinions
and the coherence score is developed in
training against the review document cor-
pus. We evaluate our method using the
reviews of commodities and restaurants.
Our method outperforms existing opinion
summarizers as indicated by its ROUGE
score. We also report the results of human
readability experiments.
1 Introduction
The Web now holds a massive number of reviews
describing the opinions of customers about prod-
ucts and services. These reviews can help the cus-
tomer to reach purchasing decisions and guide the
business activities of companies such as product
improvement. It is, however, almost impossible to
read all reviews given their sheer number.
Automatic text summarization, particularly
opinion summarization, is expected to allow all
possible reviews to be efficiently utilized. Given
multiple review documents, our summarizer out-
puts text consisting of ordered sentences. A typ-
This restaurant offers customers a delicious menu and a
relaxing atmosphere. The staff are very friendly but the
price is a little high.
Table 1: A typical summary.
ical summary is shown in Table 1. This task is
considered as multidocument summarization.
Existing summarizers focus on organizing sen-
tences so as to include important information in
the given document into a summary under some
size limitation. A serious problem is that most of
these summarizers completely ignore coherence
of the summary, which improves reader?s compre-
hension as reported by Barzilay et al (2002).
To make summaries coherent, the extracted
sentences must be appropriately ordered. How-
ever, most summarization systems delink sentence
extraction from sentence ordering, so a sentence
can be extracted that can never be ordered natu-
rally with the other extracted sentences. More-
over, due to recent advances in decoding tech-
niques for text summarization, the summarizers
tend to select shorter sentences to optimize sum-
mary content. It aggravates this problem.
Although a preceding work tackles this prob-
lem by performing sentence extraction and order-
ing simultaneously (Nishikawa et al, 2010), they
adopt beam search and dynamic programming to
search for the optimal solution, so their proposed
method may fail to locate it.
To overcome this weakness, this paper proposes
a novel Integer Linear Programming (ILP) formu-
lation for searching for the optimal solution effi-
ciently. We formulate the multidocument sum-
marization task as an ILP problem that tries to
optimize the content and coherence of the sum-
mary by extracting and ordering sentences simul-
taneously. We apply our method to opinion sum-
marization and show that it outperforms state-of-
the-art opinion summarizers in terms of ROUGE
evaluations. Although in this paper we challenge
910
our method with opinion summarization, it can be
widely applied to other text generation and sum-
marization tasks.
This paper is organized as follows: Section 2
describes related work. Section 3 describes our
proposal. Section 4 reports our evaluation experi-
ments. We conclude this paper in Section 5.
2 Related Work
2.1 Sentence Extraction
Although a lot of summarization algorithms have
been proposed, most of them solely extract sen-
tences from a set of sentences in the source docu-
ment set. These methods perform extractive sum-
marization and can be formalized as follows:
S? = argmax
S?T
L(S) (1)
s.t. length(S) ? K
T stands for all sentences in the source docu-
ment set and S is an arbitrary subset of T . L(S)
is a function indicating the score of S as deter-
mined by one or more criteria. length(S) indi-
cates the length of S, K is the maximum size of
the summary. That is, most summarization algo-
rithms search for, or decode, the set of sentences S?
that maximizes function L under the given maxi-
mum size of the summary K. Thus most stud-
ies focus on the design of function L and efficient
search algorithms (i.e. argmax operation in Eq.1).
Objective Function
Many useful L functions have been proposed
including the cosine similarity of given sentences
(Carbonell and Goldstein, 1998) and centroid
(Radev et al, 2004); some approaches directly
learn function L from references (Kupiec et al,
1995; Hirao et al, 2002).
There are two approaches to defining the score
of the summary. One defines the weight on each
sentence forming the summary. The other defines
a weight for a sub-sentence, concept, that the sum-
mary contains.
McDonald (2007) and Martins and Smith
(2009) directly weight sentences and use MMR
to avoid redundancy (Carbonell and Goldstein,
1998). In contrast to their approaches, we set
weights on concepts, not sentences. Gillick
and Favre (2009) reported that the concept-based
model achieves better performance and scalability
than the sentence-based model when it is formu-
lated as ILP.
There is a wide range of choice with regard
to the unit of the concept. Concepts include
words and the relationship between named en-
tities (Filatova and Hatzivassiloglou, 2004), bi-
grams (Gillick and Favre, 2009), and word stems
(Takamura and Okumura, 2009).
Some summarization systems that target re-
views, opinion summarizers, extract particular
information, opinion, from the input sentences
and leverage them to select important sentences
(Carenini et al, 2006; Lerman et al, 2009). In
this paper, since we aim to summarize reviews,
the objective function is defined through opinion
as the concept that the reviews contain. We ex-
plain our detailed objective function in Section 3.
We describe features of above existing summariz-
ers in Section 4 and compare our method to them
as baselines.
Decoding Method
The algorithms proposed for argmax operation
include the greedy method (Filatova and Hatzivas-
siloglou, 2004), stack decoding (Yih et al, 2007;
Takamura and Okumura, 2009) and Integer Linear
Programming (Clarke and Lapata, 2007; McDon-
ald, 2007; Gillick and Favre, 2009; Martins and
Smith, 2009). Gillick and Favre (2009) and Taka-
mura and Okumura (2009) formulate summariza-
tion as a Maximum Coverage Problem. We also
use this formulation. While these methods focus
on extracting a set of sentences from the source
document set, our method performs extraction and
ordering simultaneously.
Some studies attempt to generate a single sen-
tence (i.e. headline) from the source document
(Banko et al, 2000; Deshpande et al, 2007).
While they extract and order words from the
source document as a unit, our model uses the unit
of sentences. This problem can be formulated as
the Traveling Salesman Problem and its variants.
Banko et al (2000) uses beam search to identify
approximate solutions. Deshpande et al (2007)
uses ILP and a randomized algorithm to find the
optimal solution.
2.2 Sentence Ordering
It is known that the readability of a collection of
sentences, a summary, can be greatly improved
by appropriately ordering them (Barzilay et al,
2002). Features proposed to create the appropri-
ate order include publication date of document
(Barzilay et al, 2002), content words (Lapata,
2003; Althaus et al, 2004), and syntactic role of
911
 
  
    
 
  
             	      	

     
     
Figure 1: Graph representation of summarization.
words (Barzilay and Lapata, 2005). Some ap-
proaches use machine learning to integrate these
features (Soricut and Marcu, 2006; Elsner et al,
2007). Generally speaking, these methods score
the discourse coherence of a fixed set of sentences.
These methods are separated from the extraction
step so they may fail if the set includes sentences
that are impossible to order naturally.
As mentioned above, there is a preceding work
that attempted to perform sentence extraction and
ordering simultaneously (Nishikawa et al, 2010).
Differences between this paper and that work are
as follows:
? This work adopts ILP solver as a decoder.
ILP solver allows the summarizer to search
for the optimal solution much more rapidly
than beam search (Deshpande et al, 2007),
which was adopted by the prior work. To
permit ILP solver incorporation, we propose
in this paper a totally new ILP formulation.
The formulation can be widely used for text
summarization and generation.
? Moreover, to learn better discourse coher-
ence, we adopt the Passive-Aggressive al-
gorithm (Crammer et al, 2006) and use
Kendall?s tau (Lapata, 2006) as the loss func-
tion. In contrast, the above work adopts Av-
eraged Perceptron (Collins, 2002) and has no
explicit loss function.
These advances make this work very different
from that work.
3 Our Method
3.1 The Model
We consider a summary as a sequence of sen-
tences. As an example, document set D =
{d1, d2, d3} is given to a summarizer. We de-
fine d as a single document. Document d1,
which consists of four sentences, is describe
by d1 = {s11, s12, s13, s14}. Documents d2
and d3 consist of five sentences and three sen-
tences (i.e. d2 = {s21, s22, s23, s24, s25}, d3 =
e1 e2 e3 . . . e6 e7 e8
s11 1 0 0 1 0 0
s12 0 1 0 0 0 0
s13 0 0 0 0 0 1
.
.
.
.
.
.
s31 0 0 0 0 0 0
s32 0 0 1 0 1 0
s33 0 0 0 0 0 1
Table 2: Sentence-Concept Matrix.
{s31, s32, s33}). If the summary consists of four
sentences s11, s23, s32, s33 and they are ordered as
s11 ? s23 ? s32 ? s33, we add symbols indicat-
ing the beginning of the summary s0 and the end
of the summary s4, and describe the summary as
S = ?s0, s11, s23, s32, s33, s4?. Summary S can
be represented as a directed path that starts at s0
and ends at s4 as shown in Fig. 1.
We describe a directed arc between si and sj as
ai,j ? A. The directed path shown in Fig. 1 is de-
composed into nodes, s0, s11, s23, s32, s33, s4, and
arcs, a0,11, a11,23, a23,32, a32,33, a33,4.
To represent the discourse coherence of two ad-
jacent sentences, we define weight ci,j ? C as
the coherence score on the directed arc ai,j . We
assume that better summaries have higher coher-
ence scores, i.e. if the sum of the scores of the arcs?
ai,j?S ci,jai,j is high, the summary is coherent.
We also assume that the source document set
D includes set of concepts e ? E. Each concept
e is covered by one or more of the sentences in
the document set. We show this schema in Ta-
ble 2. According to Table 2, document set D has
eight concepts e1, e2, . . . , e7, e8 and sentence s11
includes concepts e1 and e6 while sentence s12 in-
cludes e2.
We consider each concept ei has a weight wi.
We assume that concept ei will have high weight
wi if it is important. This paper improves sum-
mary quality by maximizing the sum of these
weights.
We define, based on the above assumption, the
following objective function:
L(S) = ?ei?S wiei +
?
ai,j?S ci,jai,j (2)
s.t. length(S) ? K
Summarization is, in this paper, realized by
maximizing the sum of weights of concepts in-
cluded in the summary and the coherence score of
all adjacent sentences in the summary under the
912
limit of maximum summary size. Note that while
S and T represents the set of sentences in Eq.1,
they represent the sequence of sentences in Eq.2.
Maximizing Eq.2 is NP-hard. If each sen-
tence in the source document set has one concept
(i.e. Table 2 is a diagonal matrix), Eq.2 becomes
the Prize Collecting Traveling Salesman Problem
(Balas, 1989). Therefore, a highly efficient decod-
ing method is essential.
3.2 Parameter Estimation
Our method requires two parameters: weights
w ? W of concepts and coherence c ? C of two
adjacent sentences. We describe them here.
Content Score
In this paper, as mentioned above, since we at-
tempt to summarize reviews, we adopt opinion
as a concept. We define opinion e = ?t, a, p?
as the tuple of target t, aspect a and its polarity
p ? {?1, 0, 1}. We define target t as the tar-
get of an opinion. For example, the target t of
the sentence ?This digital camera has good im-
age quality.? is digital camera. We define aspect
a as a word that represents a standpoint appro-
priate for evaluating products and services. With
regard to digital cameras, aspects include image
quality, design and battery life. In the above ex-
ample sentence, the aspect is image quality. Po-
larity p represents whether the opinion is positive
or negative. In this paper, we define p = ?1 as
negative, p = 0 as neutral and p = 1 as posi-
tive. Thus the example sentence contains opinion
e = ?digital camera, image quality, 1?.
Opinions are extracted using a sentiment ex-
pression dictionary and pattern matching from de-
pendency trees of sentences. This opinion extrac-
tor is the same as that used in Nishikawa et al
(2010).
As the weight wi of concept ei, we use only
the frequency of each opinion in the input docu-
ment set, i.e. we assume that an opinion that ap-
pears frequently in the input is important. While
this weighting is relatively naive compared to Ler-
man et al (2009)?s method, our ROUGE evalua-
tion shows that this approach is effective.
Coherence Score
In this section, we define coherence score c.
Since it is not easy to model the global coherence
of a set of sentences, we approximate the global
coherence by the sum of local coherence i.e. the
sum of coherence scores of sentence pairs. We
define local coherence score ci,j of two sentences
x = {si, sj} and their order y = ?si, sj? repre-
senting si ? sj as follows:
ci,j = w ? ?(x, y) (3)
w??(x, y) is the inner product ofw and ?(x, y),
w is a parameter vector and ?(x, y) is a feature
vector of the two sentences si and sj .
Since coherence consists of many different el-
ements and it is difficult to model all of them,
we approximate the features of coherence as the
Cartesian product of the following features: con-
tent words, POS tags of content words, named en-
tity tags (e.g. LOC, ORG) and conjunctions. Lap-
ata (2003) proposed most of these features.
We also define feature vector ?(x,y) of the bag
of sentences x = {s0, s1, . . . , sn, sn+1} and its
entire order y = ?s0, s1, . . . , sn, sn+1? as follows:
?(x,y) =
?
x,y
?(x, y) (4)
Therefore, the score of order y is w ? ?(x,y).
Given a training set, if trained parameter vector w
assigns score w ? ?(x,yt) to correct order yt that
is higher than score w ??(x, y?) assigned to incor-
rect order y?, it is expected that the trained parame-
ter vector will give a higher score to coherently or-
dered sentences than to incoherently ordered sen-
tences.
We use the Passive-Aggressive algorithm
(Crammer et al, 2006) to find w. The Passive-
Aggressive algorithm is an online learning algo-
rithm that updates the parameter vector by taking
up one example from the training examples and
outputting the solution that has the highest score
under the current parameter vector. If the output
differs from the training example, the parameter
vector is updated as follows;
min ||wi+1 ?wi|| (5)
s.t. s(x,yt;wi+1)? s(x, y?;wi+1) ? `(y?;yt)
s(x,y;w) = w ? ?(x,y)
wi is the current parameter vector and wi+1 is
the updated parameter vector. That is, Eq.5 means
that the score of the correct order must exceed the
score of an incorrect order by more than loss func-
tion `(y?;yt) while minimizing the change in pa-
rameters.
When updating the parameter vector, this al-
gorithm requires the solution that has the highest
score under the current parameter vector, so we
have to run an argmax operation. Since we are
913
attempting to order a set of sentences, the opera-
tion is regarded as solving the Traveling Salesman
Problem (Althaus et al, 2004); that is, we locate
the path that offers the maximum score through
all n sentences where s0 and sn+1 are starting and
ending points, respectively. This operation is NP-
hard and it is difficult to find the global optimal
solution. To overcome this, we find an approxi-
mate solution by beam search.1
We define loss function `(y?;yt) as follows:
`(y?;yt) = 1? ? (6)
? = 1 ? 4 S(y?,yt)N(N ? 1) (7)
? indicates Kendall?s tau. S(y?,yt) is the mini-
mum number of operations that swap adjacent ele-
ments (i.e. sentences) needed to bring y? to yt (La-
pata, 2006). N indicates the number of elements.
Since Lapata (2006) reported that Kendall?s tau
reliably reproduces human ratings with regard to
sentence ordering, using it to minimize the loss
function is expected to yield more reliable param-
eters.
We omit detailed derivations due to space limi-
tations. Parameters are updated as per the follow-
ing equation.
wi+1 = wi + ?i(?(x,yt)? ?(x, y?)) (8)
?i = `(y?;yt) ? s(x,yt;w
i) + s(x, y?;wi)
||?(x,yt)? ?(x, y?)||2 + 12C
(9)
C in Eq.9 is the aggressiveness parameter that
controls the degree of parameter change.
Note that our method learns w from documents
automatically annotated by a POS tagger and a
named entity tagger. That is, manual annotation
isn?t required.
3.3 Decoding with Integer Linear
Programming Formulation
This section describes an ILP formulation of the
above model. We use the same notation con-
vention as introduced in Section 3.1. We use
s ? S, a ? A, e ? E as the decision variable.
Variable si ? S indicates the inclusion of the i
th sentence. If the i th sentence is part of the
summary, then si is 1. If it is not part of the
1Obviously, ILP can be used to search for the path that
maximizes the score. While beam search tends to fail to find
out the optimal solution, it is tractable and the learning al-
gorithm can estimate the parameter from approximate solu-
tions. For these reasons we use beam search.
summary, then si is 0. Variable ai,j ? A indi-
cates the adjacency of the i th and j th sentences.
If these two sentences are ordered as si ? sj ,
then ai,j is 1. Variable ei ? E indicates the in-
clusion of the i th concept ei. Taking Fig.1 as
an example, variables s0, s11, s23, s32, s33, s4 and
a0,11, a11,23, a23,32, a32,33, a33,4 are 1. ei, which
correspond to the concepts in the above extracted
sentences, are also 1.
We represent the above objective function
(Eq.2) as follows:
max
?
?
??
?
ei?E
wiei + (1 ? ?)
?
ai,j?A
ci,jai,j
?
?
? (10)
Eq.10 attempts to cover as much of the concepts
included in input document set as possible accord-
ing to their weights w ? W and orders sentences
according to discourse coherence c ? C. ? is a
scaling factor to balance w and c.
We then impose some constraints on Eq.10 to
acquire the optimum solution.
First, we range the above three variables s ?
S, a ? A, e ? E.
si, ai,j , ei ? {0, 1} ?i, j
In our model, a summary can?t include the same
sentence, arc, or concept twice. Taking Table 2
for example, if s13 and s33 are included in a sum-
mary, the summary has two e8, but e8 is 1. This
constraint avoids summary redundancy.
The summary must meet the condition of maxi-
mum summary size. The following inequality rep-
resents the size constraint:
?
si?S
lisi ? K
li ? L indicates the length of sentence si. K is
the maximum size of the summary.
The following inequality represents the rela-
tionship between sentences and concepts in the
sentences.
?
i
mijsi ? ej ?j
The above constraint represents Table 2. mi,j is
an element of Table 2. If si is not included in the
summary, the concepts in si are not included.
Symbols indicating the beginning and end of
the summary must be part of the summary.
914
s0 = 1
sn+1 = 1
n is the number of sentences in the input docu-
ment set.
Next, we describe the constraints placed on
arcs.
The beginning symbol must be followed by a
sentence or a symbol and must not have any pre-
ceding sentences/symbols. The end symbol must
be preceded by a sentence or a symbol and must
not have any following sentences/symbols. The
following equations represent these constraints:
?
i
a0,i = 1
?
i
ai,0 = 0
?
i
an+1,i = 0
?
i
ai,n+1 = 1
Each sentence in the summary must be pre-
ceded and followed by a sentence/symbol.
?
i
ai,j +
?
i
aj,i = 2sj ?j
?
i
ai,j =
?
i
aj,i ?j
The above constraints fail to prevent cycles. To
rectify this, we set the following constraints.
?
i
f0,i = n
?
i
fi,0 ? 1
?
i
fi,j ?
?
i
fj,i = sj ?j
fi,j ? nai,j ?i, j
The above constraints indicate that flows f are
sent from s0 as a source to sn+1 as a sink. n unit
flows are sent from the source and each node ex-
pends one unit of flows. More than one flow has
to arrive at the sink. By setting these constraints,
the nodes consisting of a cycle have no flow. Thus
solutions that contain a cycle are prevented. These
constraints have also been used to avoid cycles in
headline generation (Deshpande et al, 2007).
4 Experiments
This section evaluates our method in terms of
ROUGE score and readability. We tested our
method and two baselines in two domains: re-
views of commodities and restaurants. We col-
lected 4,475 reviews of 100 commodities and
2,940 reviews of 100 restaurants from websites.
The commodities included items such as digital
cameras, printers, video games, and wines. The
average document size was 10,173 bytes in the
commodity domain and 5,343 bytes in the restau-
rant domain. We attempted to generate 300 byte
summaries, so the summarization rates were about
3% and 6%, respectively.
We prepared 4 references for each review, thus
there were 400 references in each domain. The au-
thors were not those who made up the references.
These references were used for ROUGE and read-
ability evaluation.
Since our method requires the parameter vec-
tor w for determining the coherence scores. We
trained the parameter vector for each domain.
Each parameter vector was trained using 10-fold
cross validation. We used 8 samples to train, 1
to develop, and 1 to test. In the restaurant do-
main, we added 4,390 reviews to each training set
to alleviate data sparseness. In the commodity do-
main, we add 47,570 reviews.2
As the solver, we used glpk.3 According to the
development set, ? in Eq.10 was set as 0.1.
4.1 Baselines
We compare our method to the references (which
also provide the upper bound) and the opinion
summarizers proposed by Carenini et al (2006)
and Lerman et al (2009) as the baselines.
In the ROUGE evaluations, Human indicates
ROUGE scores between references. To compare
our summarizer to human summarization, we cal-
culated ROUGE scores between each reference
and the other three references, and averaged them.
In the readability evaluations, we randomly se-
lected one reference for each commodity and each
restaurant and compared them to the results of the
three summarizers.
Carenini et al (2006)
Carenini et al (2006) proposed two opinion
2The commodities domain suffers from stronger review
variation than the restaurant domain so more training data
was needed.
3http://www.gnu.org/software/glpk/
915
summarizers. One uses a natural language genera-
tion module, and other is based on MEAD (Radev
et al, 2004). Since it is difficult to mimic the natu-
ral language generation module, we implemented
the latter one. The objective function Carenini et
al. (2006) proposed is as follows:
L1(S) =
?
a?S
?
s?D
|polaritys(a)| (11)
polaritys(a) indicates the polarity of aspect a
in sentence s present in source document set D.
That is, this function gives a high score to a sum-
mary that covers aspects frequently mentioned in
the input, and whose polarities tend to be either
positive or negative.
The solution is identified using the greedy
method. If there is more than one sentence that
has the same score, the sentence that has the
higher centroid score (Radev et al, 2004) is ex-
tracted.
Lerman et al (2009)
Lerman et al (2009) proposed three objective
functions for opinion summarization, and we im-
plemented one of them. The function is as fol-
lows:
L2(S) = ?(KL(pS(a), pD(a)) (12)
+
?
a?A
KL(N (x|?aS , ?2aS ),N (x|?aD , ?
2
aD)))
KL(p, q) means the Kullback-Leibler diver-
gence between probability distribution p and q.
pS(a) and pD(a) are probability distributions in-
dicating how often aspect a ? A occurs in sum-
mary S and source document set D respectively.
N (x|?, ?2) is a Gaussian distribution indicating
distribution of polarity of an aspect whose mean
is ? and variance is ?2. ?aS , ?aD and ?2aS , ?2aD
are the means and the variances of aspect a in
summary S and source document set D, respec-
tively. These parameters are determined using
maximum-likelihood estimation.
That is, the above objective function gives high
score to a summary whose distributions of aspects
and polarities mirror those of the source document
set.
To identify the optimal solution, Lerman et al
(2009) use a randomized algorithm. First, the
summarizer randomly extracts sentences from the
source document set, then iteratively performs in-
sert/delete/swap operations on the summary to in-
crease Eq.12 until summary improvement satu-
rates. While this method is prone to lock onto
Commodity R-2 R-SU4 R-SU9
(Carenini et al, 2006) 0.158 0.202 0.186
(Lerman et al, 2009) 0.205 0.247 0.227
Our Method 0.231 0.251 0.230
Human 0.384 0.392 0.358
Restaurant R-2 R-SU4 R-SU9
(Carenini et al, 2006) 0.251 0.281 0.258
(Lerman et al, 2009) 0.260 0.296 0.273
Our Method 0.285 0.303 0.273
Human 0.358 0.370 0.335
Table 3: Automatic ROUGE evaluation.
# of Sentences
(Carenini et al, 2006) 3.79
(Lerman et al, 2009) 6.28
Our Method 7.88
Human 5.83
Table 4: Average number of sentences in the sum-
mary.
local solutions, the summarizer can reach the op-
timal solution by changing the starting sentences
and repeating the process. In this experiment, we
used 100 randomly selected starting points.
4.2 ROUGE
We used ROUGE (Lin, 2004) for evaluating the
content of summaries. We chose ROUGE-2,
ROUGE-SU4 and ROUGE-SU9. We prepared
four reference summaries for each document set.
The results of these experiments are shown in
Table 3. ROUGE scores increase in the order of
(Carenini et al, 2006), (Lerman et al, 2009) and
our method, but no method could match the per-
formance of Human. Our method significantly
outperformed Lerman et al (2009)?s method over
ROUGE-2 according to the Wilcoxon signed-rank
test, while it shows no advantage over ROUGE-
SU4 and ROUGE-SU9.
Although our weighting of the set of sentences
is relatively naive compared to the weighting pro-
posed by Lerman et al (2009), our method out-
performs their method. There are two reasons
for this; one is that we adopt ILP for decoding,
so we can acquire preferable solutions efficiently.
While the score of Lerman et al (2009)?s method
may be improved by adopting ILP, it is difficult
to do so because their objective function is ex-
tremely complex. The other reason is the coher-
ence score. Since our coherence score is based on
916
Commodity (Carenini et al, 2006) (Lerman et al, 2009) Our Method Human
(Carenini et al, 2006) - 27/45 18/29 8/46
(Lerman et al, 2009) 18/45 - 29/48 11/47
Our Method 11/29 19/48 - 5/46
Human 38/46 36/47 41/46 -
Restaurant (Carenini et al, 2006) (Lerman et al, 2009) Our Method Human
(Carenini et al, 2006) - 31/45 17/31 8/48
(Lerman et al, 2009) 14/45 - 25/47 7/46
Our Method 14/31 22/47 - 8/50
Human 40/48 39/46 42/50 -
Table 5: Readability evaluation.
content words, it may impact the content of the
summary.
4.3 Readability
Readability was evaluated by human judges.
Since it is difficult to perform absolute evalua-
tion to judge the readability of summaries, we
performed a paired comparison test. The judges
were shown two summaries of the same input and
decided which was more readable. The judges
weren?t informed which method generated which
summary. We randomly chose 50 sets of reviews
from each domain, so there were 600 paired sum-
maries.4 However, as shown in Table 4, the aver-
age numbers of sentences in the summary differed
widely from the methods and this might affect the
readability evaluation. It was not fair to include
the pairs that were too different in terms of the
number of sentences. Therefore, we removed the
pairs that differed by more than five sentences.
In the experiment, 523 pairs were used, and 21
judges evaluated about 25 summaries each. We
drew on DUC 2007 quality questions5 for read-
ability assessment.
Table 5 shows the results of the experiment.
Each element in the table indicates the number
of times the corresponding method won against
other method. For example, in the commodity do-
main, the summaries that Lerman et al (2009)?s
method generated were compared with the sum-
maries that Carenini et al (2006)?s method gener-
ated 45 times, and Lerman et al (2009)?s method
won 18 times. The judges significantly preferred
the references in both domains. There were no
significant differences between our method and
the other two methods. In the restaurant do-
4
4C2 ? 100 = 600
5http://www-nlpir.nist.gov/projects/
duc/duc2007/quality-questions.txt
main, there was a significant difference between
(Carenini et al, 2006) and (Lerman et al, 2009).
Since we adopt ILP, our method tends to pack
shorter sentences into the summary. However,
our coherence score prevents this from degrading
summary readability.
5 Conclusion
This paper proposed a novel algorithm for opinion
summarization that takes account of content and
coherence, simultaneously. Our method directly
searches for the optimum sentence sequence by
extracting and ordering sentences present in the
input document set. We proposed a novel ILP
formulation against selection-and-ordering prob-
lems; it is a powerful mixture of the Maximum
Coverage Problem and the Traveling Salesman
Problem. Experiments revealed that the algo-
rithm creates summaries that have higher ROUGE
scores than existing opinion summarizers. We
also performed readability experiments. While
our summarizer tends to extract shorter sentences
to optimize summary content, our proposed co-
herence score prevented this from degrading the
readability of the summary.
One future work includes enriching the features
used to determine the coherence score. We expect
that features such as entity grid (Barzilay and La-
pata, 2005) will improve overall algorithm perfor-
mance. We also plan to apply our model to tasks
other than opinion summarization.
Acknowledgments
We would like to sincerely thank Tsutomu Hirao
for his comments and discussions. We would also
like to thank the anonymous reviewers for their
comments.
917
References
Althaus, Ernst, Nikiforos Karamanis and Alexander Koller.
2004. Computing Locally Coherent Discourses. In Proc.
of the 42nd Annual Meeting of the Association for Com-
putational Linguistics.
Balas, Egon. 1989. The prize collecting traveling salesman
problem. Networks, 19(6):621?636.
Banko, Michele, Vibhu O. Mittal and Michael J. Witbrock.
2000. Headline Generation Based on Statistical Transla-
tion. In Proc. of the 38th Annual Meeting of the Associa-
tion for Computational Linguistics.
Barzilay, Regina, Noemie Elhadad and Kathleen McKeown.
2002. Inferring Strategies for Sentence Ordering in Mul-
tidocument Summarization. Journal of Artificial Intelli-
gence Research, 17:35?55.
Barzilay, Regina and Mirella Lapata. 2005. Modeling Lo-
cal Coherence: An Entity-based Approach. In Proc. of
the 43rd Annual Meeting of the Association for Compu-
tational Linguistics.
Carbonell, Jaime and Jade Goldstein. 1998. The use of
MMR, diversity-based reranking for reordering docu-
ments and producing summaries. In Proc. of the 21st An-
nual International ACM SIGIR Conference on Research
and Development in Information Retrieval.
Carenini, Giuseppe, Raymond Ng and Adam Pauls. 2006.
Multi-Document Summarization of Evaluative Text. In
Proc. of the 11th Conference of the European Chapter of
the Association for Computational Linguistics.
Clarke, James and Mirella Lapata. 2007. Modelling Com-
pression with Discourse Constraints. In Proc. of the 2007
Joint Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Language
Learning.
Collins, Michael. 2002. Discriminative Training Methods for
Hidden Markov Models: Theory and Experiments with
Perceptron Algorithms. In Proc. of the 2002 Conference
on Empirical Methods in Natural Language Processing.
Crammer, Koby, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz and Yoram Singer. 2006. Online passive-
aggressive algorithms. Journal of Machine Learning Re-
search, 7:551?585.
Deshpande, Pawan, Regina Barzilay and David R. Karger.
2007. Randomized Decoding for Selection-and-Ordering
Problems. In Proc. of Human Language Technologies
2007: The Conference of the North American Chapter of
the Association for Computational Linguistics.
Elsner, Micha, Joseph Austerweil and Eugene Charniak.
2007. A unified local and global model for discourse co-
herence. In Proc. of Human Language Technologies 2007:
The Conference of the North American Chapter of the As-
sociation for Computational Linguistics.
Filatova, Elena and Vasileios Hatzivassiloglou. 2004. A For-
mal Model for Information Selection in Multi-Sentence
Text Extraction. In Proc. of the 20th International Con-
ference on Computational Linguistics.
Gillick, Dan and Benoit Favre. 2009. A Scalable Global
Model for Summarization. In Proc. of Human Language
Technologies: The 2009 Annual Conference of the North
American Chapter of the Association for Computational
Linguistics Workshop on Integer Linear Programming for
NLP.
Hirao, Tsutomu, Hideki Isozaki, Eisaku Maeda and Yuji
Matsumoto. 2002. Extracting important sentences with
support vector machines. In Proc. of the 19th Interna-
tional Conference on Computational Linguistics.
Kupiec, Julian, Jan Pedersen and Francine Chen. 1995. A
Trainable Document Summarizer. In Proc. of the 18th An-
nual International ACM SIGIR Conference on Research
and Development in Information Retrieval.
Lapata, Mirella. 2003. Probabilistic Text Structuring: Exper-
iments with Sentence Ordering. In Proc. of the 41st An-
nual Meeting of the Association for Computational Lin-
guistics.
Lapata, Mirella. 2006. Automatic Evaluation of Informa-
tion Ordering: Kendall?s Tau. Computational Linguistics,
32(4):471?484.
Lerman, Kevin, Sasha Blair-Goldensohn and Ryan McDon-
ald. 2009. Sentiment Summarization: Evaluating and
Learning User Preferences. In Proc. of the 12th Confer-
ence of the European Chapter of the Association for Com-
putational Linguistics.
Lin, Chin-Yew. 2004. ROUGE: A Package for Automatic
Evaluation of Summaries. In Proc. of Text Summarization
Branches Out.
Martins, Andre F. T., and Noah A. Smith. 2009. Summariza-
tion with a Joint Model for Sentence Extraction and Com-
pression. In Proc. of Human Language Technologies: The
2009 Annual Conference of the North American Chapter
of the Association for Computational Linguistics Work-
shop on Integer Linear Programming for NLP.
McDonald, Ryan. 2007. A Study of Global Inference Algo-
rithms in Multi-document Summarization. In Proc. of the
29th European Conference on Information Retrieval.
Nishikawa, Hitoshi, Takaaki Hasegawa, Yoshihiro Matsuo
and Genichiro Kikui. 2010. Optimizing Informativeness
and Readability for Sentiment Summarization. In Proc. of
the 48th Annual Meeting of the Association for Computa-
tional Linguistics.
Radev, Dragomir R., Hongyan Jing, Magorzata Sty and
Daniel Tam. 2004. Centroid-based summarization of mul-
tiple documents. Information Processing and Manage-
ment, 40(6):919?938.
Soricut, Radu and Daniel Marcu. 2006. Discourse Genera-
tion Using Utility-Trained Coherence Models. In Proc. of
the 21st International Conference on Computational Lin-
guistics and 44th Annual Meeting of the Association for
Computational Linguistics.
Takamura, Hiroya and Manabu Okumura. 2009. Text Sum-
marization Model based on Maximum Coverage Problem
and its Variant. In Proc. of the 12th Conference of the Eu-
ropean Chapter of the Association for Computational Lin-
guistics.
Yih, Wen-tau, Joshua Goodman, Lucy Vanderwende and
Hisami Suzuki. 2007. Multi-Document Summarization by
Maximizing Informative Content-Words. In Proc. of the
20th International Joint Conference on Artificial Intelli-
gence.
918
Proceedings of the ACL 2010 Conference Short Papers, pages 325?330,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Optimizing Informativeness and Readability
for Sentiment Summarization
Hitoshi Nishikawa, Takaaki Hasegawa, Yoshihiro Matsuo and Genichiro Kikui
NTT Cyber Space Laboratories, NTT Corporation
1-1 Hikari-no-oka, Yokosuka, Kanagawa, 239-0847 Japan
{
nishikawa.hitoshi, hasegawa.takaaki
matsuo.yoshihiro, kikui.genichiro
}
@lab.ntt.co.jp
Abstract
We propose a novel algorithm for senti-
ment summarization that takes account of
informativeness and readability, simulta-
neously. Our algorithm generates a sum-
mary by selecting and ordering sentences
taken from multiple review texts according
to two scores that represent the informa-
tiveness and readability of the sentence or-
der. The informativeness score is defined
by the number of sentiment expressions
and the readability score is learned from
the target corpus. We evaluate our method
by summarizing reviews on restaurants.
Our method outperforms an existing al-
gorithm as indicated by its ROUGE score
and human readability experiments.
1 Introduction
The Web holds a massive number of reviews de-
scribing the sentiments of customers about prod-
ucts and services. These reviews can help the user
reach purchasing decisions and guide companies?
business activities such as product improvements.
It is, however, almost impossible to read all re-
views given their sheer number.
These reviews are best utilized by the devel-
opment of automatic text summarization, partic-
ularly sentiment summarization. It enables us to
efficiently grasp the key bits of information. Senti-
ment summarizers are divided into two categories
in terms of output style. One outputs lists of
sentences (Hu and Liu, 2004; Blair-Goldensohn
et al, 2008; Titov and McDonald, 2008), the
other outputs texts consisting of ordered sentences
(Carenini et al, 2006; Carenini and Cheung, 2008;
Lerman et al, 2009; Lerman and McDonald,
2009). Our work lies in the latter category, and
a typical summary is shown in Figure 1. Although
visual representations such as bar or rader charts
This restaurant offers customers delicious foods and a
relaxing atmosphere. The staff are very friendly but the
price is a little high.
Figure 1: A typical summary.
are helpful, such representations necessitate some
simplifications of information to presentation. In
contrast, text can present complex information that
can?t readily be visualized, so in this paper we fo-
cus on producing textual summaries.
One crucial weakness of existing text-oriented
summarizers is the poor readability of their results.
Good readability is essential because readability
strongly affects text comprehension (Barzilay et
al., 2002).
To achieve readable summaries, the extracted
sentences must be appropriately ordered (Barzilay
et al, 2002; Lapata, 2003; Barzilay and Lee, 2004;
Barzilay and Lapata, 2005). Barzilay et al (2002)
proposed an algorithm for ordering sentences ac-
cording to the dates of the publications from which
the sentences were extracted. Lapata (2003) pro-
posed an algorithm that computes the probability
of two sentences being adjacent for ordering sen-
tences. Both methods delink sentence extraction
from sentence ordering, so a sentence can be ex-
tracted that cannot be ordered naturally with the
other extracted sentences.
To solve this problem, we propose an algorithm
that chooses sentences and orders them simulta-
neously in such a way that the ordered sentences
maximize the scores of informativeness and read-
ability. Our algorithm efficiently searches for the
best sequence of sentences by using dynamic pro-
gramming and beam search. We verify that our
method generates summaries that are significantly
better than the baseline results in terms of ROUGE
score (Lin, 2004) and subjective readability mea-
sures. As far as we know, this is the first work to
325
simultaneously achieve both informativeness and
readability in the area of multi-document summa-
rization.
This paper is organized as follows: Section 2
describes our summarization method. Section 3
reports our evaluation experiments. We conclude
this paper in Section 4.
2 Optimizing Sentence Sequence
Formally, we define a summary S? =
?s0, s1, . . . , sn, sn+1? as a sequence consist-
ing of n sentences where s0 and sn+1 are symbols
indicating the beginning and ending of the se-
quence, respectively. Summary S? is also defined
as follows:
S? = argmax
S?T
[Info(S) + ?Read(S)] (1)
s.t. length(S) ? K
where Info(S) indicates the informativeness
score of S, Read(S) indicates the readability
score of S, T indicates possible sequences com-
posed of sentences in the target documents, ?
is a weight parameter balancing informativeness
against readability, length(S) is the length of S,
and K is the maximum size of the summary.
We introduce the informativeness score and the
readability score, then describe how to optimize a
sequence.
2.1 Informativeness Score
Since we attempt to summarize reviews, we as-
sume that a good summary must involve as many
sentiments as possible. Therefore, we define the
informativeness score as follows:
Info(S) =
?
e?E(S)
f(e) (2)
where e indicates sentiment e = ?a, p? as the tu-
ple of aspect a and polarity p = {?1, 0, 1}, E(S)
is the set of sentiments contained S, and f(e) is the
score of sentiment e. Aspect a represents a stand-
point for evaluating products and services. With
regard to restaurants, aspects include food, atmo-
sphere and staff. Polarity represents whether the
sentiment is positive or negative. In this paper, we
define p = ?1 as negative, p = 0 as neutral and
p = 1 as positive sentiment.
Notice that Equation 2 defines the informative-
ness score of a summary as the sum of the score
of the sentiments contained in S. To avoid du-
plicative sentences, each sentiment is counted only
once for scoring. In addition, the aspects are clus-
tered and similar aspects (e.g. air, ambience) are
treated as the same aspect (e.g. atmosphere). In
this paper we define f(e) as the frequency of e in
the target documents.
Sentiments are extracted using a sentiment lex-
icon and pattern matched from dependency trees
of sentences. The sentiment lexicon1 consists of
pairs of sentiment expressions and their polarities,
for example, delicious, friendly and good are pos-
itive sentiment expressions, bad and expensive are
negative sentiment expressions.
To extract sentiments from given sentences,
first, we identify sentiment expressions among
words consisting of parsed sentences. For ex-
ample, in the case of the sentence ?This restau-
rant offers customers delicious foods and a relax-
ing atmosphere.? in Figure 1, delicious and re-
laxing are identified as sentiment expressions. If
the sentiment expressions are identified, the ex-
pressions and its aspects are extracted as aspect-
sentiment expression pairs from dependency tree
using some rules. In the case of the example sen-
tence, foods and delicious, atmosphere and relax-
ing are extracted as aspect-sentiment expression
pairs. Finally extracted sentiment expressions are
converted to polarities, we acquire the set of sen-
timents from sentences, for example, ? foods, 1?
and ? atmosphere, 1?.
Note that since our method relies on only senti-
ment lexicon, extractable aspects are unlimited.
2.2 Readability Score
Readability consists of various elements such as
conciseness, coherence, and grammar. Since it
is difficult to model all of them, we approximate
readability as the natural order of sentences.
To order sentences, Barzilay et al (2002)
used the publication dates of documents to catch
temporally-ordered events, but this approach is not
really suitable for our goal because reviews focus
on entities rather than events. Lapata (2003) em-
ployed the probability of two sentences being ad-
jacent as determined from a corpus. If the cor-
pus consists of reviews, it is expected that this ap-
proach would be effective for sentiment summa-
rization. Therefore, we adopt and improve Lap-
ata?s approach to order sentences. We define the
1Since we aim to summarize Japanese reviews, we utilize
Japanese sentiment lexicon (Asano et al, 2008). However,
our method is, except for sentiment extraction, language in-
dependent.
326
readability score as follows:
Read(S) =
n
?
i=0
w>?(si, si+1) (3)
where, given two adjacent sentences si and
si+1, w>?(si, si+1), which measures the connec-
tivity of the two sentences, is the inner product of
w and ?(si, si+1), w is a parameter vector and
?(si, si+1) is a feature vector of the two sentences.
That is, the readability score of sentence sequence
S is the sum of the connectivity of all adjacent sen-
tences in the sequence.
As the features, Lapata (2003) proposed the
Cartesian product of content words in adjacent
sentences. To this, we add named entity tags (e.g.
LOC, ORG) and connectives. We observe that the
first sentence of a review of a restaurant frequently
contains named entities indicating location. We
aim to reproduce this characteristic in the order-
ing.
We also define feature vector ?(S) of the entire
sequence S = ?s0, s1, . . . , sn, sn+1? as follows:
?(S) =
n
?
i=0
?(si, si+1) (4)
Therefore, the score of sequence S is w>?(S).
Given a training set, if a trained parameter w as-
signs a score w>?(S+) to an correct order S+
that is higher than a score w>?(S?) to an incor-
rect order S?, it is expected that the trained pa-
rameter will give higher score to naturally ordered
sentences than to unnaturally ordered sentences.
We use Averaged Perceptron (Collins, 2002) to
find w. Averaged Perceptron requires an argmax
operation for parameter estimation. Since we at-
tempt to order a set of sentences, the operation is
regarded as solving the Traveling Salesman Prob-
lem; that is, we locate the path that offers maxi-
mum score through all n sentences as s0 and sn+1
are starting and ending points, respectively. Thus
the operation is NP-hard and it is difficult to find
the global optimal solution. To alleviate this, we
find an approximate solution by adopting the dy-
namic programming technique of the Held and
Karp Algorithm (Held and Karp, 1962) and beam
search.
We show the search procedure in Figure 2. S
indicates intended sentences and M is a distance
matrix of the readability scores of adjacent sen-
tence pairs. Hi(C, j) indicates the score of the
hypothesis that has covered the set of i sentences
C and has the sentence j at the end of the path,
Sentences: S = {s1, . . . , sn}
Distance matrix: M = [ai,j ]i=0...n+1,j=0...n+1
1: H0({s0}, s0) = 0
2: for i : 0 . . . n ? 1
3: for j : 1 . . . n
4: foreach Hi(C\{j}, k) ? b
5: Hi+1(C, j) = maxHi(C\{j},k)?bHi(C\{j}, k)
6: +Mk,j
7: H? = maxHn(C,k) H
n(C, k) +Mk,n+1
Figure 2: Held and Karp Algorithm.
i.e. the last sentence of the summary being gener-
ated. For example, H2({s0, s2, s5}, s2) indicates
a hypothesis that covers s0, s2, s5 and the last sen-
tence is s2. Initially, H0({s0}, s0) is assigned the
score of 0, and new sentences are then added one
by one. In the search procedure, our dynamic pro-
gramming based algorithm retains just the hypoth-
esis with maximum score among the hypotheses
that have the same sentences and the same last sen-
tence. Since this procedure is still computationally
hard, only the top b hypotheses are expanded.
Note that our method learns w from texts auto-
matically annotated by a POS tagger and a named
entity tagger. Thus manual annotation isn?t re-
quired.
2.3 Optimization
The argmax operation in Equation 1 also involves
search, which is NP-hard as described in Section
2.2. Therefore, we adopt the Held and Karp Algo-
rithm and beam search to find approximate solu-
tions. The search algorithm is basically the same
as parameter estimation, except for its calculation
of the informativeness score and size limitation.
Therefore, when a new sentence is added to a hy-
pothesis, both the informativeness and the read-
ability scores are calculated. The size of the hy-
pothesis is also calculated and if the size exceeds
the limit, the sentence can?t be added. A hypoth-
esis that can?t accept any more sentences is re-
moved from the search procedure and preserved
in memory. After all hypotheses are removed,
the best hypothesis is chosen from among the pre-
served hypotheses as the solution.
3 Experiments
This section evaluates our method in terms of
ROUGE score and readability. We collected 2,940
reviews of 100 restaurants from a website. The
327
R-2 R-SU4 R-SU9
Baseline 0.089 0.068 0.062
Method1 0.157 0.096 0.089
Method2 0.172 0.107 0.098
Method3 0.180 0.110 0.101
Human 0.258 0.143 0.131
Table 1: Automatic ROUGE evaluation.
average size of each document set (corresponds to
one restaurant) was 5,343 bytes. We attempted
to generate 300 byte summaries, so the summa-
rization rate was about 6%. We used CRFs-
based Japanese dependency parser (Imamura et
al., 2007) and named entity recognizer (Suzuki et
al., 2006) for sentiment extraction and construct-
ing feature vectors for readability score, respec-
tively.
3.1 ROUGE
We used ROUGE (Lin, 2004) for evaluating the
content of summaries. We chose ROUGE-2,
ROUGE-SU4 and ROUGE-SU9. We prepared
four reference summaries for each document set.
To evaluate the effects of the informativeness
score, the readability score and the optimization,
we compared the following five methods.
Baseline: employs MMR (Carbonell and Gold-
stein, 1998). We designed the score of a sentence
as term frequencies of the content words in a doc-
ument set.
Method1: uses optimization without the infor-
mativeness score or readability score. It also used
term frequencies to score sentences.
Method2: uses the informativeness score and
optimization without the readability score.
Method3: the proposed method. Following
Equation 1, the summarizer searches for a se-
quence with high informativeness and readability
score. The parameter vector w was trained on the
same 2,940 reviews in 5-fold cross validation fash-
ion. ? was set to 6,000 using a development set.
Human is the reference summaries. To com-
pare our summarizer to human summarization, we
calculated ROUGE scores between each reference
and the other references, and averaged them.
The results of these experiments are shown in
Table 1. ROUGE scores increase in the order of
Method1, Method2 and Method3 but no method
could match the performance of Human. The
methods significantly outperformed Baseline ac-
Numbers
Baseline 1.76
Method1 4.32
Method2 10.41
Method3 10.18
Human 4.75
Table 2: Unique sentiment numbers.
cording to the Wilcoxon signed-rank test.
We discuss the contribution of readability to
ROUGE scores. Comparing Method2 to Method3,
ROUGE scores of the latter were higher for all cri-
teria. It is interesting that the readability criterion
also improved ROUGE scores.
We also evaluated our method in terms of sen-
timents. We extracted sentiments from the sum-
maries using the above sentiment extractor, and
averaged the unique sentiment numbers. Table 2
shows the results.
The references (Human) have fewer sentiments
than the summaries generated by our method. In
other words, the references included almost as
many other sentences (e.g. reasons for the senti-
ments) as those expressing sentiments. Carenini
et al (2006) pointed out that readers wanted ?de-
tailed information? in summaries, and the reasons
are one of such piece of information. Including
them in summaries would greatly improve sum-
marizer appeal.
3.2 Readability
Readability was evaluated by human judges.
Three different summarizers generated summaries
for each document set. Ten judges evaluated the
thirty summaries for each. Before the evalua-
tion the judges read evaluation criteria and gave
points to summaries using a five-point scale. The
judges weren?t informed of which method gener-
ated which summary.
We compared three methods; Ordering sen-
tences according to publication dates and posi-
tions in which sentences appear after sentence
extraction (Method2), Ordering sentences us-
ing the readability score after sentence extrac-
tion (Method2+) and searching a document set
to discover the sequence with the highest score
(Method3).
Table 3 shows the results of the experiment.
Readability increased in the order of Method2,
Method2+ and Method3. According to the
328
Readability point
Method2 3.45
Method2+ 3.54
Method3 3.74
Table 3: Readability evaluation.
Wilcoxon signed-rank test, there was no signifi-
cance difference between Method2 and Method2+
but the difference between Method2 and Method3
was significant, p < 0.10.
One important factor behind the higher read-
ability of Method3 is that it yields longer sen-
tences on average (6.52). Method2 and Method2+
yielded averages of 7.23 sentences. The difference
is significant as indicated by p < 0.01. That is,
Method2 and Method2+ tended to select short sen-
tences, which made their summaries less readable.
4 Conclusion
This paper proposed a novel algorithm for senti-
ment summarization that takes account of infor-
mativeness and readability, simultaneously. To
summarize reviews, the informativeness score is
based on sentiments and the readability score is
learned from a corpus of reviews. The preferred
sequence is determined by using dynamic pro-
gramming and beam search. Experiments showed
that our method generated better summaries than
the baseline in terms of ROUGE score and read-
ability.
One future work is to include important infor-
mation other than sentiments in the summaries.
We also plan to model the order of sentences glob-
ally. Although the ordering model in this paper is
local since it looks at only adjacent sentences, a
model that can evaluate global order is important
for better summaries.
Acknowledgments
We would like to sincerely thank Tsutomu Hirao
for his comments and discussions. We would also
like to thank the reviewers for their comments.
References
Hisako Asano, Toru Hirano, Nozomi Kobayashi and
Yoshihiro Matsuo. 2008. Subjective Information In-
dexing Technology Analyzing Word-of-mouth Con-
tent on the Web. NTT Technical Review, Vol.6, No.9.
Regina Barzilay, Noemie Elhadad and Kathleen McK-
eown. 2002. Inferring Strategies for Sentence Or-
dering in Multidocument Summarization. Journal of
Artificial Intelligence Research (JAIR), Vol.17, pp.
35?55.
Regina Barzilay and Lillian Lee. 2004. Catching the
Drift: Probabilistic Content Models, with Applica-
tions to Generation and Summarization. In Proceed-
ings of the Human Language Technology Confer-
ence of the North American Chapter of the Associ-
ation for Computational Linguistics (HLT-NAACL),
pp. 113?120.
Regina Barzilay and Mirella Lapata. 2005. Modeling
Local Coherence: An Entity-based Approach. In
Proceedings of the 43rd Annual Meeting of the As-
sociation for Computational Linguistics (ACL), pp.
141?148.
Sasha Blair-Goldensohn, Kerry Hannan, Ryan McDon-
ald, Tyler Neylon, George A. Reis and Jeff Rey-
nar. 2008. Building a Sentiment Summarizer for Lo-
cal Service Reviews. WWW Workshop NLP Chal-
lenges in the Information Explosion Era (NLPIX).
Jaime Carbonell and Jade Goldstein. 1998. The use of
MMR, diversity-based reranking for reordering doc-
uments and producing summaries. In Proceedings of
the 21st annual international ACM SIGIR confer-
ence on Research and development in information
retrieval (SIGIR), pp. 335?356.
Giuseppe Carenini, Raymond Ng and Adam Pauls.
2006. Multi-Document Summarization of Evalua-
tive Text. In Proceedings of the 11th European
Chapter of the Association for Computational Lin-
guistics (EACL), pp. 305?312.
Giuseppe Carenini and Jackie Chi Kit Cheung. 2008.
Extractive vs. NLG-based Abstractive Summariza-
tion of Evaluative Text: The Effect of Corpus Con-
troversiality. In Proceedings of the 5th International
Natural Language Generation Conference (INLG),
pp. 33?41.
Michael Collins. 2002. Discriminative Training Meth-
ods for Hidden Markov Models: Theory and Exper-
iments with Perceptron Algorithms. In Proceedings
of the 2002 Conference on Empirical Methods on
Natural Language Processing (EMNLP), pp. 1?8.
Michael Held and Richard M. Karp. 1962. A dy-
namic programming approach to sequencing prob-
lems. Journal of the Society for Industrial and Ap-
plied Mathematics (SIAM), Vol.10, No.1, pp. 196?
210.
Minqing Hu and Bing Liu. 2004. Mining and Summa-
rizing Customer Reviews. In Proceedings of the 10th
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining (KDD), pp. 168?
177.
329
Kenji Imamura, Genichiro Kikui and Norihito Yasuda.
2007. Japanese Dependency Parsing Using Sequen-
tial Labeling for Semi-spoken Language. In Pro-
ceedings of the 45th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL) Com-
panion Volume Proceedings of the Demo and Poster
Sessions, pp. 225?228.
Mirella Lapata. 2003. Probabilistic Text Structuring:
Experiments with Sentence Ordering. In Proceed-
ings of the 41st Annual Meeting of the Association
for Computational Linguistics (ACL), pp. 545?552.
Kevin Lerman, Sasha Blair-Goldensohn and Ryan Mc-
Donald. 2009. Sentiment Summarization: Evalu-
ating and Learning User Preferences. In Proceed-
ings of the 12th Conference of the European Chap-
ter of the Association for Computational Linguistics
(EACL), pp. 514?522.
Kevin Lerman and Ryan McDonald. 2009. Contrastive
Summarization: An Experiment with Consumer Re-
views. In Proceedings of Human Language Tech-
nologies: the 2009 Annual Conference of the North
American Chapter of the Association for Computa-
tional Linguistics (NAACL-HLT), Companion Vol-
ume: Short Papers, pp. 113?116.
Chin-Yew Lin. 2004. ROUGE: A Package for Auto-
matic Evaluation of Summaries. In Proceedings of
the Workshop on Text Summarization Branches Out,
pp. 74?81.
Jun Suzuki, Erik McDermott and Hideki Isozaki. 2006.
Training Conditional Random Fields with Multi-
variate Evaluation Measures. In Proceedings of the
21st International Conference on Computational
Linguistics and 44th Annual Meeting of the ACL
(COLING-ACL), pp. 217?224.
Ivan Titov and Ryan McDonald. 2008. A Joint Model
of Text and Aspect Ratings for Sentiment Summa-
rization. In Proceedings of the 46th Annual Meet-
ing of the Association for Computational Linguis-
tics: Human Language Technologies (ACL-HLT),
pp. 308?316.
330
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 726?731,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Entity Set Expansion using Topic information
Kugatsu Sadamitsu, Kuniko Saito, Kenji Imamura and Genichiro Kikui?
NTT Cyber Space Laboratories, NTT Corporation
1-1 Hikarinooka, Yokosuka-shi, Kanagawa, 239-0847, Japan
{sadamitsu.kugatsu, saito.kuniko, imamura.kenji}@lab.ntt.co.jp
kikui@cse.oka-pu.ac.jp
Abstract
This paper proposes three modules based on
latent topics of documents for alleviating ?se-
mantic drift? in bootstrapping entity set ex-
pansion. These new modules are added to a
discriminative bootstrapping algorithm to re-
alize topic feature generation, negative exam-
ple selection and entity candidate pruning. In
this study, we model latent topics with LDA
(Latent Dirichlet Allocation) in an unsuper-
vised way. Experiments show that the accu-
racy of the extracted entities is improved by
6.7 to 28.2% depending on the domain.
1 Introduction
The task of this paper is entity set expansion in
which the lexicons are expanded from just a few
seed entities (Pantel et al, 2009). For example,
the user inputs a few words ?Apple?, ?Google? and
?IBM? , and the system outputs ?Microsoft?, ?Face-
book? and ?Intel?.
Many set expansion algorithms are based on boot-
strapping algorithms, which iteratively acquire new
entities. These algorithms suffer from the general
problem of ?semantic drift?. Semantic drift moves
the extraction criteria away from the initial criteria
demanded by the user and so reduces the accuracy
of extraction. Pantel and Pennacchiotti (2006) pro-
posed Espresso, a relation extraction method based
on the co-training bootstrapping algorithm with en-
tities and attributes. Espresso alleviates semantic-
drift by a sophisticated scoring system based on
? Presently with Okayama Prefectural University
pointwise mutual information (PMI). Thelen and
Riloff (2002), Ghahramani and Heller (2005) and
Sarmento et al (2007) also proposed original score
functions with the goal of reducing semantic-drift.
Our purpose is also to reduce semantic drift. For
achieving this goal, we use a discriminative method
instead of a scoring function and incorporate topic
information into it. Topic information means the
genre of each document as estimated by statisti-
cal topic models. In this paper, we effectively uti-
lize topic information in three modules: the first
generates the features of the discriminative mod-
els; the second selects negative examples; the third
prunes incorrect examples from candidate examples
for new entities. Our experiments show that the pro-
posal improves the accuracy of the extracted entities.
The remainder of this paper is organized as fol-
lows. In Section 2, we illustrate discriminative boot-
strapping algorithms and describe their problems.
Our proposal is described in Section 3 and experi-
mental results are shown in Section 4. Related works
are described in Section 5. Finally, Section 6 pro-
vides our conclusion and describes future works.
2 Problems of the previous Discriminative
Bootstrapping method
Some previous works introduced discriminative
methods based on the logistic sigmoid classifier,
which can utilize arbitrary features for the relation
extraction task instead of a scoring function such as
Espresso (Bellare et al, 2006; Mintz et al, 2009).
Bellare et al reported that the discriminative ap-
proach achieves better accuracy than Espresso when
the number of extracted pairs is increased because
726
multiple features are used to support the evidence.
However, three problems exist in their methods.
First, they use only local context features. The dis-
criminative approach is useful for using arbitrary
features, however, they did not identify which fea-
ture or features are effective for the methods. Al-
though the context features and attributes partly re-
duce entity word sense ambiguity, some ambiguous
entities remain. For example, consider the domain
broadcast program (PRG) and assume that PRG?s
attribute is advertisement. A false example is shown
here: ?Android ?s advertisement employs Japanese
popular actors. The attractive smartphone begins to
target new users who are ordinary people.? The en-
tity Android belongs to the cell-phone domain, not
PRG, but appears with positive attributes or contexts
because many cell-phones are introduced in adver-
tisements as same as broadcast program. By us-
ing topic, i.e. the genre of the document, we can
distinguish ?Android? from PRG and remove such
false examples even if the false entity appeared with
positive context strings or attributes. Second, they
did not solve the problem of negative example se-
lection. Because negative examples are necessary
for discriminative training, they used all remaining
examples, other than positive examples, as negative
examples. Although this is the simplest technique,
it is impossible to use all of the examples provided
by a large-scale corpus for discriminative training.
Third, their methods discriminate all candidates for
new entities. This principle increases the risk of gen-
erating many false-positive examples and is ineffi-
cient. We solve these three problems by using topic
information.
3 Set expansion using Topic information
3.1 Basic bootstrapping methods
In this section, we describe the basic method
adopted from Bellare (Bellare et al, 2006). Our
system?s configuration diagram is shown in Figure
1. In Figure 1, arrows with solid lines indicate the
basic process described in this section. The other
parts are described in the following sections. After
Ns positive seed entities are manually given, every
noun co-occurring with the seed entities is ranked
by PMI scores and then selected manually as Na
positive attributes. Ns and Na are predefined ba-
Figure 1: The structure of our system.
sic adjustment numbers. The entity-attribute pairs
are obtained by taking the cross product of seed en-
tity lists and attribute lists. The pairs are used as
queries for retrieving the positive documents, which
include positive pairs. The document set De,a in-
cluding same entity-attribute pair {e, a} is regarded
as one example Ee,a to alleviate over-fitting for con-
text features. These are called positive examples in
Figure 1. Once positive examples are constructed,
discriminative models can be trained by randomly
selecting negative examples.
Candidate entities are restricted to only the
Named Entities that lie in the close proximity to the
positive attributes. These candidates of documents,
including Named Entity and positive attribute pairs,
are regarded as one example the same as the train-
ing data. The discriminative models are used to cal-
culate the discriminative positive score, s(e, a), of
each candidate pair, {e, a}. Our system extracts Nn
types of new entities with high scores at each iter-
ation as defined by the summation of s(e, a) of all
positive attributes (AP );
?
a?AP s(e, a). Note that
we do not iteratively extract new attributes because
our purpose is entity set expansion.
3.2 Topic features and Topic models
In previous studies, context information is only used
as the features of discriminative models as we de-
scribed in Section 2. Our method utilizes not only
context features but also topic features. By utiliz-
ing topic information, our method can disambiguate
the entity word sense and alleviate semantic drift.
In order to derive the topic information, we utilize
statistical topic models, which represent the relation
727
between documents and words through hidden top-
ics. The topic models can calculate the posterior
probability p(z|d) of topic z in document d. For
example, the topic models give high probability to
topic z =?cell-phone? in the above example sen-
tences 1. This posterior probability is useful as a
global feature for discrimination. The topic feature
value ?t(z, e, a) is calculated as follows.
?t(z, e, a) =
?
d?De,a p(z|d)
?
z?
?
d?De,a p(z?|d)
.
In this paper, we use Latent Dirichlet Allocation
(LDA) as the topic models (Blei et al, 2003). LDA
represents the latent topics of the documents and the
co-occurrence between each topic.
In Figure 1, shaded part and the arrows with bro-
ken lines indicate our proposed method with its use
of topic information including the following sec-
tions.
3.3 Negative example selection
If we choose negative examples randomly, such ex-
amples are harmful for discrimination because some
examples include the same contexts or topics as the
positive examples. By contrast, negative examples
belonging to broad genres are needed to alleviate se-
mantic drift. We use topic information to efficiently
select such negative examples.
In our method, the negative examples are cho-
sen far from the positive examples according to the
measure of topic similarity. For calculating topic
similarity, we use a ranking score called ?positive
topic score?, PT (z), defined as follows, PT (z) =
?
d?DP p(z|d), where DP indicates the set of pos-
itive documents and p(z|d) is topic posterior prob-
ability for a given positive document. The bottom
50% of the topics sorted in decreasing order of pos-
itive topic score are used as the negative topics.
Our system picks up as many negative documents
as there are positive documents with each selected
negative topic being equally represented.
3.4 Candidate Pruning
Previous works discriminate all candidates for ex-
tracting new entities. Our basic system can constrain
1z is a random variable whose sample space is represented
as a discrete variable, not explicit words.
the candidate set by positive attributes, however, this
is not enough as described in Section 2. Our candi-
date pruning module, described below, uses the mea-
sure of topic similarity to remove obviously incor-
rect documents.
This pruning module is similar to negative exam-
ple selection described in the previous section. The
positive topic score, PT , is used as a candidate con-
straint. Taking all positive examples, we select the
positive topics, PZ, which including all topics z sat-
isfying the condition PT (z) > th. At least one
topic with the largest score is chosen as a positive
topic when PT (z) ? th about all topics. After se-
lecting this positive topic, the documents including
entity candidates are removed if the posterior prob-
ability satisfy p(z|d) ? th for all topics z. In this
paper, we set the threshold to th = 0.2. This con-
straint means that the topic of the document matches
that of the positive entities and can be regarded as a
hard constraint for topic features.
4 Experiments
4.1 Experimental Settings
We use 30M Japanese blog articles crawled in May
2008. The documents were tokenized by JTAG
(Fuchi and Takagi, 1998), chunked, and labeled with
IREX 8 Named Entity types by CRFs using Mini-
mum Classification Error rate (Suzuki et al, 2006),
and transformed into features. The context features
were defined using the template ?(head) entity (mid.)
attribute (tail)?. The words included in each part
were used as surface, part-of-speech and Named En-
tity label features added position information. Max-
imum word number of each part was set at 2 words.
The features have to appear in both the positive and
negative training data at least 5 times.
In the experiments, we used three domains, car
(?CAR?), broadcast program (?PRG?) and sports or-
ganization (?SPT?). The adjustment numbers for ba-
sic settings are Ns = 10, Na = 10, Nn = 100. Af-
ter running 10 iterations, we obtained 1000 entities
in total. SVM light (Joachims, 1999) with second
order polynomial kernel was used as the discrimina-
tive model. Parallel LDA, which is LDA with MPI
(Liu et al, 2011), was used for training 100 mix-
ture topic models and inference. Training corpus for
topic models consisted of the content gathered from
728
CAR PRG SPT
1. Baseline 0.249 0.717 0.781
2. Topic features + 1. 0.483 0.727 0.844
3. Negative selection + 2. 0.509 0.762 0.846
4. Candidate pruning + 3. 0.531 0.824 0.848
Table 1: The experimental results for the three domains.
Bold font indicates that the difference between accuracy
of the methods in the row and the previous row is signifi-
cant (P < 0.05 by binomial test) and italic font indicates
(P < 0.1).
14 days of blog articles. In the Markov-chain Monte
Carlo (MCMC) method, sampling was iterated 200
times for training with a burn-in taking 50 iterations.
These parameters were selected based on the results
of a preliminary experiment.
Four experimental settings were examined. First
is Baseline; it is described in Section 3.1. Second is
the first method with the addition of topic features.
Third is the second method with the addition of a
negative example selection module. Fourth is the
third method with the addition of a candidate prun-
ing module (equals the entire shaded part in Fig-
ure 1). Each extracted entity is labeled with cor-
rect or incorrect by two evaluators based on the re-
sults of a commercial search engine. The ? score for
agreement between evaluators was 0.895. Because
the third evaluator checked the two evaluations and
confirmed that the examples which were judged as
correct by either one of the evaluators were correct,
those examples were counted as correct.
4.2 Experimental Results
Table 1 shows the accuracy and significance for each
domain. Using topic features significantly improves
accuracy in the CAR and SPT domains. The nega-
tive example selection module improves accuracy in
the CAR and PRG domains. This means the method
could reduce the risk of selecting false-negative ex-
amples. Also, the candidate pruning method is ef-
fective for the CAR and PRG domains. The CAR
domain has lower accuracy than the others. This
is because similar entities such as motorcycles are
extracted; they have not only the same context but
also the same topic as the CAR domain. In the SPT
domain, the method with topic features offer signif-
icant improvements in accuracy and no further im-
provement was achieved by the other two modules.
To confirm whether our modules work properly,
we show some characteristic words belonging to
each topic that is similar and not similar to target do-
main in Table 2. Table 2 shows characteristic words
for one positive topic zh and two negative topics zl
and ze, defined as follow.
? zh (the second row) is the topic that maximizes
PT (z), which is used as a positive topic.
? zl (the fourth row) is the topic that minimizes
PT (z), which is used as a negative topic.
? ze (the fifth row) is a topic that, we consider, ef-
fectively eliminates ?drifted entities? extracted
by the baseline method. ze is eventually in-
cluded in the lower half of topic list sorted by
PT (z).
For a given topic, z, we chose topmost three words
in terms of topic-word score. The topic-word score
of a word, v, is defined as p(v|z)/p(v), where p(v)
is the unigram probability of v, which was estimated
by maximum likelihood estimation. For utilizing
candidate pruning, near topics including zh must be
similar to the domain. By contrast, for utilizing neg-
ative example selection, the lower half of topics, zl,
ze and other negative topics, must be far from the
domain. Our system succeeded in achieving this.
As shown in ?CAR? in Table 2, the nearest topic
includes ?shaken? (automobile inspection) and the
farthest topic includes ?naika? (internal medicine)
which satisfies our expectation. Furthermore, the ef-
fective negative topic is similar to the topic of drifted
entity sets (digital device). This indicates that our
method successfully eliminated drifted entities. We
can confirm that the other domains trend in the same
direction as ?CAR? domain.
5 Related Works
Some prior studies use every word in a docu-
ment/sentence as the features, such as the distribu-
tional approaches (Pantel et al, 2009). These meth-
ods are regarded as using global information, how-
ever, the space of word features are sparse, even if
the amount of data available is large. Our approach
can avoid this problem by using topic models which
729
domain CAR PRG SPT
words of the
nearest topic zh
(highest PT score)
shaken
(automobile inspection),
nosha (delivering a car),
daisha (loaner car)
Mari YAMADA,
Tohru KUSANO,
Reiko TOKITA
(Japanese stars)
toshu (pitcher),
senpatsu
(starting member),
shiai (game)
drifted entities
(using baseline)
iPod, mac
(digital device)
PS2, XBOX360
(video game)
B?z, CHAGE&ASKA
(music)
words of effective
negative topic ze
(Lower half of
PT score)
gaso (pixel),
kido (brightness),
mazabodo (mother board)
Lv. (level),
kariba (hunting area),
girumen (guild member)
sinpu (new release),
X JAPAN ,
Kazuyoshi Saito
(Japanese musicians)
words of
the farthest topic zl
(Lowest PT score)
naika (internal medicine),
hairan (ovulation),
shujii (attending doctor)
tsure (hook a fish),
choka (result of hooking),
choko (diary of hooking)
toritomento (treatment),
keana (pore),
hoshitsu (moisture retention)
Table 2: The characteristic words belonging to three topics, zh, zl and ze. zh is the nearest topic and zl is the farthest
topic for positive entity-attribute seed pairs. ze is an effective negative topic for eliminating ?drifted entities? extracted
by the baseline system.
are clustering methods based on probabilistic mea-
sures. By contrast, Pas?ca and Durme (2008) pro-
posed clustering methods that are effective in terms
of extraction, even though their clustering target is
only the surrounding context. Ritter and Etzioni
(2010) proposed a generative approach to use ex-
tended LDA to model selectional preferences. Al-
though their approach is similar to ours, our ap-
proach is discriminative and so can treat arbitrary
features; it is applicable to bootstrapping methods.
The accurate selection of negative examples is a
major problem for positive and unlabeled learning
methods or general bootstrapping methods and some
previous works have attempted to reach a solution
(Liu et al, 2002; Li et al, 2010). However, their
methods are hard to apply to the Bootstrapping al-
gorithms because the positive seed set is too small
to accurately select negative examples. Our method
uses topic information to efficiently solve both the
problem of extracting global information and the
problem of selecting negative examples.
6 Conclusion
We proposed an approach to set expansion that uses
topic information in three modules and showed that
it can improve expansion accuracy. The remaining
problem is that the grain size of topic models is not
always the same as the target domain. To resolve
this problem, we will incorporate the active learning
or the distributional approaches. Also, comparisons
with the previous works are remaining work. From
another perspective, we are considering the use of
graph-based approaches (Komachi et al, 2008) in-
corporated with the topic information using PHITS
(Cohn and Chang, 2000), to further enhance entity
extraction accuracy.
References
Kedar Bellare, Partha P. Talukdar, Giridhar Kumaran,
Fernando Pereira, Mark Liberman, Andrew McCal-
lum, and Mark Dredze. 2006. Lightly-supervised at-
tribute extraction. In Proceedings of the Advances in
Neural Information Processing Systems Workshop on
Machine Learning for Web Search.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. The Journal of Ma-
chine Learning Research, 3:993?1022.
David Cohn and Huau Chang. 2000. Learning to prob-
abilistically identify authoritative documents. In Pro-
ceedings of the 17th International Conference on Ma-
chine Learning, pages 167?174.
Takeshi Fuchi and Shinichiro Takagi. 1998. Japanese
Morphological Analyzer using Word Co-occurrence-
JTAG. In Proceedings of the 36th Annual Meeting
of the Association for Computational Linguistics and
17th International Conference on Computational Lin-
guistics, pages 409?413.
Zoubin Ghahramani and Katherine A. Heller. 2005.
Bayesian sets. In Proceedings of the Advances in Neu-
ral Information Processing Systems.
Thorsten Joachims. 1999. Making large-Scale SVM
Learning Practical. Advances in Kernel Methods -
Support Vector Learning. Software available at
http://svmlight.joachims.org/.
730
Mamoru Komachi, Taku Kudo, Masashi Shimbo, and
Yuji Matsumoto. 2008. Graph-based analysis of se-
mantic drift in Espresso-like bootstrapping algorithms.
In Proceedings of the 2008 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1011?1020.
Xiao-Li Li, Bing Liu, and See-Kiong Ng. 2010. Neg-
ative Training Data can be Harmful to Text Classi-
fication. In Proceedings of the 2010 Conference on
Empirical Methods in Natural Language Processing,
pages 218?228.
Bing Liu, Wee S. Lee, Philip S. Yu, and Xiaoli Li. 2002.
Partially supervised classification of text documents.
In Proceedings of the 19th International Conference
on Machine Learning, pages 387?394.
Zhiyuan Liu, Yuzhou Zhang, Edward Y. Chang, and
Maosong Sun. 2011. PLDA+: Parallel latent dirich-
let alocation with data placement and pipeline pro-
cessing. ACM Transactions on Intelligent Systems
and Technology, special issue on Large Scale Machine
Learning. Software available at http://code.
google.com/p/plda.
Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky.
2009. Distant supervision for relation extraction with-
out labeled data. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and the 4th
International Joint Conference on Natural Language
Processing of the AFNLP, pages 1003?1011.
Marius Pas?ca and Benjamin Van Durme. 2008. Weakly-
supervised acquisition of open-domain classes and
class attributes from web documents and query logs.
In Proceedings of the 46th Annual Meeting of the As-
sociation for Computational Linguistics, pages 19?27.
Patrick Pantel and Marco Pennacchiotti. 2006. Espresso:
Leveraging generic patterns for automatically harvest-
ing semantic relations. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics
and the 44th annual meeting of the Association for
Computational Linguistics, pages 113?120.
Patrick Pantel, Eric Crestan, Arkady Borkovsky, Ana-
Maria Popescu, and Vishnu Vyas. 2009. Web-scale
distributional similarity and entity set expansion. In
Proceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing, pages 938?
947.
Alan Ritter and Oren Etzioni. 2010. A Latent Dirich-
let Allocation method for Selectional Preferences. In
Proceedings of the 48th ACL Conference, pages 424?
434.
Luis Sarmento, Valentin Jijkuon, Maarten de Rijke, and
Eugenio Oliveira. 2007. More like these: grow-
ing entity classes from seeds. In Proceedings of the
16th ACM Conference on Information and Knowledge
Management, pages 959?962.
Jun Suzuki, Erik McDermott, and Hideki Isozaki. 2006.
Training Conditional Random Fields with Multivari-
ate Evaluation Measures. In Proceedings of the 21st
COLING and 44th ACL Conference, pages 217?224.
Michael Thelen and Ellen Riloff. 2002. A bootstrap-
ping method for learning semantic lexicons using ex-
traction pattern contexts. In Proceedings of the 2002
conference on Empirical methods in natural language
processing, pages 214?221.
731
Proceedings of the Multiword Expressions: From Theory to Applications (MWE 2010), pages 64?72,
Beijing, August 2010
Standardizing Complex Functional Expressions in Japanese  
Predicates: Applying Theoretically-Based Paraphrasing Rules  
 
Tomoko Izumi?    Kenji Imamura?    Genichiro Kikui?    Satoshi Sato? 
?NTT Cyber Space Laboratories, 
NTT Corporation 
{izumi.tomoko, imamura.kenji, 
kikui.genichiro}@lab.ntt.co.jp
?Graduate School of Engineering,  
Nagoya University 
ssato@nuee.nagoya-u.ac.jp 
 
 
 
Abstract 
In order to accomplish the deep semantic 
understanding of a language, it is essen-
tial to analyze the meaning of predicate 
phrases, a content word plus functional 
expressions. In agglutinating languages 
such as Japanese, however, sentential 
predicates are multi-morpheme expres-
sions and all the functional expressions 
including those unnecessary to the mean-
ing of the predicate are merged into one 
phrase. This triggers an increase in sur-
face forms, which is problematic for 
NLP systems. We solve this by introduc-
ing simplified surface forms of predi-
cates that retain only the crucial meaning 
of the functional expressions. We con-
struct paraphrasing rules based on syn-
tactic and semantic theories in linguistics. 
The results of experiments show that our 
system achieves the high accuracy of 
77% while reducing the differences in 
surface forms by 44%, which is quite 
close to the performance of manually 
simplified predicates. 
 
1 Introduction 
The growing need for text mining systems such 
as opinion mining and sentiment analysis re-
quires the deep semantic understanding of lan-
guages (Inui et al, 2008). In order to accomplish 
this, one needs to not only focus on the meaning 
of a single content word such as buy but also the 
meanings conveyed by function words or func-
tional expressions such as not and would like to. 
In other words, to extract and analyze a predi-
cate, it is critical to consider both the content 
word and the functional expressions (Nasukawa, 
2001). For example, the functional expressions 
would like to as in the predicate ?would like to 
buy? and can?t as in ?can?t install? are key ex-
pressions in detecting the customer?s needs and 
complaints, providing valuable information to 
marketing research applications, consumer opi-
nion analysis etc.  
Although these functional expressions are 
important, there have been very few studies that 
extensively deal with these functional expres-
sions for use in natural language processing 
(NLP) systems (e.g., Tanabe et al, 2001; Mat-
suyoshi and Sato, 2006, 2008). This is due to the 
fact that functional expressions are syntactically 
complicated and semantically abstract and so are 
poorly handled by NLP systems. 
In agglutinating languages such as Japanese, 
functional expressions appear in the form of 
suffixes or auxiliary verbs that follow the 
content word without any space. This sequence 
of a content word (c for short) plus several of 
functional expressions (f for short) forms a 
predicate in Japanese (COMP for completive 
aspect marker, NOM for nominalizer, COP for 
copular verb).   
(1) kat -chai -takat -ta -n -da 
buy -COMP -want -PAST -NOM -COP 
c -f1 -f2 -f3 -f4 -f5 
?(I) wanted to buy (it)? 
The meaning of ?want to? is expressed by -tai 
(f2) and the past tense is expressed by -ta (f3). 
64
The other functional expressions, -chai(f1), -n(f4), 
and -da(f5), only slightly alter the predicative 
meaning of ?wanted to buy,? as there is no direct 
English translation. Therefore, (1) expresses the 
same fact as (2). 
(2)  kai -takat -ta 
  buy -want -PAST 
?(I) wanted to buy (it).? 
As shown, in Japanese, once one extracts a 
predicate phrase, the number of differences in 
surface forms increases drastically regardless of 
their similarities in meaning. This is because 
sentential predicates are multi-word or multi-
morpheme expressions and there are two differ-
ent types of functional expressions, one which is 
crucial for the extraction of predicative meaning 
and the other, which is almost unnecessary for 
NLP applications. This increase in surface forms 
complicates NLP systems including text mining 
because they are unable to recognize that these 
seemingly different predicates actually express 
the same fact. 
In this study, we introduce paraphrasing rules 
to transform a predicate with complex functional 
expressions into a simple predicate. We use the 
term standardize to refer to this procedure. 
Based on syntactic and semantic theories in lin-
guistics, we construct a simple predicate struc-
ture and categorize functional expressions as 
either necessary or unnecessary. We then pa-
raphrase a predicate into one that only retains the 
crucial meaning of the functional expression by 
deleting unnecessary functional expressions 
while adding necessary ones. 
 The paper is organized as follows. In Section 
2, we provide related work on Japanese 
functional expressions in NLP systems as well as 
problems that need to be solved. Section 3 
introduces several linguistic theories and our 
standardizing rules that we constructed based on 
these theories. Section 4 describes the 
experiments conducted on our standardization 
system and the results. Section 5 discusses the 
results and concludes the paper. Throughout this 
paper, we use the term functional expressions to 
indicate not only a single function word but also 
compounds (e.g., would like to). 
 
2 Previous Studies and Problems  
Shudo et al (2004) construct abstract semantic 
rules for functional expressions and use them in 
order to find whether two different predicates 
mean the same. Matsuyoshi and Sato (2006, 
2008) construct an exhaustive dictionary of 
functional expressions, which are hierarchically 
organized, and use it to produce different func-
tional expressions that are semantically equiva-
lent to the original one.  
 Although these studies provide useful in-
sights and resources for NLP systems, if the in-
tention is to extract the meaning of a predicate, 
we find there are still problems that need to be 
solved. There are two problems that we focus on. 
 The first problem is that many functional ex-
pressions are unnecessary, i.e., they do not ac-
tually alter the meaning of a predicate.   
(3) yabure -teshimat -ta -no -dearu 
 rip -COMP -PAST -NOM -COP 
 c -f1 -f2 -f3 -f4  
 ?(something) ripped.? 
(3) can be simply paraphrased as (4) 
(4) yabure -ta 
 rip -PAST 
 c -f1  
In actual NLP applications such as text mining, 
it is essential that the system recognizes that (3) 
and (4) express the same event of something 
?ripped.? In order to achieve this, the system 
needs to recognize -teshimat, -no, and -dearu as 
unnecessary (f1, f3, f4 ??). Previous studies that 
focus on paraphrasing of one functional expres-
sion to another (f ? f?) cannot solve this prob-
lem. 
 The second problem is that we sometimes 
need to add certain functional expressions in 
order to retain the meaning of a predicate (? ?f). 
(5) (Hawai-ni) P1iki, P2nonbirishi -takat -ta 
 (Hawaii-to)  go relax -want -PAST 
   c1 c2 f1 f2 
?I wanted to go to Hawaii and relax.? 
(5) has a coordinate structure, and two verbal 
predicates, iki (P1) ?go? and nonbirishi-takat-ta 
(P2) ?wanted to relax?, are coordinated.  
 As the English translation indicates, the first 
predicate in fact means iki-takat-ta ?wanted to 
65
go,? which implies that the speaker was not able 
to go to Hawaii. If the first predicate was ex-
tracted and analyzed as iku, the base (present) 
form of ?go,? then this would result in a wrong 
extraction of predicate, indicating the erroneous 
fact of going to Hawaii in the future (Present 
tense in Japanese expresses a future event). In 
this case, we need to add the functional expres-
sions takat ?want? and ta, the past tense marker, 
to the first verbal predicate.  
As shown, there are two problems that need 
to be solved in order for a system to extract the 
actual meaning of a predicate. 
i. Several functional expressions are neces-
sary for sustaining the meaning of the event 
expressed by a predicate while others barely 
alter the meaning (f ??). 
ii. Several predicates in coordinate sentences 
lack necessary functional expressions at the 
surface level (? ?f) and this results in a 
wrong extraction of the predicate meaning. 
Based on syntactic and semantic theories in lin-
guistics, we construct paraphrasing rules and 
solve these problems by standardizing complex 
functional expressions. 
 
3 Construction of Paraphrasing Rules  
The overall flow of our standardizing system is 
depicted in Figure 1. The system works as fol-
lows. 
i. Given a parsed sentence as an input, it ex-
tracts a predicate(s) and assigns a semantic 
label to each functional expression based on 
Matsuyoshi and Sato (2006). 
ii. As for an intermediate predicate, necessary 
functional expressions are added if missing 
(? ?f). 
iii. From each predicate, delete unnecessary 
functional expressions that do not alter the 
meaning of the predicate (f ??). 
iv. Conjugate each element and generate a 
simplified predicate.  
There are two fundamental questions that we 
need to answer to accomplish this system. 
A) What are UNNECCESARY functional ex-
pressions (at least for NLP applications), 
i.e., those that do not alter the meaning of 
the event expressed by a predicate? 
B) How do we know which functional expres-
sions are missing and so should be added? 
We answer these questions by combining what is 
needed in NLP applications and what is dis-
cussed in linguistic theories. We first answer 
Question A. 
 
3.1 Categorization of Functional Expressions 
As discussed in Section 1 and in Inui et al 
(2008), what is crucial in the actual NLP appli-
cations is to be able to recognize whether two 
seemingly different predicates express the same 
fact.  
This perspective of factuality is similar to the 
truth-value approach of an event denoted by pre-
dicates as discussed in the field of formal seman-
tics (e.g., Chierchia and Mcconnel-Ginet, 2000; 
Portner, 2005). Although an extensive investiga-
tion of these theories is beyond the scope of this 
paper, one can see that expressions such as tense 
(aspect), negation as well as modality, are often 
discussed in relation to the meaning of an event 
(Partee et al, 1990; Portner, 2005). 
Tense (Aspect): Expresses the time in (at/for) 
which an event occurred. 
Negation: Reverses the truth-value of an event. 
Modality: Provides information such as possi-
bility, obligation, and the speaker?s eagerness 
with regard to an event and relate it to what is 
true in reality. 
The above three categories are indeed useful in 
explaining the examples discussed above. 
(6) kat -chai -takat -ta -n -da 
buy -COMP -want -PAST -NOM -COP 
 aspect modality tense(aspect) 
(7) kai -takat -ta 
 buy -want -PAST 
  modality tense(aspect) 
 ?wanted to buy? 
The predicate ?kat-chai-takat-ta-n-da? in (6) and 
?kai-takat-ta? in (7) express the same event be-
cause they share the same tense (past), negation 
(none), and modality (want). Although (6) has 
the completive aspect marker -chai while (7) 
does not, they still express the same fact. This is 
because the Japanese past tense marker -ta also 
has a function to express the completive aspect. 
The information expressed by -chai in (6) is re-
66
dundant and so unnecessary.  
On the other hand, the predicate ?iku? in (5) 
and ?iki-takat-ta,? which conveys the actual 
meaning of the predicate, express a different fact 
because they establish a different tense (present 
vs. past) and different modality (none vs. want).  
As shown, once we examine the abstract se-
mantic functions of functional expressions, we 
can see the factual information in a predicate is 
influenced by tense (aspect), negation, and mod-
ality. Therefore, the answer to Question A is that 
necessary functional expressions are those that 
belong to tense (aspect), negation, and modality. 
Furthermore, if there are several functional ex-
pressions that have the same semantic function, 
retaining one of them is sufficient. 
 
3.2 Adding Necessary Functional Expressions 
The next question that we need to answer is how 
we find which functional expressions are miss-
ing when standardizing an intermediate predicate 
in a coordinate structure (e.g., (5)). We solve this 
based on a detailed analysis of the syntactic 
structure of predicates. 
Coordinate structures are such that several 
equivalent phrases are coordinated by conjunc-
tions such as and, but, and or. If a predicate is 
coordinated with another predicate, these two 
predicates must share the same syntactic level. 
Therefore, the structure in (5) is indeed depicted 
as follows (What TP and ModP stand for will be 
discussed later). 
[TP[ModP[VP(Hawai-ni) iki][VPnonbirishi]takat]ta ] 
[TP[ModP[VP(Hawaii-to) go][VPrelax] want]PAST] 
This is the reason why the first predicate iki 
should be paraphrased as iki-takat-ta ?wanted to 
go.? It needs to be tagged with the modality ex-
pression tai and the past tense marker ta, which 
seems to attach only to the last predicate.  
 This procedure of adding necessary function-
al expressions to the intermediate predicate is 
not as simple as it seems, however.   
(8) nemutai-mitai-de kaeri -tagatte -tei -ta 
sleepy-seems-COP gohome-want-CONT-PAST 
?He seemed sleepy and wanted to go home.? 
In (8), the first predicate nemutai-mitai-de ?seem 
to be sleepy? should be paraphrased as nemutai-
mitai-dat-ta, ?seemed to be sleepy,? in which 
only the functional expression indicating past is 
required. The other functional expressions such 
as tagatte ?want,? and the aspect marker tei 
(CONTinuation) should not be added (nemutai-
mitai-de-tagat(want)-tei(CONT)-ta(PAST) is 
completely ungrammatical).  
Input  
A parsed Sentence 
Hontoo-wa Hawai-ni iki, nonbirishi takat ta n da kedo
Really-TOP Hawaii-to go relax want PAST NOM COP but 
?I wanted to go to Hawaii and relax if I could.? 
i. Predicate Extraction &
Labeling Semantic Classes 
to Functional Expressions 
ii. ADD necessary 
functional expressions 
(? ? f) 
iii. DELETE unnecessary 
functional expressions 
(f ? ?) 
iv. Conjugate and  
Generate simple predicates 
Output  
Simplified Predicates 
iki 
go 
c 
[[[VP] ?] ?] 
iki tai ta
go want PAST
c [??] [??]
iki takat ta
go want PAST 
nonbirishi takat ta  
relax want PAST  
iki-takat-ta
?wanted to go? 
nonbirishi-takat-ta 
?wanted to relax? 
Figure 1. The flow of Standardization. 
iki tai ta
go want PAST 
c [??] [??] 
nonbirishi takat ta n da kedo 
relax want PAST NOM COP but 
c [??] [??] [??] [??] [????]
nonbirishi takat ta n da kedo 
relax  want PAST NOM COP but 
c [??] [??] [??] [??] [????] 
[[[VP]             ModP]    TP] 
67
 Furthermore, the intermediate predicate in the 
following example does not allow any functional 
expressions to be added. 
(9) (imawa) yasui-ga (mukashiwa) takakat-ta 
(today) inexpensive-but (in old days) expensive-
PAST 
?(They) are inexpensive (today), (but) used to 
be very expensive (in the old days.)? 
In (9), the first predicate yasui ?inexpensive? 
should not be paraphrased as yasukat-ta ?was 
inexpensive? since this would result in the un-
grammatical predicate of ?*(they) were inexpen-
sive (today).?  
 In order to add necessary functional expres-
sions to an intermediate predicate, one needs to 
solve the following two problems. 
i. Find whether the target predicate indeed 
lacks necessary functional expressions.  
ii. If such a shortfall is detected, decide which 
functional expressions should be added to 
the predicate. 
We solve these problems by turning to the in-
completeness of the syntactic structure of a pre-
dicate. 
Studies such as Cinque (2006) and Rizzi 
(1999) propose detailed functional phrases such 
as TopP (Topic Phrase) in order to fully describe 
the syntactic structures of a language. We adopt 
this idea and construct a phrase structure of Jap-
anese predicates which borrows from the func-
tional phrases of TP, ModP, and FocP (Figure 2). 
 ModP stands for a modality phrase and this is 
where modality expressions can appear.1  FocP 
stands for a focus phrase. This is the phrase 
where the copula da appears. This phrase is 
needed because several modality expressions 
syntactically need the copula da in either the 
following or preceding position (Kato, 2007). 
The existence of FocP also indicates that the 
modality expressions within the phrase are com-
plete (no more modality phrase is attached). TP 
stands for a tense phrase and this is where the 
tense marker appears.  
 Note that this structure is constructed for the 
purpose of Standardization and other functional 
                                                 
1 The structure of Figure 2 is recursive. A modality expres-
sion can appear after a TP. Also, more than one ModP can 
appear although ModP and FocP are optional. 
projections such as NegP (negation phrase) will 
not be discussed although we assume there must 
be one. Based on the predicate structure in Fig-
ure 2, we solve the two problems as follows. 
 
The first problem: Detecting whether the target 
predicate lacks necessary functional expressions.  
? If the predicate has the past tense marker ta 
or if the coordinate conjunction following 
the predicate is for combining phrases with 
tense, then consider the predicate as com-
plete and do not add any functional expres-
sions. Otherwise, consider the predicate as 
incomplete and add the appropriate func-
tional expressions. 
The underlying principle of this rule is that if a 
predicate is tensed, then its syntactic structure is 
complete. As often described in syntactic theo-
ries (e.g., Adger, 2003), a sentence can be said to 
be a phrase with tense (i.e., TP). In other words, 
if a predicate is tensed, then it can stand alone as 
a sentence.  
 By adopting this idea, we judge the com-
pleteness of a predicate by the existence of tense. 
Because Japanese marks past tense by the past 
tense marker -ta, if a predicate has -ta, it is com-
plete and no functional expressions need be add-
ed.  
 However, Japanese does not hold an explicit 
present tense marker; the base form of a verb is 
also a present form. We solve this by looking at 
which conjunction follows the predicate. As dis-
cussed in Minami (1993), the finite state and the 
type of conjunction are related; some conjunc-
tions follow tensed phrases while others follow 
infinitival phrases. Following this, we categorize 
all the coordinate conjunctions based on whether 
they can combine with a tensed phrase. These 
conjunctions are listed as tensed in Table 1. If 
TP  
3 
(FocP) T:ta PAST [??] 
3 
(ModP)*   Foc:da COP [??] 
3 
VP   Mod: mitai ?seems? [??] 
4 
iku ?go? 
Figure 2. Structure of a predicate. 
68
the target phrase is followed by one of those 
conjunctions, then we do not add any functional 
expressions to them because they are complete. 
 
The second problem: Finding the appropriate 
functional expressions for incomplete interme-
diate predicates. 
As discussed, we assume that predicates are 
coordinated at one of the functional phrase levels 
in Figure 2. Functional expressions that need to 
be added are, therefore, those of the outer phras-
es of the target phrase.  
 For example, if the target phrase has da, the 
head of FocP, then it only needs the past tense 
marker to be added, which is located above the 
FocP (i.e., TP). This explains the paraphrasing 
pattern of (8). Therefore, by looking at which 
functional expressions held by the target predi-
cate, one can see that functional expressions to 
be added are those that belong to phrases above 
the target phrase. 
 As shown, the answer to Question B is that 
we only add functional expressions to incom-
plete predicates, which are judged based on the 
existence/absence of tense. The appropriate 
functional expressions to be added are those of 
outer phrases of the target phrase. 
 
3.3 Implementing the Standardization 
In this final subsection, we describe how we ac-
tually implement our theoretical observations in 
our standardization system.  
 
CATEGORIZE functional expressions 
First, we selected functional expressions that 
belong to our syntactic and semantic categories 
from those listed in Matsuyoshi and Sato (2006), 
a total of about 17,000 functional expressions 
with 95 different semantic labels. We use ab-
stract semantic labels, such as ?completion,? 
?guess,? and ?desire? for the categorization 
(Table 2).  
 We divided those that did not belong to our 
syntactic and semantic categories into Deletables 
and Undeletables. Deletables are those that do 
not alter the meaning of an event and are, there-
fore, unnecessary. Undeletables are those that 
are a part of content words, and so cannot be 
deleted (e.g., kurai [??] ?about? as in 1-man-
en-kurai-da ?is about one million yen?). Based 
on the categorization of semantic labels as well 
as surface forms of functional expressions, our 
system works as follows; 
 
ADD necessary functional expressions  
A-1: Examine whether the target predicate has 
the tense marker ta or it is followed by the 
conjunctions categorized as tensed. If not, 
then go to Step A-2. 
A-2: Based on the semantic label of the target 
predicate, decide which level of syntactic 
phrase the predicate projects. Add functional 
expressions from the last predicate that be-
longs to outer phrases. 
 
DELETE unnecessary functional expressions 
D-1: Delete all the functional expressions that 
are categorized as Deletables. 
D-2: Leave only one functional expression if 
there is more than one same semantic label. 
For those categorized as Negation, however, 
delete all if the number of negations is even. 
Otherwise, leave one. 
D-3: Delete those categorized as Focus if they 
do not follow or precede a functional expres-
sion categorized as Modality.  
 
GENERATE simple predicates 
Last, conjugate all the elements and generate 
simplified surface forms of predicates. 
 
4 Experiments and Evaluations 
4.1 Constructing Paraphrase Data 
We selected 2,000 sentences from newspaper 
and blog articles in which more than one predi-
cate were coordinated.2 We manually extracted 
predicates (c-f1-f2..fn). Half of them were those in 
which the last predicate had three or more func-
tional expressions (n ? 3). 
                                                 
2 We use Mainichi Newspapers from the year 2000. 
Table 1. Coordinate conjunctions. 
Not tensed Tensed 
gerundive 
form, te  
shi, dakedenaku, ueni, bakarika, 
hoka(ni)(wa), keredo, ga, nonitai-
shi(te),ippou(de),hanmen  
69
We then asked one annotator with a linguistic 
background to paraphrase each predicate into the 
simplest form possible while retaining the mean-
ing of the event.3 We asked another annotator, 
who also has a background in linguistics, to 
check whether the paraphrased predicates made 
by the first annotator followed our criterion, and 
if not, asked the first annotator to make at least 
one paraphrase. 424 out of 4,939 predicates 
(8.5%) were judged as not following the crite-
rion and were re-paraphrased. This means that 
the accuracy of 91.5% is the gold standard of our 
task. 
One of the authors manually assigned a cor-
rect semantic label to each functional expression. 
Procedure i in Figure 1 is, therefore, manually 
implemented in our current study. 
 
4.2 Experiments and Results 
Based on the standardization rules discussed in 
Section 3, our system automatically paraphrased 
functional expressions of test predicates into 
simple forms. We excluded instances that had 
segmentation errors and those that were judged 
as inappropriate as a predicate. 4  A total of 
1,501 intermediate predicates (287 for develop-
ment and 1,214 for test) and 1,958 last predi-
cates (391 for development and 1,567 for test) 
were transformed into simple predicates. 
 The accuracy was measured based on the ex-
act match in surface forms with the manually 
constructed paraphrases. For comparison, we 
                                                 
3 We asked to delete or add functional expressions from 
each predicate when paraphrasing. Only the surface forms 
(and not semantic labels) were used for annotation. 
4 In Japanese, a gerundive form of a verb is sometimes used 
as a postposition. The annotators excluded these examples 
as ?not-paraphrasable.? 
used the following baseline methods. 
? No Add/Delete: Do not add/delete any 
functional expression.  
? Simp Add: Simply add all functional ex-
pressions that the intermediate phrase does 
not have from the last predicate. 
Table 3 indicates the results. Our standardizing 
system achieved high accuracy of around 77% 
and 83 % in open (against the test set) and 
closed tests (against the development set) com-
pared to the baseline methods (No Add/Delete 
(open), 55%; Simp Add (open), 33%). 
We also measured the reduced rate of differ-
ences in surface forms. We counted the number 
of types of functional expressions in the last pre-
dicates (a sequence of f1-f2-f3 is counted as one) 
before and after the standardization. 
For comparison, we also counted the number 
of functional expressions of the manually pa-
raphrased predicates. Table 4 lists the results. As 
shown, our standardizing system succeeded in 
reducing surface differences in predicates from 
the original ones at the rate of 44.0%, which is 
quite close to the rate achieved by the human 
annotators (52.0%). 
 
5 Discussion and Conclusion 
Our standardization system succeeded in gene-
rating simple predicates in which only functional 
expressions crucial for the factual meaning of 
the predicate were retained.  
The predicates produced by our system 
showed fewer variations in their surface forms 
while around 77% of them exactly matched the 
simplified predicates produced by human anno-
tators, which is quite high compared to the base-
line systems.  
Table 2. Syntactic and semantic categorization of semantic labels. 
Syntactic  Semantic  Semantic Labels
T if the 
surface is ta 
Tense 
(Aspect) 
??(completion),??,??,??,??,??,??,??,???, ???, ??,?
?, ??, ?? 
 Negation ??(negation), ??,????,????,???,???,???, ???, ???
Mod Modality ??(guess),  ??(desire),??,??,??,??,??,??, ??, ??, ??, ??
??, ???, ???,???,??,???,???
Foc Focus ??(affirmation), ???,??
 Deletables ??(politeness),?-??,??,??,?-??,????,??, ??, ????, ?
?,??, ????,????,??,??,???
 Undele-
tables 
??(about), ??,??,???,???,??,??,??,??, ??, ????,?
?, ??, ??, ??, ??, ???, ???, ??, ???, ????, ????, ??, 
??, ???, ??,??,??,??,??,??,??,??,?? 
70
This was achieved because we constructed 
solid paraphrasing rules by applying linguistic 
theories in semantics and syntax. The quite low 
accuracy of the baseline method, especially 
SimpAdd, further supports our claim that im-
plementing linguistic theories in actual NLP ap-
plications can greatly improve system perfor-
mance. 
Unlike the study by Inui et al (2008), we did 
not include the meaning of a content word for 
deciding the factuality of the event nor did we 
include it in the paraphrasing rules. This lowers 
the accuracy. Several functional expressions, 
especially those expressing aspect, can be de-
leted or added depending on the meaning of the 
content word. This is because content words in-
herently hold aspectual information, and one 
needs to compare it to the aspectual information 
expressed by functional expressions. Because we 
need a really complicated system to compute the 
abstract semantic relations between a content 
word and functional expressions, we leave this 
problem for future research.  
Regardless of this, our standardizing system 
is useful for a lot of NLP applications let alne 
text mining. As mentioned in Inui et al (2008), 
bag-of-words-based feature extraction is insuffi-
cient for conducting statistically-based deep se-
mantic analysis, such as factual analysis. If stan-
dardized predicates were used instead of a single 
content word, we could expect an improvement 
in those statistically-based methods because each 
predicate holds important information about fact 
while differences in surface forms are quite li-
mited. 
In conclusion, we presented our system for 
standardizing complex functional expressions in 
Japanese predicates. Since our paraphrasing 
rules are based on linguistic theories, we suc-
ceeded in producing simple predicates that have 
only the functional expressions crucial to under-
standing of the meaning of an event. Our future 
research will investigate the relationship be-
tween the meaning of content words and those of 
functional expressions in order to achieve higher 
accuracy. We will also investigate the impact of 
our standardization system on NLP applications.  
 
References 
Adger, David (2003). Core Syntax: A minimalist ap-
proach. New York: Oxford University Press. 
Chierchia, Gennaro, & Sally McConnell-Ginet (2000). 
Meaning and grammar: An introduction to se-
mantics (2nd ed.). Cambridge, MA: The MIT 
press. 
Cinque, Guglielmo (2006). Restructuring and func-
tional heads: The cartography of syntactic struc-
tures, Vol. 4. New York: Oxford University Press. 
Haugh, Michael (2008). Utterance-final conjunctive 
particles and implicature in Japanese conversation. 
Pragmatics, 18 (3), 425-451. 
Inui, Kentaro, Shuya Abe, Kazuo Hara, Hiraku Mori-
ta, Chitose Sao, Megumi Eguchi, Asuka Sumida, 
Koji Murakami, & Suguru Matsuyoshi (2008). 
Experience mining: Building a large-scale data-
base of personal experiences and opinions from 
web documents. Proceedings of the 2008 
IEEE/WIC/ACM International Conference on 
Web Intelligence and Intelligent Agent Technolo-
gy, Vol. 1., 314-321. 
Kato, Shigehiro (2007). Nihongo-no jutsubu-kouzou 
to kyoukaisei [Predicate complex structure and 
morphological boundaries in Japanese]. The an-
nual report on cultural science, Vol. 122(6) (pp. 
97-155). Sapporo, Japan: Hokkaido University, 
Graduate School of Letters. 
Matsuyoshi, Suguru, & Satoshi Sato (2006). Compi-
lation of a dictionary of Japanese functional ex-
pressions with hierarchical organization. Proceed-
ings of the 21st International Conference on 
Computer Processing of Oriental Languages 
 Normalization No Add/Delete Simp Add 
Open (Intermediate) 77.7%(943/1214) 57.8%(702/1214) 32.8%(398/1214) 
Closed (Intermediate) 82.9%(238/287) 62.0%%(178/287) 35.2%(101/287) 
Open (Last) 76.2%(1194/1567) 51.9% (203/391) n.a 
Closed (Last) 83.4%(326/391) 48.1%(188/391) n.a. 
Table 3. Results of our normalization system. 
Original 943 types Reduced Rate
Normalization 530 types 44.0% 
Human Annotation 448 types 52.0% 
Table 4. Reduced rate of surface forms. 
71
(ICCPOL), Lecture Notes in Computer Science, 
Vol. 4285, 395-402.  
Matsuyoshi, Suguru, & Satoshi Sato (2008). Auto-
matic paraphrasing of Japanese functional expres-
sions using a hierarchically organized dictionary. 
Proceedings of the 3rd International Joint Confe-
rence on Natural Language Processing (IJCNLP), 
Vol. 1, 691-696. 
Minami, Fujio (1993). Gendai nihongobunpou-no 
rinkaku [Introduction to modern Japanese gram-
mar]. Tokyo: Taishuukan. 
Nasukawa, Tetsuya (2001). Kooru sentaa-niokeru 
tekisuto mainingu [Text mining application for 
call centers]. Journal of Japanese society for Ar-
tificial Intelligence, 16(2), 219-225. 
Partee, Barbara H., Alice ter Meulen, & Robert E. 
Wall (1990). Mathematical methods in Linguistics. 
Dordrecht, The Netherland: Kluwer. 
Portner, Paul H. (2005). What is meaning?: Funda-
mentals of formal semantics. Malden, MA: 
Blackwell. 
Rizzi, Luigi (1999). On the position ?Int(errogative)? 
in the left periphery of the clause. Ms., Universit? 
di Siena. 
Shudo, Kosho, Toshifumi Tanabe, Masahito Takaha-
shi, & Kenji Yoshimura (2004). MWEs as non-
propositional content indicators. Proceedings of 
second Association for Computational Linguistics 
(ACL) Workshops on Multiword Expressions: In-
tegrating Processing, 32-39. 
Tanabe, Toshifumi, Kenji Yoshimura & Kosho Shu-
do (2001). Modality expressions in Japanese and 
their automatic paraphrasing. Proceedings of the 
6th Natural Language Processing Pacific Rim 
Symposium (NLPRS), 507-512. 
Tsujimura, Natsuko. (2007). An Introduction to Jap-
anese Linguistics (2nd Ed.). Malden, MA: Black-
well. 
72
