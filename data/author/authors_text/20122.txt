Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 78?87,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
Multi-Granular Aspect Aggregation in Aspect-Based Sentiment Analysis
John Pavlopoulos and Ion Androutsopoulos
Department of Informatics
Athens University of Economics and Business
Patission 76, GR-104 34 Athens, Greece
http://nlp.cs.aueb.gr/
Abstract
Aspect-based sentiment analysis estimates
the sentiment expressed for each particu-
lar aspect (e.g., battery, screen) of an en-
tity (e.g., smartphone). Different words
or phrases, however, may be used to re-
fer to the same aspect, and similar as-
pects may need to be aggregated at coarser
or finer granularities to fit the available
space or satisfy user preferences. We in-
troduce the problem of aspect aggrega-
tion at multiple granularities. We decom-
pose it in two processing phases, to al-
low previous work on term similarity and
hierarchical clustering to be reused. We
show that the second phase, where aspects
are clustered, is almost a solved prob-
lem, whereas further research is needed
in the first phase, where semantic simi-
larity measures are employed. We also
introduce a novel sense pruning mecha-
nism for WordNet-based similarity mea-
sures, which improves their performance
in the first phase. Finally, we provide pub-
licly available benchmark datasets.
1 Introduction
Given a set of texts discussing a particular en-
tity (e.g., reviews of a laptop), aspect-based senti-
ment analysis (ABSA) attempts to identify the most
prominent (e.g., frequently discussed) aspects of
the entity (e.g., battery, screen) and the average
sentiment (e.g., 1 to 5 stars) for each aspect or
group of aspects, as in Fig. 1. Most ABSA systems
perform all or some of the following (Liu, 2012):
subjectivity detection to retain only sentences (or
other spans) expressing subjective opinions; as-
pect extraction to extract (and possibly rank) terms
corresponding to aspects (e.g., ?battery?); aspect
aggregation to group aspect terms that are near-
synonyms (e.g., ?price?, ?cost?) or to obtain aspects
Figure 1: Aspect groups and scores of an entity.
at a coarser granularity (e.g., ?chicken?,?steak?,
and ?fish? may be replaced by ?food? in restaurant
reviews); and aspect sentiment score estimation to
estimate the average sentiment for each aspect or
group of aspects. In this paper, we focus on aspect
aggregation, the least studied stage of the four.
Aspect aggregation is needed to avoid reporting
separate sentiment scores for aspect terms that are
very similar. In Fig. 1, for example, showing sep-
arate lines for ?money?, ?price?, and ?cost? would
be confusing. The extent to which aspect terms
should be aggregated, however, also depends on
the available space and user preferences. On de-
vices with smaller screens, it may be desirable to
aggregate aspect terms that are similar, though not
necessarily near-synonyms (e.g., ?design?, ?color?,
?feeling?) to show fewer lines (Fig. 1), but finer as-
pects may be preferable on larger screens. Users
may also wish to adjust the granularity of aspects,
e.g., by stretching or narrowing the height of Fig. 1
on a smartphone to view more or fewer lines.
Hence, aspect aggregation should be able to pro-
duce groups of aspect terms for multiple granular-
ities. We assume that the aggregated aspects are
displayed as lists of terms, as in Fig. 1. We make
no effort to order (e.g., by frequency) the terms in
each list, nor do we attempt to produce a single
(more general) term to describe each aggregated
aspect, leaving such tasks for future work.
ABSA systems usually group synonymous (or
near-synonymous) aspect terms (Liu, 2012). Ag-
78
gregating only synonyms (or near-synonyms),
however, does not allow users to select the desir-
able aspect granularity, and ignores the hierarchi-
cal relations between aspect terms. For example,
?pizza? and ?steak? are kinds of ?food? and, hence,
the three terms can be aggregated to show fewer,
coarser aspects, even though they are not syn-
onyms. Carenini et al. (2005) used a predefined
domain-specific taxonomy to hierarchically aggre-
gate aspect terms, but taxonomies of this kind
are often not available. By contrast, we use only
general-purpose taxonomies (e.g., WordNet), term
similarity measures based on general-purpose tax-
onomies or corpora, and hierarchical clustering.
We define multi-granular aspect aggregation to
be the task of partitioning a given set of aspect
terms (generated by a previous aspect extraction
stage) into k non-overlapping clusters, for multi-
ple values of k. A further constraint is that the
clusters have to be consistent for different k val-
ues, meaning that if two aspect terms t
1
, t
2
are
placed in the same cluster for k = k
1
, then t
1
and t
2
must also be grouped together (in the same
cluster) for every k = k
2
with k
2
< k
1
, i.e., for
every coarser grouping. For example, if ?waiter?
and ?service? are grouped together for k = 5, they
must also be grouped together for k = 4, 3, 2
and (trivially) k = 1, to allow the user to feel
that selecting a smaller number of aspect groups
(narrowing the height of Fig. 1) has the effect of
zooming out (without aspect terms jumping un-
expectedly to other aspect groups), and similarly
for zooming in.
1
This requirement is satisfied by
using agglomerative hierarchical clustering algo-
rithms (Manning and Sch?utze, 1999; Hastie et al.,
2001), which in our case produce term hierarchies
like the ones of Fig. 2. By using slices (nodes at a
particular depth) of the hierarchies that are closer
to the root or the leaves, we obtain fewer or more
clusters. The vertical dotted lines of Fig. 2 illus-
trate two slices for k = 4. By contrast, flat clus-
tering algorithms (e.g., k-means) do not satisfy the
consistency constraint for different k values.
Agglomerative clustering algorithms require a
measure of the distance between individuals, in
our case a measure of how similar two aspect
terms are, and a linkage criterion to specify which
clusters should be merged to form larger (coarser)
clusters. To experiment with different term sim-
1
We also require the clusters to be non-overlapping to
make this zooming in and out metaphor clearer to the user.
Figure 2: Example aspect hierarchies produced by
agglomerative hierarchical clustering.
food fish sushi dishes wine
food 5 4 4 4 2
fish 4 5 4 2 1
sushi 4 4 5 3 1
dishes 4 2 3 5 2
wine 2 1 1 2 5
Table 1: An aspect term similarity matrix.
ilarity measures and linkage criteria, we decom-
pose multi-granular aspect aggregation in two pro-
cessing phases. Phase A fills in a symmetric ma-
trix, like the one of Table 1, with scores show-
ing the similarity of each pair of input aspect
terms; the matrix in effect defines the distance
measure to be used by agglomerative clustering.
In Phase B, the aspect terms are grouped into k
non-overlapping clusters, for varying values of k,
given the matrix of Phase A and a linkage crite-
rion; a hierarchy like the ones of Fig. 2 is first
formed via agglomerative clustering, and fewer or
more clusters (for different values of k) are then
obtained by using different slices of the hierarchy,
as already discussed. Our two-phase decomposi-
tion can also accommodate non-hierarchical clus-
tering algorithms, provided that the consistency
constraint is satisfied, but we consider only ag-
glomerative hierarchical clustering in this paper.
The decomposition in two phases has three
main advantages. Firstly, it allows reusing previ-
ous work on term similarity measures (Zhang et
al., 2013), which can be used to fill in the ma-
trix of Phase A. Secondly, the decomposition al-
lows different linkage criteria to be experimen-
tally compared (in Phase B) using the same sim-
ilarity matrix (of Phase A), i.e., the same distance
79
measure. Thirdly, the decomposition leads to high
inter-annotator agreement, as we show experimen-
tally. By contrast, in preliminary experiments we
found that asking humans to directly evaluate as-
pect hierarchies produced by hierarchical cluster-
ing, or to manually create gold aspect hierarchies
led to poor inter-annotator agreement.
We show that existing term similarity measures
perform reasonably well in Phase A, especially
when combined, but there is a large scope for im-
provement. We also propose a novel sense pruning
method for WordNet-based similarity measures,
which leads to significant improvements in Phase
A. In Phase B, we experiment with agglomera-
tive clustering using four different linkage criteria,
concluding that they all perform equally well and
that Phase B is almost a solved problem when the
gold similarity matrix of Phase A is used; how-
ever, further improvements are needed in the sim-
ilarity measures of Phase A to produce a suffi-
ciently good similarity matrix. We also make pub-
licly available the datasets of our experiments.
Our main contributions are: (i) to the best
of our knowledge, we are the first to consider
multi-granular aspect aggregation (not just merg-
ing near-synonyms) in ABSA without manually
crafted domain-specific ontologies; (ii) we pro-
pose a two-phase decomposition that allows previ-
ous work on term similarity and hierarchical clus-
tering to be reused and evaluated with high inter-
annotator agreement; (iii) we introduce a novel
sense pruning mechanism that improves WordNet-
based similarity measures; (iv) we provide the first
public datasets for multi-granular aspect aggrega-
tion; (v) we show that the second phase of our de-
composition is almost a solved problem, and that
research should focus on the first phase. Although
we experiment with customer reviews of products
and services, ABSA and the work of this paper in
particular are, at least in principle, also applicable
to texts expressing opinions about other kinds of
entities (e.g., politicians, organizations).
Section 2 below discusses related work. Sec-
tions 3 and 4 present our work for Phase A and B,
respectively. Section 5 concludes.
2 Related work
Most existing approaches to aspect aggregation
aim to produce a single, flat partitioning of as-
pect terms into aspect groups, rather than aspect
groups at multiple granularities. The most com-
mon approaches (Liu, 2012) are to aggregate only
synonyms or near-synonyms, using WordNet (Liu
et al., 2005), statistics from corpora (Chen et al.,
2006; Bollegala et al., 2007a; Lin and Wu, 2009),
or semi-supervised learning (Zhai et al., 2010;
Zhai et al., 2011), or to cluster the aspect terms
using (latent) topic models (Titov and McDonald,
2008a; Guo et al., 2009; Brody and Elhadad, 2010;
Jo and Oh, 2011). Topic models do not perform
better than other methods (Zhai et al., 2010), and
their clusters may overlap.
2
The topic model of
Titov et al. (2008b) uses two granularity levels;
we consider many more (3?10 levels).
Carenini et al. (2005) used a predefined domain-
specific taxonomy and similarity measures to ag-
gregate related terms. Yu et al. (2011) used a tai-
lored version of an existing taxonomy. By con-
trast, we assume no domain-specific taxonomy.
Kobayashi et al. (2007) proposed methods to ex-
tract aspect terms and relations between them, in-
cluding hierarchical relations. They extract, how-
ever, relations by looking for clues in texts (e.g.,
particular phrases). By contrast, we employ simi-
larity measures and hierarchical clustering, which
allows us to group similar aspect terms even when
they do not cooccur in texts. Also, in contrast
to Kobayashi et al. (2007), we respect the consis-
tency constraint discussed in Section 1.
A similar task is taxonomy induction. Cimi-
ano and Staab (2005) automatically construct tax-
onomies from texts via agglomerative clustering,
much as in our Phase B, but not in the context of
ABSA, and without trying to learn a similarity ma-
trix first. They also label the hierarchy?s concepts,
a task we do not consider. Klapaftis and Manand-
har (2010) show how word sense induction can be
combined with agglomerative clustering to obtain
more accurate taxonomies, again not in the con-
text of ABSA. Our sense pruning method was in-
fluenced by their work, but is much simpler than
their word sense induction. Fountain and Lapata
(2012) study unsupervised methods to induce con-
cept taxonomies, without considering ABSA.
3 Phase A
We now discuss our work for Phase A. Recall that
in this phase the input is a set of aspect terms and
2
Topic models are typically also used to perform aspect
extraction, apart from aspect aggregation, but simple heuris-
tics (e.g., most frequent nouns) often outperform them in as-
pect extraction (Liu, 2012; Moghaddam and Ester, 2012).
80
the goal is to fill in a matrix (Table 1) with scores
showing the similarity of each pair of aspect terms.
3.1 Datasets used in Phase A
We used two benchmark datasets that we had pre-
viously constructed to evaluate ABSA methods for
subjectivity detection, aspect extraction, and as-
pect score estimation, but not aspect aggregation.
We extended them to support aspect aggregation,
and we make them publicly available.
3
The two original datasets contain sentences
from customer reviews of restaurants and laptops,
respectively. The reviews are manually split into
sentences, and each sentence is manually anno-
tated as ?subjective? (expressing opinion) or ?ob-
jective? (not expressing opinion). The restaurants
dataset contains 3,710 English sentences from the
restaurant reviews of Ganu et al. (2009). The lap-
tops dataset contains 3,085 English sentences from
394 customer reviews, collected from sites that
host customer reviews. In the experiments of this
paper, we use only the 3,057 (out of 3,710) sub-
jective restaurant sentences and the 2,631 (out of
3,085) subjective laptop sentences.
For each subjective sentence, our datasets show
the words that human annotators marked as aspect
terms. For example, in ?The dessert was divine!?
the aspect term is ?dessert?, and in ?Really bad
waiter.? it is ?waiter?. Among the 3,057 subjective
restaurant sentences, 1,129 contain exactly one as-
pect term, 829 more than one, and 1,099 no aspect
term; a subjective sentence may express an opin-
ion about the restaurant (or laptop) being reviewed
without mentioning a specific aspect (e.g., ?Really
nice restaurant!?), which is why no aspect terms
are present in some subjective sentences. There
are 558 distinct multi-word aspect terms and 431
distinct single-word aspect terms in the subjective
restaurant sentences. Among the 2,631 subjective
sentences of the laptop reviews, 823 contain ex-
actly one aspect term, 389 more than one, and
1,419 no aspect term. There are 273 distinct multi-
word aspect terms and 330 distinct single-word as-
pect terms in the subjective laptop sentences.
From each dataset, we selected the 20 (distinct)
aspect terms that the human annotators had anno-
tated most frequently, taking annotation frequency
to be an indicator of importance; there are only
two multi-word aspect terms (?hard drive?, ?bat-
3
The datasets are available at http://nlp.cs.
aueb.gr/software.html.
tery life?) among the 20 most frequent ones in the
laptops dataset, and none among the 20 most fre-
quent aspect terms of the restaurants dataset. We
then formed all the 190 possible pairs of the 20
terms and constructed an empty similarity matrix
(Fig. 1), one for each dataset, which was given
to three human judges to fill in (1: strong dis-
similarity, 5: strong similarity).
4
For each aspect
term, all the subjective sentences mentioning the
term were also provided, to help the judges un-
derstand how the terms are used in the particu-
lar domains (e.g., ?window? and ?Windows? have
domain-specific meanings in laptop reviews).
The Pearson correlation coefficient indicated
high inter-annotator agreement (0.81 for restau-
rants, 0.74 for laptops). We also measured the ab-
solute inter-annotator agreement a(l
1
, l
2
), defined
below, where l
1
, l
2
are lists containing the scores
(similarity matrix values) of two judges, N is the
length of each list, and v
max
, v
min
are the largest
and smallest possible scores (5 and 1).
a(l
1
, l
2
) =
1
N
N
?
i=1
[
1?
|l
1
(i)? l
2
(i)|
v
max
? v
min
]
The absolute interannotator agreement was also
high (0.90 for restaurants, 0.91 for laptops).
5
With
both measures, we compute the agreement of each
judge with the averaged (for each matrix cell)
scores of the other two judges, and we report the
mean of the three agreement estimates. Finally, we
created the gold similarity matrix of each dataset
by placing in each cell the average scores that the
three judges had provided for that cell.
In preliminary experiments, we gave aspect
terms to human judges, asking them to group any
terms they considered near-synonyms. We then
asked the judges to group the aspect terms into
fewer, coarser groups by grouping terms that could
be viewed as direct hyponyms of the same broader
term (e.g., ?pizza? and ?steak? are both kinds of
?food?), or that stood in a hyponym-hypernym re-
lation (e.g., ?pizza? and ?food?). We used the
Dice coefficient to measure inter-annotator agree-
ment, and we obtained reasonably good agreement
for near-synonyms (0.77 for restaurants, 0.81 for
laptops), but poor agreement for the coarser as-
4
The matrix is symmetric; hence, the judges had to fill in
only half of it. The guidelines and an annotation tool that
were given to the judges are available upon request.
5
The Pearson correlation ranges from ?1 to 1, whereas
the absolute inter-annotator agreement ranges from 0 to 1.
81
pects (0.25 and 0.11).
6
In other preliminary ex-
periments, we asked human judges to rank alter-
native aspect hierarchies that had been produced
by applying agglomerative clustering with differ-
ent linkage criteria to 20 aspect terms, but we ob-
tained very poor inter-annotator agreement (Pear-
son score ?0.83 for restaurants and 0 for laptops).
3.2 Phase A methods
We employed five term similarity measures. The
first two are WordNet-based (Budanitsky and
Hirst, 2006). The next two combine WordNet with
statistics from corpora. The fifth one is a corpus-
based distributional similarity measure.
The first measure is Wu and Palmer?s (1994). It
is actually a sense similarity measure (a term may
have multiple senses). Given two senses s
ij
, s
i
?
j
?
of terms t
i
, t
i
?
, the measure is defined as follows:
WP(s
ij
, s
i
?
j
?
) = 2 ?
depth(lcs(s
ij
, s
i
?
j
?
))
depth(s
ij
) + depth(s
ij
)
,
where lcs(s
ij
, s
i
?
j
?
) is the least common sub-
sumer, i.e., the most specific common ancestor of
the two senses in WordNet, and depth(s) is the
depth of sense s in WordNet?s hierarchy.
Most terms have multiple senses, however,
and word sense disambiguation methods (Navigli,
2009) are not yet robust enough. Hence, when
given two aspect terms t
i
, t
i
?
, rather than particular
senses of the terms, a simplistic greedy approach
is to compute the similarities of all the possible
pairs of senses s
ij
, s
i
?
j
?
of t
i
, t
i
?
, and take the sim-
ilarity of t
i
, t
i
?
to be the maximum similarity of
the sense pairs (Bollegala et al., 2007b; Zesch and
Gurevych, 2010). We use this greedy approach
with all the WordNet-based measures, but we also
propose a sense pruning mechanism below, which
improves their performance. In all the WordNet-
based measures, if a term is not in WordNet, we
take its similarity to any other term to be zero.
7
The second measure, PATH (s
ij
, s
i
?
j
?
), is sim-
ply the inverse of the length (plus one) of the short-
est path connecting the senses s
ij
, s
i
?
j
?
in WordNet
(Zhang et al., 2013). Again, the greedy approach
can be used with terms having multiple senses.
6
The Dice coefficient ranges from 0 to 1. There was a very
large number of possible responses the judges could provide
and, hence, it would be inappropriate to use Cohen?s K.
7
This never happened in the restaurants dataset. In the
laptops dataset, it only happened for ?hard drive? and ?bat-
tery life?. We use the NLTK implementation of the first four
measures (see http://nltk.org/) and our own imple-
mentation of the distributional similarity measure.
The third measure is Lin?s (1998), defined as:
LIN (s
ij
, s
i
?
j
?
) =
2 ? ic(lcs(s
ij
, s
i
?
j
?
))
ic(s
ij
) + ic(s
i
?
j
?
)
,
where s
ij
, s
i
?
j
?
are senses of terms t
i
, t
i
?
,
lcs(s
ij
, s
i
?
j
?
) is the least common subsumer of
s
ij
, s
i
?
j
?
in WordNet, and ic(s) = ? logP(s) is
the information content of sense s (Pedersen et al.,
2004), estimated from a corpus. When the cor-
pus is not sense-tagged, we follow the common
approach of treating each occurrence of a word as
an occurrence of all of its senses, when estimat-
ing ic(s).
8
We experimented with two variants of
Lin?s measure, one where the ic(s) scores were
estimated from the Brown corpus (Marcus et al.,
1993), and one where they were estimated from
the (restaurant or laptop) reviews of our datasets.
The fourth measure is Jiang and Conrath?s
(1997), defined below. Again, we experimented
with two variants of ic(s), as above.
JCN (s
ij
, s
i
?
j
?
) =
1
ic(s
ij
) + ic(s
i
?
j
?
)? 2 ? lcs(s
ij
, s
i
?
j
?
)
For all the above WordNet-based measures, we
experimented with a sense pruning mechanism,
which discards some of the senses of the aspect
terms, before applying the greedy approach. For
each aspect term t
i
, we consider all of its Word-
Net senses s
ij
. For each s
ij
and each other aspect
term t
i
?
, we compute (using PATH ) the similar-
ity between s
ij
and each sense s
i
?
j
?
of t
i
?
, and we
consider the relevance of s
ij
to t
i
?
to be:
9
rel(s
ij
, t
i
?
) = max
s
i
?
j
?
? senses(t
i
?
)
PATH (s
ij
, s
i
?
j
?
)
The relevance of s
ij
to all of the N other aspect
terms t
i
?
is taken to be:
rel(s
ij
) =
1
N
?
?
i
?
6=i
rel(s
ij
, t
i
?
)
For each aspect term t
i
, we retain only its senses
s
ij
with the top rel(s
ij
) scores, which tends to
8
http://www.d.umn.edu/
?
tpederse/Data/
README-WN-IC-30.txt. We use the default counting.
9
We also experimented with other similarity measures
when computing rel(s
ij
, t
i
?
), instead of PATH , but there
was no significant difference. We use NLTK to tokenize, re-
move punctuation, and stop-words.
82
without SP with SP
Method Rest. Lapt. Rest. Lapt.
WP 0.475 0.216 0.502 0.265
PATH 0.524 0.301 0.529 0.332
LIN@domain 0.390 0.256 0.456 0.343
LIN@Brown 0.434 0.329 0.471 0.391
JCN@domain 0.467 0.348 0.509 0.448
JCN@Brown 0.403 0.469 0.419 0.539
DS 0.283 0.517 (0.283) (0.517)
AVG 0.499 0.352 0.537 0.426
WN 0.490 0.328 0.530 0.395
WNDS 0.523 0.453 0.545 0.546
Table 2: Phase A results (Pearson correlation to
gold similarities) with and without sense pruning.
prune senses that are very irrelevant to the par-
ticular domain (e.g., laptops). This sense prun-
ing mechanism is novel, and we show experimen-
tally that it improves the performance of all the
WordNet-based similarity measures we examined.
We also implemented a distributional simi-
larity measure (Harris, 1968; Pad?o and Lap-
ata, 2007; Cimiano et al., 2009; Zhang et al.,
2013). Following Lin and Wu (2009), for
each aspect term t, we create a vector ~v(t) =
?PMI (t, w
1
), . . . ,PMI (t, w
n
)?. The vector com-
ponents are the Pointwise Mutual Information
scores of t and each word w
i
of a corpus:
PMI (t, w
i
) = ? log
P (t, w
i
)
P (t) ? P (w
i
)
We treat P (t, w
i
) as the probability of t, w
i
cooc-
curring in the same sentence, and we use the (lap-
top or restaurant) reviews of our datasets as the
corpus to estimate the probabilities. The distribu-
tional similarity DS (t, t
?
) of two aspect terms t, t
?
is the cosine similarity of ~v(t), ~v(t
?
).
10
Finally, we tried combinations of the similarity
measures: AVG is the average of all five; WN is
the average of the first four, which employ Word-
Net; and WNDS is the average of WN and DS ;
all the scores range in [0, 1]. We also tried regres-
sion (e.g., SVR), but there was no improvement.
3.3 Phase A experimental results
Each similarity measure was evaluated by comput-
ing its Pearson correlation with the scores of the
gold similarity matrix. Table 2 shows the results.
Our sense pruning consistently improves all
four WordNet-based measures. It does not apply to
10
We also experimented with Euclidean distance, a nor-
malized PMI (Bouma, 2009), and the Brown corpus, but
there was no improvement.
DS , which is why the DS results are identical with
and without pruning. A paired t test indicates that
the other differences (with and without pruning) of
Table 2 are statistically significant (p < 0.05). We
used the senses with the top five rel(s
ij
) scores for
each aspect term t
i
during sense pruning. We also
experimented with keeping fewer senses, but the
results were inferior or there was no improvement.
Lin?s measure performed better when infor-
mation content was estimated on the (much
larger, but domain-independent) Brown corpus
(LIN@Brown), as opposed to using the (domain-
specific) reviews of our datasets (LIN@domain),
but we observed no similar consistent pattern for
JCN . Given its simplicity, PATH performed re-
markably well in the restaurants dataset; it was
the best measure (including combinations) without
sense pruning, and the best uncombined measure
with sense pruning. It performed worse, however,
compared to several other measures in the laptops
dataset. Similar comments apply to WP , which is
among the top-performing uncombined measures
in restaurants, both with and without sense prun-
ing, but the worst overall measure in laptops. DS
is the best overall measure in laptops when com-
pared to measures without sense pruning, and the
third best overall when compared to measures that
use sense pruning, but the worst overall in restau-
rants both with and without pruning. LIN and
JCN , which use both WordNet and corpus statis-
tics, have a more balanced performance across the
two datasets, but they are not top-performers in
any of the two. Combinations of similarity mea-
sures seem more stable across domains, as the re-
sults of AVG , WN , and WNDS indicate, though
experiments with more domains are needed to in-
vestigate this issue. WNDS is the best overall
method with sense pruning, and among the best
three methods without pruning in both datasets.
To get a better view of the performance of
WNDS with sense pruning, i.e., the best overall
measure of Table 2, we compared it to two state of
the art semantic similarity systems. First, we ap-
plied the system of Han et al. (2013), one of the
best systems of the recent *Sem 2013 semantic
text similarity competition, to our Phase A data.
The performance (Pearson correlation with gold
similarities) of the same system on the widely used
WordSim353 word similarity dataset (Agirre et al.,
2009) is 0.73, much higher than the same system?s
performance on our Phase A data (see Table 3),
83
Method Restaurants Laptops
Han et al. (2013) 0.450 0.471
Word2Vec 0.434 0.485
WNDS with SP 0.545 0.546
Judge 1 0.913 0.875
Judge 2 0.914 0.894
Judge 3 0.888 0.924
Table 3: Phase A results (Pearson correlation to
gold similarities) of WNDS with SP against se-
mantic similarity systems and human judges.
which suggests that our data are more difficult.
11
We also employed the recent Word2Vec sys-
tem, which computes continuous vector space rep-
resentations of words from large corpora and has
been reported to improve results in word similarity
tasks (Mikolov et al., 2013). We used the English
Wikipedia to compute word vectors with 200 fea-
tures.
12
The similarity between two aspect terms
was taken to be the cosine similarity of their vec-
tors. This system performed better than Han et
al.?s with laptops, but not with restaurants.
Table 3 shows that WNDS (with sense prun-
ing) performed clearly better than the system of
Han et al. and Word2Vec. Table 3 also shows
the Pearson correlation of each judge?s scores to
the gold similarity scores, as an indication of the
best achievable results. Although WNDS (with
sense pruning) performs reasonably well in both
domains,
13
there is large scope for improvement.
4 Phase B
In Phase B, the aspect terms are to be grouped
into k non-overlapping clusters, for varying val-
ues of k, given a Phase A similarity matrix. We
experimented with both the gold similarity matrix
of Phase A and similarity matrices produced by
WNDS (with SP), the best Phase A method.
4.1 Phase B methods
We experimented with agglomerative clustering
and four linkage criteria: single, complete, av-
erage, and Ward (Manning and Sch?utze, 1999;
Hastie et al., 2001). Let d(t
1
, t
2
) be the distance of
11
The system of Han et al. (2013) is available from
http://semanticwebarchive.cs.umbc.edu/
SimService/; we use the STS similarity.
12
Word2Vec is available from https://code.
google.com/p/word2vec/. We used the continuous
bag of words model with default parameters, the first billion
characters of the English Wikipedia, and the preprocessing of
http://mattmahoney.net/dc/textdata.html.
13
Recall that the Pearson correlation ranges from ?1 to 1.
two individual instances t
1
, t
2
; in our case, the in-
stances are aspect terms and d(t
1
, t
2
) is the inverse
of the similarity of t
1
, t
2
, defined by the Phase A
similarity matrix (gold or produced by WNDS ).
Different linkage criteria define differently the dis-
tance of two clusters D(C
1
, C
2
), which affects
the choice of clusters that are merged to produce
coarser (higher-level) clusters:
D
single
(C
1
, C
2
) = min
t
1
?C
1
,t
2
?C
2
d(t
1
, t
2
)
D
compl
(C
1
, C
2
) = max
t
1
?C
1
,t
2
?C
2
d(t
1
, t
2
)
D
avg
(C
1
, C
2
) =
1
|C
1
||C
2
|
?
t
1
?C
1
?
t
2
?C
2
d(t
1
, t
2
)
Complete linkage tends to produce more compact
clusters, compared to single linkage, with average
linkage being in between. Ward minimizes the to-
tal in-cluster variance; consult Milligan (1980) for
further details.
14
4.2 Phase B experimental results
To evaluate the k clusters produced at each aspect
granularity by the different linkage criteria, we
used the Silhouette Index (SI ) (Rousseeuw, 1987),
a cluster evaluation measure that considers both
inter- and intra-cluster coherence.
15
Given a set of
clusters {C
1
, . . . , C
k
}, each SI (C
i
) is defined as:
SI (C
i
) =
1
|C
i
|
?
|C
i
|
?
j=1
b
j
? a
j
max(b
j
, a
j
)
,
where a
j
is the mean distance from the j-th in-
stance of C
i
to the other instances in C
i
, and b
j
is
the mean distance from the j-th instance of C
i
to
the instances in the cluster nearest to C
i
. Then:
SI ({C
1
, . . . , C
k
}) =
1
k
?
k
?
i=1
SI (C
i
)
We always use the correct (gold) distances of the
instances (terms) when computing the SI scores.
As shown in Fig. 3, no linkage criterion clearly
outperforms the others, when the gold matrix of
Phase A is used; all four criteria perform reason-
ably well. Note that the SI ranges from ?1 to
14
We used the SCIPY implementations of agglomera-
tive clustering with the four criteria (see http://www.
scipy.org), relying on maxclust to obtain the slice of the
resulting hierarchy that leads to k (or approx. k) clusters.
15
We used the SI implementation of Pedregosa et
al. (2011); see http://scikit-learn.org/. We also
experimented with the Dunn Index (Dunn, 1974) and the
Davies-Bouldin Index (1979), but we obtained similar results.
84
(a) restaurants (b) laptops
Figure 3: Silhouette Index (SI) results for Phase
B, using the gold similarity matrix of Phase A.
(a) restaurants (b) laptops
Figure 4: SI results for Phase B, using the WNDS
(with SP) similarity matrix of Phase A.
1, with higher values indicating better clustering.
Figure 4 shows that when the similarity matrix of
WNDS (with SP) is used, the SI scores deterio-
rate significantly; again, there is no clear winner
among the linkage criteria, but average and Ward
seem to be overall better than the others.
(a) Restaurants (b) Laptops
Figure 5: Human evaluation of aspect groups.
In a final experiment, we showed clusterings
of varying granularities (k values) to four human
judges (graduate CS students). The clusterings
were produced by two systems: one that used the
gold similarity matrix of Phase A and agglomer-
ative clustering with average linkage in Phase B,
and one that used the similarity matrix of WNDS
(with SP) and again agglomerative clustering with
average linkage. We showed all the clusterings
to all the judges. Each judge was asked to eval-
uate each clustering on a 1?5 scale. We measured
the absolute inter-annotator agreement, as in Sec-
tion 3.1, and found high agreement in all cases
(0.93 and 0.83 for the two systems, respectively,
in restaurants; 0.85 for both in laptops).
16
Figure 5 shows the average human scores of
the two systems for different granularities. The
judges considered the aspect groups always per-
fect or near-perfect when the gold similarity ma-
trix of Phase A was used, but they found the as-
pect groups to be of rather poor quality when
the similarity matrix of the best Phase A mea-
sure was used. These results, along with those of
Fig. 3?4, show that more effort needs to be devoted
to improving the similarity measures of Phase A,
whereas Phase B is in effect an almost solved
problem, if a good similarity matrix is available.
5 Conclusions
We considered a new, more demanding form of
aspect aggregation in ABSA, which aims to aggre-
gate aspects at multiple granularities, as opposed
to simply merging near-synonyms, and without as-
suming that manually crafted domain-specific on-
tologies are available. We decomposed the prob-
lem in two processing phases, which allow pre-
vious work on term similarity and hierarchical
clustering to be reused and evaluated appropri-
ately with high inter-annotator agreement. We
showed that the second phase, where we used ag-
glomerative clustering, is an almost solved prob-
lem, whereas further research is needed in the first
phrase, where term similarity measures are em-
ployed. We also introduced a sense pruning mech-
anism that significantly improves WordNet-based
similarity measures, leading to a measure that out-
performs state of the art similarity methods in the
first phase of our decomposition. We also made
publicly available the datasets of our experiments.
Acknowledgments
We thank G. Batistatos, A. Zosakis, and G. Lam-
pouras for their annotations in Phase A. We thank
A. Kosmopoulos, G. Lampouras, P. Malakasiotis,
and I. Lourentzou for their annotations in Phase B.
16
The Pearson correlation cannot be computed, as several
judges gave the same rating to the first system, for all k.
85
References
E. Agirre, E. Alfonseca, K. Hall, J. Kravalova,
M. Pas?ca, and A. Soroa. 2009. A study on similar-
ity and relatedness using distributional and wordnet-
based approaches. In Proceedings of the Annual
Conference of NAACL, pages 19?27, Boulder, CO,
USA.
D. Bollegala, Y. Matsuo, and M. Ishizuka. 2007a.
An integrated approach to measuring semantic sim-
ilarity between words using information available
on the web. In Proceedings of HLT-NAACL, pages
340?347, Rochester, NY, USA.
D. Bollegala, Y. Matsuo, and M. Ishizuka. 2007b.
Measuring semantic similarity between words using
web search engines. In Proceedings of the 16th In-
ternational Conference of WWW, volume 766, pages
757?766, Banff, Alberta, Canada.
G. Bouma. 2009. Normalized (pointwise) mutual in-
formation in collocation extraction. Proceedings of
the Biennial Conference of GSCL, pages 31?40.
S. Brody and N. Elhadad. 2010. An unsupervised
aspect-sentiment model for online reviews. In Pro-
ceedings of the Annual Conference of NAACL, pages
804?812, Los Angeles, CA, USA.
A. Budanitsky and G. Hirst. 2006. Evaluating
WordNet-based measures of lexical semantic relat-
edness. Computational Linguistics, 32(1):13?47.
G. Carenini, R. T. Ng, and E. Zwart. 2005. Extract-
ing knowledge from evaluative text. In Proceedings
of the 3rd International Conference on Knowledge
Capture, pages 11?18, Banff, Alberta, Canada.
H. Chen, M. Lin, and Y. Wei. 2006. Novel association
measures using web search with double checking.
In Proceedings of the 21st International Conference
of COLING and the 44th Annual Meeting of ACL,
pages 1009?1016, Sydney, Australia.
P. Cimiano and S. Staab. 2005. Learning concept hier-
archies from text with a guided hierarchical cluster-
ing algorithm. In Proceedings of ICML ? Workshop
on Learning and Extending Lexical Ontologies with
Machine Learning Methods, Bonn, Germany.
P. Cimiano, A. M?adche, S. Staab, and J. V?olker. 2009.
Ontology learning. In Handbook on Ontologies,
pages 245?267. Springer.
D. L. Davies and D. W. Bouldin. 1979. A cluster sepa-
ration measure. IEEE Transactions on Pattern Anal-
ysis and Machine Intelligence, 1(2):224?227.
J. C. Dunn. 1974. Well-separated clusters and optimal
fuzzy partitions. Journal of Cybernetics, 4(1):95?
104.
T. Fountain and M. Lapata. 2012. Taxonomy induction
using hierarchical random graphs. In Proceedings of
NAACL:HLT, pages 466?476, Montreal, Canada.
G. Ganu, N. Elhadad, and A. Marian. 2009. Beyond
the stars: Improving rating predictions using review
text content. In Proceedings of the 12th Interna-
tional Workshop on the Web and Databases, Prov-
idence, RI, USA.
H. Guo, H. Zhu, Z. Guo, X. Zhang, and Z. Su. 2009.
Product feature categorization with multilevel latent
semantic association. In Proceedings of the 18th
CIKM, pages 1087?1096.
L. Han, A. Kashyap, T. Finin, J. Mayfield, and
J. Weese. 2013. Umbc ebiquity-core: Semantic tex-
tual similarity systems. In Proceedings of the 2nd
Joint Conference on Lexical and Computational Se-
mantics, pages 44?52, Atlanta, GA, USA.
Z. Harris. 1968. Mathematical Structures of Lan-
guage. Wiley.
T. Hastie, R. Tibshirani, and J. Friedman. 2001. The
Elements of Statistical Learning. Springer.
J. J. Jiang and D. W. Conrath. 1997. Semantic similar-
ity based on corpus statistics and lexical taxonomy.
In Proceedings of ROCLING, pages 19?33, Taiwan,
China.
Y. Jo and A. H. Oh. 2011. Aspect and sentiment unifi-
cation model for online review analysis. In Proceed-
ings of the 4th International Conference of WSDM,
pages 815?824, Hong Kong, China.
I. P. Klapaftis and S. Manandhar. 2010. Taxonomy
learning using word sense induction. In Proceedings
of NAACL, pages 82?90, Los Angeles, CA, USA.
N. Kobayashi, K. Inui, and Y. Matsumoto. 2007. Ex-
tracting aspect-evaluation and aspect-of relations in
opinion mining. In Proceedings of the Joint Confer-
ence on EMNLP-CoNLL, pages 1065?1074, Prague,
Czech Republic.
D. Lin and X. Wu. 2009. Phrase clustering for dis-
criminative learning. In Proceedings of ACL, pages
1030?1038, Suntec, Singapore. ACL.
D. Lin. 1998. An information-theoretic definition of
similarity. In Proceedings of the 15th ICML, pages
296?304, Madison, WI, USA.
B. Liu, M. Hu, and J. Cheng. 2005. Opinion observer:
analyzing and comparing opinions on the web. In
Proceedings of the 14th International Conference of
WWW, pages 342?351, Chiba, Japan.
B. Liu. 2012. Sentiment Analysis and Opinion Mining.
Synthesis Lectures on Human Language Technolo-
gies. Morgan & Claypool.
C. D. Manning and H. Sch?utze. 1999. Foundations
of Statistical Natural Language Processing. MIT
Press, Cambridge, MA, USA.
M. P. Marcus, M. A. Marcinkiewicz, and B. Santorini.
1993. Building a large annotated corpus of en-
glish: The penn treebank. Computational Linguis-
tics, 19(2):313?330.
86
T. Mikolov, C. Kai, G. Corrado, and J. Dean. 2013.
Efficient estimation of word representations in vec-
tor space. CoRR, abs/1301.3781.
G.W. Milligan. 1980. An examination of the effect of
six types of error perturbation on fifteen clustering
algorithms. Psychometrika, 45(3):325?342.
S. Moghaddam and M. Ester. 2012. On the design of
lda models for aspect-based opinion mining. In Pro-
ceedings of the 21st CIKM, pages 803?812, Maui,
HI, USA.
R. Navigli. 2009. Word sense disambiguation: A sur-
vey. ACM Computing Surveys, 41(2):10:1?10:69.
S. Pad?o and M. Lapata. 2007. Dependency-based con-
struction of semantic space models. Computational
Linguistics, 33(2):161?199.
T. Pedersen, S. Patwardhan, and J. Michelizzi. 2004.
Wordnet::similarity: measuring the relatedness of
concepts. In Proceedings of NAACL:HTL ? Demon-
strations, pages 38?41, Boston, MA, USA.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-
sos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. 2011. Scikit-learn: Machine learn-
ing in python. Journal of Machine Learning Re-
search, 12:2825?2830.
P. Rousseeuw. 1987. Silhouettes: a graphical aid to
the interpretation and validation of cluster analysis.
Journal of Computational and Applied Mathemat-
ics, 20(1):53?65.
I. Titov and R. T. McDonald. 2008a. A joint model of
text and aspect ratings for sentiment summarization.
In Proceedings of the 46th Annual Meeting of ACL-
HLT, pages 308?316, Columbus, OH, USA.
I. Titov and R. T. McDonald. 2008b. Modeling online
reviews with multi-grain topic models. In Proceed-
ings of the 17th International Conference of WWW,
pages 111?120, Beijing, China.
Z. Wu and M. Palmer. 1994. Verbs semantics and lexi-
cal selection. In Proceedings of the 32nd ACL, pages
133?138, Las Cruces, NM, USA.
J. Yu, Z. Zha, M. Wang, K. Wang, and T. Chua. 2011.
Domain-assisted product aspect hierarchy genera-
tion: towards hierarchical organization of unstruc-
tured consumer reviews. In Proceedings of EMNLP,
pages 140?150, Edinburgh, UK.
T. Zesch and I. Gurevych. 2010. Wisdom of crowds
versus wisdom of linguists - measuring the semantic
relatedness of words. Natural Language Engineer-
ing, 16(1):25?59.
Z. Zhai, B. Liu, H. Xu, and P. Jia. 2010. Group-
ing product features using semi-supervised learning
with soft-constraints. In Proceedings of the 23rd
International Conference of COLING, pages 1272?
1280, Beijing, China.
Z. Zhai, B. Liu, H. Xu, and P. Jia. 2011. Clustering
product features for opinion mining. In Proceedings
of the 4th International Conference of WSDM, pages
347?354, Hong Kong, China.
Z. Zhang, A. Gentile, and F. Ciravegna. 2013. Re-
cent advances in methods of lexical semantic relat-
edness - a survey. Natural Language Engineering,
FirstView(1):1?69.
87
Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 33?37,
Gothenburg, Sweden, April 26-30 2014.
c?2014 Association for Computational Linguistics
A Vague Sense Classifier for Detecting Vague Definitions in Ontologies
Panos Alexopoulos
iSOCO S.A.
Madrid, Spain
palexopoulos@isoco.com
John Pavlopoulos
Department of Informatics,
Athens University of Economics and Business
Athens, Greece
annis@aueb.gr
Abstract
Vagueness is a common human knowl-
edge and linguistic phenomenon, typi-
cally manifested by predicates that lack
clear applicability conditions and bound-
aries such as High, Expert or Bad. In the
context of ontologies and semantic data,
the usage of such predicates within ontol-
ogy element definitions (classes, relations
etc.) can hamper the latter?s quality, pri-
marily in terms of shareability and mean-
ing explicitness. With that in mind, we
present in this paper a vague word sense
classifier that may help both ontology cre-
ators and consumers to automatically de-
tect vague ontology definitions and, thus,
assess their quality better.
1 Introduction
Vagueness is a common human knowledge and
language phenomenon, typically manifested by
terms and concepts like High, Expert, Bad, Near
etc., and related to our inability to precisely de-
termine the extensions of such concepts in certain
domains and contexts. That is because vague con-
cepts have typically blurred boundaries which do
not allow for a sharp distinction between the enti-
ties that fall within their extension and those that
do not (Hyde, 2008) (Shapiro, 2006). For exam-
ple, some people are borderline tall: not clearly
?tall? and not clearly ?not tall?.
Ontologies, in turn, are formal shareable con-
ceptualizations of domains, describing the mean-
ing of domain aspects in a common, machine-
processable form by means of concepts and
their interrelations (Chandrasekaran et al., January
February 1999). As such, they are widely used
for the production and sharing of structured data
and knowledge that can be commonly understood
among human and software agents.
When building ontologies and semantic data,
engineers and domain experts often use predi-
cates that are vague. While this is not always
an intentional act, the use of such predicates in-
fluences in a negative way the comprehension of
this data by other parties and limits their value as
a reusable source of knowledge (Alexopoulos et
al., 2013). The reason is the subjective interpreta-
tion of vague definitions that can cause disagree-
ments among the people who develop, maintain or
use a vague ontology. In fact, as shown in (Alex-
opoulos et al., 2013), vagueness in ontologies can
be a source of problems in scenarios involving i)
structuring data with a vague ontology (where dis-
agreements among experts on the validity of vague
statements may occur), ii) utilizing vague facts in
ontology-based systems (where reasoning results
might not meet users? expectations) and iii) in-
tegrating vague semantic information (where the
merging of particular vague elements can lead to
data that will not be valid for all its users).
In this context, our goal in this paper is to en-
able ontology producers (engineers and domain
experts) as well as consumers (i.e., practitioners
who want to reuse ontologies and semantic data) to
detect, in an automatic way, ontology element def-
initions that are potentially vague. Such a detec-
tion will help ontology creators build more com-
prehensible and shareable ontologies (by refining,
eliminating or just documenting vague definitions)
and consumers assess, in an easier way, their us-
ability and quality before deciding to use it.
Our approach towards such a detection involves
training a classifier that may distinguish between
vague and non-vague term word senses and using
it to determine whether a given ontology element
definition is vague or not. For example, the def-
inition of the ontology class ?StrategicClient? as
?A client that has a high value for the company?
is (and should be) characterized as vague while
the definition of ?AmericanCompany? as ?A com-
33
pany that has legal status in the Unites States? is
not. The classifier is trained in a supervised way,
using vague and non-vague sense examples, care-
fully constructed from WordNet.
The structure of the rest of the paper is as fol-
lows. In the next section we briefly present related
work while in section 3 we describe in detail our
vague sense classifier, including the training data
we used and the evaluation we performed. Sec-
tion 4 describes the results of applying the classi-
fier in an a publicly available ontology, illustrating
its usefulness as an ontology evaluation tool. Fi-
nally, section 5 summarizes our work and outlines
its future directions.
2 Related Work
The phenomenon of vagueness in human language
and knowledge has been studied from a logic
and philosophical point of view in a number of
works (Hyde, 2008) (Shapiro, 2006) and differ-
ent theories and paradigms have been proposed
to accommodate it, including supervaluationism
(Keefe, 2008), many-valued logic and fuzzy logic
(Klir and Yuan, 1995). Moreover, in the context
of ontologies, one may find several works focus-
ing on acquisition, conceptualization and repre-
sentation of vague knowledge, mainly following a
fuzzy logic based approach (Bobillo and Straccia,
2011) (Stoilos et al., 2008) (Abulaish, 2009). Nev-
ertheless all these approaches rely on manual iden-
tification and analysis of vague terms and concepts
by domain experts and, to the best of our knowl-
edge, no work attempts to automate this task.
Another set of related work consists of ap-
proaches for subjectivity and polarity labeling of
word senses (Wiebe and Riloff, 2005) (Wiebe
and Mihalcea, 2006) (Wilson et al., 2005) (Su
and Markert, 2008) (Esuli and Sebastiani, 2006)
(Akkaya et al., 2011). While vagueness is related
to both phenomena (as polarized words are often
vague and vague words are typically subjective),
it is not exactly the same as these (e.g., subjective
statements do not always involve vagueness) and,
thus, requires specialized treatment. To illustrate
that, we compare in subsequent sections our vague
sense classifier with the subjective sense classifier
of (Wilson et al., 2005), showing that the former
performs significantly better than the latter.
3 Supervised Classification for Vague
Term Detection
3.1 Data
We created a dataset of 2,000 adjective senses, col-
lected from WordNet, such that 1,000 of them
had a vague definition and the the rest a non vague
definition. A sample of these senses is shown in
Table 1 while the whole dataset, which to the best
of our knowledge is the first of its kind, is publicly
available for further research
1
.
The dataset was constructed by an ontology ex-
pert. As the task of classifying a text as vague or
not can be quite subjective, we asked from two
other human judges to annotate a subset of the
dataset?s definitions (100), and we measured inter-
annotator agreement between all three. We found
mean pairwise JPA (Joint Probability of Agree-
ment) equal to 0.81 and mean pairwise K (Co-
hen?s Kappa) equal to 0.64, both of which indicate
a reasonable agreement.
Figure 1: Train and test error rate, per number of
training instances.
3.2 Training and Evaluation
We used the first 80% of the data (i.e., 800 vague
and 800 non vague instances) to train a multino-
mial Naive Bayes classifier.
2
We removed stop
words and we used the bag of words assumption
to represent each instance.
3
The remaining 20%
of the data (i.e., 200 vague and 200 non vague
instances) was used as a test set. Accuracy was
found to be 84%, which is considerably high. In
Figure 1, is shown the error rate on the test and
train data, as we increase the number of training
instances. We see that the two curves, initially,
1
http://glocal.isoco.net/datasets/VagueSynsets.zip
2
We used the implementation of Scikit-Learn found at
http://scikit-learn.org/stable/.
3
We used the list of stopwords provided by Scikit-Learn.
34
Vague Adjectives Non Vague Adjectives
Abnormal: not normal, not typical or usual or
regular or conforming to a norm
Compound: composed of more than one part
Impenitent: impervious to moral persuasion Biweekly: occurring every two weeks
Notorious: known widely and usually unfavor-
ably
Irregular: falling below the manufacturer?s
standard
Aroused: emotionally aroused Outermost: situated at the farthest possible
point from a center
Yellowish: of the color intermediate between
green and orange in the color spectrum, of
something resembling the color of an egg yolk
Unfeathered: having no feathers
Table 1: Sample Vague and Non-Vague Adjective Senses
Figure 2: Accuracy on the test data, per number of
selected features.
have a big gap between them, but this is progres-
sively reduced. However, more (or more compli-
cated) features could be beneficial; we intend to
study this further in the future.
We also examined the hypothesis of the exis-
tence of a small set of words that are often found
in vague definitions, but not in definitions which
are not vague, as then it would be very easy for
a system to use these words and discriminate be-
tween the two classes. To do this, we performed
feature selection with the chi-squared statistic for
various number of features and computed the ac-
curacy (i.e., one minus the error rate). As we show
in Figure 2, accuracy for only 5 selected features
is 50%, which is the same as if we selected class
in random. However, by increasing the number of
selected features, accuracy increases significantly.
This shows that there is not a subset of words
which could be used to discriminate between the
two classes; by contrast, most of the words play
their role. Again, this is something to be further
studied in future research.
Finally, in order to verify our intuition that
vagueness is not the same phenomenon as subjec-
tiveness (as we suggested in section 2), we used
the subjective sense classifier of (Wilson et al.,
2005) to classify the data of section 3.1 as subjec-
tive or objective, assuming that vague senses are
subjective while non-vague ones objective. The
particular classifier is part of the OpinionFinder
4
system and the results of its application in the 2000
adjective senses of our dataset were as follows.
From the 1000 vague senses, only 167 were classi-
fied as subjective while from the 1000 non-vague
ones 993. These numbers do no reflect of course
the quality of OpinionFinder as a subjectivity de-
tection system, they merely illustrate the fact that
treating vagueness in the same way as subjective-
ness is not really effective and, thus, more dedi-
cated, vagueness-specific work is needed.
4 Use Case: Detecting Vagueness in
CiTO Ontology
To evaluate the effectiveness and potential of our
classifier for detecting vague ontological defini-
tions, we considered a publicly available ontology
called CiTO
5
. CiTO is an ontology that enables
characterization of the nature or type of citations
and consists primarily of relations, many of which
are vague (e.g. the relation cito:plagiarizes). In
order to compare the experts? vague/non-vague
classification with the output of our system, we
worked as follows. We selected 44 relations from
CiTO (making sure to avoid duplications by e.g.
avoiding having both a relation and its inverse) and
we had again 3 human judges manually classify
them as vague or not. In the end we got 27 vague
4
http://mpqa.cs.pitt.edu/opinionfinder/
5
http://purl.org/spar/cito/
35
Vague Relations Non Vague Relations
plagiarizes: A property indicating that the au-
thor of the citing entity plagiarizes the cited
entity, by including textual or other elements
from the cited entity without formal acknowl-
edgement of their source.
sharesAuthorInstitutionWith: Each entity has
at least one author that shares a common insti-
tutional affiliation with an author of the other
entity.
citesAsAuthority: The citing entity cites the
cited entity as one that provides an authorita-
tive description or definition of the subject un-
der discussion.
providesDataFor: The cited entity presents
data that are used in work described in the cit-
ing entity.
speculatesOn: The citing entity speculates on
something within or related to the cited entity,
without firm evidence.
retracts: The citing entity constitutes a formal
retraction of the cited entity.
supports: The citing entity provides intellec-
tual or factual support for statements, ideas or
conclusions presented in the cited entity.
includesExcerptFrom: The citing entity in-
cludes one or more excerpts from the cited en-
tity.
refutes: The citing entity refutes statements,
ideas or conclusions presented in the cited en-
tity.
citesAsSourceDocument: The citing entity
cites the cited entity as being the entity from
which the citing entity is derived, or about
which the citing entity contains metadata.
Table 2: Sample Vague and Non-Vague Relations in CiTO
relations and 17 non-vague, a sample of which is
shown in Table 2.
Then we applied the trained vagueness classifier
of the previous section on the textual definitions of
the relations. The results of this were highly en-
couraging; 36/44 (82%) relations were correctly
classified as vague/non-vague with 74% accuracy
for vague relations and 94% for non-vague ones.
Again, for completeness, we classified the same
relations with OpinionFinder (as in the previous
section), in order to check if subjectivity classifi-
cation is applicable for vagueness. The results of
this were consistent to the ones reported in the pre-
vious section with the Wordnet data: 18/44 (40%)
overall correctly classified relations with 94% ac-
curacy for non-vague relations but only 7% for
vague ones.
5 Conclusions and Future Work
In this paper we considered the problem of auto-
matically detecting vague definitions in ontologies
and we developed a vague word sense classifier
using training data from Wordnet. Experiments
with both Wordnet word senses and real ontol-
ogy definitions, showed a considerably high accu-
racy of our system, thus verifying our intuition that
vague and non-vague senses can be separable. We
do understand that vagueness is a quite complex
phenomenon and the approach we have followed
in this paper rather simple. Yet, exactly because
of its simplicity, we believe that it can be a very
good baseline for further research in this particu-
lar area. The vague/non-vague sense dataset we
provide will be also very useful for that purpose.
Our future work comprises two main directions.
On the one hand, as we mentioned in the introduc-
tion, we intend to incorporate the current classifier
into an ontology analysis tool that will help ontol-
ogy engineers and users detect vague definitions in
ontologies and thus assess their quality better. On
the other hand, we want to further study the phe-
nomenon of vagueness as manifested in textual in-
formation, improve our classifer and see whether
it is possible to build a vague sense lexicon, similar
to lexicons that have already been built for subjec-
tivity and sentiment analysis.
Acknowledgments
The research leading to this results has received
funding from the People Programme (Marie Curie
Actions) of the European Union?s 7th Frame-
work Programme P7/2007-2013 under REA grant
agreement n
o
286348.
36
References
M. Abulaish. 2009. An ontology enhancement frame-
work to accommodate imprecise concepts and rela-
tions. Journal of Emerging Technologies in Web In-
telligence, 1(1).
C. Akkaya, J. Wiebe, A. Conrad, and R. Mihal-
cea. 2011. Improving the impact of subjectivity
word sense disambiguation on contextual opinion
analysis. In Proceedings of the Fifteenth Confer-
ence on Computational Natural Language Learning,
CoNLL ?11, pages 87?96, Stroudsburg, PA, USA.
Association for Computational Linguistics.
P. Alexopoulos, B. Villazon-Terrazas, and Pan J.Z. Pan.
2013. Towards vagueness-aware semantic data. In
Fernando Bobillo, Rommel N. Carvalho, Paulo Ce-
sar G. da Costa, Claudia d?Amato, Nicola Fanizzi,
Kathryn B. Laskey, Kenneth J. Laskey, Thomas
Lukasiewicz, Trevor Martin, Matthias Nickles, and
Michael Pool, editors, URSW, volume 1073 of
CEURWorkshop Proceedings, pages 40?45. CEUR-
WS.org.
F. Bobillo and U. Straccia. 2011. Fuzzy ontology rep-
resentation using owl 2. International Journal of
Approximate Reasoning, 52(7):1073?1094, October.
B. Chandrasekaran, J. Josephson, and R. Benjamins.
January - February 1999. What are ontologies and
why do we need them? IEEE Intelligent Systems,
14(1):Page 20?26.
A. Esuli and F. Sebastiani. 2006. Sentiwordnet: A
publicly available lexical resource for opinion min-
ing. In In Proceedings of the 5th Conference on Lan-
guage Resources and Evaluation (LREC06, pages
417?422.
D. Hyde. 2008. Vagueness, Logic and Ontology. Ash-
gate New Critical Thinking in Philosophy.
R. Keefe. 2008. Vagueness: Supervaluationism. Phi-
losophy Compass, 3:315?324.
G. Klir and B. Yuan. 1995. Fuzzy Sets and Fuzzy
Logic, Theory and Applications. Prentice Hall.
S. Shapiro. 2006. Vagueness in Context. Oxford Uni-
versity Press.
G. Stoilos, G. Stamou, J.Z. Pan, N. Simou, and
V. Tzouvaras. 2008. Reasoning with the Fuzzy De-
scription Logic f-SHIN: Theory, Practice and Appli-
cations. pages 262?281.
F. Su and K. Markert. 2008. From words to senses: A
case study of subjectivity recognition. In Proceed-
ings of the 22Nd International Conference on Com-
putational Linguistics - Volume 1, COLING ?08,
pages 825?832, Stroudsburg, PA, USA. Association
for Computational Linguistics.
J. Wiebe and R. Mihalcea. 2006. Word sense and sub-
jectivity. In Proceedings of COLING-ACL 2006.
J. Wiebe and E. Riloff. 2005. Creating subjective
and objective sentence classifiers from unannotated
texts. In In CICLing2005, pages 486?497.
T. Wilson, P. Hoffmann, S. Somasundaran, J. Kessler,
J. Wiebe, Y. Choi, C. Cardie, E. Riloff, and
S. Patwardhan. 2005. Opinionfinder: A sys-
tem for subjectivity analysis. In Proceedings of
HLT/EMNLP on Interactive Demonstrations, HLT-
Demo ?05, pages 34?35, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
37
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 562?567, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
nlp.cs.aueb.gr: Two Stage Sentiment Analysis
Prodromos Malakasiotis, Rafael Michael Karampatsis
Konstantina Makrynioti and John Pavlopoulos
Department of Informatics
Athens University of Economics and Business
Patission 76, GR-104 34 Athens, Greece
Abstract
This paper describes the systems with which
we participated in the task Sentiment Analysis
in Twitter of SEMEVAL 2013 and specifically
the Message Polarity Classification. We used
a 2-stage pipeline approach employing a lin-
ear SVM classifier at each stage and several
features including BOW features, POS based
features and lexicon based features. We have
also experimented with Naive Bayes classi-
fiers trained with BOW features.
1 Introduction
During the last years, Twitter has become a very
popular microblogging service. Millions of users
publish messages every day, often expressing their
feelings or opinion about a variety of events, top-
ics, products, etc. Analysing this kind of content
has drawn the attention of many companies and re-
searchers, as it can lead to useful information for
fields, such as personalized marketing or social pro-
filing. The informal language, the spelling mis-
takes, the slang and special abbreviations that are
frequently used in tweets differentiate them from
traditional texts, such as articles or reviews, and
present new challenges for the task of sentiment
analysis.
The Message Polarity Classification is defined as
the task of deciding whether a message M conveys a
positive, negative or neutral sentiment. For instance
M1 below expresses a positive sentiment, M2 a neg-
ative one, while M3 has no sentiment at all.
M1: GREAT GAME GIRLS!! On to districts Monday
at Fox!! Thanks to the fans for coming out :)
M2: Firework just came on my tv and I just broke down
and sat and cried, I need help okay
M3: Going to a bulls game with Aaliyah & hope next
Thursday
As sentiment analysis in Twitter is a very recent
subject, it is certain that more research and improve-
ments are needed. This paper presents our approach
for the subtask of Message Polarity Classification
(Wilson et al, 2013) of SEMEVAL 2013. We used a
2-stage pipeline approach employing a linear SVM
classifier at each stage and several features includ-
ing bag of words (BOW) features, part-of-speech
(POS) based features and lexicon based features.
We have also experimented with Naive Bayes clas-
sifiers trained with BOW features.
The rest of the paper is organised as follows. Sec-
tion 2 provides a short analysis of the data used
while section 3 describes our approach. Section 4
describes the experiments we performed and the cor-
responding results and section 5 concludes and gives
hints for future work.
2 Data
Before we proceed with our system description we
briefly describe the data released by the organisers.
The training set consists of a set of IDs correspond-
ing to tweet messages, along with their annotations.
A message can be annotated as positive, negative
or neutral. In order to address privacy concerns,
rather than releasing the original Tweets, the organ-
isers chose to provide a python script for download-
ing the data. This resulted to different training sets
for the participants since tweets may often become
562
SEMEVAL STATS TRAIN (ours) TRAIN (official) Dev DEV (final) TEST (sms)
Positive 3280 37,57% 3640 37,59% 524 34,82% 575 34,76% 492 23,50%
Negative 1289 14,77% 1458 15,06% 308 20,47% 340 20,56% 394 18,82%
Neutral 4161 47,66% 4586 47,36% 673 44,72% 739 44,68% 1208 57,69%
TOTAL 8730 9684 1505 1654 2094
37,57% 
14,77% 
47,66% 
Training data class distribution 
Positive
Negative
Neutral
34,76% 
20,56% 
44,68% 
Development data class distribution 
Positive
Negative
Neutral
23,50% 
18,82% 57,69% 
Test data class distribution (sms) 
Positive
Negative
Neutral
41,23% 
15,76% 
43,01% 
Test data class distribution (twitter) 
Positive
Negative
Neutral
(a)
SEMEVAL STATS TRAIN (ours) TRAIN (official) Dev DEV (final) TEST (sms)
Positive 3280 37,57% 3640 37,59% 524 34,82% 575 34,76% 492 23,50%
Negative 1289 14,7 % 1458 15,06% 308 20,47% 340 20,56% 394 18, 2%
Neutral 4161 47,6 % 4586 47,36% 673 4 ,72% 739 4 ,68% 1208 57,69%
TOTAL 8730 9684 1505 1654 2094
37,57% 
14,7 % 
47,6 % 
Training data clas  distribution 
Positive
Negative
Neutral
34,76% 
20,56% 
4 ,68% 
Development data clas  distribution 
Posit ve
Negative
Neutral
23,50% 
18,82% 57,69% 
Test data class distribution (sms) 
Positive
Negative
Neutral
41,23% 
15,76% 
43,01% 
Test data clas  distribution (twit er) 
Posit ve
Negative
Neutral
(b)
Figure 1: Train and Development data class distribution.
unavailable due to a number of reasons. Concerning
the development and test sets the organisers down-
loaded and provided the tweets. 1 A first analysis
of the data indicates that they suffer from a class im-
balance problem. Specifically the training data we
have downloaded contain 8730 tweets (3280 posi-
tive, 1289 negative, 4161 neutral), while the devel-
opment set contains 1654 tweets (575 positive, 340
negative, 739 neutral). Figure 1 illustrates the prob-
lem on train and development sets.
3 System Overview
The system we propose is a 2?stage pipeline pro-
cedure employing SVM classifiers (Vapnik, 1998)
to detect whether each message M expresses pos-
itive, negative or no sentiment (figure 2). Specifi-
cally, during the first stage we attempt to detect if M
expresses a sentiment (positive or negative) or not.
If so, M is called ?subjective?, otherwise it is called
?objective? or ?neutral?.2 Each subjective message
is then classified in a second stage as ?positive? or
?negative?. Such a 2?stage approach has also been
suggested in (Pang and Lee, 2004) to improve sen-
timent classification of reviews by discarding objec-
tive sentences, in (Wilson et al, 2005a) for phrase-
level sentiment analysis, and in (Barbosa and Feng,
2010) for sentiment analysis on Twitter messages.
1A separate test set with SMS messages was also provided
by the organisers to measure performance of systems over other
types of message data. No training and development data were
provided for this set.
2Hereafter we will use the terms ?objective? and ?neutral?
interchangeably.
3.1 Data Preprocessing
Before we could proceed with feature engineering,
we performed several preprocessing steps. To be
more precise, a twitter specific tokeniser and part-
of-speech (POS) tagger (Ritter et al, 2011) were
used to obtain the tokens and the corresponding
POS tags which are necessary for a particular set
of features to be described later. In addition to these,
six lexicons, originating from Wilson?s (2005b) lexi-
con, were created. This lexicon contains expressions
that given a context (i.e., surrounding words) indi-
cate subjectivity. The expression that in most con-
text expresses sentiment is considered to be ?strong?
subjective, otherwise it is considered weak subjec-
tive (i.e., it has specific subjective usages). So, we
first split the lexicon in two smaller, one contain-
ing strong and one containing weak subjective ex-
pressions. Moreover, Wilson also reports the polar-
ity of each expression out of context (prior polarity)
which can be positive, negative or neutral. As a con-
sequence, we further split each of the two lexicons
into three smaller according to the prior polarity of
the expression, resulting to the following six lexi-
cons:
S+ : Contains strong subjective expressions with
positive prior polarity.
S? : Contains strong subjective expressions with
negative prior polarity.
S0 : Contains strong subjective expressions with
neutral prior polarity.
563
 Subjectivity detection 
SVM 
Polarity detection 
SVM 
Objective 
messages 
Messages 
Subjective 
messages 
Positive 
messages 
Negative 
messages 
Figure 2: Our 2?stage pipeline procedure.
W+ : Contains weak subjective expressions with
positive prior polarity.
W? : Contains weak subjective expressions with
negative prior polarity.
W0 : Contains weak subjective expressions with
neutral prior polarity.
Adding to these, three more lexicons were created,
one for each class (positive, negative, neutral). In
particular, we employed Chi Squared feature selec-
tion (Liu and Setiono, 1995) to obtain the 100 most
important tokens per class from the training set.
Very few tokens were manually erased to result to
the following three lexicons.
T+ : Contains the top-94 tokens appearing in posi-
tive tweets of the training set.
T? : Contains the top-96 tokens appearing in nega-
tive tweets of the training set.
T0 : Contains the top-94 tokens appearing in neutral
tweets of the training set.
The nine lexicons described above are used to cal-
culate precision (P (t, c)), recall (R(t, c)) and F ?
measure (F1(t, c)) of tokens appearing in a mes-
sage with respect to each class. Equations 1, 2 and 3
below provide the definitions of these metrics.
P (t, c) =
#tweets that contain token t and belong to class c
#tweets that contain token t
(1)
R(t, c) =
#tweets that contain token t and belong to class c
#tweets that belong to class c
(2)
F1(t, c) =
2 ? P (t, c) ? R(t, c)
P (t, c) + R(t, c)
(3)
3.2 Feature engineering
We employed three types of features, namely
boolean features, POS based features and lexicon
based features. Our goal is to build a system that is
not explicitly based on the vocabulary of the training
set, having therefore better generalisation capability.
3.2.1 Boolean features
Bag of words (BOW): These features indicate the
existence of specific tokens in a message. We
used feature selection with Info Gain to obtain
the 600 most informative tokens of the training
set and we then manually removed 19 of them
564
to result in 581 tokens. As a consequence we
get 581 features that can take a value of 1 if a
message contains the corresponding token and
0 otherwise.
Time and date: We observed that time and date of-
ten indicated events in the train data and such
messages tend to be objective. Therefore, we
added two more features to indicate if a mes-
sage contains time and/or date expressions.
Character repetition: Repetitive characters are of-
ten added to words by users, in order to give
emphasis or to express themselves more in-
tensely. As a consequence they indicate sub-
jectivity. So we added one more feature having
a value of 1 if a message contains words with
repeating characters and 0 otherwise.
Negation: Negation not only is a good subjectivity
indicator but it also may change the polarity of
a message. We therefore add 5 more features,
one indicating the existence of negation, and
the remaining four indicating the existence of
negation that precedes (in a distance of at most
5 tokens) words from lexicons S+, S?, W+ and
W?.
Hash-tags with sentiment: These features are im-
plemented by getting all the possible sub-
strings of the string after the symbol # and
checking if any of them match with any word
from S+, S?, W+ and W? (4 features). A
value of 1 means that a hash-tag containing a
word from the corresponding lexicon exists in
a message.
3.2.2 POS based features
Specific POS tags might be good indicators of
subjectivity or objectivity. For instance adjectives
often express sentiment (e.g., beautiful, frustrating)
while proper nouns are often reported in objective
messaged. We, therefore, added 10 more features
based on the following POS tags:
1. adjectives,
2. adverbs,
3. verbs,
4. nouns,
5. proper nouns,
6. urls,
7. interjections,
8. hash-tags,
9. happy emoticons, and
10. sad emoticons.
We then constructed our features as follows. For
each message we counted the occurrences of tokens
with these POS tags and we divided this number
with the number of tokens having any of these POS
tags. For instance if a message contains 2 adjectives,
1 adverb and 1 url then the features corresponding to
adjectives, adverbs and urls will have a value of 24 ,
1
4
and 14 respectively while all the remaining features
will be 0. These features can be thought of as a way
to express how much specific POS tags affect the
sentiment of a message.
Going a step further we calculate precision
(P (b, c)), recall (R(b, c)) and F ? measure
(F1(b, c)) of POS tags bigrams with respect to each
class (equations 4, 5 and 6 respectively).
P (b, c) =
#tweets that contain bigram b and belong to class c
#tweets that contain bigram b
(4)
R(b, c) =
#tweets that contain bigram b and belong to class c
#tweets that belong to class c
(5)
F1(b, c) =
2 ? P (b, c) ? R(b, c)
P (b, c) + R(b, c)
(6)
For each bigram (e.g., adjective-noun) in a mes-
sage we calculate F1(b, c) and then we use the aver-
age, the maximum and the minimum of these values
to create 9 additional features. We did not experi-
ment over measures that weight differently Precision
and Recall (e.g., Fb for b = 0.5) or with different
combinations (e.g., F1 and P ).
3.2.3 Lexicon based features
This set of features associates the words of the
lexicons described earlier with the three classes.
Given a message M , similarly to the equations 4 and
565
6 above, we calculate P (t, c) and F1(t, c) for every
token t ? M with respect to a lexicon. We then ob-
tain the maximum, minimum and average values of
P (t, c) and F1(t, c) in M . We note that the combi-
nation of P and F1 appeared to be the best in our
experiments while R(t, c) was not helpful and thus
was not used. Also, similarly to section 3.2.2 we
did not experiment over measures that weight differ-
ently Precision and Recall (e.g., Fb for b = 0.5). The
former metrics are calculated with three variations:
(a) Using words: The values of the metrics con-
sider only the words of the message.
(b) Using words and priors: The same as (a) but
adding to the calculated metrics a prior value.
This value is calculated on the entire lexicon,
and roughly speaking it is an indicator of how
much we can trust L to predict class c. In cases
that a token t of a message M does not appear
in a lexicon L the corresponding scores of the
metrics will be 0.
(c) Using words and their POS tags: The values
of the metrics consider the words of the message
along with their POS tags.
(d) Using words, their POS tags and priors: The
same as (c) but adding to the calculated metrics
an apriori value. The apriori value is calculated
in a similar manner as in (b) with the difference
that we consider the POS tags of the words as
well.
For case (a) we calculated minimum, maximum
and average values of P (t, c) and F1(t, c) with re-
spect to S+, S?, S0, W+, W? and W0 consider-
ing only the words of the message resulting to 108
features. Concerning case (b) we calculated average
P (t, c) and F1(t, c) with respect to S+, S?, S0, W+,
W? and W0, and average P (t, c) with respect to T+,
T? and T0 adding 45 more features. For case (c) we
calculated minimum, maximum and average P (t, c)
with respect to S+, S?, S0, W+, W? and W0 (54
features), and, finally, for case (d) we calculated av-
erage P (t, c) and F1(t, c) with respect to S+, S?,
S0, W+, W? and W0 to add 36 features.
Class F1
Positive 0.6496
Negative 0.4429
Neutral 0.7022
Average 0.5462
Table 1: F1 for development set.
4 Experiments
As stated earlier we use a 2?stage pipeline approach
to identify the sentiment of a message. Preliminary
experiments on the development data showed that
this approach is better than attempting to address the
problem in one stage during which a classifier must
classify a message as positive, negative or neutral.
To be more precise we used a Naive Bayes classifier
and BOW features using both 1?stage and 2?stage
approaches. Although we considered the 2?stage
approach with a Naive Bayes classifier as a baseline
system we used it to submit results for both twitter
and sms test sets.
Having concluded to the 2?stage approach we
employed for each stage an SVM classifier, fed with
the 855 features described in section 3.2.3 Both
SVMs use linear kernel and are tuned in order to
find the optimum C parameter. Observe that we use
the same set of features in both stages and let the
classifier learn the appropriate weights for each fea-
ture. During the first stage, the classifier is trained
on the entire training set after merging positive and
negative classes to one superclass, namely subjec-
tive. In the second stage, the classifier is trained only
on positive and negative tweets of the training and
is asked to determine whether the messages classi-
fied as subjective during the first stage are positive
or negative.
4.1 Results
In order to obtain the best set of features we trained
our system on the downloaded training data and
measured its performance on the provided develop-
ment data. Table 1 illustrates the F1 results on the
development set. A first observation is that there
is a considerable difference between the F1 of the
negative class and the other two, with the former be-
3We used the LIBLINEAR distribution (Fan et al, 2008)
566
Class F1
Positive 0.6854
Negative 0.4929
Neutral 0.7117
Average 0.5891
Table 2: F1 for twitter test set.
Class F1
Positive 0.6349
Negative 0.5131
Neutral 0.7785
Average 0.5740
Table 3: F1 for sms test set.
ing significantly decreased. This might be due to
the quite low number of negative tweets of the ini-
tial training set in comparison with the rest of the
classes. Therefore, the addition of 340 negative ex-
amples from the development set emerged from this
imbalance and proved to be effective as shown in ta-
ble 2 illustrating our results on the test set regarding
tweets. Unfortunately we were not able to submit
results with this system for the sms test set. How-
ever, we performed post-experiments after the gold
sms test set was released. The results shown on table
3 are similar to the ones obtained for the twitter test
set which means that our model has a good general-
isation ability.
5 Conclusion and future work
In this paper we presented our approach for the
Message Polarity Classification task of SEMEVAL
2013. We proposed a pipeline approach to detect
sentiment in two stages; first we discard objective
messages and then we classify subjective (i.e., car-
rying sentiment) ones as positive or negative. We
used SVMs with various extracted features for both
stages and although the system performed reason-
ably well, there is still much room for improvement.
A first problem that should be addressed is the dif-
ficulty in identifying negative messages. This was
mainly due to small number of tweets in the train-
ing data. This was somewhat alleviated by adding
the negative instances of the development data but
still our system reports lower results for this class as
compared to positive and neutral classes. More data
or better features is a possible improvement. An-
other issue that has not an obvious answer is how to
proceed in order to improve the 2?stage pipeline ap-
proach. Should we try and optimise each stage sepa-
rately or should we optimise the second stage taking
into consideration the results of the first stage?
References
Luciano Barbosa and Junlan Feng. 2010. Robust senti-
ment detection on twitter from biased and noisy data.
In Proceedings of the 23rd International Conference
on Computational Linguistics: Posters, COLING ?10,
pages 36?44, Beijing, China. Association for Compu-
tational Linguistics.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. Liblinear: A library
for large linear classification. The Journal of Machine
Learning Research, 9:1871?1874.
Huan Liu and Rudy Setiono. 1995. Chi2: Feature se-
lection and discretization of numeric attributes. In
Tools with Artificial Intelligence, 1995. Proceedings.,
Seventh International Conference on, pages 388?391.
IEEE.
Bo Pang and Lillian Lee. 2004. A sentimental edu-
cation: sentiment analysis using subjectivity summa-
rization based on minimum cuts. In Proceedings of
the 42nd Annual Meeting on Association for Compu-
tational Linguistics, ACL ?04, Barcelona, Spain. As-
sociation for Computational Linguistics.
Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.
2011. Named entity recognition in tweets: An experi-
mental study. In EMNLP, pages 1524?1534.
V. Vapnik. 1998. Statistical learning theory. John Wiley.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005a. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of the confer-
ence on Human Language Technology and Empirical
Methods in Natural Language Processing, HLT ?05,
pages 347?354, Vancouver, British Columbia, Canada.
Association for Computational Linguistics.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005b. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of the con-
ference on Human Language Technology and Empir-
ical Methods in Natural Language Processing, pages
347?354. Association for Computational Linguistics.
Theresa Wilson, Zornitsa Kozareva, Preslav Nakov, Sara
Rosenthal, Veselin Stoyanov, and Alan Ritter. 2013.
SemEval-2013 task 2: Sentiment analysis in twitter.
In Proceedings of the International Workshop on Se-
mantic Evaluation, SemEval ?13, June.
567
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 27?35,
Dublin, Ireland, August 23-24, 2014.
SemEval-2014 Task 4: Aspect Based Sentiment Analysis
Maria Pontiki
Institute for Language
and Speech Processing,
?Athena? Research Center
mpontiki@ilsp.gr
Haris Papageorgiou
Institute for Language
and Speech Processing,
?Athena? Research Center
xaris@ilsp.gr
Dimitrios Galanis
Institute for Language
and Speech Processing,
?Athena? Research Center
galanisd@ilsp.gr
Ion Androutsopoulos
Dept. of Informatics
Athens University of
Economics and Business
ion@aueb.gr
John Pavlopoulos
Dept. of Informatics,
Athens University of
Economics and Business
annis@aueb.gr
Suresh Manandhar
Dept. of Computer Science,
University of York
suresh@cs.york.ac.uk
Abstract
Sentiment analysis is increasingly viewed
as a vital task both from an academic and
a commercial standpoint. The majority of
current approaches, however, attempt to
detect the overall polarity of a sentence,
paragraph, or text span, irrespective of the
entities mentioned (e.g., laptops) and their
aspects (e.g., battery, screen). SemEval-
2014 Task 4 aimed to foster research in the
field of aspect-based sentiment analysis,
where the goal is to identify the aspects
of given target entities and the sentiment
expressed for each aspect. The task pro-
vided datasets containing manually anno-
tated reviews of restaurants and laptops, as
well as a common evaluation procedure. It
attracted 163 submissions from 32 teams.
1 Introduction
With the proliferation of user-generated content on
the web, interest in mining sentiment and opinions
in text has grown rapidly, both in academia and
business. Early work in sentiment analysis mainly
aimed to detect the overall polarity (e.g., positive
or negative) of a given text or text span (Pang et
al., 2002; Turney, 2002). However, the need for a
more fine-grained approach, such as aspect-based
(or ?feature-based?) sentiment analysis (ABSA),
soon became apparent (Liu, 2012). For example,
laptop reviews not only express the overall senti-
ment about a specific model (e.g., ?This is a great
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
laptop?), but also sentiments relating to its spe-
cific aspects, such as the hardware, software, price,
etc. Subsequently, a review may convey opposing
sentiments (e.g., ?Its performance is ideal, I wish
I could say the same about the price?) or objective
information (e.g., ?This one still has the CD slot?)
for different aspects of an entity.
ABSA is critical in mining and summarizing
opinions from on-line reviews (Gamon et al.,
2005; Titov and McDonald, 2008; Hu and Liu,
2004a; Popescu and Etzioni, 2005). In this set-
ting, ABSA aims to identify the aspects of the en-
tities being reviewed and to determine the senti-
ment the reviewers express for each aspect. Within
the last decade, several ABSA systems of this kind
have been developed for movie reviews (Thet et
al., 2010), customer reviews of electronic products
like digital cameras (Hu and Liu, 2004a) or net-
book computers (Brody and Elhadad, 2010), ser-
vices (Long et al., 2010), and restaurants (Ganu et
al., 2009; Brody and Elhadad, 2010).
Previous publicly available ABSA benchmark
datasets adopt different annotation schemes within
different tasks. The restaurant reviews dataset of
Ganu et al. (2009) uses six coarse-grained aspects
(e.g., FOOD, PRICE, SERVICE) and four overall
sentence polarity labels (positive, negative, con-
flict, neutral). Each sentence is assigned one or
more aspects together with a polarity label for
each aspect; for example, ?The restaurant was ex-
pensive, but the menu was great.? would be as-
signed the aspect PRICE with negative polarity and
FOOD with positive polarity. In the product re-
views dataset of Hu and Liu (2004a; 2004b), as-
pect terms, i.e., terms naming aspects (e.g., ?ra-
dio?, ?voice dialing?) together with strength scores
(e.g., ?radio?: +2, ?voice dialing?: ?3) are pro-
27
vided. No predefined inventory of aspects is pro-
vided, unlike the dataset of Ganu et al.
The SemEval-2014 ABSA Task is based on lap-
top and restaurant reviews and consists of four
subtasks (see Section 2). Participants were free to
participate in a subset of subtasks and the domains
(laptops or restaurants) of their choice.
2 Task Description
For the first two subtasks (SB1, SB2), datasets on
both domains (restaurants, laptops) were provided.
For the last two subtasks (SB3, SB4), datasets only
for the restaurant reviews were provided.
Aspect term extraction (SB1): Given a set of
review sentences, the task is to identify all as-
pect terms present in each sentence (e.g., ?wine?,
?waiter?, ?appetizer?, ?price?, ?food?). We require
all the aspect terms to be identified, including as-
pect terms for which no sentiment is expressed
(neutral polarity). These will be useful for con-
structing an ontology of aspect terms and to iden-
tify frequently discussed aspects.
Aspect term polarity (SB2): In this subtask,
we assume that the aspect terms are given (as de-
scribed in SB1) and the task is to determine the po-
larity of each aspect term (positive, negative, con-
flict, or neutral). The conflict label applies when
both positive and negative sentiment is expressed
about an aspect term (e.g., ?Certainly not the best
sushi in New York, however, it is always fresh?).
An alternative would have been to tag the aspect
term in these cases with the dominant polarity, but
this in turn would be difficult to agree on.
Aspect category detection (SB3): Given a
predefined set of aspect categories (e.g., PRICE,
FOOD) and a set of review sentences (but without
any annotations of aspect terms and their polari-
ties), the task is to identify the aspect categories
discussed in each sentence. Aspect categories are
typically coarser than the aspect terms as defined
in SB1, and they do not necessarily occur as terms
in the sentences. For example, in ?Delicious but
expensive?, the aspect categories FOOD and PRICE
are not instantiated through specific aspect terms,
but are only inferred through the adjectives ?deli-
cious? and ?expensive?. SB1 and SB3 were treated
as separate subtasks, thus no information linking
aspect terms to aspect categories was provided.
Aspect category polarity (SB4): For this sub-
task, aspect categories for each review sentence
are provided. The goal is to determine the polar-
ity (positive, negative, conflict, or neutral) of each
aspect category discussed in each sentence.
Subtasks SB1 and SB2 are useful in cases where
no predefined inventory of aspect categories is
available. In these cases, frequently discussed as-
pect terms of the entity can be identified together
with their overall sentiment polarities. We hope to
include an additional aspect term aggregation sub-
task in future (Pavlopoulos and Androutsopoulos,
2014b) to cluster near-synonymous (e.g., ?money?,
?price?, ?cost?) or related aspect terms (e.g., ?de-
sign?, ?color?, ?feeling?) together with their aver-
aged sentiment scores as shown in Fig. 1.
Figure 1: Aggregated aspect terms and average
sentiment polarities for a target entity.
Subtasks SB3 and SB4 are useful when a pre-
defined inventory of (coarse) aspect categories is
available. A table like the one of Fig. 1 can then
also be generated, but this time using the most
frequent aspect categories to label the rows, with
stars showing the proportion of reviews express-
ing positive vs. negative opinions for each aspect
category.
3 Datasets
3.1 Data Collection
The training and test data sizes are provided in Ta-
ble 1. The restaurants training data, consisting of
3041 English sentences, is a subset of the dataset
from Ganu et al. (2009), which included annota-
tions for coarse aspect categories (as in SB3) and
overall sentence polarities. We added annotations
for aspect terms occurring in the sentences (SB1),
aspect term polarities (SB2), and aspect category
polarities (SB4). Additional restaurant reviews
were collected and annotated (from scratch) in
the same manner and used as test data (800 sen-
tences). The laptops dataset contains 3845 English
28
sentences extracted from laptop custumer reviews.
Human annotators tagged the aspect terms (SB1)
and their polarities (SB2); 3045 sentences were
used for training and 800 for testing (evaluation).
Domain Train Test Total
Restaurants 3041 800 3841
Laptops 3045 800 3845
Total 6086 1600 7686
Table 1: Sizes (sentences) of the datasets.
3.2 Annotation Process
For a given target entity (a restaurant or a lap-
top) being reviewed, the annotators were asked to
provide two types of information: aspect terms
(SB1) and aspect term polarities (SB2). For the
restaurants dataset, two additional annotation lay-
ers were added: aspect category (SB3) and aspect
category polarity (SB4).
The annotators used BRAT (Stenetorp et al.,
2012), a web-based annotation tool, which was
configured appropriately for the needs of the
ABSA task.
1
Figure 2 shows an annotated sen-
tence in BRAT, as viewed by the annotators.
Stage 1: Aspect terms and polarities. During
a first annotation stage, the annotators tagged all
the single or multiword terms that named par-
ticular aspects of the target entity (e.g., ?I liked
the service and the staff, but not the food? ?
{?service?, ?staff?, ?food?}, ?The hard disk is very
noisy?? {?hard disk?}). They were asked to tag
only aspect terms explicitly naming particular as-
pects (e.g., ?everything about it? or ?it?s expen-
sive? do not name particular aspects). The as-
pect terms were annotated as they appeared, even
if misspelled (e.g., ?warrenty? instead of ?war-
ranty?). Each identified aspect term also had to be
assigned a polarity label (positive, negative, neu-
tral, conflict). For example, ?I hated their fajitas,
but their salads were great? ? {?fajitas?: nega-
tive, ?salads?: positive}, ?The hard disk is very
noisy?? {?hard disk?: negative}.
Each sentence of the two datasets was anno-
tated by two annotators, a graduate student (an-
notator A) and an expert linguist (annotator B).
Initially, two subsets of sentences (300 from each
dataset) were tagged by annotator A and the anno-
tations were inspected and validated by annotator
1
Consult http://brat.nlplab.org/ for more in-
formation about BRAT.
B. The disagreements between the two annotators
were confined to borderline cases. Taking into ac-
count the types of these disagreements (discussed
below), annotator A was provided with additional
guidelines and tagged the remainder of the sen-
tences in both datasets.
2
When A was not confi-
dent, a decision was made collaboratively with B.
When A and B disagreed, a decision was made
collaboratively by them and a third expert annota-
tor. Most disagreements fall into one of the fol-
lowing three types:
Polarity ambiguity: In several sentences, it was
unclear if the reviewer expressed positive or neg-
ative opinion, or no opinion at all (just reporting
a fact), due to lack of context. For example, in
?12.44 seconds boot time? it is unclear if the re-
viewer expresses a positive, negative, or no opin-
ion about the aspect term ?boot time?. In future
challenges, it would be better to allow the annota-
tors (and the participating systems) to consider the
entire review instead of each sentence in isolation.
Multi-word aspect term boundaries: In sev-
eral cases, the annotators disagreed on the exact
boundaries of multi-word aspect terms when they
appeared in conjunctions or disjunctions (e.g.,
?selection of meats and seafoods?, ?noodle and
rices dishes?, ?school or office use?). In such
cases, we asked the annotators to tag as a sin-
gle aspect term the maximal noun phrase (the en-
tire conjunction or disjunction). Other disagree-
ments concerned the extent of the aspect terms
when adjectives that may or may not have a sub-
jective meaning were also present. For example,
if ?large? in ?large whole shrimp? is part of the
dish name, then the guidelines require the adjec-
tive to be included in the aspect term; otherwise
(e.g., in ?large portions?) ?large? is a subjectivity
indicator not to be included in the aspect term. De-
spite the guidelines, in some cases it was difficult
to isolate and tag the exact aspect term, because of
intervening words, punctuation, or long-term de-
pendencies.
Aspect term vs. reference to target entity: In
some cases, it was unclear if a noun or noun phrase
was used as the aspect term or if it referred to the
entity being reviewed as whole. In ?This place
is awesome?, for example, ?place? most probably
refers to the restaurant as a whole (hence, it should
not be tagged as an aspect term), but in ?Cozy
2
The guidelines are available at: http://alt.qcri.
org/semeval2014/task4/data/uploads/.
29
Figure 2: A sentence in the BRAT tool, annotated with four aspect terms (?appetizers?, ?salads?, ?steak?,
?pasta?) and one aspect category (FOOD). For aspect categories, the whole sentence is tagged.
place and good pizza? it probably refers to the am-
bience of the restaurant. A broader context would
again help in some of these cases.
We note that laptop reviews often evaluate each
laptop as a whole, rather than expressing opinions
about particular aspects. Furthermore, when they
express opinions about particular aspects, they of-
ten do so by using adjectives that refer implicitly
to aspects (e.g., ?expensive?, ?heavy?), rather than
using explicit aspect terms (e.g., ?cost?, ?weight?);
the annotators were instructed to tag only explicit
aspect terms, not adjectives implicitly referring to
aspects. By contrast, restaurant reviews contain
many more aspect terms (Table 2, last column).
3
Dataset Pos. Neg. Con. Neu. Tot.
LPT-TR 987 866 45 460 2358
LPT-TE 341 128 16 169 654
RST-TR 2164 805 91 633 3693
RST-TE 728 196 14 196 1134
Table 2: Aspect terms and their polarities per do-
main. LPT and RST indicate laptop and restau-
rant reviews, respectively. TR and TE indicate the
training and test set.
Another difference between the two datasets
is that the neutral class is much more frequent
in (the aspect terms of) laptops, since laptop re-
views often mention features without expressing
any (clear) sentiment (e.g., ?the latest version does
not have a disc drive?). Nevertheless, the positive
class is the majority in both datasets, but it is much
more frequent in restaurants (Table 2). The ma-
jority of the aspect terms are single-words in both
datasets (2148 in laptops, 4827 in restaurants, out
of 3012 and 4827 total aspect terms, respectively).
Stage 2: Aspect categories and polarities. In
this task, each sentence needs to be tagged with
the aspect categories discussed in the sentence.
The aspect categories are FOOD, SERVICE, PRICE,
AMBIENCE (the atmosphere and environment of
3
We count aspect term occurrences, not distinct terms.
a restaurant), and ANECDOTES/MISCELLANEOUS
(sentences not belonging in any of the previous
aspect categories).
4
For example, ?The restau-
rant was expensive, but the menu was great? is
assigned the aspect categories PRICE and FOOD.
Additionally, a polarity (positive, negative, con-
flict, neutral) for each aspect category should be
provided (e.g., ?The restaurant was expensive, but
the menu was great?? {PRICE: negative, FOOD:
positive}.
One annotator validated the existing aspect cat-
egory annotations of the corpus of Ganu et al.
(2009). The agreement with the existing anno-
tations was 92% measured as average F
1
. Most
disagreements concerned additions of missing as-
pect category annotations. Furthermore, the same
annotator validated and corrected (if needed) the
existing polarity labels per aspect category anno-
tation. The agreement for the polarity labels was
87% in terms of accuracy and it was measured
only on the common aspect category annotations.
The additional 800 sentences (not present in Ganu
et al.?s dataset) were used for testing and were an-
notated from scratch in the same manner. The dis-
tribution of the polarity classes per category is pre-
sented in Table 3. Again, ?positive? is the majority
polarity class while the dominant aspect category
is FOOD in both the training and test restaurant
sentences.
Determining the aspect categories of the sen-
tences and their polarities (Stage 2) was an easier
task compared to detecting aspect terms and their
polarities (Stage 1). The annotators needed less
time in Stage 2 and it was easier to reach agree-
ment. Exceptions were some sentences where it
was difficult to decide if the categories AMBIENCE
or ANECDOTES/MISCELLANEOUS applied (e.g.,
?One of my Fav spots in the city?). We instructed
the annotators to classify those sentences only in
ANECDOTES/MISCELLANEOUS, if they conveyed
4
In the original dataset of Ganu et al. (2009), ANECDOTES
and MISCELLANEOUS were separate categories, but in prac-
tice they were difficult to distinguish and we merged them.
30
Positive Negative Conflict Neutral Total
Category Train Test Train Test Train Test Train Test Train Test
FOOD 867 302 209 69 66 16 90 31 1232 418
PRICE 179 51 115 28 17 3 10 1 321 83
SERVICE 324 101 218 63 35 5 20 3 597 172
AMBIENCE 263 76 98 21 47 13 23 8 431 118
ANECD./MISC. 546 127 199 41 30 15 357 51 1132 234
Total 2179 657 839 159 163 52 500 94 3713 1025
Table 3: Aspect categories distribution per sentiment class.
general views about a restaurant, without explic-
itly referring to its atmosphere or environment.
3.3 Format and Availability of the Datasets
The datasets of the ABSA task were provided in
an XML format (see Fig. 3). They are avail-
able with a non commercial, no redistribution li-
cense through META-SHARE, a repository de-
voted to the sharing and dissemination of language
resources (Piperidis, 2012).
5
4 Evaluation Measures and Baselines
The evaluation of the ABSA task ran in two
phases. In Phase A, the participants were asked
to return the aspect terms (SB1) and aspect cate-
gories (SB3) for the provided test datasets. Subse-
quently, in Phase B, the participants were given
the gold aspect terms and aspect categories (as
in Fig. 3) for the sentences of Phase A and they
were asked to return the polarities of the aspect
terms (SB2) and the polarities of the aspect cate-
gories of each sentence (SB4).
6
Each participat-
ing team was allowed to submit up to two runs
per subtask and domain (restaurants, laptops) in
each phase; one constrained (C), where only the
provided training data and other resources (e.g.,
publicly available lexica) excluding additional an-
notated sentences could be used, and one uncon-
strained (U), where additional data of any kind
could be used for training. In the latter case, the
teams had to report the resources they used.
To evaluate aspect term extraction (SB1) and as-
pect category detection (SB3) in Phase A, we used
5
The datasets can be downloaded from http://
metashare.ilsp.gr:8080/. META-SHARE (http:
//www.meta-share.org/) was implemented in the
framework of the META-NET Network of Excellence
(http://www.meta-net.eu/).
6
Phase A ran from 9:00 GMT, March 24 to 21:00 GMT,
March 25, 2014. Phase B ran from 9:00 GMT, March 27 to
17:00 GMT, March 29, 2014.
the F
1
measure, defined as usually:
F
1
=
2 ? P ?R
P + R
(1)
where precision (P ) and recall (R) are defined as:
P =
|S ?G|
|S|
, R =
|S ?G|
|G|
(2)
Here S is the set of aspect term or aspect category
annotations (in SB1 and SB3, respectively) that a
system returned for all the test sentences (of a do-
main), and G is the set of the gold (correct) aspect
term or aspect category annotations.
To evaluate aspect term polarity (SB2) and as-
pect category polarity (SB4) detection in Phase B,
we calculated the accuracy of each system, defined
as the number of correctly predicted aspect term
or aspect category polarity labels, respectively, di-
vided by the total number of aspect term or aspect
category annotations. Recall that we used the gold
aspect term and category annotations in Phase B.
We provided four baselines, one per subtask:
7
Aspect term extraction (SB1) baseline: A se-
quence of tokens is tagged as an aspect term in
a test sentence (of a domain), if it is listed in a
dictionary that contains all the aspect terms of the
training sentences (of the same domain).
Aspect term polarity (SB2) baseline: For each
aspect term t in a test sentence s (of a particu-
lar domain), this baseline checks if t had been
encountered in the training sentences (of the do-
main). If so, it retrieves the k most similar to s
training sentences (of the domain), and assigns to
the aspect term t the most frequent polarity it had
in the k sentences. Otherwise, if t had not been en-
countered in the training sentences, it is assigned
the most frequent aspect term polarity label of the
7
Implementations of the baselines and further information
about the baselines are available at: http://alt.qcri.
org/semeval2014/task4/data/uploads/.
31
<sentence id="11351725#582163#9">
<text>Our waiter was friendly and it is a shame that he didnt have a supportive
staff to work with.</text>
<aspectTerms>
<aspectTerm term="waiter" polarity="positive" from="4" to="10"/>
<aspectTerm term="staff" polarity="negative" from="74" to="79"/>
</aspectTerms>
<aspectCategories>
<aspectCategory category="service" polarity="conflict"/>
</aspectCategories>
</sentence>
Figure 3: An XML snippet that corresponds to the annotated sentence of Fig. 2.
training set. The similarity between two sentences
is measured as the Dice coefficient of the sets of
(distinct) words of the two sentences. For exam-
ple, the similarity between ?this is a demo? and
?that is yet another demo? is
2?2
4+5
= 0.44.
Aspect category extraction (SB3) baseline: For
every test sentence s, the k most similar to s train-
ing sentences are retrieved (as in the SB2 base-
line). Then, s is assigned the m most frequent as-
pect category labels of the k retrieved sentences;
m is the most frequent number of aspect category
labels per sentence among the k sentences.
Aspect category polarity (SB4): This baseline
assigns to each aspect category c of a test sentence
s the most frequent polarity label that c had in the
k most similar to s training sentences (of the same
domain), considering only training sentences that
have the aspect category label c. Sentence similar-
ity is computed as in the SB2 baseline.
For subtasks SB2 and SB4, we also use a major-
ity baseline that assigns the most frequent polarity
(in the training data) to all the aspect terms and as-
pect categories. The scores of all the baselines and
systems are presented in Tables 4?6.
5 Evaluation Results
The ABSA task attracted 32 teams in total and 165
submissions (systems), 76 for phase A and 89 for
phase B. Based on the human-annotation experi-
ence, the expectations were that systems would
perform better in Phase B (SB3, SB4, involving
aspect categories) than in Phase A (SB1, SB2, in-
volving aspect terms). The evaluation results con-
firmed our expectations (Tables 4?6).
5.1 Results of Phase A
The aspect term extraction subtask (SB1) attracted
24 teams for the laptops dataset and 24 teams for
the restaurants dataset; consult Table 4.
Laptops Restaurants
Team F
1
Team F
1
IHS RD. 74.55? DLIREC 84.01*
DLIREC 73.78* XRCE 83.98
DLIREC 70.4 NRC-Can. 80.18
NRC-Can. 68.56 UNITOR 80.09
UNITOR 67.95* UNITOR 79.96*
XRCE 67.24 IHS RD. 79.62?
SAP RI 66.6 UWB 79.35*
IITP 66.55 SeemGo 78.61
UNITOR 66.08 DLIREC 78.34
SeemGo 65.99 ECNU 78.24
ECNU 65.88 SAP RI 77.88
SNAP 62.4 UWB 76.23
DMIS 60.59 IITP 74.94
UWB 60.39 DMIS 72.73
JU CSE. 59.37 JU CSE. 72.34
lsis lif 56.97 Blinov 71.21*
USF 52.58 lsis lif 71.09
Blinov 52.07* USF 70.69
UFAL 48.98 EBDG 69.28*
UBham 47.49 UBham 68.63*
UBham 47.26* UBham 68.51
SINAI 45.28 SINAI 65.41
EBDG 41.52* V3 60.43*
V3 36.62* UFAL 58.88
COMMIT. 25.19 COMMIT. 54.38
NILCUSP 25.19 NILCUSP 49.04
iTac 23.92 SNAP 46.46
iTac 38.29
Baseline 35.64 Baseline 47.15
Table 4: Results for aspect term extraction (SB1).
Stars indicate unconstrained systems. The ? indi-
cates a constrained system that was not trained on
the in-domain training dataset (unlike the rest of
the constrained systems), but on the union of the
two training datasets (laptops, restaurants).
32
Restaurants Restaurants
Team F
1
Team Acc.
NRC-Can. 88.57 NRC-Can. 82.92
UNITOR 85.26* XRCE 78.14
XRCE 82.28 UNITOR 76.29*
UWB 81.55* SAP RI 75.6
UWB 81.04 SeemGo 74.63
UNITOR 80.76 SA-UZH 73.07
SAP RI 79.04 UNITOR 73.07
SNAP 78.22 UWB 72.78
Blinov 75.27* UWB 72.78*
UBham 74.79* lsis lif 72.09
UBham 74.24 UBham 71.9
EBDG 73.98* EBDG 69.75
SeemGo 73.75 SNAP 69.56
SINAI 73.67 COMMIT. 67.7
JU CSE. 70.46 Blinov 65.65*
lsis lif 68.27 Ualberta. 65.46
ECNU 67.29 JU CSE. 64.09
UFAL 64.51 ECNU 63.41
V3 60.20* UFAL 63.21
COMMIT. 59.3 iTac 62.73*
iTac 56.95 ECNU 60.39*
SINAI 60.29
V3 47.21
Baseline 65.65
Baseline 63.89 Majority 64.09
Table 5: Results for aspect category detection
(SB3) and aspect category polarity (SB4). Stars
indicate unconstrained systems.
Overall, the systems achieved significantly
higher scores (+10%) in the restaurants domain,
compared to laptops. The best F
1
score (74.55%)
for laptops was achieved by the IHS RD. team,
which relied on Conditional Random Fields (CRF)
with features extracted using named entity recog-
nition, POS tagging, parsing, and semantic anal-
ysis. The IHS RD. team used additional reviews
from Amazon and Epinions (without annotated
terms) to learn the sentiment orientation of words
and they trained their CRF on the union of the
restaurant and laptop training data that we pro-
vided; the same trained CRF classifier was then
used in both domains.
The second system, the unconstrained system of
DLIREC, also uses a CRF, along with POS and
dependency tree based features. It also uses fea-
tures derived from the aspect terms of the train-
ing data and clusters created from additional re-
views from YELP and Amazon. In the restaurants
domain, the unconstrained system of DLIREC
ranked first with an F
1
of 84.01%, but the best
unconstrained system, that of XRCE, was very
close (83.98%). The XRCE system relies on a
parser to extract syntactic/semantic dependencies
(e.g., ?dissapointed???food?). For aspect term ex-
traction, the parser?s vocabulary was enriched with
the aspect terms of the training data and a term
list extracted from Wikipedia and Wordnet. A set
of grammar rules was also added to detect multi-
word terms and associate them with the corre-
sponding aspect category (e.g., FOOD, PRICE).
The aspect category extraction subtask (SB3)
attracted 18 teams. As shown in Table 5, the best
score was achieved by the system of NRC-Canada
(88.57%), which relied on five binary (one-vs-all)
SVMs, one for each aspect category. The SVMs
used features based on various types of n-grams
(e.g., stemmed) and information from a lexicon
learnt from YELP data, which associates aspect
terms with aspect categories. The latter lexicon
significantly improved F
1
. The constrained UN-
ITOR system uses five SVMs with bag-of-words
(BoW) features, which in the unconstrained sub-
mission are generalized using distributional vec-
tors learnt from Opinosis and TripAdvisor data.
Similarly, UWB uses a binary MaxEnt classifier
for each aspect category with BoW and TF-IDF
features. The unconstrained submission of UWB
also uses word clusters learnt using various meth-
ods (e.g., LDA); additional features indicate which
clusters the words of the sentence being classi-
fied come from. XRCE uses information identi-
fied by its syntactic parser as well as BoW features
to train a logistic regression model that assigns to
the sentence probabilities of belonging to each as-
pect category. A probability threshold, tuned on
the training data, is then used to determine which
categories will be assigned to the sentence.
5.2 Results of Phase B
The aspect term polarity detection subtask (SB2)
attracted 26 teams for the laptops dataset and 26
teams for the restaurants dataset. DCU and NRC-
Canada had the best systems in both domains (Ta-
ble 6). Their scores on the laptops dataset were
identical (70.48%). On the laptops dataset, the
DCU system performed slightly better (80.95%
vs. 80.15%). For SB2, both NRC-Canada and
DCU relied on an SVM classifier with features
33
mainly based on n-grams, parse trees, and sev-
eral out-of-domain, publicly available sentiment
lexica (e.g., MPQA, SentiWordnet and Bing Liu?s
Opinion Lexicon). NRC-Canada also used two
automatically compiled polarity lexica for restau-
rants and laptops, obtained from YELP and Ama-
zon data, respectively. Furthermore, NRC-Canada
showed by ablation experiments that the most use-
ful features are those derived from the sentiment
lexica. On the other hand, DCU used only publicly
available lexica, which were manually adapted by
filtering words that do not express sentiment in
laptop and restaurant reviews (e.g., ?really?) and
by adding others that were missing and do express
sentiment (e.g., ?mouthwatering?).
The aspect category polarity detection subtask
(SB4) attracted 20 teams. NRC-Canada again had
the best score (82.92%) using an SVM classifier.
The same feature set as in SB2 was used, but it
was further enriched to capture information re-
lated to each specific aspect category. The second
team, XRCE, used information from its syntactic
parser, BoW features, and an out-of-domain senti-
ment lexicon to train an SVM model that predicts
the polarity of each given aspect category.
6 Conclusions and Future Work
We provided an overview of Task 4 of SemEval-
2014. The task aimed to foster research in aspect-
based sentiment analysis (ABSA). We constructed
and released ABSA benchmark datasets contain-
ing manually annotated reviews from two domains
(restaurants, laptops). The task attracted 163 sub-
missions from 32 teams that were evaluated in four
subtasks centered around aspect terms (detecting
aspect terms and their polarities) and coarser as-
pect categories (assigning aspect categories and
aspect category polarities to sentences). The task
will be repeated in SemEval-2015 with additional
datasets and a domain-adaptation subtask.
8
In the
future, we hope to add an aspect term aggrega-
tion subtask (Pavlopoulos and Androutsopoulos,
2014a).
Acknowledgements
We thank Ioanna Lazari, who provided an ini-
tial version of the laptops dataset, Konstantina Pa-
panikolaou, who carried out a critical part of the
8
Consult http://alt.qcri.org/semeval2015/
task12/.
Laptops Restaurants
Team Acc. Team Acc.
DCU 70.48 DCU 80.95
NRC-Can. 70.48 NRC-Can. 80.15?
SZTE-NLP 66.97 UWB 77.68*
UBham 66.66 XRCE 77.68
UWB 66.66* SZTE-NLP 75.22
lsis lif 64.52 UNITOR 74.95*
USF 64.52 UBham 74.6
SNAP 64.06 USF 73.19
UNITOR 62.99 UNITOR 72.48
UWB 62.53 SeemGo 72.31
IHS RD. 61.62 lsis lif 72.13
SeemGo 61.31 UWB 71.95
ECNU 61.16 SA-UZH 70.98
ECNU 61.16* IHS RD. 70.81
SINAI 58.71 SNAP 70.81
SAP RI 58.56 ECNU 70.72
UNITOR 58.56* ECNU 70.72*
SA-UZH 58.25 INSIGHT. 70.72
COMMIT 57.03 SAP RI 69.92
INSIGHT. 57.03 EBDG 68.6
UMCC. 57.03* UMCC. 66.84*
UFAL 56.88 UFAL 66.57
UMCC. 56.11 UMCC. 66.57
EBDG 55.96 COMMIT 65.96
JU CSE. 55.65 JU CSE. 65.52
UO UA 55.19* Blinov 63.58*
V3 53.82 iTac 62.25*
Blinov 52.29* V3 59.78
iTac 51.83* SINAI 58.73
DLIREC 36.54 DLIREC 42.32*
DLIREC 36.54* DLIREC 41.71
IITP 66.97 IITP 67.37
Baseline 51.37 Baseline 64.28
Majority 52.14 Majority 64.19
Table 6: Results for the aspect term polarity sub-
task (SB2). Stars indicate unconstrained systems.
The ? indicates a constrained system that was not
trained on the in-domain training dataset (unlike
the rest of the constrained systems), but on the
union of the two training datasets. IITP?s original
submission files were corrupted; they were resent
and scored after the end of the evaluation period.
annotation process, and Juli Bakagianni, who sup-
ported our use of the META-SHARE platform.
We are also very grateful to the participants for
their feedback. Maria Pontiki and Haris Papageor-
giou were supported by the IS-HELLEANA (09-
34
72-922) and the POLYTROPON (KRIPIS-GSRT,
MIS: 448306) projects.
References
Samuel Brody and Noemie Elhadad. 2010. An unsu-
pervised aspect-sentiment model for online reviews.
In Proceedings of NAACL, pages 804?812, Los An-
geles, California.
Michael Gamon, Anthony Aue, Simon Corston-Oliver,
and Eric K. Ringger. 2005. Pulse: Mining customer
opinions from free text. In IDA, pages 121?132,
Madrid, Spain.
Gayatree Ganu, Noemie Elhadad, and Am?elie Marian.
2009. Beyond the stars: Improving rating predic-
tions using review text content. In Proceedings of
WebDB, Providence, Rhode Island, USA.
Minqing Hu and Bing Liu. 2004a. Mining and sum-
marizing customer reviews. In Proceedings of KDD,
pages 168?177, Seattle, WA, USA.
Minqing Hu and Bing Liu. 2004b. Mining opinion fea-
tures in customer reviews. In Proceedings of AAAI,
pages 755?760, San Jose, California.
Bing Liu. 2012. Sentiment Analysis and Opinion Min-
ing. Synthesis Lectures on Human Language Tech-
nologies. Morgan & Claypool Publishers.
Chong Long, Jie Zhang, and Xiaoyan Zhu. 2010. A
review selection approach for accurate feature rating
estimation. In Proceedings of COLING (Posters),
pages 766?774, Beijing, China.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up? sentiment classification us-
ing machine learning techniques. In Proceedings of
EMNLP, pages 79?86, Philadelphia, Pennsylvania,
USA.
John Pavlopoulos and Ion Androutsopoulos. 2014a.
Aspect term extraction for sentiment analysis: New
datasets, new evaluation measures and an improved
unsupervised method. In Proceedings of LASM-
EACL, pages 44?52, Gothenburg, Sweden.
John Pavlopoulos and Ion Androutsopoulos. 2014b.
Multi-granular aspect aggregation in aspect-based
sentiment analysis. In Proceedings of EACL, pages
78?87, Gothenburg, Sweden.
Stelios Piperidis. 2012. The META-SHARE language
resources sharing infrastructure: Principles, chal-
lenges, solutions. In Proceedings of LREC-2012,
pages 36?42, Istanbul, Turkey.
Ana-Maria Popescu and Oren Etzioni. 2005. Extract-
ing product features and opinions from reviews. In
Proceedings of HLT/EMNLP, pages 339?346, Van-
couver, British Columbia, Canada.
Pontus Stenetorp, Sampo Pyysalo, Goran Topi?c,
Tomoko Ohta, Sophia Ananiadou, and Jun?ichi Tsu-
jii. 2012. BRAT: a web-based tool for NLP-assisted
text annotation. In Proceedings of EACL, pages
102?107, Avignon, France.
Tun Thura Thet, Jin-Cheon Na, and Christopher S. G.
Khoo. 2010. Aspect-based sentiment analysis of
movie reviews on discussion boards. J. Information
Science, 36(6):823?848.
Ivan Titov and Ryan T. McDonald. 2008. A joint
model of text and aspect ratings for sentiment sum-
marization. In Proceedings of ACL, pages 308?316,
Columbus, Ohio, USA.
Peter Turney. 2002. Thumbs up or thumbs down? se-
mantic orientation applied to unsupervised classifi-
cation of reviews. In Proceedings of ACL, pages
417?424, Philadelphia, Pennsylvania, USA.
35
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 114?118,
Dublin, Ireland, August 23-24, 2014.
AUEB: Two Stage Sentiment Analysis of Social Network Messages
Rafael Michael Karampatsis, John Pavlopoulos and Prodromos Malakasiotis
mpatsis13@gmail.com, annis@aueb.gr, rulller@aueb.gr
Department of Informatics
Athens University of Economics and Business
Patission 76, GR-104 34 Athens, Greece
Abstract
This paper describes the system submit-
ted for the Sentiment Analysis in Twitter
Task of SEMEVAL 2014 and specifically
the Message Polarity Classification sub-
task. We used a 2?stage pipeline approach
employing a linear SVM classifier at each
stage and several features including mor-
phological features, POS tags based fea-
tures and lexicon based features.
1 Introduction
Recently, Twitter has gained significant popularity
among the social network services. Lots of users
often use Twitter to express feelings or opinions
about a variety of subjects. Analysing this kind of
content can lead to useful information for fields,
such as personalized marketing or social profiling.
However such a task is not trivial, because the lan-
guage used in Twitter is often informal presenting
new challenges to text analysis.
In this paper we focus on sentiment analysis,
the field of study that analyzes people?s sentiment
and opinions from written language (Liu, 2012).
Given some text (e.g., tweet), sentiment analysis
systems return a sentiment label, which most often
is positive, negative, or neutral. This classification
can be performed directly or in two stages; in the
first stage the system examines whether the text
carries sentiment and in the second stage, the sys-
tem decides for the sentiment?s polarity (i.e., posi-
tive or negative).
1
This decomposition is based on
the assumption that subjectivity detection and sen-
timent polarity detection are different problems.
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
1
For instance a 2?stage approach is better suited to sys-
tems that focus on subjectivity detection; e.g., aspect based
sentiment analysis systems which extract aspect terms only
from evaluative texts.
We choose to follow the 2?stage approach, be-
cause it allows us to focus on each of the two prob-
lems separately (e.g., features, tuning, etc.). In the
following we will describe the system with which
we participated in the Message Polarity Classi-
fication subtask of Sentiment Analysis in Twit-
ter (Task 9) of SEMEVAL 2014 (Rosenthal et al.,
2014). Specifically Section 2 describes the data
provided by the organizers of the task. Sections 3
and 4 present our system and its performance re-
spectively. Finally, Section 5 concludes and pro-
vides hints for future work.
2 Data
At first, we describe the data used for this year?s
task. For system tuning the organizers released the
training and development data of SEMEVAL 2013
Task 2 (Wilson et al., 2013). Both these sets are
allowed to be used for training. The organizers
also provided the test data of the same Task to be
used for development only. As argued in (Malaka-
siotis et al., 2013) these data suffer from class im-
balance. Concerning the test data, they contained
8987 messages broken down in the following 5
datasets:
? LJ
14
: 2000 sentences from LIVEJOURNAL.
? SMS
13
: SMS test data from last year.
? TW
13
: Twitter test data from last year.
? TW
14
: 2000 new tweets.
? TWSARC
14
: 100 tweets containing sarcasm.
The details of the test data were made available to
the participants only after the end of the Task. Re-
call that SMS
13
and TW
13
were also provided as
development data. In this way the organizers were
able to check, i) the progress of the systems since
last year?s task, and ii) the generalization capabil-
ity of the participating systems.
114
3 System Overview
The main objective of our system is to detect
whether a message M expresses positive, negative
or no sentiment. To achieve that we follow a 2?
stage approach. During the first stage we detect
whether M expresses sentiment (?subjective?) or
not; this process is called subjectivity detection.
In the second stage we classify the ?subjective?
messages of the first stage as ?positive? or ?neg-
ative?. Both stages utilize a Support Vector Ma-
chine (SVM (Vapnik, 1998)) classifier with lin-
ear kernel.
2
Similar approaches have also been
proposed in (Pang and Lee, 2004; Wilson et al.,
2005; Barbosa and Feng, 2010; Malakasiotis et al.,
2013). Finally, we note that the 2?stage approach,
in datasets such the one here (Malakasiotis et al.,
2013), alleviates the class imbalance problem.
3.1 Data preprocessing
A very essential part of our system is data pre-
processing. At first, each message M is passed
through a twitter specific tokenizer and part-of-
speech (POS) tagger (Owoputi et al., 2013) to ob-
tain the tokens and the corresponding POS tags,
which are necessary for some sets of features.
3
We then use a dictionary to replace any slang with
the actual text.
4
We also normalize the text of
each message by combining a trie data structure
(De La Briandais, 1959) with an English dictio-
nary.
5
In more detail, we replace every token of M
not in the dictionary with the most similar word of
the dictionary. Finally, we obtain POS tags of all
the new tokens.
3.2 Sentiment lexicons
Another key attribute of our system is the use of
sentiment lexicons. We have used the following:
? HL (Hu and Liu, 2004).
? SENTIWORDNET (Baccianella et al., 2010).
? SENTIWORDNET lexicon with POS tags
(Baccianella et al., 2010).
? AFINN (Nielsen, 2011).
? MPQA (Wilson et al., 2005).
2
We used the LIBLINEAR distribution (Fan et al., 2008)
3
Tokens could be words, emoticons, hashtags, etc. No
lemmatization or stemming has been applied
4
See http://www.noslang.com/dictionary/.
5
We used the OPENOFFICE dictionary
? NRC Emotion lexicon (Mohammad and Tur-
ney, 2013).
? NRC S140 lexicon (Mohammad et al.,
2013).
? NRC Hashtag lexicon (Mohammad et al.,
2013).
? The three lexicons created from the training
data in (Malakasiotis et al., 2013).
Note that concerning the MPQA Lexicon we
applied preprocessing similar to Malakasiotis et al.
(2013) to obtain the following sub?lexicons:
S
+
: Contains strong subjective expressions with
positive prior polarity.
S
?
: Contains strong subjective expressions with
negative prior polarity.
S
?
: Contains strong subjective expressions with
either positive or negative prior polarity.
S
0
: Contains strong subjective expressions with
neutral prior polarity.
W
+
: Contains weak subjective expressions with
positive prior polarity.
W
?
: Contains weak subjective expressions with
negative prior polarity.
W
?
: Contains weak subjective expressions with
either positive or negative prior polarity.
W
0
: Contains weak subjective expressions with
neutral prior polarity.
3.3 Feature engineering
Our system employs several types of features
based on morphological attributes of the mes-
sages, POS tags, and lexicons of section 3.2.
6
3.3.1 Morphological features
? The existence of elongated tokens (e.g.,
?baaad?).
? The number of elongated tokens.
? The existence of date references.
? The existence of time references.
6
All the features are normalized to [?1, 1]
115
? The number of tokens that contain only upper
case letters.
? The number of tokens that contain both upper
and lower case letters.
? The number of tokens that start with an upper
case letter.
? The number of exclamation marks.
? The number of question marks.
? The sum of exclamation and question marks.
? The number of tokens containing only excla-
mation marks.
? The number of tokens containing only ques-
tion marks.
? The number of tokens containing only excla-
mation or question marks.
? The number of tokens containing only ellip-
sis (...).
? The existence of a subjective (i.e., positive or
negative) emoticon at the message?s end.
? The existence of an ellipsis and a link at the
message?s end.
? The existence of an exclamation mark at the
message?s end.
? The existence of a question mark at the mes-
sage?s end.
? The existence of a question or an exclamation
mark at the message?s end.
? The existence of slang.
3.3.2 POS based features
? The number of adjectives.
? The number of adverbs.
? The number of interjections.
? The number of verbs.
? The number of nouns.
? The number of proper nouns.
? The number of urls.
? The number of subjective emoticons.
7
? The number of positive emoticons.
8
? The number of negative emoticons.
9
? The average, maximum and minimum F
1
scores of the message?s POS bigrams for the
subjective and the neutral classes.
10
? The average, maximum and minimum F
1
scores of the message?s POS bigrams for the
positive and the negative classes.
11
For a bigram b and a class c, F
1
is calculated as:
F
1
(b, c) =
2 ? Pre(b, c) ?Rec(b, c)
Pre(b, c) +Rec(b, c)
(1)
where:
Pre(b, c) =
#messages of c containing b
#messages containing b
(2)
Rec(b, c) =
#messages of c containing b
#messages of c
(3)
3.3.3 Sentiment lexicon based features
For each lexicon we use seven different features
based on the scores provided by the lexicon for
each word present in the message.
12
? Sum of scores.
? Maximum of scores.
? Minimum of scores.
? Average of scores.
? The count of words with scores.
? The score of the last word of the message that
appears in the lexicon.
? The score of the last word of the message.
7
This feature is used only for subjectivity detection.
8
This feature is used only for polarity detection.
9
This feature is used only for polarity detection.
10
This feature is used only for subjectivity detection.
11
This feature is used only for polarity detection.
12
If a word does not appear in the lexicon it is assigned
with a score of 0 and it is not considered in the calculation of
the average, maximum, minimum and count scores. Also, we
have removed from SENTIWORDNET any instances having
positive and negative scores that sum to zero. Moreover, the
MPQA lexicon does not provide scores, so, for each word in
the lexicon we assume a score equal to 1.
116
We also created features based on the Pre and
F
1
scores of MPQA and the train data generated
lexicons in a similar manner to that described in
(Malakasiotis et al., 2013), with the difference that
the features are stage dependent. Thus, for subjec-
tivity detection we use the subjective and neutral
classes and for polarity detection we use the posi-
tive and negative classes to compute the scores.
3.3.4 Miscellaneous features
Negation. Negation not only is a good subjec-
tivity indicator but it also may change the
polarity of a message. We therefore add 7
more features, one indicating the existence
of negation, and the remaining six indicat-
ing the existence of negation that precedes
words from lexicons S
?
, S
+
, S
?
, W
?
, W
+
and W
?
.
13
Each feature is used in the appro-
priate stage.
14
We have not implement this
type of feature for other lexicons but it might
be a good addition to the system.
Carnegie Mellon University?s Twitter clusters.
Owoputi et al. (2013) released a dataset of
938 clusters containing words coming from
tweets. Words of the same clusters share
similar attributes. We try to exploit this
observation by adding 938 features, each of
which indicates if a message?s token appears
or not in the corresponding attributes.
3.4 Feature Selection
To allow our model to better scale on unseen data
we have performed feature selection. More specif-
ically, we first merged training and development
data of SEMEVAL 2013 Task 2. Then, we ranked
the features with respect to their information gain
(Quinlan, 1986) on this dataset. To obtain the best
set of features we started with a set containing the
top 50 features and we kept adding batches of 50
features until we have added all of them. At each
step we evaluated the corresponding feature set on
the TW
13
and SMS
13
datasets and chose the fea-
ture set with the best performance. This resulted in
a system which used the top 900 features for Stage
1 and the top 1150 features for Stage 2.
13
We use a list of words with negation. We assume that a
token precedes a word if it is in a distance of at most 5 tokens.
14
The features concerning S
?
and W
?
are used in subjec-
tivity detection and the remaining four in polarity detection.
Test Set AUEB Median Best
LJ
14
70.75 65.48 74.84
SMS
13
64.32 57.53 70.28
TW
13
63.92 62.88 72.12
TW
14
66.38 63.03 70.96
TWSARC
14
56.16 45.77 58.16
AVG
all
64.31 56.56 68.78
AVG
14
64.43 57.97 67.62
Table 1: F
1
(?) scores per dataset.
Test Set Ranking
LJ
14
9/50
SMS
13
8/50
TW
13
21/50
TW
14
14/50
TWSARC
14
4/50
AVG
all
6/50
AVG
14
5/50
Table 2: Rankings of our system.
4 Experimental Results
The official measure of the Task is the average F
1
score of the positive and negative classes (F
1
(?)).
Table 1 illustrates the F
1
(?) score per evaluation
dataset achieved by our system along with the me-
dian and best F
1
(?). In the same table AVG
all
corresponds to the average F
1
(?) across the five
datasets while AVG
14
corresponds to the average
F
1
(?) across LJ
14
, TW
14
and TWSARC
14
. We
observe that in all cases our results are above the
median. Table 2 illustrates the ranking of our sys-
tem according to F
1
(?). Our system ranked 6th
according to AVG
all
and 5th according to AVG
14
among the 50 participating systems. Note that our
best results were achieved on the new test sets
(LJ
14
, TW
14
, TWSARC
14
) meaning that our sys-
tem has a good generalization ability.
5 Conclusion and future work
In this paper we presented our approach for the
Message Polarity Classification subtask of the
Sentiment Analysis in Twitter Task of SEMEVAL
2014. We proposed a 2?stage pipeline approach,
which first detects sentiment and then decides
about its polarity. The results indicate that our sys-
tem handles well the class imbalance problem and
has a good generalization ability. A possible ex-
planation is that we do not use bag-of-words fea-
117
tures which often suffer from over?fitting. Never-
theless, there is still some room for improvement.
A promising direction would be to improve the
1st stage (subjectivity detection) either by adding
more data or by adding more features, mostly be-
cause the performance of stage 1 greatly affects
that of stage 2. Finally, the addition of more data
for the negative class on stage 2 might be a good
improvement because it would further reduce the
class imbalance of the training data for this stage.
References
Stefano Baccianella, Andrea Esuli, and Fabrizio Se-
bastiani. 2010. Sentiwordnet 3.0: An enhanced
lexical resource for sentiment analysis and opin-
ion mining. In Nicoletta Calzolari (Conference
Chair), Khalid Choukri, Bente Maegaard, Joseph
Mariani, Jan Odijk, Stelios Piperidis, Mike Ros-
ner, and Daniel Tapias, editors, Proceedings of the
Seventh International Conference on Language Re-
sources and Evaluation (LREC?10), Valletta, Malta,
may.
Luciano Barbosa and Junlan Feng. 2010. Robust sen-
timent detection on twitter from biased and noisy
data. In Proceedings of the 23rd International
Conference on Computational Linguistics: Posters,
COLING ?10, pages 36?44, Beijing, China.
Rene De La Briandais. 1959. File searching using
variable length keys. In Papers Presented at the the
March 3-5, 1959, Western Joint Computer Confer-
ence, IRE-AIEE-ACM ?59 (Western), pages 295?
298, New York, NY, USA.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-
Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A
library for large linear classification. The Journal of
Machine Learning Research, 9:1871?1874.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the Tenth
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, KDD ?04, pages
168?177, New York, NY, USA.
Bing Liu. 2012. Sentiment analysis and opinion min-
ing. Synthesis Lectures on Human Language Tech-
nologies, 5(1):1?167.
Prodromos Malakasiotis, Rafael Michael Karampat-
sis, Konstantina Makrynioti, and John Pavlopoulos.
2013. nlp.cs.aueb.gr: Two stage sentiment analysis.
In Second Joint Conference on Lexical and Com-
putational Semantics (*SEM), Volume 2: Proceed-
ings of the Seventh International Workshop on Se-
mantic Evaluation (SemEval 2013), pages 562?567,
Atlanta, Georgia, June.
Saif Mohammad and Peter Turney. 2013. Crowdsourc-
ing a word-emotion association lexicon. 29(3):436?
465.
Saif Mohammad, Svetlana Kiritchenko, and Xiaodan
Zhu. 2013. Nrc-canada: Building the state-of-the-
art in sentiment analysis of tweets. In Proceedings
of the Seventh International Workshop on Semantic
Evaluation (SemEval 2013), Atlanta, Georgia, USA,
June.
Finn
?
Arup Nielsen. 2011. A new anew: evaluation of
a word list for sentiment analysis in microblogs. In
Matthew Rowe, Milan Stankovic, Aba-Sah Dadzie,
and Mariann Hardey, editors, Proceedings of the
ESWC2011 Workshop on ?Making Sense of Micro-
posts?: Big things come in small packages, volume
718 of CEUR Workshop Proceedings, pages 93?98,
May.
Olutobi Owoputi, Brendan OConnor, Chris Dyer,
Kevin Gimpel, Nathan Schneider, and Noah A
Smith. 2013. Improved part-of-speech tagging for
online conversational text with word clusters. In
Proceedings of NAACL.
Bo Pang and Lillian Lee. 2004. A sentimental educa-
tion: sentiment analysis using subjectivity summa-
rization based on minimum cuts. In Proceedings of
the 42nd Annual Meeting on Association for Com-
putational Linguistics, ACL ?04, Barcelona, Spain.
Ross Quinlan. 1986. Induction of decision trees.
Mach. Learn., 1(1):81?106, March.
Sara Rosenthal, Preslav Nakov, Alan Ritter, and
Veselin Stoyanov. 2014. SemEval-2014 Task 9:
Sentiment Analysis in Twitter. In Preslav Nakov and
Torsten Zesch, editors, Proceedings of the 8th In-
ternational Workshop on Semantic Evaluation, Se-
mEval ?14, Dublin, Ireland.
Vladimir Vapnik. 1998. Statistical learning theory.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of the con-
ference on Human Language Technology and Em-
pirical Methods in Natural Language Processing,
pages 347?354.
Theresa Wilson, Zornitsa Kozareva, Preslav Nakov,
Sara Rosenthal, Veselin Stoyanov, and Alan Ritter.
2013. SemEval-2013 task 2: Sentiment analysis in
twitter. In Proceedings of the International Work-
shop on Semantic Evaluation, SemEval ?13, June.
118
Proceedings of the 5th Workshop on Language Analysis for Social Media (LASM) @ EACL 2014, pages 44?52,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
Aspect Term Extraction for Sentiment Analysis: New Datasets, New
Evaluation Measures and an Improved Unsupervised Method
John Pavlopoulos and Ion Androutsopoulos
Dept. of Informatics, Athens University of Economics and Business, Greece
http://nlp.cs.aueb.gr/
Abstract
Given a set of texts discussing a particular
entity (e.g., customer reviews of a smart-
phone), aspect based sentiment analysis
(ABSA) identifies prominent aspects of the
entity (e.g., battery, screen) and an aver-
age sentiment score per aspect. We fo-
cus on aspect term extraction (ATE), one
of the core processing stages of ABSA that
extracts terms naming aspects. We make
publicly available three new ATE datasets,
arguing that they are better than previously
available ones. We also introduce new
evaluation measures for ATE, again argu-
ing that they are better than previously
used ones. Finally, we show how a pop-
ular unsupervised ATE method can be im-
proved by using continuous space vector
representations of words and phrases.
1 Introduction
Before buying a product or service, consumers of-
ten search the Web for expert reviews, but increas-
ingly also for opinions of other consumers, ex-
pressed in blogs, social networks etc. Many useful
opinions are expressed in text-only form (e.g., in
tweets). It is then desirable to extract aspects (e.g.,
screen, battery) from the texts that discuss a par-
ticular entity (e.g., a smartphone), i.e., figure out
what is being discussed, and also estimate aspect
sentiment scores, i.e., how positive or negative
the (usually average) sentiment for each aspect is.
These two goals are jointly known as Aspect Based
Sentiment Analysis (ABSA) (Liu, 2012).
In this paper, we consider free text customer re-
views of products and services; ABSA, however,
is also applicable to texts about other kinds of
entities (e.g., politicians, organizations). We as-
sume that a search engine retrieves customer re-
views about a particular target entity (product or
Figure 1: Automatically extracted prominent as-
pects (shown as clusters of aspect terms) and aver-
age aspect sentiment scores of a target entity.
service), that multiple reviews written by different
customers are retrieved for each target entity, and
that the ultimate goal is to produce a table like the
one of Fig. 1, which presents the most prominent
aspects and average aspect sentiment scores of the
target entity. Most ABSA systems in effect perform
all or some of the following three subtasks:
Aspect term extraction: Starting from texts
about a particular target entity or entities of the
same type as the target entity (e.g., laptop re-
views), this stage extracts and possibly ranks by
importance aspect terms, i.e., terms naming as-
pects (e.g., ?battery?, ?screen?) of the target en-
tity, including multi-word terms (e.g., ?hard disk?)
(Liu, 2012; Long et al., 2010; Snyder and Barzi-
lay, 2007; Yu et al., 2011). At the end of this stage,
each aspect term is taken to be the name of a dif-
ferent aspect, but aspect terms may subsequently
be clustered during aspect aggregation; see below.
Aspect term sentiment estimation: This stage
estimates the polarity and possibly also the inten-
sity (e.g., strongly negative, mildly positive) of the
opinions for each aspect term of the target entity,
usually averaged over several texts. Classifying
texts by sentiment polarity is a popular research
topic (Liu, 2012; Pang and Lee, 2005; Tsytsarau
and Palpanas, 2012). The goal, however, in this
44
ABSA subtask is to estimate the (usually average)
polarity and intensity of the opinions about partic-
ular aspect terms of the target entity.
Aspect aggregation: Some systems group aspect
terms that are synonyms or near-synonyms (e.g.,
?price?, ?cost?) or, more generally, cluster aspect
terms to obtain aspects of a coarser granularity
(e.g., ?chicken?, ?steak?, and ?fish? may all be re-
placed by ?food?) (Liu, 2012; Long et al., 2010;
Zhai et al., 2010; Zhai et al., 2011). A polar-
ity (and intensity) score can then be computed for
each coarser aspect (e.g., ?food?) by combining
(e.g., averaging) the polarity scores of the aspect
terms that belong in the coarser aspect.
In this paper, we focus on aspect term extrac-
tion (ATE). Our contribution is threefold. Firstly,
we argue (Section 2) that previous ATE datasets are
not entirely satisfactory, mostly because they con-
tain reviews from a particular domain only (e.g.,
consumer electronics), or they contain reviews for
very few target entities, or they do not contain an-
notations for aspect terms. We constructed and
make publicly available three new ATE datasets
with customer reviews for a much larger number
of target entities from three domains (restaurants,
laptops, hotels), with gold annotations of all the
aspect term occurrences; we also measured inter-
annotator agreement, unlike previous datasets.
Secondly, we argue (Section 3) that commonly
used evaluation measures are also not entirely sat-
isfactory. For example, when precision, recall,
and F -measure are computed over distinct as-
pect terms (types), equal weight is assigned to
more and less frequent aspect terms, whereas fre-
quently discussed aspect terms are more impor-
tant; and when precision, recall, and F -measure
are computed over aspect term occurrences (to-
kens), methods that identify very few, but very fre-
quent aspect terms may appear to perform much
better than they actually do. We propose weighted
variants of precision and recall, which take into ac-
count the rankings of the distinct aspect terms that
are obtained when the distinct aspect terms are or-
dered by their true and predicted frequencies. We
also compute the average weighted precision over
several weighted recall levels.
Thirdly, we show (Section 4) how the popular
unsupervised ATE method of Hu and Liu (2004),
can be extended with continuous space word vec-
tors (Mikolov et al., 2013a; Mikolov et al., 2013b;
Mikolov et al., 2013c). Using our datasets and
evaluation measures, we demonstrate (Section 5)
that the extended method performs better.
2 Datasets
We first discuss previous datasets that have been
used for ATE, and we then introduce our own.
2.1 Previous datasets
So far, ATE methods have been evaluated mainly
on customer reviews, often from the consumer
electronics domain (Hu and Liu, 2004; Popescu
and Etzioni, 2005; Ding et al., 2008).
The most commonly used dataset is that of Hu
and Liu (2004), which contains reviews of only
five particular electronic products (e.g., Nikon
Coolpix 4300). Each sentence is annotated with
aspect terms, but inter-annotator agreement has
not been reported.
1
All the sentences appear to
have been selected to express clear positive or neg-
ative opinions. There are no sentences express-
ing conflicting opinions about aspect terms (e.g.,
?The screen is clear but small?), nor are there
any sentences that do not express opinions about
their aspect terms (e.g., ?It has a 4.8-inch screen?).
Hence, the dataset is not entirely representative of
product reviews. By contrast, our datasets, dis-
cussed below, contain reviews from three domains,
including sentences that express conflicting or no
opinions about aspect terms, they concern many
more target entities (not just five), and we have
also measured inter-annotator agreement.
The dataset of Ganu et al. (2009), on which
one of our datasets is based, is also popular. In
the original dataset, each sentence is tagged with
coarse aspects (?food?, ?service?, ?price?, ?ambi-
ence?, ?anecdotes?, or ?miscellaneous?). For exam-
ple, ?The restaurant was expensive, but the menu
was great? would be tagged with the coarse as-
pects ?price? and ?food?. The coarse aspects, how-
ever, are not necessarily terms occurring in the
sentence, and it is unclear how they were obtained.
By contrast, we asked human annotators to mark
the explicit aspect terms of each sentence, leaving
the task of clustering the terms to produce coarser
aspects for an aspect aggregation stage.
The ?Concept-Level Sentiment Analysis Chal-
lenge? of ESWC 2014 uses the dataset of Blitzer
et al. (2007), which contains customer reviews of
1
Each aspect term occurrence is also annotated with a sen-
timent score. We do not discuss these scores here, since we
focus on ATE. The same comment applies to the dataset of
Ganu et al. (2009) and our datasets.
45
DVDs, books, kitchen appliances, and electronic
products, with an overall sentiment score for each
review. One of the challenge?s tasks requires sys-
tems to extract the aspects of each sentence and a
sentiment score (positive or negative) per aspect.
2
The aspects are intended to be concepts from on-
tologies, not simply aspect terms. The ontologies
to be used, however, are not fully specified and no
training dataset with sentences and gold aspects is
currently available.
Overall, the previous datasets are not entirely
satisfactory, because they contain reviews from
a particular domain only, or reviews for very
few target entities, or their sentences are not en-
tirely representative of customer reviews, or they
do not contain annotations for aspect terms, or
no inter-annotator agreement has been reported.
To address these issues, we provide three new
ATE datasets, which contain customer reviews of
restaurants, hotels, and laptops, respectively.
3
2.2 Our datasets
The restaurants dataset contains 3,710 English
sentences from the reviews of Ganu et al. (2009).
4
We asked human annotators to tag the aspect terms
of each sentence. In ?The dessert was divine?,
for example, the annotators would tag the aspect
term ?dessert?. In a sentence like ?The restaurant
was expensive, but the menu was great?, the an-
notators were instructed to tag only the explicitly
mentioned aspect term ?menu?. The sentence also
refers to the prices, and a possibility would be to
add ?price? as an implicit aspect term, but we do
not consider implicit aspect terms in this paper.
We used nine annotators for the restaurant re-
views. Each sentence was processed by a single
annotator, and each annotator processed approxi-
mately the same number of sentences. Among the
3,710 restaurant sentences, 1,248 contain exactly
one aspect term, 872 more than one, and 1,590 no
aspect terms. There are 593 distinct multi-word
aspect terms and 452 distinct single-word aspect
terms. Removing aspect terms that occur only
once leaves 67 distinct multi-word and 195 dis-
tinct single-word aspect terms.
The hotels dataset contains 3,600 English sen-
2
See http://2014.eswc-conferences.org/.
3
Our datasets are available upon request. The datasets
of the ABSA task of SemEval 2014 (http://alt.qcri.
org/semeval2014/task4/) are based on our datasets.
4
The original dataset of Ganu et al. contains 3,400 sen-
tences, but some of the sentences had not been properly split.
tences from online customer reviews of 30 hotels.
We used three annotators. Among the 3,600 hotel
sentences, 1,326 contain exactly one aspect term,
652 more than one, and 1,622 none. There are 199
distinct multi-word aspect terms and 262 distinct
single-word aspect terms, of which 24 and 120,
respectively, were tagged more than once.
The laptops dataset contains 3,085 English sen-
tences of 394 online customer reviews. A single
annotator (one of the authors) was used. Among
the 3,085 laptop sentences, 909 contain exactly
one aspect term, 416 more than one, and 1,760
none. There are 350 distinct multi-word and 289
distinct single-word aspect terms, of which 67 and
137, respectively, were tagged more than once.
To measure inter-annotator agreement, we used
a sample of 75 restaurant, 75 laptop, and 100 hotel
sentences. Each sentence was processed by two
(for restaurants and laptops) or three (for hotels)
annotators, other than the annotators used previ-
ously. For each sentence s
i
, the inter-annotator
agreement was measured as the Dice coefficient
D
i
= 2 ?
|A
i
?B
i
|
|A
i
|+|B
i
|
, where A
i
, B
i
are the sets of
aspect term occurrences tagged by the two anno-
tators, respectively, and |S| denotes the cardinal-
ity of a set S; for hotels, we use the mean pair-
wiseD
i
of the three annotators.
5
The overall inter-
annotator agreement D was taken to be the aver-
age D
i
of the sentences of each sample. We, thus,
obtainedD = 0.72, 0.70, 0.69, for restaurants, ho-
tels, and laptops, respectively, which indicate rea-
sonably high inter-annotator agreement.
2.3 Single and multi-word aspect terms
ABSA systems use ATE methods ultimately to ob-
tain the m most prominent (frequently discussed)
distinct aspect terms of the target entity, for dif-
ferent values of m.
6
In a system like the one of
Fig. 1, for example, if we ignore aspect aggrega-
tion, each row will report the average sentiment
score of a single frequent distinct aspect term, and
m will be the number of rows, which may depend
on the display size or user preferences.
Figure 2 shows the percentage of distinct multi-
word aspect terms among themmost frequent dis-
tinct aspect terms, for different values of m, in
5
Cohen?s Kappa cannot be used here, because the annota-
tors may tag any word sequence of any sentence, which leads
to a very large set of categories. A similar problem was re-
ported by Kobayashi et al. (2007).
6
A more general definition of prominence might also con-
sider the average sentiment score of each distinct aspect term.
46
our three datasets and the electronics dataset of Hu
and Liu (2004). There are many more single-word
distinct aspect terms than multi-word distinct as-
pect terms, especially in the restaurant and hotel
reviews. In the electronics and laptops datasets,
the percentage of multi-word distinct aspect terms
(e.g., ?hard disk?) is higher, but most of the dis-
tinct aspect terms are still single-word, especially
for small values of m. By contrast, many ATE
methods (Hu and Liu, 2004; Popescu and Etzioni,
2005; Wei et al., 2010) devote much of their pro-
cessing to identifying multi-word aspect terms.
Figure 2: Percentage of (distinct) multi-word as-
pect terms among the most frequent aspect terms.
3 Evaluation measures
We now discuss previous ATE evaluation mea-
sures, also introducing our own.
3.1 Precision, Recall, F-measure
ATE methods are usually evaluated using preci-
sion, recall, and F -measure (Hu and Liu, 2004;
Popescu and Etzioni, 2005; Kim and Hovy, 2006;
Wei et al., 2010; Moghaddam and Ester, 2010;
Bagheri et al., 2013), but it is often unclear if these
measures are applied to distinct aspect terms (no
duplicates) or aspect term occurrences.
In the former case, each method is expected to
return a set A of distinct aspect terms, to be com-
pared to the set G of distinct aspect terms the hu-
man annotators identified in the texts. TP (true
positives) is |A?G|, FP (false positives) is |A\G|,
FN (false negatives) is |G\A|, and precision (P ),
recall (R), F =
2?P ?R
P+R
are defined as usually:
P =
TP
TP + FP
, R =
TP
TP + FN
(1)
This way, however, precision, recall, and F -
measure assign the same importance to all the dis-
tinct aspect terms, whereas missing, for example, a
more frequent (more frequently discussed) distinct
aspect term should probably be penalized more
heavily than missing a less frequent one.
When precision, recall, and F -measure are ap-
plied to aspect term occurrences (Liu et al., 2005),
TP is the number of aspect term occurrences
tagged (each term occurrence) both by the method
being evaluated and the human annotators, FP is
the number of aspect term occurrences tagged by
the method but not the human annotators, and FN
is the number of aspect term occurrences tagged
by the human annotators but not the method. The
three measures are then defined as above. They
now assign more importance to frequently occur-
ring distinct aspect terms, but they can produce
misleadingly high scores when only a few, but
very frequent distinct aspect terms are handled
correctly. Furthermore, the occurrence-based def-
initions do not take into account that missing sev-
eral aspect term occurrences or wrongly tagging
expressions as aspect term occurrences may not
actually matter, as long as the m most frequent
distinct aspect terms can be correctly reported.
3.2 Weighted precision, recall, AWP
What the previous definitions of precision and re-
call miss is that in practice ABSA systems use
ATE methods ultimately to obtain the m most fre-
quent distinct aspect terms, for a range of m val-
ues. Let A
m
and G
m
be the lists that contain the
m most frequent distinct aspect terms, ordered by
their predicted and true frequencies, respectively;
the predicted and true frequencies are computed
by examining how frequently the ATE method or
the human annotators, respectively, tagged occur-
rences of each distinct aspect term. Differences
between the predicted and true frequencies do not
matter, as long as A
m
= G
m
, for every m. Not
including in A
m
a term of G
m
should be penal-
ized more or less heavily, depending on whether
the term?s true frequency was high or low, respec-
tively. Furthermore, including in A
m
a term not in
G
m
should be penalized more or less heavily, de-
pending on whether the term was placed towards
the beginning or the end of A
m
, i.e., depending on
the prominence that was assigned to the term.
To address the issues discussed above, we in-
troduce weighted variants of precision and recall.
47
For each ATE method, we now compute a single
list A =
?
a
1
, . . . , a
|A|
?
of distinct aspect terms
identified by the method, ordered by decreasing
predicted frequency. For every m value (number
of most frequent distinct aspect terms to show),
the method is treated as having returned the sub-
list A
m
with the first m elements of A. Similarly,
we now take G =
?
g
1
, . . . , g
|G|
?
to be the list of
the distinct aspect terms that the human annotators
tagged, ordered by decreasing true frequency.
7
We
define weighted precision (WP
m
) and weighted
recall (WR
m
) as in Eq. 2?3. The notation 1{?}
denotes 1 if condition ? holds, and 0 otherwise.
By r(a
i
) we denote the ranking of the returned
term a
i
in G, i.e., if a
i
= g
j
, then r(a
i
) = j; if
a
i
?? G, then r(a
i
) is an arbitrary positive integer.
WP
m
=
?
m
i=1
1
i
? 1{a
i
? G}
?
m
i=1
1
i
(2)
WR
m
=
?
m
i=1
1
r(a
i
)
? 1{a
i
? G}
?
|G|
j=1
1
j
(3)
WR
m
counts how many terms of G (gold dis-
tinct aspect terms) the method returned in A
m
,
but weighting each term by its inverse ranking
1
r(a
i
)
, i.e., assigning more importance to terms the
human annotators tagged more frequently. The
denominator of Eq. 3 sums the weights of all
the terms of G; in unweighted recall applied to
distinct aspect terms, where all the terms of G
have the same weight, the denominator would be
|G| = TP + FN (Eq. 1). WP
m
counts how
many gold aspect terms the method returned in
A
m
, but weighting each returned term a
i
by its
inverse ranking
1
i
in A
m
, to reward methods that
return more gold aspect terms towards the begin-
ning of A
m
. The denominator of Eq. 2 sums the
weights of all the terms ofA
m
; in unweighted pre-
cision applied to distinct aspect terms, the denom-
inator would be |A
m
| = TP + FN (Eq. 1).
We plot weighted precision-recall curves by
computingWP
m
,WR
m
pairs for different values
of m, as in Fig. 3 below.
8
The higher the curve
of a method, the better the method. We also com-
pute the average (interpolated) weighted precision
7
In our experiments, we exclude from G aspect terms
tagged by the annotators only once.
8
With supervised methods, we perform a 10-fold cross-
validation for each m, and we macro-average WP
m
,WR
m
over the folds. We provide our datasets partitioned in folds.
(AWP ) of each method over 11 recall levels:
AWP =
1
11
?
r?{0,0.1,...,1}
WP
int
(r)
WP
int
(r) = max
m?{1,...,|A|},WR
m
? r
WP
m
AWP is similar to average (interpolated) precision
(AP ), which is used to summarize the tradeoff be-
tween (unweighted) precision and recall.
3.3 Other related measures
Yu at al. (2011) used nDCG@m (J?arvelin and
Kek?al?ainen, 2002; Sakai, 2004; Manning et al.,
2008), defined below, to evaluate each list of m
distinct aspect terms returned by an ATE method.
nDCG@m =
1
Z
m
?
i=1
2
t(i)
? 1
log
2
(1 + i)
Z is a normalization factor to ensure that a perfect
ranking gets nDCG@m = 1, and t(i) is a reward
function for a term placed at position i of the re-
turned list. In the work of Yu et al., t(i) = 1 if the
term at position i is not important (as judged by
a human), t(i) = 2 if the term is ?ordinary?, and
t(i) = 3 if it is important. The logarithm is used to
reduce the reward for distinct aspect terms placed
at lower positions of the returned list.
The nDCG@mmeasure is well known in rank-
ing systems (e.g., search engines) and it is similar
to our weighted precision (WP
m
). The denomina-
tor or Eq. 2 corresponds to the normalization fac-
tor Z of nDCG@m; the
1
i
factor of in the numer-
ator of Eq. 2 corresponds to the
1
log
2
(1+i)
degra-
dation factor of nDCG@m; and the 1{a
i
? G}
factor of Eq. 2 is a binary reward function, corre-
sponding to the 2
t(i)
? 1 factor of nDCG@m.
The main difference from nDCG@m is that
WP
m
uses a degradation factor
1
i
that is inversely
proportional to the ranking of the returned term
a
i
in the returned list A
m
, whereas nDCG@m
uses a logarithmic factor
1
log
2
(1+i)
, which reduces
less sharply the reward for distinct aspect terms
returned at lower positions in A
m
. We believe
that the degradation factor of WP
m
is more ap-
propriate for ABSA, because most users would in
practice wish to view sentiment scores for only a
few (e.g., m = 10) frequent distinct aspect terms,
whereas in search engines users are more likely to
examine more of the highly-ranked returned items.
It is possible, however, to use a logarithmic degra-
dation factor inWP
m
, as in nDCG@m.
48
Another difference is that we use a binary re-
ward factor 1{a
i
? G} in WP
m
, instead of the
2
t(i)
? 1 factor of nDCG@m that has three pos-
sibly values in the work of Yu at al. (2011). We
use a binary reward factor, because preliminary
experiments we conducted indicated that multi-
ple relevance levels (e.g., not an aspect term, as-
pect term but unimportant, important aspect term)
confused the annotators and led to lower inter-
annotator agreement. The nDCG@m measure
can also be used with a binary reward factor; the
possible values t(i) would be 0 and 1.
With a binary reward factor, nDCG@m in ef-
fect measures the ratio of correct (distinct) aspect
terms to the terms returned, assigning more weight
to correct aspect terms placed closer the top of the
returned list, like WP
m
. The nDCG@m mea-
sure, however, does not provide any indication
of how many of the gold distinct aspect terms
have been returned. By contrast, we also mea-
sure weighted recall (Eq. 3), which examines how
many of the (distinct) gold aspect terms have been
returned in A
m
, also assigning more weight to the
gold aspect terms the human annotators tagged
more frequently. We also compute the average
weighted precision (AWP ), which is a combina-
tion ofWP
m
andWR
m
, for a range of m values.
4 Aspect term extraction methods
We implemented and evaluated four ATE meth-
ods: (i) a popular baseline (dubbed FREQ) that re-
turns the most frequent distinct nouns and noun
phrases, (ii) the well-known method of Hu and Liu
(2004), which adds to the baseline pruning mech-
anisms and steps that detect more aspect terms
(dubbed H&L), (iii) an extension of the previous
method (dubbed H&L+W2V), with an extra prun-
ing step we devised that uses the recently pop-
ular continuous space word vectors (Mikolov et
al., 2013c), and (iv) a similar extension of FREQ
(dubbed FREQ+W2V). All four methods are unsu-
pervised, which is particularly important for ABSA
systems intended to be used across domains with
minimal changes. They return directly a list A of
distinct aspect terms ordered by decreasing pre-
dicted frequency, rather than tagging aspect term
occurrences, which would require computing the
A list from the tagged occurrences before apply-
ing our evaluation measures (Section 3.2).
4.1 The FREQ baseline
The FREQ baseline returns the most frequent (dis-
tinct) nouns and noun phrases of the reviews in
each dataset (restaurants, hotels, laptops), ordered
by decreasing sentence frequency (how many sen-
tences contain the noun or noun phrase).
9
This is a
reasonably effective and popular baseline (Hu and
Liu, 2004; Wei et al., 2010; Liu, 2012).
4.2 The H&L method
The method of Hu and Liu (2004), dubbed H&L,
first extracts all the distinct nouns and noun
phrases from the reviews of each dataset (lines 3?
6 of Algorithm 1) and considers them candidate
distinct aspect terms.
10
It then forms longer can-
didate distinct aspect terms by concatenating pairs
and triples of candidate aspect terms occurring in
the same sentence, in the order they appear in the
sentence (lines 7?11). For example, if ?battery
life? and ?screen? occur in the same sentence (in
this order), then ?battery life screen? will also be-
come a candidate distinct aspect term.
The resulting candidate distinct aspect terms
are ordered by decreasing p-support (lines 12?15).
The p-support of a candidate distinct aspect term t
is the number of sentences that contain t, exclud-
ing sentences that contain another candidate dis-
tinct aspect term t
?
that subsumes t. For example,
if both ?battery life? and ?battery? are candidate
distinct aspect terms, a sentence like ?The battery
life was good? is counted in the p-support of ?bat-
tery life?, but not in the p-support of ?battery?.
The method then tries to correct itself by prun-
ing wrong candidate distinct aspect terms and de-
tecting additional candidates. Firstly, it discards
multi-word distinct aspect terms that appear in
?non-compact? form in more than one sentences
(lines 16?23). Amulti-word term t appears in non-
compact form in a sentence if there are more than
three other words (not words of t) between any
two of the words of t in the sentence. For exam-
ple, the candidate distinct aspect term ?battery life
screen? appears in non-compact form in ?battery
life is way better than screen?. Secondly, if the
p-support of a candidate distinct aspect term t is
smaller than 3 and t is subsumed by another can-
9
We use the default POS tagger of NLTK, and the chun-
ker of NLTK trained on the Treebank corpus; see http:
//nltk.org/. We convert all words to lower-case.
10
Some details of the work of Hu and Liu (2004) were not
entirely clear to us. The discussion here and our implementa-
tion reflect our understanding.
49
didate distinct aspect term t
?
, then t is discarded
(lines 21?23).
Subsequently, a set of ?opinion adjectives? is
formed; for each sentence and each candidate dis-
tinct aspect term t that occurs in the sentence, the
closest to t adjective of the sentence (if there is
one) is added to the set of opinion adjectives (lines
25-27). The sentences are then re-scanned; if a
sentence does not contain any candidate aspect
term, but contains an opinion adjective, then the
nearest noun to the opinion adjective is added to
the candidate distinct aspect terms (lines 28?31).
The remaining candidate distinct aspect terms are
returned, ordered by decreasing p-support.
Algorithm 1 The method of Hu and Liu
Require: sentences: a list of sentences
1: terms = new Set(String)
2: psupport = new Map(String, int)
3: for s in sentences do
4: nouns = POSTagger(s).getNouns()
5: nps = Chunker(s).getNPChunks()
6: terms.add(nouns ? nps)
7: for s in sentences do
8: for t1, t2 in terms s.t. t1, t2 in s ?
s.index(t1)<s.index(t2) do
9: terms.add(t1 + ? ? + t2)
10: for t1, t2, t3 in s.t. t1, t2,t3 in s ?
s.index(t1)<s.index(t2)<s.index(t3) do
11: terms.add(t1 + ? ? + t2 + ? ? + t3)
12: for s in sentences do
13: for t: t in terms ? t in s do
14: if ?? t?: t? in terms ? t? in s ? t in t? then
15: psupport[term] += 1
16: nonCompact = new Map(String, int)
17: for t in terms do
18: for s in sentences do
19: if maxPairDistance(t.words())>3 then
20: nonCompact[t] += 1
21: for t in terms do
22: if nonCompact[t]>1 ? (? t?: t? in terms ? t in t? ?
psupport[t]<3) then
23: terms.remove(t)
24: adjs = new Set(String)
25: for s in sentences do
26: if ? t: t in terms ? t in s then
27: adjs.add(POSTagger(s).getNearestAdj(t))
28: for s in sentences do
29: if ?? t: t in terms ? t in s ? ? a: a in adjs ? a in s
then
30: t = POSTagger(s).getNearestNoun(adjs)
31: terms.add(t)
32: return psupport.keysSortedByValue()
4.3 The H&L+W2V method
We extended H&L by including an additional
pruning step that uses continuous vector space
representations of words (Mikolov et al., 2013a;
Mikolov et al., 2013b; Mikolov et al., 2013c).
The vector representations of the words are pro-
Centroid Closest Wikipedia words
Com. lang. only, however, so, way, because
Restaurants meal, meals, breakfast, wingstreet,
snacks
Hotels restaurant, guests, residence, bed, ho-
tels
Laptops gameport, hardware, hd floppy, pcs, ap-
ple macintosh
Table 1: Wikipedia words closest to the common
language and domain centroids.
duced by using a neural network language model,
whose inputs are the vectors of the words occur-
ring in each sentence, treated as latent variables to
be learned. We used the EnglishWikipedia to train
the language model and obtain word vectors, with
200 features per vector. Vectors for short phrases,
in our case candidate multi-word aspect terms, are
produced in a similar manner.
11
Our additional pruning stage is invoked imme-
diately immediately after line 6 of Algorithm 1. It
uses the ten most frequent candidate distinct as-
pect terms that are available up to that point (fre-
quency taken to be the number of sentences that
contain each candidate) and computes the centroid
of their vectors, dubbed the domain centroid. Sim-
ilarly, it computes the centroid of the 20 most fre-
quent words of the Brown Corpus (news category),
excluding stop-words and words shorter than three
characters; this is the common language centroid.
Any candidate distinct aspect term whose vector is
closer to the common language centroid than the
domain centroid is discarded, the intuition being
that the candidate names a very general concept,
rather than a domain-specific aspect.
12
We use co-
sine similarity to compute distances. Vectors ob-
tained from Wikipedia are used in all cases.
To showcase the insight of our pruning step,
Table 1 shows the five words from the English
Wikipedia whose vectors are closest to the com-
mon language centroid and the three domain cen-
troids. The words closest to the common language
centroid are common words, whereas words clos-
est to the domain centroids name domain-specific
concepts that are more likely to be aspect terms.
11
We use WORD2VEC, available at https://code.
google.com/p/word2vec/, with a continuous bag of
words model, default parameters, the first billion characters
of the English Wikipedia, and the pre-processing of http:
//mattmahoney.net/dc/textdata.html.
12
WORD2VEC does not produce vectors for phrases longer
than two words; thus, our pruning mechanism never discards
candidate aspect terms of more than two words.
50
Figure 3: Weighted precision ? weighted recall curves for the three datasets.
4.4 The FREQ+W2V method
As with H&L+W2V, we extended FREQ by adding
our pruning step that uses the continuous space
word (and phrase) vectors. Again, we produced
one common language and three domain cen-
troids, as before. Candidate distinct aspect terms
whose vector was closer to the common language
centroid than the domain centroid were discarded.
5 Experimental results
Table 2 shows the AWP scores of the methods.
All four methods perform better on the restaurants
dataset. At the other extreme, the laptops dataset
seems to be the most difficult one; this is due to the
fact that it contains many frequent nouns and noun
phrases that are not aspect terms; it also contains
more multi-word aspect terms (Fig. 2).
H&L performs much better than FREQ in all
three domains, and our additional pruning (W2V)
improves H&L in all three domains. By contrast
FREQ benefits from W2V only in the restaurant re-
views (but to a smaller degree than H&L), it bene-
fits only marginally in the hotel reviews, and in the
laptop reviews FREQ+W2V performs worse than
FREQ. A possible explanation is that the list of
candidate (distinct) aspect terms that FREQ pro-
duces already misses many aspect terms in the ho-
tel and laptop datasets; hence, W2V, which can
only prune aspect terms, cannot improve the re-
sults much, and in the case of laptops W2V has a
negative effect, because it prunes several correct
candidate aspect terms. All differences between
AWP scores on the same dataset are statistically
significant; we use stratified approximate random-
ization, which indicates p ? 0.01 in all cases.
13
Figure 3 shows the weighted precision and
weighted recall curves of the four methods. In
the restaurants dataset, our pruning improves
13
See http://masanjin.net/sigtest.pdf.
Method Restaurants Hotels Laptops
FREQ 43.40 30.11 9.09
FREQ+W2V 45.17 30.54 7.18
H&L 52.23 49.73 34.34
H&L+W2V 66.80 53.37 38.93
Table 2: Average weighted precision results (%).
the weighted precision of both H&L and FREQ;
by contrast it does not improve weighted re-
call, since it can only prune candidate as-
pect terms. The maximum weighted precision
of FREQ+W2V is almost as good as that of
H&L+W2V, but H&L+W2V (and H&L) reach
much higher weighted recall scores. In the hotel
reviews, W2V again improves the weighted pre-
cision of both H&L and FREQ, but to a smaller
extent; again W2V does not improve weighted re-
call; also, H&L and H&L+W2V again reach higher
weighted recall scores. In the laptop reviews,
W2V marginally improves the weighted precision
of H&L, but it lowers the weighted precision of
FREQ; again H&L and H&L+W2V reach higher
weighted recall scores. Overall, Fig. 3 confirms
that H&L+W2V is the best method.
6 Conclusions
We constructed and made publicly available three
new ATE datasets from three domains. We also
introduced weighted variants of precision, recall,
and average precision, arguing that they are more
appropriate for ATE. Finally, we discussed how
a popular unsupervised ATE method can be im-
proved by adding a new pruning mechanism that
uses continuous space vector representations of
words and phrases. Using our datasets and eval-
uation measures, we showed that the improved
method performs clearly better than the origi-
nal one, also outperforming a simpler frequency-
based baseline with or without our pruning.
51
References
A. Bagheri, M. Saraee, and F. Jong. 2013. An unsuper-
vised aspect detection model for sentiment analysis
of reviews. In Proceedings of NLDB, volume 7934,
pages 140?151.
J. Blitzer, M. Dredze, and F. Pereira. 2007. Biogra-
phies, Bollywood, boom-boxes and blenders: Do-
main adaptation for sentiment classification. In Pro-
ceedings of ACL, pages 440?447, Prague, Czech Re-
public.
X. Ding, B. Liu, and P. S. Yu. 2008. A holistic lexicon-
based approach to opinion mining. In Proceedings
of WSDM, pages 231?240, Palo Alto, CA, USA.
G. Ganu, N. Elhadad, and A. Marian. 2009. Beyond
the stars: Improving rating predictions using review
text content. In Proceedings of WebDB, Providence,
RI, USA.
M. Hu and B. Liu. 2004. Mining and summarizing
customer reviews. In Proceedings of KDD, pages
168?177, Seattle, WA, USA.
Kalervo J?arvelin and Jaana Kek?al?ainen. 2002. Cumu-
lated gain-based evaluation of IR techniques. ACM
Transactions on Information Systems, 20(4):422?
446.
S.-M. Kim and E. Hovy. 2006. Extracting opinions,
opinion holders, and topics expressed in online news
media text. In Proceedings of SST, pages 1?8, Syd-
ney, Australia.
N. Kobayashi, K. Inui, and Y. Matsumoto. 2007. Ex-
tracting aspect-evaluation and aspect-of relations in
opinion mining. In Proceedings of EMNLP-CoNLL,
pages 1065?1074, Prague, Czech Republic.
B. Liu, M. Hu, and J. Cheng. 2005. Opinion ob-
server: analyzing and comparing opinions on the
web. In Proceedings of WWW, pages 342?351,
Chiba, Japan.
B. Liu. 2012. Sentiment Analysis and Opinion Mining.
Synthesis Lectures on Human Language Technolo-
gies. Morgan & Claypool.
C. Long, J. Zhang, and X. Zhut. 2010. A review se-
lection approach for accurate feature rating estima-
tion. In Proceedings of COLING, pages 766?774,
Beijing, China.
C. D. Manning, P. Raghavan, and H. Sch?utze. 2008.
Introduction to Information Retrieval. Cambridge
University Press.
T. Mikolov, K. Chen, G. Corrado, and J. Dean. 2013a.
Efficient estimation of word representations in vec-
tor space. In Proceedings of Workshop at ICLR.
T. Mikolov, I. Sutskever, K. Chen, G. Corrado, and
J. Dean. 2013b. Distributed representations of
words and phrases and their compositionality. In
Proceedings of NIPS.
T. Mikolov, W.-T. Yih, and G. Zweig. 2013c. Linguis-
tic regularities in continuous space word representa-
tions. In Proceedings of NAACL HLT.
S. Moghaddam and M. Ester. 2010. Opinion digger:
an unsupervised opinion miner from unstructured
product reviews. In Proceedings of CIKM, pages
1825?1828, Toronto, ON, Canada.
B. Pang and L. Lee. 2005. Seeing stars: exploit-
ing class relationships for sentiment categorization
with respect to rating scales. In Proceedings of ACL,
pages 115?124, Ann Arbor, MI, USA.
Ana-Maria Popescu and Oren Etzioni. 2005. Extract-
ing product features and opinions from reviews. In
Proceedings of HLT-EMNLP, pages 339?346, Van-
couver, Canada.
T. Sakai. 2004. Ranking the NTCIR systems based
on multigrade relevance. In Proceedings of AIRS,
pages 251?262, Beijing, China.
B. Snyder and R. Barzilay. 2007. Multiple aspect rank-
ing using the good grief algorithm. In Proceedings
of NAACL, pages 300?307, Rochester, NY, USA.
M. Tsytsarau and T. Palpanas. 2012. Survey on min-
ing subjective data on the web. Data Mining and
Knowledge Discovery, 24(3):478?514.
C.-P. Wei, Y.-M. Chen, C.-S. Yang, and C. C Yang.
2010. Understanding what concerns consumers:
a semantic approach to product feature extraction
from consumer reviews. Information Systems and
E-Business Management, 8(2):149?167.
J. Yu, Z. Zha, M. Wang, and T. Chua. 2011. As-
pect ranking: identifying important product aspects
from online consumer reviews. In Proceedings of
NAACL, pages 1496?1505, Portland, OR, USA.
Z. Zhai, B. Liu, H. Xu, and P. Jia. 2010. Group-
ing product features using semi-supervised learning
with soft-constraints. In Proceedings of COLING,
pages 1272?1280, Beijing, China.
Z. Zhai, B. Liu, H. Xu, and P. Jia. 2011. Clustering
product features for opinion mining. In Proceedings
of WSDM, pages 347?354, Hong Kong, China.
52
