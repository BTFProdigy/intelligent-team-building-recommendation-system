Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 1103?1113, Dublin, Ireland, August 23-29 2014.
A Dependency Edge-based Transfer Model for Statistical Machine
Translation
Hongshen Chen
?
? Jun Xie
?
Fandong Meng
?
? Wenbin Jiang
?
Qun Liu
??
?Key Laboratory of Intelligent Information Processing
Institute of Computing Technology, Chinese Academy of Sciences
?University of Chinese Academy of Sciences
{chenhongshen,xiejun,mengfandong,jiangwenbin}@ict.ac.cn
?CNGL, School of Computing, Dublin City University
qliu@computing.dcu.ie
Abstract
Previous models in syntax-based statistical machine translation usually resort to some kinds
of synchronous procedures, few of these works are based on the analysis-transfer-generation
methodology. In this paper, we present a statistical implementation of the analysis-transfer-
generation methodology in rule-based translation. The procedures of syntax analysis, syntax
transfer and language generation are modeled independently in order to break the synchronous
constraint, resorting to dependency structures with dependency edges as atomic manipulating
units. Large-scale experiments on Chinese to English translation show that our model exhibits
state-of-the-art performance by significantly outperforming the phrase-based model. The statis-
tical transfer-generation method results in significantly better performance with much smaller
models.
1 Introduction
Researches in statistical machine translation have been flourishing in recent years. Statistical translation
methods can be divided into word-based (Brown et al., 1993), phrase-based (Marcu and Wong, 2002;
Koehn et al., 2003) and syntax-based models (Yamada and Knight, 2001; Graehl and Knight, 2004;
Chiang, 2005; Liu et al., 2006; Mi et al., 2008; Huang et al., 2006; Lin, 2004; Ding and Palmer, 2004;
Quirk et al., 2005; Shen et al., 2008; Xie et al., 2011; Meng et al., 2013). Compared with word-based and
phrase-based methods, syntax-based models perform better in long distance reordering and enjoy higher
generalization capability by leveraging the hierarchical structures in natural languages, and achieve the
state-of-the-art performance in these years.
Most syntax-based models (except for Lin (2004) ) utilize some kinds of synchronous generation
procedures which directly model the structural correspondence between two languages. In contrast,
the analysis-transfer-generation methodology in rule-based translation solves the machine translation
problem in a more divided scheme, where the processing procedures of analysis, structural transfer and
language generation are modeled separately. The analysis-transfer-generation strategy can tolerate higher
non-isomorphism between languages if with a more general transformation unit and it can facilitate
elaborating engineering of each processing procedure, however, there isn?t a statistical transfer model
that shows the comparable performance with the current state-of-the-art SMT model so far.
In this paper, we propose a novel statistical analysis-transfer-generation model for machine transla-
tion, to integrate the advantages of the transfer-generation scheme and the statistical modeling. The
procedures of transfer and generation are modeled on dependency structures with dependency edges
as atomic manipulating units. First, the source sentence is parsed by a dependency parser. Then, the
source dependency structure is transferred into a target structure by translation rules, which composed
of the source and target edges. Last, the target sentence is finally generated from the target edges which
are used as intermediate syntactic structures. By directly modeling the edge, the most basic unit in the
dependency tree, which definitely describe the modifying relationship and positional relation between
words, our model alleviates the non-isomorphic problem and shows the flexibility of reordering.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
1103
E
?ob?m?j?nti?nji?n?f?b? ?nqu?nzh?nlu? sh?n?m?n?
f?b?
?ob?m? ji?n?
*:sh?n?m?n?
nsubj advmod
dobj
f?b?
f?b? sh?n?m?n?
?nqu?n
nn
obama 
issue
will
issue
issue
*
sh?n?m?n?
zh?nlu?
nn
a statement of 
security
a statement of 
strategy
f?b?/VV
??
ji?n?/AD
?
sh?n?m?n?/NN
??
?nqu?n/NN
??
zh?nlu?/NN
??
nsubj
advmod
dobj
nn nn
?ob?m?/NN
???
obama today will issue a statement of security strategy
D ? ? ?
? ?
j?nti?n/NT
??
tmod
j?nti?n
tomod
f?b?
today
issue
?
??????????????
Figure 1: (a)An example of labeled Chinese dependency tree aligned with the corresponding English
sentence. (b) Examples of the transfer rules extracted from the tree. ?*? denotes a variable. All the inner
nodes are treated as variables. The label on the target side of a rule denotes whether the head and the
dependent are adjacent or not.
The rest of the paper is organized as follows, we first describe the dependency edge-based transfer
model (Section 2). Then, we present our rule acquisition algorithm (Section 3), the decoding and target
sentence generation process (Section 4). Finally, large-scale experiments (Section 5) on Chinese-to-
English translation show that our edge-based transfer model gains state-of-the-art performance by sig-
nificantly outperforming the phrase-based model (Koehn et al., 2003) by averaged +1.34 BLEU points on
three test sets. To the best of our knowledge, this is the first transfer-generation-based statistical machine
translation model that achieves the state-of-the-art performance.
2 Dependency Edge-based Transfer Model
2.1 Edges in Dependency Trees
Given a sentence, its dependency tree is a directed acyclic graph with words in the sentence as nodes.
An example dependency tree is shown in Figure 1 (a). An edge in the tree represents a dependency
relationship between a pair of words, a head and a dependent. When a nominal dependent acts as a
subject and modifies a verbal head, they usually have a fixed relative position. In Figure 1 (a), ?`aob?am?a?
modifies ?f?ab`u?. The grammatical relation label nsubj (Chang et al., 2009) between them denotes that a
noun phrase acts as the subject of a clause. ?`aob?am?a? is on the left of ?f?ab`u?.
Based on the above observations, we take the edge as the elementary structure of a dependency tree
and regard a dependency tree to be a set of edges.
Definition 1. An source side edge is a 4-tuple e = ?H,D,P,R?, where H is the head, D is the depen-
dent, P denotes the relative position between H and D, left or right, R is the grammatical relation label
.
In Figure 1 (b), the upper sides of transfer rules are source side edges extracted from the dependency
tree.
1104
f?b?/VV
ji?n?/AD sh?n?m?n?/NN
nsubj
advmod
dobj
?ob?m?/NN j?nti?n/NT
tmod
Transfer
select the left
adjacent edge
? ? ? ?
extend to the left
will
ad
jac
en
t
obama
issue
non
-ad
jace
nt
today
no
n-a
dja
cen
t
? ? ?
will
ad
jac
en
t
obama
issue
non
-ad
jace
nt
today
no
n-a
dja
cen
t
?? ?
H1:obama today will issue
H2:today obama will issue
extend to the right
*
adjacent
? ? ? ?
a statement of security strategy
ad
jac
en
t
issue
?
will
will issue
will
ad
jac
en
t
obama
issue
non-
adjac
ent
today
non
-ad
jac
ent
? ? ?
will
ad
jac
en
t
obama
issue
non-
adja
cent
today
non
-ad
jac
ent
?? ?
H1:obama today will issue a statement of security strategy
H2:today obama will issue a statement of security strategy
?
adjacent
?
*
Generation
Analysis
?ob?m? j?nti?n ji?n? f?b? ?nqu?n zh?nlu? sh?n?m?n?
f?b?
?ob?m? ji?n?
nsubj advmod
f?b?
obama
issue
will
non
-ad
jace
nt
issue
adj
ace
nt
j?nti?n
tomod
f?b?
today
issue
non
-ad
jace
nt
*:sh?n?m?n?
dobj
f?b?
issue
*
adjacent
Figure 2: An example partial generation of translation. The same set of rules generate two target hy-
potheses with the same words and different word order. Assume the sub-tree rooted at ?sh?engm??ng? has
been translated to the corresponding target sentence fragment.
2.2 Transfer Rules
A transfer rule of our model represents the reordering and relative positions of edges between language
pairs. For example, in Figure 1 (b), the first rule shows that when a nominal subject modifies a verb, the
target side keeps the same position relations. ?obama? is also on the left of ?issue?, the same with the
source side relative position. The 5-th and 6-th rules show the inversion relations between the source and
the target. Formally, a transfer rule can be defined as a triple ?e, f,??, where e is an edge extracted from
the source dependency tree, f is a target edge. ? denotes one-to-one correspondence between variables
in e and f .
Figure 1 (b) are part of transfer rules extracted from the word aligned sentence in Figure 1 (a). The
target edge denotes whether the target dependent is on the left or the right side of the target head, the
1105
label on the edge indicates whether the target head and the target dependent are adjacent or not. If
the dependent is an internal node(contrast with the leaf nodes in the dependency tree), then it will be
regarded as a substitution node. The dependent in the 4-th transfer rule is an internal node and the its
corresponding target side is a substitution variable.
Figure 2 shows a partial transfer-generation of our model which involves three phases. First, analysis.
Given a source language sentence, we obtain its dependency tree using a dependency parser. We assume
that the sub-tree of the substitution node has been translated. Second, transfer. For each internal node,
we transfer the source side edges between the head and all its dependents into the target sides. In the
second block of Figure 2, we transfer four edges into the target sides. Third, generation, corresponding
to the third block of Figure 2. We generate the target sentence with the target side edges starting from the
target head, ?issue?. We first try to concatenate the edges to the left. First, we select a target side edge
that is on the left side of ?issue? and adjacent to it to form a consecutive phrase. Edge 3 is selected and ?to
issue? is generated. Then, we enumerate all possible left concatenations of the other edges that are not
adjacent to ?issue?. The two sequences(1,2,3 and 2,1,3) of the edges are generated, corresponding to the
two hypotheses. After that, we extend the two hypotheses to the right. The internal node ?sh?engm??ng? is
a substitution node, so the candidate translation of the sub-tree rooted at ?sh?engm??ng? is concatenated to
the two hypotheses. Finally, we generate the two candidate translations of the input sentence.
3 Acquisition of Transfer Rules
Transfer rules can be extracted automatically from a word-aligned corpus, which is a set of triples
?T, S,A?, where T is a source dependency tree, S is a target side sentence and A is an alignment relation
between T and S. Following the dependency-to-string model (Xie et al., 2011), we extract transfer rules
from each triple ?T, S,A? by three steps:
1. Tree Annotation: Label each node in the dependency tree with the alignment information
2. Edges Identification: Identify acceptable edges from the annotated dependency tree
3. Rule induction: Induce a set of lexicalized and un-lexicalized transfer rules from the acceptable
edges.
3.1 Tree Annotation
Given a triple ?T, S,A? as Figure 3 shows, we define two attributes for every node in T: node span and
sub-tree span:
Definition 2. Given a node n, its node span nsp(n) is a set of consecutive indexes of the target words
aligned with the node n.
For example, nsp(?anqu?an)={7-8}, which corresponds to the target word ?of? and ?security?.
Definition 3. A node span nsp(n) is consistent if for any other node n
?
in the dependency tree, nsp(n)
and nsp(n
?
) are not overlapping.
For example, nsp(zh`anlu`e) is consistent, while nsp(?anqu?an) is not consistent for it corresponds to the
same word ?of? with nsp(sh?engm??ng).
Definition 4. Given a sub-tree T
?
rooted at n, the sub-tree span tsp(n) of n is a consecutive target word
indexes from the lower bound of the nsp of all the nodes in T
?
to the upper bound of those spans.
For example, tsp(sh?engm??ng)={5-9},which corresponds to the target phrase ?a statement of security
strategy?.
Definition 5. A sub-tree span tsp(n) is consistent if for any other node n
?
that is not in the sub-tree
rooted at n in the dependency tree, tsp(n) and nsp(n
?
) are not overlapping.
For example, tsp(sh?engm??ng) is consistent, even though nsp(sh?engm??ng) is not consistent, while
tsp(?anqu?an) is not consistent for ?sh?engm??ng? is not a node in sub-tree rooted at ??anqu?an? and ??anqu?an?
corresponds to the same word ?of ? with nsp(sh?engm??ng) .
1106
f?b?/VV
ji?n?/AD sh?n?m?n?/NN
?nqu?n/NN zh?nlu?/NN
nsubj advmod dobj
nn nn
?ob?m?/NN
obama today will issue a statement of security strategy
j?nti?n/NT
tmod
1 2 3 4 5 6 7 8 9
{1-1}/{1-1} {2-2}/{2-2} {3-3}/{3-3}
{4-4}/{1-9}
{5-7}/{5-9}
{7-8}/{7-8} {9-9}/{9-9}
Figure 3: An example of annotated dependency tree. Each node is annotated with two spans, the former
is node span and the latter is sub-tree span. The gray edge is not acceptable. It is different from Figure
1, because ??anqu?an? aligned with two words in Figure 3. ?of? in the target side is aligned with both
??anqu?an? and ?sh?engm??ng? which makes the gray edge un-acceptable.
3.2 Acceptable Edges Identification
We identify the edges from the annotated dependency tree that are acceptable for rule induction.
For an acceptable edge, its node span of the head nsp(head) and the sub-tree span of the dependent
tsp(dependent) satisfy the following properties:
1. nsp(head) and tsp(dependent) are consistent.
2. nsp(head) and tsp(dependent) are non-overlapping.
For example, tsp(?anqu?an) and nsp(sh?engm??ng) are neither consistent nor non-overlapping. So the
gray edge between head ?sh?engm??ng? and dependent ??anqu?an? is not an acceptable edge. nsp(f?ab`u)
and tsp(sh?engm??ng) are consistent and the two spans are non-overlapping. Thus, the edge between head
?f?ab`u? and dependent ?sh?engm??ng? is an acceptable edge.
3.3 Transfer Rule Induction
From each acceptable source side edge, we induce a set of lexicalized and un-lexicalized transfer rules.
We induce a lexicalized transfer rule from an acceptable edge by the following procedures:
1. extract the source side edge and mark the internal nodes as substitution sites. This form the input of
a transfer rule.
2. extract the position information according to nsp(head) and tsp(dependent), whether they are adja-
cent or not and whether tsp(dependent) is on the left side or the right side of nsp(head).
In Figure 4, the first transfer rule is lexicalized rule, it is induced from the edge between ?f?ab`u? and
?`aob?am?a?.
In addition to the lexicalized rules described above, we also generalized the rules by replacing the
word in an source side edge with a wild card and the part of speech of the word. For example, the rule
in Figure 4 can be generalized in two ways. The generalized versions of the rule apply to ?`aob?am?a?
modifying any verb and ?f?ab`u? modifying any noun, respectively. The generalized rules are also called
1107
Generalize head
?ob?m?
nsubj
f?b?
obama
issue
non
-adj
acen
t
?ob?m?
nsubj
*:VV
obama
*
non-
adja
cent
Generalize
dependent
*:NN
nsubj
f?b?
*
issue
non-a
djace
nt
?
?
?
Figure 4: Generalization of transfer rule.
un-lexicalized rules for the loss of word information. The single node translations of the generalized
words are also extracted.
The unaligned words of the target side is handled by extending nsp(head) and tsp(dependent) on both
left and right directions. We do this process similar with the method of Och and Ney (2004). We might
obtain m(m ? 1) extended rules from an acceptable edge. The frequency of each rule is divided by m.
We take the extracted rule set as observed data and make use of relative frequency estimator to obtain
the translation probabilities P (t|s) and P (s|t).
4 Decoding and Generation
We follow Och and Ney (2002), using a general log-linear model to score the sentence generated by each
concatenation of the target edges. Let c be concatenations that concatenate the target edges to generate
the target sentence e. The probability of e is defined as?
P (c) ?
?
i
?
i
(c)
?
i
(1)
where ?
i
(c) are features defined on concatenations and ?
i
are feature weights. In our experiments of
this paper, thirteen features are used as follows:
? Transfer rules translation probabilities P (t|s) and P (s|t), and lexical translation probabilities
P
lex
(t|s) and P
lex
(s|t);
? Bilingual phrases probabilities P
bp
(t|s) and P
bp
(s|t), and bilingual phrases lexical translation prob-
abilities P
bplex
(t|s) and P
bplex
(s|t);
? Transfer rule penalty exp(?1);
? Bilingual phrase penalty exp(?1);
? Pseudo translation rule penalty exp(?1);
? Target word penalty exp(|e|);
? Language model P
lm
(e).
Our decoder is based on a bottom-up chart-based beam-search algorithm. We regard the decoding
process as the composition of the target side edges. For a given source language sentence, we obtain its
1108
f?b?
j?nti?n?ob?m?
obama today to issue
f?b?
sh?n?m?n?
?nqu?n zh?nlu?
issue a statement of security strategy
ji?n?
Figure 5: Two examples of the phrases incorporated in our model.
dependency tree T with an external dependency parser. Each node in T is traversed in post-order. For
each internal node and root node n, we do the transfer-generation translation as the following procedures:
1. Extract all the source side edges including the lexicalized and generalized edges between n and all
its dependents using the same way we extract the source side edges of the transfer rules.
2. Transfer the source side edges into target side edges. For a generalized rule, we restore it to a lex-
icalized rule by combining it with the single word translation. For no matched edges, we construct
the pseudo translation rule according to the word order of the source head-dependent relation.
3. Generate the target sentence by bi-directional extension from an adjacent target edge. We first
group all the target edges by their heads. For each group, we generate translation hypotheses with
the following procedures:
(a) Select an adjacent target edge as the starting position;
(b) Extend to the left side and enumerate all possible permutations of the target edges directing
left;
(c) Extend to the right side and enumerate all possible permutations of the target edges directing
right.
Considering that in dependency trees, a head may relate to more than 4 edges which results in
massive search space. We reduce the time complexity by using the maximum distortion limit. The
distortion is defined as (a
i
? b
i?1
? 1), where a
i
denotes the start position of the source side edge
that is translated into the ith target side edge and b
i?1
denotes the end position of the source side
edge translated into the (i ? 1)th target side edge.
When we reach the root node, the candidate translations of the input sentence are generated.
In our model, only the adjacent target edge of a transfer rule can be regarded as a consecutive phrase
and its corresponding source side length is only 2. As we start extending the target sentence from
the target head, it is quite natural to incorporate the bilingual phrases to make the target sentences be
extended from the phrases as well as the single target head word. Due to the flexibility of our model,
we can incorporate not only the syntactic phrases which are phrases covering a whole sub-tree, but also
the non-syntactic phrases as the fixed dependency structures in Shen et al. (2008) which are consecutive
phrases covering the head. Figure 5 shows two examples of the phrases incorporated in our model.
We prune the search space in several ways. First, beam threshold ?, items with a score worse than ?
times of the best score in the same span will be discarded; second, beam size b, items with a score worse
than the bth best item will be discarded. For our experiments, we set ? = 10
?3
and b = 300; Third,
we also prune rules for the same edge with a fixed rule limit (r = 200), which denotes the maximum
number of rules we keep.
5 Experiments
In this section, the performance of our model is evaluated by comparing with phrase-based model (Koehn
et al., 2003), on the NIST Chinese-to-English translation tasks. We also present the influence of the
1109
mt02-tune mt03 mt04 mt05
0 30.81 30.03 32.44 30.09
1 33.4 32.07 34.55 31.77
2 34.39 32.7 35.4 32.59
3 34.04 32.69 35.46 32.54
4 33.59 31.75 35.15 32.17
30
31
32
33
34
35
36
0 1 2 3 4
maximum distortion limit
BL
EU
(%
)
mt02-tune
mt03
mt04
mt05
Figure 6: Effect of different maximum distortion limits on development
set (mt02) and three tests(mt03,04,05). The performance of all the sets
are consistent.
maximum distortion limit to our model. We take open source phrase-based system Moses (with default
configuration)
1
as our baseline system.
5.1 Experimental Setting
Our training corpus consists of 1.25M sentence pairs from LDC data, including LDC2002E18, LD-
C2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06.
To obtain the dependency trees of the source side, we parse the source sentences with Stanford Parser
(Klein and Manning, 2003) into projective dependency structures with nodes annotated by POS tags and
edges by dependency labels.
To obtain the word alignments, we run GIZA++ (Och and Ney, 2003) on the corpus in both directions
and apply ?grow-diag-and? refinement (Koehn et al., 2003). We extract the phrases covering no more
than 10 nodes of the fixed structures.
We use SRILM (Stolcke, 2002) to train a 4-gram language model with modified Kneser-Ney smooth-
ing on the Xinhua portion of the Gigaword corpus.
We use NISTMTEvaluation test set 2002 as our development set, 2003-2005 NIST datasets as testsets.
The quality of translations is evaluated by the case insensitive NIST BLEU-4 metric
2
.
We make use of the minimum error rate training algorithm (Och, 2003) in order to maximize the
BLEU score of the development set.
The statistical significance test is performed by sign-test (Collins et al., 2005).
5.2 Influence of Maximum Distortion Limit
Figure 6 gives the performance of our system with different maximum distortion limits in terms of
uncased BLEU of three NIST test sets. The performance of different distortion limit are consistent on
both development set and three test sets. Maximum distortion limit 2 gets the best performances. A low
distortion limit may cause the target sentence been translated more close to the sequence of the source,
especially when the distortion limit equals to 0, none of the reordering is allowed, while a high distortion
limit may lead the good translations be flooded by too many ambiguities when enumerating the possible
sequences of the target non-adjacent dependents. We choose 2 as the maximum distortion limit in the
next experiments.
1
http://www.statmt.org/moses/
2
ftp://jaguar.ncsl.nist.glv/mt/resources/mteval-v11b.pl.
1110
System Rule # MT03 MT04 MT05 Average
Moses 44.49M 32.03 32.83 31.81 32.22
DEBT 30.7M 32.7* 35.4* 32.59* 33.56
Table 1: Statistics of the extracted rules on training data and the BLEU scores (%) on the test sets.
?DEBT? denotes our edge-based transfer model. The ?*? denotes that the results are significantly better
than the baseline system (p<0.01).
5.3 Performance of Our Model
Tabel 1 illustrates the translation results of our experiments. We (DEBT) surpass the baseline over +1.34
BLEU points on average. Our model significant outperforms the baseline phrase-based model, with
p < 0.01 on statistical significance test sign-test (Collins et al., 2005).
We also list the statistical number of rules extracted from the training corpus. The number of our
transfer rules is only 69.0% of the rules extracted by Moses, thus, the total rules in our model is 31%
smaller than Moses.
6 Related Work
Transfer-based MT systems usually take a parse tree in the source language and translate it into a parse
tree in the target language with transfer rules. Both our model and some of those previous works ac-
quired transfer rules automatically from word-aligned corpus (Richardson et al., 2001; Carbonell et al.,
2002; Lavoie et al., 2002; Lin, 2004). Gimpel and Smith (2009) and Gimpel and Smith (2014) used
quasi-synchronous dependency grammar for MT and they are similar to our idea of doing transfer of
dependency syntax in a non-synchronous setting. They do the translation as monolingual lattice parsing.
As dependency-based system, Lin (2004) used path as the transfer unit and regarded the translation
problem with minimal path covering. Quirk et al. (2005) and Xiong et al. (2007) used treelets to model
the source dependency tree using synchronous grammars. Quirk et al. (2005) projected the source depen-
dency structure into target side by word alignment and faced the problem of non-isomorphism between
languages. Xiong et al. (2007) directly modeled the treelet to the corresponding target string to alleviate
the problem. Xie et al. (2011) directly specified the ordering information in head-dependents rules that
represent the source side as head-dependents relations and the target side as string.
Differently, our model uses a much simpler elementary structure, edge, which consist of only a head
and a dependent. As a transfer-generation model, we transfer an edge in the source dependency tree into
target side and incorporate the position information on the target edge , which alleviate non-isomorphism
problem and incorporate ordering among different target edges simultaneously. Moreover, our decoding
method is quite different from previous dependency tree-based works. After parsing a given source
language sentence, we transfer and generate the target sentence fragments recursively on each internal
node of the dependency tree bottom-up.
7 Conclusions and Future Work
In this paper, we present a novel dependency edge-based transfer model using dependency trees on the
source side for machine translation. We directly transfer the edges in source dependency tree into the
target sides and then generate the target sentences by beam-search. With the concise transfer rules,
our model is compatible with both the syntactic and non-syntactic phrases. Although the generation
process of our model seems relatively simple, it still exhibits a good performance and outperforms the
phrase-based model on large scale experiments. For the first time, a statistical transfer model shows a
comparable performance with the state-of-the-art translation models.
Since the translation procedure is divided into three phases and each phase can be modeled indepen-
dently, we would like to take further steps focusing on modeling the target language generation process
specifically to ensure a better grammatical translation with the help of natural language generation meth-
ods.
1111
Acknowledgments
The authors were supported by National Key Technology R&D Program (No. 2012BAH39B03), CAS
Action Plan for the Development of Western China (No. KGZD-EW-501), and Sino-Thai Scientific and
Technical Cooperation (No. 60-625J). Sincere thanks to the anonymous reviewers for their thorough
reviewing and valuable suggestions.
References
Peter F Brown, Vincent J Della Pietra, Stephen A Della Pietra, and Robert L Mercer. 1993. The mathematics of
statistical machine translation: Parameter estimation. Computational linguistics, 19(2):263?311.
Jaime Carbonell, Katharina Probst, Erik Peterson, Christian Monson, Alon Lavie, Ralf Brown, and Lori Levin.
2002. Automatic rule learning for resource-limited mt. In Machine Translation: From Research to Real Users,
pages 1?10. Springer.
Pi-Chuan Chang, Huihsin Tseng, Dan Jurafsky, and Christopher D Manning. 2009. Discriminative reordering
with chinese grammatical relations features. In Proceedings of the Third Workshop on Syntax and Structure in
Statistical Translation, pages 51?59. Association for Computational Linguistics.
David Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proceedings
of the 43rd Annual Meeting on Association for Computational Linguistics, pages 263?270. Association for
Computational Linguistics.
Michael Collins, Philipp Koehn, and Ivona Ku?cerov?a. 2005. Clause restructuring for statistical machine transla-
tion. In Proceedings of the 43rd annual meeting on association for computational linguistics, pages 531?540.
Association for Computational Linguistics.
Yuan Ding and Martha Palmer. 2004. Synchronous dependency insertion grammars: A grammar formalism for
syntax based statistical mt. In Workshop on Recent Advances in Dependency Grammars (COLING), pages
90?97.
Kevin Gimpel and Noah A Smith. 2009. Feature-rich translation by quasi-synchronous lattice parsing. In Pro-
ceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 1-Volume 1,
pages 219?228. Association for Computational Linguistics.
Kevin Gimpel and Noah A Smith. 2014. Phrase dependency machine translation with quasi-synchronous tree-to-
tree features. Computational Linguistics.
Jonathan Graehl and Kevin Knight. 2004. Training tree transducers. In Daniel Marcu Susan Dumais and Salim
Roukos, editors, HLT-NAACL 2004: Main Proceedings, pages 105?112, Boston, Massachusetts, USA, May 2 -
May 7. Association for Computational Linguistics.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006. Statistical syntax-directed translation with extended domain
of locality. In Proceedings of AMTA, pages 66?73.
Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings
of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on
Human Language Technology - Volume 1, NAACL ?03, pages 48?54, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Benoit Lavoie, Michael White, and Tanya Korelsky. 2002. Learning domain-specific transfer rules: an experiment
with korean to english translation. In Proceedings of the 2002 COLING workshop on Machine translation in
Asia-Volume 16, pages 1?7. Association for Computational Linguistics.
Dekang Lin. 2004. A path-based transfer model for machine translation. In Proceedings of Coling 2004, pages
625?630, Geneva, Switzerland, Aug 23?Aug 27. COLING.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-string alignment template for statistical machine translation.
In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting
of the Association for Computational Linguistics, pages 609?616. Association for Computational Linguistics.
Daniel Marcu and William Wong. 2002. A phrase-based, joint probability model for statistical machine transla-
tion. In Proceedings of the ACL-02 conference on Empirical methods in natural language processing-Volume
10, pages 133?139. Association for Computational Linguistics.
1112
Fandong Meng, Jun Xie, Linfeng Song, Yajuan L?u, and Qun Liu. 2013. Translation with source constituency
and dependency trees. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language
Processing, pages 1066?1076, Seattle, Washington, USA, October. Association for Computational Linguistics.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-based translation. In Proceedings of ACL-08: HLT, pages
192?199, Columbus, Ohio, June. Association for Computational Linguistics.
Franz Josef Och and Hermann Ney. 2002. Discriminative training and maximum entropy models for statistical
machine translation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics,
ACL ?02, pages 295?302, Stroudsburg, PA, USA. Association for Computational Linguistics.
Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models.
Computational linguistics, 29(1):19?51.
Franz Josef Och and Hermann Ney. 2004. The alignment template approach to statistical machine translation.
Computational linguistics, 30(4):417?449.
Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the
41st Annual Meeting on Association for Computational Linguistics-Volume 1, pages 160?167. Association for
Computational Linguistics.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. Dependency treelet translation: Syntactically informed
phrasal SMT. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics
(ACL?05), pages 271?279, Ann Arbor, Michigan, June. Association for Computational Linguistics.
Stephen Richardson, William Dolan, Arul Menezes, and Jessie Pinkham. 2001. Achieving commercial-quality
translation with example-based methods. In Proceedings of MT Summit VIII, pages 293?298. Santiago De
Compostela, Spain.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A new string-to-dependency machine translation algorithm
with a target dependency language model. In Proceedings of ACL-08: HLT, pages 577?585, Columbus, Ohio,
June. Association for Computational Linguistics.
Andreas Stolcke. 2002. Srilm-an extensible language modeling toolkit. In Proceedings of ICSLP, volume 30,
pages 901?904.
Jun Xie, Haitao Mi, and Qun Liu. 2011. A novel dependency-to-string model for statistical machine translation.
In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ?11, pages
216?226, Stroudsburg, PA, USA. Association for Computational Linguistics.
Deyi Xiong, Qun Liu, and Shouxun Lin. 2007. A dependency treelet string correspondence model for statistical
machine translation. In Proceedings of the Second Workshop on Statistical Machine Translation, StatMT ?07,
pages 40?47, Stroudsburg, PA, USA. Association for Computational Linguistics.
Kenji Yamada and Kevin Knight. 2001. A syntax-based statistical translation model. In Proceedings of the 39th
Annual Meeting on Association for Computational Linguistics, pages 523?530. Association for Computational
Linguistics.
1113
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 2042?2051, Dublin, Ireland, August 23-29 2014.
RED: A Reference Dependency Based MT Evaluation Metric
Hui Yu
??
Xiaofeng Wu
?
Jun Xie
?
Wenbin Jiang
?
Qun Liu
??
Shouxun Lin
?
?
Key Laboratory of Intelligent Information Processing
Institute of Computing Technology, Chinese Academy of Sciences
?
University of Chinese Academy of Sciences
{yuhui,xiejun,jiangwenbin,sxlin}@ict.ac.cn
?
CNGL, School of Computing, Dublin City University
{xiaofengwu,qliu}@computing.dcu.ie
Abstract
Most of the widely-used automatic evaluation metrics consider only the local fragments of the
references and translations, and they ignore the evaluation on the syntax level. Current syntax-
based evaluation metrics try to introduce syntax information but suffer from the poor pars-
ing results of the noisy machine translations. To alleviate this problem, we propose a novel
dependency-based evaluation metric which only employs the dependency information of the ref-
erences. We use two kinds of reference dependency structures: headword chain to capture the
long distance dependency information, and fixed and floating structures to capture the local con-
tinuous ngram. Experiment results show that our metric achieves higher correlations with human
judgments than BLEU, TER and HWCM on WMT 2012 and WMT 2013. By introducing extra
linguistic resources and tuning parameters, the new metric gets the state-of-the-art performance
which is better than METEOR and SEMPOS on system level, and is comparable with METEOR
on sentence level on WMT 2012 and WMT 2013.
1 Introduction
Automatic machine translation (MT) evaluation plays an important role in the evolution of MT. It not
only evaluates the performance of MT systems, but also makes the development of MT systems rapider
(Och, 2003). According to the type of the employed information, the automatic MT evaluation metrics
can be classified into three categories: lexicon-based metrics, syntax-based metrics and semantic-based
metrics.
The lexicon-based metrics, such as BLEU (Papineni et al., 2002), TER (Snover et al., 2006), METEOR
(Lavie and Agarwal, 2007) and AMBER (Chen and Kuhn, 2011; Chen et al., 2012), are good at capturing
the lexicon or phrase level information, e.g. fixed phrases or idioms. But they cannot adequately reflect
the syntax similarity. Current efforts in syntax-based metrics, such as the headword chain based metric
(HWCM) (Liu and Gildea, 2005), the LFG dependency tree based metric (Owczarzak et al., 2007) and
syntactic/semantic-role overlap (Gim?enez and M`arquez, 2007) , suffer from the parsing of the potentially
noisy machine translations, so the improvement of their performance is restricted due to the serious
parsing errors. Semantic-based metrics, such as MEANT (Lo et al., 2012; Lo and Wu, 2013), have the
similar problem that the accuracy of semantic role labeling (SRL) can also drop due to the errors in
translations. To avoid the parsing of potentially noisy translations, the CCG based metric (Mehay and
Brew, 2007) only uses the parsing result of reference and employs 2-gram dependents, but it did not
achieve the state-of-the-art performance.
In this paper, we propose a novel dependency tree based MT evaluation metric. The new metric only
employs the reference dependency tree, leaving the translation unparsed to avoid the error propagation.
We use two kinds of reference dependency structures in our metric. One is the headword chain (Liu and
Gildea, 2005) which can capture long distance dependency information. The other is fixed and floating
structure (Shen et al., 2010) which can capture local continuous ngram. When calculating the matching
score between the headword chain and the translation, we use a distance-based similarity. Experiment
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings
footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
2042
results show that our metric achieves higher correlations with human judgments than BLEU, TER and
HWCM on WMT 2012 and WMT 2013. After introducing extra resources and tuning parameters on
WMT 2010, the new metric is better than METEOR and SEMPOS on system level and comparable with
METEOR on sentence level on WMT 2012 and WMT2013.
The remainder of this paper is organized as follows. Section 2 describes our new reference dependency
based MT evaluation metric. In Section 3, we introduce some extra resources to this new metric. Section
4 presents the parameter tuning for the new metric. Section 5 gives the experiment results. Conclusions
and future work are discussed in Section 6.
2 RED: A Reference Dependency Based MT Evaluation Metric
The new metric is a REference Dependency based automatic evaluation metric, so we name it RED.
We present the new metric detailedly in this section. The description of dependency ngrams is given in
Section 2.1. The method to score the dependency ngram is presented in Section 2.2. At last, the method
of calculating the final score is introduced in Section 2.3.
2.1 Two Kinds of Dependency Ngrams
To capture both the long distance dependency information and the local continuous ngrams, we use both
the headword chain and the fixed-floating structures in our new metric, which correspond to the two
kinds of dependency ngram (dep-ngram), headword chain ngram and fixed-floating ngram.
Figure 1: An example of dependency tree.
Figure 2: Different kinds of structures extracted
from the dependency tree in Figure 1. (a): Head-
word chain. (b): Fixed structure. (c): Floating struc-
ture.
2.1.1 Headword chain
Headword chain is a sequence of words which corresponds to a path in the dependency tree (Liu and
Gildea, 2005). For example, Figure 2(a) is a 3-word headword chain extracted from the dependency tree
in Figure 1. Headword chain can represent the long distance dependency information, but cannot capture
most of the continuous ngrams. In our metric, headword chain corresponds to the headword chain ngram
in which the positions of the words are considered. So the form of headword chain ngram is expressed
as (w1
pos1
, w2
pos2
, ..., wn
posn
), where n is the length of the headword chain ngram. For example, the
headword chain in Figure 2(a) is expressed as (saw
2
, with
5
,magnifier
7
).
2.1.2 Fixed and floating structures
Fixed and floating structures are defined in Shen et al. (2010). Fixed structures consist of a sub-root with
children, each of which must be a complete constituent. They are called fixed dependency structures
because the head is known or fixed. For example, Figure 2(b) shows a fixed structure. Floating structures
consist of a number of consecutive sibling nodes of a common head, but the head itself is unspecified.
Each of the siblings must be a complete constituent. Figure 2(c) shows a floating structure. Fixed-
floating structures correspond to fixed-floating ngrams in our metric. Fixed-floating ngrams don?t need
the position information, and can be simply expressed as (w1, w2, ..., wn), where n is the length of the
2043
Figure 3: An example of calculating matching score for a headword chain ngram
(saw
2
, with
5
,magnifier
7
). dis r
1
and dis r
2
are the distances between the corresponding two
words in the reference. dis h
1
and dis h
2
are the distances between the corresponding two words in the
hypothesis.
fixed-floating ngram. For example, the fixed structure in Figure 2(b) and the floating structre in Figure
2(c) can be expressed as (I, saw, an, ant) and (an, ant, with, a,magnifier) respectively.
2.2 Scoring Dep-ngrams
Headword chain ngrams may not be continuous, while fixed-floating ngrams must be continuous. So the
scoring methods of the two kinds of dep-ngrams are different, and we introduce the two scoring methods
in Section 2.2.1 and Section 2.2.2 respectively.
2.2.1 Scoring headword chain ngram
For a headword chain ngram (w1
pos1
, w2
pos2
, ..., wn
posn
), if we can find all these n words in the string
of the translation with the same order as they appear in the reference sentence, we consider it a match and
the matching score is a distance-based similarity which is calculated by the relative distance, otherwise it
is not a match and the score is 0. The matching score is a decimal value between 0 and 1, which is more
suitable than just use integer 0 and 1. For example, if the distance between two words in reference is 1,
but the distance in two different hypotheses are 2 and 5 respectively. It?s more reasonable to score them
0.5 and 0.2 rather than 1 and 0.
The relative distance dis r
i
between every two adjacent words in this kind of dep-ngram is calculated
by Formula (1), where pos
wi
is the position of word wi in the sentence. In Formula (1), we have
1 ? i ? n ? 1 and n is the length of the dep-ngram. Then a vector (dis r
1
, dis r
2
, ..., dis r
n?1
) is
obtained. In the same way, we obtain vector (dis h
1
, dis h
2
, ..., dis h
n?1
) for the translation side.
dis r
i
= |pos
w(i+1)
? pos
wi
| (1)
The matching score p
(d,hyp)
for a headword chain ngram (d) and the translation (hyp) is calculated
according to Formula (2), where n > 1. When the length of the dep-ngram equals 1, the matching score
equals 1 if the translation has the same word, otherwise, the matching score equals 0.
p
(d,hyp)
=
?
?
?
exp(?
?
n?1
i=1
|dis r
i
? dis h
i
|
n? 1
) if match
0 if unmatch
(2)
An example illustrating the calculation of the matching score p
(d,hyp)
is shown in Figure 3. There is
a 3-word headword chain ngram (saw
2
, with
5
,magnifier
7
) in the dependency tree of the reference.
2044
For this dep-3gram, the words are represented with underline in the reference dependency tree and the
reference sentence in Figure 3. We can also find all the same three underlined words in the translation
with the same order as they appear in the reference. Therefore, there is a match for this dep-3gram. To
compute the matching score between this dep-3gram and the translation, we have:
? Calculate the distance
dis r
1
= |pos
with
? pos
saw
| = |5? 2| = 3 dis r
2
= |pos
magnifier
? pos
with
| = |7? 5| = 2
dis h
1
= |pos
with
? pos
saw
| = |5? 2| = 3 dis h
2
= |pos
magnifier
? pos
with
| = |6? 5| = 1
? Get the matching score as Formula (3) according to Formula (2). d denotes
(saw
2
, with
5
,magnifier
7
) and hyp denotes the translation in the example.
p
(d,hyp)
= exp(?
|dis r
1
? dis h
1
|+ |dis r
2
? dis h
2
|
3? 1
) = exp(?
|3? 3|+ |2? 1|
3? 1
) = exp(?0.5)
(3)
We also tried other methods to calculate the matching score, such as the cosine distance and the
absolute distance, but the relative distance performed best. For a headword chain ngram with more than
one matches in the translation, we choose the one with the highest matching score.
2.2.2 Scoring fixed-floating ngram
The words in the fixed-floating ngram are continuous, so we restrict the matched string in the translation
also to being continuous. That means, for a fixed-floating ngram (w1, w2, ..., wn), if we can find all these
n words continuous in the translation with the same order as they appear in the reference, we think the
dep-ngram can match with the translation. The matching score can be obtained by Formula (4), where d
stands for a fixed-floating ngram and hyp stands for the translation.
p
(d,hyp)
=
{
1 if match
0 if unmatch
(4)
2.3 Scoring RED
In the new metric, we use Fscore to obtain the final score. Fscore is calculated by Formula (5), where ?
is a value between 0 and 1.
Fscore =
precision ? recall
? ? precision+ (1? ?) ? recall
(5)
The dep-ngrams of the reference and the string of the translation are used to calculate the precision and
recall. In order to calculate precision, the number of the dep-ngrams in the translation should be given,
but there is no dependency tree for the translation in our method. We know that the number of dep-
ngrams has an approximate linear relationship with the length of the sentence, so we use the length of
the translation to replace the number of the dep-ngrams in the translation dependency tree. Recall can
be calculated directly since we know the number of the dep-ngrams in the reference. The precision and
recall are computed as follows.
precision =
?
d?D
n
p
(d,hyp)
len
h
, recall =
?
d?D
n
p
(d,hyp)
count
n(ref)
D
n
is the set of dep-ngrams with the length of n. len
h
is the length of the translation. count
n(ref)
is the
number of the dep-ngrams with the length of n in the reference.
2045
The final score of RED is achieved using Formula (6), in which a weighted sum of the dep-ngrams?
Fscore is calculated. w
ngram
(0 ? w
ngram
? 1) is the weight of dep-ngram with the length of n. Fscore
n
is the Fscore for the dep-ngrams with the length of n.
RED =
N
?
n=1
(w
ngram
? Fscore
n
) (6)
3 Introducing Extra Resources
Many automatic evaluation metrics can only find the exact match between the reference and the transla-
tion, and the information provided by the limited number of references is not sufficient. Some evaluation
metrics, such as TERp (Snover et al., 2009) and METOER, introduce extra resources to expand the
reference information. We also introduce some extra resources to RED, such as stem, synonym and
paraphrase. The words within a sentence can be classified into content words and function words. The
effects of the two kinds of words are different and they shouldn?t have the same matching score, so we
introduce a parameter to distinguish them. The methods of applying these resources are introduced as
follows.
? Stem and Synonym
Stem(Porter, 2001) and synonym (WordNet
1
) are introduced to RED in the following three steps.
First, we obtain the alignment with Meteor Aligner (Denkowski and Lavie, 2011) in which not only
exact match but also stem and synonym are considered. We use stem and synonym together with
exact match as three match modules. Second, the alignment is used to match for a dep-ngram. We
think the dep-ngram can match with the translation if the following conditions are satisfied. 1) Each
of the words in the dep-ngram has a matched word in the translation according to the alignment;
2) The words in dep-ngram and the matched words in translation appear in the same order; 3) The
matched words in translation must be continuous if the dep-ngram is a fixed-floating ngram. At last,
the match module score of a dep-ngram is calculated according to Formula (7). Different match
modules have different effects, so we give them different weights.
s
mod
=
?
n
i=1
w
m
i
n
, 0 ? w
m
i
? 1 (7)
m
i
is the match module (exact, stem or synonym) of the ith word in a dep-ngram. w
m
i
is the match
module weight of the ith word in a dep-ngram. n is the number of words in a dep-ngram.
? Paraphrase
When introducing paraphrase, we don?t consider the dependency tree of the reference, because
paraphrases may not be contained in the headword chain and fixed-floating structures. First, the
alignment is obtained with METEOR Aligner, only considering paraphrase. Second, the matched
paraphrases are extracted from the alignment and defined as paraphrase-ngram. The score of a
paraphrase is 1? w
par
, where w
par
is the weight of paraphrase-ngram.
? Function word
We introduce a parameter w
fun
(0 ? w
fun
? 1) to distinguish function words and content words.
w
fun
is the weight of function words. The function word score of a dep-ngram or paraphrase-ngram
is computed according to Formula (8).
s
fun
=
C
fun
? w
fun
+ C
con
? (1? w
fun
)
C
fun
+ C
con
(8)
C
fun
is the number of function words in the dep-ngram or paraphrase-ngram. C
con
is the number
of content words in the dep-ngram or paraphrase-ngram.
1
http://wordnet.princeton.edu/
2046
We use RED-plus (REDp) to represent RED with extra resources, and the final score are calculated as
Formula (9), in which Fscore
p
is obtained using precison
p
and recall
p
as Formula (10).
REDp =
N
?
n=1
(w
ngram
? Fscore
p
n
) (9)
Fscore
p
=
precision
p
? recall
p
? ? precision
p
+ (1? ?) ? recall
p
(10)
precision
p
and recall
P
in Formula (10) are calculated as follows.
precision
p
=
score
par
n
+ score
dep
n
len
h
, recall
p
=
score
par
n
+ score
dep
n
count
n
(ref) + count
n
(par)
len
h
is the length of the translation. count
n(ref)
is the number of the dep-ngrams with the length of n
in the reference. count
n
(par) is the number of paraphrases with length of n in reference. score
par
n
is
the match score of paraphrase-ngrams with the length of n. score
dep
n
is the match score of dep-ngrams
with the length of n. score
par
n
and score
dep
n
are calculated as follows.
score
par
n
=
?
par?P
n
(1? w
par
? s
fum
) , score
dep
n
=
?
d?D
n
(p
(d,hyp)
? s
mod
? s
fun
)
P
n
is the set of paraphrase-ngrams with the length of n. D
n
is the set of dep-ngrams with the length of n.
4 Parameter Tuning
There are several parameters in REDp, and different parameter values can make the performance of
REDp different. For example,w
ngram
represents the weight of dep-ngram with the length of n. The
effect of ngrams with different lengths are different, and they shouldn?t have the same weight. So we can
tune the parameters to find their best values.
We try a preliminary optimization method to tune parameters in REDp. A heuristic search is employed
and the parameters are classified into two subsets. The parameter optimization is a grid search over the
two subsets of parameters. When searching Subset 1, the parameters in Subset 2 are fixed, and then
Subset 1 and Subset 2 are exchanged to finish this iteration. Several iterations are executed to finish the
parameter tuning process. This heuristic search may not find the global optimum but it can save a lot of
time compared with exhaustive search. The optimization goal is to maximize the sum of Spearman?s ?
rank correlation coefficient on system level and Kendall?s ? correlation coefficient on sentence level. ?
is calculated using the following equation.
? = 1?
6
?
d
2
i
n(n
2
? 1)
where d
i
is the difference between the human rank and metric?s rank for system i. n is the number of
systems. ? is calculated as follows.
? =
number of concordant pairs? number of discordant pairs
number of concordant pairs + number of discordant pairs
The data of into-English tasks in WMT 2010 are used to tune parameters. The tuned parameters are
listed in Table 1.
5 Experiments
5.1 Data
The test sets in experiments are WMT 2012 and WMT 2013. The language pairs are German-to-English
(de-en), Czech-to-English (cz-en), French-to-English (fr-en), Spanish-to-English (es-en) and Russian-to-
English (ru-en). The number of translation systems for each language pair are showed in Table 2. For
each language pair, there are 3003 sentences in WMT 2012 and 3000 sentences in WMT 2013.
2047
Parameter ? w
fun
w
exact
w
stem
w
syn
w
par
w
1gram
w
2gram
w
3gram
tuned values 0.9 0.2 0.9 0.6 0.6 0.6 0.6 0.5 0.1
Table 1: Parameter values after tuning on WMT 2010. ? is from Formula (10). w
fun
is the weight of
function word. w
exact
, w
stem
andw
syn
are the weights of the three match modules ?exact stem synonym?
respectively. w
par
is the weight of paraphrase-ngram. w
1gram
, w
2gram
and w
3gram
are the weights of
dep-ngram with the length of 1, 2 and 3 respectively.
Language pairs cz-en de-en es-en fr-en ru-en
WMT2012 6 16 12 15 -
WMT2013 12 23 17 19 23
Table 2: The number of translation systems for each language pair on WMT 2012 and WMT 2013.
We parsed the reference into constituent tree by Berkeley parser
2
and then converted the constituent
tree into dependency tree by Penn2Malt
3
. Presumably, the performance of the new metric will be better
if the dependency trees are labeled by human. Reference dependency trees are labeled only once and can
be used forever so it will not increase costs.
5.2 Baselines
In the experiments, we compare the performance of our metric with the widely-used lexicon-based met-
rics such as BLEU
4
, TER
5
and METEOR
6
, dependency-based metric HWCM and semantic-based metric
SEMPOS (Mach?a?cek and Bojar, 2011) which has the best performance on system level according to the
published results of WMT 2012.
The results of BLEU are obtained using 4-gram with smoothing option. The version of TER is 0.7.25.
The results of METEOR are obtained by Version 1.4 with task option ?rank?. We re-implement HWCM
which employs an epsilon value of 10
?3
to replace zero for smoothing purpose. The correlations of
SEMPOS are obtained from the published results of WMT 2012 and WMT 2013.
5.3 Experiment Results
The experiments on both system level and sentence level are carried out. On system level, the correlations
are calculated using Spearman?s rank correlation coefficient ? (Pirie, 1988). Kendall?s rank correlation
coefficient ? (Kendall, 1938) is employed to evaluate the sentence level correlation. Our method performs
best when the maximum length of dep-ngram is set to 3, so we only present the results with the maximum
length of 3. RED represents the new metric with exact match and the parameter values are set as follows.
? = 0.5. w
1gram
= w
2gram
= w
3gram
= 1/3. REDp represents the new metric with extra resources
and tuned parameter values which are listed in Table (1).
5.3.1 System level correlations
The system level correlations are shown in Table 3. RED is better than BLEU, TER and HWCM on
average on both WMT 2012 and WMT 2013, which reflects that using syntactic information and only
parsing the reference side are helpful. REDp gets the best result on all of the language pairs except
cz-en on WMT 2012. The significant improvement from RED to REDp illustrates the effect of extra
resources and the parameter tuning. Stem, synonym and paraphrase can enrich the reference and provide
extra knowledge for automatic evaluation metric. There are several parameters in REDp, and different
parameter values can make the performance of REDp different. So the performance can be optimized
through parameter tuning. SEMPOS got the best correlation according to the published results of WMT
2
http://code.google.com/p/berkeleyparser/downloads/list
3
http://stp.lingfil.uu.se/
?
nivre/research/Penn2Malt.html
4
ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v13a.pl
5
http://www.cs.umd.edu/
?
snover/tercom
6
http://www.cs.cmu.edu/
?
alavie/METEOR/download/meteor-1.4.tgz
2048
2012, and METEOR got the best correlation according to the published results of WMT 2013 on into-
English task on system level. REDp gets better result than SEMPOS and METEOR on both WMT 2012
and WMT 2013, so REDp achieves the state-of-the-art performance on system level.
data WMT 2012 WMT 2013
Metrics cz-en de-en es-en fr-en ave cz-en de-en es-en fr-en ru-en ave
BLEU .886 .671 .874 .811 .811 .936 .895 .888 .989 .670 .876
TER .886 .624 .916 .821 .812 .800 .833 .825 .951 .581 .798
HWCM .943 .762 .937 .818 .865 .902 .904 .886 .951 .756 .880
METEOR .657 .885 .951 .843 .834 .964 .961 .979 .984 .789 .935
SEMPOS .943 .924 .937 .804 .902 .955 .919 .930 .938 .823 .913
RED 1.0 .759 .951 .818 .882 .964 .951 .930 .989 .725 .912
REDp .943 .947 .965 .843 .925 .982 .973 .986 .995 .800 .947
Table 3: System level correlations on WMT 2012 and WMT 2013. The value in bold is the best result in
each column. ave stands for the average result of the language pairs on WMT 2012 or WMT 2013.
5.3.2 Sentence level correlations
The sentence level correlations on WMT 2012 and WMT 2013 are shown in Table 4. RED is better than
BLEU and HWCM on all the language pairs, which reflects the effectiveness of syntactic information
and only parsing the reference. By introducing extra resources and parameter tuning, REDp achieves
significant improvement over RED. Stem, synonym and paraphrase can enrich the reference and provide
extra knowledge for automatic evaluation metric. There are several parameters in REDp, and different
parameter values can make the performance of REDp different. A better performance can be exploited
through parameter tuning. From the results of REDp and METEOR, we can see that REDp gets the
comparable results with METEOR on sentence level on both WMT 2012 and WMT 2013.
data WMT 2012 WMT 2013
Metrics cz-en de-en es-en fr-en ave cz-en de-en es-en fr-en ru-en ave
BLEU .157 .191 .189 .210 .187 .199 .220 .259 .224 .162 .213
HWCM .158 .207 .203 .204 .193 .187 .208 .247 .227 .175 .209
METEOR .212 .275 .249 .251 .247 .265 .293 .324 .264 .239 .277
RED .165 .218 .203 .221 .202 .210 .239 .292 .246 .196 .237
REDp .212 .271 .234 .250 .242 .259 .290 .323 .260 .223 .271
Table 4: Sentence level correlations on WMT 2012 and WMT 2013. The value in bold is the best result
in each column. ave stands for the average result of the language pairs on WMT 2012 or WMT 2013.
6 Conclusion and Future Work
In this paper, we propose a reference dependency based automatic MT evaluation metric RED. The
new metric only uses the dependency trees of the reference, which avoids the parsing of the potentially
noisy translations. Both long distance dependency information and the local continuous ngrams are
captured by the new metric. The experiment results indicate that RED achieves better correlations than
BLEU, TER and HWCM on both system level and sentence level. REDp, the improved version of RED
through adding extra resources and preliminary parameter tuning, gets state-of-the-art results which are
better than METEOR and SEMPOS on system level. On sentence level, REDp gets the comparable
performance with METEOR.
In the future, we will use the dependency forest instead of the dependency tree to reduce the effect
of parsing errors. We will also apply RED and REDp to the tuning process of SMT to improve the
translation quality.
2049
Acknowledgements
The authors were supported by National Natural Science Foundation of China (Contract 61202216)
and National Natural Science Foundation of China (Contract 61379086). Qun Liu?s work was partially
supported by the Science Foundation Ireland (Grant No. 07/CE/I1142) as part of the CNGL at Dublin
City University. Sincere thanks to the three anonymous reviewers for their thorough reviewing and
valuable suggestions.
References
Boxing Chen and Roland Kuhn. 2011. Amber: A modified bleu, enhanced ranking metric. In Proceedings of
the Sixth Workshop on Statistical Machine Translation, pages 71?77, Edinburgh, Scotland, July. Association for
Computational Linguistics.
Boxing Chen, Roland Kuhn, and George Foster. 2012. Improving amber, an mt evaluation metric. In Proceedings
of the Seventh Workshop on Statistical Machine Translation, WMT ?12, pages 59?63, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Michael Denkowski and Alon Lavie. 2011. Meteor 1.3: Automatic metric for reliable optimization and evaluation
of machine translation systems. In Proceedings of the Sixth Workshop on Statistical Machine Translation, pages
85?91. Association for Computational Linguistics.
Jes?us Gim?enez and Llu??s M`arquez. 2007. Linguistic features for automatic evaluation of heterogenous mt systems.
In Proceedings of the Second Workshop on Statistical Machine Translation, pages 256?264. Association for
Computational Linguistics.
Maurice G Kendall. 1938. A new measure of rank correlation. Biometrika, 30(1/2):81?93.
Alon Lavie and Abhaya Agarwal. 2007. Meteor: an automatic metric for mt evaluation with high levels of
correlation with human judgments. In Proceedings of the Second Workshop on Statistical Machine Translation,
StatMT ?07, pages 228?231, Stroudsburg, PA, USA. Association for Computational Linguistics.
Ding Liu and Daniel Gildea. 2005. Syntactic features for evaluation of machine translation. In Proceedings of the
ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization,
pages 25?32.
Chi-kiu Lo and Dekai Wu. 2013. MEANT at WMT 2013: A tunable, accurate yet inexpensive semantic frame
based MT evaluation metric. In Proceedings of the Eighth Workshop on Statistical Machine Translation, pages
422?428, Sofia, Bulgaria, August. Association for Computational Linguistics.
Chi-kiu Lo, Anand Karthik Tumuluru, and Dekai Wu. 2012. Fully automatic semantic mt evaluation. In Pro-
ceedings of the Seventh Workshop on Statistical Machine Translation, pages 243?252, Montr?eal, Canada, June.
Association for Computational Linguistics.
Matou?s Mach?a?cek and Ond?rej Bojar. 2011. Approximating a deep-syntactic metric for mt evaluation and tun-
ing. In Proceedings of the Sixth Workshop on Statistical Machine Translation, pages 92?98. Association for
Computational Linguistics.
Dennis Mehay and Chris Brew. 2007. BLEUTRE: Flattening Syntactic Dependencies for MT Evaluation. In
Proceedings of the 11th Conference on Theoretical and Methodological Issues in Machine Translation (TMI).
F.J. Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of the 41st Annual
Meeting on Association for Computational Linguistics-Volume 1, pages 160?167. Association for Computa-
tional Linguistics.
Karolina Owczarzak, Josef van Genabith, and Andy Way. 2007. Dependency-based automatic evaluation for
machine translation. In Proceedings of the NAACL-HLT 2007/AMTA Workshop on Syntax and Structure in Sta-
tistical Translation, SSST ?07, pages 80?87, Stroudsburg, PA, USA. Association for Computational Linguistics.
K. Papineni, S. Roukos, T. Ward, and W.J. Zhu. 2002. BLEU: a method for automatic evaluation of machine
translation. In Proceedings of the 40th annual meeting on association for computational linguistics, pages
311?318. Association for Computational Linguistics.
W Pirie. 1988. Spearman rank correlation coefficient. Encyclopedia of statistical sciences.
2050
Martin F Porter. 2001. Snowball: A language for stemming algorithms.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2010. String-to-dependency statistical machine translation. Compu-
tational Linguistics, 36(4):649?671.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A study of trans-
lation edit rate with targeted human annotation. In Proceedings of Association for Machine Translation in the
Americas, pages 223?231.
Matthew Snover, Nitin Madnani, Bonnie J Dorr, and Richard Schwartz. 2009. Fluency, adequacy, or hter?:
exploring different human judgments with a tunable mt metric. In Proceedings of the Fourth Workshop on
Statistical Machine Translation, pages 259?268. Association for Computational Linguistics.
2051
Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,
pages 2217?2226, Dublin, Ireland, August 23-29 2014.
Augment Dependency-to-String Translation with Fixed and Floating
Structures
Jun Xie
?
Jinan Xu
?
Qun Liu
??
?Key Laboratory of Intelligent Information Processing,
Institute of Computing Technology,Chinese Academy of Sciences
xiejun@ict.ac.cn
?School of Computer and Information Technology, Beijing Jiaotong University
xja2010@gmail.com
?School of Computing, Dublin City University
qliu@computing.dcu.ie
Abstract
In this paper, we propose an augmented dependency-to-string model to combine the merits of
both the head-dependents relations at handling long distance reordering and the fixed and floating
structures at handling local reordering. For this purpose, we first compactly represent both the
head-dependent relation and the fixed and floating structures into translation rules; second, in
decoding we build ?on-the-fly? new translation rules from the compact translation rules that
can incorporate non-syntactic phrases into translations, thus alleviate the non-syntactic phrase
coverage problem of dependency-to-string translation (Xie et al., 2011). Large-scale experiments
on Chinese-to-English translation show that our augmented dependency-to-string model gains
significant improvement of averaged +0.85 BLEU scores on three test sets over the dependency-
to-string model.
1 Introduction
As a representation holding both syntactic and semantic information, dependency grammar has been
attracting more and more attention in statistical machine translation. Lin (2004) took paths as the el-
ementary structures and proposed a path-based transfer model. Quirk et al. (2005) extended path to
treelets (connected subgraphs of dependency trees) and put forward dependency treelet translation. Ding
and Palmer (2005) proposed a model on the basis of dependency insertion grammar. Shen et al. (2008)
employed the fixed and floating structures as elementary structures and proposed a string-to-dependency
model with state-of-the-art performance. Xie et al. (2011) employs head-dependents relations as elemen-
tary structures and proposed a dependency-to-string model with good long distance reordering property.
A head-dependents relation (HDR) is composed of a head and all its dependents, which can be viewed
as an instance of a sentence pattern or phrase pattern.
However, since dependency trees are much flatter than constituency trees, the dependency-to-string
model suffers more severe non-syntactic phrase coverage problem (Meng et al., 2013) than constituency-
based models (Galley et al., 2004; Liu et al., 2006; Huang et al., 2006). Non-syntactic phrases are those
phrases that can not be covered by whole subtrees. To address this problem, Meng et al. (2013) proposed
to translate with both constituency and dependency trees, which can incorporate non-syntactic phrases
covered by the constituents of the constituency trees. This model requires both constituency and depen-
dency trees, thus may suffer from both constituency and dependency parse errors. Additionally, there are
only few languages that have both constituency and dependency parsers, which limits its practical use.
In this paper, we propose to address non-syntactic phrase coverage problem of the dependency-to-
string model without resort to extra resources (Section 3). To this end, we augment the dependency-to-
string model at two aspects. First, we combine the merits of both the head-dependent relations and the
fixed and floating structures (Shen et al., 2008), and compactly represent these two kinds of knowlege
into augmented HDR rules (Section 3.1). We acquire the augmented HDR rules automatically from the
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
2217
X4:VV
X5:NNX3:!*
(a)
X3 X2X5
X2:NTX1:PN
X4X1
X4:VV
X5:NNX3:!*
X3 X2X5
X2:NTX1:PN
X4X1
(b)
Figure 1: Examples of an HDR rule (a) and an augmented HDR rule (b). Where each ?*? denotes
a substitute site which is a compact representation of a whole subtree. The shadow with line border
indicates a fixed structure and the shadow with dash line border indicates a floating structure.
word-aligned source dependency tree and target string paris (Section 3.2). In decoding we propose an
?on-the-fly? rule building strategy, which builds new translation rules from the augmented HDR rules
and incorporates non-syntactic phrases into translations (Section 3.4). Large-scale experiments (Section
4) on Chinese-to-English translation show that our augmented model gains significant improvement of
averaged +0.85 BLEU points on three test sets over the dependency-to-string model.
2 Background
For convenience of the description of our augmented dependency-to-string model, we first briefly re-
view the dependency-to-string model and the fixed and floating structures of string-to-dependency model
(Shen et al., 2008).
2.1 Dependency-to-String Translation
The dependency-to-string model (Xie et al., 2011) takes head-dependents relations as the elementary
structures of dependency trees, and represents the translation rules with the source side as HDRs and
the target side as string. Since the HDRs in essence relate to phrase patterns and sentence patterns,
the HDR rules specify the reordering of these patterns. For example, Figure 1 (a) is an example HDR
rule, which represents a reordering manner of a sentence pattern composed of a proper noun (X1:PN),
a temporal noun (X2:NT), an prepositional phrase relate to ?? (give)? (X3:?), a verb (X4:VV) and a
noun (X5:NN).
With the HDR rules, the dependency-to-string model gets rid of the extra reordering heuristics and
reordering models of the previous models (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005). More
importantly, the model shows state-of-the-art performance and exhibits good long distance reordering
property.
2.2 Fixed and Floating Structures
The fixed structures and floating structures are fundamental structures of the string-to-dependency model
(Shen et al., 2008), which are introduced to handle the coverage of non-constituent rules. Given the
dependency tree d
1
d
2
...d
n
of a sentence f
1
f
2
...f
n
, where d
i
indicates the parent word index of word f
i
.
Definition 1. A dependency structure d
i...j
is fixed on the head h, where h ? [i, j], if and only if it meets
the following conditions:
- d
h
/? [i, j]
- ?k ? [i, j] and k 6= h, d
k
? [i, j]
- ?k /? [i, j], d
k
= h or d
k
/? [i, j]
A fixed structure describes a fragment with a sub-root, where all the children of the sub-root are
complete.
2218
Definition 2. A dependency tree d
i...j
is floating with children C, for a non-empty set C ? i, ..., j, if and
only if it meets the following conditions:
- ?h /? [i, j], s.t.?k ? C, d
k
= h
- ?k ? [i, j] and k /? C, d
k
? [i, j]
- ?k /? [i, j], d
k
/? [i, j]
A floating structure consists of sibling nodes of a common head, but the head itself is unspecified.
In nature, the fixed and floating structures represent the phrases under the structural constraint of
dependency trees, most of them are non-syntactic phrases.
The HDRs are good at handling long distance dependencies, while the fixed and floating structures
excels at handling local reordering. This encourages us to address the non-syntactic phrase coverage
problem of dependency-to-string model by exploiting these two kinds of structures.
3 Augmented Dependency-to-String Translation
In the following, we will describe our augmented dependency-to-string model in detail, including the
augmented HDR rules (Section 3.1), rule acquisition (Section 3.2) and ?on-the-fly? rule building in
decoding (Section 3.4).
3.1 Augmented HDR rules
Our augmented HDR rules aim at combining the merits of both the HDRs at handling long distance re-
ordering and the fixed and floating structures at handling local reordering. For this purpose, we augment
the HDR rules (Xie et al., 2011) by labelling the HDRs with the fixed and floating structures.
Figure 1 (b) shows an example augmented HDR rule. Which is an augmented version of the HDR
rule Figure 1 (a) by labelling it with a fixed structure (shadow with line border) and a floating structure
(shadow with dash line border). The labeled fixed and floating structures indicate the bilingual phrases
that we can incorporate in this sentence pattern.
3.2 Rule Acquisition
Given a word-aligned parallel corpus defined as a set of triples ?T, e, A?, where T is a dependency tree
of source sentence f
J
1
, e
I
1
is the target sentence and A is an alignment relation between f
J
1
and e
I
1
, we
acquire the augmented HDR rules by three steps: tree annotation, acceptable HDR identification and rule
induction. The process is similar with that of Xie et al. (2011). However, we make some extensions so
that we can take the fixed and floating structures into account.
3.2.1 Tree Annotation
Besides annotating each node of T with head span and dependency span as Xie et al. (2011), we also
label the tree with consistent fixed and floating structures.
Definition 3. The head span hsp(n) of a node n is the closure of the set taking the index of the target
words aligned to n as its elements.
The closure of a set contains all the elements between the minimum and the maximum of the set and
each element has only one copy. For example, the closure of set {1, 3} is {1, 2, 3}.
We say a head span is consistent with alignment if the bilingual phrase it covers is consistent with the
alignment (Koehn et al., 2003).
Definition 4. Given a subtree T
?
rooted at n, the dependency span dsp(n) of n is the closure of the union
of the consistent head spans of all the nodes of T
?
.
dsp(n) = closure(
?
n
?
?T
?
hsp(n
?
) is consistent
hsp(n
?
))
If no head spans of all the nodes of T
?
are consistent, dsp(n) = ?.
2219
!"##$%&%'$(&)'
*+",,$-&-'$-&-'
."/$0&0'$0&1'
2"/,$1&1'$1&1'
3445 647 849:;<8I
=*",>$)&)'$)&)'?"/,$(&('$(&('
@:AA B4CD:99E7
. !? =* *+2
% 0 )1 F 1-
!
"#X3:.*
X3 tonightdinner
$"%
cookI will
!"# !$#
%&'()*
+()'()*
Figure 2: An example annotated dependency tree (a) and an example lexicalized augmented HDR rule
(b) induced from the top-level HDR of (a). Each node of the dependency tree is annotated with two
spans: head span (the former) and dependency span (the latter). The shadows denote a consistent fixed
structure (shadow with line border) and a floating structure (shadow with dash line border). The ?*?
denotes a substitute site.
Definition 5. A fixed or floating structure is consistent with alignment if the phrase it covers is consistent
with alignment.
Tree annotation can be readily accomplished by a single post-order traversal of dependency tree T .
For each accessed node n, annotate it with head span and dependency span according to A. If n is an
internal node, enumerate all the fixed and floating structures relate to n, and label those consistent ones
on T . Repeat the above process till the root is accessed.
Figure 2 (a) shows an example annotated dependency tree. Where each node is annotated with two
spans: head span (the former) and dependency span (the latter). Moreover, the dependency tree is also
labeled with two consistent fixed and floating structures that cover phrases ?? ?? and ??? ? ??
respectively.
3.2.2 Acceptable HDR Identification
From the annotated dependency tree, we identify the HDRs that are suitable for rule induction. These
HDRs are called as acceptable HDRs. To this end, we traverse the annotated dependency tree in post-
order and identify the HDRs with the following properties:
- for the head, its head span is consistent;
- for the dependents, the dependency span of each dependent should not be ? unless the dependent
is a leaf node;
- the intersection of the head span of the head and the dependency spans of the dependents is ? (or
do not overlap).
Different from those acceptable HDRs of Xie et al. (2011), the acceptable HDRs here may be labeled
with fixed and floating structures. For example, the top level of Figure 2 (a) is an acceptable HDR, which
is labeled with a fixed structure and a floating structures. Typically an acceptable HDR has three types
of nodes: leaf node (of the dependency tree), internal node (of the dependency tree) and head node (an
internal node function as the head of the HDR).
3.2.3 Rule Induction
From each acceptable HDR, we induce a set of lexicalized and unlexicalized augmented HDR rules. This
process is similar with that of Xie et al. (2011) except that here we have to consider the consistent fixed
2220
!"#X3:!*
X3 tonightdinner
$"%
cookI will
!
&'())X3:!*
X3 X2X5
&*()+&,(-)
cookX1 will
!
"#X3:"*
X3 tonightdinner
$"%
cookI will
!
&'())X3:"*
X3 X2&'
&*()+&,(-)
cookX1 will
&.(//
"#X3:!*
X3 tonightdinner
$"%
X4I will
&.(//
&'())X3:!*
X3 X2X5
&*()+&,(-)
X4X1 will
&.(//
"#X3:"*
X3 tonightdinner
$"%
X4I will
&.(//
&'())X3:"*
X3 X2&'
&*()+&,(-)
X4X1 will
!"# !$#
!%# !&#
!'# !(#
!)# !*#
+,
+, +,
+,
+-+-
UH
Figure 3: Lexicalized augmented HDR rule (a) and unlexicalized augmented HDR rules (b)?(h) induced
from the top level HDR of the annotated dependency tree in Figure 2. Where ?UH?, ?UI? and ?UL?
denotes ?unlexicalize head?, ?unlexicalize internal? and ?unlexicalize leaf? , respectively. The shadows
with line border denote fixed structures and the shadows with dash line border denotes floating structures.
and floating structures.
First, we induce a lexicalized augmented HDR rule with the following principles:
1. extract the HDR, mark each internal node as a variable, and label the HDR with the floating struc-
tures that cover only variables. This forms the input of a lexicalized rule.
2. generate the target string according to head span of the head and the dependency spans of the related
dependents, and turn the word sequences covered by the dependency spans of the internal nodes into
variables. This forms the output of a lexicalized rule.
Figure 2 (b) illustrates a lexicalized augmented HDR rule induced from the top-level HDR of the
annotated dependency tree Figure 2 (a).
From each lexicalized augmented HDR rule (along with the acceptable HDR), we then induce a set of
unlexicalized augmented HDR rules with the following principles:
1. turn each type (leaf, internal or head) of nodes simultaneously into variables;
2. when turning a head or leaf node into a variable, change the counterpart of the target side into the
variable; label the unlexicalized HDR with the fixed and floating structures that cover only variables.
3. when turing an internal node into a variable, keep the counterpart of the target side unchanged.
Totally, we will obtain eight types of augmented HDR rules from an acceptable HDR. In this paper,
we call the lexicalized and unlexicalized HDRs generated by the above process as instances of the HDR.
Figure 3 illustrates the rule induction of seven unlexicalized augmented HDR rules (b)-(h) from lexi-
calized augmented HDR rule (a). Where ?UH?, ?UI? and ?UL? on the dash arrows indicate ?unlexicalize
head?, ?unlexicalize internal? and ?unlexicalized leaf?, respectively.
3.2.4 Probability Estimation
We take the augmented HDR rules acquired from word-aligned parallel corpus as the observed data, and
employ relative frequency estimation to calculate the translation probabilities of the rules. Note that, here
we take the labeled fixed and floating structures of the augmented HDR rules as indicators of bilingual
phrases that can be incorporated in the sentence patterns and phrases patterns represented by the HDRs.
So we consider only the HDRs when counting the augmented HDR rules.
2221
X2:NT
!  "# callcalled
...
$%  &  '
him yesterday
!"#$%&'$()*+)#,-(
...
X45:VV_NN
X3:&*
X3 X2
X2:NTX1:PN
X45X1
X4:VV
X5:NN
X23X5
X23:NT_P*X1:PN
X4X1
X45:VV_NN
X23
X23:NT_P*X1:PN
X45X1
(relate to X4 X5)(relate to X2 X3)
(d) (e) (f)
X4:VV
X5:NNX3:&*
X3 X2X5
X1:PN
X4X1
(a) (b) (c)
!{ }!
X4:VV
X5:NN
X4:VV
X5:NN
X2:NT X3:&*X2:NT X3:&*
Figure 4: Illustration of ?on-the-fly? translation rule building.
3.3 The model
Following Och and Ney (2002), we adopt a general log-linear model for our augmented dependency-to-
string model. Let d be a derivation that converts a source dependency tree T into a target string e. The
probability of derivation d is defined as:
P (d) ?
?
i
?
i
(d)
?
i
(1)
where ?
i
are features defined on derivation and ?
i
are feature weights.
In our implementation, we make use of eleven features, including seven features inherited from the
dependency-to-string model:
- translation probabilities P (f |e) and P (e|f) and lexical translation probabilities P
lex
(f |e) and
P
lex
(e|f) of augmented HDR rules
- rule penalty exp(?1)
- language model P
lm
(e)
- word penalty exp(|e|), where |e| is the length of the generated target string
and four extra features for bilingual phrases relate to fixed and floating structures:
- translation probabilities P
bp
(f |e) and P
bp
(e|f) and lexical translation probabilities P
bp lex
(f |e) and
P
bp lex
(e|f) of bilingual phrases
3.4 ?On-the-Fly? Decoding
The task of the decoder is to find the best derivation from all possible derivations. Our decoder is based
on bottom-up chart parsing, which characterizes at ?on-the-fly? translation rule building.
Given an input dependency tree T , the decoder traverses it in post-order. For each accessed node n,
the decoder first enumerates all instances of the HDR rooted at n as we do in rule induction, and checks
for matched augmented HDR rules. If a matched rule is labeled with fixed and floating structures, the
decoder builds new translation rules ?on the fly? with the following principles:
2222
1. check the phrases covered by the labeled fixed and floating structures for matched bilingual phrases;
2. if there are no matched bilingual phrases for all labeled fixed and floating structures, take the aug-
mented HDR rule as a HDR rule of dependency-to-string model; otherwise,
- enumerate all combinations of the fixed and floating structures with matched bilingual phrases;
- for each combination, build a new translation rule by turning the variable sequences covered
by the fixed and floating structures into new variables;
- the new-built rule inherits the translation probabilities of the deriving augmented HDR rule,
and the new variables take the matched bilingual phrases as their translation hypothesis.
Figure 4 illustrates the ?on-the-fly? rule building process. Suppose augmented HDR rule (a) is the
matched rule, and bilingual phrases (b) and (c) match the phrases covered by the labeled fixed and
floating structures of (a). There will be three combinations of the labeled fixed and floating structures
as shown in the middle of Figure 4. For each combination, the decoder builds a new translation rule by
turning variable sequences ?X2:NT X3:?*? and/or ?X4:VV X5:NN? into new variables ?X23:NT P*?
and/or ?X45:VV NN?. And we will obtain three new translation rules (d)-(f) that can incorporate non-
syntactic phrases into translations.
If there are no matched rules, the decoder builds a pseudo translation rule with monotonic reordering.
The decoder then employs cube pruning (Chiang, 2007; Huang and Chiang, 2007) to generate k-best
hypothesis with integrated language model for node n.
Repeat the above process till the root of T is accessed. The hypothesis with the highest score is output
as translation.
4 Experiments
We evaluated our augmented model by comparison with dependency-to-string model and hierarchical
phrase-based model on Chinese-to-English translation in terms of BLEU (Papineni et al., 2002).
4.1 Experimental Setup
The parallel training corpus include 1.25M Chinese-English sentence pairs.
1
We parse the Chinese
sentences with Stanford Parser (Klein and Manning, 2003) into projective dependency trees, obtain word
alignment by running GIZA++ (Och and Ney, 2003) in both directions and applying ?grow-diag-final?
refinement (Koehn et al., 2003), and train a 4-gram language model by SRI Language Modeling Toolkit
(Stolcke, 2002) with Kneser-Ney smoothing on the Xinhua portion of the Gigaword corpus.
We take NIST MT Evaluation test set 2002 as our development set, 2003 (MT03), 2004 (MT04)
and 2005 (MT05) as our test sets, evaluate the quality of translations by case insensitive NIST BLEU-
4 metric
2
, tune the feature weights by Max-BLEU strategy with MERT (Och, 2003), and check the
statistical difference between the systems with significance test (Collins et al., 2005).
4.2 Systems
We take ?Moses-Chart? of Moses
3
(Koehn et al., 2007) as hierarchical phrase-based model baseline. In
our experiments, we use the default settings.
Both the dependency-to-string baseline and our augmented model employ the same settings as those
of Xie et al. (2011), with the beam threshold, beam size and rule size are set to 10
?3
, 200 and 100
respectively. And both systems employ bilingual phrases with length ? 7 extracted by Moses.
4.3 Experiment results
Table 1 shows the results of the BLEU scores of the three systems. Where ?dep2str? and ?dep2str-aug?
denote dependency-to-string model baseline and our augmented dependency-to-string model, respec-
tively. As we can see, ?dep2str? shows better performance (+0.31 BLEU on average) than ?Moses-Chart?
1
From LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06.
2
ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v11b.pl
3
http://www.statmt.org/moses/
2223
System Rule# MT03 MT04 MT05 Average
Moses-Chart 116.4M 34.65 36.47 34.39 35.17
dep2str 37M+32.5M 34.92 36.82 34.71 35.48
dep2str-aug 37M+32.5M 35.66
*
(+0.74) 37.61
*
(+0.79) 35.74
*
(+1.03) 36.33 (+0.85)
Table 1: Statistics of the extracted rules and BLEU scores (%) on the test sets of the three systems.
Where ?37M+32.5M? denotes 37M rules and 32.5M bilingual phrases. And ?*? indicates dep2str-aug
are statistically better than dep2str with p < 0.01.
Source: !"# $ % & ' ( ) *+, -. /0 1 23 45 67 8
Reference 1: Sampaio has placed  high hopes on the Portuguese-Sino 
cooperation in the World Expo. 
Moses-Chart: Sampaio on cooperation between the two countries in the 
world expo affairs Portugal and China places great . 
Dep2Str: President placed great  cooperation between Portugal and China , 
the two countries in the World Expo affairs .
Dep2Str-aug: Sampaio placed high expectations of the Portuguese - Chinese 
cooperation in World Expo affairs .
!"#
9:;
$
9<
%
& '
( )
*+,
-=
/0 1
23
45
9>>
67
9::
8
9<?
Reference 2: Sampaio expressed his high expectations on the Sino-
Portuguese cooperation in the work of the world exposition.
Figure 5: Translation examples of ?Moses-Chart?, ?dep2str? and ?dep2str-aug?. The line border shadow
denotes the phrases successfully captured by ?dep2str-aug?.
and is a strong baseline.?Dep2str-aug? gains significant improvements of +0.74, +0.79 and +1.03 BLEU
points over ?dep2str? on the test sets, respectively.
Additionally, we compare the actual translations generated by ?Moses-Chart?, ?dep2str? and ?dep2str-
aug?. Figure 5 shows the translations of these three systems on a sentence of MT05. The source sentence
holds a common sentence pattern in Chinese, which is composed of a proper noun, a verb, a noun and
a prepositional phrases (corresponding to the top level of the dependency tree on the right). However,
the preposition phrase related to ??? holds nine words, thus the simple pattern becomes a long distance
dependency that challenges SMT systems. Limited by the phrase-based rules, ?Moses-Chart? fails to
capture the sentence pattern and outputs a messy translation with little sense. ?Dep2str?, resorting to
HDR rules, successfully captures the pattern and outputs a translation with correct reordering, but it is
still hard to understand. With the help of augmented HDR rules, ?dep2str-aug? captures both the sentence
pattern and non-syntactic phrase ?????? and gives an translation with good adequacy and fluency.
These results reveal the merits of our augmented dependency-to-string model at handling both long
distance reordering (with HDR) and local reordering (with fixed and floating structures), which is promis-
ing for translating language pairs that are syntactically divergent.
5 Conclusion and Future Work
In this paper, we propose an augmented dependency-to-string model to address the non-syntactic phrase
coverage problem for dependency-to-string model. To this purpose, we make two important augmen-
tations to the dependency-to-string model. First, we propose an compact representation to combine
both head-dependent relation and the fixed and floating structures into translation rules. Second, in de-
coding we build ?on the fly? new translation rules from the compact translation rules and incorporate
non-syntactic phrases into translations. By this way, we can combine the merits of both head-dependents
relation at handling long distance reordering and bilingual phrases at handling local reordering. Large-
2224
scale experiments show that our augmented dependency-to-string model gains significant improvements
over the dependency-to-string model.
In the future work, we would like to incorporate semantic knowledge such as typed dependencies and
WordNet
4
(Miller, 1995) so as to better direct the process of translation.
Acknowledgments
The authors were supported by National Nature Science Foundation of China ( Contract 61370130 and
61379086). Liu was partially supported by the Science Foundation Ireland (Grant No. 07/CE/I1142)
as part of the CNGL at Dublin City University. We sincerely thank the anonymous reviewers for their
careful review and insightful suggestions.
References
David Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33.
Michael Collins, Philipp Koehn, and Ivona Kucerova. 2005. Clause restructuring for statistical machine transla-
tion. In Proceedings of the ACL 2005, pages 531?540, Ann Arbor, Michigan, June.
Yuan Ding and Martha Palmer. 2005. Machine translation using probabilistic synchronous dependency insertion
grammars. In Proceedings of ACL 2005, pages 271?279.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel Marcu. 2004. What?s in a translation rule? In HLT-
NAACL 2004: Main Proceedings, pages 273?280, Boston, Massachusetts, USA, May 2 - May 7.
Liang Huang and David Chiang. 2007. Forest rescoring: Faster decoding with integrated language models. In
Proceedings of ACL 2007, pages 144?151, Prague, Czech Republic, June.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006. Statistical syntax-directed translation with extended domain
of locality. In Proceedings of AMTA, pages 66?73.
Dan Klein and Christopher D. Manning. 2003. Fast exact inference with a factored model for natural language
parsing. In In Advances in Neural Information Processing Systems 15 (NIPS), pages 3?10. MIT Press.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings
of the 2003 Human Language Technology Conference of the North American Chapter of the Association for
Computational Linguistics, Edmonton, Canada, July.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke
Cowan, Wade Shen, Christine Moran, Richard Zens, et al. 2007. Moses: Open source toolkit for statistical
machine translation. In Proceedings of ACL 2005: Interactive Poster and Demonstration Sessions, pages 177?
180.
Dekang Lin. 2004. A path-based transfer model for machine translation. In Proceedings of Coling 2004, pages
625?630, Geneva, Switzerland, Aug 23?Aug 27.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-string alignment template for statistical machine translation.
In Proceedings of ACL 2006, pages 609?616, Sydney, Australia, July.
Fandong Meng, Jun Xie, Linfeng Song, Yajuan L?u, and Qun Liu. 2013. Translation with source constituency and
dependency trees. In Proceedings of EMNLP 2013, pages 1066?1076, Seattle, Washington, USA, October.
George A Miller. 1995. Wordnet: a lexical database for english. Communications of the ACM, 38(11):39?41.
Franz Josef Och and Hermann Ney. 2002. Discriminative training and maximum entropy models for statistical
machine translation. In Proceedings of ACL 2002, pages 295?302, Philadelphia, Pennsylvania, USA, July.
Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models.
Computational Linguistics, 29(1):19?51.
Franz Josef Och. 2003. Minimum error rate training in statistical machine translation. In Proceedings of ACL-
2003, pages 160?167, Sapporo, Japan, July.
4
http://wordnet.princeton.edu
2225
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evaluation
of machine translation. In Proceedings of ACL 2002, pages 311?318, Philadelphia, Pennsylvania, USA, July.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. Dependency treelet translation: Syntactically informed
phrasal smt. In Proceedings of ACL 2005, pages 271?279.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A new string-to-dependency machine translation algorithm
with a target dependency language model. In Proceedings of ACL 2008: HLT, pages 577?585, Columbus, Ohio,
June.
Andreas Stolcke. 2002. Srilm - an extensible language modeling toolkit. In Proceedings of ICSLP, volume 30,
pages 901?904.
Jun Xie, Haitao Mi, and Qun Liu. 2011. A novel dependency-to-string model for statistical machine translation.
In Proceedings of EMNLP 2011, pages 216?226, Edinburgh, Scotland, UK., July.
2226
Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 216?226,
Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational Linguistics
A Novel Dependency-to-String Model for Statistical Machine Translation
Jun Xie, Haitao Mi and Qun Liu
Key Laboratory of Intelligent Information Processiong
Institute of Computing Technology
Chinese Academy of Sciences
P.O. Box 2704, Beijing 100190, China
{junxie,htmi,liuqun}@ict.ac.cn
Abstract
Dependency structure, as a first step towards
semantics, is believed to be helpful to improve
translation quality. However, previous works
on dependency structure based models typi-
cally resort to insertion operations to complete
translations, which make it difficult to spec-
ify ordering information in translation rules.
In our model of this paper, we handle this
problem by directly specifying the ordering
information in head-dependents rules which
represent the source side as head-dependents
relations and the target side as strings. The
head-dependents rules require only substitu-
tion operation, thus our model requires no
heuristics or separate ordering models of the
previous works to control the word order of
translations. Large-scale experiments show
that our model performs well on long dis-
tance reordering, and outperforms the state-
of-the-art constituency-to-string model (+1.47
BLEU on average) and hierarchical phrase-
based model (+0.46 BLEU on average) on two
Chinese-English NIST test sets without resort
to phrases or parse forest. For the first time,
a source dependency structure based model
catches up with and surpasses the state-of-the-
art translation models.
1 Introduction
Dependency structure represents the grammatical
relations that hold between the words in a sentence.
It encodes semantic relations directly, and has the
best inter-lingual phrasal cohesion properties (Fox,
2002). Those attractive characteristics make it pos-
sible to improve translation quality by using depen-
dency structures.
Some researchers pay more attention to use de-
pendency structure on the target side. (Shen et al,
2008) presents a string-to-dependency model, which
restricts the target side of each hierarchical rule to be
a well-formed dependency tree fragment, and em-
ploys a dependency language model to make the out-
put more grammatically. This model significantly
outperforms the state-of-the-art hierarchical phrase-
based model (Chiang, 2005). However, those string-
to-tree systems run slowly in cubic time (Huang et
al., 2006).
Using dependency structure on the source side
is also a promising way, as tree-based systems run
much faster (linear time vs. cubic time, see (Huang
et al, 2006)). Conventional dependency structure
based models (Lin, 2004; Quirk et al, 2005; Ding
and Palmer, 2005; Xiong et al, 2007) typically
employ both substitution and insertion operation to
complete translations, which make it difficult to
specify ordering information directly in the transla-
tion rules. As a result, they have to resort to either
heuristics (Lin, 2004; Xiong et al, 2007) or sepa-
rate ordering models (Quirk et al, 2005; Ding and
Palmer, 2005) to control the word order of transla-
tions.
In this paper, we handle this problem by di-
rectly specifying the ordering information in head-
dependents rules that represent the source side as
head-dependents relations and the target side as
string. The head-dependents rules have only one
substitution operation, thus we don?t face the prob-
lems appeared in previous work and get rid of the
216
heuristics and ordering model. To alleviate data
sparseness problem, we generalize the lexicalized
words in head-dependents relations with their cor-
responding categories.
In the following parts, we first describe the moti-
vation of using head-dependents relations (Section
2). Then we formalize our grammar (Section 3),
present our rule acquisition algorithm (Section 4),
our model (Section 5) and decoding algorithm (Sec-
tion 6). Finally, large-scale experiments (Section 7)
show that our model exhibits good performance on
long distance reordering, and outperforms the state-
of-the-art tree-to-string model (+1.47 BLEU on av-
erage) and hierarchical phrase-based model (+0.46
BLEU on average) on two Chinese-English NIST
test sets. For the first time, a source dependency tree
based model catches up with and surpasses the state-
of-the-art translation models.
2 Dependency Structure and
Head-Dependents Relation
2.1 Dependency Sturcture
A dependency structure for a sentence is a directed
acyclic graph with words as nodes and modification
relations as edges. Each edge direct from a head to
a dependent. Figure 1 (a) shows an example depen-
dency structure of a Chinese sentence.
2010? FIFA??????????
2010 FIFA [World Cup] in/at [South Africa]
successfully hold
Each node is annotated with the part-of-speech
(POS) of the related word.
For convenience, we use the lexicon dependency
grammar (Hellwig, 2006) which adopts a bracket
representation to express a projective dependency
structure. The dependency structure of Figure 1 (a)
can be expressed as:
((2010?) (FIFA)???) (?(??)) (??)??
where the lexicon in brackets represents the depen-
dents, while the lexicon out the brackets is the head.
To construct the dependency structure of a sen-
tence, the most important thing is to establish de-
pendency relations and distinguish the head from the
dependent. Here are some criteria (Zwicky, 1985;
x2:?2:x1:???1: x3:AD3:
??
x1 was held x3 x21  s l  3 2
?/P/???/NR/
??/NR/
??/AD/
2010?/NT/ FIFA/NRI /
??/VV/
/P?/???/NR/ ??/AD/
??/VV/
(a)
(b)
(c)
?? successfullys ssf ll(d)
Figure 1: Examples of dependency structure (a), head-
dependents relation (b), head-dependents rule (r1 of Fig-
ure 2) and head rule (d). Where ?x1:???? and
?x2:?? indicate substitution sites which can be replaced
by a subtree rooted at ????? and ??? respectively.
?x3:AD?indicates a substitution site that can be replaced
by a subtree whose root has part-of-speech ?AD?. The
underline denotes a leaf node.
Hudson, 1990) for identifying a syntactic relation
between a head and a dependent between a head-
dependent pair:
1. head determines the syntactic category of C,
and can often replace C;
2. head determines the semantic category of C;
dependent gives semantic specification.
2.2 Head-Dependents Relation
A head-dependents relation is composed of a head
and all its dependents as shown in Figure 1(b).
Since all the head-dependent pairs satisfy crite-
ria 1 and 2, we can deduce that a head-dependents
relation L holds the property that the head deter-
mines the syntactic and semantic categories of L,
and can often replace L. Therefore, we can recur-
217
sively replace the bottom level head-dependent re-
lations of a dependency structure with their heads
until the root. This implies an representation of the
generation of a dependency structure on the basis of
head-dependents relation.
Inspired by this, we represent the translation rules
of our dependency-to-string model on the founda-
tion of head-dependents relations.
3 Dependency-to-String Grammar
Figure 1 (c) and (d) show two examples of the trans-
lation rules used in our dependency-to-string model.
The former is an example of head-dependent rules
that represent the source side as head-dependents re-
lations and act as both translation rules and reorder-
ing rules. The latter is an example of head rules
which are used for translating words.
Formally, a dependency-to-string grammar is de-
fined as a tuple ??, N,?, R?, where ? is a set of
source language terminals, N is a set of categories
for the terminals in ? , ? is a set of target language
terminals, and R is a set of translation rules. A rule
r in R is a tuple ?t, s, ??, where:
- t is a node labeled by terminal from ?; or a
head-dependents relation of the source depen-
dency structures, with each node labeled by a
terminal from ? or a variable from a set X =
{x1, x2, ...} constrained by a terminal from ?
or a category from N ;
- s ? (X ??)? is the target side string;
- ? is a one-to-one mapping from nonterminals
in t to variables in s.
For example, the head-dependents rule shown in
Figure 1 (c) can be formalized as:
t = ((x1:???) (x2:?) (x3:AD)??)
s = x1 was held x3 x2
? = {x1:???? x1, x2:?? x2, x3:AD? x3}
where the underline indicates a leaf node, and
xi:letters indicates a pair of variable and its con-
straint.
A derivation is informally defined as a sequence
of steps converting a source dependency structure
into a target language string, with each step apply-
ing one translation rule. As an example, Figure 2
?/P???/NR
??/NR
??/AD
2010?/NT FIFA/NRI
??/VV
2010? FIFAI ??? ? ?? ?? ??
?/P???/NR
??/NR
??/AD
2010?/NT FIFA/NRI
was held l
?/P???/NR
??/NR2010?/NT FIFA/NRI
was held successfully l  f ll
?/P
??/NR
2010 FIFA [World Cup] was held successfully  I   [ rl  ]    l   f ll   
??2010 FIFA World Cup was held successfully in  I   rl       l   f ll   i
2010 FIFA World Cup was held successfully in [South Africa] I  rl    l  f ll  i  [ t  fri ]
parser
(a)
(b)
(c)
(d)
(e)
(f)
(g)
r3: (2010?) (FIFA) ??? 
 ?2010 FIFA World Cup
r2: ???successfully
r1: (x1:???)(x2 :?)(x3:AD)??
  ?x1 was held x3 x2
r4: ? (x2:NR)?in x2
r5: ???South Africa
Figure 2: An example derivation of dependency-to-string
translation. The dash lines indicate the reordering when
employing a head-dependents rule.
shows the derivation for translating a Chinese (CH)
sentence into an English (EN) string.
CH 2010? FIFA??????????
EN 2010 FIFA World Cup was held successfully in
South Africa
218
The Chinese sentence (a) is first parsed into a de-
pendency structure (b), which is converted into an
English string in five steps. First, at the root node,
we apply head-dependents rule r1 shown in Figure
1(c) to translate the top level head-dependents rela-
tion and result in three unfinished substructures and
target string in (c). The rule is particular interesting
since it captures the fact: in Chinese prepositional
phrases and adverbs typically modify verbs on the
left, whereas in English prepositional phrases and
adverbs typically modify verbs on the right. Second,
we use head rule r2 translating ???? into ?success-
fully? and reach situation (d). Third, we apply head-
dependents rule r3 translating the head-dependents
relation rooted at ????? and yield (e). Fourth,
head-dependents rules r5 partially translate the sub-
tree rooted at ??? and arrive situation in (f). Finally,
we apply head rule r5 translating the residual node
???? and obtain the final translation in (g).
4 Rule Acquisition
The rule acquisition begins with a word-aligned cor-
pus: a set of triples ?T, S,A?, where T is a source
dependency structure, S is a target side sentence,
and A is an alignment relation between T and S.
We extract from each triple ?T, S,A? head rules that
are consistent with the word alignments and head-
dependents rules that satisfy the intuition that syn-
tactically close items tend to stay close across lan-
guages. We accomplish the rule acquisition through
three steps: tree annotation, head-dependents frag-
ments identification and rule induction.
4.1 Tree Annotation
Given a triple ?T, S,A? as shown in Figure 3, we
first annotate each node n of T with two attributes:
head span and dependency span, which are defined
as follows.
Definition 1. Given a node n, its head span hsp(n)
is a set of index of the target words aligned to n.
For example, hsp(2010?)={1, 5}, which corre-
sponds to the target words ?2010? and ?was?.
Definition 2. A head span hsp(n) is consistent if it
satisfies the following property:
?n? ?=nhsp(n?) ? hsp(n) = ?.
?/P
{5,8}{9,10}{ , }{ , }
???/NR
{3,4}{2-4}
??/NR
{9,10}{9,10}
??/AD
{7}{7}
2010?/NT
{1,5}{}{ , }{}
FIFA/NR
{2,2}{2,2}
??/VV
{6}{2-10}
2010
1
FIFA
2
I World
3
rl held
6
l successfully
7
f ll in
8
i South
9
tCup
4
was
5
Africa
10
fri
Figure 3: An annotated dependency structure. Each node
is annotated with two spans, the former is head span and
the latter dependency span. The nodes in acceptable head
set are displayed in gray, and the nodes in acceptable de-
pendent set are denoted by boxes. The triangle denotes
the only acceptable head-dependents fragment.
For example, hsp(??) is consistent, while
hsp(2010?) is not consistent since hsp(2010?) ?
hsp(?) = 5.
Definition 3. Given a head span hsp(n), its closure
cloz(hsp(n)) is the smallest contiguous head span
that is a superset of hsp(n).
For example, cloz(hsp(2010?)) = {1, 2, 3, 4, 5},
which corresponds to the target side word sequence
?2010 FIFA World Cup was?. For simplicity, we use
{1-5} to denotes the contiguous span {1, 2, 3, 4, 5}.
Definition 4. Given a subtree T ? rooted at n, the
dependency span dsp(n) of n is defined as:
dsp(n) = cloz(
?
n??T ?
hsp(n?) is consistent
hsp(n?)).
If the head spans of all the nodes of T ? is not consis-
tent, dsp(n) = ?.
For example, since hsp(?) is not consistent,
dsp(?)=dsp(??)={9, 10}, which corresponds to
the target words ?South? and ?Africa?.
The tree annotation can be accomplished by a sin-
gle postorder transversal of T . The extraction of
head rules from each node can be readily achieved
with the same criteria as (Och and Ney, 2004). In
219
the following, we focus on head-dependents rules
acquisition.
4.2 Head-Dependents Fragments Identification
We then identify the head-dependents fragments that
are suitable for rule induction from the annotated de-
pendency structure.
To facilitate the identification process, we first de-
fine two sets of dependency structure related to head
spans and dependency spans.
Definition 5. A acceptable head set ahs(T) of a de-
pendency structure T is a set of nodes, each of which
has a consistent head span.
For example, the elements of the acceptable head
set of the dependency structure in Figure 3 are dis-
played in gray.
Definition 6. A acceptable dependent set adt(T) of
a dependency structure T is a set of nodes, each of
which satisfies: dep(n) ?= ?.
For example, the elements of the acceptable de-
pendent set of the dependency structure in Figure 3
are denoted by boxes.
Definition 7. We say a head-dependents fragments
is acceptable if it satisfies the following properties:
1. the root falls into acceptable head set;
2. all the sinks fall into acceptable dependent set.
An acceptable head-dependents fragment holds
the property that the head span of the root and the de-
pendency spans of the sinks do not overlap with each
other, which enables us to determine the reordering
in the target side.
The identification of acceptable head-dependents
fragments can be achieved by a single preorder
transversal of the annotated dependency structure.
For each accessed internal node n, we check
whether the head-dependents fragment f rooted at
n is acceptable. If f is acceptable, we output an
acceptable head-dependents fragment; otherwise we
access the next node.
Typically, each acceptable head-dependents frag-
ment has three types of nodes: internal nodes, inter-
nal nodes of the dependency structure; leaf nodes,
leaf nodes of the dependency structure; head node, a
special internal node acting as the head of the related
head-dependents relation.
?/P
{5,8}{9,10}
/
{ , }{ , }
???/NR
{3,4}{2-4}
??/AD
{7}{7}
??/VV
{6}{2-10}
heldl successfullys ssf ll[FIFA World Cup][ I  rl  ] South Africa][ t  fri ]
Input:
Output:
x2:?2:x1:???1: ??
??
x1 held successfully x21  l  s ssf ll  2
(x1:???)(x2:?)(??) ??
      ?  x1  held successfully x2
(a)
(b)
Figure 4: A lexicalized head-dependents rule (b) induced
from the only acceptable head-dependents fragment (a)
of Figure 3.
4.3 Rule Induction
From each acceptable head-dependents fragment,
we induce a set of lexicalized and unlexicalized
head-dependents rules.
4.3.1 Lexicalized Rule
We induce a lexicalized head-dependents rule
from an acceptable head-dependents fragment by
the following procedure:
1. extract the head-dependents relation and mark
the internal nodes as substitution sites. This
forms the input of a head-dependents rule;
2. place the nodes in order according to the head
span of the root and the dependency spans of
the sinks, then replace the internal nodes with
variables and the other nodes with the target
words covered by their head spans. This forms
the output of a head-dependents rule.
Figure 4 shows an acceptable head-dependents
fragment and a lexicalized head-dependents rule in-
220
duced from it.
4.3.2 Unlexicalized Rules
Since head-dependents relations with verbs as
heads typically consist of more than four nodes, em-
ploying only lexicalized head-dependents rules will
result in severe sparseness problem. To alleviate
this problem, we generalize the lexicalized head-
dependents rules and induce rules with unlexicalized
nodes.
As we know, the modification relation of a head-
dependents relation is determined by the edges.
Therefore, we can replace the lexical word of each
node with its categories (i.e. POS) and obtain new
head-dependents relations with unlexicalized nodes
holding the same modification relation. Here we call
the lexicalized and unlexicalized head-dependents
relations as instances of the modification relation.
For a head-dependents relation with m node, we can
produce 2m ? 1 instances with unlexicalized nodes.
Each instance represents the modification relation
with a different specification.
Based on this observation, from each lexical-
ized head-dependent rule, we generate new head-
dependents rules with unlexicalized nodes according
to the following principles:
1. change the aligned part of the target string into
a new variable when turning a head node or a
leaf node into its category;
2. keep the target side unchanged when turning a
internal node into its category.
Restrictions: Since head-dependents relations
with verbs as heads typically consists of more than
four nodes, enumerating all the instances will re-
sult in a massive grammar with too many kinds of
rules and inflexibility in decoding. To alleviate these
problems, we filter the grammar with the following
principles:
1. nodes of the same type turn into their categories
simultaneously.
2. as for leaf nodes, only those with open class
words can be turned into their categories.
In our experiments of this paper, we only
turn those dependents with POS tag in the
set of {CD,DT,OD,JJ,NN,NR,NT,AD,FW,PN}
into their categories.
x2:?2:x1:???1: ??
heldl successfullyf ll
??
x11 x22
(x1:???)(x2:?)(??) ??
 ? x1  held successfully x2
x2:?2:x1:???1: x3:AD:
heldl x33
??
x11 x22
(x1:???)(x2:?)(x3:AD) ??
 ? x1  held x3 x2
x2:P2:x1:NR1: ??
heldl successfullyf ll
??
x11 x22
(x1:NR)(x2:P)(??) ??
? x1  held successfully x2
x2:P2:x1:NR1: x3:AD3:
heldl x33
??
x11 x22
(x1:NR)(x2:P)(x3:AD) ??
 ? x1  held x3 x2
x2:?2:x1:???1: ??
x44 successfullyf ll
x4:VV4:
x11 x22
(x1:???)(x2:?)(??) x4:VV
 ? x1  x4 successfully x2
x2:?2:x1:???1: x3:AD3:
x44 x33
x4:VV4:
x11 x22
(x1:???)(x2:?)(x3:AD) x4:VV
 ?   x1  x4  x3 x2
x2:P2:x1:NR1: ??
x44 successfullyf ll
x4:VV4:
x11 x22
(x1:NR)(x2:P)(??) x4:VV
 ? x1 x4 successfully x2
x2:P2:x1:NR1: x3:AD3:
x44 x33
x4:VV4:
x11 x22
(x1:NR)(x2:P)(x3:AD) x4:VV
 ? x1  x4 x3 x2
generalize leaf generalize leaf
generalize internalgeneralize internal
generalize leaf generalize leaf
generalize
head
Figure 5: An illustration of rule generalization. Where
?x1:???? and ?x2:?? indicate substitution sites
which can be replaced by a subtree rooted at ?????
and ??? respectively. ?x3:AD?indicates a substitution
site that can be replaced by a subtree whose root has part-
of-speech ?AD?. The underline denotes a leaf node. The
box indicates the starting lexicalized head-dependents
rule.
Figure 5 illustrates the rule generalization process
under these restrictions.
4.3.3 Unaligned Words
We handle the unaligned words of the target side
by extending the head spans of the lexicalized head
and leaf nodes on both left and right directions.
This procedure is similar with the method of (Och
and Ney, 2004) except that we might extend several
221
Algorithm 1: Algorithm for Rule Acquisition
Input: Source dependency structure T , target string S, alignment A
Output: Translation rule set R
1 HSet? ACCEPTABLE HEAD(T ,S,A)
2 DSet? ACCEPTABLE DEPENDENT(T ,S,A)
3 for each node n ? HSet do
4 extract head rules
5 append the extracted rules to R
6 if ?n? ? child(n) n? ? DSet
7 then
8 obtain a head-dependent fragment f
9 induce lexicalized and unlexicalized head-dependents rules from f
10 append the induced rules to R
11 end
12 end
spans simultaneously. In this process, we might ob-
tain m(m ? 1) head-dependents rules from a head-
dependent fragment in handling unaligned words.
Each of these rules is assigned with a fractional
count 1/m.
4.4 Algorithm for Rule Acquisition
The rule acquisition is a three-step process, which is
summarized in Algorithm 1.
We take the extracted rule set as observed data and
make use of relative frequency estimator to obtain
the translation probabilities P (t|s) and P (s|t).
5 The model
Following (Och and Ney, 2002), we adopt a general
log-linear model. Let d be a derivation that convert
a source dependency structure T into a target string
e. The probability of d is defined as:
P (d) ?
?
i
?i(d)?i (1)
where ?i are features defined on derivations and ?i
are feature weights. In our experiments of this paper,
we used seven features as follows:
- translation probabilities P (t|s) and P (s|t);
- lexical translation probabilities Plex(t|s) and
Plex(s|t);
- rule penalty exp(?1);
- language model Plm(e);
- word penalty exp(|e|).
6 Decoding
Our decoder is based on bottom up chart parsing.
It finds the best derivation d? that convert the input
dependency structure into a target string among all
possible derivations D:
d? = argmaxd?DP (D) (2)
Given a source dependency structure T , the decoder
transverses T in post-order. For each accessed in-
ternal node n, it enumerates all instances of the re-
lated modification relation of the head-dependents
relation rooted at n, and checks the rule set for
matched translation rules. If there is no matched
rule, we construct a pseudo translation rule accord-
ing to the word order of the head-dependents rela-
tion. For example, suppose that we can not find
any translation rule about to ?(2010?) (FIFA) ?
???, we will construct a pseudo translation rule
?(x1:2010?) (x2:FIFA) x3:??? ? x1 x2 x3?.
A larger translation is generated by substituting the
variables in the target side of a translation rule with
the translations of the corresponding dependents.
We make use of cube pruning (Chiang, 2007; Huang
and Chiang, 2007) to find the k-best items with inte-
grated language model for each node.
To balance performance and speed, we prune the
search space in several ways. First, beam thresh-
222
old ? , items with a score worse than ? times of the
best score in the same cell will be discarded; sec-
ond, beam size b, items with a score worse than the
bth best item in the same cell will be discarded. The
item consist of the necessary information used in de-
coding. Each cell contains all the items standing for
the subtree rooted at it. For our experiments, we set
? = 10?3 and b = 300. Additionally, we also prune
rules that have the same source side (b = 100).
7 Experiments
We evaluated the performance of our dependency-
to-string model by comparison with replications of
the hierarchical phrase-based model and the tree-to-
string models on Chinese-English translation.
7.1 Data preparation
Our training corpus consists of 1.5M sentence
pairs from LDC data, including LDC2002E18,
LDC2003E07, LDC2003E14, Hansards portion of
LDC2004T07, LDC2004T08 and LDC2005T06.
We parse the source sentences with Stanford
Parser (Klein and Manning, 2003) into projective
dependency structures, whose nodes are annotated
by POS tags and edges by typed dependencies. In
our implementation of this paper, we make use of
the POS tags only.
We obtain the word alignments by running
GIZA++ (Och and Ney, 2003) on the corpus in
both directions and applying ?grow-diag-and? re-
finement(Koehn et al, 2003).
We apply SRI Language Modeling Toolkit (Stol-
cke, 2002) to train a 4-gram language model with
modified Kneser-Ney smoothing on the Xinhua por-
tion of the Gigaword corpus.
We use NIST MT Evaluation test set 2002 as our
development set, NIST MT Evaluation test set 2004
(MT04) and 2005 (MT05) as our test set. The qual-
ity of translations is evaluated by the case insensitive
NIST BLEU-4 metric (Papineni et al, 2002).1
We make use of the standard MERT (Och, 2003)
to tune the feature weights in order to maximize the
system?s BLEU score on the development set.
1ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v11b.pl
System Rule # MT04(%) MT05(%)
cons2str 30M 34.55 31.94
hiero-re 148M 35.29 33.22
dep2str 56M 35.82+ 33.62+
Table 1: Statistics of the extracted rules on training cor-
pus and the BLEU scores on the test sets. Where ?+?
means dep2str significantly better than cons2str with p <
0.01.
7.2 The baseline models
We take a replication of Hiero (Chiang, 2007) as
the hierarchical phrase-based model baseline. In
our experiments of this paper, we set the beam size
b = 200 and the beam threshold ? = 0. The maxi-
mum initial phrase length is 10.
We use constituency-to-string model (Liu et al,
2006) as the syntax-based model baseline which
make use of composed rules (Galley et al, 2006)
without handling the unaligned words. In our exper-
iments of this paper, we set the tatTable-limit=20,
tatTable-threshold=10?1, stack-limit=100, stack-
threshold=10?1,hight-limit=3, and length-limit=7.
7.3 Results
We display the results of our experiments in Table
1. Our dependency-to-string model dep2str signif-
icantly outperforms its constituency structure-based
counterpart (cons2str) with +1.27 and +1.68 BLEU
on MT04 and MT05 respectively. Moreover, with-
out resort to phrases or parse forest, dep2str sur-
passes the hierarchical phrase-based model (hiero-
re) over +0.53 and +0.4 BLEU on MT04 and MT05
respectively on the basis of a 62% smaller rule set.
Furthermore, We compare some actual transla-
tions generated by cons2str, hiero-re and dep2str.
Figure 6 shows two translations of our test sets
MT04 and MT05, which are selected because each
holds a long distance dependency commonly used in
Chinese.
In the first example, the Chinese input holds
a complex long distance dependencies ?? ?
? ?... ?...? ???. This dependency cor-
responds to sentence pattern ?noun+prepostional
phrase+prepositional phrase+verb?, where the for-
mer prepositional phrase specifies the position and
the latter specifies the time. Both cons2str and
hiero-re are confused by this sentence and mistak-
223
??? ? ?? ??? ? ?? ?? ?? ? ?? ?
Afterft r briefri f talkst l withit Powellll ,, thet US State Departmentt t  rt t Barnierr i r ,,saidi
MT05----Segment 163
Reference: After a brief talk with 
Powell at the US State 
Department, Barnier said:
Cons2str: Barnier after brief 
talks in US State Department 
and Powell  said:
Hiero-re: After a short meeting 
with Barnier on the US State 
Department, Powell said:
Dep2str: After brief talks with 
Powell, the US State 
Department Barnier said,
?? ?? ?? ??? ? ?? ?? 1373 2001 ?
Chinai appreciatesr i t efforts
 
ff rt off Anti -ti - Terrorismrr ri Committee itt tot promoter t allll inicountriestri
MT04----Segment 1096
Reference: China appreciates the 
efforts of the Counter-Terrorism 
Committee to promote the 
implementation of the resolution 
1373(2001) in all states and to 
help enhance the anti-terrorist 
capabilities of developing 
countries.
Cons2str: China appreciates 
Anti - Terrorist Committee for 
promoting implementation of 
the resolution No. 1373(2001) 
and help developing countries 
strength counter-terrorism  
capability building for the 
efforts,
Hiero-re: China appreciates 
Anti - Terrorism Committee to 
promote countries implement 
resolution No . 1373 ( 2001 ) 
and help developing countries 
strengthen anti-terrorism 
capacity building support for 
efforts
Dep2str: China appreciates 
efforts of Anti - Terrorism 
Committee to promote all 
countries in the implementation 
of resolution  1373 ( 2001 )  , to 
help strengthen the anti-
terrorism capability building of 
developing countries
?? ? ? ? ?? ??? ????? ? ? ?
nsubj             
prep prep
????? ? ?? ?
thet implementationi l t ti off ......
nsubj dobj
Figure 6: Actual translations produced by the baselines and our system. For our system, we also display the long
distance dependencies correspondence in Chinese and English. Here we omit the edges irrelevant to the long distance
dependencies.
enly treat ???(Powell)? as the subjective, thus
result in translations with different meaning from
the source sentence. Conversely, although ??? is
falsely translated into a comma, dep2str captures
this complex dependency and translates it into ?Af-
ter ... ,(should be at) Barnier said?, which accords
with the reordering of the reference.
In the second example, the Chinese input holds
a long distance dependency ??? ?? ... ?
?? which corresponds to a simple pattern ?noun
phrase+verb+noun phrase?. However, due to the
modifiers of ???? which contains two sub-
sentences including 24 words, the sentence looks
rather complicated. Cons2str and hiero-re fail to
capture this long distance dependency and provide
monotonic translations which do not reflect the
meaning of the source sentence. In contrast, dep2str
successfully captures this long distance dependency
and translates it into ?China appreciates efforts of
...?, which is almost the same with the reference
?China appreciates the efforts of ...?.
All these results prove the effectiveness of our
dependency-to-string model in both translation and
long distance reordering. We believe that the ad-
vantage of dep2str comes from the characteristics of
dependency structures tending to bring semantically
related elements together (e.g., verbs become adja-
cent to all their arguments) and are better suited to
lexicalized models (Quirk et al, 2005). And the in-
capability of cons2str and hiero-re in handling long
distance reordering of these sentences does not lie in
the representation of translation rules but the com-
promises in rule extraction or decoding so as to bal-
ance the speed or grammar size and performance.
The hierarchical phrase-based model prohibits any
nonterminal X from spanning a substring longer
than 10 on the source side to make the decoding al-
gorithm asymptotically linear-time (Chiang, 2005).
224
While constituency structure-based models typically
constrain the number of internal nodes (Galley et
al., 2006) and/or the height (Liu et al, 2006) of
translation rules so as to balance the grammar size
and performance. Both strategies limit the ability of
the models in processing long distance reordering of
sentences with long and complex modification rela-
tions.
8 Related Works
As a first step towards semantics, dependency struc-
tures are attractive to machine translation. And
many efforts have been made to incorporating this
desirable knowledge into machine translation.
(Lin, 2004; Quirk et al, 2005; Ding and Palmer,
2005; Xiong et al, 2007) make use of source depen-
dency structures. (Lin, 2004) employs linear paths
as phrases and view translation as minimal path cov-
ering. (Quirk et al, 2005) extends paths to treelets,
arbitrary connected subgraphs of dependency struc-
tures, and propose a model based on treelet pairs.
Both models require projection of the source depen-
dency structure to the target side via word alignment,
and thus can not handle non-isomorphism between
languages. To alleviate this problem, (Xiong et al,
2007) presents a dependency treelet string corre-
spondence model which directly map a dependency
structure to a target string. (Ding and Palmer, 2005)
presents a translation model based on Synchronous
Dependency Insertion Grammar(SDIG), which han-
dles some of the non-isomorphism but requires both
source and target dependency structures. Most im-
portant, all these works do not specify the ordering
information directly in translation rules, and resort
to either heuristics (Lin, 2004; Xiong et al, 2007) or
separate ordering models(Quirk et al, 2005; Ding
and Palmer, 2005) to control the word order of
translations. By comparison, our model requires
only source dependency structure, and handles non-
isomorphism and ordering problems simultaneously
by directly specifying the ordering information in
the head-dependents rules that represent the source
side as head-dependents relations and the target side
as strings.
(Shen et al, 2008) exploits target dependency
structures as dependency language models to ensure
the grammaticality of the target string. (Shen et al,
2008) extends the hierarchical phrase-based model
and present a string-to-dependency model, which
employs string-to-dependency rules whose source
side are string and the target as well-formed depen-
dency structures. In contrast, our model exploits
source dependency structures, as a tree-based sys-
tem, it run much faster (linear time vs. cubic time,
see (Huang et al, 2006)).
9 Conclusions and future work
In this paper, we present a novel dependency-to-
string model, which employs head-dependents rules
that represent the source side as head-dependents
relations and the target side as string. The head-
dependents rules specify the ordering information
directly and require only substitution operation.
Thus, our model does not need heuristics or order-
ing model of the previous works to control the word
order of translations. Large scale experiments show
that our model exhibits good performance in long
distance reordering and outperforms the state-of-
the-art constituency-to-string model and hierarchi-
cal phrase-based model without resort to phrases and
parse forest. For the first time, a source dependency-
based model shows improvement over the state-of-
the-art translation models.
In our future works, we will exploit the semantic
information encoded in the dependency structures
which is expected to further improve the transla-
tions, and replace 1-best dependency structures with
dependency forests so as to alleviate the influence
caused by parse errors.
Acknowledgments
This work was supported by National Natural Sci-
ence Foundation of China, Contract 60736014,
60873167, 90920004. We are grateful to the anony-
mous reviewers for their thorough reviewing and
valuable suggestions. We appreciate Yajuan Lv,
Wenbin Jiang, Hao Xiong, Yang Liu, Xinyan Xiao,
Tian Xia and Yun Huang for the insightful advices in
both experiments and writing. Special thanks goes
to Qian Chen for supporting my pursuit all through.
References
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
225
ACL 2005, pages 263?270.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33.
Yuan Ding and Martha Palmer. 2005. Machine trans-
lation using probabilistic synchronous dependency in-
sertion grammars. In Proceedings of ACL 2005.
Heidi J. Fox. 2002. Phrasal cohesion and statistical ma-
chine translation. In In Proceedings of EMNLP 2002,
pages 304?311.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training of
context-rich syntactic translation models. In Proceed-
ings of ACL 2006, pages 961?968, Sydney, Australia,
July. Association for Computational Linguistics.
Peter Hellwig. 2006. Parsing with dependency gram-
mars. In Dependenz und Valenz / Dependency and Va-
lency, volume 2, pages 1081?1109. Berlin, New York.
Liang Huang and David Chiang. 2007. Forest rescor-
ing: Faster decoding with integrated language models.
In Proceedings of ACL 2007, pages 144?151, Prague,
Czech Republic, June.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
A syntax-directed translator with extended domain of
locality. In Proceedings of the Workshop on Computa-
tionally Hard Problems and Joint Inference in Speech
and Language Processing, pages 1?8, New York City,
New York, June. Association for Computational Lin-
guistics.
Richard Hudson. 1990. English Word Grammar. Black-
ell.
Dan Klein and Christopher D.Manning. 2003. Fast exact
inference with a factored model for natural language
parsing. In In Advances in Neural Information Pro-
cessing Systems 15 (NIPS, pages 3?10. MIT Press.
Philipp Koehn, Franz J. Och, and Daniel Marcu. 2003.
Statistical phrase-based translation. In Proceedings
of the 2003 Human Language Technology Conference
of the North American Chapter of the Association for
Computational Linguistics, Edmonton, Canada, July.
Dekang Lin. 2004. A path-based transfer model for
machine translation. In Proceedings of Coling 2004,
pages 625?630, Geneva, Switzerland, Aug 23?Aug
27.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-
string alignment template for statistical machine trans-
lation. In Proceedings of ACL 2006, pages 609?616,
Sydney, Australia, July.
Franz Josef Och and Hermann Ney. 2002. Discrimi-
native training and maximum entropy models for sta-
tistical machine translation. In Proceedings of 40th
Annual Meeting of the Association for Computational
Linguistics, pages 295?302, Philadelphia, Pennsylva-
nia, USA, July.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 29(1):19?51.
Franz Josef Och and Hermann Ney. 2004. The alignment
template approach to statistical machine translation.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of ACL-
2003, pages 160?167, Sapporo, Japan, July.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic
evaluation of machine translation. In Proceedings of
ACL 2002, pages 311?318, Philadelphia, Pennsylva-
nia, USA, July.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. De-
pendency treelet translation: Syntactically informed
phrasal smt. In Proceedings of ACL 2005, pages 271?
279.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008.
A new string-to-dependency machine translation al-
gorithm with a target dependency language model.
In Proceedings of ACL 2008: HLT, pages 577?585,
Columbus, Ohio, June. Association for Computational
Linguistics.
Andreas Stolcke. 2002. Srilm - an extensible language
modeling toolkit. In Proceedings of ICSLP, volume 30,
pages 901?904.
Deyi Xiong, Qun Liu, and Shouxun Lin. 2007. A depen-
dency treelet string correspondence model for statisti-
cal machine translation. In Proceedings of the Second
Workshop on Statistical Machine Translation, pages
40?47, Prague, Czech Republic, June.
Arnold M. Zwicky. 1985. Heads. Journal of Linguistics,
21:1?29.
226
Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1066?1076,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Translation with Source Constituency and Dependency Trees
Fandong Meng?? Jun Xie? Linfeng Song?? Yajuan Lu?? Qun Liu??
?Key Laboratory of Intelligent Information Processing
Institute of Computing Technology, Chinese Academy of Sciences
?University of Chinese Academy of Sciences
{mengfandong,xiejun,songlinfeng,lvyajuan}@ict.ac.cn
?Centre for Next Generation Localisation
Faculty of Engineering and Computing, Dublin City University
qliu@computing.dcu.ie
Abstract
We present a novel translation model, which
simultaneously exploits the constituency and
dependency trees on the source side, to com-
bine the advantages of two types of trees. We
take head-dependents relations of dependency
trees as backbone and incorporate phrasal n-
odes of constituency trees as the source side
of our translation rules, and the target side as
strings. Our rules hold the property of long
distance reorderings and the compatibility
with phrases. Large-scale experimental result-
s show that our model achieves significantly
improvements over the constituency-to-string
(+2.45 BLEU on average) and dependency-
to-string (+0.91 BLEU on average) model-
s, which only employ single type of trees,
and significantly outperforms the state-of-the-
art hierarchical phrase-based model (+1.12
BLEU on average), on three Chinese-English
NIST test sets.
1 Introduction
In recent years, syntax-based models have become a
hot topic in statistical machine translation. Accord-
ing to the linguistic structures, these models can be
broadly divided into two categories: constituency-
based models (Yamada and Knight, 2001; Graehl
and Knight, 2004; Liu et al, 2006; Huang et al,
2006), and dependency-based models (Lin, 2004;
Ding and Palmer, 2005; Quirk et al, 2005; Xiong
et al, 2007; Shen et al, 2008; Xie et al, 2011).
These two kinds of models have their own advan-
tages, as they capture different linguistic phenome-
na. Constituency trees describe how words and se-
quences of words combine to form constituents, and
constituency-based models show better compatibil-
ity with phrases. However, dependency trees de-
scribe the grammatical relation between words of
the sentence, and represent long distance dependen-
cies in a concise manner. Dependency-based mod-
els, such as dependency-to-string model (Xie et al,
2011), exhibit better capability of long distance re-
orderings.
In this paper, we propose to combine the advan-
tages of source side constituency and dependency
trees. Since the dependency tree is structurally sim-
pler and directly represents long distance depen-
dencies, we take dependency trees as the backbone
and incorporate constituents to them. Our mod-
el employs rules that represent the source side as
head-dependents relations which are incorporated
with constituency phrasal nodes, and the target side
as strings. A head-dependents relation (Xie et al,
2011) is composed of a head and all its dependents in
dependency trees, and it encodes phrase pattern and
sentence pattern (typically long distance reordering
relations). With the advantages of head-dependents
relations, the translation rules of our model hold the
property of long distance reorderings and the com-
patibility with phrases.
Our new model (Section 2) extracts rules from
word-aligned pairs of source trees (constituency
and dependency) and target strings (Section 3), and
translate source trees into target strings by employ-
ing a bottom-up chart-based algorithm (Section 4).
Compared with the constituency-to-string (Liu et al,
2006) and dependency-to-string (Xie et al, 2011)
models that only employ a single type of trees, our
1066
??/VV
???/NR ?/AD ???/NN
??/NR ?/M ??/JJ
??/OD
NP1
VP2
VP3
????? ? ????? ? ????
NR AD VV NR OD M JJ NN
NP1
CLP
QP
NP
NP
VP2
ADVP
VP3
NP
IP
(a)
(c)
Intel         will   launch  Asia     first              super     laptop
Chinese: ??? ? ?? ?? ?? ? ?? ???
English:  Intel will launch the first Ultrabook in Asia
ADVP NP
(b)
Figure 1: Illustration of phrases that can not be captured by a dependency tree (b) while captured by a constituency tree
(a), where the bold phrasal nodes NP1,VP2,VP3 indicate the phrases which can not be captured by dependency syn-
tactic phrases. (c) is the corresponding bilingual sentences. The subscripts of phrasal nodes are used for distinguishing
the nodes with same phrasal categories.
approach yields encouraging results by exploiting t-
wo types of trees. Large-scale experiments (Sec-
tion 5) on Chinese-English translation show that
our model significantly outperforms the state-of-
the-art single constituency-to-string model by av-
eraged +2.45 BLEU points, dependency-to-string
model by averaged +0.91 BLEU points, and hierar-
chical phrase-based model (Chiang, 2005) by aver-
aged +1.12 BLEU points, on three Chinese-English
NIST test sets.
2 Grammar
We take head-dependents relations of dependency
trees as backbone and incorporate phrasal nodes of
constituency trees as the source side of our transla-
tion rules, and the target side as strings. A head-
dependents relation consists of a head and all its de-
pendents in dependency trees, and it can represent
long distance dependencies. Incorporating phrasal
nodes of constituency trees into head-dependents
relations further enhances the compatibility with
phrases of our rules. Figure 1 shows an example of
phrases which can not be captured by a dependen-
cy tree while captured by a constituency tree, such
as the bold phrasal nodes NP1,VP2 and VP3. The
phrasal node NP1 in the constituency tree indicates
that ??? )P? is a noun phrase and it should
be translated as a basic unit, while in the depen-
dency tree it is a non-syntactic phrase. The head-
dependents relation in the top level of the dependen-
cy tree presents long distance dependencies of the
words ?=A?, ???, ????, and ?)P? in a
concise manner, which is useful for long distance re-
ordering. We adopt this kind of rule representation
to hold the property of long distance reorderings and
the compatibility with phrases.
Figure 2 shows two examples of our translation
rules corresponding to the top level of Figure 1-(b).
We can see that r1 captures a head-dependents rela-
tion, while r2 extends r1 by incorporating a phrasal
node VP2 to replace the two nodes ???/VV? and
?)P/NN?. As shown in Figure 1-(b), VP2 con-
sists of two parts, a head node ???/VV? and a
subtree rooted at the dependent node ?)P/NN?.
Therefore, we use VP2 and the POS tags of the t-
wo nodes VV and NN to denote the part covered
by VP2 in r2, to indicate that the source sequence
covered by VP2 can be translated by a bilingual
phrase. Since VP2 covers a head node ???/VV?,
we represent r2 by constructing a new head node
1067
1
??
??? ? 1
1
2
1 2
??? ? 1
Figure 2: Two examples of our translation rules corre-
sponding to the top level of Figure 1-(b). r1 captures a
head-dependents relation, and r2 extends r1 by incorpo-
rating a phrasal node VP2. ?x1:NN? indicates a substitu-
tion site which can be replaced by a subtree whose root
has POS tag ?NN?. ?x1:VP2|||VV NN? indicates a sub-
stitution site which can be replaced by a source phrase
covered by a phrasal node VP (the phrasal node consist-
s of two dependency nodes with POS tag VV and NN,
respectively). The underline denotes a leaf node.
VP2|||VV NN. For simplicity, we use a shorten for-
m CHDR to represent the head-dependents relations
with/without constituency phrasal nodes.
Formally, our grammar G is defined as a 5-tuple
G = ??, Nc, Nd,?, R?, where ? is a set of source
language terminals, Nc is a set of constituency
phrasal categories, Nd is a set of categories (POS
tags) for the terminals in ?, ? is a set of target lan-
guage terminals, and R is a set of translation rules
that include bilingual phrases for translating source
language terminals and CHDR rules for translation
and reordering. A CHDR rule is represented as a
triple ?t, s,??, where:
? t is CHDR with each node labeled by a ter-
minal from ? or a variable from a set X =
{x1, x2, ? ? ? } constrained by a terminal from ?
or a category from Nd or a joint category (con-
structed by the categories from Nc and Nd);
? s ? (X ??) denotes the target side string;
? ? denotes one-to-one links between nontermi-
nals in t and variables in s.
We use the lexicon dependency grammar (Hellwig,
2006) which adopts a bracket representation to ex-
press the head-dependents relation and CHDR. For
example, the left-hand sides of r1 and r2 in Figure 2
can be respectively represented as follows:
(=A) (?)?? (x1:NN)
(=A) (?) x1:VP2|||VV NN
??/VV
???/NR ?/AD ???/NN
??/NR?/M ??/JJ
??/OD
NP1
VP2
VP3
??? ? ?? ?? ?? ? ?? ???
Parseing      Labelling
???/NR ?/AD launch
Intel will launch ????? in Asia
Intel will launch in Asia
(a)
(b)
(c)
(d)
(e)
NP1
?/M
??/OD
Intel will launch in Asiathe    first(f)
Ultrabook
Ultrabook
???/NN
??/NR?/M ??/JJ
??/OD
NP1
r3
r4 r5
r6
r7
?/M
??/OD
r8
(x1:NR) (x2:AD) ?? (x3:???) x1 x2 launch x3
Intel???
? will
(??)(x1:M)x2:NP1|||JJ_NN x1 x2 in Aisa 
????? Ultrabook
?? (?) the first
Translation Rules
r3
r4
r5
r6
r7
r8
(g)
Figure 3: An example derivation of translation. (g) lists
all the translation rules. r3, r6 and r8 are CHDR rules,
while r4, r5 and r7 are bilingual phrases, which are used
for translating source terminals. The dash lines indicate
the reordering when employing a translation rule.
The formalized presentation of r2 in Figure 2-(b):
t = (=A) (?) x1:VP2|||VV NN
s = Intel will x1
?= x1:VP2|||VV NN ? x1
where the underline indicates a leaf node.
Figure 3 gives an example of the translation
derivation in our model, with the translation rules
1068
listed in (g). r3, r6 and r8 are CHDR rules, while
r4, r5 and r7 are bilingual phrases, which are used
for translating source language terminals. Given a
sentence to translate in (a), we first parse it into a
constituency tree and a dependency tree, then label
the phrasal nodes from the constituency tree to the
dependency tree, and yield (b). Then, we translate
it into a target string by the following steps. At the
root node, we apply rule r3 to translate the top level
head-dependents relation and results in four unfin-
ished substructures and target strings in (c). From
(c) to (d), there are three steps (one rule for one step).
We use r4 to translate ?=A? to ?Intel?, r5 to
translate ??? to ?will?, and r6 to translate the right-
most unfinished part. Then, we apply r7 to translate
the phrase ???)P? to ?Ultrabook?, and yield
(e). Finally, we apply r8 to translate the last frag-
ment to ?the first?, and get the final result (f).
3 Rule Extraction
In this section, we describe how to extract rules from
a set of 4-tuples ?C, T, S,A?, where C is a source
constituency tree, T is a source dependency tree, S
is a target side sentence, and A is an word alignmen-
t relation between T /C and S. We extract CHDR
rules from each 4-tuple ?C, T, S,A? based on GHK-
M algorithm (Galley et al, 2004) with three steps:
1. Label the dependency tree with phrasal nodes
from the constituency tree, and annotate align-
ment information to the phrasal nodes labeled
dependency tree (Section 3.1).
2. Identify acceptable CHDR fragments from the
annotated dependency tree for rule induction
(Section 3.2).
3. Induce a set of lexicalized and generalized
CHDR rules from the acceptable fragments
(Section 3.3).
3.1 Annotation
Given a 4-tuple ?C, T, S,A?, we first label phrasal
nodes from the constituency tree C to the depen-
dency tree T , which can be easily accomplished by
phrases mapping according to the common covered
source sequences. As dependency trees can capture
some phrasal information by dependency syntactic
??/VV
{3-3}{1-8}
???/NR
{1-1}{1-1}
?/AD
{2-2}{2-2}
???/NN
{6-6}{4-8}
??/NR
{7-8}{7-8}
?/M
{null}{4-5}
??/JJ
{6-6}{6-6}
??/OD
{4-5}{4-5}
NP1
<6-6>
VP2
<3-8>
VP3
<2-8>
Figure 4: An annotated dependency tree. Each node is
annotated with two spans, the former is node span and
the latter subtree span. The fragments covered by phrasal
nodes are annotated with phrasal spans. The nodes de-
noted by the solid line box are not nsp consistent.
phrases, in order to complement the information that
dependency trees can not capture, we only label the
phrasal nodes that cover dependency non-syntactic
phrases.
Then, we annotate alignment information to the
phrasal nodes labeled dependency tree T , as shown
in Figure 4. For description convenience, we make
use of the notion of spans (Fox, 2002; Lin, 2004).
Given a node n in the source phrasal nodes labeled
T with word alignment information, the spans of n
induced by the word alignment are consecutive se-
quences of words in the target sentence. As shown
in Figure 4, we annotate each node n of phrasal n-
odes labeled T with two attributes: node span and
subtree span; besides, we annotate phrasal span to
the parts covered by phrasal nodes in each subtree
rooted at n. The three types of spans are defined as
follows:
Definition 1 Given a node n, its node span nsp(n)
is the consecutive target word sequence aligned with
the node n.
Take the node ???/NR? in Figure 4 for example,
nsp(??/NR)={7-8}, which corresponds to the tar-
get words ?in? and ?Asia?.
Definition 2 Given a subtree T ? rooted at n, the
subtree span tsp(n) of n is the consecutive target
word sequence from the lower bound of the nsp of
1069
all nodes in T ? to the upper bound of the same set of
spans.
For instance, tsp()P/NN)={4-8}, which corre-
sponds to the target words ?the first Ultrabook in A-
sia?, whose indexes are from 4 to 8.
Definition 3 Given a fragment f covered by a
phrasal node, the phrasal span psp(f) of f is
the consecutive target word sequence aligned with
source string covered by f .
For example, psp(VP2)=?3-8?, which corresponds
to the target word sequence ?launch the first Ultra-
book in Asia?.
We say nsp, tsp and psp are consistent according
to the notion in the phrase-based model (Koehn et
al., 2003). For example, nsp(??/NR), tsp()P
/NN) and psp(NP1) are consistent while nsp(?
?/JJ) and nsp()P/NN) are not consistent.
The annotation can be achieved by a single pos-
torder transversal of the phrasal nodes labeled de-
pendency tree. For simplicity, we call the annotat-
ed phrasal nodes labeled dependency tree annotated
dependency tree. The extraction of bilingual phrases
(including the translation of head node, dependen-
cy syntactic phrases and the fragment covered by a
phrasal node) can be readily achieved by the algo-
rithm described in Koehn et al, (2003). In the fol-
lowing, we focus on CHDR rules extraction.
3.2 Acceptable Fragments Identification
Before present the method of acceptable fragments
identification, we give a brief description of CHDR
fragments. A CHDR fragment is an annotated frag-
ment that consists of a source head-dependents rela-
tion with/without constituency phrasal nodes, a tar-
get string and the word alignment information be-
tween the source and target side. We identify the ac-
ceptable CHDR fragments that are suitable for rule
induction from the annotated dependency tree. We
divide the acceptable CHDR fragments into two cat-
egories depending on whether the fragments con-
tain phrasal nodes. If an acceptable CHDR frag-
ment does not contain phrasal nodes, we call it
CHDR-normal fragment, otherwise CHDR-phrasal
fragment. Given a CHDR fragment F rooted at n,
we say F is acceptable if it satisfies any one of the
following properties:
CHDR-phrasal Rules
r9: (???)(?)x1:VP2|||VV_NN Intel will x1
r10: (x1:NR)(x2:AD)x3:VP2|||VV_NN x1 x2 x3
r11: (???)x1:VP3|||AD_VV_NN Intel x1
r12: (x1:NR)x2:VP3|||AD_VV_NN x1 x2
CHDR-normal Rules
r4: (x1:NR) (x2:AD) ?? (x3:NN) x1 x2 launch x3
Intel will launch x1r3: (???) (?)?? (x1:NN)
r2: (x1:NR) (x2:AD) ?? (x3:???) x1 x2 launch x3
r1: (???) (?)?? (x1:???) Intel will launch x1
r5: (???) (?) x1:VV (x2:???) Intel will x1 x2
r8: (x1:NR) (x2:AD) x3:VV (x4:NN) x1 x2 x3 x4
r6: (x1:NR) (x2:AD) x3:VV (x4:???) x1 x2 x3 x4
Intel will x1 x2r7: (???) (?) x1:VV (x2:NN)
(d)
??/VV
???/NR ?/AD ???/NN
Intel
1
will
2
launch
3
the first Ultrabook in Asia
4-8
(a)
Intel
1
will
2
launch the first Ultrabook in Asia
3-8
VP2
??/VV
???/NR ?/AD ???/NN(b)
(c)
Intel
1
will launch the first Ultrabook in Asia
2-8
VP3
??/VV
???/NR ?/AD ???/NN
VP2|||VV_NN
VP3|||AD_VV_NN
Figure 5: Examples of a CHDR-normal fragment (a), two
CHDR-phrasal fragments (b) and (c) that are identified
from the top level of the annotated dependency tree in
Figure 4, and the corresponding CHDR rules (d) induced
from (a), (b) and (c). The underline denotes a leaf node.
1. Without phrasal nodes, the node span of the
root n is consistent and the subtree spans of
n?s all dependents are consistent. For example,
Figure 5-(a) shows a CHDR-normal fragmen-
t that identified from the top level of the an-
notated dependency tree in Figure 4, since the
nsp(??/VV), tsp(=A/NR), tsp(?/AD)
and tsp()P/NN) are consistent.
1070
2. With phrasal nodes, the phrasal spans of
phrasal nodes are consistent; and for the other
nodes, the node span of head (if it is not cov-
ered by any phrasal node) is consistent, and the
subtree spans of dependents are consistent. For
instance, Figure 5-(b) and (c) show two CHDR-
phrasal fragments identified from the top level
of Figure 4. In Figure 5-(b), psp(VP2), tsp(=
A/NR) and tsp(?/AD) are consistent. In
Figure 5-(c), psp(VP3) and tsp(=A/NR)
are consistent.
The identification of acceptable fragments can be
achieved by a single postorder transversal of the an-
notated dependency tree. Typically, each acceptable
fragment contains at most three types of nodes: head
node, head of the related CHDR; internal nodes, in-
ternal nodes of the related CHDR except head node;
leaf nodes, leaf nodes of the related CHDR.
3.3 Rule Induction
From each acceptable CHDR fragment, we induce
a set of lexicalized and generalized CHDR rules.
We induce CHDR-normal rules and CHDR-phrasal
rules from CHDR-normal fragments and CHDR-
phrasal fragments, respectively.
We first induce a lexicalized form of CHDR rule
from an acceptable CHDR fragment:
1. For a CHDR-normal fragment, we first mark
the internal nodes as substitution sites. This
forms the input of a CHDR-normal rule. Then
we generate the target string according to the
node span of the head and the subtree spans of
the dependents, and turn the word sequences
covered by the internal nodes into variables.
This forms the output of a lexicalized CHDR-
normal rule.
2. For a CHDR-phrasal fragment, we first mark
the internal nodes and the phrasal nodes as sub-
stitution sites. This forms the input of a CHDR-
phrasal rule. Then we construct the output of
the CHDR-phrasal rule in almost the same way
with constructing CHDR-normal rules, except
that we replace the target sequences covered by
the internal nodes and the phrasal nodes with
variables.
For example, rule r1 in Figure 5-(d) is a lexicalized
CHDR-normal rule induced from the CHDR-normal
fragment in Figure 5-(a). r9 and r11 are CHDR-
phrasal rules induced from the CHDR-phrasal frag-
ment in Figure 5-(b) and Figure 5-(c) respectively.
As we can see, these CHDR-phrasal rules are par-
tially unlexicalized.
To alleviate the sparseness problem, we gener-
alize the lexicalized CHDR-normal rules and par-
tially unlexicalized CHDR-phrasal rules with un-
lexicalized nodes by the method proposed in Xie
et al, (2011). As the modification relations be-
tween head and dependents are determined by the
edges, we can replace the lexical word of each n-
ode with its category (POS tag) and obtain new
head-dependents relations with unlexicalized nodes
keeping the same modification relations. We gen-
eralize the rule by simultaneously turn the nodes of
the same type (head, internal, leaf) into their cate-
gories. For example, CHDR-normal rules r2 ? r7
are generalized from r1 in Figure 5-(d). Besides, r10
and r12 are the corresponding generalized CHDR-
phrasal rules. Actually, our CHDR rules are the su-
perset of head-dependents relation rules in Xie et
al., (2011). CHDR-normal rules are equivalent with
the head-dependents relation rules and the CHDR-
phrasal rules are the extension of these rules. For
convenience of description, we use the subscript to
distinguish the phrasal nodes with the same catego-
ry, such as VP2 and VP3. In actual operation, we use
VP instead of VP2 and VP3.
We handle the unaligned words of the target side
by extending the node spans of the lexicalized head
and leaf nodes, and the subtree spans of the lexical-
ized dependents, on both left and right directions.
This procedure is similar with the method of Och
and Ney, (2004). During this process, we might ob-
tain m(m ? 1) CHDR rules from an acceptable
fragment. Each of these rules is assigned with a frac-
tional count 1/m. We take the extracted rule set as
observed data and make use of relative frequency es-
timator to obtain the translation probabilities P (t|s)
and P (s|t).
4 Decoding and the Model
Following Och and Ney, (2002), we adopt a general
loglinear model. Let d be a derivation that convert a
1071
source phrasal nodes labeled dependency tree into a
target string e. The probability of d is defined as:
P (d) ?
?
i
?i(d)?i (1)
where ?i are features defined on derivations and ?i
are feature weights. In our experiments of this paper,
the features are used as follows:
? CHDR rules translation probabilities P (t|s)
and P (s|t), and CHDR rules lexical translation
probabilities Plex(t|s) and Plex(s|t);
? bilingual phrases translation probabilities
Pbp(t|s) and Pbp(s|t), and bilingual phrases
lexical translation probabilities Pbplex(t|s) and
Pbplex(s|t);
? rule penalty exp(?1);
? pseudo translation rule penalty exp(?1);
? target word penalty exp(|e|);
? language model Plm(e).
We have twelve features in our model. The values of
the first four features are accumulated on the CHDR
rules and the next four features are accumulated on
the bilingual phrases. We also use a pseudo transla-
tion rule (constructed according to the word order of
head-dependents relation) as a feature to guarantee
the complete translation when no matched rules can
be found during decoding.
Our decoder is based on bottom-up chart-based
algorithm. It finds the best derivation that convert
the input phrasal nodes labeled dependency tree into
a target string among all possible derivations. Giv-
en the source constituency tree and dependency tree,
we first generate phrasal nodes labeled dependency
tree T as described in Section 3.1, then the decoder
transverses each node in T by postorder. For each
node n, it enumerates all instances of CHDR rooted
at n, and checks the rule set for matched translation
rules. A larger translation is generated by substitut-
ing the variables in the target side of a translation
rule with the translations of the corresponding de-
pendents. Cube pruning (Chiang, 2007; Huang and
Chiang, 2007) is used to find the k-best items with
integrated language model for each node.
To balance the performance and speed of the de-
coder, we limit the search space by reducing the
number of translation rules used for each node.
There are two ways to limit the rule table size: by
a fixed limit (rule-limit) of how many rules are re-
trieved for each input node, and by a threshold (rule-
threshold) to specify that the rule with a score low-
er than ? times of the best score should be discard-
ed. On the other hand, instead of keeping the full
list of candidates for a given node, we keep a top-
scoring subset of the candidates. This can also be
done by a fixed limit (stack-limit) and a threshold
(stack-threshold).
5 Experiments
We evaluated the performance of our model by com-
paring with hierarchical phrase-based model (Chi-
ang, 2007), constituency-to-string model (Liu et al,
2006) and dependency-to-string model (Xie et al,
2011) on Chinese-English translation. First, we de-
scribe data preparation (Section 5.1) and systems
(Section 5.2). Then, we validate that our model sig-
nificantly outperforms all the other baseline models
(Section 5.3). Finally, we give detail analysis (Sec-
tion 5.4).
5.1 Data Preparation
Our training data consists of 1.25M sentence pairs
extracted from LDC 1 data. We choose NIST MT
Evaluation test set 2002 as our development set,
NIST MT Evaluation test sets 2003 (MT03), 2004
(MT04) and 2005 (MT05) as our test sets. The qual-
ity of translations is evaluated by the case insensitive
NIST BLEU-4 metric 2.
We parse the source sentences to constituency
trees (without binarization) and projective depen-
dency trees with Stanford Parser (Klein and Man-
ning, 2002). The word alignments are obtained by
running GIZA++ (Och and Ney, 2003) on the corpus
in both directions and using the ?grow-diag-final-
and? balance strategy (Koehn et al, 2003). We get
bilingual phrases from word-aligned data with algo-
rithm described in Koehn et al (2003) by running
Moses Toolkit 3. We apply SRI Language Modeling
Toolkit (Stolcke and others, 2002) to train a 4-gram
1Including LDC2002E18, LDC2003E07, LDC2003E14,
Hansards portion of LDC2004T07, LDC2004T08 and LD-
C2005T06.
2ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v11b.pl
3http://www.statmt.org/moses/
1072
System Rule # MT03 MT04 MT05 Average
Moses-chart 116.4M 34.65 36.47 34.39 35.17
cons2str 25.4M+32.5M 33.14 35.12 33.27 33.84
dep2str 19.6M+32.5M 34.85 36.57 34.72 35.38
consdep2str 23.3M+32.5M 35.57* 37.68* 35.62* 36.29
Table 1: Statistics of the extracted rules on training data and the BLEU scores (%) on the test sets of different systems.
The ?+? denotes that the rules are composed of syntactic translation rules and bilingual phrases (32.5M). The ?*?
denotes that the results are significantly better than all the other systems (p<0.01).
language model with modified Kneser-Ney smooth-
ing on the Xinhua portion of the English Gigaword
corpus. We make use of the standard MERT (Och,
2003) to tune the feature weights in order to maxi-
mize the system?s BLEU score on the development
set. The statistical significance test is performed by
sign-test (Collins et al, 2005).
5.2 Systems
We take the open source hierarchical phrase-based
system Moses-chart (with default configuration),
our in-house constituency-to-string system cons2str
and dependency-to-string system dep2str as our
baseline systems.
For cons2str, we follow Liu et al, (Liu et al,
2006) to strict that the height of a rule tree is no
greater than 3 and phrase length is no greater than
7. To keep consistent with our proposed model,
we implement the dependency-to-string model (X-
ie et al, 2011) with GHKM (Galley et al, 2004)
rule extraction algorithm and utilize bilingual phras-
es to translate source head node and dependency
syntactic phrases. Our dep2str shows comparable
performance with Xie et al, (2011), which can be
seen by comparing with the results of hierarchical
phrase-based model in our experiments. For dep2str
and our proposed model consdep2str, we set rule-
threshold and stack-threshold to 10?3, rule-limit to
100, stack-limit to 300, and phrase length limit to 7.
5.3 Experimental Results
Table 1 illustrates the translation results of our ex-
periments. As we can see, our consdep2str sys-
tem has gained the best results on all test sets, with
+1.12 BLEU points higher than Moses-chart, +2.45
BLEU points higher than cons2str, and +0.91 BLEU
points higher than dep2str, averagely on MT03,
MT04 and MT05. Our model significantly outper-
forms all the other baseline models, with p<0.01
on statistical significance test sign-test (Collins et
al., 2005). By exploiting two types of trees on
source side, our model gains significant improve-
ments over constituency-to-string and dependency-
to-string models, which employ single type of trees.
Table 1 also lists the statistical results of rules ex-
tracted from training data by different systems. Ac-
cording to our statistics, the number of rules extract-
ed by our consdep2str system is about 18.88% larger
than dep2str, without regard to the 32.5M bilingual
phrases. The extra rules are CHDR-phrasal rules,
which can bring in BLEU improvements by enhanc-
ing the compatibility with phrases. We will conduct
a deep analysis in the next sub-section.
5.4 Analysis
In this section, we first illustrate the influence of
CHDR-phrasal rules in our consdep2str model. We
calculate the proportion of 1-best translations in test
sets that employ CHDR-phrasal rules, and we cal-
l this proportion ?CHDR-phrasal Sent.?. Besides,
the proportion of CHDR-phrasal rules in all CHDR
rules is calculated in these translations, and we cal-
l this proportion ?CHDR-phrasal Rule?. Table 2
lists the using of CHDR-phrasal rules on test sets,
showing that CHDR-phrasal Sent. on all test sets
are higher than 50%, and CHDR-phrasal Rule on al-
l three test sets are higher than 10%. These results
indicate that CHDR-phrasal rules do play a role in
decoding.
Furthermore, we compare some actual transla-
tions of our test sets generated by cons2str, de-
p2str and consdep2str systems, as shown in Fig-
ure 6. In the first example, the Chinese input hold-
s long distance dependencies ???I ?? ?
... \u ... L? '??, which correspond
to the sentence pattern ?noun+adverb+prepositional
1073
System MT03 MT04 MT05
CHDR-phrasal Sent. 50.71 61.80 56.19
CHDR-phrasal Rule 10.53 13.55 10.83
Table 2: The proportion (%) of 1-best translations that
employs CHDR-phrasal rules (CHDR-phrasal Sent.) and
the proportion (%) of CHDR-phrasal rules in all CHDR
rules in these translations (CHDR-phrasal Rule).
phrase+verb+noun?. Cons2str gives a bad result
with wrong global reordering, while our consdep2str
system gains an almost correct result since we cap-
ture this pattern by CHDR-normal rules. In the sec-
ond example, we can see that the Chinese phrase
?2g?y? is a non-syntactic phrase in the depen-
dency tree, and this phrase can not be captured by
head-dependents relation rules in Xie et al, (2011),
thus can not be translated as one unit. Since we en-
code constituency phrasal nodes to the dependency
tree, ?2g?y? is labeled by a phrasal node ?VP?
(means verb phrase), which can be captured by our
CHDR-phrasal rules and translated into the correct
result ?reemergence? with bilingual phrases.
By combining the merits of constituency and
dependency trees, our consdep2str model learns
CHDR-normal rules to acquire the property of long
distance reorderings and CHDR-phrasal rules to ob-
tain good compatibility with phrases.
6 Related Work
In recent years, syntax-based models have witnessed
promising improvements. Some researchers make
efforts on constituency-based models (Graehl and
Knight, 2004; Liu et al, 2006; Huang et al, 2006;
Zhang et al, 2007; Mi et al, 2008; Liu et al, 2009;
Liu et al, 2011; Zhai et al, 2012). Some works pay
attention to dependency-based models (Lin, 2004;
Ding and Palmer, 2005; Quirk et al, 2005; Xiong et
al., 2007; Shen et al, 2008; Xie et al, 2011). These
models are based on single type of trees.
There are also some approaches combining mer-
its of different structures. Marton and Resnik (2008)
took the source constituency tree into account and
added soft constraints to the hierarchical phrase-
based model (Chiang, 2005). Cherry (2008) u-
tilized dependency tree to add syntactic cohesion
to the phrased-based model. Mi and Liu, (2010)
proposed a constituency-to-dependency translation
model, which utilizes constituency forests on the
source side to direct the translation, and depen-
dency trees on the target side to ensure grammati-
cality. Feng et al (2012) presented a hierarchical
chunk-to-string translation model, which is a com-
promise between the hierarchical phrase-based mod-
el and the constituency-to-string model. Most work-
s make effort to introduce linguistic knowledge in-
to the phrase-based model and hierarchical phrase-
based model with constituency trees. Only the work
proposed by Mi and Liu, (2010) utilized constituen-
cy and dependency trees, while their work applied
two types of trees on two sides.
Instead, our model simultaneously utilizes con-
stituency and dependency trees on the source side to
direct the translation, which is concerned with com-
bining the advantages of two types of trees in trans-
lation rules to advance the state-of-the-art machine
translation.
7 Conclusion
In this paper, we present a novel model that si-
multaneously utilizes constituency and dependency
trees on the source side to direct the translation. To
combine the merits of constituency and dependen-
cy trees, our model employs head-dependents rela-
tions incorporating with constituency phrasal nodes.
Experimental results show that our model exhibits
good performance and significantly outperforms the
state-of-the-art constituency-to-string, dependency-
to-string and hierarchical phrase-based models. For
the first time, source side constituency and depen-
dency trees are simultaneously utilized to direct the
translation, and the model surpasses the state-of-the-
art translation models.
Since constituency tree binarization can lead
to more constituency-to-string rules and syntactic
phrases in rule extraction and decoding, which im-
prove the performance of constituency-to-string sys-
tems, for future work, we would like to do research
on encoding binarized constituency trees to depen-
dency trees to improve translation performance.
Acknowledgments
The authors were supported by National Natural Sci-
ence Foundation of China (Contracts 61202216),
1074
MT05 ---- segment 448
??? ?? ? ?? ?? ??? ?? ?? ? ?? ?? ???
cons2srt: united nations with the indonesian government have expressed concern over the time limit for foreign troops .
consdep2srt: the united nations has expressed concern over the deadline of the indonesian government on foreign troops .
reference: The United Nations has expressed concern over the deadline the Indonesian government imposed on foreign troops.
??? ?? ?? ?? ??? ?? ?? ? ??? ?? ?? ?
dobjpobj
prep
advmod
nsubj
pnuct
the united nations has the deadline of the indonesian government on foreign troopsexpressed concern over .
?? ?? ?? ? ?? ?? ??? ??? 6$56?? ??
dep2srt: ?? again severe acute respiratory syndrome ( SARS ) case ??
consdep2srt: ?? reemergence of a severe acute respiratory syndrome ( SARS ) case??
reference: ?? the reemergence of a severe acute respiratory syndrome (SARS) case ??
MT04 ---- segment 194
dep cons & dep
??/VV
??/AD ?/DEG
VP
reemergence
???/NN
??/JJ ??/JJ??/VV
??/AD ?/DEG
again
???/NN
??/JJ ??/JJ
Figure 6: Actual examples translated by the cons2str, dep2str and consdep2str systems.
863 State Key Project (No. 2011AA01A207),
and National Key Technology R&D Program (No.
2012BAH39B03), Key Project of Knowledge Inno-
vation Program of Chinese Academy of Sciences
(No. KGZD-EW-501). Qun Liu.s work was
partially supported by Science Foundation Ireland
(Grant No. 07/CE/I1142) as part of the CNGL
at Dublin City University. Sincere thanks to the
anonymous reviewers for their thorough reviewing
and valuable suggestions. We appreciate Haitao Mi,
Zhaopeng Tu and Anbang Zhao for insightful ad-
vices in writing.
References
Colin Cherry. 2008. Cohesive phrase-based decoding for
statistical machine translation. In ACL, pages 72?80.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proceedings of
the 43rd Annual Meeting on Association for Computa-
tional Linguistics, pages 263?270.
David Chiang. 2007. Hierarchical phrase-based transla-
tion. Computational Linguistics, 33(2):201?228.
Michael Collins, Philipp Koehn, and Ivona Kuc?erova?.
2005. Clause restructuring for statistical machine
translation. In Proceedings of the 43rd Annual Meet-
ing on Association for Computational Linguistics,
pages 531?540.
Yuan Ding and Martha Palmer. 2005. Machine trans-
lation using probabilistic synchronous dependency in-
sertion grammars. In Proceedings of the 43rd Annual
Meeting on Association for Computational Linguistic-
s, pages 541?548.
Yang Feng, Dongdong Zhang, Mu Li, Ming Zhou, and
Qun Liu. 2012. Hierarchical chunk-to-string transla-
tion. In Proceedings of the 50th Annual Meeting of
the Association for Computational Linguistics: Long
Papers-Volume 1, pages 950?958.
Heidi J Fox. 2002. Phrasal cohesion and statistical
machine translation. In Proceedings of the ACL-02
conference on Empirical methods in natural language
processing-Volume 10, pages 304?3111.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What?s in a translation rule. In Pro-
1075
ceedings of HLT/NAACL, volume 4, pages 273?280.
Boston.
Jonathan Graehl and Kevin Knight. 2004. Training tree
transducers. In Proc. HLT-NAACL, pages 105?112.
Peter Hellwig. 2006. Parsing with dependency gram-
mars. An International Handbook of Contemporary
Research, 2:1081?1109.
Liang Huang and David Chiang. 2007. Forest rescoring:
Faster decoding with integrated language models. In
Annual Meeting-Association For Computational Lin-
guistics, volume 45, pages 144?151.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006. S-
tatistical syntax-directed translation with extended do-
main of locality. In Proceedings of AMTA, pages 66?
73.
Dan Klein and Christopher D Manning. 2002. Fast exact
inference with a factored model for natural language
parsing. In Advances in neural information processing
systems, volume 15, pages 3?10.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proceed-
ings of the 2003 Conference of the North American
Chapter of the Association for Computational Linguis-
tics on Human Language Technology-Volume 1, pages
48?54.
Dekang Lin. 2004. A path-based transfer model for ma-
chine translation. In Proceedings of the 20th interna-
tional conference on Computational Linguistics, pages
625?630.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-to-
string alignment template for statistical machine trans-
lation. In Proceedings of the 21st International Con-
ference on Computational Linguistics and the 44th
annual meeting of the Association for Computational
Linguistics, pages 609?616.
Yang Liu, Yajuan Lu?, and Qun Liu. 2009. Improving
tree-to-tree translation with packed forests. In Pro-
ceedings of the Joint Conference of the 47th Annual
Meeting of the ACL and the 4th International Join-
t Conference on Natural Language Processing of the
AFNLP: Volume 2-Volume 2, pages 558?566.
Yang Liu, Qun Liu, and Yajuan Lu?. 2011. Adjoining
tree-to-string translation. In Proceedings of the 49th
Annual Meeting of the Association for Computational
Linguistics: Human Language Technologies-Volume
1, pages 1278?1287.
Yuval Marton and Philip Resnik. 2008. Soft syntactic
constraints for hierarchical phrased-based translation.
In Proceedings of ACL-08: HLT, pages 1003?1011.
Haitao Mi and Qun Liu. 2010. Constituency to depen-
dency translation with forests. In Proceedings of the
48th Annual Meeting of the Association for Computa-
tional Linguistics, pages 1433?1442.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
based translation. In Proceedings of ACL-08: HLT,
pages 192?199.
Franz Josef Och and Hermann Ney. 2002. Discrimi-
native training and maximum entropy models for s-
tatistical machine translation. In Proceedings of the
40th Annual Meeting on Association for Computation-
al Linguistics, pages 295?302.
Franz Josef Och and Hermann Ney. 2003. A systemat-
ic comparison of various statistical alignment models.
Computational linguistics, 29(1):19?51.
Franz Josef Och and Hermann Ney. 2004. The alignmen-
t template approach to statistical machine translation.
Computational linguistics, 30(4):417?449.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Annual Meeting on Association for Computation-
al Linguistics-Volume 1, pages 160?167.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. De-
pendency treelet translation: Syntactically informed
phrasal smt. In Proceedings of the 43rd Annual
Meeting on Association for Computational Linguistic-
s, pages 271?279.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2008. A
new string-to-dependency machine translation algo-
rithm with a target dependency language model. In
Proceedings of ACL-08: HLT, pages 577?585.
Andreas Stolcke et al 2002. Srilm-an extensible lan-
guage modeling toolkit. In Proceedings of the inter-
national conference on spoken language processing,
volume 2, pages 901?904.
Jun Xie, Haitao Mi, and Qun Liu. 2011. A nov-
el dependency-to-string model for statistical machine
translation. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing,
pages 216?226.
Deyi Xiong, Qun Liu, and Shouxun Lin. 2007. A de-
pendency treelet string correspondence model for s-
tatistical machine translation. In Proceedings of the
Second Workshop on Statistical Machine Translation,
pages 40?47.
Kenji Yamada and Kevin Knight. 2001. A syntax-based
statistical translation model. In Proceedings of the
39th Annual Meeting on Association for Computation-
al Linguistics, pages 523?530.
Feifei Zhai, Jiajun Zhang, Yu Zhou, and Chengqing
Zong. 2012. Tree-based translation without using
parse trees. In Proceedings of COLING 2012, pages
3037?3054.
Min Zhang, Hongfei Jiang, AiTi Aw, Jun Sun, Sheng Li,
and Chew Lim Tan. 2007. A tree-to-tree alignment-
based model for statistical machine translation. MT-
Summit-07, pages 535?542.
1076
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 213?218,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
The CNGL-DCU-Prompsit Translation Systems for WMT13
Raphael Rubino?, Antonio Toral?, Santiago Cort?s Va?llo?,
Jun Xie?, Xiaofeng Wu?, Stephen Doherty[, Qun Liu?
?NCLT, Dublin City University, Ireland
?Prompsit Language Engineering, Spain
?ICT, Chinese Academy of Sciences, China
?,[CNGL, Dublin City University, Ireland
?,?{rrubino, atoral, xfwu, qliu}@computing.dcu.ie
?santiago@prompsit.com
?junxie@ict.ac.cn
[stephen.doherty@dcu.ie
Abstract
This paper presents the experiments con-
ducted by the Machine Translation group
at DCU and Prompsit Language Engineer-
ing for the WMT13 translation task. Three
language pairs are considered: Spanish-
English and French-English in both direc-
tions and German-English in that direc-
tion. For the Spanish-English pair, the use
of linguistic information to select paral-
lel data is investigated. For the French-
English pair, the usefulness of the small in-
domain parallel corpus is evaluated, com-
pared to an out-of-domain parallel data
sub-sampling method. Finally, for the
German-English system, we describe our
work in addressing the long distance re-
ordering problem and a system combina-
tion strategy.
1 Introduction
This paper presents the experiments conducted
by the Machine Translation group at DCU1 and
Prompsit Language Engineering2 for the WMT13
translation task on three language pairs: Spanish-
English, French-English and German-English.
For these language pairs, the language and trans-
lation models are built using different approaches
and datasets, thus presented in this paper in sepa-
rate sections.
In Section 2, the systems built for the Spanish-
English pair in both directions are described. We
investigate the use of linguistic information to se-
lect parallel data. In Section 3, we present the sys-
tems built for the French-English pair in both di-
1http://www.nclt.dcu.ie/mt/
2http://www.prompsit.com/
rections. The usefulness of the small in-domain
parallel corpus is evaluated, compared to an out-
of-domain parallel data sub-sampling method. In
Section 4, for the German-English system, aiming
at exploring the long distance reordering problem,
we first describe our efforts in a dependency tree-
to-string approach, before combining different hi-
erarchical systems with a phrase-based system and
show a significant improvement over three base-
line systems.
2 Spanish-English
This section describes the experimental setup for
the Spanish-English language pair.
2.1 Setting
Our setup uses the MOSES toolkit, version
1.0 (Koehn et al, 2007). We use a pipeline
with the phrase-based decoder with standard pa-
rameters, unless noted otherwise. The decoder
uses cube pruning (-cube-pruning-pop-limit 2000
-s 2000), MBR (-mbr-size 800 -mbr-scale 1) and
monotone at punctuation reordering.
Individual language models (LMs), 5-gram and
smoothed using a simplified version of the im-
proved Kneser-Ney method (Chen and Goodman,
1996), are built for each monolingual corpus using
IRSTLM 5.80.01 (Federico et al, 2008). These
LMs are then interpolated with IRSTLM using
the test set of WMT11 as the development set. Fi-
nally, the interpolated LMs are merged into one
LM preserving the weights using SRILM (Stol-
cke, 2002).
We use all the parallel corpora available for
this language pair: Europarl (EU), News Com-
mentary (NC), United Nations (UN) and Common
Crawl (CC). Regarding monolingual corpora, we
use the freely available monolingual corpora (Eu-
213
roparl, News Commentary, News 2007?2012) as
well as the target side of several parallel corpora:
Common Crawl, United Nations and 109 French?
English corpus (only for English as target lan-
guage). Both the parallel and monolingual data
are tokenised and truecased using scripts from the
MOSES toolkit.
2.2 Data selection
The main contribution in our participation regards
the selection of parallel data. We follow the
perplexity-based approach to filter monolingual
data (Moore and Lewis, 2010) extended to filter
parallel data (Axelrod et al, 2011). In our case, we
do not measure perplexity only on word forms but
also using different types of linguistic information
(lemmas and named entities) (Toral, 2013).
We build LMs for the source and target sides
of the domain-specific corpus (in our case NC)
and for a random subset of the non-domain-
specific corpus (EU, UN and CC) of the same size
(number of sentences) of the domain-specific cor-
pus. Each parallel sentence s in the non-domain-
specific corpus is then scored according to equa-
tion 1 where PPIsl(s) is the perplexity of s in
the source side according to the domain-specific
LM and PPOsl(s) is the perplexity of s in the
source side according to the non-domain-specific
LM. PPItl(s) and PPOtl(s) contain the corre-
sponding values for the target side.
score(s) = 12 ? (PPIsl(s)? PPOsl(s))
+(PPItl(s)? PPOtl(s)) (1)
Table 1 shows the results obtained using four
models: word forms (forms), forms and named en-
tities (forms+nes), lemmas (lem) and lemmas and
named entities (lem+nes). Details on these meth-
ods can be found in Toral (2013).
For each corpus we selected two subsets (see in
bold in Table 1), the one for which one method
obtained the best perplexity (top 5% of EU us-
ing forms, 2% of UN using lemmas and 50% of
CC using forms and named entities) and a big-
ger one used to compare the performance in SMT
(top 14% of EU using lemmas and named entities
(lem+nes), top 12% of UN using forms and named
entities and the whole CC). These subsets are used
as training data in our systems.
As we can see in the table, the use of lin-
guistic information allows to obtain subsets with
lower perplexity than using solely word forms, e.g.
1057.7 (lem+nes) versus 1104.8 (forms) for 14%
of EU. The only exception to this is the subset that
comprises the top 5% of EU, where perplexity us-
ing word forms (957.9) is the lowest one.
corpus size forms forms+nes lem lem+nes
EU 5% 957.9 987.2 974.3 1005.514% 1104.8 1058.7 1111.6 1057.7
UN 2% 877.1 969.6 866.6 962.212% 1203.2 1130.9 1183.8 1131.6
CC 50% 573.0 547.2 574.5 546.4100% 560.1 560.1 560.1 560.1
Table 1: Perplexities in data selection
2.3 Results
Table 2 presents the results obtained. Note that
these were obtained during development and thus
the systems are tuned on WMT?s 2011 test set and
tested on WMT?s 2012 test set.
All the systems share the same LM. The first
system (no selection) is trained with the whole NC
and EU. The second (small) and third (big) sys-
tems use as training data the whole NC and sub-
sets of EU (5% and 14%, respectively), UN (2%
and 12%, respectively) and CC (50% and 100%,
respectively), as shown in Table 1.
System #sent. BLEU BLEUcased
no selection 2.1M 31.99 30.96
small 1.4M 33.12 32.05
big 3.8M 33.49 32.43
Table 2: Number of sentences and BLEU scores
obtained on the WMT12 test set for the different
systems on the EN?ES translation task.
The advantage of data selection is clear. The
second system, although smaller in size compared
to the first (1.4M sentence pairs versus 2.1M),
takes its training from a more varied set of data,
and its performance is over one absolute BLEU
point higher.
When comparing the two systems that rely on
data selection, one might expect the one that uses
data with lower perplexity (small) to perform bet-
ter. However, this is not the case, the third system
(big) performing around half an absolute BLEU
point higher than the second (small). This hints
at the fact that perplexity alone is not an optimal
metric for data selection, but size should also be
considered. Note that the size of system 3?s phrase
table is more than double that of system 2.
214
3 French-English
This section describe the particularities of the MT
systems built for the French-English language pair
in both directions. The goal of the experimen-
tal setup presented here is to evaluate the gain of
adding small in-domain parallel data into a trans-
lation system built on a sub-sample of the out-of-
domain parallel data.
3.1 Data Pre-processing
All the available parallel and monolingual data for
the French-English language pair, including the
last versions of LDC Gigaword corpora, are nor-
malised and special characters are escaped using
the scripts provided by the shared task organisers.
Then, the corpora are tokenised and for each lan-
guage a true-case model is built on the concatena-
tion of all the data after removing duplicated sen-
tences, using the scripts included in MOSES dis-
tribution. The corpora are then true-cased before
being used to build the language and the transla-
tion models.
3.2 Language Model
To build our final language models, we first build
LMs on each corpus individually. All the monolin-
gual corpora are considered, as well as the source
or target side of the parallel corpora if the data
are not already in the monolingual data. We build
modified Kneser-Ney discounted 5-gram LMs us-
ing the SRILM toolkit for each corpus and sepa-
rate the LMs in three groups: one in-domain (con-
taining news-commentary and news crawl cor-
pora), another out-of-domain (containing Com-
mon Crawl, Europarl, UN and 109 corpora), and
the last one with LDC Gigaword LMs (the data
are kept separated by news source, as distributed
by LDC). The LMs in each group are linearly in-
terpolated based on their perplexities obtained on
the concatenation of all the development sets from
previous WMT translation tasks. The same devel-
opment corpus is used to linearly interpolate the
in-domain and LDC LMs. We finally obtain two
LMs, one containing out-of-domain data which is
only used to filter parallel data, and another one
containing in-domain data which is used to filter
parallel data, tuning the translation model weights
and at decoding time. Details about the number of
n-grams in each language model are presented in
Table 3.
French English
out in out in
1-gram 4.0 3.3 4.2 10.7
2-gram 43.0 44.0 48.2 161.9
3-gram 54.2 61.8 63.4 256.8
4-gram 99.7 119.2 103.2 502.7
5-gram 136.4 165.0 125.4 680.7
Table 3: Number of n-grams (in millions) for the
in-domain and out-of-domain LMs in French and
English.
3.3 Translation Model
Two phrase-based translation models are built
using MGIZA++ (Gao and Vogel, 2008) and
MOSES3, with the default alignment heuris-
tic (grow-diag-final) and bidirectional reordering
models. The first translation model is in-domain,
built with the news-commentary corpus. The sec-
ond one is built on a sample of all the other paral-
lel corpora available for the French-English lan-
guage pair. Both corpora are cleaned using the
script provided with Moses, keeping the sentences
with a length below 80 words. For the second
translation model, we used the modified Moore-
Lewis method based on the four LMs (two per
language) presented in section 3.2. The sum of
the source and target perplexity difference is com-
puted for each sentence pair of the corpus. We set
an acceptance threshold to keep a limited amount
of sentence pairs. The kept sample finally con-
tains ? 3.7M sentence pairs to train the translation
model. Statistics about this data sample and the
news-commentary corpus are presented in Table 4.
The test set of WMT12 translation task is used to
optimise the weights for the two translation mod-
els with the MERT algorithm. For this tuning step,
the limit of target phrases loaded per source phrase
is set to 50. We also use a reordering constraint
around punctuation marks. The same parameters
are used during the decoding of the test set.
news-commentary sample
tokens FR 4.7M 98.6M
tokens EN 4.0M 88.0M
sentences 156.5k 3.7M
Table 4: Statistics about the two parallel corpora,
after pre-processing, used to train the translation
models.
3Moses version 1.0
215
3.4 Results
The two translation models presented in Sec-
tion 3.3 allow us to design three translation sys-
tems: one using only the in-domain model, one
using only the model built on the sub-sample of
the out-of-domain data, and one using both mod-
els by giving two decoding paths to Moses. For
this latter system, the MERT algorithm is also used
to optimise the translation model weights. Results
obtained on the WMT13 test set, measured with
the official automatic metrics, are presented in Ta-
ble 5. The submitted system is the one built on
the sub-sample of the out-of-domain parallel data.
This system was chosen during the tuning step be-
cause it reached the highest BLEU scores on the
development corpus, slightly above the combina-
tion of the two translation models.
News-Com. Sample Comb.
FR-EN
BLEUdev 26.9 30.0 29.9
BLEU 27.0 30.8 30.4
BLEUcased 26.1 29.8 29.3
TER 62.9 58.9 59.3
EN-FR
BLEUdev 27.1 29.7 29.6
BLEU 26.6 29.6 29.4
BLEUcased 25.8 28.7 28.5
TER 65.1 61.8 62.0
Table 5: BLEU and TER scores obtained by our
systems. BLEUdev is the score obtained on the
development set given by MERT, while BLEU,
BLEUcased and TER are obtained on the test set
given by the submission website.
For both FR-EN and EN-FR tasks, the best re-
sults are reached by the system built on the sub-
sample taken from the out-of-domain parallel data.
Using only News-Commentary to build a trans-
lation model leads to acceptable BLEU scores,
with regards to the size of the training corpus.
When the sub-sample of the out-of-domain par-
allel data is used to build the translation model,
adding a model built on News-Commentary does
not improve the results. The difference between
these two systems in terms of BLEU score (both
cased sensitive and insensitive) indicates that sim-
ilar results can be achieved, however it appears
that the amount of sentence pairs in the sample
is large enough to limit the impact of the small
in-domain corpus parallel. Further experiments
are still required to determine the minimum sam-
ple size needed to outperform both the in-domain
system and the combination of the two translation
models.
4 German-English
In this section we describe our work on German
to English subtask. Firstly we describe the De-
pendency tree to string method which we tried but
unfortunately failed due to short of time. Secondly
we discuss the baseline system and the preprocess-
ing we performed. Thirdly a system combination
method is described.
4.1 Dependency Tree to String Method
Our original plan was to address the long distance
reordering problem in German-English transla-
tion. We use Xie?s Dependency tree to string
method(Xie et al, 2011) which obtains good re-
sults on Chinese to English translation and ex-
hibits good performance at long distance reorder-
ing as our decoder.
We use Stanford dependency parser4 to parse
the English side of the data and Mate-Tool5 for
the German side. The first set of experiments did
not lead to encouraging results and due to insuffi-
cient time, we decide to switch to other decoders,
based on statistical phrase-based and hierarchical
approaches.
4.2 Baseline System
In this section we describe the three baseline sys-
tem we used as well as the preprocessing technolo-
gies and the experiments set up.
4.2.1 Preprocessing and Corpus
We first use the normalisation scripts provided by
WMT2013 to normalise both English and Ger-
man side. Then we escape special characters on
both sides. We use Stanford tokeniser for English
and OpenNLP tokeniser6 for German. Then we
train a true-case model using with Europarl and
News-Commentary corpora, and true-case all the
corpus we used. The parallel corpus is filtered
with the standard cleaning scripts provided with
4http://nlp.stanford.edu/software/
lex-parser.shtml
5http://code.google.com/p/mate-tools/
6http://opennlp.sourceforge.net/
models-1.5/
216
MOSES. We split the German compound words
with jWordSplitter7.
All the corpus provided for the shared task are
used for training our translation models, while
WMT2011 and WMT2012 test sets are used to
tune the models parameters. For the LM, we
use all the monolingual data provided, including
LDC Gigaword. Each LM is trained with the
SRILM toolkit, before interpolating all the LMs
according to their weights obtained by minimiz-
ing the perplexity on the tuning set (WMT2011
and WMT2012 test sets). As SRILM can only
interpolate 10 LMs, we first interpolate a LM with
Europarl, News Commentary, News Crawl (2007-
2012, each year individually, 6 separate parts),
then we interpolate a new LM with this interpo-
lated LM and LDC Gigawords (we kept the Gi-
gaword subsets separated according to the news
sources as distributed by LDC, which leads to 7
corpus).
4.2.2 Three baseline systems
We use the data set up described by the former
subsection and build up three baseline systems,
namely PB MOSES (phrase-based), Hiero MOSES
(hierarchical) and CDEC (Dyer et al, 2010). The
motivation of choosing Hierarchical Models is to
address the German-English?s long reorder prob-
lem. We want to test the performance of CDEC and
Hiero MOSES and choose the best. PB MOSES is
used as our benchmark. The three results obtained
on the development and test sets for the three base-
line system and the system combination are shown
in the Table 6.
Development Test
PB MOSES 22.0 24.0
Hiero MOSES 22.1 24.4
CDEC 22.5 24.4
Combination 23.0 24.8
Table 6: BLEU scores obtained by our systems on
the development and test sets for the German to
English translation task.
From the Table 6 we can see that on develop-
ment set, CDEC performs the best, and its much
better than MOSES?s two decoder, but on test
set, Hiero MOSES and CDEC performs as well as
each other, and they both performs better than PB
Model.
7http://www.danielnaber.de/
jwordsplitter/
4.3 System Combination
We also use a word-level combination strat-
egy (Rosti et al, 2007) to combine the three trans-
lation hypotheses. To combine these systems, we
first use the Minimum Bayes-Risk (MBR) (Kumar
and Byrne, 2004) decoder to obtain the 5 best hy-
pothesis as the alignment reference for the Con-
fusion Network (CN) (Mangu et al, 2000). We
then use IHMM (He et al, 2008) to choose the
backbone build the CN and finally search for and
generate the best translation.
We tune the system parameters on development
set with Simple-Simplex algorithm. The param-
eters for system weights are set equal. Other pa-
rameters like language model, length penalty and
combination coefficient are chosen when we see a
good improvement on development set.
5 Conclusion
This paper presented a set of experiments con-
ducted on Spanish-English, French-English and
German-English language pairs. For the Spanish-
English pair, we have explored the use of linguistic
information to select parallel data and use this as
the training for SMT. However, the comparison of
the performance obtained using this method and
the purely statistical one (i.e. perplexity on word
forms) remains to be carried out. Another open
question regards the optimal size of the selected
data. As we have seen, minimum perplexity alone
cannot be considered an optimal metric since us-
ing a larger set, even if it has higher perplexity,
allowed us to obtain notably higher BLEU scores.
The question is then how to decide the optimal size
of parallel data to select.
For the French-English language pair, we inves-
tigated the usefulness of the small in-domain par-
allel data compared to out-of-domain parallel data
sub-sampling. We show that with a sample con-
taining ? 3.7M sentence pairs extracted from the
out-of-domain parallel data, it is not necessary to
use the small domain-specific parallel data. Fur-
ther experiments are still required to determine the
minimum sample size needed to outperform both
the in-domain system and the combination of the
two translation models.
Finally, for the German-English language pair,
we presents our exploitation of long ordering
problem. We compared two hierarchical models
with one phrase-based model, and we also use a
system combination strategy to further improve
217
the translation systems performance.
Acknowledgments
The research leading to these results has re-
ceived funding from the European Union Seventh
Framework Programme FP7/2007-2013 under
grant agreement PIAP-GA-2012-324414 (Abu-
MaTran) and through Science Foundation Ireland
as part of the CNGL (grant 07/CE/I1142).
References
Amittai Axelrod, Xiaodong He, and Jianfeng Gao.
2011. Domain adaptation via pseudo in-domain
data selection. In Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing, EMNLP ?11, pages 355?362, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Stanley F. Chen and Joshua Goodman. 1996. An em-
pirical study of smoothing techniques for language
modeling. In Proceedings of the 34th annual meet-
ing on Association for Computational Linguistics,
ACL ?96, pages 310?318, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Chris Dyer, Jonathan Weese, Hendra Setiawan, Adam
Lopez, Ferhan Ture, Vladimir Eidelman, Juri Gan-
itkevitch, Phil Blunsom, and Philip Resnik. 2010.
cdec: A decoder, alignment, and learning framework
for finite-state and context-free translation models.
In Proceedings of the ACL 2010 System Demonstra-
tions, pages 7?12. Association for Computational
Linguistics.
Marcello Federico, Nicola Bertoldi, and Mauro Cet-
tolo. 2008. IRSTLM: an open source toolkit for
handling large scale language models. In INTER-
SPEECH, pages 1618?1621. ISCA.
Qin Gao and Stephan Vogel. 2008. Parallel implemen-
tations of word alignment tool. In Software Engi-
neering, Testing, and Quality Assurance for Natural
Language Processing, pages 49?57. Association for
Computational Linguistics.
Xiaodong He, Mei Yang, Jianfeng Gao, Patrick
Nguyen, and Robert Moore. 2008. Indirect-hmm-
based hypothesis alignment for combining outputs
from machine translation systems. In Proceedings
of the Conference on Empirical Methods in Natu-
ral Language Processing, pages 98?107. Associa-
tion for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondr?ej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
ACL ?07, pages 177?180, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Shankar Kumar and William Byrne. 2004. Minimum
bayes-risk decoding for statistical machine transla-
tion. In Proceedings of HLT-NAACL, pages 169?
176.
Lidia Mangu, Eric Brill, and Andreas Stolcke. 2000.
Finding consensus in speech recognition: word er-
ror minimization and other applications of confu-
sion networks. Computer Speech & Language,
14(4):373?400.
Robert C. Moore and William Lewis. 2010. Intelli-
gent selection of language model training data. In
Proceedings of the ACL 2010 Conference Short Pa-
pers, ACLShort ?10, pages 220?224, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
Antti-Veikko I Rosti, Necip Fazil Ayan, Bing Xiang,
Spyros Matsoukas, Richard Schwartz, and Bonnie
Dorr. 2007. Combining outputs from multiple
machine translation systems. In Human Language
Technologies 2007: The Conference of the North
American Chapter of the Association for Computa-
tional Linguistics, pages 228?235.
Andreas Stolcke. 2002. Srilm - an extensible lan-
guage modeling toolkit. In John H. L. Hansen and
Bryan L. Pellom, editors, INTERSPEECH. ISCA.
Antonio Toral. 2013. Hybrid Selection of Language
Model Training Data Using Linguistic Information
and Perplexity. In Proceedings of the Second Work-
shop on Hybrid Approaches to Machine Translation
(HyTra), ACL 2013.
Jun Xie, Haitao Mi, and Qun Liu. 2011. A novel
dependency-to-string model for statistical machine
translation. In Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing, pages 216?226. Association for Computational
Linguistics.
218
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 136?141,
Baltimore, Maryland USA, June 26?27, 2014.
c?2014 Association for Computational Linguistics
The DCU-ICTCAS MT system at WMT 2014 on German-English
Translation Task
Liangyou Li
?
, Xiaofeng Wu
?
, Santiago Cort
?
es Va??llo
?
Jun Xie
?
, Andy Way
?
, Qun Liu
??
?
CNGL Centre for Global Intelligent Content, School of Computing
Dublin City University, Dublin 9, Ireland
?
Key Laboratory of Intelligent Information Processing, Institute of Computing Technology
Chinese Academy of Sciences, Beijing, China
{liangyouli,xiaofengwu,scortes,away,qliu}@computing.dcu.ie
xiejun@ict.ac.cn
Abstract
This paper describes the DCU submis-
sion to WMT 2014 on German-English
translation task. Our system uses phrase-
based translation model with several pop-
ular techniques, including Lexicalized
Reordering Model, Operation Sequence
Model and Language Model interpolation.
Our final submission is the result of sys-
tem combination on several systems which
have different pre-processing and align-
ments.
1 Introduction
On the German-English translation task of WMT
2014, we submitted a system which is built with
Moses phrase-based model (Koehn et al., 2007).
For system training, we use all provided
German-English parallel data, and conducted sev-
eral pre-processing steps to clean the data. In ad-
dition, in order to improve the translation quali-
ty, we adopted some popular techniques, includ-
ing three Lexicalized Reordering Models (Axel-
rod et al., 2005; Galley and Manning, 2008), a 9-
gram Operation Sequence Model (Durrani et al.,
2011) and Language Model interpolation on sev-
eral datasets. And then we use system combina-
tion on several systems with different settings to
produce the final outputs.
Our phrase-based systems are tuned with k-best
MIRA (Cherry and Foster, 2012) on development
set. We set the maximum iteration to be 25.
The Language Models in our systems are
trained with SRILM (Stolcke, 2002). We trained
Corpus Filtered Out (%)
Bilingual 7.17
Monolingual (English) 1.05
Table 1: Results of language detection: percentage
of filtered out sentences
a 5-gram model with Kneser-Ney discounting
(Chen and Goodman, 1996).
In the next sections, we will describe our system
in detail. In section 2, we will explain our pre-
processing steps on corpus. Then in section 3, we
will describe some techniques we have tried for
this task and the experiment results. In section 4,
our final configuration for submitted system will
be presented. And we conclude in the last section.
2 Pre-processing
We use all the training data for German-English
translation, including Europarl, News Commen-
tary and Common Crawl. The first thing we no-
ticed is that some Non-German and Non-English
sentences are included in our training data. So we
apply Language Detection (Shuyo, 2010) for both
monolingual and bilingual corpora. For mono-
lingual data (only including English sentences in
our task), we filter out sentences which are detect-
ed as other language with probability more than
0.999995. And for bilingual data, A sentence
pair is filtered out if the language detector detect-
s a different language with probability more than
0.999995 on either the source or the target. The
filtering results are given in Table 1.
In our experiment, German compound word-
s are splitted based on frequency (Koehn and
136
Knight, 2003). In addition, for both monolingual
and bilingual data, we apply tokenization, nor-
malizing punctuation and truecasing using Moses
scripts. For parallel training data, we also filter out
sentence pairs containing more than 80 tokens on
either side and sentence pairs whose length ratio
between source and target side is larger than 3.
3 Techniques
In our preliminary experiments, we take newstest
2013 as our test data and newstest 2008-2012 as
our development data. In total, we have more
than 10,000 sentences for tuning. The tuning step
would be very time-consuming if we use them al-
l. So in this section, we use Feature Decay Al-
gorithm (FDA) (Bic?ici and Yuret, 2014) to select
2000 sentences as our development set. Table 2
shows that system performance does not increase
with larger tuning set and the system using only
2K sentences selected by FDA is better than the
baseline tuned with all the development data.
In this section, alignment model is trained
by MGIZA++ (Gao and Vogel, 2008) with
grow-diag-final-and heuristic function.
And other settings are mostly default values in
Moses.
3.1 Lexicalized Reordering Model
German and English have different word order
which brings a challenge in German-English ma-
chine translation. In our system, we adopt three
Lexicalized Reordering Models (LRMs) for ad-
dressing this problem. They are word-based LRM
(wLRM), phrase-based LRM (pLRM) and hierar-
chal LRM (hLRM).
These three models have different effect on the
translation. Word-based and phrase-based LRMs
are focus on local reordering phenomenon, while
hierarchical LRM could be applied into longer re-
ordering problem. Figure 1 shows the differences
(Galley and Manning, 2008). And Table 3 shows
effectiveness of different LRMs.
In our system based on Moses, we
use wbe-msd-bidirectional-fe,
phrase-msd-bidirectional-fe and
hier-mslr-bidirectional-fe to specify
these three LRMs. From Table 2, we could see
that LRMs significantly improves the translation.
Figure 1: Occurrence of a swap according to
the three orientation models: word-based, phrase-
based, and hierarchical. Black squares represen-
t word alignments, and gray squares represen-
t blocks identified by phrase-extract. In (a), block
b
i
= (e
i
, f
a
i
) is recognized as a swap according to
all three models. In (b), b
i
is not recognized as a
swap by the word-based model. In (c), b
i
is rec-
ognized as a swap only by the hierarchical model.
(Galley and Manning, 2008)
3.2 Operation Sequence Model
The Operation Sequence Model (OSM) (Durrani
et al., 2011) explains the translation procedure as
a linear sequence of operations which generates
source and target sentences in parallel. Durrani
et al. (2011) defined four translation operations:
Generate(X,Y), Continue Source Concept, Gener-
ate Source Only (X) and Generate Identical, as
well as three reordering operations: Insert Gap,
Jump Back(W) and Jump Forward. These oper-
ations are described as follows.
? Generate(X,Y) make the words in Y and the
first word in X added to target and source
string respectively.
? Continue Source Concept adds the word in
the queue from Generate(X,Y) to the source
string.
? Generate Source Only (X) puts X in the
source string at the current position.
? Generate Identical generates the same word
for both sides.
? Insert Gap inserts a gap in the source side for
future use.
? Jump Back (W) makes the position for trans-
lation be the Wth closest gap to the current
position.
? Jump Forward moves the position to the in-
dex after the right-most source word.
137
Systems Tuning Set newstest 2013
Baseline ? 24.1
+FDA ? 24.2
+LRMs 24.0 25.4
+OSM 24.4 26.2
+LM Interpolation 24.6 26.4
+Factored Model ? 25.9
+Sparse Feature 25.6 25.9
+TM Combination 24.1 25.4
+OSM Interpolation 24.4 26.0
Table 2: Preliminary results on tuning set and test set (newstest 2013). All scores on test set are case-
sensitive BLEU[%] scores. And scores on tuning set are case-insensitive BLEU[%] directly from tuning
result. Baseline uses all the data from newstest 2008-2012 for tuning.
Systems Tuning Set (uncased) newstest 2013
Baseline+FDA ? 24.2
+wLRM 23.8 25.1
+pLRM 23.9 25.2
+hLRM 24.0 25.4
+pLRM 23.8 25.1
+hLRM 23.7 25.2
Table 3: System BLEU[%] scores when different LRMs are adopted.
The probability of an operation sequence O =
(o
1
o
2
? ? ? o
J
) is:
p(O) =
J
?
j=1
p(o
j
|o
j?n+1
? ? ? o
j?1
) (1)
where n indicates the number of previous opera-
tions used.
In this paper we train a 9-gram OSM on train-
ing data and integrate this model directly into log-
linear framework (OSM is now available to use
in Moses). Our experiment shows OSM improves
our system by about 0.8 BLEU (see Table 2).
3.3 Language Model Interpolation
In our baseline, Language Model (LM) is trained
on all the monolingual data provided. In this sec-
tion, we try to build a large language model by in-
cluding data from English Gigaword fifth edition
(only taking partial data with size of 1.6G), En-
glish side of UN corpus and English side of 10
9
French-English corpus. Instead of training a s-
ingle model on all data, we interpolate language
models trained on each subset (monolingual data
provided is splitted into three parts: News 2007-
2013, Europarl and News Commentary) by tuning
weights to minimize perplexity of language model
measured on the target side of development set.
In our experiment, after interpolation, the lan-
guage model doesn?t get a much lower perplexity,
but it slightly improves the system, as shown in
Table 2.
3.4 Other Tries
In addition to the techniques mentioned above, we
also try some other approaches. Unfortunately al-
l of these methods described in this section are
non-effective in our experiments. The results are
shown in Table 2.
? Factored Model (Koehn and Hoang, 2007):
We tried to integrate a target POS factored
model into our system with a 9-gram POS
language model to address the problem of
word selection and word order. But ex-
periment doesn?t show improvement. The
English POS is from Stanford POS Tagger
(Toutanova et al., 2003).
? Translation Model Combination: In this ex-
periment, we try to use the method of (Sen-
nrich, 2012) to combine phrase tables or re-
ordering tables from different subsets of data
138
to minimize perplexity measured on develop-
ment set. We try to split the training data in
two ways. One is according to data source,
resulting in three subsets: Europarl, News
Commentary and Common Crawl. Another
one is to use data selection. We use FDA to
select 200K sentence pairs as in-domain data
and the rest as out-domain data. Unfortunate-
ly both experiments failed. In Table 2, we on-
ly report results of phrase table combination
on FDA-based data sets.
? OSM Interpolation: Since OSM in our sys-
tem could be taken as a special language
model, we try to use the idea of interpolation
similar with language model to make OSM
adapted to some data. Training data are s-
plitted into two subsets with FDA. We train
9-gram OSM on each subsets and interpolate
them according to OSM trained on the devel-
opment set.
? Sparse Features: For each source phrase,
there is usually more than one corresponding
translation option. Each different translation
may be optimal in different contexts. Thus
in our systems, similar to (He et al., 2008)
which proposed a Maximum Entropy-based
rule selection for the hierarchical phrase-
based model, features which describe the
context of phrases, are designed to select the
right translation. But different with (He et
al., 2008), we use sparse features to mod-
el the context. And instead of using syn-
tactic POS, we adopt independent POS-like
features: cluster ID of word. In our experi-
ment mkcls was used to cluster words into 50
groups. And all features are generalized to
cluster ID.
4 Submission
Based on our preliminary experiments in the sec-
tion above, we use LRMs, OSM and LM inter-
polation in our final system for newstest 2014.
But as we find that Language Models trained on
UN corpus and 10
9
French-English corpus have
a very high perplexity and in order to speed up
the translation by reducing the model size, in this
section, we interpolate only three language model-
s from monolingual data provided, English Giga-
word fifth edition and target side of training data.
In addition, we also try some different methods for
final submission. And the results are shown in Ta-
ble 4.
? Development Set Selection: Instead of using
FDA which is dependent on test set, we use
the method of (Nadejde et al., 2013) to se-
lect tuning set from newstest 2008-2013 for
the final system. We only keep 2K sentences
which have more than 30 words and higher
BLEU score. The experiment result is shown
in Table 4 ( The system is indicated as Base-
line).
? Pre-processing: In our preliminary exper-
iments, sentences are tokenized without
changing hyphen. Thus we build another sys-
tem where all the hyphens are tokenized ag-
gressively.
? SyMGIZA++: Better alignment could lead to
better translation. So we carry out some ex-
periments on SyMGIZA++ aligner (Junczys-
Dowmunt and Sza, 2012), which modifies the
original IBM/GIZA++ word alignment mod-
els to allow to update the symmetrized mod-
els between chosen iterations of the original
training algorithms. Experiment shows this
new alignment improves translation quality.
? Multi-alignment Selection: We also try to use
multi-alignment selection (Tu et al., 2012)
to generate a ?better? alignment from three
alignmens: MGIZA++ with function grow-
diag-final-and, SyMGIZA++ with function
grow-diag-final-and and fast alignment (Dy-
er et al., 2013). Although this method show
comparable or better result on development
set, it fails on test set.
Since we build a few systems with different
setting on Moses phrase-based model, a straight-
forward thinking is to obtain the better transla-
tion from several different translation systems. So
we use system combination (Heafield and Lavie,
2010) on the 1-best outputs of three systems (in-
dicated with
?
in table 4). And this results in our
best system so far, as shown in Table 4. In our final
submission, this result is taken as primary.
5 Conclusion
This paper describes our submitted system to
WMT 2014 in detail. This system is based on
139
Systems Tuning Set newstest 2014
Baseline
?
34.2 25.6
+SyMGIZA++
?
34.3 26.0
+Multi-Alignment Selection 34.4 25.6
+Hyphen-Splitted 33.9 25.9
+SyMGIZA++
?
34.0 26.0
+Multi-Alignment Selection 34.0 25.7
System Combination ? 26.5
Table 4: Experiment results on newstest 2014. We report case-sensitive BLEU[%] score on test set and
case-insensitive BLEU[%] on tuning set which is directly from tuning result. Baseline is the phrase-based
system with LRMs, OSM and LM interpolation on smaller datasets, tuned with selected development set.
Systems indicated with
?
are used for system combination.
Moses phrase-based model, and integrates Lexi-
calized Reordering Models, Operation Sequence
Model and Language Model interpolation. Al-
so system combination is used on several system-
s which have different pre-processing and align-
ment.
Acknowledgments
This work is supported by EC Marie-Curie initial
training Network EXPERT (EXPloiting Empiri-
cal appRoaches to Translation) project (http:
//expert-itn.eu). Thanks to Johannes Lev-
eling for his help on German compound splitting.
And thanks to Jia Xu and Jian Zhang for their ad-
vice and help on this paper and experiments.
References
Amittai Axelrod, Ra Birch Mayne, Chris Callison-
burch, Miles Osborne, and David Talbot. 2005. Ed-
inburgh system description for the 2005 iwslt speech
translation evaluation. In Proceedings of the Inter-
national Workshop on Spoken Language Translation
(IWSLT).
Ergun Bic?ici and Deniz Yuret. 2014. Optimizing in-
stance selection for statistical machine translation
with feature decay algorithms. IEEE/ACM Transac-
tions On Audio, Speech, and Language Processing
(TASLP).
Stanley F. Chen and Joshua Goodman. 1996. An em-
pirical study of smoothing techniques for language
modeling. In Proceedings of the 34th Annual Meet-
ing on Association for Computational Linguistics,
ACL ?96, pages 310?318, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Colin Cherry and George Foster. 2012. Batch tuning
strategies for statistical machine translation. In Pro-
ceedings of the 2012 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, NAA-
CL HLT ?12, pages 427?436, Stroudsburg, PA, US-
A. Association for Computational Linguistics.
Nadir Durrani, Helmut Schmid, and Alexander Fraser.
2011. A joint sequence translation model with in-
tegrated reordering. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies - Vol-
ume 1, HLT ?11, pages 1045?1054, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Chris Dyer, Victor Chahuneau, and Noah A. Smith.
2013. A simple, fast, and effective reparameteriza-
tion of ibm model 2. In Proceedings of NAACL.
Michel Galley and Christopher D. Manning. 2008. A
simple and effective hierarchical phrase reordering
model. In Proceedings of the 2008 Conference on
Empirical Methods in Natural Language Process-
ing, pages 848?856, Honolulu, Hawaii, October. As-
sociation for Computational Linguistics.
Qin Gao and Stephan Vogel. 2008. Parallel implemen-
tations of word alignment tool. In Software Engi-
neering, Testing, and Quality Assurance for Natural
Language Processing, SETQA-NLP ?08, pages 49?
57, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Zhongjun He, Qun Liu, and Shouxun Lin. 2008. Im-
proving statistical machine translation using lexical-
ized rule selection. In Proceedings of the 22nd In-
ternational Conference on Computational Linguis-
tics (Coling 2008), pages 321?328, Manchester, UK,
August. Coling 2008 Organizing Committee.
Kenneth Heafield and Alon Lavie. 2010. Combining
machine translation output with open source: The
Carnegie Mellon multi-engine machine translation
scheme. The Prague Bulletin of Mathematical Lin-
guistics, 93:27?36, January.
Marcin Junczys-Dowmunt and Arkadiusz Sza. 2012.
Symgiza++: Symmetrized word alignment models
for statistical machine translation. In Pascal Bouvry,
MieczysawA. Kopotek, Franck Leprvost, Magorza-
ta Marciniak, Agnieszka Mykowiecka, and Henryk
140
Rybiski, editors, Security and Intelligent Informa-
tion Systems, volume 7053 of Lecture Notes in Com-
puter Science, pages 379?390. Springer Berlin Hei-
delberg.
Philipp Koehn and Hieu Hoang. 2007. Factored trans-
lation models. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 868?876.
Philipp Koehn and Kevin Knight. 2003. Empirical
methods for compound splitting. In Proceedings of
the Tenth Conference on European Chapter of the
Association for Computational Linguistics - Volume
1, EACL ?03, pages 187?193, Stroudsburg, PA, US-
A. Association for Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertol-
di, Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ond?rej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the ACL
on Interactive Poster and Demonstration Sessions,
ACL ?07, pages 177?180, Stroudsburg, PA, USA.
Association for Computational Linguistics.
Maria Nadejde, Philip Williams, and Philipp Koehn.
2013. Edinburgh?s syntax-based machine transla-
tion systems. In Proceedings of the Eighth Work-
shop on Statistical Machine Translation, pages 170?
176, Sofia, Bulgaria, August. Association for Com-
putational Linguistics.
Rico Sennrich. 2012. Perplexity minimization for
translation model domain adaptation in statistical
machine translation. In Proceedings of the 13th
Conference of the European Chapter of the Associa-
tion for Computational Linguistics, pages 539?549,
Avignon, France, April. Association for Computa-
tional Linguistics.
Nakatani Shuyo. 2010. Language detection library for
java.
Andreas Stolcke. 2002. Srilm ? an extensible language
modeling toolkit. In Proceedings of the Internation-
al Conference Spoken Language Processing, pages
901?904, Denver, CO.
Kristina Toutanova, Dan Klein, Christopher D. Man-
ning, and Yoram Singer. 2003. Feature-rich part-of-
speech tagging with a cyclic dependency network.
In Proceedings of the 2003 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology
- Volume 1, NAACL ?03, pages 173?180, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Zhaopeng Tu, Yang Liu, Yifan He, Josef van Genabith,
Qun Liu, and Shouxun Lin. 2012. Combining mul-
tiple alignments to improve machine translation. In
COLING (Posters), pages 1249?1260.
141
Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 122?131,
October 25, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Transformation and Decomposition for Efficiently Implementing and
Improving Dependency-to-String Model In Moses
Liangyou Li
?
, Jun Xie
?
, Andy Way
?
and Qun Liu
??
?
CNGL Centre for Global Intelligent Content, School of Computing
Dublin City University, Dublin 9, Ireland
?
Key Laboratory of Intelligent Information Processing, Institute of Computing Technology
Chinese Academy of Sciences, Beijing, China
{liangyouli,away,qliu}@computing.dcu.ie
junxie@ict.ac.cn
Abstract
Dependency structure provides grammat-
ical relations between words, which have
shown to be effective in Statistical Ma-
chine Translation (SMT). In this paper, we
present an open source module in Moses
which implements a dependency-to-string
model. We propose a method to trans-
form the input dependency tree into a cor-
responding constituent tree for reusing the
tree-based decoder in Moses. In our ex-
periments, this method achieves compara-
ble results with the standard model. Fur-
thermore, we enrich this model via the
decomposition of dependency structure,
including extracting rules from the sub-
structures of the dependency tree during
training and creating a pseudo-forest in-
stead of the tree per se as the input dur-
ing decoding. Large-scale experiments
on Chinese?English and German?English
tasks show that the decomposition ap-
proach improves the baseline dependency-
to-string model significantly. Our sys-
tem achieves comparable results with the
state-of-the-art hierarchical phrase-based
model (HPB). Finally, when resorting to
phrasal rules, the dependency-to-string
model performs significantly better than
Moses HPB.
1 Introduction
Dependency structure models relations between
words in a sentence. Such relations indicate
the syntactic function of one word to another
word. As dependency structure directly encodes
semantic information and has the best inter-lingual
phrasal cohesion properties (Fox, 2002), it is be-
lieved to be helpful to translation.
In recent years, dependency structure has been
widely used in SMT. For example, Shen et al.
(2010) present a string-to-dependency model by
using the dependency fragments of the neighbour-
ing words on the target side, which makes it easier
to integrate a dependency language model. How-
ever such string-to-tree systems run slowly in cu-
bic time (Huang et al., 2006).
Another example is the treelet approach
(Menezes and Quirk, 2005; Quirk et al., 2005),
which uses dependency structure on the source
side. Xiong et al. (2007) extend the treelet ap-
proach to allow dependency fragments with gaps.
As the treelet is defined as an arbitrary connected
sub-graph, typically both substitution and inser-
tion operations are adopted for decoding. How-
ever, as translation rules based on the treelets
do not encode enough reordering information di-
rectly, another heuristic or separate reordering
model is usually needed to decide the best target
position of the inserted words.
Different from these works, Xie et al. (2011)
present a dependency-to-string (Dep2Str) model,
which extracts head-dependent (HD) rules from
word-aligned source dependency trees and target
strings. As this model specifies reordering infor-
mation in the HD rules, during translation only the
substitution operation is needed, because words
are reordered simultaneously with the rule being
applied. Meng et al. (2013) and Xie et al. (2014)
extend the model by augmenting HD rules with the
help of either constituent tree or fixed/float struc-
ture (Shen et al., 2010). Augmented rules are cre-
ated by the combination of two or more nodes in
122
the HD fragment, and are capable of capturing
translations of non-syntactic phrases. However,
the decoder needs to be changed correspondingly
to handle these rules.
Attracted by the simplicity of the Dep2Str
model, in this paper we describe an easy way to
integrate the model into the popular translation
framework Moses (Koehn et al., 2007). In or-
der to share the same decoder with the conven-
tional syntax-based model, we present an algo-
rithm which transforms a dependency tree into a
corresponding constituent tree which encodes de-
pendency information in its non-leaf nodes and is
compatible with the Dep2Str model. In addition,
we present a method to decompose a dependency
structure (HD fragment) into smaller parts which
enrich translation rules and also allow us to cre-
ate a pseudo-forest as the input. ?Pseudo? means
the forest is not obtained by combining several
trees from a parser, but rather that it is created
based on the decomposition of an HD fragment.
Large-scale experiments on Chinese?English and
German?English tasks show that the transforma-
tion and decomposition are effective for transla-
tion.
In the remainder of the paper, we first describe
the Dep2Str model (Section 2). Then we describe
how to transform a dependency tree into a con-
stituent tree which is compatible with the Dep2Str
model (Section 3). The idea of decomposition in-
cluding extracting sub-structural rules and creat-
ing a pseudo-forest is presented in Section 4. Then
experiments are conducted to compare translation
results of our approach with the state-of-the-art
HPB model (Section 5). We conclude in Section 6
and present avenues for future work.
2 Dependency-to-String Model
In the Dep2Str model (Xie et al., 2011), the HD
fragment is the basic unit. As shown in Figure
1, in a dependency tree, each non-leaf node is the
head of some other nodes (dependents), so an HD
fragment is composed of a head node and all of its
dependents.
1
In this model, there are two kinds of rules for
translation. One is the head rule which specifies
the translation of a source word:
Juxing
??? holds
1
In this paper, HD fragment of a node means the HD frag-
ment with this node as the head. Leaf nodes have no HD
fragments.
Boliweiya
????/NN
Juxing
??/VV
Zongtong
??/NN
Yu
?/CC
Guohui
??/NN
Xuanju
??/NN
Figure 1: Example of a dependency tree, with
head-dependent fragments being indicated by dot-
ted lines.
The other one is the HD rule which consists of
three parts: the HD fragment s of the source
side (maybe containing variables), a target string
t (maybe containing variables) and a one-to-one
mapping ? from variables in s to variables in t, as
in:
s = (
Boliweiya
????)
Juxing
?? (x
1
:
Xuanju
?? )
t = Bolivia holds x
1
? = {x
1
:
Xuanju
??? x
1
}
where the underlined element denotes the leaf
node. Variables in the Dep2Str model are con-
strained either by words (like x
1
:??) or Part-of-
Speech (POS) tags (like x
1
:NN).
Given a source sentence with a dependency tree,
a target string and the word alignment between the
source and target sentences, this model first an-
notates each node N with two annotations: head
span and dependency span.
2
These two spans
specify the corresponding target position of a node
(by the head span) or sub-tree (by the depen-
dency span). After annotation, acceptable HD
fragments
3
are utilized to induce lexicalized HD
2
Some definitions: Closure clos(S) of set S is the small-
est superset of S in which the elements (integers) are contin-
uous. Let H be the set of indexes of target words aligned to
node N . Head span hsp(N) of node N is clos(H). Head
span hsp(N) is consistent if it does not overlap with head
span of any other node. Dependency span dsp(N) of node
N is the union of all consistent head spans in the subtree
rooted at N .
3
A head-dependent fragment is acceptable if the head
span of the head node is consistent and none of the depen-
dency spans of its dependents is empty. We could see that
in an acceptable fragment, the head span of the head node
and dependency spans of dependents are not overlapped with
each other.
123
          Boliweiya Juxing  Xuanju
R ule:  ( ????)  ?? ( x1 :??) Boliv ia hold s  x1
                       Xuanju
R ule:  ( x1: N N )  ?? x1 elec tions
           Guohui
R ule:  ?? p ar liam ent
           Zongtong Yu      Guohui
R ule:    ( ??)    ( ?)  x1:?? p r es id ential and  x1
Boliweiya
????/NN
Juxing
??/VV
Zongtong
??/NN
Yu
?/CC
Guohui
??/NN
Xuanju
??/NN
Zongtong
??/NN
Yu
?/CC
Guohui
??/NNBoliv ia hold s elec tions
Boliv ia hold s
Zongtong
??/NN
Yu
?/CC
Guohui
??/NN
Xuanju
??/NN
Boliv ia hold s  p r es id ential and  p ar liam ent elec tions
Boliv ia hold s  p r es id ential and
Guohui
??/NN elec tions
( a)
( b )
( c )
( d )
( e)
Figure 2: Example of a derivation. Underlined el-
ements indicate leaf nodes.
rules (the head node and leaf node are represented
by words, while the internal nodes are replaced by
variables constrained by word) and unlexicalized
HD rules (nodes are replaced by variables con-
strained by POS tags).
In HD rules, an internal node denotes the whole
sub-tree and is always a substitution site. The head
node and leaf nodes can be represented by either
words or variables. The target side corresponding
to an HD fragment and the mapping between vari-
ables are determined by the head span of the head
node and the dependency spans of the dependents.
A translation can be obtained by applying rules
to the input dependency tree. Figure 2 shows a
derivation for translating a Chinese sentence into
an English string. The derivation proceeds from
top to bottom. Variables in the higher-level HD
rules are substituted by the translations of lower
HD rules recursively.
The final translation is obtained by finding the
best derivation d
?
from all possible derivations
D which convert the source dependency structure
into a target string, as in Equation (1):
d
?
= argmax
d?D
p(d) ? argmax
d?D
?
i
?
i
(d)
?
i
(1)
where ?
i
(d) is the ith feature defined in the deriva-
tion d, and ?
i
is the weight of the feature.
3 Transformation of Dependency Trees
In this section, we introduce an algorithm to trans-
form a dependency tree into a corresponding con-
stituent tree, where words of the source sentence
are leaf nodes and internal nodes are labelled with
head words or POS tags which are constrained by
dependency information. Such a transformation
makes it possible to use the traditional tree-based
decoder to translate a dependency tree, so we can
easily integrate the Dep2Str model into the popu-
lar framework Moses.
In a tree-based system, the CYK algorithm
(Kasami, 1965; Younger, 1967; Cocke and
Schwartz, 1970) is usually employed to translate
the input sentence with a tree structure. Each time
a continuous sequence of words (a phrase) in the
source sentence is translated. Larger phrases can
be translated by combining translations of smaller
phrases.
In a constituent tree, the source words are leaf
nodes and all non-leaf nodes covering a phrase are
labelled with categories which are usually vari-
ables defined in the tree-based model. For trans-
lating a phrase covered by a non-leaf node, the de-
coder for the constituent tree can easily find ap-
plied rules by directly matching variables in these
rules to tree nodes. However, in a dependency tree,
each internal node represents a word of the source
sentence. Variables covering a phrase cannot be
recognized directly. Therefore, to share the same
decoder with the constituent tree, the dependency
tree needs to be transformed into a constituent-
style tree.
As we described in Section 2, each variable in
the Dep2Str model represents a word (for the head
and leaf node) or a sequence of continuous words
(for the internal node). Thus it is intuitive to use
these variables to label non-leaf nodes of the pro-
duced constituent tree. Furthermore, in order to
preserve the dependency information of each HD
fragment, the created constituent node needs to be
constrained by the dependency information in the
HD fragment.
Our transformation algorithm is shown in Al-
gorithm 1, which proceeds recursively from top to
bottom on each HD fragment. There are a maxi-
mum of three types of nodes in an HD fragment:
head node, leaf nodes, and internal nodes. The
124
Algorithm 1 Algorithm for transforming a depen-
dency tree to constituent tree. Dnode means node
in dependency tree. Cnode means node in con-
stituent tree.
function CNODE(label, span)
create a new Cnode CN
CN.label? label
CN.span? span
end function
function TRANSFNODE(Dnode H)
pos? POS of H
constrain pos . with H0, like: NN:H0
CNODE(label,H.position)
for each dependent N of H do
pos? POS of N
word? word of N
constrain pos . with Li or Ri, like: NN:R1
constrain word . with Li or Ri
if N is leaf then
CNODE(pos,N.position)
else
CNODE(word,H.span)
CNODE(pos,H.span)
TRANSFNODE(N )
end if
end for
end function
leaf nodes and internal nodes are dependents of
the head node. For the leaf node and head node,
we create constituent nodes that just cover one
word. For an internal node N , we create con-
stituent nodes that cover all the words in the sub-
tree rooted at N . In Algorithm 1, N.position
means the position of the word represented by the
node N . N.span denotes indexes of words cov-
ered by the sub-tree rooted at node N .
Taking the dependency tree in Figure 1 as an
example, its transformation result for integration
with Moses is shown in Figure 3. In the Dep2Str
model, leaf nodes can be replaced by a vari-
able constrained by its POS tag, so for leaf node
?
Zongtong
?? ? in HD fragment ?
Zongtong
(??)
Yu
(?)
Guohui
???,
we create a constituent node ?NN:L2?, where
?NN? is the POS tag and ?L2? denotes that the leaf
node is the second left dependent of the head node.
For the internal node ?
Guohui
??? in the HD fragment
?
Guohui
(??)
Xuanju
???, we create two constituent nodes
Boliweiya
????
Juxing
??
Zongtong
??
Yu
?
Guohui
??
Xuanju
??
N N : L 1 V V : H 0 N N : L 2 C C : L 1 N N : H 0N N : H 0
N N : L 1
N N : R 1
S
Guohui
??: L 1
Xuanju
??: R 1
Figure 3: The corresponding constituent tree af-
ter transforming the dependency tree in Figure 1.
Note in our implementation, we do not distinguish
the leaf node and internal node of a dependency
tree in the produced constituent tree and induced
rules.
which cover all words in the dependency sub-tree
rooted at this node, with one of them labelled by
the word itself. Both nodes are constrained by de-
pendency information ?L1?. After such a transfor-
mation is conducted on each HD fragment recur-
sively, we obtain a constituent tree.
This transformation makes our implementation
of the Dep2Str model easier, because we can use
the tree-to-string decoder in Moses. All we need
to do is to write a new rule extractor which extracts
head rules and HD rules (see Section 2) from the
word-aligned source dependency trees and target
strings, and represents these rules in the format de-
fined in Moses.
4
Note that while this conversion is performed
on an input dependency tree during decoding, the
training part, including extracting rules and cal-
culating translation probabilities, does not change,
so the model is still a dependency-to-string model.
4
Taking the rule in Section 2 as an example, its represen-
tation in Moses is:
s =
Boliweiya
????
Juxing
??
Xuanju
[??:R1][X] [H1]
t = Bolivia holds
Xuanju
[??:R1][X] [X]
? = {2 ? 2}
where ?H1? denotes the position of the head word is 1, ?R1?
indicates the first right dependent of the head word, ?X? is the
general label for the target side and ? is the set of alignments
(the index-correspondences between s and t). The format has
been described in detail at http://www.statmt.org/
moses/?n=Moses.SyntaxTutorial.
125
In addition, our transformation is different from
other works which transform a dependency tree
into a constituent tree (Collins et al., 1999; Xia and
Palmer, 2001). In this paper, the produced con-
stituent tree still preserves dependency relations
between words, and the phrasal structure is di-
rectly derived from the dependency structure with-
out refinement. Accordingly, the constituent tree
may not be a linguistically well-formed syntactic
structure. However, it is not a problem for our
model, because in this paper what matters is the
dependency structure which has already been en-
coded into the (ill-formed) constituent tree.
4 Decomposition of Dependency
Structure
The Dep2Str model treats a whole HD fragment
as the basic unit, which may result in a sparse-
data problem. For example, an HD fragment with
a verb as head typically consists of more than four
nodes (Xie et al., 2011). Thus in this section, in-
spired by the treelet approach, we describe a de-
composition method to make use of smaller frag-
ments.
In an HD fragment of a dependency tree, the
head determines the semantic category, while
the dependent gives the semantic specification
(Zwicky, 1985; Hudson, 1990). Accordingly, it
is reasonable to assume that in an HD fragment,
dependents could be removed or new dependents
could be attached as needed. Thus, in this paper,
we assume that a large HD fragment is formed by
attaching dependents to a small HD fragment. For
simplicity and reuse of the decoder, such an at-
tachment is carried out in one step. This means
that an HD fragment is decomposed into two
smaller parts in a possible decomposition. This
decomposition can be formulated as Equation (2):
L
i
? ? ?L
1
HR
1
? ? ?R
j
= L
m
? ? ?L
1
HR
1
? ? ?R
n
+ L
i
? ? ?L
m+1
HR
n+1
? ? ?R
j
subject to
i ? 0, j ? 0
i ? m ? 0, j ? n ? 0
i+ j > m+ n > 0
(2)
whereH denotes the head node, L
i
denotes the ith
left dependent and R
j
denotes the jth right depen-
dent. Figure 4 shows an example.
s m ar t/ JJ
v er y/ R BS he/ P R P
s m ar t/ JJ
is / V BZ
S he/ P R P
s m ar t/ JJ
is / V BZ v er y/ R B
+
Figure 4: An example of decomposition on a head-
dependent fragment.
Algorithm 2 Algorithm for the decomposition of
an HD fragment into two sub-fragments. Index of
nodes in a fragment starts from 0.
function DECOMP(HD fragment frag)
fset ? {}
len? number of nodes in frag
hidx? the index of head node in frag
for s = 0 to hidx do
for e = hidx to len? 1 do
if 0 < e? s < len? 1 then
create sub-fragment core
core? nodes from s to e
add core to fset
create sub-fragment shell
initialize shell with head node
shell? nodes not in core
add shell to fset
end if
end for
end for
end function
Such a decomposition of an HD fragment en-
ables us to create translation rules extracted from
sub-structures and create a pseudo-forest from
the input dependency tree to make better use of
smaller rules.
4.1 Sub-structural Rules
In the Dep2Str model, rules are extracted on
an entire HD fragment. In this paper, when
the decomposition is considered, we also extract
sub-structural rules by taking each possible sub-
fragment as a new HD fragment. The algorithm
for recognizing the sub-fragments is shown in Al-
gorithm 2.
In Algorithm 2, we find all possible decom-
126
positions of an HD fragment. Each decom-
position produces two sub-fragments: core and
shell. Both core and shell include the head node.
core contains the dependents surrounding the head
node, with the remaining dependents belonging to
shell. Taking Figure 4 as an example, the bottom-
right part is core, while the bottom-left part is
shell. Each core and shell could be seen as a
new HD fragment. Then HD rules are extracted as
defined in the Dep2Str model.
Note that different from the augmented HD
rules, where Meng et al. (2013) annotate rules with
combined variables and Xie et al. (2014) create
special rules from HD rules at runtime by com-
bining several nodes, our sub-structural rules are
standard HD rules, which are extracted from the
connected sub-structures of a larger HD fragment
and can be used directly in the model.
4.2 Pseudo-Forest
Although sub-structural rules are effective in our
experiments (see Section 5), we still do not use
them to their best advantage, because we only en-
rich smaller rules in our model. During decod-
ing, for a large input HD fragment, the model is
still more likely to resort to glue rules. However,
the idea of decomposition allows us to create a
pseudo-forest directly from the dependency tree to
alleviate this problem to some extent.
As described above, an HD fragment can be
seen as being created by combining two smaller
fragments. This means, for an HD fragment in the
input dependency tree, we can translate one of its
sub-fragments first, then obtain the whole trans-
lation by combining with translations of another
sub-fragment. From Algorithm 2, we know that
the sub-fragment core covers a continuous phrase
of the source sentence. Accordingly, we can trans-
late this fragment first and then build the whole
translation by translating another sub-fragment
shell. Figure 5 gives an example of translating
an HD fragment by combining the translations of
its sub-fragments.
Instead of taking the dependency tree as the in-
put and looking for all rules for translating sub-
fragments of a whole HD, we directly encode the
decomposition into the input dependency tree with
the result being a pseudo-forest. Based on the
transformation algorithm in Section 3, the pseudo-
forest can also be represented in the constituent-
tree style, as shown in Figure 6.
          Yu  Guohui
R ule:  ( ?)  ?? and  p ar lim ent
Zongtong
??/NN
Yu
?/CC
Guohui
??/NN
Zongtong
R ule:  ( ??)  x 1 : N N p r es id ential x1
p r es id ential and  p ar liam ent
Zongtong
??        and  p ar liam ent
Guohui
??/NN
( a)
( b )
( c )
Figure 5: An example of translating a large HD
fragment with the help of translations of its de-
composed fragments.
S
N N : L 1 V V : H 0
N N : L 2 C C : L 1 N N : H 0N N : H 0
N N : R 1
Xuanju
??: R 1
N N : L 1
Guohui
??: L 1
Boliweiya
????
Juxing
??
Zongtong
??
Yu
?
Guohui
??
Xuanju
??
N N : L 1
N N : H 0
V V : H 0
V V : H 0
Figure 6: An example of a pseudo-forest for the
dependency tree in Figure 1. It is represented us-
ing the constituent-tree style described in Section
3. Edges drawn in the same type of line are owned
by the same sub-tree. Solid lines are shared edges.
In the pseudo-forest, we actually only create a
forest structure for each HD fragment. For ex-
ample, based on Figure 5, we create a constituent
node labelled with ?NN:H0? that covers the sub-
fragment ?
Yu
(?)
Guohui
???. In so doing, a new node la-
belled with ?NN:L1? is also created, which covers
the Node ?
Zongtong
?? ?, because it is now the first left
dependent in the sub-fragment ?
Zongtong
(??)
Guohui
?? ?.
Compared to the forest-based model (Mi et al.,
2008), such a pseudo-forest cannot efficiently re-
duce the influence of parsing errors, but it is easily
available and compatible with the Dep2Str Model.
127
corpus sentences words(ch) words(en)
train 1,501,652 38,388,118 44,901,788
dev 878 22,655 26,905
MT04 1,597 43,719 52,705
MT05 1,082 29,880 35,326
Table 1: Chinese?English corpus. For the English
dev and test sets, words counts are averaged across
4 references.
corpus sentences words(de) words(en)
train 2,037,209 52,671,991 55,023,999
dev 3,003 72,661 74,753
test12 3,003 72,603 72,988
test13 3,000 63,412 64,810
Table 2: German?English corpus. In the dev and
test sets, there is only one English reference for
each German sentence.
5 Experiments
We conduct large-scale experiments to exam-
ine our methods on the Chinese?English and
German?English translation tasks.
5.1 Data
The Chinese?English training corpus is from
the LDC data, including LDC2002E18,
LDC2003E07, LDC2003E14, LDC2004T07,
the Hansards portion of LDC2004T08 and
LDC2005T06. We take NIST 2002 as the de-
velopment set to tune weights, and NIST 2004
(MT04) and NIST 2005 (MT05) as the test data to
evaluate the systems. Table 1 provides a summary
of the Chinese?English corpus.
The German?English training corpus is from
WMT 2014, including Europarl V7 and News
Commentary. News-test 2011 is taken as the de-
velopment set, while News-test 2012 (test12) and
News-test 2013 (test13) are our test sets. Table 2
provides a summary of the German?English cor-
pus.
5.2 Baseline
For both language pairs, we filter sentence pairs
longer than 80 words and keep the length ratio
less than or equal to 3. English sentences are to-
kenized with scripts in Moses. Word alignment is
performed by GIZA++ (Och and Ney, 2003) with
the heuristic function grow-diag-final-and (Koehn
et al., 2003). We use SRILM (Stolcke, 2002) to
Systems MT05
XJ 33.91
D2S 33.79
Table 3: BLEU score [%] of the Dep2Str model
before (XJ) and after (D2S) dependency tree be-
ing transformed. Systems are trained on a selected
1.2M Chinese?English corpus.
train a 5-gram language model on the Xinhua por-
tion of the English Gigaword corpus 5th edition
with modified Kneser-Ney discounting (Chen and
Goodman, 1996). Minimum Error Rate Train-
ing (Och, 2003) is used to tune weights. Case-
insensitive BLEU (Papineni et al., 2002) is used to
evaluate the translation results. Bootstrap resam-
pling (Koehn, 2004) is also performed to compute
statistical significance with 1000 iterations.
We implement the baseline Dep2Str model
in Moses with methods described in this paper,
which is denoted as D2S. The first experiment we
do is to sanity check our implementation. Thus
we take a separate system (denoted as XJ) for
comparison which implements the Dep2Str model
based on (Xie et al., 2011). As shown in Table
3, using the transformation of dependency trees,
the Dep2Str model implemented in Moses (D2S)
is comparable with the standard implementation
(XJ).
In the rest of this section, we describe exper-
iments which compare our system with Moses
HPB (default setting), and test whether our de-
composition approach improves performance over
the baseline D2S.
As described in Section 2, the Dep2Str model
only extracts phrase rules for translating a source
word (head rule). This model could be enhanced
by including phrase rules that cover more than one
source word. Thus we also conduct experiments
where phrase pairs
5
are added into our system. We
set the length limit for phrase 7.
5.3 Chinese?English
In the Chinese?English translation task, the Stan-
ford Chinese word segmenter (Chang et al., 2008)
is used to segment Chinese sentences into words.
The Stanford dependency parser (Chang et al.,
2009) parses a Chinese sentence into the projec-
tive dependency tree.
5
In this paper, the use of phrasal rules is similar to that of
the HPB model, so they can be handled by Moses directly.
128
Systems MT04 MT05
Moses HPB 35.56 33.99
D2S 33.93 32.56
+pseudo-forest 34.28 34.10
+sub-structural rules 34.78 33.63
+pseudo-forest 35.46 34.13
+phrase 36.76* 34.67*
Table 4: BLEU score [%] of our method and
Moses HPB on the Chinese?English task. We use
bold font to indicate that the result of our method
is significantly better than D2S at p ? 0.01 level,
and * to indicate the result is significantly better
than Moses HPB at p ? 0.01 level.
Table 4 shows the translation results. We find
that the decomposition approach proposed in this
paper, including sub-structural rules and pseudo-
forest, improves the baseline system D2S sig-
nificantly (absolute improvement of +1.53/+1.57
(4.5%/4.8%, relative)). As a result, our sys-
tem achieves comparable (-0.1/+0.14) results with
Moses HPB. After including phrasal rules, our
system performs significantly better (absolute im-
provement of +1.2/+0.68 (3.4%/2.0%, relative))
than Moses HPB on both test sets.
6
5.4 German?English
We tokenize German sentences with scripts in
Moses and use mate-tools
7
to perform morpho-
logical analysis and parse the sentence (Bohnet,
2010). Then the MaltParser
8
converts the parse
result into the projective dependency tree (Nivre
and Nilsson, 2005).
Experimental results in Table 5 show that incor-
porating sub-structural rules improves the base-
line D2S system significantly (absolute improve-
ment of +0.47/+0.63, (2.3%/2.8%, relative)), and
achieves a slightly better (+0.08) result on test12
than Moses HPB. However, in the German?
English task, the pseudo-forest produces a neg-
ative effect on the baseline system (-0.07/-0.45),
despite the fact that our system combining both
methods together is still better (+0.2/+0.11) than
the baseline D2S. In the end, by resorting to
6
In our preliminary experiments, phrasal rules are also
able to significantly improve our system on their own on both
Chinese?English and German?English tasks, but the best per-
formance is achieved by combining them with sub-structural
rules and/or pseudo-forest.
7
http://code.google.com/p/mate-tools/
8
http://www.maltparser.org/
Systems test12 test13
Moses HPB 20.44 22.77
D2S 20.05 22.13
+pseudo-forest 19.98 21.68
+sub-structural rules 20.52 22.76
+phrase 20.91* 23.46*
+pseudo-forest 20.25 22.24
+phrase 20.75* 23.20*
Table 5: BLEU score [%] of our method and
Moses HPB on German?English task. We use
bold font to indicate that the result of our method
is significantly better than baseline D2S at p ?
0.01 level, and * to indicate the result is signifi-
cantly better than Moses HPB at p ? 0.01 level.
Systems
# Rules
CE task DE task
Moses HPB 388M 684M
D2S 27M 41M
+sub-structural rules 116M 121M
+phrase 215M 274M
Table 6: The number of rules in different sys-
tems On the Chinese?English (CE) and German?
English (DE) corpus. Note that pseudo-forest (not
listed) does not influence the number of rules.
phrasal rules, our system achieves the best perfor-
mance overall which is significantly better (abso-
lute improvement of +0.47/+0.59 (2.3%/2.6%, rel-
ative)) than Moses HPB.
5.5 Discussion
Besides long-distance reordering (Xie et al.,
2011), another attraction of the Dep2Str model is
its simplicity. It can perform fast translation with
fewer rules than HPB. Table 6 shows the number
of rules in each system. It is easy to see that all of
our systems use fewer rules than HPB. However,
the number of rules is not proportional to transla-
tion quality, as shown in Tables 4 and 5.
Experiments on the Chinese?English corpus
show that it is feasible to translate the dependency
tree via transformation for the Dep2Str model de-
scribed in Section 2. Such a transformation causes
the model to be easily integrated into Moses with-
out making changes to the decoder, while at the
same time producing comparable results with the
standard implementation (shown in Table 3).
The decomposition approach proposed in this
129
paper also shows a positive effect on the base-
line Dep2Str system. Especially, sub-structural
rules significantly improve the Dep2Str model on
both Chinese?English and German?English tasks.
However, experiments show that the pseudo-forest
significantly improves the D2S system on the
Chinese?English data, while it causes translation
quality to decline on the German?English data.
Since using the pseudo-forest in our system is
aimed at translating larger HD fragments via split-
ting it into pieces, we hypothesize that when trans-
lating German sentences, the pseudo-forest ap-
proach more likely results in much worse rules be-
ing applied. This is probably due to the shorter
Mean Dependency Distance (MDD) and freer
word order of German sentences(Eppler, 2013).
6 Conclusion
In this paper, we present an open source mod-
ule which integrates a dependency-to-string model
into Moses.
This module transforms an input depen-
dency tree into a corresponding constituent tree
during decoding which makes Moses perform
dependency-based translation without necessitat-
ing any changes to the decoder. Experiments on
Chinese?English show that the performance if our
system is comparable with that of the standard
dependency-based decoder.
Furthermore, we enhance the model by de-
composing head-dependent fragments into smaller
pieces. This decomposition enriches the Dep2Str
model with more rules during training and allows
us to create a pseudo-forest as input instead of
a dependency tree during decoding. Large-scale
experiments on Chinese?English and German?
English tasks show that this decomposition can
significantly improve the baseline dependency-
to-string model on both language pairs. On
the German?English task, sub-structural rules are
more useful than the pseudo-forest input. In the
end, by resorting to phrasal rules, our system
performs significantly better than the hierarchical
phrase-based model in Moses.
Our implementation of the dependency-to-
string model with methods described in this pa-
per is available at http://computing.dcu.
ie/
?
liangyouli/dep2str.zip. In the fu-
ture, we would like to conduct more experiments
on other language pairs to examine this model,
as well as reducing the restrictions on decompo-
sition.
Acknowledgments
This research has received funding from the Peo-
ple Programme (Marie Curie Actions) of the Eu-
ropean Union?s Seventh Framework Programme
FP7/2007-2013/ under REA grant agreement no.
317471. This research is also supported by the
Science Foundation Ireland (Grant 12/CE/I2267)
as part of the Centre for Next Generation Local-
isation at Dublin City University. The authors of
this paper also thank the reviewers for helping to
improve this paper.
References
Bernd Bohnet. 2010. Very High Accuracy and Fast
Dependency Parsing is Not a Contradiction. In Pro-
ceedings of the 23rd International Conference on
Computational Linguistics, pages 89?97, Beijing,
China.
Pi-Chuan Chang, Michel Galley, and Christopher D.
Manning. 2008. Optimizing Chinese Word Seg-
mentation for Machine Translation Performance. In
Proceedings of the Third Workshop on Statistical
Machine Translation, pages 224?232, Columbus,
Ohio.
Pi-Chuan Chang, Huihsin Tseng, Dan Jurafsky, and
Christopher D. Manning. 2009. Discriminative Re-
ordering with Chinese Grammatical Relations Fea-
tures. In Proceedings of the Third Workshop on Syn-
tax and Structure in Statistical Translation, pages
51?59, Boulder, Colorado.
Stanley F. Chen and Joshua Goodman. 1996. An
Empirical Study of Smoothing Techniques for Lan-
guage Modeling. In Proceedings of the 34th Annual
Meeting on Association for Computational Linguis-
tics, pages 310?318, Santa Cruz, California.
John Cocke and Jacob T. Schwartz. 1970. Program-
ming Languages and Their Compilers: Preliminary
Notes. Technical report, Courant Institute of Math-
ematical Sciences, New York University, New York,
NY.
Michael Collins, Lance Ramshaw, Jan Haji?c, and
Christoph Tillmann. 1999. A Statistical Parser for
Czech. In Proceedings of the 37th Annual Meeting
of the Association for Computational Linguistics on
Computational Linguistics, pages 505?512, College
Park, Maryland.
Eva M. Duran Eppler. 2013. Dependency Distance
and Bilingual Language Use: Evidence from Ger-
man/English and Chinese/English Data. In Proceed-
ings of the Second International Conference on De-
pendency Linguistics (DepLing 2013), pages 78?87,
Prague, August.
130
Heidi J. Fox. 2002. Phrasal Cohesion and Statis-
tical Machine Translation. In Proceedings of the
ACL-02 Conference on Empirical Methods in Nat-
ural Language Processing - Volume 10, pages 304?
3111, Philadelphia.
Liang Huang, Kevin Knight, and Aravind Joshi. 2006.
A Syntax-directed Translator with Extended Do-
main of Locality. In Proceedings of the Workshop
on Computationally Hard Problems and Joint Infer-
ence in Speech and Language Processing, pages 1?
8, New York City, New York.
Richard Hudson. 1990. English Word Grammar.
Blackwell, Oxford, UK.
Tadao Kasami. 1965. An Efficient Recognition and
Syntax-Analysis Algorithm for Context-Free Lan-
guages. Technical report, Air Force Cambridge Re-
search Lab, Bedford, MA.
Philipp Koehn. 2004. Statistical Significance Tests
for Machine Translation Evaluation. In Proceedings
of EMNLP 2004, pages 388?395, Barcelona, Spain,
July.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ond?rej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
In Proceedings of the 45th Annual Meeting of the
ACL on Interactive Poster and Demonstration Ses-
sions, pages 177?180, Prague, Czech Republic.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical Phrase-Based Translation. In Pro-
ceedings of the 2003 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics on Human Language Technology - Vol-
ume 1, pages 48?54, Edmonton, Canada.
Arul Menezes and Chris Quirk. 2005. Dependency
Treelet Translation: The Convergence of Statistical
and Example-Based Machine-translation? In Pro-
ceedings of the Workshop on Example-based Ma-
chine Translation at MT Summit X, September.
Fandong Meng, Jun Xie, Linfeng Song, Yajuan L?u,
and Qun Liu. 2013. Translation with Source Con-
stituency and Dependency Trees. In Proceedings of
the 2013 Conference on Empirical Methods in Natu-
ral Language Processing, pages 1066?1076, Seattle,
Washington, USA, October.
Haitao Mi, Liang Huang, and Qun Liu. 2008. Forest-
Based Translation. In Proceedings of ACL-08: HLT,
pages 192?199, June.
Joakim Nivre and Jens Nilsson. 2005. Pseudo-
Projective Dependency Parsing. In Proceedings of
the 43rd Annual Meeting on Association for Com-
putational Linguistics, pages 99?106, Ann Arbor,
Michigan.
Franz Josef Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. In Proceedings
of the 41st Annual Meeting on Association for Com-
putational Linguistics - Volume 1, pages 160?167,
Sapporo, Japan.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29(1):19?51,
March.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: A Method for Automatic
Evaluation of Machine Translation. In Proceedings
of the 40th Annual Meeting on Association for Com-
putational Linguistics, pages 311?318, Philadelphia,
Pennsylvania.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005.
Dependency Treelet Translation: Syntactically In-
formed Phrasal SMT. In Proceedings of the 43rd
Annual Meeting of the Association for Computa-
tional Linguistics (ACL?05), pages 271?279, Ann
Arbor, Michigan, June.
Libin Shen, Jinxi Xu, and Ralph Weischedel. 2010.
String-to-Dependency Statistical Machine Transla-
tion. Computational Linguistics, 36(4):649?671,
December.
Andreas Stolcke. 2002. SRILM-an Extensible Lan-
guage Modeling Toolkit. In Proceedings Interna-
tional Conference on Spoken Language Processing,
pages 257?286, November.
Fei Xia and Martha Palmer. 2001. Converting De-
pendency Structures to Phrase Structures. In Pro-
ceedings of the First International Conference on
Human Language Technology Research, pages 1?5,
San Diego.
Jun Xie, Haitao Mi, and Qun Liu. 2011. A Novel
Dependency-to-string Model for Statistical Machine
Translation. In Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing, pages 216?226, Edinburgh, United Kingdom.
Jun Xie, Jinan Xu, and Qun Liu. 2014. Augment
Dependency-to-String Translation with Fixed and
Floating Structures. In Proceedings of the 25th In-
ternational Conference on Computational Linguis-
tics, pages 2217?2226, Dublin, Ireland.
Deyi Xiong, Qun Liu, and Shouxun Lin. 2007. A De-
pendency Treelet String Correspondence Model for
Statistical Machine Translation. In Proceedings of
the Second Workshop on Statistical Machine Trans-
lation, pages 40?47, Prague, June.
Daniel H. Younger. 1967. Recognition and Parsing of
Context-Free Languages in Time n
3
. Information
and Control, 10(2):189?208.
Arnold M. Zwicky. 1985. Heads. Journal of Linguis-
tics, 21:1?29, 3.
131
