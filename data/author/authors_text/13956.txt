Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 705?708,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Is Arabic Part of Speech Tagging Feasible Without Word Segmentation?
Emad Mohamed, Sandra Ku?bler
Indiana University
Department of Linguistics
Memorial Hall 322
Bloomington, IN 47405
USA
{emohamed,skuebler}@indiana.edu
Abstract
In this paper, we compare two novel methods
for part of speech tagging of Arabic without
the use of gold standard word segmentation
but with the full POS tagset of the Penn Ara-
bic Treebank. The first approach uses com-
plex tags without any word segmentation, the
second approach is segmention-based, using
a machine learning segmenter. Surprisingly,
word-based POS tagging yields the best re-
sults, with a word accuracy of 94.74%.
1 Introduction
Arabic is a morphologically rich language, in
which a word carries not only inflections but
also clitics, such as pronouns, conjunctions, and
prepositions. This morphological complexity also
has consequences for the part-of-speech (POS)
annotation of Arabic: Since words can be com-
plex, POS tags refer to segments rather than to
whole words. Thus, the word wsyrfEwnhA
(in Buckwalter transliteration; engl.: and they
will raise it) is assigned the following POS tag:
[CONJ+FUTURE PARTICLE+IMPERFECT VERB PREFIX
+IMPERFECT VERB+IMPERFECT VERB SUFFIX MAS-
CULINE PLURAL 3RD PERSON+OBJECT PRONOUN
FEMININE SINGULAR] in the Penn Arabic Treebank
(ATB) (Bies and Maamouri, 2003); the boundaries
between segments are depicted by + signs. Auto-
matic approaches to POS tagging either must assign
such complex tags from a large tagset to complete
words, or they must segment the word first and
then assign POS tags to the segments. Previous
approaches (Diab et al, 2004; Habash and Rambow,
2005; van den Bosch et al, 2007; AlGahtani et al,
2009) chose the segmentation approach but concen-
trated on POS tagging by using the segmentation
provided by the ATB. Additionally, Diab et al and
Habash and Rambow used a reduced tagset. Diab et
al. and Habash and Rambow used Support Vector
Machines, the former with a standard windowing
approach, the latter performing a full morphological
analysis before POS tagging. Van den Bosch et
al., whose approach is the most similar to ours,
used memory-based learning with the full ATB
tagset. They report a POS tagging accuracy of
91.5% (93.3% on known words, 66.4% on unknown
words). However, they also evaluated on words
as defined in the ATB, which differs from written
Arabic in the treatment of affixes with syntactic
functions (see section 2 for details). AlGahtani et
al. used transformation-based learning combined
with a morphological analysis for unknown words
and words containing clitics. They reached a POS
tagging accuracy of 96.9% on ATB1. Surprisingly,
their results are lower for the experiment using the
whole ATB (96.1%).
In this paper, we present two methods for Ara-
bic POS tagging that do not require gold stan-
dard segmentation but can rather be used for natu-
rally occurring Arabic. We investigate two differ-
ent approaches: (1) Assigning complete POS tags
to whole words, without any segmentation, and (2)
a segmentation-based approach, for which we de-
veloped a machine learning based segmenter. In
this approach, the words are first passed to the
segmenter, then to the POS tagger. The first ap-
proach is surprisingly successful given the complex-
705
ity of the task, reaching an accuracy on the word
level of 94.74%, as compared to 93.47% for the
segmentation-based approach. Thus, the result for
the whole word approach is very close to the re-
sult obtained by using gold standard segmentation
(94.91%). However, a more detailed analysis shows
that this good performance of the word-based ap-
proach is due to its performance on known words
while the few unknown words are more often mis-
classified: we reach an accuracy of 96.61% on
known words but only 74.64% on unknown words.
2 Data, Methods, and Evaluation
Like the previous approaches, we base our experi-
ments on the ATB, specifically on the after-treebank
POS files, for extracting our training and test sets.
More specifically, we use two sections of the ATB
(P1V3 and P3V1) since those two sets do not contain
duplicate sentences. This data set contains approxi-
mately 500,000 words. In order to be as representa-
tive of real-world Arabic, we use the non-vocalized
version of the treebank. Since previous approaches,
to our knowledge, used different data sets, our re-
sults are not directly comparable.
For both segmentation and POS tagging, we mod-
ified the ATB representation of words in order to ob-
tain the text, as it would occur in newscasts. The
ATB treats inflectional affixes, including the defi-
nite article Al, as part of a word but splits off those
affixes that serve a syntactic function into separate
words. In order to obtain text as it occurs in news-
casts, we re-attached all conjunctions, prepositions,
pronouns, and any elements that constitute parts of
the word as an orthographic unit (with the excep-
tion of punctuation) to the word. The word ltxbrh
(engl.: in order to tell him), for example, is repre-
sented as three words in the ATB, l, txbr, and
h, but is treated as one single unit in our experi-
ment. Our second modification concerns the null
element in Arabic verbs. Since Arabic is pro-drop,
the ATB annotation includes a null element in place
of the omitted subject plus the POS tag it would
receive. Since this information is not available in
naturally occurring text, we delete the null element
and its tag. For example, {i$otaraY+(null)
and its tag PV+PVSUFF SUBJ: 3MS would occur
as {i$otaraY with the tag PV in our representa-
tion (we additionally remove the short vowels).
We perform 5-fold cross validation and use the
same data split for all three types of experiments: (1)
POS tagging using gold standard segmentation taken
from the ATB, (2) POS tagging using a segmenter,
and (3) POS tagging whole words with complex
POS tags. The first experiment serves as the upper
bound and as a comparison to previous approaches.
The second experiment uses an automatic segmenter
as a pre-processing component to the POS tagger.
This means that the accuracy of the segmenter is
also the upper limit of the POS tagger since errors
in segmentation inevitably lead to errors in POS tag-
ging. The last experiment uses full words and com-
plex POS tags. The purpose of this experiment is
to determine whether it is possible to tag complete
words without segmentation.
The segmenter and the two POS taggers use
memory-based learning. For segmentation, we use
TiMBL (Daelemans and van den Bosch, 2005); for
POS tagging MBT, a memory-based tagger (Daele-
mans et al, 1996). Memory-based learning is a lazy
learning paradigm that does not abstract over the
training data. During classification, the k nearest
neighbors to a new example are retrieved from the
training data, and the class that was assigned to the
majority of the neighbors is assigned to the new ex-
ample. MBT uses TiMBL as classifier; it offers the
possibility to use words from both sides of the focus
word as well as previous tagging decisions and am-
bitags as features. An ambitag is a combination of
all POS tags of the ambiguity class of the word.
Word segmentation is defined as a per-letter clas-
sification task: If a character in the word constitutes
the end of a segment, its class is ?+?, otherwise ?-?.
We use a sliding window approach with 5 characters
before and 5 characters after the focus character, the
previous decisions of the classifier, and the POS tag
of the focus word assigned by the whole word tag-
ger (cf. below) as features. The best results were
obtained for all experiments with the IB1 algorithm
with similarity computed as weighted overlap, rel-
evance weights computed with gain ratio, and the
number of k nearest neighbors equal to 1.
For POS tagging, we use the full tagset, with in-
formation about every segment in the word, rather
than the reduced tagset (RTS) used by Diab et al
and Habash and Rambow, since the RTS assumes
706
Gold Standard Segmentation Segmentation-Based Tagging Whole Words
SAR WAR SAR WAR WAR
96.72% 94.91% 94.70% 93.47% 94.74%
Table 1: POS tagging results.
a segmentation of words in which syntactically rel-
evant affixes are split from the stem. The word
w+y+bHv+wn+hA, for example, in RTS is split into
3 separate tokens, w, ybHvwn, hA. Then, each of
these tokens is assigned one POS tag, Conjunction
for w, Imperfective Verb for ybHvwn, and Pronoun
for hA. The split into tokens makes a preprocessing
step necessary, and it also affects evaluation since
a word-based evaluation is based on one word, the
RTS evaluation on 3 tokens for the above example.
For all the POS tagging experiments, we use
MBT. The best results were obtained with the Modi-
fied Value Difference Metric as a distance metric and
with k = 25. For known words, we use the IGTree
algorithm and 2 words to the left, their POS tags, the
focus word and its ambitag, 1 right context word and
its ambitag as features. For unknown words, we use
IB1 as algorithm and the unknown word itself, its
first 5 and last 3 characters, 1 left context word and
its POS tag, and 1 right context word and its ambitag
tag as features.
3 Experimental Results and Discussion
3.1 Word Segmentation
The memory-based word segmentation performs
very reliably with a word accuracy of 98.23%. This
also means that when the segmentation module is
used as a pre-processing step for POS tagging, the
accuracy of the tagger will have this accuracy as its
upper bound. While there are cases where wrong
segmentation results in the same number of seg-
ments, all of these words were assigned the wrong
POS tags in our data. In an error analysis, we found
that words of specific POS are more difficult to seg-
ment than others. Proper nouns constitute 33.87%
of all segmentation errors, possibly due to the fact
that many of these are either foreign names that re-
semble Arabic words (e.g. Knt, which is ambigu-
ous between the English name Kent, and the Ara-
bic verb I was), or they are ordinary nouns used as
proper nouns but with a different segmentation (e.g.
AlHyAp, engl.: the life). The POS tag with the
second highest error rate was the noun class with
30.67%.
3.2 Part of Speech Tagging
Table 1 shows the results of the three POS tagging
experiments described above. For the segmentation-
based experiments, we report per-segment (SAR)
and per-word (WAR) accuracy. As expected, POS
tagging using gold standard segments gives the best
results: 94.91% WAR. These results are approxi-
mately 3 percent points higher than those reported
by van den Bosch et al (2007). Although the results
are not absolutely comparable because of the dif-
ferent data sets, this experiment shows that our ap-
proach is competitive. The next experiments investi-
gate the two possibilities to perform POS tagging on
naturally occurring Arabic, i.e. when gold segmen-
tation is not available. The results of these experi-
ments show that POS tagging based on whole words
gives higher results (WAR: 94.74%) than tagging
based on automatic segmentation (WAR: 93.47%).
This result is surprising given that tagging whole
words is more difficult than assigning tags to seg-
ments, as there are 993 complex tags (22.70% of
which occur only once in the training set), versus
139 segment tags. A detailed error analysis of a pre-
vious but similar experiment can be found in Mo-
hamed and Ku?bler (2010).
We assume that these results are an artifact of the
ATB since it is based exclusively on newswire texts.
This means that there is only a limited vocabulary,
as shown by the very low rate of unknown words:
across the five folds, we calculated an average of
8.55% unknown words. In order to test our hypoth-
esis that unknown words are tagged more reliably
with a segment-based approach, we performed an
analysis on known and unknown words separately.
The results of this analysis are shown in Table 2.
This analysis shows that for all experiments, the
unknown words are tagged with a considerably
707
Gold Standard Segmentation Segmentation-Based Tagging Whole Words
Known words 95.90% 95.57% 96.61%
Unknown words 84.25% 71.06% 74.64%
Table 2: POS results for known and unknown words.
lower accuracy. However, the loss of performance
is more pronounced in the approaches without gold
segmentation. It is also evident that tagging whole
words reaches a higher accuracy than segment-based
tagging for both known words and unknown words.
From these results, we can conclude that while seg-
mentation makes properties of the words available,
it is not required for POS tagging. We also inves-
tigated the poor performance of the segmentation-
based tagger. A closer look at the results for un-
known words in segmentation-based tagging shows
that 59.68% of the tagging errors are direct results
from incorrect segmentation decisions. In compari-
son, for known words, only 6.24% of the incorrectly
tagged words are also ill-segmented. This means
that even though the quality of the segmenter is very
high, the errors still harm the POS tagging step.
To make our results more comparable to those by
Habash and Rambow (2005), we converted the test
set with the POS tags from the whole word tagger
to their tokenization and to a reduced tagset of 15
tags. In this setting, we reach a tokenization ac-
curacy of 99.36% and a POS tagging accuracy of
96.41%. This is very close to the results by Habash
and Rambow so that we conclude that high accu-
racy POS tagging for Arabic is possible without a
full morphological analysis.
4 Conclusions and Future Work
We have presented a method for POS tagging for
Arabic that does not assume gold segmentation,
which would be unrealistic for naturally occurring
Arabic. The approach we developed is competi-
tive although it uses the full POS tagset, without
any previous morphological analysis. The results
of our experiments suggest that segmentation is not
required for POS tagging. On the contrary, using
whole words as basis for POS tagging yields higher
accuracy, thus rendering a full morphological anal-
ysis or segmentation unnecessary. We reached the
best results in tagging whole words both for known
words and unknown words. These results were only
marginally worse that the results obtained by the ex-
periment based on gold segmentation.
The weakness of the segmentation-based ap-
proach is its low accuracy on unknown words. In the
future, we will investigate knowledge-richer meth-
ods for segmentation. In particular, we will inves-
tigate whether an automatic vocalization step previ-
ous to segmentation will improve POS tagging ac-
curacy for unknown words.
References
Shahib AlGahtani, William Black, and John Mc-
Naught. 2009. Arabic part-of-speech-tagging using
transformation-based learning. In Proceeedings of the
2nd International Conference on Arabic Language Re-
sources and Tools, Cairo, Egypt.
Ann Bies and Mohamed Maamouri. 2003. Penn Arabic
Treebank guidelines. Technical report, LDC, Univer-
sity of Pennsylvania.
Walter Daelemans and Antal van den Bosch. 2005.
Memory Based Language Processing. Cambridge
University Press.
Walter Daelemans, Jakub Zavrel, Peter Berck, and Steven
Gillis. 1996. MBT: A memory-based part of speech
tagger-generator. In Eva Ejerhed and Ido Dagan, ed-
itors, Proceedings of the 4th Workshop on Very Large
Corpora, pages 14?27, Copenhagen, Denmark.
Mona Diab, Kadri Hacioglu, and Daniel Jurafsky. 2004.
Automatic tagging of Arabic text: From raw text to
base phrase chunks. In Proceedings of HLT-NAACL,
Boston, MA.
Nizar Habash and Owen Rambow. 2005. Arabic tok-
enization, part-of-speech tagging and morphological
disambiguation in one fell swoop. In Proceedings of
ACL-2005, pages 573?580, Ann Arbor, MI.
Emad Mohamed and Sandra Ku?bler. 2010. Arabic part
of speech tagging. In Proceedings of LREC, Valetta,
Malta.
Antal van den Bosch, Erwin Marsi, and Abdelhadi Soudi.
2007. Memory-based morphological analysis and
part-of-speech tagging of Arabic. In Abdelhadi Soudi,
Antal van den Bosch, and Gu?nter Neumann, editors,
Arabic Computational Morphology. Springer.
708
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 176?180,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Transforming Standard Arabic to Colloquial Arabic 
 
 Emad Mohamed, Behrang Mohit and Kemal Oflazer 
 
Carnegie Mellon University - Qatar 
Doha, Qatar 
emohamed@qatar.cmu.edu, behrang@cmu.edu, ko@cs.cmu.edu  
 
 
Abstract 
We present a method for generating Colloquial 
Egyptian Arabic (CEA) from morphologically dis-
ambiguated Modern Standard Arabic (MSA). 
When used in POS tagging, this process improves 
the accuracy from 73.24% to 86.84% on unseen 
CEA text, and reduces the percentage of out-of-
vocabulary words from 28.98% to 16.66%. The 
process holds promise for any NLP task targeting 
the dialectal varieties of Arabic; e.g., this approach 
may provide a cheap way to leverage MSA data 
and morphological resources to create resources 
for colloquial Arabic to English machine transla-
tion. It can also considerably speed up the annota-
tion of Arabic dialects. 
 
1. Introduction 
Most of the research on Arabic is focused on Mod-
ern Standard Arabic. Dialectal varieties have not 
received much attention due to the lack of dialectal 
tools and annotated texts (Duh and Kirchoff, 
2005). In this paper, we present a rule-based me-
thod to generate Colloquial Egyptian Arabic (CEA) 
from Modern Standard Arabic (MSA), relying on 
segment-based part-of-speech tags. The transfor-
mation process relies on the observation that di-
alectal varieties of Arabic differ mainly in the use 
of affixes and function words while the word stem 
mostly remains unchanged. For example, given the 
Buckwalter-encoded MSA sentence ?AlAxwAn 
Almslmwn lm yfwzwA fy AlAntxbAt? the rules pro-
duce ?AlAxwAn Almslmyn mfAzw$ f AlAntxAbAt? 
(????????? ? ?????? ???????? ??????, The Muslim Bro-
therhood did not win the elections). The availabili-
ty of segment-based part-of-speech tags is essential 
since many of the affixes in MSA are ambiguous. 
For example, lm could be either a negative particle 
or a question work, and the word AlAxwAn could 
be either made of two segments (Al+<xwAn, the 
brothers), or three segments (Al+>xw+An, the two 
brothers). 
    We first introduce the transformation rules, and 
show that in many cases it is feasible to transform 
MSA to CEA, although there are cases that require 
much more than POS tags.  We then provide a typ-
ical case in which we utilize the transformed text 
of the Arabic Treebank (Bies and Maamouri, 2003) 
to build a part-of-speech tagger for CEA. The tag-
ger improves the accuracy of POS tagging on au-
thentic Egyptian Arabic by 13% absolute (from 
73.24% to 86.84%) and reduces the percentage of 
out-of-vocabulary words from 28.98% to 16.66%. 
  
  2. MSA to CEA Conversion Rules 
Table 1 shows a sentence in MSA and its CEA 
counterpart. Both can be translated into: ?We did 
not write it for them.? MSA has three words while 
CEA is more synthetic as the preposition and the 
negative particle turn into clitics.  Table 1 illu-
strates the end product of one of the Imperfect 
transformation rules, namely the case where the 
Imperfect Verb is preceded by the negative particle 
lm. 
 
 Arabic Buckwalter 
MSA ??? ?????? ?? lm nktbhA lhn 
CEA ?????????? mktbnhlhm$ 
English We did not write it for them 
Table 1: a sentence in MSA and CEA 
 
Our 103 rules cover nominals (number and case 
affixes), verbs (tense, number, gender, and modali-
ty), pronouns (number and gender), and demon-
strative pronouns (number and gender).  
    The rules also cover certain lexical items as 400 
words in MSA have been converted to their com-
176
mon CEA counterparts.  Examples of lexical con-
versions include ZlAm and Dlmp (darkness), rjl 
and rAjl (man), rjAl and rjAlp (men), and kvyr and 
ktyr (many), where the first word is the MSA ver-
sion and the second is the CEA version.  
   Many of the lexical mappings are ambiguous. 
For example, the word rjl can either mean man or 
leg. When it means man, the CEA form is rAjl, but 
the word for leg is the same in both MSA and 
CEA. While they have different vowel patterns 
(rajul and rijol respectively), the vowel informa-
tion is harder to get correctly than POS tags. The 
problem may arise especially when dealing with 
raw data for which we need to provide POS tags 
(and vowels) so we may be able to convert it to the 
colloquial form. Below, we provide two sample 
rules:  
The imperfect verb is used, inter alia, to express 
the negated past, for which CEA uses the perfect 
verb. What makes things more complicated is that 
CEA treats negative particles and prepositional 
phrases as clitics. An example of this is the word 
mktbthlhm$ (I did not write it for them) in Table 1 
above. It is made of the negative particle m, the 
stem ktb (to write), the object pronoun h, the pre-
position l, the pronoun hm (them) and the negative 
particle $. Figure 1, and the following steps show 
the conversions of lm nktbhA lhm to 
mktbnhAlhm$: 
1. Replace the negative word lm with one of 
the prefixes m, mA or the word mA. 
2. Replace the Imperfect Verb prefix with its 
Perfect Verb suffix counterpart.  For exam-
ple, the IV first person singular subject pre-
fix > turns into t in the PV. 
3. If the verb is followed by a prepositional 
phrase headed by the preposition l that con-
tains a pronominal object, convert the pre-
position to a prepositional clitic. 
4. Transform the dual to plural and the plural 
feminine to plural masculine. 
5. Add the negative suffix $ (or the variant $y, 
which is less probable) 
As alluded to in 1) above, given that colloquial 
orthography is not standardized, many affixes and 
clitics can be written in different ways. For exam-
ple, the word mktbnhlhm$, can be written in 24 
ways. All these forms are legal and possible, as 
attested by their existence in a CEA corpus (the 
Arabic Online Commentary Dataset v1.1), which 
we also use for building a language model later. 
Figure 1: One negated IV form in MSA can generate 24 
(3x2x2x2) possible forms in CEA 
 
MSA possessive pronouns inflect for gender, num-
ber (singular, dual, and plural), and person. In 
CEA, there is no distinction between the dual and 
the plural, and a single pronoun is used for the 
plural feminine and masculine. The three MSA 
forms ktAbhm, ktAbhmA and ktAbhn (their book 
for the masculine plural, the dual, and the feminine 
plural respectively) all collapse to ktAbhm.  
 
Table 2 has examples of some other rules we have 
applied.  We note that the stem, in bold, hardly 
changes, and that the changes mainly affect func-
tion segments. The last example is a lexical rule in 
which the stem has to change. 
 
Rule MSA CEA 
Future swf  yktb Hyktb/hyktb 
Future_NEG ln >ktb m$ hktb/ m$ Hktb 
IV yktbwn byktbw/ bktbw/ bktbwA 
Passive ktb Anktb/ Atktb 
NEG_PREP lys mnhn mmnhm$ 
Lexical trkhmA sAbhm 
Table 2: Examples of Conversion Rules. 
 
3. POS Tagging Egyptian Arabic 
We use the conversion above to build a POS tagger 
for Egyptian Arabic. We follow Mohamed and 
Kuebler (2010) in using whole word tagging, i.e., 
without any word segmentation. We use the Co-
lumbia Arabic Treebank 6-tag tag set: PRT (Par-
ticle), NOM (Nouns, Adjectives, and Adverbs), 
PROP (Proper Nouns), VRB (Verb), VRB-pass 
(Passive Verb), and PNX (Punctuation) (Habash 
and Roth, 2009). For example, the word 
wHnktblhm (and we will write to them, ?????????) 
receives the tag PRT+PRT+VRB+PRT+NOM. 
This results in 58 composite tags, 9 of which occur 
5 times or less in the converted ECA training set. 
177
    We converted two sections of the Arabic Tree-
bank (ATB): p2v3 and p3v2. For all the POS tag-
ging experiments, we use the memory-based POS 
tagger (MBT) (Daelemans et al, 1996) The best 
results, tuned on a dev set,  were obtained, in non-
exhaustive search,  with the Modified Value Dif-
ference Metric as a distance metric and with k  (the 
number of nearest neighbors) = 25. For known 
words, we use the IGTree algorithm and 2 words to 
the left, their POS tags, the focus word and its list 
of possible tags, 1 right context word and its list of 
possible tags as features. For unknown words, we 
use the IB1 algorithm and the word itself, its first 5 
and last 3 characters, 1 left context word and its 
POS tag, and 1 right context word and its list of 
possible tags as features. 
     
3.1. Development and Test Data 
As a development set, we use 100 user-contributed 
comments (2757 words) from the website ma-
srawy.com, which were judged to be highly collo-
quial. The test set contains 192 comments (7092 
words) from the same website with the same crite-
rion. The development and test sets were hand-
annotated with composite tags as illustrated above 
by two native Arabic-speaking students. 
The test and development sets contained spel-
ling errors (mostly run-on words). The most com-
mon of these is the vocative particle yA, which is 
usually attached to following word (e.g. yArAjl, 
(you man, ??????)). It is not clear whether it should 
be treated as a proclitic, since it also occurs as a 
separate word, which is the standard way of writ-
ing. The same holds true for the variation between 
the letters * and z, (? and ? in Arabic) which are 
pronounced exactly the same way in CEA to the 
extent that the substitution may not be considered a 
spelling error. 
 
3.2. Experiments and Results 
We ran five experiments to test the effect of MSA 
to CEA conversion on POS tagging: (a) Standard, 
where we train the tagger on the ATB MSA data, 
(b) 3-gram LM, where for each MSA sentence we 
generate all transformed sentences (see Section 2.1 
and Figure 1) and pick the most probable sentence 
according to a trigram language model built from 
an 11.5 million words of user contributed 
comments.1 This corpus is highly dialectal 
                                                 
1Available from  http://www.cs.jhu.edu/~ozaidan/AOC 
Egyptian Arabic, but like all similar collections, it 
is diglossic and demonstrates a high degree of 
code-switching between MSA and CEA. We use 
the SRILM toolkit (Stolcke, 2002) for language 
modeling and sentence scoring, (c) Random, 
where we choose a random sentence from all the 
correct sentences generated for each MSA 
sentence, (d) Hybrid, where we combine the data 
in a) with the best settings (as measured on the dev 
set) using the converted colloquial data (namely 
experiment c). Hybridization is necessary since 
most Arabic data in blogs and comments are a mix 
of MSA and CEA, and (e) Hybrid + dev, where 
we enrich the Hybrid training set with the dev data.  
  We use the following metrics for evaluation: 
KWA: Known Word Accuracy (%), UWA: 
Unknown Word Accuracy (%), TA: Total Accuracy 
(%), and UW: unknown words (%) in the 
respective set in the respective experiment. Table 
3(a) presents the results on the development set 
while Table 3(b) the results on the test set.  
 
Experiment KWA UWA TA UW 
(a) Standard 92.75 39.68 75.77 31.99 
(b) 3-gram LM 89.12 43.46 76.21 28.29 
(c) Random 92.36 43.51 79.25 26.84 
(d) Hybrid 94.13 52.22 84.87 22.09 
Table 3(a): POS results on the development set.   
 
We notice that randomly selecting a sentence from 
the correct generated sentences yields better results 
than choosing the most probable sentence accord-
ing to a language model. The reason for this may 
be that randomization guarantees more coverage of 
the various forms. We have found that the vocabu-
lary size (the number of unique word types) for the 
training set generated for the Random experiment 
is considerably larger than the vocabulary size for 
the 3-gram LM experiment (55367 unique word 
types in Random versus 51306 in 3-gram LM), 
which results in a drop of 4.6% absolute in the per-
centage of unknown words: 27.31% versus 
22.30%). This drop in the percentage of unknown 
words may indicate that generating all possible 
variations of CEA may be more useful than using a 
language model in general. Even in a CEA corpus 
of 35 million words, one third of the words gener-
ated by the rules are not in the corpus, while many 
178
of these are in both the test set and the develop-
ment set. 
 
Experiment KWA UWA TA UW 
(a) Standard 89.03 40.67 73.24 28.98 
(b) 3-gram LM 84.33 47.70 74.32 27.31 
(c) Random 90.24 48.90 79.67 22.70 
(d) Hybrid 92.22 53.92 83.81 19.45 
(e) Hybrid+dev 94.87 56.46 86.84 16.66 
Table 3(b): POS results on the test set 
 
    We also notice that the conversion alone im-
proves tagging accuracy from 75.77% to 79.25% 
on the development set, and from 73.24% to 
79.67% on the test set. Combining the original 
MSA and the best scoring converted data (Ran-
dom) raises the accuracies to 84.87% and 83.81% 
respectively.  The percentage of unknown words 
drops from 29.98% to 19.45% in the test set when 
we used the hybrid data. The fact that the percen-
tage of unknown words drops further to 16.66% in 
the Hybrid+dev experiment points out the authen-
tic colloquial data contains elements that have not 
been captured using conversion alone.    
 
4. Related Work 
To the best of our knowledge, ours is the first work 
that generates CEA automatically from morpholog-
ically disambiguated MSA, but Habash et al 
(2005) discussed root and pattern morphological 
analysis and generation of Arabic dialects within 
the MAGED morphological analyzer. MAGED 
incorporates the morphology, phonology, and or-
thography of several Arabic dialects. Diab et al 
(2010) worked on the annotation of dialectal Arab-
ic through the COLABA project, and they used the 
(manually) annotated resources to facilitate the 
incorporation of the dialects in Arabic information 
retrieval. 
  Duh and Kirchhoff (2005) successfully designed 
a POS tagger for CEA that used an MSA morpho-
logical analyzer and information gleaned from the 
intersection of several Arabic dialects.  This is dif-
ferent from our approach for which POS tagging is 
only an application.  Our focus is to use any exist-
ing MSA data to generate colloquial Arabic re-
sources that can be used in virtually any NLP task. 
   At a higher level, our work resembles that of 
Kundu and Roth (2011), in which they chose to 
adapt the text rather than the model. While they 
adapted the test set, we do so at the training set 
level. 
 
5. Conclusions and Future Work 
We have a presented a method to convert Modern 
Standard Arabic to Egyptian Colloquial Arabic 
with an example application to the POS tagging 
task. This approach may provide a cheap way to 
leverage MSA data and morphological resources to 
create resources for colloquial Arabic to English 
machine translation, for example. 
     While the rules of conversion were mainly 
morphological in nature, they have proved useful 
in handling colloquial data. However, morphology 
alone is not enough for handling key points of dif-
ference between CEA and MSA. While CEA is 
mainly an SVO language, MSA is mainly VSO, 
and while demonstratives are pre-nominal in MSA, 
they are post-nominal in CEA. These phenomena 
can be handled only through syntactic conversion.  
We expect that converting a dependency-based 
treebank to CEA can account for many of the phe-
nomena part-of-speech tags alone cannot handle 
  We are planning to extend the rules to other lin-
guistic phenomena and dialects, with possible ap-
plications to various NLP tasks for which MSA 
annotated data exist. When no gold standard seg-
ment-based POS tags are available, tools that pro-
duce segment-based annotation can be used, e.g.   
segment-based POS tagging (Mohamed and Kueb-
ler, 2010) or MADA (Habash et al 2009), although 
these are not expected to yield the same results as 
gold standard part-of-speech tags. 
 
Acknowledgements  
This publication was made possible by a NPRP 
grant (NPRP 09-1140-1-177) from the Qatar Na-
tional Research Fund (a member of The Qatar 
Foundation). The statements made herein are sole-
ly the responsibility of the authors.  
    We thank the two native speaker annotators and 
the anonymous reviewers for their instructive and 
enriching feedback. 
 
 
  
179
 References 
    Bies, Ann and Maamouri, Mohamed  (2003). Penn 
Arabic Treebank guidelines. Technical report, LDC, 
University of Pennsylvania. 
    Buckwalter, T. (2002). Arabic Morphological Analyz-
er (AraMorph). Version 1.0. Linguistic Data Consor-
tium, catalog number LDC2002L49 and ISBN 1-58563-
257- 0 
    Daelemans, Walter and van den Bosch, Antal ( 2005). 
Memory Based Language Processing. Cambridge Uni-
versity Press.   
    Daelemans, Walter; Zavrel, Jakub; Berck, Peter, and 
Steven Gillis (1996). MBT: A memory-based part of 
speech tagger-generator. In Eva Ejerhed and Ido Dagan, 
editors, Proceedings of the 4th Workshop on Very Large 
Corpora, pages 14?27, Copenhagen, Denmark. 
   Diab, Mona; Habash, Nizar; Rambow, Owen; Altan-
tawy, Mohamed, and Benajiba, Yassine. COLABA: 
Arabic Dialect Annotation and Processing. LREC 2010. 
    Duh, K. and Kirchhoff, K. (2005). POS Tagging of 
Dialectal Arabic: A Minimally Supervised Approach. 
Proceedings of the ACL Workshop on Computational 
Approaches to Semitic Languages, Ann Arbor, June 
2005. 
    Habash, Nizar; Rambow, Own and Kiraz, George 
(2005). Morphological analysis and generation for 
Arabic dialects. Proceedings of the ACL Workshop on 
Computational Approaches to Semitic Languages, pages 
17?24, Ann Arbor, June 2005 
    Habash, Nizar and Roth, Ryan. CATiB: The Colum-
bia Arabic Treebank. Proceedings of the ACL-IJCNLP 
2009 Conference Short Papers, pages 221?224, Singa-
pore, 4 August 2009. c 2009 ACL and AFNLP 
    Habash, Nizar, Owen Rambow and Ryan Roth. MA-
DA+TOKAN: A Toolkit for Arabic Tokenization, Dia-
critization, Morphological Disambiguation, POS Tag-
ging, Stemming and Lemmatization. In Proceedings of 
the 2nd International Conference on Arabic Language 
Resources and Tools (MEDAR), Cairo, Egypt, 2009 
   Kundu, Gourab abd Roth, Don (2011). Adapting Text 
instead of the Model: An Open Domain Approach. Pro-
ceedings of the Fifteenth Conference on Computational 
Natural Language Learning, pages 229?237,Portland, 
Oregon, USA, 23?24 June 2011 
    Mohamed, Emad. and Kuebler, Sandra (2010). Is 
Arabic Part of Speech Tagging Feasible Without Word 
Segmentation? Proceedings of HLT-NAACL 2010, Los 
Angeles, CA. 
    Stolcke, A. (2002). SRILM - an extensible language 
modeling toolkit. In Proc. of ICSLP, Denver, Colorado 
180
Proceedings of the Fifteenth Conference on Computational Natural Language Learning, pages 10?18,
Portland, Oregon, USA, 23?24 June 2011. c?2011 Association for Computational Linguistics
The Effect of Automatic Tokenization, Vocalization, Stemming, and POS 
Tagging on Arabic Dependency Parsing 
 
Emad Mohamed 
Suez Canal University 
Suez, Egypt 
emohamed@umail.iu.edu 
 
 
 
Abstract 
We use an automatic pipeline of word 
tokenization, stemming, POS tagging, and 
vocalization to perform real-world Arabic 
dependency parsing. In spite of the high 
accuracy on the modules, the very few errors in 
tokenization, which reaches an accuracy of 
99.34%, lead to a drop of more than 10% in 
parsing, indicating that no high quality 
dependency parsing of Arabic, and possibly 
other morphologically rich languages, can be 
reached without (semi-)perfect tokenization. The 
other module components, stemming, 
vocalization, and part of speech tagging, do not 
have the same profound effect on the 
dependency parsing process. 
 
1. Introduction 
Arabic is a morphologically rich language in which 
words may be composed of several tokens and 
hold several syntactic relations. We define word to 
be a whitespace delimited unit and token to be 
(part of) a word that has a syntactic function. For 
example, the word wsytzwjhA  (?????????)(English: 
And he will marry her)  consists of 4 tokens: a 
conjunction w, a future marker s, a verb inflected 
for the singular masculine in the perfective form 
ytzwj, and a feminine singular 3rd person object 
pronoun.  Parsing such a word requires 
tokenization, and performing dependency parsing 
in the tradition of the CoNLL-X (Buchholz and 
Marsi, 2006) and CoNLL 2007 shared task (Nivre 
et al 2007) also requires part of speech tagging, 
lemmatization, linguistic features, and 
vocalization, all of which were in the human 
annotated gold standard form in the shared task. 
The current study aims at measuring the 
effect of a pipeline of non gold standard 
tokenization, lemmatization, vocalization, 
linguistic features and POS tagging on the quality 
of Arabic dependency parsing. We only assume 
that we have gold standard sentence boundaries 
since we do not agree with the sentence boundaries 
in the data, and introducing our own will have a 
complicating effect on evaluation. The CoNLL 
shared tasks of 2006 and 2007 used gold standard 
components in all fields, which is not realistic for 
Arabic, or for any other language. For Arabic and 
other morphologically rich languages, it may be 
more unrealistic than it is for English, for example, 
since the CoNLL 2007 Arabic dataset has tokens, 
rather than white space delimited words, as entries. 
A single word may have more than one 
syntactically functional token. Dependency parsing 
has been selected in belief that it is more suitable 
for Arabic than constituent-based parsing. All 
grammatical relations in Arabic are binary 
asymmetrical relations that exist between the 
tokens of a sentence. According to Jonathan 
Owens (1997: 52): ?In general the Arabic notion of 
dependency and that defined in certain modern 
versions e.g. Tesniere (1959) rest on common 
principles?. 
With a tokenization accuracy of 99.34%, a 
POS tagging accuracy of 96.39%, and with the 
absence of linguistic features and the use of word 
stems instead of lemmas, the Labeled Attachment 
Score drops from 74.75% in the gold standard 
experiment to 63.10% in the completely automatic 
experiment. Most errors are a direct result of 
tokenization errors, which indicates that despite the 
high accuracy on tokenization, it is still not enough 
to produce satisfactory parsing numbers. 
2. Related Studies 
The bulk of literature on Arabic Dependency 
Parsing stems from the two CoNLL shared tasks of 
2006 and 2007. In CoNLL-X (Buchholz and 
10
Marsi, 2006), the average Labeled Attachment 
Score on Arabic across all results presented by the 
19 participating teams was 59.9% with a standard 
deviation of 6.5. The best results were obtained by 
McDonald et al(2006) with a score of 66.9% 
followed by Nivre et al(2006) with 66.7%. 
The best results on Arabic in the CoNLL 2007 
shared task were obtained by Hall et al(2007) as 
they obtained a Labeled Attachment Score of 
76.52%, 9.6 percentage points above the highest 
score of the 2006 shared task. Hall et alused an 
ensemble system, based on the MaltParser 
dependency parser that extrapolates from a single 
MaltParser system. The settings with the Single 
MaltParser led to a Labeled Accuracy Score of 
74.75% on Arabic. The Single MaltParser is the 
one used in the current paper. All the papers in 
both shared tasks used gold standard tokenization, 
vocalization, lemmatization, POS tags, and 
linguistic features. 
A more recent study is that by Marton et al
(2010). Although Marton et alvaried the POS 
distribution and linguistic features, they still used 
gold standard tokenization. They also used the 
Columbia Arabic Treebank, which makes both the 
methods and data different from those presented 
here.  
3. Data, Methods, and Evaluation 
3.1.Data 
The data used for the current study is the same data 
set used for the CoNLL (2007) shared task, with 
the same division into training set, and test set. 
This design helps in comparing results in a way 
that enables us to measure the effect of automatic 
pre-processing on parsing accuracy. The data is in 
the CoNLL column format. In this format, each 
token is represented through columns each of 
which has some specific information. The first 
column is the ID, the second the token, the third 
the lemma, the fourth the coarse-grained POS tag, 
the fifth the POS tag, and the sixth column is a list 
of linguistic features. The last two columns of the 
vector include the head of the token and the 
dependency relation between the token and its 
head. Linguistic features are an unordered set of 
syntactic and/or morphological features, separated 
by a vertical bar (|), or an underscore if not 
available. The features in the CoNLL 2007 Arabic 
dataset represent case, mood, definiteness, voice, 
number, gender and person. 
The data used for training the 
stemmer/tokenizer is taken from the Arabic 
Treebank (Maamouri and Bies, 2004). Care has 
been taken not to use the parts of the ATB that are 
also used in the Prague Arabic Dependency 
Treebank (Haijc et al2004) since the PADT and 
the ATB share material. 
 
3.2. Methods 
We implement a pipeline as follows 
 
(1) We build a memory-based word segmenter 
using TIMBL (Daelemans et al 2007) 
which treats segmentation as a per letter 
classification in which each word segment 
is delimited by a + sign whether it is 
syntactic or inflectional. A set of hand-
written rules then produces tokens and 
stems based on this. Tokens are 
syntactically functional units, and stems 
are the tokens without the inflectional 
segments, For example, the word 
wsytzwjhA above is segmented as 
w+s+y+tzwj+hA. The tokenizer splits this 
into four tokens w, s, ytzwj, and hA, and 
the stemmer strips the inflectional prefix 
from ytzwj to produce tzwj. In the 
segmentation experiments, the best results 
were obtained with the IB1 algorithm with 
similarity computed as weighted overlap, 
relevance weights computed with gain 
ratio, and the number of k nearest distances 
equal to 1. 
(2)  The tokens are passed to the part of 
speech tagger. We use the Memory-based 
Tagger, MBT, (Daelemans et al 2007). 
The MBT features for known words 
include the two context words to the left 
along with their disambiguated POS tags, 
the focus word itself, and one word to the 
right along with its ambitag (the set of all 
possible tags it can take). For unknown 
words, the features include the first five 
letters and the last three letters of the word, 
the, the left context tag, the right context 
11
ambitag, one word to the left, the focus 
word itself, one ambitag to the right, and 
one word to the right. 
(3) The column containing the linguistic 
features in the real world dependency 
experiment will have to remain vacant due 
to the fact that it is hard to produce these 
features automatically given only naturally 
occurring text. 
(4)  The dependency parser (MaltParser 1.3.1) 
takes all the information above and 
produces the data with head and 
dependency annotations. 
 
Although the purpose of this experiment is to 
perform dependency parsing of Arabic without any 
assumptions, one assumption we cannot avoid is 
that the input text should be divided into sentences. 
For this purpose, we use the gold standard division 
of text into sentences without trying to detect the 
sentence boundaries, although this would be 
necessary in actual real-world use of dependency 
parsing. The reason for this is that it is not clear 
how sentence boundaries are marked in the data as 
there are sentences whose length exceeds 300 
tokens. If we detected the boundaries 
automatically, then we would face the problem of 
aligning our sentences with those of the test set for 
evaluation, and many of the dependencies would 
not still hold. 
In the parsing experiments below, we will 
use the dependency parser MaltParser (Nivre et al, 
2006). We will use Single MaltParser, as used by 
Hall et al(2007), with the same settings for Arabic 
that were used in the CoNLL 2007 shared task on 
the same data to be as close as possible to the 
original results in order to be able to compare the 
effect of non gold standard elements in the parsing 
process. 
 
3.3.Evaluation 
The official evaluation metric in the CoNLL 2007 
shared task on dependency parsing was the labeled 
attachment score (LAS), i.e., the percentage of 
tokens for which a system has predicted the correct 
HEAD and DEPREL, but results reported also 
included unlabeled attachment score (UAS), i.e., 
the percentage of tokens with correct HEAD, and 
the label accuracy (LA), i.e., the percentage of 
tokens with correct DEPREL. We will use the 
same metrics here. 
One major difference between the parsing 
experiments which were performed in the 2007 
shared task and the ones performed here is 
vocalization. The data set which was used in the 
shared task was completely vocalized with both 
word-internal short vowels and case markings. 
Since vocalization in such a perfect form is almost 
impossible to produce automatically, we have 
decided to primarily use unvocalized data instead. 
We have removed the word internal short vowels 
as well as the case markings from both the training 
set and the test set. This has the advantage of 
representing naturally occurring Arabic more 
closely, and the disadvantage of losing information 
that is only available through vocalization. We 
will, however, report on the effect of vocalization 
on dependency parsing in the discussion. 
To give an estimate of the effects 
vocalization has on dependency parsing, we have 
replicated the original task with the vocalized data, 
and then re-run the experiment with the 
unvocalized version. Table 1 presents the results: 
 
 Vocalized Unvocalized 
LAS 74.77% 74.16% 
UAS 84.09% 83.53% 
LA 85.68% 85.44% 
 
Table 1: Vocalized versus unvocalized dependency 
parsing 
 
The results of the experiment indicate that 
vocalization has a positive effect on the quality of 
the parsing output, which may be due to the fact 
that ambiguity decreases with vocalization. 
Labeled attachment score drops from 74.77% on 
the vocalized data to 74.16% on unvocalized data. 
Unlabeled attachment score drops from 84.09% to 
83.53% and labeled accuracy score from 85.68% 
to 85.44%. The difference is minimal, and is 
expected to be even smaller with automatic 
vocalization 
 
4. Results and discussion 
4.1.Tokenization 
We obtain an accuracy of 99.34%. Out of the 4550 
words which the test set comprises, there are only 
30 errors affecting 21 out of the 132 sentences in 
the test set. 17 of the errors can be characterized as 
over-tokenization while the other 13 are under- 
12
tokenization. 13 of the over- tokenization cases are 
different tokens of the word blywn (Eng. billion) as 
the initial b in the words was treated as a 
preposition while it is an original part of the word. 
A closer examination of the errors in the 
tokenization process reveals that most of the words 
which are incorrectly tokenized do not occur in the 
training set, or occur there only in the form 
produced by the tokenizer. For example, the word 
blywn does not occur in the training set, but the 
form b+lywn+p occurs in the training set, and this 
is the reason the word is tokenized erroneously. 
Another example is the word bAsm, which is 
ambiguous between a one-token word bAsm (Eng. 
smiling), and a two-token word, b+Asm (Eng. in 
the name of). Although the word should be 
tokenized as b+Asm, the word occurs in the 
training set as bAsm, which is a personal name. 
In fact, only five words in the 30 mis-
tokenized words are available in the training set, 
which means that the tokenizer has a very high 
accuracy on known words. There are yet two 
examples that are worthy of discussion. The first 
one involves suboptimal orthography. The word 
r>smAl (Eng. capital in the financial sense) is in 
the training set but is nonetheless incorrectly 
tokenized in our experiments because it is written 
as brAsmAl (with the preposition b) but with an alif 
instead of the hamza. The word was thus not 
tokenized correctly. The other example involves an 
error in the tokenization in the Prague Arabic 
Dependency Treebank. The word >wjh (Eng. I 
give/address) has been tokenized in the Prague 
Arabic dependency treebank as >wj+h (Eng. its 
utmost/prime), which is not the correct 
tokenization in this context as the h is part of the 
word and is not a different token. The classifier did 
nonetheless tokenize it correctly but it was counted 
as wrong in the evaluation since it does not agree 
with the PADT gold standard.  
 
4.2.Stemming 
Since stemming involves removing all the 
inflectional prefixes and suffixes from the words, 
and since inflectional affixes are not demarcated in 
the PADT data set used in the CoNLL shared 
tasks, there is no way to know the exact accuracy 
of the stemming process in that specific 
experiment, but since stemming is a by-product of 
segmentation, and since segmentation in general 
reaches an accuracy in excess of 98%, stemming 
should be trusted as an accurate process. 
 
4.3.Part of speech tagging 
The performance of the tagger on gold standard 
data with gold standard tokenization is shown in 
table 2. The experiment yields an accuracy of 
96.39% on all tokens. Known tokens reach an 
accuracy of 97.49% while unknown tokens reach 
an accuracy of 81.48%. These numbers constitute 
the ceiling for accuracy since the real-world 
experiment makes use of automatic tokenization, 
which definitely leads to lower numbers. 
 
Unknown Known Total 
81.48% 97.49% 96.39% 
 
Table 2: Part of speech tagging on gold standard 
tokenization 
 
When we run the experiment using 
automatic tokenization we obtain an accuracy of 
95.70% which is less than 1% lower than the gold 
standard accuracy. This indicates that part of 
speech tagging has been affected by tokenization 
quality. The drop in quality in part of speech 
tagging is almost identical to the drop in quality in 
tokenization. 
While some of the errors made by the part 
of speech tagger are due to the fact that nouns, 
adjectives, and proper nouns cannot be 
distinguished by any formal features, a large 
number of the nominal class annotation in the gold 
standard data can hardly be justified. For example, 
the expression  ??????? ???????? (Eng. the European 
Union) is annotated once in the training data as 
proper noun and adjective, and another time as a 
noun and adjective. A similar confusion holds for 
the names of the months and the weekdays, which 
are sometimes tagged as nouns and sometimes as 
proper nouns.  
 
4.4. Dependency parsing 
Now that we have stems, tokens, and part of 
speech tags, we can proceed with the parsing 
experiment, the final step and the ultimate goal of 
the preprocessing modules we have introduced so 
far. In order to prepare the training data, we have 
replaced the lemmas in the training and testing sets 
with the stems since we do not have access to 
lemmas in real-world experiments. While this 
13
introduces an automatic element in the training set, 
it guarantees the similarity between the features in 
the training set and those in the test set. 
In order to discover whether the fine-
grained POS tagset is necessary, we have run two 
parsing experiments using gold standard parts of 
speech with stems instead of lemmas, but without 
any of the linguistic features included in the gold 
standard: the first experiment has the two distinct 
part of speech tags and the other one has only the 
coarse-grained part of speech tags. Table 3 outlines 
the results. 
 
 LAS UAS LA 
CPOS+POS 72.54% 82.92% 84.04% 
CPOS 73.11% 83.31% 84.39% 
CoNLL2007 74.75% 84.21% 84.21% 
 
Table 3: effect of fine-grained POS 
 
As can be seen from table 3, using two part 
of speech tagsets harms the performance of the 
dependency parser. While the one-tag dependency 
parser obtains a Labeled Accuracy Score of 
73.11%, the number goes down to 72.54% when 
we used the fine-grained part of speech set. In 
Unlabeled Attachment Score, the one tag parser 
achieves an accuracy of 83.31% compared to 
82.92% on two tag parser. The same is also true for 
Label Accuracy Score as the numbers go down 
from 84.39% when using only one tagset compared 
to 84.04% when using two tagsets. This means that 
the fine-grained tagset is not needed to perform 
real world parsing. We have thus decided to use 
the coarse-grained tagset in the two positions of the 
part of speech tags. We can also see that this 
setting produces results that are 1.64% lower than 
those of the Single MaltParser results reported in 
the CoNLL 2007 shared task in terms of Labeled 
Accuracy Score. The difference can be attributed 
to the lack of linguistic features, vocalization, and 
the use of stems instead of lemmas. The LAS of 
73.11% now constitutes the upper bound for real 
world experiments where also parts of speech and 
tokens have to be obtained automatically (since 
vocalization has been removed, linguistic features 
have been removed, and lemmas have been 
replaced with automatic stems). It should be noted 
that our experiments, with the complete set of gold 
standard features, achieve higher results than those 
reported in the CoNLL 2007 shared task: a LAS of 
74.77 (here) versus a LAS of 74.75 (CoNLL, 
2007). This may be attributed to the change of the 
parser since we use the 1.3.1 version whereas the 
parser used in the 2007 shared task was the 0.4 
version. 
Using the settings above, we have run an 
experiment to parse the test set, which is now 
automatic in terms of tokenization, lemmatization, 
and part of speech tags, and in the absence of the 
linguistic features that enrich the gold standard 
training and test sets. Table 4 presents the results 
of this experiment. 
 
 Automatic Gold Standard 
LAS 63.10% 73.11% 
UAS 72.19% 83.31% 
LA 82.61% 84.39% 
 
Table 4: Automatic dependency parsing experiment 
 
The LAS drops more than 10 percentage 
points from 73.11 to 63.10. This considerable drop 
in accuracy is expected since there is a mismatch 
in the tokenization which leads to mismatch in the 
sentences. The 30 errors in tokenization affect 21 
sentences out of a total of 129 in the test set. When 
we evaluate the dependency parsing output on the 
correctly tokenized sentences only, we obtain 
much better results (shown in Table 5). Labeled 
Attachment Score on correctly tokenized sentences 
is 71.56%, Unlabeled Attachment Score 81.91%, 
and Label Accuracy Score is 83.22%. This 
indicates that no good quality parsing can be 
obtained if there are problems in the tokenization. 
A drop of a half percent in the quality of 
tokenization causes a drop of ten percentage points 
in the quality of parsing, whereas automatic POS 
tags and stemming, and the lack of linguistic 
features do not cause the same negative effect. 
 
 Correctly-
tokenized 
Sentences 
Incorrectly-
Tokenized 
Sentences 
LAS 71.56% 33.60% 
UAS 81.91% 38.32% 
LA 83.22% 80.49% 
 
Table 5: Dependency parsing Evaluation on Correctly 
vs. Incorrectly Tokenized Sentences 
 
14
While correctly tokenized sentences yield 
results that are not extremely different from those 
using gold standard information, and the drop in 
accuracy in them can be attributed to the 
differences introduced through stemming and 
automatic parts of speech as well as the absence of 
the linguistic features, incorrectly tokenized 
sentences show a completely different picture as 
the Labeled Attachment Score now plummets to 
33.6%, which is 37.96 percentage points below 
that on correctly tokenized sentences. The 
Unlabeled Attachment Score also drops from 
81.91% in correctly tokenized sentences to 38.32% 
on incorrectly tokenized sentences with a 
difference of 43.59 percentage points. 
 
Error Analysis 
Considering the total number of errors, out of the 
5124 tokens in the test set, there are 1425 head 
errors (28%), and 891 dependency errors (17%). In 
addition, there are 8% of the tokens in which both 
the dependency and the head are incorrectly 
assigned by the parser. The POS tag with the 
largest percentage of head errors is the Adverb (D) 
with an error rate of 57%, followed by Preposition 
(P) at 34%, and Conjunctions at 34%. The 
preposition and conjunction errors are common 
among all experiments: those with gold standard 
and those with automatic information. These 
results also show that assigning the correct head is 
more difficult than assigning the correct 
dependency. This is reasonable since some tokens 
will have specific dependency types. Also, while 
there are a limited number of dependency relations, 
the number of potential heads is much larger. 
If we look at the lexicon and examine the 
tokens in which most errors occur, we can see one 
conjunction and five prepositions. The conjunction 
w (Eng. and) tops the list, followed by the 
preposition l (Eng. for, to), followed by the 
preposition fy (Eng. in), then the preposition b 
(Eng. with), then the preposition ElY (Eng. on), 
and finally the preposition mn (Eng. from, of). We 
conclude this section by examining a very short 
sentence in which we can see the effect of 
tokenization on dependency parsing. Table 6 is a 
sentence that has an instance of incorrect 
tokenization. 
 
Arabic  ????????? ????????? ??????????? ???? ????? ????? ??? ????
English The American exceptional aid to Egypt is a billion  dollars 
until March. 
Buckwalter (Gold Standard 
Tokenization) 
AlmsAEdAt Al>mrykyp AlAstvnA}yp l mSr blywn dwlAr 
HtY |*Ar 
Buckwalter (Automatic Tokenization) AlmsAEdAt Al>mrykyp AlAstvnA}yp l mSr b lywn dwlAr 
HtY |*Ar 
 
Table 6: A sentence showing the effect of tokenization 
 
The sentence has 8 words one of which 
comprises two tokens. The word lmSr comprises a 
preposition l, and the proper noun mSr (Eng. 
Egypt). The tokenizer succeeds in splitting the 
word into two tokens, but it fails on the one-token 
word blywn (Eng. billion) and splits it into two 
tokens b and lywn. The word is ambiguous 
between blywn (Eng. one billion) and b+lywn 
(Eng. in the city of Lyon), and since the second 
solution is much more frequent in the training set, 
it is the one incorrectly selected by the tokenizer. 
This tokenization decision leads to an ill-
alignment between the gold standard sentence and 
the automatic one as the gold standard has 8 tokens 
while the automatically produced one has 9. This 
thus affects the POS tagging decisions as blywn, 
which in the gold standard is a NOUN, has been 
now tagged as b/PREPOSITION and 
lywn/PROPER_NOUN. This has also affected the 
assignment of heads and dependency relations. 
While blywn is a predicate dependent on the root 
of the sentence, it has been annotated as two 
tokens: b is a preposition dependent on the subject, 
and lywn is an attribute dependent on b. 
 
Using the Penn Tags 
So far, we have used only the POS tags of the 
PADT, and have not discussed the possibility of 
using the Penn Arabic Treebank. The difference is 
that the PADT tags are basic while the ATB ones 
have detailed representations of inflections. While 
15
the word AlmtHdp is given the tag ADJ in the 
PADT, it is tagged as 
DET+ADJ+FEMININE_SINGULAR_MARKER 
in the ATB. Table 7 shows the effect of using the 
Penn tagset with the gold standard full-featured 
dataset in three different experiments as compared 
with the PADT tagset: 
 
(1) The original Unvocalized Experiment 
with the full set of features and gold 
standard components. The Penn tagset is 
not used in this experiment, and it is 
provided for reference purposes only. 
(2) Unvocalized experiment with Penn tags as 
CPOS tags. In this experiment, the Penn 
tagset is used instead of the coarse grained 
POS tagset, while the fine-grained pos 
tagset remains unchanged. 
(3) Using Penn tags as fine grained POS tags, 
while the CPOS tags remain unchanged. 
(4) Using the Penn POS tags in both 
positions. 
 
In the four experiments, the only features 
that change are the POS and CPOS features. 
 
Experiment LAS UAS 
Unvocalized Original 74.16% 83.53
% 
Using Penn Tags as CPOS 
tags 
74.12% 83.43
% 
Using Penn tags as POS 72.40% 81.79
% 
Using Penn tags in both 
positions 
69.63% 79.33
% 
 
Table 7: Using the ATB tagset with the PADT dataset 
 
As can be seen from Table 7, in all three 
cases the Penn tagset produces lower results than 
the PADT tagset. The reason for this may be that 
the tagset is automatic in both cases, and the 
perfect accuracy of the PADT tags helps the 
classifier embedded in the MaltParser parser to 
choose the correct label and head. The results also 
show that when we use the Penn tagset as the 
CPOS tagset, the results are almost no different 
from the gold standard PADT tagset (74.12% vs. 
74.16%). The fact that the Penn tagset does not  
harm the results encourages the inclusion of the 
Penn tags as CPOS tags in the automatic 
experiments that have been used throughout this 
chapter. The worst results are those obtained by 
using the Penn tags in both positions (POS and 
CPOS). 
Using the Penn tagset with the reduced 
experiments, those without the linguistic features, 
gives a different picture from that in the full 
standard experiments, as detailed in table 8. 
 
 
 
Experiment LAS UAS 
Reduced with both PADT 
tags 
72.54% 82.92
% 
Reduced with Penn tags as 
CPOS 
73.09% 83.16
% 
Reduced with Penn tags as 
CPOS and automatic 
tokenization 
63.11% 72.38
% 
 
Table 8: Including the Penn full tagset in the reduced 
experiments 
 
While the Penn tagset does not help 
improve parsing accuracy with the full-featured 
parsing experiments, it helps with the reduced 
experiments. While the experiment without the 
Penn tags score an LAS of 72.54%, replacing the 
CPOS tags in this experiment with the Penn tagset 
raises the accuracy to 73.09%, with an increase of 
0.55%. This may be due to the fact that the full 
tagset gives more information that helps the parser. 
The increase is not as noticeable in the automatic 
tokenization experiment where the accuracy 
minimally changes from 63.10% to 63.11%. 
 
Effect of Vocalization 
We have stated in the methodology section that we 
use unvocalized data since naturally occurring 
Arabic is hardly vocalized. While this is a 
reasonable approach, it is worth checking the effect 
of vocalization on dependency parsing. Table 9 
presents the results of vocalization effect in three 
experiments: (a) All the gold standard features 
with vocalization. This is the experiment reported 
in the literature on Arabic dependency parsing in 
CoNLL (2007), (b) All the gold standard features 
without the vocalization, (c) All gold standard 
features except for vocalization which is 
automatic, and (d) the automatic experiment with 
automatic vocalization. The vocalizer in the latter 2 
16
experiments is trained on the PADT. The TIMBL 
memory-based learner is used in the experiment. 
The best results are obtained with the IB1 
algorithm with similarity computed as weighted 
overlap,. Relevance weights are computed with 
gain ratio, and the number of k nearest neighbors is 
set to 1. The vocalizer has an accuracy of 93.8% on 
the PADT test set. 
 
Experiment LAS UAS 
Fully Gold Standard 
Vocalized 
74.77% 84.09
% 
Fully Gold Standard 
Unvocalized 
74.16% 83.53
% 
Full-featured with automatic 
vocalization 
74.43% 83.88
% 
Completely automatic (with 
automatic vocalization) 
63.11% 72.19
% 
Completely automatic 
without vocalization 
63.11% 72.38
% 
 
Table 9: Vocalization Effect on Dependency 
Parsing 
 
As can be seen from Table 9, gold 
standard vocalization with gold standard features 
produces the best results (LAS: 74.77%) followed 
by the same settings, but with automatic 
vocalization with a LAS of 74.43%, then 
unvocalized gold standard with a LAS of 74.16%. 
The fact that even automatic vocalization produces 
better results than unvocalized text given the same 
conditions, in spite of a token error rate of 6.2%, 
may be attributed to the ability of vocalization to 
disambiguate text even when it is not perfect. We 
can also notice that the LAS for the Automatic 
experiment is the same whether or not vocalization 
is used. This indicates that vocalization, in spite of 
its imperfections, does  not harm performance, 
although it also does not help the parser. 
Tokenization sets a ceiling for parsing accuracy. 
 
5. Conclusion 
We have presented an experiment in real world 
dependency parsing of Arabic using the same data, 
algorithm and settings used in the CoNLL (2007) 
shared task on dependency parsing. The real world 
experiment included performing tokenization, 
stemming, and part of speech tagging of the data 
before it was passed to MaltParser. 
Tokenization was performed using the 
memory-based segmenter/tokenizer/stemmer and it 
reached an accuracy of 99.34% on the CoNLL 
2007 test set. We performed stemming rather than 
lemmatization due to the many problems and 
difficulties involved in obtaining the lemmas. 
Part of speech tagging scored 96.39% on 
all tokens on gold standard tokenization, but the 
accuracy dropped to 95.70% on automatic tokens. 
We also found that using the coarse grained POS 
tagset alne yielded better results than using it in 
combination with the fine-grained POS tagset. 
The tokens, stems, and CPOS tags were 
then fed into the dependency parser, but the 
linguistic features were not since it was not 
feasible to obtain these automatically. The parser 
yielded a Labeled Accuracy Score of 63.10%, 
more than 10% below the accuracy obtained on 
when all the components are gold standard. The 
main reason behind the accuracy drop is the 
tokenization module, since tokenization is 
responsible for creating the nodes that carry 
syntactic functions. Since this process was not 
perfect, many nodes were wrong, and the right 
heads were missing. When we evaluated the parser 
on correctly tokenized sentences, we obtained a 
Labeled Accuracy Score of 71.56%. On incorrectly 
tokenized sentences, however, the LAS score drops 
to 33.60%. 
We have also found that the full tagset of 
the Penn Arabic Treebank improves parsing results 
minimally in the automatic experiments, but not in 
the gold standard experiments. 
Vocalization does not help in the real 
world experiment unlike in the gold standard one. 
These results show that tokenization is the 
major hindrance to obtaining high quality parsing 
in Arabic. Arabic computational linguistics should 
thus focus on ways to perfect tokenization, or try to 
find ways to parsing without having to perform 
tokenization. 
 
Acknowledgment 
We would like to thank Joakim Nivre for his 
helpful answers to our questions concerning 
MaltParser. 
 
 
 
 
 
17
References 
Buchholz, Sabine and Marsi, Erwin (2006). CoNLL-X 
shared task on multilingual dependency parsing. In 
Proceedings of the 10th Conference on Computational 
Natural Language Learning (CoNLL), pages 149?164. 
 
Daelemans, Walter;  Zavrel, Jakub; van der Sloot, Ko 
and van den Bosch, Antal (2007). TiMBL: Tilburg 
memory based learner ? version 6.1 ? reference guide. 
Technical Report ILK 07-09, Induction of Linguistic 
Knowledge, Computational Linguistics, Tilburg 
University. 
 
Daelemans, Walter,; Zavrel, Jakub; an den Bosch, 
Antal, and van der Sloot, Ko (2007). MBT: Memory-
Based Tagger- Version 3.1. Reference Guide. Technical 
Report ILK 07-09, Induction of Linguistic Knowledge, 
Computational Linguistics, Tilburg University. 
 
Haji?, Jan; Smr?, Otakar; Zem?nek, Petr; ?naidauf, Jan, 
and Be?ka, Emanuel (2004). Prague Arabic 
Dependency Treebank: Development in Data and Tools. 
In Proceedings of the EMLAR International Conference 
on Arabic Language Resources and Tools, pages 110-
117, Cairo, Egypt, September 2004. 
 
Hall, Johan; Nilsson, Jens; Nivre, Joakim; Eryigit, 
G?lsen; Megyesi, Be?ta; Nilsson, Mattias and Saers, 
Markus (2007). Single Malt or Blended? A Study in 
Multilingual Parser Optimization. In Proceedings of the 
CoNLL Shared Task Session of EMNLP-CoNLL 2007, 
933-939. 
 
Maamouri, Mohamed and Bies, Ann (2004) Developing 
an Arabic Treebank: Methods, Guidelines, Procedures, 
and Tools. In Proceedings of the Workshop on 
Computational Approaches to Arabic Script-based 
Languages, COLING 2004, Geneva, August 28, 2004. 
 
Marton, Yuval; Habash, Nizar; and Rambow, Owen 
(2010). Improving Arabic Dependency Parsing with 
Lexical and Inflectional Morphological Features. In 
Proceddings of The FirstWorkshop on Statistical 
Parsing of Morphologically Rich Languages (SPMRL 
2010), LA, California. 
 
McDonald, Ryan; Lerman, Kevin and Pereira, Fernando 
(2006). Multilingual dependency analysis with a two-
stage discriminative parser. CoNLLX shared task on 
multilingual dependency parsing. In Proceedings of the 
10th Conference on Computational Natural Language 
Learning 
 
Nivre, Joakim; Hall, Jonathan; Nilsson, Jens; Eryigit, 
G?lsen and Marinov, Svetsolav (2006). Labeled 
Pseudo-Projective Dependency Parsing with Support 
Vector Machines. In Proceedings of the Tenth 
Conference on Computational Natural Language 
Learning (CoNLL) 
 
Nivre, Joakim; Hall, Johan; K?bler, Sandra; McDonald, 
Ryan; Nilsson, Jens; Riedel, Sebastian, and Yuret, 
Deniz. (2007). The CoNLL 2007 shared task on 
dependency parsing. In Proceedings of the CoNLL 
Shared Task of EMNLP-CoNLL 2007, pages 915?932 
 
Owen, Jonathan. The Arabic Grammatical Tradition. In 
Hetzron, Robert (ed.) (1997). The Semitic Languages. 
Routledge, London. 
18
