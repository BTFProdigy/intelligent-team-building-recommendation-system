Proceedings of the ACL Student Research Workshop, pages 49?54,
Ann Arbor, Michigan, June 2005. c?2005 Association for Computational Linguistics
Learning Meronyms from Biomedical Text
Angus Roberts
Department of Computer Science, University of Sheffield,
Regent Court, 211 Portobello Street, Sheffield S1 4DP
a.roberts@dcs.shef.ac.uk
Abstract
The part-whole relation is of special im-
portance in biomedicine: structure and
process are organised along partitive axes.
Anatomy, for example, is rich in part-
whole relations. This paper reports pre-
liminary experiments on part-whole ex-
traction from a corpus of anatomy defi-
nitions, using a fully automatic iterative
algorithm to learn simple lexico-syntactic
patterns from multiword terms. The ex-
periments show that meronyms can be ex-
tracted using these patterns. A failure
analysis points out factors that could con-
tribute to improvements in both precision
and recall, including pattern generalisa-
tion, pattern pruning, and term match-
ing. The analysis gives insights into the
relationship between domain terminology
and lexical relations, and into evaluation
strategies for relation learning.
1 Introduction
We are used to seeing words listed alphabetically
in dictionaries. In terms of meaning, this order-
ing has little relevance beyond shared roots. In the
OED, jam is sandwiched between jalpaite (a
sulphide) and jama (a cotton gown). It is a long
way from bread and raspberry1. Vocabular-
ies, however, do have a natural structure: one that
we rely on for language understanding. This struc-
ture is defined in part by lexical, or sense, relations,
1Oxford English Dictionary, Second Edition, 1989.
such as the familiar relations of synonymy and hy-
ponymy (Cruse, 2000). Meronymy relates the lex-
ical item for a part to that for a whole, equivalent
to the conceptual relation of partOf 2. Example 1
shows a meronym. When we read the text, we un-
derstand that the frontal lobes are not a new
entity unrelated to what has gone before, but part of
the previously mentioned brain.
(1) MRI sections were taken through the
brain. Frontal lobe shrinkage suggests a
generalised cerebral atrophy.
The research described in this paper considers
meronymy, and its extraction from text. It is tak-
ing place in the context of the Clinical e-Science
Framework (CLEF) project 3, which is developing
information extraction (IE) tools to allow querying
of medical records. Both IE and querying require
domain knowledge, whether encoded explicitly or
implicitly. In IE, domain knowledge is required to
resolve co-references between textual entities, such
as those in Example 1. In querying, domain knowl-
edge is required to expand and constrain user expres-
sions. For example, the query in Example 2 should
retrieve sarcomas in the pelvis, but not in limbs.
(2) Retrieve patients on Gemcitabine with ad-
vanced sarcomas in the trunk of the body.
The part-whole relation is critical to domain
knowledge in biomedicine: the structure and func-
tion of biological organisms are organised along par-
titive axes. The relation is modelled in several medi-
cal knowledge resources (Rogers and Rector, 2000),
2Although it is generally held that partOf is not just a single
simple relation, this will not be considered here.
3http://www.clef-user.com/
49
but they are incomplete, costly to maintain, and un-
suitable for language engineering. This paper looks
at simple lexico-syntactic techniques for learning
meronyms. Section 2 considers background and re-
lated work; Section 3 introduces an algorithm for
relation extraction, and its implementation in the
PartEx system; Section 4 considers materials and
methods used for experiments with PartEx. The
experiments are reported in Section 5, followed by
conclusions and suggestions for future work.
2 Related Work
Early work on knowledge extraction from elec-
tronic dictionaries used lexico-syntactic patterns to
build relational records from definitions. This in-
cluded some work on partOf (Evens, 1988). Lex-
ical relation extraction has, however, concentrated
on hyponym extraction. A widely cited method
is that of Hearst (1992), who argues that specific
lexical relations are expressed in well-known intra-
sentential lexico-syntactic patterns. Hearst success-
fully extracted hyponym relations, but had little suc-
cess with meronymy, finding that meronymic con-
texts are ambiguous (for example, cat?s paw and
cat?s dinner). Morin (1999) reported a semi-
automatic implementation of Hearst?s algorithm.
Recent work has applied lexical relation extraction
to ontology learning (Maedche and Staab, 2004).
Berland and Charniak (1999) report what they be-
lieved to be the first work finding part-whole rela-
tions from unlabelled corpora. The method used is
similar to that of Hearst, but includes metrics for
ranking proposed part-whole relations. They report
55% accuracy for the top 50 ranked relations, using
only the two best extraction patterns.
Girju (2003) reports a relation discovery algo-
rithm based on Hearst. Girju contends that the am-
biguity of part-whole patterns means that more in-
formation is needed to distinguish meronymic from
non-meronymic contexts. She developed an algo-
rithm to learn semantic constraints for this differen-
tiation, achieving 83% precision and 98% recall with
a small set of manually selected patterns. Others
have looked specifically at meronymy in anaphora
resolution (e.g. Poesio et al(2002)).
The algorithm presented here learns relations di-
rectly between semantically typed multiword terms,
Input:
? A lexicon
? Relations between
terms
? Corpus from which
to learn
Output:
? New relations
? New terms
? Context patterns
Steps:
1. Using input resources
(a) Label terms
(b) Label relations
2. For a fixed number of iterations or until no
new relations are learned
(a) Identify contexts that contain both
participants in a relation
(b) Create patterns describing contexts
(c) Generalise the patterns
(d) Use generalised patterns to identify new
relation instances
(e) Label new terms
(f) Label new relations
Figure 1: PartEx algorithm for relation discovery
and itself contributes to term recognition. Learning
is automatic, with neither manual selection of best
patterns, nor expert validation of patterns. In these
respects, it differs from earlier work. Hearst and
others learn relations between either noun phrases
or single words, while Morin (1999) discusses how
hypernyms learnt between single words can be pro-
jected onto multi-word terms. Earlier algorithms in-
clude manual selection of initial or ?best? patterns.
The experiments differ from others in that they are
restricted to a well defined domain, anatomy, and
use existing domain knowledge resources.
3 Algorithm
Input to the algorithm consists of existing lexical and
relational resources, such as terminologies and on-
tologies. These are used to label text with training
relations. The context of these relations are found
automatically, and patterns created to describe these
contexts. These patterns are generalised and used
to discover new relations, which are fed back itera-
tively into the algorithm. The algorithm is given in
Figure 1. An example iteration is shown in Figure 2.
3.1 Discovering New Terms
Step 2e in Figure 1 labels new terms, which may be
discovered as a by-product of identifying new rela-
50
Figure 2: PartEx relation discovery between terms,
patterns represented by tokens and parts of speech.
tion instances. This is possible because there is a
distinction between the lexical item used to find the
pattern context (Step 2a), and the pattern element
against which new relations are matched (Step 2d).
For example, a pattern could be found from the con-
text (term relation term), and expressed as (noun
relation adjective noun). When applied to the
text to learn new relation instances, sequences of to-
kens taking part in this relation will be found, and
may be inferred to be terms for the next iteration.
3.2 Implementation: PartEx
Implementation was independent of any specific re-
lation, but configured, as the PartEx system, to dis-
cover partOf. Relations were usually learned be-
tween terms, although this was varied in some exper-
iments. The algorithm was implemented using the
GATE NLP framework (Cunningham et al, 2002)
and texts preprocessed using the tokeniser, sentence
splitter, and part-of-speech (POS) tagger provided
with GATE. In training, terms were labelled using
MMTx, which uses lexical variant generation to map
noun phrases to candidate terms and concepts at-
tested in a terminology database. Final candidate
selection is based on linguistic matching metrics,
and concept resolution on filtering ambiguity from
the MMTx source terminologies (Aronson, 2001).
Training relations were labelled from an existing
meronymy. Simple contexts of up to five tokens
between the participants in the relation were identi-
fied using JAPE, a regular expression language inte-
grated into GATE. For some experiments, relations
were considered between noun phrases, labelled us-
ing LT CHUNK (Mikheev and Finch, 1997). GATE
wrappers for MMTx, LT CHUNK, and other PartEx
modules are freely available 4.
Patterns describing contexts were expressed as
shallow lexico-syntactic patterns in JAPE, and a
JAPE transducer used to find new relations. A typi-
cal pattern consisted of a sequence of parts of speech
and words. Pattern generalisation was minimal, re-
moving only those patterns that were either identical
to another pattern, or that had more specific lexico-
syntactic elements of another pattern. To simplify
pattern creation for the experiments reported here,
patterns only used context between the relation par-
ticipants, and did not use regular expression quan-
tifiers. New terms found during relation discovery
were labelled using a finite state machine created
with the Termino compiler (Harkema et al, 2004).
4 Materials and Method
Lexical and relational resources were provided by
the Unified Medical Language System (UMLS), a
collection of medical terminologies 5. Term lookup
in the training phase was carried out using MMTx.
Experiments made particular use of The Univer-
sity of Washington Digital Anatomist Foundational
Model (UWDA), a knowledge base of anatomy in-
cluded in UMLS. Relation labelling in the training
phase used a meronymy derived by computing the
transitive closure of that provided with the UWDA.
The UWDA gives definitions for some terms, as
headless phrases that do not include the term be-
ing defined. A corpus was constructed from these,
for learning and evaluation. This corpus used the
first 300 UWDA terms with a definition, that had a
UMLS semantic type of ?Body Part?. These terms
included synonyms and orthographic variants given
the same definition. Complete definitions were con-
structed by prepending terms to definitions with the
copula ?is?. An example is shown in Figure 2.
4http://www.dcs.shef.ac.uk/?angus
5Version 2003AC, http://www.nlm.nih.gov/research/umls/
51
Experiments were carried out using cross valida-
tion over ten random unseen folds, with 71 unique
meronyms across all ten folds. Definitions were
pre-processed by tokenising, sentence splitting, POS
tagging and term labelling. Evaluation was carried
out by comparison of relations learned in the held
back fold, to those in an artificially generated gold
standard (described below). Evaluation was type
based, rather than instance based: unique relation
instances in the gold standard were compared with
unique relation instances found by PartEx, i.e. iden-
tical relation instances found within the same fold
were treated as a single type. Evaluation therefore
measures domain knowledge discovery.
Gold standard relations were generated using the
same context window as for Step 2a of the al-
gorithm. Pairs of terms from each context were
checked automatically for a relation in UWDA, and
this added to the gold standard. This evaluation
strategy is not ideal. First, the presence of a part
and a whole in a context does not mean that they are
being meronymically related (for example, ?found
in the hand and finger?). The number of spurious
meronyms in the gold standard has not yet been as-
certained. Second, a true relation in the text may not
appear in a limited resource such as the UWDA (al-
though this can be overcome through a failure anal-
ysis, as described in Section 4.1). Although a better
gold standard would be based on expert mark up of
the text, the one used serves to give quick feedback
with minimal cost. Standard evaluation metrics were
used. The accuracy of initial term and relation la-
belling were not evaluated, as these are identical in
both gold standard creation and in experiments.
4.1 Failure Analysis
For some experiments, a failure analysis was carried
out on missing and spurious relations. The reasons
for failure were hypothesised by examining the sen-
tence in which the relation occurred, the pattern that
led to its discovery, and the source of the pattern.
Some spurious relations appeared to be correct,
even though they were not in the gold standard.
This is because the gold standard is based on a re-
source which itself has limits. One of the aims of
the work is to supplement such resources: the algo-
rithm should find correct relations that are not in
the resource. Proper evaluation of these relations re-
quires care, and methodologies are currently being
investigated. A quick measure of their contribution
was, however, found by applying a simple method-
ology, based on the source texts being definitional,
authoritative, and describing relations in unambigu-
ous language. The methodology adjusts the number
of spurious relations, and calculates a corrected pre-
cision. By leaving the number of actual relations
unchanged, corrected precision still reflects the pro-
portion of discovered relations that were correct rel-
ative to the gold standard, but also reflects the num-
ber of correct relations not in the gold standard. The
methodology followed the steps in Figure 3.
1. Examine the context of the relation.
2. If the text gives a clear statement of
meronomy, the relation is not spurious.
3. If the text is clearly not a statement of
meronomy, the relation is spurious.
4. If the text is ambiguous, refer to a second
authoritative resource6. If this gives a
clear statement of meronomy, the relation is
not spurious.
5. If none of these apply, the relation is
spurious.
6. Calculate corrected precision from the new
number of spurious relations.
Figure 3: Calculating corrected precision.
5 Experimental Results
Table 3 shows the results of running PartEx in var-
ious configurations, and evaluating over the same
ten folds. The first configuration, labelled BASE,
used PartEx as described in Section 3.2, to give a
recall of 0.80 and precision of 0.25. A failure anal-
ysis for this configuration is given in Table 2. It
shows that the largest contribution to spurious re-
lations (i.e. to lack of precision), was due to re-
lations discovered by some pattern that is ambigu-
ous for meronymy (category PATTERN). For exam-
ple, the pattern ?[noun] and [noun]? finds the
incorrect meronym ?median partOf lateral?
from the text ?median and lateral glossoepiglottic
folds?. The algorithm learned the pattern from a cor-
rect meronym, and applying it in the next iteration,
learned spurious relations, compounding the error.
6In this case, Clinically Oriented Anatomy. K. Moore and
A. Dalley. 4th Edition. 1999. Lippincott Williams and Wilkins.
52
Category Description Count %
SPECIFIC There are one or more variant patterns that come close to matching this relation, but none specific to it. 10 50%
DISCARD Patterns that could have picked these up were discarded, as they were also generating spurious patterns. 7 35%
SCARCE The context is unique in the corpus, and so a pattern could not be learnt without generalisation. 3 15%
COMPOUND The relation is within a compound noun. These are not recognised by the discovery algorithm. 1 5%
COMPLEX Complex context, which is beyond the simple current ?part token* whole? context. 1 5%
Table 1: Failure analysis of 20 missing relations over ten folds, using PartEx configuration FILT.
Category Description BASE FILT
Count % Count %
PATTERN The pattern used to discover the relation does not encode partonomy in this case (Patterns involving:
is 33 (69%); and 10 (21%); or 3 (6%); other 2 (4%)).
48 43% 0 0%
CORRECT Although not in the gold standard, the relation is clearly correct, either from an unambiguous state-
ment of fact in the text from which it was mined, or by reference to a standard anatomy textbook.
30 27% 33 49%
DEEP The relation is within a deeper structure than the surface patterns considered. The algorithm has
found an incorrect relation that relates to this deep structure. For example, the text ?limen nasi is
subdivision of surface of viscerocranial mucosa? leads to (limen nasi partOf surface).
12 11% 14 21%
FRAGMENT:DEEP A combination of the FRAGMENT and DEEP categories. For example, given the text ?nucleus of
nerve is subdivision of neural tree?, it has learnt that (subdivision partOf neural).
10 9% 4 6%
FRAGMENT The relation is a fragment of one in the text. For example, ?plica salpingopalatine is subdivision of
viscerocranial mucosa? leads to (plica salpingopalatine partOf viscerocranial).
9 8% 12 18%
OTHER Other reason. 4 4% 3 5%
Table 2: Failure analysis of spurious part-whole relations found by PartEx, for configuration BASE (over
half the spurious relations across ten folds) and configuration FILT (all spurious relations in ten folds). In
each case, a small number of relations are in two categories.
Possible Actual Missing Spurious P R
BASE 71 56 15 168 0.25 0.80
FILT 71 51 20 67 0.43 0.73
CORR 71 51 20 34 0.58 0.73
ITR1 71 45 26 66 0.39 0.62
ITR2 71 51 20 67 0.43 0.73
TERM 71 51 20 213 0.20 0.74
TOK 30 26 4 266 0.09 0.88
NP 32 27 5 393 0.07 0.81
POS 71 21 50 749 0.03 0.32
Table 3: Evaluation of PartEx. Total number of re-
lations, mean precision (P) and mean recall (R) for
various configurations, as discussed in the text.
The bulk of the spurious results of this type were
learnt from patterns using the tokens and, is, and or.
This problem needs a principled solution, perhaps
based on pruning patterns against a held-out portion
of training data, or by learning ambiguous patterns
from a large general corpus. Such a solution is be-
ing developed. In order to mimic it for the purpose
of these experiments, a filter was built to remove pat-
terns derived from problematic contexts. Table 3
shows the results of this change, as configuration
FILT: precision rose to 0.43, and recall dropped. All
other experiments reported used this filter.
A failure analysis of missing relations from con-
figuration FILT is shown in Table 1. The drop in
recall is explained by PartEx filtering ambiguous
patterns. The biggest contribution to lack of recall
was over-specific patterns (for example, the pattern
?[term] is part of [term]? would not identify
the meronym in ?finger is a part of the hand?. Gen-
eralisation of patterns is essential to improve recall.
Improvements could also be made with more sophis-
ticated context, and by examining compounds.
A failure analysis of spurious relations for config-
uration FILT is shown in Table 2. The biggest im-
pact on precision was made by relations that could
be considered correct, as discussed in Section 4.1.
A corrected precision of 0.58 was calculated, shown
as configuration CORR in Table 3. Two other fac-
tors affecting precision can be deduced from Ta-
ble 2. First, some relations were encoded in deeper
linguistic structures than those considered (category
DEEP). Improvements could be made to precision
by considering these deeper structures. Second,
some spurious relations were found between frag-
ments of terms, due to failure of term recognition.
The algorithm used by PartEx is iterative, the im-
plementation completing in two iterations. Config-
urations ITR1 and ITR2 in Table 3 show that both
recall and precision increase as learning progresses.
Four other experiments were run, to assess the im-
pact of term recognition. Results are shown in Ta-
ble 3. Configuration TERM continued to label terms
in the training phase, but did not label new terms
found during iteration (as discussed in Section 3.1).
53
TOK and NP used no term recognition, instead find-
ing relations between tokens and noun phrases re-
spectively (the gold standard being amended to re-
flect the new task). POS omitted part-of-speech tags
from patterns. In all cases, there was a large in-
crease in spurious results, impacting precision. Term
recognition seemed to provide a constraint in rela-
tion discovery, although the nature of this is unclear.
6 Conclusions
The PartEx system is capable of fully automated
learning of meronyms between semantically typed
terms, from the experimental corpus. With simu-
lated pattern pruning, it achieves a recall of 0.73 and
a precision of 0.58. In contrast to earlier work, these
results were achieved without manual labelling of
the corpus, and without direct manual selection of
high performance patterns. Although the cost of this
automation is lower results than the earlier work,
failure analyses provide insights into the algorithm
and scope for its further improvement.
Current work includes: automated pattern prun-
ing, extending pattern context and generalisation; in-
corporating deeper analyses of the text, such as se-
mantic labelling (c.f. Girju (2003)) and the use of
dependency structures; investigating the ro?le of term
recognition in relation discovery; measures for eval-
uating new relation discovery; extraction of putative
sub-relations of meronymy. Work to scale the algo-
rithm to larger corpora is also under way, in recogni-
tion of the fact that the corpus used was small, highly
regularised, and unusually rich in meronyms.
Acknowledgements
This work was supported by a UK Medical Research
Council studentship. The author thanks his supervi-
sor Robert Gaizauskas for useful discussions, and
the reviewers for their comments.
References
A. Aronson. 2001. Effective Mapping of Biomedical
Text to the UMLS Metathesaurus: The MetaMap Pro-
gram. In Proceedings of the 2001 American Medi-
cal Informatics Association Annual Symposium, pages
17?21, Bethesda, MD.
M. Berland and E. Charniak. 1999. Finding Parts in Very
Large Corpora. In Proceedings of the 37th Annual
Meeting of the Association for Computational Linguis-
tics, pages 57?64, College Park, MD.
D. Cruse. 2000. Meaning in Language: An Introduc-
tion to Semantics and Pragmatics. Oxford University
Press.
H. Cunningham, D. Maynard, K. Bontcheva, and
V. Tablan. 2002. GATE: A Framework and Graphical
Development Environment for Robust NLP Tools and
Applications. In Proceedings of the 40th Anniversary
Meeting of the Association for Computational Linguis-
tics, pages 168?175, Philadelphia, PA.
M. Evens, editor. 1988. Relational Models of the Lexi-
con: Representing Knowledge in Semantic Networks.
Cambridge University Press.
R. Girju, A. Badulescu, and D. Moldovan. 2003. Learn-
ing Semantic Constraints for the Automatic Discovery
of Part-Whole Relations. In Proceedings of the Hu-
man Language Technology Conference / North Ameri-
can Chapter of the Association for Computational Lin-
guistics Conference, Edmonton, Canada.
H. Harkema, R. Gaizauskas, M. Hepple, N. Davis,
Y. Guo, A. Roberts, and I. Roberts. 2004. A Large-
Scale Resource for Storing and Recognizing Techni-
cal Terminology. In Proceedings of 4th International
Conference on Language Resources and Evaluation,
Lisbon, Portugal.
M. Hearst. 1992. Automatic Acquisition of Hy-
ponyms from Large Text Corpora. In Proceedings of
the Fourteenth International Conference on Computa-
tional Linguistics, pages 539?545, Nantes, France.
A. Maedche and S. Staab. 2004. Ontology Learning. In
Handbook on Ontologies, pages 173?190. Springer.
A. Mikheev and S. Finch. 1997. A Workbench for Find-
ing Structure in Texts. In Proceedings of the Fifth
Conference on Applied Natural Language Processing,
pages 372?379, Washington D.C.
E. Morin and C. Jacquemin. 1999. Projecting Corpus-
based Semantic Links on a Thesaurus. In Proceed-
ings of the 37th Annual Meeting of the Association
for Computational Linguistics, pages 389?396, Col-
lege Park, MD.
M. Poesio, T. Ishikawa, S. Schulte im Walde, and
R. Vieira. 2002. Acquiring Lexical Knowledge for
Anaphora Resolution. In Proceedings of the Third In-
ternational Conference on Language Resources and
Evaluation, Las Palmas, Canary Islands.
J. Rogers and A. Rector. 2000. GALEN?s Model of
Parts and Wholes: Experience and Comparisons. In
Proceedings of the 2000 American Medical Informat-
ics Association Annual Symposium, pages 714?718,
Philadelphia, PA.
54
A Large Scale Terminology Resource for Biomedical Text Processing
Henk Harkema, Robert Gaizauskas, Mark Hepple, Angus Roberts,
Ian Roberts, Neil Davis, Yikun Guo
Department of Computer Science, University of Sheffield, UK
biomed@dcs.shef.ac.uk
Abstract
In this paper we discuss the design, implemen-
tation, and use of Termino, a large scale termi-
nological resource for text processing. Dealing
with terminology is a difficult but unavoidable
task for language processing applications, such
as Information Extraction in technical domains.
Complex, heterogeneous information must be
stored about large numbers of terms. At the
same time term recognition must be performed
in realistic times. Termino attempts to recon-
cile this tension by maintaining a flexible, ex-
tensible relational database for storing termino-
logical information and compiling finite state
machines from this database to do term look-
up. While Termino has been developed for
biomedical applications, its general design al-
lows it to be used for term processing in any
domain.
1 Introduction
It has been widely recognized that the biomedical litera-
ture is now so large, and growing so quickly, that it is be-
coming increasingly difficult for researchers to access the
published results that are relevant to their research. Con-
sequently, any technology that can facilitate this access
should help to increase research productivity. This has
led to an increased interest in the application of natural
language processing techniques for the automatic capture
of biomedical content from journal abstracts, complete
papers, and other textual documents (Gaizauskas et al,
2003; Hahn et al, 2002; Pustejovsky et al, 2002; Rind-
flesch et al, 2000).
An essential processing step in these applications is
the identification and semantic classification of techni-
cal terms in text, since these terms often point to enti-
ties about which information should be extracted. Proper
semantic classification of terms also helps in resolving
anaphora and extracting relations whose arguments are
restricted semantically.
1.1 Challenge
Any technical domain generates very large numbers of
terms ? single or multiword expressions that have some
specialised use or meaning in that domain. For exam-
ple, the UMLS Metathesaurus (Humphreys et al, 1998),
which provides a semantic classification of terms from a
wide range of vocabularies in the clinical and biomedical
domain, currently contains well over 2 million distinct
English terms.
For a variety of reasons, recognizing these terms in
text is not a trivial task. First of all, terms are often
long multi-token sequences, e.g. 3-methyladenine-DNA
glycosylase I. Moreover, since terms are referred to re-
peatedly in discourses there is a benefit in their being
short and unambiguous, so they are frequently abbre-
viated and acronymized, e.g. CvL for chromobacterium
viscosum lipase. However, abbreviations may not al-
ways occur together with their full forms in a text, the
method of abbreviation is not predictable in all cases, and
many three letter abbreviations are highly overloaded.
Terms are also subject to a high degree of orthographic
variation as a result of the representation of non-Latin
characters, e.g. a-helix vs. alpha-helix, capitalization,
e.g. DNA vs. dna, hyphenation, e.g. anti-histamine vs. an-
tihistamine, and British and American spelling variants,
e.g. tumour vs. tumor. Furthermore, biomedical science
is a dynamic field: new terms are constantly being in-
troduced while old ones fall into disuse. Finally, certain
classes of biomedical terms exhibit metonomy, e.g. when
a protein is referred to by the gene that expresses it.
To begin to address these issues in term recognition, we
are building a large-scale resource for storing and recog-
nizing technical terminology, called Termino. This re-
source must store complex, heterogeneous information
about large numbers of terms. At the same time term
recognition must be performed in realistic times. Ter-
mino attempts to reconcile this tension by maintaining a
                                            Association for Computational Linguistics.
                   Linking Biological Literature, Ontologies and Databases, pp. 53-60.
                                                HLT-NAACL 2004 Workshop: Biolink 2004,
flexible, extensible relational database for storing termi-
nological information and compiling finite state machines
from this database to do term look-up.
1.2 Context
Termino is being developed in the context of two ongoing
projects: CLEF, for Clinical E-Science Framework (Rec-
tor et al, 2003) and myGrid (Goble et al, 2003). Both
these projects involve an Information Extraction compo-
nent. Information Extraction is the activity of identifying
pre-defined classes of entities and relationships in natural
language texts and storing this information in a structured
format enabling rapid and effective access to the informa-
tion, e.g. Gaizauskas and Wilks (1998), Grishman (1997).
The goal of the CLEF project is to extract information
from patient records regarding the treatment of cancer.
The treatment of cancer patients may extend over several
years and the resulting clinical record may include many
documents, such as clinic letters, case notes, lab reports,
discharge summaries, etc. These documents are gener-
ally full of medical terms naming entities such as body
parts, drugs, problems (i.e. symptoms and diseases), in-
vestigations and interventions. Some of these terms are
particular to the hospital from which the document origi-
nates. We aim to identify these classes of entities, as well
as relationships between such entities, e.g. that an investi-
gation has indicated a particular problem, which, in turn,
has been treated with a particular intervention. The infor-
mation extracted from the patient records is potentially of
value for immediate patient care, but can also be used to
support longitudinal and epidemiological medical stud-
ies, and to assist policy makers and health care managers
in regard to planning and clinical governance.
The myGrid project aims to present research biolo-
gists with a unified workbench through which component
bioinformatic services can be accessed using a workflow
model. These services may be remotely located from the
user and will be exploited via grid or web-service chan-
nels. A text extraction service will form one of these ser-
vices and will facilitate access to information in the sci-
entific literature. This text service comprises an off-line
and an on-line component. The off-line component in-
volves pre-processing a large biological sciences corpus,
in this case the contents of Medline, in order to identify
various biological entities such as genes, enzymes, and
proteins, and relationships between them such as struc-
tural and locative relations. These entities and relation-
ships are referred to in Medline abstracts by a very large
number of technical terms and expressions, which con-
tributes to the complexity of processing these texts. The
on-line component supports access to the extracted infor-
mation, as well as to the raw texts, via a SOAP interface
to an SQL database.
Despite the different objectives for text extraction
within the CLEF and myGrid projects, many of the tech-
nical challenges they face are the same, such as the
need for extensive capabilities to recognize and classify
biomedical entities as described using complex techni-
cal terminology in text. As a consequence we are con-
structing a general framework for the extraction of infor-
mation from biomedical text: AMBIT, a system for ac-
quiring medical and biological information from text. An
overview of the AMBIT logical architecture is shown in
figure 1.
The AMBIT system contains several engines, of which
Termino is one. The Information Extraction Engine pulls
selected information out of natural language text and
pushes this information into a set of pre-defined tem-
plates. These are structured objects which consists of one
or more slots for holding the extracted entities and rela-
tions. The Query Engine allows users to access informa-
tion through traditional free text search and search based
on the structured information produced by the Informa-
tion Extraction Engine, so that queries may refer to spe-
cific entities and classes of entities, and specific kinds of
relations that are recognised to hold between them. The
Text Indexing Engine is used to index text and extracted,
structured information for the purposes of information re-
trieval. The AMBIT system contains two further compo-
nents: an interface layer, which provides a web or grid
channel to allow user and program access to the system;
and a database which holds free text and structured infor-
mation that can be searched through the Query Engine.
Termino interacts with the Query Engine and the Text
Indexing Engine to provide terminological support for
query formulation and text indexation. It also provides
knowledge for the Information Extraction Engine to use
in identifying and classifying biomedical entities in text.
The Terminology Engine can furthermore be called by
users and remote programs to access information from
the various lexical resources that are integrated in the ter-
minological database.
2 Related Work
Since identification and classification of technical terms
in biomedical text is an essential step in information
extraction and other natural language processing tasks,
most natural language processing systems contain a
terminological resource of some sort. Some systems
make use of existing terminological resources, notably
the UMLS Metathesaurus, e.g. Rindflesch et al (2000),
Pustejovski et al (2002); other systems rely on re-
sources that have been specifically built for the applica-
tion, e.g. Humphreys et al (2000), Thomas et al (2000).
The UMLS Metathesaurus provides a semantic classi-
fication of terms drawn from a wide range of vocabularies
in the clinical and biomedical domain (Humphreys et al,
1998). It does so by grouping strings from the source vo-
from Hospital 1
Clinical Records
Journals
On?line
Abstracts
Medline
Literature
Biomedical
Engine
Indexing
Text
Engine
Extraction
...
(Termino)
Engine
Terminology
Ambit
from Hospital 2
Clinical Records
Web GRID
Interface layer
Raw text
(entities / relations)
Structured InfoFree text
  search
Engine
Query
Information
SOAP /
     HTTP
& Annotations
Structured Info
Figure 1: AMBIT Architecture
cabularies that are judged to have the same meaning into
concepts, and mapping these concepts onto nodes or se-
mantic types in a semantic network. Although the UMLS
Metathesaurus is used in a number of biomedical natural
language processing applications, we have decided not to
adopt the UMLS Metathesaurus as the primary terminol-
ogy resource in AMBIT for a variety of reasons.
One of the reasons for this decision is that the Metathe-
saurus is a closed system: strings are classified in terms
of the concepts and the semantic types that are present
in the Metathesaurus and the semantic network, whereas
we would like to be able to link our terms into multi-
ple ontologies, including in-house ontologies that do not
figure in any of the Metathesaurus? source vocabularies
and hence are not available through the Metathesaurus.
Moreover, we would also like to be able to have access to
additional terminological information that is not present
in the Metathesaurus, such as, for example, the annota-
tions in the Gene Ontology (The Gene Ontology Con-
sortium, 2001) assigned to a given human protein term.
While the terms making up the the tripartite Gene On-
tology are present in the UMLS Metathesaurus, assign-
ments of these terms to gene products are not recorded
in the Metathesaurus. Furthermore, as new terms appear
constantly in the biomedical field we would like to be
able to instantly add these to our terminological resource
and not have to wait until they have been included in the
UMLS Metathesaurus. Additionally, some medical terms
appearing in patient notes are hospital-specific and are
unlikely to be included in the Metathesaurus at all.
With regard to systems that do not use the UMLS
Metathesaurus, but rather depend on terminological re-
sources that have been specifically built for an applica-
tion, we note that these terminological resources tend to
be limited in the following two respects. First, the struc-
ture of these resources is often fixed and in some cases
amounts to simple gazetteer lists. Secondly, because of
their fixed structure, these resources are usually popu-
lated with content from just a few sources, leaving out
many other potentially interesting sources of terminolog-
ical information.
Instead, we intend for Termino to be an exten-
sible resource that can hold diverse kinds of termi-
nological information. The information in Termino
is either imported from existing, outside knowledge
sources, e.g. the Enzyme Nomenclature (http://www.
chem.qmw.ac.uk/iubmb/enzyme/), the Structural Classi-
fication of Proteins database (Murzin et al, 1995), and
the UMLS Metathesaurus, or it is induced from on-line
raw text resources, e.g. Medline abstracts. Termino thus
provides uniform access to terminological information
aggregated across many sources. Using Termino re-
moves the need for multiple, source-specific terminolog-
ical components within text processing systems that em-
ploy multiple terminological resources.
3 Architecture
Termino consists of two components: a database holding
terminological information and a compiler for generating
term recognizers from the contents of the database. These
two components will be discussed in the following two
sections.
STRINGS
string str id
. . . . . .
neurofibromin str728
abdomen str056
mammectomy str176
mastectomy str183
. . . . . .
TERMOID STRINGS
trm id str id
. . . . . .
trm023 str056
trm656 str056
trm924 str728
trm369 str728
trm278 str176
trm627 str183
. . . . . .
PART OF SPEECH
trm id pos
. . . . . .
trm023 N
. . . . . .
SYNONYMY
syn id trm id scl id
. . . . . . . . .
syn866 trm278 syn006
syn435 trm627 syn006
. . . . . . . . .
GO ANNOTATIONS
trm id annotation version
. . . . . . . . .
trm924 GO:0004857 9/2003
trm369 GO:0008285 9/2003
. . . . . . . . .
UMLS
trm id cui lui sui version
. . . . . . . . . . . . . . .
trm278 C0024881 L0024669 S0059711 2003AC
trm656 C0000726 L0000726 S0414154 2003AC
. . . . . . . . . . . . . . .
Figure 2: Structure of the terminological database
3.1 Terminological Database
The terminological database is designed to meet three re-
quirements. First of all, it must be capable of storing large
numbers of terms. As we have seen, the UMLS Metathe-
saurus contains over 2 million distinct terms. However,
as UMLS is just one of many resources whose terms may
need to be stored, many millions of terms may need to
be stored in total. Secondly, Termino?s database must
also be flexible enough to hold a variety of information
about terms, including information of a morpho-syntactic
nature, such as part of speech and morphological class;
information of a semantic nature, such as quasi-logical
form and links to concepts in ontologies; and provenance
information, such as the sources of the information in the
database. The database will also contain links to connect
synonyms and morphological and orthographic variants
to one another and to connect abbreviations and acronyms
to their full forms. Finally, the database must be orga-
nized in such a way that it allows for fast and efficient
recognition of terms in text.
As mentioned above, the information in Termino?s
database is either imported from existing, outside knowl-
edge sources or induced from text corpora. Since these
sources are heterogeneous in both information content
and format, Termino?s database is ?extensional?: it stores
strings and information about strings. Higher-order con-
cepts such as ?term? emerge as the result of interconnec-
tions between strings and information in the database.
The database is organized as a set of relational tables,
each storing one of the types of information mentioned
above. In this way, new information can easily be in-
cluded in the database without any global changes to the
structure of the database.
Terminological information about any given string is
usually gathered from multiple sources. As information
about a string accumulates in the database, we must make
sure that co-dependencies between various pieces of in-
formation about the string are preserved. This considera-
tion leads to the fundamental element of the terminologi-
cal database, a termoid. A termoid consists of a string to-
gether with associated information of various kinds about
the string. Information in one termoid holds conjunc-
tively for the termoid?s string, while multiple termoids
for the same string express disjunctive alternatives.
For instance, taking an example from UMLS, we may
learn from one source that the string cold as an adjective
refers to a temperature, whereas another source may tell
us that cold as a noun refers to a disease. This informa-
tion is stored in the database as two termoids: abstractly,
?cold, adjective, temperature? and ?cold, noun, disease?.
A single termoid ?cold, adjective, noun, temperature, dis-
ease? would not capture the co-dependency between the
part of speech and the ?meaning? of cold.1 This example
illustrates that a string can be in more than one termoid.
1Note that the UMLS Metathesaurus has no mechanism for
storing this co-dependency between grammatical and semantic
information.
Each termoid, however, has one and only one string.
Figure 2 provides a detailed example of part of the
structure of the terminological database. In the table
STRINGS every unique string is assigned a string iden-
tifier (str id). In the table TERMOID STRINGS each string
identifier is associated with one or more termoid iden-
tifiers (trm id). These termoid identifiers then serve as
keys into the tables holding terminological information.
Thus, in this particular example, the database includes
the information that in the Gene Ontology the string
neurofibromin has been assigned the terms with identi-
fiers GO:0004857 and GO:0008285. Furthermore, in the
UMLS Metathesaurus version 2003AC, the string mam-
mectomy has been assigned the concept-unique identifier
C0024881 (CUI), the lemma-unique identifier L0024669
(LUI), and the string-unique identifier S0059711 (SUI).
Connections between termoids such as those arising
from synonymy and orthographic variation are recorded
in another set of tables. For example, the table SYN-
ONYMY in figure 2 indicates that termoids 278 and
627 are synonymous, since they have the same syn-
onymy class identifier (scl id).2 The synonymy identifier
(syn id) identifies the assignment of a termoid to a partic-
ular synonymy class. This identifier is used to record the
source on which the assignment is based. This can be a
reference to a knowledge source from which synonymy
information has been imported into Termino, or a refer-
ence to both an algorithm by which and a corpus from
which synonyms have been extracted. Similarly there are
tables containing provenance information for strings, in-
dexed by str id, and termoids, indexed by trm id. These
tables are not shown in he example.
With regard to the first requirement for the design of
the terminological database mentioned at the beginning
of this section ? scalability ?, an implementation of Ter-
mino in MySQL has been loaded with 427,000 termoids
for 363,000 strings (see section 4 for more details). In it
the largest table, STRINGS, measures just 16MB, which is
nowhere near the default limit of 4GB that MySQL im-
poses on the size of tables. Hence, storing a large num-
ber of terms in Termino is not a problem size-wise. The
second requirement, flexibility of the database, is met by
distributing terminological information over a set of rela-
tively small tables and linking the contents of these tables
to strings via termoid identifiers. In this way we avoid the
strictures of any one fixed representational scheme, thus
making it possible for the database to hold information
from disparate sources. The third requirement on the de-
sign of the database, efficient recognition of terms, will
2The function of synonymy class identifiers in Termino is
similar to the function of CUIs in the UMLS Metathesaurus.
However, since we are not bound to a classification into UMLS
CUIs, we can assert synonymy between terms coming from ar-
bitrary sources.
be addressed in the next section.
3.2 Term Recognition
To ensure fast term recognition with Termino?s vast ter-
minological database, the system comes equipped with
a compiler for generating finite state machines from the
strings in the terminological database discussed in the
previous section. Direct look-up of strings in the database
is not an option, because it is unknown in advance at
which positions in a text terms will start and end. In order
to be complete, one would have to look up all sequences
of words or tokens in the text, which is very inefficient.
Compilation of a finite state recognizer proceeds in
the following way. First, each string in the database is
broken into tokens, where a token is either a contigu-
ous sequence of alpha-numeric characters or a punctu-
ation symbol. Next, starting from a single initial state, a
path through the machine is constructed, using the tokens
of the string to label transitions. For example, for the
string Graves? disease the machine will include a path
with transitions on Graves, ?, and disease. New states are
only created when necessary. The state reached on the fi-
nal token of a string will be labeled final and is associated
with the identifiers of the termoids for that string.
To recognize terms in text, the text is tokenized and the
finite state machine is run over the text, starting from the
initial state at each token in the text. For each sequence
of tokens leading to a final state, the termoid identifiers
associated with that state are returned. These identifiers
are then used to access the terminological database and
retrieve the information contained in the termoids. Where
appropriate the machine will produce multiple termoid
identifiers for strings. It will also recognize overlapping
and embedded strings.
Figure 3 shows a small terminological database and a
finite state recognizer derived from it. Running this rec-
ognizer over the phrase . . . thyroid dysfunction, such as
Graves? disease . . . produces four annotations: thyroid
is assigned the termoid identifiers trm1 and trm2; thyroid
dysfunction, trm3; and Graves? disease, trm4.
It should be emphasised at this point that term recog-
nition as performed by Termino is in fact term look-up
and not the end point of term processing. Term look-up
might return multiple possible terms for a given string,
or for overlapping strings, and subsequent processes may
apply to filter these alternatives down to the single option
that seems most likely to be correct in the given context.
Furthermore, more flexible processes of term recognition
might apply over the results of look-up. For example, a
term grammar might be provided for a given domain, al-
lowing longer terms to be built from shorter terms that
have been identified by term look-up.
The compiler can be parameterized to produce finite
state machines that match exact strings only, or that ab-
STRINGS
string str id
thyroid str12
thyroid disfunction str15
Graves? disease str25
TERMOID STRINGS
trm id str id
trm1 str12
trm2 str12
trm3 str15
trm4 str25
? trm4disease
thyroid
Graves
trm3
trm2
trm1
disfunction
Figure 3: Sample terminological database and finite state term recognizer
stract away from morphological and orthographical vari-
ation. At the moment, morphological information about
strings is supplied by a component outside Termino. In
our current term recognition system, this component ap-
plies to a text before the recognition process and asso-
ciates all verbs and nouns with their base form. Similarly,
the morphological component applies to the strings in the
terminological database before the compilation process.
The set-up in which term recognizers are compiled
from the contents of the terminological database turns
Termino into a general terminological resource which is
not restricted to any single domain or application. The
database can be loaded with terms from multiple domains
and compilation can be restricted to particular subsets of
strings by selecting termoids from the database based on
their source, for example. In this way one can produce
term recognizers that are tailored towards specific do-
mains or specific applications within domains.
4 Implementation & Performance
A first version of Termino has been implemented. It uses
a database implemented in MySQL and currently con-
tains over 427,000 termoids for around 363,000 strings.
Content has been imported from various sources by
means of source-specific scripts for extracting relevant
information from sources and a general script for load-
ing this extracted information into Termino. More specif-
ically, to support information extraction from patient
records, we have included in Termino strings from the
UMLS Metathesaurus falling under the following seman-
tic types: pharmacologic substances, anatomical struc-
tures, therapeutic procedure, diagnostic procedure, and
several others. We have also loaded a list of hu-
man proteins and their assignments to the Gene Ontol-
ogy as produced by the European Bioinformatics Insti-
tute (http://www.ebi.ac.uk/GOA/) into Termino. Further-
more, we have included several gazetteer lists containing
terms in the fields of molecular biology and pharmacol-
ogy that were assembled for previous information extrac-
tion projects in our NLP group. A web services (SOAP)
API to the database is under development. We plan to
make the resource available to researchers as a web ser-
vice or in downloadable form.3
The compiler to construct finite state recognizers from
the database is fully implemented, tested, and integrated
into AMBIT. The compiled recognizer for the 363,000
strings of Termino has 1.2 million states and an on-disk
size of around 80MB. Loading the matcher from disk
into memory requires about 70 seconds (on an UltraSparc
900MHz), but once loaded recognition is a very fast pro-
cess. We have been able to annotate a corpus of 114,200
documents, drawn from electronic patient records from
the Royal Marsden NHS Trust in London and each ap-
proximately 1kB of text, in approximately 44 hours ? an
average rate of 1.4 seconds per document, or 42 docu-
ments per minute. On average, about 30 terms falling un-
der the UMLS ?clinical? semantic types mentioned above
were recognized in each document. We are currently an-
notating a bench-mark corpus in order to obtain precision
and recall figures. We are also planning to compile rec-
ognizers for differently sized subsets of the terminologi-
cal database and measure their recognition speed over a
given collection of texts. This will provide some indica-
tion as to the scalability of the system.
Since Termino currently contains many terms imported
from the UMLS Metathesaurus, it is interesting to com-
pare its term recognition performance against the per-
formance of MetaMap. MetaMap is a program avail-
able from at the National Library of Medicine ? the de-
velopers of UMLS ? specifically designed to discover
UMLS Metathesaurus concepts referred to in text (Aron-
son, 2001). An impressionistic comparison of the per-
formance of Termino and MetaMap on the CLEF patient
records shows that the results differ in two ways. First,
MetaMap recognizes more terms than Termino. This
is simply because MetaMap draws on a comprehensive
version of UMLS, whereas Termino just contains a se-
lected subset of the strings in the Metathesaurus. Sec-
ondly, MetaMap is able to recognize variants of terms,
e.g. it will map the verb to treat and its inflectional forms
onto the term treatment, whereas Termino currently does
not do this. To recognize term variants MetaMap re-
lies on UMLS?s SPECIALIST lexicon, which provides
3Users may have to sign license agreements with third par-
ties in order to be able to use restricted resources that have been
integrated into Termino.
syntactic, morphological, and orthographic information
for many of the terms occurring in the Metathesaurus.
While the performance of both systems differs in favor
of MetaMap, it is important to note that the source of
these differences is unrelated to the actual design of Ter-
mino?s terminological database or Termino?s use of fi-
nite state machines to do term recognition. Rather, the
divergence in performance follows from a difference in
breadth of content of both systems at the moment. With
regard to practical matters, the comparison showed that
term recognition with Termino is much faster than with
MetaMap. Also, compiling a finite state recognizer from
the terminological database in Termino is a matter of min-
utes, whereas setting up MetaMap can take several hours.
However, since MetaMap?s processing is more involved
than Termino?s, e.g. MetaMap parses the input first, and
hence requires more resources, these remarks should be
backed up with a more rigorous comparison between Ter-
mino and MetaMap, which is currently underway.
The advantage of term recognition with Termino over
MetaMap and UMLS or any other recognizer with a sin-
gle source, is that it provides immediate entry points
into a variety of outside ontologies and other knowledge
sources, making the information in these sources avail-
able to processing steps subsequent to term recognition.
For example, for a gene or protein name recognized in a
text, Termino will return the database identifiers of this
term in the HUGO Nomenclature database (Wain et al,
2002) and the OMIM database (Online Mendelian Inher-
itance in Man, OMIM (TM), 2000). These identifiers
give access to the information stored in these databases
about the gene or protein, including alternative names,
gene map locus, related disorders, and references to rele-
vant papers.
5 Conclusions & Future Work
Dealing with terminology is an essential step in natural
language processing in technical domains. In this paper
we have described the design, implementation, and use of
Termino, a large scale terminology resource for biomedi-
cal language processing.
Termino includes a relational database which is de-
signed to store a large number of terms together with
complex, heterogeneous information about these terms,
such as morpho-syntactic information, links to concepts
in ontologies, and other kinds of annotations. The
database is also designed to be extensible: it is easy to
include terms and information about terms found in out-
side biological databases and ontologies. Term look-up
in text is done via finite state machines that are compiled
from the contents of the database. This approach allows
the database to be very rich without sacrificing speed at
look-up time. These three features make Termino a flexi-
ble tool for inclusion in a biomedical text processing sys-
tem.
As noted in section 3.2, Termino has not been designed
to be used as a stand-alone term recognition system but
rather as the first component, the lexical look-up com-
ponent, in a multi-component term processing system.
Since Termino may return multiple terms for a given
string, or for overlapping strings, some post-filtering of
these alternatives is necessary. Secondly, it is likely that
better term recognition performance will be obtained by
supplementing Termino look-up with a term parser which
uses a grammar to give a term recognizer the generative
capacity to recognize previously unseen terms. For ex-
ample, many terms for chemical compounds conform to
grammars that allow complex terms to be built out of sim-
pler terms prefixed or suffixed with numerals separated
from the simpler term with hyphens. It does not make
sense to attempt to store in Termino all of these variants.
Termino provides a firm basis on which to build large-
scale biomedical text processing applications. However,
there are a number of directions where further work can
be done. First, as noted in 3.2, morphological informa-
tion is currently not held in Termino, but rather resides
in an external morphological analyzer. We are working
to extend the Termino data model to enable information
about morphological variation to be stored in Termino,
so that Termino serves as a single source of information
for the terms it contains. Secondly, we are working to
build term induction modules to allow Termino content
to be automatically acquired from corpora, in addition
to deriving it from manually created resources such as
UMLS. Finally, while we have already incorporated Ter-
mino into the AMBIT system where it collaborates with
a term parser to perform more complete term recogni-
tion, more work can be done to with respect to such an
integration. For example, probabilities could be incorpo-
rated into Termino to assist with probabilistic parsing of
terms; or, issues of trade-off between what should be in
the term lexicon versus the term grammar could be fur-
ther explored by looking to see which compound terms
in the lexicon contain other terms as substrings and at-
tempt to abstract away from these to grammar rules. For
example, in the example thyroid disfunction above, both
thyroid and disfunction are terms, the first of class ?body
part?, the second of class ?problem?. Their combination
thyroid disfunction is a term of class ?problem?, suggest-
ing a rule of the form ?problem?   ?body part? ?problem?.
References
A.R. Aronson. 2001. Effective mapping of biomedical
text to the UMLS Metathesaurus: the MetaMap pro-
gram. In Proceedings of the American Medical Infor-
matics Association Symposium, pages 17?21.
R. Gaizauskas and Y. Wilks. 1998. Information extrac-
tion: Beyond document retrieval. Journal of Docu-
mentation, 54(1):70?105.
R. Gaizauskas, G. Demetriou, P. Artymiuk, and P. Wil-
lett. 2003. Protein structures and information extrac-
tion from biological texts: The PASTA system. Jour-
nal of Bioinformatics, 19(1):135?143.
C.A. Goble, C.J. Wroe, R. Stevens, and the my-
Grid consortium. 2003. The myGrid project:
Services, architecture and demonstrator. In
S. Cox, editor, Proceedings of UK e-Science
All Hands Meeting 2003, Nottingham, UK.
http://www.nesc.ac.uk/events/ahm2003/AHMCD/.
R. Grishman. 1997. Information extraction: Techniques
and challenges. In Maria Teresa Pazienza, editor, In-
formation Extraction, pages 10?27. Springer Verlag.
U. Hahn, M. Romacker, and S. Schulz. 2002. Creating
knowledge repositories from biomedical reports: the
medSynDiKATe text mining system. In Proceedings
of the Pacific Symposium on Biocomputing, pages 338?
349.
L. Humphreys, D.A.B. Lindberg, H.M. Schoolman, and
G.O. Barnett. 1998. The Unified Medical Language
System: An informatics research collaboration. Jour-
nal of the American Medical Informatics Association,
1(5):1?13.
K. Humphreys, G. Demetriou, and R. Gaizauskas. 2000.
Two applications of information extraction to biolog-
ical science journal articles: Enzyme interactions and
protein structures. In Proceedings of the Pacific Sym-
posium on Biocomputing, pages 505?516.
A.G. Murzin, S.E. Brenner, T. Hubbard, and C. Chothia.
1995. SCOP: A structural classification of proteins
database for the investigation of sequences and struc-
tures. Journal of Molecular Biology, (247):536?540.
(http://scop.mrc-lmb.cam.ac.uk/scop/).
Online Mendelian Inheritance in Man, OMIM (TM).
2000. McKusick-Nathans Institute for Genetic
Medicine, Johns Hopkins University (Baltimore, MD)
and National Center for Biotechnology Informa-
tion, National Library of Medicine (Bethesda, MD).
http://www.ncbi.nlm.nih.gov/omim/.
J. Pustejovsky, J. Castan?o, R. Saur??, A. Rumshisky,
J. Zhang, and W. Luo. 2002. Medstract: Creat-
ing large-scale information servers for biomedical li-
braries. In Proceedings of the Workshop on Natural
Language Processing in the Biomedical Domain, As-
sociation for Computational Linguistics 40th Anniver-
sary Meeting (ACL-02), pages 85?92.
A. Rector, J. Rogers, A. Taweel, D. Ingram, D. Kalra,
J. Milan, R. Gaizauskas, M. Hepple, D. Scott,
and R. Power. 2003. Joining up health care
with clinical and post-genomic research. In
S. Cox, editor, Proceedings of UK e-Science
All Hands Meeting 2003, Nottingham, UK.
http://www.nesc.ac.uk/events/ahm2003/AHMCD/.
C.T. Rindflesch, J.V. Rajan, and L. Hunter. 2000. Ex-
tracting molecular binding relationships from biomed-
ical text. In Proceedings of the 6th Applied Natu-
ral Language Processing conference / North American
chapter of the Association for Computational Linguis-
tics annual meeting, pages 188?915.
The Gene Ontology Consortium. 2001. Creating the
gene ontology resource: design and implementation.
Genome Research, 11(8):1425?1433.
J. Thomas, D. Milward, C. Ouzounis, and S. Pulman.
2000. Automatic extraction of protein interactions
from scientific abstracts. In Proceedings of the Pacific
Symposium on Biocomputing, pages 538?549.
H.M. Wain, M. Lush, F. Ducluzeau, and S. Povey.
2002. Genew: The human nomenclature
database. Nucleic Acids Research, 30(1):169?171.
(http://www.gene.ucl.ac.uk/nomenclature/).
BioNLP 2008: Current Trends in Biomedical Natural Language Processing, pages 10?18,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Extracting Clinical Relationships from Patient Narratives
Angus Roberts, Robert Gaizauskas, Mark Hepple
Department of Computer Science, University of Sheffield,
Regent Court, 211 Portobello, Sheffield S1 4DP
{initial.surname}@dcs.shef.ac.uk
Abstract
The Clinical E-Science Framework (CLEF)
project has built a system to extract clin-
ically significant information from the tex-
tual component of medical records, for clin-
ical research, evidence-based healthcare and
genotype-meets-phenotype informatics. One
part of this system is the identification of rela-
tionships between clinically important entities
in the text. Typical approaches to relationship
extraction in this domain have used full parses,
domain-specific grammars, and large knowl-
edge bases encoding domain knowledge. In
other areas of biomedical NLP, statistical ma-
chine learning approaches are now routinely
applied to relationship extraction. We report
on the novel application of these statistical
techniques to clinical relationships.
We describe a supervised machine learning
system, trained with a corpus of oncology nar-
ratives hand-annotated with clinically impor-
tant relationships. Various shallow features
are extracted from these texts, and used to
train statistical classifiers. We compare the
suitability of these features for clinical re-
lationship extraction, how extraction varies
between inter- and intra-sentential relation-
ships, and examine the amount of training data
needed to learn various relationships.
1 Introduction
The application of Natural Language Processing
(NLP) is widespread in biomedicine. Typically, it
is applied to improve access to the ever-burgeoning
research literature. Increasingly, biomedical re-
searchers need to relate this literature to pheno-
typic data: both to populations, and to individ-
ual clinical subjects. The computer applications
used in biomedical research, including NLP appli-
cations, therefore need to support genotype-meets-
phenotype informatics and the move towards trans-
lational biology. Such support will undoubtedly in-
clude linkage to the information held in individual
medical records: both the structured portion, and the
unstructured textual portion.
The Clinical E-Science Framework (CLEF)
project (Rector et al, 2003) is building a frame-
work for the capture, integration and presentation of
this clinical information, for research and evidence-
based health care. The project?s data resource is a
repository of the full clinical records for over 20000
cancer patients from the Royal Marsden Hospital,
Europe?s largest oncology centre. These records
combine structured information, clinical narratives,
and free text investigation reports. CLEF uses infor-
mation extraction (IE) technology to make informa-
tion from the textual portion of the medical record
available for integration with the structured record,
and thus available for clinical care and research. The
CLEF IE system analyses the textual records to ex-
tract entities, events and the relationships between
them. These relationships give information that is
often not available in the structured record. Why
was a drug given? What were the results of a physi-
cal examination? What problems were not present?
We have previously reported entity extraction in the
CLEF IE system (Roberts et al, 2008b). This paper
examines relationship extraction.
Extraction of relationships from clinical text is
usually carried out as part of a full clinical IE sys-
tem. Several such systems have been described.
They generally use a syntactic parse with domain-
specific grammar rules. The Linguistic String
project (Sager et al, 1994) used a full syntactic and
10
clinical sublanguage parse to fill template data struc-
tures corresponding to medical statements. These
were mapped to a database model incorporating
medical facts and the relationships between them.
MedLEE (Friedman et al, 1994), and more recently
BioMedLEE (Lussier et al, 2006) used a semantic
lexicon and grammar of domain-specific semantic
patterns. The patterns encode the possible relation-
ships between entities, allowing both entities and the
relationships between them to be directly matched
in the text. Other systems have incorporated large-
scale domain-specific knowledge bases. MEDSYN-
DIKATE (Hahn et al, 2002) employed a rich dis-
course model of entities and their relationships, built
using a dependency parse of texts and a descrip-
tion logic knowledge base re-engineered from exist-
ing terminologies. MENELAS (Zweigenbaum et al,
1995) also used a full parse, a conceptual represen-
tation of the text, and a large scale knowledge base.
In other applications of biomedical NLP, a sec-
ond paradigm has become widespread: the appli-
cation of statistical machine learning techniques to
feature-based models of the text. Such approaches
have typically been applied to journal texts. They
have been used both for entity recognition and ex-
traction of various relations, such as protein-protein
interactions (see, for example, Grover et al(2007)).
This follows on from the success of these methods
in general NLP (see for example Zhou et al(2005)).
Statistical machine learning has also been applied to
clinical text, but its use has generally been limited
to entity recognition. The Mayo Clinic text analysis
system (Pakhomov et al, 2005), for example, uses a
combination of dictionary lookup and a Na??ve Bayes
classifier to identify entities for information retrieval
applications. To the best of our knowledge, statisti-
cal methods have not been previously applied to ex-
traction of clinical relationships from text.
This paper describes experiments in the statistical
machine learning of relationships from a novel text
type: oncology narratives. The set of relationships
extracted are considered to be of interest for clinical
and research applications down line of IE, such as
querying to support clinical research. We apply Sup-
port Vector Machine (SVM) classifiers to learn these
relationships. The classifiers are trained and eval-
uated using novel data: a gold standard corpus of
clinical text, hand-annotated with semantic entities
and relationships. In order to test the applicability
of this method to the clinical domain, we train clas-
sifiers using a number of comparatively simple text
features, and look at the contribution of these fea-
tures to system performance. Clinically interesting
relationships may span several sentences, and so we
compare classifiers trained for both intra- and inter-
sentential relationships (spanning one or more sen-
tence boundaries). We also examine the influence of
training corpus size on performance, as hand anno-
tation of training data is the major expense in super-
vised machine learning.
2 Relationship Schema
Relationship Argument 1 Argument 2
has target Investigation Locus
Intervention Locus
has finding Investigation Condition
Investigation Result
has indication Drug or device Condition
Intervention Condition
Investigation Condition
has location Condition Locus
negation modifies Negation modifier Condition
laterality modifies Laterality modifier Intervention
Laterality modifier Locus
sub-location modifies Sub-location modifier Locus
Table 1: Relationship types and their argument type con-
straints.
The CLEF application extracts entities, relation-
ships and modifiers from text. By entity, we mean
some real-world thing, event or state referred to in
the text: the drugs that are mentioned, the tests that
were carried out, etc. Modifiers are words that qual-
ify an entity in some way, referring e.g. to the lat-
erality of an anatomical locus, or the negation of a
condition (?no sign of inflammation?). Entities are
connected to each other and to modifiers by rela-
tionships: e.g. linking a drug entity to the condition
entity for which it is indicated, linking an investiga-
tion to its results, or linking a negating phrase to a
condition.
The entities, modifiers, and relationships are de-
scribed by both a formal XML schema, and by a
set of detailed definitions. These were developed by
a group of clinical experts through an iterative pro-
cess, until acceptable agreement was reached. Entity
types are mapped to types from the UMLS seman-
tic network (Lindberg et al, 1993), each CLEF en-
11
tity type covering several UMLS types. Relationship
types are those that were felt necessary to capture the
essential clinical dependencies between entities re-
ferred to in patient documents, and to support CLEF
end user applications.
Each relationship type is constrained to exist be-
tween limited pairs of entity types. For example,
the has location relationship can only exist be-
tween a Condition entity and a Locus entity.
Some relationships can exist between multiple type
pairs. The full set of relationships and their argu-
ment type constraints are shown in Table 1. Ex-
amples of each relationship are given in Roberts et
al (2008a).
Some of the relationships considered important
by the clinical experts were not obvious without do-
main knowledge. For example,
He is suffering from nausea and severe
headaches. Dolasteron was prescribed.
Without domain knowledge, it is not clear that there
is a has indication relationship between the
?Dolasteron? Drug or device entity and the
?nausea? Condition entity. As in this example,
many of this type of relationship are intra-sentential.
A single real-world entity may be referred to sev-
eral times in the same text. Each of these co-
referring expressions is a mention of the entity. The
gold standard includes annotation of co-reference
between different textual mentions of the same en-
tity. For the work reported in this paper, however,
co-reference is not considered. Each entity is as-
sumed to have a single mention. Relationships be-
tween entities can be considered, by extension, as
relationships between the single mentions of those
entities. The implications of this are discussed fur-
ther below.
3 Gold Standard Corpus
The schema and definitions were used to hand-
annotate the entities and relationships in 77 oncol-
ogy narratives, to provide a gold standard for sys-
tem training and evaluation. Corpora of this size
are typical in supervised machine learning, and re-
flect the expense of hand annotation. Narratives
were carefully selected and annotated according to
a best practice methodology, as described in Roberts
et al(2008a). Narratives were annotated by two in-
dependent, clinically trained, annotators, and a con-
sensus created by a third. We will refer to this corpus
as C77.
Annotators were asked to first mark the mentions
of entities and modifiers, and then to go through
each of these in turn, deciding if any had relation-
ships with mentions of other entities. Although the
annotators were marking co-reference between men-
tions of the same entity, they were asked to ignore
this with respect to relationship annotation. Both
the annotation tool that they were using and their
annotation guidelines, enforced the creation of rela-
tionships between mentions, and not between enti-
ties. The gold standard is thus analogous to the style
of relationship extraction reported here, in which
we extract relations between single mention entities,
and do not consider co-reference. Annotators were
further told that relationships could span multiple
sentences, and that it was acceptable to use clini-
cal domain knowledge to infer that a relationship
existed between two mentions. Counts of all rela-
tionships annotated in C77 are shown in Table 2,
sub-divided by the number of sentence boundaries
spanned by a relationship.
4 Relationship Extraction
The system we have built uses the GATE NLP
toolkit (Cunningham et al, 2002) 1. The system is
shown in Figure 1, and is described below.
Narratives are first pre-processed using standard
GATE modules. Narratives were tokenised, sen-
tences found with a regular expression-based sen-
tence splitter, part-of-speech (POS) tagged, and
morphological roots found for tokens. Each to-
ken was also labelled with a generalised POS tag,
the first two characters of the full POS tag. This
takes advantage of the Penn Treebank tagset used
by GATE?s POS tagger, in which related POS tags
share the first two characters. For example, all six
verb POS tags start with the letters ?VB?.
After pre-processing, mentions of entities within
the text are annotated. In the experiments reported,
we assume perfect entity recognition, as given by
the entities in the human annotated gold standard
1We used a development build of GATE 4.0, downloadable
from http://gate.ac.uk
12
Sentence boundaries between arguments
0 1 2 3 4 5 6 7 8 9 >9 Total
has finding 265 46 25 7 5 4 3 2 2 2 0 361
has indication 139 85 35 32 14 11 6 4 5 5 12 348
has location 360 4 1 1 1 1 1 0 0 0 4 373
has target 122 14 4 2 2 4 3 1 0 1 0 153
laterality modifies 128 0 0 0 0 0 0 0 0 0 0 128
negation modifies 100 1 0 0 0 0 0 0 0 0 0 101
sub location modifies 76 0 0 0 0 0 0 0 0 0 0 76
Total 1190 150 65 42 22 20 13 7 7 8 16 1540
Cumulative total 1190 1340 1405 1447 1469 1489 1502 1509 1516 1524 1540
Table 2: Count of relationships in 77 gold standard documents.
described above. Our results are therefore higher
than would be expected in a system with automatic
entity recognition. It is useful and usual to fix en-
tity recognition in this way, to allow tuning specific
to relationship extraction, and to allow the isolation
of relation-specific problems. We accept, however,
that ultimately, relation extraction does depend on
the quality of entity recognition. The relation extrac-
tion described here is used as part of an operational
IE system in which clinical entity recognition is per-
formed by a combination of lexical lookup and su-
pervised machine learning. We have described our
entity extraction system elsewhere (Roberts et al,
2008b).
4.1 Classification
We treat clinical relationship extraction as a classi-
fication task, training classifiers to assign a relation-
ship type to an entity pair. An entity pair is a pairing
of entities that may or may not be the arguments of
a relation. For a given document, we create all pos-
sible entity pairs within two constraints. First, en-
tities that are paired must be within n sentences of
each other. For all of the work reported here, unless
stated, n ? 1 (crossing 0 or 1 sentence boundaries).
Second, we can constrain the entity pairs created
by argument type (Rindflesch and Fiszman, 2003).
For example, there is little point in creating an en-
tity pair between a Drug or device entity and
a Result entity, as no relationships, as specified
by the schema, exist between entities of these types.
Entity pairing is carried out by a GATE component
developed specifically for clinical relationship ex-
traction. In addition to pairing entities according to
the above constraints, this component also assigns
features to each pair that characterise its lexical and
syntactic qualities (described further in Section 4.2).
Entity pairs correspond to classifier training and
test instances. In classifier training, if an entity
pair corresponds to the arguments of a relationship
present in the gold standard, then it is assigned a
class of that relationship type. If it does not corre-
spond to such a relation, then it is assigned the class
null. The classifier builds a model of these entity
pair training instances, from their features. In classi-
fier application, entity pairs are created from unseen
text, under the above constraints. The classifier as-
signs one of our seven relationship types, or null,
to each entity pair.
We use Support Vector machines (SVMs) as train-
able classifiers, as these have proved to be robust and
efficient for a range of NLP tasks, including relation
extraction. We use an SVM implementation devel-
oped within our own group, and provided as part
of the GATE toolkit. This is a variant on the orig-
inal SVM algorithm, SVM with uneven margins, in
which classification may be biased towards positive
training examples. This is particularly suited to NLP
applications, in which positive training examples are
often rare. Full details of the classifier are given in
Li et al(2005). We used the implementation ?out of
the box?, with default parameters as determined in
experiments with other data sets.
SVMs are binary classifiers: the multi-class prob-
lem of classifying entity pairs must therefore be
mapped to a number of binary classification prob-
lems. There are several ways in which a multi-
class problem can be recast as binary problems. The
commonest are one-against-one in which one classi-
fier is trained for every possible pair of classes, and
one-against-all in which a classifier is trained for
a binary decision between each class and all other
13
classes, including null, combined. We have car-
ried out extensive experiments (not reported here),
with these two strategies, and have found little dif-
ference between them for our data. We have chosen
to use one-against-all, as it needs fewer classifiers
(for an n class problem, it needs n classifiers, as op-
posed to (n?1)!2 for one-against-one).
The resultant class assignments by multiple bi-
nary classifiers must be post-processed to deal with
ambiguity. In application to unseen text, it is possi-
ble that several classifiers assign different classes to
an entity pair (test instance). To disambiguate these
cases, the output of each one-against-all classifier is
transformed into a probability, and the class with
the highest probability is assigned. Re-casting the
multi-class relation problem as a number of binary
problems, and post-processing to resolve ambigui-
ties, is handled by the GATE Learning API.
Figure 1: The relationship extraction system.
4.2 Features for Classification
The SVM classification model is built from lexical
and syntactic features assigned to tokens and en-
tity pairs prior to classification. We use features
developed in part from those described in Zhou et
al (2005) and Wang et al(2006). These features are
split into 11 sets, as described in Table 3.
The tokN features are POS and surface string
taken from a window of N tokens on each side of
each paired entity?s mention. For N = 6, this
gives 48 features. The rationale behind these sim-
ple features is that there is useful information in the
words surrounding two mentions, that helps deter-
mine any relationship between them. The gentokN
features generalise tokN to use morphological root
and generalised POS. The str features are a set
of 14 surface string features, encoding the full sur-
face strings of both entity mentions, their heads,
their heads combined, the surface strings of the first,
last and other tokens between the mentions, and
of the two tokens immediately before and after the
leftmost and rightmost mentions respectively. The
pos, root, and genpos feature sets are similarly
constructed from the POS tags, roots, and gener-
alised POS tags of the entity mentions and their sur-
rounding tokens. These four feature sets differ from
tokN and gentokN, in that they provide more fine-
grained information about the position of features
relative to the paired entity mentions.
For the event feature set, the main entities
were divided into events (Investigation and
Intervention) and non-events (all others). Fea-
tures record whether the entity pair consists of two
events, two non-events, one of each, and whether
there are any intervening events and non-events.
This feature set gives similar information to atype
(semantic types of arguments) and inter (inter-
vening entities), but at a coarser level of typing.
5 Evaluation
We used a standard ten-fold cross validation
methodology and standard evaluation metrics. Met-
rics are defined in terms of true positive, false pos-
itive and false negative matches between relation-
ships in a system annotated response document and
a gold standard key document. A response relation-
ship is a true positive if a relationship of the same
type, and with the exact same arguments, exists in
the key. Corresponding definitions apply for false
positive and false negative. Counts of these matches
are used to calculate standard metrics of Recall (R),
Precision (P ) and F1 measure.
The metrics do not say how hard relationship ex-
traction is. We therefore provide a comparison with
Inter Annotator Agreement (IAA) scores from the
gold standard. The IAA score gives the agreement
between the two independent double annotators. It
is equivalent to scoring one annotator against the
other using the F1 metric. IAA scores are not di-
rectly comparable here, as relationship annotation is
14
Feature set Size Description
tokN 8N Surface string and POS of tokens surrounding the arguments, windowed ?N to +N , N = 6 by default
gentokN 8N Root and gerenalised POS of tokens surrounding the argument entities, windowed ?N to +N , N = 6 by default
atype 1 Concatenated semantic type of arguments, in arg1-arg2 order
dir 1 Direction: linear text order of the arguments (is arg1 before arg2, or vice versa?)
dist 2 Distance: absolute number of sentence and paragraph boundaries between arguments
str 14 Surface string features based on Zhou et al(2005), see text for full description
pos 14 POS features, as above
root 14 Root features, as above
genpos 14 Generalised POS features, as above
inter 11 Intervening mentions: numbers and types of intervening entity mentions between arguments
event 5 Events: are any of the arguments, or intevening entities, events?
allgen 96 All features in root and generalised POS forms, i.e. gentok6+atype+dir+dist+root+genpos+inter+event
notok 48 All except tokN features, others in string and POS forms, i.e. atype+dir+dist+str+pos+inter+event
Table 3: Feature sets used for learning relationships. The size of a set is the number of features in that set.
a slightly different task for the human annotators.
The relationship extraction system is given entities,
and finds relationships between them. Human an-
notators must find both the entities and the relation-
ships. Therefore, were one human annotator to fail
to find a particular entity, they could never find rela-
tionships with that entity. The raw IAA score does
not take this into account: if an annotator fails to
find an entity, then they will also be penalised for
all relationships with that entity. We therefore give a
Corrected IAA, CIAA, in which annotators are only
compared on those relations for which they have
both found the entities involved. Both forms of IAA
are shown in Table 4. It is clear that it is hard for
annotators to reach agreement on relationships, and
that this is compounded massively by lack of perfect
agreement on entities. Note that the gold standard
used in training and evaluation reflects a further con-
sensus annotation, to correct this poor agreement.
6 Results
6.1 Feature Selection
The first group of experiments reported looks at the
performance of relation extraction with various fea-
ture sets. We followed an additive strategy for fea-
ture selection. Starting with basic features, we added
further features one set at a time. We measured the
performance of the resulting classifier each time we
added a new feature set. Results are shown in Ta-
ble 4. The initial classifier used a tok6+atype
feature set. Addition of both dir and dist fea-
tures give significant improvements in all metrics, of
around 10% F1 overall, in each case. This suggests
that the linear text order of arguments, and whether
relations are intra- or inter-sentential is important to
classification. Addition of the str features also give
good improvement in most metrics, again 10% F1
overall. Addition of part-of-speech information, in
the form of pos features, however, leads to a drop
in some metrics, overall F1 dropping by 1%. Unex-
pectedly, POS seems to provide little extra informa-
tion above that in surface string. Errors in POS tag-
ging cannot be dismissed, and could be the cause of
this. The existence of intervening entities, as coded
in feature set inter, provides a small benefit. The
inclusion of information about events, in the event
feature set, is less clear-cut.
We were interested to see if generalising features
could improve performance, as this had benefited
our previous work in entity extraction. We replaced
all surface string features with their root form, and
POS features with their generalised POS form. This
gave the results shown in column allgen. Results
are not clear cut, in some cases better and in some
worse than the previous best. Overall, there is no
difference in F1. There is a slight increase in over-
all recall, and a corresponding drop in precision ?
as might be expected.
Both the tokN, and the str and pos feature sets
provide surface string and POS information about
tokens surrounding and between relationship argu-
ments. The former gives features from a window
around each argument. The latter two give a greater
amount of positional information. Do these two pro-
vide enough information on their own, without the
windowed features? To test this, we removed the
tokN features from the full cumulative feature set,
from column +event. Results are given in column
15
Relation Metric tok6+atype +dir +dist +str +pos +inter +event allgen notok IAA CIAA
has finding P 44 49 58 63 62 64 65 63 63
R 39 63 78 80 80 81 81 82 82
F1 39 54 66 70 69 71 72 71 71 46 80
has indication P 37 23 38 42 40 41 42 37 44
R 14 14 46 44 44 47 47 45 47
F1 18 16 39 39 38 41 42 38 41 26 50
has location P 36 36 50 68 71 72 72 73 73
R 28 28 74 79 79 81 81 83 83
F1 30 30 58 72 74 76 75 77 76 55 80
has target P 9 9 32 63 57 60 62 60 59
R 11 11 51 68 67 67 66 68 68
F1 9 9 38 64 60 63 63 63 62 42 63
laterality modifies P 21 38 73 84 83 84 84 86 86
R 9 55 82 89 86 88 88 87 89
F1 12 44 76 85 83 84 84 84 85 73 94
negation modifies P 19 54 85 81 80 79 79 77 81
R 12 82 97 98 93 92 93 93 93
F1 13 63 89 88 85 84 85 83 85 66 93
sub location modifies P 2 2 55 88 86 86 88 88 87
R 1 1 62 94 92 95 95 95 95
F1 1 1 56 90 86 89 91 91 90 49 96
Overall P 33 38 50 63 62 64 65 64 64
R 22 36 70 74 73 75 75 76 76
F1 26 37 58 68 67 69 69 69 70 47 75
Table 4: Variation in performance by feature set. Features sets are abbreviated as in Table 3. For the first seven
columns, features were added cumulatively to each other. The next two columns, allgen and notok, are as de-
scribed in Table 3. The final two columns give inter annotator agreement and corrected inter annotator agreement, for
comparison.
notok. There is no clear change in performance,
some relationships improving, and some worsening.
Overall, there is a 1% improvement in F1.
It appears that the bulk of performance is attained
through entity type and distance features, with some
contribution from positional surface string informa-
tion. Performance is between 1% and 9% lower than
CIAA for the same relationship, with a best overall
F1 of 70%, compared to a CIAA of 75%.
6.2 Sentences Spanned
Table 2 shows that although most relationships are
intra-sentential, 23% are inter-sentential, 10% of all
relationships being between arguments in adjacent
sentences. If we consider a relationship to cross n
sentence boundaries, then the classifiers described in
the previous section were all trained on relationships
crossing n ? 1 sentence boundaries, i.e. with argu-
ments in the same or adjacent sentences. What effect
does including more distant relationships have on
performance? We trained classifiers on only intra-
sentential relationships, and on relationships span-
ning up to n sentence boundaries, for n ? {1...5}.
We also trained a classifier on relationships with
1 ? n ? 5, comprising 85% of all inter-sentential
relationships. In each case, the cumulative feature
set +event from Table 4 was used. Results are
shown in Table 5. It is clear from the results that
the feature sets used do not perform well on inter-
sentential relationships. There is a 6% drop in over-
all F1 when including relationships with n = 1 to-
gether with n < 1. Performance continues to drop as
more inter-sentential relationships are included, and
is very poor for just inter-sentential relationships.
A preliminary error analysis suggests that the
more distant relationship arguments are from each
other, the more likely clinical knowledge is required
to extract the relationship. This raises additional dif-
ficulties for extraction, which the simple features de-
scribed here are unable to address.
6.3 Size of Training Corpus
The provision of sufficient training data for super-
vised learning algorithms is a limitation on their use.
We examined the effect of training corpus size on
relationship extraction. The C77 corpus, compris-
16
Number of sentence boundaries between arguments
inter- intra- inter- and intra-sentential Corpus size
Relation Metric 1 ? n ? 5 n < 1 n ? 1 n ? 2 n ? 3 n ? 4 n ? 5 C25 C50 C77
has finding P 24 68 65 62 60 61 61 66 63 65
R 18 89 81 79 78 78 77 74 74 81
F1 18 76 72 69 67 68 67 67 67 72
has indication P 18 49 42 42 36 32 30 22 25 42
R 17 59 47 42 42 39 38 30 31 47
F1 16 51 42 39 37 34 33 23 25 42
has location P 0 74 72 73 72 72 72 72 71 72
R 0 83 81 81 81 82 82 76 80 81
F1 0 77 75 76 75 76 76 73 74 75
has target P 3 64 62 59 60 59 58 65 49 62
R 1 75 66 64 62 61 61 60 65 66
F1 2 68 63 61 60 60 59 59 54 63
laterality modifies P 0 86 84 86 86 86 87 77 78 84
R 0 89 88 88 88 87 88 69 68 88
F1 0 85 84 85 86 85 86 72 69 84
negation modifies P 0 80 79 79 80 80 80 78 79 79
R 0 94 93 91 93 93 93 80 93 93
F1 0 86 85 84 85 86 85 78 84 85
sub location modifies P 0 89 88 88 89 89 89 64 91 88
R 0 95 95 95 95 95 95 64 85 95
F1 0 91 91 91 91 91 91 64 86 91
Overall P 22 69 65 64 62 61 60 62 63 65
R 17 83 75 73 71 70 70 65 71 75
F1 19 75 69 68 66 65 65 63 66 69
Table 5: Variation in performance, by number of sentence boundaries (n), and by training corpus size.
ing 77 narratives and used in the previous experi-
ments, was subsetted to give corpora of 25 and 50
narratives, which will be referred to as C25 and C50
respectively. We trained two further classifiers on
these new corpora. Again, the cumulative feature
set +event from Table 4 was used. Results are
shown in Table 5. Overall, performance improves as
training corpus size increases (F1 rising from 63%
to 69%). We were struck however, by the fact that
increasing from 50 to 77 documents has little effect
on a few relationships (negation modifies and
has location). It may well be that the amount
of training data required has plateaued for those re-
lationships.
7 Conclusion
We have shown that it is possible to extract clini-
cal relationships from text, using shallow features,
and supervised statistical machine learning. Judg-
ing from poor inter annotator agreement, the task
is hard. Our system achieves a reasonable perfor-
mance, with an overall F1 just 5% below a cor-
rected inter annotator agreement. This performance
is reached largely by using features of the text that
encode entity type, distance between arguments, and
some surface string information. Performance does,
however, vary with the number of sentences spanned
by the relationships. Learning inter-sentential rela-
tionships does not seem amenable to this approach,
and may require the use of domain knowledge.
A major concern when using supervised learning
algorithms is the expense and availability of training
data. We have shown that while this concern is jus-
tified in some cases, larger training corpora may not
improve performance for all relationships.
The technology used has proved scalable. The
full CLEF IE system, including automatic entity
recognition, is able to process a document in sub-
second time on a commodity workstation. We
have used the system to extract 6 million relations
from over half a million patient documents, for use
in downstream CLEF applications (Roberts et al,
2008a). Our future work on relationship extrac-
tion in CLEF includes integration of a dependency
parse into the feature set, further analysis to deter-
mine what knowledge may be required to learn inter-
sentential relations, and integration of relationship
extraction with a co-reference algorithm.
17
Availability All of the software described here
is open source and can be downloaded as part of
GATE, with the exception of the entity pairing com-
ponent, which will be released shortly. We are cur-
rently preparing a UK research ethics committee ap-
plication, requesting permission to release our anno-
tated corpus.
Acknowledgements
CLEF is funded by the UK Medical Research Coun-
cil. We would like to thank the Royal Marsden
Hospital for providing the corpus, and our clinical
partners in CLEF for assistance in developing the
schema, and for gold standard annotation.
References
H. Cunningham, D. Maynard, K. Bontcheva, and
V. Tablan. 2002. GATE: A framework and graphi-
cal development environment for robust NLP tools and
applications. In Proceedings of the 40th Anniversary
Meeting of the Association for Computational Linguis-
tics, pages 168?175, Philadelphia, PA, USA, July.
C. Friedman, P. Alderson, J. Austin, J. Cimino, and
S. Johnson. 1994. A general natural-language text
processor for clinical radiology. Journal of the Amer-
ican Medical Informatics Association, 1(2):161?174,
March.
C. Grover, B. Haddow, E. Klein, M. Matthews,
L. Nielsen, R. Tobin, and X. Wang. 2007. Adapting
a relation extraction pipeline for the BioCreAtIvE II
task. In Proceedings of the BioCreAtIvE II Workshop
2007, Madrid, Spain.
U. Hahn, M. Romacker, and S. Schulz. 2002. MEDSYN-
DIKATE ? a natural language system for the ex-
traction of medical information from findings reports.
International Journal of Medical Informatics, 67(1?
3):63?74, December.
Y. Li, K. Bontcheva, and H. Cunningham. 2005.
SVM based learning system for information extrac-
tion. In Deterministic and statistical methods in ma-
chine learning: first international workshop, number
3635 in Lecture Notes in Computer Science, pages
319?339. Springer.
D. Lindberg, B. Humphreys, and A. McCray. 1993. The
Unified Medical Language System. Methods of Infor-
mation in Medicine, 32(4):281?291.
Y. Lussier, T. Borlawsky, D. Rappaport, Y. Liu, and
C. Friedman. 2006. PhenoGO: Assigning phenotypic
context to Gene Ontology annotations with natural lan-
guage processing. In Biocomputing 2006, Proceed-
ings of the Pacific Symposium, pages 64?75, Hawaii,
USA, January.
S. Pakhomov, J. Buntrock, and P. Duffy. 2005. High
throughput modularized NLP system for clinical text.
In Proceedings of the 43rd Annual Meeting of the
Association for Computational Linguistics (ACL?05),
interactive poster and demonstration sessions, pages
25?28, Ann Arbor, MI, USA, June.
A. Rector, J. Rogers, A. Taweel, D. Ingram, D. Kalra,
J. Milan, P. Singleton, R. Gaizauskas, M. Hepple,
D. Scott, and R. Power. 2003. CLEF ? joining up
healthcare with clinical and post-genomic research. In
Proceedings of UK e-Science All Hands Meeting 2003,
pages 264?267, Nottingham, UK.
T. Rindflesch and M. Fiszman. 2003. The interaction of
domain knowledge and linguistic structure in natural
language processing: interpreting hypernymic propo-
sitions in biomedical text. Journal of Biomedical In-
formatics, 36(6):462?477.
A. Roberts, R. Gaizauskas, M. Hepple, G. Demetriou,
Y. Guo, A. Setzer, and I. Roberts. 2008a. Seman-
tic annotation of clinical text: The CLEF corpus. In
Proceedings of Building and evaluating resources for
biomedical text mining: workshop at LREC 2008,
Marrakech, Morocco, May. In press.
A. Roberts, R. Gaizauskas, M. Hepple, and Y. Guo.
2008b. Combining terminology resources and statis-
tical methods for entity recognition: an evaluation.
In Proceedings of the Sixth International Conference
on Language Resources and Evaluation, LREC 2008,
Marrakech, Morocco, May. In press.
N. Sager, M. Lyman, C. Bucknall, N. Nhan, and L. Tick.
1994. Natural language processing and the representa-
tion of clinical data. Journal of the American Medical
Informatics Association, 1(2):142?160, March-April.
T. Wang, Y. Li, K. Bontcheva, H. Cunningham, and
J. Wang. 2006. Automatic extraction of hierarchical
relations from text. In The Semantic Web: Research
and Applications. 3rd European Semantic Web Con-
ference, ESWC 2006, number 4011 in Lecture Notes
in Computer Science, pages 215?229. Springer.
G. Zhou, J. Su, J. Zhang, and M. Zhang. 2005. Ex-
ploring Various Knowledge in Relation Extraction. In
Proceedings of the 43rd Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL?05), pages
427?434, Ann Arbor, MI, USA, June.
P. Zweigenbaum, B. Bachimont, J. Bouaud, J. Charlet,
and J-F. Boisvieux. 1995. A multi-lingual architec-
ture for building a normalised conceptual representa-
tion from medical language. In Proceedings of the An-
nual Symposium on Computer Applications in Medical
Care, pages 357?361, New York, NY, USA.
18
Proceedings of the Workshop on BioNLP: Shared Task, pages 95?98,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Tunable Domain-Independent Event Extraction in the MIRA Framework
Georgi Georgiev1 Kuzman Ganchev1 Vassil Momtchev1
georgi.georgiev@ontotext.com kuzman.ganchev@ontotext.com vassil.momtchev@ontotext.com
Deyan Peychev1 Preslav Nakov1 Angus Roberts2
deyan.peychev@ontotext.com preslav.nakov@ontotext.com a.roberts@dcs.shef.ac.uk
1 Ontotext AD, 135 Tsarigradsko Chaussee, Sofia 1784, Bulgaria
2 The Department of Computer Science, Regent Court 211 Portobello, Sheffield, S1 4DP. UK.
Abstract
We describe the system of the PIKB team
for BioNLP?09 Shared Task 1, which targets
tunable domain-independent event extraction.
Our approach is based on a three-stage clas-
sification: (1) trigger word tagging, (2) sim-
ple event extraction, and (3) complex event
extraction. We use the MIRA framework for
all three stages, which allows us to trade pre-
cision for increased recall by appropriately
changing the loss function during training. We
report results for three systems focusing on re-
call (R = 28.88%), precision (P = 65.58%),
and F1-measure (F1 = 33.57%), respectively.
1 Introduction
Molecular interactions have been the focus of inten-
sive research in the development of in-silico biology.
Recent developments like the Pathway and Interac-
tion Knowledge Base (PIKB) aim to make available
to the user the large semantics of the existing molec-
ular interactions data using massive knowledge syn-
dication. PIKB is part of LinkedLifeData1, a plat-
form for semantic data integration based on RDF2
syndication and lightweight reasoning.
Our system is based on the MIRA framework
where, by appropriately changing the loss function
on training, we can achieve any desirable balance
between precision and recall. For example, low pre-
cision with high recall would be appropriate in a
search that aims to identify as many potential candi-
dates as possible to be further examined by the user,
1http://www.linkedlifedata.com
2http://www.w3.org/RDF/
while high precision might be essential when adding
relations to a knowledge base. Such a tunable sys-
tem is practical for a variety of important tasks, in-
cluding but not limited to, populating extracted facts
in PIKB and reasoning on top of new and old data.
Our system is based on a three-stage classification
process: (1) trigger word tagging using a linear se-
quence model, (2) simple event extraction, and (3)
complex event extraction. In stage (2), we generate
relations between a trigger word and one or more
proteins, while in stage (3), we look for complex in-
teractions between simple events, trigger words and
proteins. We use MIRA for all three stages with a
loss function tuned for high recall.
2 One-best MIRA and Loss Functions
In what follows, xi will denote a generic input sen-
tence, and yi will be the ?gold? labeling of xi. For
each pair of a sentence xi and a labeling y, we com-
pute a vector-valued feature representation f(xi, y).
Given a weight vector w, the dot-product w ? f(x, y)
ranks the possible labelings y of x; we will denote
the top scoring labeling as yw(x). As with hidden
Markov models (Rabiner, 1989), yw(x) can be com-
puted efficiently for suitable feature functions using
dynamic programming.
The learning portion of our method requires find-
ing a weight vector w that scores the correct labeling
of the training data higher than any incorrect label-
ing. We used a one-best version of MIRA (Cram-
mer, 2004; McDonald et al, 2005) to choose w.
MIRA is an online learning algorithm that updates
the weight vector w for each training sentence xi
according to the following rule:
95
wnew = argmin
w
?w ? wold?
s.t. w ? f(xi, yi) ? w ? f(x, y?) ? L(yi, y?)
where L(yi, y) is a measure of the loss of using y in-
stead of the correct labeling yi, and y? is a shorthand
for ywold(xi). In case of a single constraint, this pro-
gram has a closed-form solution. The most straight-
forward and the most commonly used loss function
is the Hamming loss, which sets the loss of labeling
y with respect to the gold labeling yi as the number
of training examples where the two labelings dis-
agree. Since Hamming loss is not flexible enough
for targeted training towards recall or precision, we
use a number of task-specific loss functions (see
Sections 3 and 5 for details). We implemented one-
best MIRA and the corresponding loss functions in
an in-house toolkit called Edlin. Edlin provides gen-
eral machine learning architecture for linear models
and a framework with implementations of popular
learning algorithms including Naive Bayes, percep-
tron, maximum entropy, one-best MIRA, and condi-
tional random fields (CRF) among others.
3 Trigger Word Tagging
The training and the development abstracts were
first tokenized and split into sentences using maxi-
mum entropy models trained on the Genia3 corpora.
Subsequently, we trained several sequence taggers
in order to identify the trigger words in text. All
our experiments used the standard BIO encoding
(Ramshaw and Marcus, 1995) with different feature
sets and learning procedures. We focused on recall
since it determines the upper bound on the perfor-
mance of our final system. In our experiments, we
found that simultaneously identifying trigger words
and the event types they trigger yielded low recall;
thus, we settled on identifying trigger words in text
as one kind of entity, regardless of event types.
In our initial experiments, we used a CRF-
based sequence tagger (Lafferty et al, 2001), which
yielded R=43.51%. We further tried feature induc-
tion (McCallum, 2003) and second-order Markov
assumptions for the CRF, achieving 44.72% and
49.64% recall, respectively.
3http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA/home/wiki.cgi
Feature Set R P F1
Baseline (current word) 44.82 2.86 05.38
+ POS & char 3-gram 77.41 27.96 41.09
+ previous POS tag 79.77 29.32 42.88
+ lexicon (final tagger) 80.44 29.65 43.33
Table 1: Recall (R), precision (P), and F1-measure for the
trigger words tagger (in %s) on the development dataset
for different feature sets using MIRA training with false
negatives as a loss function.
Feature Sets
entity type of e1 and e2
words in e1 and e2
word bigrams in e1 and e2
POS of e1 and e2
words between e1 and e2
word bigrams between e1 and e2
POS between e1 and e2
distance between e1 and e2
distance between e1 and e2 in the dependency graph
steps in parse tree to get e1 and e2 in the same phrase
various combinations of the above features
Table 2: Our feature set for the MIRA classifier that pre-
dicts binary relations. Here e1 and e2 can be proteins
and/or trigger words.
Subsequently, we settled on using MIRA so that
we can trade-off precision for recall. In order to
boost recall, we defined the loss function as the num-
ber of false negative trigger chunks. Thus, a larger
loss update was made whenever the model failed to
discover a trigger word, while discovering spurious
trigger words was penalized less severely. We ex-
perimented with popular feature sets previously used
for named entity (McCallum and Li, 2003) and gene
(McDonald and Pereira, 2005) recognition including
orthographic, part-of-speech (POS), shallow parsing
and gazetteers. However, we found that only a small
number of them was really helpful; a summary is
presented in Table 1. In order to boost recall even
further, we prepared a gazetteer of trigger chunks
derived from the training data, and we extended it
with the corresponding WordNet synsets; we thus
achieved 80.44% recall for our final tagger.
4 Event Extraction
The input to our event extraction algorithm is a list
of trigger words and a list of genes or gene prod-
96
ucts (e.g., proteins); the output is a set of relations
as defined for Task 1. Our algorithm works in two
stages. First, we generate events corresponding to
relations between a trigger word and one or more
proteins (simple events); then we generate events for
relations between trigger words, proteins and simple
events (complex events). The two stages differ only
in the input data; thus, below we will describe our
system for the first stage only.
For each sentence, we considered all pairs of en-
tities (trigger words and proteins), and we used an
unstructured classifier to determine the relationship
for a given pair. These relationships encoded both
the type of event (e.g., binding, regulation) and enti-
ties? roles in that event (e.g., theme, cause); there
was also a special relationship for unrelated enti-
ties. We constructed labeled examples to train a
MIRA classifier using the training data provided by
the task organizers; n-ary relations were then recon-
structed from classifier?s predictions. The features
we used are summarized in Table 2: they are over
the words separating the two entities and their part-
of-speech tags. We further used some simple fea-
tures from syntactic phrases (OpenNLP4 parser) and
dependency parse trees (McDonald et al, 2005), ex-
tracted using parsers trained on Genia corpora.
After some initial experiments, we found that our
features were not sufficiently rich to allow us to learn
the relationships between proteins that are part of the
same event: we achieved a very low recall of about
20%. Consequently, we focused on the relationships
between a trigger word and a protein. Since the com-
petition stipulated that each trigger could be associ-
ated with only one type of event, we first chose the
event type for each trigger by selecting the protein-
label pair with the highest score. We then fixed the
event type for this trigger word, and we discarded all
proteins for which our classifier assigned a different
event type to the target trigger-protein pair. Finally,
we added to our output list all binary relations where
the role of the protein was theme.
For some event classes ? binding, regulation, pos-
itive regulation and negative regulation ? the output
of the binary classifier was further transformed so
that n-ary relations can be formed. However, the
way we did this was somewhat ad-hoc. For bind-
4http://opennlp.sourceforge.net
Event Class R P F1
Localization 10.92 82.61 19.29
Binding 7.20 39.68 12.20
Gene expression 30.47 74.58 43.26
Transcription 10.95 39.47 17.14
Protein catabolism 28.57 57.14 38.10
Phosphorylation 34.07 86.79 48.94
Event Total 21.52 68.68 32.77
Regulation 1.37 26.67 2.61
Positive regulation 1.12 25.58 2.14
Negative regulation 0.26 100.00 0.53
Regulation Total 0.97 27.12 1.87
Overall 10.84 64.13 18.55
Table 3: Our official results: for an erroneous submission.
ing events, we added a 3-ary relation between the
trigger, the highest scoring protein, and the second
highest scoring protein. For regulation events, we
added a 3-ary relation between the trigger and every
pair of proteins where one was a theme and the other
one was a cause. This aggressive addition of poten-
tial matches slightly reduced the overall precision,
but helped improve the recall for the final system.
5 Results and Discussion
Unfortunately, we made an error when making our
official submission, which resulted in low scores;
Table 3 shows the results for that submission.
The rest of this section describes the results and
the implementation for the system we intended to
submit. All reported results are for exact span
matches and were obtained using the online tool pro-
vided by the task organizers.
As stated in Section 4, we used a linear model
trained using one-best MIRA with ten runs over
the data for the event extraction system. We over-
sampled the unstructured training instances that cor-
responded to a relation so that they become roughly
equal in number to those that do not correspond to a
relation. Finally, we performed parameter averaging
as described in (Freund and Schapire, 1999). These
details turned out to be very important for the system
performance.
Table 4 shows the results for three different loss
functions that gave the best results in our experi-
ments. In describing the loss functions, we define
three different types of errors: (1) if the system cor-
rectly predicted that a relation should be present,
97
0-1 Loss High Recall High Precision
Event Class R P F1 R P F1 R P F1
Localization 33.33 69.05 44.96 39.08 48.23 43.17 25.86 86.54 39.82
Binding 38.33 32.60 35.23 46.97 24.51 32.21 24.50 37.95 29.77
Gene expression 57.89 65.72 61.56 64.82 53.49 58.61 47.65 76.27 58.65
Transcription 30.66 33.87 32.18 33.58 22.12 26.67 21.17 47.54 29.29
Protein catabolism 42.86 85.71 57.14 42.86 60.00 50.00 42.86 85.71 57.14
Phosphorylation 75.56 77.86 76.69 77.78 65.22 70.95 52.59 82.56 64.25
Event total 49.64 54.60 52.00 55.98 41.55 47.70 37.93 65.83 48.13
Regulation 0.00 0.00 0.00 2.41 22.58 4.35 0.00 0.00 0.00
Positive regulation 1.73 30.91 3.28 5.29 25.24 8.75 0.20 28.57 0.40
Negative regulation 0.53 40.00 1.04 1.06 23.53 2.02 0.26 100.00 0.53
Regulation Total 1.15 30.16 2.21 3.81 24.80 6.61 0.18 37.50 0.36
Overall 24.45 53.54 33.57 28.88 39.71 33.44 18.32 65.58 28.64
Table 4: Results (in %s) for one-best MIRA with different loss functions.
but guessed the wrong type, we call this a cross-
labeling; (2) a false positive occurs when the learner
guessed some relation while there should have been
none; (3) the reverse is a false negative. All loss
functions we considered had a cross-labeling loss of
1. The 0-1 loss also has a loss of 1 for false positives
and false negatives. The high-recall loss function
penalizes false positives with 0.1 and false negatives
with 5. The high-precision loss function penalizes
false negatives with 0.1 and false positives with 5.
The values 0.1 and 5 were chosen on the develop-
ment data, but were not optimized aggressively.
In conclusion, we have built three domain-
independent event extraction systems based on the
MIRA framework, each using a different loss func-
tion. Overall, they perform quite well and would
have been ranked second on precision5, and 6th on
recall, and 7th on F1-measure.
6 Future Work
After integrating domain knowledge, which should
improve the recall for complex events and should
boost the overall precision, we intend to transform
the system output into RDF and add it to the PIKB
repository. The required efforts discouraged us from
building a middle ontology between the BioNLP and
the PIKB data models, especially given the time lim-
itations for the present task competition. However,
we believe this is a promising direction, which we
plan to pursue in future work.
5Our official submission is second on precision as well.
Acknowledgments
The work reported in this paper was partially sup-
ported by the EU FP7 - 215535 LarKC.
References
Koby Crammer. 2004. Online Learning of Complex Cat-
egorial Problems. Ph.D. thesis, Hebrew University of
Jerusalem.
Yoav Freund and Robert E. Schapire. 1999. Large mar-
gin classification using the perceptron algorithm. In
Machine Learning, pages 277?296.
John Lafferty, Andrew McCallum, and Fernando Pereira.
2001. Conditional random fields: Probabilistic mod-
els for segmenting and labeling sequence data. In Pro-
ceedings of ICML. Morgan Kaufmann.
Andrew McCallum and Wei Li. 2003. Early results
for named entity recognition with conditional random
fields, feature induction and web-enhanced lexicons.
In Proceedings of CoNLL.
Andrew McCallum. 2003. Efficiently inducing features
of conditional random fields. In Proceedings of UAI.
Ryan McDonald and Fernando Pereira. 2005. Identify-
ing gene and protein mentions in text using conditional
random fields. BMC Bioinformatics, (Suppl 1):S6(6).
Ryan McDonald, Koby Crammer, and Fernando Pereira.
2005. Online large-margin training of dependency
parsers. In Proceedings of ACL. ACL.
Lawrence Rabiner. 1989. A tutorial on hidden Markov
models and selected applications in speech recogni-
tion. Proceedings of the IEEE, 77(2).
Lance Ramshaw and Mitch Marcus. 1995. Text chunk-
ing using transformation-based learning. In David
Yarovsky and Kenneth Church, editors, Proceedings
of the Third Workshop on Very Large Corpora. ACL.
98
