Combining Context Features by Canonical Belief Network for Chinese
Part-Of-Speech Tagging
Hongzhi Xu and Chunping Li
School of Software, Tsinghua University
Key Laboratory for Information System Security, Ministry of Education China
xuhz05@mails.tsinghua.edu.cn
cli@tsinghua.edu.cn
Abstract
Part-Of-Speech(POS) tagging is the essen-
tial basis of Natural language process-
ing(NLP). In this paper, we present an al-
gorithm that combines a variety of context
features, e.g. the POS tags of the words next
to the word a that needs to be tagged and the
context lexical information of a by Canoni-
cal Belief Network to together determine the
POS tag of a. Experiments on a Chinese cor-
pus are conducted to compare our algorithm
with the standard HMM-based POS tagging
and the POS tagging software ICTCLAS3.0.
The experimental results show that our algo-
rithm is more effective.
1 Introduction
Part-Of-Speech(POS) tagging is the essential basis
of Natural language processing(NLP). It is the pro-
cess in which each word is assigned to a correspond-
ing POS tag that describes how this word be used in
a sentence. Typically, the tags can be syntactic cat-
egories, such as noun, verb and so on. For Chinese
language, word segmentation must be done before
POS tagging, because, different from English sen-
tences, there is no distinct boundary such as white
space to separate different words(Sun, 2001). Also,
Chinese word segmentation and POS tagging can be
done at the same time(Ng, 2004)(Wang, 2006).
There are two main approaches for POS tagging:
rule-based and statistical algorithms(Merialdo,
1994). Rule based POS tagging methods ex-
tratct rules from training corpus and use these
rules to tag new sentences(Brill, 1992)(Brill,
1994). Statistic-based algorithms based on Belief
Network(Murphy, 2001) such as Hidden-Markov-
Model(HMM)(Cutting, 1992)(Thede, 1999), Lex-
icalized HMM(Lee, 2000) and Maximal-Entropy
model(Ratnaparkhi, 1996) use the statistical infor-
mation of a manually tagged corpus as background
knowledge to tag new sentences. For example, the
verb is mostly followed by a noun, an adverb or
nothing, so if we are sure that a word a is a verb,
we could say the word b following a has a large
probability to be a noun. This could be helpful
specially when b has a lot of possible POS tags or it
is an unknown word.
Formally, this process relates to Pr(noun|verb),
Pr(adverb|verb) and Pr(nothing|verb), that can
be estimated from the training corpus. HMM-
based tagging is mainly based on such statistical
information. Lexicalized HMM tagging not only
considers the POS tags information to determine
whether b is noun, adverb or nothing, but also
considers the lexical information a itself. That
is, it considers the probabilities Pr(noun|a, verb),
Pr(adverb|a, verb) and Pr(nothing|a, verb) for
instance. Since combining more context informa-
tion, Lexicalized HMM tagging gets a better perfor-
mance(Lee, 2000).
The main problem of Lexicalized HMM is that
it suffers from the data sparseness, so parameter
smoothing is very important. In this paper, we
present a new algorithm that combines several con-
text information, e.g. the POS tags information and
lexical information as features by Canonical Belief
Network(Turtle, 1991) to together determine the tag
907
of a new word. The experiments show that our algo-
rithm really performs well. Here, we don?t explore
Chinese word segmentation methods, and related in-
formation can be found in(Sun, 2001).
The rest of the paper is organized as follows. In
section 2 and section 3, we describe the standard
HMM-based tagging and Lexicalized HMM tagging
respectively which are relevant to our algorithm. In
section 4, we describe the Belief Network as a pre-
liminary. In section 5, we present our algorithm that
is based on Canonical Belief Network. Section 6 is
the experiments and their results. In section 7, we
have the conclusion and the future work.
2 Standard Hidden Markov Model
The problem of POS tagging can be formally de-
fined as: given an observation(sentence) w =
{w1, w2, ..., wT } and a POS tag set TS =
{t1, t2, ..., tM}, the task is to find a tag sequence
t = {t1, t2, ..., tT }, where ti ? TS, that is the most
possible one to explain the observation. That is to
find t to maximize the probability Pr(t|w). It can
be rewritten by Bayesian rule as follows.
Pr(t|w) = Pr(w|t)? Pr(t)Pr(w)
As for any sequence t, the probability Pr(w) is con-
stant, we could ignore Pr(w). For Pr(t), it can be
decomposed by the chain rule as follows.
Pr(t) = Pr(t1, t2, ..., tT )
= Pr(t1)? Pr(t2|t1)? Pr(t3|t1, t2)?
...? Pr(tT |t1, t2, ..., tT?1)
Through this formula, we could find that the calcu-
lation is impossible because of the combination ex-
plosion of different POS tags. Generally, we use a
n-gram especially n = 2 model to calculate Pr(t)
approximately as follows.
Pr(t) = Pr(t1|t0)? Pr(t2|t1)? Pr(t3|t2)?
...? Pr(tT |tT?1)
where t0 is nothing. For Pr(w|t), with an indepen-
dent assumption, it can be calculated approximately
as follows.
Pr(w|t) = Pr(w1|t1)? Pr(w2|t2)? Pr(w3|t3)
...? Pr(wT |tT )
Usually, the probability Pr(ti|ti?1) is called tran-
sition probability, and Pr(wi|ti) is called the emis-
sion probability. They both can be estimated from
the training set. This means that the tag ti of word
wi is only determined by the tag ti?1 of word wi?1.
So, we could find the best sequence through a for-
ward(left to right) process.
If we state all possible POS tags(stats) of each
word and connect all possible ti?1 with all possi-
ble ti and each edge is weighted by Pr(ti|ti?1),
we could get a Directed Acyclic Graph(DAG). The
searching process(decoding) that is involved in find-
ing t that maximizes Pr(t|w) can be explained as
finding the path with the maximal probability. For
this sub task, Viterbi is an efficient algorithm that
can be used(Allen, 1995).
3 Lexicalized Hidden Markov Model
Lexicalized HMM is an improvement to the stan-
dard HMM. It substitutes the probability Pr(ti|ti?1)
with Pr(ti|ti?J,i?1, wi?L,i?1), and the probability
Pr(wi|ti) with Pr(wi|ti?K,i, wi?I,i?1). In other
words, the tag of word wi is determined by the tags
of the J words right before wi and L words right be-
fore wi. It uses more context information of wi to
determine its tag.
However, it will suffer from the data sparse-
ness especially when the values of J , L, K and
I are large, which means it needs an explosively
larger training corpus to get a reliable estimation of
these parameters, and smoothing techniques must be
adopted to mitigate the problem. Back-off smooth-
ing is used by Lexicalized HMM. In the back-off
model, if a n-gram occurs more than k times in
training corpus, then the estimation is used but dis-
counted, or the estimation will use a shorter n-gram
e.g. (n-1)-gram estimation as a back-off probabil-
ity. So, it is a recursive process to estimate a n-gram
parameter.
4 Belief Network
Belief Network is a probabilistic graphical model,
which is also a DAG in which nodes represent
random variables, and the arcs represent condi-
tional independence assumptions. For example, the
probability Pr(A,B) = Pr(A) ? Pr(B|A) can
be depicted as Figure 1(a), and if we decompose
908
%$&
D
%$&
E
%
$
$
%
F G
Figure 1: Some Belief Networks.
Pr(A,B) = Pr(B) ? Pr(A|B), it can be de-
picted as Figure 1(b). Similarly, the probability
Pr(A,B,C) = Pr(A)? Pr(B|A)? Pr(C|A,B)
can be depicted as Figure 1(c).
As we have analyzed above, such decomposition
would need us to estimate a large amount of pa-
rameters. In the belief network, a conditional in-
dependence relationship can be stated as follows: a
node is independent of its ancestors given its par-
ents, where the ancestor/parent relationship is with
respect to some fixed topological ordering of the
nodes. For example, if we simplify the graph Figure
1(c) to graph Figure 1(d), it is equivalent to the de-
composition: Pr(A,B,C) = Pr(A)?Pr(B|A)?
Pr(C|B), which is actually the same as that of
HMM. More details about Belief Network can found
in(Murphy, 2001).
5 Canonical Belief Network Based
Part-Of-Speech Tagging
5.1 Canonical Belief Network
Canonical Belief Network was proposed by Turtle
in 1991(Turtle, 1991), and it was used in informa-
tion retrieval tasks. Four canonical forms are pre-
sented to combine different features, that is and, or,
wsum and sum to simplify the probability combi-
nation further. With the and relationship, it means
that if a node in a DAG is true, then all of its parents
must be true. With the or relationship, it means that
if a node in a DAG is true, then at least one of its par-
ents is true. With the wsum relationship, it means
that if a node in a DAG is true, it is determined by all
of its parents and each parent has a different weight.
With the sum relationship, it means that if a node in
a DAG is true, it is determined by all of its parents
and each parent has an equal weight.
For example, we want to evaluate the probabil-
ity Pr(D|A) or Pr(D = true|A = true), and
$
&%
'
DQG
D
$
&%
'
RU
E
$
&%
'
ZVXP
F
$
&%
'
VXP
G
Figure 2: Canonical Belief Networks for
Pr(A,B,C,D).
node D has two parents B and C, we could use
the four canonical forms to evaluate Pr(D|A) as
shown in Figure 2. Suppose that Pr(B|A) = p1
and Pr(C|A) = p2, with the four canonical form
and, or, wsum and sum, we could get the follow-
ing estimations respectively.
Pand(D|A) = p1 ? p2
Por(D|A) = 1? (1? p1)? (1? p2)
Pwsum(D|A) = w1p1 + w2p2
Psum(D|A) = (p1 + p2)/2
The standard Belief Network actually supposes that
all the relationships are and. However, in real world,
it is not the case. For example, we want to evaluate
the probability that a person will use an umbrella,
and there are two conditions that a person will use
it: raining or a violent sunlight. If we use the stan-
dard Belief Network, it is impossible to display such
situation, because it could not be raining and sunny
at the same time. The or relationship could easily
solve this problem.
5.2 Algorithm Description
Definition: A feature is defined as the context in-
formation of a tag/word, which can be POS tags,
words or both. For example, {Ti?J , ..., Ti?1} is a
feature of tag ti, {Ti?J , ..., Ti} is a feature of word
wi, {Ti?J , ..., Ti?1,Wi?L, ...,Wi?1} is a feature of
tag ti, {Ti?K , ..., Ti,Wi?I , ...,Wi?1} is a feature of
word wi.
In our algorithm, we select 6 features for tag ti,
and select 2 features for word wi, which are shown
in Table 1. We can see that f1t , f2t and f3t are actually
the n-gram features used in HMM, f4t , f5t and f6t are
actually features used by lexicalized HMM.
We adopt the canonical form or to combine them
as shown in Figure 3, and use the canonical form
909
Features
f1t : Ti?3, Ti?2, Ti?1
f2t : Ti?2, Ti?1
ti f3t : Ti?1
f4t : Ti?3, Ti?2, Ti?1, Wi?3, Wi?2, Wi?1
f5t : Ti?2, Ti?1, Wi?2, Wi?1
f6t : Ti?1, Wi?1
wi f1w: Ti?1, Ti
f2w: Ti
Table 1: Features used for ti and wi.
and to combine features of ti and wi. Because we
think that the POS tag of a new word can be de-
termined if any one of the features can give a high
confidence or implication of a certain POS tag. The
probabilities Pr(f it |ti?1), i = 1, ..., 6. are all 1,
which means that all the features in the Canonical
Belief Network are considered to estimate the tag
ti of word wi when we have already estimated the
tag ti?1 of word wi?1. So, the transition probability
could be calculated as follows.
ptransi?1,i = 1?
6?
j=1
[1? Pr(ti|f jt )]
In the same way, the probabilities Pr(f iw|ti), i =
1, 2. are all 1. The emission probability could be
calculated as follows.
pomiti = 1?
2?
j=1
[1? Pr(wi|f jw)]
Let?s return to the POS tagging problem which
needs to find a tag sequence t that maximizes the
probability Pr(t|w), given a word sequence w de-
fined in Section 2. It is involved in evaluating two
probabilities Pr(t) and Pr(w|t). With the Canon-
ical Belief Network we just defined, they could be
calculated as follows.
Pr(t) = ?Ti=1 ptransi?1,i
Pr(w|t) = ?Ti=1 pomiti
Pr(w, t) = Pr(t)? Pr(w|t)
The canonical form or would not suffer from
the data sparseness even though it refers to 4-
gram, because if a 4-gram feature(f1t for example)
doesn?t appear in the training corpus, the probability
IW
RU
IWIW IWIW IW
WL
WL
RU
IZ
ZL
IZ
D E
WL
Figure 3: Canonical Belief Networks used in our al-
gorithm.
Pr(ti|f1t ) is estimated as zero, which means the fea-
ture contributes nothing to determine the probability
that word wi gets a tag ti, which is actually deter-
mined by a lower n-grams. Cases are the same for
3-gram, 2-gram and so on. In a special case, when a
4-gram (f4t for example) appears in the training cor-
pus and appears only once, the probability Pr(ti|f1t )
will be 1, which means that the sentence or phrase
we need to tag may have appeared in the training
corpus, so we can tag the sentence or phrase with
reference to the appeared sentence or phrase in the
training corpus. This is an intuitional comprehen-
sion of our algorithm and its motivation.
Decoding: The problem of using high n-gram
is the combination explosion especially for high
grams. For example, consider the feature , suppose
one word has 3 possible tags on average, then we
have to evaluate 33 = 27 cases for f1t , further, dif-
ferent features could get different combinations and
the number of combinations will be 272?92?32 =
531441. To solve the problem, we constrain all fea-
tures to be consistent. For example, the tag ti?1 of
feature f1t must be same as that of feature f2t , f3t ,
f4t , f5t and f6t at one combination. The following
features are not consistent, because the ti?1 in f1t is
V BP , while the ti?1 in f4t is NN .
f1t = JJ,NNS, V BP
f4t = JJ,NNS,NN, little, boys, book
This will decrease the total combination to 33 = 27.
We use a greedy search scheme that is based on the
classic decoding algorithm Viterbi. Suppose that the
Viterbi algorithm has reached the state ti?1, to cal-
culate the best path from the start to ti, we only use
the tags on the best path from the start to ti?1 to cal-
culate the probability. This decreases the total com-
910
bination to 3(the number of possible tags of ti?1),
which is the same as that of standard HMM.
6 Experiments
Dataset: We conduct our experiments on a Chinese
corpus consisting of all news from January, 1998 of
People?s Daily, tagged with the tag set of Peking
University(PKU), which contains 46 POS tags1. For
the corpus, we randomly select 90% as the training
set and the remaining 10% as the test set. The cor-
pus information is shown in Table 2, where unknown
words are the words that appear in test set but not in
training set. The experiments are run on a machine
with 2.4GHZ CPU, and 1GB memory.
Training set Test set
Words 1021592 112321
Sentences 163419 17777
Unknow words 2713
Table 2: Chinese corpus information.
Unknown Words: In our experiments, we first
store all the words with their all possible POS tags
in a dictionary. So, our algorithm gets all possible
tags of a word through a dictionary. As for the word
in the test set that doesn?t appear in the training set,
we give the probability Pr(wi|f jw) value 1, with all
j. This processing is quite simple, however, it is
enough to observe the relative performances of dif-
ferent POS taggers.
For Chinese word segmentation, we use the seg-
mentation result of ICTCLAS3.02. The segmenta-
tion result is shown in Table 3. Sen-Prec is the ratio
of the sentences that are correctly segmented among
all sentences in the test set.
Precision Recall F1 Sen-Prec
0.9811 0.9832 0.9822 0.9340
Table 3: Segmentation Result by ICTCLAS.
Open Test: We compare the POS tagging per-
formance of our algorithm with the standard HMM,
1http://icl.pku.edu.cn/Introduction/corpustagging.htm
2ICTCLAS3.0 is a commercial software developed by Insti-
tute of Computing Technology, Chinese Academy of Science,
that is used for Chinese word segmentation and POS tagging.
and ICTCLAS3.0. The experimental result is shown
in Table 4. Prec-Seg is the POS tagging precision
on the words that are correctly segmented. Prec-
Sen is the ratio of the sentences that are correctly
tagged among all sentences in the test set. Prec-Sen-
Seg is the ratio of sentences that are correctly tagged
among the sentences that are correctly segmented.
With the experiments, we can see that, our algo-
rithm always gets the best performance. The ICT-
CLAS3.0 doesn?t perform very well. However, this
is probably because of that the tag set used by ICT-
CLAS3.0 is different from that of PKU. Even though
it provides a mapping scheme from their tags to
PKU tags, they may be not totally consistent. The
published POS tagging precision of ICTCLAS3.0 is
94.63%, also our algorithm is a little better. This has
proved that our algorithm is more effective for POS
tagging task.
ICTCLAS HMM CBN
Precision 0.9096 0.9388 0.9465
Recall 0.9115 0.9408 0.9485
F1 0.9105 0.9398 0.9475
Prec-Seg 0.9271 0.9569 0.9647
Prec-Sen 0.6342 0.7404 0.7740
Prec-Sen-Seg 0.6709 0.7927 0.8287
Table 4: Open test comparison result on Chinese
corpus.
Close Test: As we have analyzed above in Sec-
tion 5.2 that our algorithm takes advantage of more
information in the training set. When a sentence or a
phrase appears in the training set, it will help a lot to
tag the new sentence correctly. To test whether this
case really happens, we conduct a new experiment
that is the same as the first one except that the test
set is also added to the training set. The experimen-
tal result is shown in Table 5. We can see that the
performance of our algorithm is greatly improved,
while the HMM doesn?t improve much, which fur-
ther proves our analysis.
Even though our algorithm gives a satisfying per-
formance, it may be able to be improved by adopt-
ing smoothing techniques to take advantage of more
useful features, e.g. to make the probabilities such
as Pr(ti|f1t ), Pr(ti|f2t ) not be zero. In addition, the
adoption of techniques to deal with unknown words
911
ICTCLAS HMM CBN
Precision 0.9096 0.9407 0.9658
Recall 0.9115 0.9427 0.9678
F1 0.9105 0.9417 0.9668
Prec-Seg 0.9271 0.9588 0.9843
Prec-Sen 0.6342 0.7476 0.8584
Prec-Sen-Seg 0.6709 0.8004 0.9191
Table 5: Close test comparison result on Chinese
corpus.
and techniques to combine with rules may also im-
prove the performance of our algorithm. If we have
a larger training corpus, it may be better to remove
some confusing features such as f3t and f2w, because
they contain weak context information and this is
why a higher n-gram model always performs better
than a lower n-gram model when the training corpus
is large enough. However, this should be validated
further.
7 Conclusion and Future Work
In this paper, we present a novel algorithm that
combines useful context features by Canonical Be-
lief Network to together determine the tag of a new
word. The ?or? node can allow us to use higher n-
gram model although the training corpus may be not
sufficient. In other words, it can overcome the data
sparseness problem and make use of more informa-
tion from the training corpus. We conduct experi-
ments on a Chinese popular corpus to evaluate our
algorithm, and the results have shown that it is pow-
erful even in case that we don?t deal with the un-
known words and smooth the parameters.
We think that our algorithm could also be used
for tagging English corpus. In addition, we only ex-
tract simple context information as features. We be-
lieve that there exists more useful features that can
be used to improve our algorithm. For example, the
syntax analysis could be combined as a new fea-
ture, because a POS sequence may be illegal even
though it gets the maximal probability through our
algorithm. Yet, these will be our future work.
Acknowledgement This work was supported by
Chinese 973 Research Project under grant No.
2002CB312006.
References
Adwait Ratnaparkhi. 1996. A Maximum Entropy Model
for Part-Of-Speech Tagging. In Proc. of the Empiri-
cal Methods in Natural Language Processing Confer-
ence(EMNLP?96), 133-142.
Bernard Merialdo. 1994. Tagging English Text with
a Probabilistic Model. Computational Linguistics,
20(2):155?172.
Doug Cutting, Julian Kupied, Jan Pedersen and Penelope
Sibun. 1992. A Practical part-of-speech tagger. In
Proceedings of the 3rd Conference on Applied Natural
Language Processing(ANLP?92), 133-140.
Eric Brill. 1992. A simple rule-based part of speech tag-
ger. In Proc. of the 30th Conference on Applied Com-
putational Linguistics(ACL?92), Trento, Italy, 112-
116.
Eric Brill. 1994. Some Advances in Transformation-
Based Part of Speech Tagging. In Proc. of
the 12th National Conference on Artificial Intelli-
gence(AAAI?94), 722-727.
Howard Turtle and W. Bruce Croft. 1991. Evaluation of
an Inference Network-Based Retrieval Model. ACM
Transactions on Information Systems, 9(3):187-222.
Hwee Tou Ng and Jin Kiat Low. 2004. Chinese Part-
of-Speech Tagging: One-at-a-Time or All-at-Once?
Word-Based or Character-Based?. In Proc. of the Em-
pirical Methods in Natural Language Processing Con-
ference(EMNLP?04).
James Allen. 1995. Natural Language Understanding.
The Benjamin/Cummings Publishing Company.
Kevin P. Murphy. 2001. An introduction to graphical
models. Technical report, Intel Research Technical
Report.
Maosong Sun and Jiayan Zou. 2001. A critical appraisal
of the research on Chinese word segmentation(In Chi-
nese). Contemporary Linguistics, 3(1):22-32.
Mengqiu Wang and Yanxin Shi. 2006. Using Part-of-
Speech Reranking to Improve Chinese Word Segmen-
tation. In Proc. of the 5th SIGHAN Workshop on Chi-
nese Language Processing, 205-208.
Sang-Zoo Lee, Jun-ichi Tsujii and Hae-Chang Rim.
2000. Lexicalized Hidden Markov Models for Part-of-
Speech Tagging. In Proc. of 18th International Con-
ference on Computational Linguistics(COLING?00),
Saarbrucken, Germany, 481-487.
Scott M. Thede and Mary P. Harper. 1999. A Second-
Order Hidden Markov Model for Part-of-Speech Tag-
ging. In Proc. of the 37th Conference on Applied
Computational Linguistics(ACL?99), 175-182.
912
Proceedings of the First Workshop on Applying NLP Tools to Similar Languages, Varieties and Dialects, pages 1?10,
Dublin, Ireland, August 23 2014.
   Corpus-based Study and Identification of Mandarin Chinese Light 
Verb Variations 
Chu-Ren Huang1      Jingxia Lin2     Menghan Jiang1      Hongzhi Xu1 
1Department of CBS, The Hong Kong Polytechnic University 
2Nanyang Technological University 
churen.huang@polyu.edu.hk, jingxialin@ntu.edu.sg, 
menghan.jiang@connect.polyu.hk, hongz.xu@gmail.com 
   
Abstract 
When PRC was founded on mainland China and the KMT retreated to Taiwan in 1949, the 
relation between mainland China and Taiwan became a classical Cold War instance. Neither 
travel, visit, nor correspondences were allowed between the people until 1987, when 
government on both sides started to allow small number of Taiwan people with relatives in 
China to return to visit through a third location. Although the thawing eventually lead to 
frequent exchanges, direct travel links, and close commercial ties between Taiwan and 
mainland China today, 38 years of total isolation from each other did allow the language use to 
develop into different varieties, which have become a popular topic for mainly lexical studies 
(e.g., Xu, 1995; Zeng, 1995; Wang & Li, 1996). Grammatical difference of these two variants, 
however, was not well studied beyond anecdotal observation, partly because the near identity 
of their grammatical systems. This paper focuses on light verb variations in Mainland and 
Taiwan variants and finds that the light verbs of these two variants indeed show distributional 
tendencies. Light verbs are chosen for two reasons: first, they are semantically bleached hence 
more susceptible to changes and variations. Second, the classification of light verbs is a 
challenging topic in NLP. We hope our study will contribute to the study of light verbs in 
Chinese in general. The data adopted for this study was a comparable corpus extracted from 
Chinese Gigaword Corpus and manually annotated with contextual features that may 
contribute to light verb variations. A multivariate analysis was conducted to show that for each 
light verb there is at least one context where the two variants show differences in tendencies 
(usually the presence/absence of a tendency rather than contrasting tendencies) and can be 
differentiated. In addition, we carried out a K-Means clustering analysis for the variations and 
the results are consistent with the multivariate analysis, i.e. the light verbs in Mainland and 
Taiwan indeed have variations and the variations can be successfully differentiated. 
1 Introduction: Language Variations in the Chinese Context 
Commonly dichotomy of language and dialect is not easily maintained in the context of Chinese 
language(s). Cantonese, Min, Hakka, and Wu are traditionally referred to as dialects of Chinese but 
are mutually unintelligible. However, they do share a common writing system and literary and textual 
tradition, which allows speakers to have a shared linguistic identity. To overcome the mutual 
unintelligibility problem, a variant of Northern Mandarin Chinese, is designated as the common 
language about a hundred years ago (called ??? Putonghua ?common language? in Mainland 
China, and ??  Guoyu ?national language? in Taiwan). Referred to as Mandarin or Mandarin 
Chinese, or simply Chinese nowadays, this is the one of the most commonly learned first or second 
languages in the world now. However, not unlike English, with the fast globalization of the Chinese 
language, both the term ?World Chineses? and the recognition that there are different variants of 
Chinese emerged. In this paper, we studied two of the most important variants of Chinese, Mainland 
Mandarin and Taiwan Mandarin. 
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer 
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 
1
1.1 Variations between Mainland and Taiwan Mandarin: Previous studies 
The lexical differences between Mainland and Taiwan Mandarin have been the focus of research in 
Chinese Linguistics in the recent years. A number of studies were carried out on lexical variations 
between these two variants of Mandarin Chinese, including variations in the meanings of the same 
word or using different words to express the same meaning (e.g., Xu, 1995; Zeng, 1995; Wang,1996). 
Some dictionaries also list the lexical differences between Mainland Mandarin and Taiwan Mandarin 
(e.g., Qiu, 1990; Wei & Sheng, 2000).  
By contrast, only a few of such studies were corpus driven (e.g. Hong and Huang 2008, 2013; 
Huang and Lin, 2013), and even few studies have been done on the grammatical variations of 
Mainland and Taiwan Chinese. Huang et al. (2013), the only such study based on comparable corpora 
so far, suggested that the subtlety of the underlining grammatical variations of these two dialectal 
variants at early stage of divergence may have contributed to the challenge as well as scarcity of 
previous studies.  
1.2 Light Verbs in Light Verb Variations 
The study of English light verb constructions (LVCs) (e.g., take a look, make an offer) has been an 
important topic in linguistics (Jespersen, 1965; Butt and Geuder, 2003; among others) as well as in 
Computational Linguistics (Tu and Dan, 2011; Nagy et al., 2013; Hwang et al., 2010; among others). 
Identification of LVCs is a fundamental crucial task for Natural Language Processing (NLP) 
applications, such as information retrieval and machine translation. For example, Tu and Dan (2011) 
proposed a supervised learning system to automatically identify English LVCs by training with groups 
of contextual or statistical features. Nagy et al. (2013) introduced a system that enables the full 
coverage identification of English LVCs in running context by using a machine leaning approach.  
However, little work has been done to identify Chinese LVCs, especially between different 
variants of Chinese (cf. Hwang et al., 2010). Chinese LVCs are similar to English LVCs in the sense 
that the light verb itself is semantically bleached and does not contain any eventive or contentive 
information, so the predicative information of the construction mainly comes from the complement 
taken by the light verb (e.g., Zhu, 1985; Zhou, 1987; Cai, 1982). For instance, ?? jinxing originally 
meant ?move forward/proceed?, but in an LVC such as???? jinxing taolun proceed discuss ?to 
discuss?, ?? jinxing only contributes aspectual information whereas the core meaning of the LVC 
comes from the complement ?? taolun ?discuss?. Chinese also differs from English in that many of 
the Chinese light verbs have similar usages and thus are often interchangeable, e.g., all the five light 
verbs ?? congshi, ? gao, ?? jiayi, ?? jinxing, and ? zuo can take ?? yanjiu ?do research? 
as their complement and form a LVC. But Huang et al. (2013) also observed that differences in 
collocation constraints can sometimes be found between different variants of Mandarin Chinese. For 
instance, constructions like ???? jinxing tou-piao proceed cast-ticket ?to cast votes?, where the 
complement is in the V(erb)-O(bject) form, usually can only be found in Taiwan Mandarin. Hence, 
Chinese LVCs are challenging for both linguistic studies and computational applications in two 
aspects: (a) to identify collocation constraints of the different light verbs in order to automatically 
classify and predict their uses in context, and (b) to identify the collocation constraints of the same 
light verb in order to differentiate and predict the two Chinese variants based on the use of such light 
verbs. The first issue has been explored in Lin et al. (2014): by analyzing Mainland and Taiwan 
Mandarin data extracted from comparable corpora with statistical and machine learning approaches, 
the authors find the five light verbs?? congshi, ? gao, ?? jiayi, ?? jinxing, and ? zuo can 
be reliably differentiated from each other in each variety. But to the best of our knowledge, there has 
been no previous computational study on modeling the light verb variations, or other syntactic 
variations of Chinese dialects or variants of the same dialect. Therefore, this paper builds on the study 
of Lin et al. (2014) and will adopt a comparable corpus driven approach to model light verb variations 
in Mainland and Taiwan Mandarin. 
2 Data and annotation 
Our study focuses on five light verbs, ?? jiayi, ?? jinxing, ?? congshi, ? gao and ? zuo 
(these words literally meant ?proceed?, ?inflict?, ?engage?, ?do?, and ?do? respectively). These five are 
2
chosen for two reasons. First, they are the most frequently used light verbs in Mandarin Chinese (Diao, 
2004); second, although the definition of Chinese light verbs is still debatable, these five are 
considered the most typical light verbs in most previous studies.   
The data for this study was extracted from the Annotated Chinese Gigaword Corpus (Huang, 2009) 
maintained by LDC which contains over 1.1 billion Chinese words, consisting of 700 million 
characters from Taiwan Central News Agency (CNA) and 400 million characters from Mainland 
Xinhua News Agency (XNA). For each of the five light verbs, 400 sentences were randomly selected, 
half from the Mainland XNA corpus and the other half from the Taiwan CNA Corpus, which results in 
2,000 sentences in total.  
Previous studies (Zhu, 1985; Zhou, 1987; Cai, 1982; Huang et al., 2013; among others) have 
proposed several syntactic and semantic features to compare and identify the similarities and 
differences among light verbs. For example, while Taiwan ?? congshi can take informal or 
semantically negative event complements such as ??? xingjiaoyi ?sexual trade?, Mainland ?? 
congshi is rarely found with such complements (Huang et al. 2013). 
In our study, we selected 11 features covering both syntactic and semantic features which may help 
to identify light verb variations, as in Table 1. All 2,000 sentences with light verbs were manually 
annotated with the 11 features. The annotator is a trained expert on Chinese linguistics. All ambiguous 
cases were discussed with another two experts in order to reach an agreement (the features and 
annotation were the same with Lin et al. (2014)). 
 
3 Modelling and Predicting Two Variants 
We carried out both a multivariate analysis and machine learning algorithm to explore the possible 
differences existing between Mainland and Taiwan Mandarin light verbs. Our analysis shows that for 
each light verb, there is at least one context where the two variants of Mandarin show differences in 
usage tendencies and thus can be differentiated, although the differences more often lie in the 
presence/absence of a tendency rather than complementary distribution.   
3.1 Multivariate Analysis of Light Verb Variations 
As introduced in Section 1, the five or some of the five light verbs sometimes can be interchangeably 
used in both Mainland and Taiwan Mandarin. This indicates that the interchangeable light verbs share 
some features. In other words, it is unlikely that a particular feature is preferred by only one light verb 
and thus differentiates the verb from the others. This is also proved in Lin et al. (2014). For instance, 
their study finds both Mainland and Taiwan ?? congshi and ? gao significantly prefer nominal 
complements (POS.N). Therefore, to better explore the light verb differences in the two variants, we 
adopt a multivariate analysis for this study.  
The multivariate analysis we used is polytomous logistic regression (Arppe 2008, cf. Han et al. 
2013, Bresnan et al. 2007), and the tool we used is the Polytomous() function in the Polytoumous 
package in R (Arppe 2008). The polytomous logistic regression is an extension of standard logistic 
regression; it calculates the odds of the occurrence of a particular light verb when a particular feature 
is present, with all other features being equal (Arppe, 2008). In addition, it also allows for 
simultaneous estimation of the occurrence probability of all the five light verbs. 
Before we discuss the light verb variations based on multivariate analysis, we will show that the 
polytomous multivariate model adopted is reliable for our study. Table 2 presents the probability 
estimates of Mainland and Taiwan light verbs calculated by the model. The results indicate that the 
overall performance of the model is good: the most frequently predicted light verb (in each column) 
corresponds to the light verb that actually occurs in the data (in each row) (see the numbers in bold).  
In addition, the recall, precision, and F-measure of the estimates given in Table 3 show that each 
light verb in each variant can be successfully identified with a F-score better than chance (0.2), while 
the performance varies from light verb to light verb, which is thus consistent with the results in Lin et 
al. (2014). The only exception is ? gao in Mainland Mandarin, but the low F-score of ? gao (0.14) is 
consistent with the linguistic observation that this verb is rarely used as a light verb in Mainland 
Mandarin. More detailed information of the factors that can distinguish the five light verbs in each 
3
variant can also be found in Table 4. In the following of this section, we focus on the variations of 
each light verb in Mainland and Taiwan Mandarin.  
 
 
 
Feature ID Explanation Values (example) 
1. OTHERLV Whether a light verb co-
occurs with another light 
verbs 
Yes (?????? kaishi jinxing taolun Start proceed 
discuss ?start to discuss?) 
No (???? jinxing taolun proceed discuss ?to 
discuss?) 
2. ASP 
 
Whether a light verb is 
affixed with an aspectual 
marker (e.g., perfective ? 
le, durative ? zhe, 
experential ? guo) 
ASP.le (????? jinxing-le zhandou ?fighted?) 
ASP.zhe ( ? ? ? ? ?  jinxing-zhe zhandou ?is 
fighting?) 
ASP.guo (????? jinxing-guo zhandou ?fighted?) 
ASP.none (???? jinxing zhandou ?fight?) 
3. EVECOMP Event complement of a light 
verb is in subject position 
Yes (??????? bisai zai xuexiao jinxing game at 
school proceed ?The game was held at the school?) 
No (??????? zai xuexiao jinxing bisai at 
school proceed game ?the game was held at the school?)  
4. POS 
 
The part-of-speech of the 
complement taken by a light 
verb  
Noun (???? jinxing zhanzheng proceed fight ?to 
fight?) 
Verb (???? jinxing zhandou proceed fight ?to 
fight?) 
5. ARGSTR 
 
The argument structure of 
the complement of a light 
verb, i.e. the number of 
arguments (subject and/or 
objects) that can be taken by 
the complement  
One (???? jinxing zhandou proceed fight ?to fight?) 
Two (???? jinxing piping proceed criticize ?to 
criticize?)  
Zero (???? jinxing zhanzheng proceed fight ?to 
fight?) 
6. VOCOMP Whether the complement of 
a light verb is in the V(erb)-
O(bject) form  
Yes (???? jinxing tou-piao proceed cast-ticket ?to 
vote?) 
No (???? jinxing zhan-dou proceed fight-fight ?to 
fight?) 
7. DUREVT Whether the event denoted 
by the complement of a light 
verb is durative 
Yes (???? jinxing zhandou proceed fight-fight ?to 
fight?) 
No (???? jiayi jujue inflict reject ?to reject?)  
8. FOREVT Whether the event denoted 
by the complement of a light 
verb is formal or official 
Yes (?????? jinxing guoshi fangwen proceed 
state visit ?to pay a state visit?) 
No (???? zuo xiao maimai do small business ?run a 
small business?)  
9. PSYEVT Whether the event denoted 
by the complement of a light 
verb is mental or 
psychological activity 
Yes (???? jiayi fanxing inflict retrospect ?to 
retrospect?) 
No (???? jiayi diaocha inflict investigate ?to 
investigate?)  
10. INTEREVT Whether the event denoted 
by the complement of a light 
verb involves interaction 
among participants 
Yes (???? jinxing taolun proceed discuss ?to 
discuss?)  
No (???? jiayi piping inflict criticize ?to criticize?) 
11. ACCOMPEVT Whether the event denoted 
by the complement of a light 
verb is an accomplishment 
Yes (???? jinxing jiejue proceed solve ?to solve?) 
No (???? jinxing zhandou proceed fight-fight ?to 
fight?) 
Table 1: Features used to differentiate five Chinese light verbs. 
 
 
 
4
Predicted 
 
Observed   
congshi gao jiayi jinxing zuo 
ML TW ML TW ML TW ML TW ML TW 
congshi 131 64 1 87 62 39 1 10 5 0 
gao 69 8 16 139 86 36 16 16 13 1 
jiayi 1 0 1 0 192 190 6 6 0 4 
jinxing 31 18 9 34 47 80 62 67 51 1 
zuo 50 24 5 16 44 114 4 14 97 32 
Table 2: Probability estimates of Mainland (ML) and Taiwan (TW) light verbs.  
 
 Recall Precision  F-measure 
ML TW ML TW ML TW 
congshi 0.66 0.32 0.46 0.56 0.54 0.41 
gao 0.08 0.70 0.5 0.5 0.14 0.58 
jiayi 0.96 0.95 0.45 0.41 0.61 0.58 
jinxing 0.31 0.34 0.70 0.59 0.43 0.43 
zuo 0.49 0.16 0.58 0.84 0.53 0.27 
Table 3: Recall, precision, and F-measure of the polytomous multivariate estimates. 
 
 
congshi gao jiayi jinxing zuo 
ML TW ML TW ML TW ML TW ML TW 
(Intercept) (1/Inf) (1/Inf) 0.02271 (1/Inf) (1/Inf) (1/Inf) (1/Inf) (1/Inf) (1/Inf) (1/Inf) 
ACCOMPEVTyes (1/Inf) (0.3419) 0.09863 (1/Inf) 56.25 11.33 0.1849 (0.1607) (1/Inf) 0.2272 
ARGSTRtwo 0.2652 0.1283 2.895 (0.7613) 76.47 (Inf) (1.481) (0.7062) 0.2177 (1.217) 
ARGSTRzero (1.097) (0.6219) 3.584 7.228 (1/Inf) (4.396) (1.179) 0.5393 0.245 0.2068 
ASPle (0.7487) (1/Inf) (0.1767) (1/Inf) (0.8257) (0.3027) (0.9196) (Inf) (1.853) 32.98 
ASPno (Inf) (0.9273) (1.499) (0.6967) (Inf) (Inf) (0.2307) (Inf) (0.2389) (0.2385) 
ASPzhe (1.603) 
 
(1/Inf) 
 
(0.4571) 
 
(Inf) 
 
(1/Inf) 
 
DUREVTyes (Inf) (Inf) (2.958) (Inf) (1/Inf) (1/Inf) (Inf) (0.9575) (Inf) (Inf) 
EVECOMPyes (1/Inf) (1/Inf) (1.726) (0.8491) (1/Inf) (1/Inf) 3.975 8.113 (1.772) (0.5019) 
FOREVTyes (2.744) 0.0867 (1.227) (Inf) (Inf) (Inf) (0.7457) (1.437) 0.2679 (1.467) 
INTEREVTyes 0.03255 0.1896 (0.5281) (1/Inf) (0.5432) (0.951) 18.67 10.47 0.08902 (0.398) 
PSYEVTyes (1/Inf) (1/Inf) (1/Inf) (1/Inf) 19.87 (1.395) (1/Inf) (1/Inf) (0.9619) (3.323) 
VOCOMPyes (0.1346) 0.18 (3.043) (2.35) 23.54 (Inf) (1.086) 3.161 (0.5344) (0.5956) 
Table 4: Multivariate analysis of light verb variations in Mainland and Taiwan Mandarin. 
Table 4 summarizes the results estimated by the Polytomous multivaraite analysis. The numbers in 
the table are the odds for the features in favor of or against the occurrence of each light verb: odds 
larger than 1 indicate that the chance of the occurrence of a light verb is significantly increased by the 
feature, e.g., the chance of Mainland ?? jiayi occurring is significantly increased by ARGSTRtwo 
(76.47: 1), followed by ACCOMPEVTyes (56.25: 1), VOCOMPyes (23.54: 1), PSYEVTyes (19.87: 
1); odds smaller than 1 indicate that the chance of the occurrence of a light verb is significantly 
descreased by the feature, e.g., the chance of Mainland ??  jinxing occurring is significantly 
decreased by ACCOMPEVTyse (0.1849: 1); in addition, ?inf? and ?1/inf? refer to odds larger than 
10,000 and smaller than 1/10,000 respectively, and non-significant odds (p-value < 0.05) are given in 
parentheses, regardless of the odds value.  
Table 4 finds that Mainland and Taiwan Mandarin indeed show some variations in each light verb. 
Furthermore, the variations of each light verb mainly lie in non-complementary distributional patterns. 
That is, as highlighted in dark grey colour in Table 4, the odds differences are more often between 
non-significance (odds in parentheses) and significance (odds larger or smaller than 1), rather than 
between significant preference (odds larger than 1) and significant dis-preference (odds smaller than 
1). In other words, the difference of a light verb in the two variants is more comparative, rather than 
5
contrastive.  This explains why the variations are not easily found by traditional linguistic studies. 
The following summarizes the key variations of each light verb.  
 
?? congshi 
?? congshi in both Mainland and Taiwan Mandarin has no feature significantly in its favor and it 
is significantly disfavored by ARGSTRtwo (taking two-argument complements, e.g., ?? yanjiu ?to 
research?) and INTEREVTyes (taking complements denoting interactive activities, e.g., ?? 
shangliang ?to discuss?). However, Taiwan ??  congshi is differentiated from Mainland ?? 
congshi in that the former is also disfavored by FOREVTyes (taking complements denoting formal 
events, e.g., ?? yanjiu ?to research?) and VOCOMPyes (taking complements in the form of V(erb)-
O(bject), e.g., ?? toupiao ?cast a vote?), whereas the latter is not. The finding that Taiwan ?? 
congshi is less likely to take formal event as its complement is consistent with that in Huang et al. 
(2013).  
 
? gao 
Both Mainland and Taiwan ? gao are significantly favored by ARGSTRzero (taking zero-argument 
complements, i.e. noun complement in this study). However, compared with Taiwan Mandarin, 
Mainland ? gao is more likely to take two-argument complements (ARGSTRtwo), but less likely to 
take complements denoting accomplishment events (ACCOMPEVTyes, e.g., ?? jiejue ?to solve?), 
and it is also disfavored by the aggregate of default variable values (i.e. the intercept, 0.02: 1).  
 
??  jiayi  
Both Mainland and Taiwan ? ?  jiayi are favored by the feature ACCOMPEVTyes 
(accomplishment complement such as ??  jiejue ?to solve?), but the chance of occurrence of 
Mainland ??  jiayi increases with the presence of two-argument complements (ARGSTRtwo), 
complements in VO form (VOCOMPyes), and complements denoting mental or phychological 
activities (PSYEVTyes, e.g., ?? fanxing ?to introspect?).  
 
?? jinxing 
Both Mainland and Taiwan ?? jinxing have INTEREVTyes (taking complements denoting 
interactive activities) and EVECOMPyes (allowing event complements in subject position, e.g., ??
???? huiyi jinxing shunli meeting procced smoothly ?The meeting proceeded smoothly?) in their 
favor. However, ??  jinxing in Mainland Mandarin is less likely to take accomplishment 
complements (ACCOMPEVTyes); whereas ?? jinxing in Taiwan Mandarin is more disfavored by 
ARGSTRzero, but more likely to take complements in VO form, which is also consistent with the 
findings in Huang et al. (2013).  
 
? zuo  
The occurrence of ? zuo in Mainland Mandarin is decreased by factors such as ARGSTRtwo, 
FOREVTyes, and INTEREVTyes, whereas the occurrence of ? zuo in Taiwan Mandarin is decreased 
by ACCOMPTEVTyes, but significantly increased by ASPle. It is obvious to linguists that ? zuo in 
both Mainland and Taiwan Mandarin are frequently found with the perfective marker ? le, but our 
analysis reveals that the affixation ? le to Taiwan ? zuo is much more frequent than that in Mainland.  
3.2 Clustering Analysis of Light Verb Variations 
We adopted a vector space model (VSM) to represent the use of light verbs. The features in Table 1 
could be expanded to 17 binary features. For example, ASP could be expanded into four binary 
features: ASP.le, ASP.zhe, ASP.guo, ASP.none. Each instance of a light verb in the corpus was 
represented by a vector with 17 dimensions. Each dimension stores the value of one of the 17 binary 
features determined by the context where the light verb is used. 
6
 
Cluster ID 0 1 2 3 4 5 6 7 8 9 
congshi TW 39 43 1 84 2 21 4 4 1 1 
ML 62 48 0 83 1 4 1 1 0 0 
gao TW 38 141 0 0 9 10 2 0 4 0 
ML 88 64 3 8 11 5 10 4 6 4 
jiayi TW 152 0 6 28 11 2 0 4 0 0 
ML 117 3 6 62 18 2 5 14 1 1 
jinxing TW 26 79 7 2 38 30 0 3 15 1 
ML 23 80 16 0 55 22 5 2 1 0 
zuo TW 20 3 0 2 23 130 20 2 1 6 
ML 23 44 3 16 38 45 20 11 8 3 
Table 5: The distribution of data origin by the clustering result. 
Then we adopt a clustering algorithm K-Means to identify the variations of light verbs in Taiwan 
and Mainland Mandarin. The assumption is that the instances of a light verb will form different 
clusters in the hyperspace according to the distances among them. Each cluster reflects a special use of 
a light verb. For example, there could be one cluster, where all the instances take non-accomplishment 
event argument, e.g., ????/??/??  jiayi fenxi/ yanjiu/ pinglun inflict analyze/ research/ 
comment ?to analyze/ research/ comment?, etc.  
In this sense, if there are light verb variations between Mainland and Taiwan Mandarin, the light 
verbs will be distributed to two clusters, one with data mainly from Mainland Mandarin, whereas the 
other mainly from Taiwan Mandarin. Meanwhile, if a cluster contains much more data from one 
variant than the other, it indicates the usage of a light verb is mainly restricted to the variant with more 
data; or if a cluster contains data of similar amount from both Mainland and Taiwan Mandarin, it 
indicates that the two variants share common usages regarding the light verbs. Therefore, for each 
light verb, all 400 examples from both Mainland and Taiwan Mandarin are mixed together for the 
analysis.  
As the K-Means algorithm requires an input of the number N of the clusters, the selection of N is 
then an issue we need to consider. Remembering that the clusters reflect the use of a light verb rather 
than data origin, the selection of N should be based on the consideration of how many different uses a 
light verb may have. As there are 17 expanded binary features, the whole space of the values of the 
vectors is 217 = 128K. However, the number of different uses for a light verb should not be too large. 
There is no problem if N is set slightly larger than the real number of different uses of a light verb. For 
example, if there are 5 different uses for a light verb and we set N=6, then we can imagine that there 
may be two clusters that reflect the same use of the light verb. On the contrary, if N is set too small, all 
different uses will be mixed together. Then, the clustering result may not be able to show any 
interesting result we expected. In our experiments, we set N=10 for all the five light verbs. Especially, 
we use the WEKA (Hall et al., 2009) implementation of the simple K-Means for our experiments. The 
result is shown in Table 5. The key variations of each light verb are summarized as follows.  
 
?? congshi 
Cluster 5 shows that Mainland ?? congshi prefers to take complements denoting formal or 
official events in Mainland Mandarin. However, Taiwan ?? congshi does not show such preference 
as it can take both formal and informal events. Clusters 6 and 9 show that Taiwan ?? congshi can 
also take complements in VO form, e.g., ???? jinxing kaipiao proceed ballot counting ?to proceed 
with ballot counting?, but this is not preferred by Mainland ?? congshi. 
 
? gao 
Clusters 6 and 7 together show that the argument of Mainland ? gao can occur in the subject 
position in addition to the complement position, but such word order is rarely found in Taiwan data. 
Cluster 3 shows a possibility for Mainland ? gao to take arguments denoting events involving 
interactions of participants (e.g., ?? taolun ?to discuss?). In addition, Cluster 9 shows the possibility 
7
that Mainland? gao can take complements describing informal events, while the complements to 
Taiwan Mainland ? gao are more often formal events (especially political activities).  
 
?? jiayi 
Cluster 7 suggests Mainland ?? jiayi show a preference over complements denoting mental or 
psychological events. However, although Clusters 1 and 6 show some difference between Mainland 
and Taiwan ?? jiayi, our closer examination of the original data found that such differences actually 
do not reflect any variant-specific uses.  
 
?? jinxing 
Cluster 6 suggests that Mainland?? jinxing show a preference over the aspectual marker ? -le, 
but such preference is not seen in Taiwan ?? jinxing. Cluster 8 shows a preference by Taiwan??  
jinxing that it could take VO compound (e.g., ?? toupiao cast-ticket ?to vote?) as complements, 
while this rarely happens in Mainland. 
 
? zuo 
Clusters 1 and 3 show that in Mainland Mandarin, it is common for ? zuo to take the aspectual 
marker ? -le, but such use of ? zuo in Taiwan is not as common as in Mainland. 
 
To sum up, the results from the machine learning method are consistent with that from the 
multivariate statistical analysis in Section 3.1. Bringing together, we find that while the light verbs in 
Mainland and Taiwan Mandarin show similarities (as the speakers of these two regions can 
communicate without difficulty), there are indeed also variations in the two variants.   
4 Concluding Remarks 
Our study is the one of the first comparable corpus driven computational modeling studies on newly 
emergent language variants. The automatic identification of Mainland and Taiwan syntactic variations 
has very significant linguistic and computational implications. Linguistically, we showed that our 
comparable corpus driven statistical approach can identify comparative differences which are 
challenging for human analysis. The fact that newly emergent variants differ from each other 
comparatively rather than contrastively may also have important linguistics implications. In addition, 
by successfully differentiating these two variants based on their uses of light verbs, the result also 
suggests that variations among such newly emergent variants may arise from categories that are 
semantically highly bleached and tend to be/or have been grammaticalized. Computationally, the 
ability of machine learning approaches to differentiate Mainland and Taiwan variants of Mandarin 
Chinese potentially contributes to overcoming the challenge of automatic identification of subtle 
language/dialect variations among other light verbs, other lexical categories, as well as other 
languages/dialects.  
 
Acknowledgements 
The work is supported by a General Research Fund (GRF) sponsored by the Research Grants Council 
(Project no. 543512) and NTU Grant no. M4081117.100.500000. 
 
References 
Arppe, Antti. 2008. Univariate, bivariate and multivariate methods in corpus-based lexicography - a 
study of synonymy. Publications of the Department of General Linguistics, University of Helsinki, 
volume 44.  
Butt, Miriam and Wilhelm, Geuder. 2003. On the (semi) lexical status of light verbs. Semi-lexical 
Categories, Pages 323-370. 
8
Bresnan, Joan, Anna Cueni, Tatiana Nikitina, and R. Harald Baayen 2007. Predicting the dative 
alternation. In: Cognitive Foundations of Interpretation. Boume, G., I. Kraemer, and J. Zwarts. 
Amsterdam: Royal Netherlands Academy of Science, pp. 69-94.  
Cai, Wenlan. (1982). Issues on the complement of jinxing (????????). Chinese Language 
Learning (????) (3), 7-11. 
Diao, Yanbin. 2004. ?????????? (Research on Delexical Verb in Modern Chinese).  Dalian: 
Liaoning Normal University Press. 
Hall, Mark, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann and Ian H. Witten. 
2009. The WEKA data mining software: An update. SIGKDD Explorations, 11(1):10-18.  
Han, Weifeng, Antti Arppe, and John Newman. 2013. Topic marking in a Shanghainese corpus: from 
observation to prediction. Corpus Linguistics and Linguistic Theory (preprint).  
Hong, Jia-fei, and Chu-Ren Huang. 2013. ????????????????????? (Cross-strait 
lexical differences: A comparative study based on Chinese Gigaword Corpus). Computational 
Linguistics and Chinese Language Processing. 18(2):19-34. 
Hong, Jia-fei, and Chu-Ren Huang. 2008. ??????????????. (A corpus-based approach 
to the discovery of cross-strait lexical contrasts). Language and Linguistics. 9 (2):221-238. 
Huang, Chu-Ren. 2009. Tagged Chinese Gigaword Version 2.0. Philadelphia: Lexical Data 
Consortium, University of Pennsylvania. ISBN  1-58563-516-2 
Huang, Chu-Ren and Jingxia Lin. 2013. The ordering of Mandarin Chinese light verbs. In Proceedings 
of the 13th Chinese Lexical Semantics Workshop. D. Ji and G. Xiao (Eds.): CLSW 2012, LNAI 
7717, pages 728-735. Heidelberg: Springer. 
Huang, Chu-Ren, Jingxia Lin, and Huarui Zhang. 2013. World Chineses based on comparable corpus:  
The case of grammatical variations of jinxing. ??????????, pages  397-414. 
Hwang, Jena D., Archna Bhatia, Clare Bonial, Aous Mansouri, Ashwini Vaidya, Nianwen Xue, 
Martha Palmer. 2010. PropBank annotation of multilingual light verb constructions. Proceedings of 
the Fourth Linguistic Annotation Workshop, ACL 2010, 82?90. Jespersen, Otto. 1965. A Modern 
English Grammar on Historical Principles. Part VI, Morphology. London: George Allen and 
Unwin Ltd. 
Lin, Jingxia, Hongzhi Xu, Menghan Jiang and Chu-Ren Huang. 2014.  Annotation and classification 
of light verbs and light verb variations in Mandarin Chinese. COLING Workshop on Lexical and 
Grammatical Resources for Language Processing. Dublin, August 24.  
Nagy, Istv?n, Veronika Vincze, and Rich?rd Farkas. 2013. Full-coverage identification of English 
light verb constructions. In Proceedings of the International Joint Conference on Natural Language 
Processing, pages 329-337. 
Qiu, Zhipu, 1990. ????????? (Dictionary of Mainland and Taiwan Mandarin). Nanjing 
University press. 
Tu, Yuancheng and Dan Roth. 2011. Learning English light verb constructions: Contextual or 
statistical. In Proceedings of the Workshop on Multiword Expressions: from Parsing and Generation 
to the Real World. Association for Computational Linguistics. 
Wang Tiekun and Li Xingjian, 1996. ?????????? (Research on lexical differences between 
Mainland and Taiwan Mandarin), World Chinese (??????), volume 81. 
Wei Li and Sheng Yuqi, 2000. ?????????????. (Comparative Dictionary of Lexical use 
in Mainland, Hong Kong, Macau and Taiwan), Beijing Industry University Press.  
Xu Danhui, 1995. ????????? (Lexical difference between Mainland and Taiwan Chinese). 
1st symposium on Cross-Strait Lexical and Character differences (???????????????
????).   
9
Zeng Rongfen, 1995. ???????????  (Opinion on cross-Strait language differences)1st 
symposium on Cross-Strait Lexical and Character differences (????????????????
???). 
Zhou, Gang. 1987. ???????? (Subdivision of dummy verbs). Chinese Language Learning (?
???), volume 1, pages 11-14. 
Zhu, Dexi. (1985). ???????????????? (Dummy verbs and NV in Modern Chinese). 
Journal of Peking University (Humanities and Social Sciences) (??????(???????)), 
volume 5, pages 1-6.  
 
 
 
 
10
Proceedings of the Workshop on Lexical and Grammatical Resources for Language Processing, pages 75?82,
Coling 2014, Dublin, Ireland, August 24 2014.
Annotation and Classification of Light Verbs and Light Verb 
Variations in Mandarin Chinese
?
 
Jingxia Lin1      Hongzhi Xu2     Menghan Jiang2     Chu-Ren Huang2 
1Nanyang Technological University 
2Department of CBS, The Hong Kong Polytechnic University 
jingxialin@ntu.edu.sg, hongz.xu@gmail.com, 
menghan.jiang@connect.polyu.hk, churen.huang@polyu.edu.hk 
 
  
Abstract 
Light verbs pose an a challenge in linguistics because of its syntactic and semantic versatility and  its 
unique distribution different from regular verbs with higher semantic content and selectional resrictions. 
Due to its light grammatical content, earlier natural language processing studies typically put light verbs 
in a stop word list and ignore them. Recently, however, classification and identification of light verbs 
and light verb construction have become a focus of study in computational linguistics, especially in the 
context of multi-word expression, information retrieval, disambiguation, and parsing. Past linguistic and 
computational studies on light verbs had very different foci. Linguistic studies tend to focus on the sta-
tus of light verbs and its various selectional constraints. While NLP studies have focused on light verbs 
in the context of either a multi-word expression (MWE) or a construction to be identified, classified, or 
translated, trying to overcome the apparent poverty of semantic content of light verbs. There has been 
nearly no work attempting to bridge these two lines of research. This paper takes this challenge by pro-
posing a corpus-bases study which classifies and captures syntactic-semantic difference among all light 
verbs. In this study, we first incorporate results from past linguistic studies to create annotated light verb 
corpora with syntactic-semantics features. We next adopt a statistic method for automatic identification 
of light verbs based on this annotated corpora. Our results show that a language resource based method-
ology optimally incorporating linguistic information can resolve challenges posed by light verbs in NLP. 
 
1 Introduction 
Identification of Light Verb Construction (LVC) plays an important role and poses a special challenge 
in many Natural Language Processing (NLP) applications, e.g. information retrieval and machine 
translation. In addition to addressing issues related to LVC as a contributing factor to errors for vari-
ous applications, a few computational linguistics studies have targeted LVC in English specifically 
(e.g., Tu and Roth, 2011; Nagy et al., 2013). To the best of our knowledge, however, there has been no 
computational linguistic study dealing with LVCs in Chinese specifically. It is important to know that, 
due to their lack of semantic content, light verbs can behave rather idiosyncratically in each language. 
Chinese LVC, in particular, has the characteristic that allows many different light verbs to share simi-
lar usage and be interchangeable in some context. We should also note that light verbs in Chinese can 
take both verbs, deverabal nouns, and eventive nouns, while the morphological status of these catego-
ries are typically unmarked, Hence, it is often difficult to differentiate a light verb from its non-light 
verb uses without careful analysis of the data. 
      It has been observed that some Chinese light verbs can be used interchangeably but will have 
different selectional restrictions in some (and generally more limited) contexts. For example, the five 
light verbs congshi, gao, jiayi, jinxing, zuo (these words originally meant ?engage?, ?do?, ?inflict?, 
?proceed?, ?do? respectively) can all take yanjiu ?to do research? as their complement and form a LVC. 
However, only the light verbs gao and jinxing can take bisai ?to play games? as complements, where-
as the other light verbs congshi, jiayi, and zuo cannot. Since light verbs are often interchangeable yet 
                                                 
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer 
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 
75
each also has its own selectional restrictions, it makes the identification of light verbs themselves both 
a challenging and necessary task. It is also observed that this kind of selectional versatility actually led 
to variations among different variants of Mandarin Chinese, such as Mainland and Taiwan. The versa-
tility of Chinese light verbs makes the identification of LVCs more complicated than English. 
Therefore, to study the differences among different light verbs and different variants of Chinese is 
important but challenging in both linguistic studies and computational applications. With annotated 
data from comparable corpora of Mainland and Taiwan Mandarin Chinese, this paper proposes both 
statistical and machine learning approaches to differentiate five most frequently used light verbs in 
both variants based on their syntactic and semantic features. The experimental results of our approach 
show that we can reliably differentiate different light verbs from each other in each variety of Manda-
rin Chinese.  
There are several contributions in our work. Firstly, rather than focusing on only two light verbs 
jiayi and jinxing as in previous linguistic studies, we extended the study to more light verbs that are 
frequently used in Chinese. Actually, we will show that although jiayi and jinxing were often dis-
cussed in a pair in previous literature, the two are quite different from each other. Secondly, we show 
that statistical analysis and machine learning approaches are effective to identify the differences of 
light verbs and the variations demonstrated by the same light verb in different variants of Chinese. 
Thirdly, we provide a corpus that covers all typical uses of Chinese light verbs. Finally, the feature set 
we used in our study could be potentially used in the identification of Chinese LVCs in NLP applica-
tions. 
This paper is organized as follows. Section 2 describes the data and annotation of the data. In Sec-
tion 3, we conducted both statistical and machine learning methodologies to classify the five light 
verbs in both Mainland and Taiwan Mandarin. We discussed the implications and applications of our 
methodologies and the findings of our study in Section 4. Section 5 presents the conclusion and our 
future work. 
2 Corpus Annotation 
2.1 Data Collection 
The data for this study is extracted from Annotated Chinese Gigaword corpus (Huang, 2009) which 
was collected and available from LDC and contains over 1.1 billion Chinese words, with 700 million 
characters from Taiwan Central News Agency and 400 million characters from Mainland Xinhua 
News Agency.  
The light verbs to be studied are congshi, gao, jiayi, jinxing, zuo; these five are among the most fre-
quently used light verbs in Chinese (Diao, 2004). 400 sentences are randomly selected for each light 
verb, half from the Mainland Gigaword subcorpus and the other from the Taiwan Gigaword subcorpus, 
which resulted in 2,000 sentences in total. The selection follows the principle that it could cover the 
different uses of each light verb.  
2.2 Feature Annotation 
Previous studies (Zhu, 1985; Zhou, 1987; Cai, 1982; Huang et al., 1995; Huang et al., 2013, among 
others) have proposed several syntactic and semantic features to identify the similarities and differ-
ences among light verbs, especially between the two most typical ones, i.e. jinxing (originally ?pro-
ceed?) and jiayi (originally ?inflict?). For example, jinxing can take aspectual markers like zhe ?pro-
gressive marker?, le ?aspect marker?, and guo ?experiential aspect marker? while jiayi cannot (Zhou, 
1987);  congshi can take nominal phrases such as disan chanye?the tertiary industry? as its comple-
ment while jiayi cannot. A few features are also found to be variant-specific; for example, Huang and 
Lin (2013) find that only the congshi in Taiwan, but not in Mainland Mandarin, can take informal and 
negative event complements like xingjiaoyi ?sexual trade?. 
In our study, we selected 11 features which may help to differentiate different light verbs in each 
Mandarin variant as well as light verb variations among Mandarin variants, as in Table 1. All 2,000 
examples collected for analysis were manually annotated based on the 11 features. The annotator is a 
trained expert on Chinese linguistics. Any ambiguous cases were discussed with another two experts 
in order to reach an agreement. 
 
76
 
 
Feature ID Explanation Values (example) 
1. OTHERLV Whether a light verb co-occurs 
with another light verbs 
Yes (kaishi jinxing taolun Start proceed discuss 
?start to discuss?) 
No (jinxing taolun proceed discuss ?to discuss?) 
2. ASP 
 
Whether a light verb is affixed 
with an aspectual marker (e.g., 
perfective le, durative zhe, experi-
ential guo) 
ASP.le (jinxing-le zhandou ?fighted?) 
ASP.zhe (jinxing-zhe zhandou ?is fighting?) 
ASP.guo (jinxing-guo zhandou ?fighted?) 
ASP.none (jinxing zhandou ?fight?) 
3. EVECOMP Event complement of a light verb 
is in subject position 
Yes (bisai zai xuexiao jinxing game at school pro-
ceed ?The game was held at the school?) 
No (zai xuexiao jinxing bisai at school proceed 
game ?the game was held at the school?)  
4. POS 
 
The part-of-speech of the com-
plement taken by a light verb  
Noun (jinxing zhanzheng proceed fight ?to fight?) 
Verb (jinxing zhandou proceed fight ?to fight?) 
5. ARGSTR 
 
The argument structure of the 
complement of a light verb, i.e. 
the number of arguments (subject 
and/or objects) that can be taken 
by the complement  
One (jinxing zhandou proceed fight ?to fight?) 
Two (jinxing piping proceed criticize ?to criticize?)  
Zero (jinxing zhanzheng proceed fight ?to fight?) 
6. VOCOMP Whether the complement of a 
light verb is in the V(erb)-
O(bject) form  
Yes (jinxing tou-piao proceed cast-ticket ?to vote?) 
No (jinxing zhan-dou proceed fight-fight ?to fight?) 
7. DUREVT Whether the event denoted by the 
complement of a light verb is du-
rative 
Yes (jinxing zhandou proceed fight-fight ?to fight?) 
No (jiayi jujue inflict reject ?to reject?)  
8. FOREVT Whether the event denoted by the 
complement of a light verb is 
formal or official 
Yes (jinxing guoshi fangwen proceed state visit ?to 
pay a state visit?) 
No (zuo xiao maimai do small business ?run a 
small business?)  
9. PSYEVT Whether the event denoted by the 
complement of a light verb is 
mental or psychological activity 
Yes (jiayi fanxing inflict retrospect ?to retrospect?) 
No (jiayi diaocha inflict investigate ?to investi-
gate?)  
10. INTEREVT Whether the event denoted by the 
complement of a light verb in-
volves interaction among partici-
pants 
Yes (jinxing taolun proceed discuss ?to discuss?)  
No (jiayi piping inflict criticize ?to criticize?) 
11. ACCOMPEVT Whether the event denoted by the 
complement of a light verb is an 
accomplishment 
Yes (jinxing jiejue proceed solve ?to solve?) 
No (jinxing zhandou proceed fight-fight ?to fight?) 
Table 1: Features used to differentiate five Chinese light verbs. 
 
3 Identification of light verbs based on annotated corpora  
In this section, we adopted both statistical analysis and machine learning approaches to identify the 
five light verbs (jiayi, jinxing, congshi, gao and zuo) on the corpora with 2,000 annotated examples. 
The results of all approaches show that the five light verbs can be differentiated from each other in 
both Mainland and Taiwan Mandarin. 
3.1 Identifying light verbs by statistical analysis 
Both univariate analysis and multivariate analysis were used in our study for the identification. The 
tool we used is the Polytomous Package in R (Arppe, 2008).  
 
 
77
3.1.1 Univariate analysis  
Among the 11 independent features, one was found with only one level in both Mainland and Taiwan 
variants, i.e. all five light verbs in the two variants show the same preference over the features and 
thus excluded from the analysis. The feature is OTHERLV (all light verbs do not co-occur with another 
light verb in a sentence). Chi-squared tests were conducted for the significance of the co-occurrence of 
the remaining ten features with individual light verbs in both Mainland and Taiwan variants. The 
chisq.posthoc() function in the Polytoumous Package (Arppe, 2008) in R was used for the tests. The 
results are presented in Table 2, where the ?+? and ?-? signs indicate respectively a statistically signif-
icant overuse and underuse of a light verb with a feature, and ?0? refers to a lack of statistical signifi-
cance.  
 
  
Feature 
  
N 
Mainland Mandarin Taiwan Mandarin 
congshi  gao jiayi jinxing zuo congshi gao jiayi jinxing zuo 
POS.N 585 + + - 0 0 + + - - - 
POS.V 1415 - - + 0 0 - - + + + 
ARGSTR.one 376 0 - - 0 + + - - + 0 
ARGSTR.two 1039 - 0 + 0 - - - + - + 
ARGSTR.zero 585 + + - 0 0 + + - - - 
VOCOMP.no 1939 0 0 0 0 0 0 0 + - 0 
VOCOMP.yes 61 0 0 0 0 0 0 0 - + 0 
EVECOMP.no 1919 + - + - - + 0 + - 0 
EVECOMP.yes 81 - + - + + - 0 - + 0 
ASP.guo 9 0 0 0 0 0 0 0 0 0 0 
ASP.le 155 - - - + + - - - - + 
ASP.no 1835 + + + - - + + + + - 
ASP.zhe 1 0 0 0 + 0           
DUREVT.no 35 - 0 + - - 0 0 + 0 0 
DUREVT.yes 1965 + 0 - + + 0 0 - 0 0 
FOREVT.no 66 0 0 - 0 + + - - 0 0 
FOREVT.yes 1934 0 0 + 0 - - + + 0 0 
PSYEVT.no 1981 0 0 - 0 0 0 0 0 0 - 
PSYEVT.yes 19 0 0 + 0 0 0 0 0 0 + 
INTEREVT.no 1870 + 0 + - + + + 0 - 0 
INTEREVT.yes 130 - 0 - + - - - 0 + 0 
ACCOMPEVT.no 1904 + + - + + + + - + 0 
ACCOMPEVT.yes 96 - - + - - - - + - 0 
Table 2: Identifying light verbs in Mainland and Taiwan Mandarin via univariate analysis.  
 
Table 2 suggests that in both Mainland and Taiwan Mandarin, each light verb shows significant 
preference for certain features, and thus can be distinguished from each other. For example, in Main-
land Mandarin, although both congshi and gao show significant preference for the features POS.N and 
ACCOMPEVT.no, congshi differs from gao in that it also significantly prefers DUREVT.yes (taking 
complements denoting durative events, e.g., yanjiu ?to research?), EVECOMP.no (event complements 
do not occur in subject position), and INTEREVT.no (not taking complements denoting events involv-
ing interaction among participants, e.g., taolun ?to discuss?), whereas gao shows either a dis-
preference or no significant preference over these features. Take gao and zuo in Taiwan Mandarin as 
another example. While both light verbs literally means ?to do?, there is no single feature preferred by 
both: gao prefers POS.N, ARGSTR.zero, FOREVT.yes, INTEREVT.no, ACCOMPEVT.no, whereas zuo 
shows significant preferences for POS.V, ARGSTR.two, ASP.le, and PSYEVT.yes.  
 
3.1.2 Multivariate analysis  
As shown in Table 2, in both Mainland and Taiwan Mandarin, some of the five light verbs share some 
features, which thus explains why sometimes they can be interchangeably used. This also indicates (a) 
that a particular feature is unlikely to be preferred by only one light verb and thus differentiates the 
verb from the others; (b) a certain context may allow the occurrence of more than one light verb. In 
78
this sense, a multivariate analysis was adopted to better classify the five light verbs in each variant. 
The multivariate analysis used in the current study is polytomous logistic regression (Arppe, 2008), 
and the tool we used is the Polytomous() function in the Polytoumous Package (Arppe, 2008) in R.  
The results from the multivariate analysis were summarized in Table 3.  The numbers shown in the 
table are the odds for the features in favor of or against the occurrence of each light verb: when the 
estimated odd is larger than 1, the chance of the occurrence of a light verb is significantly increased by 
the feature, e.g., the chance of Mainland jiayi occurring is significantly increased by ARGSTRtwo 
(76.47:1), followed by ACCOMPEVTyes (56:1), VOCOMPyes (23.54: 1), and PSYEVTyes (19.87: 1). 
When the estimated odd is smaller than 1, the chance of the occurrence of a light verb is significantly 
decreased by the feature, e.g., the chance of Mainland jinxing occurring is significantly decreased by 
ACCOMPEVTyes (0.1849: 1); in addition, ?inf? and ?1/inf? refer to odds larger than 10,000 and 
smaller than 1/10,000 respectively, whereas non-significant odds (p-value < 0.05) are given in paren-
theses.  
 
 
Mainland Mandarin Taiwan Mandarin 
congshi gao jiayi jinxing zuo congshi gao jiayi jinxing zuo 
(Intercept) (1/Inf) 0.02271 (1/Inf) (1/Inf) (1/Inf) (1/Inf) (1/Inf) (1/Inf) (1/Inf) (1/Inf) 
ACCOMPEVTyes (1/Inf) 0.09863 56.25 0.1849 (1/Inf) (0.3419) (1/Inf) 11.33 (0.1607) 0.2272 
ARGSTRtwo 0.2652 2.895 76.47 (1.481) 0.2177 0.1283 (0.7613) (Inf) (0.7062) (1.217) 
ARGSTRzero (1.097) 3.584 (1/Inf) (1.179) 0.245 (0.6219) 7.228 (4.396) 0.5393 0.2068 
ASPle (0.7487) (0.1767) (0.8257) (0.9196) (1.853) (1/Inf) (1/Inf) (0.3027) (Inf) 32.98 
ASPno (Inf) (1.499) (Inf) (0.2307) (0.2389) (0.9273) (0.6967) (Inf) (Inf) (0.2385) 
ASPzhe (1.603) (1/Inf) (0.4571) (Inf) (1/Inf) 
     
DUREVTyes (Inf) (2.958) (1/Inf) (Inf) (Inf) (Inf) (Inf) (1/Inf) (0.9575) (Inf) 
EVECOMPyes (1/Inf) (1.726) (1/Inf) 3.975 (1.772) (1/Inf) (0.8491) (1/Inf) 8.113 (0.5019) 
FOREVTyes (2.744) (1.227) (Inf) (0.7457) 0.2679 0.0867 (Inf) (Inf) (1.437) (1.467) 
INTEREVTyes 0.03255 (0.5281) (0.5432) 18.67 0.08902 0.1896 (1/Inf) (0.951) 10.47 (0.398) 
PSYEVTyes (1/Inf) (1/Inf) 19.87 (1/Inf) (0.9619) (1/Inf) (1/Inf) (1.395) (1/Inf) (3.323) 
VOCOMPyes (0.1346) (3.043) 23.54 (1.086) (0.5344) 0.18 (2.35) (Inf) 3.161 (0.5956) 
Table 3: identifying light verbs in Mainland and Taiwan Mandarin via multivariate analysis.  
 
   As shown in Table 3, each of the light verbs in each Mandarin variant shows its favor and disfavor 
of certain features. Take Mainland Mandarin for example: although congshi has no feature significant-
ly in its favor, but it is significantly disfavored by ARGSTRtwo (0.27:1) and ITEREVTyes (0.03:1); gao 
is disfavored by the aggregate of default variable values (0.02:1), and ACCOMPEVTyes (0.1:1), but is 
significantly favored by ARGSTRtwo and ARGSTRzero; the chance of jiayi?s ocucrrence is significant-
ly increased by ARGSTRtwo(76.47:1), ACCOMPEVTyes (56.25:1), VOCOMPyes (23.54:1), and 
PSYEVTyes (19:87:1); jinxing has INTEREVTyes and EVECOMPyes in its favor, but ACOMPEVTyes 
in its disfavor; no feature is significantly in the favor of zuo, but this light verb is significantly disfa-
vored by ARGSTRtwo, ARGSTRzero, FOREVTyes and INTEREVTyes.   
     The results in Table 3 also show that sometimes one key feature is able to identify two light verbs 
from each other, although not all five light verbs. Take Mainland Mandarin again for example. Most 
combinations of two light verbs from the five can be effectively differentiated by one feature. For in-
stance, the feature ARGSTRtwo can differentiate congshi/gao, congshi/jiayi, jiayi/zuo and gao/zuo; the 
feature INTEREVTyes can differentiate congshi/jinxing and jinxing/zuo; the feature ACCOMPEVTyes 
can differentiate the pairs gao/jiayi and jinxing/jiayi. 
3.2 Identifying light verbs by classification 
In this section, we resorted to machine learning technologies to study the same issue. Different classi-
fiers were adopted to discriminate the five light verbs with the annotated corpora: ID3, Logistic Re-
gression, Na?ve Bayesian and SVM that are implemented in WEKA (Hall et al., 2009) and 10-fold 
cross validations were performed separately on the Taiwan and Mainland corpora.  
79
The results were presented in Table 4. We can see that different classifiers provide similar results 
on both corpora, which means that the classification results are reliable and the features we annotated 
are effective in identifying the five light verbs. Overall, ID3 out-performs SVM slightly, with Logistic 
and NB not far behind. ID3 performs the best since the data is in low dimension. The detailed results 
including precision, recall and F-measure by ID3 on both corpora are shown in Table 5. The corre-
sponding confusion matrixes are presented in Table 6. The confusion matrixes suggest two very im-
portant generalizations: (a) all five verbs can be classified with good confidence, and (b) the overall 
classification patterns of the Mainland and Taiwan Mandarin are very similar, which is consistent with 
the fact that Mainland and Taiwan Mandarin are two variants. However, we also observe that the con-
fusion matrixes between various light verb pairs may differ between Mainland and Taiwan Chineses. 
This is the difference we would like to explore in the next section to propose a way to automatically 
predict these two variants. In addition, it is worth noting that all classifiers identify jiayi more effec-
tively than other light verbs, which thus shows a potential different usage of jiayi from the others.  
 
 ID3 Logistic NB SVM 
TW ML TW ML TW ML TW ML 
jingxing 0.365 0.494 0.372 0.455 0.411 0.444 0.422 0.485 
gao 0.612 0.391 0.609 0.364 0.598 0.377 0.575 0.354 
zuo 0.571 0.566 0.568 0.582 0.525 0.576 0.574 0.561 
jiayi 0.759 0.800 0.758 0.807 0.752 0.794 0.759 0.767 
congshi 0.552 0.646 0.526 0.643 0.486 0.648 0.523 0.633 
Average 0.574 0.585 0.567 0.576 0.555 0.573 0.571 0.565 
Table 4: Result in F1-score of 10-fold cross validation of the classification of the five light verbs with 
different classifiers on the Taiwan (TW) and Mainland (ML) Corpora. 
 
 Precision Recall F-Measure 
TW ML TW ML TW ML 
jingxing 0.442 0.593 0.311 0.423 0.365 0.494 
gao 0.681 0.449 0.557 0.347 0.612 0.391 
zuo 0.610 0.570 0.537 0.562 0.571 0.566 
jiayi 0.634 0.720 0.946 0.900 0.759 0.800 
congshi 0.528 0.583 0.579 0.724 0.552 0.646 
Average 0.580 0.586 0.588 0.599 0.574 0.585 
Table 5: 10-fold cross validation result of ID3 algorithm on both corpora. 
 
 jingxing gao zuo jiayi congshi 
 TW ML TW ML TW ML TW ML TW ML 
jingxing 61 83 15 27 36 40 38 11 46 35 
gao 20 16 113 70 13 23 24 39 33 54 
zuo 24 25 8 28 108 118 39 25 22 14 
jiayi 5 11 0 6 5 6 192 206 1 0 
congshi 28 5 30 25 15 20 10 5 114 144 
Table 6: Confusion matrix of the classification with ID3 algorithm on both corpora. 
 
3.3 Identifying light verbs by automatic clustering 
We further used the clustering algorithm to test the differentiability of the five light verbs in both 
Mainland and Taiwan Mandarin. The results using the simple K-Means clustering algorithm on Tai-
wan and Mainland corpora are shown in Table 7. The results show that the light verb jiayi behaves 
80
quite differently from the other four light verbs in both Mainland and Taiwan corpora, which is similar 
to the analysis based on statistical methods in Section 3.1 and classification methods in Section 3.2. In 
both corpora, jiayi has a narrower usage than the other light verbs. Meanwhile, we can also find a clus-
ter which is mainly formed by instances of jiayi from the Mainland corpus (i.e. cluster 0). After closer 
examination of the examples in this cluster, we found that it mainly includes sentences where jiayi 
takes complements denoting accomplishment events, e.g. gaizheng ?to correct? and jiejue ?to solve?. 
However, jiayi in Taiwan corpus mainly takes complements denoting activity events, and thus almost 
all instances of Taiwan jiayi are mixed with those of the other light verbs. Meanwhile, our results 
show a tendency that all other light verbs (jinxing, congshi, zuo, and gao) mostly take activity com-
plements but fewer accomplishment complements in both Taiwan and Mainland corpora. More dis-
cussion on the light verb variations between Mainland and Taiwan Mandarin can be found in (Huang 
et al., 2014).  
 
 
 
 Mainland  Taiwan 
0 1 2 3 4 0 1 2 3 4 
jinxing 2 32 110 23 37 30 10 77 20 64 
gao 2 33 116 41 11 120 23 30 0 31 
zuo 0 36 80 14 81 19 4 47 5 132 
jiayi 68 0 161 0 0 0 0 1 6 196 
congshi 0 67 66 21 46 90 20 68 0 22 
Table 7: Clustering results on Mainland and Taiwan corpora.  
 
4 Applications and Implications 
4.1 Implications for Future Studies 
In the study above, we were able to annotate a corpus with all the types of significant context and, 
based on this annotated corpus, we were able to use statistic model to differentiate the use of different 
light verbs in different contexts. Such a module of generic linguistic tools can have several potentially 
very useful applications. First, in translation, LVC is one of the most difficult constructions as there is 
less grammatical or contextual information to make the correct translation. Our approach is especially 
promising. As we encode contextual selection information for all light verbs, the same approach can 
be applied to the other languages in the target-source pair to produce optimal pair. Second, in infor-
mation extraction, selection of different light verbs often conveys subtle difference in meanings. Our 
ability to differentiate similar light verbs in the same context could have great potential in extracting 
the subtle information change/increase in the same context. Lastly, in second language learning as well 
as error detection, light verbs have been one of the most challenging ones. Our studies can be readily 
applied to either error detection or second language learning environment to provide the correct con-
text where a certain light very is preferred over another. 
4.2 From light verb variations to variants for the same language 
One of the biggest challenges in computational processing of languages is probably to identify newly 
emergent variants, such as the cross-strait variations of Mandarin Chinese. For these two variants, the 
most commonly cited ones were on lexical differences. Systematic grammatical differences were 
much more difficult to study and hence rarely reported (comp. Huang et al., 2009). As these are two 
newly divergent variants, their main grammars are almost all identical, except for some subtle differ-
ences, such as the selection between different light verbs and their complements. Our preliminary re-
sults of univariate and multivariate analysis can be found in Table 2 and 3. It shows not only the simi-
larities/differences among the light verbs in each variety (e.g., both ML and TW congshi and gao 
show preferences over POS.N, whereas both ML and TW jiayi show dispreference), but also the simi-
larities/differences of the corresponding light verbs in Mainland and Taiwan Mandarin. For instance, 
jinxing in TW tends to take VO compounds as its complements e.g., jinxing toupiao ?cast a vote?, 
81
which is consistent with the analysis in (Huang et al., 2013) (see more in Huang et al., 2014). But one 
thing should be pointed out is the difference is more between a significant and non-significant feature, 
rather than between a significant positive and significant negative feature.  
5 Conclusion 
In this paper, we addressed the issue of automatic classification of Chinese light verbs based on their 
usage distribution, based on an annotated corpus marking relevant contextual information for light 
verbs. We used both statistical methods and machine learning technologies to address this issue. It is 
found that our approaches are effective in identifying light verbs and their variations. The automatic 
generated semantic and syntactic features can also be used for future studies on other light verbs as 
well as other lexical categories. The result suggested that richly annotated language resources paired 
with appropriate tool can lead to effective general solution for some common issues faced by linguis-
tics and natural language processing. 
Acknowledgements 
The work is supported by a General Research Fund (GRF) sponsored by the Research Grants Council 
(Project no. 543512) and NTU Grant NO. M4081117.100.500000. 
Reference 
Antti Arppe. 2008. Univariate, bivariate and multivariate methods in corpus-based lexicography - a study of 
synonymy. Publications of the Department of General Linguistics, University of Helsinki, volume 44.  
Wenlan Cai. (1982). Issues on the complement of jinxing (????????). Chinese Language Learning (?
???) (3), 7-11. 
Yanbin Diao. 2004. Research on Delexical Verb in Modern Chinese (??????????).  Dalian: Liao-
ning Normal University Press. 
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann and Ian H. Witten. 2009. The 
WEKA Data Mining Software: An Update. SIGKDD Explorations, 11(1):10-18.  
Chu-ren Huang, Meili Yeh, and Li-ping Chang. 1995. Two light verbs in Mandarin Chinese. A corpus-based 
study of nominalization and verbal semantics. Proceedings of NACCL6, 1: 100-112. 
Chu-Ren Huang. 2009. Tagged Chinese Gigaword Version 2.0. Philadelphia: Lexical Data Consortium, Univer-
sity of Pennsylvania. ISBN  1-58563-516-2 
Chu-Ren Huang and Jingxia Lin. 2013. The ordering of Mandarin Chinese light verbs. In Proceedings of the 
13th Chinese Lexical Semantics Workshop. D. Ji and G. Xiao (Eds.): CLSW 2012, LNAI 7717, pages 728-
735. Heidelberg: Springer. 
Chu-Ren Huang, Jingxia Lin, and Huarui Zhang. 2013. World Chineses based on comparable corpus:  The case 
of grammatical variations of jinxing. ??????????, pages  397-414. 
Chu-Ren Huang, Jingxia Lin, Menghan Jiang and Hongzhi Xu. 2014. Corpus-based Study and Identification of 
Mandarin Chinese Light Verb Variations. COLING Workshop on Applying NLP Tools to Similar Languages, 
Varieties and Dialects. Dublin, August 23.  
Istv?n Nagy, Veronika Vincze, and Rich?rd Farkas. 2013. Full-coverage Identification of English Light Verb 
Constructions. In Proceedings of the International Joint Conference on Natural Language Processing, pages 
329-337. 
Yuancheng Tu and Dan Roth. 2011. Learning English light verb constructions: Contextual or statistical. In Pro-
ceedings of the Workshop on Multiword Expressions: from Parsing and Generation to the Real World. Asso-
ciation for Computational Linguistics. 
Gang Zhou. 1987. Subdivision of Dummy Verbs (????????). Chinese Language Learning (????), 
volume 1, pages 11-14. 
Dexi Zhu. (1985). Dummy Verbs and NV in Modern Chinese (????????????????). Journal 
of Peking University (Humanities and Social Sciences) (??????(???????)), volume 5, pages 
1-6.  
82
Proceedings of the Workshop on Lexical and Grammatical Resources for Language Processing, pages 157?166,
Coling 2014, Dublin, Ireland, August 24 2014.
Annotate and Identify Modalities, Speech Acts and Finer-Grained Event
Types in Chinese Text
Hongzhi Xu
Department of CBS
The Hong Kong Polytechnic University
hongz.xu@gmail.com
Chu-Ren Huang
Faculty of Humanities
The Hong Kong Polytechnic University
churenhuang@gmail.com
Abstract
Discriminating sentences that denote modalities and speech acts from the ones that describe or
report events is a fundamental task for accurate event processing. However, little attention has
been paid on this issue. No Chinese corpus is available by now with all different types of sen-
tences annotated with their main functionalities in terms of modality, speech act or event. This
paper describes a Chinese corpus with all the information annotated. Based on the five event
types that are usually adopted in previous studies of event classification, namely state, activi-
ty, achievement, accomplishment and semelfactive, we further provide finer-grained categories,
considering that each of the finer-grained event types has different semantic entailments. To d-
ifferentiate them is useful for deep semantic processing and will thus benefit NLP applications
such as question answering and machine translation, etc. We also provide experiments to show
that the different types of sentences are differentiable with a promising performance.
1 Introduction
Event classification is a fundamental task for NLP applications, such as question answering and ma-
chine translation, which need deep understanding of the text. Previous work (Siegel, 1999; Siegel and
McKeown, 2000; Palmer et al., 2007; Zarcone and Lenci, 2008; Cao et al., 2006; Zhu et al., 2000)
aims to classify events into four categories, namely state, activity, accomplishment and achievement, i.e.
Vendler?s framework adopted from linguistic studies (Vendler, 1967; Smith, 1991). High performance
was reported on the classification, however based on the assumption that all sentences describe an even-
t, which is not case in real text. Modalities and speech acts are not considered and no finer-grained
classification is proposed.
The aim for aspectual classification for a specific language is to build verb classes. In such framework,
viewpoint aspect in terms of perfective vs. imperfective is not considered. For example, he is eating a
sandwich and he ate a sandwich are all instances of accomplishment. However, we argue that this
framework is not enough for more accurate event processing. It is obvious that the two sentences have
different meanings and different consequences. The situation described by the first sentence is still going
on at the speech time, while the second sentence implies that the event has finished. So, in the perspective
of event processing, it is necessary and important to discriminate the two different aspects.
Another important issue is that not all sentences describe events. For example, Austin (1975) discrim-
inated two different types of sentences: constative and performative. Sentences that report or describe
events are in the first category. Sentences of the performative category mainly refer to speech (illocu-
tionary) acts, actions that are done by speech. For example, by uttering the sentence I declare that the
new policy will take effect from now on, the authorized speaker brings a new policy into effect. In this
case, uttering the sentence itself is an event. Discriminating speech acts are especially useful in speech
corpora, e.g. (Avila and Mello, 2013).
Modality is important due to its interaction with factuality and truth of the embedded propositions. For
example, he can eat two sandwiches describes a dynamic modality about the subject?s ability of eating.
This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer
are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/
157
However, no eating event has actually happened. Modality has been considered in modeling speaker?s
opinions (Benamara et al., 2012), machine translation (Baker et al., 2012), etc.
Sauri et al. (2006; 2012) proposed a framework for modeling modalities. However, their definition of
modality is a little different from that used by linguists. The main motivation of their work is to predict
the factuality of a proposition. As a result, all factors that may affect the factuality of propositions
are regarded as modalities. In our framework, we will adopt the definition in linguistic studies that
modality expresses a speaker?s belief or attitude on an embedded proposition (Palmer, 2001). Factuality
is determined by many factors other than modalities. However, we don?t want to mix all the factors
together in linguistic perspective.
In this paper, we will describe a Chinese corpus in which different sentence types are discriminated.
Finer-grained event types are also incorporated with a theory proposed in (Xu and Huang, 2013). The
details of the framework will be discussed in the next section.
The remaining of the paper is organized as follows. Section 2 introduces the theoretical framework we
shall adopt for our annotation. Section 3 describes a Chinese corpus we annotated with some statistical
information. Section 4 describes a classification experiment based on the annotated corpus. Section 5 is
the conclusion and our future work.
2 The Annotation Framework
In this section, we will give an introduction to the theoretical framework from a linguistic perspective.
There are two main levels for the classification. Sentences are first discriminated according to their main
functions, e.g. constative and performative (Austin, 1975). Constative sentences are further divided into
modality which mainly expresses the addresser?s propositional attitude and event which is a description
or report of a real situation without the speaker?s attitude. One basic assumption is that one sentence only
has one main function in terms of expressing speaker?s modality, speech act or describing an event. So,
there is no overlap among the three types of sentences.
2.1 Modality
Sentences denoting modalities are different from the sentences reporting events in that the former only
refers to a proposition upon which the speaker expresses his attitude its truth value, while the later is a
fact without incorporating speakers? opinions but only speaker?s perception. It is possible that speakers
can make mistakes in their perceptions. However it is beyond the linguistic level and there is no way to
predict the correctness based on the surface of the sentence. Thus, it is another issue out of the discussion
of this paper. We adopt the modal theory by Palmer (2001). According to him, modality could be divided
into epistemic, deontic and dynamic.
Epistemic modality expressed the speaker?s opinion on the truth of the embedded proposition in terms
of necessity and possibility. Informally, epistemic modality expresses what may be in the world. For
example, ta1 ken3ding4 zai4 ban4gong1shi4 ?he must be in his office? describes an epistemic modality
of the speaker that he is sure about the truth of the embedded proposition.
Deontic modality expresses what should be in the world, according to speaker?s expectations, certain
rules, laws and so on. For example, ni3 bi4xu1 zun1shou3 gui1ze2 ?You must obey the rules?.
Dynamic modality describes the abilities of a subject, such as ta1 hui4 you2you3 ?he can swim?, wo3
de0 ban4gong1shi4 ke3yi3 kan4jian4 da4hai3 ?you can see the ocean from my office?.
Evaluation is also treated as a modality in our framework. Evaluation describes the speaker?s opinion
on a proposition. It is different from epistemic in that it suggests rather than makes judgment on the truth
of a proposition. For example, ta1 suan4shi4 shi4jie4shang4 zui4hao3 de0 ge1shou3 le0 ?he should be
the best singer in the world?. Evaluative sentences only refer to those that contain explicit markers, e.g.
suan4shi4 ?should be?. The sentence ta1 shi4 shi4jie4shang4 zui4hao3 de0 ge1shou3 ?he is the best
singer in the world? is not treated as evaluation. In this sense, evaluative is not equivalent to subjective.
158
Exclamation is treated as a subset of evaluation. Take nian2qing1 ren2 a0 ! ?Young people!? for
example, it mostly expresses an implicit evaluation, e.g. only young people could do crazy things of
some kind, based on which the exclamation is expressed by the speaker.
2.2 Speech act
For speech act (illocutionary act), we adopt the theory by Searle (1976), where five different categories
are proposed, namely assertive, expressive, directive, commissive and declaration. In addition, we also
put interrogative sentences under this category. Speech act sentences only refer to those sentences that
are explicit utterances, e.g. the sentences quoted in text.
Assertive is to commit the speaker (in varying degrees) to something?s being the case or the truth of
the expressed proposition. For example, wo3 zheng4ming2 ta1 shi4 xue2sheng1 ?I certify that he is a
student?.
Expressive expresses the psychological state specified in the sincerity condition about a state of af-
fairs specified in the propositional content. Verbs for expressive speech act includes xie4xie4 ?thank?,
bao4qian4 ?apologize?, huan1ying2 ?welcome?, dui4bu4qi3 ?sorry? etc. For example, xie4xie4
bang1mang2 ?Thanks for your help?.
Directive is usually a command or requirement of the speaker to get the hearer to do something. For
example, ni3 guo4lai2 yi1xia4 ?Come here please?.
Commissive is to commit the speaker (in varying degrees) to some future course of action. For exam-
ple, wo3 hui4 bang1 ni3 ?I shall help you?.
Declaration is to bring about the correspondence between the propositional content and reality. Suc-
cessful performance guarantees that the propositional content corresponds to the world. For example,
wo3 xuan1bu4 ben3 ci4 hui4yi4 zheng4shi4 kai1mu4 ?The conference now start?.
Interrogative is an illocutionary act of the speaker that requires the hearer to provided some infor-
mation. For example, ni3 jiao4 shen2me0 ming2zi4 ? ?What?s your name?? and ni3 qu4 ting1 na4 ge4
jiang3zuo4 ma0 ? ?Will you attend the speech?? Interrogative sentences are usually with a question mark
???. However, not all sentences with question mark are interrogative. For example, rhetorical questions
usually don?t need the answer from the hearer. Instead, it actually expresses the speaker?s evaluation on
a situation. For example, the sentence wo3 zen3me0 ke3yi3 bu4 jin4xin1 zhao4gu4 ? ?How could I not
take care of him carefully?? should be labelled as evaluative modality rather than interrogative speech
act.
2.3 Events
Here, we describe a new framework by incorporating finer-grained event categories as described in (X-
u and Huang, 2013). Each of the finer-grained categories corresponds to only one of the five coarse
categories. So, it is an extension of and is compatible with the Vendler?s framework.
2.3.1 Primitive Events
According to Xu and Huang (2013), there are three event primitives, namely static state (S), dynamic state
(D), and change of state. Static state is equivalent to the previous notion state, which is a homogeneous
process, where all subparts are of the same kind of event. Dynamic state refers to an ongoing dynamic
process, e.g. running, eating etc., that is perceived like a state. Change of state is then defined as a change
from one state, either static or dynamic, to another state.
Change of state actually refers to the previous notion achievement. Theoretically, there are four type-
s of changes: static-static change (SS), static-dynamic change (SD), dynamic-static change (DS) and
dynamic-dynamic change (DD). In detail, SD change is somewhat equivalent to inceptive achievement,
and DS change is somewhat equivalent to terminative or completive achievement.
159
Event Type Representation Example
Static State ?- ta1 hen3 gao1 he is tall
Dynamic State ???? ta1 zai4 pao3bu4 he is running
SS Change ?|? ta1 bing4 le0 he got ill
SD Change ?|??? ta1 kai1shi3 pao3bu4 le0 he started running
DS Change ???|? ta1 ting2zhi3 pao3bu4 le0 he stopped running
DD Change ???|??? dian4nao3 qi3dong4 hao3 le0 the computer finished startup
Table 1: Primitives of Events.
Table 1 shows the extended event primitives with some illustrative examples. We use ??? and ????? to
denote static state and dynamic state respectively. ?|? is used to denote a temporal boundary. In case of
change of state, the temporal boundary overlap with the logical boundary, i.e. the change.
Negations usually denote static state. In Chinese, there are two negation adverbs, bu4 ?not? and
mei2you3 ?not?. However, they are different in that the former negates a generic event meaning that
such event doesn?t happen, while the latter negates the existence of an event instance. For example, ta1
bu4 he1jiu3 ?he doesn?t drink? describes an attribute of the subject, which is intrinsically a static state.
ta1 mei2you3 he1jiu3 ?he didn?t drink? describes a fact that there is no event instance of his drinking,
which is also a static state. Negation of a modality is still a modality. For example, ta1 bu4 ke3neng2
zai4 ban4gong1shi4 ?he cannot be in his office? still describes an epistemic modality.
2.3.2 Complex Events
Based on the primitives, we can compose complex events. Delimitative describes a temporal bounded
static state that has a potential starting point and ending point, within which the static state holds, e.g. ta1
bing4 le0 yi1 ge4 xing1qi1 ?he was ill for one week?. Process describes a temporal bounded dynamic
state that has a potential starting point and ending point, within which the dynamic state holds, e.g.
ta1 pao3 le0 yi1 ge4 xiao3shi2 ?he ran for one hour?. Semelfactive is different from Process in that its
durations is quite short and is usually perceived as instantaneous. In other words, the temporal boundaries
of semelfactive is usually naturally determined. For example, ta1 qiao1 le0 yi1 xia4 men2 ?he knocked
the door once?. There is no way to length the duration of the knocking action. However, a series of
iterative semelfactives could form dynamic process. For example, ta1 qiao1 le0 yi1 ge4 xiao3shi2 de0
men2 ?he knocked the door for an hour? gives a reading of iterative knocks.
For static state and dynamic state, we can only refer to their holding at a certain time point. In other
words, delimitative and process describe the life cycle of a state. For example, ta1 bing4 zhe0 ne0 ?he is
ill? and ta1 wan3shang4 jiu3dian3 de0 shi2hou0 zai4 pao3bu4 ?He was running at 9:00pm?. It is also
possible to claim that in a certain period, which for some reason became the focus of a conversation, a
state holds. For example, ta1 na4 liang3 tian1 dou1 bing4 zhe0 ?he was ill in that two days? and ta1
wan3shang4 jiu3dian3 dao4 shi2dian3 de0 shi2hou0 zai4 pao3bu4 ?From 9:00pm to 10:00pm, he was
running?. In this case, they are also state rather than delimitative or process. The difference is that there
is no information about the starts and the ends, while delimitative and process do.
Accomplishment is composed by a process with a final state. For example, ta1 xie3 le0 yi1 feng1 xin4
?he wrote a letter? describes an accomplishment composed by a writing process with a final state, i.e.
the existence of the letter. The final state of an accomplishment could also be dynamic. For example, ta1
ba3 dian4nao3 qi3dong4 le0 ?he started up the computer? describe an accomplishment with a dynamic
final state, i.e. the normal working of computer.
Some Resultative Verb Compounds (RVCs) in Chinese can denote achievements. However, they are
easy to be confused with accomplishment. Based on the representation, the difference of them is that
accomplishment encodes the start of the dynamic process, while achievement doesn?t. For example, ta1
xie3 wan2 le0 na4 feng1 xin4 ?He (write-)finished the letter? describes a DS change. To differentiate
them, we can use the yi3qian2 ?before? test. As in this example, ta1 xie3 wan2 na4 feng1 xin4 yi3 qian2
?before he finished the letter? refers to the period that includes the writing process. This means that
160
RVCs only focus on the final culminating point and are thus achievements. On the other hand, ta1 xie3
na4 feng1 xin4 zhi1 qian2 ?before he wrote the letter? refers to the period before the writing process. So,
ta1 xie3 le0 yi1 feng1 xin4 ?he wrote a letter? is then an accomplishment.
There is a counterpart for accomplishment, which is composed by an instantaneous dynamic process
(semelfactive) with a final state. RVCs can also denote instantaneous accomplishment. For example, ta1
da3sui4 le0 yi1 ge4 bei1zi0 ?he hit and broke a cup? is an accomplishment composed by a semelfactive
hitting action with a final state, i.e. the broken of the cup. Similarly, the final state could also be
dynamic. For example, in ta1 tan2zhuan4 le0 yi1 ge4 shai3zi0 ?He flicked and putted a spin on the
dice?, the predicate tan2zhuan4 ?flick-spin? is a compound that combines the predicate tan2 ?flick? and
zhuan4 ?spin?. The whole event is composed by a semelfactive flicking and a final dynamic state of the
dice?s spin.
Table 2 shows the seven event types with examples. Theoretically, there could be unlimited number
of complex events. However, the notions listed here are important in that they are the lexicalized units
which reflect the human?s cognition of real world events. For the perspective of computational linguis-
tics, discriminating all these linguistic events will be a fundamental step for deeper natural language
understanding.
2.3.3 The Neutral Aspect
Some sentences don?t include an explicit viewpoint aspect, e.g. without any aspectual markers. For
example, ta1 kan4 xiao3shuo1 ?he read novel? can possibly denote different event types in different
contexts. yi3qian2, ta1 kan4 xiao3shuo1 ?he read novel before? denotes an attribute of the subject
that he reads novels, while da4jia1 dou1 hen3mang2, xiao3hai2er0 xie3 zuo4ye4, ta1 kan4 xiao3shuo1
?Everyone is busy, children are doing homework, he is reading novels? describes a dynamic state. The
aspects of these examples are given by the specified contexts. Such sentences are usually called with
NEUTRAL aspect (Smith, 1991). In our framework, such sentences are ignored for now, unless the
context can help the annotator to figure out the aspectual information.
Semelfactive |?| ta1 qiao1 le0 qiao1 men2 ?he knocked the door?
Delimitative |?-| ta1 bing4 le0 yi1 ge4 xing1qing1 ?he was ill for one week?
Process |????| ta1 pao3 le0 yi1 ge4 xiao3shi2 ?he ran for an hour?
Instantaneous |?|? ta1 da3sui4 le0 bei1zi0 ?he broke the cup?
Accomplishment |?|??? ta1 tan2zhuan4 le0 yi1 ge4 shai3zi0 ?He putted a spin on the dice?
Accomplishment |????|? ta1 xie3 le0 yi1 feng1 xin4 ?he wrote a letter?
|????|??? ta1 ba3 dian4nao3 qi3dong4 le0 ?he started up the computer?
Table 2: Complex event types that are composed by more than one primitives.
The overall hierarchy is shown in Figure 1. Some traditional notions are kept in use e.g. accomplish-
ment and achievement. However, they now refer to event types rather than verb classes.
3 Annotating a Chinese Corpus
3.1 Data Selection
For annotation, we choose Sinica Treebank 3.0 (Huang et al., 2000), which contains more than 60,000
trees. Sinica Treebank is a subset of Sinica Corpus (Chen et al., 1996), which is a balanced corpus that
contains different genres of materials, including news, novels and some transcripts of spoken Chinese.
Sinica Treebank is annotated based on the Information-based Case Grammar (Chen and Huang, 1990).
The annotated syntactic and semantic information is kept for further studies, e.g. feature evaluation and
selection.
For annotation, we only select the sentences that are labeled as S and end with punctuation of period
???, exclamation ???, semicolon ??? and question mark ???. After removing duplicate sentences, we
get 5612 sentences Table 3 shows the detailed information of the raw corpus. There are 45728 tokens
from 11681 types in the corpus. For the heads of the sentences, there are 2127 different verbs.
161
Figure 1: Sentence type hierarchy.
Sentences Different Verbs Different Words Tokens Characters
5612 2127 11681 45728 75960
Table 3: Distribution information of the corpus for annotation.
3.2 Annotation Result
Each sentence is labeled as one specific finer-grained category from the 23 categories described in Sec-
tion 2. Whenever an example could not be decided by the annotator, it is discussed with another two
linguistic experts to make the final decision. However, we also did agreement test, which will be dis-
cussed later.
Finally, we annotated 1044 instances in modality, 764 speech act instances and 3811 event instances.
The distribution information is shown in Table 4. We can see that some event types, although theoretically
exist, don?t encounter any examples, such as the instantaneous accomplishment with dynamic final
state: |?|???.
Static state contains more than 40% instances. We think that it reflects the real distribution of event
types as we don?t make any bias for selecting data. Static state can be further divided into several
subcategories, e.g. attributive, relational, habitual, etc., which will be our future work.
Type No. Type No. Type No. Type No. Type No.
Epistemic 303 Assertive 64 ? 2475 ?|? 471 |?|? 257
Deontic 219 Expressive 13 ??? 166 |?|??? 0
Dynamic 111 Directive 65 |?| 6 ?|??? 96 |????|? 163
Evaluation 411 Commissive 58 |???| 48 ???|? 79 |????|??? 40
Interrogative 559 Declarative 2 |?| 4 ???|??? 2
Table 4: Distribution of different event types in the annotated corpus.
Table 5 shows the number of the main verbs regarding howmany event types they can denote excluding
modality and speech act. We can see that more than 200 verbs correspond to more than one category.
This shows that the verbs alone sometimes could not determine the event type.
162
No. of Event Types 1 2 3 4 5 6 7
No. of Verbs 1395 155 44 9 7 1 1
Table 5: Number of verbs with regard to how many event types they can denote.
Accuracy F1-Measure Kappa
Annotator 1 0.862 0.762 0.837
Annotator 2 0.821 0.677 0.784
Annotator 1+2 0.842 0.716 0.811
Table 6: Annotation agreements between the main annotator and annotator1, annotator 2, annotator 1+2.
Annotator 1+2 means the combination result of the two annotators, i.e. all the 2000 examples.
3.3 Agreement Evaluation
In order to test the reliability of the annotation, we randomly select 2000 examples from the corpus and
let another two linguists annotate them. Each of the linguists annotate half of them. The annotation
results are then compared with the main annotator. The agreements between the main annotator and the
other two annotators in terms of accuracy, F1 measure and Kappa value are shown in Table 6. The F1
measures are calculated based on the assumption that the main annotator?s result is the gold standard. The
result shows a very high agreement which means that our new framework for event type classification is
reliable and easy for annotation.
4 Automatic Classification of Chinese Sentences and Event Types
In this section, we conduct two classification experiments. The first is to discriminate the three sentence
types regarding their main functions, speech act, modality and event. The second is the classification
with the finer-grained categories. Before the experiments, we will first discuss the features that may help
for the classification.
4.1 Features
As suggested in previous literatures (Siegel, 1999; Siegel and McKeown, 2000; Zhu et al., 2000; Cao et
al., 2006), the following features are considered as important for event type classification.
Main verbs and their complements including argument structure are the most important indicators to
an event type. Negation of the main verb is a strong indicator for static state, as discussed above.
Aspectual markers, ? zhe0 ?ZHE?, ? le0 ?LE?, ? guo4 ?GUO? and some aspectual light verbs,
e.g. ? zai4 ?be doing?, ?? kai1shi3 ?start?, ?? ji4xu4 ?continue?, ?? ting2zhi3 ?stop?, ??
wan2cheng2 ?finish?, are strong indicators for different event types.
Temporal adverbials are also important features, which could potentially disambiguate neutral sen-
tences, e.g., yi3qian2, ta1 kan4 xiao3shuo1 ?he read novel before? as discussed above.
Frequency adverbs, such as?? jing1chang2 ?often?,?? ou3er3 ?sometimes?, etc., are indicators
for habitual states. For example, ta1 jing1chang2 qu4 he1jiu3 ?he often goes for drinking? is a habitual
state rather than a specific event.
Modalities could be expressed by auxiliaries, adverbs, sentence final particles etc. in Chinese. Adverbs
that modify the main verb, such as ?? ke3neng2 ?possibly?, are important features for identifying
modalities. Sentence final particles (SFP) and punctuation marks are also good indicators to evaluative
modality.
Since we don?t maintain a dictionary for the above indicators, we use a general feature set including
the dependency structure and the combinations of the dependent constituents. We suggest that the above
linguistic rules could be reflected by the dependency structures, which could be captured by the classi-
fiers. Meanwhile, the experiment result here is only to serve as a baseline for future comparisons. In all,
the features are listed in Table 7 with some examples.
163
ID Feature Example
f
1
Head head:word:kan4, head:pos:verb,
head:subj:word:ta1, head:subj:pos:pron,
head:obj:xp:NP, head:obj:xp:noun-noun
f
2
Dependency dep:word:ta1, dep:pos:pron,
dep:word:bu4, dep:pos:adv,
dep:word:xiao3shuo1, dep:pos:noun,
dep:word:le0, dep:pos:particle,
f
3
COMB subj:word:ta1-head:word:kan4-obj:xp:noun-noun,
subj:pos:pron-head:pos:verb-obj:xp:NP,
Table 7: Feature template we use for our classification of event types. Feature examples are based on the
sentence ta1 (he) bu4 (not) kan4 (read) zhen1tan4 (detective) xiao3shuo1 (novel) le0 (LE) ?he doesn?t
read detective novels any more?.
f
1
+f
2
+f
3
Prec Rec F1 Prec Rec F1 Prec Rec F1
Event 0.709 0.939 0.807 0.853 0.969 0.908 0.833 0.974 0.898
Modality 0.395 0.124 0.189 0.731 0.473 0.574 0.744 0.431 0.545
SpeechAct 0.430 0.130 0.199 0.829 0.664 0.737 0.845 0.609 0.707
MacroAvg 0.511 0.398 0.399 0.804 0.702 0.740 0.807 0.671 0.717
Accuracy 0.679 0.836 0.824
Table 8: Coarse level classification result.
4.2 Experimental Result
To give a real performance, the annotated syntactic and semantic information are not used. Instead, we
use the Stanford word segmenter (Tseng et al., 2005) and Stanford parser (Chang et al., 2009) to get the
syntactic structure of the sentences. All the experiment are results of 5-fold cross validation with a SVM
classifier implemented in LibSVM (Chang and Lin, 2011).
The result of the coarse level classification for modality, event and speech act is shown in Table 8.
We can see that the overall performance is reasonable. The F-Measure for modality is not as good as
the others. This is due to the fact that the modal markers and operators are quite critical for identifying
modalities, which may be sparse in our corpus. We suggest that maintaining a comprehensive dictionary
of modal operators could benefit the identification of the modalities. We can also see that the feature set
f
3
harms the performance, which is also caused by the feature sparseness problem.
For finer-grained classification, we use two different ways. The first way is to use a hierarchical
classification scheme. An instance is first classified as event, modality or speech act. According to the
result of the first round classification, the instance is put into the corresponding finer-grained model for
further classification. The second way is to classify all instances all at once based on a model trained on
all finer-grained categories.
Considering that some categories contain only few examples, which will provide unreliable evaluation
of the performance, we combined accomplishments with static final state and dynamic state, so does for
instantaneous accomplishment. We use ?=? to denote a general state, which could be either static or dy-
namic. Static state and delimitative are combined together, while dynamic state, process and semelfactive
are combined. Expressive, declarative and DD change are ignored in the experiments. The classification
results with feature sets f
1
and f
2
are shown in Table 9. The hierarchical classification is slightly better
than the all-at-once classification. Meanwhile, the accuracy for hierarchical classification is 0.621, which
is much better than the predominant guess 0.443.
We should note that parsing accuracy will significantly affect the result of event type classification.
This is true in the sense that the semantic content of words and their syntactic relations are all critical
164
All-At-Once Hierarchical
Precision Recall F1 Precision Recall F1
? 0.609 0.952 0.743 0.627 0.938 0.751
??? 0.840 0.078 0.142 0.830 0.069 0.127
?|? 0.454 0.384 0.415 0.473 0.418 0.443
?|??? 0.583 0.083 0.142 0.537 0.104 0.173
???|? 0 0 0 0 0 0
|????|=== 0.438 0.084 0.140 0.394 0.108 0.168
|?|=== 0.496 0.159 0.239 0.516 0.210 0.295
Epistemic 0.710 0.419 0.524 0.638 0.442 0.520
Deontic 0.629 0.360 0.455 0.573 0.383 0.457
Dynamic 0.388 0.233 0.290 0.391 0.287 0.330
Evaluation 0.592 0.319 0.412 0.523 0.302 0.382
Interrogative 0.844 0.789 0.815 0.818 0.789 0.803
Directive 0.692 0.309 0.418 0.695 0.354 0.458
Assertive 0 0 0 0.1 0.031 0.047
Commissive 0.83 0.277 0.409 0.713 0.155 0.246
MacroAvg 0.540 0.296 0.343 0.522 0.306 0.347
Accuracy 0.620 0.621
Table 9: 5-fold cross validation result of finer-grained classification with f
1
and f
2
features.
for the classification. Besides the parsing problem, there are other linguistic issues behind. Many modal
operators could result in different modalities, such as?? ying1gai1 ?should?,? hui4 ?will/can/may?,
? yao4 ?want/will/should/must? etc. Sometimes, it is hard to decide which meaning is correct in a
context. There may be also other linguistic issues that we have not discovered yet. This corpus thus
could be used for both linguistic study and computational applications, e.g. event processing.
5 Conclusion
In this paper, we present a Chinese corpus annotated with modalities, speech acts and finer-grained even-
t types. We also provide experiments on classification in different levels of categories with a general
feature set. The experimental result is acceptable concerning the difficult linguistic issues behind. In fu-
ture, we would like to continue our research work on improving the corpus and exploring more semantic
information including lexical semantic structures and lexical relations such as WordNet to improve the
performance of the classification.
Acknowledgements
The work is supported by a General Research Fund (GRF) sponsored by the Research Grants Council
(Project no. 543810 and 543512).
References
John Langshaw Austin. 1975. How to do things with words: Second Edition. Harvard University Press, Cam-
bridge, MA.
Luciana Beatriz Avila and Heliana Mello. 2013. Challenges in modality annotation in a brazilian portuguese
spontaneous speech corpus. Proceedings of WAMM-IWCS2013.
Kathryn Baker, Michael Bloodgood, Bonnie Dorr, Chris Callison-Burch, Nathaniel Filardo, Christine Piatko, Lori
Levin, and Scott Miller. 2012. Use of modality and negation in semantically-informed syntactic mt. Language
in Society, 38(2).
165
Farah Benamara, Baptiste Chardon, Yannick Mathieu, Vladimir Popescu, and Nicholas Asher. 2012. How do
negation and modality impact on opinions? In Proceedings of the Workshop on Extra-Propositional Aspects of
Meaning in Computational Linguistics, pages 10?18.
Defang Cao, Wenjie Li, Chunfa Yuan, and Kam-Fai Wong. 2006. Automatic chinese aspectual classification using
linguistic indicators. International Journal of Information Technology, 12(4):99?109.
Chih-Chung Chang and Chih-Jen Lin. 2011. Libsvm: a library for support vector machines. ACM Transactions
on Intelligent Systems and Technology, 2(3):1?27.
Pi-Chuan Chang, Huihsin Tseng, Dan Jurafsky, and Christopher D. Manning. 2009. Discriminative reordering
with chinese grammatical relations features. In Proceedings of the Third Workshop on Syntax and Structure in
Statistical Translation, pages 51?59.
Keh-Jiann Chen and Chu-Ren Huang. 1990. Information-based case grammar. In Proceedings of the 13th confer-
ence on Computational linguistics, pages 54?59.
Keh-Jiann Chen, Chu-Ren Huang, Li-Ping Chang, and Hui-Li Hsu. 1996. Sinica corpus: Design methodology
for balanced corpora. In Proceedings of Pacific Asia Conference on Language, Information and Computing
(PACLIC), pages 167?176.
Chu-Ren Huang, Feng-Yi Chen, Keh-Jiann Chen, Zhao ming Gao, and Kuang-Yu Chen. 2000. Sinica treebank:
design criteria, annotation guidelines, and on-line interface. In Proceedings of the second workshop on Chinese
language processing: held in conjunction with the 38th Annual Meeting of the Association for Computational
Linguistics, pages 29?37.
Alexis Palmer, Elias Ponvert, Jason Baldridge, and Carlota Smith. 2007. A sequencing model for situation entity
classification. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,
pages 896?903.
Frank Robert Palmer. 2001. Mood and Modality. Cambridge University Press, Cambridge.
Roser Sauri and James Pustejovsky. 2012. Are you sure that this happened? assessing the factuality degree of
events in text. Computational Linguistics, 38(2):261?299.
Roser Sauri, Marc Verhagen, and James Pustejovsky. 2006. Annotating and recognizing event modality in text. In
Proceedings of 19th International FLAIRS Conference, pages 333?338.
John R. Searle. 1976. A classification of illocutionary acts. Language in Society, 5(1):1?23.
Eric V. Siegel and Kathleen R. McKeown. 2000. Learning methods to combine linguistic indicators: Improving
aspectual classification and revealing linguistic insights. Computational Linguistics, 26(4):595?628.
Eric V. Siegel. 1999. Corpus-based linguistic indicators for aspectual classification. In Proceedings of the 37th
annual meeting of the Association for Computational Linguistics on Computational Linguistics, pages 112?119.
Carlotta Smith. 1991. The Parameter of Aspect. Kluwer Academic Publishers, Dordrecht.
Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel Jurafsky, and Christopher Manning. 2005. A conditional
random field word segmenter for sighan bakeoff 2005. In Proceedings of the Fourth SIGHAN Workshop on
Chinese Language Processing, volume 171.
Zeno Vendler, 1967. Linguistics in Philosophy, chapter Verbs and times, pages 97?121. Cornell University Press,
Ithaca.
Hongzhi Xu and Chu-Ren Huang. 2013. Primitives of events and the semantic representation. In Proceedings of
the 6th International Conference on Generative Approaches to the Lexicon, pages 54?61.
Alessandra Zarcone and Alessandro Lenci. 2008. Computational models for event type classification in context.
In Proceedings of the International Conference on Language Resource and Evaluation (LREC), pages 1232?
1238.
Xiaodan Zhu, Chunfa Yuan, Kam-Fai Wong, and Wenjie Li. 2000. An algorithm for situation classification of
chinese verbs. In Proceedings of the second workshop on Chinese language processing: held in conjunction
with the 38th Annual Meeting of the Association for Computational Linguistics, volume 12, pages 140?145.
166
