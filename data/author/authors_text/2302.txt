An Algorithm for Anaphora Resolution in 
Spanish Texts 
Manuel Palomar* 
University of Alicante 
Lidia Moreno t
Valencia University of Technology 
Jesfis Peral* 
University of Alicante 
Rafael Mufioz* 
University of Alicante 
Antonio Ferr~indez* 
University of Alicante 
Patricio Martinez-Barco* 
University of Alicante 
Maximiliano Saiz-Noeda* 
University of Alicante 
This paper presents an algorithm for identifying noun phrase antecedents ofthird person personal 
pronouns, demonstrative pronouns, reflexive pronouns, and omitted pronouns (zero pronouns) 
in unrestricted Spanish texts. We define a list of constraints and preferences for different ypes 
of pronominal expressions, and we document in detail the importance of each kind of knowledge 
(lexical, morphological, syntactic, and statistical) in anaphora resolution for Spanish. The paper 
also provides a definition for syntactic onditions on Spanish NP-pronoun oncoreference using 
partial parsing. The algorithm has been evaluated on a corpus of 1,677 pronouns and achieved 
a success rate of 76.8%. We have also implemented four competitive algorithms and tested their 
performance in a blind evaluation on the same test corpus. This new approach could easily be 
extended to other languages uch as English, Portuguese, Italian, or Japanese. 
1. Introduction 
We present an algorithm for identifying noun phrase antecedents of personal pro- 
nouns, demonstrative pronouns, reflexive pronouns, and omitted pronouns (zero pro- 
nouns) in Spanish. The algorithm identifies both intrasentential and intersentential 
antecedents and is applied to the syntactic analysis generated by the slot unifica- 
t ion parser (SUP) (Ferr~ndez, Palomar, and Moreno 1998b). It also combines different 
forms of knowledge by distinguishing between constraints and preferences. Whereas 
constraints are used as combinations of several kinds of knowledge (lexical, mor- 
phological, and syntactic), preferences are defined as a combination of heuristic rules 
extracted from a study of different corpora. 
We present he following main contributions in this paper: 
? an algorithm for anaphora resolution in Spanish texts that uses different 
kinds of knowledge 
* Department of Software and Computing Systems, Alicante, Spain. E-mail: (Palomar) 
mpalomar@dlsi.ua.es, (F rr~ndez) antonio@dlsi.ua.es, (Martfnez-Barco) patricio@dlsi.ua.es, (Peral) 
jperal@dlsi.ua.es, (Saiz-Noeda) max@dlsi.ua.es, (Mufioz) rafael@dlsi.ua.es 
t Department of Information Systems and Computation, Valencia, Spain. E-mail: hnoreno@dsic.upv.es 
@ 2001 Association for Computational Linguistics 
Computational Linguistics Volume 27, Number 4 
? an exhaustive study of the importance of each kind of knowledge in 
Spanish anaphora resolution 
? a proposal concerning syntactic onditions on NP-pronoun 
noncoreference in Spanish that can be evaluated on a partial parse tree 
? a proposal regarding preferences that are appropriate for resolving 
anaphora in Spanish and that could easily be extended to other 
languages 
? a blind test of the algorithm 
? a comparison with other approaches to anaphora resolution that we have 
applied to Spanish texts using the same blind test 
In Section 2, we show the classification scheme we used to identify the different ypes 
of anaphora that we would be resolving. In Section 3, we present he algorithm and 
discuss its main properties. In Section 4, we evaluate the algorithm. In Section 5, we 
compare our algorithm with several other approaches to anaphora resolution. Finally, 
we present our conclusions. 
2. Our Classification Scheme for Pronominal Expressions in Spanish 
In this section, we present our classification scheme for identifying the different ypes 
of anaphora that we will be resolving. Personal pronouns (PPR), demonstrative pro- 
nouns (DPR), reflexive pronouns (RPR), and omitted pronouns (OPa) are some of the 
most frequent ypes of anaphoric expressions found in Spanish and are the main 
subject of this study. Personal and demonstrative pronouns are further classified ac- 
cording to whether they appear within a prepositional phrase (PP) or whether they 
are complement personal pronouns (clitic pronouns1). We present examples for each 
of the four types of common anaphora. Each example is presented in three forms: as a 
Spanish sentence, as a word-to-word translation into English, and correctly translated 
into English. 2
2.1 Clitic Personal Pronouns (CPPR) 
In the case of clitic personal pronouns, I0, la, le 'him, her, it' and los, las, les 'them', we 
consider that the third person personal pronoun plays the role of the complement. 
(1) Ana abre \[la verja\]i y lai cierra tras de si. 
Ana opens \[the gate\]/ and it/ closes after herself 
'Ana opens the gate and closes it after herself.' 
2.2 Personal Pronouns Not Included in a PP (PPanotPP) 
We include in this class the personal pronouns ~l, ella, ello 'he, she, it' and ellas, ellos 
'they'. 
(2) Andr6si es mi vecino, t~li vive en el segundo piso. 
Andr6si is my neighbor Hei lives on the second floor 
'Andr6s is my neighbor. He lives on the second floor.' 
1 According to Mathews (1997), aclitic pronoun is a pronoun that is treated as an independent word in 
syntax but that forms a phonological unit with the verb that precedes or follows it. 
2 Coindexing indicates coreference b tween anaphor and antecedent. 
546 
Palomar et al Anaphora Resolution in Spanish Texts 
2.3 Personal Pronouns Included in a PP (PPRinPP) 
We include in this class the personal pronouns dl, ella, ello 'him, her, it' and ellas, ellos 
'them'. 
(3) Juan/ debe asistir pero Pedro lo har~i por 61i. 
Juani must attend but Pedro it will do for himi 
'Juan must attend but Pedro will do it for him.' 
2.4 Demonstrative Pronouns Not Included in a PP (DPRnotPP) 
We include in this class the demonstrative pronouns ~ste, dsta, esto 'this'; ~stos, ~stas 
'these'; dse, ~sa, aqu~l, aqu~lla 'that'; and dsos, dsas, aqu~llos, aqudllas 'those'. 
(4) E1 Ferrarii gan6 al Ford. t~stei es el mejor. 
the Ferrarii beat the Ford This/ is the best 
'The Ferrari beat the Ford. This is the best.' 
2.5 Demonstrative Pronouns Included in a PP (DPRinPP) 
We include in this class the demonstrative pronouns ~ste, ~sta, esto 'this'; ~stos, ~stas 
'these'; dse, ~sa, aqudl, aqudlla 'that'; and dsos, ~sas, aqu~llos, aqudllas 'those'. 
(5) Ana vive con Pacoi y cocina para 6stei diariamente. 
Ana lives with Pacoi and cooks for this/ every day 
'Ana lives with Paco and cooks for him every day.' 
2.6 Reflexive Pronouns (RPR) 
We include in this class the reflexive pronouns e, sL si mismo 'himself, herself, itself' 
and consigo, consigo mismo 'themselves'. 
(6) Anai abre la verja y la cierra tras de sfi. 
Anai opens the gate and it closes after herself/ 
'Ana opens the gate and closes it after herself.' 
2.7 Omitted Pronouns (Zero Pronouns OPa) 
The omitted pronoun is the most frequent ype of anaphoric expression in Spanish, as 
we will show in Section 4.2. Omitted pronouns occur when the pronominal subject is 
omitted. This kind of pronoun also occurs in other languages, such as Portuguese or 
Japanese; in these languages, it can also appear in object position, whereas in Spanish 
or Italian, it can appear only in subject position. In the following example, the omission 
is represented by the symbol 13 (the symbol does not appear in the correct ranslation 
into English). 
(7) Anai abre la verja y (~i la cierra tras de sf. 
Anai opens the gate and Oi it closes after herself 
'Ana opens the gate and she closes it after herself.' 
3. Anaphora Resolution Algorithm 
In the algorithm, all the types of anaphora are identified from left to right as they 
appear in the sentence. The most important proposals for anaphora resolution--such 
as those of Baldwin (1997), Lappin and Leass (1994), Hobbs (1978), or Kennedy and 
Boguraev (1996)--are based on a separation between constraints and preferences. 
547 
Computational Linguistics Volume 27, Number 4 
Constraints discard some of the candidates, whereas preferences simply sort the re- 
maining candidates. A constraint defines a property that must be satisfied in order 
for any candidate to be considered as a possible solution of the anaphor. For example, 
pronominal anaphors and antecedents must agree in person, gender, and number. 3 
Otherwise, the candidate is discarded as a possible solution. A preference is a charac- 
teristic that is not always satisfied by the solution of an anaphor. The application of 
preferences usually involves the use of heuristic rules in order to obtain a ranked list 
of candidates. 
Each type of anaphora has its own set of constraints and preferences, although 
they all follow the same general algorithm: constraints are applied first, followed by 
preferences. 
Based on the preceding description, our algorithm contains the following main 
components: 
? identification of the type of pronoun 
? constraints 
- -  morphological greement (person, gender, and number) 
- -  syntactic onditions on NP-pronoun oncoreference 
? preferences 
In order to apply this algorithm to unrestricted texts, it has been necessary to use 
partial parsing. In our partial-parsing scheme, as presented in Ferr~ndez, Palomar, and 
Moreno (1999), we only parse coordinated NPs and PPs, verbal chunks, pronouns, and 
what we have called free conjunctions (i.e., conjunctions that do not join coordinated 
NPs or PPs). Words that do not appear within these constituents are simply ignored. 
The NP constituents include coordinated adjectives, relative clauses, coordinated PPs, 
and appositives as modifiers. 
With this partial-parsing scheme, we divide a sentence into clauses by parsing first 
the free conjunction and then the verbs, as in the following example: 
(8) Pedro compr6 un regalo y se lo dio a Ana. 
Pedro bought a gift and her it gave to Ana 
'Pedro bought a gift and gave it to Ana.' 
In this example, we have parsed the following constituents: np(Pedro), v(comprO), np(un 
regalo),freeconj(y), pron(se), pron(lo), v(dio), pp(a Ana). We are able to divide this sentence 
into two clauses because it contains the free conjunction y 'and' and the two verbs 
compr6 'bought' and clio 'gave'. 
3.1 Identification of the Kind of Pronoun 
The algorithm uses partial-parse trees to automatically identify omitted pronouns by 
employing the following steps: 
? The sentence is divided into clauses (by parsing the free conjunction 
followed by the verbs). 
3 In our implementation, thismorphological information is extracted from the part-of-speech tagger. 
548 
Palomar et al Anaphora Resolution in Spanish Texts 
An NP or pronoun is sought for each clause by analyzing the clause 
constituents on the left-hand side of the verb, unless the verb is 
imperative or impersonal. The chosen NP or pronoun must agree in 
person and number with the clausal verb. (In evaluating this algorithm, 
Ferr~ndez and Peral \[2000\] achieved a success rate of 88% for detecting 
omitted pronouns.) 
The remaining pronouns are identified based on part-of-speech (POS) tagger out- 
puts. 
3.2 Morphological Agreement 
Person, gender, and number agreement are checked in order to discard potential an- 
tecedents. For example, in the sentence 
(9) Juanj vio a Rosa/. Ella/ estaba muy feliz. 
Juanj saw to Rosa/ Shei was very happy 
'Juan saw Rosa. She was very happy.' 
there are two possible antecedents for ella 'she', whose slot structures 4 are 
np (conc (sing, masc), X, Juan) 
np (conc (sing, fem), Y, Rosa) 
whereas the slot structure of the pronoun is 
pron (conc (sing, fem), Z, ella). 
In order to decide between the two antecedents, he unification of both slot struc- 
tures (pronoun and candidate) is carried out by the slot unification parser (Ferr~ndez, 
Palomar, and Moreno 1999). In this example, the candidate Juan is rejected by this 
morphological greement constraint. 
3.3 Syntactic Conditions on NP-Pronoun Noncoreference 
These conditions are based on c-command and minimal-governing-category constraints 
as formulated by Reinhart (1983) and on the noncoreference conditions of Lappin and 
Leass (1994). They are of great importance in any anaphora resolution system that 
does not use semantic information, as is the case with our proposal. In such systems, 
recency is important in selecting the antecedent of an anaphor. That is to say, the 
closest NP to the anaphor has a better chance of being selected as the solution. One 
problem, however, is that such constraints are formulated using full parsing, whereas 
if we want to work with unrestricted texts we should be using partial parsing, as 
previously defined. 
We have therefore proposed a set of noncoreference onditions for Spanish, using 
partial parsing, although they could easily be extended to other languages such as En- 
glish. In our system, the following types of pronouns are noncoreferential with a noun 
phrase (NP) under the conditions noted (noncoindexing indicates that a candidate is
rejected by these conditions). 
4 The term slot structure is defined in Ferr~ndez, Palomar, and Moreno (1998b). The slot structure stores 
morphological and syntactic information related to the different constituents of a sentence. 
549 
Computational Linguistics Volume 27, Number 4 
. 
(a) 
(b) 
(c) 
. 
(a) 
Reflexive pronouns are noncoreferential when: 
(b) 
(10) 
the NP is included in another constituent (e.g., the NP is 
included in a PP) 
Ante Luisj sei frot6 con la toalla. 
in front of Luisj himself/ rubbed with the towel 
'He rubbed himself with the towel in front of Luis.' 
In this sentence, we would have obtained the following sequence 
of constituents after our partial-parsing scheme: pp(prep(ante), 
np(Luis )) , pron(se) , v(frot6 ) , pp(prep( con) , np(la toalla) . Following 
the above-stated condition, the NP Luis cannot corefer with the 
reflexive pronoun se since Luis is included in a PP (ante Luis). 
the NP is in a different clause or sentence 
(11) Anaj trajo un cuchillo y Eva/ sei cort6. 
Anaj brought a knife and Eva/ herself/ cut 
'Ana brought a knife and Eva cut herself.' 
the NP appears after the verb and there is another NP in the 
same clause before the verb 
(12) 
(13) 
Juan/ sei cort6 con el cuchilloj. 
Juan/ himself/ cut with the knifej 
'Juan cut himself with the knife.' 
Under these conditions, coreference is allowed between the NP 
and the reflexive pronoun, since both are in the same clause. For 
example: 
Juan/ queria verlo por s~ mismoi. 
Juan/ wanted see it for himself/ 
'Juan wanted to see it for himself.' 
In this example, Juan and the reflexive pronoun si mismo 
'himself' corefer since Juan is in the same clause as the anaphor, 
it is not included in another constituent, and it appears before 
the verb. 
Clitic pronouns are noncoreferential when: 
the NP is included in a PP (except hose headed by the 
preposition a 'to') 
(14) Con Juan/ loj compr6. 
with Juan/ itj bought 
'I bought it with Juan.' 
the NP is located more than three constituents before the clitic 
pronoun in the same clause 
(15) En casai \[el martillo\]j no se loj di. 
at home/ \[the hammer\]j not him itj gave 
'I didn't give him the hammer at home.' 
550 
Palomar et al Anaphora Resolution in Spanish Texts 
. 
(a) 
(17) 
(b) 
In this example, the direct object el martillo 'the hammer '  has 
been moved from its common position after the verb, and it is 
necessary to fill the resulting gap with the pronoun lo 'it' even 
though it does not appear in the English translation. This 
phenomenon 5 can be considered an exception to the c-command 
constraints as formulated by Reinhart when applied to Spanish 
clitic pronouns. 
Moreover, if the last two conditions are not fulfilled by the NP and the 
verb is in the first or second person, then this NP will necessarily be the 
solution of the pronoun: 
(16) \[El boligrafo\]i 1Oi comprar~s en esa tienda. 
\[The pen\]/ iti will buy in that shop 
'You will buy the pen in that shop.' 
Personal and demonstrative (nonclitic) pronouns are noncoreferential 
when the NP is in the same clause as the anaphor, and: 
the pronoun comes before the verb (in full parsing, this would 
mean that it is the subject of its clause) 
Ante Luisi 61j salud6 a Pedrok. 
in front of Luisi hey greeted to Pedrok 
'He greeted Pedro in front of Luis.' 
the pronoun comes after the verb (in full parsing, this would 
mean that it is the object of the verb) and the NP is not included 
in another NP 
(18) \[El padre de Juanj\]i le venci6 a 41j. 
\[Juanj's father\]/ him beat to himj 
'Juan's father beat him.' 
In this example, the pronoun ~I 'him' cannot corefer with the NP 
el padre de Juan 'Juan's father', but it can corefer with Juan since it 
is a modifier of the NP el padre de Juan. 
It should be mentioned that the clitic pronoun le is another 
form of the pronoun dl 'him'. This is a typical phenomenon i
Spanish, where clitic pronouns occupy the object position. 
Sometimes both the clitic pronoun and the object appear in the 
same clause, as occurs in the previous example and in the 
following one: 
(19) A Pedro/ yo lei vi ayer. 
to Pedroj I himi saw yesterday 
'I saw Pedro yesterday.' 
This example also illustrates the previously mentioned exception 
of c-command constraints for Spanish clitic pronouns. In this 
case, the direct object a Pedro 'to Pedro' has been moved before 
the verb, and the clitic pronoun le 'him' has been added. It 
should also be remarked that, as noted earlier, the clitic pronoun 
does not appear in the English translation. 
5 Mathews (1997) calls this phenomenon "clitic doubling" and defines it as the use of a clitic pronoun 
with the same referent and in the same syntactic function as another element in the same clause. 
551 
Computational Linguistics Volume 27, Number 4 
(c) the pronoun is included in a PP that is not included in another 
constituent and the NP is not included in another constituent 
(NP or PP) 
(20) \[El padre de Luisj\]i juega con 61j. 
\[Luisj's father\]/ plays with himj 
'Luis's father plays with him.' 
In this example, the pronoun ~I 'him' is included in a PP (which 
is not included in another constituent) and the NP el padre de 
Luis is not included in another NP or PP. Therefore, the NP 
cannot corefer with the pronoun. However, the NP Luis can 
corefer because it is included in the NP el padre de Luis. 
(d) the pronoun is included in an NP, so that the NP in which the 
pronoun is included cannot corefer with the pronoun 
(21) Pedro/ vio \[al hermano de ~li\] j. 
Pedro/ saw \[the brother of himi\]j 
'Pedro saw his brother.' 
(e) the pronoun is coordinated with other NPs, so that the other 
coordinated NPs cannot corefer with the pronoun 
(22) Juan/, \[el tio de Ana\]j, y 61k fueron de pesca. 
Juan/, \[Ana's uncle\]j, and hek went fishing 
'He, Juan, and Ana's uncle went fishing.' 
(f) the pronoun is included in a relative clause, and the following 
condition is met: 
. 
(24) 
i. the NP in which the relative clause is included does not 
corefer with the pronoun 
(23) Pedroj vio a \[un amigo que juega con 41j\]i. 
Pedroj saw to \[a friend that plays with himj\]i 
'Pedro saw a friend that he plays with.' 
ii. the NPs that are included in the relative clause follow 
the previous conditions 
iii. the remaining NPs outside the relative clause could 
corefer with the pronoun 
Personal and demonstrative (nonclitic) pronouns are noncoreferential 
when the NP is not in the same clause as the pronoun. (In this case, the 
NP can corefer with the pronoun, except when this NP also appears in 
the same sentence and clause as the pronoun, in which case it will have 
been discarded by the previous noncoreference onditions.) 
Anaj y Evai son amigas. Evai lej ayuda mucho. 
Anay and Evai are friends Evai herj helps a lot 
'Ana and Eva are friends. Eva helps her a lot.' 
It is important o note that the above-mentioned conditions refer to those coor- 
dinated NPs and PPs that have been partially parsed. Moreover, as previously men- 
tioned, NPs can include relative clauses, appositives, coordinated PPs, and adjectives. 
552 
Palomar et al Anaphora Resolution in Spanish Texts 
We should also remark that we consider aconstituent A to be included in a constituent 
B if A modifies the head of B. Let us consider the following NP: 
(25) \[el hombre que ama a \[una mujer que lei ama\]j\]i 
\[the man who loves to \[a woman who him/ loveslj\]i 
'the man who loves a woman who loves him.' 
We consider that the pronoun le 'him' is included in the relative clause that mod- 
ifies the NP una mujer que le ama 'a woman who loves him', which then cannot corefer 
with it due to noncoreference ondition 3(f)i. Under condition 3(f)iii, however, the 
pronoun le 'him' could corefer with the entire NP el hombre que area a una mujer que le 
area 'the man who loves a woman who loves him'. 
Another example might be the following: 
(26) Eva/ tiene \[un tio que lei toma el pelo\]j. 
Evai has \[an uncle that heri teases\]j 
'Eva has an uncle who teases her.' 
In this example, the pronoun is included within the relative clause that modifies un 
tio 'an uncle', and therefore cannot corefer with it. But, following condition 3(f)iii, it 
can corefer with Eva. 
3.4 Preferences 
To obtain the different sets of preferences, we utilized the training corpus to identify 
the importance of each kind of knowledge that is used by humans when tracking 
down the NP antecedent of a pronoun. Our results are shown in Table 1. For our 
analysis, the antecedents for each pronoun in the text were identified, along with their 
configurational characteristics with reference to the pronoun. Thus, the table shows 
how often each configurational characteristic is valid for the solution of a particular 
pronoun. For example, the solution of a reflexive pronoun is a proper noun 53% of the 
time. The total number of pronoun occurrences in the study was 575. Thus, we were 
able to define the different patterns of Spanish pronoun resolution and apply them in 
order to obtain the evaluation results that are presented in this paper. The order of 
importance was determined by first sorting the preferences according to the percentage 
of each configurational characteristic; that is, preferences with higher percentages were 
applied before those with lower percentages. After several experiments on the training 
corpus, an optimal order--the one that produced the best performance--was obtained. 
Since in this evaluation phase we processed texts from different genres and by different 
authors, we can state that the final set of preferences obtained and their order of 
application can be used with confidence on any Spanish text. 
Based on the results presented in Table 1, we have extracted a set of preferences for 
each type of anaphora (listed below). We have distinguished between those pronouns 
that are included within PPs and those that are not. That is because when a pronoun 
is included in a PP, the preposition of this PP sets a preference. 
Preferences of omitted pronouns (OPR): 
1. NPs that are not of time, direction, quantity, or abstract type; that is to 
say, inanimate candidates are rejected (e.g., hal~past en, Market Street, 
three pounds, or a thing) 
2. NPs in the same sentence as the omitted pronotm 
553 
Computat ional  Linguistics Volume 27, Number  4 
Table 1 
Percentage validity of types of pronouns for different configuration characteristics of the 
training corpus (n = 575). 
CPPR RPR OPR PPRinPP DPRinPP PPRnotPP DPRnotPP 
Intrasentential 66 97 57 70 100 60 75 
Intersentential 34 3 43 30 0 40 25 
NPSentAnt ~ 9 3 4 16 50 9 38 
AntPPin b 7 9 14 27 50 20 25 
AntProper c 57 53 63 35 0 43 0 
AntIndef a 13 0 7 0 0 6 13 
AntRepeaff 72 66 79 65 50 71 50 
AntWithVerb f 14 94 20 24 0 26 25 
EqualPP g 100 100 100 78 100 97 100 
EqualPosVerb h 79 84 89 46 0 86 38 
BeforeVerb i 83 91 89 65 50 86 13 
NoTime d 100 100 100 100 100 100 100 
NoQuant i ty  k 100 100 100 100 100 97 100 
NoDirect ion I 100 100 100 97 100 100 100 
NoAbstract m 100 100 100 100 100 100 100 
NoCompany n 100 100 100 100 100 100 100 
a If the NP 
b If the NP 
c If the NP 
d If the NP 
e If the NP 
f If the NP 
g If the NP 
h If the NP 
i If the NP 
j If the NP 
k If the NP 
1 If the NP 
m If the NP 
n If the NP 
is included in another NP 
is included in a PP with the preposition en 'in' 
is a proper noun 
is an indefinite NP 
has been repeated more than once in the text 
has appeared with the verb of the anaphor more than once in the text 
has appeared in a PP more than once in the text 
occupies the same position with reference to the verb as the anaphor (before or after) 
appears before its verb 
is not a time-type 
is not a quantity-type 
is not a direction-type 
is not an abstract-type 
is not a company-type 
3. NPs  that  are in the same sentence  as the  anaphor  and  are also the  
so lu t ion  for  another  omi t ted  pronotm 
4. NPs  that  are in the prev ious  sentence  
5. NPs  that  are not  inc luded  in another  NP  (e.g., when they  appear  ins ide  
a re la t ive  c lause  or  appos i t i ve )  
6. NPs  that  are not  inc luded  in a PP or  are  inc luded  in a PP  when its 
p repos i t ion  is a ' to '  or  de ' o f '  
7. NPs  that  appear  be fore  the  verb  
8. NPs  that  have  been  repeated  more  than  once  in the  text  
Preferences of clitic personal pronouns (CPPR): 
1. NPs  that  are not  of  t ime,  d i rect ion ,  quant i ty ,  or  abst ract  type  
2. NPs  that  are in  the  same sentence  as the  anaphor  
554 
Palomar et al Anaphora Resolution in Spanish Texts 
3. NPs that are in the previous sentence 
4. NPs that are not included in another NP (e.g., when they appear inside 
a clause or appositive) 
5. NPs that are not included in a PP or are included in a PP when its 
preposit ion is a 'to' or de 'of '  
6. NPs that have appeared with the verb of the anaphor more than once 
Preferences of personal and demonstrative pronouns that are included in a PP 
(PPRinPP and DPRinPP): 
1. NPs that are not of time, direction, quantity, or abstract ype; moreover, 
in the case of personal pronouns, the NP cannot be a company type 
2. NPs that are in the same sentence as the anaphor 
3. NPs that are in the previous sentence 
4. NPs that are not included in another NP (e.g., when they appear inside 
a relative clause or appositive) 
5. NPs that have been repeated more than once in the text 
6. NPs that are included in a PP 
7. NPs that occupy the same position (before or after) with respect o the 
verb as the anaphor 
Preferences of personal and demonstrative pronouns that are not included in a PP 
and of reflexive pronouns (PPRnotPP, DPRnotPP, and RPR): 
1. NPs that are not of time, direction, quantity, or abstract ype; moreover, 
in the case of personal pronouns, the NP cannot be a company type 
2. NPs that are in the same sentence as the anaphor 
3. NPs that are in the previous sentence 
4. NPs that are not included in another NP (e.g., when they appear inside 
a relative clause or appositive) 
5. NPs that are not included in a PP or that are included in a PP when its 
preposit ion is a 'to' or de 'of '  
6. For the case of personal pronouns (PPRnotPP), NPs that are not 
included in a PP with the preposit ion en ' in' 
7. NPs that appear before their verbs (i.e., the verb of the sentence in 
which the NP appears) 
3.5 Resolution Procedure 
The resolution procedure consists of the following steps: 
1. Identify the type of anaphora: pronominal  (PPRinPP or PPRnotPP), 
demonstrat ive (DPRinPP or DPRnotPP), reflexive (RPR), or omitted 
(oPR). 
555 
Computational Linguistics Volume 27, Number 4 
2. Identify the NP candidate antecedents of a pronoun in order to create a 
list L. The list created will depend on the type of anaphor and the 
anaphoric accessibility space (empirically obtained from a deep study 
of the training corpus) and will be developed according to the 
following criteria: 
? For pronominal anaphora, demonstrative anaphora, and 
omitted pronouns, NP candidates will appear in the same 
sentence as the anaphor and in the four previous sentences. 
? For reflexive anaphora, NP candidates will appear in the same 
sentence as the anaphor. 
3. Apply constraints to L to obtain LI: 
(a) morphological agreement 
(b) syntactic onditions on NP-pronoun noncoreference 
4. If the number of elements of L1 - 1, then the solution is that element. 
5. If the number of elements of L1 = 0, then the solution is an exophor. 
6. If the number of elements of L1 > 1, then apply preferences to L1 to 
obtain L2. Depending on the type of anaphora, a different set and order 
of preferences will be applied (see Section 3.4). 
7. If the number of elements of L2 = 1, then the solution is that element. 
8. If the number of elements of L2 > 1, then apply the following three 
basic preferences in the order shown until only one candidate remains 
(these three preferences are common to all the pronouns): 
? NPs most repeated in the text 
? NPs that have appeared most with the verb of the anaphor 
? the first candidate of the remaining list (the closest one to the 
anaphor) 
After applying these basic preferences, the antecedent is obtained. 
4. Empirical Evaluation 
4.1 Description of Corpora 
We have tested the algorithm on both technical manuals and literary texts. In the first 
instance, we used a portion of the Spanish edition of the Blue Book corpus. 6 This 
corpus contains the handbook of the International Telecommunications Union CCITT, 
published in English, French, and Spanish; it is one of the most important collections of 
telecommunications texts available and contains 5,000,000 words automatically tagged 
by the Xerox tagger. In the second instance, the algorithm was tested on Lexesp, a 
corpus 7 that contains Spanish literary texts from different genres and by different 
6 CRATER (Proyecto CRATER 1994-1995) Corpus Resources and Terminology Extraction Project. Project 
supported by the European Community Commission (DG-XIII). Computational Linguistics Laboratory, 
Faculty of Philosophy and Fine Arts, Autonomous University of Madrid, Spain. 
7 The Lexesp corpus belongs to the project of the same name carried out by the Psychology Department 
of the University of Oviedo and developed by the Computational Linguistics Group of the University 
of Barcelona, with the collaboration fthe Language Processing Group of the Catalonia University of 
Technology, Spain. 
556 
Palomar et al Anaphora Resolution in Spanish Texts 
Table 2 
Pronoun occurrences in two types of texts. 
Total BB Corpus Lexesp Corpus 
Number of pronoun occurrences 
in the training corpus 575 123 
Number of pronoun occurrences 
in the test corpus 1,677 375 
452 
1,302 
authors. These texts were mainly obtained from newspapers and were automatically 
tagged by a different agger than the one used to tag the Blue Book. The portion of 
the Lexesp corpus that we processed contained various stories, related by a narrator, 
and written by different authors. As was the case for the Blue Book corpus, this 
corpus also contained 5,000,000 words. Since we worked on texts from different genres 
and by different authors, the applicability of our proposal to other kinds of texts is 
assured. 
We selected a subset of the Blue Book corpus and another subset of the Lex- 
esp corpus, and both were annotated with respect o coreference. One portion of the 
coreferentially tagged corpus (training corpus) was used for improving the rules for 
anaphora resolution (constraints and preferences), and another portion was reserved 
for test data (Table 2). 
The annotation phase was accomplished in the following manner: (1) two annota- 
tors were selected, (2) an agreement was reached between the annotators with regard 
to the annotation scheme, (3) each annotator annotated the corpus, and, finally, (4) a 
reliability test (Carletta et al 1997) was done on the annotation in order to guaran- 
tee the results. The reliability test used the kappa statistic that measures agreement 
between the annotations of two annotators in making judgments about categories. In 
this way, the annotation is considered a classification task consisting of defining an ad- 
equate solution among the candidate list. According to Vieira (1998), the classification 
task when tagging anaphora resolution can be reduced to a decision about whether 
each candidate is the solution or not. Thus, two different categories are considered 
for each anaphor: one for the correct antecedent and another for nonantecedents. Our 
experimentation showed one correct antecedent among an average of 14.5 possible 
candidates per anaphor after applying constraints. For computing the kappa statistic 
(k), see Siegel and Castellan (1988). 
According to Carletta et al (1997), a k measurement such as 0.68 < k < 0.8 allows 
us to draw encouraging conclusions, and a measurement k > 0.8 means there is to- 
tal reliability between the results of the two annotators. In our tests, we obtained a 
kappa measurement of k = 0.81. We therefore consider the annotation obtained for the 
evaluation to be totally reliable. 
4.2 Experimental Work 
We conducted a blind test over the entire test corpus of unrestricted Spanish texts by 
applying the algorithm to the partial syntactic structure generated by the slot unifica- 
tion parser. 
Over these corpora, our algorithm attained a success rate for anaphora resolution 
of 76.8%. We define "success rate" as the number of pronouns successfully resolved, 
divided by the total number of resolved pronouns. The total number of resolved pro- 
nouns was 1,677, including personal, demonstrative, reflexive, and omitted pronouns. 
557 
Computational Linguistics Volume 27, Number 4 
Table 3 
Results of blind test. 
CPPR RPR OPR PPRinPP DPRinPP PPRnotPP DPRnotPP Total 
Num. of 
pronoun 
occurrences 228 80 1,099 107 20 94 49 1,677 
Num. of 
cases 
correctly 162 74 868 70 17 64 34 1,289 
resolved 
Success 
rate 71.0% 92.5% 78.9% 65.4% 85.0% 68.0% 69.3% 76.8% 
All of them were in the third person, with a noun phrase that appeared before the 
anaphor as their antecedent. Our algorithm's "recall percentage," defined as the num- 
ber of pronouns correctly resolved, divided by the total number of pronouns in the 
text, was therefore 76.8%. A breakdown of success rate results for each kind of pro- 
noun is also shown in Table 3. The pronouns were classified so as to provide the 
option of applying different kinds of knowledge to resolve each category of pronoun. 
One of the factors that affected the results was the complexity of the Lexesp corpus, 
due mainly to its complex narratives. On average, 16 words per sentence and 27 
candidates per anaphor were found in this corpus. 
In our experiment, a "successful resolution" occurred if the head of the solution 
offered by our algorithm was the same as that offered by two human experts. We 
adopted this definition of "success" because it allowed the system to be totally auto- 
matic: solutions given by the annotators were stored in a file and were later automat- 
ically compared with the solutions given by our system. Since semantic information 
was not used at all, PP attachments were not always correctly disambiguated. Hence, 
at times the differences imply corresponded to different subconstituents. 
After the evaluation process, we tested the results in order to identify the lim- 
itations of the algorithm with respect to the resolution process. We identified the 
following: 
? There were some mistakes in the POS tagging (causing an error rate of 
around 3%). 
? There were some mistakes in the partial parsing with respect o the 
identification of complex noun phrases (causing an error rate of around 
7%) (Palomar et al 1999). 
? Semantic information was not considered (causing an error rate of 
around 32%). An example of this type of error can be seen in the 
following text extracted from the Lexesp corpus: 
(27) Recuerdo, pot ejemplo, \[un pequefio claro en un bosque en 
medio de las montafias canadienses\]i, con tres lagunas diminutas 
que, a causa de los sedimentos del agua. tenfan distintos y chocantes 
colores. Esta rareza habia hecho del sitioi un espacio sagrado al que 
peregrinaron los indios durante siglos y seguramente antes los 
pobladores paleolfticos. Y eso se notaba. 
558 
Palomar et al Anaphora Resolution in Spanish Texts 
(28) 
Canad~i es un pals muy hermoso, y aqu41i no era, ni mucho 
rnenos, el lugar m~s bello: pero guardaba tranquilamente d ntro de sf 
toda su arrnonfa, como los melocotones guardan dentro de sf el duro 
hueso. 
'1 remember, for example, \[a small clearing in the woods in the 
middle of the Canadian mountains\]/, with three tiny lagoons that, 
due to the water sediments, had different and astonishing colors. 
This peculiarity had made the place/into a sacred site, to which the 
Indians made pilgrimages over the centuries, and surely even the 
Paleolithic Indians before them. And you could feel it. 
'Canada is a very beautiful country and that one/was by no 
means the most beautiful place: but it calmly kept within itself all of 
its harmony, like peaches that keep the hard seeds within.' 
In this text, the demonstrative pronoun aqudl 'that one' corefers with the 
antecedent un peque~o claro en un bosque n medio de las monta~as canadienses 
'a small clearing in the woods in the middle of the Canadian mountains', 
which is also linked to the definite noun phrase el sitio 'the place'. Our 
algorithm identified the proper noun Canadd, which is in the same 
sentence, as the anaphor, since the proper noun could only have been 
discarded by means of semantic information. 
As an example of an anaphor that was correctly resolved by the 
algorithm, we present he following sentence xtracted from the Blue 
Book corpus. In this case, the antecedent los sistemas de transmisidn 
analdgica 'the systems of analogue transmission' was correctly chosen for 
the personal pronoun ellos 'them': ' 
En las conexiones largas o de Iongitud media, es probable que la 
fuente principal de ruido de circuito estribe en \[los sistemas de 
transmisi6n anal6gica\]i, ya queen ellosi la potencia de ruido suele 
set proporcional  la Iongitud del circuito. 
'In long or medium connections, it is probable that the main source of 
circuit noise comes from \[the systems of analogue transmission\]/, 
since in them/the noise capacity is usually proportional to the length 
of the circuit.' 
The remainder of the errors were due to split antecedents (10%), 
cataphora (2%), exophora (3%), or exceptions in the application of 
preferences (43%). 
5. Comparison with Other Approaches to Anaphora Resolution 
5.1 Anaphora Resolution Approaches 
Common among all languages i the fact that the anaphora phenomenon requires im- 
ilar strategies for its resolution (e.g., pronouns or definite descriptions). All languages 
employ different kinds of knowledge, but their strategies differ only in the manner by 
which this knowledge is coordinated. For example, in some strategies just one kind 
of knowledge becomes the main selector for identifying the antecedent, with other 
kinds of knowledge being used merely to confirm or reject the proposed antecedent. 
In such cases, the typical kind of knowledge used as the selector is that of discourse 
structure. Centering theory, as employed by Strube and Hahn (1999) or Okumura and 
Tamura (1996), uses this type of approach. Other approaches, however, give equal 
559 
Computational Linguistics Volume 27, Number 4 
importance to each kind of knowledge and generally distinguish between constraints 
and preferences (Baldwin 1997; Lappin and Leass 1994; Carbonell and Brown 1988). 
Whereas constraints tend to be absolute and therefore discard possible antecedents, 
preferences tend to be relative and require the use of additional criteria (e.g., the use of 
heuristics that are not always satisfied by all antecedents). Nakaiwa and Shirai (1996) 
use this sort of resolution model, which involves the use of semantic and pragmatic 
constraints, such as constraints based on modal expressions, or constraints based on 
verbal semantic attributes or conjunctions. 
Our approach to anaphora resolution belongs in the latter category, since it com- 
bines different kinds of knowledge and no knowledge based on discourse structure 
is included. We choose to ignore discourse structure because obtaining this kind of 
knowledge requires not only an understanding of semantics but also knowledge about 
world affairs and the ability to almost perfectly parse any text under discussion (Az- 
zam, Humphreys, and Gaizauskas 1998). 
Still other approaches to anaphora resolution are based either on machine learn- 
ing techniques (Connolly, Burger, and Day 1994; Yamamoto and Sumita 1998; Paul, 
Yamamato, and Sumita 1999) or on the principles of uncertainty reasoning (Mitkov 
1995). 
Computational processing of semantic and domain information is relatively expen- 
sive when compared with other kinds of knowledge. Consequently, current anaphora 
resolution methods rely mainly on constraint and preference heuristics, which employ 
morpho-syntactic information or shallow semantic analysis (see, for example, Mitkov 
\[1998\]). Such approaches have performed notably well. Lappin and Leass (1994) de- 
scribe an algorithm for pronominal anaphora resolution that achieves a high rate of 
correct analyses (85%). Their approach, however, operates almost exclusively on syn- 
tactic information. More recently, Kennedy and Boguraev (1996) proposed an algorithm 
for anaphora resolution that is actually a modified and extended version of the one 
developed by Lappin and Leass (1994). It works from the output of a POS tagger and 
achieves an accuracy rate of 75%. 
There are other approaches based on POS tagger outputs as well. For example, 
Mitkov and Stys (1997) propose a knowledge-poor approach to resolving pronouns 
in technical manuals in both English and Polish. The knowledge mployed in these 
approaches i limited to a small noun phrase grammar, a list of terms, and a set of 
antecedent indicators (definiteness, term preference, lexical reiteration, etc.). 
Still other approaches are based on statistical information, including the work of 
Dagan and Itai (1990, 1991) and Ge, Hale, and Charniak (1998), all of whom present a 
probabilistic model for pronoun resolution. 
We have adopted their ideas and adapted their algorithms to partial parsing and 
to Spanish texts in order to compare our results with their approaches. 
With reference to the differences between English and Spanish anaphora resolu- 
tion, we have made the following observations: 
Syntactic parallelism has played a more important role in English texts 
than in Spanish texts, since Spanish sentence structure is more flexible 
than English sentence structure. Spanish is a free-word-order language 
and has different syntactic onditions, which increases the difficulty of 
resolving Spanish pronouns (hence, the greater accuracy rate for English 
texts). 
? A greater number of possible antecedents was observed for Spanish 
pronouns than for English pronouns, due mainly to the greater average 
560 
Palomar et al Anaphora Resolution in Spanish Texts 
length of Spanish sentences (which also makes the resolution of Spanish 
pronouns more difficult). 
Spanish pronouns usually bear more morphological information. One 
result is that this constraint tends to discard more candidates in Spanish 
than in English. 
For comparison purposes, we implemented the following approaches on the same 
Spanish texts that were tested and described in Section 4.1. 
5.2 Hobbs's Algorithm 
Hobbs's algorithm (Hobbs 1978) is applied to the surface parse trees of sentences in 
a text. A surface parse tree represents the grammatical structure of a sentence. By 
reading the leaves of the parse tree from left to right, the original English sentence is
formed. The algorithm parses the tree in a predefined order and searches for a noun 
phrase of the correct gender and number. Hobbs tested his algorithm for the pronouns 
he, she, it, and they, using 100 examples taken from three different sources. Although 
the algorithm is very simple, it was successful 81.8% of the time. 
We implemented a version of Hobbs's algorithm for slot unification grammar for 
Spanish texts. Since full parsing was not done, our specifications for the algorithm 
were adjusted, as follows: 
? NPs were tested from left to right, as they were parsed in the sentence. 
? Afterward, the NPs that were included in an NP (breadth-first) were 
tested. 
? This test was interrupted when an NP agreed in gender and number 
with the anaphor. 
The problems we encountered in implementing Hobbs's algorithm are similar to 
those found in implementing other approaches: the adaptation to partial parsing, and 
the inherent difficulty of the Spanish language (i.e., its free-word-order characteristics). 
The results of our test of this version of Hobbs's algorithm on the test corpus 
appear in Table 4. 
5.3 Approaches Based on Constraints and Proximity Preference 
Our approach as also been compared with the typical baseline approach consisting of 
constraints and proximity preference; that is, the antecedent that appears closest o the 
anaphor is chosen from among those that satisfy the constraints. For this comparison, 
the same constraints that were used previously (i.e., morphological greement and 
syntactic onditions) were applied here. Then the antecedent a the head of the list of 
antecedents was proposed as the solution of the anaphor. These results are also listed 
in Table 4. As can be seen from the table, success rates were lower than those obtained 
through the joint application of all the preferences. 
5.4 Lappin and Leass's Algorithm 
An algorithm for identifying the noun phrase antecedents of third person pronouns 
and lexical anaphors (reflexive and reciprocal) is presented in Lappin and Leass (1994); 
this algorithm has exhibited a high rate (85%) of correct analyses in English texts. It 
relies on measures of salience that are derived from syntactic structures and on simple 
dynamic models of attentional state to select he antecedent oun phrase of a pronoun 
from a list of candidates. 
561 
Computational Linguistics Volume 27, Number 4 
We have implemented a version of Lappin and Leass's algorithm for Spanish texts. 
The original formulation of the algorithm proposes a syntactic filter on NP-pronoun 
coreference. This filter consists of six conditions for NP-pronoun oncoreference within 
any sentence (Lappin and Leass 1994, page 537). In applying this algorithm to Span- 
ish texts, we changed these conditions o as to capture the appropriate context. As 
mentioned previously, our algorithm does not have access to full syntactic knowledge. 
Accordingly, we employed partial parsing over the text in our application of Lappin 
and Leass's algorithm. The salience parameters were weighted (weight appears in 
parentheses) and applied in the following way: 
? Sentence recency (100): Applied when the NP appeared in the same 
sentence as the anaphor. 
? Subject emphasis (80): Applied when the NP was located before the 
verb of the clause in which it appeared. This heuristic was necessary 
because of our algorithm's lack of syntactic knowledge. It should be 
noted, however, that since Spanish is a nearly free-word-order language 
and the exchange of subject and object positions within Spanish 
sentences i common, the heuristic is often invalid. For example, the two 
Spanish sentences Pedro compr6 un regalo 'Pedro bought a present' and Un 
regalo compr6 Pedro 'A present bought Pedro' are equivalent to one 
another and to the English sentence Pedro bought a present. 
? Existential emphasis (70): In this instance, we applied the parameter in 
the same way as Lappin and Leass, since the entire NP was fully parsed, 
which allowed us to tell when it was a definite or an indefinite NP. 
? Accusative emphasis (50): Applied when the NP appeared after the verb 
of the clause in which it appeared and the NP did not appear inside 
another NP or PP. For example, in the sentence Pedro encontr6 el libro de 
Juana 'Pedro found Juana's book', a value was assigned to el libro de Juana 
'Juana's book' but not to Juana. Once again, it should be noted that this 
heuristic was necessary because of our algorithm's lack of syntactic 
knowledge. 
? Indirect object and oblique complement emphasis (40): Applied when 
the NP appeared in a PP with the Spanish preposition a 'to', which 
usually preceded the indirect object of its sentence. 
? Head noun emphasis (80): Applied when the NP was not contained in 
another NP. 
? Nonadverbial emphasis (50): Applied when the NP was not contained 
in an adverbial PP. In this case, its application depended on the kind of 
preposition in which the NP was included. 
? Parallelism reward (35): Applied when the NP occupied the same 
position as the anaphor with reference to the verb of the sentence (before 
or after the verb). 
Finally, we followed Lappin and Leass in assigning the additional salience value 
to NPs in the current sentence and in degrading the salience of NPs in preceding 
sentences. 
Our results exhibited some similarities with Lappin and Leass's experiments. 
For example, anaphora was strongly preferred over cataphora, and both approaches 
562 
Palomar et al Anaphora Resolution in Spanish Texts 
preferred intrasentential NPs to intersentential ones. These results can be seen in 
Table 4. 
5.5 Centering Approach 
The centering model proposed by Grosz, Joshi, and Weinstein (1983, 1995) provides 
a framework for modeling the local coherence of discourse. The model has two con- 
structs, a list of forward-looking centers and a backward-looking center, that can be 
assigned to each utterance Ui. The list of forward-looking centers Cf(Ui) ranks dis- 
course entities within the utterance Ui. The backward-looking center Cb(Ui+l) con- 
stitutes the most highly ranked element of Cf(Ui) that is finally realized in the next 
utterance Ui+l. In this way, the ranking imposed over Cf(Ui) must reflect he fact that 
the preferred center Cp(U/) (i.e., the most highly ranked element of Cf(Ui)) is most 
likely to be Cb(Ui+l). 
The ranking criteria used by Grosz, Joshi, and Weinstein (1995) order items in 
the Cf list using grammatical roles. Thus, entities with a subject role are preferred to 
entities with an object role, and objects are preferred to others (adjuncts, etc.). 
Grosz, Joshi, and Weinstein (1995) state that if any element of Cf(Ui) is realized 
by a pronoun in Ui+l, then Cb(Ui+l) must also be realized by a pronoun. 
Brennan, Friedman, and Pollard (1987) applied the centering model to pronoun 
resolution. They based their algorithm on the fact that centering transition relations 
will hold across adjacent utterances. 
Moreover, one crucial point in centering is the ranking of the forward-looking 
centers. Grosz, Joshi, and Weinstein (1995) state that Cf may be ordered using different 
factors, but they only use information about grammatical roles. However, both Strube 
(1998) and Strube and Hahn (1999) point out that it is difficult to define grammatical 
roles in free-word-order languages like German or Spanish. For languages like these, 
they propose other anking criteria dependent upon the information status of discourse 
entities. They claim that information about familiarity is crucial for the ranking of 
discourse ntities, at least in free-word-order languages. 
According to Strube's ranking criteria, two different sets of expressions, hearer- 
old discourse ntities (OLD) and hearer-new discourse ntities (NEW), can be distin- 
guished. OLD discourse ntities consist of evoked entities---coreferring resolved ex- 
pressions (pronominal and nominal anaphora, previously mentioned proper names, 
relative pronouns, appositives)--and unused entities (proper names and titles). The re- 
maining entities are assigned to the NEW set. The basic ranking criteria for pronominal 
anaphora resolution prefer OLD entities over NEW entities. 8 
Strube (1998) thus proposes the following adaptation to the centering model: 
The Cf list is replaced by the list of salient discourse ntities (S-list) 
containing discourse ntities that are realized in the current and previous 
utterance. 
? The elements of the S-list are ranked according to the basic ranking 
criteria and position information: 
If X E OLD and y C NEW, then x precedes y. 
If x, y ~ OLD or x, y E NEW, 
8 To resolve functional naphora,  third set, MED, which includes inferable information, must be added 
between the OLD and the NEW sets. However, this set is not needed to resolve pronominal naphora 
(Strube and Hahn 1999). 
563 
Computational Linguistics Volume 27, Number 4 
Table 4 
Comparative r sults of blind test. 
Total CPPR RPR OPR PPRinPP DPRinPP PPRnotPP DPRnotPP 
Num. of 
pronoun 1,677 228 80 1,099 107 20 94 49 
occurrences 
Hobbs's 
algorithm 62.7% 61% 85% 62% 62% 50% 66% 52% 
Lappin & 
Leass's 67.4% 66% 86% 67% 65% 60% 67% 60% 
algorithm 
Proximity 52.9% 55% 86% 47% 65% 85% 61% 65% 
Centering 
approach 62.6% 60% 85% 62% 61% 60% 62% 58% 
Our 
algorithm 76.8% 71% 92% 79% 65% 85% 68% 69% 
then if utterance(y) precedes utterance(x), then x precedes y, 
if utterance(y) = utterance(x) and pos(x) < pos(y), then x precedes y. 
Since there is not a clear definition of what an utterance is, the following 
criteria are assumed: tensed clauses are defined as utterances on their 
own and untensed clauses are processed with the main clause in order to 
constitute only one utterance. 
Incorporating these adaptations, Strube (1998) then proposes the following algo- 
rithm: 
1. If a referring expression is encountered, 
(a) if it is a pronoun, test the elements of the S-list in order until the 
test succeeds; 
(b) update the S-list using information about this referring 
expression. 
2. If the analysis of utterance U is finished, remove all discourse ntities 
from the S-list that are not realized in U. 
The evaluation of this algorithm was performed in Strube (1998) and obtained a 
precision of 85.4% for English, improving upon the results of the centering algorithm 
by Brennan, Friedman, and Pollard (1987), which achieved only 72.9% precision when 
it was applied to the same corpus. 
Consequently, in adapting the centering model to Spanish anaphora resolution, we 
followed Strube's indications. The success rate of the algorithm was not satisfactory, 
as can be seen in Table 4. 
6. Conclus ions 
In this paper, we have presented an algorithm for identifying noun phrase antecedents 
of third person personal pronouns, demonstrative pronouns, reflexive pronouns, and 
564 
Palomar et al Anaphora Resolution in Spanish Texts 
omitted pronouns in Spanish. The algorithm is applied to the syntactic structure gen- 
erated by the slot unification parser--see Ferrdndez, Palomar, and Moreno (1998a, 
1998b, 1999)--and coordinates different kinds of knowledge (lexical, morphological, 
and syntactic) by distinguishing between constraints and preferences. 
The main contribution ofthis paper is the introduction ofan algorithm for anaphora 
resolution for Spanish. In our work, we have undertaken an exhaustive study of the 
importance of each kind of knowledge in anaphora resolution for Spanish. Moreover, 
we have developed a definition of syntactic onditions of NP-pronoun noncorefer- 
ence in Spanish with partial parsing. We have also adapted our anaphora resolution 
algorithm to the problem of partial syntactic knowledge, that is to say, when partial 
parsing of the text is accomplished. 
For unrestricted texts, our approach is somewhat less accurate, since semantic 
information is not taken into account. For such texts, we are dealing with the output 
of a POS tagger, which does not provide this sort of knowledge. In order to test our 
approach with texts of different genres by different authors, we have worked with 
two different Spanish corpora, literary texts (the Lexesp corpus) and technical texts 
(the Blue Book), containing a total of 1,677 pronoun occurrences. 
The algorithm successfully identified the antecedent of the pronoun for 76.8% 
of these pronoun occurrences. Other algorithms usually work with different kinds 
of knowledge, different texts, and different languages. In order to make a more valid 
comparison of our algorithm with others, we adapted the other algorithms so that they 
would operate using only partial-parsing knowledge. In this evaluation, our algorithm 
has always obtained better esults. 
Moreover, based on the results on our study of the importance of each kind 
of knowledge, we can emphasize that constraints are very important for resolving 
anaphora successfully, since they considerably reduce the number of possible candi- 
dates. 
In future studies, we will attempt to evaluate the importance of semantic informa- 
tion in unrestricted texts for anaphora resolution in Spanish texts (Saiz-Noeda, Su~rez, 
and Peral 1999). This information will be obtained from a lexical tool (e.g., Spanish 
WordNet), which can be automatically consulted (since the tagger does not provide 
this information). 
Acknowledgments 
The authors wish to thank Ferran Pla, 
Natividad Prieto, and Antonio Molina for 
contributing their tagger (Pla 2000); and 
Richard Evans, Mikel Forcada, and Rafael 
Carrasco for their helpful revisions of the 
ideas presented in this paper. We are also 
grateful to several anonymous reviewers of 
Computational Linguistics for helpful 
comments on earlier drafts of this paper. 
Our work has been supported by the 
Spanish government (CICYT) with Grant 
TIC97-0671-C02-01/02. 
References 
Azzam, Saliha, Kevin Humphreys, and 
Robert Gaizauskas. 1998. Evaluating a
focus-based approach to anaphora 
resolution. In Proceedings ofthe 36th Annual 
Meeting of the Association for Computational 
Linguistics and 17th International Conference 
on Computational Linguistics 
(COLING-ACL'98), pages 74-78, Montreal 
(Canada). 
Baldwin, Breck. 1997. CogNIAC: High 
precision coreference with limited 
knowledge and linguistic resources. In
Proceedings of the ACL/EACL Workshop on 
Operational Factors in Practical, Robust 
Anaphora Resolution for Unrestricted Texts, 
pages 38--45, Madrid (Spain). 
Brennan, Susan E., Marilyn W. Friedman, 
and Carl J. Pollard. 1987. A centering 
approach to pronouns. In Proceedings ofthe 
25th Annual Meeting of the Association for 
Computational Linguistics (ACL'87), pages 
155-162, Stanford, CA (USA). 
Carbonell, Jaime G. and Ralf D. Brown. 
1988. Anaphora resolution: A
multi-strategy approach. In Proceedings of
the 12th International Conference on 
565 
Computational Linguistics Volume 27, Number 4 
Computational Linguistics (COLING'88), 
pages 96-101, Budapest (Hungary). 
Carletta, Jean, Amy Isard, Stephen Isard, 
Jacqueline C. Kowtko, Gwyneth 
Doherty-Sneddon, and Anne H. 
Anderson. 1997. The reliability of a 
dialogue structure coding scheme. 
Computational Linguistics, 23(1):13-32. 
Connolly, Dennis, John D. Burger, and 
David S. Day. 1994. A machine learning 
approach to anaphoric reference. In
Proceedings ofthe International Conference on 
New Methods in Language Processing 
(NEMLAP'94), pages 255-261, Manchester 
(UK). 
Dagan, Ido and Alon Itai. 1990. Automatic 
processing of large corpora for the 
resolution of anaphora references. In
Proceedings ofthe 13th International 
Conference on Computational Linguistics 
(COLING'90), pages 330-332, Helsinki 
(Finland). 
Dagan, Ido and Alon Itai. 1991. A statistical 
filter for resolving pronoun references. In
Yishai A. Feldman and Alfred Bruckstein, 
editors, Artificial Intelligence and Computer 
Vision. Elsevier Science Publishers B. V. 
(North-Holland), Amsterdam, pages 
125-135. 
Ferrlindez, Antonio, Manuel Palomar, and 
Lidia Moreno. 1998a. A computational 
approach to pronominal anaphora, 
one-anaphora and surface count 
anaphora. In Proceedings ofthe Second 
Colloquium on Discourse Anaphora nd 
Anaphora Resolution (DAARC'98), pages 
117-128, Lancaster (UK). 
Ferr~ndez, Antonio, Manuel Palomar, and 
Lidia Moreno. 1998b. Anaphora 
resolution in unrestricted texts with 
partial parsing. In Proceedings ofthe 36th 
Annual Meeting of the Association for 
Computational Linguistics and 17th 
International Conference on Computational 
Linguistics (COLING-ACL'98), pages 
385-391, Montreal (Canada). 
Ferr~fndez, Antonio, Manuel Palomar, and 
Lidia Moreno. 1999. An empirical 
approach to Spanish anaphora resolution. 
Machine Translation, 14(3/4):191-216. 
Ferr~indez, Antonio and Jestis Peral. 2000. A 
computational pproach to zero-pronouns 
in Spanish. In Proceedings ofthe 38th 
Annual Meeting of the Association for 
Computational Linguistics (ACL'O0), pages 
166-172, Hong Kong (China). 
Ge, Niyu, John Hale, and Eugene Charniak. 
1998. A statistical approach to anaphora 
resolution. In Proceedings ofthe Sixth 
Workshop on Ven d Large Corpora, pages 
161-170, Montreal (Canada). 
Grosz, Barbara, Aravind Joshi, and Scott 
Weinstein. 1983. Providing a unified 
account of definite noun phrases in 
discourse. In Proceedings ofthe 21st Annual 
Meeting of the Association for Computational 
Linguistics (ACL'83), pages 44-50, 
Cambridge, MA (USA). 
Grosz, Barbara, Aravind Joshi, and Scott 
Weinstein. 1995. Centering: A framework 
for modeling the local coherence of 
discourse. Computational Linguistics, 
21(2):203-225. 
Hobbs, Jerry R. 1978. Resolving pronoun 
references. Lingua, 44:311-338. 
Kennedy, Christopher and Branimir 
Boguraev. 1996. Anaphora for everyone: 
Pronominal anaphora resolution without 
a parser. In Proceedings ofthe 16th 
International Conference on Computational 
Linguistics (COLING'96), pages 113-118, 
Copenhagen (Denmark). 
Lappin, Shalom and Herbert Leass. 1994. 
An algorithm for pronominal anaphora 
resolution. Computational Linguistics, 
20(4):535-561. 
Mathews, Peter H. 1997. The Concise Oxford 
Dictionary of Linguistics. Oxford University 
Press, Oxford (UK). 
Mitkov, Ruslan. 1995. An uncertainty 
reasoning approach to anaphora 
resolution. In Proceedings ofthe Natural 
Language Pacific Rim Symposium (NLPRS 
"95), pages 149-154, Seoul (Korea). 
Mitkov, Ruslan. 1998. Robust pronoun 
resolution with limited knowledge. In 
Proceedings ofthe 36th Annual Meeting of the 
Association for Computational Linguistics and 
17 th International Conference on 
Computational Linguistics 
(COLING-ACL'98), pages 869-875, 
Montreal (Canada). 
Mitkov, Ruslan and Malgorzata Stys. 1997. 
Robust reference resolution with limited 
knowledge: High precision genre-specific 
approach for English and Polish. In 
Proceedings ofthe International Conference on 
Recent Advances in Natural Language 
Processing (RANLP'97), pages 74-81, 
Tzigov Chark (Bulgaria). 
Nakaiwa, Hiromi and Satoshi Shirai. 1996. 
Anaphora resolution of Japanese zero 
pronouns with deictic reference. In
Proceedings ofthe 16th International 
Conference on Computational Linguistics 
(COLING'96), pages 812-817, Copenhagen 
(Denmark). 
Okumura, Manabu and Kouji Tamura. 1996. 
Zero pronoun resolution in Japanese 
discourse based on centering theory. In 
Proceedings ofthe 16th International 
Conference on Computational Linguistics 
566 
Palomar et al Anaphora Resolution in Spanish Texts 
(COLING'96), pages 871-876, Copenhagen 
(Denmark). 
Palomar, Manuel, Antonio Ferra'ndez, Lidia 
Moreno, Maximiliano Saiz-Noeda, Rafael 
Mu~oz, Patricio Martfnez-Barco, Jestis 
Peral, and Borja Navarro. 1999. A robust 
partial parsing strategy based on the slot 
unification grammars. In Proceedings ofthe 
6th Conference on Natural Language 
Processing (TALN'99), pages 263-272, 
Corsica (France). 
Paul, Michael, Kazuhide Yamamoto, and 
Eiichiro Sumita. 1999. Corpus-based 
anaphora resolution towards antecedent 
preference. In Proceedings ofthe ACL 
Workshop on Coreference and Its Applications, 
pages 47-52, College Park, MD (USA). 
Pla, Ferran. 2000. Etiquetado Ldxico y Andlisis 
Sintdctico Super~'cial Basado en Modelos 
Estadfsticos. Ph.D. thesis, Valencia 
University of Technology, Valencia 
(Spain). 
Proyecto CRATER. 1994-1995. Corpus 
Resources And Terminology ExtRaction. 
MLAP-93/20. http: //www.lllf.uam.es / 
proyectos/crater.html (page visited on 
04/17/01). 
Reinhart, Tanya. 1983. Anaphora nd Semantic 
Interpretation. Croom Hehn Linguistics 
series. Croom Helm Ltd., Beckenham, 
Kent (UK). 
Saiz-Noeda, Maximiliano, Armando Sudrez, 
and Jestis Peral. 1999. Propuesta de 
incorporacidn de informaci6n sem~ntica 
desde Wordnet alandlisis sintdctico 
parcial orientado a la resoluci6n de la 
an~ffora. Procesamiento del Lenguaje Natural, 
25:167-173. 
Siegel, Sidney and John N. Castellan. 1988. 
Nonparametric Statistics for the Behavioral 
Sciences. McGraw-Hill, New York, NY 
(USA), 2nd edition. 
Strube, Michael. 1998. Never look back: An 
alternative to centering. In Proceedings of
the 36th Annual Meeting of the Association for 
Computational Linguistics and 17th 
International Conference on Computational 
Linguistics (COLING-ACL'98), pages 
1251-1257, Montreal (Canada). 
Strube, Michael and Udo Hahn. 1999. 
Functional centering: Grounding 
referential coherence in information 
structure. Computational Linguistics, 
25(3):309-344. 
Vieira, Renata. 1998. Processing of Definite 
Descriptions in Unrestricted Texts. Ph.D. 
thesis, University of Edinburgh, 
Edinburgh (UK). 
Yamamoto, Kazuhide and Eiichiro Sumita. 
1998. Feasibility study for ellipsis 
resolution in dialogues by 
machine-learning technique. In 
Proceedings ofthe 36th Annual Meeting of the 
Association for Computational Linguistics and 
17th International Conference on 
Computational Linguistics 
(COLING-ACL'98), pages 385-391, 
Montreal (Canada). 
567 

A Computational Approach to Zero-pronouns in
Spanish
Antonio Ferr?ndez and Jes?s Peral
Dept. Languages and Information Systems, University of Alicante
Carretera San Vicente S/N
03080 ALICANTE, Spain
{antonio, jperal}@dlsi.ua.es
Abstract
In this paper, a computational approach for
resolving zero-pronouns in Spanish texts is
proposed. Our approach has been evaluated
with partial parsing of the text and the
results obtained show that these pronouns
can be resolved using similar techniques that
those used for pronominal anaphora.
Compared to other well-known baselines on
pronominal anaphora resolution, the results
obtained with our approach have been
consistently better than the rest.
Introduction
In this paper, we focus specifically on the
resolution of a linguistic problem for Spanish
texts, from the computational point of view:
zero-pronouns in the ?subject? grammatical
position. Therefore, the aim of this paper is not
to present a new theory regarding zero-
pronouns, but to show that other algorithms,
which have been previously applied to the
computational resolution of other kinds of
pronoun, can also be applied to resolve zero-
pronouns.
The resolution of these pronouns is
implemented in the computational system called
Slot Unification Parser for Anaphora resolution
(SUPAR). This system, which was presented in
Ferr?ndez et al (1999), resolves anaphora in
both English and Spanish texts. It is a modular
system and currently it is being used for
Machine Translation and Question Answering,
in which this kind of pronoun is very important
to solve due to its high frequency in Spanish
texts as this paper will show.
We are focussing on zero-pronouns in
Spanish texts, although they also appear in other
languages, such as Japanese, Italian and
Chinese. In English texts, this sort of pronoun
occurs far less frequently, as the use of subject
pronouns is generally compulsory in the
language. While in other languages, zero-
pronouns may appear in either the subject?s or
the object?s grammatical position, (e.g.
Japanese), in Spanish texts, zero-pronouns only
appear in the position of the subject.
In the following section, we present a
summary of the present state-of-the-art for zero-
pronouns resolution. This is followed by a
description of the process for the detection and
resolution of zero-pronouns. Finally, we present
the results we have obtained with our approach.
1 Background
Zero-pronouns have already been studied in
other languages, such as Japanese, (e.g. Nakaiwa
and Shirai (1996)). They have not yet been
studied in Spanish texts, however. Among the
work done for their resolution in different
languages, nevertheless, there are several points
that are common for Spanish. The first point is
that they must first be located in the text, and
then resolved. Another common point among,
they all employ different kinds of knowledge
(e.g. morphologic or syntactic) for their
resolution. Some of these works are based on the
Centering Theory (e.g. Okumura and Tamura
(1996)). Other works, however, distinguish
between restrictions and preferences (e.g.
Lappin and Leass (1994)). Restrictions tend to
be absolute and, therefore, discard any possible
antecedents, whereas preferences tend to be
relative and require the use of additional criteria,
i.e. heuristics that are not always satisfied by all
anaphors. Our anaphora resolution approach
belongs to the second group.
In computational processing, semantic and
domain information is computationally
inefficient when compared to other kinds of
knowledge. Consequently, current anaphora
resolution methods rely mainly on restrictions
and preference heuristics, which employ
information originating from morpho-syntactic
or shallow semantic analysis, (see Mitkov
(1998) for example). Such approaches,
nevertheless, perform notably well. Lappin and
Leass (1994) describe an algorithm for
pronominal anaphora resolution that achieves a
high rate of correct analyses (85%). Their
approach, however, operates almost exclusively
on syntactic information. More recently,
Kennedy and Boguraev (1996) propose an
algorithm for anaphora resolution that is actually
a modified and extended version of the one
developed by Lappin and Leass (1994). It works
from a POS tagger output and achieves an
accuracy rate of 75%.
2 Detecting zero-pronouns
In order to detect zero-pronouns, the sentences
should be divided into clauses since the subject
could only appear between the clause
constituents. After that, a noun-phrase (NP) or a
pronoun that agrees in person and number with
the clause verb is sought, unless the verb is
imperative or impersonal.
As we are also working on unrestricted texts
to which partial parsing is applied, zero-
pronouns must also be detected when we do not
dispose of full syntactic information. In
Ferr?ndez et al (1998), a partial parsing strategy
that provides all the necessary information for
resolving anaphora is presented. That study
shows that only the following constituents were
necessary for anaphora resolution: co-ordinated
prepositional and noun phrases, pronouns,
conjunctions and verbs, regardless of the order
in which they appear in the text.
H1 Let us assume that the beginning of a new clause has
been found when a verb is parsed and a free conjunction
is subsequently parsed.
When partial parsing is carried out, one
problem that arises is to detect the different
clauses of a sentence. Another problem is how to
detect the zero-pronoun, i.e. the omission of the
subject from each clause. With regard to the first
problem, the heuristic H1 is applied to identify a
new clause.
 (1)John y Jane llegaron tarde al trabajo porque ?1 se
durmieron (John and Jane were late for work because
[they]? over-slept)
                                                     
1
 The symbol ? will always show the position of the
In this particular case, a free conjunction
does not imply conjunctions2 that join co-
ordinated noun and prepositional phrases. It
refers, here, to conjunctions that are parsed in
our partial parsing scheme. For instance, in
sentence (1), the following sequence of
constituents is parsed:
np(John and Jane), verb(were), freeWord3(late), pp(for
work), conj(because), pron(they), verb(over-slept )
Since the free conjunction porque (because)
has been parsed after the verb llegaron (were),
the new clause with a new verb durmieron
(over-slept) can be detected.
With reference to the problem about
detecting the omission of the subject from each
clause with partial parsing, it is solved by
searching through the clause constituents that
appear before the verb. In sentence (1), we can
verify that the first verb, llegaron (were), does
not have its subject omitted since there appears a
np(John and Jane). However, there is a zero-
pronoun, (they)?, for the second verb durmieron
(over-slept).
(2) Pedroj vio a Anak en el parque. ?k Estaba muy guapa
(Peterj saw Annk in the park. [She]?k was very beautiful)
When the zero-pronoun is detected, our
computational system inserts the pronoun in the
position in which it has been omitted. This
pronoun will be resolved in the following
module of anaphora resolution. Person and
number information is obtained from the clause
verb. Sometimes in Spanish, gender information
of the pronoun can be obtained when the verb is
copulative. For example, in sentence (2), the
verb estaba (was) is copulative, so that its
subject must agree in gender and number with
its object whenever the object can have either a
masculine or a feminine linguistic form (guapo:
masc, guapa: fem). We can therefore obtain
information about its gender from the object,
guapa (beautiful in its feminine form) which
automatically assigns it to the feminine gender
so the omitted pronoun would have to be she
rather than he. Gender information can be
obtained from the object of the verb with partial
                                                                          
omitted pronoun.
2
 For example, it would include punctuation marks
such as a semicolon.
3
 The free words consist of constituents that are not
covered by this partial parsing (e.g. adverbs).
parsing as we simply have to search for a NP on
the right of the verb.
3 Zero-pronoun resolution
In this module, anaphors (i.e. anaphoric
expressions such as pronominal references or
zero-pronouns) are treated from left to right as
they appear in the sentence, since, at the
detection of any kind of anaphor, the appropriate
set of restrictions and preferences begins to run.
The number of previous sentences considered
in the resolution of an anaphora is determined by
the kind of anaphora itself. This feature was
arrived at following an in depth study of Spanish
texts. For pronouns and zero-pronouns, the
antecedents in the four previous sentences, are
considered.
The following restrictions are first applied to
the list of candidates: person and number
agreement, c-command4 constraints and
semantic consistency5. This list is sorted by
proximity to the anaphor. Next, if after applying
the restrictions there is still more than one
candidate, the preferences are then applied, with
the degree of importance shown in Figure 1.
This sequence of preferences (from 1 to 10)
stops whenever only one candidate remains after
having applied a given preference. If after all the
preferences have been applied there is still more
than one candidate left, the most repeated
candidates6 in the text are then extracted from
the list, and if there is still more than one
candidate, then the candidates that have
appeared most frequently with the verb of the
anaphor are extracted from the previous list.
Finally, if after having applied all the previous
preferences, there is still more than one
candidate left, the first candidate of the resulting
list (the closest to the anaphor) is selected.
The set of constraints and preferences
required for Spanish pronominal anaphora
presents two basic differences: a) zero-pronoun
resolution has the restriction of agreement only
                                                     
4
 The usage of c-command restrictions on partial
parsing is presented in  Ferr?ndez et. al. (1998).
5
  Semantic knowledge is only used when working on
restricted texts.
6
 Here, we mean that we first obtain the maximum
number of repetitions for an antecedent in the
remaining list. After that, we extract the antecedents
that have this value of repetition from the list.
in person and number, (whereas pronominal
anaphora resolution requires gender agreement
as well), and b) a different set of preferences.
1 ) C a n d id a te s  in  th e  s a m e  s e n te n c e  a s  th e
a n a p h o r.
2 )  C a n d id a te s  in  th e  p re v io u s  s e n te n c e .
3 ) P re fe re n c e  fo r  c a n d id a te s  in  th e  s a m e
s e n te n c e  a s  th e  a n a p h o r a n d  th o s e  th a t
h a v e  b e e n  th e  s o lu t io n  o f  a  z e ro -p ro n o u n  in
th e  s a m e  s e n te n c e  a s  th e  a n a p h o r.
4 )  P re fe re n c e  fo r  p ro p e r  n o u n s  o r  in d e f in ite
N P s .
5 )  P re fe re n c e  fo r  p ro p e r  n o u n s .
6 )  C a n d id a te s  th a t  h a v e  b e e n  re p e a te d  m o re
th a n  o n c e  in  th e  te x t .
7 )  C a n d id a te s  th a t  h a v e  a p p e a re d  w ith  th e
v e rb  o f  th e  a n a p h o r m o re  th a n  o n c e .
8 )  P re fe re n c e  fo r  n o u n  p h ra s e s  th a t  a re  n o t
in c lu d e d  in  a  p re p o s it io n a l p h ra s e  o r  th o s e
th a t  a re  c o n n e c te d  to  a n  In d ire c t  O b je c t .
9 ) C a n d id a te s  in  th e  s a m e  p o s it io n  a s  th e
a n a p h o r,  w ith  re fe re n c e  to  th e  v e rb  (b e fo re
th e  v e rb ) .
1 0 )  I f  th e  z e ro -p ro n o u n  h a s  g e n d e r
in fo rm a t io n ,  th o s e  c a n d id a te s  th a t  a g re e  in
g e n d e r .
Figure 1. Anaphora resolution preferences.
The main difference between the two sets of
preferences is the use of two new preferences in
our algorithm: Nos. 3 and 10. Preference 10 is
the last preference since the POS tagger does not
indicate whether the object has both masculine
and feminine linguistic forms7 (i.e. information
obtained from the object when the verb is
copulative). Gender information must therefore
be considered a preference rather than a
restriction. Another interesting fact is that
syntactic parallelism (Preference No. 9)
continues to be one of the last preferences,
which emphasizes the unique problem that arises
in Spanish texts, in which syntactic structure is
quite flexible (unlike English).
4 Evaluation
4.1 Experiments accomplished
Our computational system (SUPAR) has been
trained with a handmade corpus8 with 106 zero-
                                                     
7
 For example in: Peter es un genio (Peter is a
genius), the tagger does not indicate that the object
does not have both masculine and feminine linguistic
forms. Therefore, a feminine subject would use the
same form: Jane es un genio (Jane is a genius).
Consequently, although the tagger says that the verb,
es (is), is copulative, and the object, un genio (a
genius) is masculine, this gender could not be used as
a restriction for the zero-pronoun in the following
sentence: ? Es un genio.
8
 This corpus has been provided by our colleagues in
pronouns. This training has mainly supposed the
improvement of the set of preferences, i.e. the
optimum order of preferences in order to obtain
the best results. After that, we have carried out a
blind evaluation on unrestricted texts.
Specifically, SUPAR has been run on two
different Spanish corpora: a) a part of the
Spanish version of The Blue Book corpus, which
contains the handbook of the International
Telecommunications Union CCITT, published
in English, French and Spanish, and
automatically tagged by the Xerox tagger, and b)
a part of the Lexesp corpus, which contains
Spanish texts from different genres and authors.
These texts are taken mainly from newspapers,
and are automatically tagged by a different
tagger than that of The Blue Book. The part of
the Lexesp corpus that we processed contains ten
different stories related by a sole narrator,
although they were written by different authors.
Having worked with different genres and
disparate authors, we feel that the applicability
of our proposal to other sorts of texts is assured.
In Figure 2, a brief description of these corpora
is given. In these corpora, partial parsing of the
text with no semantic information has been used.
Number
of words
Number of
sentences
Words per
sentence
Lexesp corpus Text 1 972 38 25.6
Text 2 999 55 18.2
Text 3 935 34 27.5
Text 4 994 36 27.6
Text 5 940 67 14
Text 6 957 34 28.1
Text 7 1025 59 17.4
Text 8 981 40 24.5
Text 9 961 36 26.7
Text 10 982 32 30.7
The Blue Book corpus 15,571 509 30.6
Figure 2. Description of the unrestricted
corpora used in the evaluation.
4.2 Evaluating the detection of zero-
pronouns
To achieve this sort of evaluation, several
different tasks may be considered. Each verb
must first be detected. This task is easily
                                                                          
the University of Alicante, which were required to
propose sentences with zero-pronouns.
accomplished since both corpora have been
previously tagged and manually reviewed. No
errors are therefore expected on verb detection.
Therefore, a recall9 rate of 100% is
accomplished. The second task is to classify the
verbs into two categories: a) verbs whose
subjects have been omitted, and b) verbs whose
subjects have not. The overall results on this sort
of detection are presented in Figure 3 (success10
rate of 88% on 1,599 classified verbs, with no
significant differences seen between the
corpora). We should also remark that a success
rate of 98% has been obtained in the detection of
verbs whose subjects were omitted, whereas
only 80% was achieved for verbs whose subjects
were not. This lower success rate is justified,
however, for several reasons. One important
reason is the non-detection of impersonal verbs
by the POS tagger. This problem has been partly
resolved by heuristics such as a set of
impersonal verbs (e.g. llover (to rain)), but it has
failed in some impersonal uses of some verbs.
For example, in sentence (3), the verb es (to be)
is not usually impersonal, but it is in the
following sentence, in which SUPAR would
fail:
(3) ? Es hora de desayunar ([It]? is time to have breakfast)
Two other reasons for the low success rate
achieved with verbs whose subjects were not
omitted are the lack of semantic information and
the inaccuracy of the grammar used. The second
reason is the ambiguity and the unavoidable
incompleteness of the grammars, which also
affects the process of clause splitting.
In Figure 3, an interesting fact can be
observed: 46% of the verbs in these corpora
have their subjects omitted. It shows quite
clearly the importance of this phenomenon in
Spanish. Furthermore, it is even more important
in narrative texts, as this figure shows: 61% with
the Lexesp corpus, compared to 26% with the
technical manual. We should also observe that
The Blue Book has no verbs in either the first or
the second person. This may be explained by the
style  of  the   technical   manual,  which  usually
                                                     
9
 By  ?recall rate?, we mean the number of verbs
classified, divided by the total number of verbs in the
text.
10
 By  ?success rate?, we mean the number of verbs
successfully classified, divided by the total number of
verbs in the text.
Verbs with their subject omitted Verbs with their subject no-omitted
First person Second person Third person First person Second person Third person
Total %
Success
Total %
Success
Total %
Success
Total %
Success
Total %
Success
Total %
Success
111 100% 42 100% 401 99% 21 81% 3 100% 328 76%
20% 7% 73% 7% 1% 92%
Lexesp
corpus
554 (61%) (success rate: 99%) 352 (39%) (success rate: 76%)
0 0% 0 0% 180 97% 0 0% 0 0% 513 82%
0% 0% 100% 0% 0% 100%
Blue
Book
corpus
180 (26%) (success rate: 97%) 513 (74%) (success rate: 82%)
734 (46%) (success rate: 98%) 865 (54%) (success rate: 80%)Total
1,599 (success rate: 88%)
Figure 3. Results obtained in the detection of zero-pronouns.
consists of a series of isolated definitions, (i.e.
many paragraphs that are not related to one
another). This explanation is confirmed by the
relatively small number of anaphors that are
found in that corpus, as compared to the Lexesp
corpus.
We have not considered comparing our
results with those of other published works,
since, (as we have already explained in the
Background section), ours is the first study that
has been done specifically for Spanish texts, and
the designing of the detection stage depends
mainly on the structure of the language in
question. Any comparisons that might be made
concerning other languages, therefore, would
prove to be rather insignificant.
4.3 Evaluating anaphora resolution
As we have already shown in the previous
section, (Figure 3), of the 1,599 verbs classified
in these two corpora, 734 of them have zero-
pronouns. Only 581 of them, however, are in
third person and will be resolved. In Figure 4,
we present a classification of these third person
zero-pronouns, which have been conveniently
divided into three categories: cataphoric,
exophoric and anaphoric. The first category is
comprised of those whose antecedent, i.e. the
clause subject, comes after the verb. For
example, in sentence (4) the subject, a boy,
appears after the verb compr? (bought).
(4) ?k Compr? un ni?ok en el supermercado (A boyk bought
in the supermarket)
This kind of verb is quite common in
Spanish, as can be seen in this figure (49%).
This fact represents one of the main difficulties
found in resolving anaphora in Spanish: the
structure of a sentence is more flexible than in
English. These represent intonationally marked
sentences, where the subject does not occupy its
usual position in the sentence, i.e. before the
verb. Cataphoric zero-pronouns will not be
resolved in this paper, since semantic
information is needed to be able to discard all of
their antecedents and to prefer those that appear
within the same sentence and clause after the
verb. For example, sentence (5) has the same
syntactic structure than sentence (4), i.e. verb,
np, pp, where the subject function of the np can
only be distinguished from the object by means
of semantic knowledge.
(5) ? Compr? un regalo en el supermercado ([He]? bought
a present in the supermarket)
The second category consists of those zero-
pronouns whose antecedents do not appear,
linguistically, in the text (they refer to items in
the external world rather than things referred to
in the text). Finally, the third category is that of
pronouns that will be resolved by our
computational system, i.e., those whose
antecedents come before the verb: 228 zero-
pronouns. These pronouns would be equivalent
to the full pronoun he, she, it or they.
AnaphoricCataphoric Exophoric
Number Success
Lexesp
corpus
171 (42%) 56 (12%) 174 (46%) 78%
The Blue
Book corpus
113 (63%) 13 (7%) 54 (30%) 68%
Total 284 (49%) 69 (12%) 228 (39%) 75%
Figure 4. Classification of third person zero-
pronouns.
The different accuracy results are also shown
in Figure 4: A success rate of 75% was attained
for the 228 zero-pronouns. By ?successful
resolutions? we mean that the solutions offered
by our system agree with the solutions offered
by two human experts.
For each zero-pronoun there is, on average,
355 candidates before the restrictions are
applied, and 11 candidates after restrictions.
Furthermore, we repeated the experiment
without applying restrictions and the success
rate was significantly reduced.
Since the results provided by other works
have been obtained on different languages, texts
and sorts of knowledge (e.g. Hobbs and Lappin
full parse the text), direct comparisons are not
possible. Therefore, in order to accomplish this
comparison, we have implemented some of
these approaches in SUPAR. Although some of
these approaches were not proposed for zero-
pronouns, we have implemented them since as
our approach they could also be applied to solve
this kind of pronoun. For example, with the
baseline presented by Hobbs (1977) an accuracy
of 49.1% was obtained, whereas, with our
system, we achieved 75% accuracy. These
results highlight the improvement accomplished
with our approach, since Hobbs? baseline is
frequently used to compare most of the work
done on anaphora resolution11. The reason why
Hobbs? algorithm works worse than ours is due
to the fact that it carries out a full parsing of the
text. Furthermore, the way to explore the
syntactic tree with Hobbs? algorithm is not the
best one for the Spanish language since it is
nearly a free-word-order language.
Our proposal has also been compared with
the typical baseline of morphological agreement
and proximity preference, (i.e., the antecedent
                                                     
11
 In Tetreault (1999), for example, it is compared
with an adaptation of the Centering Theory by Grosz
et al (1995), and Hobbs? baseline out-performs it.
that appears closest to the anaphor is chosen
from among those that satisfy the restrictions).
The result is a 48.6% accuracy rate. Our system,
therefore, improves on this baseline as well.
Lappin and Leass (1994) has also been
implemented in our system and an accuracy of
64% was attained. Moreover, in order to
compare our proposal with Centering approach,
Functional Centering by Strube and Hahn (1999)
has also been implemented, and an accuracy of
60% was attained.
One of the improvements afforded by our
proposal is that statistical information from the
text is included with the rest of information
(syntactic, morphologic, etc.). Dagan and Itai
(1990), for example, developed a statistical
approach for pronominal anaphora, but the
information they used was simply the patterns
obtained from the previous analysis of the text.
To be able to compare our approach to that of
Dagan and Itai, and to be able to evaluate the
importance of this kind of information, our
method was applied with statistical
information12 only. If there is more than one
candidate after applying statistical information,
preference, and then proximity preference are
applied. The results obtained were lower than
when all the preferences are applied jointly:
50.8%. These low results are due to the fact that
statistical information has been obtained from
the beginning of the text to the pronoun. A
previous training with other texts would be
necessary to obtain better results.
Regarding the success rates reported in
Ferr?ndez et al (1999) for pronominal
references (82.2% for Lexesp, 84% for Spanish
version of The Blue Book, and 87.3% for the
English version), are higher than our 75%
success rate for zero-pronouns. This reduction
(from 84% to 75%) is due mainly to the lack of
gender information in zero-pronouns.
Mitkov (1998) obtains a success rate of
89.7% for pronominal references, working with
English technical manuals. It should be pointed
out, however, that he used some knowledge that
was very close to the genre13 of the text. In our
                                                     
12
 This statistical information consists of the number
of times that a word appears in the text and the
number of times that it appears with a verb.
13
 For example, the antecedent indicator section
heading preference, in which if a NP occurs in the
heading of the section, part of which is the current
study, such information was not used, so we
consider our approach to be more easily
adaptable to different kinds of texts. Moreover,
Mitkov worked exclusively with technical
manuals whereas we have worked with narrative
texts as well. The difference observed is due
mainly to the greater difficulty found in
narrative texts than in technical manuals which
are generally better written. In any case, the
applicability of our proposal to different genres
of texts seems to have been well proven.
Anyway, if the order of application of the
preferences14 is varied to each different text, an
80% overall accuracy rate is attained. This fact
implies that there is another kind of knowledge,
close to the genre and author of the text that
should be used for anaphora resolution.
Conclusion
In this paper, we have proposed the first
algorithm for the resolution of zero-pronouns in
Spanish texts. It has been incorporated into a
computational system (SUPAR). In the
evaluation, several baselines on pronominal
anaphora resolution have been implemented, and
it has achieved better results than either of them
have.
As a future project, the authors shall attempt
to evaluate the importance of semantic
information for zero-pronoun resolutions in
unrestricted texts. Such information will be
obtained from a lexical tool, (e.g.
EuroWordNet), which could be consulted
automatically. We shall also evaluate our
proposal in a Machine Translation application,
where we shall test its success rate by its
generation of the zero-pronoun in the target
language, using the algorithm described in Peral
et al (1999).
References
Ido Dagan and Alon Itai (1990) Automatic
processing of large corpora for the resolution of
anaphora references. In Proceedings of the 13th
                                                                          
sentence, it is considered to be the preferred
candidate.
14
 The difference between  the individual sets of
preferences is  the degree of  importance of the
preferences for proper nouns and syntactic
parallelism.
International Conference on Computational
Linguistics, COLING (Helsinki, Finland).
Antonio Ferr?ndez, Manuel Palomar and Lidia
Moreno (1998) Anaphora resolution in unrestricted
texts with partial parsing. In Proceedings of the 36th
Annual Meeting of the Association for
Computational Linguistics and 17th International
Conference on Computational Linguistics,
COLING - ACL (Montreal, Canada). pp. 385-391.
Antonio Ferr?ndez, Manuel Palomar and Lidia
Moreno (1999) An empirical approach to Spanish
anaphora resolution. To appear in Machine
Translation 14(2-3).
Jerry Hobbs (1977) Resolving pronoun references.
Lingua, 44. pp. 311-338.
Cristopher Kennedy and Bran Boguraev (1996)
Anaphora for Everyone: Pronominal Anaphora
resolution without a Parser. In Proceedings of the
16th International Conference on Computational
Linguistics, COLING (Copenhagen, Denmark). pp.
113-118.
Shalom Lappin and Herb Leass (1994) An algorithm
for pronominal anaphora resolution. Computational
Linguistics, 20(4). pp. 535-561.
Ruslan Mitkov (1998) Robust pronoun resolution
with limited knowledge. In Proceedings of the 36th
Annual Meeting of the Association for
Computational Linguistics and 17th International
Conference on Computational Linguistics,
COLING - ACL (Montreal, Canada). pp. 869-875.
Hiromi Nakaiwa and Satoshi Shirai (1996) Anaphora
Resolution of Japanese Zero Pronouns with Deictic
Reference. In Proceedings of the 16th International
Conference on Computational Linguistics,
COLING (Copenhagen, Denmark). pp. 812-817.
Manabu Okumura and Kouji Tamura (1996) Zero
Pronoun Resolution in Japanese Discourse Based
on Centering Theory. In Proceedings of the 16th
International Conference on Computational
Linguistics, COLING (Copenhagen, Denmark). pp.
871-876.
Jes?s Peral, Manuel Palomar and Antonio Ferr?ndez
(1999) Coreference-oriented Interlingual Slot
Structure and Machine Translation. In Proceedings
of ACL Workshop on Coreference and its
Applications (College Park, Maryland, USA). pp.
69-76.
Michael Strube and Udo Hahn (1999) Functional
Centering ? Grounding Referential Coherence in
Information Structure. Computational Linguistics,
25(5). pp. 309-344.
Importance of Pronominal Anaphora resolution in Question
Answering systems
Jose L. Vicedo and Antonio Ferrandez
Departamento de Lenguajes y Sistemas Informaticos
Universidad de Alicante
Apartado 99. 03080 Alicante, Spain
fvicedo,antoniog@dlsi.ua.es
Abstract
The main aim of this paper is
to analyse the eects of applying
pronominal anaphora resolution to
Question Answering (QA) systems.
For this task a complete QA system
has been implemented. System eval-
uation measures performance im-
provements obtained when informa-
tion that is referenced anaphorically
in documents is not ignored.
1 Introduction
Open domain QA systems are dened as
tools capable of extracting the answer to
user queries directly from unrestricted do-
main documents. Or at least, systems that
can extract text snippets from texts, from
whose content it is possible to infer the an-
swer to a specic question. In both cases,
these systems try to reduce the amount of
time users spend to locate a concrete infor-
mation.
This work is intended to achieve two princi-
pal objectives. First, we analyse several docu-
ment collections to determine the level of in-
formation referenced pronominally in them.
This study gives us an overview about the
amount of information that is discarded when
these references are not solved. As second ob-
jective, we try to measure improvements of
solving this kind of references in QA systems.
With this purpose in mind, a full QA system
has been implemented. Benets obtained by
solving pronominal references are measured
by comparing system performance with and
without taking into account information ref-
erenced pronominally. Evaluation shows that
solving these references improves QA perfor-
mance.
In the following section, the state-of-the-
art of open domain QA systems will be sum-
marised. Afterwards, importance of pronom-
inal references in documents is analysed.
Next, our approach and system components
are described. Finally, evaluation results are
presented and discussed.
2 Background
Interest in open domain QA systems is quite
recent. We had little information about this
kind of systems until the First Question An-
swering Track was held in last TREC confer-
ence (TRE, 1999). In this conference, nearly
twenty dierent systems were evaluated with
very dierent success rates. We can clas-
sify current approaches into two groups: text-
snippet extraction systems and noun-phrase
extraction systems.
Text-snippet extraction approaches are
based on locating and extracting the most rel-
evant sentences or paragraphs to the query by
supposing that this text will contain the cor-
rect answer to the query. This approach has
been the most commonly used by participants
in last TREC QA Track. Examples of these
systems are (Moldovan et al, 1999) (Singhal
et al, 1999) (Prager et al, 1999) (Takaki,
1999) (Hull, 1999) (Cormack et al, 1999).
After reviewing these approaches, we can
notice that there is a general agreement
about the importance of several Natural Lan-
guage Processing (NLP) techniques for QA
task. Pos-tagging, parsing and Name En-
tity recognition are used by most of the sys-
tems. However, few systems apply other NLP
techniques. Particularly, only four systems
model some coreference relations between en-
tities in the query and documents (Morton,
1999)(Breck et al, 1999) (Oard et al, 1999)
(Humphreys et al, 1999). As example, Mor-
ton approach models identity, denite noun-
phrases and non-possessive third person pro-
nouns. Nevertheless, benets of applying
these coreference techniques have not been
analysed and measured separately.
The second group includes noun-phrase ex-
traction systems. These approaches try to
nd the precise information requested by
questions whose answer is dened typically by
a noun phrase.
MURAX is one of these systems (Kupiec,
1999). It can use information from dierent
sentences, paragraphs and even dierent doc-
uments to determine the answer (the most rel-
evant noun-phrase) to the question. However,
this system does not take into account the
information referenced pronominally in docu-
ments. Simply, it is ignored.
With our system, we want to determine the
benets of applying pronominal anaphora res-
olution techniques to QA systems. Therefore,
we apply the developed computational sys-
tem, Slot Unication Parser for Anaphora res-
olution (SUPAR) over documents and queries
(Ferrandez et al, 1999). SUPAR's architec-
ture consists of three independent modules:
lexical analysis, syntactic analysis, and a reso-
lution module for natural language processing
problems, such as pronominal anaphora.
For evaluation, a standard based IR system
and a sentence-extraction QA system have
been implemented. Both are based on Salton
approach (1989). After IR system retrieves
relevant documents, our QA system processes
these documents with and without solving
pronominal references in order to compare -
nal performance.
As results will show, pronominal anaphora
resolution improves greatly QA systems per-
formance. So, we think that this NLP tech-
nique should be considered as part of any
open domain QA system.
3 Importance of pronominal
information in documents
Trying to measure the importance of informa-
tion referenced pronominally in documents,
we have analysed several text collections used
for QA task in TREC-8 Conference as well
as others used frequently for IR system test-
ing. These collections were the following: Los
Angeles Times (LAT), Federal Register (FR),
Financial Times (FT), Federal Bureau Infor-
mation Service (FBIS), TIME, CRANFIELD,
CISI, CACM, MED and LISA. This analy-
sis consists on determining the amount and
type of pronouns used, as well as the number
of sentences containing pronouns in each of
them. As average measure of pronouns used
in a collection, we use the ratio between the
quantity of pronouns and the number of sen-
tences containing pronouns. This measure ap-
proximates the level of information that is ig-
nored if these references are not solved. Fig-
ure 1 shows the results obtained in this anal-
ysis.
As we can see, the amount and type of pro-
nouns used in analysed collections vary de-
pending on the subject the documents talk
about. LAT, FBIS, TIME and FT collections
are composed from news published in dier-
ent newspapers. The ratio of pronominal ref-
erence used in this kind of documents is very
high (from 35,96% to 55,20%). These doc-
uments contain a great number of pronomi-
nal references in third person (he, she, they,
his, her, their) whose antecedents are mainly
people's names. In this type of documents,
pronominal anaphora resolution seems to be
very necessary for a correct modelling of rela-
tions between entities. CISI and MED collec-
tions appear ranked next in decreasing ratio
level order. These collections are composed
by general comments about document man-
aging, classication and indexing and doc-
uments extracted from medical journals re-
spectively. Although the ratio presented by
these collections (24,94% and 22,16%) is also
high, the most important group of pronominal
references used in these collections is formed
by "it" and "its" pronouns. In this case,
TEXT COLLECTION LAT FBIS TIME FT CISI MED CACM LISA FR CRANFIELD
Pronoun type
HE, SHE, THEY 38,59% 29,15% 31,20% 26,20% 15,38% 15,07% 8,59% 12,24% 13,31% 6,54%
HIS, HER, THEIR 25,84% 21,54% 35,01% 20,52% 22,96% 21,46% 15,69% 31,03% 20,70% 10,35%
IT, ITS 26,92% 39,60% 22,43% 46,68% 52,11% 57,41% 67,61% 47,86% 61,06% 79,76%
HIM, THEM 7,04% 7,08% 7,82% 4,44% 6,38% 3,96% 4,87% 6,30% 3,45% 1,60%
HIM, HER,IT(SELF), THEMSELVES 1,61% 2,63% 3,54% 2,17% 3,17% 2,10% 3,25% 2,57% 1,48% 1,75%
Pronouns in Sentences
Containing 0  pronouns 44,80% 48,09% 51,37% 64,04% 75,06% 77,84% 79,06% 83,79% 84,92% 90,95%
Containing 1 pronoun 30,40% 31,37% 29,46% 23,07% 17,17% 15,02% 17,54% 13,01% 11,64% 8,10%
Containing 2 pronouns 14,94% 12,99% 12,26% 8,54% 5,27% 4,75% 2,79% 2,56% 2,57% 0,85%
Containing +2 pronouns 9,86% 7,55% 6,90% 4,34% 2,51% 2,39% 0,60% 0,64% 0,88% 0,09%
Ratio of pronominal reference 55,20% 51,91% 48,63% 35,96% 24,94% 22,16% 20,94% 16,21% 15,08% 9,05%
Figure 1: Pronominal references in text collections
antecedents of these pronominal references
are mainly concepts represented typically by
noun phrases. It seems again important solv-
ing these references for a correct modelling
of relations between concepts expressed by
noun-phrases. The lowest ratio results are
presented by CRANFIELD collection with a
9,05%. The reason of this level of pronominal
use is due to text contents. This collection is
composed by extracts of very high technical
subjects. Between the described percentages
we nd the CACM, LISA and FR collections.
These collections are formed by abstracts and
documents extracted from the Federal Regis-
ter, from the CACM journal and from Library
and Information Science Abstracts, respec-
tively. As general behaviour, we can notice
that as more technical document contents be-
come, the pronouns "it" and "its" become the
most appearing in documents and the ratio
of pronominal references used decreases. An-
other observation can be extracted from this
analysis. Distribution of pronouns within sen-
tences is similar in all collections. Pronouns
appear scattered through sentences contain-
ing one or two pronouns. Using more than
two pronouns in the same sentence is quite
infrequent.
After analysing these results an important
question may arise. Is it worth enough to
solve pronominal references in documents? It
would seem reasonable to think that resolu-
tion of pronominal anaphora would only be
accomplished when the ratio of pronominal
occurrence exceeds a minimum level. How-
ever, we have to take into account that the
cost of solving these references is proportional
to the number of pronouns analysed and con-
sequently, proportional to the amount of in-
formation a system will ignore if these refer-
ences are not solved.
As results above state, it seems reason-
able to solve pronominal references in queries
and documents for QA tasks. At least, when
the ratio of pronouns used in documents rec-
ommend it. Anyway, evaluation and later
analysis (section 5) contribute with empiri-
cal data to conclude that applying pronom-
inal anaphora resolution techniques improve
QA systems performance.
4 Our Approach
Our system is made up of three modules. The
rst one is a standard IR system that retrieves
relevant documents for queries. The second
module will manage with anaphora resolution
in both, queries and retrieved documents. For
this purpose we use SUPAR computational
system (section 4.1). And the third one is
a sentence-extraction QA system that inter-
acts with SUPAR module and ranks sentences
from retrieved documents to locate the an-
swer where the correct answer appears (sec-
tion 4.2).
For the purpose of evaluation an IR sys-
tem has been implemented. This system is
based on the standard information retrieval
approach to document ranking described in
Salton (1989). For QA task, the same ap-
proach has been used as baseline but using
sentences as text unit. Each term in the query
and documents is assigned an inverse docu-
ment frequency (idf ) score based on the same
corpus. This measure is computed as:
idf(t) = log(
N
df(t)
) (1)
where N is the total number of documents
in the collection and df(t) is the number of
documents which contains term t. Query ex-
pansion consists of stemming terms using a
version of the Porter stemmer. Document and
sentence similarity to the query was computed
using the cosine similarity measure. The LAT
corpus has been selected as test collection due
to his high level of pronominal references.
4.1 Solving pronominal anaphora
In this section, the NLP Slot Unication
Parser for Anaphora Resolution (SUPAR)
is briey described (Ferrandez et al, 1999;
Ferrandez et al, 1998). SUPAR's architec-
ture consists of three independent modules
that interact with one other. These modules
are lexical analysis, syntactic analysis, and a
resolution module for Natural Language Pro-
cessing problems.
Lexical analysis module. This module
takes each sentence to parse as input, along
with a tool that provides the system with all
the lexical information for each word of the
sentence. This tool may be either a dictio-
nary or a part-of-speech tagger. In addition,
this module returns a list with all the neces-
sary information for the remaining modules
as output. SUPAR works sentence by sen-
tence from the input text, but stores informa-
tion from previous sentences, which it uses in
other modules, (e.g. the list of antecedents of
previous sentences for anaphora resolution).
Syntactic analysis module. This mod-
ule takes as input the output of lexical analy-
sis module and the syntactic information rep-
resented by means of grammatical formalism
Slot Unication Grammar (SUG). It returns
what is called slot structure, which stores all
necessary information for following modules.
One of the main advantages of this system is
that it allows carrying out either partial or
full parsing of the text.
Module of resolution of NLP prob-
lems. In this module, NLP problems
(e.g. anaphora, extra-position, ellipsis or PP-
attachment) are dealt with. It takes the slot
structure (SS) that corresponds to the parsed
sentence as input. The output is an SS in
which all the anaphors have been resolved. In
this paper, only pronominal anaphora resolu-
tion has been applied.
The kinds of knowledge that are going to
be used in pronominal anaphora resolution in
this paper are: pos-tagger, partial parsing,
statistical knowledge, c-command and mor-
phologic agreement as restrictions and several
heuristics such as syntactic parallelism, pref-
erence for noun-phrases in same sentence as
the pronoun preference for proper nouns.
We should remark that when we work with
unrestricted texts (as it occurs in this paper)
we do not use semantic knowledge (i.e. a
tool such as WorNet). Presently, SUPAR re-
solves both Spanish and English pronominal
anaphora with a success rate of 87% and 84%
respectively.
SUPAR pronominal anaphora resolution
diers from those based on restrictions and
preferences, since the aim of our preferences
is not to sort candidates, but rather to dis-
card candidates. That is to say, preferences
are considered in a similar way to restrictions,
except when no candidate satises a prefer-
ence, in which case no candidate is discarded.
For example in sentence: "Rob was asking us
about John. I replied that Peter saw John yes-
terday. James also saw him." After applying
the restrictions, the following list of candi-
dates is obtained for the pronoun him: [John,
Peter, Rob], which are then sorted according
to their proximity to the anaphora. If pref-
erence for candidates in same sentence as the
anaphora is applied, then no candidate satis-
es it, so the following preference is applied on
the same list of candidates. Next, preference
for candidates in the previous sentence is ap-
plied and the list is reduced to the following
candidates: [John, Peter ]. If syntactic par-
allelism preference is then applied, only one
candidate remains, [John], which will be the
antecedent chosen.
Each kind of anaphora has its own set of
restrictions and preferences, although they all
follow the same general algorithm: rst come
the restrictions, after which the preferences
are applied. For pronominal anaphora, the
set of restrictions and preferences that apply
are described in Figure 2.
Procedure SelectingAntecedent ( INPUT L: ListOfCandidates,
                     OUTPUT Solution: Antecedent )
Apply restrictions to L with a result of L1
Morphologic agreement
C-command constraints
Semantic consistency
Case of:
NumberOfElements (L1) = 1
Solution = TheFirstOne (L1)
NumberOfElements (L1) = 0
Exophora or cataphora
NumberOfElements (L1) > 1
Apply preferences to L1 with a result of L2
1) Candidates in the same sentence as anaphor.
2) Candidates in the previous sentence
3) Preference for proper nouns.
4) Candidates in the same position as the anaphor
with reference to the verb (before or after).
5) Candidates with the same number of parsed
constituents as the anaphora
6) Candidates that have appeared with the verb of
the anaphor more than once
7) Preference for indefinite NPs.
Case of:
NumberOfElements (L2) = 1
       Solution = TheFirstOne (L2)
NumberOfElements (L2) > 1
Extract from L2 in L3 those candidates that have
been repeated most in the text
If NumberOfElements (L3) > 1
Extract from L3 in L4 those candidates that
have appeared most with the verb of the
anaphora
Solution = TheFirstOne (L4)
Else
Solution = TheFirstOne (L3)
EndIf
EndCase
EndCase
EndProcedure
Figure 2: Pronominal anaphora resolution al-
gorithm
The following restrictions are rst applied
to the list of candidates: morphologic agree-
ment, c-command constraints and semantic
consistency. This list is sorted by proximity to
the anaphor. Next, if after applying restric-
tions there is still more than one candidate,
the preferences are then applied, in the order
shown in this gure. This sequence of prefer-
ences (from 1 to 7 ) stops when, after having
applied a preference, only one candidate re-
mains. If after applying preferences there is
still more than one candidate, then the most
repeated candidates
1
in the text are extracted
from the list after applying preferences. After
this is done, if there is still more than one can-
didate, then those candidates that have ap-
peared most frequently with the verb of the
anaphor are extracted from the previous list.
Finally, if after having applied all the previ-
ous preferences, there is still more than one
candidate left, the rst candidate of the re-
sulting list, (the closest one to the anaphor),
is selected.
4.2 Anaphora resolution and QA
Our QA approach provides a second level of
processing for relevant documents: Analysing
matching documents and Sentence ranking.
Analysing Matching Documents. This
step is applied over the best matching docu-
ments retrieved from the IR system. These
documents are analysed by SUPAR module
and pronominal references are solved. As re-
sult, each pronoun is associated with the noun
phrase it refers to in the documents. Then,
documents are split into sentences as basic
text unit for QA purposes. This set of sen-
tences is sent to the sentence ranking stage.
Sentence Ranking. Each term in the
query is assigned a weight. This weight is
the sum of inverse document frequency mea-
sure of terms based on its occurrence in the
LAT collection described earlier. Each docu-
ment sentence is weighted the same way. The
only dierence with baseline is that pronouns
are given the weight of the entity they refer
to. As we only want to analyse the eects
of pronominal reference resolution, no more
changes are introduced in weighting scheme.
For sentence ranking, cosine similarity is used
between query and document sentences.
5 Evaluation
For this evaluation, several people unac-
quainted with this work proposed 150 queries
1
Here, we mean that rstly we obtain the maxi-
mum number of repetitions for an antecedent in the
remaining list. After that, we extract from that list
the antecedents that have this value of repetition.
whose correct answer appeared at least once
into the analysed collection. These queries
were also selected based on their expressing
the user's information need clearly and their
being likely answered in a single sentence.
First, relevant documents for each query
were retrieved using the IR system described
earlier. Only the best 50 matching docu-
ments were selected for QA evaluation. As
the document containing the correct answer
was included into the retrieved sets for only
93 queries (a 62% of the proposed queries),
the remaining 57 queries were excluded for
this evaluation.
Once retrieval of relevant document sets
was accomplished for each query, the sys-
tem applied anaphora resolution algorithm to
these documents. Finally, sentence matching
and ranking was accomplished as described in
section 4.2 and the system presented a ranked
list containing the 10 most relevant sentences
to each query.
For a better understanding of evaluation re-
sults, queries were classied into three groups
depending on the following characteristics:
 Group A. There are no pronominal ref-
erences in the target sentence (sentence
containing the correct answer).
 Group B. The information required as
answer is referenced via pronominal
anaphora in the target sentence.
 Group C. Any term in the query is ref-
erenced pronominally in the target sen-
tence.
Group A was made up by 37 questions.
Groups B and C contained 25 and 31 queries
respectively. Figure 3 shows examples of
queries classied into groups B and C.
Evaluation results are presented in Figure
4 as the number of target sentences appear-
ing into the 10 most relevant sentences re-
turned by the system for each query and also,
the number of these sentences that are con-
sidered a correct answer. An answer is con-
sidered correct if it can be obtained by sim-
ply looking at the target sentence. Results
Question: ?Who is the village head man of Digha ??
Answer: ?He is the sarpanch, or village head man of
Digha, a hamlet or mud-and-straw huts  10
miles from ...?
Group B Example
Anaphora resolution: Ram Bahadu
Question: ?What did Democrats propose for low-income
families??
Answer: ?They also want to provide small subsidies for
low-income families in which both parents work
at outside jobs.?
Group C Example
Anaphora resolution: Democrats
Figure 3: Group B and C query examples
are classied based on question type intro-
duced above. The number of queries pertain-
ing to each group appears in the second col-
umn. Third and fourth columns show base-
line results (without solving anaphora). Fifth
and sixth columns show results obtained when
pronominal references have been solved.
Results show several aspects we have to
take into account. Benets obtained from ap-
plying pronominal anaphora resolution vary
depending on question type. Results for
group A and B queries show us that relevance
to the query is the same as baseline system.
So, it seems that pronominal anaphora res-
olution does not achieve any improvement.
This is true only for group A questions. Al-
though target sentences are ranked similarly,
for group B questions, target sentences re-
turned by baseline can not be considered as
correct because we do not obtain the an-
swer by simply looking at returned sentences.
The correct answer is displayed only when
pronominal anaphora is solved and pronom-
inal references are substituted by the noun
phrase they refer to. Only if pronominal ref-
erences are solved, the user will not need to
read more text to obtain the correct answer.
For noun-phrase extraction QA systems the
improvement is greater. If pronominal ref-
erences are not solved, this information will
                    Baseline              Anaphora solved
Answer Type      Number Target included Correct answer Target included Correct answer
A 37 (39,78%) 18 (48,65%) 18 (48,65%) 18 (48,65%) 18 (48,65%)
B 25 (26,88%) 12 (48,00%) 0 (0,00%) 12 (48,00%) 12 (48,00%)
C 31 (33,33%) 9 (29,03%) 9 (29,03%) 21 (67,74%) 21 (67,74%)
A+B+C 93 (100,00%) 39 (41,94%) 27 (29,03%) 51 (54,84%) 51 (54,84%)
Figure 4: Evaluation results
not be analysed and probably a wrong noun-
phrase will be given as answer to the query.
Results improve again if we analyse group
C queries performance. These queries have
the following characteristic: some of the
query terms were referenced via pronominal
anaphora in the relevant sentence. When
this situation occurs, target sentences are re-
trieved earlier in the nal ranked list than in
the baseline list. This improvement is because
similarity increases between query and target
sentence when pronouns are weighted with
the same score as their referring terms. The
percentage of target sentences obtained in-
creases 38,71 points (from 29,03% to 67,74%).
Aggregate results presented in Figure 4
measure improvement obtained considering
the system as a whole. General percentage
of target sentences obtained increases 12,90
points (from 41,94% to 54,84%) and the level
of correct answers returned by the system in-
creases 25,81 points (from 29,03% to 54,84%).
At this point we need to consider the follow-
ing question: Will these results be the same
for any other question set? We have analysed
test questions in order to determine if results
obtained depend on question test set. We ar-
gue that a well-balanced query set would have
a percentage of target sentences that contain
pronouns (PTSC) similar to the pronominal
reference ratio of the text collection that is
being queried. Besides, we suppose that the
probability of nding an answer in a sentence
is the same for all sentences in the collec-
tion. Comparing LAT ratio of pronominal
reference (55,20%) with the question test set
PTSC we can measure how a question set can
aect results. Our question set PTSC value
is a 60,22%. We obtain as target sentences
containing pronouns only a 5,02% more than
expected when test queries are randomly se-
lected. In order to obtain results according to
a well-balanced question set, we discarded ve
questions from both groups B and C. Figure 5
shows that results for this well-balanced ques-
tion set are similar to previous results. Aggre-
gate results show that general percentage of
target sentences increases 10,84 points when
solving pronominal anaphora and the level
of correct answers retrieved increases 22,89
points (instead of 12,90 and 25,81 obtained
in previous evaluation respectively).
As results show, we can say that pronom-
inal anaphora resolution improves QA sys-
tems performance in several aspects. First,
precision increases when query terms are ref-
erenced anaphorically in the target sentence.
Second, pronominal anaphora resolution re-
duces the amount of text a user has to read
when the answer sentence is displayed and
pronominal references are substituted with
their coreferent noun phrases. And third,
for noun phrase extraction QA systems it is
essential to solve pronominal references if a
good performance is pursued.
6 Conclusions and future research
The analysis of information referenced
pronominally in documents has revealed to
be important to tasks where high level of
recall is required. We have analysed and
measured the eects of applying pronominal
anaphora resolution in QA systems. As
results show, its application improves greatly
QA performance and seems to be essential in
some cases.
Three main areas of future work have ap-
peared while investigation has been devel-
oped. First, IR system used for retrieving
relevant documents has to be adapted for QA
                    Baseline              Anaphora solved
Answer Type      Number Target included Correct answer Target included Correct answer
A 37 (39,78%) 18 (48,65%) 18 (48,65%) 18 (48,65%) 18 (48,65%)
B 20 (21,51%) 10 (50,00%) 0 (0,00%) 10 (50,00%) 10 (50,00%)
C 26 (27,96%) 9 (34,62%) 9 (34,62%) 18 (69,23%) 18 (69,23%)
A+B+C 83 (89,25%) 37 (44,58%) 27 (32,53%) 46 (55,42%) 46 (55,42%)
Figure 5: Well-balanced question set results
tasks. The IR used, obtained the document
containing the target sentence only for 93 of
the 150 proposed queries. Therefore, its preci-
sion needs to be improved. Second, anaphora
resolution algorithm has to be extended to
dierent types of anaphora such as denite
descriptions, surface count, verbal phrase and
one-anaphora. And third, sentence ranking
approach has to be analysed to maximise the
percentage of target sentences included into
the 10 answer sentences presented by the sys-
tem.
References
Eric Breck, John Burger, Lisa Ferro, David House,
Marc Light, and Inderjeet Mani. 1999. A Sys
Called Quanda. In Eighth Text REtrieval Con-
ference (TRE, 1999).
Gordon V. Cormack, Charles L. A. Clarke,
Christopher R. Palmer, and Derek I. E.
Kisman. 1999. Fast Automatic Passage Rank-
ing (MultiText Experiments for TREC-8). In
Eighth Text REtrieval Conference (TRE, 1999).
Antonio Ferrandez, Manuel Palomar, and Lidia
Moreno. 1998. Anaphora resolution in unre-
striced texts with partial parsing. In 36th An-
nual Meeting of the Association for Computa-
tional Linguistics and 17th International Con-
ference on Computational Lingustics COLING-
ACL.
Antonio Ferrandez, Manuel Palomar, and Lidia
Moreno. 1999. An empirical approach to Span-
ish anaphora resolution. To appear in Machine
Translation.
David A. Hull. 1999. Xerox TREC-8 Question
Answering Track Report. In Eighth Text RE-
trieval Conference (TRE, 1999).
Kevin Humphreys, Robert Gaizauskas, Mark
Hepple, and Mark Sanderson. 1999. University
of Sheeld TREC-8 Q&A System. In Eighth
Text REtrieval Conference (TRE, 1999).
Julian Kupiec, 1999. MURAX: Finding and Or-
ganising Answers from Text Search, pages 311{
331. Kluwer Academic, New York.
Dan Moldovan, Sanda Harabagiu, Marius Pasca,
Rada Mihalcea, Richard Goodrum, Roxana
G^rju, and Vasile Rus. 1999. LASSO: A Tool
for Surng the Answer Net. In Eighth Text RE-
trieval Conference (TRE, 1999).
Thomas S. Morton. 1999. Using Coreference in
Question Answering. In Eighth Text REtrieval
Conference (TRE, 1999).
Douglas W. Oard, Jianqiang Wang, Dekang Lin,
and Ian Soboro. 1999. TREC-8 Experiments
at Maryland: CLIR, QA and Routing. In
Eighth Text REtrieval Conference (TRE, 1999).
John Prager, Dragomir Radev, Eric Brown, Anni
Coden, and Valerie Samn. 1999. The Use of
Predictive Annotation for Question Answering.
In Eighth Text REtrieval Conference (TRE,
1999).
Gerard A. Salton. 1989. Automatic Text Process-
ing: The Transformation, Analysis, and Re-
trieval of Information by Computer. Addison
Wesley, New York.
Amit Singhal, Steve Abney, Michiel Bacchiani,
Michael Collins, Donald Hindle, and Fernando
Pereira. 1999. ATT at TREC-8. In Eighth Text
REtrieval Conference (TRE, 1999).
Toru Takaki. 1999. NTT DATA: Overview of sys-
tem approach at TREC-8 ad-hoc and question
answering. In Eighth Text REtrieval Confer-
ence (TRE, 1999).
TREC-8. 1999. Eighth Text REtrieval Confer-
ence.
An Application of the Interlingua System ISS for Spanish-English 
Pronominal Anaphora Generation, 
Jesfis Peral and Antonio Ferrfindez 
Research Group on Language Processing and Information Systems. 
Department ofSoftware and Computing Systems. University of Alicante. 
03690 San Vicente del Raspeig. Aiicante, Spain. 
{jperal, antonio} @dlsi.ua.es 
Abstract 
In this ~paper, we present he Interlingua 
system ISS to generate the pronominal 
anaphora into the Spanish and English 
languages. We also describe the main 
problems in the pronoun generation into 
both languages such as zero-subject 
constructions and number, gender and 
syntactic differences. Our system improves 
other proposals presented so far due to the 
fact that we are able to solve and generate 
intersentential anaphora, to detect 
coreference chains and to generate Spanish 
zero-pronouns into English, issues that are 
hardly considered by other systems. Finally, 
we provide outstanding results of our system 
on unrestricted corpora. 
Introduction 
One of the main problems of many commercial 
Machine Translation (MT) and experimental 
systems is that they do not carry out a correct 
pronominal anaphora generation. Solving the 
anaphora nd extracting the antecedent are key 
issues in a correct generation into the target 
language. Unfortunately, the majority of MT 
systems do not deal with anaphora resolution 
and their successful operation usually does not 
go beyond the sentence l vel. In this paper, we 
present acomplete approach that allows pronoun 
resolution and generation into the target 
language. 
Our approach works on unrestricted texts 
unlike other systems, like the KANT interlingua 
system (Leavitt et al (1994)), that are designed 
for well-defined omains. Although full parsing 
of these texts could be applied, we have used 
partial parsing of the texts due to the 
unavoidable incompleteness of the grammar. 
This is a main difference with the majority of the 
interlingua systems such as the DLT system 
based on a modification of Esperanto (Witkam 
(1983)), the Rosetta system which is 
experimenting with Montague semantics as the 
basis for an interlingua (Appelo and 
Landsbergen (1986)), the KANT system, etc. as 
they use full parsing of the text. 
After the parsing and solving pronominal 
anaphora, an interlingua representation f the 
whole text is obtained. In the interlingua 
representation no semantic information is used 
as input, unlike some approaches that have as 
input semantic information of the constituents 
(Miyoshi et al (1997), Castell6n et al (1998), 
the DLT system, etc). 
From this interlingua representation, the 
generation of anaphora (including intersentential 
anaphora), the detection of coreference hains of 
the whole text and the generation of Spanish 
zero-pronouns into English have been carried 
out, issues that are hardly considered by other 
systems. Furthermore, this approach can be used 
for other different applications, e.g. Information 
Retrieval, Summarization, etc. 
The paper is organized as follows: In section 1, 
the complete approach that includes Analysis, 
Interlingua and Generation modules will be 
described. These modules will be explained in 
detail in the next three sections. In section 5, the 
Generation module has been evaluated in order 
! This paper has been partly financed by the collaborative r search project between Spain and The United Kingdom 
number HB 1998-0068. 
42 
to measure the efficiency of our proposal. To do 
so, two experiments have been accomplished: 
the generation of Spanish zero-pronouns into 
English (syntactic generation module) and the 
generation of English pronouns into Spanish 
ones (morphological generation module). 
Finally, the conclusions of this work will be 
presented. 
1 System Architecture 
The complete approach that solves and generates 
the anaphor is based on the scheme of Figure 1. 
Translation is carried out in two stages: from the 
source language to the Interlingua, nd from the 
Interlingua into the target language. Modules for 
analysis are independent from modules for 
generation..In this paper, although we have only 
studied the Spanish and English languages, our 
approach is easily extended to other languages, 
i.e. multilingual system, in the sense that any 
analysis module can be linked to any generation 
module. 
1 
I NLPproblem I NLPproblem I i 
\[ Spanish 8enefation \[ \[ English generation I 
Figure 1. System architecture. 
As can be observed in Figure 1, there are 
three independent modules in the process of 
generation: Analysis, Interlingua nd Generation 
modules. 
2 Analysis module 
The analysis is carried out by means of SUPAR 
(Slot Unification Parser for Anaphora 
resolution) system, presented in Femindez et al 
(2000). SUPAR is a computational system 
focused on anaphora resolution. It can deal with 
several kinds of anaphora, such as pronominal 
anaphora, one-anaphora, surface-count anaphora 
and definite descriptions. In this paper, we focus 
on pronominal anaphora resolution and 
generation into the target language. In 
pronominal anaphora resolution in both the 
Spanish and English languages, the system has 
achieved an accuracy of 84% and 87% 
respectively. 
A grammar defined by means of the 
grammatical formalism SUG (Slot Unification 
Grammar) is used as input of SUPAR. A 
translator that transforms SUG rules into Prolog 
clauses has been developed. This translator will 
provide a Prolog program that will parse each 
sentence. SUPAR allows to carry out either a full 
or a partial parsing of the text, with the same 
parser and grammar. Here, partial parsing 
techniques have been used due to the 
unavoidable incompleteness of the grammar and 
the use of unrestricted texts (corpora) as inputs. 
These unrestricted corpora used as input for 
the partial parser contain the words tagged with 
their grammatical categories obtained from the 
output of a part-of-speech (POS) tagger. The 
word, as it appears in the corpus, its lemma nd 
its POS tag (with morphological information) is
supplied for each word in the corpus. The corpus 
is split into sentences before applying the 
parsing. 
The output of the parsing module will be 
the Slot Structure (SS) that stores the necessary 
information 2 for Natural Language Processing 
(NLP) problem resolution. This SS will be the 
input for the following module in which NLP 
problems (anaphora, extraposition, ellipsis, etc.) 
will be treated and solved. 
In Fernindez et al (1998), a partial parsing 3 
strategy that provides all the necessary 
information for resolving anaphora is presented. 
This partial parsing shows that only the 
following constituents are necessary for 
anaphora resolution: co-ordinated prepositional 
and noun phrases, pronouns, conjunctions and 
2 The SS stores for each constituent the following 
information: constituent name (NP, PP, etc.), 
semantic and morphologic information, discourse 
marker (identifier of the entity or discourse object) 
and the SS of its subconstituents. 
3 It is important to emphasize that he system allows 
to carry out a full parsing of the text. In this paper, 
partial parsing with no semantic nformation is used 
in the evaluation ofour approach. 
43 
verbs, regardless of the order in which they 
appear in the text. The free words consist of 
constituents hat are not covered by this partial 
parsing (e.g. adverbs). 
After applying the anaphora resolution 
module, a new Slot Structure (SS') is obtained. 
In this new structure the correct antecedent 
(chosen from the possible candidates) for each 
anaphoric expression will be stored together 
with its morphological and semantic 
information. SS' will be the input for the 
lnterlingua system. 
3 Interl ingua system (ISS) 
As said before, the Interlingua system takes the 
SS of the sentence after applying the anaphora 
resolution module as input. This system, named 
lnterlingua Slot Structure (1SS), generates an 
interlingua representation from the SS of the 
sentence. 
SUPAR generates one SS for each sentence 
from the whole text and it solves intrasentential 
and intersententiai anaphora. Then, 1SS 
generates the interlingua representation f the 
whole text. This is one of the main advantages 
of 1SS because it is possible to generate 
intersentential pronominal naphora. 
To begin with, 1SS splits sentences into 
clauses 4.To identify a new clause when partial 
parsing has been carried out, the following 
heuristic has been applied: 
H1 Let us assume that the beginning of a new 
clause has been found when a verb is parsed 
and a free conjunction is subsequently parsed. 
In this particular case, a free conjunction 
does not imply conjunctions that join co- 
ordinated noun and prepositional phrases. It 
refers, here, to conjunctions that are parsed in 
our partial parsing scheme. 
Once the text has been split into clauses, 
the next stage is to generate the interlingua 
representation for clauses. We have used a 
complex feature structure for each clause. In 
Figure 2 the information of the first clause of the 
example (1) is presented: 
(1) The boys of the mountains were in the 
garden. They were catching flowers. 
4 A clause could be defined as "a group of words 
containing a verb". 
( Verb: be "\] 
1Number: plural 1 
ACTION= .~ Person: third 
1 Tense: past \] 
L Type: impersonalJ 
("Cat: NP "~ 
\]Identifier: X \[ 
I Head: boy \[ 
) Number: plural 
AGENT= ~ Gender: masculine 
I Person: third \[ 
I MODIFIER: of the mountains \[ 
~Sem_Re f: human J 
THEME={} 
f Cat: PP 
Identifier: Y 
Prep: in . ~Cat: NP 
\[Identifier: Z 
MODIFIER= ~ I Head: garden 
L 
..~ Number: singular 
ENTITY= ~ (3ender: masculine 
\[Person: third 
~, Sere Ref: location 
Figure 2. lnterlingua representation fa clause. 
As can be observed in Figure 25 , the 
interlingua is a frame composed of semantic 
roles and features extracted from the SS of the 
clause. Semantic roles that have been used in 
this approach are the following: ACTION, 
AGENT, THEME and MODIFIER that 
correspond to verb, subject, object and 
prepositional phrases of the clause respectively. 
The notation we have used is based on the 
representation used in KANT interlingua. To 
identify these semantic roles when partial 
parsing has been carried out and no semantic 
knowledge is used, the following heuristic has 
been applied: 
H2 Let us assume that the NP parsed before 
the verb is the agent of the clause. In the same 
way, the NP parsed after the verb is the theme 
of the clause. Finally., all the PP found in the 
clause are its modifiers. 
5 Only the relevant attributes of each semantic role 
appear in a simplified way in the picture. Additional 
attributes are added to the semantic roles in order to 
complete all the necessary information for the 
interlingua representation. 
44 
In Figure 2 the following elements have 
been found: ACTION= 'were', AGENT = 'the 
boys of the mountains', THEME= ~ (it has not 
been found any NP after the verb) and 
MODIFIER = 'in the garden'. These elements 
are represented by a simple feature structure. 
Features are represented asattributes with their 
corresponding values. 
The semantic role ACTION has the 
following attributes: Verb with the value of the 
lemma of the verb; Number, Person and Tense 
(grammatical features) and Type with the type of 
the verb: impersonal, transitive, etc. 
'The semantic role AGENT has the 
following fiitributes: Cat that contains the 
syntactic ategory of the constituent; Identifier 
with the value of the discourse marker; Head 
that contains the lemma of the constituent's 
head; Number, Gender and Person contain 
grammatical features of the constituent; 
MODIFIER that contains all the information 
about he modifiers (PP) of the NP, and Sem_Ref 
that contains semantic information about the 
constituent's head if this information is 
available. The semantic role THEME has the 
same attributes as the semantic role AGENT, i.e. 
the difference is that THEME is the object of the 
clause and AGENT is the subject. 
Finally, the semantic role MODIFIER has 
the following attributes: Cat that contains the 
syntactic ategory of the constituent; Identifier 
with the value of the discourse marker; Prep 
with the preposition of the constituent and 
ENTITY, which is the object of the PP and 
contains the same attributes as the THEME. 
One clause can have more than one MODIFIER 
depending on the number of PP that it has. It is 
important to emphasize that all this information 
is extracted from the SS of the constituents 
parsed in the clause. 
As said before, instead of representing the 
clauses independently, we are interested in the 
interlingua representation of the whole input 
text. With the global representation f the input 
text we will be able to generate intrasentential 
and intersentential anaphora. Furthermore, it will 
be possible to solve and generate coreference 
chains. Thereby, the scheme of Figure 2 is 
extended in order to represent all the discourse 
using the clauses as main units of this 
representation. In Figure 3 the interlingua 
representation f the whole text of the example 
(1) can be observed. 
ENITTY I 
Cat: NP 
Identifier: V 
Head: boy 
Numb, Gend, Pers: 
plural, mase, thlrd 
MODIFIER: 
Sem_Ret~ human 
ENI'IIT 2 
Cat:. NP 
Identifier: X 
Head: mountain 
Numb, Gend, Pets: 
pl, fern, third 
Sern_Ref: location 
ENTITY 3 
Cat: NP t Identifier: Y 
Head: garden 
Numb, Oend, Pets: 
sing, masc., third 
Sum Ref: location 
ENIITY 4 
Cat: NP 
Identifier: Z 
Head: flower 
Numb, Gend, Pets: 
plural, fem, third 
Sem_Ref: thing 
CLAUSE 1 
Sentence 1D: 1 
ACTION~ be 
MODIFIER: 
Cat: PP, Id: Q, 
Prep: in 
COREFERENCE 
CHAIN 
Cl.d USE 2 
Sentence ID: 2 
ACTIO/'~ catch 
.THEME: Z 
AGENT: 
\[ Type, Num, Gcnd 
\[ Person, Head 
I ENTITY:V 
Figure 3. Interlingua representation f 
example (1). 
On the left side of Figure 3 the new objects 
or entities of the discourse are represented. 
These objects are named ENTITIES and 
contain the following attributes: Cat, Identifier, 
Head, Number, Gender, Person and Sem_Ref, 
due to they can represent an AGENT, a 
THEME or an object in a MODIFIER. 
On the right side, the CLAUSES of the 
text are represented in a simplified way. They 
contain the semantic role. ACTION with its 
attributes and the semantic roles AGENT, 
THEME and MODIFIER that have appeared 
in the clause. These semantic roles are linked to 
the ENTITIES that they refer to. It also contains 
the identifier of the sentence in which the 
CLAUSE appears (Sentence lD) and the 
Conjunction that joints two or more CLAUSES 
in a sentence. 
45 
In the picture, four ENTITIES and two 
CLAUSES can be distinguished. The 
ENTITIES are as follows: ENTITY 1 ('boy'), 
ENTITY 2 ('mountain), ENTITY 3 ('garden') 
and ENTITY 4 ('flower'). Moreover, a relation 
between two ENTITIES (number 1and number 
2) appears in the picture due to the ENTITY1 
(NP) contains aMODIFIER (PP). 
The CLAUSE 1 contains: Sentence 1D 
(' 1 '), ACTION ('be'), AGENT ('V', the link to 
ENTITY 1) and MODIFIER (which is a PP 
and contains the link to ENTITY 3). The 
CLAUSE 2 contains: Sentence_lD ('2'), 
'ACTION ('catch'), AGENT (which is a 
PRONOUN and contains the link to ENTITY 1) 
and THEME ('Z', the link to ENTITY 4). 
The coreference chain can be identified 
thanks to AGENTS of CLAUSE 1 and 
CLAUSE 2 ('the boys' and 'they') have their 
links to the same ENTITY. As can be seen, 
these links can occur between constituents of 
different clauses or different sentences. Then, 
the global system is able to generate 
intersentential anaphora and identify the 
coreferenee chains of the text. 
4 Generation module 
The Generation module takes the interlingua 
representation f the text as input and generates 
it into the target language. In this paper, we are 
only describing the generation of pronouns. The 
generation phase is split into two modules: 
syntactic generation and morphological 
generation. In the next two subsections they will 
be studied in detail. Although the approach 
presented here is multilingual, we have focused 
on the generation i to the Spanish and English 
languages. 
4.1 Syntactic generation 
In syntactic generation the interlingua 
representation is converted by 'transformational 
rules' into an ordered surface-structure tr e, with 
appropriate labeling of the leaves with target 
language grammatical functions and features. 
The basic task of syntactic generation is to order 
constituents in the correct sequence for the target 
language. However, the aim of this work is only 
the generation of pronominal anaphora into the 
target language, so we have only focused on the 
differences between the Spanish and English 
languages in the generation of the pronoun. 
These differences are what we have named 
discrepancies (a study of Spanish-English- 
Spanish discrepancies is showed in Peral et aL 
(1999)). In syntactic generation the following 
discrepancies can be found: syntactic 
discrepancies and Spanish elliptical zero-subject 
constructions. 
4.1.1 Syntactic discrepancies 
This discrepancy is due to the fact that the 
surface structures of the Spanish sentences are 
more flexible than the English ones. The 
constituents of the Spanish sentences can appear 
without a specific order in the sentence. In order 
to carry out a correct generation i to English, we 
must firstly reorganize the Spanish sentence. 
Nevertheless, in the English-Spanish translation, 
in general, this reorganization is not necessary. 
Let us see an example with the Spanish 
sentence 
(2) A Pedro Io vi ayer. 
(I saw Peter yesterday.) 
In (2), the object of the verb, A Pedro (to 
Peter), appears before the verb (in the position 
of the theoretically subject) and the subject is 
omitted (this phenomena is usual in Spanish and 
it will be explained in the next subsection). The 
PP A Pedro (to Peter) functions as an indirect 
object of the verb (because it has the preposition 
A (to)). We can find out the subject since the 
verb is in first person and singular, so the subject 
would be the pronoun Yo (1). Moreover, there is 
a pronoun, lo (him) that functions as 
complement of the verb vi (saw). This pronoun 
in Spanish refers to the object of the verb, Peter, 
when it is moved from its theoretical place after 
the verb (as it occurs in this sentence). 
As explained before, it is possible to 
identify the semantic roles (AGENT, ACTION, 
etc.) of the previous constituents in the 
CLAUSE applying a series of heuristics. Once 
the semantic roles of the constituents have been 
established, they will be stored in the interlingua 
representation. The generation i to English will 
be a new clause in which the order of the 
constituents i the usual in English: AGENT, 
ACTION, THEME and MODIFIERS. 
46 
4.1.2 Elliptical zero-subject constructions 
(zero-pronouns) 
As commented before, the Spanish language 
allows to omit the pronominal subject of the 
sentences. These omitted pronouns are usually 
named zero-pronouns. While in other languages, 
zero-pronouns may appear in either the subject's 
or the object's grammatical position, (e.g. 
Japanese), in Spanish texts, zero-pronouns only 
appear in the position of the subject. In English 
texts, this sort of pronoun occurs far less 
frequently, as the use of them are generally 
compulsory in the language. Nevertheless, some 
examples can be found: "Ross carefully folded 
his trousers and ~.climbed into bed". (The 
symbol ~ shows the position of the omitted 
pronoun).. Target languages with typical 
elliptical (zero) constructions corresponding to
source English pronouns are Italian, Thai, 
Chinese or Japanese. 
In order to generate Spanish zero-pronouns 
into English, they must first be located in the 
text (ellipsis detection), and then resolved 
(anaphora resolution). At the ellipsis detection 
stage, information about the zero-pronoun (e.g. 
person, gender, and number) must first be 
obtained from the verb of the clause and then 
used to identify the antecedent of the zero- 
pronoun (resolution stage). The detection 
process depends on the knowledge about the 
structure of the language itself, which gives us 
clues to the use of each type of zero-pronoun. 
The resolution of zero-pronouns has been 
implemented in SUPAR. As we may work on 
unrestricted texts to which partial parsing is 
applied, zero-pronouns must also be detected 
when we do not dispose of full syntactic 
information. Once the input text has been split 
into clauses after applying the heuristic H1, the 
next problem consists of the detection of the 
omission of the subject from each clause. 
If partial parsing techniques have been 
applied, we can establish the following heuristic 
to detect he omission of the subject from each 
clause: 
H3 After the sentence has been divided into 
clauses, a noun phrase or a pronoun is sought, 
for each clause, through the clause constituents 
on the left-hand side of the verb, unless it is 
imperative or impersonal. Such a noun phrase 
or pronoun must agree in person and number 
with the verb of the clause. 
Sometimes, gender information of the 
pronoun can be obtained when the verb is 
copulative. For example, in: 
(3) Pedroj vio a Anak en el parque. Ok Estaba 
muy guapa. 
(Peterj saw Annk in the park. Shek was very 
beautiful.) 
In this example, the verb estaba (was) is 
copulative, so that its subject must agree in 
gender and number with its object whenever the 
object can have either a masculine or a feminine 
linguistic form (guapo: masc, guapa: fem). We 
can therefore get information about its gender 
from the object, guapa ("beautiful" in its 
feminine form) which automatically assigns it to 
the feminine gender so the omitted pronoun 
would have to be she rather than he. 
After the zero-pronoun has been detected, 
SUPAR inserts the pronoun (with its information 
of person, gender and number) in the position in 
which it has been omitted. This pronoun will be 
detected and resolved in the following module of 
anaphora resolution. After that, ISS generates the 
interlingua representation f the text. 
In the example (3), two CLAUSES are 
identified. In the second CLAUSE the zero- 
pronoun is detected (third person, singular and 
feminine -she-) and solved (third person, 
singular and feminine -Ann-). So the AGENT of 
this CLAUSE is the PRONOUN she and it has a 
link to the ENTITY Ann (the chosen 
antecedent). 
Now, the generation of Spanish zero- 
pronouns into English is easy because all the 
information that it is needed is located in the 
interlingua representatio.n. The English 
pronoun's information is extracted in the 
following way: number and person information 
are obtained from the PRONOUN and gender 
information is obtained from the Head of its 
antecedent. 
47 
4.2 Morphological generation 
In the morphological generation we mainly have 
to treat and solve number and gender 
discrepancies in the generation of pronouns. 
4.2.1 Number discrepancies 
This problem is generated by the discrepancy 
between words of different languages that 
express the same concept. These words can be 
referred to a singular pronoun in the source 
language and to a plural pronoun in the target 
language. For example, in English the concept 
.people is plural, whereas in Spanish is singular. 
(4) Tile stadium was full of peoplej. They~ were 
very angry withthe referee. 
(5) El estadio estaba /leno de gentei. ~stai 
estaba muy enfadada con el ~rbitro. 
In (4), it can be observed that the name 
people in English has been replaced with the 
plural pronoun they, whereas in Spanish (5) the 
name gente has been replaced with the singular 
pronoun dsta (it). Gender discrepancies also 
exist in the translation of other languages such 
as in the German-English translation. 
In order to take into account number 
discrepancies in the generation of the pronoun 
into the target language a set of morphological 
(number) rules is constructed. 
In the generation of the pronoun They into 
Spanish in the example (4), the interlingua 
representation has a PRONOUN ('they', third 
person and plural) that it is linked to the 
ENTITY ('police', plural). For the correct 
generation into Spanish the following 
morphological rule is constructed: 
pronoun + third..person + plural + antecedent 
(~olice') ~ ~sta (pronoun, third person, 
feminine and singular) 
The left-hand side of the morphological 
rule contains the interlingua representation of
the pronoun and the right-hand side contains the 
pronoun in the target language. 
In the same way, a set of morphological 
rules is constructed in order to generate English 
pronouns. Next, an example of these rules is 
shown: 
pronoun + third_person + singular + antecedent 
('policla~ ~ they (pronoun, third person and 
plural) 
4.2.2 Gender discrepancies 
English has less morphological information than 
Spanish. With reference to plural personal 
pronouns, the pronoun we can be translated into 
nosotros (masculine) or nosotras (feminine), you 
into ustedes (masculine/feminine), vosotros 
(masculine) or vosotras (feminine) and they into 
ellos or elias. Furthermore, the singular personal 
pronoun it can be translated into dl/dste 
(masculine) or ella/dsta (feminine). For 
example: 
(6) Women~ were in the shop. They~ were buying 
gifts for their husbands. 
(7) Las mujeresl estaban en la tienda. Ellasi 
estaban comprando regains pard sus maridos. 
In Spanish, the plural name mujeres 
(women) is feminine and is replaced by the 
personal pronoun elias (plural feminine) (7), 
whereas in English they is valid for masculine as 
well as for feminine (6). ", 
These discrepancies do not always mean 
that Spanish anaphors bear more information 
than English one. For example, Spanish 
possessive adjectives ~ casa) do not carry 
gender information whereas English possessive 
adjectives do (his~her house). 
We can find similar discrepancies among 
other languages. For example, in the 
French-German translation, gender is assigned 
arbitrarily in both languages (although in French 
is not as arbitrarily as in German). The English- 
German translation, like English-Spanish, 
supposes a translation from a language with 
neutral gender into a language that assigns 
gender grammatically. 
As commented, it is important to 
emphasize that the omission of the pronominal 
subject is very usual in Spanish. If we want to 
stress the subject of a clause or distinguish 
between different possible subjects, we will have 
to write the pronominal subject. Otherwise, 
pronominal subject could be omitted. We are 
interested, however, in the correct generation of 
pronouns, and therefore, they will never be 
omitted. 
Thanks to the fact that our system solves 
only personal pronouns in third person, we have 
only studied gender discrepancies in the 
generation of the third person pronouns. The 
48 
study has been divided into pronouns with 
subject role and pronouns with complement role. 
a) Pronouns with subject role. This kind of 
pronouns can be identified in the interlingua 
representation because they have the semantic 
role of AGENT in a CLAUSE. Their 
antecedents are established with the links to the 
ENTITIES. 
The main problem in the pronoun 
generation into English consists of the 
generation of pronoun it. If we have a pronoun 
with the following attributes: masculine, 
singular and third person in the interlingua 
representation, this can be generated into the 
Spanish pron6uns he or it. If the antecedent of
the pronoun refers to a person, we will generate 
it into he. If the antecedent of the pronoun is an 
animal or a thing we will generate it into it. 
These characteristics of the antecedent can be 
obtained from the semantic information stored in 
its attribute Sem_Ref. A similar strategy is used 
to generate the pronouns she or it. With 
reference to plural personal pronouns: 
masculine/feminine, plural and third person, 
they are generated into the English pronoun 
they. 
In Figure 4, the set of morphological rules 
to treat gender discrepancies in English 
generation of pronouns is shown: 
pron + third_person + masculine + sing + antec (person) ~ he 
pmn + thirdperson + masculine + sing + antec (animal or thing) --) it 
pron + thirdperson + feminine + sing + antec (person) ..-) she 
pron + third_person + feminine + sing + antec (animal orthing) ~ it 
pron + thitrlperson + feminine/masculine +plural ~ they 
Figure 4. 
In Spanish generation, the main problem 
consists of the translation of pronoun it The set 
of morphological rules to treat his case is shown 
in Figure 5: 
pron + third_person + sing + antec (animal with masculine gender) ~ ~1 
\[ pron + third_person + sing + antec (thing with masculine gender) .-) dste 
\] pron + third_person + sing + antec (animal with feminine gender) ~ ella 
l pmn + third_person + sing + antec (thing with feminine gender) .-y 6sta 
Figure 5. 
b) Pronouns with complement role. This kind of 
pronouns can be identified in the interlingua 
representation because they have the semantic 
role of THEME or they are in a MODIFIER in 
a CLAUSE. 
In the pronoun generation i to English, the 
set of morphological rules of Figure 6 is 
applied: 
pron + third_parson + sing + antec (person with masculine gander) ~ him 
pron + third_person + sing + antec (person with feminine gender) ~ her 
pron + third_person + sing + antec (animal orthing) ...) it 
pron ? thirdperson + plural + antec (person) ~ them 
Figure 6. 
In the process of generating a pronoun with 
the semantic role of THEME into Spanish, the 
set of morphological rules of Figure 7 is 
applied: 
pron ? third_person ? plural ~ les 
Figure 7. 
On the other hand, if the pronoun is in a 
MODIFIER, the rules of Spanish generation 
will be as shown in Figure 8: 
pron + third_person + sing + antec (masculine gender) ~ ~1 
pron + third_person + sing + antec (feminine gender) ...) ella 
pron + third.person + plural + antec (masculine gender) ~ ellos 
pron + third_person + plural + antec (feminine gender) ~ elias 
Figure 8. 
5 Evaluation 
The syntactic generation and morphological 
generation modules of our approach ave been 
evaluated. To do so, one experiment for each 
module has been accomplished. In the first one, 
the generation of Spanish zero-pronouns into 
English, using the techniques described above in 
subsection 4.1.2, has been evaluated 6. In the 
second one, the generation of English 
pronouns into Spanish ones has been evaluated. 
In this experiment number and gender 
discrepancies and their resolution, described 
above in section 4.2, have been taken into 
account. 
With reference to the first experiment, our 
computational system has been trained with a 
6 Syntactic discrepancies has not been evaluated due 
to the aim of this work is only the pronominal 
anaphora generation i to the target language, so the 
evaluation of the generation of the whole sentence 
into the target language has been omitted. 
49 
handmade corpus 7 that contains 106 zero- 
pronouns. With this training, we have extracted 
the degree of importance of the preferences that 
are used in the anaphora resolution module of 
the system. Furthermore, we have been able to 
check and correct the techniques used in the 
detection and generation of zero-pronouns into 
English. After that, we have carried out a blind 
evaluation on unrestricted texts using the set of 
preferences and the generation techniques 
learned during the training phase. In this case, 
partial parsing of the text with no semantic 
information has been used. 
With regard to unrestricted texts, our 
system has been run on two different Spanish 
corpora: a) a fragment of the Spanish version of 
The Blue Book corpus (15,571 words), which 
contains the handbook of the International 
Telecommunications Union CCITT, and b) a 
fragment of the Lexesp corpus (9,746 words), 
which contains ten Spanish texts from different 
genres and authors. These texts are taken mainly 
from newspapers. These corpora have been 
POS-tagged. Having worked with different 
genres and disparate authors, we feel that the 
applicability of our proposal to other sorts of 
texts is assured. 
To evaluate the generation of Spanish zero- 
pronouns into English three tasks have been 
accomplished: a) the evaluation of the detection 
of zero-pronouns, b) the evaluation of anaphora 
resolution and c) the evaluation of generation. 
a) Evaluating the detection of zero-pronouns. 
To do this, verbs have been classified into 
two categories: 1) verbs whose subjects 
have been omitted, and 2) verbs whose 
subjects have not. We have obtained a 
success rate s of 88% on 1,599 classified 
verbs, with no significant differences seen 
between the corpora. We should also 
remark that a success rate of 98% has been 
obtained in the detection of verbs whose 
subjects were omitted, whereas only 80% 
was achieved for verbs whose subjects 
were not. This lower success rate is 
This corpus contains entences with zero-pronouns 
made by different researchers of our Research Group. 
g By "success rate", we mean the number of verbs 
successfully classified, divided by the total number of 
verbs in the text. 
justified for several reasons. One important 
reason is the non-detection of impersonal 
verbs by the POS tagger. Two other 
reasons are. the lack of semantic 
information and the inaccuracy of the 
grammar used. It is important to note that 
46% of the verbs in these corpora have 
their subjects omitted. It shows quite 
clearly the importance of this phenomenon 
in Spanish. 
b) Evaluating anaphora resolution. In this 
task, the evaluation of zero-pronoun 
resolution is accomplished. Of the 1,599 
verbs classified in these two corpora, 734 
of them have zero-pronouns. Only 228 of 
them 9, however, are in third person and will 
be anaphorically resolved. A success rate of 
75% was attained for the 228 zero- 
pronouns. By "successful resolutions" we 
mean that the solutions offered by our 
system agree with the solutions offered by 
two human experts. 
c) Evaluating zero-pronoun generation. The 
generation of the 228 Spanish zero- 
pronouns into English has been evaluated. 
The following results in the generation 
have been obtained: a success rate of 70% 
in Lexesp and a success rate of 899'o in The 
Blue Book. In general (both corpora) a 
success rate of 75% has been achieved. The 
errors are mainly produced by fails in 
anaphora resolution and fails in the 
generation of pronouns he/she/it (some 
heuristics 10, which have failed sometimes, 
have been applied due to the used corpora 
do not include semantic information). 
In the second experiment, we have 
evaluated the generation of Spanish personal 
pronouns with subject role into the English ones. 
A fragment of the English version of The Blue 
Book corpus (70,319 words) containing 165 
9 The remaining pronouns are not in third person or 
they are cataphoric (the antecedent appears after the 
anaphor) or exophoric (the antecedent does not 
appear, linguistically, inthe text). 
J0 For instance: "all the pronouns in third person and 
singular whose antecedents are proper nouns have 
boon translated into he (antecedent with masculine 
gender) or she (antecedent with feminine gender); 
otherwise they have been translated into it". 
50 
pronouns with subject role has been used in 
order to carry out a blind evaluation. A success 
rate of 85.41% has been achieved. The errors are 
mainly produced by fails in anaphora resolution 
and in the correct choice of the gender of the 
antecedent's Head in Spanish. With reference to 
the choice of the gender of the antecedent's 
Head, an electronic dictionary has been used in 
order to translate the original English word into 
the Spanish one, and subsequently, the gender is 
extracted from the Spanish word. Several 
problems have occurred when using this 
electronic dictionary: 
1) ' the word to be translated does not appear in 
the dic-tionary, and therefore, a heuristic is 
applied to assign the gender 
2) the correct sense of the English word is not 
chosen, and therefore, the gender could be 
assigned incorrectly. 
Conclusion 
In this paper a complete approach to solve and 
generate pronominal anaphora in the Spanish 
and English languages is presented. The 
approach works on unrestricted texts to which 
partial parsing techniques have been applied. 
After the parsing and solving pronominal 
anaphora, an interlingua representation (based 
on semantic roles and features) of the whole text 
is obtained. The representation f the whole text 
is one of the main advantages of our system due 
to several problems, that are hardly solved by 
the majority of MT systems, can be treated and 
solved. These problems are the generation of 
intersentential anaphora, the detection of 
coreference hains and the generation of Spanish 
zero-pronouns into English. Generation of zero- 
pronouns and Spanish personal pronouns has 
been evaluated obtaining a success rate of 75% 
and 85.41% respectively. 
References 
Appelo, L. and Landsbergen, J. (1986) The machine 
translation project Rose. In Proceedings of I. 
International Conference on the State of the Art in 
Machine Translation in America, Asia and Europe, 
1A1-MT'86 (Saarbr0cken). pp. 34-51. 
Castell6n, I.; Fern~Sndez, A.; Mart|, M.A.; Morante, 
R. and V~zquez, G. (1998) An lnterlingua 
Representation Based on the Lexieo-Semantie 
Information. In Proceedings of the Second AMTA 
SIG-1L Workshop on lnterlinguas (Philadelphia, 
USA, 1998). 
Ferrfindez, A.; Palomar, M. and Moreno, L. (1998) 
Anaphora resolution in unrestricted texts with 
partial parsing. In Proceedings of the 36th Annual 
Meeting of the Association for Computational 
Linguistics and 17th International Conference on 
Computational Linguistics, COLING - ACL'98 
(Montreal, Canada, 1998). pp. 385-391. 
Ferrfindez, A.; Palomar, M. and Moreno, L. (2000) 
An empirical approach to Spanish anaphora 
resolution. To appear in Machine Translation 
(Special Issue on anaphora resolution in Machine 
Translation). 2000. 
Leavitt, J.R.R.; Lonsdale, D. and Franz, A. (1994) A 
Reasoned Interlingua for knowledge-Based 
Machine Translation. InProceedings of CSCS1-94. 
Miyoshi, H.; Ogino, T. and Sugiyarna, K. (1997) 
EDR's Concept Classification and Description for 
Interlingual Representation. I  Proceedings of the 
First Workshop on lnterlinguas (San Diego, USA, 
1997). 
Peral, J.; Palomar, M. and Ferffmdez, A. (1999) 
Coreference-oriented Interlingual Slot Structure 
and Machine Translation. In Proceedings of ACL 
Workshop on Coreference and its Applications 
(College Park, Maryland, USA, 1999). pp. 69-76. 
Witkam, A.P.M. (1983) Distributed language 
translation: feasibility study of multi!ingual facility 
for videotex information etworks. BSO, Utrecht. 
51 
 
Passage Selection to Improve Question Answering 
 
Fernando LLopis 
Departamento de Lenguajes y 
Sistemas Inform?ticos 
Alicante (Spain) 03800 
llopis@dlsi.ua.es 
Jos? Luis Vicedo 
 Departamento de Lenguajes y 
Sistemas Inform?ticos 
Alicante (Spain) 03800 
vicedo@dlsi.ua.es 
Antonio Ferr?ndez 
Departamento de Lenguajes y 
Sistemas Inform?ticos 
Alicante (Spain) 03800 
antonio@dlsi.ua.es 
 
Abstract  
Open-Domain Question Answering systems 
(QA) performs the task of detecting text 
fragments in a collection of documents that 
contain the response to user?s queries. 
These systems use high complexity tools that 
reduce its applicability to the treatment of 
small amounts of text. Consequently, when 
working on large document collections, QA 
systems apply Information Retrieval (IR) 
techniques to reduce drastically text 
collections to a tractable quantity of 
relevant text. In this paper, we propose a 
novel Passage Retrieval (PR) model that 
performs this task with better performance 
for QA purposes than current best IR 
systems 
1 Introduction 
Information Retrieval (IR) systems receive as 
input a user?s query, and they have to return a 
set of documents sorted by their relevance to the 
query. There are different techniques to carry 
out the document extraction process, but most of 
them are based on pattern matching modules that 
depend on the number of times that a query term 
appear in each document, as well as the 
importance or discrimination value of each term 
in the document collection. Question Answering 
(QA) systems try to improve the output 
generated by IR systems by means of returning 
just small pieces of text that are supposed to 
contain the response. Usually, QA systems 
combine IR and Natural Language Processing 
(NLP) techniques to perform their task. This 
combination allows text understanding until a 
minimum level that permits a precise answer 
detection and extraction. Nevertheless, since 
NLP techniques are computationally expensive, 
QA systems need to reduce the amount of text 
where these techniques have to be applied. In 
this way, they usually work on the output of IR  
systems [10] that select the most relevant 
documents to the query by supposing that they 
will contain the answer required. Most applied 
IR systems are mainly based on three models: 
the cosine model [15], the pivoted cosine model1 
[17], and the probabilistic model (OKAPI [18]). 
Moreover, IR systems usually employ query 
expansion techniques that frequently improve 
their precision. These techniques can be based 
on thesaurus [21] or on the incorporation of the 
most frequent terms in the top M relevant 
documents [7]. 
Currently, several Passage Retrieval (PR) 
systems have also been proposed for this task 
[2][5][8][9]. PR systems deal with fragments of 
text in order to determine the relevance of a 
document to a query, as well as to detect 
document extracts that are likely to contain the 
expected answer (instead of full documents). 
Although PR systems apply IR-based techniques 
to perform their work, they have revealed to be 
more effective than IR systems for QA tasks. 
In this paper, we are analysing the importance of 
the IR-n PR system for QA n [11] as it was used 
in last TREC-10 Conference [19]. The following 
section briefly presents the backgrounds in IR, 
PR and QA. Section 3 shows the architecture of 
IR-n. Section 4 presents the evaluation 
accomplished and finally, section 5 details 
conclusions and work in progress.  
                                                   
1 It is a modification of the cosine model. It tries to 
reduce the problem of the preference for bigger 
documents. 
2 Backgrounds in Question Answering and 
Passage Retrieval 
2.1 Information Retrieval and Passage Retrieval  
Given a question, an IR system sorts the 
documents by its relevance to the query. It 
computes the similarity between each document 
and the question by taking into account the 
frequency of each query term in the document. 
This fact usually produces that bigger 
documents are preferred. A possible alternative 
to IR models is based on obtaining the similarity 
in accordance with the relevance of the passages 
contained in the document. This new approach, 
called Passage Retrieval (PR), has several 
advantages. When used for document retrieval, 
as the relevance of a document will depend on 
the relevance of the passages it contains, this 
measure will not be affected by the length of the 
full document. Moreover, these techniques allow 
to detect high relevant information embedded in 
a long document obtaining, this way, better 
performance than IR approaches [2][9]. On the 
other hand, when applied for QA tasks, PR 
systems allow reducing the amount of text to be 
processed with costly NLP tools by returning 
passages instead of whole documents. 
Two classifications can be accomplished in PR. 
The first one is in accordance with the way of 
dividing the documents into passages. The 
second one is in accordance with the moment in 
which the passage segmentation is carried out. 
With reference to the first one, PR community 
generally agrees with the classification proposed 
in [2], where the author distinguishes between 
discourse models, semantic models, and window 
models. The first one uses the structural 
properties of the documents, such as sentences 
or paragraphs [13][16] in order to define the 
passages. The second one divides each 
document into semantic pieces according to the 
different topics in the document [5]. The last one 
uses windows of a fixed size (usually a number 
of terms) to determine passage boundaries [2] 
[8]. 
At first glance, we could think that discourse-
based models would be the most effective, in 
retrieval terms, since they use the structure of 
the document itself. However, this model 
greatest problem relies on detecting passage 
boundaries since it depends on the writing style 
of the author of each document. On the other 
hand, window models have as main advantage 
that they are simpler to accomplish, since the 
passages have a previously known size, whereas 
the remaining models have to bear in mind the 
variable size of each passage. Nevertheless, 
discourse-based and semantic models have the 
main advantage that they return full information 
units of the document, which is quite important 
if these units are used as input by other 
applications such as QA. 
According to the second classification, we can 
distinguish between approaches that segment 
documents into passages for indexing purposes, 
and those that perform segmentation after the 
query is posed. The first one allows a quicker 
calculation; nevertheless, the second one allows 
different segmentation models in accordance 
with the kind of query. 
The passage extraction model that we propose 
(IR-n) allows us to benefit from the advantages 
of discourse-based models since self-contained 
information units of text, such as sentences, are 
used for building passages. Moreover, another 
novel proposal in our PR system is the relevance 
measure which, unlike other discourse-based 
models, is not based on the number of passage 
terms, but on a fixed number of passage 
sentences. This fact allows a simpler calculation 
of this measure unlike other discourse-based or 
semantic models. Although each passage is 
made up by a fixed number of sentences, we 
consider that our proposal differs from the 
window models since our passages do not have a 
fixed size (i.e. a fixed number of words) since 
we use sentences with a variable size. 
Furthermore, IR-n document segmentation into 
passages is accomplished after the query is 
posed, which allows us to determine the number 
of sentences to be considered in accordance with 
the kind of the query. 
2.2 Question Answering 
Open domain QA systems are defined as tools 
capable of extracting the answer to user queries 
directly from unrestricted domain documents. Or 
at least, systems that can extract text snippets 
from texts, from whose content it are possible to 
infer the answer to a specific question. In both 
cases, these systems try to reduce the amount of 
time users spend to locate a concrete 
information. 
Interest in QA systems is quite recent. We had 
little information about this kind of systems until 
the ?First Question Answering Track? was held 
in TREC-8 Conference. This track tries to 
benefit from large-scale evaluation that was 
previously carried out on IR systems, in 
previous TREC conferences.  
If a QA system wants to successfully obtain a 
user?s request, it needs to understand both texts 
and questions to a minimum level. From a 
linguistic perspective, ?understanding? means to 
carry out many of the typical steps on natural 
language analysis: lexical, syntactic and 
semantic. This analysis takes much more time 
than the statistical analysis that is usually carried 
out in IR. Besides, as QA systems have to 
manage with as much text as done for IR tasks, 
and the user needs the answer in a limited 
interval of time, it is nearly mandatory that first, 
an IR system processes the query and second, 
the QA process continues with its output. In this 
way, the time of analysis is highly decreased. 
The analysis of current best systems [3] [4] [14] 
[6] allows identifying main QA sub-components 
where document retrieval is accomplished by 
using IR technology: 
? Question Analysis.  
? Document Retrieval.  
? Passage Selection. 
? Answer Extraction. 
3 IR-n overview  
In this section, we describe the architecture of 
the proposed PR system, namely IR-n, focusing 
on its three main modules: indexing, passage 
retrieval and query expansion.  
3.1 Indexing module 
The main aim of this module is to generate the 
dictionaries that contain all the required 
information for the passage retrieval module. It 
requires the following information for each 
term: 
? The number of documents that contain 
this term. 
? For each document: 
? The number of times this term 
appears in the document. 
? The position of each term in the 
document represented as the number 
of sentence it appears in. 
 
As term, we consider the stem produced by the 
Porter stemmer on those words that do not 
appear in a list of stop-words, list that is similar 
to those generally used for IR. On the other 
hand, query terms are also extracted in the same 
way, that is to say, we only consider the stems of 
query words that do not appear in the stop-words 
list.  
3.2 Passage retrieval module 
This module extracts the passages according to 
its similarity with the user?s query. The scheme 
in this process is the following:  
1. Query terms are sorted according to the 
number of documents they appear in. Terms that 
appear in fewer documents are processed firstly. 
2. The documents that contain any query term 
are selected.  
3. The following similarity measure is calculated 
for each passage p (contained in the selected 
documents) with the query q: 
 
Similarity_measure(p, q) = ? ?? qpt tq,tp, W?W  
Wp,t = loge( fp,t + 1). 
Wq, t= loge( fq,t + 1) ? idf 
idf  = loge( N / ft + 1) 
 
Where fp,t  is the number of times that the term t 
appears in the passage p. fq,t  represents the 
number of times that the term t appears in the 
query q. N is the number of documents in the 
collection and ft is refers to the number of 
documents that contain the term t. 
4. Only the most relevant passage of each 
document is selected for retrieval. 
5. The selected passages are sorted by their 
similarity measure. 
6. Passages are associated with the document 
they pertain and they are presented in a ranked 
list form.   
 
As we can notice, the similarity measure is 
similar to the cosine measure presented in [15]. 
The only difference is that the size of each 
passage (the number of terms) is not used to 
normalise the results. This proposal performs 
normalization according to the fixed number of 
sentences per passage. This difference makes the 
calculation simpler than other discourse-based 
PR or IR systems. Another important detail to 
remark is that we are using N as the number of 
documents in the collection, instead of the 
number of passages according to the 
considerations presented in [9]. 
As it has been commented, our PR system uses 
variable-sized passages that are based on a fixed 
number of sentences (with different number of 
terms per passage). The passages overlap each 
other, that is to say, if a passage contains N 
sentences, the first passage will be formed by the 
sentences from 1 to N, the second one from 2 to 
N+1, and so on. We decided to overlap just one 
sentence according to the experiments and 
results presented in [12]. This work studied the 
optimum number of overlapping sentences in 
each passage for retrieval purposes concluding, 
that best results were obtained when only one 
overlapping sentence was used. Regarding to the 
optimum number (N) of sentences per passage 
considered in this paper, it will be 
experimentally obtained. 
4 Evaluation 
This section presents the experiments developed 
for training and evaluating our approach. The 
experiments have been run on the TREC-9 QA 
Track question set and document collections.  
4.1 Data collection 
TREC-9 question test set is made up by 682 
questions with answers included in the 
document collection. The document set consists 
of 978,952 documents from the TIPSTER and 
TREC following collections: AP Newswire, 
Wall Street Journal, San Jose Mercury News, 
Financial Times, Los Angeles Times, Foreign 
Broadcast Information Service. 
4.2 Training 
Training experiments had two objectives. They 
were designed (1) to calculate the optimum 
number of sentences (N) that define passage 
length and (2) to test two different possible ways 
of applying our method.  
First training experiment consists of working on 
the output of one of the current best performing 
IR systems (the ATT system). This experiment 
re-sorts its output (the first 1,000 ranked 
documents) by using IR-n. Second experiment 
consists of using our proposal as the main IR 
system, that is, indexing the whole collections 
by means of IR-n. For each experiment, a 
different number of sentences per passage were 
tested: 5, 10, 15 and 20 sentences. The relevance 
of each returned document was measured by 
means of the tool provided by TREC 
organization that allows us to determine if a 
passage contains the right answer. The two 
experiments are summed up in Figure 1. 
 
ATTIR-system
Documents
Questions
IR-n system
QA system
1000 more
relevant
documents
200 morerelevant
passages
Documents
IR-n system
200 more relevant
passages
Answers
  
Figure 1. Training Experiments 
These experiments were performed using only 
the first 100 questions included in the data 
collection. Table 1 shows training results for 
passages of 5, 10, 15 and 20 sentences using 
both approaches. This results measure the 
number of questions whose correct answer was 
included into the top n retrieved passages (or 
documents) for the training question set. The 
first experiment (IR-n Ref) uses IR-n on the 
1,000 documents returned by ATT system while 
the second one (IR-n) applies passage retrieval 
overall collections. 
As we can see, IR-n Ref and IR-n test obtain 
similar results although using our approach to 
re-rank the output of a good IR system presents 
a slight better performance than applying IR-n 
overall document collection. Regarding to the 
number of sentences to be taken into account to 
define passage length, we can observe that best 
results are obtained with passages of 20 
sentences. In this case, both tests improve 
significantly the performance of ATT-system. It 
ranges from 12 (IR-n Ref) and 10 (IR-n) points 
on a passage length of 20 sentences (for only the 
first 5 documents retrieved) to 8 and 7 points 
when the first 200 documents are taken into 
account respectively. 
 
Answer  
included 
At 
5 
docs 
At 
10 
docs 
At 
20 
docs 
At 
30 
docs 
At 
50 
docs 
At 
100 
docs 
At 
200 
docs
IR-n Ref. 
5 Sent 57 66 78 83 85 88 93 
10 Sent 63 76 80 89 93 96 97 
15 Sent 70 78 83 89 94 95 96 
20 Sent 74 83 87 91 93 96 97 
IR-n  
5 Sent 55 63 75 80 84 89 90 
10 Sent 60 73 78 87 92 95 97 
15 Sent 70 76 82 87 93 95 95 
20 Sent 72 80 86 90 92 96 96 
ATT system 
 62 69 77 82 83 87 89 
 
Table 1. Number of questions rightly answered 
(training set of 100 questions). 
4.3 Experiment 
In order to evaluate our proposal we decided to 
compare the quality of the information retrieved 
by our approaches with the ranked list retrieved 
by the ATT IR system. For this evaluation, the 
682 questions included in the data collection 
were processed and the number N of sentences 
per passage was set to 20. Table 2 shows the 
results of this evaluation experiment. This table 
shows the percentage of questions whose answer 
can be found into the first n documents returned 
by the ATT IR system and the best n passages 
returned by IR-n Ref and IR-n respectively. 
These results are also presented in Figure 2 
 
These data confirm training results. In this case, 
both approaches perform better than ATT 
system and improvements range form 6 to 12 
points for 20 sentences passage length. 
 
 
Answer  
Included 
ATT 
system IR-n Ref IR-n 
At 5 docs 64.90% 74.59% 72.21% 
At 10 docs 70.33% 82.73% 80.37% 
At 20 docs 75.91% 87.37% 86.35% 
At 30 docs 79.14% 89.96% 89.31% 
At 50 docs 83.70% 91.62% 91.52% 
At 100 docs 87.37% 94.56% 95.55% 
At 200 docs 90.01% 96.03% 95.92% 
Table 2. ATT-system versus IR-n systems. 
60
65
70
75
80
85
90
95
100
5 10 20 30 50 100 200
Number of documents
%
Qu
es
tio
ns
IR-n Ref IR-n ATT system
Figure 2.  Comparative of ATT-system and 
experiments with IR-n (Passages of 20 sentences) 
5 Conclusions and future works 
In this paper, we have analysed the improvement 
obtained by our passage retrieval system, called 
IR-n, with reference to a high-performance IR 
system (ATT) regarding to is application for QA 
tasks. This improvement has been evaluated on 
the TREC-9 QA track data set. The achieved 
improvements are twofold: First, our approach 
obtains a better precision by retrieving more 
passages that contain the answer to users? 
queries than ATT system does. Second, since 
our approach returns passages (instead of 
documents), it significantly reduces the amount 
of text to be processed with costly techniques by 
the QA system. The related experiments show 
that the optimal passage length for this task is 20 
when passages are made up by a fixed number 
of sentences. Moreover, we have tested two 
different ways of applying our model. As we 
have seen, IR-n presents similar results when it 
works on the output of an IR system, than when 
it works on the whole collections. Nevertheless, 
in both cases, benefits range from 6 to 12 points 
with reference to ATT system depending on the 
number of first documents or passages retrieved 
to be processed for QA tasks.  
As future work, in order to improve our system 
precision, we intend to obtain the optimum size 
of passages in accordance with the kind of 
question. Besides, we need to investigate the 
effects of query expansion techniques on IR-n 
system. Furthermore, we are also trying to 
improve the relationship between IR-n and the 
following QA system, in order to detect the 
minimum number of passages to extract for each 
query without affecting QA performance. 
References  
[1] Bertoldi, N and Federico, M. ITC-irst at CLEF-2001 , 
Working Notes for the Clef 2001 Darmstdt, Germany , 
pp  41-44 
[2] Callan, J. Passage-Level Evidence in Document 
Retrieval. In Proceedings of the 17 th Annual ACM 
SIGIR Conference on Research and Development in 
Information Retrieval, Dublin, Ireland   1994, pp. 302-
310. 
[3] Clarke, C.; Cormack, g, Kisman, D and Lynam, T. 
Question Answering by Passage Selection(Multitext 
Experiments for TREC-9) Proceedings of the Tenth 
Text REtrieval Conference, TREC-9. Gaithersburg , 
USA 2000, pp 673-683 
[4] Harabagiu, S.; Moldovan, D.; Pasca, M.; Mihalcea, R.; 
Surdeanu, M.; Bunescu, R.; G?rju, R.; Rus, V. and 
Morarescu, P. FALCON: Boosting Knowledge for 
Answer Engines. In Nineth Text REtrieval Conference, 
Gaithersburg  USA 2000.pp 479- 
[5] Hearst, M. and Plaunt, C. Subtopic structuring for full-
length document access. Proceedings of the Sixteenth 
Annual International ACM SIGIR Conference on 
Research and Development in Information Retrieval, 
Pittsburgh, PA USA 1993 , pp 59-68 
[6] Ittycheriah, A.; Franz, M.; Zu, W. and Ratnaparkhi, A. 
IBM's Statistical Question Answering System. In 
Nineth Text REtrieval Conference, Gaithersburg  USA  
2000., pp 231-236 
[7] J. Xu and W. Croft.  Query expansion using local and 
global document analysis. In Proceedings of the 19th 
Annual International ACM SIGIR, Zurich, 
Switzerland,  1996 pp 4?11, 18?22. 
[8] Kaskiel, M. and  Zobel, J. Passage Retrieval Revisited 
SIGIR '97: Proceedings of the 20th Annual 
International ACM  Philadelphia, PA 1997, USA, pp 
27-31 
[9] KaszKiel, M. and  Zobel, J. Effective Ranking with 
Arbitrary Passages. Journal of the American Society 
for Information Science, Vol 52, No. 4, February 
2001, pp 344-364. 
[10] Litkowski, k, Syntactic Clues and Lexical Resources 
in Question-Answering In Nineth Text REtrieval 
Conference,  Gaithersburg  USA  2000  pp177-188 
[11] Llopis,  F. and  Vicedo, J.  Ir-n system, a passage 
retrieval system  at CLEF 2001  Working Notes for 
the Clef 2001 Darmstdt, Germany  2001, pp  115-120 . 
To appear in Lecture Notes in Computer Science 
[12] Llopis,  F.; Ferr?ndez, and  Vicedo, J.   Text 
Segmentation for efficient Information Retrieval Third 
International Conference on 
Intelligent Text Processing and 
Computational Linguistics. Mexico 2002 To appear in 
Lecture Notes in Computer Science 
[13] Namba, I Fujitsu Laboratories TREC9 Report. 
Proceedings of the Nineth Text REtrieval Conference, 
TREC-9. Gaithersburg,USA.2000, pp 203-208 
[14] Prager, J.; Brown, E.; Radev, D. and Czuba, K. One 
Search Engine or Two for QuestionAnswering. In 
Nineth Text REtrieval Conference, 
Gaithersburg,USA. 2000.   
[15] Salton G.  Automatic Text Processing: The 
Transformation, Analysis, and Retrieval of 
Information by Computer, Addison Wesley 
Publishing, New York. 1989 
[16] Salton, G.;  Allan, J. Buckley Approaches to passage 
retrieval in full text information systems. In R 
Korfhage, E Rasmussen & P Willet (Eds.) 
Prodeedings of the 16 th annual international ACM-
SIGIR conference on research and development in 
information retrieval. Pittsburgh PA USA , pp 49-58  
[17] Singhal, A.; Buckley, C. and  Mitra, M. Pivoted 
document length normalization. Proceedings of the 
19th annual international ACM- 1996. 
[18] Venner, G. and Walker, S. Okapi '84: `Best match' 
system. Microcomputer networking in libraries II. 
Vine, 48,1983, pp 22-26. 
[19] Vicedo, J.; Ferrandez, A and Llopis, F. University of 
Alicante al TREC-10. In Tenth Text REtrieval 
Conference, Gaithersburg,USA. 2001 
[20] Vicedo, J.; Ferrandez, A; A semantic approach to 
Question Answering systems. In Nineth Text REtrieval 
Conference, 2000  pp 440-444. 
[21] Y. Jing and W. B. Croft. An association thesaurus for 
information retrieval. In RIAO 94 Conference 
Proceedings, , New York, 1994. pp 146--160 
  
Proceedings of the 4th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 94?99,
Atlanta, Georgia, 14 June 2013. c?2013 Association for Computational Linguistics
 
RA-SR: Using a ranking algorithm to automatically building resources 
for subjectivity analysis over annotated corpora 
Yoan Guti?rrez, Andy Gonz?lez 
University of Matanzas, Cuba 
yoan.gutierrez@umcc.cu, 
andy.gonzalez@infonet.umcc.cu 
Antonio Fern?ndez Orqu?n, Andr?s 
Montoyo, Rafael Mu?oz 
University of Alicante, Spain 
antonybr@yahoo.com, {montoyo, 
rafael}@dlsi.ua.es 
 
Abstract 
In this paper we propose a method that 
uses corpora where phrases are annotated 
as Positive, Negative, Objective and 
Neutral, to achieve new sentiment 
resources involving words dictionaries 
with their associated polarity. Our 
method was created to build sentiment 
words inventories based on senti-
semantic evidences obtained after 
exploring text with annotated sentiment 
polarity information. Through this 
process a graph-based algorithm is used 
to obtain auto-balanced values that 
characterize sentiment polarities well 
used on Sentiment Analysis tasks. To 
assessment effectiveness of the obtained 
resource, sentiment classification was 
made, achieving objective instances over 
80%. 
1 Introduction 
In recent years, textual information has become 
one of the most important sources of knowledge 
to extract useful data. Texts can provide factual 
information, such as: descriptions, lists of 
characteristics, or even instructions to opinion-
based information, which would include reviews, 
emotions or feelings. These facts have motivated 
dealing with the identification and extraction of 
opinions and sentiments in texts that require 
special attention. Among most widely used terms 
in Natural Language Processing, in concrete in 
Sentiment Analysis (SA) and Opinion Mining, is 
the subjectivity term proposed by (Wiebe, 1994). 
This author defines it as ?linguistic expression of 
somebody?s opinions, sentiments, emotions, 
evaluations, beliefs and speculations?. Another 
important aspect opposed to subjectivity is the 
objectivity, which constitute a fact expression 
(Balahur, 2011). Other interesting terms also 
proposed by (Wiebe et al, 2005) considers, 
private state, theses terms involve opinions, 
beliefs, thoughts, feelings, emotions, goals, 
evaluations and judgments.  
Many researchers such as (Balahur et al, 2010; 
Hatzivassiloglou et al, 2000; Kim and Hovy, 
2006; Wiebe et al, 2005) and many others have 
been working in this way and related areas. To 
build systems able to lead SA challenges it is 
necessary to achieve sentiment resources 
previously developed. These resources could be 
annotated corpora, affective semantic structures, 
and sentiment dictionaries.  
In this paper we propose a method that uses 
annotated corpora where phrases are annotated as 
Positive, Negative, Objective and Neutral, to 
achieve new resources for subjectivity analysis 
involving words dictionaries with their 
associated polarity.  
The next section shows different sentiment and 
affective resources and their main characteristics. 
After that, our proposal is developed in section 3. 
Section 4, present a new sentiment resource 
obtained after evaluating RA-SR over many 
corpora. Section 5 described the evaluation and 
analysis of the obtained resource, and also an 
assessment of the obtained resource in Sentiment 
Classification task. Finally, conclusion and 
further works are presented in section 6. 
2 Related work 
It is known that the use of sentiment resources 
has proven to be a necessary step for training and 
evaluation for systems implementing sentiment 
analysis, including also fine-grained opinion 
mining (Balahur, 2011). 
Different techniques have been used into 
product reviews to obtain lexicons of subjective 
words with their associated polarity. We can 
study the relevant research promoted by (Hu and 
Liu, 2004) which start with a set of seed 
adjectives (?good? and ?bad?) and reinforce the 
semantic knowledge applying a expanding the 
lexicon with synonymy and antonymy relations 
provided by WordNet (Miller et al, 1990). As 
result of Hu and Liu researches an Opinion 
Lexicon is obtained with around 6800 positive 
94
 and negative English words (Hu and Liu, 2004; 
Liu et al, 2005). 
A similar approach has been used in building 
WordNet-Affect (Strapparava and Valitutti, 
2004). In this case the building method starting 
from a larger of seed affective words set. These 
words are classified according to the six basic 
categories of emotion (joy, sadness, fear, 
surprise, anger and disgust), are also expanded 
increase the lexicon using paths in WordNet. 
Other widely used in SA has been 
SentiWordNet resource (Esuli and Sebastiani, 
2006)). The main idea that encouraged its 
construction has been that ?terms with similar 
glosses in WordNet tend to have similar 
polarity?. 
Another popular lexicon is MicroWNOp 
(Cerini et al, 2007). It contains opinion words 
with their associated polarity. It has been built on 
the basis of a set of terms extracted from the 
General Inquirer1 (Stone et al, 1996).  
The problem is that these resources do not 
consider the context in which the words appear. 
Some methods tried to overcome this critique 
and built sentiment lexicons using the local 
context of words. 
We can mentioned to (Pang et al, 2002) whom 
built a lexicon with associated polarity value, 
starting with a set of classified seed adjectives 
and using conjunctions (?and?) disjunctions 
(?or?, ?but?) to deduce orientation of new words 
in a corpus. 
(Turney, 2002) classifies words according to 
their polarity based on the idea that terms with 
similar orientation tend to co-occur in 
documents.  
On the contrary in (Balahur and Montoyo, 
2008b), is computed the polarity of new words 
using ?polarity anchors? (words whose polarity 
is known beforehand) and Normalized Google 
Distance (Cilibrasi and Vit?nyi, 2007) scores 
using as training examples opinion words 
extracted from ?pros and cons reviews? from the 
same domain. This research achieved the lexical 
resource Emotion Triggers (Balahur and 
Montoyo, 2008a). 
Another approach that uses the polarity of the 
local context for computing word polarity is the 
one presented by (Popescu and Etzioni, 2005), 
who use a weighting function of the words 
around the context to be classified. 
All described resources have been obtained 
manually or semi-automatically. Therefore, we 
                                                 
1
 http://www.wjh.harvard.edu/~inquirer/ 
focus our target in archiving automatically new 
sentiment resources supported over some of 
aforementioned resources. In particular, we will 
offer contributions related with methods to build 
sentiment lexicons using the local context of 
words. 
3 Our method 
We propose a method named RA-SR (using 
Ranking Algorithms to build Sentiment 
Resources) to build sentiment words inventories 
based on senti-semantic evidences obtained after 
exploring text with annotated sentiment polarity 
information. Through this process a graph-based 
algorithm is used to obtain auto-balanced values 
that characterize sentiment polarities widely used 
on Sentiment Analysis tasks. This method 
consists of three main stages: (I) Building 
contextual words graphs; (II) Applying ranking 
algorithm; and (III) Adjusting sentiment polarity 
values. 
 
Figure 1. Resource walkthrough development process. 
These stages are represented in the diagram of 
Figure 1, where the development process begins 
introducing two corpuses of annotated sentences 
with positive and negative sentences 
respectively. Initially, a preprocessing of the text 
is made applying Freeling pos-tagger (Atserias et 
al., 2006) version 2.2 to convert all words to 
lemmas2. After that, all lemmas lists obtained are 
introduced in RA-SR, divided in two groups (i.e. 
positive and negative candidates, ????  and ????).  
3.1 Building contextual words graphs 
Giving two sets of sentences (???? and ????) 
annotated as positive and negative respectively, 
where ????	 = [?????, ? , ?????]  and ????	 =[?????, ? , ?????]	  contains list ?  involving 
words lemmatized by Freeling 2.2 Pos-Tagger 
                                                 
2
 Lemma denotes canonic form of the words. 
Corpora
Phrase 3
Phrase 2
W1 W2 W3 W4
W5 W3 W2
W3 W4 W5 W6
W1
W7
Phrase 1 Positve
Phrases
W5 W6 W8 W9
W8 W9 W7
W6 W9 W10 W11
W6
W1 W8
Negative
Phrases
Phrase 3
Phrase 2
Phrase 1
Positive 
Words
Negative 
Words
W1 W2 W3 W4
W5
W6 W7
W5
W6
W7
W8
W9
W10
W11
(I)
(II) Reinforcing words 
Weight = 1
(II) (II) 
(I)
Weight =1
Weight =1
Weight =1
Weight =1
W1 W2 W3 W4 W5 W6 W7 W8 W9 W10 W11
W1 W2 W3 W4 W5 W6 W7 W8 W9 W10 W11
(III) 
W1
Default Weight = 1/N Default Weight = 1/N
95
 (Atserias et al, 2006), a process to build two 
lexical contextual graphs, ????  and ????  is 
applied. Those sentences are manually annotated 
as positive and negative respectively. These 
graphs involve lemmas from the positive and 
negative sentences respectively. 
A contextual graph ?  is defined as an 
undirected graph ? =	 (?, ?) , where ?  denotes 
the set of vertices and ? the set of edges. Given 
the list ?	 = [?1 	? ??]  a lemma graph is created 
establishing links among all lemmas of each 
sentence, where words involved allow to 
interconnect sentences ??  in ? . As a result 
word/lemma networks ????  and ????  are 
obtained, where ?	 = 	?	 = [?? 	? ??]	  and for 
every edge (?? , ??)?	? being ??, ???	?. Therefore, ?? and ??	 are the same. 
Then, having two graphs, we proceed to 
initialize weight to apply graph-based ranking 
techniques in order to auto-balance the particular 
importance of each ?? into ???? and ????. 
3.2 Applying ranking algorithm 
To apply a graph-based ranking process, it is 
necessary to assign weights to the vertices of the 
graph. Words involved into ???? and ???? take 
the default value 1/N as their weight to define 
the weight of ?  vector, which is used in our 
proposed ranking algorithm. In the case where 
words are identified on the sentiment repositories 
(see Table 2) as positive or negative, in relation 
to their respective graph, a weight value of 1 (in 
a range [0?1] ) is assigned. ?  represents the 
maximum quantity of words in the current graph. 
Thereafter, a graph-based ranking algorithm is 
applied in order to structurally raise the graph 
vertexes? voting power. Once the reinforcement 
values are applied, the proposed ranking 
algorithm is able to increase the significance of 
the words related to these empowered vertices. 
The PageRank (Brin and Page, 1998) 
adaptation, which was popularized by (Agirre 
and Soroa, 2009) in Word Sense Disambiguation 
thematic, and the one that has obtained relevant 
results, was an inspiration to us in this work. The 
main idea behind this algorithm is that, for each 
edge between ?i and ?j in graph ?, a vote is made 
from ?i to ?j. As a result, the relevance of ?j is 
increased. 
On top of that, the vote strength from ?  to ? 
depends on ????  relevance. The philosophy 
behind it is that, the more important the vertex is, 
the more strength the voter would have. Thus, 
PageRank is generated by applying a random 
walkthrough from the internal interconnection of 
?, where the final relevance of ??  represents the 
random walkthrough probability over ? , and 
ending on ??. 
In our system, we apply the following equation 
and configuration:  
 
??	 = 	????	 +	(1 ? ?)? (1) 
Where: ?	 is a probabilistic transition matrix 
?	?	? , being ??,?  = ???	  if a link from ? i to ? j 
exist, in other case zero is assigned; ? is a vector ?	?	1	with values previously described in this 
section; ?? is the probabilistic structural vector 
obtained after a random walkthrough to arrive to 
any vertex; ?	  is a dumping factor with value 
0.85, and like in (Agirre and Soroa, 2009) we 
used 30 iterations. 
A detailed explanation about the PageRank 
algorithm can be found in (Agirre and Soroa, 
2009). 
After applying PageRank, in order to obtain 
standardized values for both graphs, we 
normalize the rank values by applying the 
following equation: 
 
??? = ???/???(??) (2) 
Where ???(??)  obtains the maximum rank 
value of ?? vector. 
3.3 Adjusting sentiment polarity values 
After applying the PageRank algorithm on ???? 
and ???? , and having normalized their ranks, 
we proceed to obtain a final list of lemmas 
(named ?? ) while avoiding repeated elements. ??	is represented by ???  lemmas, which would 
have, at that time, two assigned values: Positive, 
and Negative, which correspond to a calculated 
rank obtained by the PageRank algorithm.  
At that point, for each lemma from ??,  the 
following equations are applied in order to select 
the definitive subjectivity polarity for each one: 
??? = 	 ???? ? ???	; 	??? > ???0																; ????????? ? (3) 
??? = 	 ???? ? ???	; 	??? > ???0																; ????????? ? (4) 
Where ??? is the Positive value and ??? the 
Negative value related to each lemma in ??. 
In order to standardize the ??? and ??? values 
again and making them more representative in a 
[0?1] scale, we proceed to apply a 
normalization process over the ???  and ??? 
values. 
Following and based on the objective features 
commented by (Baccianella et al, 2010), we 
assume their same premise to establish objective 
values of the lemmas. Equation (5) is used to this 
96
 proceeding, where ???  represent the objective 
value. ??? = 1 ? |??? ? ???| (5) 
4 Sentiment Resource obtained 
At the same time we have obtained a ?? where 
each word is represented by ???, ??? and ??? 
values, acquired automatically from annotated 
sentiment corpora. With our proposal we have 
been able to discover new sentiment words in 
concordance of contexts in which the words 
appear. Note that the new obtained resource 
involves all lemmas identified into the annotated 
corpora. ???, ???, and ??? are nominal values 
between range [0? 	1]. 
5 Evaluation 
In the construction of the sentiment resource we 
used the annotated sentences provided from 
corpora described on Table 1. Note that we only 
used the sentences annotated positively and 
negatively. The resources involved into this table 
were a selection made to prove the functionality 
of the words annotation proposal of subjectivity 
and objectivity. 
The sentiment lexicons used were provided 
from WordNetAffect_Categories 3 and opinion-
words4 files and shown in detail in Table 2. 
Corpus Neg Pos Obj Neu Obj 
or Neu Unknow Total 
computational-
intelligence5 6982 6172 - - - - 13154 
tweeti-b-
sub.dist_out.tsv6 176 368 110 34 - - 688 
b1_tweeti-
objorneu-
b.dist_out.tsv6 
828 1972 788 1114 1045 - 5747 
stno7 1286 660 
 
384 - 10000 12330 
Total 9272 9172 898 1532 1045 10000 31919 
Table 1. Corpora used to apply RA-SR. 
Sources Pos Neg Total 
WordNet-Affects_Categories 
(Strapparava and Valitutti, 2004) 
629 907 1536 
opinion-words (Hu and Liu, 2004; Liu 
et al, 2005) 
2006 4783 6789 
Total 2635 5690 8325 
Table 2. Sentiment Lexicons. 
Some issues were taking into account through 
this process. For example, after obtaining a 
                                                 
3
 http://wndomains.fbk.eu/wnaffect.html 
4
 http://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html 
5
 A sentimental corpus obtained applying techniques 
developed by GPLSI department. See 
(http://gplsi.dlsi.ua.es/gplsi11/allresourcespanel) 
6
 Train dataset of Semeval-2013 (Task 2. Sentiment 
Analysis in Twitter, subtask b.) 
7
 Test dataset of NTCIR Multilingual Opinion Analysis 
Task (MOAT) http://research.nii.ac.jp/ntcir/ntcir-
ws8/meeting/ 
contextual graph ? factotum words are present in 
mostly of the involved sentences (i.e. verb ?to 
be?). This aspect is very dangerous after 
applying PageRank algorithm, because this 
algorithm because this algorithm strengthens the 
nodes possessing many linked elements. For that 
reason, the subtractions ??? ? ???  and ??? ????  are applied, where the most frequently 
words in all contexts obtains high values and 
being the subtraction a damping factor.  
Following an example; when we take the verb 
?to be?, before applying equation (2), verb ?to 
be? archives the highest values into each context 
graph (????  and ???? ), 9.94 and 18.67 rank 
values respectively. These values, applying 
equation (2), are normalized obtaining both ???	 = 	1  and ???	 = 	1  in a range [0...1]. 
Finally, when the next steps are executed 
(Equations (3) and (4)) verb ?to be? 
achieves ???	 = 0 , ??? = 0  and 
therefore 	???	 = 1 . Through this example it 
seems as we subjectively discarded words that 
appear frequently in both contexts (Positive and 
Negative contexts). 
Using the corpora from Table 1 we obtain 
25792 sentimentally annotated lemmas with ???, ??? and ???  features. Of them 12420 positive 
and 11999 negative lemmas were discovered, , 
and 1373 words already derived from existing 
lexical resources. 
Another contribution has been the ??? , ??? 
and ???  scores assigned to words of lexical 
inventory, which were used to reinforce the 
contextual graphs in the building process. Those 
words in concordance to our scenario count 842 
Positives and 383 Negatives.  
5.1 Sentiment Resource Applied on 
Sentiment Analysis 
To know if our method offers resources that 
improve the SA state of the art, we propose a 
baseline supported on the sentiment dictionaries, 
and other method (Ranking Sentiment Resource 
(RSR)) supported over our obtained resource. 
The baseline consists on analyzing sentences 
applying Equation (6) and Equation (7). 
?????????? = ?????????????????	 (6) 
?????????? = ?????????????????	 (7) 
Where: ???????? is the total of positive words 
(aligned with the sentiment dictionaries) in the 
sentence; ????????  is the total of negative 
words (aligned with the sentiment dictionaries) 
97
 in the sentence; ?????????  is the total of 
words in the sentence.  
Using these measures over the analyzed 
sentences, for each sentence, we obtain two 
attributes, ?????????? and	??????????; and 
a third attribute (named Classification) 
corresponding to its classification. 
On the other hand, we propose RSR. This SA 
method uses in a different way the Equation (6) 
and Equation (7), and introduces Equation (8).  
?????????? = ?????????????????	 (8) 
Being ???????? the sum of Positive ranking 
values of the sentence words, aligned with the 
obtained resource (??); ????????  the sum of 
Negative ranking values of the sentence words, 
aligned with the obtained resource (?? ); and ???????  the sum of Objective ranking values 
of the sentence words, aligned with the obtained 
resource (??). 
In RSR method we proved with two approach, 
RSR (1/di) and RSR (1-(1/di)). The first approach 
is based on a resource developed using 
PageRank with  ??,? = 1/??   and the other 
approach is using ??,? = 1 ? (1/??) . Table 3 
shows experimentation results. 
The evaluation has been applied over a corpus 
provided by ?Task 2. Sentiment Analysis in 
Twitter, subtask b?, in particular tweeti-b-
sub.dist_out.tsv file. This corpus contains 597 
annotated phrases, of them Positives (314), 
Negatives (155), Objectives (98) or Neutrals 
(30). For our understanding this quantity of 
instances offers a representative perception of 
RA-SR contribution; however we will think to 
evaluate RA-SR over other corpora in further 
researches. 
 
C I R. Pos (%) 
R. Neg 
(%) 
R. Obj 
(%) 
R. 
Neu 
(%) 
Total 
P. 
(%) 
Total 
R. 
(%) 
Baseline 366 231 91.1 51.6 0.0 0.0 48.2 61.3 
RSR(1/di) 416 181 87.3 39.4 80.6 6.7% 67.8 69.7 
RSR(1-(1/di) 469 128 88.5 70.3 81.6 6.7% 76.8 78.6 
Table 3. Logistic function (Cross-validation 10 folds) 
over tweeti-b-sub.dist_out.tsv8 corpus (597 instances). 
Recall (R), Precision (P), Correct (C), Incorrect (I). 
As we can see the baseline only is able to 
dealing with negative and positive instances. Is 
important to remark that our proposal starting up 
knowing only the words used in baseline and is 
able to growing sentiment information to other 
words related to them. We can see this fact on 
                                                 
8
 Semeval-2013 (Task 2. Sentiment Analysis in Twitter, 
subtask b.) 
Table 3, RSR is able to classify objective 
instances over 80% of Recall and the baseline 
does not.  
Other relevant element is the recall difference 
between RSR (1/di) and RSR (1 ? (1/??) . 
Traditionally (1/??) result value has been 
assigned to ? in PageRank algorithm. We have 
demonstrated that in lexical contexts RSR (1-
(1/di)) approach offers a better performance of 
PageRank algorithm, showing recall differences 
around 10 perceptual points. 
6 Conclusion and further works 
As a conclusion we can say that our proposal is 
able to automatically increase sentiment 
information, obtaining 25792 sentimentally 
annotated lemmas with ??? , ???  and ??? 
features. Of them 12420 positive and 11999 
negative lemmas were discovered. 
In other hand, The RSR is capable to classify 
objective instances over 80% and negatives over 
70%. We cannot tackle efficiently neutral 
instances, perhaps it is due to the lack of neutral 
information in the sentiment resource we used. 
Also, it could be due to the low quantity of 
neutral instances in the evaluated corpus. 
In further research we will evaluate RA-SR 
over different corpora, and we are also going to 
deal with the number of neutral instances. 
The variant RSR (1 ? (1/??)  performs better 
than RSR(1/??) one. This demonstrates that in 
lexical contexts using PageRank with ??,? = 1 ?(1/??) offers a better performance. Other further 
work consists in exploring Social Medias to 
expand our retrieved sentiment resource 
obtaining real time evidences that occur in Web 
2.0. 
Acknowledgments 
This research work has been partially funded by 
the Spanish Government through the project 
TEXT-MESS 2.0 (TIN2009-13391-C04), 
"An?lisis de Tendencias Mediante T?cnicas de 
Opini?n Sem?ntica" (TIN2012-38536-C03-03) 
and ?T?cnicas de Deconstrucci?n en la 
Tecnolog?as del Lenguaje Humano? (TIN2012-
31224); and by the Valencian Government 
through the project PROMETEO 
(PROMETEO/2009/199). 
 
References 
Agirre, E. and A. Soroa. Personalizing PageRank for 
Word Sense Disambiguation. Proceedings of the 
12th conference of the European chapter of the 
98
 Association for Computational Linguistics (EACL-
2009), Athens, Greece, 2009. p.  
Atserias, J.; B. Casas; E. Comelles; M. Gonz?lez; L. 
Padr? and M. Padr?. FreeLing 1.3: Syntactic and 
semantic services in an opensource NLP library. 
Proceedings of LREC'06, Genoa, Italy, 2006. p.  
Baccianella, S.; A. Esuli and F. Sebastiani. 
SENTIWORDNET 3.0: An Enhanced Lexical 
Resource for Sentiment Analysis and Opinion 
Mining. 7th Language Resources and Evaluation 
Conference, Valletta, MALTA., 2010. 2200-2204 
p.  
Balahur, A. Methods and Resources for Sentiment 
Analysis in Multilingual Documents of Different 
Text Types. Department of Software and 
Computing Systems. Alacant, Univeristy of 
Alacant, 2011. 299. p. 
Balahur, A.; E. Boldrini; A. Montoyo and P. 
Martinez-Barco. The OpAL System at NTCIR 8 
MOAT. Proceedings of NTCIR-8 Workshop 
Meeting, Tokyo, Japan., 2010. 241-245 p.  
Balahur, A. and A. Montoyo. Applying a culture 
dependent emotion trigger database for text 
valence and emotion classification. Procesamiento 
del Lenguaje Natural, 2008a. p.  
Balahur, A. and A. Montoyo. Building a 
recommender system using community level social 
filtering. 5th International Workshop on Natural 
Language and Cognitive Science (NLPCS), 2008b. 
32-41 p.  
Brin, S. and L. Page The anatomy of a large-scale 
hypertextual Web search engine Computer 
Networks and ISDN Systems, 1998, 30(1-7): 107-
117. 
Cerini, S.; V. Compagnoni; A. Demontis; M. 
Formentelli and G. Gandini Language resources 
and linguistic theory: Typology, second language 
acquisition, English linguistics (Forthcoming), 
chapter Micro-WNOp: A gold standard for the 
evaluation of automatically compiled lexical 
resources for opinion mining., 2007. 
Cilibrasi, R. L. and P. M. B. Vit?nyi The Google 
Similarity Distance IEEE TRANSACTIONS ON 
KNOWLEDGE AND DATA ENGINEERING, 
2007, VOL. 19, NO 3. 
Esuli, A. and F. Sebastiani. SentiWordNet: A Publicly 
Available Lexical Resource for Opinion Mining. 
Fifth international conference on Languaje 
Resources and Evaluation Genoa - ITaly., 2006. 
417-422 p.  
Hatzivassiloglou; Vasileios and J. Wiebe. Effects of 
Adjective Orientation and Gradability on Sentence 
Subjectivity. International Conference on 
Computational Linguistics (COLING-2000), 2000. 
p.  
Hu, M. and B. Liu. Mining and Summarizing 
Customer Reviews. Proceedings of the ACM 
SIGKDD International Conference on Knowledge 
Discovery and Data Mining (KDD-2004), USA, 
2004. p.  
Kim, S.-M. and E. Hovy. Extracting Opinions, 
Opinion Holders, and Topics Expressed in Online 
News Media Text. In Proceedings of workshop on 
sentiment and subjectivity in text at proceedings of 
the 21st international conference on computational 
linguistics/the 44th annual meeting of the 
association for computational linguistics 
(COLING/ACL 2006), Sydney, Australia, 2006. 1-
8 p.  
Liu, B.; M. Hu and J. Cheng. Opinion Observer: 
Analyzing and Comparing Opinions on the Web. 
Proceedings of the 14th International World Wide 
Web conference (WWW-2005), Japan, 2005. p.  
Miller, G. A.; R. Beckwith; C. Fellbaum; D. Gross 
and K. Miller Introduction to WordNet: An On-line 
Lexical Database International Journal of 
Lexicography, 3(4):235-244., 1990. 
Pang, B.; L. Lee and S. Vaithyanathan. Thumbs up? 
Sentiment Classification using machine learning 
techniquies. EMNLP -02, the Conference on 
Empirical Methods in Natural Language 
Processing, USA, 2002. 79-86 p.  
Popescu, A. M. and O. Etzioni. Extracting product 
features and opinions from reviews. Proccedings of 
HLT-EMNLP, Canada, 2005. p.  
Stone, P.; D. C.Dumphy; M. S. Smith and D. M. 
Ogilvie The General Inquirer: A Computer 
Approach to Content Analysis The MIT Press, 
1996. 
Strapparava, C. and A. Valitutti. WordNet-Affect: an 
affective extension of WordNet. Proceedings of the 
4th International Conference on Language 
Resources and Evaluation (LREC 2004), Lisbon, 
2004. 1083-1086 p.  
Turney, P. D. Thumbs up or thumbs down? Semantic 
orientation applied to unsupervised classification 
of reviews. Proceeding 40th Annual Meeting of the 
Association for Computational Linguistic. ACL 
2002, USA, 2002. 417-424 p.  
Wiebe, J. Tracking point of view in narrative 
Computational Linguistic, 1994, 20(2): 233-287. 
Wiebe, J.; T. Wilson and C. Cardie. Annotating 
Expressions of Opinions and Emotions in 
Language. Kluwer Academic Publishers, 
Netherlands, 2005. p.  
99
