Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 778?788,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Training continuous space language models:
some practical issues
Le Hai Son and Alexandre Allauzen and Guillaume Wisniewski and Franc?ois Yvon
Univ. Paris-Sud, France and LIMSI/CNRS
BP 133, 91403 Orsay Cedex
Firstname.Lastname@limsi.fr
Abstract
Using multi-layer neural networks to esti-
mate the probabilities of word sequences is
a promising research area in statistical lan-
guage modeling, with applications in speech
recognition and statistical machine transla-
tion. However, training such models for large
vocabulary tasks is computationally challeng-
ing which does not scale easily to the huge
corpora that are nowadays available. In this
work, we study the performance and behav-
ior of two neural statistical language models
so as to highlight some important caveats of
the classical training algorithms. The induced
word embeddings for extreme cases are also
analysed, thus providing insight into the con-
vergence issues. A new initialization scheme
and new training techniques are then intro-
duced. These methods are shown to greatly re-
duce the training time and to significantly im-
prove performance, both in terms of perplexity
and on a large-scale translation task.
1 Introduction
Statistical language models play an important role in
many practical applications, such as machine trans-
lation and automatic speech recognition. Let V be
a finite vocabulary, statistical language models de-
fine distributions over sequences of words wL1 in V
?
usually factorized as:
P (wL1 ) = P (w1)
L?
l=1
P (wl|w
l?1
1 )
Modeling the joint distribution of several discrete
random variables (such as words in a sentence) is
difficult, especially in real-world Natural Language
Processing applications where V typically contains
dozens of thousands words.
Many approaches to this problem have been pro-
posed over the last decades, the most widely used
being back-off n-gram language models. n-gram
models rely on a Markovian assumption, and de-
spite this simplification, the maximum likelihood es-
timate (MLE) remains unreliable and tends to under-
estimate the probability of very rare n-grams, which
are hardly observed even in huge corpora. Con-
ventional smoothing techniques, such as Kneser-
Ney and Witten-Bell back-off schemes (see (Chen
and Goodman, 1996) for an empirical overview,
and (Teh, 2006) for a Bayesian interpretation), per-
form back-off on lower order distributions to pro-
vide an estimate for the probability of these unseen
events. n-gram language models rely on a discrete
space representation of the vocabulary, where each
word is associated with a discrete index. In this
model, the morphological, syntactic and semantic
relationships which structure the lexicon are com-
pletely ignored, which negatively impact the gen-
eralization performance of the model. Various ap-
proaches have proposed to overcome this limita-
tion, notably the use of word-classes (Brown et al,
1992; Niesler, 1997), of generalized back-off strate-
gies (Bilmes et al, 1997) or the explicit integration
of morphological information in the random-forest
model (Xu and Jelinek, 2004; Oparin et al, 2008).
One of the most successful alternative to date is to
use distributed word representations (Bengio et al,
2003), where distributionally similar words are rep-
resented as neighbors in a continuous space. This
778
turns n-grams distributions into smooth functions
of the word representations. These representations
and the associated probability estimates are jointly
computed in a multi-layer neural network architec-
ture. This approach has showed significant and
consistent improvements when applied to automatic
speech recognition (Schwenk, 2007; Emami and
Mangu, 2007; Kuo et al, 2010) and machine trans-
lation tasks (Schwenk et al, 2006). Hence, contin-
uous space language models are becoming increas-
ingly used. These successes have revitalized the re-
search on neuronal architectures for language mod-
els, and given rise to several new proposals (see, for
instance, (Mnih and Hinton, 2007; Mnih and Hinton,
2008; Collobert and Weston, 2008)). A major diffi-
culty with these approaches remains the complexity
of training, which does not scale well to the mas-
sive corpora that are nowadays available. Practical
solutions to this problem are discussed in (Schwenk,
2007), which introduces a number of optimization
and tricks to make training doable. Even then, train-
ing a neuronal language model typically takes days.
In this paper, we empirically study the conver-
gence behavior of two multi-layer neural networks
for statistical language modeling, comparing the
standard model of (Bengio et al, 2003) with the log-
bilinear (LBL) model of (Mnih and Hinton, 2007).
Our contributions are the following: we first pro-
pose a reformulation of Mnih and Hinton?s model,
which reveals its similarity with extant models, and
allows a direct and fair comparison with the stan-
dard model. For the standard model, these results
highlight the impact of parameter initialization. We
first investigate a re-initialization method which al-
lows to escape from the local extremum the standard
model converges to. While this method yields a sig-
nificative improvement, the underlying assumption
about the structure of the model does not meet the
requirement of very large-scale tasks. We therefore
introduce a different initialization strategy, called
one vector initialization. Experimental results show
that these novel training strategies drastically reduce
the total training time, while delivering significant
improvements both in terms of perplexity and in a
large-scale translation task.
The rest of this paper is organized as follows. We
first describe, in Section 2, the standard and the LBL
language models. By reformulating the latter, we
show that both models are very similar and empha-
size the remaining differences. Section 2.4 discusses
complexity issues and possible solutions to reduce
the training time. We then report, in Section 3, pre-
liminary experimental results that enlighten some
caveats of the standard approach. Based on these
observations, we introduce in Section 4 novel and
more efficient training schemes, yielding improved
performance and a reduced training time both on
small and large scale experiments.
2 Continuous space language models
Learning a language model amounts to estimate the
parameters of the discrete conditional distribution
over words given each possible history, where the
history corresponds to some function of the preced-
ing words. For an n-gram model, the history con-
tains the n ? 1 preceding words, and the model
parameters correspond to P (wl|w
l?1
l?n+1). Continu-
ous space language models aim at computing these
estimates based on a distributed representation of
words (Bengio et al, 2003), thereby reducing the
sparsity issues that plague conventional maximum
likelihood estimation. In this approach, each word
in the vocabulary is mapped into a real-valued vec-
tor and the conditional probability distributions are
then expressed as a (parameterized) smooth func-
tion of these feature vectors. The formalism of neu-
ral networks allows to express these two steps in a
well-known framework, where, crucially, the map-
ping and the model parameters can be learned in
conjunction. In the next paragraphs, we describe the
two continuous space language models considered
in our study and present the various issues associ-
ated with the training of such models, as well as their
most common remedies.
2.1 The standard model
In the following, we will consider words as indices
in a finite dictionary of size V ; depending on the
context, w will either refer to the word or to its in-
dex in the dictionary. A word w can also be repre-
sented by a 1-of-V coding vector v of RV in which
all elements are null except the wth. In the standard
approach of (Bengio et al, 2003), the feed-forward
network takes as input the n?1 word history and de-
livers an estimate of the probability P (wl|w
l?1
l?n+1)
779
as its output. It consists of three layers.
The first layer builds a continuous representation
of the history by mapping each word into its real-
valued representation. This mapping is defined by
RTv, where R ? RV?m is a projection matrix
and m is the dimension of the continuous projection
word space. The output of this layer is a vector i of
(n ? 1)m real numbers obtained by concatenating
the representations of the context words. The pro-
jection matrix R is shared along all positions in the
history vector and is learned automatically.
The second layer introduces a non-linear trans-
form, where the output layer activation values are
defined by h = tanh (Wihi + bih) , where i is the
input vector, Wih ? RH?(n?1)m and bih ? RH are
the parameters of this layer. The vector h ? RH can
be considered as an higher (more abstract) represen-
tation of the context than i.
The third layer is an output layer that estimates the
desired probability, thanks to the softmax function:
P (wl = k|w
l?1
l?n+1) =
exp(ok)
?
k? exp(ok?)
(1)
o = Whoh + bho, (2)
where Who ? RV?H and bho ? RV are respec-
tively the projection matrix and the bias term associ-
ated with this layer. The wth component in P corre-
sponds to the estimated probability of the wth word
of the vocabulary given the input history vector.
The standard model has two hyper-parameters
(the dimension of projection space m and the size of
hidden layer, H) that define the architecture of the
neural network and a set of free parameters ? that
need to be learned from data: the projection matrix
R, the weight matrix Wih, the bias vector bih, the
weight matrix Who and the bias vector bho.
In this model, the projection matrices R and Who
play similar roles as they define maps between the
vocabulary and the hidden representation. The fact
that R assigns similar representations to history
words w1 and w2 implies that these words can be
exchanged with little impact on the resulting prob-
ability distribution. Likewise, the similarity of two
lines in Who is an indication that the corresponding
words tend to have a similar behavior, i.e. tend to
have a similar probabilities of occurrence in all con-
texts. In the remainder, we will therefore refer to R
as the matrix representing the context space, and to
Who as the matrix for the prediction space.
2.2 The log-bilinear model
The work reported (Mnih and Hinton, 2007) de-
scribes another parameterization of the architecture
introduced in the previous section. This parameter-
ization is based on Factored Restricted Boltzmann
Machine. According to (Mnih and Hinton, 2007),
this model, termed the log-bilinear language model
(LBL), achieves, for large vocabulary tasks, bet-
ter results in terms of perplexity than the standard
model, even if the reasons beyond this improvement
remain unclear.
In this section, we will describe this model and
show how it relates to the standard model. The LBL
model estimates the n-gram parameters by:
P (wl|w
l?1
l?n+1) =
exp(?E(wl;w
l?1
l?n+1))
?
w exp(?E(w;w
l?1
l?n+1))
(3)
In this equation, E is an energy function defined as:
E(wl;w
l?1
1 ) = ?
(
l?1?
k=l?n+1
vk
TRCTk
)
RTvl
(4)
? brTRTvl ? bv
Tvl
= ?vTl R
(
l?1?
k=l?n+1
CkR
Tvk + br
)
? vTl bv (5)
where R is the projection matrix introduced above,
(vk)l?n+1?k?l?1 are the 1-of-V coding vectors for
the history words and vl is the coding vector for wl;
Ck ? Rm?m is a combination matrix and br and bv
denote bias vectors. All these parameters need to be
learned during training.
Equation (4) can be rewritten using the notations
introduced for the standard model. We then rename
br and bv respectively bih and bho. We also denote
i the concatenation of the (n ? 1) vectors RTvk;
likewise Wih denotes the H ? (n? 1)m matrix ob-
tained by concatenating row-wise the (n ? 1) ma-
trices Ck. With these new notations, equations (4)
780
and (3) can be rewritten as:
h = Wihi + bih
o = Rh + bho
P (wl = k|w
l?1
l?n+1) =
exp(ok)
?
k? exp(ok?)
This formulation allows to highlight the similarity of
the LBL model and the standard model. These two
models differ only by the activation function of their
hidden layer (linear for the LBL model and tangent
hyperbolic for the standard model) and by their def-
inition of the prediction space: for the LBL model,
the context space and the prediction space are the
same (R = Who, and thus H = m), while in the
standard model, the prediction space is defined in-
dependently from the context space. This restriction
drastically reduces the number of free parameters of
the LBL model.
It is finally noteworthy to outline the similarity
of this model with standard maximum entropy lan-
guage models (Lau et al, 1993; Rosenfeld, 1996).
Let x denote the binary vector formed by stacking
the (n-1) 1-of-V encodings of the history words;
then the conditional probability distributions esti-
mated in the model are proportional to expF (x),
where F is an affine transform of x. The main dif-
ference with MaxEnt language models are thus the
restricted form of the feature functions, which only
test one history word, and the particular representa-
tion of F , which is defined as:
F (x) = RWihR
?Tv + Rbih + bho
where, as before, R? is formed by concatenating
(n? 1) copies of the projection matrix R.
2.3 Training and inference
Training the two models introduced above can be
achieved by maximizing the log-likelihood L of the
parameters ?. This optimization is usually per-
formed by stochastic back-propagation as in (Ben-
gio et al, 2003). For all our experiments, the learn-
ing rate is fixed at 5?10?3. The learning weight de-
cay and the the weight decay (respectively 1? 10?9
and 0) seem to have a minor impact on the results.
Learning starts with a random initialization of the
parameters under the uniform distribution and con-
verges to a local maximum of the log-likelihood
function. Moreover, to prevent overfitting, an early
stopping strategy is adopted: after each epoch, train-
ing is stopped when the likelihood of a validation set
stops increasing.
2.4 Complexity issues
The main problem with neural language models is
their computational complexity. For the two mod-
els presented in this section, the number of floating
point operations needed to predict the label of a sin-
gle example is1:
((n? 1) ?m + 1)?H + (H + 1)? V (6)
where the first term of the sum corresponds to the
computation of the hidden layer and the second one
to the computation of the output layer. The projec-
tion of the context words amounts to select one row
of the projection matrix R, as the words are repre-
sented with a 1-of-V coding vector. We can there-
fore assume that the computation complexity of the
first layer is negligible. Most of the computation
time is thus spent in the output layer, which implies
that the computing time grows linearly with the vo-
cabulary size. Training these models for large scale
tasks is therefore challenging, and a number of tricks
have been introduced to make training and inference
tractable (Schwenk and Gauvain, 2002; Schwenk,
2007).
Short list A simple method to reduce the com-
plexity in inference and in learning is to reduce
the size of the output vocabulary (Schwenk, 2007):
rather than estimating the probability P (wl =
w|wl?1l?n+1) for all words in the vocabulary, we only
estimate it for the N most frequent words of the
training set (the so-called short-list). In this case,
two vocabularies need to be considered, correspond-
ing respectively to the context vocabulary Vc used to
define the history; and the prediction vocabulary Vp.
However, this method fails to deliver any probability
estimate for words outside of the prediction vocab-
ulary, meaning that a fall-back strategy needs to be
defined for those words. In practice, neural network
1Recall that learning requires to repeatedly predict the label
for all the examples in the training set.
781
language models are combined with a conventional
n-gram model as described in (Schwenk, 2007).
Batch mode and resampling Additional speed-
ups can be obtained by propagating several exam-
ples at once through the network (Bilmes et al,
1997). This ?batch mode? allows to factorize the
matrix operations and cut down both inference and
training time. In all our experiments, we used a
batch size of 64. Moreover, the training time is lin-
ear in the number of examples in the training data2.
Training on very large corpora, which, nowadays,
comprise billions of word tokens, cannot be per-
formed exhaustively and requires to adopt resam-
pling strategies, whereby, at each epoch, the system
is trained with only a small random subset of the
training data. This approach enables to effectively
estimate neural language models on very large cor-
pora; it has also been observed empirically that sam-
pling the training data can increase the generaliza-
tion performance (Schwenk, 2007).
3 A head-to-head comparison
In this section, we analyze a first experimental
study of the two neural network language models
introduced in Section 2 in order to better under-
stand the differences between these models espe-
cially in terms of the word representations they in-
duce. Based on this study, we will propose, in the
next section, improvements of both the speed and
the prediction capacity of the models. In all our ex-
periments, 4-gram language models are used.
3.1 Corpus
The data we use for training is a large monolingual
corpus, containing all the English texts in the par-
allel data of the Arabic to English NIST 2009 con-
strained task3. It consists of 176 millions word to-
kens with 532, 557 different word types as the size
of vocabulary. The perplexity is computed with re-
spect to the 2006 NIST test data, which is used here
as our development data.
2Equation (6) gives the complexity of inference for a single
example.
3http://www.itl.nist.gov/iad/mig/tests/
mt/2009/MT09_ConstrainedResources.pdf
3.2 Convergence study
In a first experiment, we trained the two models in
the same setting: we choose to consider a small
vocabulary comprising the 10, 000 most frequent
words. The same vocabulary is used to constrain
the words occurring in the history and the words
to be predicted. The size of hidden layer is set to
m = H = 200, the history contains the 3 preceding
words, we use a batch size of 64, a resampling rate
of 5% and no weight decay.
Figure 1 displays the perplexity convergence
curve measured on the development data for the
standard and the LBL models4. The convergence
perplexities after the combination with the standard
back-off model are also provided for all the mod-
els in table 2 (see section 4.3). We can observe
that the LBL model converges faster than the stan-
dard model: the latter needs 13 epochs to reach
the stopping criteria, while the former only needs
6 epochs. However, upon convergence, the stan-
dard model reaches a lower perplexity than the LBL
model.
0 2 4 6 8 10 12 14120
130
140
150
160
170
180
epochs
per
plex
ity
Perplexity
standardlog bilinear
Figure 1: Convergence rate of the standard and the LBL
models evaluated by the evolution of the perplexity on a
development set
As described in Section 2.2, the main difference
between the standard and the LBL model is the way
the context and the prediction spaces are defined: in
the standard model, the two spaces are distinct; in
4The use of a back-off 4-model estimated with the modified
Knesser-Ney smoothing on the same training data achieves a
perplexity of 141 on the development data.
782
the LBL model, they are bound to be the same. With
a smaller number of parameters, the LBL model can
not capture as many characteristics of the data as the
standard model, but it converges faster5. This differ-
ence in convergence can be explained by the scarcity
of the updates in the projection matrix R in the
standard model: during backpropagation, only those
weights that are associated with words in the history
are updated. By contrast, each training sample up-
dates all the weights in the prediction matrix Who.
3.3 An analysis of the continuous word space
To deepen our understanding, we propose to further
analyze the induced word embeddings by finding,
for some randomly selected words, the five nearest
neighbors (according to the Euclidian distance) in
the context space and in the prediction space of the
two models. Results are presented in Table 1.
If we look first at the standard model, the global
picture is that for frequent words (is, are, and, to
a lesser extend, have), both spaces seem to define
meaningful neighborhood, corresponding to seman-
tic and syntactic similarities; this is less true for rarer
words, where we see a greater discrepancy between
the context and prediction spaces. For instance, the
date 1947 seems to be randomly associated in the
context space, while the 5 nearest words in the pre-
diction space form a consistent set of dates. The
same trend is also observed for the word Castro. Our
interpretation is that for less frequent words, the pro-
jection vectors are hardly ever updated and remain
close to their original random initialization.
By contrast, the similarities in the (unique) pro-
jection space of the LBL remain consistent for all
frequency ranges, and are very similar to the predic-
tion space of the standard model. This seems to val-
idate our hypothesis that in the standard model, the
prediction space is learned much faster than the con-
text space and corroborates our interpretation of the
impact of the scarce updates of rare words. Another
possible explanation is that there is no clear relation
5We could increase the number of parameters of the LBL
model for a fairer comparison with the standard model. How-
ever, this would also increase the size of the vocabulary and
cause two new issues: on one hand, the time complexity would
drastically increase for the LBL model, and on the other hand,
both models would not be comparable in terms of perplexity as
their vocabulary would be different.
between the context space and the target function:
the context space is learned only indirectly by back-
propagation. As a result, due to the random initial-
ization of the parameters and to data sparsity, many
vectors of R might be blocked in some local max-
ima, meaning that similar vectors cannot be grouped
in a consistent way and that the induced similarity is
more ?loose?.
4 Improving the standard model
In Section 3.2, we observed that slightly better re-
sults can be obtained with the standard rather than
with the LBL model. The latter is however much
faster to train, and seems to induce better projection
matrices. Both effects can be attributed to the partic-
ular parameterization of this model, which uses the
same projection matrix both for the context and for
the prediction spaces. In this section, we propose
several new learning regimes that allowed us to im-
prove the standard model in terms of both speed and
prediction capacity. All these improvements rely on
the idea of sharing word representations. While this
idea is not new (see for instance (Collobert and We-
ston, 2008)), our analysis enables to better under-
stand its impact on the convergence rate. Finally, the
improvements we propose are evaluated on a real-
word machine translation task.
4.1 Improving performances with
re-initialization
The experiments reported in the previous section
suggest that it is possible to improve the perfor-
mances of the standard model by building a better
context space. Thus, we introduce a new learning
regime, called re-initialization which aims to im-
prove the context space by re-injecting the informa-
tion on word neighborhoods that emerges in the pre-
diction space. One possible implementation of this
idea is as follows:
1. train a standard model until convergence;
2. use the prediction space of this model to ini-
tialize the context space of a new model; the
prediction space is chosen randomly;
3. train this new model.
783
Table 1: The 5 closest words in the representation spaces of the standard and LBL language models.
word (frequency) model space 5 most closest words
is standard context was are were be been
900, 350 standard prediction was has would had will
LBL both was reveals proves are ON
are standard context were is was be been
478, 440 standard prediction were could will have can
LBL both were is was FOR ON
have standard context had has of also the
465, 417 standard prediction are were provide remain will
LBL both had has Have were embrace
meeting standard context meetings conference them 10 talks
150, 317 standard prediction undertaking seminar meetings gathering project
LBL both meetings summit gathering festival hearing
Imam standard context PCN rebellion 116. Cuba 49
787 standard prediction Castro Sen Nacional Al- Ross
LBL both Salah Khaled Al- Muhammad Khalid
1947 standard context 36 Mercosur definite 2002-2003 era
774 standard prediction 1965 1945 1968 1964 1975
LBL both 1965 1976 1964 1968 1975
Castro standard context exclusively 12. Boucher Zeng Kelly
768 standard prediction Singh Clark da Obasanjo Ross
LBL both Clark Singh Sabri Rafsanjani Sen
Figure 2: Evolution of the perplexity on a development
set for various initialization regimes.
The evolution of the perplexity with respect to train-
ing epochs for this new method is plotted on Fig-
ure 2, where we only represent the evolution of the
perplexity during the third training step. As can be
seen, at convergence, the perplexity the model esti-
mated with this technique is about 10% smaller than
the perplexity of the standard model.
This result can be explained by considering the re-
initialization as a form of annealing technique: re-
initializing the context space allows to escape from
the local extrema the standard model converges to.
The fact that the prediction space provides a good
initialization of the context space also confirms our
analysis that one difficulty with the standard model
is the estimation of the context space parameters.
4.2 Iterative re-initialization
The re-initialization policy introduced in the previ-
ous section significantly reduces the perplexity, at
the expense of a longer training time, as it requires
to successively train two models. As we now know
that the parameters of the prediction space are faster
to converge, we introduce a second training regime
called iterative re-initialization which aims to take
advantage of this property. We summarize this new
training regime as follows:
1. Train the model for one epoch.
2. Use the prediction space parameters to reini-
tialize the context space.
3. Iterate steps (1) and (2) until convergence.
784
Figure 3: Evolution of the perplexity on the training data
for various initialization regimes.
This regimes yields a model that is somewhat in-
between the standard and LBL models as it adds a
relationship between the two representation spaces,
which lacks in the former model. This relationship is
however not expressed through the tying of the cor-
responding parameters; instead we let the prediction
space guide the convergence of the context space.
As a consequence, we hope that it can achieve a con-
vergence speed as fast as the one of the LBL model
without degrading its prediction capacity.
The result plotted on Figure 2 shows that this in-
deed the case: using this training regime, we ob-
tained a perplexity similar to the one of the stan-
dard model, while at the same time reducing the
total training time by more than a half, which is
of great practical interest (each epoch lasts approxi-
mately 8 hours on a 3GHz Xeon processor).
Figure 3 displays the perplexity convergence
curve measured on the training data for the standard
learning regime as well as for the re-initialization
and iterative re-initialization. These results show
the same trend as for the perplexity measured on
the development data, and suggest a regularization
effect of the re-initialization schemes rather than al-
lowing the models to escape local optima.
4.3 One vector initialization
Principle The new training regimes introduced
above outperform the standard training regime both
in terms of perplexity and of training time. However,
exchanging information between the context and
prediction spaces is only possible when the same
vocabulary is used in both spaces. As discussed
in Section 2.4, this configuration is not realistic for
very large-scale tasks. This is because increasing the
number of predicted word types is much more com-
putationally demanding than increasing the number
of types in the context vocabulary. Thus, the former
vocabulary is typically order of magnitudes larger
than the latter, which means that the re-initialization
strategies can no longer be directly used.
It is nonetheless possible to continue drawing in-
spirations from the observations made in Section 3,
and, crucially, to question the random initialization
strategy. As discussed above, this strategy may ex-
plain why the neighborhoods in the induced con-
text space for the less frequent types were diffi-
cult to interpret. As a straightforward alternative,
we consider a different initialization strategy where
all the words in the context vocabulary are initially
projected onto the same (random) point in the con-
text space. The intuition is that it will be easier to
build meaningful neighborhoods, especially for rare
types, if all words are initially considered similar
and only diverge if there is sufficient evidence in the
training data to suggest that they should. This model
is termed the one vector initialization model.
Experimental evaluation To validate this ap-
proach, we compare the convergence of a standard
model trained (with the standard learning regime)
with the one vector initialization regime. The con-
text vocabulary is defined by the 532, 557 words oc-
curring in the training data and the prediction vo-
cabulary by the 10, 000 most frequent words6. All
other parameters are the same as in the previous
experiments. Based on the curves displayed on
Figure 4, we can observe that the model obtained
with the one vector initialization regime outperforms
the model trained with a completely random ini-
tialization. Moreover, the latter reaches conver-
gence in only 14 epochs, while the learning regime
we propose only needs 9 epochs. Convergence is
even faster than when we used the standard training
regime and a small context vocabulary.
6In this case, the distinction between the context and the pre-
diction vocabulary rules out the possibility of a relevant compar-
ison based on perplexity between the continuous space language
model and a standard back-off language model.
785
0 5 10 15100
110
120
130
140
150
160
170
180
epochs
perp
lexit
y
Perplexity
standardone vector initialization
Figure 4: Perplexity with all-10, 000, 200? 200 models
Table 2: Summary of the perplexity (PPX) results mea-
sured on the same development set with the different con-
tinuous space language models. For all of them, the prob-
abilities are combined with the back-off n-gram model
Vc size Model # epochs PPX
10000 log bilinear 6 239
standard 13 227
iterative reinit. 6 223
reinit. 11 211
all standard 14 276
one vector init. 9 260
To illustrate the impact of our initialization
scheme, we also used a principal component anal-
ysis to represent the induced word representations
in a two dimensional space. Figure 5 represents the
vectors associated with numbers7 in red, while all
other words are represented in blue. Two different
models are used: the standard model on the left, and
the one vector initialization model on the right. We
can observe that, for the standard model, most of
the red points are scattered all over a large portion
of the representation space. On the opposite, for
the one vector initialization model, points associated
with numbers are much more concentrated: this is
simply because all the points are originally identi-
cal, and the training aim to spread the point around
this starting point. We also created the closest word
list reported in Table 3, in a manner similar to Ta-
ble 1. Clearly, the new method seems to yield more
7Number are all the words consisting only of digits, with an
optional sign, point or comma such as: 1947; 0,001; -8,2.
(a) with the standard model (b) with the one vector initial-
ization model
Figure 5: Comparison of the word embedding in the con-
text space for numbers (red points).
meaningful neighborhoods in the context space.
It is finally noteworthy to mention that when used
with a small context vocabulary (as in the experi-
mental setting of Section 4.1) this initialization strat-
egy underperforms the standard initialization. This
is simply due to the much greater data sparsity in
the large context vocabulary experiments, where the
rarer word types are really rare (they typically occur
once or twice). By contrast, the rarer words in the
small vocabulary tasks occurred more than several
hundreds times in the training corpus, which was
more than sufficient to guide the model towards sat-
isfactory projection matrices. This finally suggests
that there still exists room for improvement if we
can find more efficient initialization strategies than
starting from one or several random points.
4.4 Statistical machine translation experiments
As a last experiment, we compare the various mod-
els on a large scale machine translation task. Sta-
tistical language models are key component of cur-
rent statistical machine translation systems (Koehn,
2010), where they both help disambiguate lexical
choices in the target language and influence the
choice of the right word ordering. The integration of
a neural network language model in such a system is
far from easy, given the computational cost of com-
puting word probabilities, a task that is performed
repeatedly during the search of the best translation.
We then had to resort to a two pass decoding ap-
proach: the first pass uses a conventional back-off
language model to produce a n-best list (the n most
likely translations and their associated scores); in the
second pass, the probability of the neural language
model is computed for each hypothesis and the n-
786
Table 3: The 5 closest words in the context space of the standard and one vector initialization language models
word (freq.) model 5 closest words
is standard was are were been remains
900, 350 1 vector init. was are be were been
conducted standard undertaken launched $270,900 Mufamadi 6.44-km-long
18, 388 1 vector init. pursued conducts commissioned initiated executed
Cambodian standard Shyorongi $3,192,700 Zairian depreciations teachers
2, 381 1 vector init. Danish Latvian Estonian Belarussian Bangladeshi
automatically standard MSSD Sarvodaya $676,603,059 Kissana 2,652,627
1, 528 1 vector init. routinely occasionally invariably inadvertently seldom
Tosevski standard $12.3 Action,3 Kassouma 3536 Applique
34 1 vector init. Shafei Garvalov Dostiev Bourloyannis-Vrailas Grandi
October-12 standard 39,572 anti-Hutu $12,852,200 non-contracting Party?s
8 1 vector init. March-26 April-11 October-1 June-30 August4
3727th standard Raqu Tatsei Ayatallah Mesyats Langlois
1 1 vector init. 4160th 3651st 3487th 3378th 3558th
best list is accordingly reordered to produce the final
translations.
The different language models discussed in this
article are evaluated on the Arabic to English
NIST 2009 constrained task. For the continuous
space language model, the training data consists
in the parallel corpus used to train the translation
model (previously described in section 3.1). The de-
velopment data is again the 2006 NIST test set and
the test data is the official 2008 NIST test set. Our
system is built using the open-source Moses toolkit
(Koehn et al, 2007) with default settings. To set
up our baseline results, we used an extensively op-
timized standard back-off 4-grams language model
using Kneser-Ney smoothing described in (Allauzen
et al, 2009). The weights used during the reranking
are tuned using the Minimum Error Rate Training
algorithm (Och, 2003). Performance is measured
based on the BLEU (Papineni et al, 2002) scores,
which are reported in Table 4.
Table 4: BLEU scores on the NIST MT08 test set with
different language models.
Vc size Model # epochs BLEU
all baseline - 37.8
10000 log bilinear 6 38.2
standard 13 38.3
iterative reinit. 6 38.4
reinit. 11 38.4
all standard 14 38.6
one vector init. 9 38.7
All the experimented neural language models
yield to a significant BLEU increase. The best re-
sult is obtained by the one vector initialization stan-
dard model which achieves a 0.9 BLEU improve-
ment. While this results is similar to the one ob-
tained with the standard model, the training time is
reduced here by a third.
5 Conclusion
In this work, we proposed three new methods
for training neural network language models and
showed their efficiency both in terms of computa-
tional complexity and generalization performance in
a real-word machine translation task. These meth-
ods rely on conclusions drawn from a careful study
of the convergence rate of two state-of-the-art mod-
els and are based on the idea of sharing the dis-
tributed word representations during training.
Our work highlights the impact of the initializa-
tion and the training scheme for neural network lan-
guage models. Both our experimental results and
our new training methods can be closely related to
the pre-training techniques introduced by (Hinton
and Salakhutdinov, 2006). Our future work will thus
aim at studying the connections between our empir-
ical observations and the deep learning framework.
Acknowledgments
This work was partly realized as part of the Quaero
Program, funded by OSEO, the French agency for
innovation.
787
References
Alexandre Allauzen, Josep Crego, Aure?lien Max, and
Franc?ois Yvon. 2009. LIMSI?s statistical transla-
tion systems for WMT?09. In Proceedings of the
Fourth Workshop on Statistical Machine Translation,
pages 100?104, Athens, Greece, March. Association
for Computational Linguistics.
Yoshua Bengio, Re?jean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. JMLR, 3:1137?1155.
J. Bilmes, K. Asanovic, C. Chin, and J. Demmel. 1997.
Using phipac to speed error back-propagation learn-
ing. Acoustics, Speech, and Signal Processing, IEEE
International Conference on, 5:4153.
Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vin-
cent J. Della Pietra, and Jenifer C. Lai. 1992. Class-
based n-gram models of natural language. Comput.
Linguist., 18(4):467?479.
Stanley F. Chen and Joshua Goodman. 1996. An empiri-
cal study of smoothing techniques for language model-
ing. In Proc. ACL?96, pages 310?318, San Francisco.
Ronan Collobert and Jason Weston. 2008. A uni-
fied architecture for natural language processing: deep
neural networks with multitask learning. In Proc.
of ICML?08, pages 160?167, New York, NY, USA.
ACM.
Ahmed Emami and Lidia Mangu. 2007. Empirical study
of neural network language models for Arabic speech
recognition. In Proc. ASRU?07, pages 147?152, Ky-
oto. IEEE.
Geoffrey E. Hinton and Ruslan R. Salakhutdinov. 2006.
Reducing the dimensionality of data with neural net-
works. Science, 313(5786):504?507, July.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proc.
ACL?07, pages 177?180, Prague, Czech Republic.
Philipp Koehn. 2010. Statistical Machine Translation.
Cambridge University Press.
Hong-Kwang Kuo, Lidia Mangu, Ahmad Emami, and
Imed Zitouni. 2010. Morphological and syntactic fea-
tures for arabic speech recognition. In Proc. ICASSP
2010.
Raymond Lau, Ronald Rosenfeld, and Salim Roukos.
1993. Adaptive language modeling using the maxi-
mum entropy principle. In Proc HLT?93, pages 108?
113, Princeton, New Jersey.
Andriy Mnih and Geoffrey Hinton. 2007. Three new
graphical models for statistical language modelling. In
Proc. ICML ?07, pages 641?648, New York, NY, USA.
Andriy Mnih and Geoffrey E Hinton. 2008. A scalable
hierarchical distributed language model. In D. Koller,
D. Schuurmans, Y. Bengio, and L. Bottou, editors, Ad-
vances in Neural Information Processing Systems 21,
volume 21, pages 1081?1088.
Thomas R. Niesler. 1997. Category-based statistical
language models. Ph.D. thesis, University of Cam-
bridge.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proc. ACL?03, pages
160?167, Sapporo, Japan.
Ilya Oparin, Ondr?ej Glembek, Luka?s? Burget, and Jan
C?ernocky?. 2008. Morphological random forests for
language modeling of inflectional languages. In Proc.
SLT?08, pages 189?192.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic evalu-
ation of machine translation. In Proc. ACL?02, pages
311?318, Philadelphia.
Ronald Rosenfeld. 1996. A maximum entropy approach
to adaptive statistical language modeling. Computer,
Speech and Language, 10:187?228.
Holger Schwenk and Jean-Luc Gauvain. 2002. Connec-
tionist language modeling for large vocabulary contin-
uous speech recognition. In Proc. ICASSP, pages 765?
768, Orlando, FL.
Holger Schwenk, Daniel De?chelotte, and Jean-Luc Gau-
vain. 2006. Continuous space language models
for statistical machine translation. In Proc. COL-
ING/ACL?06, pages 723?730.
Holger Schwenk. 2007. Continuous space language
models. Comput. Speech Lang., 21(3):492?518.
Yeh W. Teh. 2006. A hierarchical Bayesian language
model based on Pitman-Yor processes. In Proc. of
ACL?06, pages 985?992, Sidney, Australia.
Peng Xu and Frederik Jelinek. 2004. Random forests in
language modeling. In Proceedings of EMNLP?2004,
pages 325?332, Barcelona, Spain.
788
2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 39?48,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Continuous Space Translation Models with Neural Networks
Le Hai Son and Alexandre Allauzen and Franc?ois Yvon
Univ. Paris-Sud, France and LIMSI/CNRS
rue John von Neumann, 91403 Orsay cedex, France
Firstname.Lastname@limsi.fr
Abstract
The use of conventional maximum likelihood
estimates hinders the performance of existing
phrase-based translation models. For lack of
sufficient training data, most models only con-
sider a small amount of context. As a par-
tial remedy, we explore here several contin-
uous space translation models, where transla-
tion probabilities are estimated using a con-
tinuous representation of translation units in
lieu of standard discrete representations. In
order to handle a large set of translation units,
these representations and the associated esti-
mates are jointly computed using a multi-layer
neural network with a SOUL architecture. In
small scale and large scale English to French
experiments, we show that the resulting mod-
els can effectively be trained and used on top
of a n-gram translation system, delivering sig-
nificant improvements in performance.
1 Introduction
The phrase-based approach to statistical machine
translation (SMT) is based on the following infer-
ence rule, which, given a source sentence s, selects
the target sentence t and the underlying alignment a
maximizing the following term:
P (t,a|s) =
1
Z(s)
exp
( K?
k=1
?kfk(s, t,a)
)
, (1)
where K feature functions (fk) are weighted by a
set of coefficients (?k), and Z is a normalizing fac-
tor. The phrase-based approach differs from other
approaches by the hidden variables of the translation
process: the segmentation of a parallel sentence pair
into phrase pairs and the associated phrase align-
ments.
This formulation was introduced in (Zens et al,
2002) as an extension of the word based mod-
els (Brown et al, 1993), then later motivated within
a discriminative framework (Och and Ney, 2004).
One motivation for integrating more feature func-
tions was to improve the estimation of the translation
model P (t|s), which was initially based on relative
frequencies, thus yielding poor estimates.
This is because the units of phrase-based mod-
els are phrase pairs, made of a source and a tar-
get phrase; such units are viewed as the events of
discrete random variables. The resulting representa-
tions of phrases (or words) thus entirely ignore the
morphological, syntactic and semantic relationships
that exist among those units in both languages. This
lack of structure hinders the generalization power of
the model and reduces its ability to adapt to other
domains. Another consequence is that phrase-based
models usually consider a very restricted context1.
This is a general issue in statistical Natural Lan-
guage Processing (NLP) and many possible reme-
dies have been proposed in the literature, such as,
for instance, using smoothing techniques (Chen and
Goodman, 1996), or working with linguistically en-
riched, or more abstract, representations. In statisti-
cal language modeling, another line of research con-
siders numerical representations, trained automat-
ically through the use of neural network (see eg.
1typically a small number of preceding phrase pairs for the
n-gram based approach (Crego and Marin?o, 2006), or no con-
text at all, for the standard approach of (Koehn et al, 2007).
39
(Collobert et al, 2011)). An influential proposal,
in this respect, is the work of (Bengio et al, 2003)
on continuous space language models. In this ap-
proach, n-gram probabilities are estimated using a
continuous representation of words in lieu of stan-
dard discrete representations. Experimental results,
reported for instance in (Schwenk, 2007) show sig-
nificant improvements in speech recognition appli-
cations. Recently, this model has been extended in
several promising ways (Mikolov et al, 2011; Kuo
et al, 2010; Liu et al, 2011). In the context of SMT,
Schwenk et al (2007) is the first attempt to esti-
mate translation probabilities in a continuous space.
However, because of the proposed neural architec-
ture, the authors only consider a very restricted set
of translation units, and therefore report only a slight
impact on translation performance. The recent pro-
posal of (Le et al, 2011a) seems especially relevant,
as it is able, through the use of class-based models,
to handle arbitrarily large vocabularies and opens the
way to enhanced neural translation models.
In this paper, we explore various neural architec-
tures for translation models and consider three dif-
ferent ways to factor the joint probability P (s, t)
differing by the units (respectively phrase pairs,
phrases or words) that are projected in continuous
spaces. While these decompositions are theoreti-
cally straightforward, they were not considered in
the past because of data sparsity issues and of the
resulting weaknesses of conventional maximum like-
lihood estimates. Our main contribution is then to
show that such joint distributions can be efficiently
computed by neural networks, even for very large
context sizes; and that their use yields significant
performance improvements. These models are eval-
uated in a n-best rescoring step using the framework
of n-gram based systems, within which they inte-
grate easily. Note, however that they could be used
with any phrase-based system.
The rest of this paper is organized as follows. We
first recollect, in Section 2, the n-gram based ap-
proach, and discuss various implementations of this
framework. We then describe, in Section 3, the neu-
ral architecture developed and explain how it can be
made to handle large vocabulary tasks as well as lan-
guage models over bilingual units. We finally re-
port, in Section 4, experimental results obtained on
a large-scale English to French translation task.
2 Variations on the n-gram approach
Even though n-gram translation models can be
integrated within standard phrase-based systems
(Niehues et al, 2011), the n-gram based frame-
work provides a more convenient way to introduce
our work and has also been used to build the base-
line systems used in our experiments. In the n-
gram based approach (Casacuberta and Vidal, 2004;
Marin?o et al, 2006; Crego and Marin?o, 2006), trans-
lation is divided in two steps: a source reordering
step and a translation step. Source reordering is
based on a set of learned rewrite rules that non-
deterministically reorder the input words so as to
match the target order thereby generating a lattice
of possible reorderings. Translation then amounts
to finding the most likely path in this lattice using a
n-gram translation model 2 of bilingual units.
2.1 The standard n-gram translation model
n-gram translation models (TMs) rely on a spe-
cific decomposition of the joint probability P (s, t),
where s is a sequence of I reordered source words
(s1, ..., sI ) and t contains J target words (t1, ..., tJ ).
This sentence pair is further assumed to be de-
composed into a sequence of L bilingual units
called tuples defining a joint segmentation: (s, t) =
u1, ..., uL. In the approach of (Marin?o et al, 2006),
this segmentation is a by-product of source reorder-
ing, and ultimately derives from initial word and
phrase alignments. In this framework, the basic
translation units are tuples, which are the analogous
of phrase pairs, and represent a matching u = (s, t)
between a source s and a target t phrase (see Fig-
ure 1). Using the n-gram assumption, the joint prob-
ability of a segmented sentence pair decomposes as:
P (s, t) =
L?
i=1
P (ui|ui?1, ..., ui?n+1) (2)
A first issue with this model is that the elementary
units are bilingual pairs, which means that the under-
lying vocabulary, hence the number of parameters,
can be quite large, even for small translation tasks.
Due to data sparsity issues, such models are bound
2Like in the standard phrase-based approach, the translation
process also involves additional feature functions that are pre-
sented below.
40
to face severe estimation problems. Another prob-
lem with (2) is that the source and target sides play
symmetric roles, whereas the source side is known,
and the target side must be predicted.
2.2 A factored n-gram translation model
To overcome some of these issues, the n-gram prob-
ability in equation (2) can be factored by decompos-
ing tuples in two (source and target) parts :
P (ui|ui?1, ..., ui?n+1) =
P (ti|si, si?1, ti?1, ..., si?n+1, ti?n+1)
? P (si|si?1, ti?1..., si?n+1, ti?n+1)
(3)
Decomposition (3) involves two models: the first
term represents a TM, the second term is best viewed
as a reordering model. In this formulation, the TM
only predicts the target phrase, given its source and
target contexts.
Another benefit of this formulation is that the el-
ementary events now correspond either to source or
to target phrases, but never to pairs of such phrases.
The underlying vocabulary is thus obtained as the
union, rather than the cross product, of phrase in-
ventories. Finally note that the n-gram probability
P (ui|ui?1, ..., ui?n+1) could also factor as:
P (si|ti, si?1, ti?1, ..., si?n+1, ti?n+1)
? P (ti|si?1, ti?1, ..., si?n+1, ti?n+1)
(4)
2.3 A word factored translation model
A more radical way to address the data sparsity is-
sues is to take (source and target) words as the basic
units of the n-gram TM. This may seem to be a step
backwards, since the transition from word (Brown et
al., 1993) to phrase-based models (Zens et al, 2002)
is considered as one of the main recent improvement
in MT. One important motivation for considering
phrases rather than words was to capture local con-
text in translation and reordering. It should then be
stressed that the decomposition of phrases in words
is only re-introduced here as a way to mitigate the
parameter estimation problems. Translation units
are still pairs of phrases, derived from a bilingual
segmentation in tuples synchronizing the source and
target n-gram streams, as defined by equation (3).
In fact, the estimation policy described in section 3
will actually allow us to design n-gram models with
longer contexts than is typically possible in the con-
ventional n-gram approach.
Let ski denote the k
th word of source tuple si.
Considering again the example of Figure 1, s111 is
to the source word nobel, s411 is to the source word
paix, and similarly t211 is the target word peace. We
finally denote hn?1(tki ) the sequence made of the
n ? 1 words preceding tki in the target sentence: in
Figure 1, h3(t211) thus refers to the three word con-
text receive the nobel associated with the target word
peace. Using these notations, equation (3) is rewrit-
ten as:
P (s, t) =
L?
i=1
[ |ti|?
k=1
P
(
tki |h
n?1(tki ), h
n?1(s1i+1)
)
?
|si|?
k=1
P
(
ski |h
n?1(t1i ), h
n?1(ski )
)] (5)
This decomposition relies on the n-gram assump-
tion, this time at the word level. Therefore, this
model estimates the joint probability of a sentence
pair using two sliding windows of length n, one for
each language; however, the moves of these win-
dows remain synchronized by the tuple segmenta-
tion. Moreover, the context is not limited to the cur-
rent phrase, and continues to include words in ad-
jacent phrases. Using the example of Figure 1, the
contribution of the target phrase t11 = nobel, peace
to P (s, t) using a 3- gram model is
P
(
nobel|[receive, the], [la, paix]
)
?P
(
peace|[the, nobel], [la, paix]
)
.
Likewise, the contribution of the source phrase
s11 =nobel, de, la, paix is:
P
(
nobel|[receive, the], [recevoir,le]
)
? P
(
de|[receive, the], [le,nobel]
)
? P
(
la|[receive, the], [nobel, de]
)
? P
(
paix|[receive, the], [de,la]
)
.
A benefit of this new formulation is that the involved
vocabularies only contain words, and are thus much
smaller. These models are thus less bound to be af-
fected by data sparsity issues. While the TM defined
by equation (5) derives from equation (3), a variation
can be equivalently derived from equation (4).
41
 s?
8
: ? 
 t
?
8
: to 
 s?
9
: recevoir 
 t
?
9
: receive 
 s?
10
: le 
 t
?
10
: the 
 s?
11
: nobel de la paix 
 t
?
11
: nobel peace 
 s?
12
: prix 
 t
?
12
: prize 
 u
8
  u
9
  u
10
  u
11
  u
12
 
S :   .... 
T :   .... 
? recevoir le prix nobel de la paixorg :   ....
....
....
Figure 1: Extract of a French-English sentence pair segmented in bilingual units. The original (org) French sentence
appears at the top of the figure, just above the reordered source s and target t. The pair (s, t) decomposes into a
sequence of L bilingual units (tuples) u1, ..., uL. Each tuple ui contains a source and a target phrase: si and ti.
3 The SOUL model
In the previous section, we defined three different
n-gram translation models, based respectively on
equations (2), (3) and (5). As discussed above, a
major issue with such models is to reliably estimate
their parameters, the numbers of which grow expo-
nentially with the order of the model. This problem
is aggravated in natural language processing, due to
well known data sparsity issues. In this work, we
take advantage of the recent proposal of (Le et al,
2011a): using a specific neural network architecture
(the Structured OUtput Layer model), it becomes
possible to handle large vocabulary language mod-
eling tasks, a solution that we adapt here to MT.
3.1 Language modeling in a continuous space
Let V be a finite vocabulary, n-gram language mod-
els (LMs) define distributions over finite sequences
of tokens (typically words) wL1 in V
+ as follows:
P (wL1 ) =
L?
i=1
P (wi|w
i?1
i?n+1) (6)
Modeling the joint distribution of several discrete
random variables (such as words in a sentence) is
difficult, especially in NLP applications where V
typically contains dozens of thousands words.
In spite of the simplifying n-gram assump-
tion, maximum likelihood estimation remains un-
reliable and tends to underestimate the proba-
bility of very rare n-grams. Smoothing tech-
niques, such as Kneser-Ney and Witten-Bell back-
off schemes (see (Chen and Goodman, 1996) for an
empirical overview, and (Teh, 2006) for a Bayesian
interpretation), perform back-off to lower order dis-
tributions, thus providing an estimate for the proba-
bility of these unseen events.
One of the most successful alternative to date is to
use distributed word representations (Bengio et al,
2003), where distributionally similar words are rep-
resented as neighbors in a continuous space. This
turns n-grams distributions into smooth functions
of the word representations. These representations
and the associated estimates are jointly computed
in a multi-layer neural network architecture. Fig-
ure 2 provides a partial representation of this kind
of model and helps figuring out their principles. To
compute the probability P (wi|w
i?1
i?n+1), the n ? 1
context words are projected in the same continu-
ous space using a shared matrix R; these continuous
word representations are then concatenated to build
a single vector that represents the context; after a
non-linear transformation, the probability distribu-
tion is computed using a softmax layer.
The major difficulty with the neural network ap-
proach remains the complexity of inference and
training, which largely depends on the size of the
output vocabulary (i.e. the number of words that
have to be predicted). One practical solution is to re-
strict the output vocabulary to a short-list composed
of the most frequent words (Schwenk, 2007). How-
ever, the usual size of the short-list is under 20k,
which does not seem sufficient to faithfully repre-
sent the translation models of section 2.
3.2 Principles of SOUL
To circumvent this problem, Structured Output
Layer (SOUL) LMs are introduced in (Le et al,
2011a). Following Mnih and Hinton (2008), the
SOULmodel combines the neural network approach
with a class-based LM (Brown et al, 1992). Struc-
42
turing the output layer and using word class informa-
tion makes the estimation of distributions over the
entire vocabulary computationally feasible.
To meet this goal, the output vocabulary is struc-
tured as a clustering tree, where each word belongs
to only one class and its associated sub-classes. If
wi denotes the ith word in a sentence, the sequence
c1:D(wi) = c1, . . . , cD encodes the path for wordwi
in the clustering tree, with D being the depth of the
tree, cd(wi) a class or sub-class assigned to wi, and
cD(wi) being the leaf associated with wi (the word
itself). The probability of wi given its history h can
then be computed as:
P (wi|h) =P (c1(wi)|h)
?
D?
d=2
P (cd(wi)|h, c1:d?1).
(7)
There is a softmax function at each level of the tree
and each word ends up forming its own class (a leaf).
The SOULmodel, represented on Figure 2, is thus
the same as for the standard model up to the output
layer. The main difference lies in the output struc-
ture which involves several layers with a softmax ac-
tivation function. The first (class layer) estimates
the class probability P (c1(wi)|h), while other out-
put sub-class layers estimate the sub-class probabili-
ties P (cd(wi)|h, c1:d?1). Finally, theword layers es-
timate the word probabilities P (cD(wi)|h, c1:D?1).
Words in the short-list remain special, since each of
them represents a (final) class.
Training a SOULmodel can be achieved by maxi-
mizing the log-likelihood of the parameters on some
training corpus. Following (Bengio et al, 2003),
this optimization is performed by stochastic back-
propagation. Details of the training procedure can
be found in (Le et al, 2011b).
Neural network architectures are also interesting
as they can easily handle larger contexts than typical
n-grammodels. In the SOUL architecture, enlarging
the context mainly consists in increasing the size of
the projection layer, which corresponds to a simple
look-up operation. Increasing the context length at
the input layer thus only causes a linear growth in
complexity in the worst case (Schwenk, 2007).
0...0100
10...000
0...0010
wi-1
wi-2
wi-3
R
R
R
shared input space
input layer
hidden layers
shortlist
sub-classlayers
wordlayers
classlayer
Figure 2: The architecture of a SOUL Neural Network
language model in the case of a 4-gram model.
3.3 Translation modeling with SOUL
The SOUL architecture was used successfully to
deliver (monolingual) LMs probabilities for speech
recognition (Le et al, 2011a) and machine transla-
tion (Allauzen et al, 2011) applications. In fact,
using this architecture, it is possible to estimate n-
gram distributions for any kind of discrete random
variables, such as a phrase or a tuple. The SOUL ar-
chitecture can thus be readily used as a replacement
for the standard n-gram TM described in section 2.1.
This is because all the random variables are events
over the same set of tuples.
Adopting this architecture for the other n-gram
TM respectively described by equations (3) and (5)
is more tricky, as they involve two different lan-
guages and thus two different vocabularies: the pre-
dicted unit is a target phrase (resp. word), whereas
the context is made of both source and target phrases
(resp. words). A subsequent modification of the
SOUL architecture was thus performed to make up
for ?mixed? contexts: rather than projecting all the
context words or phrases into the same continuous
space (using the matrix R, see Figure 2), we used
two different projection matrices, one for each lan-
guage. The input layer is thus composed of two vec-
tors in two different spaces; these two representa-
tions are then combined through the hidden layer,
the other layers remaining unchanged.
43
4 Experimental Results
We now turn to an experimental comparison of the
models introduced in Section 2. We first describe
the tasks and data that were used, before presenting
our n-gram based system and baseline set-up. Our
results are finally presented and discussed.
Let us first emphasize that the design and inte-
gration of a SOUL model for large SMT tasks is
far from easy, given the computational cost of com-
puting n-gram probabilities, a task that is performed
repeatedly during the search of the best translation.
Our solution was to resort to a two pass approach:
the first pass uses a conventional back-off n-gram
model to produce a k-best list (the k most likely
translations); in the second pass, the probability of
a m-gram SOUL model is computed for each hy-
pothesis, added as a new feature and the k-best list
is accordingly reordered3. In all the following ex-
periments, we used a fixed context size for SOUL of
m = 10, and used k = 300.
4.1 Tasks and corpora
The two tasks considered in our experiments
are adapted from the text translation track of
IWSLT 2011 from English to French (the ?TED?
talk task): a small data scenario where the only
training data is a small in-domain corpus; and a large
scale condition using all the available training data.
In this article, we only provide a short overview of
the task; all the necessary details regarding this eval-
uation campaign are on the official website4.
The in-domain training data consists of 107, 058
sentence pairs, whereas for the large scale task, all
the data available for the WMT 2011 evaluation5 are
added. For the latter task, the available parallel data
includes a large Web corpus, referred to as the Gi-
gaWord parallel corpus. This corpus is very noisy
and is accordingly filtered using a simple perplexity
criterion as explained in (Allauzen et al, 2011). The
total amount of training data is approximately 11.5
million sentence pairs for the bilingual part, and
about 2.5 billion of words for the monolingual part.
As the provided development data was quite small,
3The probability estimated with the SOULmodel is added as
a new feature to the score of an hypothesis given by Equation 1.
The coefficients are retuned before the reranking step.
4iwslt2011.org
5www.statmt.org/wmt11/
Model Vocabulary size
Small task Large task
src trg src trg
Standard 317k 8847k
Phrase factored 96k 131k 4262k 3972k
Word factored 45k 53k 505k 492k
Table 1: Vocabulary sizes for the English to French tasks
obtained with various SOUL translation (TM) models.
For the factored models, sizes are indicated for both
source (src) and target (trg) sides.
development and test set were inverted, and we fi-
nally used a development set of 1,664 sentences, and
a test set of 934 sentences. The table 1 provides the
sizes of the different vocabularies. The n-gram TMs
are estimated over a training corpus composed of tu-
ple sequences. Tuples are extracted from the word-
aligned parallel data (using MGIZA++6 with default
settings) in such a way that a unique segmentation
of the bilingual corpus is achieved, allowing to di-
rectly estimate bilingual n-gram models (see (Crego
and Marin?o, 2006) for details).
4.2 n-gram based translation system
The n-gram based system used here is based on an
open source implementation described in (Crego et
al., 2011). In a nutshell, the TM is implemented as
a stochastic finite-state transducer trained using a n-
gram model of (source, target) pairs as described in
section 2.1. Training this model requires to reorder
source sentences so as to match the target word or-
der. This is performed by a non-deterministic finite-
state reordering model, which uses part-of-speech
information generated by the TreeTagger to gener-
alize reordering patterns beyond lexical regularities.
In addition to the TM, fourteen feature functions
are included: a target-language model; four lexi-
con models; six lexicalized reordering models (Till-
mann, 2004; Crego et al, 2011); a distance-based
distortion model; and finally a word-bonus model
and a tuple-bonus model. The four lexicon mod-
els are similar to the ones used in standard phrase-
based systems: two scores correspond to the rela-
tive frequencies of the tuples and two lexical weights
are estimated from the automatically generated word
6geek.kyloo.net/software
44
alignments. The weights associated to feature func-
tions are optimally combined using the Minimum
Error Rate Training (MERT) (Och, 2003). All the
results in BLEU are obtained as an average of 4 op-
timization runs7.
For the small task, the target LM is a standard
4-gram model estimated with the Kneser-Ney dis-
counting scheme interpolated with lower order mod-
els (Kneser and Ney, 1995; Chen and Goodman,
1996), while for the large task, the target LM is ob-
tained by linear interpolation of several 4-grammod-
els (see (Lavergne et al, 2011) for details). As for
the TM, all the available parallel corpora were sim-
ply pooled together to train a 3-gram model. Results
obtained with this large-scale system were found to
be comparable to some of the best official submis-
sions.
4.3 Small task evaluation
Table 2 summarizes the results obtained with the
baseline and different SOUL models, TMs and a
target LM. The first comparison concerns the stan-
dard n-gram TM, defined by equation (2), when es-
timated conventionally or as a SOULmodel. Adding
the latter model yields a slight BLEU improvement
of 0.5 point over the baseline. When the SOUL TM
is phrased factored as defined in equation (3) the
gain is of 0.9 BLEU point instead. This difference
can be explained by the smaller vocabularies used
in the latter model, and its improved robustness to
data sparsity issues. Additional gains are obtained
with the word factored TM defined by equation (5):
a BLEU improvement of 0.8 point over the phrase
factored TM and of 1.7 point over the baseline are
respectively achieved. We assume that the observed
improvements can be explained by the joint effect of
a better smoothing and a longer context.
The comparison with the condition where we only
use a SOUL target LM is interesting as well. Here,
the use of the word factored TM still yields to a 0.6
BLEU improvement. This result shows that there
is an actual benefit in smoothing the TM estimates,
rather than only focus on the LM estimates.
Table 3 reports a comparison among the dif-
ferent components and variations of the word
7The standard deviations are below 0.1 and thus omitted in
the reported results.
Model BLEU
dev test
Baseline 31.4 25.8
Adding a SOUL model
Standard TM 32.0 26.3
Phrase factored TM 32.7 26.7
Word factored TM 33.6 27.5
Target LM 32.6 26.9
Table 2: Results for the small English to French task ob-
tained with the baseline system and with various SOUL
translation (TM) or target language (LM) models.
Model BLEU
dev test
Adding a SOUL model
+ P
(
tki |h
n?1(tki ), h
n?1(s1i+1)
)
32.6 26.9
+ P
(
ski |h
n?1(t1i ), h
n?1(ski )
)
32.1 26.2
+ the combination of both 33.2 27.5
+ P
(
ski |h
n?1(ski ), h
n?1(t1i+1)
)
31.7 26.1
+ P
(
tki |h
n?1(s1i ), h
n?1(tki )
)
32.7 26.8
+ the combination of both 33.4 27.2
Table 3: Comparison of the different components and
variations of the word factored translation model.
factored TM. In the upper part of the table,
the model defined by equation (5) is evaluated
component by component: first the translation
term P
(
tki |h
n?1(tki ), h
n?1(s1i+1)
)
, then its distor-
tion counterpart P
(
ski |h
n?1(t1i ), h
n?1(ski )
)
and fi-
nally their combination, which yields the joint prob-
ability of the sentence pair. Here, we observe that
the best improvement is obtained with the transla-
tion term, which is 0.7 BLEU point better than the
latter term. Moreover, the use of just a translation
term only yields a BLEU score equal to the one ob-
tained with the SOUL target LM, and its combina-
tion with the distortion term is decisive to attain the
additional gain of 0.6 BLEU point. The lower part of
the table provides the same comparison, but for the
variation of the word factored TM. Besides a similar
trend, we observe that this variation delivers slightly
lower results. This can be explained by the restricted
context used by the translation term which no longer
includes the current source phrase or word.
45
Model BLEU
dev test
Baseline 33.7 28.2
Adding a word factored SOUL TM
+ in-domain TM 35.2 29.4
+ out-of-domain TM 34.8 29.1
+ out-of-domain adapted TM 35.5 29.8
Adding a SOUL LM
+ out-of-domain adapted LM 35.0 29.2
Table 4: Results for the large English to French trans-
lation task obtained by adding various SOUL translation
and language models (see text for details).
4.4 Large task evaluation
For the large-scale setting, the training material in-
creases drastically with the use of the additional out-
of-domain data for the baseline models. Results are
summarized in Table 4. The first observation is the
large increase of BLEU (+2.4 points) for the base-
line system over the small-scale baseline. For this
task, only the word factored TM is evaluated since
it significantly outperforms the others on the small
task (see section 4.3).
In a first scenario, we use a word factored TM,
trained only on the small in-domain corpus. Even
though the training corpus of the baseline TM is one
hundred times larger than this small in-domain data,
adding the SOUL TM still yields a BLEU increase
of 1.2 point8. In a second scenario, we increase the
training corpus for the SOUL, and include parts of
the out-of-domain data (the WMT part). The result-
ing BLEU score is here slightly worse than the one
obtained with just the in-domain TM, yet delivering
improved results with the respect to the baseline.
In a last attempt, we amended the training regime
of the neural network. In a fist step, we trained con-
ventionally a SOUL model using the same out-of-
domain parallel data as before. We then adapted this
model by running five additional epochs of the back-
propagation algorithm using the in-domain data. Us-
ing this adapted model yielded our best results to
date with a BLEU improvement of 1.6 points over
the baseline results. Moreover, the gains obtained
using this simple domain adaptation strategy are re-
8Note that the in-domain data was already included in the
training corpus of the baseline TM.
spectively of +0.4 and +0.8 BLEU, as compared
with the small in-domain model and the large out-
of-domain model. These results show that the SOUL
TM can scale efficiently and that its structure is well
suited for domain adaptation.
5 Related work
To the best of our knowledge, the first work on ma-
chine translation in continuous spaces is (Schwenk
et al, 2007), where the authors introduced the model
referred here to as the the standard n-gram trans-
lation model in Section 2.1. This model is an ex-
tension of the continuous space language model
of (Bengio et al, 2003), the basic unit is the tuple
(or equivalently the phrase pair). The resulting vo-
cabulary being too large to be handled by neural net-
works without a structured output layer, the authors
had thus to restrict the set of the predicted units to a
8k short-list . Moreover, in (Zamora-Martinez et al,
2010), the authors propose a tighter integration of a
continuous space model with a n-gram approach but
only for the target LM.
A different approach, described in (Sarikaya et
al., 2008), divides the problem in two parts: first the
continuous representation is obtained by an adapta-
tion of the Latent Semantic Analysis; then a Gaus-
sian mixture model is learned using this continu-
ous representation and included in a hidden Markov
model. One problem with this approach is the sep-
aration between the training of the continuous rep-
resentation on the one hand, and the training of the
translation model on the other hand. In comparison,
in our approach, the representation and the predic-
tion are learned in a joined fashion.
Other ways to address the data sparsity issues
faced by translation model were also proposed in the
literature. Smoothing is obviously one possibility
(Foster et al, 2006). Another is to use factored lan-
guage models, introduced in (Bilmes and Kirchhoff,
2003), then adapted for translation models in (Koehn
and Hoang, 2007; Crego and Yvon, 2010). Such ap-
proaches require to use external linguistic analysis
tools which are error prone; moreover, they did not
seem to bring clear improvements, even when trans-
lating into morphologically rich languages.
46
6 Conclusion
In this paper, we have presented possible ways to use
a neural network architecture as a translation model.
A first contribution was to produce the first large-
scale neural translation model, implemented here in
the framework of the n-gram based models, tak-
ing advantage of a specific hierarchical architecture
(SOUL). By considering several decompositions of
the joint probability of a sentence pair, several bilin-
gual translation models were presented and dis-
cussed. As it turned out, using a factorization which
clearly distinguishes the source and target sides, and
only involves word probabilities, proved an effec-
tive remedy to data sparsity issues and provided sig-
nificant improvements over the baseline. Moreover,
this approach was also experimented within the sys-
tems we submitted to the shared translation task of
the seventh workshop on statistical machine trans-
lation (WMT 2012). These experimentations in a
large scale setup and for different language pair cor-
roborate the improvements reported in this article.
We also investigated various training regimes for
these models in a cross domain adaptation setting.
Our results show that adapting an out-of-domain
SOUL TM is both an effective and very fast way to
perform bilingual model adaptation. Adding up all
these novelties finally brought us a 1.6 BLEU point
improvement over the baseline. Even though our
experiments were carried out only within the frame-
work of n-gram basedMT systems, using such mod-
els in other systems is straightforward. Future work
will thus aim at introducing them into conventional
phrase-based systems, such as Moses (Koehn et al,
2007). Given that Moses only implicitly uses n-
gram based information, adding SOUL translation
models is expected to be even more helpful.
Acknowledgments
This work was partially funded by the French State
agency for innovation (OSEO), in the Quaero Pro-
gramme.
References
Alexandre Allauzen, Gilles Adda, He?le`ne Bonneau-
Maynard, Josep M. Crego, Hai-Son Le, Aure?lien Max,
Adrien Lardilleux, Thomas Lavergne, Artem Sokolov,
Guillaume Wisniewski, and Franc?ois Yvon. 2011.
LIMSI @ WMT11. In Proceedings of the Sixth Work-
shop on Statistical Machine Translation, pages 309?
315, Edinburgh, Scotland.
Yoshua Bengio, Re?jean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. JMLR, 3:1137?1155.
Jeff A. Bilmes and Katrin Kirchhoff. 2003. Factored
language models and generalized parallel backoff. In
NAACL ?03: Proceedings of the 2003 Conference of
the North American Chapter of the Association for
Computational Linguistics on Human Language Tech-
nology, pages 4?6.
Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vin-
cent J. Della Pietra, and Jenifer C. Lai. 1992. Class-
based n-gram models of natural language. Computa-
tional Linguistics, 18(4):467?479.
Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della
Pietra, and Robert L. Mercer. 1993. The mathemat-
ics of statistical machine translation: parameter esti-
mation. Computational Linguistics, 19(2):263?311.
Francesco Casacuberta and Enrique Vidal. 2004. Ma-
chine translation with inferred stochastic finite-state
transducers. Computational Linguistics, 30(3):205?
225.
Stanley F. Chen and Joshua Goodman. 1996. An empiri-
cal study of smoothing techniques for language model-
ing. In Proc. ACL?96, pages 310?318, San Francisco.
Ronan Collobert, Jason Weston, Le?on Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011.
Natural language processing (almost) from scratch.
Journal of Machine Learning Research, 12:2493?
2537.
Josep M. Crego and Jose? B. Marin?o. 2006. Improving
statistical MT by coupling reordering and decoding.
Machine Translation, 20(3):199?215.
Josep M. Crego and Franc?ois Yvon. 2010. Factored
bilingual n-gram language models for statistical ma-
chine translation. Machine Translation, pages 1?17.
Josep M. Crego, Franc?ois Yvon, and Jose? B. Marin?o.
2011. N-code: an open-source Bilingual N-gram SMT
Toolkit. Prague Bulletin of Mathematical Linguistics,
96:49?58.
George Foster, Roland Kuhn, and Howard Johnson.
2006. Phrase-table smoothing for statistical machine
translation. In Proceedings of the 2006 Conference on
Empirical Methods in Natural Language Processing,
pages 53?61, Sydney, Australia.
Reinhard Kneser and Hermann Ney. 1995. Improved
backing-off for m-gram language modeling. In Pro-
ceedings of the IEEE International Conference on
Acoustics, Speech and Signal Processing, volume I,
pages 181?184, Detroit, Michigan.
47
Philipp Koehn and Hieu Hoang. 2007. Factored transla-
tion models. In Proceedings of the 2007 Joint Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and Computational Natural Language Learn-
ing (EMNLP-CoNLL), pages 868?876.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proc.
ACL?07, pages 177?180, Prague, Czech Republic.
Hong-Kwang Kuo, Lidia Mangu, Ahmad Emami, and
Imed Zitouni. 2010. Morphological and syntactic fea-
tures for Arabic speech recognition. In Proc. ICASSP
2010.
Thomas Lavergne, Alexandre Allauzen, Hai-Son Le, and
Franc?ois Yvon. 2011. LIMSI?s experiments in do-
main adaptation for IWSLT11. In Proceedings of
the eight International Workshop on Spoken Language
Translation (IWSLT), San Francisco, CA.
Hai-Son Le, Ilya Oparin, Alexandre Allauzen, Jean-Luc
Gauvain, and Franc?ois Yvon. 2011a. Structured out-
put layer neural network language model. In Proceed-
ings of ICASSP?11, pages 5524?5527.
Hai-Son Le, Ilya Oparin, Abdel Messaoudi, Alexan-
dre Allauzen, Jean-Luc Gauvain, and Franc?ois Yvon.
2011b. Large vocabulary SOUL neural network lan-
guage models. In Proceedings of InterSpeech 2011.
Xunying Liu, Mark J. F. Gales, and Philip C. Woodland.
2011. Improving lvcsr system combination using neu-
ral network language model cross adaptation. In IN-
TERSPEECH, pages 2857?2860.
Jose? B. Marin?o, Rafael E. Banchs, Josep M. Crego, Adria`
de Gispert, Patrick Lambert, Jose? A.R. Fonollosa, and
Marta R. Costa-Jussa`. 2006. N-gram-based machine
translation. Computational Linguistics, 32(4):527?
549.
Tomas Mikolov, Stefan Kombrink, Lukas Burget, Jan
Cernocky?, and Sanjeev Khudanpur. 2011. Extensions
of recurrent neural network language model. In Proc.
of ICASSP?11, pages 5528?5531.
Andriy Mnih and Geoffrey E Hinton. 2008. A scalable
hierarchical distributed language model. In D. Koller,
D. Schuurmans, Y. Bengio, and L. Bottou, editors, Ad-
vances in Neural Information Processing Systems 21,
volume 21, pages 1081?1088.
Jan Niehues, Teresa Herrmann, Stephan Vogel, and Alex
Waibel. 2011. Wider context by using bilingual lan-
guage models in machine translation. In Proceedings
of the Sixth Workshop on Statistical Machine Trans-
lation, pages 198?206, Edinburgh, Scotland. Associa-
tion for Computational Linguistics.
Franz Josef Och and Hermann Ney. 2004. The align-
ment template approach to statistical machine transla-
tion. Computational Linguistics, 30:417?449, Decem-
ber.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In ACL ?03: Proc. of
the 41st Annual Meeting on Association for Computa-
tional Linguistics, pages 160?167.
Ruhi Sarikaya, Yonggang Deng, Mohamed Afify, Brian
Kingsbury, and Yuqing Gao. 2008. Machine trans-
lation in continuous space. In Proceedings of Inter-
speech, pages 2350?2353, Brisbane, Australia.
Holger Schwenk, Marta R. Costa-Jussa`, and Jose? A.R.
Fonollosa. 2007. Smooth bilingual n-gram transla-
tion. In Proceedings of the 2007 Joint Conference
on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
(EMNLP-CoNLL), pages 430?438, Prague, Czech Re-
public.
Holger Schwenk. 2007. Continuous space language
models. Computer Speech and Language, 21(3):492?
518.
Yeh W. Teh. 2006. A hierarchical Bayesian language
model based on Pitman-Yor processes. In Proc. of
ACL?06, pages 985?992, Sidney, Australia.
Christoph Tillmann. 2004. A unigram orientation model
for statistical machine translation. In Proceedings of
HLT-NAACL 2004, pages 101?104.
Francisco Zamora-Martinez, Maria Jose? Castro-Bleda,
and Holger Schwenk. 2010. N-gram-based Machine
Translation enhanced with Neural Networks for the
French-English BTEC-IWSLT?10 task. In Proceed-
ings of the seventh International Workshop on Spoken
Language Translation (IWSLT), pages 45?52.
Richard Zens, Franz Josef Och, and Hermann Ney. 2002.
Phrase-based statistical machine translation. In KI
?02: Proceedings of the 25th Annual German Con-
ference on AI, pages 18?32, London, UK. Springer-
Verlag.
48
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 309?315,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
LIMSI @ WMT11
Alexandre Allauzen
He?le`ne Bonneau-Maynard
Hai-Son Le
Aure?lien Max
Guillaume Wisniewski
Franc?ois Yvon
Univ. Paris-Sud and LIMSI-CNRS
B.P. 133, 91403 Orsay cedex, France
Gilles Adda
Josep M. Crego
Adrien Lardilleux
Thomas Lavergne
Artem Sokolov
LIMSI-CNRS
B.P. 133, 91403 Orsay cedex, France
Abstract
This paper describes LIMSI?s submissions to
the Sixth Workshop on Statistical Machine
Translation. We report results for the French-
English and German-English shared transla-
tion tasks in both directions. Our systems
use n-code, an open source Statistical Ma-
chine Translation system based on bilingual
n-grams. For the French-English task, we fo-
cussed on finding efficient ways to take ad-
vantage of the large and heterogeneous train-
ing parallel data. In particular, using a sim-
ple filtering strategy helped to improve both
processing time and translation quality. To
translate from English to French and Ger-
man, we also investigated the use of the
SOUL language model in Machine Trans-
lation and showed significant improvements
with a 10-gram SOUL model. We also briefly
report experiments with several alternatives to
the standard n-best MERT procedure, leading
to a significant speed-up.
1 Introduction
This paper describes LIMSI?s submissions to the
Sixth Workshop on Statistical Machine Translation,
where LIMSI participated in the French-English and
German-English tasks in both directions. For this
evaluation, we used n-code, our in-house Statistical
Machine Translation (SMT) system which is open-
source and based on bilingual n-grams.
This paper is organized as follows. Section 2 pro-
vides an overview of n-code, while the data pre-
processing and filtering steps are described in Sec-
tion 3. Given the large amount of parallel data avail-
able, we proposed a method to filter the French-
English GigaWord corpus (Section 3.2). As in our
previous participations, data cleaning and filtering
constitute a non-negligible part of our work. This
includes detecting and discarding sentences in other
languages; removing sentences which are also in-
cluded in the provided development sets, as well as
parts that are repeated (for the monolingual news
data, this can reduce the amount of data by a fac-
tor 3 or 4, depending on the language and the year);
normalizing the character set (non-utf8 characters
which are aberrant in context, or in the case of the
GigaWord corpus, a lot of non-printable and thus in-
visible control characters such as EOT (end of trans-
mission)1).
For target language modeling (Section 4), a stan-
dard back-off n-gram model is estimated and tuned
as described in Section 4.1. Moreover, we also in-
troduce in Section 4.2 the use of the SOUL lan-
guage model (LM) (Le et al, 2011) in SMT. Based
on neural networks, the SOUL LM can handle an
arbitrary large vocabulary and a high order marko-
vian assumption (up to 10-gram in this work). Fi-
nally, experimental results are reported in Section 5
both in terms of BLEU scores and translation edit
rates (TER) measured on the provided newstest2010
dataset.
2 System Overview
Our in-house n-code SMT system implements the
bilingual n-gram approach to Statistical Machine
Translation (Casacuberta and Vidal, 2004). Given a
1This kind of characters was used for Teletype up to the sev-
enties or early eighties.
309
source sentence sJ1, a translation hypothesis t?
I
1 is de-
fined as the sentence which maximizes a linear com-
bination of feature functions:
t?I1 = argmax
tI1
{
M
?
m=1
?mhm(sJ1, tI1)
}
(1)
where sJ1 and t
I
1 respectively denote the source and
the target sentences, and ?m is the weight associated
with the feature function hm. The translation fea-
ture is the log-score of the translation model based
on bilingual units called tuples. The probability as-
signed to a sentence pair by the translation model is
estimated by using the n-gram assumption:
p(sJ1, t
I
1) =
K
?
k=1
p((s, t)k|(s, t)k?1 . . .(s, t)k?n+1)
where s refers to a source symbol (t for target) and
(s, t)k to the kth tuple of the given bilingual sentence
pair. It is worth noticing that, since both languages
are linked up in tuples, the context information pro-
vided by this translation model is bilingual. In ad-
dition to the translation model, eleven feature func-
tions are combined: a target-language model (see
Section 4 for details); four lexicon models; two lex-
icalized reordering models (Tillmann, 2004) aim-
ing at predicting the orientation of the next transla-
tion unit; a ?weak? distance-based distortion model;
and finally a word-bonus model and a tuple-bonus
model which compensate for the system preference
for short translations. The four lexicon models are
similar to the ones used in a standard phrase-based
system: two scores correspond to the relative fre-
quencies of the tuples and two lexical weights are
estimated from the automatically generated word
alignments. The weights associated to feature func-
tions are optimally combined using a discriminative
training framework (Och, 2003) (Minimum Error
Rate Training (MERT), see details in Section 5.4),
using the provided newstest2009 data as develop-
ment set.
2.1 Training
Our translation model is estimated over a training
corpus composed of tuple sequences using classi-
cal smoothing techniques. Tuples are extracted from
a word-aligned corpus (using MGIZA++2 with de-
fault settings) in such a way that a unique segmenta-
tion of the bilingual corpus is achieved, allowing to
estimate the n-gram model. Figure 1 presents a sim-
ple example illustrating the unique tuple segmenta-
tion for a given word-aligned pair of sentences (top).
Figure 1: Tuple extraction from a sentence pair.
The resulting sequence of tuples (1) is further re-
fined to avoid NULL words in the source side of the
tuples (2). Once the whole bilingual training data is
segmented into tuples, n-gram language model prob-
abilities can be estimated. In this example, note that
the English source words perfect and translations
have been reordered in the final tuple segmentation,
while the French target words are kept in their orig-
inal order.
2.2 Inference
During decoding, source sentences are encoded
in the form of word lattices containing the most
promising reordering hypotheses, so as to reproduce
the word order modifications introduced during the
tuple extraction process. Hence, at decoding time,
only those encoded reordering hypotheses are trans-
lated. Reordering hypotheses are introduced using
a set of reordering rules automatically learned from
the word alignments.
In the previous example, the rule [perfect transla-
tions ; translations perfect] produces the swap of
the English words that is observed for the French
and English pair. Typically, part-of-speech (POS)
information is used to increase the generalization
power of such rules. Hence, rewriting rules are built
using POS rather than surface word forms. Refer
2http://geek.kyloo.net/software
310
to (Crego and Marin?o, 2007) for details on tuple ex-
traction and reordering rules.
3 Data Pre-processing and Selection
We used all the available parallel data allowed in
the constrained task to compute the word align-
ments, except for the French-English tasks where
the United Nation corpus was not used to train our
translation models. To train the target language
models, we also used all provided data and mono-
lingual corpora released by the LDC for French
and English. Moreover, all parallel corpora were
POS-tagged with the TreeTagger (Schmid, 1994).
For German, the fine-grained POS information used
for pre-processing was computed by the RFTag-
ger (Schmid and Laws, 2008).
3.1 Tokenization
We took advantage of our in-house text process-
ing tools for the tokenization and detokenization
steps (De?chelotte et al, 2008). Previous experi-
ments have demonstrated that better normalization
tools provide better BLEU scores (Papineni et al,
2002). Thus all systems are built in ?true-case.?
As German is morphologically more complex
than English, the default policy which consists in
treating each word form independently is plagued
with data sparsity, which poses a number of diffi-
culties both at training and decoding time. Thus,
to translate from German to English, the German
side was normalized using a specific pre-processing
scheme (described in (Allauzen et al, 2010)), which
aims at reducing the lexical redundancy and splitting
complex compounds.
Using the same pre-processing scheme to trans-
late from English to German would require to post-
process the output to undo the pre-processing. As in
our last year?s experiments (Allauzen et al, 2010),
this pre-processing step could be achieved with a
two-step decoding. However, by stacking two de-
coding steps, we may stack errors as well. Thus, for
this direction, we used the German tokenizer pro-
vided by the organizers.
3.2 Filtering the GigaWord Corpus
The available parallel data for English-French in-
cludes a large Web corpus, referred to as the Giga-
Word parallel corpus. This corpus is very noisy, and
contains large portions that are not useful for trans-
lating news text. The first filter aimed at detecting
foreign languages based on perplexity and lexical
coverage. Then, to select a subset of parallel sen-
tences, trigram LMs were trained for both French
and English languages on a subset of the available
News data: the French (resp. English) LM was used
to rank the French (resp. English) side of the cor-
pus, and only those sentences with perplexity above
a given threshold were selected. Finally, the two se-
lected sets were intersected. In the following exper-
iments, the threshold was set to the median or upper
quartile value of the perplexity. Therefore, half (or
75%) of this corpus was discarded.
4 Target Language Modeling
Neural networks, working on top of conventional
n-gram models, have been introduced in (Bengio
et al, 2003; Schwenk, 2007) as a potential means
to improve conventional n-gram language models
(LMs). However, probably the major bottleneck
with standard NNLMs is the computation of poste-
rior probabilities in the output layer. This layer must
contain one unit for each vocabulary word. Such a
design makes handling of large vocabularies, con-
sisting of hundreds thousand words, infeasible due
to a prohibitive growth in computation time. While
recent work proposed to estimate the n-gram dis-
tributions only for the most frequent words (short-
list) (Schwenk, 2007), we explored the use of the
SOUL (Structured OUtput Layer Neural Network)
language model for SMT in order to handle vocabu-
laries of arbitrary sizes.
Moreover, in our setting, increasing the order of
standard n-gram LM did not show any significant
improvement. This is mainly due to the data spar-
sity issue and to the drastic increase in the number of
parameters that need to be estimated. With NNLM
however, the increase in context length at the input
layer results in only a linear growth in complexity
in the worst case (Schwenk, 2007). Thus, training
longer-context neural network models is still feasi-
ble, and was found to be very effective in our system.
311
4.1 Standard n-gram Back-off Language
Models
To train our language models, we assumed that the
test set consisted in a selection of news texts dat-
ing from the end of 2010 to the beginning of 2011.
This assumption was based on what was done for
the 2010 evaluation. Thus, for each language, we
built a development corpus in order to optimize the
vocabulary and the target language model.
Development set and vocabulary In order to
cover different periods, two development sets were
used. The first one is newstest2008. This corpus is
two years older than the targeted time period; there-
fore, a second development corpus named dev2010-
2011 was collected by randomly sampling bunches
of 5 consecutive sentences from the provided news
data of 2010 and 2011.
To estimate such large LMs, a vocabulary
was first defined for each language by including
all tokens observed in the Europarl and News-
Commentary corpora. For French and English, this
vocabulary was then expanded with all words that
occur more than 5 times in the French-English Gi-
gaWord corpus, and with the most frequent proper
names taken from the monolingual news data of
2010 and 2011. As for German, since the amount
of training data was smaller, the vocabulary was ex-
panded with the most frequent words observed in the
monolingual news data of 2010 and 2011. This pro-
cedure resulted in a vocabulary containing around
500k words in each language.
Language model training All the training data al-
lowed in the constrained task were divided into sev-
eral sets based on dates or genres (resp. 9 and 7
sets for English and French). On each set, a stan-
dard 4-gram LM was estimated from the 500k words
vocabulary using absolute discounting interpolated
with lower order models (Kneser and Ney, 1995;
Chen and Goodman, 1998).
All LMs except the one trained on the news cor-
pora from 2010-2011 were first linearly interpolated.
The associated coefficients were estimated so as to
minimize the perplexity evaluated on dev2010-2011.
The resulting LM and the 2010-2011 LM were fi-
naly interpolated with newstest2008 as development
data. This procedure aims to avoid overestimating
the weight associated to the 2010-2011 LM.
4.2 The SOUL Model
We give here a brief overview of the SOUL LM;
refer to (Le et al, 2011) for the complete training
procedure. Following the classical work on dis-
tributed word representation (Brown et al, 1992),
we assume that the output vocabulary is structured
by a clustering tree, where each word belongs to
only one class and its associated sub-classes. If wi
denotes the i-th word in a sentence, the sequence
c1:D(wi) = c1, . . . ,cD encodes the path for the word
wi in the clustering tree, with D the depth of the tree,
cd(wi) a class or sub-class assigned to wi, and cD(wi)
the leaf associated with wi (the word itself). The
n-gram probability of wi given its history h can then
be estimated as follows using the chain rule:
P(wi|h) = P(c1(wi)|h)
D
?
d=2
P(cd(wi)|h,c1:d?1)
Figure 2 represents the architecture of the NNLM
to estimate this distribution, for a tree of depth
D = 3. The SOUL architecture is the same as for
the standard model up to the output layer. The
main difference lies in the output structure which in-
volves several layers with a softmax activation func-
tion. The first softmax layer (class layer) estimates
the class probability P(c1(wi)|h), while other out-
put sub-class layers estimate the sub-class proba-
bilities P(cd(wi)|h,c1:d?1). Finally, the word layers
estimate the word probabilities P(cD(wi)|h,c1:D?1).
Words in the short-list are a special case since each
of them represents its own class without any sub-
classes (D = 1 in this case).
5 Experimental Results
The experimental results are reported in terms of
BLEU and translation edit rate (TER) using the
newstest2010 corpus as evaluation set. These auto-
matic metrics are computed using the scripts pro-
vided by the NIST after a detokenization step.
5.1 English-French
Compared with last year evaluation, the amount of
available parallel data has drastically increased with
about 33M of sentence pairs. It is worth noticing
312
wi-1
w
i-2
w
i-3
R
R
R
W
ih
 shared context space
input layer
hidden layer:
tanh activation
word layers
sub-class 
layers
}
short list
Figure 2: Architecture of the Structured Output Layer
Neural Network language model.
that the provided corpora are not homogeneous, nei-
ther in terms of genre nor in terms of topics. Never-
theless, the most salient difference is the noise car-
ried by the GigaWord and the United Nation cor-
pora. The former is an automatically collected cor-
pus drawn from different websites, and while some
parts are indeed relevant to translate news texts, us-
ing the whole GigaWord corpus seems to be harm-
ful. The latter (United Nation) is obviously more
homogeneous, but clearly out of domain. As an il-
lustration, discarding the United Nation corpus im-
proves performance slightly.
Table 1 summarizes some of our attempts at deal-
ing with such a large amount of parallel data. As
stated above, translation models are trained with
the news-commentary, Europarl, and GigaWord cor-
pora. For this last data set, results show the reward of
sentence pair selection as described in Section 3.2.
Indeed, filtering out 75% of the corpus yields to
a significant BLEU improvement when translating
from English to French and of 1 point in the other
direction (line upper quartile in Table 1). More-
over, a larger selection (50% in the median line) still
increases the overall performance. This shows the
room left for improvement by a more accurate data
selection process such as a well optimized thresh-
old in our approach, or a more sophisticated filtering
strategy (see for example (Foster et al, 2010)).
Another issue when using such a large amount
System en2fr fr2en
BLEU TER BLEU TER
All 27.4 56.6 26.8 55.0
Upper quartile 27.8 56.3 28.4 53.8
Median 28.1 56.0 28.6 53.5
Table 1: English-French translation results in terms of
BLEU score and TER estimated on newstest2010 with
the NIST script. All means that the translation model is
trained on news-commentary, Europarl, and the whole
GigaWord. The rows upper quartile and median corre-
spond to the use of a filtered version of the GigaWord.
of data is the mismatch between the target vocab-
ulary derived from the translation model and that of
the LM. The translation model may generate words
which are unknown to the LM, and their probabili-
ties could be overestimated. To avoid this behaviour,
the probability of unknown words for the target LM
is penalized during the decoding step.
5.2 English-German
For this translation task, we compare the impact of
two different POS-taggers to process the German
part of the parallel data. The results are reported
in Table 2. Results show that to translate from En-
glish to German, the use of a fine-grained POS infor-
mation (RFTagger) leads to a slight improvement,
whereas it harms the source reordering model in the
other direction. It is worth noticing that to translate
from German to English, the RFTagger is always
used during the data pre-processing step, while a dif-
ferent POS tagger may be involved for the source
reordering model training.
System en2de de2en
BLEU TER BLEU TER
RFTagger 22.8 60.1 16.3 66.0
TreeTagger 23.1 59.4 16.2 66.0
Table 2: Translation results in terms of BLEU score
and translation edit rate (TER) estimated on newstest2010
with the NIST scoring script.
5.3 The SOUL Model
As mentioned in Section 4.2, the order of a con-
tinuous n-gram model such as the SOUL LM can
be raised without a prohibitive increase in complex-
ity. We summarize in Table 3 our experiments with
313
SOUL LMs of orders 4, 6, and 10. The SOUL LM
is introduced in the SMT pipeline by rescoring the
n-best list generated by the decoder, and the asso-
ciated weight is tuned with MERT. We observe for
the English-French task: a BLEU improvement of
0.3, as well as a similar trend in TER, when intro-
ducing a 4-gram SOUL LM; an additional BLEU
improvement of 0.3 when increasing the order from
4 to 6; and a less important gain with the 10-gram
SOUL LM. In the end, the use of a 10-gram SOUL
LM achieves a 0.7 BLEU improvement and a TER
decrease of 0.8. The results on the English-German
task show the same trend with a 0.5 BLEU point
improvement.
SOUL LM en2fr en2de
BLEU TER BLEU TER
without 28.1 56.0 16.3 66.0
4-gram 28.4 55.5 16.5 64.9
6-gram 28.7 55.3 16.7 64.9
10-gram 28.8 55.2 16.8 64.6
Table 3: Translation results from English to French and
English to German measured on newstest2010 using a
100-best rescoring with SOUL LMs of different orders.
5.4 Optimization Issues
Along with MIRA (Margin Infused Relaxed Al-
gorithm) (Watanabe et al, 2007), MERT is the
most widely used algorithm for system optimiza-
tion. However, standard MERT procedure is known
to suffer from instability of results and very slow
training cycle with approximate estimates of one de-
coding cycle for each training parameter. For this
year?s evaluation, we experimented with several al-
ternatives to the standard n-best MERT procedure,
namely, MERT on word lattices (Macherey et al,
2008) and two differentiable variants to the BLEU
objective function optimized during the MERT cy-
cle. We have recast the former in terms of a spe-
cific semiring and implemented it using a general-
purpose finite state automata framework (Sokolov
and Yvon, 2011). The last two approaches, hereafter
referred to as ZHN and BBN, replace the BLEU
objective function, with the usual BLEU score on
expected n-gram counts (Rosti et al, 2010) and
with an expected BLEU score for normal n-gram
counts (Zens et al, 2007), respectively. All expecta-
tions (of the n-gram counts in the first case and the
BLEU score in the second) are taken over all hy-
potheses from n-best lists for each source sentence.
Experiments with the alternative optimization
methods achieved virtually the same performance in
terms of BLEU score, but 2 to 4 times faster. Neither
approach, however, showed any consistent and sig-
nificant improvement for the majority of setups tried
(with the exception of the BBN approach, that had
almost always improved over n-best MERT, but for
the sole French to English translation direction). Ad-
ditional experiments with 9 complementary transla-
tion models as additional features were performed
with lattice-MERT, but neither showed any substan-
tial improvement. In the view of these rather incon-
clusive experiments, we chose to stick to the classi-
cal MERT for the submitted results.
6 Conclusion
In this paper, we described our submissions to
WMT?11 in the French-English and German-
English shared translation tasks, in both directions.
For this year?s participation, we only used n-code,
our open source Statistical Machine Translation sys-
tem based on bilingual n-grams. Our contributions
are threefold. First, we have shown that n-gram
based systems can achieve state-of-the-art perfor-
mance on large scale tasks in terms of automatic
metrics such as BLEU. Then, as already shown by
several sites in the past evaluations, there is a signifi-
cant reward for using data selection algorithms when
dealing with large heterogeneous data sources such
as the GigaWord. Finally, the use of a large vocab-
ulary continuous space language model such as the
SOUL model has enabled to achieve significant and
consistent improvements. For the upcoming evalua-
tion(s), we would like to suggest that the important
work of data cleaning and pre-processing could be
shared among all the participants instead of being
done independently several times by each site. Re-
ducing these differences could indeed help improve
the reliability of SMT systems evaluation.
Acknowledgment
This work was achieved as part of the Quaero Pro-
gramme, funded by OSEO, French State agency for
innovation.
314
References
Alexandre Allauzen, Josep M. Crego, I?lknur Durgar El-
Kahlout, and Francois Yvon. 2010. LIMSI?s statis-
tical translation systems for WMT?10. In Proc. of the
Joint Workshop on Statistical Machine Translation and
MetricsMATR, pages 54?59, Uppsala, Sweden.
Yoshua Bengio, Re?jean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. JMLR, 3:1137?1155.
P.F. Brown, P.V. de Souza, R.L. Mercer, V.J. Della Pietra,
and J.C. Lai. 1992. Class-based n-gram models of nat-
ural language. Computational Linguistics, 18(4):467?
479.
Francesco Casacuberta and Enrique Vidal. 2004. Ma-
chine translation with inferred stochastic finite-state
transducers. Computational Linguistics, 30(3):205?
225.
Stanley F. Chen and Joshua T. Goodman. 1998. An
empirical study of smoothing techniques for language
modeling. Technical Report TR-10-98, Computer Sci-
ence Group, Harvard University.
Josep Maria Crego and Jose? Bernardo Marin?o. 2007. Im-
proving statistical MT by coupling reordering and de-
coding. Machine Translation, 20(3):199?215.
Daniel De?chelotte, Gilles Adda, Alexandre Allauzen,
Olivier Galibert, Jean-Luc Gauvain, He?le`ne Mey-
nard, and Franc?ois Yvon. 2008. LIMSI?s statisti-
cal translation systems for WMT?08. In Proc. of the
NAACL-HTL Statistical Machine Translation Work-
shop, Columbus, Ohio.
George Foster, Cyril Goutte, and Roland Kuhn. 2010.
Discriminative instance weighting for domain adapta-
tion in statistical machine translation. In Proceedings
of the 2010 Conference on Empirical Methods in Natu-
ral Language Processing, pages 451?459, Cambridge,
MA, October.
Reinhard Kneser and Herman Ney. 1995. Improved
backing-off for m-gram language modeling. In Pro-
ceedings of the International Conference on Acoustics,
Speech, and Signal Processing, ICASSP?95, pages
181?184, Detroit, MI.
Hai-Son Le, Ilya Oparin, Alexandre Allauzen, Jean-Luc
Gauvain, and Franc?ois Yvon. 2011. Structured output
layer neural network language model. In IEEE Inter-
national Conference on Acoustics, Speech and Signal
Processing (ICASSP 2011), Prague (Czech Republic),
22-27 May.
Wolfgang Macherey, Franz Josef Och, Ignacio Thayer,
and Jakob Uszkoreit. 2008. Lattice-based minimum
error rate training for statistical machine translation.
In Proc. of the Conf. on EMNLP, pages 725?734.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In ACL ?03: Proc. of
the 41st Annual Meeting on Association for Computa-
tional Linguistics, pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In ACL ?02: Proc. of
the 40th Annual Meeting on Association for Compu-
tational Linguistics, pages 311?318. Association for
Computational Linguistics.
Antti-Veikko I. Rosti, Bing Zhang, Spyros Matsoukas,
and Richard Schwartz. 2010. BBN system description
for wmt10 system combination task. In Proceedings of
the Joint Fifth Workshop on Statistical Machine Trans-
lation and MetricsMATR, WMT ?10, pages 321?326,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Helmut Schmid and Florian Laws. 2008. Estimation
of conditional probabilities with decision trees and an
application to fine-grained POS tagging. In Proceed-
ings of the 22nd International Conference on Com-
putational Linguistics (Coling 2008), pages 777?784,
Manchester, UK, August.
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In Proc. of International
Conference on New Methods in Language Processing,
pages 44?49, Manchester, UK.
Holger Schwenk. 2007. Continuous space language
models. Computer, Speech & Language, 21(3):492?
518.
Artem Sokolov and Franc?ois Yvon. 2011. Minimum er-
ror rate training semiring. In Proceedings of the 15th
Annual Conference of the European Association for
Machine Translation, EAMT?2011, May.
Christoph Tillmann. 2004. A unigram orientation model
for statistical machine translation. In Proceedings of
HLT-NAACL 2004, pages 101?104. Association for
Computational Linguistics.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and Hideki
Isozaki. 2007. Online large-margin training for sta-
tistical machine translation. In Proceedings of the
2007 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natu-
ral Language Learning (EMNLP-CoNLL), pages 764?
773, Prague, Czech Republic.
Richard Zens, Sasa Hasan, and Hermann Ney. 2007.
A systematic comparison of training criteria for sta-
tistical machine translation. In Proceedings of the
2007 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natu-
ral Language Learning (EMNLP-CoNLL), pages 524?
532.
315
NAACL-HLT 2012 Workshop: Will We Ever Really Replace the N-gram Model? On the Future of Language Modeling for HLT, pages 1?10,
Montre?al, Canada, June 8, 2012. c?2012 Association for Computational Linguistics
Measuring the Influence of Long Range Dependencies with Neural Network
Language Models
Le Hai Son and Alexandre Allauzen and Franc?ois Yvon
Univ. Paris-Sud and LIMSI/CNRS
rue John von Neumann, 91 403 Orsay cedex, France
Firstname.Lastname@limsi.fr
Abstract
In spite of their well known limitations,
most notably their use of very local con-
texts, n-gram language models remain an es-
sential component of many Natural Language
Processing applications, such as Automatic
Speech Recognition or Statistical Machine
Translation. This paper investigates the po-
tential of language models using larger con-
text windows comprising up to the 9 previ-
ous words. This study is made possible by
the development of several novel Neural Net-
work Language Model architectures, which
can easily fare with such large context win-
dows. We experimentally observed that ex-
tending the context size yields clear gains in
terms of perplexity and that the n-gram as-
sumption is statistically reasonable as long as
n is sufficiently high, and that efforts should
be focused on improving the estimation pro-
cedures for such large models.
1 Introduction
Conventional n-gram Language Models (LMs) are a
cornerstone of modern language modeling for Natu-
ral Language Processing (NLP) systems such as sta-
tistical machine translation (SMT) and Automatic
Speech Recognition (ASR). After more than two
decades of experimenting with these models in a
variety of languages, genres, datasets and appli-
cations, the vexing conclusion is that these mod-
els are very difficult to improve upon. Many vari-
ants of the simple n-gram model have been dis-
cussed in the literature; yet, very few of these vari-
ants have shown to deliver consistent performance
gains. Among these, smoothing techniques, such as
Good-Turing, Witten-Bell and Kneser-Ney smooth-
ing schemes (see (Chen and Goodman, 1996) for an
empirical overview and (Teh, 2006) for a Bayesian
interpretation) are used to compute estimates for the
probability of unseen events, which are needed to
achieve state-of-the-art performance in large-scale
settings. This is because, even when using the sim-
plifying n-gram assumption, maximum likelihood
estimates remain unreliable and tend to overeresti-
mate the probability of those rare n-grams that are
actually observed, while the remaining lots receive
a too small (null) probability.
One of the most successful alternative to date is
to use distributed word representations (Bengio et
al., 2003) to estimate the n-gram models. In this
approach, the discrete representation of the vocabu-
lary, where each word is associated with an arbitrary
index, is replaced with a continuous representation,
where words that are distributionally similar are rep-
resented as neighbors. This turns n-gram distribu-
tions into smooth functions of the word representa-
tion. These representations and the associated esti-
mates are jointly computed using a multi-layer neu-
ral network architecture. The use of neural-networks
language models was originally introduced in (Ben-
gio et al, 2003) and successfully applied to large-
scale speech recognition (Schwenk and Gauvain,
2002; Schwenk, 2007) and machine translation
tasks (Allauzen et al, 2011). Following these ini-
tial successes, the neural approach has recently been
extended in several promising ways (Mikolov et al,
2011a; Kuo et al, 2010; Liu et al, 2011).
Another difference between conventional and
1
neural network language models (NNLMs) that has
often been overlooked is the ability of the latter to
fare with extended contexts (Schwenk and Koehn,
2008; Emami et al, 2008); in comparison, standard
n-gram LMs rarely use values of n above n = 4
or 5, mainly because of data sparsity issues and
the lack of generalization of the standard estimates,
notwithstanding the complexity of the computations
incurred by the smoothing procedures (see however
(Brants et al, 2007) for an attempt to build very
large models with a simple smoothing scheme).
The recent attempts of Mikolov et al (2011b)
to resuscitate recurrent neural network architectures
goes one step further in that direction, as a recur-
rent network simulates an unbounded history size,
whereby the memory of all the previous words ac-
cumulates in the form of activation patterns on the
hidden layer. Significant improvements in ASR us-
ing these models were reported in (Mikolov et al,
2011b; Mikolov et al, 2011a). It must however be
emphasized that the use of a recurrent structure im-
plies an increased complexity of the training and in-
ference procedures, as compared to a standard feed-
forward network. This means that this approach can-
not handle large training corpora as easily as n-gram
models, which makes it difficult to perform a fair
comparison between these two architectures and to
assess the real benefits of using very large contexts.
The contribution is this paper is two-fold. We
first analyze the results of various NNLMs to assess
whether long range dependencies are efficient in lan-
guage modeling, considering history sizes ranging
from 3 words to an unbounded number of words (re-
current architecture). A by-product of this study is a
slightly modified version of n-gram SOUL model
(Le et al, 2011a) that aims at quantitatively esti-
mating the influence of context words both in terms
of their position and their part-of-speech informa-
tion. The experimental set-up is based on a large
scale machine translation task. We then propose a
head to head comparison between the feed-forward
and recurrent NNLMs. To make this comparison
fair, we introduce an extension of the SOUL model
that approximates the recurrent architecture with a
limited history. While this extension achieves per-
formance that are similar to the recurrent model on
small datasets, the associated training procedure can
benefit from all the speed-ups and tricks of standard
feedforward NNLM (mini-batch and resampling),
which make it able to handle large training corpora.
Furthermore, we show that this approximation can
also be effectively used to bootstrap the training of a
?true? recurrent architecture.
The rest of this paper is organized as follows. We
first recollect, in Section 2, the basics of NNLMs ar-
chitectures. We then describe, in Section 3, a num-
ber of ways to speed up training for our ?pseudo-
recurrent? model. We finally report, in Section 4,
various experimental results aimed at measuring the
impact of large contexts, first in terms of perplexity,
then on a realistic English to French translation task.
2 Language modeling in a continuous
space
Let V be a finite vocabulary, language models de-
fine distributions over sequences1 of tokens (typi-
cally words) wL1 in V
+ as follows:
P (wL1 ) =
L?
i=1
P (wi|w
i?1
1 ) (1)
Modeling the joint distribution of several discrete
random variables (such as words in a sentence) is
difficult, especially in NLP applications where V
typically contains hundreds of thousands words. In
the n-gram model, the context is limited to the n?1
previous words, yielding the following factorization:
P (wL1 ) =
L?
i=1
P (wi|w
i?1
i?n+1) (2)
Neural network language models (Bengio et al,
2003) propose to represent words in a continuous
space and to estimate the probability distribution as
a smooth function of this representation. Figure 1
provides an overview of this approach. The context
words are first projected in a continuous space using
the shared matrix R. Denoting v the 1-of-V coding
vector of word v (all null except for the vth compo-
nent which is set to 1), its projection vector is the
vth line of R: RTv. The hidden layer h is then
computed as a non-linear function of these vectors.
Finally, the probability of all possible outcomes are
computed using one or several softmax layer(s).
1wji denotes a sequence of tokens wi . . . j when j ? i, or
the empty sequence otherwise.
2
  
0...0100
10...000
0...0010
v-3
v-2
v-1
R
R
R
shared input space
input layer
hidden layers
shortlist
sub-classlayers
wordlayers
classlayer
input part output part
W
Figure 1: 4-gram model with SOUL at the output layer.
This architecture can be divided in two parts, with
the hidden layer in the middle: the input part (on the
left hand side of the graph) which aims at represent-
ing the context of the prediction; and the output part
(on the right hand side) which computes the proba-
bility of all possible successor words given the con-
text. In the remaining of this section, we describe
these two parts in more detail.
2.1 Input Layer Structure
The input part computes a continuous representation
of the context in the form of a context vector h to be
processed through the hidden layer.
2.1.1 N -gram Input Layer
Using the standard n-gram assumption of equa-
tion (2), the context is made up of the sole n?1 pre-
vious words. In a n-gram NNLM, these words are
projected in the shared continuous space and their
representations are then concatenated to form a sin-
gle vector i, as illustrated in the left part of Figure 1:
i = {RTv?(n?1);R
Tv?(n?2); . . . ;R
Tv?1}, (3)
where v?k is the kth previous word. A non-linear
transformation is then applied to compute the first
hidden layer h as follows:
h = sigm (Wi+ b) , (4)
with sigm the sigmoid function. This kind of archi-
tecture will be referred to as a feed-forward NNLM.
Conventional n-gram LMs are usually limited to
small values of n, and using n greater that 4 or 5
does not seem to be of much use. Indeed, previ-
ous experiments using very large speech recognition
systems indicated that the gain obtained by increas-
ing the n-gram order from 4 to 5 is almost negli-
gible, whereas the model size increases drastically.
While using large context seems to be very imprac-
tical with back-off LMs, the situation is quite dif-
ferent for NNLMs due to their specific architecture.
In fact, increasing the context length for a NNLM
mainly implies to expend the projection layer with
one supplementary projection vector, which can fur-
thermore be computed very easily through a sim-
ple look-up operation. The overall complexity of
NNLMs thus only grows linearly with n in the worst
case (Schwenk, 2007).
In order to better investigate the impact of each
context position in the prediction, we introduce a
slight modification of this architecture in a man-
ner analog to the proposal of Collobert and Weston
(2008). In this variation, the computation of the hid-
den layer defined by equation (4) is replaced by:
h = sigm
(
max
k
[
WkR
Tv?k
]
+ b
)
, (5)
where Wk is the sub-matrix of W comprising the
columns related to the kth history word, and the max
is to be understood component-wise. The product
WkRT can then be considered as defining the pro-
jection matrix for the kth position. After the projec-
tion of all the context words, the max function se-
lects, for each dimension l, among the n ? 1 values
([WkRTv?k]l) the most active one, which we also
assume to be the most relevant for the prediction.
2.1.2 Recurrent Layer
Recurrent networks are based on a more complex
architecture designed to recursively handle an arbi-
trary number of context words. Recurrent NNLMs
are described in (Mikolov et al, 2010; Mikolov et
al., 2011b) and are experimentally shown to outper-
form both standard back-off LMs and feed-forward
NNLMs in terms of perplexity on a small task. The
key aspect of this architecture is that the input layer
for predicting the ith word wi in a text contains both
a numeric representation vi?1 of the previous word
and the hidden layer for the previous prediction.
3
The hidden layer thus acts as a representation of the
context history that iteratively accumulates an un-
bounded number of previous words representations.
Our reimplementation of recurrent NNLMs
slightly differs from the feed-forward architecture
mainly by its input part.We use the same deep archi-
tecture to model the relation between the input word
presentations and the input layer as in the recurrent
model. However, we explicitly restrict the context to
the n?1 previous words. Note that this architecture
is just a convenient intermediate model that is used
to efficiently train a recurrent model, as described in
Section 3. In the recurrent model, the input layer is
estimated as a recursive function of both the current
input word and the past input layer.
i = sigm(Wi?1 +RTv?1) (6)
As in the standard model, RTv?k associates each
context word v?k to one feature vector (the corre-
sponding row in R). This vector plays the role of
a bias at subsequent input layers. The input part is
thus structured in a series of layers, the relation be-
tween the input layer and the first previous word be-
ing at level 1, the second previous word is at level 2
and so on. In (Mikolov et al, 2010; Mikolov et al,
2011b), recurrent models make use of the entire con-
text, from the current word position all the way back
to the beginning of the document. This greatly in-
creases the complexity of training, as each document
must be considered as a whole and processed posi-
tion per position. By comparison, our reimplemen-
tation only considers a fixed context length, which
can be increased at will, thus simulating a true recur-
rent architecture; this enables us to take advantage
of several techniques during training that speed up
learning (see Section 3). Furthermore, as discussed
below, our preliminary results show that restricting
the context to the current sentence is sufficient to at-
tain optimal performance 2.
2.2 Structured Output Layer
A major difficulty with the neural network approach
is the complexity of inference and training, which
largely depends on the size of the output vocabu-
2The test sets used in MT experiments are made of various
News extracts. Their content is thus not homogeneous and us-
ing words from previous sentences doesn?t seem to be relevant.
lary ,i.e. of the number of words that have to be pre-
dicted. To overcome this problem, Le et al (2011a)
have proposed the structured Output Layer (SOUL)
architecture. Following (Mnih and Hinton, 2008),
the SOUL model combines the neural network ap-
proach with a class-based LM (Brown et al, 1992).
Structuring the output layer and using word class in-
formation makes the estimation of distribution over
large output vocabulary computationally feasible.
In the SOUL LM, the output vocabulary is struc-
tured in a clustering tree, where every word is asso-
ciated to a unique path from the root node to a leaf
node. Denoting wi the ith word in a sentence, the se-
quence c1:D(wi) = c1, . . . , cD encodes the path for
word wi in this tree, with D the tree depth, cd(wi)
the class or sub-class assigned to wi, and cD(wi) the
leaf associated with wi, comprising just the word it-
self. The probability of wi given its history h can
then be computed as:
P (wi|h) =P (c1(wi)|h)
?
D?
d=2
P (cd(wi)|h, c1:d?1).
(7)
There is a softmax function at each level of the
tree and each word ends up forming its own class
(a leaf). The SOUL architecture is represented in
the right part of Figure 1. The first (class layer)
estimates the class probability P (c1(wi)|h), while
sub-class layers estimate the sub-class probabili-
ties P (cd(wi)|h, c1:d?1), d = 2 . . . (D ? 1). Fi-
nally, the word layer estimates the word probabili-
ties P (cD(wi)|h, c1:D?1). As in (Schwenk, 2007),
words in the short-list remain special, as each of
them represents a (final) class on its own right.
3 Efficiency issues
Training a SOUL model can be achieved by maxi-
mizing the log-likelihood of the parameters on some
training corpus. Following (Bengio et al, 2003),
this optimization is performed by Stochastic Back-
Propagation (SBP). Recurrent models are usually
trained using a variant of SBP called the Back-
Propagation Through Time (BPTT) (Rumelhart et
al., 1986; Mikolov et al, 2011a).
Following (Schwenk, 2007), it is possible to
greatly speed up the training of NNLMs using,
4
for instance, n-gram level resampling and bunch
mode training with parallelization (see below); these
methods can drastically reduce the overall training
time, from weeks to days. Adapting these meth-
ods to recurrent models are not straightforward. The
same goes with the SOUL extension: its training
scheme requires to first consider a restricted output
vocabulary (the shortlist), that is then extended to in-
clude the complete prediction vocabulary (Le et al,
2011b). This technique is too time consuming, in
practice, to be used when training recurrent mod-
els. By bounding the recurrence to a dozen or so
previous words, we obtain a recurrent-like n-gram
model that can benefit from a variety of speed-up
techniques, as explained in the next sections.
Note that the bounded-memory approximation is
only used for training: once training is complete, we
derive a true recurrent network using the parameters
trained on its approximation. This recurrent archi-
tecture is then used for inference.
3.1 Reducing the training data
Our usual approach for training large scale models
is based on n-gram level resampling a subset of the
training data at each epoch. This is not directly com-
patible with the recurrent model, which requires to
iterate over the training data sentence-by-sentence in
the same order as they occur in the document. How-
ever, by restricting the context to sentences, data re-
sampling can be carried out at the sentence level.
This means that the input layer is reinitialized at
the beginning of each sentence so as to ?forget?, as
it were, the memory of the previous sentences. A
similar proposal is made in (Mikolov et al, 2011b),
where the temporal dependencies are limited to the
level of paragraph. Another useful trick, which is
also adopted here, is to use different sampling rates
for the various subparts of the data, thus boosting the
use of in-domain versus out-of-domain data.
3.2 Bunch mode
Bunch mode training processes sentences by batches
of several examples, thus enabling matrix operation
that are performed very efficiently by the existing
BLAS library. After resampling, the training data is
divided into several sentence flows which are pro-
cessed simultaneously. While the number of exam-
ples per batch can be as high as 128 without any
visible loss of performance for n-gram NNLM, we
found, after some preliminary experiments, that the
value of 32 seems to yield a good tradeoff between
the computing time and the performance for recur-
rent models. Using such batches, the training time
can be speeded up by a factor of 8 at the price of a
slight loss (less than 2%) in perplexity.
3.3 SOUL training scheme
The SOUL training scheme integrates several steps
aimed at dealing with the fact that the output vocab-
ulary is split in two sub-parts: very frequent words
are in the so-called short-list and are treated differ-
ently from the less frequent ones. This setting can
not be easily reproduced with recurrent models. By
contrast, using the pseudo-recurrent n-gram NNLM,
the SOUL training scheme can be adopted; the re-
sulting parameter values are then plugged in into a
truly recurrent architecture. In the light of the results
reported below, we content ourselves with values of
n in the range 8-10.
4 Experimental Results
We now turn to the experimental part, starting with a
description of the experimental setup. We will then
present an attempt to quantify the relative impor-
tance of history words, followed by a head to head
comparison of the various NNLM architectures dis-
cussed in the previous sections.
4.1 Experimental setup
The tasks considered in our experiments are derived
from the shared translation track of WMT 2011
(translation from English to French). We only pro-
vide here a short overview of the task; all the neces-
sary details regarding this evaluation campaign are
available on the official Web site3 and our system
is described in (Allauzen et al, 2011). Simply note
that our parallel training data includes a large Web
corpus, referred to as the GigaWord parallel cor-
pus. After various preprocessing and filtering steps,
the total amount of training data is approximately
12 million sentence pairs for the bilingual part, and
about 2.5 billion of words for the monolingual part.
To built the target language models, the mono-
lingual corpus was first split into several sub-parts
3http://www.statmt.org/wmt11
5
based on date and genre information. For each of
these sub-corpora, a standard 4-gram LM was then
estimated with interpolated Kneser-Ney smoothing
(Chen and Goodman, 1996). All models were cre-
ated without any pruning nor cutoff. The baseline
back-off n-gram LM was finally built as a linear
combination of several these models, where the in-
terpolation coefficients are chosen so as to minimize
the perplexity of a development set.
All NNLMs are trained following the prescrip-
tions of Le et al (2011b), and they all share the
same inner structure: the dimension of the projec-
tion word space is 500; the size of two hidden lay-
ers are respectively 1000 and 500; the short-list con-
tains 2000 words; and the non-linearity is introduced
with the sigmoid function. For the recurrent model,
the parameter that limits the back-propagation of er-
rors through time is set to 9 (see (Mikolov et al,
2010) for details). This parameter can be considered
to play a role that is similar to the history size in
our pseudo-recurrent n-gram model: a value of 9 in
the recurrent setting is equivalent to n = 10. All
NNLMs are trained with the following resampling
strategy: 75% of in-domain data (monolingual News
data 2008-2011) and 25% of the other data. At each
epoch, the parameters are updated using approxi-
mately 50 millions words for the last training step
and about 140 millions words for the previous ones.
4.2 The usefulness of remote words
In this section, we analyze the influence of each con-
text word with respect to their distance from the pre-
dicted word and to their POS tag. The quantitative
analysis relies on the variant of the n-gram architec-
ture based on (5) (see Section 2.1), which enables
us to keep track of the most important context word
for each prediction. Throughout this study, we will
consider 10-gram NNLMs.
Figure 2 represents the selection rate with respect
to the word position and displays the percentage of
coordinates in the input layer that are selected for
each position. As expected, close words are the most
important, with the previous word accounting for
more than 35% of the components. Remote words
(at a distance between 7 and 9) have almost the
same, weak, influence, with a selection rate close to
2.5%. This is consistent with the perplexity results
of n-gram NNLMs as a function of n, reported in
Tag Meaning Example
ABR abreviation etc FC FMI
ABK other abreviation ONG BCE CE
ADJ adjective officielles alimentaire mondial
ADV adverb contrairement assez alors
DET article; une les la
possessive pronoun ma ta
INT interjection oui adieu tic-tac
KON conjunction que et comme
NAM proper name Javier Mercure Pauline
NOM noun surprise inflation crise
NUM numeral deux cent premier
PRO pronoun cette il je
PRP preposition; de en dans
preposition plus article au du aux des
PUN punctuation; : , -
punctuation citation ?
SENT sentence tag ? . !
SYM symbol %
VER verb ont fasse parlent
<s> start of sentence
Table 1: List of grouped tags from TreeTagger.
Table 2: the difference between all orders from 4-
gram to 8-gram are significant, while the difference
between 8-gram and 10-gram is negligible.
POS tags were computed using the TreeTag-
ger (Schmid, 1994); sub-types of a main tag are
pooled to reduce the total number of categories. For
example, all the tags for verbs are merged into the
same VER class. Adding the token <s> (sentence
start), our tagset contains 17 tags (see Table 1).
The average selection rates for each tag are shown
in Figure 3: for each category, we display (in bars)
the average number of components that correspond
to a word in that category when this word is in pre-
vious position. Rare tags (INT, ABK , ABR and
SENT) seem to provide a very useful information
and have very high selection rates. Conversely, DET,
PUN and PRP words occur relatively frequently and
belong to the less selective group. The two most
frequent tags (NOM and VER ) have a medium se-
lection rate (approximately 0.5).
4.3 Translation experiments
The integration of NNLMs for large SMT tasks is
far from easy, given the computational cost of com-
puting n-gram probabilities, a task that is performed
repeatedly during the search of the best translation.
Our solution was to resort to a two-pass approach:
the first pass uses a conventional back-off n-gram
model to produce a list of the k most likely trans-
lations; in the second pass, the NNLMs probability
6
1 2 3 4 5 6 7 8 90.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
Figure 2: Average selection rate per word position for the
max-based NNLM, computed on newstest2009-2011. On
x axis, the number k represents the kth previous word.
0 5 10 150.0
0.2
0.4
0.6
0.8
1.0
PUN DET SYM PRP NUM KON ADV SENT PRO VER <s> ADJ NOM ABR NAM ABK INT
Figure 3: Average selection rate of max function of the
first previous word in terms of word POS-tag information,
computed on newstest2009-2011. The green line repre-
sents the distribution of occurrences of each tag.
of each hypothesis is computed and the k-best list is
accordingly reordered. The NNLM weights are op-
timized as the other feature weights using Minimum
Error Rate Training (MERT) (Och, 2003). For all
our experiments, we used the value k = 300.
To clarify the impact of the language model or-
der in translation performance, we considered three
different ways to use NNLMs. In the first setting,
the NNLM is used alone and all the scores provided
by the MT system are ignored. In the second set-
ting (replace), the NNLM score replaces the score
of the standard back-off LM. Finally, the score of
the NNLM can be added in the linear combination
(add). In the last two settings, the weights used for
Model Perplexity BLEU
alone replace add
Baseline 90 29.4 31.3 -
4-gram 92 29.8 31.1 31.5
6-gram 82 30.2 31.6 31.8
8-gram 78 30.6 31.6 31.8
10-gram 77 30.5 31.7 31.8
recurrent 81 30.4 31.6 31.8
Table 2: Results for the English to French task obtained
with the baseline system and with various NNLMs. Per-
plexity is computed on newstest2009-2011 while BLEU is
on the test set (newstest2010).
n-best reranking are re-tuned with MERT.
Table 2 summarizes the BLEU scores obtained on
the newstest2010 test set. BLEU improvements are
observed with feed-forward NNLMs using a value
of n = 8 with respect to the baseline (n = 4).
Further increase from 8 to 10 only provides a very
small BLEU improvement. These results strengthen
the assumption made in Section 3.3: there seem to
be very little information in remote words (above
n = 7-8). It is also interesting to see that the 4-gram
NNLM achieves a comparable perplexity to the con-
ventional 4-gram model, yet delivers a small BLEU
increase in the alone condition.
Surprisingly4, on this task, recurrent models seem
to be comparable with 8-gram NNLMs. The rea-
son may be the deep architecture of recurrent model
that makes it hard to be trained in a large scale task.
With the recurrent-like n-gram model described in
Section 2.1.2, it is feasible to train a recurrent model
on a large task. With 10% of perplexity reduction as
compared to a backoff model, its yields comparable
performances as reported in (Mikolov et al, 2011a).
To the best of our knowledge, it is the first recurrent
NNLM trained on a such large dataset (2.5 billion
words) in a reasonable time (about 11 days).
5 Related work
There have been many attempts to increase the
context beyond a couple of history words (see eg.
(Rosenfeld, 2000)), for example: by modeling syn-
4Pers. com. with T. Mikolov: on the ?small? WSJ data
set, the recurrent model described in (Mikolov et al, 2011b)
outperforms the 10-gram NNLM.
7
tactic information, that better reflects the ?distance?
between words (Chelba and Jelinek, 2000; Collins
et al, 2005; Schwartz et al, 2011); with a unigram
model of the whole history (Kuhn and Mori, 1990);
by using trigger models (Lau et al, 1993); or by try-
ing to model document topics (Seymore and Rosen-
feld, 1997). One interesting proposal avoids the n-
gram assumption by estimating the probability of a
sentence (Rosenfeld et al, 2001). This approach
relies on a maximum entropy model which incor-
porates arbitrary features. No significant improve-
ments were however observed with this model, a fact
that can be attributed to two main causes: first, the
partition function can not be computed exactly as it
involves a sum over all the possible sentences; sec-
ond, it seems that data sparsity issues for this model
are also adversely affecting the performance.
The recurrent network architecture for LMs was
proposed in (Mikolov et al, 2010) and then ex-
tended in (Mikolov et al, 2011b). The authors pro-
pose a hierarchical architecture similar to the SOUL
model, based however on a simple unigram clus-
tering. For large scale tasks (? 400M training
words), advanced training strategies were investi-
gated in (Mikolov et al, 2011a). Instead of resam-
pling, the data was divided into paragraphs, filtered
and then sorted: the most in-domain data was thus
placed at the end of each epoch. On the other hand,
the hidden layer size was decreased by simulating a
maximum entropy model using a hash function on
n-grams. This part represents direct connections be-
tween input and output layers. By sharing the pre-
diction task, the work of the hidden layer is made
simpler, and can thus be handled with a smaller
number of hidden units. This approach reintroduces
into the model discrete features which are somehow
one main weakness of conventional backoff LMs as
compared to NNLMs. In fact, this strategy can be
viewed as an effort to directly combine the two ap-
proaches (backoff-model and neural network), in-
stead of using a traditional way, through interpola-
tion. Training simultaneously two different models
is computationally very demanding for large vocab-
ularies, even with help of hashing technique; in com-
parison, our approach keeps the model architecture
simple, making it possible to use the efficient tech-
niques developed for n-gram NNLMs.
The use the max, rather than a sum, on the hid-
den layer of neural network is not new. Within the
context of language modeling, it was first proposed
in (Collobert et al, 2011) with the goal to model a
variable number of input features. Our motivation
for using this variant was different, and was mostly
aimed at analyzing the influence of context words
based on the selection rates of this function.
6 Conclusion
In this paper, we have investigated several types
of NNLMs, along with conventional LMs, in or-
der to assess the influence of long range dependen-
cies within sentences in the language modeling task:
from recurrent models that can recursively handle
an arbitrary number of context words to n-gram
NNLMs with n varying between 4 and 10. Our con-
tribution is two-fold.
First, experimental results showed that the influ-
ence of word further than 9 can be neglected for the
statistical machine translation task 5. Therefore, the
n-gram assumption with n ? 10 appears to be well-
founded to handle most sentence internal dependen-
cies. Another interesting conclusion of this study
is that the main issue of the conventional n-gram
model is not its conditional independence assump-
tions, but the use of too small values for n.
Second, by restricting the context of recurrent net-
works, the model can benefit of the advanced train-
ing schemes and its training time can be divided by
a factor 8 without loss on the performances. To the
best of our knowledge, it is the first time that a re-
current NNLM is trained on a such large dataset in
a reasonable time. Finally, we compared these mod-
els within a large scale MT task, with monolingual
data that contains 2.5 billion words. Experimental
results showed that using long range dependencies
(n = 10) with a SOUL language model significantly
outperforms conventional LMs. In this setting, the
use of a recurrent architecture does not yield any im-
provements, both in terms of perplexity and BLEU.
Acknowledgments
This work was achieved as part of the Quaero Pro-
gramme, funded by OSEO, the French State agency
for innovation.
5The same trend is observed in speech recognition.
8
References
Alexandre Allauzen, Gilles Adda, He?le`ne Bonneau-
Maynard, Josep M. Crego, Hai-Son Le, Aure?lien Max,
Adrien Lardilleux, Thomas Lavergne, Artem Sokolov,
Guillaume Wisniewski, and Franc?ois Yvon. 2011.
LIMSI @ WMT11. In Proceedings of the Sixth Work-
shop on Statistical Machine Translation, pages 309?
315, Edinburgh, Scotland.
Y Bengio, R Ducharme, P Vincent, and C Jauvin. 2003.
A neural probabilistic language model. Journal of Ma-
chine Learning Research, 3(6):1137?1155.
Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och,
and Jeffrey Dean. 2007. Large language models in
machine translation. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 858?867.
Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vin-
cent J. Della Pietra, and Jenifer C. Lai. 1992. Class-
based n-gram models of natural language. Comput.
Linguist., 18(4):467?479.
Ciprian Chelba and Frederick Jelinek. 2000. Structured
language modeling. Computer Speech and Language,
14(4):283?332.
Stanley F. Chen and Joshua Goodman. 1996. An empiri-
cal study of smoothing techniques for language model-
ing. In Proc. ACL?96, pages 310?318, San Francisco.
Michael Collins, Brian Roark, and Murat Saraclar.
2005. Discriminative syntactic language modeling for
speech recognition. In Proceedings of the 43rd Annual
Meeting of the Association for Computational Linguis-
tics (ACL?05), pages 507?514, Ann Arbor, Michigan,
June. Association for Computational Linguistics.
Ronan Collobert and Jason Weston. 2008. A uni-
fied architecture for natural language processing: deep
neural networks with multitask learning. In Proc.
of ICML?08, pages 160?167, New York, NY, USA.
ACM.
Ronan Collobert, Jason Weston, Le?on Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011.
Natural language processing (almost) from scratch.
Journal of Machine Learning Research, 12:2493?
2537.
Ahmad Emami, Imed Zitouni, and Lidia Mangu. 2008.
Rich morphology based n-gram language models for
arabic. In INTERSPEECH, pages 829?832.
R. Kuhn and R. De Mori. 1990. A cache-based natural
language model for speech recognition. IEEE Trans-
actions on Pattern Analysis and Machine Intelligence,
12(6):570?583, june.
Hong-Kwang Kuo, Lidia Mangu, Ahmad Emami, and
Imed Zitouni. 2010. Morphological and syntactic fea-
tures for arabic speech recognition. In Proc. ICASSP
2010.
Raymond Lau, Ronald Rosenfeld, and Salim Roukos.
1993. Adaptive language modeling using the maxi-
mum entropy principle. In Proc HLT?93, pages 108?
113, Princeton, New Jersey.
Hai-Son Le, Ilya Oparin, Alexandre Allauzen, Jean-Luc
Gauvain, and Franc?ois Yvon. 2011a. Structured out-
put layer neural network language model. In Proceed-
ings of ICASSP?11, pages 5524?5527.
Hai-Son Le, Ilya Oparin, Abdel. Messaoudi, Alexan-
dre Allauzen, Jean-Luc Gauvain, and Franc?ois Yvon.
2011b. Large vocabulary SOUL neural network lan-
guage models. In Proceedings of InterSpeech 2011.
Xunying Liu, Mark J. F. Gales, and Philip C. Woodland.
2011. Improving lvcsr system combination using neu-
ral network language model cross adaptation. In IN-
TERSPEECH, pages 2857?2860.
Toma?s? Mikolov, Martin Karafia?t, Luka?s? Burget, Jan
C?ernocky?, and Sanjeev Khudanpur. 2010. Recurrent
neural network based language model. In Proceedings
of the 11th Annual Conference of the International
Speech Communication Association (INTERSPEECH
2010), volume 2010, pages 1045?1048. International
Speech Communication Association.
Toma?s? Mikolov, Anoop Deoras, Daniel Povey, Luka?s?
Burget, and Jan C?ernocky?. 2011a. Strategies for train-
ing large scale neural network language models. In
Proceedings of ASRU 2011, pages 196?201. IEEE Sig-
nal Processing Society.
Toma?s? Mikolov, Stefan Kombrink, Lukas Burget, Jan
Cernocky?, and Sanjeev Khudanpur. 2011b. Exten-
sions of recurrent neural network language model. In
Proc. of ICASSP?11, pages 5528?5531.
Andriy Mnih and Geoffrey E Hinton. 2008. A scalable
hierarchical distributed language model. In D. Koller,
D. Schuurmans, Y. Bengio, and L. Bottou, editors, Ad-
vances in Neural Information Processing Systems 21,
volume 21, pages 1081?1088.
Franz Josef Och. 2003. Minimum error rate training
in statistical machine translation. In Proceedings of
the 41st Annual Meeting on Association for Compu-
tational Linguistics - Volume 1, ACL ?03, pages 160?
167, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
Ronald Rosenfeld, Stanley F. Chen, and Xiaojin Zhu.
2001. Whole-sentence exponential language models:
A vehicle for linguistic-statistical integration. Com-
puters, Speech and Language, 15:2001.
R. Rosenfeld. 2000. Two decades of statistical language
modeling: Where do we go from here ? Proceedings
of the IEEE, 88(8).
D. E. Rumelhart, G. E. Hinton, and R. J. Williams. 1986.
Parallel distributed processing: explorations in the mi-
crostructure of cognition, vol. 1. chapter Learning
9
internal representations by error propagation, pages
318?362. MIT Press, Cambridge, MA, USA.
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In Proceedings of Interna-
tional Conference on New Methods in Language Pro-
cessing.
Lane Schwartz, Chris Callison-Burch, William Schuler,
and Stephen Wu. 2011. Incremental syntactic lan-
guage models for phrase-based translation. In Pro-
ceedings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Language
Technologies, pages 620?631, Portland, Oregon, USA,
June. Association for Computational Linguistics.
Holger Schwenk and Jean-Luc Gauvain. 2002. Connec-
tionist language modeling for large vocabulary contin-
uous speech recognition. In Proc. ICASSP, pages 765?
768, Orlando, FL.
H. Schwenk and P. Koehn. 2008. Large and diverse lan-
guage models for statistical machine translation. In
International Joint Conference on Natural Language
Processing, pages 661?666, Janv 2008.
Holger Schwenk. 2007. Continuous space language
models. Comput. Speech Lang., 21(3):492?518.
Kristie Seymore and Ronald Rosenfeld. 1997. Using
story topics for language model adaptation. In Proc. of
Eurospeech ?97, pages 1987?1990, Rhodes, Greece.
Yeh W. Teh. 2006. A hierarchical Bayesian language
model based on Pitman-Yor processes. In Proc. of
ACL?06, pages 985?992, Sidney, Australia.
10
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 322?329,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
Joint WMT 2012 Submission of the QUAERO Project
?Markus Freitag, ?Stephan Peitz, ?Matthias Huck, ?Hermann Ney,
?Jan Niehues, ?Teresa Herrmann, ?Alex Waibel,
?Le Hai-son, ?Thomas Lavergne, ?Alexandre Allauzen,
?Bianka Buschbeck, ?Josep Maria Crego, ?Jean Senellart
?RWTH Aachen University, Aachen, Germany
?Karlsruhe Institute of Technology, Karlsruhe, Germany
?LIMSI-CNRS, Orsay, France
?SYSTRAN Software, Inc.
?surname@cs.rwth-aachen.de
?firstname.surname@kit.edu
?firstname.lastname@limsi.fr ?surname@systran.fr
Abstract
This paper describes the joint QUAERO sub-
mission to the WMT 2012 machine transla-
tion evaluation. Four groups (RWTH Aachen
University, Karlsruhe Institute of Technol-
ogy, LIMSI-CNRS, and SYSTRAN) of the
QUAERO project submitted a joint translation
for the WMT German?English task. Each
group translated the data sets with their own
systems and finally the RWTH system combi-
nation combined these translations in our final
submission. Experimental results show im-
provements of up to 1.7 points in BLEU and
3.4 points in TER compared to the best single
system.
1 Introduction
QUAERO is a European research and develop-
ment program with the goal of developing multi-
media and multilingual indexing and management
tools for professional and general public applica-
tions (http://www.quaero.org). Research in machine
translation is mainly assigned to the four groups
participating in this joint submission. The aim of
this WMT submission was to show the quality of a
joint translation by combining the knowledge of the
four project partners. Each group develop and main-
tain their own different machine translation system.
These single systems differ not only in their general
approach, but also in the preprocessing of training
and test data. To take the advantage of these dif-
ferences of each translation system, we combined
all hypotheses of the different systems, using the
RWTH system combination approach.
This paper is structured as follows. In Section
2, the different engines of all four groups are in-
troduced. In Section 3, the RWTH Aachen system
combination approach is presented. Experiments
with different system selections for system combi-
nation are described in Section 4. Finally in Section
5, we discuss the results.
2 Translation Systems
For WMT 2012 each QUAERO partner trained their
systems on the parallel Europarl and News Com-
mentary corpora. All single systems were tuned
on the newstest2009 or newstest2010 development
set. The newstest2011 dev set was used to train
the system combination parameters. Finally, the
newstest2008-newstest2010 dev sets were used to
compare the results of the different system combina-
tion settings. In this Section all four different system
engines are presented.
2.1 RWTH Aachen Single Systems
For the WMT 2012 evaluation the RWTH utilized
RWTH?s state-of-the-art phrase-based and hierar-
chical translation systems. GIZA++ (Och and Ney,
2003) was employed to train word alignments, lan-
guage models have been created with the SRILM
toolkit (Stolcke, 2002).
2.1.1 Phrase-Based System
The phrase-based translation (PBT) system is
similar to the one described in Zens and Ney (2008).
After phrase pair extraction from the word-aligned
parallel corpus, the translation probabilities are esti-
mated by relative frequencies. The standard feature
322
set alo includes an n-gram language model, phrase-
level IBM-1 and word-, phrase- and distortion-
penalties, which are combined in log-linear fash-
ion. The model weights are optimized with standard
Mert (Och, 2003) on 200-best lists. The optimiza-
tion criterium is BLEU.
2.1.2 Hierarchical System
For the hierarchical setups (HPBT) described in
this paper, the open source Jane toolkit (Vilar et
al., 2010) is employed. Jane has been developed at
RWTH and implements the hierarchical approach as
introduced by Chiang (2007) with some state-of-the-
art extensions. In hierarchical phrase-based transla-
tion, a weighted synchronous context-free grammar
is induced from parallel text. In addition to contigu-
ous lexical phrases, hierarchical phrases with up to
two gaps are extracted. The search is typically car-
ried out using the cube pruning algorithm (Huang
and Chiang, 2007). The model weights are opti-
mized with standard Mert (Och, 2003) on 100-best
lists. The optimization criterium is 4BLEU ?TER.
2.1.3 Preprocessing
In order to reduce the source vocabulary size
translation, the German text was preprocessed
by splitting German compound words with the
frequency-based method described in (Koehn and
Knight, 2003a). To further reduce translation com-
plexity for the phrase-based approach, we performed
the long-range part-of-speech based reordering rules
proposed by (Popovic? et al, 2006).
2.1.4 Language Model
For both decoders a 4-gram language model is ap-
plied. The language model is trained on the par-
allel data as well as the provided News crawl, the
109 French-English, UN and LDC Gigaword Fourth
Edition corpora. For the 109 French-English, UN
and LDC Gigaword corpora RWTH applied the data
selection technique described in (Moore and Lewis,
2010).
2.2 Karlsruhe Institute of Technology Single
System
2.2.1 Preprocessing
We preprocess the training data prior to training
the system, first by normalizing symbols such as
quotes, dashes and apostrophes. Then smart-casing
of the first words of each sentence is performed. For
the German part of the training corpus we use the
hunspell1 lexicon to learn a mapping from old Ger-
man spelling to new German spelling to obtain a cor-
pus with homogenous spelling. In addition, we per-
form compound splitting as described in (Koehn and
Knight, 2003b). Finally, we remove very long sen-
tences, empty lines, and sentences that probably are
not parallel due to length mismatch.
2.2.2 System Overview
The KIT system uses an in-house phrase-based
decoder (Vogel, 2003) to perform translation and op-
timization with regard to the BLEU score is done us-
ing Minimum Error Rate Training as described in
Venugopal et al (2005).
2.2.3 Translation Models
The translation model is trained on the Europarl
and News Commentary Corpus and the phrase ta-
ble is based on a discriminative word alignment
(Niehues and Vogel, 2008).
In addition, the system applies a bilingual lan-
guage model (Niehues et al, 2011) to extend the
context of source language words available for trans-
lation.
Furthermore, we use a discriminative word lexi-
con as introduced in (Mauser et al, 2009). The lex-
icon was trained and integrated into our system as
described in (Mediani et al, 2011).
At last, we tried to find translations for
out-of-vocabulary (OOV) words by using quasi-
morphological operations as described in Niehues
and Waibel (2011). For each OOV word, we try to
find a related word that we can translate. We modify
the ending letters of the OOV word and learn quasi-
morphological operations to be performed on the
known translation of the related word to synthesize
a translation for the OOV word. By this approach
we were for example able to translate Kaminen into
chimneys using the known translation Kamin # chim-
ney.
2.2.4 Language Models
We use two 4-gram SRI language models, one
trained on the News Shuffle corpus and one trained
1http://hunspell.sourceforge.net/
323
on the Gigaword corpus. Furthermore, we use a 5-
gram cluster-based language model trained on the
News Shuffle corpus. The word clusters were cre-
ated using the MKCLS algorithm. We used 100
word clusters.
2.2.5 Reordering Model
Reordering is performed based on part-of-speech
tags obtained using the TreeTagger (Schmid, 1994).
Based on these tags we learn probabilistic continu-
ous (Rottmann and Vogel, 2007) and discontinuous
(Niehues and Kolss, 2009) rules to cover short and
long-range reorderings. The rules are learned from
the training corpus and the alignment. In addition,
we learned tree-based reordering rules. Therefore,
the training corpus was parsed by the Stanford parser
(Rafferty and Manning, 2008). The tree-based rules
consist of the head node of a subtree and all its
children as well as the new order and a probability.
These rules were applied recursively. The reordering
rules are applied to the source sentences and the re-
ordered sentence variants as well as the original se-
quence are encoded in a word lattice which is used
as input to the decoder. For the test sentences, the
reordering based on parts-of-speech and trees allows
us to change the word order in the source sentence
so that the sentence can be translated more easily.
In addition, we build reordering lattices for all train-
ing sentences and then extract phrase pairs from the
monotone source path as well as from the reordered
paths.
2.3 LIMSI-CNRS Single System
LIMSI?s system is built with n-code (Crego et al,
2011), an open source statistical machine translation
system based on bilingual n-gram2. In this approach,
the translation model relies on a specific decomposi-
tion of the joint probability of a sentence pair P(s, t)
using the n-gram assumption: a sentence pair is de-
composed into a sequence of bilingual units called
tuples, defining a joint segmentation of the source
and target. In the approach of (Marin?o et al, 2006),
this segmentation is a by-product of source reorder-
ing which ultimately derives from initial word and
phrase alignments.
2http://ncode.limsi.fr/
2.3.1 An Overview of n-code
The baseline translation model is implemented as
a stochastic finite-state transducer trained using a
n-gram model of (source,target) pairs (Casacuberta
and Vidal, 2004). Training this model requires to
reorder source sentences so as to match the target
word order. This is performed by a stochastic finite-
state reordering model, which uses part-of-speech
information3 to generalize reordering patterns be-
yond lexical regularities.
In addition to the translation model, eleven fea-
ture functions are combined: a target-language
model; four lexicon models; two lexicalized reorder-
ing models (Tillmann, 2004) aiming at predicting
the orientation of the next translation unit; a ?weak?
distance-based distortion model; and finally a word-
bonus model and a tuple-bonus model which com-
pensate for the system preference for short transla-
tions. The four lexicon models are similar to the ones
used in a standard phrase based system: two scores
correspond to the relative frequencies of the tuples
and two lexical weights estimated from the automat-
ically generated word alignments. The weights asso-
ciated to feature functions are optimally combined
using a discriminative training framework (Och,
2003), using the newstest2009 development set.
The overall search is based on a beam-search
strategy on top of a dynamic programming algo-
rithm. Reordering hypotheses are computed in a
preprocessing step, making use of reordering rules
built from the word reorderings introduced in the tu-
ple extraction process. The resulting reordering hy-
potheses are passed to the decoder in the form of
word lattices (Crego and Marin?o, 2007).
2.3.2 Continuous Space Translation Models
One critical issue with standard n-gram transla-
tion models is that the elementary units are bilingual
pairs, which means that the underlying vocabulary
can be quite large. Unfortunately, the parallel data
available to train these models are typically smaller
than the corresponding monolingual corpora used to
train target language models. It is very likely then,
that such models should face severe estimation prob-
lems. In such setting, using neural network language
3Part-of-speech labels for English and German are com-
puted using the TreeTagger (Schmid, 1995).
324
model techniques seem all the more appropriate. For
this study, we follow the recommendations of Le et
al. (2012), who propose to factor the joint proba-
bility of a sentence pair by decomposing tuples in
two (source and target) parts, and further each part
in words. This yields a word factored translation
model that can be estimated in a continuous space
using the SOUL architecture (Le et al, 2011).
The design and integration of a SOUL model for
large SMT tasks is far from easy, given the computa-
tional cost of computing n-gram probabilities. The
solution used here was to resort to a two pass ap-
proach: the first pass uses a conventional back-off
n-gram model to produce a k-best list; in the second
pass, the k-best list is reordered using the probabil-
ities of m-gram SOUL translation models. In the
following experiments, we used a fixed context size
for SOUL of m = 10, and used k = 300.
2.3.3 Corpora and Data Preprocessing
The parallel data is word-aligned using
MGIZA++4 with default settings. For the En-
glish monolingual training data, we used the same
setup as last year5 and thus the same target language
model as detailed in (Allauzen et al, 2011).
For English, we took advantage of our in-house
text processing tools for tokenization and detok-
enization steps (De?chelotte et al, 2008) and our sys-
tem was built in ?true-case?. As German is mor-
phologically more complex than English, the default
policy which consists in treating each word form
independently is plagued with data sparsity, which
is detrimental both at training and decoding time.
Thus, the German side was normalized using a spe-
cific pre-processing scheme (Allauzen et al, 2010;
Durgar El-Kahlout and Yvon, 2010), which notably
aims at reducing the lexical redundancy by (i) nor-
malizing the orthography, (ii) neutralizing most in-
flections and (iii) splitting complex compounds.
2.4 SYSTRAN Software, Inc. Single System
The data submitted by SYSTRAN were obtained by
a system composed of the standard SYSTRAN MT
engine in combination with a statistical post editing
(SPE) component.
4http://geek.kyloo.net/software
5The fifth edition of the English Gigaword (LDC2011T07)
was not used.
The SYSTRAN system is traditionally classi-
fied as a rule-based system. However, over the
decades, its development has always been driven by
pragmatic considerations, progressively integrating
many of the most efficient MT approaches and tech-
niques. Nowadays, the baseline engine can be con-
sidered as a linguistic-oriented system making use of
dependency analysis, general transfer rules as well
as of large manually encoded dictionaries (100k -
800k entries per language pair).
The SYSTRAN phrase-based SPE component
views the output of the rule-based system as the
source language, and the (human) reference trans-
lation as the target language, see (L. Dugast and
Koehn, 2007). It performs corrections and adaptions
learned from the 5-gram language model trained on
the parallel target-to-target corpus. Moreover, the
following measures - limiting unwanted statistical
effects - were applied:
? Named entities, time and numeric expressions
are replaced by special tokens on both sides.
This usually improves word alignment, since
the vocabulary size is significantly reduced. In
addition, entity translation is handled more re-
liably by the rule-based engine.
? The intersection of both vocabularies (i.e. vo-
cabularies of the rule-based output and the ref-
erence translation) is used to produce an addi-
tional parallel corpus to help to improve word
alignment.
? Singleton phrase pairs are deleted from the
phrase table to avoid overfitting.
? Phrase pairs not containing the same number
of entities on the source and the target side are
also discarded.
The SPE language model was trained on 2M bilin-
gual phrases from the news/Europarl corpora, pro-
vided as training data for WMT 2012. An addi-
tional language model built from 15M phrases of
the English LDC Gigaword corpus using Kneser-
Ney (Kneser and Ney, 1995) smoothing was added.
Weights for these separate models were tuned by
the Mert algorithm provided in the Moses toolkit
(P. Koehn et al, 2007), using the provided news de-
velopment set.
325
3 RWTH Aachen System Combination
System combination is used to produce consensus
translations from multiple hypotheses produced with
different translation engines that are better in terms
of translation quality than any of the individual hy-
potheses. The basic concept of RWTH?s approach
to machine translation system combination has been
described by Matusov et al (2006; 2008). This ap-
proach includes an enhanced alignment and reorder-
ing framework. A lattice is built from the input hy-
potheses. The translation with the best score within
the lattice according to a couple of statistical models
is selected as consensus translation.
4 Experiments
This year, we tried different sets of single systems
for system combination. As RWTH has two dif-
ferent translation systems, we put the output of
both systems into system combination. Although
both systems have the same preprocessing and lan-
guage model, their hypotheses differ because of
their different decoding approach. Compared to
the other systems, the system by SYSTRAN has a
completely different approach (see section 2.4). It
is mainly based on a rule-based system. For the
German?English pair, SYSTRAN achieves a lower
BLEU score in each test set compared to the other
groups. However, since the SYSTRAN system is
very different to the others, we still obtain an im-
provement when we add it also to system combina-
tion.
We did experiments with different optimization
criteria for the system combination optimization.
All results are listed in Table 1 (unoptimized), Table
2 (optimized on BLEU) and Table 3 (optimized on
TER-BLEU). Further, we investigated, whether we
will loose performance, if a single system is dropped
from the system combination. The results show that
for each optimization criteria we need all systems to
achieve the best results.
For the BLEU optimized system combination, we
obtain an improvement compared to the best sin-
gle systems for all dev sets. For newstest2008, we
get an improvement of 1.5 points in BLEU and 1.5
points in TER compared to the best single system of
Karlsruhe Institute of Technology. For newstest2009
we get an improvement of 1.9 points in BLEU and
1.5 points in TER compared to the best single sys-
tem. The system combination of all systems outper-
forms the best single system with 1.9 points in BLEU
and 1.9 points in TER for newstest2010. For new-
stest2011 the improvement is 1.3 points in BLEU
and 2.9 points in TER.
For the TER-BLEU optimized system combina-
tion, we achieved more improvement in TER com-
pared to the BLEU optimized system combination.
For newstest2008, we get an improvement of 0.8
points in BLEU and 3.0 points in TER compared to
the best single system of Karlsruhe Institute of Tech-
nology. The system combinations performs better
on newstest2009 with 1.3 points in BLEU and 2.7
points in TER. For newstest2010, we get an im-
provement of 1.7 points in BLEU and 3.4 points in
TER and for newstest2011 we get an improvement
of 0.7 points in BLEU and 2.5 points in TER.
5 Conclusion
The four statistical machine translation systems of
Karlsruhe Institute of Technology, RWTH Aachen
and LIMSI and the very structural approach of SYS-
TRAN produce hypotheses with a huge variability
compared to the others. Finally, the RWTH Aachen
system combination combined all single system hy-
potheses to one hypothesis with a higher BLEU and
a lower TER score compared to each single sys-
tem. For each optimization criteria the system com-
binations using all single systems outperforms the
system combinations using one less single system.
Although the single system of SYSTRAN has the
worst error scores and the RWTH single systems are
similar, we achieved the best result in using all single
systems. For the WMT 12 evaluation, we submitted
the system combination of all systems optimized on
BLEU.
Acknowledgments
This work was achieved as part of the Quaero Pro-
gramme, funded by OSEO, French State agency for
innovation.
References
Alexandre Allauzen, Josep M. Crego, I?lknur Durgar El-
Kahlout, and Francois Yvon. 2010. LIMSI?s statis-
tical translation systems for WMT?10. In Proc. of the
326
Table 1: All systems for the WMT 2012 German?English translation task (truecase). BLEU and TER results are in
percentage. sc denotes system combination. All system combinations are unoptimized.
system newstest2008 newstest2009 newstest2010 newstest2011
BLEU TER BLEU TER BLEU TER BLEU TER TER-BLEU
KIT 22.2 61.8 21.3 61.0 24.1 59.0 22.4 60.2 37.9
RWTH.PBT 21.4 62.0 21.3 61.1 23.9 59.1 21.4 61.2 39.7
Limsi 22.2 63.0 22.0 61.8 23.9 59.9 21.8 62.0 40.2
RWTH.HPBT 21.5 62.6 21.5 61.6 23.6 60.2 21.5 61.8 40.4
SYSTRAN 18.3 64.6 17.9 63.4 21.1 60.5 18.3 63.1 44.8
sc-withAllSystems 23.4 59.7 22.9 59.0 26.2 56.5 23.3 58.8 35.5
sc-without-RWTH.PBT 23.2 59.8 22.8 59.0 25.9 56.6 23.1 58.7 35.6
sc-without-RWTH.HPBT 23.2 59.6 22.7 58.9 26.1 56.2 23.1 58.7 35.6
sc-without-Limsi 22.7 60.1 22.4 59.2 25.5 56.7 22.8 58.8 36.0
sc-without-SYSTRAN 23.0 60.3 22.5 59.5 25.7 57.2 23.1 59.2 36.1
sc-without-KIT 23.0 59.9 22.5 59.1 25.9 56.6 22.9 59.1 36.3
Table 2: All systems for the WMT 2012 German?English translation task (truecase). BLEU and TER results are in
percentage. sc denotes system combination. All system combinations are optimized on BLEU .
system newstest2008 newstest2009 newstest2010 newstest2011
BLEU TER BLEU TER BLEU TER BLEU TER TER-BLEU
sc-withAllSystems 23.7 60.3 23.2 59.5 26.0 57.1 23.7 59.2 35.6
sc-without-RWTH.PBT 23.4 61.1 23.1 59.8 25.5 57.6 23.5 59.5 36.1
sc-without-SYSTRAN 23.3 61.1 22.6 60.5 25.3 58.1 23.5 60.0 36.5
sc-without-Limsi 23.1 60.7 22.6 59.7 25.4 57.5 23.3 59.4 36.2
sc-without-KIT 23.4 60.7 23.0 59.7 25.6 57.7 23.3 59.8 36.5
sc-without-RWTH.HPBT 23.3 59.4 22.8 58.6 26.1 56.0 23.1 58.4 35.2
Table 3: All systems for the WMT 2012 German?English translation task (truecase). BLEU and TER results are in
percentage. sc denotes system combination. All system combinations are optimized on TER-BLEU .
system newstest2008 newstest2009 newstest2010 newstest2011
BLEU TER BLEU TER BLEU TER BLEU TER TER-BLEU
sc-withAllSystems 23.0 58.8 22.4 58.3 25.8 55.6 23.1 57.7 34.6
sc-without-RWTH.PBT 23.0 59.3 22.5 58.5 25.6 56.0 23.1 58.0 34.9
sc-without-RWTH.HPBT 23.1 59.0 22.6 58.3 25.8 55.6 23.0 58.0 35.0
sc-without-SYSTRAN 22.9 59.7 22.4 59.1 25.6 56.7 23.2 58.5 35.3
sc-without-Limsi 22.7 59.4 22.2 58.7 25.3 56.1 22.7 58.1 35.5
sc-without-KIT 22.9 59.3 22.4 58.5 25.7 55.8 22.7 58.1 35.4
327
Joint Workshop on Statistical Machine Translation and
MetricsMATR, pages 54?59, Uppsala, Sweden.
Alexandre Allauzen, Gilles Adda, He?le`ne Bonneau-
Maynard, Josep M. Crego, Hai-Son Le, Aure?lien Max,
Adrien Lardilleux, Thomas Lavergne, Artem Sokolov,
Guillaume Wisniewski, and Franc?ois Yvon. 2011.
LIMSI @ WMT11. In Proceedings of the Sixth Work-
shop on Statistical Machine Translation, pages 309?
315, Edinburgh, Scotland, July. Association for Com-
putational Linguistics.
F. Casacuberta and E. Vidal. 2004. Machine translation
with inferred stochastic finite-state transducers. Com-
putational Linguistics, 30(3):205?225.
D. Chiang. 2007. Hierarchical Phrase-Based Transla-
tion. Computational Linguistics, 33(2):201?228.
J.M. Crego and J.B. Marin?o. 2007. Improving statistical
MT by coupling reordering and decoding. Machine
Translation, 20(3):199?215.
Josep M. Crego, Franois Yvon, and Jos B. Mario.
2011. N-code: an open-source Bilingual N-gram SMT
Toolkit. Prague Bulletin of Mathematical Linguistics,
96:49?58.
D. De?chelotte, O. Galibert G. Adda, A. Allauzen, J. Gau-
vain, H. Meynard, and F. Yvon. 2008. LIMSI?s statis-
tical translation systems for WMT?08. In Proc. of the
NAACL-HTL Statistical Machine Translation Work-
shop, Columbus, Ohio.
Ilknur Durgar El-Kahlout and Franois Yvon. 2010. The
pay-offs of preprocessing for German-English Statis-
tical Machine Translation. In Marcello Federico, Ian
Lane, Michael Paul, and Franois Yvon, editors, Pro-
ceedings of the seventh International Workshop on
Spoken Language Translation (IWSLT), pages 251?
258.
L. Huang and D. Chiang. 2007. Forest Rescoring: Faster
Decoding with Integrated Language Models. In Proc.
Annual Meeting of the Association for Computational
Linguistics, pages 144?151, Prague, Czech Republic,
June.
R. Kneser and H. Ney. 1995. Improved backing-off for
m-gram language modeling. In Proceedings of the In-
ternational Conference on Acoustics, Speech, and Sig-
nal Processing, ICASSP?95, pages 181?184, Detroit,
MI.
P. Koehn and K. Knight. 2003a. Empirical Methods for
Compound Splitting. In EACL, Budapest, Hungary.
P. Koehn and K. Knight. 2003b. Empirical Methods
for Compound Splitting. In Proceedings of European
Chapter of the ACL (EACL 2009), pages 187?194.
J. Senellart L. Dugast and P. Koehn. 2007. Statistical
post-editing on systran?s rule-based translation system.
In Proceedings of the Second Workshop on Statisti-
cal Machine Translation, StatMT ?07, pages 220?223,
Stroudsburg, PA, USA. Association for Computational
Linguistics.
Hai-Son Le, Ilya Oparin, Alexandre Allauzen, Jean-Luc
Gauvain, and Franc?ois Yvon. 2011. Structured output
layer neural network language model. In Proceedings
of ICASSP?11, pages 5524?5527.
Hai-Son Le, Alexandre Allauzen, and Franc?ois Yvon.
2012. Continuous space translation models with neu-
ral networks. In NAACL ?12: Proceedings of the
2012 Conference of the North American Chapter of the
Association for Computational Linguistics on Human
Language Technology.
Jose? B. Marin?o, R. Banchs, J.M. Crego, A. de Gispert,
P. Lambert, J.A.R. Fonollosa, and M.R. Costa-jussa`.
2006. N-gram-based machine translation. Computa-
tional Linguistics, 32(4).
E. Matusov, N. Ueffing, and H. Ney. 2006. Computing
Consensus Translation from Multiple Machine Trans-
lation Systems Using Enhanced Hypotheses Align-
ment. In Conference of the European Chapter of the
Association for Computational Linguistics (EACL),
pages 33?40.
E. Matusov, G. Leusch, R.E. Banchs, N. Bertoldi,
D. Dechelotte, M. Federico, M. Kolss, Y.-S. Lee,
J.B. Mari no, M. Paulik, S. Roukos, H. Schwenk, and
H. Ney. 2008. System Combination for Machine
Translation of Spoken and Written Language. IEEE
Transactions on Audio, Speech and Language Pro-
cessing, 16(7):1222?1237.
Arne Mauser, Sas?a Hasan, and Hermann Ney. 2009. Ex-
tending Statistical Machine Translation with Discrim-
inative and Trigger-based Lexicon Models. In Pro-
ceedings of the 2009 Conference on Empirical Meth-
ods in Natural Language Processing: Volume 1 - Vol-
ume 1, EMNLP ?09, Singapore.
Mohammed Mediani, Eunah Cho, Jan Niehues, Teresa
Herrmann, and Alex Waibel. 2011. The KIT English-
French Translation Systems for IWSLT 2011. In Pro-
ceedings of the Eighth International Workshop on Spo-
ken Language Translation (IWSLT).
R.C. Moore and W. Lewis. 2010. Intelligent Selection
of Language Model Training Data. In ACL (Short Pa-
pers), pages 220?224, Uppsala, Sweden, July.
J. Niehues and M. Kolss. 2009. A POS-Based Model for
Long-Range Reorderings in SMT. In Fourth Work-
shop on Statistical Machine Translation (WMT 2009),
Athens, Greece.
J. Niehues and S. Vogel. 2008. Discriminative Word
Alignment via Alignment Matrix Modeling. In Proc.
of Third ACL Workshop on Statistical Machine Trans-
lation, Columbus, USA.
Jan Niehues and Alex Waibel. 2011. Using Wikipedia
to Translate Domain-specific Terms in SMT. In Pro-
328
ceedings of the Eighth International Workshop on Spo-
ken Language Translation (IWSLT), San Francisco,
CA.
Jan Niehues, Teresa Herrmann, Stephan Vogel, and Alex
Waibel. 2011. Wider Context by Using Bilingual Lan-
guage Models in Machine Translation. In Sixth Work-
shop on Statistical Machine Translation (WMT 2011),
Edinburgh, UK.
F.J. Och and H. Ney. 2003. A Systematic Comparison of
Various Statistical Alignment Models. Computational
Linguistics, 29(1):19?51.
F.J. Och. 2003. Minimum Error Rate Training for Statis-
tical Machine Translation. In Proc. Annual Meeting of
the Association for Computational Linguistics, pages
160?167, Sapporo, Japan, July.
A. Birch P. Koehn, H. Hoang, C. Callison-Burch, M. Fed-
erico, N. Bertoldi, B. Cowan, W. Shen, C. Moran,
R. Zens, C. Dyer, O. Bojar, A. Constantin, and
E. Herbst. 2007. Moses: open source toolkit for
statistical machine translation. In Proceedings of the
45th Annual Meeting of the ACL on Interactive Poster
and Demonstration Sessions, ACL ?07, pages 177?
180, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
M. Popovic?, D. Stein, and H. Ney. 2006. Statistical
Machine Translation of German Compound Words.
In FinTAL - 5th International Conference on Natural
Language Processing, Springer Verlag, LNCS, pages
616?624.
Anna N. Rafferty and Christopher D. Manning. 2008.
Parsing three German treebanks: lexicalized and un-
lexicalized baselines. In Proceedings of the Workshop
on Parsing German.
K. Rottmann and S. Vogel. 2007. Word Reordering in
Statistical Machine Translation with a POS-Based Dis-
tortion Model. In TMI, Sko?vde, Sweden.
H. Schmid. 1994. Probabilistic Part-of-Speech Tagging
Using Decision Trees. In International Conference
on NewMethods in Language Processing, Manchester,
UK.
Helmut Schmid. 1995. Improvements in part-of-speech
tagging with an application to German. In Evelyne
Tzoukermann and SusanEditors Armstrong, editors,
Proceedings of the ACL SIGDATWorkshop, pages 47?
50. Kluwer Academic Publishers.
A. Stolcke. 2002. SRILM - an extensible language mod-
eling toolkit. In Proc. Int. Conf. on Spoken Language
Processing, volume 2, pages 901?904, Denver, Col-
orado, USA, September.
C. Tillmann. 2004. A unigram orientation model for sta-
tistical machine translation. In Proceedings of HLT-
NAACL 2004, pages 101?104. Association for Com-
putational Linguistics.
A. Venugopal, A. Zollman, and A. Waibel. 2005. Train-
ing and Evaluation Error Minimization Rules for Sta-
tistical Machine Translation. In Workshop on Data-
drive Machine Translation and Beyond (WPT-05), Ann
Arbor, MI.
D. Vilar, S. Stein, M. Huck, and H. Ney. 2010. Jane:
Open Source Hierarchical Translation, Extended with
Reordering and Lexicon Models. In ACL 2010 Joint
Fifth Workshop on Statistical Machine Translation and
Metrics MATR, pages 262?270, Uppsala, Sweden,
July.
S. Vogel. 2003. SMT Decoder Dissected: Word Re-
ordering. In Int. Conf. on Natural Language Process-
ing and Knowledge Engineering, Beijing, China.
R. Zens and H. Ney. 2008. Improvements in Dynamic
Programming Beam Search for Phrase-based Statisti-
cal Machine Translation. In Proc. of the Int. Workshop
on Spoken Language Translation (IWSLT), Honolulu,
Hawaii, October.
329
Proceedings of the 7th Workshop on Statistical Machine Translation, pages 330?337,
Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguistics
LIMSI @ WMT?12
Hai-Son Le1,2, Thomas Lavergne2, Alexandre Allauzen1,2,
Marianna Apidianaki2, Li Gong1,2, Aure?lien Max1,2,
Artem Sokolov2, Guillaume Wisniewski1,2, Franc?ois Yvon1,2
Univ. Paris-Sud1 and LIMSI-CNRS2
rue John von Neumann, 91403 Orsay cedex, France
{firstname.lastname}@limsi.fr
Abstract
This paper describes LIMSI?s submissions to
the shared translation task. We report results
for French-English and German-English in
both directions. Our submissions use n-code,
an open source system based on bilingual
n-grams. In this approach, both the transla-
tion and target language models are estimated
as conventional smoothed n-gram models; an
approach we extend here by estimating the
translation probabilities in a continuous space
using neural networks. Experimental results
show a significant and consistent BLEU im-
provement of approximately 1 point for all
conditions. We also report preliminary experi-
ments using an ?on-the-fly? translation model.
1 Introduction
This paper describes LIMSI?s submissions to the
shared translation task of the Seventh Workshop
on Statistical Machine Translation. LIMSI partic-
ipated in the French-English and German-English
tasks in both directions. For this evaluation, we
used n-code, an open source in-house Statistical
Machine Translation (SMT) system based on bilin-
gual n-grams1. The main novelty of this year?s
participation is the use, in a large scale system, of
the continuous space translation models described
in (Hai-Son et al, 2012). These models estimate the
n-gram probabilities of bilingual translation units
using neural networks. We also investigate an alter-
native approach where the translation probabilities
of a phrase based system are estimated ?on-the-fly?
1http://ncode.limsi.fr/
by sampling relevant examples, instead of consider-
ing the entire training set. Finally we also describe
the use in a rescoring step of several additional fea-
tures based on IBM1 models and word sense disam-
biguation information.
The rest of this paper is organized as follows. Sec-
tion 2 provides an overview of the baseline systems
built with n-code, including the standard transla-
tion model (TM). The continuous space translation
models are then described in Section 3. As in our
previous participations, several steps of data pre-
processing, cleaning and filtering are applied, and
their improvement took a non-negligible part of our
work. These steps are summarized in Section 5.
The last two sections report experimental results ob-
tained with the ?on-the-fly? system in Section 6 and
with n-code in Section 7.
2 System overview
n-code implements the bilingual n-gram approach
to SMT (Casacuberta and Vidal, 2004; Marin?o et al,
2006; Crego and Marin?o, 2006). In this framework,
translation is divided in two steps: a source reorder-
ing step and a (monotonic) translation step. Source
reordering is based on a set of learned rewrite rules
that non-deterministically reorder the input words.
Applying these rules result in a finite-state graph of
possible source reorderings, which is then searched
for the best possible candidate translation.
2.1 Features
Given a source sentence s of I words, the best trans-
lation hypothesis t? is defined as the sequence of J
words that maximizes a linear combination of fea-
330
ture functions:
t? = argmax
t,a
{
M?
m=1
?mhm(a, s, t)
}
(1)
where ?m is the weight associated with feature func-
tion hm and a denotes an alignment between source
and target phrases. Among the feature functions, the
peculiar form of the translation model constitute one
of the main difference between the n-gram approach
and standard phrase-based systems. This will be fur-
ther detailled in section 2.2 and 3.
In addition to the translation model, fourteen
feature functions are combined: a target-language
model (Section 5.3); four lexicon models; six lexi-
calized reordering models (Tillmann, 2004; Crego
et al, 2011) aiming at predicting the orientation of
the next translation unit; a ?weak? distance-based
distortion model; and finally a word-bonus model
and a tuple-bonus model which compensate for the
system preference for short translations. The four
lexicon models are similar to the ones used in stan-
dard phrase-based systems: two scores correspond
to the relative frequencies of the tuples and two lexi-
cal weights are estimated from the automatic word
alignments. The weights vector ? is learned us-
ing a discriminative training framework (Och, 2003)
(Minimum Error Rate Training (MERT)) using the
newstest2009 as development set and BLEU (Pap-
ineni et al, 2002) as the optimization criteria.
2.2 Standard n-gram translation models
n-gram translation models rely on a specific de-
composition of the joint probability of a sentence
pair P (s, t): a sentence pair is assumed to be
decomposed into a sequence of L bilingual units
called tuples defining a joint segmentation: (s, t) =
u1, ..., uL2. In the approach of (Marin?o et al, 2006),
this segmentation is a by-product of source reorder-
ing obtained by ?unfolding? initial word alignments.
In this framework, the basic translation units are
tuples, which are the analogous of phrase pairs and
represent a matching u = (s, t) between a source
s and a target t phrase (see Figure 1). Using the
n-gram assumption, the joint probability of a seg-
2From now on, (s, t) thus denotes an aligned sentence pair,
and we omit the alignment variable a in further developments.
mented sentence pair decomposes as:
P (s, t) =
L?
i=1
P (ui|ui?1, ..., ui?n+1) (2)
During the training phase (Marin?o et al, 2006), tu-
ples are extracted from a word-aligned corpus (us-
ing MGIZA++3 with default settings) in such a
way that a unique segmentation of the bilingual
corpus is achieved. A baseline n-gram translation
model is then estimated over a training corpus com-
posed of tuple sequences using modified Knesser-
Ney Smoothing (Chen and Goodman, 1998).
2.3 Inference
During decoding, source sentences are represented
in the form of word lattices containing the most
promising reordering hypotheses, so as to reproduce
the word order modifications introduced during the
tuple extraction process. Hence, only those reorder-
ing hypotheses are translated and they are intro-
duced using a set of reordering rules automatically
learned from the word alignments.
In the example in Figure 1, the rule [prix no-
bel de la paix ; nobel de la paix prix] repro-
duces the invertion of the French words that is ob-
served when translating from French into English.
Typically, part-of-speech (POS) information is used
to increase the generalization power of these rules.
Hence, rewrite rules are built using POS rather than
surface word forms (Crego and Marin?o, 2006).
3 SOUL translation models
A first issue with the model described by equa-
tion (2) is that the elementary units are bilingual
pairs. As a consequence, the underlying vocabulary,
hence the number of parameters, can be quite large,
even for small translation tasks. Due to data sparsity
issues, such model are bound to face severe estima-
tion problems. Another problem with (2) is that the
source and target sides play symmetric roles: yet,
in decoding, the source side is known and only the
target side must be predicted.
3.1 A word factored translation model
To overcome these issues, the n-gram probability in
equation (2) can be factored by decomposing tuples
3http://www.kyloo.net/software/doku.php
331
 s?
8
: ? 
 t
?
8
: to 
 s?
9
: recevoir 
 t
?
9
: receive 
 s?
10
: le 
 t
?
10
: the 
 s?
11
: nobel de la paix 
 t
?
11
: nobel peace 
 s?
12
: prix 
 t
?
12
: prize 
 u
8
  u
9
  u
10
  u
11
  u
12
 
S :   .... 
T :   .... 
? recevoir le prix nobel de la paixorg :   ....
....
....
Figure 1: Extract of a French-English sentence pair segmented into bilingual units. The original (org) French sentence
appears at the top of the figure, just above the reordered source s and target t. The pair (s, t) decomposes into a
sequence of L bilingual units (tuples) u1, ..., uL. Each tuple ui contains a source and a target phrase: si and ti.
in two parts (source and target), and by taking words
as the basic units of the n-gram TM. This may seem
to be a regression with respect to current state-of-
the-art SMT systems, as the shift from the word-
based model of (Brown et al, 1993) to the phrase-
based models of (Zens et al, 2002) is usually con-
sidered as a major breakthrough of the recent years.
Indeed, one important motivation for considering
phrases was to capture local context in translation
and reordering. It should however be emphasized
that the decomposition of phrases into words is only
re-introduced here as a way to mitigate the param-
eter estimation problems. Translation units are still
pairs of phrases, derived from a bilingual segmen-
tation in tuples synchronizing the source and target
n-gram streams. In fact, the estimation policy de-
scribed in section 4 will actually allow us to take into
account larger contexts than is possible with conven-
tional n-gram models.
Let ski denote the k
th word of source tuple si.
Considering the example of Figure 1, s111 denotes
the source word nobel, s411 the source word paix.
We finally denote hn?1(tki ) the sequence made of
the n? 1 words preceding tki in the target sentence:
in Figure 1, h3(t211) thus refers to the three words
context receive the nobel associated with t211 peace.
Using these notations, equation (2) is rewritten as:
P (a, s, t) =
L?
i=1
[ |ti|?
k=1
P
(
tki |h
n?1(tki ), h
n?1(s1i+1)
)
?
|si|?
k=1
P
(
ski |h
n?1(t1i ), h
n?1(ski )
)] (3)
This decomposition relies on the n-gram assump-
tion, this time at the word level. Therefore, this
model estimates the joint probability of a sentence
pair using two sliding windows of length n, one for
each language; however, the moves of these win-
dows remain synchronized by the tuple segmenta-
tion. Moreover, the context is not limited to the cur-
rent phrase, and continues to include words from ad-
jacent phrases. Using the example of Figure 1, the
contribution of the target phrase t11 = nobel, peace
to P (s, t) using a 3- gram model is:
P
(
nobel|[receive, the], [la, paix]
)
?P
(
peace|[the, nobel], [la, paix]
)
.
A benefit of this new formulation is that the vo-
cabularies involved only contain words, and are thus
much smaller that tuple vocabularies. These models
are thus less at risk to be plagued by data sparsity is-
sues. Moreover, the decomposition (3) now involves
two models: the first term represents a TM, the sec-
ond term is best viewed as a reordering model. In
this formulation, the TM only predicts the target
phrase, given its source and target contexts.
P (s, t) =
L?
i=1
[ |si|?
k=1
P
(
ski |h
n?1(ski ), h
n?1(t1i+1)
)
?
|ti|?
k=1
P
(
tki |h
n?1(s1i ), h
n?1(tki )
)] (4)
4 The principles of SOUL
In section 3.1, we defined a n-gram translation
model based on equations (3) and (4). A major diffi-
culty with such models is to reliably estimate their
parameters, the numbers of which grow exponen-
tially with the order of the model. This problem
is aggravated in natural language processing due to
332
the well-known data sparsity issue. In this work,
we take advantage of the recent proposal of (Le et
al., 2011). Using a specific neural network architec-
ture (the Structured OUtput Layer or SOUL model),
it becomes possible to handle large vocabulary lan-
guage modeling tasks. This approach was experi-
mented last year for target language models only and
is now extended to translation models. More details
about the SOUL architecture can be found in (Le et
al., 2011), while its extension to translation models
is more precisely described in (Hai-Son et al, 2012).
The integration of SOUL models for large SMT
tasks is carried out using a two-pass approach: the
first pass uses conventional back-off n-gram trans-
lation and language models to produce a k-best list
(the k most likely translations); in the second pass,
the probability of a m-gram SOUL model is com-
puted for each hypothesis and the k-best list is ac-
cordingly reordered. In all the following experi-
ments, we used a context size for SOUL of m = 10,
and used k = 300. The two decompositions of equa-
tions (3) and (4) are used by introducing 4 scores
during the rescoring step.
5 Corpora and data pre-processing
Concerning data pre-processing, we started from our
submissions from last year (Allauzen et al, 2011)
and mainly upgraded the corpora and the associated
language-dependent pre-processing routines.
5.1 Pre-processing
We used in-house text processing tools for the to-
kenization and detokenization steps (De?chelotte et
al., 2008). Previous experiments have demonstrated
that better normalization tools provide better BLEU
scores: all systems are thus built in ?true-case?.
Compared to last year, the pre-processing of utf-8
characters was significantly improved.
As German is morphologically more complex
than English, the default policy which consists in
treating each word form independently is plagued
with data sparsity, which severely impacts both
training (alignment) and decoding (due to unknown
forms). When translating from German into En-
glish, the German side is thus normalized using a
specific pre-processing scheme (described in (Al-
lauzen et al, 2010; Durgar El-Kahlout and Yvon,
2010)), which aims at reducing the lexical redun-
dancy by (i) normalizing the orthography, (ii) neu-
tralizing most inflections and (iii) splitting complex
compounds. All parallel corpora were POS-tagged
with the TreeTagger (Schmid, 1994); in addition, for
German, fine-grained POS labels were also needed
for pre-processing and were obtained using the RF-
Tagger (Schmid and Laws, 2008).
5.2 Bilingual corpora
As for last year?s evaluation, we used all the avail-
able parallel data for the German-English language
pair, while only a subpart of the French-English par-
allel data was selected. Word alignment models
were trained using all the data, whereas the transla-
tion models were estimated on a subpart of the par-
allel data: the UN corpus was discarded for this step
and about half of the French-English Giga corpus
was filtered based on a perplexity criterion as in (Al-
lauzen et al, 2011)).
For French-English, we mainly upgraded the
training material from last year by extracting the
new parts from the common data. The word
alignment models trained last year were then up-
dated by running a forced alignment 4 of the new
data. These new word-aligned data was added to
last year?s parallel corpus and constitute the train-
ing material for the translation models and feature
functions described in Section 2. Given the large
amount of available data, three different bilingual
n-gram models are estimated, one for each source of
data: News-Commentary, Europarl, and the French-
English Giga corpus. These models are then added
to the weighted mixture defined by equation (1). For
German-English, we simply used all the available
parallel data to train one single translation models.
5.3 Monolingual corpora and language models
For the monolingual training data, we also used the
same setup as last year. For German, all the train-
ing data allowed in the constrained task were di-
vided into several sets based on dates or genres:
News-Commentary, the news crawled from the Web
grouped by year, and Europarl. For each subset,
a standard 4-gram LM was estimated using inter-
polated Kneser-Ney smoothing (Kneser and Ney,
4The forced alignment step consists in an additional EM it-
eration.
333
1995; Chen and Goodman, 1998). The resulting
LMs are then linearly combined using interpolation
coefficients chosen so as to minimize the perplexity
of the development set. The German vocabulary is
created using all the words contained in the parallel
data and expanded to reach a total of 500k words by
including the most frequent words observed in the
monolingual News data for 2011.
For French and English, the same monolingual
corpora as last year were used5. We did not observe
any perplexity decrease in our attempts to include
the new data specifically provided for this year?s
evaluation. We therefore used the same language
models as in (Allauzen et al, 2011).
6 ?On-the-fly? system
We also developped an alternative approach imple-
menting ?on-the-fly? estimation of the parameter of
a standard phase-based model, using Moses (Koehn
et al, 2007) as the decoder. Implementing on-the-
fly estimation for n-code, while possible in the-
ory, is less appealing due to the computational cost
of estimating a smoothed language model. Given
an input source file, it is possible to compute only
those statistics which are required to translate the
phrases it contains. As in previous works on on-
the-fly model estimation for SMT (Callison-Burch
et al, 2005; Lopez, 2008), we compute a suffix
array for the source corpus. This further enables
to consider only a subset of translation examples,
which we select by deterministic random sampling,
meaning that the sample is chosen randomly with
respect to the full corpus but that the same sample
is always returned for a given value of sample size,
hereafter denoted N . In our experiments, we used
N = 1, 000 and computed from the sample and the
word alignments (we used the same tokenization and
word alignments as in all other submitted systems)
the same translation6 and lexical reordering models
as the standard training scripts of the Moses system.
Experiments were run on the data sets used for
WMT English-French machine translation evalua-
tion tasks, using the same corpora and optimization
5The fifth edition of the English Gigaword (LDC2011T07)
was not used.
6An approximation is used for p(f |e), and coherent transla-
tion estimation is used; see (Lopez, 2008).
procedure as in our other experiments. The only no-
table difference is our use of the Moses decoder in-
stead of the n-gram-based system. As shown in Ta-
ble 1, our on-the-fly system achieves a result (31.7
BLEU point) that is slightly worst than the n-code
baseline (32.0) and slightly better than the equiva-
lent Moses baseline (31.5), but does it much faster.
Model estimation for the test file is reduced to 2
hours and 50 minutes, with an additional overhead
for loading and writing files of one and a half hours,
compared to roughly 210 hours for our baseline sys-
tems under comparable hardware conditions.
7 Experimental results
7.1 n-code with SOUL
Table 1 summarizes the experimental results sub-
mitted to the shared translation for French-English
and German-English in both directions. The perfor-
mances are measured in terms of BLEU on new-
stest2011, last year?s test set, and this year?s test
set newstest2012. For the former, BLEU scores are
computed with the NIST script mteva-v13.pl, while
we provide for newstest2012 the results computed
by the organizers 7. The Baseline results are ob-
tained with standard n-gram models estimated with
back-off, both for the bilingual and monolingual tar-
get models. With standard n-gram estimates, the or-
der is limited to n = 4. For instance, the n-code
French-English baseline achieves a 0.5 BLEU point
improvement over a Moses system trained with the
same data setup in both directions.
From Table 1, it can be observed that adding
the SOUL models (translation models and target
language model) consistently improves the base-
line, with an increase of 1 BLEU point. Con-
trastive experiments show that the SOUL target LM
does not bring significant gain when added to the
SOUL translation models. For instance, a gain of
0.3 BLEU point is observed when translating from
French to English with the addition of the SOUL tar-
get LM. In the other translation directions, the differ-
ences are negligible.
7All results come from the official website: http://
matrix.statmt.org/matrix/.
334
Direction System BLEU
test2011 test2012?
en2fr Baseline 32.0 28.9
+ SOUL TM 33.4 29.9
on-the-fly 31.7 28.6
fr2en Baseline 30.2 30.4
+ SOUL TM 31.1 31.5
en2de Baseline 15.4 16.0
+ SOUL TM 16.6 17.0
de2en Baseline 21.8 22.9
+ SOUL TM 22.8 23.9
Table 1: Experimental results in terms of BLEU scores
measured on the newstest2011 and newstest2012. For
newstest2012, the scores are provided by the organizers.
7.2 Experiments with additional features
For this year?s evaluation, we also investigated sev-
eral additional features based on IBM1 models and
word sense disambiguation (WSD) information in
rescoring. As for the SOUL models, these features
are added after the n-best list generation step.
In previous work (Och et al, 2004; Hasan, 2011),
the IBM1 features (Brown et al, 1993) are found
helpful. As the IBM1 model is asymmetric, two
models are estimated, one in both directions. Con-
trary to the reported results, these additional features
do not yield significant improvements over the base-
line system. We assume that the difficulty is to add
information to an already extensively optimized sys-
tem. Moreover, the IBM1 models are estimated on
the same training corpora as the translation system,
a fact that may explain the redundancy of these ad-
ditional features.
In a separate series of experiments, we also add
WSD features calculated according to a variation of
the method proposed in (Apidianaki, 2009). For
each word of a subset of the input (source lan-
guage) vocabulary, a simple WSD classifier pro-
duces a probability distribution over a set of trans-
lations8. During reranking, each translation hypoth-
esis is scanned and the word translations that match
one of the proposed variant are rewarded using an
additional score. While this method had given some
8The difference with the method described in (Apidianaki,
2009) is that no sense clustering is performed, and each transla-
tion is represented by a separate weighted source feature vector
which is used for disambiguation
small gains on a smaller dataset (IWSLT?11), we did
not observe here any improvement over the base-
line system. Additional analysis hints that (i) most
of the proposed variants are already covered by the
translation model with high probabilities and (ii) that
these variants are seldom found in the reference sen-
tences. This means that, in the situation in which
only one reference is provided, the hypotheses with
a high score for the WSD feature are not adequately
rewarded with the actual references.
8 Conclusion
In this paper, we described our submissions to
WMT?12 in the French-English and German-
English shared translation tasks, in both directions.
As for our last year?s participation, our main sys-
tems are built with n-code, the open source Statis-
tical Machine Translation system based on bilingual
n-grams. Our contributions are threefold. First, we
have experimented a new kind of translation mod-
els, where the bilingual n-gram distribution are es-
timated in a continuous space with neural networks.
As shown in past evaluations with target language
model, there is a significant reward for using this
kind of models in a rescoring step. We observed that,
in general, the continuous space translation model
yields a slightly larger improvement than the target
translation model. However, their combination does
not result in an additional gain.
We also reported preliminary results with a sys-
tem ?on-the-fly?, where the training data are sam-
pled according to the data to be translated in order
to train contextually adapted system. While this sys-
tem achieves comparable performance to our base-
line system, it is worth noticing that its total train-
ing time is much smaller than a comparable Moses
system. Finally, we investigated several additional
features based on IBM1 models and word sense dis-
ambiguation information in rescoring. While these
methods have sometimes been reported to help im-
prove the results, we did not observe any improve-
ment here over the baseline system.
Acknowledgment
This work was partially funded by the French State
agency for innovation (OSEO) in the Quaero Pro-
gramme.
335
References
Alexandre Allauzen, Josep M. Crego, I?lknur Durgar El-
Kahlout, and Franc?ois Yvon. 2010. LIMSI?s statis-
tical translation systems for WMT?10. In Proc. of the
Joint Workshop on Statistical Machine Translation and
MetricsMATR, pages 54?59, Uppsala, Sweden.
Alexandre Allauzen, Gilles Adda, He?le`ne Bonneau-
Maynard, Josep M. Crego, Hai-Son Le, Aure?lien Max,
Adrien Lardilleux, Thomas Lavergne, Artem Sokolov,
Guillaume Wisniewski, and Franc?ois Yvon. 2011.
LIMSI @ WMT11. In Proceedings of the Sixth Work-
shop on Statistical Machine Translation, pages 309?
315, Edinburgh, Scotland, July. Association for Com-
putational Linguistics.
Marianna Apidianaki. 2009. Data-driven semantic anal-
ysis for multilingual WSD and lexical selection in
translation. In Proceedings of the 12th Conference of
the European Chapter of the ACL (EACL 2009), pages
77?85, Athens, Greece, March. Association for Com-
putational Linguistics.
Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della
Pietra, and Robert L. Mercer. 1993. The mathemat-
ics of statistical machine translation: parameter esti-
mation. Comput. Linguist., 19(2):263?311.
Chris Callison-Burch, Colin Bannard, and Josh
Schroeder. 2005. Scaling phrase-based statisti-
cal machine translation to larger corpora and longer
phrases. In Proceedings of the 43rd Annual Meeting
of the Association for Computational Linguistics
(ACL?05), pages 255?262, Ann Arbor, Michigan,
June. Association for Computational Linguistics.
Francesco Casacuberta and Enrique Vidal. 2004. Ma-
chine translation with inferred stochastic finite-state
transducers. Computational Linguistics, 30(3):205?
225.
Stanley F. Chen and Joshua T. Goodman. 1998. An
empirical study of smoothing techniques for language
modeling. Technical Report TR-10-98, Computer Sci-
ence Group, Harvard Un iversity.
Josep M. Crego and Jose? B. Marin?o. 2006. Improving
statistical MT by coupling reordering and decoding.
Machine Translation, 20(3):199?215.
Josep M. Crego, Franc?ois Yvon, and Jose? B. Marin?o.
2011. N-code: an open-source Bilingual N-gram SMT
Toolkit. Prague Bulletin of Mathematical Linguistics,
96:49?58.
Ilknur Durgar El-Kahlout and Franc?ois Yvon. 2010. The
pay-offs of preprocessing for German-English Statis-
tical Machine Translation. In Marcello Federico, Ian
Lane, Michael Paul, and Franc?ois Yvon, editors, Pro-
ceedings of the seventh International Workshop on
Spoken Language Translation (IWSLT), pages 251?
258.
Daniel De?chelotte, Gilles Adda, Alexandre Allauzen,
Olivier Galibert, Jean-Luc Gauvain, He?le`ne May-
nard, and Franc?ois Yvon. 2008. LIMSI?s statisti-
cal translation systems for WMT?08. In Proc. of the
NAACL-HTL Statistical Machine Translation Work-
shop, Columbus, Ohio.
Hai-Son, Alexandre Allauzen, and Franc?ois Yvon. 2012.
Continuous space translation models with neural net-
works. In NAACL ?12: Proceedings of the 2012 Con-
ference of the North American Chapter of the Associ-
ation for Computational Linguistics on Human Lan-
guage Technology.
Sas?a Hasan. 2011. Triplet Lexicon Models for Statisti-
cal Machine Translation. Ph.D. thesis, RWTH Aachen
University.
Reinhard Kneser and Herman Ney. 1995. Improved
backing-off for m-gram language modeling. In Pro-
ceedings of the International Conference on Acoustics,
Speech, and Signal Processing, ICASSP?95, pages
181?184, Detroit, MI.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open source
toolkit for statistical machine translation. In Proceed-
ings of the 45th Annual Meeting of the Association for
Computational Linguistics Companion Volume Pro-
ceedings of the Demo and Poster Sessions, pages 177?
180, Prague, Czech Republic, June. Association for
Computational Linguistics.
Hai-Son Le, Ilya Oparin, Alexandre Allauzen, Jean-Luc
Gauvain, and Franc?ois Yvon. 2011. Structured output
layer neural network language model. In Proceedings
of ICASSP?11, pages 5524?5527.
Adam Lopez. 2008. Tera-scale translation models via
pattern matching. In Proceedings of the 22nd Interna-
tional Conference on Computational Linguistics (Col-
ing 2008), pages 505?512, Manchester, UK, August.
Coling 2008 Organizing Committee.
Jose? B. Marin?o, Rafael E. Banchs, Josep M. Crego, Adria`
de Gispert, Patrick Lambert, Jose? A.R. Fonollosa, and
Marta R. Costa-Jussa`. 2006. N-gram-based machine
translation. Computational Linguistics, 32(4):527?
549.
Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur,
Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar
Kumar, Libin Shen, David Smith, Katherine Eng,
Viren Jain, Zhen Jin, and Dragomir Radev. 2004. A
smorgasbord of features for statistical machine trans-
lation. In Daniel Marcu Susan Dumais and Salim
Roukos, editors, HLT-NAACL 2004: Main Proceed-
ings, pages 161?168, Boston, Massachusetts, USA,
336
May 2 - May 7. Association for Computational Lin-
guistics.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In ACL ?03: Proc. of
the 41st Annual Meeting on Association for Computa-
tional Linguistics, pages 160?167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In ACL ?02: Proc. of
the 40th Annual Meeting on Association for Compu-
tational Linguistics, pages 311?318. Association for
Computational Linguistics.
Helmut Schmid and Florian Laws. 2008. Estimation
of conditional probabilities with decision trees and an
application to fine-grained POS tagging. In Proceed-
ings of the 22nd International Conference on Com-
putational Linguistics (Coling 2008), pages 777?784,
Manchester, UK, August.
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In Proc. of International
Conference on New Methods in Language Processing,
pages 44?49, Manchester, UK.
Christoph Tillmann. 2004. A unigram orientation model
for statistical machine translation. In Proceedings of
HLT-NAACL 2004, pages 101?104. Association for
Computational Linguistics.
Richard Zens, Franz Josef Och, and Hermann Ney. 2002.
Phrase-based statistical machine translation. In KI
?02: Proceedings of the 25th Annual German Con-
ference on AI, pages 18?32, London, UK. Springer-
Verlag.
337
Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 62?69,
Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational Linguistics
LIMSI @ WMT?13
Alexandre Allauzen1,2, Nicolas Pe?cheux1,2, Quoc Khanh Do1,2, Marco Dinarelli2,
Thomas Lavergne1,2, Aure?lien Max1,2, Hai-Son Le3, Franc?ois Yvon1,2
Univ. Paris-Sud1 and LIMSI-CNRS2
rue John von Neumann, 91403 Orsay cedex, France
{firstname.lastname}@limsi.fr
Vietnamese Academy of Science and Technology3, Hanoi, Vietnam
lehaison@ioit.ac.vn
Abstract
This paper describes LIMSI?s submis-
sions to the shared WMT?13 translation
task. We report results for French-English,
German-English and Spanish-English in
both directions. Our submissions use
n-code, an open source system based on
bilingual n-grams, and continuous space
models in a post-processing step. The
main novelties of this year?s participation
are the following: our first participation
to the Spanish-English task; experiments
with source pre-ordering; a tighter integra-
tion of continuous space language mod-
els using artificial text generation (for Ger-
man); and the use of different tuning sets
according to the original language of the
text to be translated.
1 Introduction
This paper describes LIMSI?s submissions to the
shared translation task of the Eighth Workshop on
Statistical Machine Translation. LIMSI partici-
pated in the French-English, German-English and
Spanish-English tasks in both directions. For this
evaluation, we used n-code, an open source in-
house Statistical Machine Translation (SMT) sys-
tem based on bilingual n-grams1, and continuous
space models in a post-processing step, both for
translation and target language modeling.
This paper is organized as follows. Section 2
contains an overview of the baseline systems built
with n-code, including the continuous space mod-
els. As in our previous participations, several
steps of data pre-processing, cleaning and filter-
ing are applied, and their improvement took a non-
negligible part of our work. These steps are sum-
marized in Section 3. The rest of the paper is de-
voted to the novelties of the systems submitted this
1http://ncode.limsi.fr/
year. Section 4 describes the system developed for
our first participation to the Spanish-English trans-
lation task in both directions. To translate from
German into English, the impact of source pre-
ordering is investigated, and experimental results
are reported in Section 5, while for the reverse di-
rection, we explored a text sampling strategy us-
ing a 10-gram SOUL model to allow a tighter in-
tegration of continuous space models during the
translation process (see Section 6). A final section
discusses the main lessons of this study.
2 System overview
n-code implements the bilingual n-gram approach
to SMT (Casacuberta and Vidal, 2004; Marin?o
et al, 2006; Crego and Marin?o, 2006). In this
framework, translation is divided in two steps: a
source reordering step and a (monotonic) transla-
tion step. Source reordering is based on a set of
learned rewrite rules that non-deterministically re-
order the input words. Applying these rules result
in a finite-state graph of possible source reorder-
ings, which is then searched for the best possible
candidate translation.
2.1 Features
Given a source sentence s of I words, the best
translation hypothesis t? is defined as the sequence
of J words that maximizes a linear combination of
feature functions:
t? = argmax
t,a
{ M?
m=1
?mhm(a, s, t)
}
(1)
where ?m is the weight associated with feature
function hm and a denotes an alignment between
source and target phrases. Among the feature
functions, the peculiar form of the translation
model constitutes one of the main difference be-
tween the n-gram approach and standard phrase-
based systems.
62
In addition to the translation model (TM), four-
teen feature functions are combined: a target-
language model; four lexicon models; six lexical-
ized reordering models (Tillmann, 2004; Crego et
al., 2011) aimed at predicting the orientation of
the next translation unit; a ?weak? distance-based
distortion model; and finally a word-bonus model
and a tuple-bonus model which compensate for the
system preference for short translations. The four
lexicon models are similar to the ones used in stan-
dard phrase-based systems: two scores correspond
to the relative frequencies of the tuples and two
lexical weights are estimated from the automatic
word alignments. The weight vector ? is learned
using the Minimum Error Rate Training frame-
work (MERT) (Och, 2003) and BLEU (Papineni
et al, 2002) measured on nt09 (newstest2009) as
the optimization criteria.
2.2 Translation Inference
During decoding, source sentences are represented
in the form of word lattices containing the most
promising reordering hypotheses, so as to repro-
duce the word order modifications introduced dur-
ing the tuple extraction process. Hence, only those
reordering hypotheses are translated and are intro-
duced using a set of reordering rules automatically
learned from the word alignments. Part-of-speech
(POS) information is used to increase the gen-
eralization power of these rules. Hence, rewrite
rules are built using POS, rather than surface word
forms (Crego and Marin?o, 2006).
2.3 SOUL rescoring
Neural networks, working on top of conventional
n-gram back-off language models (BOLMs), have
been introduced in (Bengio et al, 2003; Schwenk
et al, 2006) as a potential means to improve dis-
crete language models (LMs). As for our last year
participation (Le et al, 2012c), we take advantage
of the recent proposal of Le et al (2011). Using
a specific neural network architecture (the Struc-
tured OUtput Layer or SOUL model), it becomes
possible to estimate n-gram models that use large
vocabulary, thereby making the training of large
neural network LMs (NNLMs) feasible both for
target language models and translation models (Le
et al, 2012a). We use the same models as last year,
meaning that the SOUL rescoring was used for all
systems, except for translating into Spanish. See
section 6 and (Le et al, 2012c) for more details.
3 Corpora and data pre-processing
Concerning data pre-processing, we started from
our submissions from last year (Le et al, 2012c)
and mainly upgraded the corpora and the associ-
ated language-dependent pre-processing routines.
We used in-house text processing tools for the to-
kenization and detokenization steps (De?chelotte
et al, 2008). Previous experiments have demon-
strated that better normalization tools provide bet-
ter BLEU scores: all systems are thus built using
the ?true-case? scheme.
As German is morphologically more complex
than English, the default policy which consists in
treating each word form independently is plagued
with data sparsity, which severely impacts both
training (alignment) and decoding (due to un-
known forms). When translating from German
into English, the German side is thus normalized
using a specific pre-processing scheme (Allauzen
et al, 2010; Durgar El-Kahlout and Yvon, 2010)
which aims at reducing the lexical redundancy by
(i) normalizing the orthography, (ii) neutralizing
most inflections and (iii) splitting complex com-
pounds. All parallel corpora were POS-tagged
with the TreeTagger (Schmid, 1994); in addition,
for German, fine-grained POS labels were also
needed for pre-processing and were obtained us-
ing the RFTagger (Schmid and Laws, 2008).
For Spanish, all the availaible data are tokenized
using FreeLing2 toolkit (Padro? and Stanilovsky,
2012), with default settings and some added rules.
Sentence splitting and morphological analysis are
disabled except for del ? de el and al ? a el.
Moreover, a simple ?true-caser? based on upper-
case word frequency is used, and the specific
Spanish punctuation signs ??? and ??? are removed
and heuristically reintroduced in a post-processing
step. All Spanish texts are POS-tagged also using
Freeling. The EAGLES tag set is however sim-
plified by truncating the category label to the first
two symbols, in order to reduce the sparsity of the
reordering rules estimated by n-code.
For the CommonCrawl corpus, we found that
many sentences are not in the expected language.
For example, in the French side of the French-
English version, most of the first sentences are
in English. Therefore, foreign sentence pairs are
filtered out with a MaxEnt classifier that uses n-
grams of characters as features (n is between 1
and 4). This filter discards approximatively 10%
2http://nlp.lsi.upc.edu/freeling/
63
of the sentence pairs. Moreover, we also observe
that a lot of sentence pairs are not translation of
each other. Therefore, an extra sentence alignment
step is carried out using an in-house implementa-
tion of the tool described in (Moore, 2002). This
last step discards approximately 20% of the cor-
pus. For the Spanish-English task, the same filter-
ing is applied to all the available corpora.
4 System development for the
Spanish-English task
This is our first participation to the Spanish-
English translation task in both directions. This
section provides details about the development of
n-code systems for this language pair.
4.1 Data selection and filtering
The CommonCrawl and UN corpora can be con-
sidered as very noisy and out-of-domain. As de-
scribed in (Allauzen et al, 2011), to select a subset
of parallel sentences, trigram LMs were trained for
both Spanish and English languages on a subset of
the available News data: the Spanish (resp. En-
glish) LM was used to rank the Spanish (resp. En-
glish) side of the corpus, and only those sentences
with perplexity above a given threshold were se-
lected. Finally, the two selected sets were in-
tersected. In the following experiments, the fil-
tered versions of these corpora are used to train
the translation systems unless explicitly stated.
4.2 Spanish language model
To train the language models, we assumed that the
test set would consist in a selection of recent news
texts and all the available monolingual data for
Spanish were used, including the Spanish Giga-
word, Third Edition. A vocabulary is first defined
by including all tokens observed in the News-
Commentary and Europarl corpora. This vocab-
ulary is then expanded with all words that occur
more than 10 times in the recent news texts (LDC-
2007-2011 and news-crawl-2011-2012). This pro-
cedure results in a vocabulary containing 372k
words. Then, the training data are divided into
7 sets based on dates or genres. On each set, a
standard 4-gram LM is estimated from the vocab-
ulary using absolute discounting interpolated with
lower order models (Kneser and Ney, 1995; Chen
and Goodman, 1998). The resulting LMs are then
linearly interpolated using coefficients chosen so
Corpora BLEU
dev nt11 test nt12
es2en N,E 30.2 33.2
N,E,C 30.6 33.7
N,E,U 30.3 33.6
N,E,C,U 30.6 33.7
N,E,C,U (nf) 30.7 33.6
en2es N,E 32.2 33.3
N,E,C,U 32.3 33.6
N,E,C,U (nf) 32.5 33.9
Table 1: BLEU scores achieved with different
sets of parallel corpora. All systems are base-
line n-code with POS factor models. The follow-
ing shorthands are used to denote corpora, : ?N?
stands for News-Commentary, ?E? for Europarl,
?C? for CommonCrawl, ?U? for UN and (nf) for
non filtered corpora.
as to minimise the perplexity evaluated on the de-
velopment set (nt08).
4.3 Experiments
All reported results are averaged on 3 MERT runs.
Table 1 shows the BLEU scores obtained with dif-
ferent corpora setups. We can observe that us-
ing the CommonCrawl corpus improves the per-
formances in both directions, while the impact of
the UN data is less important, especially when
combined with CommonCrawl. The filtering strat-
egy described in Section 4.2 has a slightly posi-
tive impact of +0.1 BLEU point for the Spanish-
to-English direction but yields a 0.2 BLEU point
decrease in the opposite direction.
For the following experiments, all the available
corpora are therefore used: News-Commentary,
Europarl, filtered CommonCrawl and UN. For
each of these corpora, a bilingual n-gram model
is estimated and used by n-code as one individual
model score. An additionnal TM is trained on the
concatenation all these corpora, resulting in a to-
tal of 5 TMs. Moreover, n-code is able to handle
additional ?factored? bilingual models where the
source side words are replaced by the correspond-
ing lemma or even POS tag (Koehn and Hoang,
2007). Table 2 reports the scores obtained with
different settings.
In Table 2, big denotes the use of a wider
context for n-gram TMs (n = 4, 5, 4 instead
of 3, 4, 3 respectively for word-based, POS-based
and lemma-based TMs). Using POS factored
64
Condition BLEU
dev nt11 test nt12
es2en base 30.3 33.5
pos 30.6 33.7
big-pos 30.7 33.7
big-pos-lem 30.7 33.8
en2es base 32.0 33.4
pos 32.3 33.6
big-pos 32.3 33.8
big-pos-pos+ 32.2 33.4
Table 2: BLEU scores for different configuration
of factored translation models. The big prefix de-
notes experiments with the larger context for n-
gram translation models.
models yields a significant BLEU improvement,
as well as using a wider context for n-gram TMs.
Since Spanish is morphologically richer than En-
glish, lemmas are introduced only on the Span-
ish side. An additionnal BLEU improvement is
achieved by adding factored models based on lem-
mas when translating from Spanish to English,
while in the opposite direction it does not seem
to have any clear impact.
For English to Spanish, we also experimented
with a 5-gram target factored model, using the
whole morphosyntactic EAGLES tagset, (pos+ in
Table 2), to add some syntactic information, but
this, in fact, proved harmful.
As several tuning sets were available, experi-
ments were carried out with the concatenation of
nt09 to nt11 as a tuning data set. This yields an im-
provement between 0.1 and 0.3 BLEU point when
testing on nt12 when translating from Spanish to
English.
4.4 Submitted systems
For both directions, the submitted systems are
trained on all the available training data, the cor-
pora CommonCrawl and UN being filtered as de-
scribed previously. A word-based TM and a POS
factored TM are estimated for each training set.
To translate from Spanish to English, the system
is tuned on the concatenation of the nt09 to nt11
datasets with an additionnal 4-gram lemma-based
factored model, while in the opposite direction, we
only use nt11.
dev nt09 test nt11
en2de 15.43 15.35
en-mod2de 15.06 15.00
Table 3: BLEU scores for pre-ordering experi-
ments with a n-code system and the approach pro-
posed by (Neubig et al, 2012)
5 Source pre-ordering for English to
German translation
While distorsion models can efficiently handle
short range reorderings, they are inadequate to
capture long-range reorderings, especially for lan-
guage pairs that differ significantly in their syn-
tax. A promising workaround is the source pre-
ordering method that can be considered similar,
to some extent, to the reordering strategy imple-
mented in n-code; the main difference is that the
latter uses one deterministic (long-range) reorder-
ing on top of conventional distortion-based mod-
els, while the former only considers one single
model delivering permutation lattices. The pre-
ordering approach is illustrated by the recent work
of Neubig et al (2012), where the authors use a
discriminatively trained ITG parser to infer a sin-
gle permutation of the source sentence.
In this section, we investigate the use of this
pre-ordering model in conjunction with the bilin-
gual n-gram approach for translating English into
German (see (Collins et al, 2005) for similar ex-
periments with the reverse translation direction).
Experiments are carried out with the same settings
as described in (Neubig et al, 2012): given the
source side of the parallel data (en), the parser is
estimated to modify the original word order and to
generate a new source side (en-mod); then a SMT
system is built for the new language pair (en-mod
? de). The same reordering model is used to re-
order the test set, which is then translated with the
en-mod? de system.
Results for these experiments are reported in Ta-
ble 3, where nt09 and nt11 are respectively used
as development and test sets. We can observe that
applying pre-ordering on source sentences leads to
small drops in performance for this language pair.
To explain this degradation, the histogram of to-
ken movements performed by the model on the
pre-ordered training data is represented in Fig-
ure 1. We can observe that most of the movements
are in the range [?4,+6] (92% of the total occur-
65
Figure 1: Histogram of token movement size ver-
sus its occurrences performed by the model Neu-
big on the source english data.
rences), which can be already taken into account
by the standard reordering model of the baseline
system. This is reflected also by the following
statistics: surprisingly, only 16% of the total num-
ber of sentences are changed by the pre-ordering
model, and the average sentence-wise Kendall?s ?
and the average displacement of these small parts
of modified sentences are, respectively, 0.027 and
3.5. These numbers are striking for two reasons:
first, English and German have in general quite
different word order, thus our experimental con-
dition should be somehow similar to the English-
Japanese scenario studied in (Neubig et al, 2012);
second, since the model is able to perform pre-
ordering basically at any distance, it is surprising
that a large part of the data remains unmodified.
6 Artificial Text generation with SOUL
While the context size for BOLMs is limited (usu-
ally up to 4-grams) because of sparsity issues,
NNLMs can efficiently handle larger contexts up
to 10-grams without a prohibitive increase of the
overall number of parameters (see for instance the
study in (Le et al, 2012b)). However the major
bottleneck of NNLMs is the computation cost dur-
ing both training and inference. In fact, the pro-
hibitive inference time usually implies to resort to
a two-pass approach: the first pass uses a conven-
tional BOLM to produce a k-best list (the k most
likely translations); in the second pass, the prob-
ability of a NNLM is computed for each hypoth-
esis, which is then added as a new feature before
the k-best list is reranked. Note that to produce the
k-best list, the decoder uses a beam search strategy
to prune the search space. Crucially, this pruning
does not use the NNLMs scores and results in po-
tentially sub-optimal k-best-lists.
6.1 Sampling texts with SOUL
In language modeling, a language is represented
by a corpus that is approximated by a n-gram
model. Following (Sutskever et al, 2011; Deoras
et al, 2013), we propose an additionnal approxi-
mation to allow a tighter integration of the NNLM:
a 10-gram NNLM is first estimated on the training
corpus; texts then are sampled from this model to
create an artificial training corpus; finally, this arti-
ficial corpus is approximated by a 4-gram BOLM.
The training procedure for the SOUL NNLM is
the same as the one described in (Le et al, 2012c).
To sample a sentence from the SOUL model, first
the sentence length is randomly drawn from the
empirical distribution, then each word of the sen-
tence is sampled from the 10-gram distribution es-
timated with the SOUL model.
The convergence of this sampling strategy can
be evaluated by monitoring the perplexity evolu-
tion vs. the number of sentences that are gener-
ated. Figure 2 depicts this evolution by measuring
perplexity on the nt08 set with a step size of 400M
sampled sentences. The baseline BOLM (std) is
estimated on all the available training data that
consist of approximately 300M of running words.
We can observe that the perplexity of the BOLM
estimated on sampled texts (generated texts) de-
creases when the number of sample sentences in-
creases, and tends to reach slowly the perplex-
ity of the baseline BOLM. Moreover, when both
BOLMs are interpolated, an even lower perplex-
ity is obtained, which further decreases with the
amount of sampled training texts.
6.2 Translation results
Experiments are run for translation into German,
which lacks a GigaWord corpus. An artificial cor-
pus containing 3 billions of running words is first
generated as described in Section 6.1. This corpus
is used to estimate a BOLM with standard settings,
that is then used for decoding, thereby approxi-
mating the use of a NNLM during the first pass.
Results reported in Table 4 show that adding gen-
erated texts improves the BLEU scores even when
the SOUL model is added in a rescoring step. Also
note that using the LM trained on the sampled cor-
pus yields the same BLEU score that using the
standard LM.
66
 190 200 210 220 230 240 250 260
 270 280
 2  4  6  8  10  12ppx times 400M sampled sentences
artificial textsartificial texts+stdstd
Figure 2: Perplexity measured on nt08 with the
baseline LM (std), with the LM estimated on the
sampled texts (generated texts), and with the inter-
polation of both.
Therefore, to translate from English to German,
the submitted system includes three BOLMs: one
trained on all the monolingual data, one on artifi-
cial texts and a third one that uses the freely avail-
able deWack corpus3 (1.7 billion words).
target LM BLEU
dev nt09 test nt10
base 15.3 16.5
+genText 15.5 16.8
+SOUL 16.4 17.6
+genText+SOUL 16.5 17.8
Table 4: Impact of the use of sampled texts.
7 Different tunings for different original
languages
As shown by Lembersky et al (2012), the original
language of a text can have a significant impact on
translation performance. In this section, this effect
is assessed on the French to English translation
task. Training one SMT system per original lan-
guage is impractical, since the required informa-
tion is not available for most of parallel corpora.
However, metadata provided by the WMT evalua-
tion allows us to split the development and test sets
according to the original language of the text. To
ensure a sufficient amount of texts for each con-
dition, we used the concatenation of newstest cor-
pora for the years 2008, 2009, 2011, and 2012,
leaving nt10 for testing purposes.
Five different development sets have been cre-
ated to tune five different systems. Experimental
results are reported in Table 7 and show a drastic
3http://wacky.sslmit.unibo.it/doku.php
baseline adapted
original language tuning
cz 22.31 23.83
en 36.41 39.21
fr 31.61 32.41
de 18.46 18.49
es 30.17 29.34
all 29.43 30.12
Table 5: BLEU scores for the French-to-English
translation task measured on nt10 with systems
tuned on development sets selected according to
their original language (adapted tuning).
improvement in terms of BLEU score when trans-
lating back to the original English and a significant
increase for original text in Czech and French. In
this year?s evaluation, Russian was introduced as
a new language, so for sentences originally in this
language, the baseline system was used. This sys-
tem is used as our primary submission to the eval-
uation, with additional SOUL rescoring step.
8 Conclusion
In this paper, we have described our submis-
sions to the translation task of WMT?13 for
the French-English, German-English and Spanish-
English language pairs. Similarly to last year?s
systems, our main submissions use n-code, and
continuous space models are introduced in a post-
processing step, both for translation and target lan-
guage modeling. To translate from English to
German, we showed a slight improvement with
a tighter integration of the continuous space lan-
guage model using a text sampling strategy. Ex-
periments with pre-ordering were disappointing,
and the reasons for this failure need to be better
understood. We also explored the impact of using
different tuning sets according to the original lan-
guage of the text to be translated. Even though the
gain vanishes when adding the SOUL model in a
post-processing step, it should be noted that due to
time limitation this second step was not tuned ac-
cordingly to the original language. We therefore
plan to assess the impact of using different tuning
sets on the post-processing step.
Acknowledgments
This work was partially funded by the French State
agency for innovation (OSEO), in the Quaero Pro-
gramme.
67
References
Alexandre Allauzen, Josep M. Crego, I?lknur Durgar El-
Kahlout, and Franc?ois Yvon. 2010. LIMSI?s statis-
tical translation systems for WMT?10. In Proc. of
the Joint Workshop on Statistical Machine Transla-
tion and MetricsMATR, pages 54?59, Uppsala, Swe-
den.
Alexandre Allauzen, Gilles Adda, He?le`ne Bonneau-
Maynard, Josep M. Crego, Hai-Son Le, Aure?lien
Max, Adrien Lardilleux, Thomas Lavergne, Artem
Sokolov, Guillaume Wisniewski, and Franc?ois
Yvon. 2011. LIMSI @ WMT11. In Proceedings of
the Sixth Workshop on Statistical Machine Transla-
tion, pages 309?315, Edinburgh, Scotland, July. As-
sociation for Computational Linguistics.
Yoshua Bengio, Re?jean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. JMLR, 3:1137?1155.
Francesco Casacuberta and Enrique Vidal. 2004. Ma-
chine translation with inferred stochastic finite-state
transducers. Computational Linguistics, 30(3):205?
225.
Stanley F. Chen and Joshua T. Goodman. 1998. An
empirical study of smoothing techniques for lan-
guage modeling. Technical Report TR-10-98, Com-
puter Science Group, Harvard Un iversity.
Michael Collins, Philipp Koehn, and Ivona Kucerova.
2005. Clause restructuring for statistical machine
translation. In Proceedings of the 43rd Annual
Meeting of the Association for Computational Lin-
guistics (ACL?05), pages 531?540, Ann Arbor,
Michigan.
Josep M. Crego and Jose? B. Marin?o. 2006. Improving
statistical MT by coupling reordering and decoding.
Machine Translation, 20(3):199?215.
Josep M. Crego, Franois Yvon, and Jos B. Marin?o.
2011. N-code: an open-source Bilingual N-gram
SMT Toolkit. Prague Bulletin of Mathematical Lin-
guistics, 96:49?58.
Daniel De?chelotte, Gilles Adda, Alexandre Allauzen,
Olivier Galibert, Jean-Luc Gauvain, Hlne Maynard,
and Franois Yvon. 2008. LIMSI?s statistical
translation systems for WMT?08. In Proc. of the
NAACL-HTL Statistical Machine Translation Work-
shop, Columbus, Ohio.
Anoop Deoras, Toma?s? Mikolov, Stefan Kombrink, and
Kenneth Church. 2013. Approximate inference: A
sampling based modeling technique to capture com-
plex dependencies in a language model. Speech
Communication, 55(1):162 ? 177.
Ilknur Durgar El-Kahlout and Franois Yvon. 2010.
The pay-offs of preprocessing for German-English
Statistical Machine Translation. In Marcello Fed-
erico, Ian Lane, Michael Paul, and Franois Yvon, ed-
itors, Proceedings of the seventh International Work-
shop on Spoken Language Translation (IWSLT),
pages 251?258.
Reinhard Kneser and Herman Ney. 1995. Improved
backing-off for m-gram language modeling. In Pro-
ceedings of the International Conference on Acous-
tics, Speech, and Signal Processing, ICASSP?95,
pages 181?184, Detroit, MI.
Philipp Koehn and Hieu Hoang. 2007. Factored trans-
lation models. In Proceedings of the 2007 Joint
Conference on Empirical Methods in Natural Lan-
guage Processing and Computational Natural Lan-
guage Learning (EMNLP-CoNLL), pages 868?876.
Hai-Son Le, Ilya Oparin, Alexandre Allauzen, Jean-
Luc Gauvain, and Franc?ois Yvon. 2011. Structured
output layer neural network language model. In Pro-
ceedings of ICASSP?11, pages 5524?5527.
Hai-Son Le, Alexandre Allauzen, and Franc?ois Yvon.
2012a. Continuous space translation models with
neural networks. In NAACL ?12: Proceedings of
the 2012 Conference of the North American Chap-
ter of the Association for Computational Linguistics
on Human Language Technology.
Hai-Son Le, Alexandre Allauzen, and Franc?ois Yvon.
2012b. Measuring the influence of long range de-
pendencies with neural network language models.
In Proceedings of the NAACL-HLT 2012 Workshop:
Will We Ever Really Replace the N-gram Model? On
the Future of Language Modeling for HLT, pages 1?
10, Montre?al, Canada.
Hai-Son Le, Thomas Lavergne, Alexandre Al-
lauzen, Marianna Apidianaki, Li Gong, Aure?lien
Max, Artem Sokolov, Guillaume Wisniewski, and
Franc?ois Yvon. 2012c. Limsi @ wmt12. In
Proceedings of the Seventh Workshop on Statisti-
cal Machine Translation, pages 330?337, Montre?al,
Canada.
Gennadi Lembersky, Noam Ordan, and Shuly Wint-
ner. 2012. Language models for machine trans-
lation: Original vs. translated texts. Comput. Lin-
guist., 38(4):799?825, December.
Jose? B. Marin?o, Rafael E. Banchs, Josep M. Crego,
Adria` de Gispert, Patrick Lambert, Jose? A.R. Fonol-
losa, and Marta R. Costa-Jussa`. 2006. N-gram-
based machine translation. Computational Linguis-
tics, 32(4):527?549.
Robert C. Moore. 2002. Fast and accurate sen-
tence alignment of bilingual corpora. In Proceed-
ings of the 5th Conference of the Association for
Machine Translation in the Americas on Machine
Translation: From Research to Real Users, AMTA
?02, pages 135?144, Tiburon, CA, USA. Springer-
Verlag.
68
Graham Neubig, Taro Watanabe, and Shinsuke Mori.
2012. Inducing a discriminative parser to optimize
machine translation reordering. In Proceedings of
the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
Natural Language Learning, pages 843?853, Jeju
Island, Korea, July. Association for Computational
Linguistics.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In ACL ?03: Proc. of
the 41st Annual Meeting on Association for Compu-
tational Linguistics, pages 160?167.
Llu??s Padro? and Evgeny Stanilovsky. 2012. Freeling
3.0: Towards wider multilinguality. In Proceedings
of the Language Resources and Evaluation Confer-
ence (LREC 2012), Istanbul, Turkey, May. ELRA.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In ACL ?02:
Proc. of the 40th Annual Meeting on Association for
Computational Linguistics, pages 311?318. Associ-
ation for Computational Linguistics.
Helmut Schmid and Florian Laws. 2008. Estima-
tion of conditional probabilities with decision trees
and an application to fine-grained POS tagging. In
Proceedings of the 22nd International Conference
on Computational Linguistics (Coling 2008), pages
777?784, Manchester, UK, August.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proc. of Interna-
tional Conference on New Methods in Language
Processing, pages 44?49, Manchester, UK.
Holger Schwenk, Daniel De?chelotte, and Jean-Luc
Gauvain. 2006. Continuous space language models
for statistical machine translation. In Proc. COL-
ING/ACL?06, pages 723?730.
Ilya Sutskever, James Martens, and Geoffrey Hinton.
2011. Generating text with recurrent neural net-
works. In Lise Getoor and Tobias Scheffer, editors,
Proceedings of the 28th International Conference
on Machine Learning (ICML-11), ICML ?11, pages
1017?1024, New York, NY, USA, June. ACM.
Christoph Tillmann. 2004. A unigram orientation
model for statistical machine translation. In Pro-
ceedings of HLT-NAACL 2004, pages 101?104. As-
sociation for Computational Linguistics.
69
