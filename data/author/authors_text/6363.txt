Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 648?657,
Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Improving Web Search Relevance with Semantic Features
Yumao Lu Fuchun Peng Gilad Mishne Xing Wei Benoit Dumoulin
Yahoo! Inc.
701 First Avenue
Sunnyvale, CA, 94089
yumaol,fuchun,gilad,xwei,benoitd@yahoo-inc.com
Abstract
Most existing information retrieval (IR)
systems do not take much advantage of
natural language processing (NLP) tech-
niques due to the complexity and limited
observed effectiveness of applying NLP
to IR. In this paper, we demonstrate that
substantial gains can be obtained over a
strong baseline using NLP techniques, if
properly handled. We propose a frame-
work for deriving semantic text matching
features from named entities identified in
Web queries; we then utilize these features
in a supervised machine-learned ranking
approach, applying a set of emerging ma-
chine learning techniques. Our approach
is especially useful for queries that contain
multiple types of concepts. Comparing to
a major commercial Web search engine,
we observe a substantial 4% DCG5 gain
over the affected queries.
1 Introduction
Most existing IR models score documents pri-
marily based on various term statistics. In tra-
ditional models?from classic probabilistic mod-
els (Croft and Harper, 1979; Fuhr, 1992), through
vector space models (Salton et al, 1975; Narita
and Ogawa, 2000), to well studied statistical lan-
guage models (Ponte and Croft, 2000; Lafferty
and Zhai, 2001)?these term statistics have been
captured directly in the ranking formula. More re-
cently, learning to rank approaches to IR (Fried-
man, 2002) have become prominent; in these
frameworks, that aim at learning a ranking func-
tion from data, term statistics are often modeled
as term matching features in a machine learning
process.
Traditional text matching features are mainly
based on frequencies of n-grams of the user?s
query in a variety of document sections, such as
the document title, body text, anchor text, and so
on. Global information such as frequency of term
or term group in the corpus may also be used, as
well as its combination with local statistics ? pro-
ducing relative scores such as tf ? idf or BM25
scores (Robertson et al, 1995). Matching may
be restricted to certain window sizes to enforce
proximity, or may be more lenient, allowing un-
ordered sequences and nonconsecutive sequences
for a higher recall.
Even before machine learning was applied to
IR, NLP techniques such as Named Entity Recog-
nition (NER), Part-of-Speech (POS) tagging, and
parsing have been applied to both query model-
ing and document indexing (Smeaton and van Ri-
jsbergen, 1988; Narita and Ogawa, 2000; Sparck-
Jones, 1999). For example, statistical concept
language models generalize classic n-gram mod-
els to concept n-gram model by enforcing query
term proximity within each concept (Srikanth and
Srihari, 2003). However, researchers have of-
ten reported limited gains or even decreased per-
formance when applying NLP to IR (Voorhees,
1999).
Typically, concepts detected through NLP tech-
niques either in the query or in documents are
used as proximity constraints for text match-
ing (Sparck-Jones, 1999), ignoring the actual con-
cept type. The machine learned approach to docu-
ment ranking provides us with an opportunity to
revisit the manner in which NLP information is
used for ranking. Using knowledge gained from
NLP application as features rather than heuris-
tically allows us much greater flexibility in the
amount and variability of information used ? e.g.,
incorporating knowledge about the actual entity
types. This has several benefits: first, entity types
appearing in queries are an indicator of the user?s
intent. A query consisting of a business category
and a location (e.g., hotels Palo Alto) appears to be
648
informational, and perhaps is best answered with
a page containing a list of hotels in Palo Alto.
Queries containing a business name and a location
(e.g., Fuki Sushi Palo Alto) are more navigational
in nature ? for many users, the intent is finding the
home page of a specific business. Similarly, entity
types appearing in documents are an indicator of
the document type. For example, if ?Palo Alto?
appears ten times in document?s body text, it is
more likely to be a local listing page than a home
page. For the query hotels Palo Alto, a local listing
page may be a good page, while for the query Fuki
Sushi Palo Alto a listing page is not a good page.
In addition, knowledge of the particular entities
in queries allows us to incorporate external knowl-
edge about these entities, such as entity-specific
stopwords (?inc.? as in Yahoo Inc. or ?services?
as in kaiser medical service), and so on.
Finally, even when using named entities only
for deriving proximity-related features, we can
benefit from applying different levels of proxim-
ity for different entities. For example, for enti-
ties like cities (e.g., ?River Side?), the proximity
requirement is fairly strict: we should not allow
extra words between the original terms, and pre-
serve their order. For other entities the proximity
constraint can be relaxed?for example, for per-
son names, due to the middle name convention:
Hillary Clinton vs. Hillary R. Clinton.
In this paper, we propose a systematic approach
to modeling semantic features, incorporating con-
cept types extracted from query analysis. Ver-
tical attributes, such as city-state relationships,
metropolitan definition, or idf scores from a do-
main specific corpus, are extracted for each con-
cept type from vertical database. The vertical at-
tributes, together with the concept attributes, are
used to compose a set of semantic features for ma-
chine learning based IR models. A few machine
learning techniques are discussed to further im-
prove relevance for subclass of difficult queries
such as queries containing multiple types of con-
cepts. Figure 1 shows an overview of our ap-
proach; after discussing related work in Section 2,
we spend Sections 3 to 5 of the paper describing
the components of our system. We then evaluate
the effectiveness of our approach both using gen-
eral queries and with a set of ?difficult? queries;
our results show that the techniques are robust, and
particularly effective for this type of queries. We
conclude in Section 7.
Tagger1 Tagger2 Tagger n
Resolution Module
Query
with
Annotations
Query
Query
Linguistic
Analysis
Vertical AttributeLocation 
DB
Vertical AttributeBusiness
DB
Vertical Attribute
Semantic Text Matching
Document 
Index
Semantic Features
Specialized
Ranking
Module
Specialized
RankingModule
Specialized
RankingModule
...
DB
Name
...
......
Figure 1: Ranking with Semantic Features
2 Related Work
There is substantial body of work involving us-
age of NLP techniques to improve information re-
trieval (Brants, 2003; Strzalkowski et al, 1996).
Allan and Ragahavan (Allan and Raghavan, 2002)
use Part-of-Speech tagging to reduce ambiguity
of difficult queries by converting short queries
to questions. In other POS-tags work, Aram-
patzis et al (Arampatzis et al, 1990) observed
an improvement when using nouns only for re-
trieval. Croft et al (Croft et al, 1991) and Tong
et al (Buckley et al, 1993; Tong et al, 1996) ex-
plored phrases and structured queries and found
phrases are effective in improving retrieval per-
formance. Voorhees (Voohees, 1993) uses word
sense disambiguation to improve retrieval perfor-
mance. One IR domain that consistently benefits
from usage of various NLP techniques is question
answering, where queries are formed in natural
language format; e.g., (Peng et al, 2005).
In general, however, researchers often observe
limited gains or even degraded performance when
applying NLP to IR (Voorhees, 1999). Having
said this, most past studies use small datasets and
a modest baseline; it is unclear whether a similar
conclusion would be reached when using a state-
of-art system such as a commercial web search
engine as a baseline, and a full-web corpus ? as
we do in this paper. This leads to another differ-
ence between this work and existing work involv-
ing named entity recognition for retrieval. Most
649
previous research on usage of named entities in
IR combines entity detection in documents and
queries (Prager et al, 2000). Entity detection in
document has a high indexing cost that is often
overlooked, but cannot be ignored in the case of
commercial search engines. For this reason, we
restrict NLP processing to queries only ? although
we believe that document-side NLP processing
will provide additional useful information.
3 Query Analysis
We begin by briefly describing our approach to
named entity recognition in web queries, which
serves as the basis for deriving the semantic text
matching features.
Named entity recognition (NER) is the task of
identifying and classifying entities, such as per-
son names or locations, in text. The majority of
state-of-the-art NER methods utilize a statistical
approach, attempting to learn a mapping between
a sequence of observations (words) and a sequence
of tags (entity types). In these methods, the se-
quential nature of the data is often central to the
model, as named entities tend to appear in particu-
lar context in text. For example, for most types of
text, in the two sequences met with X and buy the
Y, the likelihood of X being a person name is sub-
stantially higher than the corresponding likelihood
of Y . Indeed, many named entity taggers perform
well when applied to grammatical text with suf-
ficient contexts, such as newswire text (Sang and
Meulder, 2003).
Web queries, however, tend to be short, with
most queries consisting of 1?3 words, and lack
context ? posing a particular challenge for iden-
tifying named entities in them. Existing work on
NER in web queries focuses on tailoring a solu-
tion for a particular entity type and its usage in
web search (Wang et al, 2005; Shen et al, 2008);
in contrast, we aim at identifying a large range
of possible entities in web queries, and using a
generic solution for all of them.
In web queries, different entity types may bene-
fit from different detection techniques. For exam-
ple, an entity type with a large variability among
instances as well as existence of external resources
like product name calls for an approach that can
make use of many features, such as a conditional
random field; for entity types that are more struc-
tured like person names, a grammar-based ap-
proach can be more effective (Shen et al, 2008).
To this end, we utilize multiple approaches for en-
tity detection and combine them into a single, co-
herent ?interpretation? of the query.
Given a query, we use several entity recogniz-
ers in parallel, one for each of the common en-
tity types found in web queries. The modeling
types may differ between the recognizers: some
are Markovian models, while others are just dic-
tionary lookups; the accuracy of each recognizer
is also different. We then have a machine-learned
disambiguation module that combines output from
different taggers, ranking the tagging sequences.
The details of scoring is out of the scope of this
paper, and we omit it for simplicity.
4 Semantic Text Matching Features
Our proposed semantic features operate at the
semantic type level rather than at the term level:
instead of matching a term (or set of terms) in doc-
uments, we match their semantic type. Given the
query San Francisco colleges and the annotation
[San Francisco]
CityName
[colleges]
BusinessCategory
,
the semantic text matching features would de-
scribe how relevant a document section is for a en-
tity of type CityName, for BusinessCategory,
and for their combination.
Concretely, we exploit a set of features that
attempts to capture proximity, general relevance,
and vertical relevance for each type of semantic
tag and for each section of the document. We now
review these feature by their broad types.
4.1 Semantic Proximity Features
Proximity features?features that capture the de-
gree to which search terms appear close to each
other in a document?are among the most impor-
tant feature sets in ranking functions. Traditional
proximity features are typically designed for all
query terms (Metzler and Croft, 2005) and may
suffer from wrong segmentations of the query. For
example, for the query New York city bus char-
ter, a traditional proximity feature may treat ?city
bus? similarly to ?York city.? But given detailed
information about the entities in the query in their
types, we can enforce proximity for ?New York
city? and ?bus charter? more accurately. Different
types of entities usually have different proximity
characteristics in relevant documents. Strongly-
bound entities such as city names typically have
very high proximity in relevant documents, while
entities such as business names may have much
650
lower proximity: a search for Kaiser medical of-
fice, for example, may be well-served with docu-
ments referring to Kaiser Permanente medical of-
fice, and as we mentioned before, person names
matches may also benefit from lenient proximity
enforcement. This is naturally addressed by treat-
ing each entity type differently.
We propose a set of semantic proximity fea-
tures that associate each semantic tag type with
generic proximity measures. We also consider tag-
ging confidence together with term group proxim-
ity; we discuss these two approaches next.
4.1.1 Semantic Minimum Coverage (SMC)
Minimum Coverage (MC) is a popular span based
proximity distance measure, which is defined as
the length of the shortest document segment that
cover the query term at least once in a docu-
ment (Tao and Zhai, 2007). We extend this mea-
sure to Semantic Minimum Coverage (SMC) for
each semantic type t in document section s and
define it as
SMC
t,s
=
1
|{k|T
k
= t}|
?
i?{k|T
k
=t}
w
i
MC
i,s
,
where w
i
is a weight for tagged term group i,
MC
i,s
is the the minimum coverage of term group
i in document section s, {k|T
k
= t} denotes the
set of all concepts having type t, and |{k|T
k
= t}|
is the size of the set. The definition of the weight
w is flexible. We list a few candidate weight-
ing schemes in this paper: uniform weights (wu),
weights based on idf scores (widf) and ?strength?-
based weight (ws), which we define as follows:
w
u
= 1;
w
idf
=
c
f
q
where c is a constant and f
q
is the frequency of the
term group in a large query log;
w
s
= min
l
MI
l
where MI
l
is the point-wise mutual information of
the l-th consecutive pair within the semantic tag.
We can also combine strength and idf scores such
that the weight reflects both relative importance
and constraints in proximity. In this paper, we use
w
si
= w
s
w
idf
.
In Section 6, we use all four weighting schemes
mentioned above in the semantic feature set.
4.1.2 Semantic Moving Average BM25
(SMABM25)
BM25, a commonly-used bag-of-words relevance
estimation method (Robertson et al, 1995), is de-
fined (when applied to document sections) as
BM25 =
?
j
idf
j
f
j,s
(c
1
+ 1)
f
i,s
+ c
1
(1? c
2
+ c
2
l
s
?
l
s
)
where f
j,s
is the frequency of term j in section s,
l
s
is the length of section s, ?l
s
is the average length
of document section s, c
1
, c
2
, c
3
are constants and
the idf score of term j is defined as
idf
j
= log
c
4
? d
j
+ c
5
d
j
+ c
5
,
where d
j
is the number of sections in all collec-
tions that contains term j and c
4
, c
5
are constants.
To characterize proximity, we could use a fixed
length sliding window and calculate the average
BM25. We further associate each sliding average
BM25 with each type of semantic term groups.
This results in a Semantic Moving Average BM25
(SMABM25) of type t, which we define as fol-
lows:
1
|{k|T
k
= t}|
?
i?{k|T
k
=t}
(1/M)
?
m
BM25
m
where m is a fixed length sliding window m and
M is the total number of sliding windows (that de-
pends on the length of the section window size).
4.2 Semantic Vertical Relevance Features
Vertical databases contain a large amount of struc-
tured domain knowledge typically discarded by
traditional web relevance features. Having access
to the semantic types in queries, we can tap into
that knowledge to improve accuracy. For exam-
ple, term frequencies in different corpora can as-
sist in determining relevance given an entity type.
As we mentioned in Section 1, we observe that
term frequency in a database of business names
provides an indication of the business brand, the
key part of the business name phrase. While both
?yahoo? and ?inc? are very common terms on the
web, in a database of businesses only ?inc? is com-
mon enough to be considered a stopword in the
context of business names.
We propose a Vertical Moving Average BM25
(VMABM25) as a feature aiming at quantifying
the vertical knowledge for web search. The ba-
sic idea here is to replace the idf score idf
j
of
651
SMABM25 with an idf score calculated from a
vertical database for type t, namely idft
j
:
1
|{k|T
k
= t}|
?
i?{k|T
k
=t}
(1/M)
?
m
BM25
m,t
where
BM25
m,t
=
?
j
idft
j
f
j,s
(c1 + 1)
f
i,s
+ c
1
(1? c
2
+ c
2
l
s
?
l
s
)
where the idft
j
is associated with the semantic type
t and calculated from the corpus associated with
that type.
VMABM25 links vertical knowledge, proxim-
ity, and page relevance together; we show later that
it is one of most salient features among all seman-
tic features.
4.3 Generalized Semantic Features
Finally, we develop a generalized feature based on
the previous features by removing tags. Semantic
features are often sparse, as many queries contain
one entity or no entities at all; generalized features
increase their coverage by combining the basic se-
mantic features. An entity without tag is essen-
tially a segment.
A segment feature x
i
for query i does not have
entity type and can be expressed as
x
i
=
1
K
i
K
i
?
k=1
x
T (k)
where K
i
is the number of segments in the query
and T (k) is the semantic type associated with kth
concept.
Although these features are less informative
than type-specific features, one advantage of using
them is that they have substantially higher cover-
age. In our experiments, more than 40% of the
queries have some identified entity. Another rel-
atively subtle advantage is that segment features
have no type related errors: the only possible error
is a mistake in entity boundaries.
5 Ranking Function Optimization
The ultimate goal of the machine learning ap-
proach to web search is to learn a ranking func-
tion h(x
i
), where x
i
is a feature vector of a query-
document pair i, such that the error
L(h) ?
N
?
i=1
(y
i
? h(x
i
))
2 (1)
is minimized. Here, y
i
is the actual relevance score
for the query-document pair i (typically assigned
by a human) and N is the number of training sam-
ples.
As mentioned in the previous Section, an inher-
ent issue with semantic features is their sparse-
ness. User queries are usually short, with an av-
erage length of less than 3 words. Text matching
features that are associated with the semantic type
of query term or term groups are clearly sparse
comparing with traditional, non-entity text match-
ing features ? that can be derived for any query.
When a feature is very sparse, it is unlikely that
it would play a very meaningful role in a machine
learned ranking function, since the error L would
largely depend on other samples that do not con-
tain the specific semantic features at all. To over-
come the spareness issue and take advantage of
semantic features, we suggested generalizing our
features; but we also exploit a few ranking func-
tion modeling techniques.
First, we use a ?divide-and-conquer? approach.
Long queries usually contain multiple concepts
and could be difficult to retrieve relevant docu-
ments. Semantic features, however, are rich in
this set of queries. We may train special models
to further optimize our ranking function for those
queries. The loss function over ranking function h
becomes
L
C
(h) ?
?
i?C
(y
i
? h(x
i
))
2 (2)
where C is the training set that falls into a pre-
defined subclass. For example, queries containing
both location and business name, queries contains
both location and business category, etc, are good
candidates to apply semantic features.
To this end, we first classify queries into several
classes, each of which has multiple types of enti-
ties. The semantic features of those types would
be dense for this subclass of queries. We then
train models that may rank the specific class of
queries well. This approach, however, may suf-
fer from significantly less training samples due to
training data partition resulted from the query clas-
sification. Increasing the modeling accuracy, then,
comes at a cost of reduced data available for train-
ing. We apply two techniques to address this is-
sue. The first approach is to over-weight subclass
training samples such that the subclass of queries
plays a more important role in modeling while still
652
keeping a large pool of the overall training sam-
ples. The second approach is model adaptation:
a generalized incremental learning method. Here,
instead of being over-weighted in a joint optimiza-
tion, the subclass of training data is used to mod-
ify an existing model such that the new model is
?adapted? to the subclass problem. We elaborate
on our approaches as follows.
5.1 Weighted Training Samples
To take advantage of both large a pool of training
samples and sparse related semantic features for
a subclass of queries, we could modify the loss
function as follows
L
w
C
(h) ? w
?
i?C
(y
i
? h(x
i
))
2
+
?
i?
?
C
(y
i
? h(x
i
))
2
,
(3)
where ?C is the complement of set C. Here, the
weight w is a compromise between loss function
(1) and (2). When w = 1, we have
L
1
C
(h) ? L(h);
when w? > ?
L
?
C
(h) ? L
C
(h).
A large weight may help optimize the training for
a special subclass of queries, and a small weight
may help to preserve good generality of the ranker.
We could use cross-validation to select the weight
w to optimize a the ranking function for a sub-
class of queries. In practice, a small w is desired
to avoid overfitting.
5.2 Model Adaptation
Model adaptation is an emerging machine learn-
ing technique that is used for information retrieval
applications with limited amount of training data.
In this paper, we apply Trada, proposed by Chen
et al (Chen et al, 2008), as our adaptation algo-
rithm.
The Trada algorithm aims at adapting tree-
based models. A popular tree based regression ap-
proach is Gradient Boosting Trees (GBT) , which
is an additive model h(x) =
?
K
k=1
?
k
h
k
(x),
where each regression tree h
k
is sequentially op-
timized with a hill-climbing procedure. As with
other decision trees, a binary regression tree h
k
(x)
consists of a set of decision nodes; each node is
associated with a feature variable and a splitting
value that partition the data into two parts, with the
corresponding predicted value defined in the leave
node. The basic idea of Trada is to apply piece-
wise linear transformation to the base model based
on the new training data. A set of linear transfor-
mations are applied to each decision node, either
predict or split point or both, such that the new pre-
dict or the split point of a node in a decision tree
satisfies
v = (1? p
C
)v? + p
C
v
C
where v? denotes predict or split point of that node
in the base mode and v
C
denotes predict or split
point of that node using new data set C, and
the weight p
C
depends on the number of origi-
nal training data and new training data that fall
through the node. For each node, the split or pre-
dict can be estimated by
p
C
=
?n
C
n+ ?n
C
,
where n is the number of training sample of the
base model that fall through the node, n
C
is the
number of new training sample that fall through
the node, and ? is a parameter that can be deter-
mined using cross validation. The parameter ?
is used to over-weight new training data, an ap-
proach that is very effective in practice. For new
features that are not included in the base model,
more trees are allowed to be added to incorporate
them.
6 Experiments
We now measure the effectiveness of our proposal,
and answer related questions, through extensive
experimental evaluation. We begin by examining
the effectiveness of features as well as the model-
ing approaches introduced in Section 5 on a par-
ticular class of queries?those with a local intent.
We proceed by evaluating whether if the type asso-
ciated with each entity really matters by compar-
ing results with type dependent semantic features
and segment features. Finally, we examine the ro-
bustness of our features by measuring the change
in the accuracy of our resulting ranking function
when the query analysis is wrong; we do this by
introducing simulated noise into the query analy-
sis results.
6.1 Dataset
Our training, validation and test sets are human-
labeled query-document pairs. Each item in the
653
sets consists of a feature vector x
i
represent-
ing the query and the document, and a judg-
ment score y
i
assigned by a human. There are
around 600 features in each vector, including both
the newly introduced semantic features and exist-
ing features; features are either query-dependent
ones, document-dependent ones, or query-and-
document-dependent features.
The training set is based on uniformly sampled
Web queries from our query log, and top ranked
documents returned by commercial search engines
for these queries; this set consists of 1.24M query-
document pairs.
We use two additional sets for validation and
testing. One set is based on uniformly sampled
Web queries, and contains 42790 validation sam-
ples and 70320 test samples. The second set is
based on uniformly sampled local queries. By lo-
cal queries, we mean queries that contain at least
two types of semantic tags: a location tag (such
as street, city or state name) and a business tag (a
business name or business category). We refer to
this class of queries ?local queries,? as users often
type this kind of queries in local vertical search.
The local query set consists of 11040 validation
samples and 39169 test samples. In the training
set we described above, there are 56299 training
samples out of the 1.24M total number of training
samples that satisfy the definition of local queries.
We call this set local training subset.
6.2 Evaluation Metrics
To evaluate the effectiveness of our semantic
features we use Discounted Cumulative Gain
(DCG) (Jarvelin and Kekalainen, 2000), a widely-
used metric for measuring Web search relevance.
(Jarvelin and Kekalainen, 2000). Given a query
and a ranked list of K documents (K = 5 in our
experiments), the DCG for this query is defined as
DCG(K) =
K
?
i=1
y
i
log
2
(1 + i)
. (4)
where y
i
? [0, 10] is a relevance score for the
document at position i, typically assigned by a hu-
man, where 10 is assigned to the most relevant
documents and 0 to the least relevant ones.
To measure statistical significance, we use the
Wilcoxon test (Wilcoxon, 1945); when the p-value
is below 0.01 we consider a difference to be statis-
tically significant and mark it with a bold font in
the result table.
6.3 Experimental Results
We use Stochastic Gradient Boosting Trees
(SGBT) (Friedman, 2002), a robust none linear
regression algorithm, for training ranking func-
tions and, as mentioned earlier, Trada (Chen et al,
2008) for model adaptation.
Training parameters are selected to optimize the
relevance on a separated validation set. The best
resulting is evaluated against the test set; all results
presented here use the test set for evaluation.
6.3.1 Feature Effectiveness with Ranking
Function Modeling
We apply the modeling approaches introduced in
Section 5 to improve feature effectiveness on ?dif-
ficult? queries?those more than one entity type;
we evaluate these approaches with the semantic-
feature-rich set, the local query test set. We split
training sets into two parts: one set belongs to the
local queries, the other is the rest. We first weight
the local queries and use the combined dataset as
training data to learn the ranking functions; we
train functions with and without the semantic fea-
tures. We evaluate these functions against the lo-
cal query test set. The results are summarized in
Table 1, where w denotes the weight assigned to
the local training set, bolded numbers are statis-
tically significant result compared to the baseline,
uniformly weighted training data without seman-
tic features (with superscript b). It is interesting
to observe that without semantic features, over-
weighted local training data does not have statis-
tically significant impact on the test performance;
with semantic features, a proper weight over train-
ing samples does improve test performance sub-
stantially.
Table 1: Evaluation of Ranking Models Trained
Against Over-weighted Local Queries with Se-
mantic Features on the Local Query Test Set
w/o semantic features w/ semantic features
Weight DCG(5) Impr. DCG(5) Impr.
w = 0 8.09
b
- 8.25 2.0%
w = 2 8.09 0.02% 8.26 2.1%
w = 4 8.13 0.49% 8.34 3.1%
w = 8 8.13 0.49% 8.42 4.1%
w = 16 8.13 0.49% 8.30 2.6%
w = 32 8.04 ?0.60% 8.27 2.2%
Next, we use the local query training set as ?new
data? in the tree adaptation approach. In tree adap-
tations, all parameters are set to optimize the per-
formance over the local validation set. We com-
654
pare two major adaptation approaches proposed
in (Chen et al, 2008): adapting predict only and
adapting both predict and split. We use the model
trained with the combined training and uniform
weights as the baseline; results are summarized in
Table 2.
Table 2: Trada Algorithms with Semantic Features
on Local Query Test Set
w/o semantic feat. w/ semantic feat.
Ada. Appr. DCG(5) Impr. DCG(5) Impr.
Combined data 8.09b - 8.25 2.0%
Ada. predict 8.02 ?0.1% 8.14 0.6%
Ada. predict 8.00 ?0.1% 8.17 1.0%
& split
Comparing Tables 1 and 2, note that using the
combined training data with local query training
samples over-weighted achieves better results than
tree adaption. The latter approach, however, has
the advantage of far less training time, since the
adaptation is over a much smaller local query
training set. With the same hardware, it takes
just a few minutes to train an adaptation model,
while it takes days to train a model over the entire,
combined training data. Considering that massive
model validation tasks are required to select good
training parameters, training many different mod-
els with over a million training samples becomes
prohibitly costly. Applying tree adaptation tech-
niques makes research and prototyping of these
models feasible.
6.3.2 Type Dependent Semantic Features vs.
Segment Features
Our next experiment compares type-dependent
features and segment features, evaluating models
trained with these features against the local query
test set. No special modeling approach is applied
here; results are summarized in Table 3. We ob-
serve that by using type-dependent semantic fea-
tures only, we can achieve as much as by using
all semantic features. Since segment features only
convey proximity information while the base fea-
ture set aleady contain a systematic set of prox-
imity measures, the improvement through segment
features is not as significant as the the type depen-
dent ones.
6.3.3 Robustness of Semantic Features
Our final set of experiments aims at evaluating the
robustness of our semantic features by introducing
Table 3: Type-dependent Semantic Features vs.
Segment Features
Feature set DCG(5)
base + type dependent semantic features 8.23
base + segment features 8.19
base + all semantic features 8.25
simulated errors to the output of our query analy-
sis. Concretely, we manipulate the precision and
the recall of a specific type of entity tagger, t, on
the training and test set. To decrease the recall of
type t, we uniformly remove a set of a% tags of
type t ? preserving precision. To decrease preci-
sion, we uniformly select a set of query segments
(viewing the entity detection as simple segmenta-
tion, as detailed earlier) and assign the semantic
type t to those segments. Since the newly added
term group are selected from query segmentation
results, the introduced errors are rather semantic
type error than boundary error or proximity error.
The total number of newly assigned type t tags are
b% of the original number of type t tags in the
training set. By doing this, we decrease the preci-
sion of type t while keeping the recall of it at the
same level.
Suppose the original tagger achieves precision
p and recall r. By removing a% of tags, we have
estimated precision p? and recall r? defined as fol-
lows:
r? =
100r ? ar
100
,
p? = p.
By adding b% more term group to this type, we
have estimated precision and recall as
p? =
100p
100 + bp
,
r? = r.
In the experiment reported here we use BUSI-
NESS NAME as the target semantic type for this ro-
bustness experiment. An editorial test shows that
our tagger achieves 74% precision and 66% recall
based on a random set of human labeled queries
for this entity type. We train ranking models with
various values of a and b. When we reduce the
estimated recall, we evaluate these models against
the local test set since other data are not affected.
The results are summarized in Table 4.
When we reduce the precision, we evaluate the
resulting models against the general test set as
655
Table 4: Relevance with simulated error on local
query test set
a b p? r? DCG(5) Impr.
0 0 0.74 0.66 8.25 ?
10 0 0.74 0.594 8.21 0.48%
20 0 0.74 0.528 8.19 0.72%
40 0 0.74 0.396 8.18 0.85%
Table 5: Search relevance with simulated error for
semantic features on general test set
a b p? r? DCG(5) Impr.
0 0 0.74 0.66 10.11 -
0 10 0.689 0.66 10.11 0.00%
0 20 0.645 0.66 10.12 0.10%
0 40 0.571 0.66 10.12 0.10%
0 60 0.513 0.66 10.12 0.10%
0 80 0.465 0.66 10.11 0.00%
0 100 0.425 0.66 10.10 ?0.10%
simulated errors would virtually affect any sam-
ples with certain probability. Results appear in
Table 5. The results are quite interesting: when
the recall of business name entity decreases, we
observe statistically significant relevance degrada-
tion: if less entities are discovered, search rele-
vance is hit. The experiments with simulated pre-
cision error, however, are less conclusive. One
may note the experiments are conducted over the
general test set. Therefore, it is not clear if the pre-
cision of the NER system really has insignificant
impact on the IR relevance or just the impact is
diluted in a larger test set.
6.4 Case Analysis
In this section, we take a close look at a few
cases where our new semantic features help
most and where they fail. For the query sil-
verado ranch in irving texas, with no semantic
features, the ranking function ranks a local
listing page for this business, http://local.
yahoo.com/info-28646193, as the top
document. With semantic features, the ranking
function ranks the business home page: http:
//www.silveradoranchparties.com/
as top URL. Examining the two documents, the
local listing page actually contains much more rel-
evant anchor text, which are the among the most
salient features in traditional ranking models. The
home page, however, contains almost no relevant
anchor text: for a small business home page, this
is not a rare situation. Looking at the semantic
features of these two pages, the highest resolution
of location, the city name ?Irving,? appears in the
document body text 19 times in the local listing
page body text, and only 2 times in the home page
body text. The training process learns, then, that
for a query for a local business name (rather than
a business category), home pages?even with
fewer location terms in them?are likely to be
more relevant than a local listing page that usually
contain high frequency location terms.
In some cases, however, our new features do
hurt performance. For the query pa treasur-
ers office, the ranking function with no seman-
tic features ranks the document http://www.
patreasury.org highest, while the one with
semantic features ranks the page http://www.
pikepa.org/treasurer.htm higher. The
latter page is somewhat relevant: it is a treasurer?s
office in Pennsylvania. However, it belongs to a
specific county, which makes it less relevant than
the former page. This is a classic error that we ob-
serve: a mismatch of the intended location area.
While users are looking for state level business,
we provide results of county level. To resolve
this type of error, query analysis and semantic text
matching are no longer enough: here, the rank-
ing function needs to know that Pike County is a
county in Pennsylvania, Milford is a city in Pike
County, and neither are referred to by the user.
Document-side entity recognition, however, may
provide this type of information, helping to ad-
dress this type of errors.
7 Conclusion and Future Research
In this paper, we investigate how semantic features
can improve search relevance in a large-scale in-
formation retrieval setting; to our knowledge, it is
the first study of this approach on a web scale. We
present a set of features that incorporate semantic
and vertical knowledge into the retrieval process,
propose techniques to handle the sparseness prob-
lem for these features, and describe how they fit
in the learning process. We demonstrate that these
carefully designed features significantly improve
relevance, particularly for difficult queries ? long
queries with multiple entities.
The work reported here focuses on query-side
processing, avoiding the indexing cost of docu-
ment processing. We are currently investigating
document-side analysis to complement the query-
side work, and believe that this will further boost
the retrieval accuracy; we hope to report on this in
a follow-up study.
656
References
J. Allan and H. Raghavan. 2002. Using Part-of-Speech
Patterns to Reduce Query Ambiguity. In Proceed-
ings of SIGIR.
A. T. Arampatzis, Th. P. Weide, C. H. A. Koster,
and P. Bommel. 1990. Text Filtering using
Linguistically-motivated Indexing Terms. Techni-
cal Report CSI-R9901, Computing Science Institute,
University of Nijmegen, Nijmegen,The Netherlands.
Thorsten Brants. 2003. Natural Language Processing
in Information Retrieval. In Proceedings of CLIN.
C. Buckley, J. Allan, and G. Salton. 1993. Auto-
matic Routing and Ad-hoc Retrieval Using SMART:
TREC 2. In Proceedings of TREC-2.
Keke Chen, Rongqing Lu, C.K. Wong, Gordon Sun,
Larry Heck, and Belle Tseng. 2008. Trada: tree
based ranking function adaptation. In Proceedings
of CIKM.
W.B. Croft and D.J. Harper. 1979. Using Probabilistic
Models of Document Retrieval without Relevance
Information. Journal of Documentation, 37:285?
295.
W. Bruce Croft, Howard R. Turtle, and David D. Lewis.
1991. The Use of Phrases and Structured queries in
Information Retrieval. In Proceedings of SIGIR.
J. H. Friedman. 2002. Stochastic Gradient Boost-
ing. Computational Statistics and Data Analysis,
38(4):367?378.
N. Fuhr. 1992. Probabilistic Models in Information
Retrieval. The Computer Journal, 35:243?255.
K. Jarvelin and J. Kekalainen. 2000. IR Evalua-
tion Methods for Retrieving Highly Relevant Doc-
uments. In Proceedings of SIGIR.
J. Lafferty and C. Zhai. 2001. Document Language
Models, Query Models and Risk Minimization for
Information Retrieval. In Proceedings of SIGIR.
Donald Metzler and W. Bruce Croft. 2005. A markov
random field model for term dependencies. In Pro-
ceedings of SIGIR.
M. Narita and Y. Ogawa. 2000. The Use of Phrases
from Query Texts in Information Retrieval. In Pro-
ceedings of SIGIR.
Fuchun Peng, Ralph Weischedel, Ana Licuanan, and
Jinxi Xu. 2005. Combining Deep Linguistics
Analysis and Surface Pattern Learning: A Hybrid
Approach to Chinese Definitional Question Answer-
ing. In In Proceedings of the HLT-EMNLP.
J. Ponte and W.B. Croft. 2000. A Language Modeling
Approach to Informaiton Retrieval. In Proceedings
of SIGIR.
John Prager, Eric Brown, Anni Coden, and Dragomir
Radev. 2000. Question-answering by Predictive
Annotation. In Proceedings of SIGIR.
Stephen Robertson, Steve Walker, Susan Jones, Miche-
line Hancock-Beaulieu, and Mike Gatford. 1995.
Okapi at TREC-3. In Proceedings of TREC-3.
G. Salton, C. S. Yang, and C. T. Yu. 1975. A Theory of
Term Importance in Automatic Text Analysis. Jour-
nal of the Ameican Society of Information Science,
26:33?44.
Erik Tjong Kim Sang and Fien De Meulder. 2003.
Introduction to the CoNLL-2003 Shared Task:
Language-Independent Named Entity Recognition.
In Proceedings of CoNLL.
Dou Shen, Toby Walkery, Zijian Zheng, Qiang Yang,
and Ying Li. 2008. Personal Name Classification in
Web Queries. In Proceedings of WSDM.
A.F. Smeaton and C.J. van Rijsbergen. 1988. Experi-
ments on Incorporating Syntactic Processing of User
Queries into a Document Retrieval Stragegy. In Pro-
ceedings of SIGIR.
K. Sparck-Jones, 1999. What is the Role of NLP in Text
Retrieval, pages 1?25. Kluwer.
M. Srikanth and R.K. Srihari. 2003. Incorporating
Query Term Dependencies in Language Models for
Document Retrieval. In Proceedings of SIGIR.
Tomek Strzalkowski, Jose Perez-Carballo, Jussi Karl-
gren, Anette Hulth, Pasi Tapanainen, and Timo
Lahtinen. 1996. Natural Language Information Re-
trieval: TREC-8 Report. In Proceedings of TREC-8.
Tao Tao and ChengXiang Zhai. 2007. An exploration
of proximity measures in information retrieval. In
Proceedings of SIGIR.
X. Tong, C. Zhai, N. Millic-Frayling, and D Evans.
1996. Evaluation of Syntactic Phrase Indexing ?
CLARIT NLP Track Report. In Proceedings of
TREC-5.
E. Voohees. 1993. Using WordNet to Disambiguate
Word Senses for Text Retrieval. In Proceedings of
SIGIR.
Ellen Voorhees. 1999. Natural Language Processing
and Information Retrieval. Lecture Notes in Com-
puter Science, 1714:32?48.
Lee Wang, Chuang Wang, Xing Xie, Josh Forman,
Yansheng Lu, Wei-Ying Ma, and Ying Li. 2005.
Detecting dominant locations from search queries.
In Proceedings of SIGIR.
F. Wilcoxon. 1945. Individual Comparisons by Rank-
ing Methods. Biometrics, 1:80?83.
657
Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 173?176,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Chinese Named Entity Recognition with Conditional Probabilistic 
Models 
 
 
Aitao Chen 
Yahoo 
701 First Avenue 
Sunnyvale, CA 94089 
aitao@yahoo-inc.com 
Fuchun Peng 
Yahoo 
701 First Avenue 
Sunnyvale, CA 94089 
fuchun@yahoo-inc.com 
 
Roy Shan 
Yahoo 
701 First Avenue 
Sunnyvale, CA 94089 
                   rshan@yahoo-inc.com 
Gordon Sun 
Yahoo 
701 First Avenue 
Sunnyvale, CA 94089 
gzsun@yahoo-inc.com 
 
Abstract 
This paper describes the work on Chinese 
named entity recognition performed by 
Yahoo team at the third International 
Chinese Language Processing Bakeoff. 
We used two conditional probabilistic 
models for this task, including condi-
tional random fields (CRFs) and maxi-
mum entropy models. In particular, we 
trained two conditional random field rec-
ognizers and one maximum entropy rec-
ognizer for identifying names of people, 
places, and organizations in un-
segmented Chinese texts. Our best per-
formance is 86.2% F-score on MSRA 
dataset, and 88.53% on CITYU dataset. 
1 Introduction 
At the third International Chinese Language 
Processing Bakeoff, we participated in the closed 
test in the Named Entity Recognition (NER) task 
using the MSRA corpus and the CITYU corpus. 
The named entity types include person, place, 
and organization.  The training data consist of 
texts that are segmented into words with names 
of people, places, and organizations labeled. And 
the testing data consist of un-segmented Chinese 
texts, one sentence per line. 
There are many well known models for Eng-
lish named recognition, among which Condi-
tional Random Fields (Lafferty et al 2001) and 
maximum entropy models (Berger et al 2001) 
have achieved good performance in English in 
CoNLL NER tasks. To understand the perform-
ance of these two models on Chinese, we both 
models to Chinese NER task on MSRA data and 
CITYU data.  
2 Named Entity Recognizer 
2.1 Models 
We trained two named entity recognizers based 
on conditional random field and one based on 
maximum entropy model.  Both conditional ran-
dom field and maximum entropy models are ca-
pable of modeling arbitrary features of the input, 
thus are well suit for many language processing 
tasks. However, there exist significant differ-
ences between these two models. To apply a 
maximum entropy model to NER task, we have 
to first train a maximum entropy classifier to 
classify each individual word and then build a 
dynamic programming for sequence decoding. 
While in CRFs, these two steps are integrated 
together. Thus, in theory, CRFs are superior to 
maximum entropy models in sequence modeling 
problem and this will also confirmed in our Chi-
nese NER experiments. The superiority of CRFs 
on Chinese information processing was also 
demonstrated in word segmentation (Peng et al 
2004). However, the training speed of CRFs is 
much slower than that of maximum entropy 
models since training CRFs requires expensive 
forward-backward algorithm to compute the par-
tition function. 
 
173
We used Taku?s CRF package1  to train the first 
CRF recognizer, and the MALLET 2  package 
with BFGS optimization to train the second CRF 
recognizer. We used a C++ implementation3 of 
maximum entropy modeling and wrote our own 
second order dynamic programming for decod-
ing. 
 
2.2 Features 
The first CRF recognizer used the features C-2, C-
1, C0, C-1, C2, C-2C-1, C-1C0, C0C-1, C1C2, and C-
1C1, where C0 is the current character, C1 the next 
character, C2 the second character after C0, C-1 
the character preceding C0, and C-2 the second 
character before C0.  
The second CRF recognizer used the same set 
of basic features but the feature C2. In addition, 
the first CRF recognizer used the tag bigram fea-
ture, and the second CRF recognizer used word 
and character cluster features, obtained automati-
cally from the training data only with distribu-
tional word clustering (Tishby and Lee, 1993). 
The maximum entropy recognizer used the 
following unigram, bigram features, and type 
features: C-2, C-1, C0, C1, C2, C-4C-3, C-3C-2, C-2C-1, 
C-1C0, C0C1, C1C2, C2C3, C3C4, and T-2T-1. 
When using the first CRF package, we found 
the labeling scheme OBIE performs better than 
the OBIE scheme.  In the OBI scheme, the first 
character of a named entity is labeled as ?B?, the 
remaining characters, including the last character, 
are all labeled as ?I?. And any character that is 
not part of a named entity is labeled as ?O?. In 
the OBIE scheme, the last character of a named 
entity is labeled as ?E?. The other characters are 
labeled in the same way as in OBIE scheme. The 
first CRF recognizer used the OBIE labeling 
scheme, and the second CRF recognizer used the 
OBI scheme. 
We tried a window size of seven characters 
(three characters preceding the current character 
and three characters following the current char-
acter) with almost no difference in performance 
from using the window size of five characters. 
When a named entity occurs frequently in the 
training data, there is a very good chance that it 
will be recognized when appearing in the testing 
data. However, for entity names of rare occur-
rence, they are much harder to recognize in the 
                                                 
1 Available from http://chasen.org/~taku/software/CRF++ 
2 Available at http://mallet.cs.umass.edu 
3 Available at 
http://homepages.inf.ed.ac.uk/s0450736/maxent_toolkit.htm 
testing data. Thus it may be beneficial to exam-
ine the testing data to identify the named entities 
that occur in the training data, and assign them 
the same label as in the training data. From the 
training data, we extracted the person names of 
at least three characters, the place names of at 
least four characters, and the organization names 
of at least four characters. We removed from the 
dictionary the named entities that are also com-
mon words. We did not include the short names 
in the dictionary because they may be part of 
long names. We produced a run first using one of 
the NER recognizers, and then replaced the la-
bels of a named entity assigned by a recognizer 
with the labels of the same named entity in the 
training data without considering the contexts.  
3 Results 
Run ID Precision Recall F-Score 
msra_a 91.22% 81.71% 86.20 
msra_b 88.43% 82.88% 85.56 
msra_f 88.45% 79.31% 83.63 
msra_g 86.61% 80.32% 83.35 
msra_r 87.48% 71.68% 78.80 
Table 1: Official results in the closed test of the 
NER task on MSRA corpus. 
 
Table 1 presents the official results of five runs 
in the closed test of the NER task on MSRA cor-
pus.  The first two runs, msra_a and msra_b, are 
produced using the first CRF recognizer; the next 
two runs, msra_f and msra_g, are produced using 
the second CRF recognizer which used randomly 
selected 90% of the MSRA training data. When 
we retrained the second CRF recognizer with the 
whole set of the MSRA training data, the overall 
F-Score is 85.00, precision 90.28%, and recall 
80.31%. The last run, msra_r, is produced using 
the MaxEnt recognizer.  
    The msra_a run used the set of basic features 
with a window size of five characters. Slightly 
over eight millions features are generated from 
the MSRA training data, excluding features oc-
curred only once. The training took 321 itera-
tions to complete.  The msra_b run is produced 
from the msra_a run by substituting the labels 
assigned by the recognizer to a named entity with 
the labels of the named entity in the training data 
if it occurs in the training data.  For example, in 
the MSRA training data, the text????? in 
the sentence ???????????  is 
tagged as a place name. The same entity also ap-
peared in MSRA testing data set. The first CRF 
recognizer failed to mark the text ????? as 
174
a place name instead it tagged ??? as a per-
son name. In post-processing, the text????
? in the testing data is re-tagged as a place name. 
As another example, the person name ??? 
appears both in the training data and in the test-
ing data. The first CRF recognizer failed to rec-
ognize it as a person name. In post-processing 
the text ??? is tagged as a person name be-
cause it appears in the training data as a person 
name. The text ??????????????
???? was correctly tagged as an organization 
name. It is not in the training data, but the texts 
??????, ?????????, and ????
??? are present in the training data and are all 
labeled as organization names. In our post-
processing, the correctly tagged organization 
name is re-tagged incorrectly as three organiza-
tion names. This is the main reason why the per-
formance of the organization name got much 
worse than that without post-processing. 
 
 Precision Recall F-score 
LOC 94.19% 87.14% 90.53 
ORG 83.59% 80.39% 81.96 
PER 92.35% 74.66% 82.57 
Table 2: The performance of the msra_a run bro-
ken down by entity type. 
 
 Precision Recall F-score 
LOC 93.09% 87.35% 90.13 
ORG 75.51% 78.51 76.98 
PER 91.52 79.27 84.95 
Table 3: The performance of the msra_b run bro-
ken down by entity type. 
 
Table 2 presents the performance of the msra_a 
run by entity type. Table 3 shows the perform-
ance of the msra_b run by entity type. While the 
post-processing improved the performance of 
person name recognition, but it degraded the per-
formance of organization name recognition. 
Overall the performance was worse than that 
without post-processing. In our development 
testing, we saw large improvement in organiza-
tion name recognition with post-processing.      
 
Run ID Precision Recall F-Score 
cityu_a 92.66% 84.75% 88.53 
cityu_b 92.42% 84.91% 88.50 
cityu_f 91.88% 82.31% 86.83 
cityu_g 91.64% 82.46% 86.81 
Table 4: Official results in the closed test of the 
NER task on CITYU corpus. 
 
Table 4 presents the official results of four runs 
in the closed test of the NER task on CITYU cor-
pus.  The first two runs, msra_a and msra_b, are 
produced using the first CRF recognizer; the next 
two runs, msra_f and msra_g, are produced using 
the second CRF recognizer. The system configu-
rations are the same as used on the MSRA cor-
pus. The cityu_b run is produced from cityu_a 
run with post-processing, and the cityu_g run 
produced from cityu_f run with post-processing. 
We used the whole set of CITYU to train the first 
CRF model, and 80% of the CITYU training data 
to train the second CRF model. No results on full 
training data are available at the time of submis-
sion. 
All the runs we submitted are based characters. 
We tried word-based approach but found it was 
not as effective as character-based approach.  
4 Discussions 
Table 4 is shows the confusion matrix of the la-
bels. The rows are the true labels and the col-
umns are the predicated labels. An entry at row x 
and column y in the table is the number of char-
acters that are predicated as y while the true label 
is x. Ideally, all entries except the diagonal 
should be zero.   
 
The table was obtained from the result of our 
development dataset for MSRA data, which are 
the last 9,364 sentences of the MSRA training 
data (we used the first 37,000 sentences for train-
ing in the model developing phase). As we can 
see, most of the errors lie in the first column, in-
dicating many of the entities labels are predi-
cated as O. This resulted low recall for entities. 
Another major error is on detecting the begin-
ning of ORG (B-O). Many of them are misla-
beled as O and beginning of location (B-L), re-
sulting low recall and low precision for ORG.  
 
 O B-L I-L B-O I-O B-P I-P 
O 406798 86 196 213 973 46 111 
B-L 463 5185 54 73 29 19 7 
I-L 852 25 6836 0 197 1 44 
B-O 464 141 3 2693 62 17 0 
I-O 1861 28 276 55 12626 2 39 
B-P 472 16 2 22 3 2998 8 
I-P 618 0 14 1 49 10 5502 
Table 4: Confusion matrix of on the MSRA de-
velopment dataset  
 
A second interesting thing to notice is the 
numbers presented in Table 2. They may suggest 
that person name recognition is more difficult 
175
than location name recognition, which is con-
trary to what we believe, since Chinese person 
names are short and have strict structure and they 
should be easier to recognize than both location 
and organization names. We examined the 
MSRA testing data and found out that 617 out 
1,973 person names occur in a single sentence as 
a list of person names. In this case, simple rule 
may be more effective. When we excluded the 
sentence with 617 person names, for person 
name recognition of our msra_a run, the F-score 
is 90.74, precision 93.44%, and recall 88.20%. 
Out of the 500 person names that were not rec-
ognized in our msra_a run, 340 occurred on the 
same line of 617 person names. 
5 Conclusions 
We applied Conditional Random Fields and 
maximum entropy models to Chinese NER tasks 
and achieved satisfying performance. Three sys-
tems with different implementations and differ-
ent features are reported. Overall, CRFs are su-
perior to maximum entropy models in Chinese 
NER tasks. Useful features include using BIOE 
tags instead of BIO tags and word and character 
clustering features.  
References 
Adam Berger, Stephen Della Pietra, and Vincent 
Della Pietra, A Maximum Entropy Approach to 
Natural Language Processing, Computational Lin-
guistics, 22 (1) 
John Lafferty, Andrew McCallum, and Fernando 
Pereira, Conditional random fields: Probabilistic 
models for segmenting and labeling sequence data. 
In: Proc. 18th International Conf. on Machine 
Learning, Morgan Kaufmann, San Francisco, CA 
(2001) 282?289 
Fuchun Peng, Fangfang Feng, and Andrew 
McCallum, Chinese Segmentation and New Word 
Detection using Conditional Random Fields, In 
Proceedings of The 20th International Conference 
on Computational Linguistics (COLING 2004) , 
pages 562-568, August 23-27, 2004, Geneva, Swit-
zerland 
Naftali Tishby and Lillian Lee, Distributional Cluster-
ing of English Words, In Proceedings of the 31st 
Annual Conference of Association for Computa-
tional Linguistics, pp 183--190, 1993. 
176
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 307?314, Vancouver, October 2005. c?2005 Association for Computational Linguistics
Combining Deep Linguistics Analysis and Surface Pattern Learning:
A Hybrid Approach to Chinese Definitional Question Answering
Fuchun Peng, Ralph Weischedel, Ana Licuanan, Jinxi Xu
BBN Technologies
50 Moulton Street, Cambridge, MA, 02138
 
fpeng, rweisched, alicuan, jxu  @bbn.com
Abstract
We explore a hybrid approach for Chinese
definitional question answering by com-
bining deep linguistic analysis with sur-
face pattern learning. We answer four
questions in this study: 1) How helpful are
linguistic analysis and pattern learning? 2)
What kind of questions can be answered
by pattern matching? 3) How much an-
notation is required for a pattern-based
system to achieve good performance? 4)
What linguistic features are most useful?
Extensive experiments are conducted on
biographical questions and other defini-
tional questions. Major findings include:
1) linguistic analysis and pattern learning
are complementary; both are required to
make a good definitional QA system; 2)
pattern matching is very effective in an-
swering biographical questions while less
effective for other definitional questions;
3) only a small amount of annotation is
required for a pattern learning system to
achieve good performance on biographi-
cal questions; 4) the most useful linguistic
features are copulas and appositives; re-
lations also play an important role; only
some propositions convey vital facts.
1 Introduction
Due to the ever increasing large amounts of online
textual data, learning from textual data is becom-
ing more and more important. Traditional document
retrieval systems return a set of relevant documents
and leave the users to locate the specific information
they are interested in. Question answering, which
combines traditional document retrieval and infor-
mation extraction, solves this problem directly by
returning users the specific answers. Research in
textual question answering has made substantial ad-
vances in the past few years (Voorhees, 2004).
Most question answering research has been focus-
ing on factoid questions where the goal is to return
a list of facts about a concept. Definitional ques-
tions, however, remain largely unexplored. Defini-
tional questions differ from factoid questions in that
the goal is to return the relevant ?answer nuggets?
of information about a query. Identifying such an-
swer nuggets requires more advanced language pro-
cessing techniques. Definitional QA systems are
not only interesting as a research challenge. They
also have the potential to be a valuable comple-
ment to static knowledge sources like encyclopedias.
This is because they create definitions dynamically,
and thus answer definitional questions about terms
which are new or emerging (Blair-Goldensoha et
al., 2004).
One success in factoid question answering
is pattern based systems, either manually con-
structed (Soubbotin and Soubbotin, 2002) or ma-
chine learned (Cui et al, 2004). However, it is
unknown whether such pure pattern based systems
work well on definitional questions where answers
are more diverse.
Deep linguistic analysis has been found useful in
factoid question answering (Moldovan et al, 2002)
and has been used for definitional questions (Xu et
al., 2004; Harabagiu et al, 2003). Linguistic analy-
307
sis is useful because full parsing captures long dis-
tance dependencies between the answers and the
query terms, and provides more information for in-
ference. However, merely linguistic analysis may
not be enough. First, current state of the art lin-
guistic analysis such as parsing, co-reference, and
relation extraction is still far below human perfor-
mance. Errors made in this stage will propagate and
lower system accuracy. Second, answers to some
types of definitional questions may have strong local
dependencies that can be better captured by surface
patterns. Thus we believe that combining linguistic
analysis and pattern learning would be complemen-
tary and be beneficial to the whole system.
Work in combining linguistic analysis with pat-
terns include Weischedel et al (2004) and Jijkoun et
al. (2004) where manually constructed patterns are
used to augment linguistic features. However, man-
ual pattern construction critically depends on the do-
main knowledge of the pattern designer and often
has low coverage (Jijkoun et al, 2004). Automatic
pattern derivation is more appealing (Ravichandran
and Hovy, 2002).
In this work, we explore a hybrid approach to
combining deep linguistic analysis with automatic
pattern learning. We are interested in answering
the following four questions for Chinese definitional
question answering:
  How helpful are linguistic analysis and pattern
learning in definitional question answering?
  If pattern learning is useful, what kind of ques-
tion can pattern matching answer?
  How much human annotation is required for a
pattern based system to achieve reasonable per-
formance?
  If linguistic analysis is helpful, what linguistic
features are most useful?
To our knowledge, this is the first formal study of
these questions in Chinese definitional QA. To an-
swer these questions, we perform extensive experi-
ments on Chinese TDT4 data (Linguistic Data Con-
sortium, 2002-2003). We separate definitional ques-
tions into biographical (Who-is) questions and other
definitional (What-is) questions. We annotate some
question-answer snippets for pattern learning and
we perform deep linguistic analysis including pars-
ing, tagging, name entity recognition, co-reference,
and relation detection.
2 A Hybrid Approach to Definitional Ques-
tion Answering
The architecture of our QA system is shown in Fig-
ure 1. Given a question, we first use simple rules to
classify it as a ?Who-is? or ?What-is? question and
detect key words. Then we use a HMM-based IR
system (Miller et al, 1999) for document retrieval
by treating the question keywords as a query. To
speed up processing, we only use the top 1000 rel-
evant documents. We then select relevant sentences
among the returned relevant documents. A sentence
is considered relevant if it contains the query key-
word or contains a word that is co-referent to the
query term. Coreference is determined using an in-
formation extraction engine, SERIF (Ramshaw et
al., 2001). We then conduct deep linguistic anal-
ysis and pattern matching to extract candidate an-
swers. We rank all candidate answers by predeter-
mined feature ordering. At the same time, we per-
form redundancy detection based on  -gram over-
lap.
2.1 Deep Linguistic Analysis
We use SERIF (Ramshaw et al, 2001), a linguistic
analysis engine, to perform full parsing, name entity
detection, relation detection, and co-reference reso-
lution. We extract the following linguistic features:
1. Copula: a copula is a linking verb such as ?is?
or ?become?. An example of a copula feature
is ?Bill Gates is the CEO of Microsoft?. In this
case, ?CEO of Microsoft? will be extracted as
an answer to ?Who is Bill Gates??. To extract
copulas, SERIF traverses the parse trees of the
sentences and extracts copulas based on rules.
In Chinese, the rule for identifying a copula is
the POS tag ?VC?, standing for ?Verb Copula?.
The only copula verb in Chinese is ?

?.
2. Apposition: appositions are a pair of noun
phrases in which one modifies the other. For
example, In ?Tony Blair, the British Prime Min-
ister, ...?, the phrase ?the British Prime Min-
ister? is in apposition to ?Blair?. Extraction
of appositive features is similar to that of cop-
ula. SERIF traverses the parse tree and iden-
tifies appositives based on rules. A detailed
description of the algorithm is documented
308
Question Classification
Document Retrieval
Linguistic Analysis
Semantic Processing
Phrase Ranking
Redundancy Remove
Lists of Response
Answer Annotation
Name Tagging
Parsing
Preposition finding
Co?reference
Relation Extraction Training data
TreeBank
Name Annotation
Linguistic motivated
Pattern motivated
Question
Pattern MatchingPattern Learning
Figure 1: Question answering system structure
in (Ramshaw et al, 2001).
3. Proposition: propositions represent predicate-
argument structures and take the form:
predicate(    : 	 
  , ...,    :  
  ). The
most common roles include logical subject,
logical object, and object of a prepositional
phrase that modifies the predicate. For ex-
ample, ?Smith went to Spain? is represented
as a proposition, went(logical subject: Smith,
PP-to: Spain).
4. Relations: The SERIF linguistic analysis en-
gine also extracts relations between two ob-
jects. SERIF can extract 24 binary relations
defined in the ACE guidelines (Linguistic Data
Consortium, 2002), such as spouse-of, staff-of,
parent-of, management-of and so forth. Based
on question types, we use different relations, as
listed in Table 1.
Relations used for Who-Is questions
ROLE/MANAGEMENT, ROLE/GENERAL-STAFF,
ROLE/CITIZEN-OF, ROLE/FOUNDER,
ROLE/OWNER, AT/RESIDENCE,
SOC/SPOUSE, SOC/PARENT,
ROLE/MEMBER, SOC/OTHER-PROFESSIONAL
Relation used for What-Is questions
AT/BASED-IN, AT/LOCATED, PART/PART-OF
Table 1: Relations used in our system
Many relevant sentences do not contain the query
key words. Instead, they contain words that are co-
referent to the query. For example, in ?Yesterday UN
Secretary General Anan Requested Every Side...,
He said ... ?. The pronoun ?He? in the second sen-
tence refers to ?Anan? in the first sentence. To select
such sentences, we conduct co-reference resolution
using SERIF.
In addition, SERIF also provides name tagging,
identifying 29 types of entity names or descriptions,
such as locations, persons, organizations, and dis-
eases.
We also select complete sentences mentioning the
term being defined as backup answers if no other
features are identified.
The component performance of our linguistic
analysis is shown in Table 2.
Pre. Recall F
Parsing 0.813 0.828 0.820
Co-reference 0.920 0.897 0.908
Name-entity detection 0.765 0.753 0.759
Table 2: Linguistic analysis component performance
for Chinese
2.2 Surface Pattern Learning
We use two kinds of patterns: manually constructed
patterns and automatically derived patterns. A man-
ual pattern is a commonly used linguistic expression
that specifies aliases, super/subclass and member-
ship relations of a term (Xu et al, 2004). For exam-
ple, the expression ?tsunamis, also known as tidal
waves? gives an alternative term for tsunamis. We
309
use 23 manual patterns for Who-is questions and 14
manual patterns for What-is questions.
We also classify some special propositions as
manual patterns since they are specified by compu-
tational linguists. After a proposition is extracted,
it is matched against a list of predefined predicates.
If it is on the list, it is considered special and will
be ranked higher. In total, we designed 22 spe-
cial propositions for Who-is questions, such as  
 (become),   (elected as), and  (resign),
14 for What-is questions, such as 
	 (located at),

	 (created at), and   (also known as).
However, it is hard to manually construct such
patterns since it largely depends on the knowledge
of the pattern designer. Thus, we prefer patterns
that can be automatically derived from training data.
Some annotators labeled question-answer snippets.
Given a query question, the annotators were asked
to highlight the strings that can answer the question.
Though such a process still requires annotators to
have knowledge of what can be answers, it does not
require a computational linguist. Our pattern learn-
ing procedure is illustrated in Figure 2.
Generate Answer Snippet
Pattern Generalization
Pattern Selection
POS Tagging
Merging POS Tagging
and Answer Tagging
Answer Annotation
Figure 2: Surface Pattern Learning
Here we give an example to illustrate how pat-
tern learning works. The first step is annotation. An
example of Chinese answer annotation with English
translation is shown in Figure 3. Question words are
assigned the tag QTERM, answer words are tagged
ANSWER, and all other words are assigned BKGD,
standing for background words (not shown in the ex-
ample to make the annotation more readable).
To obtain patterns, we conduct full parsing to ob-
tain the full parse tree for a sentence. In our current
Chinese annotation:  ? Chinese Segmentation and New Word Detection
using Conditional Random Fields
Fuchun Peng, Fangfang Feng, Andrew McCallum
Computer Science Department, University of Massachusetts Amherst
140 Governors Drive, Amherst, MA, U.S.A. 01003
{fuchun, feng, mccallum}@cs.umass.edu
Abstract
Chinese word segmentation is a difficult, im-
portant and widely-studied sequence modeling
problem. This paper demonstrates the abil-
ity of linear-chain conditional random fields
(CRFs) to perform robust and accurate Chi-
nese word segmentation by providing a prin-
cipled framework that easily supports the in-
tegration of domain knowledge in the form of
multiple lexicons of characters and words. We
also present a probabilistic new word detection
method, which further improves performance.
Our system is evaluated on four datasets used
in a recent comprehensive Chinese word seg-
mentation competition. State-of-the-art perfor-
mance is obtained.
1 Introduction
Unlike English and other western languages, many
Asian languages such as Chinese, Japanese, and
Thai, do not delimit words by white-space. Word
segmentation is therefore a key precursor for lan-
guage processing tasks in these languages. For Chi-
nese, there has been significant research on find-
ing word boundaries in unsegmented sequences
(see (Sproat and Shih, 2002) for a review). Un-
fortunately, building a Chinese word segmentation
system is complicated by the fact that there is no
standard definition of word boundaries in Chinese.
Approaches to Chinese segmentation fall roughly
into two categories: heuristic dictionary-based
methods and statistical machine learning methods.
In dictionary-based methods, a predefined dictio-
nary is used along with hand-generated rules for
segmenting input sequence (Wu, 1999). However
these approaches have been limited by the impos-
sibility of creating a lexicon that includes all pos-
sible Chinese words and by the lack of robust sta-
tistical inference in the rules. Machine learning ap-
proaches are more desirable and have been success-
ful in both unsupervised learning (Peng and Schuur-
mans, 2001) and supervised learning (Teahan et al,
2000).
Many current approaches suffer from either lack
of exact inference over sequences or difficulty in in-
corporating domain knowledge effectively into seg-
mentation. Domain knowledge is either not used,
used in a limited way, or used in a complicated way
spread across different components. For example,
the N-gram generative language modeling based ap-
proach of Teahan et al(2000) does not use domain
knowledge. Gao et al(2003) uses class-based lan-
guage for word segmentation where some word cat-
egory information can be incorporated. Zhang et
al (2003) use a hierarchical hidden Markov Model
to incorporate lexical knowledge. A recent advance
in this area is Xue (2003), in which the author uses
a sliding-window maximum entropy classifier to tag
Chinese characters into one of four position tags,
and then covert these tags into a segmentation using
rules. Maximum entropy models give tremendous
flexibility to incorporate arbitrary features. How-
ever, a traditional maximum entropy tagger, as used
in Xue (2003), labels characters without considering
dependencies among the predicted segmentation la-
bels that is inherent in the state transitions of finite-
state sequence models.
Linear-chain conditional random fields (CRFs)
(Lafferty et al, 2001) are models that address
both issues above. Unlike heuristic methods, they
are principled probabilistic finite state models on
which exact inference over sequences can be ef-
ficiently performed. Unlike generative N-gram or
hidden Markov models, they have the ability to
straightforwardly combine rich domain knowledge,
for example in this paper, in the form of multiple
readily-available lexicons. Furthermore, they are
discriminatively-trained, and are often more accu-
rate than generative models, even with the same fea-
tures. In their most general form, CRFs are arbitrary
undirected graphical models trained to maximize
the conditional probability of the desired outputs
given the corresponding inputs. In the linear-chain
special case we use here, they can be roughly un-
derstood as discriminatively-trained hidden Markov
models with next-state transition functions repre-
sented by exponential models (as in maximum en-
tropy classifiers), and with great flexibility to view
the observation sequence in terms of arbitrary, over-
lapping features, with long-range dependencies, and
at multiple levels of granularity. These beneficial
properties suggests that CRFs are a promising ap-
proach for Chinese word segmentation.
New word detection is one of the most impor-
tant problems in Chinese information processing.
Many machine learning approaches have been pro-
posed (Chen and Bai, 1998; Wu and Jiang, 2000;
Nie et al, 1995). New word detection is normally
considered as a separate process from segmentation.
However, integrating them would benefit both seg-
mentation and new word detection. CRFs provide a
convenient framework for doing this. They can pro-
duce not only a segmentation, but also confidence
in local segmentation decisions, which can be used
to find new, unfamiliar character sequences sur-
rounded by high-confidence segmentations. Thus,
our new word detection is not a stand-alone process,
but an integral part of segmentation. Newly detected
words are re-incorporated into our word lexicon,
and used to improve segmentation. Improved seg-
mentation can then be further used to improve new
word detection.
Comparing Chinese word segmentation accuracy
across systems can be difficult because many re-
search papers use different data sets and different
ground-rules. Some published results claim 98% or
99% segmentation precision and recall, but these ei-
ther count only the words that occur in the lexicon,
or use unrealistically simple data, lexicons that have
extremely small (or artificially non-existant) out-
of-vocabulary rates, short sentences or many num-
bers. A recent Chinese word segmentation competi-
tion (Sproat and Emerson, 2003) has made compar-
isons easier. The competition provided four datasets
with significantly different segmentation guidelines,
and consistent train-test splits. The performance of
participating system varies significantly across dif-
ferent datasets. Our system achieves top perfor-
mance in two of the runs, and a state-of-the-art per-
formance on average. This indicates that CRFs are a
viable model for robust Chinese word segmentation.
2 Conditional Random Fields
Conditional random fields (CRFs) are undirected
graphical models trained to maximize a conditional
probability (Lafferty et al, 2001). A common
special-case graph structure is a linear chain, which
corresponds to a finite state machine, and is suitable
for sequence labeling. A linear-chain CRF with pa-
rameters ? = {?1, ...} defines a conditional proba-
bility for a state (label) sequence y = y1...yT (for
example, labels indicating where words start or have
their interior) given an input sequence x = x1...xT
(for example, the characters of a Chinese sentence)
to be
P?(y|x) = 1Zx exp
( T?
t=1
?
k
?kfk(yt?1, yt,x, t)
)
,
(1)
where Zx is the per-input normalization that makes
the probability of all state sequences sum to one;
fk(yt?1, yt,x, t) is a feature function which is of-
ten binary-valued, but can be real-valued, and ?k is
a learned weight associated with feature fk. The
feature functions can measure any aspect of a state
transition, yt?1 ? yt, and the entire observation se-
quence, x, centered at the current time step, t. For
example, one feature function might have value 1
when yt?1 is the state START, yt is the state NOT-
START, and xt is a word appearing in a lexicon of
people?s first names. Large positive values for ?k
indicate a preference for such an event; large nega-
tive values make the event unlikely.
The most probable label sequence for an input x,
y? = argmaxy P?(y|x),
can be efficiently determined using the Viterbi al-
gorithm (Rabiner, 1990). An N -best list of label-
ing sequences can also be obtained using modi-
fied Viterbi algorithm and A* search (Schwartz and
Chow, 1990).
The parameters can be estimated by maximum
likelihood?maximizing the conditional probability
of a set of label sequences, each given their cor-
responding input sequences. The log-likelihood of
training set {(xi, yi) : i = 1, ...M} is written
L? =
?
i
logP?(yi|xi)
=
?
i
( T?
t=1
?
k
?kfk(yt?1, yt,x, t)? logZxi
)
.
Traditional maximum entropy learning algorithms,
such as GIS and IIS (della Pietra et al, 1995), can
be used to train CRFs. However, our implemen-
tation uses a quasi-Newton gradient-climber BFGS
for optimization, which has been shown to converge
much faster (Malouf, 2002; Sha and Pereira, 2003).
The gradient of the likelihood is ?P?(y|x)/??k =
?
i,t
fk(yt?1, y(i)t ,x(i), t)
?
?
i,y,t
P?(y|x(i))fk(yt?1, yt,x(i), t)
CRFs share many of the advantageous properties
of standard maximum entropy classifiers, including
their convex likelihood function, which guarantees
that the learning procedure converges to the global
maximum.
2.1 Regularization in CRFs
To avoid over-fitting, log-likelihood is usually pe-
nalized by some prior distribution over the parame-
ters. A commonly used prior is a zero-mean Gaus-
sian. With a Gaussian prior, log-likelihood is penal-
ized as follows.
L? =
?
i
logP?(yi|xi)?
?
k
?2k
2?2k
(2)
where ?2k is the variance for feature dimension k.
The variance can be feature dependent. However
for simplicity, constant variance is often used for
all features. We experiment an alternate version of
Gaussian prior in which the variance is feature de-
pendent. We bin features by frequency in the train-
ing set, and let the features in the same bin share
the same variance. The discounted value is set to be
?k
dck/Me??2 where ck is the count of features, M is
the bin size set by held out validation, and dae is the
ceiling function. See Peng and McCallum (2004)
for more details and further experiments.
2.2 State transition features
Varying state-transition structures with different
Markov order can be specified by different CRF
feature functions, as determined by the number of
output labels y examined together in a feature func-
tion. We define four different state transition feature
functions corresponding to different Markov orders.
Higher-order features capture more long-range de-
pendencies, but also cause more data sparseness
problems and require more memory for training.
The best Markov order for a particular application
can be selected by held-out cross-validation.
1. First-order: Here the inputs are examined in
the context of the current state only. The
feature functions are represented as f(yt,x).
There are no separate parameters for state tran-
sitions.
2. First-order+transitions: Here we add parame-
ters corresponding to state transitions. The fea-
ture functions used are f(yt,x), f(yt?1, yt).
3. Second-order: Here inputs are examined in the
context of the current and previous states. Fea-
ture function are represented as f(yt?1, yt,x).
4. Third-order: Here inputs are examined in
the context of the current, and two previous
states. Feature function are represented as
f(yt?2, yt?1, yt,x).
3 CRFs for Word Segmentation
We cast the segmentation problem as one of se-
quence tagging: Chinese characters that begin a new
word are given the START tag, and characters in
the middle and at the end of words are given the
NONSTART tag. The task of segmenting new, un-
segmented test data becomes a matter of assigning
a sequence of tags (labels) to the input sequence of
Chinese characters.
Conditional random fields are configured as a
linear-chain (finite state machine) for this purpose,
and tagging is performed using the Viterbi algo-
rithm to efficiently find the most likely label se-
quence for a given character sequence.
3.1 Lexicon features as domain knowledge
One advantage of CRFs (as well as traditional max-
imum entropy models) is its flexibility in using ar-
bitrary features of the input. To explore this advan-
tage, as well as the importance of domain knowl-
edge, we use many open features from external re-
sources. To specifically evaluate the importance of
domain knowledge beyond the training data, we di-
vide our features into two categories: closed fea-
tures and open features, (i.e., features allowed in the
competition?s ?closed test? and ?open test? respec-
tively). The open features include a large word list
(containing single and multiple-character words), a
character list, and additional topic or part-of-speech
character lexicons obtained from various sources.
The closed features are obtained from training data
alone, by intersecting the character list obtained
from training data with corresponding open lexi-
cons.
Many lexicons of Chinese words and characters
are available from the Internet and other sources.
Besides the word list and character list, our lexicons
include 24 lists of Chinese words and characters ob-
tained from several Internet sites1 cleaned and aug-
mented by a local native Chinese speaker indepen-
dently of the competition data. The list of lexicons
used in our experiments is shown in Figure 1.
3.2 Feature conjunctions
Since CRFs are log-linear models, feature conjunc-
tions are required to form complex, non-linear de-
cision boundaries in the original feature space. We
1http://www.mandarintools.com,
ftp://xcin.linux.org.tw/pub/xcin/libtabe,
http://www.geocities.com/hao510/wordlist
noun (e.g.,?,?) verb (e.g.,?)
adjective (e.g.,?,?) adverb (e.g.,!,?)
auxiliary (e.g.,,?) preposition (e.g.,?)
number (e.g.,,) negative (e.g.,X,:)
determiner (e.g.,?,?,Y) function (e.g. ?,?)
letter (English character) punctuation (e.g., # $)
last name (e.g.,K) foreign name (e.g.,?)
maybe last-name (e.g.,?,[) plural character (e.g.,?,?)
pronoun (e.g.,fi,?,?) unit character (e.g.,G,?)
country name (e.g.,?,?) Chinese place name (e.g.,?)
organization name title suffix (e.g.,?,?)
title prefix (e.g.,,?) date (e.g.,#,?,?)
Figure 1: Lexicons used in our experiments
C?2: second previous character in lexicon
C?1: previous character in lexicon
C1: next character in lexicon
C2: second next character in lexicon
C0C1: current and next character in lexicon
C?1C0: current and previous character in lexicon
C?2C?1: previous two characters in lexicon
C?1C0C1: previous, current, and next character in the lexicon
Figure 2: Feature conjunctions used in experiments
use feature conjunctions in both the open and closed
tests, as listed Figure 2.
4 Probabilistic New Word Identification
Since no vocabulary list could ever be complete,
new word (unknown word) identification is an im-
portant issue in Chinese segmentation. Unknown
words cause segmentation errors in that these out-
of-vocabulary words in input text are often in-
correctly segmented into single-character or other
overly-short words (Chen and Bai, 1998). Tradi-
tionally, new word detection has been considered as
a standalone process. We consider here new word
detection as an integral part of segmentation, aiming
to improve both segmentation and new word detec-
tion: detected new words are added to the word list
lexicon in order to improve segmentation; improved
segmentation can potentially further improve new
word detection. We measure the performance of
new word detection by its improvements on seg-
mentation.
Given a word segmentation proposed by the CRF,
we can compute a confidence in each segment. We
detect as new words those that are not in the existing
word list, yet are either highly confident segments,
or low confident segments that are surrounded by
high confident words. A confidence threshold of 0.9
is determined by cross-validation.
Segment confidence is estimated using con-
strained forward-backward (Culotta and McCal-
lum, 2004). The standard forward-backward algo-
rithm (Rabiner, 1990) calculates Zx, the total like-
lihood of all label sequences y given a sequence x.
Constrained forward-backward algorithm calculates
Z ?x, total likelihood of all paths passing through
a constrained segment (in our case, a sequence of
characters starting with a START tag followed by a
few NONSTART tags before the next START tag).
The confidence in this segment is then Z
?
x
Zx , a real
number between 0 and 1.
In order to increase recall of new words, we con-
sider not only the most likely (Viterbi) segmen-
tation, but the segmentations in the top N most
likely segmentations (an N -best list), and detect
new words according to the above criteria in all N
segmentations.
Many errors can be corrected by new word de-
tection. For example, person name ????? hap-
pens four times. In the first pass of segmentation,
two of them are segmented correctly and the other
two are mistakenly segmented as ?? ? ?? (they
are segmented differently because Viterbi algorithm
decodes based on context.). However, ?????
is identified as a new word and added to the word
list lexicon. In the second pass of segmentation, the
other two mistakes are corrected.
5 Experiments and Analysis
To make a comprehensive evaluation, we use all
four of the datasets from a recent Chinese word seg-
mentation bake-off competition (Sproat and Emer-
son, 2003). These datasets represent four different
segmentation standards. A summary of the datasets
is shown in Table 1. The standard bake-off scoring
program is used to calculate precision, recall, F1,
and OOV word recall.
5.1 Experimental design
Since CTB and PK are provided in the GB encod-
ing while AS and HK use the Big5 encoding, we
convert AS and HK datasets to GB in order to make
cross-training-and-testing possible. Note that this
conversion could potentially worsen performance
slightly due to a few conversion errors.
We use cross-validation to choose Markov-order
and perform feature selection. Thus, each training
set is randomly split?80% used for training and the
remaining 20% for validation?and based on vali-
dation set performance, choices are made for model
structure, prior, and which word lexicons to include.
The choices of prior and model structure shown in
Table 2 are used for our final testing.
We conduct closed and open tests on all four
datasets. The closed tests use only material from the
training data for the particular corpus being tested.
Open tests allows using other material, such as lex-
icons from Internet. In open tests, we use lexi-
cons obtained from various resources as described
Corpus Abbrev. Encoding #Train words #Test Words OOV rate (%)
UPenn Chinese Treebank CTB GB 250K 40K 18.1
Beijing University PK GB 1.1M 17K 6.9
Hong Kong City U HK Big 5 240K 35K 7.1
Academia Sinica AS Big 5 5.8M 12K 2.2
Table 1: Datasets statistics
bin-Size M Markov order
CTB 10 first-order + transitions
PK 15 first-order + transitions
HK 1 first-order
AS 15 first-order + transitions
Table 2: Optimal prior and Markov order setting
in Section 3.1. In addition, we conduct cross-dataset
tests, in which we train on one dataset and test on
other datasets.
5.2 Overall results
Final results of CRF based segmentation with new
word detection are summarized in Table 3. The up-
per part of the table contains the closed test results,
and the lower part contains the open test results.
Each entry is the performance of the given metric
(precision, recall, F1, and Roov) on the test set.
Closed
Precision Recall F1 Roov
CTB 0.828 0.870 0.849 0.550
PK 0.935 0.947 0.941 0.660
HK 0.917 0.940 0.928 0.531
AS 0.950 0.962 0.956 0.292
Open
Precision Recall F1 Roov
CTB 0.889 0.898 0.894 0.619
PK 0.941 0.952 0.946 0.676
HK 0.944 0.948 0.946 0.629
AS 0.953 0.961 0.957 0.403
Table 3: Overall results of CRF segmentation on
closed and open tests
To compare our results against other systems,
we summarize the competition results reported
in (Sproat and Emerson, 2003) in Table 4. XXc and
XXo indicate the closed and open runs on dataset
XX respectively. Entries contain the F1 perfor-
mance of each participating site on different runs,
with the best performance in bold. Our results are
in the last row. Column SITE-AVG is the average
F1 performance over the datasets on which a site re-
ported results. Column OUR-AVG is the average F1
performance of our system over the same datasets.
Comparing performance across systems is diffi-
cult since none of those systems reported results
on all eight datasets (open and closed runs on 4
datasets). Nevertheless, several observations could
be made from Table 4. First, no single system
achieved best results in all tests. Only one site (S01)
achieved two best runs (CTBc and PKc) with an av-
erage of 91.8% over 6 runs. S01 is one of the best
segmentation systems in mainland China (Zhang et
al., 2003). We also achieve two best runs (ASo and
HKc), with a comparable average of 91.9% over the
same 6 runs, and a 92.7% average over all the 8 runs.
Second, performance varies significantly across dif-
ferent datasets, indicating that the four datasets have
different characteristics and use very different seg-
mentation guidelines. We also notice that the worst
results were obtained on CTB dataset for all sys-
tems. This is due to significant inconsistent segmen-
tation in training and testing (Sproat and Emerson,
2003). We verify this by another test. We randomly
split the training data into 80% training and 20%
testing, and run the experiments for 3 times, result-
ing in a testing F1 of 97.13%. Third, consider a
comparison of our results with site S12, who use
a sliding-window maximum entropy model (Xue,
2003). They participated in two datasets, with an
average of 93.8%. Our average over the same two
runs is 94.2%. This gives some empirical evidence
of the advantages of linear-chain CRFs over sliding-
window maximum entropy models, however, this
comparison still requires further investigation since
there are many factors that could affect the perfor-
mance such as different features used in both sys-
tems.
To further study the robustness of our approach
to segmentation, we perform cross-testing?that is,
training on one dataset and testing on other datasets.
Table 5 summarizes these results, in which the rows
are the training datasets and the columns are the
testing datasets. Not surprisingly, cross testing re-
sults are worse than the results using the same
ASc ASo CTBc CTBo HKc HKo PKc PKo SITE-AVG OUR-AVG
S01 93.8 88.1 88.1 90.1 95.1 95.3 91.8 91.9
S02 87.4 91.2 89.3 87.2
S03 87.2 82.9 88.6 92.5 87.8 93.6
S04 93.9 93.7 93.8 94.4
S05 94.2 73.2 89.4 85.6 91.5
S06 94.5 82.9 92.4 92.4 90.6 91.9
S07 94.0 94.0 94.6
S08 90.4 95.6 93.6 93.8 93.4 94.0
S09 96.1 94.6 95.4 94.9
S10 83.1 90.1 94.7 95.9 91.0 90.8
S11 90.4 88.4 87.9 88.6 88.8 93.6
S12 95.9 91.6 93.8 94.2
95.6 95.7 84.9 89.4 92.8 94.6 94.1 94.6 92.7
Table 4: Comparisons against other systems: the first column contains the 12 sites participating in bake-off
competition; the second to the ninth columns contain their results on the 8 runs, where a bold entry is the
winner of that run; column SITE-AVG contains the average performance of the site over the runs in which it
participated, where a bold entry indicates that this site performs better than our system; column OUR-AVG
is the average of our system over the same runs, where a bolded entry indicates our system performs better
than the other site; the last row is the performance of our system over all the runs and the overall average.
source as training due to different segmentation
policies, with an exception on CTB where mod-
els trained on other datasets perform better than the
model trained on CTB itself. This is due to the data
problem mentioned above. Overall, CRFs perform
robustly well across all datasets.
From both Table 3 and 5, we see, as expected,
improvement from closed tests to open tests, indi-
cating the significant contribution of domain knowl-
edge lexicons.
Closed
CTB PK HK AS
CTB 0.822 0.810 0.815
PK 0.816 0.824 0.830
HK 0.790 0.807 0.825
AS 0.890 0.844 0.864
Open
CTB PK HK AS
CTB 0.863 0.870 0.894
PK 0.852 0.862 0.871
HK 0.861 0.871 0.889
AS 0.898 0.867 0.871
Table 5: Crossing test of CRF segmentation
5.3 Effects of new word detection
Table 6 shows the effect of new word detection
on the closed tests. An interesting observation is
CTB PK HK AS
w/o NWD 0.792 0.934 0.916 0.956
NWD 0.849 0.941 0.928 0.946
Table 6: New word detection effects: w/o NWD is
the results without new word detection and NWD is
the results with new word detection.
that the improvement is monotonically related to the
OOV rate (OOV rates are listed in Table 1). This
is desirable because new word detection is most
needed in situations that have high OOV rate. At
low OOV rate, noisy new word detection can result
in worse performance, as seen in the AS dataset.
5.4 Error analysis and discussion
Several typical errors are observed in error anal-
ysis. One typical error is caused by inconsistent
segmentation labeling in the test set. This is most
notorious in CTB dataset. The second most typi-
cal error is in new, out-of-vocabulary words, espe-
cially proper names. Although our new word detec-
tion fixes many of these problems, it is not effective
enough to recognize proper names well. One solu-
tion to this problem could use a named entity ex-
tractor to recognize proper names; this was found to
be very helpful in Wu (2003).
One of the most attractive advantages of CRFs
(and maximum entropy models in general) is its the
flexibility to easily incorporate arbitrary features,
here in the form domain-knowledge-providing lex-
icons. However, obtaining these lexicons is not a
trivial matter. The quality of lexicons can affect
the performance of CRFs significantly. In addition,
compared to simple models like n-gram language
models (Teahan et al, 2000), another shortcoming
of CRF-based segmenters is that it requires signifi-
cantly longer training time. However, training is a
one-time process, and testing time is still linear in
the length of the input.
6 Conclusions
The contribution of this paper is three-fold. First,
we apply CRFs to Chinese word segmentation and
find that they achieve state-of-the art performance.
Second, we propose a probabilistic new word de-
tection method that is integrated in segmentation,
and show it to improve segmentation performance.
Third, as far as we are aware, this is the first work
to comprehensively evaluate on the four benchmark
datasets, making a solid baseline for future research
on Chinese word segmentation.
Acknowledgments
This work was supported in part by the Center for In-
telligent Information Retrieval, in part by The Cen-
tral Intelligence Agency, the National Security Agency
and National Science Foundation under NSF grant #IIS-
0326249, and in part by SPAWARSYSCEN-SD grant
number N66001-02-1-8903.
References
K.J. Chen and M.H. Bai. 1998. Unknown Word Detec-
tion for Chinese by a Corpus-based Learning Method.
Computational Linguistics and Chinese Language
Processing, 3(1):27?44, Feburary.
A. Culotta and A. McCallum. 2004. Confidence Esti-
mation for Information Extraction. In Proceedings of
Human Language Technology Conference and North
American Chapter of the Association for Computa-
tional Linguistics(HLT-NAACL).
S. della Pietra, V. della Pietra, and J. Lafferty. 1995. In-
ducing Features Of Random Fields. IEEE Transac-
tions on Pattern Analysis and Machine Intelligence,
19(4).
J. Gao, M. Li, and C. Huang. 2003. Improved Source-
Channel Models for Chinese Word Segmentation. In
Proceedings of the 41th Annual Meeting of Associa-
tion of Computaional Linguistics (ACL), Japan.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Condi-
tional Random Fields: Probabilistic Models for Seg-
menting and Labeling Sequence Data. In Proceedings
of the 18th International Conf. on Machine Learning,
pages 282?289.
R. Malouf. 2002. A Comparison of Algorithms for Max-
imum Entropy Parameter Estimation. In Sixth Work-
shop on Computational Language Learning (CoNLL).
J. Nie, M. Hannan, and W. Jin. 1995. Unknown Word
Detection and Segmentation of Chinese using Statis-
tical and Heuristic Knowledge. Communications of
the Chinese and Oriental Languages Information Pro-
cessing Society, 5:47?57.
F. Peng and A. McCallum. 2004. Accurate Informa-
tion Extraction from Research Papers using Condi-
tional Random Fields. In Proceedings of Human
Language Technology Conference and North Amer-
ican Chapter of the Association for Computational
Linguistics(HLT-NAACL), pages 329?336.
F. Peng and D. Schuurmans. 2001. Self-Supervised Chi-
nese Word Segmentation. In F. Hoffmann et al, ed-
itor, Proceedings of the 4th International Symposium
of Intelligent Data Analysis, pages 238?247. Springer-
Verlag Berlin Heidelberg.
L. Rabiner. 1990. A Tutorial on Hidden Markov Mod-
els and Selected Applications in Speech Recognition.
In Alex Weibel and Kay-Fu Lee, editors, Readings in
Speech Recognition, pages 267?296.
R. Schwartz and Y. Chow. 1990. The N-best Algorithm:
An Efficient and Exact Procedure for Finding the N
most Likely Sentence Hypotheses. In Proceedings of
IEEE International Conference on Acoustics, Speech,
and Signal Processing (ICASSP).
F. Sha and F. Pereira. 2003. Shallow Parsing with Con-
ditional Random Fields. In Proceedings of Human
Language Technology Conference and North Amer-
ican Chapter of the Association for Computational
Linguistics(HLT-NAACL).
R. Sproat and T. Emerson. 2003. First International Chi-
nese Word Segmentation Bakeoff. In Proceedings of
the Second SIGHAN Workshop on Chinese Language
Processing.
R. Sproat and C. Shih. 2002. Corpus-based Methods
in Chinese Morphology and Phonology. In Proceed-
ings of the 19th International Conference on Compu-
tational Linguistics (COLING).
W. J. Teahan, Y. Wen, R. McNab, and I. H. Wit-
ten. 2000. A Compression-based Algorithm for Chi-
nese Word Segmentation. Computational Linguistics,
26(3):375?393.
A. Wu and Z. Jiang. 2000. Statistically-Enhanced New
Word Identification in a Rule-Based Chinese System.
In Proceedings of the Second Chinese Language Pro-
cessing Workshop, pages 46?51, Hong Kong, China.
Z. Wu. 1999. LDC Chinese Segmenter.
http://www.ldc.upenn.edu/ Projects/ Chinese/ seg-
menter/ mansegment.perl.
A. Wu. 2003. Chinese Word Segmentation in MSR-
NLP. In Proceedings of the Second SIGHAN Work-
shop on Chinese Language Processing, Japan.
N. Xue. 2003. Chinese Word Segmentation as Charac-
ter Tagging. International Journal of Computational
Linguistics and Chinese Language Processing, 8(1).
H. Zhang, Q. Liu, X. Cheng, H. Zhang, and H. Yu.
2003. Chinese Lexical Analysis Using Hierarchical
Hidden Markov Model. In Proceedings of the Second
SIGHAN Workshop, pages 63?70, Japan.
Accurate Information Extraction from Research Papers
using Conditional Random Fields
Fuchun Peng
Department of Computer Science
University of Massachusetts
Amherst, MA 01003
fuchun@cs.umass.edu
Andrew McCallum
Department of Computer Science
University of Massachusetts
Amherst, MA 01003
mccallum@cs.umass.edu
Abstract
With the increasing use of research paper
search engines, such as CiteSeer, for both lit-
erature search and hiring decisions, the accu-
racy of such systems is of paramount impor-
tance. This paper employs Conditional Ran-
dom Fields (CRFs) for the task of extracting
various common fields from the headers and
citation of research papers. The basic the-
ory of CRFs is becoming well-understood, but
best-practices for applying them to real-world
data requires additional exploration. This paper
makes an empirical exploration of several fac-
tors, including variations on Gaussian, expo-
nential and hyperbolic-L1 priors for improved
regularization, and several classes of features
and Markov order. On a standard benchmark
data set, we achieve new state-of-the-art perfor-
mance, reducing error in average F1 by 36%,
and word error rate by 78% in comparison with
the previous best SVM results. Accuracy com-
pares even more favorably against HMMs.
1 Introduction
Research paper search engines, such as CiteSeer
(Lawrence et al, 1999) and Cora (McCallum et al,
2000), give researchers tremendous power and conve-
nience in their research. They are also becoming increas-
ingly used for recruiting and hiring decisions. Thus the
information quality of such systems is of significant im-
portance. This quality critically depends on an informa-
tion extraction component that extracts meta-data, such
as title, author, institution, etc, from paper headers and
references, because these meta-data are further used in
many component applications such as field-based search,
author analysis, and citation analysis.
Previous work in information extraction from research
papers has been based on two major machine learn-
ing techniques. The first is hidden Markov models
(HMM) (Seymore et al, 1999; Takasu, 2003). An
HMM learns a generative model over input sequence
and labeled sequence pairs. While enjoying wide his-
torical success, standard HMM models have difficulty
modeling multiple non-independent features of the ob-
servation sequence. The second technique is based
on discriminatively-trained SVM classifiers (Han et al,
2003). These SVM classifiers can handle many non-
independent features. However, for this sequence label-
ing problem, Han et al (2003) work in a two stages pro-
cess: first classifying each line independently to assign it
label, then adjusting these labels based on an additional
classifier that examines larger windows of labels. Solving
the information extraction problem in two steps looses
the tight interaction between state transitions and obser-
vations.
In this paper, we present results on this research paper
meta-data extraction task using a Conditional Random
Field (Lafferty et al, 2001), and explore several practi-
cal issues in applying CRFs to information extraction in
general. The CRF approach draws together the advan-
tages of both finite state HMM and discriminative SVM
techniques by allowing use of arbitrary, dependent fea-
tures and joint inference over entire sequences.
CRFs have been previously applied to other tasks such
as name entity extraction (McCallum and Li, 2003), table
extraction (Pinto et al, 2003) and shallow parsing (Sha
and Pereira, 2003). The basic theory of CRFs is now
well-understood, but the best-practices for applying them
to new, real-world data is still in an early-exploration
phase. Here we explore two key practical issues: (1) reg-
ularization, with an empirical study of Gaussian (Chen
and Rosenfeld, 2000), exponential (Goodman, 2003), and
hyperbolic-L1 (Pinto et al, 2003) priors; (2) exploration
of various families of features, including text, lexicons,
and layout, as well as proposing a method for the bene-
ficial use of zero-count features without incurring large
memory penalties.
We describe a large collection of experimental results
on two traditional benchmark data sets. Dramatic im-
provements are obtained in comparison with previous
SVM and HMM based results.
2 Conditional Random Fields
Conditional random fields (CRFs) are undirected graph-
ical models trained to maximize a conditional probabil-
ity (Lafferty et al, 2001). A common special-case graph
structure is a linear chain, which corresponds to a finite
state machine, and is suitable for sequence labeling. A
linear-chain CRF with parameters ? = {?, ...} defines
a conditional probability for a state (or label1) sequence
y = y1...yT given an input sequence x = x1...xT to be
P?(y|x) =
1
Zx
exp
( T
?
t=1
?
k
?kfk(yt?1, yt,x, t)
)
,
(1)
where Zx is the normalization constant that makes
the probability of all state sequences sum to one,
fk(yt?1, yt,x, t) is a feature function which is often
binary-valued, but can be real-valued, and ?k is a learned
weight associated with feature fk. The feature functions
can measure any aspect of a state transition, yt?1 ? yt,
and the observation sequence, x, centered at the current
time step, t. For example, one feature function might
have value 1 when yt?1 is the state TITLE, yt is the state
AUTHOR, and xt is a word appearing in a lexicon of peo-
ple?s first names. Large positive values for ?k indicate a
preference for such an event, while large negative values
make the event unlikely.
Given such a model as defined in Equ. (1), the most
probable labeling sequence for an input x,
y? = argmax
y
P?(y|x),
can be efficiently calculated by dynamic programming
using the Viterbi algorithm. Calculating the marginal
probability of states or transitions at each position in
the sequence by a dynamic-programming-based infer-
ence procedure very similar to forward-backward for hid-
den Markov models.
The parameters may be estimated by maximum
likelihood?maximizing the conditional probability of
a set of label sequences, each given their correspond-
ing input sequences. The log-likelihood of training set
1We consider here only finite state models in which there is
a one-to-one correspondence between states and labels; this is
not, however, strictly necessary.
?6 ?5 ?4 ?3 ?2 ?1 0 1 2 3
0
2
4
6
8
10
12
lambda
co
u
n
ts
 o
f l
am
da
 (in
 lo
g s
ca
le)
Figure 1: Empirical distribution of ?
{(xi, yi) : i = 1, ...M} is written
L? =
?
i
logP?(yi|xi)
=
?
i
( T
?
t=1
?
k
?kfk(yt?1, yt,x, t) ? logZxi
)
.
(2)
Maximizing (2) corresponds to satisfying the follow-
ing equality, wherein the the empirical count of each fea-
ture matches its expected count according to the model
P?(y|x).
?
i
fk(yt?1, yt, xi, t) =
?
i
P?(y|x)fk(yt?1, yt, xi, t)
CRFs share many of the advantageous properties of
standard maximum entropy models, including their con-
vex likelihood function, which guarantees that the learn-
ing procedure converges to the global maximum. Tra-
ditional maximum entropy learning algorithms, such as
GIS and IIS (Pietra et al, 1995), can be used to train
CRFs, however, it has been found that a quasi-Newton
gradient-climber, BFGS, converges much faster (Malouf,
2002; Sha and Pereira, 2003). We use BFGS for opti-
mization. In our experiments, we shall focus instead on
two other aspects of CRF deployment, namely regulariza-
tion and selection of different model structure and feature
types.
2.1 Regularization in CRFs
To avoid over-fitting, log-likelihood is often penalized by
some prior distribution over the parameters. Figure 1
shows an empirical distribution of parameters, ?, learned
from an unpenalized likelihood, including only features
with non-zero count in the training set. Three prior dis-
tributions that have shape similar to this empirical dis-
tribution are the Gaussian prior, exponential prior, and
hyperbolic-L1 prior, each shown in Figure 2. In this pa-
per we provide an empirical study of these three priors.
?10 ?8 ?6 ?4 ?2 0 2 4 6 8 10
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
Gaussian varianec=2
Exponential a=0.5
Hyperbolic
Figure 2: Shapes of prior distributions
2.1.1 Gaussian prior
With a Gaussian prior, log-likelihood (2) is penalized
as follows:
L? =
?
i
logP?(yi|xi) ?
?
k
?2k
2?2k
, (3)
where ?2k is a variance.
Maximizing (3) corresponds to satisfying
?
i
fk(yt?1, yt, xi, t)?
?k
?2k
=
?
i
P?(y|x)fk(yt?1, yt, xi, t)
This adjusted constraint (as well as the adjustments im-
posed by the other two priors) is intuitively understand-
able: rather than matching exact empirical feature fre-
quencies, the model is tuned to match discounted feature
frequencies. Chen and Rosenfeld (2000) discuss this in
the context of other discounting procedures common in
language modeling. We call the term subtracted from the
empirical counts (in this case ?k/?2) a discounted value.
The variance can be feature dependent. However for
simplicity, constant variance is often used for all features.
In this paper, however, we experiment with several alter-
nate versions of Gaussian prior in which the variance is
feature dependent.
Although Gaussian (and other) priors are gradually
overcome by increasing amounts of training data, per-
haps not at the right rate. The three methods below all
provide ways to alter this rate by changing the variance
of the Gaussian prior dependent on feature counts.
1. Threshold Cut: In language modeling, e.g, Good-
Turing smoothing, only low frequency words are
smoothed. Here we apply the same idea and only
smooth those features whose frequencies are lower
than a threshold (7 in our experiments, following
standard practice in language modeling).
2. Divide Count: Here we let the discounted value
for a feature depend on its frequency in the training
set, ck =
?
i
?
t fk(yt?1, yt,x, t). The discounted
value used here is ?kck??2 where ? is a constant over
all features. In this way, we increase the smoothing
on the low frequency features more so than the high
frequency features.
3. Bin-Based: We divide features into classes based
on frequency. We bin features by frequency in the
training set, and let the features in the same bin share
the same variance. The discounted value is set to be
?k
dck/Ne??2 where ck is the count of features, N is
the bin size, and dae is the ceiling function. Alterna-
tively, the variance in each bin may be set indepen-
dently by cross-validation.
2.1.2 Exponential prior
Whereas the Gaussian prior penalizes according to the
square of the weights (an L2 penalizer), the intention here
is to create a smoothly differentiable analogue to penal-
izing the absolute-value of the weights (an L1 penalizer).
L1 penalizers often result in more ?sparse solutions,? in
which many features have weight nearly at zero, and thus
provide a kind of soft feature selection that improves gen-
eralization.
Goodman (2003) proposes an exponential prior,
specifically a Laplacian prior, as an alternative to Gaus-
sian prior. Under this prior,
L? =
?
i
logP?(yi|xi)?
?
k
?k|?k| (4)
where ?k is a parameter in exponential distribution.
Maximizing (4) would satisfy
?
i
fk(yt?1, yt, xi, t)??k =
?
i
P?(y|x)fk(yt?1, yt, xi, t)
This corresponds to the absolute smoothing method in
language modeling. We set the ?k = ?; i.e. all features
share the same constant whose value can be determined
using absolute discounting? = n1n1+2n2 , where n1 and n2
are the number of features occurring once and twice (Ney
et al, 1995).
2.1.3 Hyperbolic-L1 prior
Another L1 penalizer is the hyperbolic-L1 prior, de-
scribed in (Pinto et al, 2003). The hyperbolic distribution
has log-linear tails. Consequently the class of hyperbolic
distribution is an important alternative to the class of nor-
mal distributions and has been used for analyzing data
from various scientific areas such as finance, though less
frequently used in natural language processing.
Under a hyperbolic prior,
L? =
X
i
logP?(yi|xi) ?
X
k
log(e
?k + e??k
2 ) (5)
which corresponds to satisfying
X
i
fk(yt?1, yt, xi, t) ?
e|?k| ? e?|?k|
e|?k| + e?|?k| =
X
i
P?(y|x)fi(yt?1, yt, xi, t)
The hyperbolic prior was also tested with CRFs in Mc-
Callum and Li (2003).
2.2 Exploration of Feature Space
Wise choice of features is always vital the performance
of any machine learning solution. Feature induction (Mc-
Callum, 2003) has been shown to provide significant im-
provements in CRFs performance. In some experiments
described below we use feature induction. The focus in
this section is on three other aspects of the feature space.
2.2.1 State transition features
In CRFs, state transitions are also represented as fea-
tures. The feature function fk(yt?1, yt,x, t) in Equ. (1)
is a general function over states and observations. Differ-
ent state transition features can be defined to form dif-
ferent Markov-order structures. We define four differ-
ent state transitions features corresponding to different
Markov order for different classes of features. Higher
order features model dependencies better, but also create
more data sparse problem and require more memory in
training.
1. First-order: Here the inputs are examined in the con-
text of the current state only. The feature functions
are represented as f(yt,x). There are no separate
parameters or preferences for state transitions at all.
2. First-order+transitions: Here we add parameters
corresponding to state transitions. The feature func-
tions used are f(yt,x), f(yt?1, yt).
3. Second-order: Here inputs are examined in the con-
text of the current and previous states. Feature func-
tion are represented as f(yt?1, yt,x).
4. Third-order: Here inputs are examined in the con-
text of the current, two previous states. Feature func-
tion are represented as f(yt?2, yt?1, yt,x).
2.2.2 Supported features and unsupported features
Before the use of prior distributions over parameters
was common in maximum entropy classifiers, standard
practice was to eliminate all features with zero count
in the training data (the so-called unsupported features).
However, unsupported, zero-count features can be ex-
tremely useful for pushing Viterbi inference away from
certain paths by assigning such features negative weight.
The use of a prior allows the incorporation of unsup-
ported features, however, doing so often greatly increases
the number parameters and thus the memory require-
ments.
Below we experiment with models containing and not
containing unsupported features?both with and without
regularization by priors, and we argue that non-supported
features are useful.
We present here incremental support, a method of in-
troducing some useful unsupported features without ex-
ploding the number of parameters with all unsupported
features. The model is trained for several iterations with
supported features only. Then inference determines the
label sequences assigned high probability by the model.
Incorrect transitions assigned high probability by the
model are used to selectively add to the model those un-
supported features that occur on those transitions, which
may help improve performance by being assigned nega-
tive weight in future training. If desired, several iterations
of this procedure may be performed.
2.2.3 Local features, layout features and lexicon
features
One of the advantages of CRFs and maximum entropy
models in general is that they easily afford the use of arbi-
trary features of the input. One can encode local spelling
features, layout features such as positions of line breaks,
as well as external lexicon features, all in one framework.
We study all these features in our research paper extrac-
tion problem, evaluate their individual contributions, and
give some guidelines for selecting good features.
3 Empirical Study
3.1 Hidden Markov Models
Here we also briefly describe a HMM model we used
in our experiments. We relax the independence assump-
tion made in standard HMM and allow Markov depen-
dencies among observations, e.g., P (ot|st, ot?1). We
can vary Markov orders in state transition and observa-
tion transitions. In our experiments, a model with second
order state transitions and first order observation transi-
tions performs the best. The state transition probabilities
and emission probabilities are estimated using maximum
likelihood estimation with absolute smoothing, which
was found to be effective in previous experiments, includ-
ing Seymore et al (1999).
3.2 Datasets
We experiment with two datasets of research paper con-
tent. One consists of the headers of research papers. The
other consists of pre-segmented citations from the refer-
ence sections of research papers. These data sets have
been used as standard benchmarks in several previous
studies (Seymore et al, 1999; McCallum et al, 2000;
Han et al, 2003).
3.2.1 Paper header dataset
The header of a research paper is defined to be all of
the words from the beginning of the paper up to either
the first section of the paper, usually the introduction,
or to the end of the first page, whichever occurs first.
It contains 15 fields to be extracted: title, author, affil-
iation, address, note, email, date, abstract, introduction,
phone, keywords, web, degree, publication number, and
page (Seymore et al, 1999). The header dataset contains
935 headers. Following previous research (Seymore et
al., 1999; McCallum et al, 2000; Han et al, 2003), for
each trial we randomly select 500 for training and the re-
maining 435 for testing. We refer this dataset as H.
3.2.2 Paper reference dataset
The reference dataset was created by the Cora
project (McCallum et al, 2000). It contains 500 refer-
ences, we use 350 for training and the rest 150 for test-
ing. References contain 13 fields: author, title, editor,
booktitle, date, journal, volume, tech, institution, pages,
location, publisher, note. We refer this dataset as R.
3.3 Performance Measures
To give a comprehensive evaluation, we measure per-
formance using several different metrics. In addition to
the previously-used word accuracy measure (which over-
emphasizes accuracy of the abstract field), we use per-
field F1 measure (both for individual fields and averaged
over all fields?called a ?macro average? in the informa-
tion retrieval literature), and whole instance accuracy for
measuring overall performance in a way that is sensitive
to even a single error in any part of header or citation.
3.3.1 Measuring field-specific performance
1. Word Accuracy: We define A as the number of true
positive words, B as the number of false negative
words, C as the number of false positive words, D
as the number of true negative words, and A+ B +
C +D is the total number of words. Word accuracy
is calculated to be A+DA+B+C+D
2. F1-measure: Precision, recall and F1 measure are
defined as follows. Precision = AA+C Recall =
A
A+B
F1 = 2?Precision?RecallPrecision+Recall
3.3.2 Measuring overall performance
1. Overall word accuracy: Overall word accuracy
is the percentage of words whose predicted labels
equal their true labels. Word accuracy favors fields
with large number of words, such as the abstract.
2. Averaged F-measure: Averaged F-measure is com-
puted by averaging the F1-measures over all fields.
Average F-measure favors labels with small num-
ber of words, which complements word accuracy.
Thus, we consider both word accuracy and average
F-measure in evaluation.
3. Whole instance accuracy: An instance here is de-
fined to be a single header or reference. Whole
instance accuracy is the percentage of instances in
which every word is correctly labeled.
3.4 Experimental Results
We first report the overall results by comparing CRFs
with HMMs, and with the previously best benchmark re-
sults obtained by SVMs (Han et al, 2003). We then break
down the results to analyze various factors individually.
Table 1 shows the results on dataset H with the best re-
sults in bold; (intro and page fields are not shown, fol-
lowing past practice (Seymore et al, 1999; Han et al,
2003)). The results we obtained with CRFs use second-
order state transition features, layout features, as well as
supported and unsupported features. Feature induction
is used in experiments on dataset R; (it didn?t improve
accuracy on H). The results we obtained with the HMM
model use a second order model for transitions, and a first
order for observations. The results on SVM is obtained
from (Han et al, 2003) by computing F1 measures from
the precision and recall numbers they report.
HMM CRF SVM
Overall acc. 93.1% 98.3% 92.9%
Instance acc. 4.13% 73.3% -
acc. F1 acc. F1 acc. F1
Title 98.2 82.2 99.7 97.1 98.9 96.5
Author 98.7 81.0 99.8 97.5 99.3 97.2
Affiliation 98.3 85.1 99.7 97.0 98.1 93.8
Address 99.1 84.8 99.7 95.8 99.1 94.7
Note 97.8 81.4 98.8 91.2 95.5 81.6
Email 99.9 92.5 99.9 95.3 99.6 91.7
Date 99.8 80.6 99.9 95.0 99.7 90.2
Abstract 97.1 98.0 99.6 99.7 97.5 93.8
Phone 99.8 53.8 99.9 97.9 99.9 92.4
Keyword 98.7 40.6 99.7 88.8 99.2 88.5
Web 99.9 68.6 99.9 94.1 99.9 92.4
Degree 99.5 68.8 99.8 84.9 99.5 70.1
Pubnum 99.8 64.2 99.9 86.6 99.9 89.2
Average F1 75.6 93.9 89.7
Table 1: Extraction results for paper headers on H
Table 2 shows the results on dataset R. SVM results
are not available for these datasets.
3.5 Analysis
3.5.1 Overall performance comparison
From Table (1, 2), one can see that CRF performs
significantly better than HMMs, which again supports
the previous findings (Lafferty et al, 2001; Pinto et al,
HMM CRF
Overall acc. 85.1% 95.37%
instance acc. 10% 77.33%
acc. F1 acc. F1
Author 96.8 92.7 99.9 99.4
Booktitle 94.4 0.85 97.7 93.7
Date 99.7 96.9 99.8 98.9
Editor 98.8 70.8 99.5 87.7
Institution 98.5 72.3 99.7 94.0
Journal 96.6 67.7 99.1 91.3
Location 99.1 81.8 99.3 87.2
Note 99.2 50.9 99.7 80.8
Pages 98.1 72.9 99.9 98.6
Publisher 99.4 79.2 99.4 76.1
Tech 98.8 74.9 99.4 86.7
Title 92.2 87.2 98.9 98.3
Volume 98.6 75.8 99.9 97.8
Average F1 77.6% 91.5%
Table 2: Extraction results for paper references on R
2003). CRFs also perform significantly better than SVM-
based approach, yielding new state of the art performance
on this task. CRFs increase the performance on nearly all
the fields. The overall word accuracy is improved from
92.9% to 98.3%, which corresponds to a 78% error rate
reduction. However, as we can see word accuracy can be
misleading since HMM model even has a higher word ac-
curacy than SVM, although it performs much worse than
SVM in most individual fields except abstract. Interest-
ingly, HMM performs much better on abstract field (98%
versus 93.8% F-measure) which pushes the overall accu-
racy up. A better comparison can be made by compar-
ing the field-based F-measures. Here, in comparison to
the SVM, CRFs improve the F1 measure from 89.7% to
93.9%, an error reduction of 36%.
3.5.2 Effects of regularization
The results of different regularization methods are
summarized in Table (3). Setting Gaussian variance of
features depending on feature count performs better, from
90.5% to 91.2%, an error reduction of 7%, when only
using supported features, and an error reduction of 9%
when using supported and unsupported features. Re-
sults are averaged over 5 random runs, with an aver-
age variance of 0.2%. In our experiments we found the
Gaussian prior to consistently perform better than the
others. Surprisingly, exponential prior hurts the perfor-
mance significantly. It over penalizes the likelihood (sig-
nificantly increasing cost?defined as negative penalized
log-likelihood). We hypothesized that the problem could
be that the choice of constant ? is inappropriate. So we
tried varying ? instead of computing it using absolute
discounting, but found the alternatives to perform worse.
These results suggest that Gaussian prior is a safer prior
support feat. all features
Method F1 F1
Gaussian infinity 90.5 93.3
Gaussian variance = 0.1 81.7 91.8
Gaussian variance = 0.5 87.2 93.0
Gaussian variance = 5 90.1 93.7
Gaussian variance = 10 89.9 93.5
Gaussian cut 7 90.1 93.4
Gaussian divide count 90.9 92.8
Gaussian bin 5 90.9 93.6
Gaussian bin 10 90.2 92.9
Gaussian bin 15 91.2 93.9
Gaussian bin 20 90.4 93.2
Hyperbolic 89.4 92.8
Exponential 80.5 85.6
Table 3: Regularization comparisons: Gaussian infinity is
non-regularized, Gaussian variance = X sets variance to
be X. Gaussian cut 7 refers to the Threshold Cut method,
Gaussian divide count refers to the Divide Count method,
Gaussian bin N refers to the Bin-Based method with bin
size equals N, as described in 2.1.1
to use in practice.
3.5.3 Effects of exploring feature space
State transition features and unsupported features.
We summarize the comparison of different state tran-
sition models using or not using unsupported features in
Table 4. The first column describes the four different state
transition models, the second column contains the overall
word accuracy of these models using only support fea-
tures, and the third column contains the result of using
all features, including unsupported features. Comparing
the rows, one can see that the second-order model per-
forms the best, but not dramatically better than the first-
order+transitions and the third order model. However, the
first-order model performs significantly worse. The dif-
ference does not come from sharing the weights, but from
ignoring the f(yt?1, yt). The first order transition feature
is vital here. We would expect the third order model to
perform better if enough training data were available.
Comparing the second and the third columns, we can
see that using all features including unsupported features,
consistently performs better than ignoring them. Our
preliminary experiments with incremental support have
shown performance in between that of supported-only
and all features, and are still ongoing.
Effects of layout features
To analyze the contribution of different kinds of fea-
tures, we divide the features into three categories: local
features, layout features, and external lexicon resources.
The features we used are summarized in Table 5.
support all
first-order 89.0 90.4
first-order+trans 95.6 -
second-order 96.0 96.5
third-order 95.3 96.1
Table 4: Effects of using unsupported features and state
transitions on H
Feature name Description
Local features
INITCAP Starts with a capitalized letter
ALLCAPS All characters are capitalized
CONTAINSDIGITS Contains at least one digit
ALLDIGITS All characters are digits
PHONEORZIP Phone number or zip code
CONTAINSDOTS Contains at least one dot
CONTAINSDASH Contains at least one -
ACRO Acronym
LONELYINITIAL Initials such as A.
SINGLECHAR One character only
CAPLETTER One capitalized character
PUNC Punctuation
URL Regular expression for URL
EMAIL Regular expression for e-address
WORD Word itself
Layout features
LINE START At the beginning of a line
LINE IN In middle of a line
LINE END At the end of a line
External lexicon features
BIBTEX AUTHOR Match word in author lexicon
BIBTEX DATE Words like Jan. Feb.
NOTES Words like appeared, submitted
AFFILIATION Words like institution, Labs, etc
Table 5: List of features used
The results of using different features are shown in Ta-
ble 6. The layout feature dramatically increases the per-
formance, raising the F1 measure from 88.8% to 93.9%,
whole sentence accuracy from 40.1% to 72.4%. Adding
lexicon features alone improves the performance. How-
ever, when combing lexicon features and layout fea-
tures, the performance is worse than using layout features
alone.
The lexicons were gathered from a large collection of
BibTeX files, and upon examination had difficult to re-
move noise, for example words in the author lexicon that
were also affiliations. In previous work, we have gained
significant benefits by dividing each lexicon into sections
based on point-wise information gain with respect to the
lexicon?s class.
3.5.4 Error analysis
Table 7 is the classification confusion matrix of header
extraction (field page is not shown to save space). Most
Word Acc. F1 Inst. Acc.
local feature 96.5% 88.8% 40.1%
+ lexicon 96.9% 89.9% 53.1%
+ layout feature 98.2% 93.4% 72.4%
+ layout + lexicon 98.0% 93.0% 71.7%
Table 6: Results of using different features on H
errors happen at the boundaries between two fields. Es-
pecially the transition from author to affiliation, from ab-
stract to keyword. The note field is the one most con-
fused with others, and upon inspection is actually labeled
inconsistently in the training data. Other errors could
be fixed with additional feature engineering?for exam-
ple, including additional specialized regular expressions
should make email accuracy nearly perfect. Increasing
the amount of training data would also be expected to
help significantly, as indicated by consistent nearly per-
fect accuracy on the training set.
4 Conclusions and Future Work
This paper investigates the issues of regularization, fea-
ture spaces, and efficient use of unsupported features in
CRFs, with an application to information extraction from
research papers.
For regularization we find that the Gaussian prior with
variance depending on feature frequencies performs bet-
ter than several other alternatives in the literature. Feature
engineering is a key component of any machine learn-
ing solution?especially in conditionally-trained mod-
els with such freedom to choose arbitrary features?and
plays an even more important role than regularization.
We obtain new state-of-the-art performance in extract-
ing standard fields from research papers, with a signifi-
cant error reduction by several metrics. We also suggest
better evaluation metrics to facilitate future research in
this task?especially field-F1, rather than word accuracy.
We have provided an empirical exploration of a few
previously-published priors for conditionally-trained log-
linear models. Fundamental advances in regularization
for CRFs remains a significant open research area.
5 Acknowledgments
This work was supported in part by the Cen-
ter for Intelligent Information Retrieval, in part by
SPAWARSYSCEN-SD grant number N66001-02-1-
8903, in part by the National Science Foundation Co-
operative Agreement number ATM-9732665 through a
subcontract from the University Corporation for Atmo-
spheric Research (UCAR) and in part by The Cen-
tral Intelligence Agency, the National Security Agency
and National Science Foundation under NSF grant #IIS-
0326249. Any opinions, findings and conclusions or rec-
title auth. pubnum date abs. aff. addr. email deg. note ph. intro k.w. web
title 3446 0 6 0 22 0 0 0 9 25 0 0 12 0
author 0 2653 0 0 7 13 5 0 14 41 0 0 12 0
pubnum 0 14 278 2 0 2 7 0 0 39 0 0 0 0
date 0 0 3 336 0 1 3 0 0 18 0 0 0 0
abstract 0 0 0 0 53262 0 0 1 0 0 0 0 0 0
affil. 19 13 0 0 10 3852 27 0 28 34 0 0 0 1
address 0 11 3 0 0 35 2170 1 0 21 0 0 0 0
email 0 0 1 0 12 2 3 461 0 2 2 0 15 0
degree 2 2 0 2 0 2 0 5 465 95 0 0 2 0
note 52 2 9 6 219 52 59 0 5 4520 4 3 21 3
phone 0 0 0 0 0 0 0 1 0 2 215 0 0 0
intro 0 0 0 0 0 0 0 0 0 32 0 625 0 0
keyword 57 0 0 0 18 3 15 0 0 91 0 0 975 0
web 0 0 0 0 2 0 0 0 0 31 0 0 0 294
Table 7: Confusion matrix on H
ommendations expressed in this material are the author(s)
and do not necessarily reflect those of the sponsor.
References
S. Chen and R. Rosenfeld. 2000. A Survey of Smoothing
Techniques for ME Models. IEEE Trans. Speech and
Audio Processing, 8(1), pp. 37?50. January 2000.
J. Goodman. 2003. Exponential Priors for Maximum
Entropy Models. MSR Technical report, 2003.
H. Han, C. Giles, E. Manavoglu, H. Zha, Z. Zhang, and E.
Fox. 2003. Automatic Document Meta-data Extrac-
tion using Support Vector Machines. In Proceedings
of Joint Conference on Digital Libraries 2003.
J. Lafferty, A. McCallum and F. Pereira. 2001. Condi-
tional Random Fields: Probabilistic Models for Seg-
menting and Labeling Sequence Data. In Proceed-
ings of International Conference on Machine Learning
2001.
S. Lawrence, C. L. Giles, and K. Bollacker. 1999. Digital
Libraries and Autonomous Citation Indexing. IEEE
Computer, 32(6): 67-71.
R. Malouf. 2002. A Comparison of Algorithms for Max-
imum Entropy Parameter Estimation. In Proceedings
of the Sixth Conference on Natural Language Learning
(CoNLL)
A. McCallum. 2003. Efficiently Inducing Features
of Conditional Random Fields. In Proceedings of
Conference on Uncertainty in Articifical Intelligence
(UAI).
A. McCallum, K. Nigam, J. Rennie, K. Seymore. 2000.
Automating the Construction of Internet Portals with
Machine Learning. Information Retrieval Journal,
volume 3, pages 127-163. Kluwer. 2000.
A. McCallum and W. Li. 2003. Early Results for Named
Entity Recognition with Conditional Random Fields,
Feature Induction and Web-Enhanced Lexicons. In
Proceedings of Seventh Conference on Natural Lan-
guage Learning (CoNLL).
H. Ney, U. Essen, and R. Kneser 1995. On the Estima-
tion of Small Probabilities by Leaving-One-Out. IEEE
Transactions on Pattern Analysis and Machine Intelli-
gence, 17(12):1202-1212, 1995.
S. Pietra, V. Pietra, J. Lafferty 1995. Inducing Fea-
tures Of Random Fields. IEEE Transactions on Pat-
tern Analysis and Machine Intelligence, Vol. 19, No.
4.
D. Pinto, A. McCallum, X. Wei and W. Croft. 2003. Ta-
ble Extraction Using Conditional Random Fields. In
Proceedins of the 26th Annual International ACM SI-
GIR Conference on Research and Development in In-
formation Retrieval (SIGIR?03)
K. Seymore, A. McCallum, R. Rosenfeld. 1999. Learn-
ing Hidden Markov Model Structure for Information
Extraction. In Proceedings of AAAI?99 Workshop on
Machine Learning for Information Extraction.
F. Sha and F. Pereira. 2003. Shallow Parsing with Con-
ditional Random Fields. In Proceedings of Human
Language Technology Conference and North Ameri-
can Chapter of the Association for Computational Lin-
guistics (HLT-NAACL?03)
A. Takasu. 2003. Bibliographic Attribute Extrac-
tion from Erroneous References Based on a Statistical
Model. In Proceedings of Joint Conference on Digital
Libraries 2003.
Investigating the Relationship between Word Segmentation
Performance and Retrieval Performance in Chinese IR
Fuchun Peng and Xiangji Huang and Dale Schuurmans and Nick Cercone
School of Computer Science, University of Waterloo
200 University Ave. West, Waterloo, Ontario, Canada, N2L 3G1
{f3peng, jhuang, dale, ncercone}@uwaterloo.ca
Abstract
It is commonly believed that word segmentation ac-
curacy is monotonically related to retrieval perfor-
mance in Chinese information retrieval. In this pa-
per we show that, for Chinese, the relationship be-
tween segmentation and retrieval performance is in
fact nonmonotonic; that is, at around 70% word
segmentation accuracy an over-segmentation phe-
nomenon begins to occur which leads to a reduction
in information retrieval performance. We demon-
strate this effect by presenting an empirical inves-
tigation of information retrieval on Chinese TREC
data, using a wide variety of word segmentation al-
gorithms with word segmentation accuracies ranging
from 44% to 95%. It appears that the main reason
for the drop in retrieval performance is that correct
compounds and collocations are preserved by accu-
rate segmenters, while they are broken up by less
accurate (but reasonable) segmenters, to a surpris-
ing advantage. This suggests that words themselves
might be too broad a notion to conveniently capture
the general semantic meaning of Chinese text.
1 Introduction
Automated processing of written languages such
as Chinese involves an inherent word segmentation
problem that is not present in western languages like
English. Unlike English, Chinese words are not ex-
plicitly delimited by whitespace, and therefore to
perform automated text processing tasks (such as
information retrieval) one normally has to first seg-
ment the text collection. Typically this involves seg-
menting the text into individual words. Although
the text segmentation problem in Chinese has been
heavily investigated recently (Brent and Tao, 2001;
Chang, 1997; Ge et al, 1999; Hockenmaier and
Brew, 1998; Jin, 1992; Peng and Schuurmans, 2001;
Sproat and Shih, 1990; Teahan et al 2001) most
research has focused on the problem of segmenting
character strings into individual words, rather than
useful constituents. However, we have found that
focusing exclusively on words may not lead to the
most effective segmentation from the perspective of
broad semantic analysis (Peng et al 2002).
In this paper we will focus on a simple form of se-
mantic text processing: information retrieval (IR).
Although information retrieval does not require a
deep semantic analysis, to perform effective retrieval
one still has to accurately capture the main topic of
discourse and relate this to a given query. In the con-
text of Chinese, information retrieval is complicated
by the fact that the words in the source text (and
perhaps even the query) are not separated by whites-
pace. This creates a significant amount of additional
ambiguity in interpreting sentences and identifying
the underlying topic of discourse.
There are two standard approaches to information
retrieval in Chinese text: character based and word
based. It is usually thought that word based ap-
proaches should be superior, even though character
based methods are simpler and more commonly used
(Huang and Robertson, 2000). However, there has
been recent interest in the word based approach, mo-
tivated by recent advances in automatic segmenta-
tion of Chinese text (Nie et al 1996; Wu and Tseng,
1993). A common presumption is that word segmen-
tation accuracy should monotonically influence sub-
sequent retrieval performance (Palmer and Burger,
1997). Consequently, many researchers have focused
on producing accurate word segmenters for Chinese
text indexing (Teahan et al 2001; Brent and Tao,
2001). However, we have recently observed that low
accuracy word segmenters often yield superior re-
trieval performance (Peng et al 2002). This obser-
vation was initially a surprise, and motivated us to
conduct a more thorough study of the phenomenon
to uncover the reason for the performance decrease.
The relationship between Chinese word segmenta-
tion accuracy and information retrieval performance
has recently been investigated in the literature. Foo
and Li (2001) have conducted a series of experiments
which suggests that the word segmentation approach
does indeed have effect on IR performance. Specif-
ically, they observe that the recognition of words of
length two or more can produce better retrieval per-
formance, and the existence of ambiguous words re-
sulting from the word segmentation process can de-
crease retrieval performance. Similarly, Palmer and
Burger (1997) observe that accurate segmentation
tends to improve retrieval performance. All of this
previous research has indicated that there is indeed
some sort of correlation between word segmentation
performance and retrieval performance. However,
the nature of this correlation is not well understood,
and previous research uniformly suggests that this
relationship is monotonic.
One reason why the relationship between seg-
mentation and retrieval performance has not been
well understood is that previous investigators have
not considered using a variety of Chinese word seg-
menters which exhibit a wide range of segmenta-
tion accuracies, from low to high. In this paper,
we employ three families of Chinese word segmenta-
tion algorithms from the recent literature. The first
technique we employed was the standard maximum
matching dictionary based approach. The remaining
two algorithms were selected because they can both
be altered by simple parameter settings to obtain
different word segmentation accuracies. Specifically,
the second Chinese word segmenter we investigated
was the minimum description length algorithm of
Teahan et al (2001), and the third was the EM
based technique of Peng and Schuurmans (2001).
Overall, these segmenters demonstrate word identi-
fication accuracies ranging from 44% to 95% on the
PH corpus (Brent and Tao, 2001; Hockenmaier and
Brew, 1998; Teahan et al 2001).
Below we first describe the segmentation algo-
rithms we used, and then discuss the information
retrieval environment considered (in Sections 2 and
3 respectively). Section 4 then reports on the out-
come of our experiments on Chinese TREC data,
and in Section 5 we attempt to determine the reason
for the over-segmentation phenomenon witnessed.
2 Word Segmentation Algorithms
Chinese word segmentation has been extensively re-
searched. However, in Chinese information retrieval
the most common tokenziation methods are still
the simple character based approach and dictionary-
based word segmentation. In the character based
approach sentences are tokenized simply by taking
each character to be a basic unit. In the dictionary
based approach, on the other hand, one pre-defines a
lexicon containing a large number of words and then
uses heuristic methods such as maximum matching
to segment sentences. Below we experiment with
these standard methods, but in addition employ two
recently proposed segmentation algorithms that al-
low some control of how accurately words are seg-
mented. The details of these algorithms can be
found in the given references. For the sake of com-
pleteness we briefly describe the basic approaches
here.
2.1 Dictionary based word segmentation
The dictionary based approach is the most popu-
lar Chinese word segmentation method. The idea is
to use a hand built dictionary of words, compound
words, and phrases to index the text. In our experi-
ments we used the longest forward match method in
which text is scanned sequentially and the longest
matching word from the dictionary is taken at each
successive location. The longest matched strings are
then taken as indexing tokens and shorter tokens
within the longest matched strings are discarded. In
our experiments we used two different dictionaries.
The first is the Chinese dictionary used by Gey et
al. (1997), which includes 137,659 entries. The sec-
ond is the Chinese dictionary used by Beaulieu et al
(1997), which contains 69,353 words and phrases.
2.2 Compression based word segmentation
The PPM word segmentation algorithm of Teahan et
al. (2001) is based on the text compression method
of Cleary and Witten (1984). PPM learns an n-gram
language model by supervised training on a given set
of hand segmented Chinese text. To segment a new
sentence, PPM seeks the segmentation which gives
the best compression using the learned model. This
has been proven to be a highly accurate segmenter
(Teahan et al 2001). Its quality is affected both by
the amount of training data and by the order of the
n-gram model. By controlling the amount of train-
ing data and the order of language model we can
control the resulting word segmentation accuracy.
2.3 EM based word segmentation
The ?self-supervised? segmenter of Peng and Schu-
urmans (2001) is an unsupervised technique based
on a variant of the EM algorithm. This method
learns a hidden Markov model of Chinese words, and
then segments sentences using the Viterbi algorithm
(Rabiner, 1989). It uses a heuristic technique to
reduce the size of the learned lexicon and prevent
the acquisition of erroneous word agglomerations.
Although the segmentation accuracy of this unsu-
pervised method is not as high as the supervised
PPM algorithm, it nevertheless obtains reasonable
performance and provides a fundamentally different
segmentation scheme from PPM. The segmentation
performance of this technique can be controlled by
varying the number of training iterations and by ap-
plying different lexicon pruning techniques.
3 Information Retrieval Method
We conducted our information retrieval experiments
using the OKAPI system (Huang and Robertson,
2000; Robertson et al, 1994). In an attempt to en-
sure that the phenomena we observe are not specific
to a particular retrieval technique, we experimented
with a parameterized term weighting scheme which
allowed us to control the quality of retrieval per-
formance. We considered a refined term weighting
scheme based on the the standard term weighting
function
w0 = logN ? n+ 0.5n+ 0.5 (1)
where N is the number of indexed documents in the
collection, and n is the number of documents con-
taining a specific term (Spark Jones, 1979). Many
researchers have shown that augmenting this basic
function to take into account document length, as
well as within-document and within-query frequen-
cies, can be highly beneficial in English text retrieval
(Beaulieu et al, 1997). For example, one standard
augmentation is to use
w1 = w0 ? (c1 + 1) ? tfK + tf ?
(c2 + 1 ) ? qtf
c2 + qtf (2)
where
K = c1 ?
(
1? c3 + c3 dlavdl
)
Here tf is within-document term frequency, qtf is
within-query term frequency, dl is the length of
the document, avdl is the average document length,
and c1, c2, c3 are tuning constants that depend on
the database, the nature of the queries, and are
empirically determined. However, to truly achieve
state-of-the-art retrieval performance, and also to
allow for the quality of retrieval to be manipulated,
we further augmented this standard term weighting
scheme with an extra correction term
w2 = w1 ? kd ? y (3)
This correction allows us to more accurately account
for the length of the document. Here ? indicates
that the component is added only once per docu-
ment, rather than for each term, and
y =
?
????
????
ln( dlavdl ) + ln(c4) if dl ? rel avdl
(ln( rel avdlavdl ) + ln(c4)
) (1? dl?rel avdlc5?avdl?rel avdl
)
if dl > rel avdl
where rel avdl is the average relevant document
length calculated from previous queries based on the
same collection of documents. Overall, this term
weighting formula has five tuning constants, c1 to
c5, which are all set from previous research on En-
glish text retrieval and some initial experiments on
Chinese text retrieval. In our experiments, the val-
ues of the five arbitrary constants c1, c2, c3, c4 and
c5 were set to 2.0, 5.0, 0.75, 3 and 26 respectively.
The key constant is the quantity kd, which is the
new tuning constant that we manipulate to control
the influence of correction factor, and hence control
the retrieval quality. By setting kd to different val-
ues, we have different term weighting methods in our
experiments. In our experiments, we tested kd set
to values of 0, 6, 8, 10, 15, 20, 50.
4 Experiments
We conducted a series of experiments in word based
Chinese information retrieval, where we varied both
the word segmentation method and the information
retrieval method. We experimented with word seg-
mentation techniques of varying accuracy, and infor-
mation retrieval methods with varying performance.
In almost every case, we witness a nonmonotonic
relationship between word segmentation accuracy
and retrieval performance, robustly across retrieval
methods. Before describing the experimental results
in detail however, we first have to describe the per-
formance measures used in the experiments.
4.1 Measuring segmentation performance
We evaluated segmentation performance on the
Mandarin Chinese corpus, PH, due to Guo Jin. This
corpus contains one million words of segmented Chi-
nese text from newspaper stories of the Xinhua news
agency of the People?s Republic of China published
between January 1990 and March 1991.
To make the definitions precise, first define the
original segmented test corpus to be S. We then
collapse all the whitespace between words to make
a second unsegmented corpus U , and then use the
segmenter to recover an estimate S? of the original
segmented corpus. We measure the segmentation
performance by precision, recall, and F-measure on
detecting correct words. Here, a word is considered
to be correctly recovered if and only if (Palmer and
Burger, 1997)
1. a boundary is correctly placed in front of the
first character of the word
2. a boundary is correctly placed at the end of the
last character of the word
3. and there is no boundary between the first and
last character of the word.
Let N1 denote the number of words in S, let N2 de-
note the number of words in the estimated segmen-
tation S?, and let N3 denote the number of words
correctly recovered. Then the precision, recall and
F measures are defined
precision: p = N3N2
recall: r = N3N1
F-measure: F = 2?p?rp+r
In this paper, we only report the performance in
F-measure, which is a comprehensive measure that
combines precision and the recall.
4.2 Measuring retrieval performance
We used the TREC relevance judgments for each
topic that came from the human assessors of the
National Institute of Standards and Technology
(NIST). Our statistical evaluation was done by
means of the TREC evaluation program. The mea-
sures we report are Average Precision: average pre-
cision over all 11 recall points (0.0, 0.1, 0.2,..., 1.0);
and R Precision: precision after the number of doc-
uments retrieved is equal to the number of known
relevant documents for a query. Detailed descrip-
tions of these measures can be found in (Voorhees
and Harman, 1998).
4.3 Data sets
We used the information retrieval test collections
from TREC-5 and TREC-6 (Voorhees and Harman,
1998). (Note that the document collection used in
the TREC-6 Chinese track was identical to the one
used in TREC-5, however the topic queries differ.)
This collection of Chinese text consists of 164,768
documents and 139,801 articles selected from the
People?s Daily newspaper, and 24,988 articles se-
lected from the Xinhua newswire. The original arti-
cles are tagged in SGML, and the Chinese characters
in these articles are encoded using the GB (Guo-
Biao) coding scheme. Here 0 bytes is the minimum
file size, 294,056 bytes is the maximum size, and 891
bytes is the average file size.
To provide test queries for our experiments, we
considered the 54 Chinese topics provided as part
of the TREC-5 and TREC-6 evaluations (28 for
TREC-5 and 26 for TREC-6).
Finally, for the two learning-based segmentation
algorithms, we used two separate training corpora
but a common test corpus to evaluate segmentation
accuracy. For the PPM segmenter we used 72% of
the PH corpus as training data. For the the self-
supervised segmenter we used 10M of data from the
data set used in (Ge et al, 1999), which contains one
year of People?s Daily news service stories. We used
the entire PH collection as the test corpus (which
gives an unfair advantage to the supervised method
PPM which is trained on most of the same data).
4.4 Segmentation accuracy control
By using the forward maximum matching segmen-
tation strategy with the two dictionaries, Berkeley
and City, we obtain the segmentation performance
of 71% and 85% respectively. For the PPM algo-
rithm, by controlling the order of the n-gram lan-
guage model used (specifically, order 2 and order
3) we obtain segmenters that achieve 90% and 95%
word recognition accuracy respectively. Finally, for
the self-supervised learning technique, by controlling
the number of EM iterations and altering the lexi-
con pruning strategy we obtain word segmentation
accuracies of 44%, 49%, 53%, 56%, 59%, 70%, 75%,
and 77%. Thus, overall we obtain 12 different seg-
menters that achieve segmentation performances of
44%, 49%, 53%, 56%, 59%, 70%, 71%, 75%, 77%,
85%, 90%, and 95%.
4.5 Experimental results
Now, given the 12 different segmenters, we con-
ducted extensive experiments on the TREC data
sets using different information retrieval methods
(achieved by tuning the kd constant in the term
weighting function described in Section 3).
Table 1 shows the average precision and R-
precision results obtained on the TREC-5 and
TREC-6 queries when basing retrieval on word seg-
mentations at 12 different accuracies, for a single
retrieval method, kd = 10. To illustrate the results
graphically, we re-plot this data in Figure 1, in which
the x-axis is the segmentation performance and the
y-axis is the retrieval performance.
seg. accuracy TREC-5 TREC-6
44% 0.2231/0.2843 0.3424/0.3930
49% 0.2647/0.3259 0.3848/0.4201
53% 0.2999/0.3376 0.4492/0.4801
56% 0.3056/0.3462 0.4473/0.4727
59% 0.3097/0.3533 0.4740/0.4960
70% 0.3721/0.3988 0.5044/0.5072
71% 0.3656/0.4088 0.5133/0.5116
75% 0.3652/0.4000 0.4987/0.5097
77% 0.3661/0.4027 0.4968/0.4973
85% 0.3488/0.3898 0.5049/0.5047
90% 0.3213/0.3663 0.4983/0.5008
95% 0.3189/0.3669 0.4867/0.4933
Table 1: Average precision and R-precision results
on TREC queries when kd = 10.
0.4 0.6 0.8 10.2
0.25
0.3
0.35
0.4
0.45 Relation of segmentation performance and retrieval performance on TREC5 (kd=10)
P?precisionR?precision
0.4 0.6 0.8 10.2
0.3
0.4
0.5
0.6 Relation of segmentation performance and retrieval performance on TREC6 (kd=10)
P?precisionR?precision
Figure 1: Retrieval F-measure (y-axis) versus seg-
mentation accuracy (x-axis) for kd = 10.
Clearly these curves demonstrate a nonmonotonic
relationship between retrieval performance (on the
both P-precision and the R-precision) and segmen-
tation accuracy. In fact, the curves show a clear
uni-modal shape, where for segmentation accura-
cies 44% to 70% the retrieval performance increases
steadily, but then plateaus for segmentation accu-
racies between 70% and 77%, and finally decreases
slightly when the segmentation performance increase
to 85%,90% and 95%.
This phenomenon is robustly observed as we
alter the retrieval method by setting kd =
0, 6, 8, 15, 20, 50, as shown in Figures 2 to 7 respec-
tively.
To give a more detailed picture of the results, Fig-
ures 8 and 9 we illustrate the full precision-recall
curves for kd = 10 at each of the 12 segmentation
accuracies, for TREC-5 and TREC-6 queries respec-
tively. In these figures, the 44%, 49% segmentations
are marked with stars, the 53%, 56%, 59% segmen-
tations are marked with circles, the 70%, 71%, 75%,
77% segmentations are marked with diamonds, the
85% segmentation is marked with hexagrams, and
the 90% and 95% segmentations are marked with
triangles. We can see that the curves with the dia-
monds are above the others, while the curves with
stars are at the lowest positions.
0.4 0.6 0.8 10.1
0.15
0.2
0.25
0.3
0.35
0.4 Relation of segmentation performance and retrieval performance on TREC5 (kd=0)
P?precisionR?precision
0.4 0.6 0.8 10.2
0.3
0.4
0.5 Relation of segmentation performance and retrieval performance on TREC6 (kd=0)
P?precisionR?precision
Figure 2: Results for kd = 0.
5 Discussion
The observations were surprising to us at first, al-
though they suggest that there is an interesting phe-
nomenon at work. To attempt to identify the under-
lying cause, we break the explanation into two parts:
one for the first part of the curves where retrieval
performance increases with increasing segmentation
accuracy, and a second effect for the region where
retrieval performance plateaus and eventually de-
creases with increasing segmentation accuracy.
The first part of these performance curves seems
easy to explain. At low segmentation accuracies the
segmented tokens do not correspond to meaningful
0.4 0.6 0.8 10.2
0.25
0.3
0.35
0.4
0.45 Relation of segmentation performance and retrieval performance on TREC5 (kd=6)
P?precisionR?precision
0.4 0.6 0.8 10.2
0.3
0.4
0.5
0.6 Relation of segmentation performance and retrieval performance on TREC6 (kd=6)
P?precisionR?precision
Figure 3: Results for kd = 6.0.4 0.6 0.8 10.2
0.25
0.3
0.35
0.4
0.45 Relation of segmentation performance and retrieval performance on TREC5 (kd=8)
P?precisionR?precision
0.4 0.6 0.8 10.2
0.3
0.4
0.5
0.6 Relation of segmentation performance and retrieval performance on TREC6 (kd=8)
P?precisionR?precision
Figure 4: Results for kd = 8.
linguistic terms, such as words, which hampers re-
trieval performance because the term weighting pro-
cedure is comparing arbitrary tokens to the query.
However, as segmentation accuracy improves, the to-
kens behave more like true words and the retrieval
engine begins to behave more conventionally.
However, after a point, when the second regime
is reached, retrieval performance no longer increases
with improved segmentation accuracy, and eventu-
ally begins to decrease. One possible explanation
for this which we have found is that a weak word
segmenter accidentally breaks compound words into
smaller constituents, and this, surprisingly yields a
beneficial effect for Chinese information retrieval.
For example, one of the test queries, Topic 34,
is about the impact of droughts in various parts of
China. Retrieval based on the EM-70% segmenter
retrieved 84 of 95 relevant documents in the col-
lection, whereas retrieval based on the PPM-95%
segmenter retrieved only 52 relevant documents. In
fact, only 2 relevant documents were missed by EM-
70% but retrieved by PPM-95%, whereas 34 docu-
0.4 0.6 0.8 10.2
0.25
0.3
0.35
0.4
0.45 Relation of segmentation performance and retrieval performance on TREC5 (kd=15)
P?precisionR?precision
0.4 0.6 0.8 10.2
0.3
0.4
0.5
0.6 Relation of segmentation performance and retrieval performance on TREC6 (kd=15)
P?precisionR?precision
Figure 5: Results for kd = 15.
0.4 0.6 0.8 10.2
0.25
0.3
0.35
0.4
0.45 Relation of segmentation performance and retrieval performance on TREC5 (kd=20)
P?precisionR?precision
0.4 0.6 0.8 10.2
0.3
0.4
0.5
0.6 Relation of segmentation performance and retrieval performance on TREC6 (kd==20)
P?precisionR?precision
Figure 6: Results for kd = 20.
ments retrieved by EM-70% and not by PPM-95%.
In investigating this phenomenon, one finds that the
performance drop appears to be due to the inherent
nature of written Chinese. That is, in written Chi-
nese many words can often legally be represented
their subparts. For example, @*?(agriculture
plants) is sometimes represented as*?(plants). So
for example in Topic 34, the PPM-95% segmenter
correctly segments B? as B?(drought disas-
ter) and @*? correctly as @*? (agriculture
plants), whereas the EM-70% segmenter incorrectly
segments B? as B(drought) and ?(disaster), and
incorrectly segments @*? as @(agriculture) and
*?(plants). However, by inspecting the relevant
documents for Topic 34, we find that there are many
Chinese character strings in these documents that
are closely related to the correctly segmented word
B?(drought disaster). These alternative words are
B?B??IB??B?fiB?BK etc. For
example, in the relevant document ?pd9105-832?,
which is ranked 60th by EM-70% and 823rd by
PPM-95%, the correctly segmented word B? does
0.4 0.6 0.8 10.1
0.15
0.2
0.25
0.3
0.35
0.4 Relation of segmentation performance and retrieval performance on TREC5 (kd=50)
P?precisionR?precision
0.4 0.6 0.8 10.2
0.3
0.4
0.5 Relation of segmentation performance and retrieval performance on TREC6 (kd=50)
P?precisionR?precision
Figure 7: Results for kd = 50.
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 10
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
Recall
P?P
recis
ion
Overview of the TREC5 results 44%49%53%56%59%70%71%75%77%85%90%95%
Figure 8: TREC5 precision-recall comprehensive
view at kd = 10
not appear at all. Consequently, the correct seg-
mentation for B? by PPM-95% leads to a much
weaker match than the incorrect segmentation of
EM-70%. Here EM-70% segments B? into B and
? , which is not regarded as a correct segmentation.
However, there are many matches between the topic
and relevant documents which contain onlyB. This
same phenomenon happens with the query word @
*? since many documents only contain the frag-
ment *? instead of @*?, and these documents
are all missed by PPM-95% but captured by EM-
70%.
Although straightforward, these observations sug-
gest a different trajectory for future research on Chi-
nese information retrieval. Instead of focusing on
achieving accurate word segmentation, we should
pay more attention to issues such as keyword weight-
ing (Huang and Robertson, 2000) and query key-
word extraction (Chien et al 1997). Also, we find
that the weak unsupervised segmentation method
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 10
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1 Overview of the TREC?6 results
Recall
P?P
recis
ion
44%49%53%56%59%70%71%75%77%85%90%95%
Figure 9: TREC6 precision-recall comprehensive
view at kd = 10
based yields better Chinese retrieval performance
than the other approaches, which suggests a promis-
ing new avenue to apply machine learning techniques
to IR (Sparck Jones, 1991). Of course, despite these
results we expect highly accurate word segmenta-
tion to still play an important role in other Chinese
information processing tasks such as information ex-
traction and machine translation. This suggests that
some different evaluation standards for Chinese word
segmentation should be given to different NLP ap-
plications.
6 Acknowledgments
Research supported by Bell University Labs, MI-
TACS and NSERC. We sincerely thank Dr. William
Teahan for supplying us the PPM segmenters.
References
Beaulieu, M. and Gatford, M. and Huang, X. and
Robertson, S. and Walker, S. and Williams, P.
1997. Okapi at TREC-5. In Proceedings TREC-5.
Brent, M. and Tao, X. 2001, Chinese Text Segmen-
tation With MBDP-1: Making the Most of Train-
ing Corpora. In Proceedings ACL-2001.
Buckley, C., Singhal, A., and Mitra, M. 1997. Us-
ing query zoning and correlation within SMART:
TREC-5. In Proceedings TREC-5.
Chang, J.-S. and Su, K.-Y. 1997, An Unsupervised
Iterative Method for Chinese New Lexicon Extrac-
tion, In Int J Comp Ling & Chinese Lang Proc.
Chen, A. and He, J. and Xu, L. and Gey, F. and
Meggs, J. 1997. Chinese Text Retrieval Without
Using a Dictionary. In Proceedings SIGIR-97.
Chien L. and Huang, T. and Chien, M. 1997 In
Proceedings SIGIR-97.
Cleary, J. and Witten, I. 1984. Data compression
using adaptive coding and partial string matching.
In IEEE Trans Communications, 32(4): 396-402.
Foo, S. and Li, H. 2001 Chinese Word Segmentation
Accuracy and Its Effects on Information Retrieval.
In TEXT Technology.
Ge, X., Pratt, W. and Smyth, P. 1999. Discover-
ing Chinese Words from Unsegmented Text. In
Proceedings SIGIR-99.
Gey, F., Chen, A., He, J., Xu, L. and Meggs, J.
1997 Term Importance, Boolean Conjunct Train-
ning Negative Terms, and Foreign Language Re-
trieval: Probabilistic Algorithms at TREC-5. In
Proceedings TREC-5.
Hockenmaier, J. and Brew C. 1998. Error driven
segmentation of Chinese. In Comm. COLIPS,
8(1): 69-84.
Huang, X. and Robertson, S. 2000. A probabilistic
approach to Chinese information retrieval: theory
and experiments. In Proceedings BCS-IRSG 2000.
Jin, W. 1992, Chinese Segmentation and its Disam-
biguation, Tech report, New Mexico State Univ.
Nie, J., Brisebois, M. and Ren, X. 1996. On Chinese
text retrieval. In Proceedings SIGIR-96.
Palmer, D. and Burger, J. 1997. Chinese Word Seg-
mentation and Information Retrieval. In AAAI
Symp Cross-Language Text and Speech Retrieval.
Peng, F., Huang, X., Schuurmans, D., Cercone, N.,
and Robertson, S. 2002. Using Self-supervised
Word Segmentation in Chinese Information Re-
trieval. In Proceedings SIGIR-02.
Peng, F. and Schuurmans, D. 2001. Self-supervised
Chinese Word Segmentation. In Proceedings IDA-
01, LNCS 2189.
Rabiner, L. 1989. A Tutorial on Hidden Markov
Models and Selected Applications in Speech
Recognition. In Proceedings of IEEE, 77(2).
Robertson, S. and Walker, S. 1994. Some Simple
Effective Approximations to the 2-Poisson Model
for Probabilistic Weighted Retrieval. SIGIR-94.
Sparck Jones, K. 1991 The Role of Artificial In-
telligence in Information Retrieval J. Amer. Soc.
Info. Sci., 42(8): 558-565.
Sparck Jones, K. 1979. Search Relevance Weight-
ing Given Little Relevance Information. In J. of
Documentation, 35(1).
Sproat, R. and Shih, C. 1990. A Statistical Method
for Finding Word Boundaries in Chinese Text, In
Comp Proc of Chinese and Oriental Lang, 4.
Teahan, W. J. and Wen, Y. and McNab, R. and
Witten I. H. 2001 A Compression-based Algo-
rithm for Chinese Word Segmentation. In Com-
put. Ling., 26(3):375-393.
Voorhees, E. and Harman, D. 1998. Overview of
the Sixth Text REtrieval Conference (TREC-6),
In Proceedings TREC-6.
Wu, Z. and Tseng, G. 1993. Chinese Text Segmen-
tation for Text Retrieval: Achievements and Prob-
lems. In J. Amer. Soc. Info. Sci., 44(9): 532-542.
267
268
269
270
271
272
273
274
Language and Task Independent Text Categorization
with Simple Language Models
Fuchun Peng Dale Schuurmans Shaojun Wang
School of Computer Science, University of Waterloo
200 University Avenue West, Waterloo, Ontario, Canada, N2L 3G1
{f3peng, dale, sjwang}@cs.uwaterloo.ca
Abstract
We present a simple method for language inde-
pendent and task independent text categoriza-
tion learning, based on character-level n-gram
language models. Our approach uses simple
information theoretic principles and achieves
effective performance across a variety of lan-
guages and tasks without requiring feature se-
lection or extensive pre-processing. To demon-
strate the language and task independence of
the proposed technique, we present experimen-
tal results on several languages?Greek, En-
glish, Chinese and Japanese?in several text
categorization problems?language identifica-
tion, authorship attribution, text genre classifi-
cation, and topic detection. Our experimental
results show that the simple approach achieves
state of the art performance in each case.
1 Introduction
Text categorization concerns the problem of automati-
cally assigning given text passages (paragraphs or doc-
uments) into predefined categories. Due to the rapid ex-
plosion of texts in digital form, text categorization has
become an important area of research owing to the need
to automatically organize and index large text collections
in various ways. Such techniques are currently being ap-
plied in many areas, including language identification,
authorship attribution (Stamatatos et al, 2000), text genre
classification (Kesseler et al, 1997; Stamatatos et al,
2000), topic identification (Dumais et al, 1998; Lewis,
1992; McCallum, 1998; Yang, 1999), and subjective sen-
timent classification (Turney, 2002).
Many standard machine learning techniques have been
applied to automated text categorization problems, such
as naive-Bayes classifiers, support vector machines, lin-
ear least squares models, neural networks, and K-nearest
neighbor classifiers (Yang, 1999; Sebastiani, 2002). A
common aspect of these approaches is that they treat text
categorization as a standard classification problem, and
thereby reduce the learning process to two simple steps:
feature engineering, and classification learning over the
feature space. Of these two steps, feature engineering is
critical to achieving good performance in text categoriza-
tion problems. Once good features are identified, almost
any reasonable technique for learning a classifier seems
to perform well (Scott, 1999).
Unfortunately, the standard classification learning
methodology has several drawbacks for text categoriza-
tion. First, feature construction is usually language de-
pendent. Various techniques such as stop-word removal
or stemming require language specific knowledge to de-
sign adequately. Moreover, whether one can use a purely
word-level approach is itself a language dependent issue.
In many Asian languages such as Chinese or Japanese,
identifying words from character sequences is hard, and
any word-based approach must suffer added complexity
in coping with segmentation errors. Second, feature se-
lection is task dependent. For example, tasks like au-
thorship attribution or genre classification require atten-
tion to linguistic style markers (Stamatatos et al, 2000),
whereas topic detection systems rely more heavily on bag
of words features. Third, there are an enormous num-
ber of possible features to consider in text categorization
problems, and standard feature selection approaches do
not always cope well in such circumstances. For exam-
ple, given an enormous number of features, the cumu-
lative effect of uncommon features can still have an im-
portant effect on classification accuracy, even though in-
frequent features contribute less information than com-
mon features individually. Consequently, throwing away
uncommon features is usually not an appropriate strat-
egy in this domain (Aizawa, 2001). Another problem is
that feature selection normally uses indirect tests, such
as ?2 or mutual information, which involve setting arbi-
                                                               Edmonton, May-June 2003
                                                             Main Papers , pp. 110-117
                                                         Proceedings of HLT-NAACL 2003
trary thresholds and conducting a heuristic greedy search
to find good feature sets. Finally, by treating text cate-
gorization as a classical classification problem, standard
approaches can ignore the fact that texts are written in
natural language, meaning that they have many implicit
regularities that can be well modeled with specific tools
from natural language processing.
In this paper, we propose a straightforward text cate-
gorization learning method based on learning category-
specific, character-level, n-gram language models. Al-
though this is a very simple approach, it has not yet been
systematically investigated in the literature. We find that,
surprisingly, we obtain competitive (and often superior)
results to more sophisticated learning and feature con-
struction techniques, while requiring almost no feature
engineering or pre-processing. In fact, the overall ap-
proach requires almost no language specific or task spe-
cific pre-processing to achieve effective performance.
The success of this simple method, we think, is due to
the effectiveness of well known statistical language mod-
eling techniques, which surprisingly have had little sig-
nificant impact on the learning algorithms normally ap-
plied to text categorization. Nevertheless, statistical lan-
guage modeling is also concerned with modeling the se-
mantic, syntactic, lexicographical and phonological regu-
larities of natural language?and would seem to provide a
natural foundation for text categorization problems. One
interesting difference, however, is that instead of explic-
itly pre-computing features and selecting a subset based
on arbitrary decisions, the language modeling approach
simply considers all character (or word) subsequences
occurring in the text as candidate features, and implic-
itly considers the contribution of every feature in the fi-
nal model. Thus, the language modeling approach com-
pletely avoids a potentially error-prone feature selection
process. Also, by applying character-level language mod-
els, one also avoids the word segmentation problems that
arise in many Asian languages, and thereby achieves a
language independent method for constructing accurate
text categorizers.
2 n-Gram Language Modeling
The dominant motivation for language modeling has tra-
ditionally come from speech recognition, but language
models have recently become widely used in many other
application areas.
The goal of language modeling is to predict the prob-
ability of naturally occurring word sequences, s =
w1w2...wN ; or more simply, to put high probability on
word sequences that actually occur (and low probability
on word sequences that never occur). Given a word se-
quence w1w2...wN to be used as a test corpus, the quality
of a language model can be measured by the empirical
perplexity and entropy scores on this corpus
Perplexity = N
????
N?
i=1
1
Pr(wi|w1...wi?1) (1)
Entropy = log2 Perplexity (2)
where the goal is to minimize these measures.
The simplest and most successful approach to lan-
guage modeling is still based on the n-gram model. By
the chain rule of probability one can write the probability
of any word sequence as
Pr(w1w2...wN ) =
N?
i=1
Pr(wi|w1...wi?1) (3)
An n-gram model approximates this probability by
assuming that the only words relevant to predicting
Pr(wi|w1...wi?1) are the previous n? 1 words; i.e.
Pr(wi|w1...wi?1) = Pr(wi|wi?n+1...wi?1)
A straightforward maximum likelihood estimate of n-
gram probabilities from a corpus is given by the observed
frequency of each of the patterns
Pr(wi|wi?n+1...wi?1) = #(wi?n+1...wi)#(wi?n+1...wi?1) (4)
where #(.) denotes the number of occurrences of a spec-
ified gram in the training corpus. Although one could at-
tempt to use simple n-gram models to capture long range
dependencies in language, attempting to do so directly
immediately creates sparse data problems: Using grams
of length up to n entails estimating the probability of Wn
events, where W is the size of the word vocabulary. This
quickly overwhelms modern computational and data re-
sources for even modest choices of n (beyond 3 to 6).
Also, because of the heavy tailed nature of language (i.e.
Zipf?s law) one is likely to encounter novel n-grams that
were never witnessed during training in any test corpus,
and therefore some mechanism for assigning non-zero
probability to novel n-grams is a central and unavoidable
issue in statistical language modeling. One standard ap-
proach to smoothing probability estimates to cope with
sparse data problems (and to cope with potentially miss-
ing n-grams) is to use some sort of back-off estimator.
Pr(wi|wi?n+1...wi?1)
=
?
???
???
P?r(wi|wi?n+1...wi?1),
if #(wi?n+1...wi) > 0
?(wi?n+1...wi?1)? Pr(wi|wi?n+2...wi?1),
otherwise
(5)
where
P?r(wi|wi?n+1...wi?1) = discount#(wi?n+1...wi)#(wi?n+1...wi?1)
(6)
is the discounted probability and ?(wi?n+1...wi?1) is a
normalization constant
?(wi?n+1...wi?1) =
1?
?
x?(wi?n+1...wi?1x)
P?r(x|wi?n+1...wi?1)
1?
?
x?(wi?n+1...wi?1x)
P?r(x|wi?n+2...wi?1)
(7)
The discounted probability (6) can be computed
with different smoothing techniques, including absolute
smoothing, Good-Turing smoothing, linear smoothing,
and Witten-Bell smoothing (Chen and Goodman, 1998).
The details of the smoothing techniques are omitted here
for simplicity.
The language models described above use individual
words as the basic unit, although one could instead con-
sider models that use individual characters as the ba-
sic unit. The remaining details remain the same in this
case. The only difference is that the character vocabu-
lary is always much smaller than the word vocabulary,
which means that one can normally use a much higher
order, n, in a character-level n-gram model (although the
text spanned by a character model is still usually less
than that spanned by a word model). The benefits of the
character-level model in the context of text classification
are several-fold: it avoids the need for explicit word seg-
mentation in the case of Asian languages, it captures im-
portant morphological properties of an author?s writing,
it models the typos and misspellings that are common in
informal texts, it can still discover useful inter-word and
inter-phrase features, and it greatly reduces the sparse
data problems associated with large vocabulary models.
In this paper, we experiment with character-level models
to achieve flexibility and language independence.
3 Language Models as Text Classifiers
Our approach to applying language models to text cat-
egorization is to use Bayesian decision theory. Assume
we wish to classify a text D into a category c ? C =
{c1, ..., c|C|}. A natural choice is to pick the category c
that has the largest posterior probability given the text.
That is,
c? = argmax
c?C
{Pr(c|D)} (8)
Using Bayes rule, this can be rewritten as
c? = argmax
c?C
{Pr(D|c) Pr(c)} (9)
= argmax
c?C
{Pr(D|c)} (10)
= argmax
c?C
{ N?
i=1
Prc(wi|wi?n+1...wi?1)
}
(11)
where deducing Eq. (10) from Eq. (9) assumes uniformly
weighted categories (since we have no other prior knowl-
edge). Here, Pr(D|c) is the likelihood of D under cate-
gory c, which can be computed by Eq. (11). Likelihood is
related to perplexity and entropy by Eq. (1) and Eq. (2).
Therefore, our approach is to learn a separate language
model for each category, by training on a data set from
that category. Then, to categorize a new text D, we sup-
ply D to each language model, evaluate the likelihood
(or entropy) of D under the model, and pick the winning
category according to Eq. (10).
The inference of an n-gram based text classifier is
very similar to a naive-Bayes classifier. In fact, n-gram
classifiers are a straightforward generalization of naive-
Bayes: A uni-gram classifier with Laplace smoothing
corresponds exactly to the traditional naive-Bayes clas-
sifier. However, n-gram language models, for larger n,
possess many advantages over naive-Bayes classifiers, in-
cluding modeling longer context and applying superior
smoothing techniques in the presence of sparse data.
4 Experimental Comparison
We now proceed to present our results on several text
categorization problems on different languages. Specif-
ically, we consider language identification, Greek author-
ship attribution, Greek genre classification, English topic
detection, Chinese topic detection and Japanese topic de-
tection.
For the sake of consistency with previous re-
search (Aizawa, 2001; He et al, 2000; Stamatatos et
al., 2000), we measure categorization performance by the
overall accuracy, which is the number of correctly iden-
tified texts divided by the total number of texts consid-
ered. We also measure the performance with Macro F-
measure, which is the average of the F-measures across
all categories. F-measure is a combination of precision
and recall (Yang, 1999).
4.1 Language Identification
The first text categorization problem we examined was
language identification?a useful pre-processing step in
information retrieval. Language identification is proba-
bly the easiest text classification problem because of the
significant morphological differences between languages,
n Absolute Good-Turing Linear Witten-Bell
Acc. F-Mac Acc. F-Mac Acc. F-Mac Acc. F-Mac
1 0.57 0.53 0.55 0.49 0.55 0.49 0.55 0.49
2 0.85 0.84 0.80 0.75 0.84 0.83 0.84 0.82
3 0.90 0.89 0.79 0.72 0.89 0.88 0.89 0.87
4 0.87 0.85 0.79 0.72 0.85 0.82 0.88 0.86
5 0.86 0.85 0.79 0.72 0.87 0.85 0.86 0.83
6 0.86 0.83 0.79 0.73 0.87 0.85 0.86 0.83
Table 1: Results on Greek authorship attribution
even when they are based on the same character set.1 In
our experiments, we considered one chapter of Bible that
had been translated into 6 different languages: English,
French, German, Italian, Latin and Spanish. In each
case, we reserved twenty sentences from each language
for testing and used the remainder for training. For this
task, with only bi-gram character-level models and any
smoothing technique, we achieved 100% accuracy.
4.2 Authorship Attribution
The second text categorization problem we examined was
author attribution. A famous example is the case of the
Federalist Papers, of which twelve instances are claimed
to have been written both by Alexander Hamilton and
James Madison (Holmes and Forsyth, 1995). Authorship
attribution is more challenging than language identifica-
tion because the difference among the authors is much
more subtle than that among different languages. We con-
sidered a data set used by (Stamatatos et al, 2000) con-
sisting of 20 texts written by 10 different modern Greek
authors (totaling 200 documents). In each case, 10 texts
from each author were used for training and the remain-
ing 10 for testing.
The results using different orders of n-gram models
and different smoothing techniques are shown in Table 1.
With 3-grams and absolute smoothing, we observe 90%
accuracy. This result compares favorably to the 72%
accuracy reported in (Stamatatos et al, 2000) which is
based on linear least square fit (LLSF).
4.3 Text Genre Classification
The third problem we examined was text genre classifi-
cation, which is an important application in information
retrieval (Kesseler et al, 1997; Lee et al, 2002). We con-
sidered a Greek data set used by (Stamatatos et al, 2000)
consisting of 20 texts of 10 different styles extracted from
various sources (200 documents total). For each style, we
used 10 texts as training data and the remaining 10 as test-
ing.
1Language identification from speech is much harder.
n Absolute Good-Turing Linear Witten-Bell
Acc. F-Mac Acc. F-Mac Acc. F-Mac Acc. F-Mac
1 0.31 0.55 0.30 0.54 0.30 0.54 0.30 0.54
2 0.86 0.86 0.60 0.52 0.82 0.81 0.86 0.86
3 0.77 0.75 0.65 0.59 0.79 0.77 0.85 0.85
4 0.69 0.65 0.58 0.50 0.74 0.69 0.76 0.74
5 0.66 0.61 0.56 0.49 0.69 0.66 0.73 0.70
6 0.62 0.57 0.49 0.53 0.67 0.63 0.71 0.68
7 0.63 0.58 0.49 0.53 0.66 0.62 0.70 0.68
Table 2: Results on Greek text genre classification
The results of learning an n-gram based text classifier
are shown in Table 2. The 86% accuracy obtained with
bi-gram models compares favorably to the 82% reported
in (Stamatatos et al, 2000), which again is based on a
much deeper NLP analysis.
4.4 Topic Detection
The fourth problem we examined was topic detection in
text, which is a heavily researched text categorization
problem (Dumais et al, 1998; Lewis, 1992; McCallum,
1998; Yang, 1999; Sebastiani, 2002). Here we demon-
strate the language independence of the language mod-
eling approach by considering experiments on English,
Chinese and Japanese data sets.
4.4.1 English Data
The English 20 Newsgroup data has been widely used
in topic detection research (McCallum, 1998; Rennie,
2001).2 This collection consists of 19,974 non-empty
documents distributed evenly across 20 newsgroups. We
use the newsgroups to form our categories, and randomly
select 80% of the documents to be used for training and
set aside the remaining 20% for testing.
In this case, as before, we merely considered text to be
a sequence of characters, and learned character-level n-
gram models. The resulting classification accuracies are
reported in in Table 3. With 3-gram (or higher order)
models, we consistently obtain accurate performance,
peaking at 89% accuracy in the case of 6-gram models
with Witten-Bell smoothing. (We note that word-level
models were able to achieve 88% accuracy in this case.)
These results compare favorably to the state of the art re-
sult of 87.5% accuracy reported in (Rennie, 2001), which
was based on a combination of an SVM with error correct
output coding (ECOC).
4.4.2 Chinese Data
Chinese topic detection is often thought to be more
challenging than English, because words are not white-
space delimited in Chinese text. This fact seems to
2http://www.ai.mit.edu/? jrennie/20Newsgroups/
n Absolute Good-Turing Linear Witten-Bell
Acc. F-Mac Acc. F-Mac Acc. F-Mac Acc. F-Mac
1 0.22 0.21 0.22 0.21 0.22 0.21 0.22 0.21
2 0.68 0.66 0.69 0.67 0.68 0.67 0.67 0.65
3 0.86 0.86 0.86 0.86 0.86 0.85 0.86 0.86
4 0.88 0.88 0.88 0.87 0.87 0.87 0.89 0.88
5 0.89 0.88 0.87 0.87 0.88 0.88 0.89 0.89
6 0.89 0.88 0.88 0.88 0.88 0.88 0.89 0.89
7 0.89 0.88 0.88 0.87 0.88 0.88 0.89 0.89
8 0.88 0.88 0.87 0.87 0.88 0.88 0.89 0.89
9 0.88 0.88 0.87 0.87 0.88 0.88 0.89 0.89
Table 3: Topic detection results on English 20 Newsgroup
data
n Absolute Good-Turing Linear Witten-Bell
Acc. F-Mac Acc. F-Mac Acc. F-Mac Acc. F-Mac
1 0.77 0.77 0.76 0.77 0.76 0.76 0.77 0.77
2 0.80 0.80 0.80 0.80 0.79 0.79 0.80 0.80
3 0.80 0.80 0.81 0.81 0.80 0.80 0.80 0.80
4 0.80 0.80 0.81 0.81 0.81 0.80 0.80 0.80
Table 4: Chinese topic detection results
require word segmentation to be performed as a pre-
processing step before further classification (He et al,
2000). However, we avoid the need for explicit segmen-
tation by simply using a character level n-gram classifier.
For Chinese topic detection we considered a data set
investigated in (He et al, 2000). The corpus in this case
is a subset of the TREC-5 data set created for research on
Chinese text retrieval. To make the data set suitable for
text categorization, documents were first clustered into
101 groups that shared the same headline (as indicated
by an SGML tag) and the six most frequent groups were
selected to make a Chinese text categorization data set.
In each group, 500 documents were randomly selected
for training and 100 documents were reserved for testing.
We observe over 80% accuracy for this task, using bi-
gram (2 Chinese characters) or higher order models. This
is the same level of performance reported in (He et al,
2000) for an SVM approach using word segmentation
and feature selection.
4.4.3 Japanese Data
Japanese poses the same word segmentation issues as
Chinese. Word segmentation is also thought to be neces-
sary for Japanese text categorization (Aizawa, 2001), but
we avoid the need again by considering character level
language models.
We consider the Japanese topic detection data inves-
tigated by (Aizawa, 2001). This data set was con-
n Absolute Good-Turing Linear Witten-Bell
Acc. F-Mac Acc. F-Mac Acc. F-Mac Acc. F-Mac
1 0.33 0.29 0.34 0.29 0.34 0.29 0.34 0.29
2 0.66 0.62 0.66 0.61 0.66 0.63 0.66 0.62
3 0.75 0.72 0.75 0.72 0.76 0.73 0.75 0.72
4 0.81 0.77 0.81 0.76 0.82 0.76 0.81 0.77
5 0.83 0.77 0.83 0.76 0.83 0.76 0.83 0.77
6 0.84 0.76 0.83 0.75 0.83 0.75 0.84 0.77
7 0.84 0.75 0.83 0.74 0.83 0.74 0.84 0.76
8 0.83 0.74 0.83 0.73 0.83 0.73 0.84 0.76
Table 5: Japanese topic detection results
verted from the NTCIR-J1 data set originally created for
Japanese text retrieval research. The data has 24 cate-
gories. The testing set contains 10,000 documents dis-
tributed unevenly between categories (with a minimum of
56 and maximum of 2696 documents per category). This
imbalanced distribution causes some difficulty since we
assumed a uniform prior over categories. Although this is
easily remedied, we did not fix the problem here. Never-
theless, we obtain experimental results in Table 5 that still
show an 84% accuracy rate on this problem (for 6-gram
or higher order models). This is the same level of per-
formance as that reported in (Aizawa, 2001), which uses
an SVM approach with word segmentation, morphology
analysis and feature selection.
5 Analysis
The perplexity of a test document under a language model
depends on several factors. The two most influential fac-
tors are the order, n, of the n-gram model and the smooth-
ing technique used. Different choices will result in differ-
ent perplexities, which could influence the final decision
in using Eq. (10). We now experimentally assess the in-
fluence of each of these factors below.
5.1 Effects of n-Gram Order
The order n is a key factor in n-gram language models.
If n is too small then the model will not capture enough
context. However, if n is too large then this will create
severe sparse data problems. Both extremes result in a
larger perplexity than the optimal context length. Figures
1 and 2 illustrate the influence of order n on classifica-
tion performance and on language model quality in the
previous five experiments (all using absolute smoothing).
Note that in this case the entropy (bits per character) is
the average entropy across all testing documents. From
the curves, one can see that as the order increases, classi-
fication accuracy increases and testing entropy decreases,
presumably because the longer context better captures the
regularities of the text. However, at some point accu-
1 2 3 4 5 6 7 80.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
order n or n?gram model
Over
all ac
cura
cy
Greek authorship attributionGreek genre classificationEnglish Topic DetectionChinese Topic DetectionJapanese Topic Detection
Figure 1: Influence of the order n on the classification
performance
1 2 3 4 5 6 7 8 93
4
5
6
7
8
9
10
11
order n or n?gram model
Entr
opy
Greek authorship attributionGreek genre classification  English Topic Detection     Chinese Topic Detection     Japanese Topic Detection    
Figure 2: The entropy of different n-gram models
racy begins to decrease and entropy begins to increase
as the sparse data problems begin to set in. Interest-
ingly, the effect is more pronounced in some experiments
(Greek genre classification) but less so in other experi-
ments (topic detection under any language). The sensi-
tivity in the Greek genre case could still be attributed to
the sparse data problem (the over-fitting problem in genre
classification could be more serious than the other prob-
lems, as seen from the entropy curves).
5.2 Effects of Smoothing Technique
Another key factor affecting the performance of a lan-
guage model is the smoothing technique used. Figures 3
and 4 show the effects of smoothing techniques on clas-
sification accuracy and testing entropy (Chinese topic de-
tection and Japanese topic detection are not shown in the
figure to save space).
Here we find that, in most cases, the smoothing tech-
nique does not have a significant effect on text catego-
rization accuracy, because of the small vocabulary size of
1 1.5 2 2.5 3 3.5 4 4.5 5 5.5 60.4
0.6
0.8
1 Greek authorship attribution
Over
all A
ccura
cy
1 2 3 4 5 6 70.2
0.4
0.6
0.8
1 Greek Genre Clasification
Over
all A
ccura
cy
1 2 3 4 5 6 7 8 90.2
0.4
0.6
0.8
1 English Topic Detection
Over
all A
ccura
cy
order n of n?gram models
AbsoluteGood?TuringLinearWitten?Bell
Figure 3: Influence of smoothing on accuracy
1 1.5 2 2.5 3 3.5 4 4.5 5 5.5 63
3.5
4
4.5
5 Greek authorship attribution
Entr
opy
1 2 3 4 5 6 73.5
4
4.5
5
5.5 Greek Genre Clasification
Entr
opy
1 2 3 4 5 6 7 8 93
3.5
4
4.5
5 English Topic Detection
Entr
opy
order n of n?gram models
Absolute   Good?TuringLinear     Witten?Bell
Figure 4: The entropy of different smoothing
character level n-gram models. However, there are two
exceptions?Greek authorship attribution and Greek text
genre classification?where Good-Turing smoothing is
not as effective as other techniques, even though it gives
better test entropy than some others. Since our goal is to
make a final decision based on the ranking of perplexi-
ties, not just their absolute values, a superior smoothing
method in the sense of perplexity reduction (i.e. from
the perspective of classical language modeling) does not
necessarily lead to a better decision from the perspec-
tive of categorization accuracy. In fact, in all our exper-
iments we have found that it is Witten-Bell smoothing,
not Good-Turing smoothing, that gives the best results in
terms of classification accuracy. Our observation is con-
sistent with previous research which reports that Witten-
Bell smoothing achieves benchmark performance in char-
acter level text compression (Bell et al, 1990). For the
most part, however, one can use any standard smooth-
ing technique in these problems and obtain comparable
performance, since the rankings they produce are almost
always the same.
5.3 Relation to Previous Research
In principle, any language model can be used to perform
text categorization based on Eq. (10). However, n-gram
models are extremely simple and have been found to be
effective in many applications. For example, character
level n-gram language models can be easily applied to
any language, and even non-language sequences such as
DNA and music. Character level n-gram models are
widely used in text compression?e.g., the PPM model
(Bell et al, 1990)?and have recently been found to be
effective in text classification problems as well (Teahan
and Harper, 2001). The PPM model is a weighted lin-
ear interpolation n-gram models and has been set as a
benchmark in text compression for decades. Building an
adaptive PPM model is expensive however (Bell et al,
1990), and our back-off models are relatively much sim-
pler. Using compression techniques for text categoriza-
tion has also been investigated in (Benedetto et al, 2002),
where the authors seek a model that yields the minimum
compression rate increase when a new test document is
introduced. However, this method is found not to be gen-
erally effective nor efficient (Goodman, 2002). In our ap-
proach, we evaluate the perplexity (or entropy) directly
on test documents, and find the outcome to be both effec-
tive and efficient.
Many previous researchers have realized the impor-
tance of n-gram models in designing language indepen-
dent text categorization systems (Cavnar and Trenkle,
1994; Damashek, 1995). However, they have used n-
grams as features for a traditional feature selection pro-
cess, and then deployed classifiers based on calculating
feature-vector similarities. Feature selection in such a
classical approach is critical, and many required proce-
dures, such as stop word removal, are actually language
dependent. In our approach, all n-grams are considered
as features and their importance is implicitly weighted by
their contribution to perplexity. Thus we avoid an error
prone preliminary feature selection step.
6 Conclusion
We have presented an extremely simple approach for lan-
guage and task independent text categorization based on
character level n-gram language modeling. The approach
is evaluated on four different languages and four differ-
ent text categorization problems. Surprisingly, we ob-
serve state of the art or better performance in each case.
We have also experimentally analyzed the influence of
two factors that can affect the accuracy of this approach,
and found that for the most part the results are robust
to perturbations of the basic method. The wide appli-
cability and simplicity of this approach makes it imme-
diately applicable to any sequential data (such as natu-
ral language, music, DNA) and yields effective baseline
performance. We are currently investigating more chal-
lenging problems like multiple category classification us-
ing the Reuters-21578 data set (Lewis, 1992) and subjec-
tive sentiment classification (Turney, 2002). To us, these
results suggest that basic statistical language modeling
ideas might be more relevant to other areas of natural lan-
guage processing than commonly perceived.
7 Acknowledgments
Research supported by Bell University Labs and MI-
TACS.
References
A. Aizawa. 2001. Linguistic Techniques to Improve
the Performance of Automatic Text Categorization. In
Proceedings of the Sixth Natural Language Processing
Pacific Rim Symposium (NLPRS2001).
T. Bell, J. Cleary, and I. Witten. 1990. Text Compression.
Prentice Hall.
D. Benedetto, E. Caglioti, and V. Loreto. 2002. Lan-
guage Trees and Zipping. Physical Review Letters, 88.
W. Cavnar, J. Trenkle. 1994. N-Gram-Based Text
Categorization. Proceedings of 3rd Annual Sympo-
sium on Document Analysis and Information Retrieval
(SDAIR-94).
S. Chen and J. Goodman. 1998. An Empirical Study of
Smoothing Techniques for Language Modeling. Tech-
nical report, TR-10-98, Harvard University.
M. Damashek. 1995. Gauging Similarity with N-Grams:
Language-Independent Categorization of Text?. Sci-
ence, Vol. 267, 10 February, 843 - 848
S. Dumais, J. Platt, D. Heckerman and M. Sahami. 1998.
Inductive Learning Algorithms And Representations
For Text Categorization. In Proceedings of ACM Con-
ference on Information and Knowledge Management
(CIKM98), Nov. 1998, pp. 148-155.
J. Goodman. 2002. Comment on Language Trees and
Zipping. Unpublished Manuscript.
J. He, A. Tan, and C. Tan. 2000. A Comparative Study on
Chinese Text Categorization Methods. In Proceedings
of PRICAI?2000 International Workshop on Text and
Web Mining, p24-35.
D. Holmes, and R. Forsyth. 1995. The Federalist Revis-
ited: New Directions in Authorship Attribution. Liter-
ary and Linguistic Computing, 10, 111-127.
B. Kessler, G. Nunberg and H. Schu?ze. 1997. Automatic
Detection of Text Genre. Proceedings of the Thirty-
Fifth Annual Meeting of the Association for Computa-
tional Linguistics (ACL1997).
Y. Lee and S. Myaeng. 2002. Text Genre Classifica-
tion with Genre-Revealing and Subject-Revealing Fea-
tures. Proceedings of ACM SIGIR Conference on Re-
search and Development in Information Retrieval (SI-
GIR2002).
D. Lewis. 1992. Representation and Learning in Infor-
mation Retrieval Phd thesis, Computer Science Dept-
ment, Univ. of Massachusetts.
A. McCallum and K. Nigam. 1998. A Comparison
of Event Models for Naive Bayes Text Classification.
Proceedings of AAAI-98 Workshop on ?Learning for
Text Categorization?, AAAI Presss.
J. Rennie. 2001. Improving Multi-class Text Classifi-
cation with Naive Bayes. Master?s Thesis. M.I.T. AI
Technical Report AITR-2001-004. 2001.
S. Scott and S. Matwin. 1999. Feature Engineering for
Text Classification. In Proceedings of the Sixteenth In-
ternational Conference on Machine Learning (ICML?
99), pp. 379-388.
F. Sebastiani. 2002. Machine learning in automated text
categorization. ACM Computing Surveys, 34(1):1-47.
E. Stamatatos, N. Fakotakis and G. Kokkinakis. 2000.
Automatic Text Categorization in Terms of Genre and
Author. Computational Linguistics, 26 (4), 471-495.
W. Teahan and D. Harper. 2001. Using Compression-
Based Language Models for Text Categorization. Pro-
ceedings of 2001 Workshop on Language Modeling
and Information Retrieval.
P. Turney. 2002. Thumbs Up or Thumbs Down? Seman-
tic Oritentation Applied to Unsupervised Classification
of Reviews. Proceedings of 40th Annual Conference of
Association for Computational Linguistics (ACL 2002)
Y. Yang. 1999. An Evaluation of Statistical Approaches
to Text Categorization. Information Retrieval, 1(1/2),
pp. 67?88.
Text Classication in Asian Languages without Word Segmentation
Fuchun Peng   Xiangji Huang   Dale Schuurmans   Shaojun Wang  
 
School of Computer Science, University of Waterloo, Ontario, Canada

Department of Computer Science, University of Massachusetts, Amherst, MA, USA

Department of Statistics, University of Toronto, Ontario, Canada

f3peng, jhuang, dale, sjwang  @ai.uwaterloo.ca
Abstract
We present a simple approach for Asian
language text classification without word
segmentation, based on statistical  -gram
language modeling. In particular, we ex-
amine Chinese and Japanese text classi-
fication. With character  -gram models,
our approach avoids word segmentation.
However, unlike traditional ad hoc  -gram
models, the statistical language model-
ing based approach has strong informa-
tion theoretic basis and avoids explicit fea-
ture selection procedure which potentially
loses significantly amount of useful infor-
mation. We systematically study the key
factors in language modeling and their in-
fluence on classification. Experiments on
Chinese TREC and Japanese NTCIR topic
detection show that the simple approach
can achieve better performance compared
to traditional approaches while avoiding
word segmentation, which demonstrates
its superiority in Asian language text clas-
sification.
1 Introduction
Text classification addresses the problem of assign-
ing a given passage of text (or a document) to one or
more predefined classes. This is an important area
of information retrieval research that has been heav-
ily investigated, although most of the research activ-
ity has concentrated on English text (Dumais, 1998;
Yang, 1999). Text classification in Asian languages
such as Chinese and Japanese, however, is also an
important (and relatively more recent) area of re-
search that introduces a number of additional diffi-
culties. One difficulty with Chinese and Japanese
text classification is that, unlike English, Chinese
and Japanese texts do not have explicit whitespace
between words. This means that some form of
word segmentation is normally required before fur-
ther processing. However, word segmentation itself
is a difficult problem in these languages. A second
difficulty is that there is a lack of standard bench-
mark data sets for these languages. Nevertheless,
recently, there has been significant notable progress
on Chinese and Japanese text classification (Aizawa,
2001; He et al, 2001).
Many standard machine learning techniques have
been applied to text categorization problems, such
as naive Bayes classifiers, support vector machines,
linear least squares models, neural networks, and k-
nearest neighbor classifiers (Sebastiani, 2002; Yang,
1999). Unfortunately, most current text classi-
fiers work with word level features. However, word
identification in Asian languages, such as Chinese
and Japanese, is itself a hard problem. To avoid
the word segmentation problems, character level  -
gram models have been proposed (Cavnar and Tren-
kle, 1994; Damashek, 1995). There, they used  -
grams as features for a traditional feature selection
process and then deployed classifiers based on cal-
culating feature-vector similarities. This approach
has many shortcomings. First, there are an enor-
mous number of possible features to consider in text
categorization, and standard feature selection ap-
proaches do not always cope well in such circum-
stances. For example, given a sufficiently large num-
ber of features, the cumulative effect of uncommon
features can still have an important effect on clas-
sification accuracy, even though infrequent features
contribute less information than common features
individually. Therefore, throwing away uncommon
features is usually not an appropriate strategy in this
domain (Aizawa, 2001). Another problem is that
feature selection normally uses indirect tests, such as

	 or mutual information, which involve setting ar-
bitrary thresholds and conducting a heuristic greedy
search to find a good subset of features. Moreover,
by treating text categorization as a classical classifi-
cation problem, standard approaches can ignore the
fact that texts are written in natural language, which
means that they have many implicit regularities that
can be well modeled by specific tools from natural
language processing.
In this paper, we present a simple text categoriza-
tion approach based on statistical  -gram language
modeling to overcome the above shortcomings in a
principled fashion. An advantage we exploit is that
the language modeling approach does not discard
low frequency features during classification, as is
commonly done in traditional classification learning
approaches. Also, the language modeling approach
uses  -gram models to capture more contextual in-
formation than standard bag-of-words approaches,
and employs better smoothing techniques than stan-
dard classification learning. These advantages are
supported by our empirical results on Chinese and
Japanese data.
2 Language Model Text Classifiers
The goal of language modeling is to predict the
probability of natural word sequences; or more sim-
ply, to put high probability on word sequences that
actually occur (and low probability on word se-
quences that never occur). Given a word sequence

	
 to be used as a test corpus, the quality of
a language model can be measured by the empirical
perplexity (or entropy) on this corpus
Perplexity  





Coling 2010: Poster Volume, pages 1318?1326,
Beijing, August 2010
Search with Synonyms: Problems and Solutions
Xing Wei, Fuchun Peng, Huishin Tseng, Yumao Lu, Xuerui Wang, Benoit Dumoulin
Yahoo! Labs	

{xwei,fuchun,huihui,yumaol,xuerui,benoitd}@yahoo-inc.com
Abstract
Search with synonyms is a challenging
problem for Web search, as it can eas-
ily cause intent drifting. In this paper,
we propose a practical solution to this is-
sue, based on co-clicked query analysis,
i.e., analyzing queries leading to clicking
the same documents. Evaluation results
on Web search queries show that syn-
onyms obtained from this approach con-
siderably outperform the thesaurus based
synonyms, such as WordNet, in terms of
keeping search intent.
1 Introduction
Synonym discovery has been an active topic in a
variety of language processing tasks (Baroni and
Bisi, 2004; Fellbaum, 1998; Lin, 1998; Pereira
et al, 1993; Sanchez and Moreno, 2005; Turney,
2001). However, due to the difficulties of syn-
onym judgment (either automatically or manu-
ally) and the uncertainty of applying synonyms
to specific applications, it is still unclear how
synonyms can help Web scale search task. Previ-
ous work in Information Retrieval (IR) has been
focusing mainly on related words (Bai et al,
2005; Wei and Croft, 2006; Riezler et al, 2008).
But Web scale data handling needs to be precise
and thus synonyms are more appropriate than re-
lated words for introducing less noise and alle-
viating the efficiency concern of query expan-
sion. In this paper, we explore both manually-
built thesaurus and automatic synonym discov-
ery, and apply a three-stage evaluation by sep-
arating synonym accuracy from relevance judg-
ment and user experience impact.
The main difficulties of discovering synonyms
for Web search are the following:
1. Synonym discovery is context sensitive.
Although there are quite a few manually built
thesauri available to provide high quality syn-
onyms (Fellbaum, 1998), most of these syn-
onyms have the same or nearly the same mean-
ing only in some senses. If we simply replace
them in search queries in all occurrences, it is
very easy to trigger search intent drifting. Thus,
Web search needs to understand different senses
encountered in different contexts. For example,
?baby? and ?infant? are treated as synonyms in
many thesauri, but ?Santa Baby? has nothing to
do with ?infant?. ?Santa Baby? is a song title,
and the meaning of ?baby? in this entity is dif-
ferent than the usual meaning of ?infant?.
2. Context can not only limit the use of syn-
onyms, but also broaden the traditional definition
of synonyms. For instance, ?dress? and ?attire?
sometimes have nearly the same meaning, even
though they are not associated with the same en-
try in many thesauri; ?free? and ?download? are
far from synonyms in traditional definition, but
?free cd rewriter? may carry the same query in-
tent as ?download cd rewriter?.
3. There are many new synonyms devel-
oped from the Web over time. ?Mp3? and
?mpeg3? were not synonyms twenty years ago;
?snp newspaper? and ?snp online? carry the
same query intent only after snponline.com was
published. Manually editing synonym list is pro-
hibitively expensive. Thus, we need an auto-
matic synonym discovery system that can learn
from huge amount of data and update the dictio-
nary frequently.
1318
In summary, synonym discovery for Web
search is different from traditional thesaurus
mining; it needs to be context sensitive and needs
to be updated timely. To address these prob-
lems, we conduct context based synonym dis-
covery from co-clicked queries, i.e., queries that
share similar document click distribution. To
show the effectiveness of our synonym discov-
ery method on Web search, we use several met-
rics to demonstrate significant improvements:
(1) synonym discovery accuracy that measures
how well it keeps the same search intent; (2)
relevance impact measured by Discounted Cu-
mulative Gain (DCG) (Jarvelin and Kekalainen.,
2002); and (3) user experience impact measured
by online experiment.
The rest of the paper is organized as follows.
In Section 2, we first discuss related work and
differentiate our work from existing work. Then
we present the details of our synonym discov-
ery approach in Section 3. In Section 4 we show
our query rewriting strategy to include synonyms
in Web search. We conduct experiments on ran-
domly sampled Web search queries and run the
three-stage evaluation in Section 5 and analyze
the results in Section 6. WordNet based syn-
onym reformulation and a current commercial
search engine are the baselines for the three-
stage evaluation respectively. Finally we con-
clude the paper in Section 7.
2 Related Works
Automatically discovering synonyms from large
corpora and dictionaries has been popular top-
ics in natural language processing (Sanchez and
Moreno, 2005; Senellart and Blondel, 2003; Tur-
ney, 2001; Blondel and Senellart, 2002; van der
Plas and Tiedemann, 2006), and hence, there has
been a fair amount of work in calculating word
similarity (Porzel and Malaka, 2004; Richardson
et al, 1998; Strube and Ponzetto, 2006; Bolle-
gala et al, 2007) for the purpose of discovering
synonyms, such as information gain on ontology
(Resnik, 1995) and distributional similarity (Lin,
1998; Lin et al, 2003). However, the definition
of synonym is application dependent and most
of the work has been applied to a specific task
(Turney, 2001) or restricted in one domain (Ba-
roni and Bisi, 2004). Synonyms extracted us-
ing these traditional approaches cannot be easily
adopted in Web search where keeping search in-
tent is critical.
Our work is also related to semantic matching
in IR: manual techniques such as using hand-
crafted thesauri and automatic techniques such
as query expansion and clustering all attempts to
provide a solution, with varying degrees of suc-
cess (Jones, 1971; van Rijsbergen, 1979; Deer-
wester et al, 1990; Liu and Croft, 2004; Bai
et al, 2005; Wei and Croft, 2006; Cao et al,
2007). These works focus mainly on adding in
loosely semantically related words to expand lit-
eral term matching. But related words may be
too coarse for Web search considering the mas-
sive data available.
3 Synonym Discovery based on
Co-clicked Queries
In this section, we discuss our approach to syn-
onym discovery based on co-clicked queries in
Web search in detail.
3.1 Co-clicked Query Clustering
Clustering has been extensively studied in many
applications, including query clustering (Wen et
al., 2002). One of the most successful tech-
niques for clustering is based on distributional
clustering (Lin, 1998; Pereira et al, 1993). We
adopt a similar approach to our co-clicked query
clustering. Each query is associated with a set
of clicked documents, which in turn associated
with the number of views and clicks. We then
compute the distance between a pair of queries
by calculating the Jensen-Shannon(JS) diver-
gence (Lin, 1991) between their clicked URL
distributions. We start with that every query
is a separate cluster, and merge clusters greed-
ily. After clusters are generated, pairs of queries
within the same cluster can be considered as
co-clicked/related queries with a similarity score
computed from their JS divergence.
Sim(qk|ql) = DJS(qk||ql) (1)
1319
3.2 Query Pair Alignment
To make sure that words are replacement for
each other in the co-clicked queries, we align
words in the co-clicked query pairs that have
the same length (number of terms), and have
the same terms for all positions except one.
This is a simplification for complicated aligning
processes. Previous work on machine transla-
tion (Brown et al, 1993) can be used when com-
plete alignment is needed for modeling. How-
ever, as we have tremendous amount of co-
clicked query data, our restricted version of
alignment is sufficient to obtain a reasonable
number of synonyms. In addition, this restricted
approach eliminates much noise introduced in
those complicated aligning processes.
3.2.1 Synonym Discovery from Co-clicked
Query Pair
Synonyms discovered from co-clicked queries
have two aspects of word meaning: (1) gen-
eral meaning in language and (2) specific mean-
ing in the query. These two aspects are related.
For example, if two words are more likely to
carry the same meaning in general, then they are
more likely to carry the same meaning in spe-
cific queries; on the other hand, if two words of-
ten carry the same meaning in a variety of spe-
cific queries, then we tend to believe that the two
words are synonyms in general language. How-
ever, neither of these two aspects can cover the
other. Synonyms in general language may not
be used to replace each other in a specific query.
For example, ?sea? and ?ocean? have nearly the
same meaning in language, but in the specific
query ?sea boss boat?, ?sea? and ?ocean? cannot
be treated as synonyms because ?sea boss? is a
brand; also, in the specific query ?women?s wed-
ding attire?, ?dress? can be viewed as a synonym
to ?attire?, but in general language, these two
words are not synonyms. Therefore, whether
two words are synonyms or not for a specific
query is a synthesis judgment based on both of
general meaning and specific context.
We develop a three-step process for synonym
discovery based on co-clicked queries, consider-
ing the above two aspects.
Step 1: Get al synonym candidates for word
wi in general meaning.
In this step, we would like to get al syn-
onym candidates for a word. This step corre-
sponds to Aspect (1) to catch the general mean-
ing of words in language. We consider all the
co-clicked queries with the word and sum over
them, as in Eq. 2
P (wj |wi) =
?
k simk(wi ? wj)?
wj
?
k sim(wi ? wj)
(2)
where simk(wi ? wj) represents the similarity
score (see Section 3.1) of a query qk that aligns
wi to wj . So intuitively, we aggregate scores of
all query pairs that align wi to wj , and normalize
it to a probability over the vocabulary.
Step 2: Get synonyms for word wi in query
qk.
In this step, we would like to get synonyms for
a word in a specific query. We define the prob-
ability of reformulating wi with wj for query qk
as the similarity score shown in Eq. 3.
P (wj |wi, qk) = simk(wi ? wj) (3)
Step 3: Combine the above two steps.
Now we have two sets of estimates for the syn-
onym probability, which is used to reformulate
wi with wj . One set of values are based on gen-
eral language information and another set of val-
ues are based on specific queries. We apply three
combination approaches to integrate the two sets
of values for a final decision of synonym dis-
covery: (1) two independent thresholds for each
probability, (2) linear combination with a coeffi-
cient, and (3) linear combination in log scale as
in Eq. 4, with ? as a mixture coefficient.
Pqk(wj |wi) ? ? log P (wj |wi)
+(1 ? ?) log P (wj |wi, qk) (4)
In experiments we found that there is no sig-
nificant difference with the results from different
combination methods by finely tuned parameter
setting.
3.2.2 Concept based Synonyms
The simple word alignment strategy we used
can only get the synonym mapping from single
1320
term to single term. But there are a lot of phrase-
to-phrase, term-to-phrase, or phrase-to-term syn-
onym mappings in language, such as ?babe in
arms? to ?infant?, and ?nyc? to ?new york city?.
We perform query segmentation on queries to
identify concept units from queries based on
an unsupervised segmentation model (Tan and
Peng, 2008). Each unit is a single word or sev-
eral consecutive words that represent a meaning-
ful concept.
4 Synonym Handling in Web Search
The automatic synonym discovery methods de-
scribed in Section 3 generate synonym pairs for
each query. A simple and straightforward way
to use the synonym pairs would be ?equalizing?
them in search, just like the ?OR? function in
most commercial search engines.
Another method would be to re-train the
whole ranking system using the synonym fea-
ture, but it is expensive and requires a large size
training set. We consider this to be future work.
Besides general equalization in all cases, we
also apply a restriction, specially, on whether or
not to allow synonyms to participate in document
selection. For the consideration of efficiency,
most Web search engines has a document selec-
tion step to pre-select a subset of documents for
full ranking. For the general equalization, the
synonym pair is treated as the same even in the
document selection round; in a conservative vari-
ation, we only use the original word for docu-
ment selection but use the synonyms in the sec-
ond phase finer ranking.
5 Experiments
In this section, we present the experimental re-
sults for our approaches with some in-depth dis-
cussion.
5.1 Evaluation Metrics
We have several metrics to evaluate the synonym
discovery system for Web search queries. They
corresponds to the three stages during the system
development. Each of them measures a different
aspect.
Stage 1: accuracy. Because we are more in-
terested in the application of reformulating Web
search queries, our guideline to the editorial
judgment focuses on the query intent change and
context-based synonyms. For example, ?trans-
porters? and ?movers? are good synonyms in
the context of ?boat? because ?boat transporters?
and ?boat movers? keep the same search intent,
but ?ocean? is not a good synonym to ?sea? in
the query of ?sea boss boats? because ?sea boss?
is a brand name and ?ocean boss? does not re-
fer to the same brand. Results are measured with
accuracy by the number of discovered synonyms
(which reflects coverage).
Stage 2: relevance. To evaluate the effec-
tiveness of our semantic features we use DCG,
a widely-used metric for measuring Web search
relevance.
Stage 3: user experience. In addition to the
search relevance, we also evaluate the practical
user experience after logging all the user search
behaviors during a two-week online experiment.
Web CTR: the Web click through rate (Sher-
man and Deighton, 2001; Lee et al, 2005) is de-
fined as
CTR = number of clicks
total page views
,
where a page view (PV) is one result page that a
search engine returns for a query.
Abandon rate: the percentage of queries that
are abandoned by user neither clicking a result
nor issuing a query refinement.
5.2 Data
A period of Web search query log with clicked
URLs are used to generate co-clicked query set.
After word alignment that extracts the co-clicked
query pairs with same number of units and with
only one different unit, we obtain 12.1M unseg-
mented query pairs and 11.9M segmented query
pairs.
Since we run a three-stage evaluation, there
are three independent evaluation set respectively:
1. accuracy test set. For the evaluation of syn-
onym discovery accuracy, we randomly sampled
42K queries from two weeks of query log, and
1321
evaluate the effectiveness of our synonym dis-
covery model with these queries. To test the syn-
onym discovery model built on the segmented
data, we segment the queries before using them
as evaluation set.
2. relevance test set. To evaluate the relevance
impact by the synonym discovery approach, we
run experiments on another two weeks of query
log and randomly sampled 1000 queries from the
affected queries (queries that have differences in
the top 5 results after synonym handling).
3. user experience test set. The user experi-
ence test is conducted online with a commercial
search engine.
5.3 Results of Synonym Discovery
Accuracy
Here we present the results of WordNet the-
saurus based query synonym discovery, co-
clicked based term-to-term query synonym dis-
covery, and co-click concept based query syn-
onym discovery.
5.3.1 Thesaurus-based Synonym
Replacement
The WordNet thesaurus-based synonym re-
placement is a baseline here. For any word that
has synonyms in the thesaurus, thesaurus-based
synonym replacement will rewrite the word with
synonyms from the thesaurus.
Although thesaurus often provides clean in-
formation, synonym replacement based on the-
saurus does not consider query context and in-
troduces too many errors and noise. Our exper-
iments show that only 46% of the discovered
synonyms are correct synonyms in query. The
accuracy is too low to be used for Web search
queries.
5.3.2 Co-clicked Query-based Context
Synonym Discovery
Here we present the results from our approach
based on co-clicked query data (in this section
the queries are all original queries without seg-
mentation). Figure 1 shows the accuracy of syn-
onyms by the number of discovered synonyms.
By applying different thresholds as cut-off lines
to Eq. 4, we get different numbers of synonyms
from the same test set. As we can see, loosening
the threshold can give us more synonym pairs,
but it could hurt the accuracy.
Figure 1: Accuracy versus number of synonyms
with term based synonym discovery
Figure 1 demonstrates how accuracy changes
with the number of synonyms. Y-axis repre-
sents the percentage of correctly discovered syn-
onyms, and X-axis represents the number of
discovered synonyms, including both of correct
ones and wrong ones. The three different lines
represents three different parameter settings of
mixture weights (? in Eq. 4, which is 0.2, 0.3,
or 0.4 in the figure). The figure shows accuracy
drops by increasing the number of synonyms.
More synonym pairs lead to lower accuracy.
From Figure 1 we can see: Firstly, three
curves with different thresholds almost over-
lap, which means the effectiveness of synonym
discovery is not very sensitive to the mixture
weight. Secondly, accuracy is monotonically de-
creasing as more synonyms are detected. By
getting more synonyms, the accuracy decreases
from 100% to less than 80% (we are not in-
terested in accuracies lower than 80% due to
the high precision requirement of Web search
tasks, so the graph contains only high-accuracy
results). This trend also confirms the effective-
ness of our approach (the accuracy for a random
approach would be a constant).
5.3.3 Concept based Context Synonym
Discovery
We present results from our model based on
segmented co-clicked query data in this section.
1322
Original Query New Query with Synonyms Intent
Examples of thesaurus-based based synonym replacement
basement window wells drainage basement window wells drain
billabong boardshorts sale billabong boardshorts sales event same
bigger stronger faster documentary larger stronger faster documentary
yahoo hayseed
maryland judiciary case search maryland judiciary pillowcase search different
free cell phone number lookup free cell earpiece number lookup
Examples of term-to-term synonym discovery
airlines jobs airlines careers
area code finder area code search same
acai berry acai fruit
acai berry acai juice
ace hardware different
crest toothpaste coupon crest whitestrips coupon
Examples of concept based synonym discovery
ae american eagle outfitters
apartments for rent apartment rentals same
arizona time zone arizona time
cortrust bank credit card cortrust bank mastercard
david beckham beckham different
dodge caliber dodge
Table 1: Examples of query synonym discovery: the first section is thesaurus based, second sec-
tion is co-clicked data based term-to-term synonym discovery, and the last section is concept based
synonym discovery.
The modeling part is the same as the one for
Section 5.3.2, and the only difference is that
the data were segmented. We have shown in
Section 5.3.2 that the mixture weight is not an
crucial factor within a reasonable range, so we
present only the result with one mixture weight
in Figure 2. As in Section 5.3.2, the figure shows
that the accuracy of synonym discovery is sensi-
tive to the threshold. It confirms that our model
is effective and setting threshold to Eq. 4 is a fea-
sible and sound way to discover not only single
term synonyms but also phrase synonyms.
Figure 2: Accuracy versus number of synonyms
with concept based synonym discovery
Table 1 shows some anecdotal examples of
query synonyms with the thesaurus-based syn-
onym replacement, context sensitive synonym
discovery, and concept based context sensitive
synonym discovery. In contrast, the upper part
of each section shows positive examples (query
intents remain the same after synonym replace-
ment) and the lower part shows negative ex-
amples (query intents change after synonym re-
placement).
5.4 Results of Relevance Impact
We run relevance test on 1000 randomly sampled
affected queries. With the automatic synonym
discovery approach we apply our synonym han-
dling method described in Section 4. Results of
DCG improvements by different thresholds and
synonym handling settings are presented in Ta-
ble 2. Thresholds are selected empirically from
the accuracy test in Section 5.3 (we run a small
size relevance test on the accuracy test set and
set the range of thresholds based on that). Note
that in our relevance experiments we use term-
to-term synonym pairs only. For the relevance
impact of concept-based synonym discovery, we
would like to study it in our future work.
1323
From Table 2 we can see that the automatic
synonym discovery approach we presented sig-
nificantly improves search relevance on various
settings, which confirms the effectiveness of our
synonym discovery for Web search queries. We
conjecture that avoiding synonym in document
selection is of help. This is because precision is
more important to Web search than recall for the
huge amount of data available on the Web.
Relevance impact with synonym handling
doc-selection
threshold1 threshold2 participation DCG
0.8 0.02 no +1.7%
0.8 0.02 yes +1.3%
0.8 0.05 no +1.8%
0.8 0.05 yes +1.4%
Table 2: Relevance impact with synonym han-
dling by different parameter settings. ?Thresh-
old1? is the threshold for context-based similar-
ity score?Eq. 3; ?threshold2? is the threshold
for general case similarity score?Eq. 2; ?doc-
selection participation? refers to whether or not
let synonym handling participate in document
selection. All improvements are statistically sig-
nificant by Wilcox significance test.
5.5 Results of User Experience Impact
In addition to the relevance impact, we also eval-
uated the practical user experience impact by
CTR and abandon rate (defined in Section 5.1)
through a two-week online run. Results show
that the synonym discovery method presented in
this paper improves Web CTR by 2%, and de-
creases abandon rate by 11.4%. All changes
are statistically significant, which indicates syn-
onyms are indeed beneficial to user experience.
6 Discussion and Error Analysis
From Table 1, we can see that our approach can
catch not only traditional synonyms, which are
the synonyms that can be found in manually-
built thesaurus, but also context-based syn-
onyms, which may not be treated as synonyms
in a standard dictionary or thesaurus. There are
a variety of synonyms our approach discovered:
1. Synonyms that are not considered as syn-
onyms in traditional thesaurus, such as ?berry?
and ?fruit? in the context of ?acai?. ?acai berry?
and ?acai fruit? refer to the same fruit.
2. Synonyms that have different part-of-
speech features than the corresponding original
words, such as ?finder? and ?search?. Users
searching ?area code finder? and users search-
ing ?area code search? are looking for the same
content. In the context of Web search queries,
part-of-speech is not an important factor as most
queries are not grammatically perfect.
3. Synonyms that show up in recent concepts,
such as ?webmail? and ?email? in the context
of ?cox?. The new concept of ?webmail? or
?email? has not been added to many thesauri yet.
4. Synonyms not limited by length, such as
?crossword puzzles? and ?crossword?, ?homes
for sale? and ?real estate?. The segmenter
helps our system discover synonyms in various
lengths.
With these many variations, the synonyms dis-
covered by our approach are not the ?synonyms?
in the traditional meaning. They are context sen-
sitive, Web data oriented and search effective
synonyms. These synonyms are discovered by
the statistical model we presented and based on
Web search queries and clicked data.
However, the click data themselves contain a
huge amount of noise. Although they can re-
flect the users? intents in some big picture, in
many specific cases synonyms discovered from
co-clicked data are biased by the click noise. In
our application?Web search query reformula-
tion with synonyms, accuracy is the most im-
portant thing and thus we are interested in er-
ror analysis. The errors that our model makes
in synonym discovery are mainly caused by the
following reasons:
(1) There are some concepts well accepted
such as ?cnn? means ?news? and ?amtrak?
means ?train?. And users searching ?news? tend
to click CNN Web site; users searching ?train?
tend to click Amtrak Web site. With our model,
?cnn? and ?news?, ?amtrak? and ?train? are dis-
covered to be synonyms, which may hurt the
search of ?news? or ?train? in general meaning.
1324
(2) Same clicks by different intents. Although
clicking on same documents generally indicates
same search intent, different intents could re-
sult in same or similar clicks, too. For exam-
ple, the queries of ?antique style wedding rings?
and ?antique style engagement rings? carry dif-
ferent intents, but very usually, these two differ-
ent intents lead to the clicks on the same Web
site. ?Booster seats? and ?car seats?, ?brighton
handbags? and ?brighton shoes? are other two
examples in the same case. For these examples,
clicking on Web URLs are not precise enough
to reflect the subtle difference of language con-
cepts.
(3) Bias from dominant user intents. Most
people searching ?apartment? are looking for an
apartment to rent. So ?apartment for rent? and
?apartment? have similar clicked URLs. But
these two are not synonyms in language. In these
cases, popular user intents dominate and bias the
meaning of language, which causes problems.
?Airline baggage restrictions? and ?airline travel
restrictions? is another example.
(4) Antonyms. Many context-based synonym
discovery methods suffer from the antonym
problem, because antonyms can have very simi-
lar contexts. In our model, the problem has been
reduced by integrating clicked-URLs. But still,
there are some examples, such as ?spyware? and
?antispyware?, resulting in similar clicks. To
learn how to ?protect a Web site?, a user often
needs to learn what are the main methods to ?at-
tack a Web site?, and these different-intent pairs
lead to the same clicks because different intents
do not have to mean different interests in many
specific cases.
Although these problems are not common, but
when they happen, they cause a bad user search
experience. We believe a solution to these prob-
lems might need more advanced linguistic anal-
ysis.
7 Conclusions
In this paper, we have developed a synonym dis-
covery approach based on co-clicked query data,
and improved search relevance and user experi-
ence significantly based on the approach.
For future work, we are investigating more
synonym handling methods to further improve
the synonym discovery accuracy, and to handle
the discovered synonyms in more ways than just
the query side.
References
Bai, J., D. Song, P. Bruza, J.Y. Nie, and G. Cao.
2005. Query Expansion using Term Relationships
in Language Models for Information Retrieval. In
Proceedings of the ACM 14th Conference on In-
formation and Knowledge Management.
Baroni, M. and S. Bisi. 2004. Using Cooccurrence
Statistics and the Web to Discover Synonyms in a
Technical Language. In LREC.
Blondel, V. and P. Senellart. 2002. Automatic Ex-
traction of Synonyms in a Dictionary. In Proc. of
the SIAM Workshop on Text Mining.
Bollegala, D., Y. Matsuo, and M. Ishizuka. 2007.
Measuring Semantic Similarity betweenWords us-
ing Web Search Engines. In Proceedings of the
16th international conference on World Wide Web
(WWW).
Brown, P. F., S. A. Della Pietra, V. J. Della Pietra, and
R. L. Mercer. 1993. The Mathematics of Statis-
tical Machine Translation: Parameter Estimation.
Computational Linguistics, 19(2):263.
Cao, G., J.Y. Nie, and J. Bai. 2007. Using Markov
Chains to Exploit Word Relationships in Informa-
tion Retrieval. In Proceedings of the 8th Confer-
ence on Large-Scale Semantic Access to Content.
Deerwester, S., S. T. Dumais, G. W. Furnas, T. K.
Landauer, and R. Harshman. 1990. Indexing by
Latent Semantic Analysis. Journal of the Amer-
ican Society for Information Science, 41(6):391?
407.
Fellbaum, C., editor. 1998. WordNet: An Electronic
Lexical Database. MIT Press, Cambridge, Mass.
Jarvelin, K. and J. Kekalainen. 2002. Cumulated
Gain-Based Evaluation Evaluation of IR Tech-
niques. ACM TOIS, 20:422?446.
Jones, K. S., 1971. Automatic Keyword Classification
for Information Retrieval. London: Butterworths.
Lee, Uichin, Zhenyu Liu, and Junghoo Cho. 2005.
Automatic Identification of User Goals in Web
Search. In In the World-Wide Web Conference
(WWW).
1325
Lin, D., S. Zhao, L. Qin, and M. Zhou. 2003. Iden-
tifying Synonyms among Distributionally Similar
Words. In Proceedings of International Joint Con-
ference on Artificial Intelligence (IJCAI).
Lin, J. 1991. Divergence measures based on the
shannon entropy. IEEE Transactions on Informa-
tion Theory, 37(1):145?151.
Lin, D. 1998. Automatic Retrieval and Clustering of
Similar Words. In Proceedings of COLING/ACL-
98, pages 768?774.
Liu, X. and B. Croft. 2004. Cluster-based Retrieval
using LanguageModels. In Proceedings of SIGIR.
Pereira, F., N. Tishby, and L. Lee. 1993. Distribu-
tional Clustering of English Words. In Proceed-
ings of ACL, pages 183 ? 190.
Porzel, R. and R. Malaka. 2004. A Task-based Ap-
proach for Ontology Evaluation. In ECAI Work-
shop on Ontology Learning and Population.
Resnik, P. 1995. Using Information Content to Eval-
uate Semantic Similarity in a Taxonomy. In Pro-
ceedings of IJCAI-95, pages 448 ? 453.
Richardson, S., W. Dolan, and L. Vanderwende.
1998. MindNet: Acquiring and Structuring Se-
mantic Information from Text. In 36th Annual
meeting of the Association for Computational Lin-
guistics.
Riezler, Stefan, Yi Liu, and Alexander Vasserman.
2008. Translating Queries into Snippets for Im-
proved Query Expansion. In Proceedings of the
22nd International Conference on Computational
Linguistics (COLING?08).
Sanchez, D. and A. Moreno. 2005. Automatic Dis-
covery of Synonyms and Lexicalizations from the
Web. In Proceedings of the 8th Catalan Confer-
ence on Artificial Intelligence.
Senellart, P. and V. D. Blondel. 2003. Automatic
Discovery of Similar Words. In Berry, M., editor,
A Comprehensive Survey of Text Mining. Springer-
Verlag, New York.
Sherman, L. and J. Deighton. 2001. Banner ad-
vertising: Measuring effectiveness and optimiz-
ing placement. Journal of Interactive Marketing,
15(2):60?64.
Strube, M. and S. P. Ponzetto. 2006. WikiRe-
late! Computing Semantic Relatedness Using
Wikipedia. In Proceedings of AAAI.
Tan, B. and F. Peng. 2008. Unsupervised Query Seg-
mentation using Generative Language Models and
Wikipedia. In Proceedings of the 17th Interna-
tional World Wide Web Conference (WWW), pages
347?356.
Turney, P. 2001. Mining the Web for Synonyms:
PMI-IR versus LSA on TOEFL. In Proceedings
of the Twelfth European Conference on Machine
Learning.
van der Plas, Lonneke and Jorg Tiedemann. 2006.
Finding Synonyms using Automatic Word Align-
ment and Measures of Distributional Similarity.
In Proceedings of the COLING/ACL 2006, pages
866?873.
van Rijsbergen, C.J., 1979. Information Retrieval.
London: Butterworths.
Wei, X. and W. B. Croft. 2006. LDA-based Doc-
ument Models for Ad-hoc Retrieval. In Proceed-
ings of SIGIR, pages 178?185.
Wen, J.R., J.Y. Nie, and H.J. Zhang. 2002. Query
Clustering Using User Logs. ACM Transactions
on Information Systems, 20(1):59?81.
1326
