Semantic Role Labeling using Maximum Entropy Model
Joon-Ho Lim, Young-Sook Hwang, So-Young Park, Hae-Chang Rim
Department of Computer Science & Engineering Korea University
5-ka, Anam-dong, SEOUL, 136-701, KOREA
{jhlim, yshwang, ssoya, rim}@nlp.korea.ac.kr
Abstract
In this paper, we propose a semantic role label-
ing method using a maximum entropy model,
which enables not only to exploit rich features
but also to alleviate the data sparseness prob-
lem in a well-founded model. For applying the
maximum entropy model to semantic role la-
beling, we take a incremental approach as fol-
lows: firstly, the semantic roles are assigned to
the arguments in the immediate clause includ-
ing a predicate, and then, the semantic roles are
assigned to the arguments in the upper clauses
by using previously assigned labels. The exper-
imental result shows that the proposed method
has about 64.76% (F1-measure) on the test set.
1 Introduction
The semantic role represents the relationship between a
predicate and an argument. It provides a general semantic
interpretation of the sentence, and it can play a key role
in NLP. The shared task of CoNLL-2004 concerns the
automatic semantic role labeling (Carreras, 2004). The
challenge for this task is to come forward with machine
learning approaches which based on only partial syntactic
information such as words, POS tags, chunks, clauses,
and named entities.
Some machine learning approaches for semantic role
labeling have been previously developed (Gildea, 2002;
Pradhan, 2003; Thompson, 2003). Gildea (2002) pro-
posed a probabilistic discriminative model to assign a
semantic roles to the constituent. However, it needs a
complex interpolation for smoothing because of the data
sparseness problem. Pradhan (2003) applied a support
vector machine to semantic role labeling, but if it use
a polynomial kernel function for the dependencies be-
tween features, it requires high computational complex-
ity. Futhermore, becuase the SVM is a binary classifier,
one-vs-rest or pairwise method is required for multi-class
classification. Thompson (2003) proposed a probabilis-
tic generative model which the constituents is generated
by the semantic roles. In this model, because a con-
stituent depends only on the role that generated it, and
constituents are independent of each other, so this model
can not utilize contextual information or a relational in-
formation between the constituent and the predicate.
In this paper, we propose a semantic role labeling
method using a maximum entropy model. It is moti-
vated by the thought of that for building a successful
model, some knowledge of the task are reflected into
the model based on the machine learning technique. In
this method, we try to combine the structural linguistic
knowledge linking syntax to semantics into the machine
learning technique. It is realized in terms of two aspects:
one is the model framework, the other is the design of
feature sets. First of all, for the model framework, we uti-
lize the syntactic knowledge of representing the semantic
roles in a clause: the arguments of a predicate are located
in the immediate clause or the upper clauses. Secondly,
for the feature sets, we consider the relation between syn-
tactic and semantic characteristics of a given context. For
implementing the method with a machine learning algo-
rithm, we take a maximum entropy model, which enables
not only to exploit rich features but also to alleviate the
data sparseness problem in a well-founded model.
The remaining of the paper is organized as follows:
section 2 describes the proposed semantic role labeling
method using a maximum entropy model. Section 3
presents feature sets for semantic role labeling. Section 4
shows some experimental results of the proposed method.
Finally, section 5 concludes with some directions of fu-
ture works.
2 Semantic Role Labeling using ME
In the maximum entropy framework (Berger, 1996), the
conditional probability of predicting an outcome y given
Figure 1: An example of the semantic role labels and an incremental approach.
a history x is defined as follows :
P (y|x) =
1
Z(x)
exp
(
k
?
i=1
?
i
f
i
(x, y)
)
where f
i
(x, y) is the feature function, ?
i
is the weighting
parameter of f
i
(x, u), k is the number of features, and
Z(x) is the normalization factor for
?
y
p(y|x) = 1.
Given a predicate and its partial parse tree represented
by constituents such as chunks and clauses, the proba-
bilistic model for semantic role labeling assigns the se-
mantic role labels to the constituents as described in the
equation (1).
R
best
= argmax
R
P (R|c
1n
, pred)
= argmax
R
?
n
i=1
P (r
i
|c
1n
, pred, r
1...i?1
) (1)
where R is a sequence of the semantic roles, c
1n
is a se-
quence of constituents, pred is the given predicate, r
i
is
the i-th semantic role, n is the number of constituents.
In order to apply the equation (1) to an incremental
approach, we classify clauses into the immediate clause
and the upper clause. The immediate clause is the clause
which contains the target predicate, and the upper clause
is the clause which includes the immediate clause. Gen-
erally, most of the arguments of the predicate are located
in the immediate clause while some of them are located
in the upper clauses, especially the first or second up-
per clauses. Since it is much easier and more reliable to
identify the arguments in the immediate clause, the pro-
posed method first assigns the semantic role labels to the
constituents 1 in the immediate clause. Then, it assigns
the semantic role labels to the constituents in the upper
clauses by using previously assigned labels. This incre-
mental approach is described in the equation (2) derived
from the equation (1).
R
best
= argmax
R
?
n
i=1
P (r
i
|c
1n
, pred, r
1...i?1
)
? argmax
R
?
m
i=1
P (r
i
|?
1
(c
1n
, pred, r
1...i?1
))
?
?
n
i=m+1
P (r
i
|?
2
(c
1n
, pred, r
1...i?1
)) (2)
1Here, we regard a chunk or a clause as a constituent.
where m is the number of constituents covered by the im-
mediate clause, ?
1
is a feature set for immediate clause,
and ?
2
is a feature set for upper clauses.
A semantic role label(r
i
) is represented by using a BIO
notation such as B-A*, I-A*, etc. However, O is too fre-
quently occurred than other semantic role labels, it can
have a somewhat high probability than others. Therefore,
to degrade its probability, we divide the single O into O-,
O+, O0with respect to the position of a constituent which
is relative to the predicate. Therefore, B-A*, I-A*, O-,
O+, and O0 are used as semantic roles as shown in Fig-
ure 1.
After processing the equation (2), we use some heuris-
tic to attach the some semantic roles and to adjust the
boundary of semantic arguments in the post-processing
step. More specifically, we use some rules to attach the
V, AM-MOD, and AM-NEG, and extend the boundary
of core roles to include to infinitive of the VP chunk like
?expect/B-VP (A1 to/I-VP take/I-VP dive/B-NP)?.
3 Feature Sets for Semantic Role Labeling
For accurate semantic role labeling, we regard that the
following information is important: the contextual infor-
mation of the constituent, the syntactic information of the
predicate, and the relation between the constituent and
the predicate. Therefore, we use the features presented in
Table 1 for semantic role labeling. For example, Figure
previous-label(pl)
predicate-POS(predpos), predicate-lex(predlex)
predicate-type(predtype)
tag(ctag), voice(v), position(p), path(path)
head-lex(hl), head-POS(hp), content-head(chl)
prev-tag(ptag), prev-head-lex(phl)
next-tag(ntag), next-head-lex(nhl)
path-immediate-clause(path-im-cl)
path-begin-end(path-beg-end)
level-of-clause(l-cl), is-clause-boundary(cl-bn)
immediate-clause-roles(im-cl-roles)
Table 1: Features for semantic role labeling.
Figure 2: Some instances extracted from example of Figure 1.
feature set ?
1
for immediate clause feature set ?
2
for upper clause
pl, ctag, ctag+v+p, ctag+v+p+pl pl, ctag, ctag+v+p
ptag+ctag, ctag+ntag ptag+ctag, ctag+ntag
hp+p, hp+p+ntag hp+p, hl+ctag,
predtype+ctag predtype+ctag
predlex+hl, predlex+ctag+v+p, predlex+ctag+pl predlex+hl, predlex+ctag+v+p, predlex+ctag+pl
predpos+p, predpos+hp+pl, predpos+ctag
path, path+hp+v, path+nhl, path+predlex path-im-cl, path-im-cl+ctag+v, path-beg-end
hl+p, hl+ctag, hl+ctag+predlex ctag+l-cl, ptag+ctag+l-cl, ctag+ntag+l-cl
chl+pl, chl+pl+predlex ctag+cl-bn, ptag+ctag+cl-bn, ctag+ntag+cl-bn
chl+phl, chl+phl+predlex im-cl-roles
Table 2: Conjoined Feature Sets
2 shows how the features in Table 1 are used for labeling
semantic roles to the proposition in Figure 1.
Because the maximum entropy model assumes the in-
dependence of features, we should conjoin the coherent
features. As presented in Table 2, we use the conjoined
feature sets to assign semantic roles to the constituents of
the immediate clause and the upper clauses.
The predicate-type feature represents the predicate us-
age such as to-infinitive form (TO), the beginning of the
immediate clause (BEG), and otherwise (SEN). The tag
feature represents the tag of the current constituent. If it
is a clause, it is subdivided into a relative pronoun, a in-
finitival relative clause, etc according to its represented
form.
The path feature indicates the sequence of constituent
tags between the current constituent and the predicate.
The voice feature is determined to be an active or pas-
sive voice of the predicate, and the position feature is as-
signed by the constituent position with respect to pred-
icate. These features implicitly represent the predicate-
argument relation such as predicate-subject or predicate-
object.
For the headword feature, we use the Collins? head-
word rules, and as a complementary feature to the head
word feature, a content word feature2 is used to represent
the content of the PP, VP, or CONJP chunk.
The path-immediate-clause feature is the sequence of
constituent tags between the current constituent and the
immediate clause, and the path-begin-end feature is the
sequence between current constituent and beginning/end
of clause. The level-of-clause feature indicates whether
the current constituent is located in the first upper clause
or in the second upper clause, and the is-clause-boundary
feature is the binary value which indicates the existence
of the starting clause. The immediate-clause-roles fea-
tures are the binary indicators to represent whether the
core arguments exist in the immediate clause or not.
The path-immediate-clause, path-begin-end, level-of-
clause, is-clause-boundary, and immediate-clause-roles
features are used only in the second phase, and the others
except the path feature and the content-head feature are
used in common.
2For example, if the PP-chunk is because of, the headword
feature is of, and the content word feature is because.
Precision Recall F
?=1
Overall 68.42% 61.47% 64.76
A0 79.20% 75.73% 77.42
A1 67.41% 64.65% 66.00
A2 52.65% 45.94% 49.07
A3 52.53% 34.67% 41.77
A4 63.16% 48.00% 54.55
A5 0.00% 0.00% 0.00
AM-ADV 46.75% 35.18% 40.15
AM-CAU 57.69% 30.61% 40.00
AM-DIR 48.28% 28.00% 35.44
AM-DIS 60.11% 50.23% 54.73
AM-EXT 58.33% 50.00% 53.85
AM-LOC 35.56% 35.09% 35.32
AM-MNR 51.26% 23.92% 32.62
AM-MOD 89.77% 91.10% 90.43
AM-NEG 86.15% 88.19% 87.16
AM-PNC 48.98% 28.24% 35.82
AM-PRD 100.00% 33.33% 50.00
AM-TMP 59.51% 42.70% 49.73
R-A0 86.96% 75.47% 80.81
R-A1 57.89% 62.86% 60.27
R-A2 50.00% 33.33% 40.00
R-A3 0.00% 0.00% 0.00
R-AM-LOC 33.33% 25.00% 28.57
R-AM-MNR 0.00% 0.00% 0.00
R-AM-PNC 0.00% 0.00% 0.00
R-AM-TMP 42.86% 21.43% 28.57
V 97.99% 97.99% 97.99
Table 3: Experimental results on the test set.
4 Experiments
To test the proposed method, we have experimented on
CoNLL-2004 datasets. For our experiments, we use the
Zhang le?s MaxEnt toolkit 3, and the L-BFGS parame-
ter estimation algorithm with Gaussian Prior smoothing
(Chen, 1999). The results on the test set are shown in
Table 3, and Table 4 shows the overall results when the
model is tested on the training set, the development set,
and the test set.
From these experimental results, we can find that the
proposed model has relatively high performance on the
labels related to A0 and A1, while it has relatively low
performance on the other labels. This may be caused by
following two reasons. Firstly, the instances of A0 or A1
are provided enough for accurate semantic role labeling.
Secondly, the thematic roles of A0 and A1 are more clear
than other core semantic roles. For example, agent is la-
beled as mainly A0 while benefactive can be labeled as
A2 or A3. Therefore, the maximum entropy model can
3http://www.nlplab.cn/zhangle/maxent toolkit.html
Precision Recall F
?=1
Overall(training) 96.40% 92.28% 94.29
Overall(dev) 69.78% 62.56% 65.97
Overall(test) 68.42% 61.47% 64.76
Table 4: The results when the model is tested on the train-
ing set, the development set, and the test set
get a good generalize performance in case of A0 or A1,
but can?t generalize well in other cases.
5 Conclusion
In this paper, we propose a semantic role labeling method
using a maximum entropy model. Because the maximum
entropy model enables not only to exploit rich features
but also to alleviate the data sparseness problem, we use
it to model the probability of a semantic role label se-
quence. The proposed method has following characteris-
tics: firstly, it assigns the semantic role labels to the con-
stituents in the immediate clause, and then assigns role
labels to the constituents in the upper clauses, and it uti-
lizes the relation between syntactic and semantic charac-
teristics of a given context.
For the future work, we will device a method of clus-
tering for the path and predicate features, and include the
clustering results as additional features.
References
Adam Berger, Stephen Della Pietra, and Vincent Della
Pietra. 1996. A maximum entropy approach to natu-
ral language processing . Computational Linguistics,
22(1):39?71.
Xavier Carreras, and Lluis Marquez. 2004. Introduc-
tion to the CoNLL-2004 Shared Task: Semantic Role
Labeling . Proceedings of CoNLL-2004.
S. Chen and R. Rosenfeld. 1999. A Gaussian prior for
smoothing maximum entropy models . Technical Re-
port CMUCS-99-108, Carnegie Mellon University.
Daniel Gildea, and Daniel Jurafsky. 2002. Automatic La-
beling of Semantic Roles . Computational Linguistics,
28(3):245-288.
Sameer Pradhan, Kadri Hacioglu, Valerie Krugler,
Wayne Ward, James H. Martin and Daniel Jurafsky.
Oct 2003. Support Vector Learning for Semantic Ar-
gument Classification . Technical Report, TR-CSLR-
2003-03.
Cynthia A. Thompson, Roger Levy and Christopher D.
Manning. A Generative Model for Semantic Role La-
beling . ECML 2003, 397-408.
Tree Annotation Tool using Two-phase Parsing
to Reduce Manual Effort for BuildingaTreebank
So-Young Park, Yongjoo Cho, Sunghoon Son,
College of Computer Software & Media Technology
SangMyung University
7 Hongji-dong, Jongno-gu
SEOUL, KOREA
{ssoya,ycho,shson}@smu.ac.kr
Ui-Sung Song and Hae-Chang Rim
Dept. of CSE
Korea University,
5-ka 1, Anam-dong, Seongbuk-ku
SEOUL, KOREA
{ussong,rim}@nlp.korea.ac.kr
Abstract
In this paper, we propose a tree annota-
tion tool using a parser in order to build
a treebank. For the purpose of mini-
mizing manual effort without any mod-
ification of the parser, it performs two-
phase parsing for the intra-structure of
each segment and the inter-structure af-
ter segmenting a sentence. Experimen-
tal results show that it can reduce man-
ual effort about 24.5% as compared
with a tree annotation tool without seg-
mentation because an annotation?s in-
tervention related to cancellation and
reconstruction remarkably decrease al-
though it requires the annotator to seg-
ment some long sentence.
1 Introduction
A treebank is a corpus annotated with syntactic
information, and the structural analysis of each
sentence is represented as a bracketed tree struc-
ture. This kind of corpus has served as an ex-
tremely valuable resource for computational lin-
guistics applications such as machine translation
and question answering (Lee et al, 1997; Choi,
2001), and has also proved useful in theoretical
linguistics research (Marcus et al, 1993).
However, for the purpose of building the tree-
bank, an annotator spends a lot of time and man-
ual effort. Furthermore, it is too difficult to main-
tain the consistency of the treebank based on only
the annotator (Hindle, 1989; Chang et al, 1997).
Therefore, we require a tree annotation tool to re-
duce manual effort by decreasing the frequency
of the human annotators? intervention. Moreover,
the tool can improve the annotating efficiency,
and help maintain the consistency of the treebank
(Kwak et al, 2001; Lim et al, 2004).
In this paper, we propose a tree annotation tool
using a parser in order to reduce manual effort for
building a treebank. Fundamentally, it generates a
candidate syntactic structure by utilizing a parser.
And then, the annotator cancels the incorrect con-
stituents in the candidate syntactic structure, and
reconstructs the correct constituents.
2 Previous Works
Up to data, several approaches have been devel-
oped in order to reduce manual effort for build-
ing a treebank. They can be classified into the
approaches using the heuristics (Hindle, 1989;
Chang et al, 1997) and the approaches using
the rules extracted from an already built treebank
(Kwak et al, 2001; Lim et al, 2004).
The first approaches are used for Penn Tree-
bank (Marcus et al, 1993) and the KAIST lan-
guage resource (Lee et al, 1997; Choi, 2001).
Given a sentence, the approaches try to assign an
unambiguous partial syntactic structure to a seg-
ment of each sentence based on the heuristics.
The heuristics are written by the grammarians so
that they are so reliable (Hindle, 1989; Chang et
al., 1997). However, it is too difficult to modify
the heuristics, and to change the features used for
constructing the heuristics (Lim et al, 2004).
The second approaches are used for SEJONG
treebank (Kim and Kang, 2002). Like the first
238
approaches, they also try to attach the partial syn-
tactic structure to each sentence according to the
rules. The rules are automatically extracted from
an already built treebank. Therefore, the ex-
tracted rules can be updated whenever the anno-
tator wants (Kwak et al, 2001; Lim et al, 2004).
Nevertheless, they place a limit on the manual ef-
fort reduction and the annotating efficiency im-
provement because the extracted rules are less
credible than the heuristics.
In this paper, we propose a tree annotation tool
using a parser for the purpose of shifting the re-
sponsibility of extracting the reliable syntactic
rules to the parser. It is always ready to change the
parser into another parser. However, most parsers
still tend to show low performance on the long
sentences (Li et al, 1990; Doi et al, 1993; Kim et
al., 2000). Besides, one of the reasons to decrease
the parsing performance is that the initial syntac-
tic errors of a word or a phrase propagates to the
whole syntactic structure.
In order to prevent the initial errors from propa-
gating without any modification of the parser, the
proposed tool requires the annotator to segment a
sentence. And then, it performs two-phase pars-
ing for the intra-structure of each segment and
the inter-structure. The parsing methods using
clause-based segmentation have been studied to
improve the parsing performance and the parsing
complexity (Kim et al, 2000; Lyon and Dicker-
son, 1997; Sang and Dejean, 2001). Nevertheless,
the clause-based segmentation can permit a short
sentence to be splitted into shorter segments un-
necessarily although too short segments increase
manual effort to build a treebank.
For the sake of minimizing manual effort, the
proposed tree annotation tool induces the annota-
tor to segment a sentence according to few heuris-
tics verified by experimentally analyzing the al-
ready built treebank. Therefore, the heuristics can
prefer the specific length unit rather than the lin-
guistic units such as phrases and clauses.
3 Tree Annotation Tool
The tree annotation tool is composed of segmen-
tation, tree annotation for intra-structure, and tree
annotation for inter-structure as shown in Figure1.
Figure 1: tree annotation tool
3.1 Sentence Segmentation
The sentence segmentation consists of three steps:
segmentation step, examination step, and cancel-
lation step. In the segmentation step, the annota-
tor segments a long sentence. In the examination
step, the tree annotation tool checks the length
of each segment. In the cancellation step, it in-
duces the annotator to merge the adjacent short
segments by cancelling some brackets. Given a
short sentence, the tree annotation tool skips over
the sentence segmentation.
As shown in the top of Figure 2, the annota-
tor segments a sentence by clicking the button
?)(? between each pair of words. Since the tool
regards segments as too short given the segment
length 9, it provides the cancel buttons. And then,
the annotator merges some segments by clicking
the third button, the fifth button, and the sixth but-
ton of the middle figure. The bottom figure does
not include the fourth button of the middle figure
because a new segment will be longer than the
segment length 9. When every segment is suit-
able, the annotator exclusively clicks the confirm
button ignoring the cancel buttons.
Segmentation Step:
Cancellation Step (1):
Cancellation Step (2):
Figure 2: Sentence Segmentation
239
Generation Step:
Cancellation Step:
Reconstruction Step:
Figure 3: Tree Annotation for Intra-Structure
3.2 Tree Annotation for Intra-Structure
The tree annotation for intra-structure consists of
three steps: generation step, cancellation step,
and reconstruction step. In the generation step,
the parser generates a candidate intra-structure for
each segment of a sentence. And then, the tree
annotation tool shows the annotator the candidate
intra-structure. In the cancellation step, the an-
notator can cancel some incorrect constituents in
the candidate intra-structure. In the reconstruc-
tion step, the annotator reconstructs the correct
constituents to complete a correct intra-structure.
For example, the tree annotation tool shows the
candidate intra-structure of a segment as shown
in the top of Figure 3. Assuming that the
candidate intra-structure includes two incorrect
constituents, the annotator cancels the incorrect
constituents after checking the candidate intra-
structure as represented in the middle figure. And
then, the annotator reconstructs the correct con-
stituent, and the intra-structure is completed as
described in the bottom of Figure 3.
3.3 Tree Annotation for Inter-Structure
The tree annotation for inter-structure also con-
sists of three steps: generation step, cancella-
tion step, and reconstruction step. In the gen-
eration step, the parser generates a candidate
inter-structure based on the given correct intra-
structures. And then, the tree annotation tool
shows an annotator the candidate syntactic struc-
ture which includes both the intra-structures and
the inter-structure. In the cancellation step, an
Generation Step:
Cancellation Step:
Reconstruction Step:
Figure 4: Tree Annotation for Inter-Structure
annotator can cancel incorrect constituents. In
the reconstruction step, the annotator reconstructs
correct constituents to complete a correct syntac-
tic structure.
For example, the tree annotation tool represents
the candidate syntactic structure of a sentence as
illustrated in the top of Figure 4. Assuming that
the candidate syntactic structure includes two in-
correct constituents, the annotator cancels the in-
correct constituents, and reconstructs the correct
constituent. Finally, the intra-structure is com-
pleted as described in the bottom of Figure 3.
4 Experiments
In order to examine how much the proposed tree
annotation tool reduces manual effort for build-
ing a Korean treebank, it is integrated with a
parser (Park et al, 2004), and then it is evaluated
on the test sentences according to the following
criteria. The segment length (Length) indicates
that a segment of a sentence is splitted when it
240
is longer than the given segment length. There-
fore, the annotator can skip the sentence segmen-
tation when a sentence is shorter than the seg-
ment length. The number of segments (#Seg-
ments) indicates the number of segments splitted
by the annotator. The number of cancellations
(#Cancellations) indicates the number of incor-
rect constituents cancelled by the annotator where
the incorrect constituents are generated by the
parser. The number of reconstructions (#Recon-
structions) indicates the number of constituents
reconstructed by the annotator. Assume that the
annotators are so skillful that they do not can-
cel their decision unnecessarily. On the other
hand, the test set includes 3,000 Korean sentences
which never have been used for training the parser
in a Korean treebank, and the sentences are the
part of the treebank converted from the KAIST
language resource (Choi, 2001). In this section,
we analyze the parsing performance and the re-
duction effect of manual effort according to seg-
ment length for the purpose of finding the best
segment length to minimize manual effort.
4.1 Parsing Performance According to
Segment Length
For the purpose of evaluating the parsing per-
formance given the correct segments, we clas-
sify the constituents in the syntactic structures
into the constituents in the intra-structures of seg-
ments and the constituents in the inter-structures.
Besides, we evaluate each classified constituents
based on labeled precision, labeled recall, and dis-
tribution ratio. The labeled precision (LP) indi-
cates the ratio of correct candidate constituents
from candidate constituents generated by the
parser, and the labeled recall (LR) indicates the
ratio of correct candidate constituents from con-
stituents in the treebank (Goodman, 1996). Also,
the distribution ratio (Ratio) indicates the distri-
bution ratio of constituents in the intra-structures
from all of constituents in the original structure.
Table 1 shows that the distribution ratio of the
constituents in the intra-structures increases ac-
cording to the longer segment length while the
distribution ratio of the constituents in the inter-
structures decreases. Given the segment length 1,
the constituents in the inter-structures of a sen-
tence are the same as the constituents of the sen-
Table 1: Parsing Performance
Intra-Structure Inter-Structure
Length LP LR Ratio LP LR Ratio
1 0.00 0.00 0.00 87.62 86.06 100.00
2 100.00 93.42 52.25 74.45 74.08 47.75
3 100.00 97.27 66.61 60.63 58.55 33.39
4 98.93 96.47 74.27 62.11 59.71 25.73
5 97.68 96.05 79.47 65.30 63.65 20.53
6 96.50 95.47 83.24 67.88 66.68 16.76
7 95.45 94.45 86.12 70.84 69.81 13.88
8 94.34 93.10 88.47 74.24 73.23 11.53
9 93.41 92.36 90.40 76.85 76.25 9.60
10 92.65 91.47 92.01 78.99 78.31 7.99
11 91.91 90.65 93.43 81.53 80.72 6.57
12 91.19 89.86 94.59 84.39 83.60 5.41
13 90.54 89.23 95.62 86.92 85.97 4.38
14 89.87 88.61 96.54 88.82 88.19 3.46
15 89.34 87.99 97.30 90.39 89.41 2.70
16 88.98 87.65 97.97 90.50 89.86 2.03
17 88.64 87.27 98.51 91.64 89.61 1.49
18 88.37 86.99 98.98 92.92 90.84 1.02
19 88.15 86.76 99.34 92.97 91.30 0.66
20 87.98 86.57 99.58 92.00 91.62 0.42
21 87.83 86.42 99.76 92.73 92.36 0.24
22 87.76 86.36 99.83 93.60 93.20 0.17
23 87.74 86.36 99.89 94.46 93.81 0.11
24 87.69 86.27 99.94 97.67 94.74 0.06
25 87.65 86.26 99.97 100.00 95.45 0.03
26 87.63 86.24 99.99 100.00 100.00 0.01
27 87.63 86.16 99.99 100.00 100.00 0.01
28 87.62 86.06 100.00 0.00 0.00 0.00
tence because there is no evaluated constituent on
intra-structure of one word. In the same way, the
constituents in the intra-structures of a sentence
are the same as the constituents of the sentence
given the segment length 28 because all test sen-
tences are shorter than 28 words.
As described in Table 1, the labeled preci-
sion and the labeled recall decrease on the intra-
structures according to the longer segment length
because both the parsing complexity and the pars-
ing ambiguity increase more and more. On the
other hand, the labeled precision and the labeled
recall tend to increase on the inter-structures since
the number of constituents in the inter-structures
decrease, and it makes the parsing problem of
the inter-structure easy. It is remarkable that the
labeled precision and the labeled recall get rel-
atively high performance on the inter-structures
given the segment length less than 2 because it
is so easy that the parser generates the syntactic
structure for 3 or 4 words.
4.2 Reduction Effect of Manual Effort
In order to examine the reduction effect of manual
effort according to segment length, we measure
the frequency of the annotator?s intervention on
the proposed tool, and also classify the frequency
into the frequency related to the intra-structure
and the frequency related to the inter-structure.
241
Figure 5: Reduction of Manual Effort
As shown in Figure 5, the total manual effort in-
cludes the number of segments splitted by the an-
notator, the number of constituents cancelled by
the annotator, and the number of constituents re-
constructed by the annotator.
Figure 5 shows that too short segments can ag-
gravate the total manual effort since the short seg-
ments require too excessive segmentation work.
According to longer length, the manual effort on
the intra-structures increase because the number
of the constituents increase and the labeled preci-
sion and the labeled recall of the parser decrease.
On the other hand, the manual effort on the inter-
structures decreases according to longer length on
account of the opposite reasons. As presented in
Figure 5, the total manual effort is reduced best at
the segment length 9. It describes that the annota-
tion?s intervention related to cancellation and re-
construction remarkably decrease although it re-
quires the annotator to segment some sentences.
Also, Figure 5 describes that we hardly expect the
effect of two-phase parsing based on the long seg-
ments while the short segments require too exces-
sive segmentation work.
4.3 Comparison with Other Methods
In this experiment, we compare the manual effort
of the four methods: the manual tree annotation
tool (only human), the tree annotation tool using
the parser (no segmentation), the tree annotation
tool using the parser with the clause-based sen-
tence segmentation (clause-based segmentation),
and the tree annotation tool using the parser with
the length-based sentence segmentation (length-
based segmentation) where the segment length is
9 words.
As shown in Figure 6, the first method (only
Figure 6: Comparison with Other Models
human) does not need any manual effort related
to segmentation and cancellation but it requires
too expensive reconstruction cost. The second
method (no segmentation) requires manual effort
related to cancellation and reconstruction without
segmentation. As compared with the first method,
the rest three methods using the parser can re-
duce manual effort by roughly 50% although the
parser generates some incorrect constituents. Fur-
thermore, the experimental results of the third and
fourth methods shows that the two-phase parsing
methods with sentence segmentation can reduce
manual effort more about 9.4% and 24.5% each
as compared with the second method.
Now, we compare the third method (clause-
based segmentation) and the fourth method
(length-based segmentation) in more detail. As
represented in Figure 6, the third method is some-
what better than the fourth method on manual ef-
fort related to intra-structure. It show that the
parser generates more correct constituents given
the clause-based segments because the intra-
structure of the clause-based segments is more
formalized than the intra-structure of the length-
based segments. However, the third method
is remarkably worse than the fourth method on
manual effort related to inter-structure. It de-
scribes that the third method can split a short sen-
tence into shorter segments unnecessarily since
the third method allows the segment length covers
a wide range. As already described in Figure 5,
too short segments can aggravate the manual ef-
fort. Finally, the experimental results shows that
the length-based segments help the tree annota-
tion tool to reduce manual effort rather than the
clause-based segments.
242
5 Conclusion
In this paper, we propose the tree annotation tool
which performs two-phase parsing for the intra-
structure of each segment and the inter-structure
after segmenting a sentence. The proposed tree
annotation tool has the following characteristics.
First, it can reduce manual effort to build a tree-
bank. Experimental results show that it can im-
prove approximately 60.0% and 24.5% as com-
pared with the manual tree annotation tool and
the tree annotation tool using one phase parsing.
Second, it can prevent the initial syntactic errors
of a word or a phrase from propagating to the
whole syntactic structure without any modifica-
tion of the parser because it takes sentence seg-
mentation. Third, it can shift the responsibility of
extracting the reliable syntactic rules to the parser.
For future works, we will try to develop an auto-
matic segmentation method to minimize manual
effort.
Acknowledgements
This work was supported by Ministry of Educa-
tion and Human Resources Development through
Embedded Software Open Education Resource
Center(ESC) at Sangmyung University.
References
Byung-Gyu Chang, Kong Joo Lee, Gil Chang Kim.
1997. Design and Implementation of Tree Tagging
Workbench to Build a Large Tree Tagged Corpus
of Korean. In Proceedings of Hangul and Korean
Information Processing Conference, 421-429.
Ki-Sun Choi. 2001. ?KAIST Language Resources
ver. 2001.? The Result of Core Software
Project from Ministry of Science and Technology,
http://kibs.kaist.ac.kr. (written in Korean)
Shinchi Doi, Kazunori Muraki, Shinichiro Kamei and
Kiyoshi Yamabana. 1993. Long sentence analysis
by domain-specific pattern grammar. In Proceed-
ings of the 6th Conference on the European Chap-
ter of the Association of Computational Linguistics,
466.
Joshua Goodman. 1996. Parsing Algorithms and Met-
rics. In Proceedings of the Annual Meeting of the
Association for Computational Linguistics, pp.177?
183.
Donald Hindle. 1989. Acquiring disambiguation rules
from text. In Proceedings of the Annual Meeting of
the Association for Computational Linguistics, 118-
125.
Sungdong Kim, Byungtak Zhang and Yungtaek Kim.
2000. Reducing Parsing Complexity by Intra-
Sentence Segmentation based on Maximum En-
tropy Model. In Proceedings of the Joint SIGDAT
Conference on Empirical Methods in Natural Lan-
guage Processing and Very Large Corpora, 164-
171.
Ui-Su Kim, and Beom-Mo Kang. 2002. Principles,
Methods and Some Problems in Compiling a Ko-
rean Treebank. In Proceedings of Hangul and
Korean Information Processing Conference 1997.
155-162.
Yong-Jae Kwak, Young-Sook Hwang, Hoo-Jung
Chung, So-Young Park, Hae-Chang Rim. 2001.
FIDELITY: A Framework for Context-Sensitive
Grammar Development. In Proceedings of Interna-
tional Conference on Computer Processing of Ori-
ental Languages, 305-308.
Kong Joo Lee, Byung-Gyu Chang, Gil Chang Kim.
1997. Bracketing Guidlines for Korean Syntac-
tic Tree Tagged Corpus Version 1. Technical Re-
port CS/TR-976-112, KAIST, Dept. of Computer
Science.
Wei-Chuan Li, Tzusheng Pei, Bing-Huang Lee and
Chuei-Feng Chiou. 1990. Parsing long English sen-
tences with pattern rules. In Proceedings of the
13th International Conference on Computational
Linguistics, 410-412.
Joon-Ho Lim, So-Young Park, Yong-Jae Kwak, and
Hae-Chang Rim. 2004. A Semi-automatic Tree
Annotating Workbench for Building a Korean
Treebank. Lecture Note in Computer Science,
2945:253-257.
Caroline Lyon and Bob Dickerson. 1997. Reducing
the Complexity of Parsing by a Method of Decom-
position. In International Workshop on Parsing
Technology.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. Building a Large Annotated Cor-
pus of English: the Penn Treebank. Computational
Linguistics, 19(2):313-330.
So-Young Park, Yong-Jae Kwak, Joon-Ho Lim, Hae-
Chang Rim, and Soo-Hong Kim. 2004. Partially
Lexicalized Parsing Model Utilizing Rich Features.
In Proceedings of the 8th International Conference
on Spoken Language Processing, 3:2201-2204.
Erik F. Tjong Kim Sang and Herve Dejean. 2001. In-
troduction to the CoNLL-2001 Shared Task: Clause
Identi-fication. In Proceedings of the Conference
on Natural Language Learning, 53-57.
243
