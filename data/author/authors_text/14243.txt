Event Detection and Summarization in Weblogs with Temporal Collocations 
Chun-Yuan Teng and Hsin-Hsi Chen 
Department of Computer Science and Information Engineering 
National Taiwan University 
Taipei, Taiwan 
{r93019, hhchen}@csie.ntu.edu.tw 
Abstract 
 
This paper deals with the relationship between weblog content and time. With the proposed temporal mutual information, we analyze 
the collocations in time dimension, and the interesting collocations related to special events. The temporal mutual information is 
employed to observe the strength of term-to-term associations over time. An event detection algorithm identifies the collocations that 
may cause an event in a specific timestamp. An event summarization algorithm retrieves a set of collocations which describe an event. 
We compare our approach with the approach without considering the time interval. The experimental results demonstrate that the 
temporal collocations capture the real world semantics and real world events over time. 
 
1. 
2. 
Introduction 
Compared with traditional media such as online news 
and enterprise websites, weblogs have several unique 
characteristics, e.g., containing abundant life experiences 
and public opinions toward different topics, highly 
sensitive to the events occurring in the real world, and 
associated with the personal information of bloggers. 
Some works have been proposed to leverage these 
characteristics, e.g., the study of the relationship between 
the content and bloggers? profiles (Adamic & Glance, 
2005; Burger & Henderson, 2006; Teng & Chen, 2006), 
and content and real events (Glance, Hurst & Tornkiyo, 
2004; Kim, 2005; Thelwall, 2006; Thompson, 2003). 
In this paper, we will use temporal collocation to 
model the term-to-term association over time.  In the past, 
some useful collocation models (Manning & Sch?tze, 
1999) have been proposed such as mean and variance, 
hypothesis test, mutual information, etc. Some works 
analyze the weblogs from the aspect of time like the 
dynamics of weblogs in time and location (Mei, et al, 
2006), the weblog posting behavior (Doran, Griffith & 
Henderson, 2006; Hurst, 2006), the topic extraction (Oka, 
Abe & Kato, 2006), etc. The impacts of events on social 
media are also discussed, e.g., the change of weblogs after 
London attack (Thelwall, 2006), the relationship between 
the warblog and weblogs (Kim, 2005; Thompson, 2003), 
etc. 
This paper is organized as follows. Section 2 defines 
temporal collocation to model the strength of term-to-term 
associations over time.  Section 3 introduces an event 
detection algorithm to detect the events in weblogs, and 
an event summarization algorithm to extract the 
description of an event in a specific time with temporal 
collocations. Section 4 shows and discusses the 
experimental results.  Section 5 concludes the remarks. 
Temporal Collocations 
We derive the temporal collocations from Shannon?s 
mutual information (Manning & Sch?tze, 1999) which is 
defined as follows (Definition 1). 
Definition 1 (Mutual Information) The mutual 
information of two terms x and y is defined as: 
)()(
),(log),(),(
yPxP
yxPyxPyxI =  
where P(x,y) is the co-occurrence probability of x and y, 
and P(x) and P(y) denote the occurrence probability of x 
and y, respectively. 
Following the definition of mutual information, we 
derive the temporal mutual information modeling the 
term-to-term association over time, and the definition is 
given as follows.  
 Definition 2 (Temporal Mutual Information) Given 
a timestamp t and a pair of terms x and y, the temporal 
mutual information of x and y in t is defined as: 
)|()|(
)|,(log)|,()|,(
tyPtxP
tyxPtyxPtyxI =
where P(x,y|t) is the probability of co-occurrence of terms 
x and y in timestamp t, P(x|t) and P(y|t) denote the 
probability of occurrences of x and y in timestamp t, 
respectively. 
To measure the change of mutual information in time 
dimension, we define the change of temporal mutual 
information as follows. 
Definition 3 (Change of Temporal Mutual 
Information) Given time interval [t1, t2], the change of 
temporal mutual information is defined as: 
12
12
21
)|,()|,(),,,(
tt
tyxItyxIttyxC ?
?=  
where C(x,y,t1,t2) is the change of temporal mutual 
information of terms x and y in time interval [t1, t2], I(x,y| 
t1) and I(x,y| t2) are the temporal mutual information in 
time t1 and t2, respectively. 
3. Event Detection 
Event detection aims to identify the collocations 
resulting in events and then retrieve the description of 
events. Figure 1 sketches an example of event detection. 
The weblog is parsed into a set of collocations. All 
collocations are processed and monitored to identify the 
plausible events.  Here, a regular event ?Mother?s day? 
and an irregular event ?Typhoon Chanchu? are detected.  
The event ?Typhoon Chanchu? is described by the words  
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1: An Example of Event Detection
?Typhoon?, ?Chanchu?, ?2k?, ?Eye?, ?Path? and 
?chinaphillippine?.  
The architecture of an event detection system includes 
a preprocessing phase for parsing the weblogs and 
retrieving the collocations; an event detection phase 
detecting the unusual peak of the change of temporal 
mutual information and identifying the set of collocations 
which may result in an event in a specific time duration; 
and an event summarization phase extracting the 
collocations related to the seed collocations found in a 
specific time duration. 
The most important part in the preprocessing phase is 
collocation extraction. We retrieve the collocations from 
the sentences in blog posts. The candidates are two terms 
within a window size. Due to the size of candidates, we 
have to identify the set of tracking terms for further 
analysis. In this paper, those candidates containing 
stopwords or with low change of temporal mutual 
information are removed. 
In the event detection phase, we detect events by 
using the peak of temporal mutual information in time 
dimension.  However, the regular pattern of temporal 
mutual information may cause problems to our detection. 
Therefore, we remove the regular pattern by seasonal 
index, and then detect the plausible events by measuring 
the unusual peak of temporal mutual information. 
If a topic is suddenly discussed, the relationship 
between the related terms will become higher. Two 
alternatives including change of temporal mutual 
information and relative change of temporal mutual 
information are employed to detect unusual events. Given 
timestamps t1 and t2 with temporal mutual information 
MI1 and MI2, the change of temporal mutual information 
is calculated by (MI2-MI1). The relative change of 
temporal mutual information is calculated by (MI2-
MI1)/MI1. 
For each plausible event, there is a seed collocation, 
e.g., ?Typhoon Chanchu?. In the event description 
retrieval phase, we try to select the collocations with the 
highest mutual information with the word w in a seed 
collocation. They will form a collocation network for the 
event.  Initially, the seed collocation is placed into the 
network.  When a new collocation is added, we compute 
the mutual information of the multiword collocations by 
the following formula, where n is the number of 
collocations in the network up to now. 
?= n iMInInformatioMutualMultiwo  
If the multiword mutual information is lower than a 
threshold, the algorithm stops and returns the words in the 
collocation network as a description of the event.  Figure 
2 sketches an example.  The collocations ?Chanchu?s 
path?, ?Typhoon eye?, and ?Chanchu affects? are added 
into the network in sequence based on their MI. 
We have two alternatives to add the collocations to 
the event description. The first method adds the 
collocations which have the highest mutual information 
as discussed above. In contrast, the second method adds 
the collocations which have the highest product of mutual 
information and change of temporal mutual information. 
 
 
 
 
 
 
Figure 2: An Example of Collocation network 
4. 
4.1. 
Experiments and Discussions 
Temporal Mutual Information versus 
Mutual Information 
In the experiments, we adopt the ICWSM weblog data 
set (Teng & Chen, 2007; ICWSM, 2007). This data set 
collected from May 1, 2006 through May 20, 2006 is 
about 20 GB. Without loss of generality, we use the 
English weblog of 2,734,518 articles for analysis. 
To evaluate the effectiveness of time information, we 
made the experiments based on mutual information 
(Definition 1) and temporal mutual information 
(Definition 2). The former called the incremental 
approach measures the mutual information at each time 
point based on all available temporal information at that 
time. The latter called the interval-based approach 
considers the temporal mutual information in different 
time stamps.  Figures 3 and 4 show the comparisons 
between interval-based approach and incremental 
approach, respectively, in the event of Da Vinci Code.   
We find that ?Tom Hanks? has higher change of 
temporal mutual information compared to ?Da Vinci 
Code?. Compared to the incremental approach in Figure 4, 
the interval-based approach can reflect the exact release 
date of ?Da Vinci Code.? 
 rd
=i 1 4.2. Evaluation of Event Detection 
We consider the events of May 2006 listed in 
wikipedia1 as gold standard. On the one hand, the events 
posted in wikipedia are not always complete, so that we 
adopt recall rate as our evaluation metric.  On the other 
hand, the events specified in wikipedia are not always 
discussed in weblogs.  Thus, we search the contents of 
blog post to verify if the events were touched on in our 
blog corpus. Before evaluation, we remove the events 
listed in wikipedia, but not referenced in the weblogs. 
 
 
 
 
 
 
 
 
 
 
 
Figure 3: Interval-based Approach in Da Vinci Code  
 
 
 
 
 
 
 
 
Figure 4: Incremental Approach in Da Vinci Code 
gure 5 sketches the idea of evaluation.  The left side 
of t s figure shows the collocations detected by our event 
dete tion system, and the right side shows the events 
liste  in wikipedia.  After matching these two lists, we 
can find that the first three listed events were correctly 
identified by our system.  Only the event ?Nepal Civil 
War? was listed, but not found. Thus, the recall rate is 
75% in this case. 
 
 
 
 
 
 
 
Figure 5: Evaluation of Event Detection Phase 
As discussed in Section 3, we adopt change of 
temporal mutual information, and relative change of 
temporal mutual information to detect the peak. In Figure 
6, we compare the two methods to detect the events in 
weblogs. The relative change of temporal mutual 
information achieves better performance than the change 
of temporal mutual information. 
                                                     
1 http://en.wikipedia.org/wiki/May_2006 
Table 1 and Table 2 list the top 20 collocations based 
on these two approaches, respectively. The results of the 
first approach show that some collocations are related to 
the feelings such as ?fell left? and time such as ?Saturday 
night?. In contrast, the results of the second approach 
show more interesting collocations related to the news 
events at that time, such as terrorists ?zacarias 
moussaoui? and ?paramod mahajan.? These two persons 
were killed in May 3. Besides, ?Geena Davis? got the 
golden award in May 3. That explains why the 
collocations detected by relative change of temporal 
mutual information are better than those detected by 
change of temporal mutual information. 
-20
-15
-10
-5
0
5
10
1 3 5 7 9 11 13 15 17 19
Time (day)
M
ut
ua
l i
nf
or
m
at
io
n
Da-Vinci Tom Hanks
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 6: Performance of Event Detection Phase 
-15
-10
-5
0
5
10
1 3 5 7 9 11 13 15 17 19
Time (day)
M
ut
ua
l i
nf
or
m
at
io
n
Da-Vinci Tom Hanks
Collocations CMI Collocations CMI 
May 03 9276.08 Current music 1842.67
Illegal immigrants 5833.17 Hate studying 1722.32
Feel left 5411.57 Stephen Colbert 1709.59
Saturday night 4155.29 Thursday night 1678.78
Past weekend 2405.32 Can?t believe 1533.33
White house 2208.89 Feel asleep 1428.18
Red sox 2208.43 Ice cream 1373.23
Album tool 2120.30 Oh god 1369.52
Sunday morning 2006.78 Illegalimmigration 1368.12
16.56
f 
CMI
32.50
31.63
29.09
28.45
28.34
28.13Sunday night 1992.37 Pretty cool 13
Table 1: Top 20 collocations with highest change o
temporal mutual information 
Collocations CMI Collocations 
casinos online 618.36 Diet sodas 
zacarias moussaoui 154.68 Ving rhames 
Tsunami warning 107.93 Stock picks 
Conspirator zacarias 71.62 Happy hump 
Artist formerly 57.04 Wong kan 
Federal  
Jury 
41.78 Sixapartcom 
movabletype Wed 3 39.20 Aaron echolls 27.48
Pramod mahajan 35.41 Phnom penh 25.78
BBC  
Version 
35.21 Livejournal 
sixapartcom 
23.83  Fi
hi
c
dGeena davis 33.64 George yeo 20.34
Table 2: Top 20 collocations with highest relative change 
of mutual information 
4.3. Evaluation of Event Summarization 
As discussed in Section 3, we have two methods to 
include collocations to the event description. Method 1 
employs the highest mutual information, and Method 2 
utilizes the highest product of mutual information and 
change of temporal mutual information. Figure 7 shows 
the performance of Method 1 and Method 2. We can see 
that the performance of Method 2 is better than that of 
Method 1 in most cases. 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 7: Overall Performance of Event Summarization 
The results of event summarization by Method 2 are 
shown in Figure 8. Typhoon Chanchu appeared in the 
Pacific Ocean on May 10, 2006, passed through 
Philippine and China and resulted in disasters in these 
areas on May 13 and 18, 2006.  The appearance of the 
typhoon Chanchu cannot be found from the events listed 
in wikipedia on May 10.  However, we can identify the 
appearance of typhoon Chanchu from the description of 
the typhoon appearance such as ?typhoon named? and 
?Typhoon eye.  In addition, the typhoon Chanchu?s path 
can also be inferred from the retrieved collocations such 
as ?Philippine China? and ?near China?. The response of 
bloggers such as ?unexpected typhoon? and ?8 typhoons? 
is also extracted.   
 
 
 
 
 
 
 
 
 
 
Figure 8: Event Summarization for Typhoon Chanchu 
5. Concluding Remarks 
This paper introduces temporal mutual information to 
capture term-term association over time in weblogs. The 
extracted collocation with unusual peak which is in terms 
of relative change of temporal mutual information is 
selected to represent an event.  We collect those 
collocations with the highest product of mutual 
information and change of temporal mutual information 
to summarize the specific event.  The experiments on 
ICWSM weblog data set and evaluation with wikipedia 
event lists at the same period as weblogs demonstrate the 
feasibility of the proposed temporal collocation model 
and event detection algorithms. 
Currently, we do not consider user groups and 
locations. This methodology will be extended to model 
the collocations over time and location, and the 
relationship between the user-preferred usage of 
collocations and the profile of users. 
Acknowledgments 
Research of this paper was partially supported by 
National Science Council, Taiwan (NSC96-2628-E-002-
240-MY3) and Excellent Research Projects of National 
Taiwan University (96R0062-AE00-02). 
References 
Adamic, L.A., Glance, N. (2005). The Political 
Blogosphere and the 2004 U.S. Election: Divided 
They Blog. In: Proceedings of the 3rd International 
Workshop on Link Discovery, pp. 36--43. 
Burger, J.D., Henderson J.C. (2006). An Exploration of 
Observable Features Related to Blogger Age. In: 
Proceedings of AAAI 2006 Spring Symposium on 
Computational Approaches to Analysing Weblogs, pp. 
15--20. 
Doran, C., Griffith, J., Henderson, J. (2006). Highlights 
from 12 Months of Blogs. In: Proceedings of AAAI 
2006 Spring Symposium on Computational 
Approaches to Analysing Weblogs, pp. 30--33. 
Glance, N., Hurst, M., Tornkiyo, T. (2004). Blogpulse: 
Automated Trend Discovery for Weblogs. In: 
Proceedings of WWW 2004 Workshop on the 
Weblogging Ecosystem: Aggregation, Analysis, and 
Dynamics. 
Hurst, M. (2006). 24 Hours in the Blogosphere. In: 
Proceedings of AAAI 2006 Spring Symposium on 
Computational Approaches to Analysing Weblogs, pp. 
73--77. 
ICWSM (2007). http://www.icwsm.org/data.html 
Kim, J.H. (2005). Blog as an Oppositional Medium? A 
Semantic Network Analysis on the Iraq War Blogs. In: 
Internet Research 6.0: Internet Generations. 
 
Manning, C.D., Sch?tze, H. (1999). Foundations of 
Statistical Natural Language Processing, The MIT 
Press, London England. 
Mei, Q., Liu, C., Su, H., Zhai, C. (2006). A Probabilistic 
Approach to Spatiotemporal Theme Pattern Mining on 
Weblogs. In: Proceedings of the 15th International 
Conference on World Wide Web, Edinburgh, Scotland, 
pp. 533--542. 
Oka, M., Abe, H., Kato, K. (2006). Extracting Topics 
from Weblogs Through Frequency Segments. In: 
Proceedings of WWW 2006 Annual Workshop on the 
Weblogging Ecosystem: Aggregation, Analysis, and 
Dynamics. 
Teng, C.Y., Chen, H.H. (2006). Detection of Bloggers? 
Interest: Using Textual, Temporal, and Interactive 
Features. In: Proceeding of IEEE/WIC/ACM 
International Conference on Web Intelligence, pp. 
366--369. 
Teng, C.Y., Chen, H.H. (2007). Analyzing Temporal 
Collocations in Weblogs. In: Proceeding of 
International Conference on Weblogs and Social 
Media, 303--304. 
Thelwall, M. (2006). Blogs During the London Attacks: 
Top Information Sources and Topics. In: Proceedings 
of 3rd Annual Workshop on the Weblogging 
Ecosystem: Aggregation, Analysis and Dynamics. 
Thompson, G. (2003). Weblogs, Warblogs, the Public 
Sphere, and Bubbles. Transformations, 7(2). 
J 
In: Proceedings of CoNLL-2000 and LLL-2000, pages 199-208, Lisbon, Portugal, 2000. 
Inductive Logic Programming for 
Corpus-Based Acquisition of Semantic Lexicons 
Pasca le  S4bi l lot  
IRISA - Campus de Beaulieu - 35042 Rennes cedex - France 
sebillot@irisa, fr 
P ier re t te  Bou i l lon  
TIM/ ISSCO - ETI - Universit4 de Gen~ve - 40 Bvd du Pont-d'Arve - 
CH-1205 Geneva-  Switzerland 
Pierrette. Bouillon@issco. unige, ch 
C4ci le Fabre  
ERSS - Universit@ de Toulouse II - 5 all@es A. Machado - 31058 Toulouse cedex - France 
cfabre@univ-tlse2, fr 
Abst ract  
In this paper, we propose an Inductive Logic 
Programming learning method which aims at 
automatically extracting special Noun-Verb (N- 
V) pairs from a corpus in order to build up 
semantic lexicons based on Pustejovsky's Gen- 
erative Lexicon (GL) principles (Pustejovsky, 
1995). In one of the components of this lex- 
ical model, called the qualia structure, words 
are described in terms of semantic roles. For 
example, the relic role indicates the purpose or 
function of an item (cut for knife), the agen- 
tive role its creation mode (build for house), 
etc. The qualia structure of a noun is mainly 
made up of verbal associations, encoding rela- 
tional information. The Inductive Logic Pro- 
gramming learning method that we have devel- 
oped enables us to automatically extract from 
a corpus N-V pairs whose elements axe linked 
by one of the semantic relations defined in the 
qualia structure in GL, and to distinguish them, 
in terms of surrounding categorial context from 
N-V pairs also present in sentences ofthe corpus 
but not relevant. This method has been theoret- 
ically and empirically validated, on a technical 
corpus. The N-V pairs that have been extracted 
will further be used in information retrieval ap- 
plications for index expansion 1. 
1This works is funded by the Agence universi- 
taire de la Francophonie (AUF) (Action de recherche 
partag4e "Acquisition automatique d'dldments du Lex- 
Keywords:  Lexicon learning, Generative 
Lexicon, Inductive Logic Programming, Infor- 
mation indexing. 
1 In t roduct ion  
Information retrieval (IR) systems aim at pro- 
viding a user who asks a query to a database of 
documents with the most relevant exts. The 
quality of these systems is usually measured 
with the help of two criteria: the recall rate, 
which corresponds to the proportion of relevant 
answers that have been given by the system 
compared to the total number of relevant an- 
swers in the database, and the precision rate, 
which denotes the proportion of relevant an- 
swers that are present among the given answers. 
In these IR systems, texts and queries are 
usually represented by indexes, that is, a col- 
lection of some of the words that they contain. 
The quality of the systems therefore highly de- 
pends on the type of indexing language that has 
been chosen. Two kinds of indexes exist: sim- 
ple indexes, which correspond to simple nouns 
(N), verbs (V) and/or adjectives (A) that oc- 
cur in a text or a query 2, and complex indexes, 
which correspond to the compounds (for exam- 
ple, NN compounds) present in the document or 
ique Gdndratif pour amdliorer les performances de 
syst~mes de recherche d'information", r@seau FRAN-  
CIL). 
2All the simple N, V and/or A can be kept as indexes, 
or the most frequent ones for a given text, or those whose 
frequencies in this text are especially high compared to 
their frequencies in the database, etc. 
199 
the question. The solutions that are given for 
a user query are the texts whose indexes better 
match the query index. 
In order to obtain the hightest performances, 
IR systems usually offer some possibilities to 
expand both query and text indexes. Tra- 
ditional index expansion concerns morpho- 
syntactic similarities; for example, the same in- 
dex words in plural and singular forms can be 
matched. Some other systems deal with a kind 
of semantic similarities: if they possess a lin- 
guistic knowledge database, they can, for ex- 
ample, expand a nominal index by following 
synonymy or hyperonymy links. These systems 
are however usually limited to intra-categorial 
expansion, especially N-to-N one. Here we 
deal with a new kind of expansion that has 
been proven particularly useful (Grefenstette, 
1997; Fabre and S~billot, 1999) for document 
database questioning. It concerns N-V links 
and aims at allowing matching between ominal 
and verbal formulations that are semantically 
close. For example, our objective is to permit a 
matching between a query index disk store and 
the text formulation to sell disks, related by the 
typical function of a store. 
N-V index expansion however has to be con- 
trolled in order to ensure that the same con- 
cept is involved in the two formulations. We 
have chosen Pustejovsky's Generative Lexicon 
(GL) framework (Pustejovsky, 1995; Bouillon 
and Busa, 2000) to define what a relevant N- 
V link is, that is, what is a N-V pair in which 
the N and the V are related by a semantic link 
which is close, and which can therefore be used 
to expand indexes. 
In GL formalism, lexical entries consist in 
structured sets of predicates that define a word. 
In one of the components of this lexical model, 
called the qualia structure, words are described 
in terms of semantic roles. The telic role in- 
dicates the purpose or function of an item (for 
example, cut for knife), the agentive role its cre- 
ation mode (build for house), the constitutive 
role its constitutive parts (handle for handcup) 
and the formal role its semantic ategory (con- 
tain (information) for book). The qualia struc- 
ture of a noun is mainly made up of verbal as- 
sociations, encoding relational information. We 
assert hat these N-V links are especially rele- 
vant for index expansion in IR systems (Fabre 
and S~billot, 1999), and what we call a relevant 
N-V pair afterwards in the paper is a pair com- 
posed of a N and a V which are related by one of 
the four semantic relations defined in the qualia 
structure in GL. 
GL is however currently just a formalism; no 
generative l xicons exist that are precise nough 
for every domain and every application (for eg. 
IR), and the cost of a manual construction of 
a lexicon based on GL principles is prohibitive. 
Moreover the real N-V links that are the key- 
point of this formalism cannot be defined a pri- 
ori and have to be acquired from corpora of 
the studied domain. The aim of this paper is 
therefore to present a machine learning method, 
developed in the Inductive Logic Programming 
framework, that enables us to automatically ex- 
tract from a corpus N-V pairs whose elements 
are linked by one of the semantic relations de- 
fined in the qualia structure in GL, and to dis- 
tinguish them, in terms of surrounding cate- 
gorial (Part-of-Speech, POS) context from N- 
V pairs also present in sentences of the corpus 
but not relevant. It will be divided in three 
parts. Section 2 focusses on the motivation of 
this project regarding the use of GL. Section 3 
explains the machine learning method that we 
have developed. Section 4 is dedicated to its 
theoretical nd empirical validations, and to the 
results of its application to a technical corpus. 
2 Mot ivat ion  
As stated in the introduction, our work makes 
two strong claims: firstly N-V associations de- 
fined in GL are relevant for IR and secondly 
this information can be acquired from a corpus 
on the basis of surrounding POS context. These 
presuppositions have to be motivated before ex- 
plaining the learning method: 
1. The aim of GL is to define underspec- 
ified lexical representations that will acquire 
their specifications in context. For example, the 
qualia structure of book indicates that its de- 
fault function is read and that it is created by 
the act of writing. But this information has to 
be enriched in context in order to characterize 
how words are used in specific domains. For 
example, the qualia structure of book will also 
have to indicate that the book can be shelved or 
indexed if this information is necessary to inter- 
pret texts from information science domain. GL 
200 
is therefore a theory of words in context. It can 
also be seen as a way to structure information 
in corpora and, in that sense, the relations it 
defines are therefore privileged information for 
IR. In this perspective, GL has been preferred 
to existing lexical resources uch as WordNet 
(Fellbaum, 1998) for two main reasons: lexical 
relations that we want to exhibit - namely N-V 
links - are unavailable in WordNet, which fo- 
cuses on paradigmatic lexical relations; Word- 
Net is a domain-independent, static resource, 
which can not be used as such to describe lexi- 
cal associations in specific texts, considering the 
great variability of semantic associations from 
one domain to another. 
2. In GL, the qualia structures are not arbi- 
trary repository of information. They contain 
the information ecessary to explain the syn- 
tactic behaviour of the item. We would there- 
fore expect that there are strong connections 
between some specific syntactic phenomena and 
some specific qualia relations. For example, the 
middle construction seems to be only possible if 
a telic relation holds between the N and V (Bas- 
sac and Bouillon, 2000) (for example: ??this 
book writes well vs this book reads well). Sim- 
ilarly, imperative constructions (e.g. open the 
door, follow the links) or adjectival sentences (a 
book difficult to write/read) may also indicate 
a qualia relation. These are some of the con- 
structions that we want to identify primilarly 
in corpora by the learning method. 
3 The  mach ine  learn ing  method 
Trying to infer lexical semantic information 
from corpora is not new: lots of works have 
already been conducted on this subject, espe- 
cially in the statistical learning domain (see 
(Grefenstette, 1994b), for e.g., or (Habert et 
al., 1997) and (Pichon and S~billot, 1997) for 
surveys of this field). Following Harris's frame- 
work (Harris et al, 1989), such research tries to 
extract both syntagmatic and paradigmatic n- 
formation, respectively studying the words that 
appear in the same window-based or syntactic 
contexts as a considered lexical unit (first or- 
der word affinities (Grefenstette, 1994a)), or the 
words that generate the same contexts as the 
key word (second order word affinities). For ex- 
ample, (Briscoe and Carroll, 1997) and (Faure 
and N~dellec, 1999) try to automatically learn 
verbal argument structures and selectional re- 
strictions; (Agarwal, 1995) and (Bouaud et al, 
1997) build semantic classes; (Hearst, 1992) 
and (Morin, 1997) focus on particular lexi- 
cal relations, like hyperonymy. Some of these 
works are concerned with automatically ob- 
taining more complete lexical semantic repre- 
sentations ((Grefenstette, 1994b; Pichon and 
S~billot, 1999). Among these studies, (Puste- 
jovsky et al, 1993) presents a research whose 
aim is to acquire GL nominal qualia structures 
from a corpus; this work is however quite dif- 
ferent from ours because it supposes that the 
qualia structure contents are initialized and are 
only refined with the help of the corpus by using 
the type coercion 3 mechanism. 
In order to automatically acquire N-V pairs 
whose elements are linked by one of the seman- 
tic relations defined in the qualia structure in 
GL, we have decided to use a machine learning 
method. This section is devoted to the expla- 
nation of this choice and to the description of 
the method that we have developed. 
Machine learning aims at automatically 
building programs from examples that are 
known to be positive or negative examples of 
their runnings. According to Mitchell (Mitchell, 
1997), "a computer program is said to learn 
from experience E with respect to some class 
of tasks T and performance measure P, if  its 
performance at tasks in T, as measured by P, 
improve with experience E". 
Among different machine learning techniques, 
we have chosen the Inductive Logic Program- 
ming framework (ILP) (Muggleton and De- 
Raedt, 1994) to learn from a textual corpus N-V 
pairs that are related in terms of one of the re- 
lations defined in the qualia structure in GL. 
Programs that are infered from a set of facts 
and a background knowledge are here logic pro- 
grams, that is, sets of Horn clauses. In the ILP 
framework, the main idea is to obtain a set of 
generalized clauses that is sufficiently generic 
to cover the majority of the positive examples 
(E+), and sufficiently specific to rightly corre- 
spond to the concept we want to learn and to 
cover no (or a few - some noise can be allowed) 
negative xample(s) (E - ) .  For our experiment, 
3A semantic operation that converts an argument to 
the type which is expected by a function, where it would 
otherwise result in a type error. 
201 
we furnish a set of N-V pairs related by one of 
the qualia relations within a POS context (E+), 
and a set of N-V pairs that are not semantically 
linked (E-),  and the method infers general rules 
(clauses) that explain these E +. This particular 
explanatory characteristic of ILP has motivated 
our choice: ILP does not just provide a predic- 
tor (this N-V pair is relevant, this one is not) 
but also a data-based theory. Contrary to some 
statistical methods, it does not just give raw 
results but explains the concept hat is learnt 4. 
We use Progol (Muggleton, 19915) for our 
project, Muggleton's ILP implementation that 
has already been proven well suited to deal with 
a big amount of data in multiple domains, and 
to lead to results comparable to other ILP im- 
plementations (Roberts et al, 1998). 
In this section we briefly describe the corpus 
on which our experiment has been conducted. 
We then explain the elaboration of E + and E -  
for Progol. We finally present he generalized 
clauses that we obtain. The validation of the 
method is detailed in section 4. 
3.1 The corpus 
The French corpus used in this project is 
a 700 kBytes handbook of helicopter main- 
tenance, given to us by MATRA CCR 
A@rospatiale, which contains more than 104000 
word occurrences 5. The MATRA CCR corpus 
has some special characteristics that are espe- 
cially well suited for our task: it is coherent; 
it contains lots of concrete terms (screw, door, 
etc.) that are frequently used in sentences to- 
gether with verbs indicating their telic (screws 
must be tightened, etc.) or agentive roles. 
This corpus has been POS-tagged with the 
help of annotation tools developed in the MUL- 
TEXT project (Armstrong, 1996); sentences and 
words are first segmented with MtSeg; words 
are analyzed and lemmatized with Mmorph (Pe- 
titpierre and Russell, 1998; Bouillon et al, 
1998), and finally disambiguated by the Tatoo 
tool, a Hidden Markov Model tagger (Arm- 
strong et al, 1995). Each word therefore only 
receive one POS-tag, with less than 2% of er- 
4Learning with ILP has already been successfully 
used in natural language processing, for example incor- 
pus POS-tagging (Cussens, 1996) or semantic nterpre- 
tation (Mooney, 1999). 
5104212 word occurrences. 
rors. 
3.2 Example  const ruct ion  
The first task consists in building up E + and 
E -  for Progol, in order for it to infer gener- 
alized clauses that explain what, in the POS 
context of N-V pairs, distinguishes the relevant 
pairs from the not relevant ones. Work has to 
be done to determine what is the most appro- 
priate context for this task. We just present 
here the solution we have finally chosen. Sec- 
tion 4 describes methods and measures to eval- 
uate the "quality" of the learning that enable 
us to choose between the different contextual 
possibilities. Here is our methodology for the 
construction of the examples. 
We first consider all the nouns of the MA- 
TRA CCR corpus. More precisely, we only deal 
with a 81314 word occurrence subcorpus of the 
MATRA CCR corpus, which is formed by all 
the sentences that contain at least one N and 
one V. This subcorpus contains 1489 different 
N (29633 noun occurrences) and 567 different 
V (9522 verb occurrences). For each N of this 
subcorpus, the 10 most strongly associated V, in 
terms of Chi-square, are selected. This first step 
both produces pairs that are really bound by 
one qualia relation ((dcrou, serrer)) 6 and pairs 
that are fully irrelevant ((roue, prescrire)) 7.
Each pair is manually annotated as relevant 
or irrelevant according to Pustejovsky's qualia 
structure principles. A Perl program is then 
used to find the occurrences of these N-V pairs 
in the sentences of the corpus. 
For each occurrence of each pair that is sup- 
posed to be used to build one E +, that is for 
each of the previous pairs that has been glob- 
ally annotated as relevant, a manual control has 
to be done to ensure that the N and the V really 
are in the expected relation within the studied 
sentence. After this control, a second Perl pro- 
gram automatically produces the E +. Here is 
the form of the positive examples: 
POSITiVE(category_before_N, category_after.N, 
category_before_V, V_type, distance, position). 
where V_type indicates if the V is an infinitive 
form, etc., distance corresponds to the number 
6(nut, tighten). 
7(wheel, prescribe) 
202 
of verbs between the N and the V, and position 
is POS (for positive) if the V appears before the 
N in the sentence, NEG if the N appears before 
the V. 
For example, 
POSITIVE(VRBINF, P_DE, VID, VRBINF~ 0, 
POS). 
means that a N-V pair, in which the N is 
surrounded with an infinitive verb on its left 
(VRBINF) and a preposition de s (P.DE) on its 
right, in which the V is preceded by nothing 9
(VID) 1? and is an infinitive one (VRBINF), in 
which no verb exists between the N and the V 
(0), and in which the V appears before the N 
in the sentence (POS), is a relevant pair (for ex- 
ample, in ouvrir la porte de ...). 
The E -  are elaborated in the same way than 
the E +, with the same Perl program. E -  and 
E + forms are identical, except he presence of a 
sign :- before the predicate POSITIVE to denote 
aE- :  
:-POSITIVE (category_before.N, 
category_after_N, category_before_V, V_type, 
distance, position). 
These E -  are automatically built from the 
previous highly correlated N-V pairs that have 
been manually annotated as irrelevant. For ex- 
ample, 
:-POSITIVE(VID, P_PAR, NC, VRBPP, 0, NEG). 
means that a N-V pair, in which the N has noth- 
ing on its left (VID) and a preposition par n 
(P_PAR) on its right, in which the V is preceded 
by a noun (NC) and is a past participle (VRBPP), 
in which no verb exists between the N and the 
V (0), and in which the V appears after the N 
in the sentence (NEG), is an irrelevant pair (for 
example, in freinage par goupilles fendues). 
4031 E + and about 7000 E -  are automati- 
cally produced this way from the corpus. 
sOl. 
9Or by one of the three categories that we do not 
consider for example laboration, that is, determiners, 
adverbs and adjectives. 
1?Empty. 
nBy. 
3.3 Learn ing  w i th  the  he lp  of  P rogo l  
These E + and E -  are then furnish to Progol 
in order for it to try to infer generalized clauses 
that explain the concept "qualia pair" versus 
"not qualia pair". We do not discuss here ei- 
ther parameter setting that concerns the choice 
of the example POS context, or evaluation cri- 
teria; this discussion is postponed to next sec- 
tion; we simply present he learning method and 
the type of generalized clauses that we have ob- 
tained. 
Some information have to be given to Progol 
for it to know what are the categories that can 
undergo a generalization. For example, if two 
E + are identical but possess different locative 
prepositions as second arguments (for eg. sur 12 
and sous13), must Progol produce a generaliza- 
tion corresponding to the same clause except 
that the second argument is replaced by the 
general one: locative-preposition, or by a still 
more general one: preposition? 
The background knowledge used by Progol is 
knowledge on the domain. For example here, it 
contains the fact that a verb can be found in 
the corpus in an infinitive or a conjugated form, 
etc. 
verbe( V ) :- infinitif( V ). 
verbe( V ) :- conjugue( V ). 
and that an infinitive form is denoted by the 
tag VERBINF, and a conjugated form by the tags 
VERB-PL and VER.B-SG, etc. 
infinitif( verbinf ). 
conjugue( verb-pl ). 
conjugue( verb-sg ). 
When Progol is provided with all this knowl- 
edge, learning can begun. The output of Progol 
is of two kinds: some clauses that have not at 
all been generalized (that is, some of the E+), 
and some generalized clauses; we call the set of 
these generalized clauses G, and it is this set G 
that interests us here. Here is an example of one 
of the generalized clauses that we have obtained 
in our experiment: 
POSITIVE(A, C, C, D, E, F) :- 
PREPOSITIONLIEU(A), VIDE(C), VERBINF(D), 
PRES(E). (1) 
12On" 
13Under. 
203 
which means that N-V pairs (i) in which the 
category before the N is a locative preposition 
(PREPOSITIONLIEU(A)), (ii) in which there is 
nothing after the N and before the V (VIDE(C) 
for the second and third arguments), (iii) in 
which the V is an infinitive one (VERBINF(D)), 
and (iv) in which there is no verb between the N 
and the V (proximity denoted by P:aEs(E)14), 
are relevant. No constraint is set on N/V order 
in the sentences. 
This generalized clause covers, for example, 
the following E+: 
POSITIVE(P_SUR, VID, VID, VERBINF, 0, POS). 
which corresponds to the relevant pair (prise, 
brancher) 15 that is detected in the corpus in the 
sentence "Brancher les connecteurs sur les prises 
~lectriques.". 
Some of the generalized clauses in G cover 
lots of E +, others far less. We now present a 
method to detect what the "good" c, lauses are, 
that is, the clauses that explain the concept that 
we want to learn, and a measure of the "quality" 
of the learning that has been conducted. 
4 Learn ing  va l idat ion  and  resu l ts  
This section is dedicated to two aspects of 
the validation of our machine learning method. 
First we define the theoretical validation of the 
learning, that is, we focus on the determination 
of a means to detect what are the "good" gen- 
eralized clauses, and of a measure of the quality 
of the concept learning; this parameter setting 
and evaluation criterion phase explains how we 
have chosen the precise POS context for N-V 
pairs in the E + and E -  (as described in subsec- 
tion 3.2): the six contextual elements in exam- 
ples are the combination that leads to the best 
results in terms of the learning quality measure 
that we have chosen. The second step of the 
validation is the empirical one. We have applied 
the generalized clauses that have been selected 
to the Mat ra  CCR corpus and  haw~ evaluated 
the quality of the results in terms of pairs that 
are indicated relevant or not. Here  are these 
two phases. 
14Close(E). 
l~(plug, to plug in). 
4.1 Theoret ical  val idation 
As we have previously noticed, among the gen- 
eralized clauses produced from our E + and E -  
by Progol (set G), some of them cover a lot of 
E +, others only a few of them. What we want 
is to get a way to automatically find what are 
the generalized clauses that have to be kept in 
order to explain the concept we want to learn. 
We have first defined a measure of the theo- 
retical generality of the clauses 16. The theoreti- 
cal generality of a generalized clause is the num- 
ber of not generalized clauses (E +) that this 
clause can cover. For example, both 
POSITIVE(P_AUTOURDE, VID, VID, VERBINF, 
0, NEG). 
and 
POSITIVE(P_CHEZ, VID, VID, VERBINF, 0, 
POS). 
can be covered by clause (1) (cf. subsec- 
tion 3.3). During the study of, for example, 
the distribution of the number of clauses in G 
on these different heoretical generality values, 
our "hope" is to obtain a gaussian-like graph 
in order to automatically select all the clauses 
present under the gaussian plot, or to calculate 
two thresholds that cover 95% of these clauses 
and to reject the other 5%. This distribution is
however not a gaussian one. 
Our second try has not only concerned the 
theoretical coverage of G clauses but also their 
empirical coverage. This second measure that 
we have defined is the number of E + that are 
really covered by each clause of G. We then con- 
sider the distribution of the empirical coverage 
of G clauses on the theoretical coverages of these 
clauses, that is, we consider the graph in which, 
for each different heoretical measure value for 
G clauses, we draw a line whose length corre- 
sponds to the total number of E + covered by 
the G clauses that have this theoretical cover- 
age value. Here two gaussians clearly appear 
(cf. figure 1), one for rather specific lauses and 
the other for more general ones. We have there- 
fore decided to keep all the generalized clauses 
produced by Progol. 
16We thank J. Nicolas, INRIA researcher at IRISA, for 
his help on this point. 
204 
800 
700 
600 
5OO 
400 
ul 300  
200 
100 
!!iiii~i~ii!ii!i!iiii!iiii!iiii~iiiii!iil 
Theoretical coverage 
Figure 1: Distribution of 
The second point concerns the determination 
of a measure of the quality of the learning for the 
parameter setting. We are especially interested 
in the percentage ofE + that are covered by the 
generalized clauses, and if we permit some noise 
in Progol parameter adjustment to allow more 
generalizations, by the percentage of E -  that 
are rejected by these generalized clauses. The 
measure of the recall and the precision rates of 
the learning method can be summarized in a 
Pearson coefficient: 
Pearson = (TP ,TN) - (FP ,FN)  
x /P rP*PrN*AP*AN 
where A = actual, Pr = predicated, P -- pos- 
itive, N= negative, T= true, F= false; the more 
close to 1 this value is, the better the learning 
is. 
The results for our learning method with a 
rate of Progol noise equal to 0 are the following: 
from the 4031 initial E + and the 6922 initial E- ,  
the 109 generalized clauses produced by Progol 
cover 2485 E + and 0 E-; 1546 E + and 6922 E-  
positive examples on clauses 
are therefore uncovered; the value of the Pear- 
son coefficient is 0.71. (NB: Figure 1 illustrates 
these results). 
We have developed a Perl program whose role 
is to find which Progol noise rate leads to the 
best results. This Progol noise rate is equal to 
37. With this rate, the results are the following: 
from the 4031 initial E + and the 6922 initial E-,  
the 66 generalized clauses produced by Progol 
cover 3547 E + and 348 E-; 484 E + and 6574 E-  
are therefore uncovered; the value of the Pear- 
son coefficient is 0.84. The stability of the set 
of learnt generalized clauses has been tested. 
4.2 Empir ica l  val idat ion 
In order to evaluate the empirical validity of our 
learning method, we have applied the 66 gen- 
eralized clauses to the Matra CCR corpus and 
have studied the appropriateness of the pairs 
that are stated relevant or irrelevant by them. 
Of course, it is impossible to test all the N-V 
combinations present in such a corpus. Our 
evaluation has focussed on some of the signif- 
205 
icant nouns of the domain. 
A Perl program presents to one expert all the 
N-V pairs that appear in one sentence in a part 
of the corpus and include one of the studied 
nouns. The expert manually tags each pair as 
relevant or not. This tagging is then compared 
to the results obtained for these N-V pairs of 
the same part of the corpus by the application 
of the generalized clauses learnt wit\]h Progol. 
The results for seven significant nouns (vis, 
@crou, porte, voyant, prise, capot, bouchon) 17 
are presented in table 1. In the left column, one 
N-V pair is considered as tagged "relevant" by 
the generalized clauses if at least one of them 
covers this pair; in the right one, at least six 
different clauses of G must cover a pair for it 
to be said correctly detected by the generalized 
clauses; the aim of this second test is to reduce 
noise in the results. 
1 occurrence 6 occurrences 
correctly found: 49 correctly found: 23 
incorrectly found: 54 incorrectly found: 4 
not found: 10 not found: 36 
Pearson = 0.5138 Pearson = 0.5209 
Table 1: Empirical validation on Matra CCR 
corpus 
The results are quite promising, especially if 
we compare them to those obtain by Chi-square 
correlation (cf. table 2). This comparison is 
interesting because Chi-square is the first step 
of our selection of N-V couples in the corpus (cf. 
subsection 3.2). 
correctly found: 38 
incorrectly found: 124 
not found: 21 
Pearson = 0.1206 
Table 2: Chi-square results on Matra CCR cor- 
pus 
5 Conc lus ion 
The Inductive Logic Programming learning 
method that we have proposed in order to de- 
fine what is a N-V pair whose elements are 
17(screw, nut, door, indicator signal, plug, cowl, cap). 
bound by one of the qualia relations in Puste- 
jovsky's Generative Lexicon formalism leads to 
very promising results: 83.05% of relevant pairs 
(after one occurrence) are detected for seven sig- 
nificant nouns; these results have to be com- 
pared with the 64% results of Chi-square. It 
is worth noticing that beyond this simple com- 
parison with one of the possible pure statis- 
tics based method is, the interest of using ILP 
learning is its explanatory characteristic; and it 
is this characteristic that have motivated our 
choice: contrary to statistical methods, our ILP 
method does not just extract statistically corre- 
lated pairs but it permits to automatically earn 
rules that distinguish relevant pairs from others. 
The fact that noise has to be used in Progol to 
obtain these results however means that some- 
thing is missing in our E + to fully define the 
concept "qualia pair" versus "not qualia pair"; 
some E -  have to be covered to define it better. 
A piece of information, maybe syntactic and/or 
semantic is missing in our E + to fully character- 
ize it. This fact can be easily illustrated by the 
following example: 'Verbinf det N' structures 
are generally relevant (ouvrir la porte 19, etc.), 
except when the N indicates a collection of ob- 
jects (nettoyer l'ensemble du rdservoir 2?) or a 
part of an object (vider le fond du rdservoir21). 
A simple POS-tagging of the sentences offers 
no difference between them. We are currently 
working on a semantic tagging of the Matra 
CCR corpus in order to improve the results. 
Another future work concerns the automatic 
distinction between the various qualia roles dur- 
ing learning. The last phase of the project will 
deal with the real use of the N-V pairs obtained 
by the machine learning method within one in- 
formation retrieval system and the evaluation of 
the improvement of its performances. 
Re ferences  
Rajeev Agarwal. 1995. Semantic Feature Extraction 
from Technical Texts with Limited Human Inter- 
vention. Ph.D. thesis, Mississippi State Univer- 
sity, USA. 
Susan Armstrong, Pierrette Bouillon, and 
Gilbert Robert. 1995.  Tagger Overview. 
lSThis comparison could be extended to other corpus 
frequency based technics (mutual information, etc.). 
19Open the door. 
2?Clean the whole tank. 
21Empty the tank bottom. 
206 
Technical report, ISSCO, (http://issco- 
www.unige.ch/staff/robert/tatoo/tagger.html). 
Susan Armstrong. 1996. Multext: Multilingual 
Text Tools and Corpora. In H. Feldweg and 
W. Hinrichs, editors, Lexikon und Text, pages 
107-119. Tiibingen: Niemeyer. 
Christian Bassac and Pierrette Bouillon. 2000. The 
Polymorphism of Verbs Exhibiting Middle Tran- 
sitive Alternations in English. Technical report, 
ISSCO. 
Jacques Bouaud, Beno~t Habert, Adeline Nazarenko, 
and Pierre Zweigenbaum. 1997. Regroupe- 
ments issus de d@pendances syntaxiques en cor- 
pus : cat@gorisation et confrontation avec deux 
mod@lisations conceptuelles. In Proceedings of 
Ingdnierie de la Connaissance, Roscoff, France. 
Pierrette Bouillon and Federica Busa. 2000. Gener- 
ativity in the Lexicon. CUP:Cambridge, In Press. 
Pierrette Bouillon, Sabine Lehmann, Sandra 
Manzi, and Dominique Petitpierre. 1998. 
DSveloppement de lexiques ~ grande @chelle. 
In Proceedings of colloque de Tunis 1997 "La 
mgmoire des mots', Tunis, Tunisie. 
Ted Briscoe and John Carroll. 1997. Automatic Ex- 
traction of Subcategorisation from Corpora. In 
Proceedings of 5th ACL conference on Applied 
Natural Language Processing, Washington, USA. 
James Cussens. 1996. Part-of-Speech Disambigua- 
tion using ILP. Technical report, Oxford Univer- 
sity Computing Laboratory. 
C@cile Fabre and Pascale S~billot. 1999. Seman- 
tic Interpretation of Binominal Sequences and In- 
formation Retrieval. In Proceedings of Interna- 
tional ICSC Congress on Computational Intelli- 
gence: Methods and Applications, CIMA '99, Sym- 
posium on Advances in Intelligent Data Analysis 
AIDA '99, Rochester, N.Y., USA. 
David Faure and Claire N@dellec. 1999. Knowledge 
Acquisition of Predicate Argument Structures 
from Technical Texts using Machine Learning: 
the System ASIUM. In Dieter Fensel Rudi Studer, 
editor, Proceedings of 11th European Workshop 
EKAW'99, Dagstuhl, Germany. Springer-Verlag. 
Christiane Fellbaum, editor. 1998. WordNet: An 
Electronic Lexical Database. MIT Press, Cam- 
bridge, MA. 
Gregory Grefenstette. 1994a. Corpus-Derived First, 
Second and Third-Order Word Affinities. In 
Proceedings of EURALEX'9~, Amsterdam, The 
Netherlands. 
Gregory Grefenstette. 1994b. Explorations in Auto- 
matic Thesaurus Discovery. Dordrecht: Kluwer 
Academic Publishers. 
Gregory Grefenstette. 1997. SQLET: Short Query 
Linguistic Expansion Techniques, Palliating One- 
Word Queries by Providing Intermediate Struc- 
ture to Text. In McGill-University, editor, Pro- 
ceedings of Recherche d'Informations Assistde 
par Ordinateur, RIAO'g7, Montr@al, Qu@bec, 
Canada. 
Beno~t Habert, Adeline Nazarenko, and Andr@ 
Salem. 1997. Les linguistiques de corpus. Ar- 
mand Collin/Masson, Paris. 
Zelig Harris, Michael Gottfried, Thomas Ryckman, 
Paul Mattick(Jr), Anne Daladier, Tzvee N. Har- 
ris, and Suzanna Harris. 1989. The Form of 
Information in Science, Analysis of Immunology 
Sublanguage. Kluwer Academic Publisher, Dor- 
drecht. 
Marti A. Hearst. 1992. Automatic Acquisition 
of Hyponyms from Large Text Corpora. In 
Proceedings of 15th International Conference on 
Computational Linguistics, COLING-92, Nantes, 
France. 
Tom M. Mitchell. 1997. Machine Learning. 
McGraw-Hill. 
Raymond Mooney. 1999. Learning for Semantic In- 
terpretation: Scaling Up without Dumbing Down. 
In Proceedings of Learning Language in Logic, 
LLL99, Bled, Slovenia. 
Emmanuel Morin. 1997: Extraction de liens 
s@mantiques entre termes dans des corpus de 
textes techniques : application ~ l'hyponymie. 
In Proceedings of Traitement Automatique des 
Langues Naturelles, TALN'97, Grenoble, France. 
Stephen Muggleton and Luc De-Raedt. 1994. In- 
ductive Logic Programming: Theory and Meth- 
ods. Journal of Logic Programming, 19-20:629- 
679. 
Stephen Muggleton. 1995. Inverse Entailment and 
Progol. New Generation Computing, 13(3-4):245- 
286. 
Dominique Petitpierre and Graham Russell. 1998. 
Mmorph - the Multext Morphology Program. 
Technical report, ISSCO. 
Ronan Pichon and Pascale S~billot. 1997. Acquisi- 
tion automatique d'informations lexicales ~ partir 
de corpus : un brian. Research report n?3321, IN- 
RIA, Rennes. 
Ronan Pichon and Pascale S@billot. 1999. From 
Corpus to Lexicon: from Contexts to Semantic 
Features. In Proceedings of Practical Applications 
in Language Corpora, PALC'99, to appear, Lodz, 
Poland. 
James Pustejovsky, Peter Anick, and Sabine Bergler. 
1993. Lexical Semantic Techniques for Corpus 
Analysis. Computational Linguistics, 19(2). 
James Pustejovsky. 1995. The Generative Lexicon. 
Cambridge:MIT Press. 
Sam Roberts, Wim Van-Laer, Nico Jacobs, Stephen 
Muggleton, and Jeremy Broughton. 1998. A 
Comparison of ILP and Propositional Systems on 
207 
Propositional Data. In Springer-Verlag, editor, 
Proceedings of 8th International Workshop on In- 
ductive Logic Programming, ILP-98, :Berlin, Ger- 
many. LNAI 1446. 
208 
  	

Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1314?1324,
Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics
Leveraging lexical cohesion and disruption for topic segmentation
Anca S?imon
Universite? de Rennes 1
IRISA & INRIA Rennes
Guillaume Gravier
CNRS
IRISA & INRIA Rennes
anca-roxana.simon@irisa.fr
guillaume.gravier@irisa.fr
pascale.sebillot@irisa.fr
Pascale Se?billot
INSA de Rennes
IRISA & INRIA Rennes
Abstract
Topic segmentation classically relies on one
of two criteria, either finding areas with co-
herent vocabulary use or detecting discontinu-
ities. In this paper, we propose a segmenta-
tion criterion combining both lexical cohesion
and disruption, enabling a trade-off between
the two. We provide the mathematical formu-
lation of the criterion and an efficient graph
based decoding algorithm for topic segmenta-
tion. Experimental results on standard textual
data sets and on a more challenging corpus
of automatically transcribed broadcast news
shows demonstrate the benefit of such a com-
bination. Gains were observed in all condi-
tions, with segments of either regular or vary-
ing length and abrupt or smooth topic shifts.
Long segments benefit more than short seg-
ments. However the algorithm has proven ro-
bust on automatic transcripts with short seg-
ments and limited vocabulary reoccurrences.
1 Introduction
Topic segmentation consists in evidentiating the se-
mantic structure of a document: Algorithms devel-
oped for this task aim at automatically detecting
frontiers which define topically coherent segments
in a text.
Various methods for topic segmentation of tex-
tual data are described in the literature, e.g., (Rey-
nar, 1994; Hearst, 1997; Ferret et al, 1998; Choi,
2000; Moens and Busser, 2001; Utiyama and Isa-
hara, 2001), most of them relying on the notion of
lexical cohesion, i.e., identifying segments with a
consistent use of vocabulary, either based on words
or on semantic relations between words. Reoccur-
rences of words or related words and lexical chains
are two popular methods to evidence lexical cohe-
sion. This general principle of lexical cohesion is
further exploited for topic segmentation with two
radically different strategies. On the one hand, a
measure of the lexical cohesion can be used to deter-
mine coherent segments (Reynar, 1994; Moens and
Busser, 2001; Utiyama and Isahara, 2001). On the
other hand, shifts in the use of vocabulary can be
searched for to directly identify the segment fron-
tiers by measuring the lexical disruption (Hearst,
1997).
Techniques based on the first strategy yield more
accurate segmentation results, but face a problem of
over-segmentation which can, up to now, only be
solved by providing prior information regarding the
distribution of segment length or the expected num-
ber of segments. In this paper, we propose a segmen-
tation criterion combining both cohesion and dis-
ruption along with the corresponding algorithm for
topic segmentation. Such a criterion ensures a co-
herent use of vocabulary within each resulting seg-
ment, as well as a significant difference of vocabu-
lary between neighboring segments. Moreover, the
combination of these two strategies enables regular-
izing the number of segments found without resort-
ing to prior knowledge.
This piece of work uses the algorithm of Utiyama
and Isahara (2001) as a starting point, a versatile and
performing topic segmentation algorithm cast in a
statistical framework. Among the benefits of this al-
gorithm are its independency to any particular do-
main and its ability to cope with thematic segments
1314
of highly varying lengths, two interesting features
to obtain a generic solution to the problem of topic
segmentation. Moreover, the algorithm has proven
to be up to the state of the art in several studies, with
no need of a priori information about the number of
segments (contrary to algorithms in (Malioutov and
Barzilay, 2006; Eisenstein and Barzilay, 2008) that
can attain a higher segmentation accuracy). It also
provides an efficient graph based implementation of
which we take advantage.
To account both for cohesion and disruption, we
extend the formalism of Isahara and Utiyama using
a Markovian assumption between segments in place
of the independence assumption of the original algo-
rithm. Keeping unchanged their probabilistic mea-
sure of lexical cohesion, the Markovian assumption
enables to introduce the disruption between two con-
secutive segments. We propose an extended graph
based decoding strategy, which is both optimal and
efficient, exploiting the notion of generalized seg-
ment model or semi hidden Markov models. Tests
are performed on standard textual data sets and on
a more challenging corpus of automatically tran-
scribed broadcast news shows.
The seminal idea of this paper was partially pub-
lished in (Simon et al, 2013) in the French language.
The current paper significantly elaborates on the lat-
ter, with a more detailed description of the algo-
rithm and additional contrastive experiments includ-
ing more data sets. In particular, new experiments
clearly demonstrate the benefit of the method in a
realistic setting with statistically significant gains.
The organization of the article is as follows. Ex-
isting work on topic segmentation is presented in
Section 2, emphasizing the motivations of the model
we propose. Section 3 details the baseline method
of Utiyama and Isahara before introducing our algo-
rithm. Experimental protocol and results are given
in Section 4. Section 5 summarizes the finding and
concludes with a discussion of future work.
2 Related work
Defining the concept of theme precisely is not trivial
and a large number of definitions have been given by
linguists. Brown and Yule (1983) discuss at length
the difficulty of defining a topic and note: ?The
notion of ?topic? is clearly an intuitively satisfac-
tory way of describing the unifying principle which
makes one stretch of discourse ?about? something
and the next stretch ?about? something else, for it
is appealed to very frequently in the discourse anal-
ysis literature. Yet the basis for the identification of
?topic? is rarely made explicit?. To skirt the issue of
defining a topic, they suggest to focus on topic-shift
markers and to identify topic changes, what most
current topic segmentation methods do.
Various characteristics can be exploited to iden-
tify thematic changes in text data. The most popular
ones rely either on the lexical distribution informa-
tion to measure lexical cohesion (i.e., word reoccur-
rences, lexical chains) or on linguistic markers such
as discourse markers which indicate continuity or
discontinuity (Grosz and Sidner, 1986; Litman and
Passonneau, 1995). Linguistic markers are however
often specific to a type of text and cannot be consid-
ered in a versatile approach as the one we are target-
ing, where versatility is achieved relying on the sole
lexical cohesion.
The key point with lexical cohesion is that a sig-
nificant change in the use of vocabulary is consid-
ered to be a sign of topic shift. This general idea
translates into two families of methods, local ones
targeting a local detection of lexical disruptions and
global ones relying on a measure of the lexical cohe-
sion to globally find segments exhibiting coherence
in their lexical distribution.
Local methods (Hearst, 1997; Ferret et al, 1998;
Hernandez and Grau, 2002; Claveau and Lefe`vre,
2011) locally compare adjacent fixed size regions,
claiming a boundary when the similarity between
the adjacent regions is small enough, thus identify-
ing points of high lexical disruption. In the seminal
work of Hearst (1997), a fixed size window divided
into two adjacent blocks is used, consecutively cen-
tered at each potential boundary. Similarity between
the adjacent blocks is computed at each point, the re-
sulting similarity profile being analyzed to find sig-
nificant valleys which are considered as topic bound-
aries.
On the contrary, global methods (Reynar, 1994;
Choi, 2000; Utiyama and Isahara, 2001; Ji and Zha,
2003; Malioutov and Barzilay, 2006; Misra et al,
2009) seek to maximize the value of the lexical co-
hesion on each segment resulting from the segmen-
tation globally on the text. Several approaches have
1315
been taken relying on self-similarity matrices, such
as dot plots, or on graphs. A typical and state-of-the-
art algorithm is that of Utiyama and Isahara (2001)
whose principle is to search globally for the best
path in a graph representing all possible segmenta-
tions and where edges are valued according to the
lexical cohesion measured in a probabilistic way.
When the lengths of the respective topic segments
in a text (or between two texts) are very differ-
ent from one another, local methods are challenged.
Finding out an appropriate window size and extract-
ing boundaries become critical with segments of
varying length, in particular when short segments
are present. Short windows will render compari-
son of adjacent blocks difficult and unreliable while
long windows cannot handle short segments. The
lack of a global vision also makes it difficult to nor-
malize properly the similarities between blocks and
to deal with statistics on segment length. While
global methods override these drawbacks, they face
the problem of over-segmentation due to the fact that
they mainly rely on the sole lexical cohesion. Short
segments are therefore very likely to be coherent
which calls for regularization introduced as priors
on the segments length.
These considerations naturally lead to the idea of
methods combining lexical cohesion and disruption
to make the best of both worlds. While the two cri-
teria rely on the same underlying principle of lex-
ical coherence (Grosz et al, 1995) and might ap-
pear as redundant, the resulting algorithms are quite
different in their philosophy. A first (and, to the
best of our knowledge, unique) attempt at captur-
ing a global view of the local dissimilarities is de-
scribed in Malioutov and Barzilay (2006). However,
this method assumes that the number of segments to
find is known beforehand which makes it difficult
for real-world usage.
3 Combining lexical cohesion and
disruption
We extend the graph-based formalism of Utiyama
and Isahara to jointly account for lexical cohesion
and disruption in a global approach. Clearly, other
formalisms than the graph-based one could have
been considered. However, graph-based probabilis-
tic topic segmentation has proven very accurate and
versatile, relying on very minimal prior knowledge
on the texts to segment. Good results at the state-of-
the-art have also been reported in difficult conditions
with this approach (Misra et al, 2009; Claveau and
Lefe`vre, 2011; Guinaudeau et al, 2012).
We briefly recall the principle of probabilistic
graph-based segmentation before detailing a Marko-
vian extension to account for disruption.
3.1 Probabilistic graph-based segmentation
The idea of the probabilistic graph-based segmen-
tation algorithm is to find the segmentation into the
most coherent segments constrained by a prior dis-
tribution on segments length. This problem is cast
into finding the most probable segmentation of a se-
quence of t basic units (i.e., sentences or utterances
composed of words) W = ut1 among all possible
segmentations, i.e.,
S? = arg max
S
P [W |S]P [S] . (1)
Assuming that segments are mutually independent
and assuming that basic units within a segment are
also independent, the probability of a text W for a
segmentation S = Sm1 is given by
P [W |Sm1 ] =
m?
i=1
ni?
j=1
P [wij |Si] , (2)
where ni is the number of words in the segment
Si, wij is the j
th word in Si and m the number of
segments. The probability P [wij |Si] is given by a
Laplace law where the parameters are estimated on
Si, i.e.,
P [wij |Si] =
fi(wij) + 1
ni + k
, (3)
where fi(wij) is the number of occurrences of w
i
j
in Si and k is the total number of distinct words in
W , i.e., the size of the vocabulary V . This probabil-
ity favors segments that are homogeneous, increas-
ing when words are repeated and decreasing consis-
tently when they are different. The prior distribu-
tion on segment length is given by a simple model,
P [Sm1 ] = n
?m, where n is the total number of
words, exhibiting a large value for a small number
of segments and conversely.
The optimization of Eq. 1 can be efficiently im-
plemented as the search for the best path in a
1316
weighted graph which represents all the possible
segmentations. Each node in the graph corresponds
to a possible frontier placed between two utterances
(i.e., we have a node between each pair of utter-
ances), the arc between nodes i and j representing a
segment containing utterances ui+1 to uj . The cor-
responding arc weight is the generalized probability
of the words within segment Si?j according to
v(i, j) =
j?
k=i+1
ln(P [uk|Si?j ])? ?ln(n)
where the probability is given as in Eq. 3. The factor
? is introduced to control the trade-off between the
segments length and the lexical cohesion.
3.2 Introduction of the lexical disruption
Eq. 2 derives from the assumption that each segment
Si is independent from the others, which makes it
impossible to consider disruption between two con-
secutive segments. To do so, the weight of an arc
corresponding to a segment Si should take into ac-
count how different this segment is from Si?1. This
is typically handled using a Markovian assumption
of order 1. Under this assumption, Eq. 2 is reformu-
lated as
P [W |Sm1 ] = P [W |S1]
m?
i=2
P [W |Si, Si?1] ,
where the notion of disruption can be embedded in
the term P [W |Si, Si?1] which explicitly mentions
both segments. Formally, P [W |Si, Si?1] is defined
as a probability. However, arbitrary scores which do
not correspond to probabilities can be used instead
as the search for the best path in the graph of possi-
ble segmentations makes no use of probability the-
ory. In this study, we define the score of a segment
Si given Si?1 as
lnP [W |Si, Si?1] = lnP [Wi|Si]? ??(Wi,Wi?1)
(4)
where Wi designates the set of utterances in Si
and the rightmost part reflects the disruption be-
tween the content of Si and of Si?1. Eq. 4 clearly
combines the measure of lexical cohesion with a
measure of the disruption between consecutive seg-
ments: ?(Wi,Wi?1) > 0 measures the coherence
between Si and Si?1, the substraction thus account-
ing for disruption by penalizing consecutive coher-
ent segments. The underlying assumption is that the
bigger ?(Wi,Wi?1), the weaker the disruption be-
tween the two segments. Parameter ? controls the
respective contributions of cohesion and disruption.
We initially adopted a probabilistic measure
of disruption based on cross probabilities, i.e.,
P [Wi|Si?1] and P [Wi?1|Si], which proved to have
limited impact on the segmentation. We therefore
prefer to rely on a cosine similarity measure be-
tween the word vectors representing two adjacent
segments, building upon a classical strategy of lo-
cal methods such as TextTiling (Hearst, 1997). The
cosine similarity measure is calculated between vec-
tors representing the content of resp. Si and Si?1,
denoted vi and vi?1, where vi is a vector contain-
ing the (tf-idf) weight of each term of V in Si. The
cosine similarity is classically defined as
cos(vi?1,vi) =
?
v?V
vi?1(v) vi(v)
??
v?V
v2i?1(v)
?
v?V
v2i (v)
. (5)
?(Wi,Wi?1) is calculated from the cosine similar-
ity measure as
?(Wi,Wi?1) = (1? cos(vi?1,vi))
?1 , (6)
thus yielding a small penalty in Eq. 4 for highly dis-
rupting boundaries, i.e., corresponding to low simi-
larity measure.
Given the quantities defined above, the algorithm
boils down to finding the best scoring segmentation
as given by
S? = arg max
S
m?
i=1
ln(P [Wi|Si])?
?
m?
i=2
?(Wi,Wi?1)? ?mln(n) . (7)
3.3 Segmentation algorithm
Translating Eq. 7 into an efficient algorithm is not
straightforward since all possible combinations of
adjacent segments need be considered. To do so in a
graph based approach, one needs to keep separated
the paths of different lengths ending in a given node.
In other words, only paths of the same length ending
1317
at a given point, with different predecessors, should
be recombined so that disruption can be considered
properly in subsequent steps of the algorithm. Note
that, in standard decoding as in Utiyama and Isa-
hara?s algorithm, only one of such paths, the best
scoring one, would be retained. We employ a strat-
egy inspired from the decoding strategy of segment
models or semi-hidden Markov model with explicit
duration model (Ostendorf et al, 1996; Delakis et
al., 2008).
Search is performed through a lattice L =
{V,E}, with V the set of nodes representing poten-
tial boundaries and E the set of edges representing
segments, i.e., a set of consecutive utterances. The
set V is defined as
V = {nij |0 ? i, j ? N} ,
where nij represents a boundary after utterance ui
reached by a segment of length j utterances and
N = t+1. In the lattice example of Fig. 1, it is trivial
to see that for a given node, all incoming edges cover
the same segment. For example, the node n42 is po-
sitioned after u4 and all incoming segments contain
the two utterances u3 and u4. Edges are defined as
E = {eip,jl|0 ? i, p, j, l ? N ;
i < j; i = j ? l;Lmin ? l ? Lmax} ,
where eip,jl connects nip and njl with the constraint
that l = j ? i and Lmin ? l ? Lmax. Thus, an edge
eip,jl represents a segment of length l containing ut-
terances from ui+1 to uj , denoted Si?j . In Fig. 1,
e01,33 represents a segment of length 3 from n01 to
n33, covering utterances u1 to u3. To avoid explo-
sion of the lattice, a maximum segment length Lmax
is defined. Symmetrically, a minimum segment size
can be used.
The property of this lattice, where, by construc-
tion, all edges out of a node have the same segment
as a predecessor, makes it possible to weight each
edge in the lattice according to Eq. 4. Consider a
node nij for which all incoming edges encompass
utterances ui?j to ui. For each edge out of nij ,
whatever the target node (i.e., the edge length), one
can therefore easily determine the lexical cohesion
as defined by the generalized probability of Eq. 3
and the disruption with respect to the previous seg-
ment as defined by Eq. 6.
Algorithm 1 Maximum probability segmentation
Step 0. Initialization
q[0][j] = 0 ?j ? [Lmin, Lmax]
q[i][j] = ?? ?i ? [1, N ], j ? [Lmin, Lmax]
Step 1. Assign best score to each node
for i = 0? t do
for j = Lmin ? Lmax do
for k = Lmin ? Lmax do
/* extend path ending after ui with a
segment of length j with an arc of length k */
q[i+k][k] = max
?
?????
?????
q[i+ k][k],
q[i][j]+
Cohesion(ui+1 ? ui+k)?
??(ui?j ? ui;ui+1 ? ui+k)
end for
end for
end for
Step 2. Backtrack from nNj with best score
q[N ][j]
Given the weighted decoding graph, the solution
to Eq. 7 is obtained by finding out the best path in
the decoding lattice, which can be done straightfor-
wardly by scanning nodes in topological order. The
decoding algorithm is summarized in Algorithm 1
with an efficient implementation in o(NL2max) which
does not require explicit construction of the lattice.
4 Experiments
Experiments are performed on three distinct corpora
which exhibit different characteristics, two contain-
ing textual data and one spoken data. We first
present the corpora before presenting and discussing
results on each.
4.1 Corpora
The artificial data set of Choi (2000) is widely used
in the literature and enables comparison of a new
segmentation method with existing ones. Choi?s
data set consist of 700 documents, each created by
concatenating the first z sentences of 10 articles ran-
domly chosen from the Brown corpus, assuming
each article is on a different topic. Table 1 provides
1318
Figure 1: An example of a lattice L.
z = 3?11 3?5 6?8 9?11
# samples 400 100 100 100
Table 1: Number of documents in Choi?s corpus (Choi,
2000).
the corpus statistics, where z=3?11 means z is ran-
domly chosen in the range [3, 11]. Hence, Choi?s
corpus is adapted to test the ability of our model
to deal with variable segments length, z=3?11 be-
ing the most difficult condition. Moreover, Choi?s
corpus provides a direct comparison with results re-
ported in the literature.
One of the main criticism of Choi?s data set is the
presence of abrupt topic changes due to the artifi-
cial construction of the corpus. We therefore re-
port results on a textual corpus with more natural
topic changes, also used in (Eisenstein and Barzi-
lay, 2008). The data set consists of 277 chapters
selected from (Walker et al, 1990), a medical text-
book, where each chapter?considered here as a
document?was divided by its author into themat-
ically coherent sections. The data set has a total of
1,136 segments with an average of 5 segments per
document and an average of 28 sentences per seg-
ment. This data set is used to study the impact of
smooth, natural, topic changes.
Finally, results are reported on a corpus of au-
tomatic transcripts of TV news spoken data. The
data set consists of 56 news programs (?1/2 hour
each), broadcasted in February and March 2007 on
the French television channel France 2, and tran-
scribed by two different automatic speech recogni-
tion (ASR) systems, namely IRENE (Huet et al,
2010) and LIMSI (Gauvain et al, 2002), with re-
spective word error rates (WER) around 36 % and
30 %. Each news program consists of successive
reports of short duration (2-3 min), possibly with
consecutive reports on different facets of the same
news. The reference segmentation was established
by associating a topic with each report, i.e., plac-
ing a boundary at the beginning of a report?s in-
troduction (and hence at the end of the closing re-
marks). The TV transcript data set, which corre-
sponds to some real-world use cases in the multi-
media field, is very challenging for several reasons.
On the one hand, segments are short, with a reduced
number of repetitions, synonyms being frequently
employed. Moreover, smooth topic shifts can be
found, in particular at the beginning of each pro-
gram with different reports dedicated to the head-
line. On the other hand, transcripts significantly dif-
fer from written texts: no punctuation signs or capi-
tal letters; no sentence structure but rather utterances
which are only loosely syntactically motivated; pres-
ence of transcription errors which may imply an ac-
centuated lack of word repetitions.
All data were preprocessed in the same way:
Words were tagged and lemmatized with TreeTag-
1319
ger1 and only the nouns, non modal verbs and adjec-
tives were retained for segmentation. Inverse docu-
ment frequencies used to measure similarity in Eq. 5
are obtained on a per document basis, referring to
the number of sentences in textual data and of utter-
ances in spoken data.
4.2 Results
Performance is measured by comparison of hypoth-
esized frontiers with reference ones. Alignment as-
sumes a tolerance of 1 sentence on texts and of 10
seconds on transcripts, which corresponds to stan-
dard values in the literature. Results are reported us-
ing recall, precision and F1-measure. Recall refers
to the proportion of reference frontiers correctly de-
tected; Precision corresponds to the ratio of hypoth-
esized frontiers that belong to the reference seg-
mentation; F1-measure combines recall and preci-
sion in a single value. These evaluation measures
were selected because recall and precision are not
sensitive to variations of segment length contrary
to the Pk measure (Beeferman et al, 1997) and
do not favor segmentations with a few number of
frontiers as WindowDiff (Pevzner and Hearst, 2002)
(see (Niekrasz and Moore, 2010) for a rigorous an-
alytical explanation of the biases of Pk and Win-
dowDiff ).
Several configurations were considered in the ex-
periments; due to space constraints, only the most
salient experiments are presented here. In Eq. 7, the
parameter ?, which controls the contribution of the
prior model with respect to the lexical cohesion and
disruption, allows for different trade-offs between
precision and recall. For any given value of ?, ?
is thus varied, providing the range of recall/precision
values attainable. Results are compared to a baseline
system corresponding to the application of the orig-
inal algorithm of Utiyama and Isahara (i.e., setting
? = 0). This baseline has been shown to be a high-
performance algorithm, in particular with respect to
local methods that exploit lexical disruption. Differ-
ences in F1-measure between this baseline and our
system presented below are all statistically signifi-
cant at the level of p < 0.01 (paired t-test).
Choi?s corpus. Figure 2 reports results obtained
on Choi?s data set, each graphic corresponding to
1http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger
z ?
F1 Confidence interval 95 %
gain UI Combined
3-5 0 -0.2 [66.6,74.26] [75.23,78.08]
3-5 1 0.7 [72.25,83.4] [87.88,92.13]
3-11 1 0.23 [68.5,79.3] [86.6,87.43]
6-8 1 0.4 [68.48,80.99] [76.9,85.17]
9-11 0 1.6 [64.35,75.16] [81.31,84.86]
9-11 1 1.4 [68.39,80.39] [84.37,88.9]
Table 2: Gain in F1-measure for Choi?s corpus when us-
ing lexical cohesion and disruption, and the correspond-
ing 95 % confidence intervals for the F1-measure. Re-
sults are reported for different tolerance ? . UI denotes
the baseline and Combined the proposed model.
a specific variation in the size of the thematic seg-
ments forming the documents (e.g., 9 to 11 sen-
tences for the top left graphic). Results are provided
for different values of ? in terms of F1-measure
boxplots, i.e., variations of the F1-measure when ?
varies (same range of variation for ? considered for
each plot), where the leftmost boxplot, denoted by
UI , corresponds to the baseline. Box and whisker
plots graphically depicts the distribution of the F1-
measures that can be attained by varying ?, plotting
the median value, the first and third quartile and the
extrema.
Figure 2 shows that, whatever the segments
length, results globally improve according to the im-
portance given to the disruption (? variable). More-
over, the variation in F1-measure diminishes when
disruption is considered, thus indicating the influ-
ence of the prior model diminishes. When the seg-
ments size decreases (see Figs. 2(b), 2(c), 2(d)), the
difference in the maximum F1-measure between our
results and that of the baseline lowers, however still
in favor of our model. This can be explained by the
fact that our approach is based on the distribution of
words, thus more words better help discriminate be-
tween potential thematic frontiers. Finally, using too
large values for ? can lead to under-segmentation, as
can be seen in Fig. 2(d) where, for ? = 3, the varia-
tion of F1-measure increases and the distribution be-
comes negatively skewed (i.e., the median is closer
to the third quartile than to the first).
These results are confirmed by Table 2 which
presents the gain in F1-measure (i.e., the differ-
ence between the highest F1-measure obtained when
1320
(a) (b)
(c) (d)
Figure 2: F1-measure variation obtained on Choi?s corpus. In each graphic, the leftmost boxplot UI corresponds to
results obtained by using the sole lexical cohesion (baseline), while the ? value is the importance given to the lexical
disruption in our approach. Results are provided for the same range of variation of factor ?, allowing a tolerance of 1
sentence between the hypothesized and reference frontiers.
combining lexical cohesion and disruption and the
highest value for the baseline) for each of the four
sets of documents in Choi?s corpus, together with
the 95 % confidence intervals: The effect of using
the disruption is higher when segment size is longer,
whether evaluation allows or not for a tolerance ?
between the hypothesized frontiers and the reference
ones. A qualitative analysis of the segmentations
obtained confirmed that employing disruption helps
eliminate wrong hypothesis and shift hypothesized
frontiers closer to the reference ones (explaining the
higher gain at tolerance 0 for 9-11 data set). When
smaller segments?thus few word repetitions?and
no tolerance are considered (e.g., 3?5), disruption
cannot improve segmentation. Our model is glob-
ally stable with respect to segment length, with rel-
atively similar gain for 3?11 and 6?8 data sets in
which the average number of words (distinct or re-
peated) is close.
Results discussed up to now are optimistic as they
correspond to the best F1 value attainable computed
a posteriori. Stability of the results was confirmed
z = 3?5 3?11 6?8 9?11
UI 91.9 87.0 93.1 92.8
Combined 92.9 87.5 93.5 94.0
Table 3: F1 results using cross-validation on Choi?s data
set.
using cross-validation with 5 folds (10 folds for
z=3?11): Parameters ? and ? maximizing the F1-
measure are determined on all but one fold, this last
fold being used for evaluation. Results, averaged
over all folds, are reported in Tab. 3 for the baseline
and the method combining cohesion and disruption.
Medical textbook corpus. The medical textbook
corpus was previously used for topic segmentation
by Eisenstein and Barzilay (2008) with their algo-
rithm BayesSeg2. We thus compare our results with
those obtained by BayesSeg and by the baseline.
When considering the best F1-measure (i.e., the best
F1-measure which can be achieved by varying ? and
2The code and the data set are available at
http://groups.csail.mit.edu/rbg/code/bayesseg/
1321
(a) (b)
Figure 3: Boxplots showing F1-measure variation on transcripts obtained using IRENE and LIMSI automatic speech
recognition systems.
?), we achieved an improvement of 2.2 with respect
to BayesSeg when no tolerance is allowed, and of
0.5 when the tolerance is of 1 sentence. The corre-
sponding figures with respect to the baseline are 0.6
and 0.4. When considering the F1-measure value
for which the number of hypothesized frontiers is
the closest to the number of reference boundaries,
improvement is of resp. 1.5 and 0.5 with respect to
BayesSeg, -0.1 and 0.4 with respect to the baseline.
These results show that our model combining lexi-
cal cohesion and disruption is also able to deal with
topic segmentation of corpora from a homogeneous
domain, with smooth topic changes and segments of
regular size.
One can argue that the higher number of free pa-
rameters in our method explains most of the gain
with respect to BayesSeg. While BayesSeg has only
one free parameter (as opposed to two in our case),
the number of segments is assumed to be provided
as prior knowledge. This assumption can be seen
as an additional free parameter, i.e., the number of
segments, and is a much stronger constraint than we
are using. Moreover, cross-validation experiments
on the Choi data set show that improvement is not
due to over-fitting of the development data thanks to
an additional parameter. Gains on development set
with parameters tuned on the development set itself
and with parameters tuned on a held-out set in cross-
validation experiments are in the same range.
TV news transcripts corpus Figure 3 provides
results, in terms of F1-measure variation, for TV
news transcripts obtained with the two ASR sys-
tems. On this highly challenging corpus, with short
segments, wrongly transcribed spoken words, and
thus few word repetitions, the capabilities of our
model to overcome the baseline system are reduced.
Yet, an improvement of the quality of the segmen-
tation of these noisy data is still observed, and
general conclusions are quite similar?though a bit
weaker?to those already made for Choi?s corpus.
Results are confirmed in Table 4 which presents the
gain in F1-measure of our model together with the
95 % confidence interval, where F1-measure values
correspond to that of segmentations with a num-
ber of hypothesized frontiers the closest to the ref-
erence. The two first lines show that the gain is
smaller for IRENE transcripts which have a higher
WER, thus fewer words available to discriminate be-
tween segments belonging to different topics. The
impact of transcription errors is illustrated in the
last three lines, when segmenting six TV news for
which manual reference transcripts were available
(line 3), where the higher the WER, the smaller the
F1-measure gain.
5 Conclusions
We have proposed a method to combine lexical co-
hesion and disruption for topic segmentation. Ex-
perimental results on various data sets with various
characteristics demonstrate the impact of taking into
account disruption in addition to lexical cohesion.
We observed gains both on data sets with segments
of regular length and on data sets exhibiting seg-
ments of highly varying length within a document.
Unsurprisingly, bigger gains were observed on doc-
1322
Corpus
F1 Confidence interval 95 %
gain UI Combined
IRENE 0.3 [54.4,57.6] [56.92,59]
LIMSI 0.86 [56.7,60.2] [59.44,61.95]
MANUAL (6) 0.77 [70.39,72.29] [71.7,73.29]
IRENE (6) 0.2 [56.81,60.94] [59.51,63.43]
LIMSI (6) 0.5 [64.27,68.64] [67.7,71.56]
Table 4: Gain in F1-measure for TV news corpus auto-
matic and manual transcripts when using lexical cohesion
and disruption, and the corresponding 95 % confidence
intervals. Last three rows report results on only 6 shows
for which manual reference transcripts are available.
uments containing relatively long segments. How-
ever the segmentation algorithm has proven to be
robust on automatic transcripts with short segments
and limited vocabulary reoccurrences. Finally, we
tested both abrupt topic changes and smooth ones
with good results on both. Further work can be con-
sidered to improve segmentation of documents char-
acterized by small segments and few words repe-
titions, such as using semantic relations or vector-
ization techniques to better exploit implicit relations
not considered by lexical reoccurrence.
References
Doug Beeferman, Adam Berger, and John Lafferty.
1997. Text segmentation using exponential models.
In 2nd Conference on Empirical Methods in Natural
Language Processing, pages 35?46.
Gillian Brown and George Yule. 1983. Discourse analy-
sis. Cambridge University Press.
Freddy Y. Y. Choi. 2000. Advances in domain inde-
pendent linear text segmentation. In 1st International
Conference of the North American Chapter of the As-
sociation for Computational Linguistics, pages 26?33.
Vincent Claveau and Se?bastien Lefe`vre. 2011. Topic
segmentation of TV-streams by mathematical mor-
phology and vectorization. In 12th International Con-
ference of the International Speech Communication
Association, pages 1105?1108.
Manolis Delakis, Guillaume Gravier, and Patrick Gros.
2008. Audiovisual integration with segment models
for tennis video parsing. Computer Vision and Image
Understanding, 111(2):142?154.
Jacob Eisenstein and Regina Barzilay. 2008. Bayesian
unsupervised topic segmentation. In Conference on
Empirical Methods in Natural Language Processing,
pages 334?343.
Olivier Ferret, Brigitte Grau, and Nicolas Masson. 1998.
Thematic segmentation of texts: Two methods for two
kinds of texts. In 36th Annual Meeting of the As-
sociation for Computational Linguistics and 17th In-
ternational Conference on Computational Linguistics,
pages 392?396.
Jean-Luc Gauvain, Lori Lamel, and Gilles Adda. 2002.
The LIMSI broadcast news transcription system.
Speech Communication, 37(1?2):89?108.
Barbara J. Grosz and Candace L. Sidner. 1986. Atten-
tion, intentions, and the structure of discourse. Com-
putational Linguistics, 12(3):175?204.
Barbara J. Grosz, Scott Weinstein, and Aravind K. Joshi.
1995. Centering: a framework for modeling the lo-
cal coherence of discourse. Computational Linguis-
tics, 21(2):203?225, June.
Camille Guinaudeau, Guillaume Gravier, and Pascale
Se?billot. 2012. Enhancing lexical cohesion measure
with confidence measures, semantic relations and lan-
guage model interpolation for multimedia spoken con-
tent topic segmentation. Computer Speech and Lan-
guage, 26(2):90?104.
Marti A. Hearst. 1997. TextTiling: Segmenting text into
multi-paragraph subtopic passages. Computational
Linguistics, 23(1):33?64.
Nicolas Hernandez and Brigitte Grau. 2002. Analyse
the?matique du discours : segmentation, structuration,
description et repre?sentation. In 5e colloque interna-
tional sur le document e?lectronique, pages 277?285.
Ste?phane Huet, Guillaume Gravier, and Pascale Se?billot.
2010. Morpho-syntactic post-processing of n-best lists
for improved French automatic speech recognition.
Computer Speech and Language, 24(4):663?684.
Xiang Ji and Hongyuan Zha. 2003. Domain-independent
text segmentation using anisotropic diffusion and dy-
namic programming. In 26th Annual International
ACM SIGIR Conference on Research and Develop-
ment in Information Retrieval, pages 322?329.
Diane J. Litman and Rebecca J. Passonneau. 1995. Com-
bining multiple knowledge sources for discourse seg-
mentation. In 33rd Annual Meeting of the Association
for Computational Linguistics, pages 108?115.
Igor Malioutov and Regina Barzilay. 2006. Minimum
cut model for spoken lecture segmentation. In 21st In-
ternational Conference on Computational Linguistics
and the 44th Annual Meeting of the Association for
Computational Linguistics, pages 25?32.
Hemant Misra, Franc?ois Yvon, Joemon M. Jose, and
Olivier Cappe. 2009. Text segmentation via topic
modeling: an analytical study. In Proc. ACM con-
ference on Information and knowledge management,
pages 1553?1556.
Marie-Francine Moens and Rik De Busser. 2001.
Generic topic segmentation of document texts. In 24th
1323
International Conference on Research and Develope-
ment in Information Retrieval, pages 418?419.
John Niekrasz and Johanna D. Moore. 2010. Unbiased
discourse segmentation evaluation. In Spoken Lan-
guage Technology, pages 43?48.
Mari Ostendorf, Vassilios V. Digalakis, and Owen A.
Kimball. 1996. From HMM?s to segment models: a
unified view of stochastic modeling for speech recog-
nition. IEEE Transactions on Speech and Audio Pro-
cessing, 4(5):360?378.
Lev Pevzner and Marti A. Hearst. 2002. A critique and
improvement of an evaluation metric for text segmen-
tation. Computational Linguistics, 28:19?36.
Jeffrey C. Reynar. 1994. An automatic method of finding
topic boundaries. In 32nd Annual Meeting on Associ-
ation for Computational Linguistics, pages 331?333.
Anca Simon, Guillaume Gravier, and Pascale Se?billot.
2013. Un mode`le segmental probabiliste combinant
cohe?sion lexicale et rupture lexicale pour la segmen-
tation the?matique. In 20e confe?rence Traitement Au-
tomatique des Langues Naturelles, pages 202?214.
Masao Utiyama and Hitoshi Isahara. 2001. A statistical
model for domain-independent text segmentation. In
39th Annual Meeting on the Association for Computa-
tional Linguistics, pages 499?506.
Kenneth H. Walker, Dallas W. Hall, and Willis J. Hurst.
1990. Clinical Methods: The History, Physical, and
Laboratory Examinations. Butterworths.
1324
