Unsupervised Relation Extraction from Web Documents
Kathrin Eichler, Holmer Hemsen and Gu?nter Neumann
DFKI GmbH, LT-Lab, Stuhlsatzenhausweg 3 (Building D3 2), D-66123 Saarbru?cken
{FirstName.SecondName}@dfki.de
Abstract
The IDEX system is a prototype of an interactive dynamic Information Extraction (IE) system. A user of the system
expresses an information request in the form of a topic description, which is used for an initial search in order to retrieve
a relevant set of documents. On basis of this set of documents, unsupervised relation extraction and clustering is done by
the system. The results of these operations can then be interactively inspected by the user. In this paper we describe the
relation extraction and clustering components of the IDEX system. Preliminary evaluation results of these components are
presented and an overview is given of possible enhancements to improve the relation extraction and clustering components.
1. Introduction
Information extraction (IE) involves the process of au-
tomatically identifying instances of certain relations of
interest, e.g., produce(<company>, <product>, <lo-
cation>), in some document collection and the con-
struction of a database with information about each
individual instance (e.g., the participants of a meet-
ing, the date and time of the meeting). Currently, IE
systems are usually domain-dependent and adapting
the system to a new domain requires a high amount
of manual labour, such as specifying and implement-
ing relation?specific extraction patterns manually (cf.
Fig. 1) or annotating large amounts of training cor-
pora (cf. Fig. 2). These adaptations have to be made
offline, i.e., before the specific IE system is actually
made. Consequently, current IE technology is highly
statical and inflexible with respect to a timely adapta-
tion to new requirements in the form of new topics.
Figure 1: A hand-coded rule?based IE?system (schemat-
ically): A topic expert implements manually task?specific
extraction rules on the basis of her manual analysis of a
representative corpus.
1.1. Our goal
The goal of our IE research is the conception and im-
plementation of core IE technology to produce a new
Figure 2: A data?oriented IE system (schematically): The
task?specific extraction rules are automatically acquired by
means of Machine Learning algorithms, which are using
a sufficiently large enough corpus of topic?relevant docu-
ments. These documents have to be collected and costly
annotated by a topic?expert.
IE system automatically for a given topic. Here, the
pre?knowledge about the information request is given
by a user online to the IE core system (called IDEX)
in the form of a topic description (cf. Fig. 3). This
initial information source is used to retrieve relevant
documents and extract and cluster relations in an un-
supervised way. In this way, IDEX is able to adapt
much better to the dynamic information space, in par-
ticular because no predefined patterns of relevant re-
lations have to be specified, but relevant patterns are
determined online. Our system consists of a front-end,
which provides the user with a GUI for interactively in-
specting information extracted from topic-related web
documents, and a back-end, which contains the rela-
tion extraction and clustering component. In this pa-
per, we describe the back-end component and present
preliminary evaluation results.
1.2. Application potential
However, before doing so we would like to motivate
the application potential and impact of the IDEX ap-
Figure 3: The dynamic IE system IDEX (schematically):
a user of the IDEX IE system expresses her information
request in the form of a topic description which is used for
an initial search in order to retrieve a relevant set of doc-
uments. From this set of documents, the system extracts
and collects (using the IE core components of IDEX) a set
of tables of instances of possibly relevant relations. These
tables are presented to the user (who is assumed to be the
topic?expert), who will analyse the data further for her in-
formation research. The whole IE process is dynamic, since
no offline data is required, and the IE process is interactive,
since the topic expert is able to specify new topic descrip-
tions, which express her new attention triggered by a novel
relationship she was not aware of beforehand.
proach by an example application. Consider, e.g., the
case of the exploration and the exposure of corruptions
or the risk analysis of mega construction projects. Via
the Internet, a large pool of information resources of
such mega construction projects is available. These
information resources are rich in quantity, but also
in quality, e.g., business reports, company profiles,
blogs, reports by tourists, who visited these construc-
tion projects, but also web documents, which only
mention the project name and nothing else. One of
the challenges for the risk analysis of mega construc-
tion projects is the efficient exploration of the possibly
relevant search space. Developing manually an IE sys-
tem is often not possible because of the timely need
of the information, and, more importantly, is proba-
bly not useful, because the needed (hidden) informa-
tion is actually not known. In contrast, an unsuper-
vised and dynamic IE system like IDEX can be used
to support the expert in the exploration of the search
space through pro?active identification and clustering
of structured entities. Named entities like for example
person names and locations, are often useful indicators
of relevant text passages, in particular, if the names are
in some relationship. Furthermore, because the found
relationships are visualized using an advanced graph-
ical user interface, the user can select specific names
and find associated relationships to other names, the
documents they occur in or she can search for para-
phrases of sentences.
2. System architecture
The back-end component, visualized in Figure 4, con-
sists of three parts, which are described in detail in this
section: preprocessing, relation extraction and relation
clustering.
2.1. Preprocessing
In the first step, for a specific search task, a topic of
interest has to be defined in the form of a query. For
this topic, documents are automatically retrieved from
the web using the Google search engine. HTML and
PDF documents are converted into plain text files. As
the tools used for linguistic processing (NE recogni-
tion, parsing, etc.) are language-specific, we use the
Google language filter option when downloading the
documents. However, this does not prevent some doc-
uments written in a language other than our target
language (English) from entering our corpus. In ad-
dition, some web sites contain text written in several
languages. In order to restrict the processing to sen-
tences written in English, we apply a language guesser
tool, lc4j (Lc4j, 2007) and remove sentences not clas-
sified as written in English. This reduces errors on
the following levels of processing. We also remove sen-
tences that only contain non-alphanumeric characters.
To all remaining sentences, we apply LingPipe (Ling-
Pipe, 2007) for sentence boundary detection, named
entity recognition (NER) and coreference resolution.
As a result of this step database tables are created,
containing references to the original document, sen-
tences and detected named entities (NEs).
2.2. Relation extraction
Relation extraction is done on the basis of parsing po-
tentially relevant sentences. We define a sentence to be
of potential relevance if it at least contains two NEs.
In the first step, so-called skeletons (simplified depen-
dency trees) are extracted. To build the skeletons, the
Stanford parser (Stanford Parser, 2007) is used to gen-
erate dependency trees for the potentially relevant sen-
tences. For each NE pair in a sentence, the common
root element in the corresponding tree is identified and
the elements from each of the NEs to the root are col-
lected. An example of a skeleton is shown in Figure 5.
In the second step, information based on dependency
types is extracted for the potentially relevant sen-
tences. Focusing on verb relations (this can be ex-
tended to other types of relations), we collect for each
verb its subject(s), object(s), preposition(s) with ar-
guments and auxiliary verb(s). We can now extract
verb relations using a simple algorithm: We define a
verb relation to be a verb together with its arguments
(subject(s), object(s) and prepositional phrases) and
consider only those relations to be of interest where at
least the subject or the object is an NE. We filter out
relations with only one argument.
2.3. Relation clustering
Relation clusters are generated by grouping relation
instances based on their similarity.
web documents document
retrieval
topic specific documents plain text documents
sentence/documents+
 NE tables
languagefiltering
syntactic +typed dependencyparsing 
sov?relationsskeletons +
clustering
conversion
Preprocessing
Relation extraction
Relation clustering
sentencesrelevant
filtering of
relationfiltering
table of clustered relations
sentence boundary
resolutioncoreference
detection,NE recognition,
Figure 4: System architecture
Figure 5: Skeleton for the NE pair ?Hohenzollern? and ?Brandenburg? in the sentence ?Subsequent members of
the Hohenzollern family ruled until 1918 in Berlin, first as electors of Brandenburg.?
The comparably large amount of data in the corpus
requires the use of an efficient clustering algorithm.
Standard ML clustering algorithms such as k-means
and EM (as provided by the Weka toolbox (Witten
and Frank, 2005)) have been tested for clustering the
relations at hand but were not able to deal with the
large number of features and instances required for an
adequate representation of our dataset. We thus de-
cided to use a scoring algorithm that compares a re-
lation to other relations based on certain aspects and
calculates a similarity score. If this similarity score ex-
ceeds a predefined threshold, two relations are grouped
together.
Similarity is measured based on the output from the
different preprocessing steps as well as lexical informa-
tion from WordNet (WordNet, 2007):
? WordNet: WordNet information is used to deter-
mine if two verb infinitives match or if they are in
the same synonym set.
? Parsing: The extracted dependency information is
used to measure the token overlap of the two sub-
jects and objects, respectively. We also compare
the subject of the first relation with the object of
the second relation and vice versa. In addition,
we compare the auxiliary verbs, prepositions and
preposition arguments found in the relation.
? NE recognition: The information from this step
is used to count how many of the NEs occurring
in the contexts, i.e., the sentences in which the
two relations are found, match and whether the
NE types of the subjects and objects, respectively,
match.
? Coreference resolution: This type of information
is used to compare the NE subject (or object) of
one relation to strings that appear in the same
coreference set as the subject (or object) of the
second relation.
Manually analyzing a set of extracted relation in-
stances, we defined weights for the different similarity
measures and calculated a similarity score for each re-
lation pair. We then defined a score threshold and clus-
tered relations by putting two relations into the same
cluster if their similarity score exceeded this threshold
value.
3. Experiments and results
For our experiments, we built a test corpus of doc-
uments related to the topic ?Berlin Hauptbahnhof?
by sending queries describing the topic (e.g., ?Berlin
Hauptbahnhof?, ?Berlin central station?) to Google
and downloading the retrieved documents specifying
English as the target language. After preprocessing
these documents as described in 2.1., our corpus con-
sisted of 55,255 sentences from 1,068 web pages, from
which 10773 relations were automatically extracted
and clustered.
3.1. Clustering
From the extracted relations, the system built 306 clus-
ters of two or more instances, which were manually
evaluated by two authors of this paper. 81 of our clus-
ters contain two or more instances of exactly the same
relation, mostly due to the same sentence appearing in
several documents of the corpus. Of the remaining 225
clusters, 121 were marked as consistent, 35 as partly
consistent, 69 as not consistent. We defined consis-
tency based on the potential usefulness of a cluster to
the user and identified three major types of potentially
useful clusters:
? Relation paraphrases, e.g.,
accused (Mr Moore, Disney, In letter)
accused (Michael Moore, Walt Disney
Company)
? Different instances of the same pattern, e.g.,
operates (Delta, flights, from New York)
offers (Lufthansa, flights, from DC)
? Relations about the same topic (NE), e.g.,
rejected (Mr Blair, pressure, from Labour
MPs)
reiterated (Mr Blair, ideas, in speech, on
March)
created (Mr Blair, doctrine)
...
Of our 121 consistent clusters, 76 were classified as be-
ing of the type ?same pattern?, 27 as being of the type
?same topic? and 18 as being of the type ?relation para-
phrases?. As many of our clusters contain two instances
only, we are planning to analyze whether some clusters
should be merged and how this could be achieved.
3.2. Relation extraction
In order to evaluate the performance of the relation ex-
traction component, we manually annotated 550 sen-
tences of the test corpus by tagging all NEs and verbs
and manually extracting potentially interesting verb
relations. We define ?potentially interesting verb rela-
tion? as a verb together with its arguments (i.e., sub-
ject, objects and PP arguments), where at least two
of the arguments are NEs and at least one of them
is the subject or an object. On the basis of this crite-
rion, we found 15 potentially interesting verb relations.
For the same sentences, the IDEX system extracted 27
relations, 11 of them corresponding to the manually
extracted ones. This yields a recall value of 73% and
a precision value of 41%.
There were two types of recall errors: First, errors in
sentence boundary detection, mainly due to noisy in-
put data (e.g., missing periods), which lead to parsing
errors, and second, NER errors, i.e., NEs that were
not recognised as such. Precision errors could mostly
be traced back to the NER component (sequences of
words were wrongly identified as NEs).
In the 550 manually annotated sentences, 1300 NEs
were identified as NEs by the NER component. 402
NEs were recognised correctly by the NER, 588
wrongly and in 310 cases only parts of an NE were
recognised. These 310 cases can be divided into three
groups of errors. First, NEs recognised correctly, but
labeled with the wrong NE type. Second, only parts
of the NE were recognised correctly, e.g., ?Touris-
mus Marketing GmbH? instead of ?Berlin Tourismus
Marketing GmbH?. Third, NEs containing additional
words, such as ?the? in ?the Brandenburg Gate?.
To judge the usefulness of the extracted relations, we
applied the following soft criterion: A relation is con-
sidered useful if it expresses the main information given
by the sentence or clause, in which the relation was
found. According to this criterion, six of the eleven
relations could be considered useful. The remaining
five relations lacked some relevant part of the sen-
tence/clause (e.g., a crucial part of an NE, like the
?ICC? in ?ICC Berlin?).
4. Possible enhancements
With only 15 manually extracted relations out of 550
sentences, we assume that our definition of ?potentially
interesting relation? is too strict, and that more inter-
esting relations could be extracted by loosening the ex-
traction criterion. To investigate on how the criterion
could be loosened, we analysed all those sentences in
the test corpus that contained at least two NEs in order
to find out whether some interesting relations were lost
by the definition and how the definition would have to
be changed in order to detect these relations. The ta-
ble in Figure 6 lists some suggestions of how this could
be achieved, together with example relations and the
number of additional relations that could be extracted
from the 550 test sentences.
In addition, more interesting relations could be
found with an NER component extended by more
types, e.g., DATE and EVENT. Open domain NER
may be useful in order to extract NEs of additional
types. Also, other types of relations could be inter-
esting, such as relations between coordinated NEs,
option example additional relations
extraction of relations,
where the NE is not the
complete subject, object or
PP argument, but only part
of it
Co-operation with <ORG>M.A.X.
2001<\ORG> <V>is<\V> clearly of
benefit to <ORG>BTM<\ORG>.
25
extraction of relations with
a complex VP
<ORG>BTM<\ORG> <V>invited and or
supported<\V> more than 1,000 media rep-
resentatives in <LOC>Berlin<\LOC>.
7
resolution of relative pro-
nouns
The <ORG>Oxford Centre for Maritime
Archaeology<\ORG> [...] which will
<V>conduct<\V> a scientific symposium in
<LOC>Berlin<\LOC>.
2
combination of several of the
options mentioned above
<LOC>Berlin<\LOC> has <V>developed to
become<\V> the entertainment capital of
<LOC>Germany<\LOC>.
7
Figure 6: Table illustrating different options according to which the definition of ?potentially interesting relation?
could be loosened. For each option, an example sentence from the test corpus is given, together with the number
of relations that could be extracted additionally from the test corpus.
e.g., in a sentence like The exhibition [...] shows
<PER>Clemens Brentano<\PER>, <PER>Achim
von Arnim<\PER> and <PER>Heinrich von
Kleist<\PER>, and between NEs occurring in the
same (complex) argument, e.g., <PER>Hanns Peter
Nerger<\PER>, CEO of <ORG>Berlin Tourismus
Marketing GmbH (BTM) <\ORG>, sums it up [...].
5. Related work
Our work is related to previous work on domain-
independent unsupervised relation extraction, in par-
ticular Sekine (2006), Shinyama and Sekine (2006) and
Banko et al (2007).
Sekine (2006) introduces On-demand information ex-
traction, which aims at automatically identifying
salient patterns and extracting relations based on these
patterns. He retrieves relevant documents from a
newspaper corpus based on a query and applies a POS
tagger, a dependency analyzer and an extended NE
tagger. Using the information from the taggers, he ex-
tracts patterns and applies paraphrase recognition to
create sets of semantically similar patterns. Shinyama
and Sekine (2006) apply NER, coreference resolution
and parsing to a corpus of newspaper articles to ex-
tract two-place relations between NEs. The extracted
relations are grouped into pattern tables of NE pairs
expressing the same relation, e.g., hurricanes and their
locations. Clustering is performed in two steps: they
first cluster all documents and use this information to
cluster the relations. However, only relations among
the five most highly-weighted entities in a cluster are
extracted and only the first ten sentences of each arti-
cle are taken into account.
Banko et al (2007) use a much larger corpus, namely
9 million web pages, to extract all relations between
noun phrases. Due to the large amount of data, they
apply POS tagging only. Their output consists of mil-
lions of relations, most of them being abstract asser-
tions such as (executive, hired by, company) rather
than concrete facts.
Our approach can be regarded as a combination of
these approaches: Like Banko et al (2007), we extract
relations from noisy web documents rather than com-
parably homogeneous news articles. However, rather
than extracting relations from millions of pages we re-
duce the size of our corpus beforehand using a query in
order to be able to apply more linguistic preprocessing.
Like Sekine (2006) and Shinyama and Sekine (2006),
we concentrate on relations involving NEs, the assump-
tion being that these relations are the potentially in-
teresting ones. The relation clustering step allows us
to group similar relations, which can, for example, be
useful for the generation of answers in a Question An-
swering system.
6. Future work
Since many errors were due to the noisiness of the ar-
bitrarily downloaded web documents, a more sophisti-
cated filtering step for extracting relevant textual infor-
mation from web sites before applying NE recognition,
parsing, etc. is likely to improve the performance of
the system.
The NER component plays a crucial role for the qual-
ity of the whole system, because the relation extraction
component depends heavily on the NER quality, and
thereby the NER quality influences also the results of
the clustering process. A possible solution to improve
NER in the IDEX System is to integrate a MetaNER
component, combining the results of several NER com-
ponents. Within the framework of the IDEX project
a MetaNER component already has been developed
(Heyl, to appear 2008), but not yet integrated into the
prototype. The MetaNER component developed uses
the results from three different NER systems. The out-
put of each NER component is weighted depending on
the component and if the sum of these values for a pos-
sible NE exceeds a certain threshold it is accepted as
NE otherwise it is rejected.
The clustering step returns many clusters containing
two instances only. A task for future work is to in-
vestigate, whether it is possible to build larger clus-
ters, which are still meaningful. One way of enlarging
cluster size is to extract more relations. This could
be achieved by loosening the extraction criteria as de-
scribed in section 4. Also, it would be interesting to see
whether clusters could be merged. This would require
a manual analysis of the created clusters.
Acknowledgement
The work presented here was partially supported by a
research grant from the?Programm zur Fo?rderung von
Forschung, Innovationen und Technologien (ProFIT)?
(FKZ: 10135984) and the European Regional Develop-
ment Fund (ERDF).
7. References
Michele Banko, Michael J. Cafarella, Stephen Soder-
land, Matthew Broadhead, and Oren Etzioni. 2007.
Open information extraction from the web. In Proc.
of the International Joint Conference on Artificial
Intelligence (IJCAI).
Andrea Heyl. to appear 2008. Unsupervised relation
extraction. Master?s thesis, Saarland University.
Lc4j. 2007. Language categorization library for Java.
http://www.olivo.net/software/lc4j/.
LingPipe. 2007. http://www.alias-i.com/lingpipe/.
Satoshi Sekine. 2006. On-demand information extrac-
tion. In ACL. The Association for Computer Lin-
guistics.
Yusuke Shinyama and Satoshi Sekine. 2006. Preemp-
tive information extraction using unrestricted re-
lation discovery. In Proc. of the main conference
on Human Language Technology Conference of the
North American Chapter of the Association of Com-
putational Linguistics, pages 304?311. Association
for Computational Linguistics.
Stanford Parser. 2007. http://nlp.stanford.edu/
downloads/lex-parser.shtml.
Ian H. Witten and Eibe Frank. 2005. Data Min-
ing: Practical machine learning tools and techniques.
Morgan Kaufmann, San Francisco, 2nd edition.
WordNet. 2007. http://wordnet.princeton.edu/.
Proceedings of the 2010 Workshop on NLP and Linguistics: Finding the Common Ground, ACL 2010, pages 10?17,
Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational Linguistics
Evidentiality for Text Trustworthiness Detection 
 
 
 
Qi Su1, 2, Chu-Ren Huang and Helen Kai-yun Chen 
1Depart of Chinese & Bilingual Studies, The Hong Kong Polytechnic University 
2Key Laboratory of Computational Linguistics, Peking University 
sukia@pku.edu.cn, {helenkychen, churen.huang}@gmail.com 
 
  
 
Abstract 
 
Evidentiality is the linguistic representation of 
the nature of evidence for a statement. In 
other words, it is the linguistically encoded 
evidence for the trustworthiness of a state-
ment. In this paper, we aim to explore how 
linguistically encoded information of eviden-
tiality can contribute to the prediction of 
trustworthiness in natural language processing 
(NLP). We propose to incorporate evidential-
ity into a framework of machine learning 
based text classification. We first construct a 
taxonomy of evidentials. Then experiments 
involving collaborative question answering 
(CQA) are designed and implemented using 
this taxonomy. The experimental results con-
firm that evidentiality is an important clue for 
text trustworthiness detection. With the bi-
narized vector setting, evidential based text 
representation model has considerably per-
formaned better than both the bag-of-word 
model and the content word based model. 
Most crucially, we show that the best trust-
worthiness detection result is achieved when 
evidentiality is incorporated in a linguistically 
sophisticated model where their meanings are 
interpreted in both semantic and pragmatic 
terms. 
1 Introduction 
With the exponential increase in web sites and 
documents, the amount of information is no 
longer a main concern for automatic knowledge 
acquisition. This trend raises, however, at least 
two new issues. The first is how to locate the 
information which exactly meets our needs 
among the vast web content. Efforts to address 
this issue can be exemplified by advanced re-
search in information retrieval, information ex-
traction, etc. The second is how to judge the va-
lidity of the acquired information, that is, the 
trustworthiness of information. This issue has 
attracted considerable interest in some related 
research areas recently. Taking the specific in-
formation retrieval task, question answering 
(QA) as an example, a QA system attempts to 
retrieve the most appropriate answers to ques-
tions from web resources. To determine the 
trustworthiness of the extracted candidate an-
swers, a common approach is to exploit the co-
occurrence frequency of questions and candidate 
answers. That is, if a candidate answer co-occurs 
more frequently with the question than other 
candidates, the QA system may judge it as the 
best answer (Magnini, 2002). This approach pre-
supposes and relies crucially on information re-
dundancy. Although this heuristic method is 
simple and straightforward, it is not applicable 
to all cases. For the applications which don?t 
involve much information redundancy, the heu-
ristic could cease to be effective. The task of 
collaborative question answering (CQA) which 
we will address in this paper is just one of such 
examples. For a user posted question, there are 
usually only few answers provided. So, the heu-
ristic is not useful in providing the best answer. 
In addition, since the spread of unsubstantiated 
rumors on the Internet is so pervasive, the high-
frequency information on the Web sometimes 
may mislead the judgment of trustworthiness. In 
terms of the above consideration, it is essential 
to look for other approaches which allow di-
rectly modeling of the trustworthiness of a text.  
Given that non-textual features (such as user's 
Web behavior) used in text trustworthiness de-
tection are often manipulated by information 
providers, as well as no directly related textual 
features for the task has been proposed up to 
10
date, we need a more felicitous model for detect-
ing the trustworthiness of statements. Noting 
that evidentiality is often linguistically encoded 
and hence provides inherent information on 
trustworthiness for a statement, we propose to 
incorporate the linguistic model of evidentiality 
in our study. Specifically, we incorporate evi-
dentiality into a machine learning based text 
classification framework, and attempt to verify 
the validity of evidentiality in trustworthiness 
prediction of text information in the context of 
collaborative question answering. The experi-
mental results show that evidentials are impor-
tant clues in predicting the trustworthiness of 
text. Since none of the task-specific heuristics 
has been incorporated, the current approach 
could also be easily adapted to fit other natural 
language processing applications. 
The paper proceeds as follows. In section 2 
we discuss related work on text trustworthiness 
detection. The section is divided into two parts: 
the current methodology and the textual features 
for analysis in the task. Section 3 introduces the 
linguistic researches on evidentiality and our 
taxonomy of evidentials based on the trustwor-
thiness indication. Section 4 presents the ex-
periment settings and results. Finally, in section 
5 we discuss the experiment results and con-
clude the current research. 
2 Related Work 
The research of text trustworthiness is very 
helpful for many other natural language process-
ing applications. For example, in their research 
on question answering, Banerjee and Han (2009) 
modulate answer grade by using a weighted 
combination of the original score and answer 
credibility evaluation. Also, Weerkamp and Ri-
jke (2008) incorporate textual credibility indica-
tors in the retrieval process to improve topical 
blog posts retrieval. Gyongyi et al(2004) pro-
pose a TrustRank algorithm for semi-
automatically separating reputable, good Web 
pages from spams. 
2.1 General Approaches for Text Trust-
worthiness Detection 
In past research, the judgment for the trustwor-
thiness or credibility of a given text content is 
usually tackled from two aspects: entity oriented 
and content oriented (Rubin and Liddy, 2005). 
The former approach takes into consideration 
the information providers? individual profiles, 
such as their identity, reputation, authority and 
past web behavior; whereas the latter approach 
considers the actual content of texts. Metzger 
(2007) reviews several cognitive models of 
credibility assessment and points out that credi-
bility is a multifaceted concept with two primary 
dimensions: expertise and trustworthiness. Fol-
lowing Matzger?s framework, Rubin and Liddy 
(2005) compile a list of factors that users may 
take into account in assessing credibility of blog 
sites. This list could also be summarized as the 
above mentioned two-folds: the bloggers? pro-
files and the information posted in the entries.  
Comparing these two aspects, most existing 
research on text trustworthiness focuses on the 
user oriented features. Lots of user oriented fea-
tures have been proposed in the research of 
credibility detection. To score the user oriented 
features such as user?s authority, a common ap-
proach is based on a graph-based ranking algo-
rithm such as HITS and PageRank (Zhang et al 
2007; Bouguessa et al 2008).  
In the research of text trustworthiness detec-
tion, the overwhelmingly adaption of non-
textual features such as entity profiles over text 
content based features reflect some researchers? 
belief that superficial textual features cannot 
meet the need of text credibility identification 
(Jeon et al 2006). In this paper, we examine the 
lexical semantic feature of evidential and argue 
that evidentiality, as a linguistically instantiated 
representation of quality of information content, 
offers a robust processing model for text trust-
worthiness detection. 
The detection of information trustworthiness 
also has promising application values. Google 
News 1  is just such an application that ranks 
search results according to the credibility of the 
news. Other online news aggregation service, 
such as NewTrust 2, also focuses on providing 
users with credible and high quality news and 
stories. The existed applications, however, rely 
on either the quality of web sites or user voting.  
So, it is anticipated that the improvement on the 
technology of text trustworthiness detection by 
incorporating lexical semantic cues such as evi-
dentiality may shed light on these applications. 
2.2 Textual Feature Based Text Trustwor-
thiness Detection 
Although non-textual features have been popular 
in text credibility detection, there has been a few 
research focusing on textual features so far. Gil 
                                               
1 http://news.google.com/ 
2 http://www.newstrust.net/ 
11
and Artz (2006) argue that the degree of trust in 
an entity is only one ingredient in deciding 
whether or not to trust the information it pro-
vides. They further point out that entity-centered 
issues are made with respect to publicly avail-
able data and services, and thus will not be pos-
sible in many cases. In their research of topical 
blog posts retrieval, Weerkamp and Rijke (2008) 
also consider only textual credibility indicators 
since they mentioned that additional resources 
(such as bloggers? profiles) is hard to obtain for 
technical or legal reasons.   
However, most research which utilizes textual 
features in text trustworthiness detection usually 
equates writing quality of document with its 
trustworthiness. Therefore, some secondary fea-
tures which may not directly related to trustwor-
thiness are proposed, including spelling errors, 
the lack of leading capitals, the large number of 
exclamation markers, personal pronouns and 
text length (Weerkamp and Rijke, 2008). There 
has not been attempted to directly evaluate in-
herent linguistic cues for trustworthiness of a 
statement. 
3 On Evidentiality in Text 
Evidentiality, as an explicit linguistic system to 
encode quality of information, offers obvious 
and straightforward evidence for text trustwor-
thiness detection. Yet it has not attracted the at-
tention which it deserves in most of the natural 
language processing studies. In this paper, we 
aim to explore how we can incorporate the lin-
guistic model of evidentiality into a robust and 
efficient machine learning based text classifica-
tion framework.   
Aikhenvald (2003) observes that every lan-
guage has some way of making reference to the 
source of information. Once the language is be-
ing used, it always imprinted with the subjective 
relationship from the speakers towards the in-
formation. Evidentiality is information provid-
ers? specifications for the information sources 
and their attitudes toward the information. As a 
common linguistic phenomenon to all the lan-
guages, it has attracted linguists? attention since 
the beginning of 20th century. In any language, 
evidentiality is a semantic category which could 
be expressed on both grammatical level (as in 
some American Indian language) and lexical 
level (as in English, Chinese and many other 
languages). The linguistic expressions of eviden-
tiality are named as evidentials or evidential 
markers.  
Mushin (2000) defines evidential as a marker 
which qualifies the reliability of information.  It 
is an explicit expression of the speaker?s atti-
tudes toward the trustworthiness of information 
source. For instance, 
a). It?s probably raining. 
b). It must be raining. 
c). It sounds like it?s raining. 
d). I think/guess/suppose it?s raining. 
e). I can hear/see/feel/smell it raining. 
It is obvious that the information provided in 
the above examples is subjective. The informa-
tion expresses the personal experience or atti-
tudes, while at the same time reflects the speak-
ers? estimation for the trustworthiness of the 
statement by information providers. 
3.1 The Definition of Evidentiality 
There are two dimensions of the linguistic defi-
nition for evidentiality. The term evidentiality is 
originally introduced by Jakobson (1957) as a 
label for the verbal category indicating the al-
leged source of information about the narrated 
events. In line with Jakobson?s definition, the 
narrow definition of evidentiality proposed by 
other researchers focuses mainly on the specifi-
cation of the information sources, that is, the 
evidence through which information is acquired 
(DeLancey, 2001). Comparing with the narrow 
definition, the board definition explains eviden-
tiality in a much wider sense, and characterizes 
evidentiality as expressions of speaker?s attitude 
toward information, typically expressed by mo-
dalities (Chafe, 1986; Mushin, 2000). 
Ifantidou (2001) also holds that evidential has 
two main functions: 1) indicating the source of 
knowledge; 2) indicating the speaker?s degree of 
certainty about the proposition expressed. He 
further divides them in details as follows. 
a) Information can be acquired in various 
ways, including observation (e.g. see), hearsay 
(e.g. hear, reportedly), inference (e.g. must, de-
duce), memory (e.g. recall). 
b) Evidentiality can indicate the speaker?s de-
gree of certainty, including certain propositional 
attitude (e.g. think, guess) and adverbials (e.g. 
certainly, surely), also epistemic models (e.g. 
may, ought to). 
3.2 The Taxonomy of Evidentials 
Evidentiality has its hierarchy which forms a 
continuum that marks from the highest to the 
least trustworthiness. Up to now, there are many 
hierarchical schemes proposed by researchers. 
12
 
Table 1. The Categorization and Inside Items of Evidentiality 
 
Oswalt (1986) suggests a priority hierarchy of 
evidentials as: 
Performative > Factual > Visual > Auditory > 
Inferential > Quotative 
In this evidential hierarchy, performative car-
ries the highest degree of trustworthiness since 
Oswalt considers that the speaker is speaking of 
the act he himself is performing. It is the most 
reliable source of evidence for the knowledge of 
that event. 
Whereas Barners (1984) proposes the follow-
ing hierarchy: 
Visual > Non-visual > Apparent > Second-
hand > Assumed 
He points out that visual evidence takes 
precedence over the auditory evidence and is 
more reliable.  
The above two hierarchies are based on the 
narrow definition of evidentiality mentioned 
above. There are also some hierarchies involving 
the board definition of evidentiality, such as 
Chafe (1986)?s categories of evidentiality.   
In this paper, we adopt a broad definition of 
evidentiality and focus on a trustworthiness 
categorization. This categorization follows the 
model of four-dimensional certainty categoriza-
tion by Rubin et al(2005). In this model, it is 
suggested that the division of the certainty level 
dimension into four categories - Absolute, High, 
Moderate and Low. With some revision, there 
are different items of evidential words and 
phrased that we extracted from the corpus. 
These items from each category to be adopted in 
our experiments are presented in Table 1. 
4 Incorporating Evidentiality into Ma-
chine Learning for Trustworthiness 
Detection 
In this section, we apply evidentiality in an ac-
tual implement of text trustworthiness detection. 
It is based on a specific web application service, 
collaborative question answering (CQA), in 
which the trustworthiness of text content is very 
helpful for finding the best answers in the ser-
vice.  
With the development of Web2.0, the services 
of CQA in community media have largely at-
tracted people?s attention. Comparing with the 
general ad hoc information searching, question 
answering could help in finding the most accu-
rate answers extracted from the vast web content. 
Whereas in the collaborative question answering, 
the CQA community media just provide a web 
space in which users can freely post their ques-
tions, and at the same time other users may an-
swer these questions based on their knowledge 
and interests. Due to the advantage of interactiv-
ity, CQA usually could settle some questions 
which cannot be dealt with by ad hoc informa-
tion retrieval. However, since the platform is 
open to anyone, the quality of the answers pro-
vided by users is hard to identify. People may 
present answers of various qualities due to the 
limitation of their knowledge, attitude and pur-
pose of answering the questions. As a result, the 
issue of how to identify the most trustworthy 
answers from the user-provided content turns 
out to be the most challenging part to the system. 
As mentioned previously, the trustworthiness 
of text content could be identified from two di-
mensions. The first one relies on the features 
related with information distributors. The second 
one relies on the content of a text. In current re-
search we focus on textual features, especially 
the feature of evidentiality in texts. The feature 
will be incorporated into a machine learning 
based text classification framework in order to 
identify the best answers for CQA questions.  
 Absolute High Moderate Low 
Attributive/modal 
adverb 
certainly, sure, of 
course, definitely, ab-
solutely, undoubtedly 
clearly, obviously, ap-
parently, really, always 
 Seemingly, 
probably  
maybe, personally, 
perhaps, possibly, 
presumably 
Lexical verb 
report, certain believe, see seem, think, sound doubt, wish, wonder, 
infer, assume, fore-
cast, fell, heard 
Auxiliary verb  must ought, should, would, could, can 
may, might 
Epistemic adjec-
tive 
definite  possible, likely, 
unlikely, probable, 
positive, potential 
not sure, doubtful 
13
4.1 The Dataset 
For the experiments, we use the snapshot of Ya-
hoo! Answers dataset which is crawled by Emo-
ry University3. Since our experiments only in-
volve text features, we use the answer parts from 
it without considering the question sets and user 
profiles. Such information could be incorporated 
to achieve a higher performance in the future.  
With regard to the text classification problems, 
there is typically a substantial class distribution 
skew (Forman, 2003). For the Yahoo! Answers 
dataset, a question only has one best answer and 
accordingly all the other answers will be marked 
as non-best answers. Thus the class of best an-
swer contains much fewer texts than the class of 
non-best answers. In our dataset (a proportion of 
the overall CQA dataset provided by Emory 
University), the number of best answers is 2,165, 
and the number of non-best answers is 17,654. 
The proportion of the size of the two answer sets 
is around 1:8.15, showing a significant skews. 
For a better comparison of experimental results, 
we use a balanced dataset which is generated 
from a normal distribution dataset.  
A 10-fold validation is used for the evaluation, 
where the datasets of best and non-best answers 
are divided into 10 subsets of approximately 
equal size respectively. In the normally distrib-
uted dataset, we use one of the ten subsets as the 
test set, while the other nine are combined to-
gether to from the training set. In the balanced 
dataset, for each subset of the non-best answers, 
we only use the first k answers, in which k is the 
size of each subset of best answers. The training 
data and test data used in the machine learning 
process are shown in Table 2. 
 
 Training 
/Test Set 
Best  
answer 
Non-best 
 answer 
training 19,490 158,889 normal distribution 
dataset test 2,165 17,654 
training 19,490 19,490 balanced 
dataset test 2,165 2,165 
 
Table 2. The Dataset Used for the Experiments  
4.2 Experiment Settings 
To conduct a machine learning based classifica-
tion for best answers and non-best answers, we 
first need to construct the feature vectors. The 
representation of text is the core issue in the ma-
                                               
3 http://ir.mathcs.emory.edu/shared 
chine learning model for text classification. In 
text domains, feature selection plays an essential 
role to make the learning task efficient and more 
accurate. As the baseline comparison, we use the 
following feature vector settings. 
?Baseline1 represents using all the words in 
the text as features (when the frequency of the 
word in the dataset is bigger than a predefined 
threshold j). 
? Baseline2 represents using all the content 
words (here we include the four main categories 
of content words - nouns, verbs, adjectives and 
adverbs identified by a POS tagger) in the data-
set as features. 
We use both the above two baselines. The 
bag-of-word model of Baseline1 is a conven-
tional method in text representation. However, 
since not all the words are linguistically signifi-
cant, in Baseline2, we consider only the content 
words in the dataset, since content words convey 
the core meaning of a sentence.  
For the evidentiality-based classification, we 
adopt the following feature vector settings.  
?Evidential represents using all the evidentials 
in text as features. 
?Evidential? represents using all the eviden-
tials except for those in the category of Moder-
ate as features. 
?Evid.cat4 represents using the four eviden-
tiality categories of Absolute, High, Moderate 
and Low from Table 1. 
?Evid.cat2 represents using the two categories 
of Absolute and High as the positive evidential 
and Moderate and Low as the negative evidential. 
?Evid.cat2? omits the evidential category of 
Moderate, and represents using the two catego-
ries of Absolute and High as the positive eviden-
tial and only the category of Low as the negative 
evidential feature. 
Some researchers have proved that usually a 
Boolean indicator of whether the feature item 
occurred in the document is sufficient for classi-
fication (Forman, 2003). Although there are also 
some other feature weighting schemes such as 
term frequency (TF), document frequency (DF), 
etc, comparison of these different weighting 
schemes is not the object of the current research. 
So in this paper, we only consider Boolean 
weighting. In the Boolean text representation 
model, each feature represents the Boolean oc-
currence of a word, evidential, or evidential cat-
egory according to the different feature settings. 
By the experimental settings, we want to verify 
the hypothesis that incorporating the knowledge 
14
of evidentiality into text representation can lead 
to improvement in classification performance. 
In our experiment, we perform text preproc-
essing including word segmentation and part-of-
speech (POS) tagging. The Stanford Log-linear 
Part-Of-Speech Tagger (http://nlp.stanford.edu/ 
software/tagger.shtml) is used for POS tagging. 
We adopt support vector machine (SVM) as the 
machine learning model to classify best answers 
from non-best ones, and use the SVMlight pack-
age (http://svmlight.joachims.org) as the classi-
fier with the default parameters and a linear ker-
nel. For the evaluation, we use the metrics of 
precision (Prec. as in table 3), recall (Rec. as in 
table 3), accuracy (Acc. as in table 3) and F1: 
F1-measure, the harmonic mean of the precision 
and recall. 
4.3 Evaluation 
Table 3 shows the experimental results using the 
balanced dataset with Boolean weighting. The 
focus of the experiment evaluation is on identi-
fying the best answers, so the evaluation metrics 
are all for the best answers collection. From the 
table, we see increases of the two feature vector 
setting of evidentials over both baseline results. 
The highest improvement is 14.85%, achieved 
by the feature set of Evidential?. However, there 
is no increase found in the settings of using evi-
dential categories. This means that although the 
category of evidentials in indicating text trust-
worthiness is obvious for human, it is not neces-
sary a preferred feature for machine learning. 
 
 Prec. Rec. Acc. F1 
Baseline1 45.62% 51.51% 45.15% 47.94% 
Baseline2 59.58% 39.20% 56.30% 47.28% 
Evidential 67.78% 44.18% 61.59% 53.49% 
Evidential? 47.40% 90.12% 45.06% 62.13% 
Evid.cat4 64.15% 25.85% 55.70% 36.85% 
Evid.cat2 60.86% 28.21% 55.03% 38.55% 
Evid.cat2? 40.35% 25.85% 43.81% 31.51% 
 
Table 3. Experimental Results Using the Bal-
anced Training/Test Dataset (with Boolean 
Weighting) 
 
To eliminate the potential effect of term 
weighting scheme on performance trend among 
different text representation models, we also 
conduct experiments using TF weighting. By the 
experiments, we aim to compare the relative per-
formances of different feature vectors con-
structed with evidentials, and the results are 
demonstrated in Table 4. 
 
 Prec. Rec. Acc. F1 
Evidential 66.78% 45.57% 61.45% 54.17% 
Evidential? 59.66% 20.82% 53.37% 30.87% 
Evid.cat4 50.00% 18.14% 50.00% 26.63% 
Evid.cat2 55.91% 16.39% 51.73% 25.35% 
 
Table 4. Experimental Results Using the Bal-
anced Training/Test Dataset (with TF Weighting) 
 
From the table, it can be observed that using 
evidentials as features shows better improve-
ment in the performance than the category of 
evidentials as a feature. A similar performance 
has been summarized in Table 3. 
Finally, but not the least, to better understand 
the effect of evidential category on the machine 
learning performance, we design additional ex-
periments as follows. 
?Evid_cat1 stands for combining the four evi-
dential categories into one, and uses only this 
one category of evidential as a feature. The ap-
proach of Boolean weighting is actually the 
same as a rule-based approach that classifies the 
test dataset according to whether evidential oc-
curs or not. 
 
BOOL Prec. Rec. Acc. F1 
Evid_cat1 59.42% 61.59% 59.76% 60.49% 
 
Table 5. Experimental Results Using the Bal-
anced Training/Test Dataset (with Boolean 
Weighting; Only One Evidential Category) 
 
Table 5 presents a set of interesting experi-
mental result. In the result, all the four evalua-
tion metrics show performance increases com-
paring to the baseline, and it even outperforms 
almost all the other results from both weighting 
schemes. Based on this result, it is suggested 
that evidentiality still may contribute to the task 
of text trustworthiness detection. Moreover, it 
can significantly reduce the dimensionality of 
feature space (e.g. for Baseline 1, the dimen-
sionality of feature dimension is 218,328 in one 
of our cases; while for the experiment of Evi-
dential, it reduced to only 51 as shown in Table 
15
1). However, we should address the question of 
why not all types of evidential features demon-
strate improvement of detection. We will further 
discuss the issue from a  pragmatic viewpoint in 
the next section. 
5 Conclusion and Discussion 
In this paper, we propose to incorporate the lin-
guistic knowledge of evidentiality in the NLP 
task of trustworthiness prediction. As evidential-
ity is an integral and inherent part of any state-
ment and explicitly expresses information about 
the trustworthiness of this statement, it should 
provide the most robust and direct model for 
trustworthiness detection. We first set up the 
taxonomy of lexical evidentials. By incorporat-
ing evidentiality into a machine learning based 
text classification framework, we conduct ex-
periments for a specific application, CQA.  The 
evidentials in the dataset are extracted to form 
different text representation schemes. Our ex-
perimental results using evidentials show im-
provements up to 14.85% over the baselines. 
However, not all types of evidential features 
contributed to the improvement of detection. We 
also compared the effect of different types of 
evidential based feature representation schemes 
on the classification performance.  
The way to model evidentiality for trustwor-
thiness detection which we adopted in our initial 
experiment design actually could also be ex-
plained by Grice?s Maxim of Quality: be truthful. 
As the Maxim of Quality requires one ?not to 
say that for which one lacks adequate evidence?, 
we hypothesize that evidential constructions 
mark the adequacy of evidence and should indi-
cate reliable answers. However, the results from 
our experiments only partially supported this 
hypothesis. The results showed a satisfactory 
performance was achieved when all evidential 
markers were treated as negative evidence for 
reliability. This result could then be accounted 
by invoking another Gricean maxim: Quantity. 
The Maxim of Quantity requires that ?one 
makes his/her contribution as informative as is 
required, and at the same time does not make the 
contribution more informative than is required.? 
As evidentiality is not grammaticalized in Eng-
lish, the use of evidentiality is not a required 
grammatical element. An answer marked by evi-
dentials would violate Maxim of Quantity if it is 
correct. The Maxim of Quantity predicts that 
good answers are plain statements without evi-
dential markers. On the frequent use of eviden-
tial markers for less reliable answers can be ac-
counted for by speakers? attempt to follow both 
Maxims of Quality and Quantity. The evidential 
marks are used to compensate for the fact that 
speakers are not very confident about the answer, 
yet would like to adhere to the Maxim of Quality. 
In other words, evidentials are not likely to be 
used in reliable answers because of the Maxim 
of Quality, but it is likely used in less reliable 
answers because the speakers may try to provide 
proof of adequate evidence by a grammatical 
device instead of providing true answer. 
Therefore, this model elaborated above takes 
into account not only the grammatical function 
of evidential constructions but also how this lin-
guistic structure is used as a pragmatic/discourse 
device. In other words, this study suggests that 
modeling linguistic theory in NLP needs to take 
a more comprehensive approach than the simple 
modular approach where only one module 
(based on evidentiality) is used. Linguistic mod-
eling needs to consider both how linguistic 
structure/knowledge is represented and proc-
essed, we also need to model how a particular 
linguistic device in use. 
In the further works, we plan to continue de-
veloping and elaborate on a multi-modular lin-
guistic model of evidentiality for knowledge 
acquisition. We will also explore the possibility 
of incorporating other features, both textual and 
non-textual, to further improve performances in 
the tasks of text trustworthiness detection. 
References  
Agichtein E, Castillo C, and etc. 2008. Finding high-
quality content in social media. In Proceedings of  
WSDM2008. 
Aikhenvald A and Dixon, ed. 2003. Studies in evi-
dentiality. Amsterdam/Philadelphia: John Benja-
mins Publishing Company 
Banerjee P, Han H. 2009. Credibility: A Language 
Modeling Approach to Answer Validation, In Pro-
ceedings of NAACL HLT 2009, Boulder, Bolorado, 
US 
Barners J. 1984. Evidentials in the Tuyuca Verb. IN 
International Journal of American Linguistics, 50 
Bouguessa M, Dumoulin B, Wang S. 2008. Identify-
ing Authoritative Actors in Question-Answering 
Forums - The Case of Yahoo! Answers, In Pro-
ceedings of KDD?08, Las Vegas, Nevada, USA 
Chafe W. 1986. Evidentiality: The Linguistic Coding 
of Epistemology, Evidentiality in English Conver-
sation and Academic Writing. In Chafe and Nich-
16
ols, (ed.). Evidentiality: The Linguistic Coding of 
Epistemology. Norwood, NJ: Ablex 
DeLancey S. 2001. The mirative and evidentiality. In 
Journal of Pragmatic, 33 
Forman G. 2003. An Extensive Empirical Study of 
Feature Selection Metrics for Text Classification, 
In Journal of Machine Learning Research, 3 
Gil Y, Artz D. 2006. Towards Content Trust of Web 
Resources, In Proceedings of the 15th International 
World Wide Web Conference, Edinburgh, Scotland 
Gyongyi Z, Molina H, Pedersen J. 2004. Combating 
Web Spam with TrustRank. In Proceedings of the 
30th VLDB Conference, Toronto, Canada 
Ifantidou E. 2001. Evidentials and Relevance. John 
Benjamins Publishing Company. 
Jeon J, Croft W, Lee J and Park S. 2006. A Frame-
work to Predict the Quality of Answers with Non-
textual Features, In Proceedings of SIGIR?06, Se-
attle, Washington, USA 
Leopold E, Kindermann J. 2002. Text Categorization 
with Support Vector Machines. How to Represent 
Texts in Input Space?, In Machine Learning, 46, 
423-444 
Oswalt R. 1986. The evidential system of Kashaya. 
IN Chafe W and Nichols (Eds.), Evidentiality: The 
linguistic coding of epistemology. Norwood, NJ: 
Ablex 
Rubin V, Liddy E, Kando N. 2005. Certainty Identi-
fication in Texts: Categorization Model and Man-
ual Tagging Results, In Shanahan J and et al(Eds.), 
Computing Attitude and Affect in Text: Theory and 
Applications (The Information Retrieval Series): 
Springer-Verlag New York, Inc. 
Rubin V, Liddy E. 2006. Assessing Credibility of 
Weblogs, In Proceedings of the AAAI Spring Sym-
posium: Computational Approaches to Analyzing 
Weblogs (CAAW) 
Magnini B, Negri M, Prevete R and Tanev H. 2002. 
Is It the Right Answer? Exploiting Web Redun-
dancy for Answer Validation, In Proceedings of 
the 40th Annual Meeting of the Association for 
Computational Linguistics, Philadelphia, PA 
Metzger M. 2007. Evaluating Online Information and 
Recommendations for Future Research, Journal of 
the American Society for Information Science and 
Technology, 58(13) 
Mushin  I. 2000. Evidentiality and Deixis in Retelling, 
In Journal of Pragmatics, 32 
Weerkamp W, Rijke M. 2008. Credibility Improves 
Topical Blog Post Retrieval. In Proceedings of 
ACL08: HLT 
Zhang J, Ackerman M, Adamic L. 2007. Expertise 
Networks in Online Communities: Structure and 
Algorithms. In Proceedings of the 16th ACM Inter-
national World Wide Web Conference (WWW?07) 
17
