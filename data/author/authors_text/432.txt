Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 274?283,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Who is Who and What is What:  
Experiments in Cross-Document Co-Reference  
Alex Baron 
BBN Technologies 
10 Moulton Street 
Cambridge, MA 02138 
abaron@bbn.com 
Marjorie Freedman  
BBN Technologies 
10 Moulton Street 
Cambridge, MA 02138 
mfreedma@bbn.com 
 
Abstract 
This paper describes a language-independent, 
scalable system for both challenges of cross-
document co-reference: name variation and 
entity disambiguation. We provide system re-
sults from the ACE 2008 evaluation in both 
English and Arabic. Our English system?s ac-
curacy is 8.4% relative better than an exact 
match baseline (and 14.2% relative better over 
entities mentioned in more than one docu-
ment). Unlike previous evaluations, ACE 
2008 evaluated both name variation and entity 
disambiguation over naturally occurring 
named mentions.  An information extraction 
engine finds document entities in text. We de-
scribe how our architecture designed for the 
10K document ACE task is scalable to an 
even larger corpus.  Our cross-document ap-
proach uses the names of entities to find an 
initial set of document entities that could refer 
to the same real world entity and then uses an 
agglomerative clustering algorithm to disam-
biguate the potentially co-referent document 
entities. We analyze how different aspects of 
our system affect performance using ablation 
studies over the English evaluation set. In ad-
dition to evaluating cross-document co-
reference performance, we used the results of 
the cross-document system to improve the ac-
curacy of within-document extraction, and 
measured the impact in the ACE 2008 within-
document evaluation.  
1 Introduction 
Cross-document entity co-reference is the problem 
of identifying whether mentions from different 
documents refer to the same or distinct entities. 
There are two principal challenges: the same entity 
can be referred to by more than one name string 
(e.g. Mahmoud Abbas and Abu Mazen) and the 
same name string can be shared by more than one 
entity (e.g. John Smith). Algorithms for solving the 
cross-document co-reference problem are neces-
sary for systems that build knowledge bases from 
text, question answering systems, and watch list 
applications.  
There are several challenges in evaluating and 
developing systems for the cross-document co-
reference task. (1) The annotation process required 
for evaluation and for training is expensive; an an-
notator must cluster a large number of entities 
across a large number of documents. The annotator 
must read the context around each instance of an 
entity to make reliable judgments. (2) On randomly 
selected text, a baseline of exact string match will 
do quite well, making it difficult to evaluate pro-
gress. (3) For a machine, there can easily be a scal-
ability challenge since the system must cluster a 
large number of entities.  
Because of the annotation challenges, many 
previous studies in cross-document co-reference 
have focused on only the entity disambiguation 
problem (where one can use string retrieval to col-
lect many documents that contain same name); or 
have used artificially ambiguated data. 
Section 2 describes related work; section 3 in-
troduces ACE, where the work was evaluated; sec-
tion 4 describes the underlying information 
extraction engine; sections 5 and 6 address the 
challenges of coping with name variation and dis-
ambiguating entities; sections 7, 8, and 9 present 
empirical results, improvement of entity extraction 
274
within documents using cross-document corefer-
ence, and a difference in performance on person 
versus organization entities. Section 10 discusses 
the scalability challenge. Section 11 concludes.  
2 Related Work 
Person disambiguation given a person name 
string. Bagga and Baldwin (1998b) produced one 
of the first works in cross-document co-reference. 
Their work presented a vector space model for the 
problem of entity disambiguation, clustering 197 
articles that contained the name ?John Smith?.  
Participants in the 2007 Sem-Eval Web People 
Search(WEPS) task clustered 100-document sets 
based on which person a name string of interest 
referenced. WEPS document sets were collected 
by selecting the top 100 web search results to que-
ries about a name string (Artiles, et al, 2007).  
Mann and Yarowsky (2003) and Gooi and 
Allan (2004) used artificially ambiguous data to 
allow for much larger experiments in clustering 
documents around a known person of interest.  
Clustering different variants of the same name. 
Lloyd et. al (2006) use a combination of ?morpho-
logical similarity? and ?contextual similarity? to 
cluster name variants that refer to the same entity.  
Clustering and disambiguation. The John Hop-
kins 2007 Summer Workshop produced a cross-
document annotated version of the ACE 2005 cor-
pus (18K document entities, 599 documents) con-
sisting of 5 entity types (Day, et. al, 2007). There 
was little ambiguity or variation in the corpus. Par-
ticipants demonstrated that disambiguation im-
provements could be achieved with a Metropolis-
Hastings clustering algorithm. The study assumed 
human markup of document-level entities. 
Our work. The work reported in this paper ad-
dresses both entity clustering and name variation 
for both persons and organizations in a corpus of 
10K naturally occurring documents selected to be 
far richer than the ACE 2005 data by NIST and 
LDC. We investigated a new approach in both 
English and Arabic, and evaluated on document-
level entities detected by information extraction. 
3 ACE Evaluation 
NIST?s ACE evaluation measures system perform-
ance on a predetermined set of entities, relations, 
and events. For the 2008 global entity detection 
and recognition task (GEDR)1, system perform-
ance was measured on named instances of person 
and organization entities. The GEDR task was run 
over both English and Arabic documents. Partici-
pants processed over 10K documents for each lan-
guage. References were produced for about 400 
documents per language (NIST, 2008). The evalua-
tion set included documents from several genres 
over a 10 year time period. Document counts are 
provided in Table 1. This evaluation differed from 
previous community cross-document coreference 
evaluations in that it (a) covered both organizations 
and people; (b) required processing a relatively 
large data set; (c) evaluated entity disambiguation 
and name variation simultaneously; and (d) meas-
ured cross-document co-reference over system-
detected document-level entities and mentions.  
 
 English Arabic 
broadcast conversation 8 38 
broadcast news  72 19 
meeting  18 --- 
newswire 237 314 
telephone 18 12 
usenet 15 15 
weblog 47 14 
Table 1: Documents per genre in ACE2008 test set 
 
The evaluation set was selected to include in-
teresting cases for cross-document co-reference 
(e.g cases with spelling variation and entities with 
shared names). This is necessary because annota-
tion is difficult to produce and naturally sampled 
data has a high percentage of entities resolvable 
with string match. The selection techniques were 
unknown to ACE participants.  
4 Extraction System Overview 
Our cross-document co-reference system relies on 
SERIF, a state-of-the-art information extraction 
(IE) system (Ramshaw, et. al, 2001) for document-
level information extraction. The IE system uses 
statistically trained models to detect and classify 
mentions, link mentions into entities, and detect 
and classify relations and events. English and Ara-
bic SERIF share the same general models, al-
though there are differences in the specific features 
used by the models.  Arabic SERIF does not per-
form event detection. While Arabic SERIF does 
                                                          
1
 NIST?s evaluation of cross-document co-reference. 
275
make use of some morphological features, the 
cross-document co-reference system, which fo-
cused specifically on entity names, does not use 
these features.   
Figure 1 and Figure 2 illustrate the architecture 
and algorithms of the cross-document co-reference 
system respectively. Our system separately ad-
dresses two aspects of the cross-document co-
reference problem: name variation (Section  5) and 
entity disambiguation (Section  6). This leads to a 
scalable solution as described in Section  10. 
 
Figure 1: Cross-document Co-reference Architechure 
 
The features used by the cross-document co-
reference system can be divided into four classes: 
World Knowledge (W), String Similarity (S), Pre-
dictions about Document Context (C), and Meta-
data (M). Name variation (V) features operate over 
unique corpus name strings. Entity disambiguation 
features (D) operate over document-level entity 
instances. During disambiguation, the agglomera-
tive clustering algorithm merges two clusters when 
conditions based on the features are met. For ex-
ample, two clusters are merged when they share at 
least half the frequently occurring nouns that de-
scribe an entity (e.g. president).  As shown in 
Table 2, features from the same class were often 
used in both variation and disambiguation. All 
classes of features were used in both English and 
Arabic. Because very little training data was avail-
able, both the name variation system and the dis-
ambiguation system use manually tuned heuristics 
to combine the features. Tuning was done using 
the ACE2008 pilot data (LDC, 2008b), documents 
from the SemEval WEPS task (Artiles, et al, 
2007), and some internally annotated documents. 
Internal annotation was similar in style to the 
WEPS annotation and did not include full ACE 
annotation. Annotators simply clustered documents 
based on potentially confusing entities. Internal 
annotation was done for ~100 names in both Eng-
lish and Arabic. 
Feature Class Stage Class 
Wikipedia knowledge D, V W 
Web-mined aliases V W 
Word-based similarity  D, V S 
Character-based similarity V S 
Translation dictionaries V S 
Corpus Mined Aliases D, V C 
SERIF extraction D,V C 
Predicted Document Topics D C 
Metadata (source, date, etc.) D M 
Table 2: Features for Cross-Document Co-Reference 
5 Name Variation 
The name variation component (Block 1 of Figure 
1) collects all name strings that appear in the 
document set and provides a measure of similarity 
between each pair of name strings.2 Regions (A) 
and (B) of Figure 2 illustrate the input and output 
of the name variation component.  
This component was initially developed for 
question answering applications, where when 
asked the question ?Who is George Bush?? relevant 
answers can refer to both George W and George 
HW (the question is ambiguous). However when 
asked ?Who leads al Qaeda?? the QA system must 
be able to identify spelling variants for the name al 
Qaeda. For the cross-document co-reference prob-
lem, separating the name variation component 
from the disambiguation component improves the 
scalability of the system (described in Section  10). 
The name variation component makes use of a 
variety of features including web-mined alias lists, 
aliases mined from the corpus (e.g ?John aka J?), 
statistics about the relations and co-reference deci-
sions predicted by SERIF, character-based edit 
distance, and token subset trees. The token subset 
trees algorithm measures similarity using word 
overlap by building tree-like structures from the 
unique corpus names based on overlapping tokens. 
Translation dictionaries (pulled from machine 
                                                          
2
 For the majority of pairs, this similarity score will be 0.  
Input 
Documents 
IE System 
Cross-Document  
Name Variation 
Entity 
Featurizer 
 
Name Similarity 
DB 
 
Entity-based 
Feature DB 
 
Clusters DB 
World 
Knowledge DB 
Output 
Documents 
Entity 
Disambiguation 
 
Information 
Extraction DB 
(1) 
 
(2) 
 
276
translation training and cross-language links in 
Wikipedia) account for names that have a canoni-
cal form in one language but may appear in many 
forms in another language.   
 
 
Figure 2: Cross-document Co-reference Process 
 
The features are combined with hand-tuned 
weights resulting in a unidirectional similarity 
score for each pair of names. The similarity be-
tween two name strings is also influenced by the 
similarity between the contexts in which the two 
names appear (for example the modifiers or titles 
that precede a name). This information allows the 
system to be more lenient with edit distance when 
the strings appear in a highly similar context, for 
example increasing the similarity score between 
?Iranian President Ahmadinejad? and ?Iranian 
President Nejad.? 
6 Entity Disambiguation  
We use a complete link agglomerative cluster-
ing algorithm for entity disambiguation. To make 
agglomerative clustering feasible over a 10K 
document corpus, rather than clustering all docu-
ment-level entities together, we run agglomerative 
clustering over subsets of the corpus entities. For 
each name string, we select the set of names that 
the variation component chose as valid variants. In 
Figure 2 region C, we have selected Mahmoud 
Abbas and 3 variants.  
We then run a three stage agglomerative clus-
tering algorithm over the set of document entities 
that include any of the name string variants or the 
original name. Figure 2 region D illustrates three 
document-level entities. 
The name variation links are not transitive, and 
therefore a name string can be associated with 
more than one clustering instance. Furthermore 
document-level entities can include more than one 
name string. However once a document-level en-
tity has been clustered, it remains linked to entities 
that were a part of that initial clustering. Because 
of this, the order in which the algorithm selects 
name strings is important. We sort the name strings 
so that those names about which we have the most 
information and believe are less likely to be am-
biguous are clustered first. Name strings that are 
more ambiguous or about which less information is 
available are clustered later.  
 The clustering procedure starts by initializing 
singleton clusters for each document entity, except 
those document entities that have already partici-
pated in an agglomerative clustering process. For 
those entities that have already been clustered, the 
clustering algorithm retrieves the existing clusters.  
The merging decisions are based on the similar-
ity between two clusters as calculated through fea-
ture matches. Many features are designed to 
capture the context of the document in which enti-
ties appear. These features include the document 
topics (as predicted by the unsupervised topic de-
tection system (Sista, et al, 2002), the publication 
date and source of a document, and the other 
names that appear in the document (as predicted by 
SERIF).  Other features are designed to provide 
information about the specific context in which an 
entity appears for example: the noun phrases that 
refer to an entity and the relationships and events 
in which an entity participates (as predicted by 
SERIF).  Finally some features, such as the 
uniqueness of a name in Wikipedia are designed to 
provide the disambiguation component with world 
knowledge about the entity. Since each cluster 
represents a global entity, as clusters grow through 
merges, the features associated with the clusters 
expand. For example, the set of associated docu-
ment topics the global entity participates in grows.   
While we have experimented with statistically 
learning the threshold for merging, because of the 
small amount of available training data, this 
threshold was set manually for the evaluation.  
Abu Abbas, Abu Mazen, Adam Smith, 
A Smith, Andy Smith, Mahmoud Abbas,  
Muhammed Abbas ?. 
(A) Name Strings:  
(B) Name String 
Pairs with Score:  
0.9 Mahmoud AbbasAbu Mazen 
0.7 Mahmoud AbbasAbu Abbas 
0.8 Mahmoud AbbasMuhammad Abbas  
 ?.  
(C) Set of Equivalent 
Name Strings:  
Abu Mazen,  
Mahmoud Abbas,  
 Muhammed Abbas,  
Abu Abbas 
(D) Document Entity  
Mentions:  
Palestinian President Mahmoud Abbas ... Abbas said 
Abu Abbas was arrested ? Abbas hijacked  
? election of Abu Mazen 
? 
(E) Entity Clusters:  
Abu Mazen 
Mahmoud Abbas 
Palestinian Leader 
convicted terrorist 
Muhammed Abbas  
Abu Abbas 
277
Clustering over these subsets of similar strings 
has the additional benefit of limiting the number of 
global decisions that are affected by a mistake in 
the within-document entity linking. For example, if 
in one document, the system linked Hillary Clinton 
to Bill Clinton; assuming that the two names are 
not chosen as similar variants, we are likely to end 
up with a cluster made largely of mentions of 
Hillary with one spurious mention of Bill and a 
separate cluster that contains all other mentions of 
Bill. In this situation, an agglomerative clustering 
algorithm that linked over the full set of document-
level entities is more likely to be led astray and 
create a single ?Bill and Hillary? entity. 
7 Experimental Results 
Table 3 and Table 4  include preliminary ACE 
results3 for the highest, lowest, and average system 
in the local and cross-document tasks respectively. 
While a single participant could submit more than 
one entry, these numbers reflect only the primary 
submissions. The ACE scorer maps system pro-
duced entities to reference entities and produces 
several metrics. For the within-document task, 
metrics include ACE Value, B3, and a variant of 
B3 weighted to reflect ACE value weightings.  For 
the cross-document task, the B3 metric is replaced 
with F (NIST, 2008). ACE value has traditionally 
been the official metric of the ACE evaluation. It 
puts a higher cost on certain classes of entities (e.g. 
people are more important than facilities), certain 
classes of mentions (e.g. names are more important 
than pronouns), and penalizes systems for mistakes 
in type and subtype detection as well as linking 
mistakes. Assigning a mention to the wrong entity 
is very costly in terms of value score. If the men-
tion is a name, a system is penalized 1.0 for the 
missed mention and an additional 0.75 for a men-
tion false alarm. We will report ACE Value and 
value weighted B3/F. Scores on the local task are 
not directly comparable to scores on the global 
task. The local entity detection and recognition 
task (LEDR) includes entity detection for five 
(rather than two) classes of entities and includes 
pronoun and nominal (e.g. ?the group?) mentions in 
addition to names. 
 
                                                          
3
 Results in this paper use v2.1 of the references and v17 of 
the ACE scorer. Final results will be posted to 
http://www.nist.gov/speech/tests/ace/2008/ 
 English Arabic 
 Val B3Val Val B3Val 
Top 52.6 71.5 43.6 69.1 
Average -53.3 50.0 17.3 47.6 
Low4 -269.1 25.8 -9.1 26.1 
BBN-A-edropt 52.1 71.5 43.0 68.9 
BBN-B-st-mg 52.6 71.5 43.6 69.1 
BBN-B-st-mg-
fix5 
57.2 77.4 44.6 71.3 
Table 3: ACE 2008 Within-Document Results (LEDR) 
 
 English Arabic 
 Val FVal Val FVal 
Top 53.0 73.8 28.2 58.7 
Average 21.1 59.1 24.7 56.8 
Low -64.1 31.6 21.2 54.8 
BBN-B-med 53.0 73.8 28.2 58.7 
BBN-B-low 53.2 73.8 28.7 59.3 
BBN-B-med-fix5 61.7 77 31.4 60.1 
Table 4: ACE 2008 Cross-Document Results (GEDR) 
 
Our cross-document co-reference system used 
BBN-A-edropt as input. BBN-B-st-mg is the result 
of using cross-document co-reference to improve 
local results (Section  9). For cross-document co-
reference, our primary submission, BBN-B-med, 
was slightly outperformed by an alternate system 
BBN-B-low. The two submissions differed only in 
a parameter setting for the topic detection system 
(BBN-B-low requires more documents to predict a 
?topic?). BBN-A-st-mg-fix and BBN-B-med-fix 
are the result of post-processing the BBN output to 
account for a discrepancy between the training and 
evaluation material.5   
In addition to releasing results, NIST also re-
leased the references. Table 5 includes the ACE 
score for our submitted English system and the 
score when the system was run over only the 415 
documents with references. The system performs 
slightly better when operating over the full docu-
ment set. This suggests that the system is using 
information from the corpus even when it is not 
directly scored.  
                                                          
4
 There was a swap in rank between metrics, so the low num-
bers reflect two different systems.   
5
 There were discrepancies between the ACE evaluation and 
training material with respect to the portions of text that 
should be processed.  Therefore our initial system included a 
number of spurious entities. NIST has accepted revised output 
that removes these entities. Experiments in this paper reflect 
the corrected system.   
278
  
 FVal 
10K documents processed (415 scored) 
(BBN-B-med-fix) 
77 
Only 415 documents processed 76.3 
Table 5: Full English System ACE Evaluation Results 
 
We have run a series of ablation experiments 
over the 415 files in the English test set to evaluate 
the effectiveness of different feature classes. These 
experiments were run using only the annotated 
files (and not the full 10K document set). We ran 
two simple baselines. The first baseline (?No 
Link?) does not perform any cross-document co-
reference, all document entities are independent 
global entities. The second baseline (?Exact 
Match?) links document-level entities using exact 
string match. We ran 6 variations of our system: 
o Configuration 1 is the most limited system. It 
uses topics and IE system output for disambigua-
tion, and aliases mined from the documents for 
the name variation component.  
o Configuration 2 includes Configuration 1 fea-
tures with the addition of string similarity (edit 
distance, token subset trees) algorithms for the 
name variation stage.  
o Configuration 3 includes Configuration 2 fea-
tures and adds context-based features (e.g. titles 
and premodifiers) for name variation.  
o Configuration 4 adds information from docu-
ment metadata to the disambiguation component.  
o Configuration 5 adds web-mined information 
(alias lists, Wikipedia, etc.) to both the variation 
and disambiguation components. This is the con-
figuration that was used for our NIST submission.  
o Configuration 5a is identical to Configuration 
5 except that the string-based edit distance was 
removed from the name variation component.  
As noted previously, the ACE collection was 
selected to include challenging entities. The selec-
tion criteria of the corpus (which are not known by 
ACE participants) can affect the importance of fea-
tures. For example, a corpus that included very few 
transliterated names would make less use of fea-
tures based on edit distance.  
Figure 3 and Figure 4 show performance (with 
value weighted F) on the eight conditions over sys-
tem predicted within-document extraction and ref-
erence within-document extraction respectively. 
Figure 3 also includes configuration 5 run over all 
10K documents. We provide two sets of results. 
The first evaluates system performance over all 
entities. The relatively high score of the ?No Link? 
baseline indicates that a high percentage of the 
document-level entities in the corpus are only men-
tioned in one document. The second set of num-
bers measures system performance on those 
entities appearing in more than one reference 
document. While this metric does not give a com-
plete picture of the cross-document co-reference 
task (sometimes a singleton entity must be disam-
biguated from a large entity that shares the same 
name); it does provide useful insights given the 
frequency of singleton entities. 
System Document Level Entities
30
40
50
60
70
80
90
100
Sp
lit A
ll
Ex
ac
t M
atc
h 1 2 3 4 5a 5
5 (1
0k
 
do
cs
)
Configuration
Va
lu
e 
W
e
ig
ht
ed
 
F
All Entities
Entities in > 1
Documents
 
Figure 3: Performance on System Document Entities 
 
Reference Document Level Entities
30
40
50
60
70
80
90
100
Split All Exact
Match
1 2 3 4 5a 5
Configuration
Va
lu
e
 
W
e
ig
ht
ed
 
F
All Entities
Entities in >1
Documents
 
Figure 4: Performance on Perfect Document Entities 
 
Overall system performance improved as fea-
tures were added. Configuration 1, which disam-
biguated entities with a small set of features, 
performed worse than a more aggressive exact 
string match strategy. The nature of our agglom-
erative clustering algorithm leads to entity merges 
only when there is sufficient evidence for the 
merge. The relatively high performance of the ex-
act match strategy suggests that in the ACE corpus, 
most entities that shared a name string referred to 
279
the same entity, and therefore aggressive merging 
leads to better performance. As additional features 
are added, our system becomes more confident and 
merges more document-level entities.  
With the addition of string similarity measures 
(Configuration 2) our system outperforms the exact 
match baseline. The submitted results on system 
entities (Configuration 5) provide a 8.4% relative 
reduction in error over the exact match baseline. If 
scored only on entities that occur in more than one 
document, Configuration 5 gives a 14.2% relative  
redution in error over the exact match baseline.  
The context based features (Configuration 3) al-
low for more aggressive edit-distance-based name 
variation when two name strings frequently occur 
in the same context. In Configuration 3, ?Sheik 
Hassan Nasrallah? was a valid variant of ?Hassan 
Nasrallah? because both name strings were com-
monly preceded by ?Hezbollah leader?. Similarly, 
?Dick Cheney? became a valid variant of ?Richard 
Bruce Cheney? because both names were preceded 
by ?vice president?. In Configuration 2 the entities 
included in both sets of name strings had remained 
unmerged because the strings were not considered 
valid variants. With the addition of contextual in-
formation (Configuration 3), the clustering algo-
rithm created a single global entity. For the ?Dick 
Cheney? cluster, this was correct. ?Sheik Hassan 
Nassrallah? was a more complex instance, in some 
cases linking was correct, in others it was not.  
The impact of the metadata features (Configu-
ration 4) was both positive and negative. An article 
about the ?Arab League Secretary General Amru 
Moussa? was published on the same day in the 
same source as an article about ?Intifada Fatah 
movement leader Abu Moussa?. With the addition 
of metadata features, these two distinct global enti-
ties were merged. However, the addition of meta-
data features correctly led to the merging of three 
instances of the name ?Peter? in ABC news text 
(all referring ABC?s Peter Jennings).  
Web-mined information (Configuration 5) pro-
vides several variation and disambiguation fea-
tures. As we observed, the exact match baseline 
has fairly high accuracy but is obviously also too 
aggressive of a strategy. However, for certain very 
famous global entities, any reference to the name 
(especially in corpora made of primarily news text) 
is likely to be a reference to a single global entity. 
Because these people/organizations are famous, 
and commonly mentioned, many of the topic and 
extraction based features will provide insufficient 
evidence for merging. The same famous person 
will be mentioned in many different contexts. We 
use Wikipedia as a resource for such entities. If a 
name is unambiguous in Wikipedia, then we merge 
all instances of this name string. In the evaluation 
corpus, this led to the merging of many different 
instances of ?Osama Bin Laden? into a single en-
tity. Web-mined information is also a resource for 
aliases and acronyms. These alias lists, allowed us 
to merge ?Abu Muktar? with ?Khadafi Montanio? 
and ?National Liberation Army? with ?ELN?. 
Interestingly, removing the string edit distance 
algorithm (System 5a), is a slight improvement 
over System 5. Initial error analysis has shown that 
while the string edit distance algorithm did im-
prove accuracy on some entities (e.g linking ?Sam 
Alito? with ?Sam Elito? and linking ?Andres Pas-
trana? with ?Andreas Pastrana?); in other cases, 
the algorithm allowed the system to overlink two 
entities, for example linking ?Megawati Soekar-
noputri? and her sister ?Rachmawati Sukarnoputri?.  
8 Improving Document-Level Extraction 
with Global Information  
In addition to evaluating the cross-document sys-
tem performance on the GEDR task, we ran a pre-
liminary set of experiments using the cross-
document co-reference system to improve within-
document extraction. Global output modified 
within-document extraction in two ways. 
First, the cross-document co-reference system 
was used to modify the within-document system?s 
subtype classification. In addition to evaluating 
entity links and type classification, the ACE task 
measures subtype classification. For example, for 
organization entities, systems distinguish between 
Media and Entertainment organizations. The IE 
system uses all mentions in a given entity to assign 
a subtype. The cross-document co-reference sys-
tem has merged several document-level entities, 
and therefore has even more information with 
which to assign subtypes. The cross-document sys-
tem also has access to a set of manual labels that 
have been assigned to Wikipedia categories.  
Secondly, we used the cross-document co-
reference system?s linking decisions to merge 
within-document entities. If the cross-document 
co-reference system merged two entities in the 
280
same document, then those entities were merged in 
the within-document output.  
Table 6 includes results for our within-
document IE system, the IE system with improved 
subtypes, and the IE system with improved sub-
types and merged entities.  
 
 B3Val Val 
Local 77.3 56.7 
+ Subtypes 77.3 56.9 
+ Merge 77.4 57.2 
Table 6: Within-document Results 
 
While these preliminary experiments yield rela-
tively small improvements in accuracy, an analysis 
of the system?s output suggests that the merging 
approach is quite promising. The output that has 
been corrected with global merges includes the 
linking entities with ?World Knowledge? acronyms 
(e.g. linking ?FARC? with ?Armed Revolutionary 
Forces of Colombia?); linking entities despite 
document-level extraction mistakes (e.g. ?Lady 
Thatcher? with ?Margaret Thatcher?); and linking 
entities despite spelling mistakes in a document 
(e.g linking ?Avenajado? with ?Robert Aventa-
jado?). However, as we have already seen, the 
cross-document co-reference system does make 
mistakes and these mistakes can propagate to the 
within-document output.  
In particular, we have noticed that the cross-
document system has a tendency to link person 
names with the same last name when both names 
appear in a single document. As we think about the 
set of features used for entity disambiguation, we 
can see why this would be true. These names may 
have enough similarity to be considered equivalent 
names. Because they appear in the same document, 
they will have the same publication date, document 
source, and document topics. Adjusting the cross-
document system to either use a slightly different 
approach to cluster document-level entities from 
the same document or at the very least to be more 
conservative in applying merges that are the result 
primarily of document metadata and context to the 
within-document output could improve accuracy.  
9 Effect of LEDR on GEDR 
Unlike previous evaluations of cross-document co-
reference performance, the ACE 2008 evaluation 
included both person and organization entities. We 
have noticed that the performance of the cross-
document co-reference system on organizations 
lags behind the performance of the system on peo-
ple. In contrast, for LEDR, the extraction system?s 
performance is quite similar between the two entity 
classes. Furthermore, the difference between 
global organization and person accuracy in the 
GEDR is smaller when the GEDR task performed 
with perfect document-level extraction. Scores are 
shown in Table 7. These differences suggest that 
part of the reason for the low performance on or-
ganizations in GEDR is within-document accuracy.  
 
 
LEDR GEDR-  
System 
GEDR-
Perfect 
 B3Val Val FVal Val FVal Val 
Org 75.1 51.7 67.8 45.9 91.5 84.0 
Per 76.2 52.9 83.2 71.4 94.3 89.5 
Table 7: Performance on ORG and PER Entities 
 
The LEDR task evaluates names, nominals, and 
pronouns. GEDR, however only evaluates over 
name strings. To see if this was a part of the differ-
ence in accuracy, we removed all pronoun and 
nominal mentions from both the IE system?s local 
output and the reference set. As shown in Table 8, 
the gap in performance between organizations and 
people is much larger in this setting.  
 
 LEDR- Name Only 
 B3Val Val 
ORG 82.6 83.0 
PER 90.1 90.4 
Table 8: Local Performance on Name Only Task 
 
Because the GEDR task focuses exclusively on 
names and excludes nominals and pronouns, mis-
takes in mention type labeling (e.g. labeling a 
name as a nominal) become misses and false 
alarms rather than type substitutions. As the task is 
currently defined, type substitutions are much less 
costly than a missing or false alarm entity.  
Intuitively, correctly labeling the name of a per-
son as a name and not a nominal is simple. The 
distinction for organizations may be fuzzier. For 
example the string ?the US Department of Justice? 
could conceivably contain one name, two names, 
or a name and a nominal. The ACE guidelines 
(LDC, 2008a) suggest that this distinction can be 
difficult to make, and in fact have a lengthy set of 
rules for classifying such cases. However, these 
rules can seem unintuitive, and may be difficult for 
machines to learn. For example ?Justice Depart-
ment? is not a name but ?Department of Justice? is. 
In some sense, this is an artificial distinction en-
forced by the task definition, but the accuracy 
281
numbers suggest that the distinction has a negative 
effect on system evaluation.  
10 Scalability 
One of the challenges for systems participating 
in the ACE task was the need to process a rela-
tively large document set (10K documents). In 
question answering applications, our name varia-
tion algorithms have been applied to even larger 
corpora (up to 1M documents). There are two fac-
tors that make our solution scalable.  
First, much of the name variation work is 
highly parallelizable. Most of the time spent in this 
algorithm is spent in the name string edit distance 
calculation. This is also the only algorithm in the 
name variation component that scales quadratically 
with the number of name strings. However, each 
calculation is independent, and could be done si-
multaneously (with enough machines). For the 
10K document set, we ran this algorithm on one 
machine, but when working with larger document 
sets, these computations were run in parallel.  
Second, the disambiguation algorithm clusters 
subsets of document-level entities, rather than run-
ning the clustering over all entities in the document 
set. In the English ACE corpus, the IE system 
found more than 135K document-level entities that 
were candidates for global entity resolution. There 
were 62,516 unique name strings each of which 
was used to initialize an agglomerative clustering 
instance. As described in Section  6, a document 
entity is only clustered one time. Consequently, 
36% of these clustering instances are ?skipped? 
because they contain only already clustered docu-
ment entities. Even the largest clustering instance 
contained only 1.4% of the document-level enti-
ties.  
The vast majority of agglomerative clustering 
instances disambiguated a small number of docu-
ment-level entities and ran quickly. 99.7% of the 
agglomerative clustering runs took less than 1 sec-
ond. 99.9% took 90 seconds or less.  
A small number of clustering instances in-
cluded a large number of document entities, and 
took significant time. The largest clustering in-
stance, initialized with the name string ?Xinhua,? 
contained 1848 document-level entities (1.4% of 
the document-level entities in the corpus). This 
instance took 2.6 hours (27% of the total time 
spent running agglomerative clustering). Another 
frequent entity ?George Bush? took 1.2 hours.  
As described in Section  6, the clustering proce-
dure can combine unresolved document-level enti-
ties into existing global entities. For large cluster 
sets (e.g entities referred to by the string ?Xinhua?), 
speed would be improved by running many smaller 
clustering instances on subsets of the document-
level entities and then merging the results.  
11 Conclusions and Future Work 
We have presented a cross-document co-reference 
clustering algorithm for linking entities across a 
corpus of documents that  
? addresses both the challenges of name varia-
tion and entity disambiguation. 
? is language-independent, 
? is scalable  
As measured in ACE 2008, for English our sys-
tem produced an .8.4% relative reduction in error 
over a baseline that used exact match of name 
strings. When measured on only entities that ap-
peared in more than one document, the system 
gave a 14.2% relative reduction in error. For the 
Arabic task, our system produced a 7% reduction 
in error over exact match (12.4% when scored over 
entities that appear in more than one document). 
We have shown how a variety of features are im-
portant for addressing different aspects of the 
cross-document co-reference problem. Our current 
features are merged with hand-tuned weights. As 
additional development data becomes available, we 
believe it would be feasible to statistically learn the 
weights. With statistically learned weights, a larger 
feature set could improve accuracy even further.  
 Global information from the cross-document 
co-reference system improved within-document 
information extraction. This suggests both that a 
document-level IE system operating over a large 
corpus text can improve its accuracy with informa-
tion that it learns from the corpus; and also that 
integrating an IE system more closely with a 
source of world knowledge (e.g. a knowledge 
base) could improve extraction accuracy.  
Acknowledgements 
This work was supported by the United States De-
partment of Homeland Security. Elizabeth Boschee 
and Ralph Weischedel provided useful insights 
during this work.   
282
References 
 
Artiles, Javier, Julio Gonzalo. & Felisa Verdejo.. 2005. 
A Testbed for People Searching Strategies. In the 
WWW. SIGIR 2005 Conference. Salvador, Brazil. 
Artiles, Javier, Julio Gonzalo. & Satochi Sekine.. 2007. 
The SemEval-2007 WePS Evaluation: Establishing a 
benchmark for the Web People Search Task. Pro-
ceedings of the 4th International Workshop on Se-
mantic Evaluations (SemEval-2007), pages 64?69, 
Prague, Czech.  
Bagga, Amit & Breck Baldwin. 1998a. Algorithms for 
Scoring Coreference Chains. In Proceedings of the 
Linguistic Coreference Workshop at the First Inter-
national Conference on Language Resources and 
Evaluation (LREC'98), pages 563-566. 
Bagga, Amit & Breck Baldwin. 1998b. Entity-Based 
Cross-Document Coreferencing Using the Vector 
Space Model. In Proceedings of the 36th Annual 
Meeting of the Association for Computational Lin-
guistics and the 17th International Conference on 
Computational Linguistics (COLING-ACL?98), pages 
79-85. 
Day, David.,Jason Duncan, Claudio Guiliano, Rob Hall, 
Janet Hitzeman,Su Jian, Paul McNamee, Gideon 
Mann, Stanley Yong & Mike Wick. 2007. CDC Fea-
tures. Johns Hopkins Summer Workshop on Cross-
Document Entity Disambiguation. 
http://www.clsp.jhu.edu/ws2007/groups/elerfed/docu
ments/fullCDED.ppt 
Gooi, Chung Heong & James Allan. 2004. Cross-
document coreference on a large scale corpus. In 
Human Language Technology Conf. North American 
Chapter Association for Computational Linguistics, 
pages 9?16, Boston, Massachusetts, USA. 
Lloyd, Levon., Andrew Mehler & Steven Skiena 2006. 
Identifying Co-referential Names Across Large Cor-
pora. Combinatorial Pattern Matching. 2006, pages 
12-23, Barcelona, Spain. 
Linguistic Data Consortium 2008a. ACE (Automatic 
Content Extraction) English Annotation Guidelines 
for Entities Version 6.6 2008.06.13. .  Linguistic 
Data Consortium, Philadelphia. 
http://projects.ldc.upenn.edu/ace/docs/English-
Entities-Guidelines_v6.6.pdf 
Linguistic Data Consortium, 2008b. ACE 2008 XDOC 
Pilot Data V2.1. LDC2007E64.  Linguistic Data 
Consortium, Philadelphia. 
Mann, Gideon S. & Yarowsky, David. 2003. Unsuper-
vised Personal Name Disambiguation In Proceedings 
of the seventh conference on Natural language learn-
ing at HLT-NAACL, pages 33-40. 
 
 
NIST Speech Group. 2008. The ACE 2008 evaluation 
plan: Assessment of Detection and Recognition of 
Entities and Relations Within and Across Docu-
ments. 
http://www.nist.gov/speech/tests/ace/2008/doc/ace08
-evalplan.v1.2d.pdf 
Ramshaw, Lance, E. Boschee, S. Bratus, S. Miller, R. 
Stone, R. Weischedel and A. Zamanian: ?Experi-
ments in Multi-Modal Automatic Content Extrac-
tion?; in Proc. of HLT-01, San Diego, CA, 2001. 
Sista, S, R. Schwartz, T. Leek, and J. Makhoul. An Al-
gorithm for Unsupervised Topic Discovery from 
Broadcast News Stories. In Proceedings of ACM 
HLT, San Diego, CA, 2002. 
 
 
283
Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language
Processing (HLT/EMNLP), pages 652?659, Vancouver, October 2005. c?2005 Association for Computational Linguistics
A Methodology for Extrinsically Evaluating Information Extraction  
Performance 
 
Michael Crystal, Alex Baron, Katherine Godfrey, Linnea Micciulla, Yvette Tenney, and 
Ralph Weischedel  
BBN Technologies 
10 Moulton St. 
Cambridge, MA 02138-1119 
mcrystal@bbn.com 
 
Abstract 
This paper reports a preliminary study 
addressing two challenges in measuring 
the effectiveness of information extrac-
tion (IE) technology: 
? Developing a methodology for ex-
trinsic evaluation of IE; and, 
? Estimating the impact of improving 
IE technology on the ability to per-
form an application task. 
The methodology described can be em-
ployed for further controlled experi-
ments regarding information extraction. 
1 Introduction 
Intrinsic evaluations of information extraction 
(IE) have a history dating back to the Third Mes-
sage Understanding Conference1 (MUC-3) and 
continuing today in the Automatic Content Ex-
traction (ACE) evaluations.2  Extrinsic evalua-
tions of IE, measuring the utility of IE in a task, 
are lacking and needed (Jones, 2005).   
In this paper, we investigate an extrinsic 
evaluation of IE where the task is question an-
swering (QA) given extracted information.  In 
addition, we propose a novel method for explor-
ing hypothetical performance questions, e.g., if 
IE accuracy were x% closer to human accuracy, 
how would speed and accuracy in a task, e.g., 
QA, improve? 
                                                          
1 For more information on the MUC conferences, see 
http://www.itl.nist.gov/iad/894.02/related_projects/muc/.   
2 For an overview of ACE evaluations see 
http://www.itl.nist.gov/iad/894.01/tests/ace/.  
We plot QA accuracy and time-to-complete 
given eight extracted data accuracy levels rang-
ing from the output of SERIF, BBN?s state-of-
the-art IE system, to manually extracted data. 
2 Methodology 
Figure 1 gives an overview of the methodol-
ogy. The left portion of the figure shows source 
documents provided both to a system and a hu-
man to produce two extraction databases, one 
corresponding to SERIF?s automated perform-
ance and one corresponding to double-
annotated, human accuracy.  By merging por-
tions of those two sources in varying degrees 
(?blends?), one can derive several extracted da-
tabases ranging from machine quality, through 
varying percentages of improved performance, 
up to human accuracy. This method of blending 
databases provides a means of answering hypo-
thetical questions, i.e., what if the state-of-the-
art were x% closer to human accuracy, with a 
single set of answer keys. 
A person using a given extraction database 
performs a task, in our case, QA.  The measures 
of effectiveness in our study were time to com-
plete the task and percent of questions answered 
correctly.  An extrinsic measure of the value of 
improved IE technology performance is realized 
by rotating users through different extraction 
databases and questions sets.   
In our preliminary study, databases of fully 
automated IE and manual annotation (the gold 
standard) were populated with entities, relation-
ships, and co-reference links from 946 docu-
ments. The two initial databases representing 
machine extraction and human extraction re-
spectively were then blended to produce a con-
tinuum of database qualities from machine to 
652
human performance. ACE Value Scores3 were 
measured for each database. Pilot studies were 
conducted to develop questions for a QA task. 
Each participant answered four sets of questions, 
each with a different extraction database repre-
senting a different level of IE accuracy. An an-
swer capture tool recorded the time to answer 
each question and additional data to confirm that 
the participant followed the study protocol. The 
answers were then evaluated for accuracy and 
the relationship between QA performance and 
IE quality was established.  
Each experiment used four databases. The first ex-
periment used databases spanning the range from 
solely machine extraction to solely human extraction. 
Based on the results of this experiment, two further 
experiments focused on smaller ranges in database 
quality to study the relationship between IE and QA 
performance.  
2.1 Source Document Selection, Annota-
tion, and Extraction 
Source documents were selected based on the 
availability of manual annotation.  We identified 
946 broadcast news and newswire articles from 
recent ACE efforts, all annotated by the LDC 
according to the ACE guidelines for the relevant 
year (2002, 2003, 2004). Entities, relations, and 
within-document co-reference were marked.  
Inter-document co-reference annotation was 
added by BBN.  The 946 news articles com-
prised 363 articles (187,720 words) from news-
wire and 583 (122,216 words) from broadcast 
news. With some corrections to deal with errors 
and changes in guidelines, the annotations were 
loaded as the human (DB-quality 100) database. 
                                                          
3 The 2004 ACE evaluation plan, available at 
http://www.nist.gov/speech/tests/ace/ace04/doc/ace04-evalplan-
v7.pdf, contains a full description of the scoring metric used in the 
evaluation.  Entity type weights were 1 and the level weights were 
NAM=1.0, NOM=0.5, and PRO=0.1. 
SERIF, BBN?s automatic IE system based on its 
predecessor, SIFT (Miller, 2000), was run on the 
946 ACE documents to create the machine (DB-
quality 0) database. SERIF is a statistically 
trained software system that automatically per-
forms entity, co-reference, and relationship in-
formation extraction. 
Intermediate IE performance was simulated 
by blending the human and automatically gener-
ated databases in various degrees using an inter-
polation algorithm developed specifically for 
this study. To create a blended database, DB-
quality n, all of the entities, relationships, and 
co-reference links common to the human and 
automatically generated databases are copied 
into a new one. Then, n% of the entity mentions 
in the human database (100), but not in the 
automatic IE system output (0), are copied; and, 
(100 ? n)% of the entity mentions in the auto-
matically generated database, but not in the hu-
man database, are copied. Next, the relationships 
for which both of the constituent entity mentions 
have been copied are also copied to the blended 
database. Finally, co-reference links and entities 
for the already copied entity mentions are copied 
into the blended database. 
For the first experiment, two intermediate ex-
traction databases were created: DB-qualities 33 
and 67. For the second experiment, two addi-
tional databases were created: 16.5 and 50. The 
first intermediate databases were both created 
using the 0 and 100 databases as seeds. The 16.5 
database was created by mixing the 0 and the 33 
databases in a 50% blend. The 50 database was 
created by doing the same with the 33 and 67 
databases.  For Experiment 3, 41 and 58 data-
bases were created by mixing the 33 and 50, and 
50 and 67 databases respectively.  
100
0
100
67
33
0<<IE Tool>>
Annotators
Source
docs
++
+
0
-
Blended
Extraction
Accuracy
Measure QA
Accuracy &
Speed
QA Task
Study establishes 
curve
++
+
0
-
10067330
Imputed Accuracy 
Requirement
A Priori Utility 
Threshold
Q
A
 P
er
fo
rm
an
ce
IE Accuracy (% human annotation)
Figure 1: Study Overview 
653
 
  DB Blend 
  
0 
(Machine) 16.5 33 41 50 58 67 
100 
(Human) 
  Ent Rel Ent Rel Ent Rel Ent Rel Ent Rel Ent Rel Ent Rel Ent Rel 
Recall 64 33 70 40 74 45 76 48 79 54 82 58 86 65 100 100 
Pre. 74 50 77 62 79 67 80 70 83 75 85 78 89 82 100 100 
Value 60 29 67 37 71 42 73 45 77 51 80 56 84 63 100 100 
Table 1: Precision, Recall and Value Scores for Entities and Relations for each DB Blend 
 
  0 
(Machine) 16.5 33 41 50 58 67 
100 
(Human)
Entities 17,117 18,269 18,942 19,398 19,594 19,589 19,440 18,687 
Relations 6,684 6,675 6,905 7,091 7,435 7,808 8,406 11,032 
Descriptions 18,666 18,817 19,135 19,350 19,475 19,639 19,752 20,376 
Table 2: Entity, Relation and Description Counts for each DB Blend 
 
To validate the interpolation algorithm and 
blending procedure, we applied NIST?s 2004 
ACE Scorer to the eight extraction databases. 
Polynomial approximations were fitted against 
both the entity and relation extraction curves. 
Entity performance was found to vary linearly 
with DB blend (R2 = .9853) and relation per-
formance was found to vary with the square of 
DB blend (R2 = .9961). Table 1 shows the scores 
for each blend, and Table 2 shows the counts of 
entities, relationships, and descriptions. 
2.2 Question Answering Task 
Extraction effectiveness was measured by how 
well a person could answer questions given a 
database of facts, entities, and documents. Par-
ticipants answered four sets of questions using 
four databases. They accessed the database using 
BBN?s FactBrowser (Miller, 2001) and recorded 
their answers and source citations in a separate 
tool developed for this study, AnswerPad. 
Each database represented a different data-
base quality. In some databases, facts were miss-
ing, or incorrect facts were recorded. 
Consequently, answers were more accessible in 
some databases than in others, and participants 
had to vary their question answering strategy 
depending on the database. 
Participants were given five minutes to an-
swer each question. To ensure that they had ac-
tually located the answer rather than relied on 
world knowledge, they were required to provide 
source citations for every answer. The instruc-
tions emphasized that the investigation was a 
test of the system, and not of their world knowl-
edge or web search skills. Compliance with 
these instructions was high. Users resorted to 
knowledge-based proper noun searches only one 
percent of the time. In addition, keyword search 
was disabled to force participants to rely on the 
database features. 
2.3 Participants 
Study participants were recruited through local 
web lists and at local colleges and universities.  
Participants were restricted to college students 
and recent graduates with PC (not Mac) experi-
ence, without reading disabilities, for whom 
English was their native language. No other 
screening was necessary because the design 
called for each participant to serve as his or her 
own control, and because opportunities to use 
world knowledge in answering the questions 
were minimized through the interface and pro-
cedures. 
During the first two months of the study 23 
participants were used to help develop questions, 
participant criteria, and the overall test proce-
dure. Then, experiments were conducted com-
paring the 0, 33, 67, and 100 database blends 
(Experiment 1, 20 subjects); the 0, 16.5, 33, and 
50 database blends (Experiment 2, 20 subjects), 
and the 33, 41, 50, and 58 database blends (Ex-
periment 3, 24 subjects). 
654
2.4 Question Selection and Validation 
Questions were developed over two months of 
pilot studies. The goal was to find a set of ques-
tions that would be differentially supported by 
the 0, 33, 67, and 100 databases. We explored 
both ?random? and ?engineered? approaches. 
The random approach called for creating ques-
tions using only the documents, without refer-
ence to the kind of information extracted. Using 
a list of keywords, one person generated 86 
questions involving relationships and entities 
pertaining to politics and the military by scan-
ning the 946 ACE documents to find references 
to each keyword and devising questions based 
on the information she found.  
The alternative, engineered approach involved 
eliminating questions that were not supported by 
the types of information extracted by SERIF, 
and generating additional questions to fit the 
desired pattern of increasing support with in-
creased human annotation. This approach en-
sured that the question sets reflected the 
structural differences that are assumed to exist in 
the database, and produced psychophysical data 
that link degree of QA support to human per-
formance parameters. The IE results from four 
of the databases (0, 33, 67 and 100) were used to 
develop questions that received differential sup-
port from the different quality databases. For 
example, such a question could be answered us-
ing the automatically extracted results, but might 
be more straightforwardly answered given hu-
man annotation. 
Sixty-four questions, plus an additional ten 
practice questions, were created using the engi-
neering approach. Additional criteria that were 
followed in creating the question sets were: 1) 
Questions had to contain at least one reasonable 
entry hook into all four databases, e.g., the terms 
U.S. and America were considered too broad to 
be reasonable; and, 2) For ease of scoring, list-
type questions had to specify the number of an-
swers required. Alternative criteria were consid-
ered but rejected because they correlated with 
the aforementioned set.  The following are ex-
amples of engineered questions. 
? Identify eight current or former U.S. State 
Department workers. 
? In what two West Bank towns does Fatah 
have an office? 
? Name two countries where Osama bin 
Laden has been. 
? Were Lebanese women allowed to vote in 
municipal elections between two Shiite 
groups in the year 1998? 
Two question lists, one with 86 questions 
generated by the random procedure and one with 
64 questions generated by the engineered proce-
dure, were analyzed with respect to the degree of 
support afforded by each of the four databases as 
viewed through FactBrowser. Four a priori cri-
teria were established to assess degree of support 
? or its opposite, the degree of expected diffi-
culty ? for each question in each of the four da-
tabases. Ranked from easiest to hardest, they are 
listed in Table 3. 
The question can be answered? 
1. Directly with fact or description (answer 
is highlighted in FactBrowser citation) 
2. Indirectly with fact or description (an-
swer is not highlighted) 
3. With name mentioned in question (long 
list of mentions without context) 
4. Via database crawling 
Table 3: A Priori Question Difficulty Character-
istics, listed from easiest to hardest 
Table 4 shows the question difficulty levels 
for both question types, for each of four data-
bases. Analysis of the engineered set was done 
on all 64 questions.  Analysis for randomly gen-
erated questions was done on a random sample 
of 44 of the 86 questions.  Fifteen questions did 
not meet the question criteria, leaving 29.  
The randomly generated questions showed a 
statistically significant, but small, variation in 
expected difficulty, in part due to the number of 
unanswerable questions. While the questions 
were made up with respect to information found 
in the documents, the process did not consider 
the types of extracted entities and relations. This 
problem might have been mitigated by limiting 
the search to questions involving entities and 
relations that were part of the extraction task. 
By contrast, the engineered question set 
showed a highly significant decrease in expected 
difficulty as the percentage of human annotation 
in the database increased (P < 0.0001 for chi-
square analysis). This result is not surprising, 
given that the questions were constructed with 
reference to the list of entities in the four data-
655
bases. The analysis confirms that the experimen-
tal manipulation of different degrees of support 
provided by the four databases was achieved for 
this question set. 
Random Question Generation 
Difficulty 
Level        
(easiest to 
hardest) 
0% 
Human 
33% 
Human 
67% 
Human 
100% 
Human 
1 Fact-
Highlight 
7 10 13 15
2 Fact-
Indirect 
14 10 8 10
3 Mention 3 5 2 1
4 Web Crawl 5 4 6 3
Total 29 29 29 29
     
Engineered Question Generation 
Difficulty 
Level               
(from easiest 
to hardest) 
0% 
Human 
33% 
Human 
67 
Human 
100% 
Human 
1 Fact-
Highlight 
16 25 35 49
2 Fact-
Indirect 
23 20 18 14
3 Mention 7 14 11 1
4 Web Crawl 18 5 0 0
Total 64 64 64 64
Table 4: Anticipated Difficulty of Questions as a 
Function of Database Quality 
Preliminary human testing with both question 
sets suggested that the a priori difficulty indica-
tors predict human question answering perform-
ance. Experiments with the randomly generated 
questions, therefore, were unlikely to reveal 
much about the databases or about human ques-
tion answering performance. On the other hand, 
an examination of how different levels of data-
base quality affect human performance, in a psy-
chophysical experiment where structure is varied 
systematically, promised to address the question 
of how much support is needed for good per-
formance. 
Based on the question difficulties, and pilot 
study timing and performance results, the 64 
questions were grouped into four, 16-question 
balanced sets. 
2.5 Procedure 
Participants were tested individually at our site, 
in sessions lasting roughly four hours. Training 
prior to the test lasted for approximately a half 
hour. Training consisted of a walk-through of 
the interface features followed by guided prac-
tice with sample questions. The test consisted of 
four question sets, each with a different data-
base.  Participants were informed that they 
would be using a different database for each 
question set and that some might be easier to use 
than others. 
Questions were automatically presented and 
responses were captured in AnswerPad, a soft-
ware tool designed for the study. AnswerPad is 
shown in Figure 2.  
Key features of the tool include: 
? Limiting view to current question set ? 
disallowing participants to view previous 
question sets 
? Automatically connecting to correct db 
? Logging time spent on each question 
? Enforcing five-minute limit per question 
? Enforcing requirement that all answers in-
clude a citation 
 
Figure 2: AnswerPad Question Presentation and 
Answer Capture Interface 
Participants were given written documenta-
tion as part of their training. The participants 
were instructed to cut-and-paste question an-
swers and document citations from source 
documents into AnswerPad. 
Extracted facts and entities, and source docu-
ments were accessed through FactBrowser. 
FactBrowser, shown in Figure 3, is web-browser 
based and is invoked via a button in AnswerPad. 
FactBrowser allows one to enter a string, which 
656
is matched against the database of entity men-
tions. The list of entities that have at least one 
mention partially matching the string are re-
turned (e.g., ?Laura Bush?) along with an icon 
indicating the type of the entity and the number 
of documents in which the entity appears.  
Clicking on the entity in the left panel causes the 
top right panel to display all of the descriptions, 
facts, and mentions for the entity. Selecting one 
of these displays citations in which the descrip-
tion, fact, or mention occurs. Clicking on the 
citation opens up a document view in the lower 
right corner of the screen and highlights the ex-
tracted information in the text. When a docu-
ment is displayed, all of the entities detected in 
the document are listed down the left side of the 
document viewer.  
 
 
Figure 3: Browsing Tool Interface 
The browsing tool was instrumented to record 
command invocations so that the path a partici-
pant took to answer a question could be recre-
ated, and the participant?s adherence to protocol 
could be verified. Furthermore, the find function 
(Ctrl-F) was disabled to prevent users from per-
forming ad hoc searches of the documents in-
stead of using the extracted data. 
The order of question sets and the order of da-
tabase conditions were counterbalanced across 
participants, so that, for every four participants, 
every question set and database appeared once in 
every ordinal position, and every question set 
was paired once with every database. This 
avoided carryover effects from question order. 
2.6 Data Collected 
Based on the initial results from Experiment 1, a 
70% target effectiveness threshold was identi-
fied to occur between the 33 and 67 database 
blends. To refine and verify this finding, Ex-
periment 2 examined the 0, 16.5, 33, and 50 da-
tabase blends. Experiment 3 examined the 33, 
41, 50, and 58 database blends. 
AnswerPad collected participant-provided an-
swers to questions and the corresponding cita-
tions. In addition, AnswerPad recorded the time 
spent answering the questions. A limit of five 
minutes was imposed based on pilot study re-
sults. The browsing tool logged commands in-
voked while the user searched the fact-base for 
question answers. Questions were manually 
scored based on the answers in the provided 
corpus. No partial credit was given. The maxi-
mum score, for each database condition, was 16, 
for a total maximum score of 64. 
3 Results 
Figure 4 shows the question answer scores 
and times for each of the three individual ex-
periments, and for Experiments 1 and 2 com-
bined. Database quality affects both task speed 
(downward-sloping line) and task accuracy (up-
ward-sloping line) in the expected direction. A 
logistic fit, as for a binary-response curve, was 
used to fit the relationship between blend per-
centage and accuracy in each experiment. The 
logistic fit Goodman-Theil quasi-R2 was .9973 
for Experiment 1, .9594 for Experiment 2, .8936 
for Experiment 3, and .9959 for Experiments 1 
and 2 combined. 
For the target accuracy of 70%, the 95% con-
fidence interval for the required blend is (35,56) 
around a predicted 46% blend for Experiment 1, 
and (41,56) around a predicted 49% for Experi-
ments 1 and 2 combined. 
 
657
Experiment 1 Performance and Time vs DB Blend
56
68
75
82
202
174
152
140
50
55
60
65
70
75
80
85
90
95
100
0.0 16.7 33.3 50.0 66.7 83.3 100.0
DB Blend (% Human)
P
er
fo
rm
an
ce
 (%
 C
or
re
ct
)
100
125
150
175
200
225
Ti
m
e 
(s
ec
on
ds
)
Experiment 2 Performance and Time vs DB Blend
52
61
64
70
210
187
183
166
50
55
60
65
70
75
80
85
90
95
100
0.0 16.7 33.3 50.0 66.7 83.3 100.0
DB Blend (% Human)
P
er
fo
rm
an
ce
 (%
 C
o
rr
ec
t)
100
125
150
175
200
225
T
im
e 
(s
ec
o
n
ds
)
Experiment 3 Performance and Time vs DB Blend
61
63
68 67
173
171
164
178
50
55
60
65
70
75
80
85
90
95
100
0.0 16.7 33.3 50.0 66.7 83.3 100.0
DB Blend (% Human)
P
er
fo
rm
an
ce
 (%
 C
or
re
ct
)
100
125
150
175
200
T
im
e 
(s
ec
o
n
d
s)
Experiments  1 & 2 Performance and Time vs DB Blend
54
61
66
70
75
82
206
187
179
166
152
140
50
55
60
65
70
75
80
85
90
95
100
0.0 16.7 33.3 50.0 66.7 83.3 100.0
DB Blend (% Human)
P
er
fo
rm
an
ce
 (%
 C
or
re
ct
)
100
125
150
175
200
225
Ti
m
e 
(s
ec
on
ds
)
% Correct Blend Lower Bound Logistic Fit for Blend
Blend Upper Bound Time  
Figure 4 QA Performance (upward-sloping) and QA Time (downward-sloping) vs. Extraction Blend 
Error Bars are Plus/Minus Standard Error of Mean (SEM) Within Each Blend 
Upper and Lower Bounds Are Approximate 95% Confidence Intervals Based on the Logistic Fit 
For the Blend (X) to Produce a Given Performance (Y) 
(Read these bounds horizontally, as bounds on X, with the upper bound to the right of the lower bound.) 
 
The downward-sloping line in each graph 
displays the average time to answer a question 
as a function of the extraction blend. For this 
analysis we used strict time, the time it took the 
participant to answer the question if he or she 
answered correctly, or the full 5 minutes allowed 
for any incorrectly answered question. This ad-
dresses the situation where a person quickly an-
swers all of the questions incorrectly.  The 
average question-answer time drops 32% as one 
moves from a machine generated extraction da-
tabase to a human generated database. A 
straight-line fit to the Experiment 1 and 2 com-
bined data predicts a drop of 6.5 seconds as the 
human proportion of the database increases by 
10 percentage points. 
A one-way repeated measures analysis of 
variance (ANOVA) was performed for Experi-
ment 1 (0-33-67-100), Experiment 2 (0-16.5-33-
50), and Experiment 3 (33-41-50-58). Table 5 
summarizes the results. In Experiments 1 and 2 
the impact of database quality on QA perform-
ance and on QA time were highly significant (P 
< 0.0001), but not for the narrower range of da-
tabases in Experiment 3. Other ANOVAs 
showed that the impact of trial order and ques-
tion set on QA performance were both non-
significant (P > 0.05). 
 
658
Experiment QA 
Performance 
Strict Time 
1 F(3,57) = 30.98, 
P < .0001  
F(3, 57) = 28.36 
P < .0001 
2 F(3,57)= 19.32, 
P < .0001  
F(3, 57) =  15.37,
P < .0001 
3 F(3,69)= 2.023, 
P = .1187 
F(3,69)= 1.053, 
P = .3747 
Table 5: ANOVA Analyses for QA Performance 
Expt. 1 used db blends of 0, 33, 67, and 100% 
Expt. 2 used db blends of 0, 16.5, 33, and 50% 
Expt. 3 used db blends of 33, 41, 50, and 58% 
In Experiment 1, Newman-Keuls contrasts 
indicate that the 0, 33, 67, and 100 databases 
differ significantly (P < .05) on their impact on 
QA quality. For Experiment 2, however, the 
16.5 and 33 database qualities were not shown to 
be different, nor were any of the database blends 
in Experiment 3. The data suggest that nearly 
half the improvement in QA quality from 0 to 
100 occurs by the 33 database blend, and more 
than half the improvement in QA quality from 0 
to 50 occurs by the 16.5 blend: a little ?human? 
goes a long way. Experiment 3 suggests that 
small differences in data blends make no practi-
cal difference in the results.  Alternatively, there 
might be real differences that are small enough 
such that a larger number of participants would 
be required to detect them. Experiment 3 also 
had two participants with atypical patterns of 
QA against blend, which might account for the 
failure to detect a difference between the 33 and 
50 or 58 blends as suggested by the results from 
Experiment 2. Furthermore, larger experiments 
could reveal whether the atypical participants 
were representatives of a subpopulation, or sim-
ply outliers. Bearing the possibility of outliers in 
mind, we used the combination of Experiments 
1 and 2 for the combined logistic analysis. 
4 Conclusions 
We presented a methodology for assessing in-
formation extraction effectiveness using an ex-
trinsic study. In addition, we demonstrated how 
a novel database blending (merging) strategy 
allows interpolating extraction quality from 
automated performance up through human accu-
racy, thereby decreasing the resources required 
to conduct effectiveness evaluations. 
Experiments showed QA accuracy and speed 
increased with higher IE performance, and that 
the database blend percentage was a good proxy 
for ACE value scores.  We emphasize that the 
study was not to show that IE supports QA bet-
ter than other technologies, rather to isolate util-
ity gains due to IE performance improvements. 
QA performance was plotted against human-
machine IE blend and, for example, 70% QA 
performance was achieved with a database blend 
between 41% and 46% machine extraction.  This 
corresponded to entity and relationship value 
scores of roughly 74 and 47 respectively. 
The logistic dose-response model provided a 
good fit and allowed for computation of confi-
dence bounds for the IE associated with a par-
ticular level of performance. The constraints 
imposed by AnswerPad and FactBrowser en-
sured that world knowledge was neutralized, and 
the repeated-measures design (using participants 
as their own controls across multiple levels of 
database quality) excluded inter-participant vari-
ability from experimental error, increasing the 
ability to detect differences with relatively small 
sample sizes. 
Acknowledgement 
This material is based upon work supported in 
part by the Department of the Interior under 
Contract No. NBCHC030014.  Any opinions, 
findings and conclusions or recommendations 
expressed in this material are those of the au-
thors and do not necessarily reflect the views of 
the Department of the Interior. 
References  
S. Miller, H. Fox, L. Ramshaw, and R. Weischedel, 
"A Novel Use of Statistical Parsing to Extract In-
formation from Text", in Proceedings of 1st Meet-
ing of the North American Chapter of the ACL, 
Seattle, WA., pp.226-233, 2000.  
S. Miller, S. Bratus, L. Ramshaw, R. Weischedel, and 
A. Zamanian. "FactBrowser Demonstration", Hu-
man Language Technology Conference, San 
Diego, 2001. 
D. Jones and E. Walton, ?Measuring the Utility of 
Human Language Technology for Intelligence 
Analysis,? 2005 International Conference on Intel-
ligence Applications, McLean, VA May, 2005. 
659
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 341?345,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Language Use: What can it Tell us? 
[name] 
[address1] 
[address2] 
[address3] 
[email] 
[name] 
[address1] 
[address2] 
[address3] 
[email] 
[name] 
[address1] 
[address2] 
[address3] 
[email] 
 
 
Abstract 
For 20 years, information extraction has fo-
cused on facts expressed in text. In contrast, 
this paper is a snapshot of research in progress 
on inferring properties and relationships 
among participants in dialogs, even though 
these properties/relationships need not be ex-
pressed as facts. For instance, can a machine 
detect that someone is attempting to persuade 
another to action or to change beliefs or is as-
serting their credibility? We report results on 
both English and Arabic discussion forums. 
1 Introduction 
Extracting explicitly stated information has been 
tested in MUC1 and ACE2 evaluations. For exam-
ple, for the text Mushaima'a, head of the opposi-
tion Haq movement, an ACE system extracts the 
relation LeaderOf(Mushaima'a, HaqMovement). In 
TREC QA3 systems answered questions, e.g.  
?When was Mozart born??, for which the answer is 
contained in one or a few extracted text phrases.  
Sentiment analysis uses implicit meaning of 
text, but has focused primarily on text known to be 
rich in opinions (product reviews, editorials) and 
delves into only one aspect of implicit meaning.  
Our long-term goal is to predict social roles in 
informal group discussion from language uses 
(LU), even if those roles are not explicitly stated; 
for example, using the communication during a 
meeting, identify the leader of a group. This paper 
provides a snapshot of preliminary, ongoing re-
search in predicting two classes of language use: 
                                                          
1
 http://www-nlpir.nist.gov/related_projects/muc/ 
2
 http://www.nist.gov/speech/tests/ace/ 
3
 http://trec.nist.gov/data/qa.html 
Establish-Credibility and Attempt-To-Persuade. 
Technical challenges include dealing with the facts 
that those LUs are rare and subjective and that hu-
man judgments have low agreement.  
Our hybrid statistical & rule-based approach 
detects those two LUs in English and Arabic. Our 
results are that (1) annotation at the message (turn) 
level provides training data useful for predicting 
rare phenomena at the discussion level while re-
ducing the requirement for turn-level predictions to 
be accurate; (2)weighing subjective judgments 
overcomes the need for high annotator consistency. 
Because the phenomena are rare, always predicting 
the absence of a LU is a very high baseline. For 
English, the system beats those baselines. For Ara-
bic, more work is required, since only 10-20% of 
the amount of training data exists so far.  
2 Language Uses (LUs) 
A language use refers to an aspect of the social 
intention of how a communicator uses language.  
The information that supports a decision about an 
implicit social action or role is likely to be distrib-
uted over more than one turn in a dialog; therefore, 
a language use is defined, annotated, and predicted 
across a thread in the dialog. Because our current 
work uses discussion forums, threads provide a 
natural, explicit unit of analysis. Our current work 
studies two language uses.  
An Attempt-to-Persuade occurs when a poster 
tries to convince other participants to change their 
beliefs or actions over the course of a thread. Typi-
cally, there is at least some resistance on the part of 
the posters being persuaded. To distinguish be-
tween actual persuasion and discussions that in-
volve differing opinions, a poster needs to engage 
341
in multiple persuasion posts (turns) to be consid-
ered exhibiting the LU.  
Establish-Credibility occurs when a poster at-
tempts to increase their standing within the group. 
This can be evidenced with any of several moves, 
e.g., explicit statements of authority, demonstration 
expertise through knowledge, providing verifiable 
information (e.g., from a trusted source or citing 
confirmable facts), or providing a justified opinion 
(e.g., a logical argument or personal experience).  
3 Challenges 
There were two significant challenges: (a) sparsity 
of the LUs, and (b) inter-annotator agreement. To 
address the sparsity of data, we tried to automati-
cally select data that was likely to contain content 
of interest. Data selection focused on the number 
of messages and posters in a thread, as well as the 
frequency of known indicators like quotations. 
(withheld). Despite these efforts, the LUs of inter-
est were rare, especially in Arabic.  
Annotation was developed using cycles of 
guideline development, annotation, evaluation of 
agreement, and revision of guidelines. Elsewhere, 
similar, iterative annotation processes have yielded 
significant improvements in agreement for word 
sense and coreference (Hovy et al, 2006). While 
LUs were annotated for a poster over the full 
thread, annotators also marked specific messages 
in the thread for presence of evidence of the lan-
guage use. Table 1 includes annotator consistency 
at both the evidence (message) and LU level.   
 English Arabic 
 Msg LU Msg LU 
 Agr # Agr # Agr # Agr # 
Per. 0.68 4722 0.75 2151 0.57 652 0.49 360 
Cred. 0.66 3594 0.68 1609 0.35 652 0.45 360 
Table 1: Number of Annotated Data Units and Annota-
tor Agreement (measured as F) 
The consistency numbers for this task were sig-
nificantly lower than we have seen in other lan-
guage processing tasks. Discussions suggested that 
disagreement did not come from a misunderstand-
ing of the task but was the result of differing intui-
tions about difficult-to-define labels. In the 
following two sections, we describe how the eval-
uation framework and system development pro-
ceeded despite low levels of consistency.  
4 Evaluation Framework 
Task. The task is to predict for every participant in 
a given thread, whether the participant exhibits 
Attempt-to-Persuade and/or Establish-Credibility. 
If there is insufficient evidence of an LU for a par-
ticipant, then the LU value for that poster is nega-
tive. The external evaluation measured LU 
predictions. Internally we measured predictions of 
message-level evidence as well. 
Corpora. For English, 139 threads from 
Google Groups and LiveJournal have been anno-
tated for Attempt-to-Persuade, and 103 threads for 
Attempt-to-Establish-Credibility. For Arabic, 
threads were collected from al-handasa.net.4 31 
threads were annotated for both tasks. Counts of 
annotated messages appear in Table 1. 
Measures. Due to low annotator agreement, at-
tempting to resolve annotation disagreement by the 
standard adjudication process was too time-
consuming. Instead, the evaluation scheme, similar 
to the pyramid scheme used for summarization 
evaluation, assigns scores to each example based 
on its level of agreement among the annotators. 
Specifically, each example is assigned positive and 
negative scores, p = n+/N and n = n-/N, where n+ is 
the number of annotators that annotate the example 
as positive, and n- for the negative. N is the total 
number of annotators. A system that outputs posi-
tive on the example results in p correct and n incor-
rect. The system gets p incorrect and n correct for 
predicting negative. Partial accuracy and F-
measure can then be computed. 
Formally, let X = {xi} be a set of examples. 
Each example xi is associated with positive and 
negative scores, pi and ni. Let ri = 1 if the system 
outputs positive for example xi and 0 for negative. 
The partial accuracy, recall, precision, and F-
measure can be computed by: 
pA = 100??i(ripi+(1-ri)ni) / ?i(pi+ni) 
pR = 100??iripi / ?ipi 
pP = 100? ?iripi / ?iri 
pF = 2 pR pP/(pR+pP) 
The maximum pA and pF may be less than 100 
when there is disagreement between annotators. To 
achieve accuracy and F scores on a scale of 100, 
pA and pF are normalized using the maximum 
achievable scores with respect to the data. 
npA = 100?pA/max(pA) 
npF = 100?pF/max(pF) 
                                                          
4
 URLs and judgments are available by email. 
342
5 System and Empirical Results 
Our architecture is shown in Figure 1. We process 
a thread in three stages: (1) linguistic analysis of 
each message (post) to yield features, (2) Predic-
tion of message-level properties using an SVM on 
the extracted features, and (3) Simple rules that 
predict language uses over the thread.  
 
Figure 1: Message and LU Prediction 
Phase 1: The SERIF Information Extraction 
Engine extracts features which are designed to cap-
ture different aspects of the posts. The features in-
clude simple features that can be extracted from 
the surface text of the posts and the structure of the 
posts within the threads. These may correlate di-
rectly or indirectly correlate to the language uses. 
In addition, more syntactic and semantic-driven 
features are also used. These can indicate the spe-
cific purpose of the sentences; specifically target-
ing directives, imperatives, or shows authority. The 
following is a partial list of features which are used 
both in isolation and in combination with each oth-
er. 
Surface and structural features: average sen-
tence length; number of names, pronouns, and dis-
tinct entities; number of sentences, URLs (links), 
paragraphs and out-of-vocabulary words; special 
styles (bold, italics, stereotypical punctuation e.g. 
!!!! ), depth in thread, and presence of a quotation. 
Syntactic and semantic features: predicate-
argument structure including the main verb, sub-
ject, object, indirect object, adverbial modifier, 
modal modifier, and negation, imperative verbs, 
injection words, subjective words, and mentions of 
attack events. 
Phase 2: Given training data from the message 
level (Section 3), an SVM predicts if the post con-
tains evidence for an LU. The motivation for this 
level is (1) Posts provide a compact unit with reli-
ably extractable, specific, explicit features. (2) 
There is more training data at the post level. (3) 
Pointing to posts offers a more clear justification 
for the predictions. (4) In our experiments, errors 
here do not seem to percolate to the thread level. In 
fact, accuracy at the message level is not directly 
predictive of accuracy at the thread level. 
Phase 3: Given the infrequency of the Attempt-
to-Persuade and Establish-Credibility LUs, we 
wrote a few rules to predict LUs over threads, giv-
en the predictions at the message level. For in-
stance, if the number of messages with evidence 
for persuasion is greater than 2 from a given partic-
ipant, then the system predicts AttemptToPer-
suade. Phase 3 is by design somewhat robust to 
errors in Phase 2. To predict that a poster is exhib-
iting the Attempt-to-Persuade LU, the system need 
not find every piece of evidence that the LU is pre-
sent, but rather just needs to find sufficient evi-
dence for identifying the LU.  
Our message level classifiers were trained with 
an SVM that optimizes F-measure (Joachims, 
2005). Because annotation disagreement is a major 
challenge, we experimented with various ways to 
account for (and make use of) noisy, dual annotat-
ed text. Initially, we resolved the disagreement au-
tomatically, i.e. removing examples with 
disagreement; treating an example as negative if 
any annotator marked the example negative; and 
treating an example as positive if any annotator 
marked the example as positive. An alternative 
(and more principled) approach is to incorporate 
positive and negative scores for each example into 
the optimization procedure. Because each example 
was annotated by the same number of annotators (2 
in this case), we are able to treat each annotator?s 
decision as an independent example without aug-
menting the SVM optimization process.  
The results below use the training procedure 
that performed best on the leave-one-thread-out 
cross validation results (Table 23 and Table 34). 
Counts of threads appear in Section 4. We compare 
our system?s performance (S) with two simple 
baselines. Baseline-A (A) always predicts absent 
for the LU/evidence. Baseline-P (P) predicts posi-
tive (present) for all messages/LUs. Table 4Table 3 
shows results for predicting message level evi-
dence of an LU (Phase 2). Table 5Table 4 shows 
performance on the task of predicting an LU for 
each poster. 
The results show significantly worse perfor-
mance in Arabic than English-- not surprising con-
sidering 5-10-fold difference in training examples. 
Additionally, Arabic messages are much shorter, 
and the phenomena is even more rare (as illustrated 
by the high npA, accuracy, of the A baseline).  
343
  Persuade Establish Credibility 
npA npF npA npF 
En Ar En Ar En Ar En Ar 
A 72.5 83.2 0.0 0.0 77.6 95.0 0.0 0.0 
P 40.4 29.7 61.1 50.7 33.9 14.4 54.5 30.9 
S 86.5 81.3 79.2 61.9 86.7 95.5 73.9 54.0 
Table 43: Performance on Message Level Evidence 
 Persuade Establish Credibility 
npA npF npA npF 
En Ar En Ar En Ar En Ar 
A 90.9 86.7 0.0 0.0 87.7 90.2 0.0 0.0 
P 12.1 27.0 23.8 48.2 18.0 21.5 33.7 41.1 
S 94.6 88.3 76.8 38.8 95.1 92.4 80.0 36.0 
Table 54: Cross Validation Performance on Poster LUs  
Table 6Table 5 shows LU prediction results 
from an external evaluation on held out data. Un-
like our dataset, each example in the external eval-
uation dataset was annotated by 3 annotators. The 
results are similar to our internal experiment. 
 Persuade Establish Credibility 
npA npF npA npF 
En Ar En Ar En Ar En Ar 
A 96.2 98.4 0.0 0.0 93.6 94.0 93.6 0.0 
P 13.1 4.2 27.6 11.7 11.1 10.1 11.1 22.2 
S 96.5 94.6 75.1 59.1 97.7 92.5 97.7 24.7 
Table 65: External, Held-Out Results on Poster LUs  
6 Related Research 
Research in authorship profiling (Chung & Penne-
baker, 2007; Argamon et al in press; and Abbasi 
and Chen, 2005) has identified traits, such as sta-
tus, sex, age, gender, and native language. Models 
and predictions in this field have primarily used 
simple word-based features, e.g. occurrence and 
frequency of function words. 
Social science researchers have studied how so-
cial roles develop in online communities (Fisher, et 
al., 2006), and have attempted to categorize these 
roles in multiple ways (Golder and Donath 2004; 
Turner et al, 2005). Welser et al (2007) have in-
vestigated the feasibility of detecting such roles 
automatically using posting frequency (but not the 
content of the messages). 
Sentiment analysis requires understanding the 
implicit nature of the text. Work on perspective 
and sentiment analysis frequently uses a corpus 
known to be rich in sentiment such as reviews or 
editorials (e.g. (Hardisty, 2010), (Somasundaran& 
Weibe, 2009). The MPQA corpus (Weibe, 2005) 
annotates polarity for sentences in newswire, but 
the focus of this corpus is at the sentence level. 
Both the MPQA corpus and the various corpora of 
editorials and reviews have tended towards more 
formal, edited, non-conversational text. Our work 
in contrast, specifically targets interactive discus-
sions in an informal setting. Work outside of com-
putational linguistics that has looked at persuasion 
has tended to examine language in a persuasive 
context (e.g. sales, advertising, or negotiations).  
Like the current work, Strzalkowski, et al 
(2010) investigates language uses over informal 
dialogue. Their work focuses on chat transcripts in 
an experimental setting designed to be rich in the 
phenomena of interest. Like our work, their predic-
tions operate over the conversation, and not a sin-
gle utterance. The specific language uses in their 
work (topic/task control, involvement, and disa-
greement) are different than those discussed here. 
Our work also differs in the data type of interest. 
We work with threaded online discussions in 
which the phenomena in question are rare. Our 
annotators and system must distinguish between 
the language use and text that is opinionated with-
out an intention to persuade or establish credibility.   
7 Conclusions and Future Work 
In this work in progress, we presented a hybrid 
statistical & rule-based approach to detecting prop-
erties not explicitly stated, but evident from lan-
guage use. Annotation at the message (turn) level 
provided training data useful for predicting rare 
phenomena at the discussion level while reducing 
the need for turn-level predictions to be accurate. 
Weighing subjective judgments overcame the need 
for high annotator consistency. For English, the 
system beats both baselines with respect to accura-
cy and F, despite the fact that because the phenom-
ena are rare, always predicting the absence of a 
language use is a high baseline. For Arabic, more 
work is required, particularly since only 10-20% of 
the amount of training data exists so far. 
This work has explored LUs, the implicit, social 
purpose behind the words of a message. Future 
work will explore incorporating LU predictions to 
predict the social roles played by the participants in 
a thread, for example using persuasion and credi-
bility to establish which participants in a discus-
sion are serving as informal leaders.  
344
Acknowledgement 
This research was funded by the Office of the Director 
of National Intelligence (ODNI), Intelligence Advanced 
Research Projects Activity (IARPA), through the _____.  
All statements of fact, opinion or conclusions contained 
herein are those of the authors and should not be con-
strued as representing the official views or policies of 
IARPA, the ODNI or the U.S. Government. 
References 
Argamon, S., Koppel, M., Pennebaker, J.W., and Schler, 
J. (2009). ?Automatically profiling the author of 
an anonymous text?. Communications of the Asso-
ciation for Computing Machinery (CACM). Vol-
ume 52 Issue 2. 
Abbasi A., and Chen H. (2005). ?Applying authorship 
analysis to extremist-group web forum messages?. 
In IEEE Intelligent Systems, 20(5), pp. 67?75. 
Boyd, D, Golder, S, and Lotan, G. (2010). ?Tweet, 
Tweet, Retweet: Conversational Aspects of Re-
tweeting on Twitter.? HICSS-43. IEEE: Kauai, HI. 
Chung, C.K., and Pennebaker, J.W. (2007). ?The psy-
chological functions of function words?. In K. 
Fiedler (Ed.), Social communication, pp. 343-359. 
New York: Psychology Press. 
Golder S., and Donath J. (2004) "Social Roles in Elec-
tronic Communities," presented at the Association 
of Internet Researchers (AoIR). Brighton, England 
Hovy E., Marcus M., Palmer M., Ramshaw L., and 
Weischedel R. (2006). ?Ontonotes: The 90% solu-
tion?. In Proceedings of the Human Language 
Technology Conference of the NAACL, Compan-
ion Volume: Short Papers, pp. 57?60. Association 
for Computational Linguistics, New York City, 
USA. 
Joachims, T. (2005), ?A Support Vector Method for 
Multivariate Performance Measures?, Proceedings 
of the International Conference on Machine 
Learning (ICML). 
Kelly, J., Fisher, D., Smith, D., (2006) ?Friends, foes, 
and fringe: norms and structure in political discus-
sion networks?, Proceedings of the 2006 interna-
tional conference on Digital government research.  
NIST Speech Group. (2008). ?The ACE 2008 evalua-
tion plan: Assessment of Detection and Recogni-
tion of Entities and Relations Within and Across 
Documents?. 
http://www.nist.gov/speech/tests/ace/2008/doc/ace
08 -evalplan.v1.2d.pdf 
Ranganath, R., Jurafsky, D., and McFarland, D. (2009) 
?It?s Not You, it?s Me: Detecting Flirting and its 
Misperception in Speed-Dates? Proceedings of the 
2009 Conference on Empirical Methods in Natural 
Language Processing, pages 334?342. 
Somasundaran, S & Wiebe, J
 
(2009). Recognizing 
Stances in Online Debates. ACL-IJCNLP 2009. 
Strzalkowski, T, Broadwell, G, Stromer-Galley, J, 
Shaikh, S, Taylor, S and Webb, N. (2010) ?Model-
ing Socio-Cultural Phenomena in Discourse?. 
Proceedings of the 23rd International Conference 
on Computational Linguistics (Coling 2010), pag-
es 1038?1046, Beijing, August 2010 
Turner T. C., Smith M. A., Fisher D., and Welser H. T. 
(2005) ?Picturing Usenet: Mapping computer-
mediated collective action?. In Journal of Com-
puter-Mediated Communication, 10(4). 
Voorhees, E. & Tice, D. (2000)."Building a Question 
Answering Test Collection", Proceedings of 
SIGIR, pp. 200-207. 
Welser H. T., Gleave E., Fisher D., and Smith M., 
(2007). "Visualizing the signatures of social roles in 
online discussion groups," In The Journal of Social 
Structure, vol. 8, no. 2. 
Wiebe, J, Wilson, T and Cardie, C (2005). Annotating 
expressions of opinions and emotions in language. 
Language Resources and Evaluation, volume 39, is-
sue 2-3, pp. 165-210. 
 
 
345
