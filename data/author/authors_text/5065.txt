The Annotation Graph Toolkit:
Software Components for
Building Linguistic Annotation Tools
Kazuaki Maeda, Steven Bird, Xiaoyi Ma and Haejoong Lee
Linguistic Data Consortium, University of Pennsylvania
3615 Market St., Philadelphia, PA 19104-2608 USA
fmaeda, sb, xma, haejoongg@ldc.upenn.edu
ABSTRACT
Annotation graphs provide an efficient and expressive data model
for linguistic annotations of time-series data. This paper reports
progress on a complete software infrastructure supporting the rapid
development of tools for transcribing and annotating time-series
data. This general-purpose infrastructure uses annotation graphs
as the underlying model, and allows developers to quickly create
special-purpose annotation tools using common components. An
application programming interface, an I/O library, and graphical
user interfaces are described. Our experience has shown us that it
is a straightforward task to create new special-purpose annotation
tools based on this general-purpose infrastructure.
Keywords
transcription, coding, annotation graph, interlinear text, dialogue
annotation
1. INTRODUCTION
Annotation graphs (AGs) provide an efficient and expressive
data model for linguistic annotations of time-series data [2]. This
paper reports progress on a complete software infrastructure sup-
porting the rapid development of tools for transcribing and anno-
tating time-series data. This general-purpose infrastructure uses
annotation graphs as the underlying model, and allows developers
to quickly create special-purpose annotation tools using common
components. This work is being done in cooperation with the
developers of other widely used annotation systems, Transcriber
and Emu [1, 3].
The infrastructure is being used in the development of a series
of annotation tools at the Linguistic Data Consortium. Several
such tools are shown in the paper: one for dialogue annotation,
one for telephone conversation transcription, and one for interlinear
transcription aligned to speech.
This paper will cover the following points: the application pro-
gramming interfaces for manipulating annotation graph data and
importing data from other formats; the model of inter-component
.
communication which permits easy reuse of software components;
and the design of the graphical user interfaces, which have been
tailored to be maximally ergonomic for the tasks.
The project homepage is: [http://www.ldc.upenn.edu/
AG/]. The software tools and software components described in
this paper are available through a CVS repository linked from this
homepage.
2. ARCHITECTURE
2.1 General Architecture
Existing annotation tools are based on a two level model (Fig-
ure 1 Top). The systems we demonstrate are based around a three
level model, in which annotation graphs provide a logical level
independent of application and physical levels (Figure 1 Bottom).
The application level represents special-purpose tools built on top
of the general-purpose infrastructure at the logical level.
The system is built from several components which instantiate
this model. Figure 2 shows the architecture of the tools currently
being developed. Annotation tools, such as the ones discussed
below, must provide graphical user interface components for signal
visualization and annotation. The communication between compo-
nents is handled through an extensible event language. An appli-
cation programming interface for annotation graphs (AG-API) has
been developed to support well-formed operations on annotation
graphs. This permits applications to abstract away from file format
issues, and deal with annotations purely at the logical level.
2.2 The Annotation Graph API
The complete IDL definition of the AG-API is provided in the
appendix (also online). Here we describe a few salient features of
the API.
The API provides access to internal objects (signals, anchors,
annotations etc) using identifiers. Identifiers are strings which con-
tain internal structure. For example, an AG identifier is quali-
fied with an AGSet identifier: AGSetId:AGId. Annotations and
anchors are doubly qualified: AGSetId:AGId:AnnotationId,
AGSetId:AGId:AnchorId. Thus, it is possible to determine from
any given identifiers, its membership in the overall data structure.
The functioning of the API will now be illustrated with a series
of examples. Suppose we have already constructed an AG and now
wish to create a new anchor. We might have the following API call:
CreateAnchor( "agSet12:ag5", 15.234, "sec" );
This call would construct a new anchor object and return its
identifier: agSet12:ag5:anchor34. Alternatively, if we already
Physical
Level
Application
Level
Query
Systems
Evaluation
Software
Annotation
ToolsExtraction
Systems
Visualization
& Exploration
Conversion
Tools
RDB
Format XML Tab delimited
flat files
Automatic
Aligners
Physical
Level
Application
Level
Logical
Level
Tab delimited
flat files
RDB
Format
XML
Query
Systems
Automatic
Aligners
Conversion
Tools
Extraction
Systems
Visualization
& Exploration
Evaluation
Software
Annotation
Tools
AG-API
Figure 1: The Two and Three-Level Architectures for Speech
Annotation
Figure 2: Architecture for Annotation Systems
have an anchor identifier that we wish to use for this new anchor
(e.g. because we are reading previously created annotation data
from a file and do not wish to assign new identifiers), then we could
have the following API call:
CreateAnchor( "agset12:ag5:anchor34", 15.234, "sec" );
This call will return agset12:ag5:anchor34.
Once a pair of anchors have been created it is possible to create
an annotation which spans them:
CreateAnnotation( "agSet12:ag5",
"agSet12:ag5:anchor34",
"agSet12:ag5:anchor35",
"phonetic" );
This call will construct an annotation object and return an iden-
tifier for it, e.g. agSet12:ag5:annotation41. We can now add
features to this annotation:
SetFeature( "agSet12:ag5:annotation41",
"date", "1999-07-02" );
The implementation maintains indexes on all the features, and
also on the temporal information and graph structure, permitting
efficient search using a family of functions such as:
GetAnnotationSetByFeature( "agSet12:ag5",
"date", "1999-07-02" );
2.3 A File I/O Library
A file I/O library (AG-FIO) to support creation and export of AG
data has been developed. This will eventually handle all widely
used annotation formats. Formats currently supported by the AG-
FIO library include the TIMIT, BU, Treebank, AIF (ATLAS Inter-
change Format), Switchboard and BAS Partitur formats.
2.4 Inter-component Communication
Figure 3 shows the structure of an annotation tool in terms of
components and their inter-communications.
Main program - a small script
Waveform
display
Transcription
editor
Internal
representation
File input
/ output
AG-GUI-API
AG-GUI-API AG-API
AG-FIO-API
Figure 3: The Structure of an Annotation Tool
The main program is typically a small script which sets up the
widgets and provides callback functions to handle widget events.
In this example there are four other components which are reused
by several annotation tools. The AG and AG-FIO components
have already been described. The waveform display component
(of which there may be multiple instances) receives instructions to
pan and zoom, to play a segment of audio data, and so on. The tran-
scription editor is an annotation component which is specialized for
a particular coding task. Most tool customization is accomplished
by substituting for this component.
Both GUI components and the main program support a com-
mon API for transmitting and receiving events. For example, GUI
components have a notion of a ?current region? ? the timespan
which is currently in focus. A waveform component can change
an annotation component?s idea of the current region by sending a
SetRegion event (Figure 4). The same event can also be used in
the reverse direction. The main program routes the events between
GUI components, calling the AG-API to update the internal repre-
sentation as needed. With this communication mechanism, it is a
straightforward task to add new commands, specific to the annota-
tion task.
Main program
Waveform display AG-API Transcription editor
User types Control-G Update Display
SetRegion t1 t2 AG::SetAnchorOffset SetRegion t1 t2
Update
Internal Representation
Figure 4: Inter-component Communication
2.5 Reuse of Software Components
The architecture described in this paper allows rapid develop-
ment of special-purpose annotation tools using common compo-
nents. In particular, our model of inter-component communica-
tion facilitates reuse of software components. The annotation tools
described in the next section are not intended for general purpose
annotation/transcription tasks; the goal is not to create an ?emacs
for linguistic annotation?. Instead, they are special-purpose tools
based on the general purpose infrastructure. These GUI com-
ponents can be modified or replaced when building new special-
purpose tools.
3. GRAPHICAL USER INTERFACES
3.1 A Spreadsheet Component
The first of the annotation/transcription editor components we
describe is a spreadsheet component. In this section, we show two
tools that use the spreadsheet component: a dialogue annotation
tool and a telephone conversation transcription tool.
Dialogue annotation consists of assigning a field-structured record
to each utterance in each speaker turn. A key challenge is to
handle overlapping speaker turns and back-channel cues without
disrupting the structure of individual speaker contributions. The
tool solves these problems and permits annotations to be aligned
to a (multi-channel) recording. The records are displayed in a
spreadsheet. Clicking on a row of the spreadsheet causes the corre-
sponding extent of audio signal to be highlighted. As an extended
recording is played back, annotated sections are highlighted (both
waveform and spreadsheet displays).
Figure 5 shows the tool with a section of the TRAINS/DAMSL
corpus [4]. Figure 6 shows another tool designed for transcribing
telephone conversations. This latter tool is a version of the dialogue
annotation tool, with the columns changed to accommodate the
needed fields: in this case, speaker turns and transcriptions. Both
of these tools are for two-channel audio files. The audio channel
corresponding to the highlighted annotation in the spreadsheet is
also highlighted.
3.2 An Interlinear Transcription Component
Interlinear text is a kind of text in which each word is anno-
tated with phonological, morphological and syntactic information
(displayed under the word) and each sentence is annotated with a
free translation. Our tool permits interlinear transcription aligned
to a primary audio signal, for greater accuracy and accountability.
Whole words and sub-parts of words can be easily aligned with
the audio. Clicking on a piece of the annotation causes the corre-
sponding extent of audio signal to be highlighted. As an extended
recording is played back, annotated sections are highlighted (both
waveform and interlinear text displays).
The following screenshot shows the tool with some interlinear
text from Mawu (a Manding language of the Ivory Coast, West
Africa).
Figure 7: Interlinear Transcription Tool
3.3 A Waveform Display Component
The tools described above utilize WaveSurfer and Snack devel-
oped by Ka?re Sjo?lander and Jonas Beskow [7, 8]. WaveSurfer
allows developers to specify event callbacks through a plug-in
architecture. We have developed a plug-in for WaveSurfer that
enables the inter-component communication described in this paper.
In addition to waveforms, it is also possible to show spectrograms
and pitch contours of a speech file if the given annotation task
requires phonetic analysis of the speech data.
4. FUTURE WORK
4.1 More GUI Components
In addition to the software components discussed in this paper,
we plan to develop more components to support various annotation
tasks. For example, a video component is being developed, and it
will have an associated editor for gestural coding. GUI components
for Conversation Analysis (CA) [6] and CHAT [5] are also planned.
4.2 An Annotation Graph Server
We are presently designing a client-side component which presents
the same AG-API to the annotation tool, but translates all calls
Figure 5: Dialogue Annotation Tool for the TRAINS/DAMSL Corpus
Figure 6: Telephone Conversation Transcription Tool for the CALLFRIEND Spanish Corpus
into SQL and then transmits them to a remote SQL server (see
Figure 8). A centralized server could house a potentially large
quantity of annotation data, permitting multiple clients to collabo-
ratively construct annotations of shared data. Existing methods for
authentication and transaction processing will be be used to ensure
the integrity of the data.
AG-API
Mapping to SQL
SQL
RDB server and
persistent storage
Main program - a small script
Waveform
display
Transcription
editor
File input
/ output
AG-GUI-API
AG-GUI-API
AG-FIO-API
network
Figure 8: Annotation Tool Connecting to Annotation Server
4.3 Timeline for Development
A general distribution (Version 1.0) of the tools is planned for the
early summer, 2001. Additional components and various improve-
ments will be added to future releases. Source code will be
available through a source code distribution service, SourceForge
([http://sourceforge.net/projects/agtk/]). Further
schedule for updates will be posted on our web site: [http:
//www.ldc.upenn.edu/AG/].
5. CONCLUSION
This paper has described a comprehensive infrastructure for
developing annotation tools based on annotation graphs. Our expe-
rience has shown us that it is a simple matter to construct new
special-purpose annotation tools using high-level software compo-
nents. The tools can be quickly created and deployed, and replaced
by new versions as annotation tasks evolve. The components and
tools reported here are all being made available under an open
source license.
6. ACKNOWLEDGMENT
This material is based upon work supported by the National
Science Foundation under Grant No. 9978056 and 9983258.
7. REFERENCES
[1] C. Barras, E. Geoffrois, Z. Wu, and M. Liberman. Transcriber:
development and use of a tool for assisting speech corpora
production. Speech Communication, 33:5?22, 2001.
[2] S. Bird and M. Liberman. A formal framework for linguistic
annotation. Speech Communication, 33:23?60, 2001.
[3] S. Cassidy and J. Harrington. Multi-level annotation of
speech: An overview of the emu speech database management
system. Speech Communication, 33:61?77, 2001.
[4] D. Jurafsky, E. Shriberg, and D. Biasca. Switchboard
SWBD-DAMSL Labeling Project Coder?s Manual, Draft 13.
Technical Report 97-02, University of Colorado Institute of
Cognitive Science, 1997. [http://stripe.colorado.
edu/?jurafsky/manual.august1.html].
[5] B. MacWhinney. The CHILDES Project: Tools for Analyzing
Talk. Mahwah, NJ: Lawrence Erlbaum., second edition, 1995.
[http://childes.psy.cmu.edu/].
[6] E. Schegloff. Reflections on studying prosody in
talk-in-interaction. Language and Speech, 41:235?60, 1998.
[http://www.sscnet.ucla.edu/soc/faculty/
schegloff/prosody/].
[7] K. Sjo?lander. The Snack sound toolkit, 2000.
[http://www.speech.kth.se/snack/].
[8] K. Sjo?lander and J. Beskow. WaveSurfer ? an open source
speech tool. In Proceedings of the 6th International
Conference on Spoken Language Processing, 2000.
[http://www.speech.kth.se/wavesurfer/].
APPENDIX
A. IDL DEFINITION FOR FLAT AG API
interface AG {
typedef string Id; // generic identifier
typedef string AGSetId; // AGSet identifier
typedef string AGId; // AG identifier
typedef string AGIds;
// AG identifiers (space separated list)
typedef string AnnotationId;
// Annotation identifier
typedef string AnnotationType; // Annotation type
typedef string AnnotationIds;
// Annotation identifiers (list)
typedef string AnchorId; // Anchor identifier
typedef string AnchorIds;
// Anchor identifiers (list)
typedef string TimelineId; // Timeline identifier
typedef string SignalId; // Signal identifier
typedef string SignalIds;
// Signal identifiers (list)
typedef string FeatureName; // feature name
typedef string FeatureNames; // feature name (list)
typedef string FeatureValue; // feature value
typedef string Features;
// feature=value pairs (list)
typedef string URI;
// a uniform resource identifier
typedef string MimeClass; // the MIME class
typedef string MimeType; // the MIME type
typedef string Encoding; // the signal encoding
typedef string Unit; // the unit for offsets
typedef string AnnotationRef;
// an annotation reference
typedef float Offset; // the offset into a signal
//// AGSet ////
// Id is AGSetId or AGId
AGId CreateAG( in Id id
in TimelineId timelineId );
boolean ExistsAG( in AGId agId );
void DeleteAG( in AGId agId );
AGIds GetAGIds( in AGSetId agSetId );
//// Signals ////
TimelineId CreateTimeline( in URI uri,
in MimeClass mimeClass,
in MimeType mimeType,
in Encoding encoding,
in Unit unit,
in Track track );
TimelineId CreateTimeline( in TimelineId timelineId,
in URI uri,
in MimeClass mimeClass,
in MimeType mimeType,
in Encoding encoding,
in Unit unit,
in Track track);
boolean ExistsTimeline( in TimelineId timelineId );
void DeleteTimeline( in TimelineId timelineId );
// Id may be TimelineId or SignalId
SignalId CreateSignal( in Id id,
in URI uri,
in MimeClass mimeClass,
in MimeType mimeType,
in Encoding encoding,
in Unit unit,
in Track track );
boolean ExistsSignal( in SignalId signalId );
void DeleteSignal( in SignalId signalId );
SignalIds GetSignals( in TimelineId timelineId );
MimeClass
GetSignalMimeClass( in SignalId signalId );
MimeType
GetSignalMimeType( in SignalId signalId );
Encoding GetSignalEncoding( in SignalId signalId );
string GetSignalXlinkType( in SignalId signalId );
string GetSignalXlinkHref( in SignalId signalId );
string GetSignalUnit( in SignalId signalId );
Track GetSignalTrack( in SignalId signalId );
//// Annotation ////
// Id may be AGId or AnnotationId
AnnotationId CreateAnnotation( in Id id,
in AnchorId anchorId1,
in AnchorId anchorId2,
in AnnotationType annotationType );
boolean ExistsAnnotation
(in AnnotationId annotationId );
void DeleteAnnotation
(in AnnotationId annotationId );
AnnotationId CopyAnnotation
(in AnnotationId annotationId );
AnnotationIds SplitAnnotation
(in AnnotationId annotationId );
AnnotationIds NSplitAnnotation(
in AnnotationId annotationId, in short N );
AnchorId
GetStartAnchor( in AnnotationId annotationId);
AnchorId GetEndAnchor(
in AnnotationId annotationId);
void SetStartAnchor( in AnnotationId annotationId,
in AnchorId anchorId );
void SetEndAnchor( in AnnotationId annotationId,
in AnchorId anchorId );
Offset
GetStartOffset( in AnnotationId annotationId );
Offset GetEndOffset(
in AnnotationId annotationId );
void SetStartOffset( in AnnotationId annotationId,
in Offset offset );
void SetEndOffset( in AnnotationId annotationId,
in Offset offset );
// this might be necessary to package up an id
// into a durable reference
AnnotationRef GetRef( in Id id );
//// Features ////
// this is for both the content of an annotation,
// and for the metadata associated with AGSets,
// AGs, Timelines and Signals.
void SetFeature( in Id id,
in FeatureName featureName,
in FeatureValue featureValue );
boolean ExistsFeature( in Id id,
in FeatureName featureName );
void DeleteFeature( in Id id,
in FeatureName featureName );
string GetFeature( in Id id,
in FeatureName featureName );
void UnsetFeature( in Id id,
in FeatureName featureName );
FeatureNames GetFeatureNames( in Id id );
void SetFeatures( in Id id,
in Features features );
Features GetFeatures( in Id id );
void UnsetFeatures( in Id id );
//// Anchor ////
// Id may be AGId or AnchorId
AnchorId CreateAnchor( in Id id,
in Offset offset,
in Unit unit,
in SignalIds signalIds );
AnchorId CreateAnchor( in Id id,
in SignalIds signalIds );
AnchorId CreateAnchor( in Id id );
boolean ExistsAnchor( in AnchorId anchorId );
void DeleteAnchor( in AnchorId anchorId );
void SetAnchorOffset( in AnchorId anchorId,
in Offset offset );
Offset GetAnchorOffset( in AnchorId anchorId );
void UnsetAnchorOffset( in AnchorId anchorId );
AnchorId SplitAnchor( in AnchorId anchorId );
AnnotationIds GetIncomingAnnotationSet(
in AnchorId anchorId );
AnnotationIds GetOutgoingAnnotationSet(
in AnchorId anchorId );
//// Index ////
AnchorIds GetAnchorSet( in AGId agId );
AnchorIds GetAnchorSetByOffset( in AGId agId,
in Offset offset,
in float epsilon );
AnchorIds GetAnchorSetNearestOffset(
in AGId agId,
in Offset offset );
AnnotationIds
GetAnnotationSetByFeature( in AGId agId,
in FeatureName featureName );
AnnotationIds
GetAnnotationSetByOffset( in AGId agId,
in Offset offset );
AnnotationIds
GetAnnotationSetByType( in AGId agId,
in AnnotationType annotationType );
//// Ids ////
// Id may be AGId, AnnotationId, AnchorId
AGSetId GetAGSetId( in Id id );
// Id may be AnnotationId or AnchorId
AGId GetAGId( in Id id );
// Id may be AGId or SignalId
TimelineId GetTimelineId( in Id id );
};
Annotation Tools Based on the Annotation Graph API
Steven Bird, Kazuaki Maeda, Xiaoyi Ma and Haejoong Lee
Linguistic Data Consortium, University of Pennsylvania
3615 Market Street, Suite 200, Philadelphia, PA 19104-2608, USA
fsb,maeda,xma,haejoongg@ldc.upenn.edu
Abstract
Annotation graphs provide an efficient
and expressive data model for linguistic
annotations of time-series data. This
paper reports progress on a complete
open-source software infrastructure
supporting the rapid development of
tools for transcribing and annotating
time-series data. This general-
purpose infrastructure uses annotation
graphs as the underlying model, and
allows developers to quickly create
special-purpose annotation tools using
common components. An application
programming interface, an I/O library,
and graphical user interfaces are
described. Our experience has shown
us that it is a straightforward task to
create new special-purpose annotation
tools based on this general-purpose
infrastructure.
1 Introduction
In the past, standardized file formats and coding
practices have greatly facilitated data sharing and
software reuse. Yet it has so far proved impossible
to work out universally agreed formats and codes
for linguistic annotation. We contend that this is a
vain hope, and that the interests of sharing and
reuse are better served by agreeing on the data
models and interfaces.
Annotation graphs (AGs) provide an efficient
and expressive data model for linguistic anno-
tations of time-series data (Bird and Liberman,
Figure 1: Architecture for Annotation Systems
2001). Recently, the LDC has been develop-
ing a complete software infrastructure supporting
the rapid development of tools for transcribing
and annotating time-series data, in cooperation
with NIST and MITRE as part of the ATLAS
project, and with the developers of other widely
used annotation systems, Transcriber and Emu
(Bird et al, 2000; Barras et al, 2001; Cassidy and
Harrington, 2001).
The infrastructure is being used in the devel-
opment of a series of annotation tools at the Lin-
guistic Data Consortium. Two tools are shown in
the paper: one for dialogue annotation and one
for interlinear transcription. In both cases, the
transcriptions are time-aligned to a digital audio
signal.
This paper will cover the following points: the
application programming interfaces for manipu-
lating annotation graph data and importing data
from other formats; the model of inter-component
communication which permits easy reuse of soft-
ware components; and the design of the graphical
user interfaces.
2 Architecture
2.1 General architecture
Figure 1 shows the architecture of the tools
currently being developed. Annotation tools,
such as the ones discussed below, must provide
graphical user interface components for signal
visualization and annotation. The communication
between components is handled through an
extensible event language. An application
programming interface for annotation graphs
has been developed to support well-formed
operations on annotation graphs. This permits
applications to abstract away from file format
issues, and deal with annotations purely at the
logical level.
2.2 The annotation graph API
The application programming interface provides
access to internal objects (signals, anchors,
annotations etc) using identifiers, represented
as formatted strings. For example, an AG
identifier is qualified with an AGSet identifier:
AGSetId:AGId. Annotations and anchors are
doubly qualified: AGSetId:AGId:AnnotationId,
AGSetId:AGId:AnchorId. Thus, the identifier
encodes the unique membership of an object in
the containing objects.
We demonstrate the behavior of the API with
a series of simple examples. Suppose we have
already constructed an AG and now wish to create
a new anchor. We might have the following API
call:
CreateAnchor("agSet12:ag5", 15.234, "sec");
This call would construct a new anchor object
and return its identifier: agSet12:ag5:anchor34.
Alternatively, if we already have an anchor iden-
tifier that we wish to use for the new anchor (e.g.
because we are reading previously created anno-
tation data from a file and do not wish to assign
new identifiers), then we could have the following
API call:
CreateAnchor("agset12:ag5:anchor34",
15.234, "sec");
This call will return agset12:ag5:anchor34.
Once a pair of anchors have been created it
is possible to create an annotation which spans
them:
CreateAnnotation("agSet12:ag5",
"agSet12:ag5:anchor34",
"agSet12:ag5:anchor35",
"phonetic" );
This call will construct an annotation
object and return an identifier for it, e.g.
agSet12:ag5:annotation41. We can now add
features to this annotation:
SetFeature("agSet12:ag5:annotation41",
"date", "1999-07-02" );
The implementation maintains indexes on all
the features, and also on the temporal information
and graph structure, permitting efficient search
using a family of functions such as:
GetAnnotationSetByFeature(
"agSet12:ag5", "date", "1999-07-02");
2.3 A file I/O library
A file I/O library (AG-FIO) supports input and
output of AG data to existing formats. Formats
currently supported by the AG-FIO library
include the TIMIT, BU, Treebank, AIF (ATLAS
Interchange Format), Switchboard and BAS
Partitur formats. In time, the library will handle
all widely-used signal annotation formats.
2.4 Inter-component communication
Figure 2 shows the structure of an annotation tool
in terms of components and their communication.
The main program is typically a small script
which sets up the widgets and provides callback
functions to handle widget events. In this
example there are four other components which
Main program - a small script
Waveform
display
Transcription
editor
Internal
representation
File input
/ output
AG-GUI-API
AG-GUI-API AG-API
AG-FIO-API
Figure 2: The Structure of an Annotation Tool
Main program
Waveform display AG-API Transcription editor
User types Control-G Update Display
SetRegion t1 t2 AG::SetAnchorOffset SetRegion t1 t2
Update
Internal Representation
Figure 3: Inter-component Communication
are reused by several annotation tools. The AG
and AG-FIO components have already been
described. The waveform display component (of
which there may be multiple instances) receives
instructions to pan and zoom, to play a segment
of audio data, and so on. The transcription
editor is an annotation component which is
specialized for a particular coding task. Most tool
customization is accomplished by substituting for
this component.
Both GUI components and the main program
support a common API for transmitting and
receiving events. For example, GUI components
have a notion of a ?current region? ? the
timespan which is currently in focus. A
waveform component can change an annotation
component?s idea of the current region by
sending a SetRegion event (Figure 3). The
same event can also be used in the reverse
direction. The main program routes the events
between GUI components, calling the annotation
graph API to update the internal representation as
needed. With this communication mechanism, it
is straightforward to add new commands, specific
to the annotation task.
2.5 Reuse of software components
The architecture described in this paper allows
rapid development of special-purpose annotation
tools using common components. In particular,
our model of inter-component communication
facilitates reuse of software components.
The annotation tools described in the next
section are not intended for general purpose
annotation/transcription tasks; the goal is not
to create an ?emacs for linguistic annotation?.
Instead, they are special-purpose tools based on
the general purpose infrastructure. These GUI
Figure 4: Dialogue Annotation Tool for the
TRAINS/DAMSL Corpus
components can be modified or replaced when
building new special-purpose tools.
3 Graphical User Interfaces
3.1 A spreadsheet component
Dialogue annotation typically consists of assign-
ing a field-structured record to each utterance in
each speaker turn. A key challenge is to handle
overlapping turns and back-channel cues without
disrupting the structure of individual speaker con-
tributions. The tool side-steps these problems by
permitting utterances to be independently aligned
to a (multi-channel) recording. The records are
displayed in a spreadsheet; clicking on a row of
the spreadsheet causes the corresponding extent
of audio signal to be highlighted. As an extended
recording is played back, annotated sections are
highlighted, in both the waveform and spread-
sheet displays.
Figure 4 shows the tool with a section of the
TRAINS/DAMSL corpus (Jurafsky et al, 1997).
Note that the highlighted segment in the audio
channel corresponds to the highlighted annotation
in the spreadsheet.
3.2 An interlinear transcription component
Interlinear text is a kind of text in which
each word is annotated with phonological,
morphological and syntactic information
(displayed under the word) and each sentence
is annotated with a free translation. Our tool
Figure 5: Interlinear Transcription Tool
permits interlinear transcription aligned to a
primary audio signal, for greater accuracy and
accountability. Whole words and sub-parts of
words can be easily aligned with the audio.
Clicking on a piece of the annotation causes
the corresponding extent of audio signal to be
highlighted. As an extended recording is played
back, annotated sections are highlighted (both
waveform and interlinear text displays).
The screenshot in Figure 5 shows the tool with
some interlinear text from Mawu (a Manding lan-
guage of the Ivory Coast, West Africa).
3.3 A waveform display component
The tools described above utilize WaveSurfer
and Snack (Sjo?lander, 2000; Sjo?lander and
Beskow, 2000). We have developed a plug-in
for WaveSurfer to support the inter-component
communication described in this paper.
4 Available Software and Future Work
The Annotation Graph Toolkit, version 1.0, con-
tains a complete implementation of the annota-
tion graph model, import filters for several for-
mats, loading/storing data to an annotation server
(MySQL), application programming interfaces in
C++ and Tcl/tk, and example annotation tools for
dialogue, ethology and interlinear text. The sup-
ported formats are: xlabel, TIMIT, BAS Parti-
tur, Penn Treebank, Switchboard, LDC Callhome,
CSV and AIF level 0. All software is distributed
under an open source license, and is available
from http://www.ldc.upenn.edu/AG/.
Future work will provide Python and Perl inter-
faces, more supported formats, a query language
and interpreter, a multichannel transcription tool,
and a client/server model.
5 Conclusion
This paper has described a comprehensive infras-
tructure for developing annotation tools based on
annotation graphs. Our experience has shown us
that it is a simple matter to construct new special-
purpose annotation tools using high-level soft-
ware components. The tools can be quickly cre-
ated and deployed, and replaced by new versions
as annotation tasks evolve.
Acknowledgements
This material is based upon work supported by the
National Science Foundation under Grant Nos.
9978056, 9980009, and 9983258.
References
Claude Barras, Edouard Geoffrois, Zhibiao Wu, and Mark
Liberman. 2001. Transcriber: development and use of a
tool for assisting speech corpora production. Speech
Communication, 33:5?22.
Steven Bird and Mark Liberman. 2001. A formal
framework for linguistic annotation. Speech
Communication, 33:23?60.
Steven Bird, David Day, John Garofolo, John Henderson,
Chris Laprun, and Mark Liberman. 2000. ATLAS: A
flexible and extensible architecture for linguistic annotation.
In Proceedings of the Second International Conference on
Language Resources and Evaluation. Paris: European
Language Resources Association.
Steve Cassidy and Jonathan Harrington. 2001. Multi-level
annotation of speech: An overview of the emu speech
database management system. Speech Communication,
33:61?77.
Daniel Jurafsky, Elizabeth Shriberg, and Debra Biasca.
1997. Switchboard SWBD-DAMSL Labeling Project
Coder?s Manual, Draft 13. Technical Report 97-02,
University of Colorado Institute of Cognitive Science.
[stripe.colorado.edu/?jurafsky/manual.august1.html].
Ka?re Sjo?lander and Jonas Beskow. 2000. Wavesurfer ? an
open source speech tool. In Proceedings of the 6th
International Conference on Spoken Language Processing.
http://www.speech.kth.se/wavesurfer/.
Ka?re Sjo?lander. 2000. The Snack sound toolkit.
http://www.speech.kth.se/snack/.
Proceedings of the 7th Workshop on Asian Language Resources, ACL-IJCNLP 2009, pages 55?62,
Suntec, Singapore, 6-7 August 2009. c?2009 ACL and AFNLP
Basic Language Resources for Diverse Asian Languages:
A Streamlined Approach for Resource Creation
Heather Simpson, Kazuaki Maeda, Christopher Cieri
Linguistic Data Consortium
University of Pennsylvania
3600 Market St., Suite 810
Philadelphia, PA 19104, USA
{hsimpson, maeda, ccieri}@ldc.upenn.edu
Abstract
The REFLEX-LCTL (Research on En-
glish and Foreign Language Exploitation-
Less Commonly Taught Languages) pro-
gram, sponsored by the United States
government, was an effort in simultane-
ous creation of basic language resources
and technologies for under-resourced lan-
guages, with the aim to enrich sparse ar-
eas in language technology resources and
encourage new research. We were tasked
to produce basic language resources for 8
Asian languages: Bengali, Pashto, Pun-
jabi, Tamil, Tagalog, Thai, Urdu and
Uzbek, and 5 languages from Europe and
Africa, and distribute them to research and
development also funded by the program.
This paper will discuss the streamlined ap-
proach to language resource development
we designed to support simultaneous cre-
ation of multiple resources for multiple lan-
guages.
1 Introduction
Over the past decade, the scope of interest in lan-
guage resource creation has increased across mul-
tiple disciplines. The differing approaches of these
disciplines are reflected in the terms used for newly
targeted groups of languages. Less commonly
targeted languages (LCTLs) research may focus
on development of basic linguistic technologies
or language-aware applications. The REFLEX-
LCTL (Research on English and Foreign Language
Exploitation-Less Commonly Taught Languages)
program, sponsored by the United States govern-
ment, was an effort in simultaneous creation of ba-
sic language resources and technologies for LCTLs,
namely languages which have large numbers of
speakers, but are nonetheless infrequently studied
by language learners or researchers in the U.S.
Under the REFLEX-LCTL program, we pro-
duced ?language packs? of basic language resources
for 13 such languages. This paper focuses on the
resources created for the 8 Asian languages: Ben-
gali, Pashto, Punjabi, Tamil, Tagalog, Thai, Urdu
and Uzbek.1
Our approach to language pack creation was to
maximize our resources. We accomplished this in a
number of ways. We engaged in thorough planning
to identify required tasks and their interdependen-
cies, and the human and technical resources needed
to accomplish them. We focused intensely on iden-
tifying available digital resources at the start of
language pack creation, so we could immediately
begin assessment of their usability, and work on
filling the resource gaps identified. We developed
annotation tools usable for multiple tasks and all
languages and designed to make manual annota-
tion more efficient. We developed standards for
data representations, to support efficient creation,
processing, and end use.
2 Planning For Basic Language
Resources Creation
2.1 Language Selection
The selection of REFLEX-LCTL target languages
was based on a number of criteria, while operating
within a fixed budget. The most basic criterion
was that the language be spoken by large num-
ber of native speakers, but poorly represented in
terms of available language resources. The Indic
languages (Bengali, Punjabi, Urdu) were chosen
to give researchers the opportunity to experiment
with bootstrapping techniques with resources in re-
lated languages. In order to maximize the useful-
ness and generality of our methods, the project
adopted the additional goals of variation in the ex-
pected availability of raw resources and also varia-
tion in linguistic characteristics both within the set
of selected languages and in comparison to more
well-resourced languages. Though we are focus-
ing, in this paper, on the Asian languages, LCTL
languages are linguistically and geographically di-
verse, representing major language families and
major geographical regions.
The following short descriptions of the Asian
LCTL languages are intended to provide some con-
text for the language pack discussions. The lan-
1The other languages were: Amazigh (Berber),
Hungarian, Kurdish, Tigrinya and Yoruba.
55
guage demographic information is taken from Eth-
nologue (Gordon, 2005).
2.2 Bengali
Bengali is spoken mostly in Bangladesh and India.
The language pack for Bengali was the first com-
plete language pack we created, and it served as
a pilot language pack for the rest of the project.
There were a relatively large number of raw mate-
rials and existing lexicons to support our lexicon
development, and our language pack included the
largest lexicon among the Asian language packs.
2.3 Urdu
Urdu, spoken primarily in Pakistan and part of
India, is closely related to Hindi. Urdu is tra-
ditionally written using Perso-Arabic script, and
has vocabulary borrowed from Arabic and Persian.
This was a language which had a large amount of
available digital resources in comparison with other
LCTLs, but did not meet our original expectations
for raw digital text.
2.4 Thai
Thai is a Tai-Kadai language spoken by more than
20 million people, mainly in Thailand. Thai was
another language which was relatively rich in avail-
able digital language resources. The Thai language
pack includes the largest amount of monolingual
text and found parallel text among the language
packs. For Thai, tokenization, or word segmenta-
tion, was probably the most challenging aspect of
the resource creation effort. For the initial version
of the language pack, we used a tokenization tool
adopted from an opensource software package.
2.5 Tamil
Tamil is a Dravidian language with more than 60
million speakers in India, Sri Lanka, Singapore and
other countries. We benefited from having local
language experts available for consultation on this
language pack
2.6 Punjabi
Punjabi, also written as Panjabi, is an Indo-
European language spoken in both India and Pak-
istan. Ethnologue and ISO 639-3 distinguish three
variations of Punjabi: Eastern, Mirpur and West-
ern, and the Eastern variation has the largest pop-
ulation of speakers. We were able to obtain rela-
tively large amounts of monolingual text and ex-
isting parallel text.
2.7 Tagalog
Tagalog is an Austronesian language spoken by 15
million people, primarily in the Philippines. The
monolingual text we produced is the smallest (774
K words) among the eight Asian language packs,
due in part to the prevalence of English in for-
mal communication mediums such as news publi-
cations.
2.8 Pashto
Pashto an Indo-European language spoken primar-
ily in Afghanistan and parts of Pakistan. It is one
of the two official languages in Afghanistan. Eth-
nologue and ISO 639-3 distinguish three varieties
of Pashto: Northern, Central and Southern. Ma-
jor sources of data for this language pack included
BBC, Radio Free America and Voice of America.
2.9 Uzbek
Uzbek is primarily spoken in Uzbekistan and in
other Asian republics of the former Soviet Union.
The creation of the language pack for Uzbek, which
is a Turkic language, and the official language of
Uzbekistan, was outsourced to BUTE (Budapest
University of Technology and Economics) in Hun-
gary. Even though the Uzbek government officially
decided to use a Latin script in 1992, the Cyrillic
alphabet used between the 1940?s and 1990?s are
still commonly found. Our language pack contains
all resources in Latin script and includes an en-
coding converter for converting between the Latin
script and the Cyrillic script.
2.10 Designing Language Packs
Within the REFLEX-LCTL program, a language
pack is a standardized package containing the fol-
lowing language resources:
? Documentation
? Grammatical Sketch
? Data
? Monolingual Text
? Parallel Text
? Bilingual Lexicon
? Named Entity Annotated Text
? POS Tagged Text
? Tools
? Tokenizer
? Sentence Segmenter
? Character Encoding Conversion Tool
? Name Transliterators
? Named Entity Tagger
? POS Tagger
? Morphological Analyzer
Grammatical sketches are summaries (approxi-
mately 50 pages) of the features of the written lan-
guage. The primary target audience are language
engineers with a basic grounding in linguistic con-
cepts.
56
Monolingual text is the foundation for all other
language pack resources. We provided monolingual
text in both tokenized and non-tokenized format.
Parallel text is an important resource for devel-
opment of machine translation technologies, and
allows inductive lexicon creation. The bilingual
lexicons also support a variety of language tech-
nologies. The named entity annotations and part
of speech tagged text can be used to create auto-
matic taggers.
The language packs also include basic data pro-
cessing and conversion tools, such as tokenizers,
sentence segmenters, character encoding convert-
ers and name transliterators, as well as more ad-
vanced tools, such as POS taggers, named entity
taggers, and morphological analyzers.
These language packs include 6 of the 9 text re-
sources and tools in 4 of the 15 text-based modules
listed in the current BLARK matrix (ELDA, 2008).
When we had a relatively stable definition of the
deliverables for language pack, we were able to be-
gin planning for the downstream processes
3 Standards for Data
Representation
An important step in planning was to define stan-
dards for language pack data representation, to
allow all downstream processes to run more effi-
ciently.
3.1 Language Codes
We decided to use the ISO 639-32 (also Ethnologue,
15th edition (Gordon, 2005)) three-letter language
codes throughout the language packs. For exam-
ple, the language code is stored in every text data
file in the language packs. The ISO 639-3 lan-
guage codes for our eight languages are as follows:
Urdu (URD), Thai (THA), Bengali (BEN), Tamil
(TAM), Punjabi (PAN), Tagalog (TGL), Pashto
(PBU) and Uzbek (UZN). When there were multi-
ple ISO 639-3 codes for a target language, the code
for the sublanguage for which the majority of the
written text can be obtained was used.
3.2 File Formats
One of the first tasks in planning for this data
creation effort was to define file formats for the
monolingual text, parallel text, lexicons and an-
notation files. This designing process was led by
us and a group of experts selected from the re-
search sites participating in the REFLEX-LCTL
program. The requirements included the follow-
ing:
? Monolingual and parallel text files should be
able to represent sentence segmentation, and
2See http://www.sil.org/iso639-3/ for more de-
tails
both tokenized and non-tokenized text.
? For parallel text, the text and the target lan-
guage and the translations in English should
be stored in separate aligned files.
? Unique IDs should be assigned to sen-
tences/segments, so that the segment-level
mapping in parallel text is clear.
? Annotation files should be in a stand-off for-
mat: i.e., annotations should be separate from
the source text files.
? Lexicon files should be able to represent at the
minimum of word, stem, part-of-speech, gloss
and morphological analysis.
? File formats should be XML-based.
? Files should be encoded in UTF-8 (UNI-
CODE).
After several cycles of prototyping and exchang-
ing feedback, we settled on the following original
file formats named ?LCTL text format? (LTF - file
name extension: .ltf.xml), ?LCTL annotation for-
mat? (LAF - file name extension: .laf.xml), and
?LCTL lexicon format? (LLF - file name exten-
sion .llf.xml. Appendix A shows the DTD for LTF
format.
3.3 Evaluation and Training Data
Partition
To support evaluation of language technologies
based on the data included in the language packs,
we designated approximately 10% of our primary
data as the evaluation data and the rest as the
training data. Any data that was included as ?as-
is?, (e.g. found parallel text), was included in the
training partition.
3.4 Directory Structure and File Naming
Conventions
Giving all language packs a consistent design and
structure allows users to navigate the contents with
ease. As such, we defined the directory structure
within each language pack to be the following.
The top directory was named as follows.
LCTL_{Language}_{Version}/
For example, the version 1.0 of the Urdu lan-
guage pack would have the top directory named
LCTL Urdu v1.0.
The top directory name is also used as the official
title for the package, so the full name rather than
the language code was used for maximum clarity
for users not familiar with the ISO coding.
Under the main directory, the following subdi-
rectories are defined:
57
Documentation/
Grammatical_Sketch/
Tools/
Lexicon/
Monolingual_Text/
Parallel_Text/
Named_Entity_Annotations/
POS_Tagged_Text/
The Parallel Text directory was divided into
?Found? and ?Translation? directories. The Found
directory contains parallel text that was available
as raw digital text, which we processed into our
standardized formats. The Translation directory
contains manually translated text, created by our
translators or subcontractors as well as part of
found parallel text which we were able to align
at the sentence-level. The data directories (e.g.,
Monolingual Text, Parallel Text, Named Entity
Annotation, POS Tagged Text) were further di-
vided into evaluation data (?Eval?) and training
data (?Train?) directories as requested by the pro-
gram.
We used the following format for text corpora
file names wherever possible:
{SRC}_{LNG}_{YYYYMMDD}.{SeqID}
{SRC} is a code assigned for the source; {LNG}
is the ISO 639-3 language code; {YYYYMMDD}
is the publication/posting date of the document;
and {SeqID} is a unique ID within the documents
from the same publication/posting date.
4 Building Technical Infrastructure
In creating the language resources included in the
LCTL language packs, we developed a variety of
software tools designed for humans, including na-
tive speaker informants without technical exper-
tise, to provide data needed for the resource cre-
ation efforts as efficiently as possible. In particular,
the following tools played crucial roles in the cre-
ation of language packs.
4.1 Annotation Collection Kit Interface
(ACK)
In order to facilitate efficient annotation of a vari-
ety of tasks and materials, we created a web-based
judgment/annotation tool, named the Annotation
Collection Kit interface (ACK). ACK allows a task
manager to create annotation ?kits,? which consist
of question text and predefined list and/or free-
form answer categories. Any UTF-8 text may be
specified for questions or answers. ACK is ideal
for remote creation of multiple types of text-based
annotation, by allowing individual ?kits? to be up-
loaded onto a specific server URL which any re-
mote user can access. In fact, using this tool
we were able to support native speaker annota-
tors working on part-of-speech (POS) annotation
in Thailand.
When annotators make judgments in ACK, they
are stored in a relational database. The results can
be downloaded in CSV (comma-separated value) or
XML format, so anyone with secure access to the
server can easily access the results.
ACK was designed so that anyone with even a
basic knowledge of a scripting language such as
Perl or Python would be able to create the ACK
annotation kits, which are essentially sets of data
corresponding to a sets of annotation decisions in
the form of radio buttons, check boxes, pull-down
menus, or comment fields. Indeed some of the lin-
guists on the LCTL project created their own ACK
kits when needed. Although they are limited in
scope, creative use of ACK kits can yield a great
deal of helpful types of annotation.
For example, for POS annotation, the annota-
tors were given monolingual text from our corpus,
word by word, in order, and asked to select the
correct part of speech for that word in context.
We also used ACK to add/edit glosses and part
of speech tags for lexicon entries, to perform mor-
phological tagging, and various other tasks that
required judgment from native speakers.
Figure 1: ACK - Annotation Collection Kit
Figure 1 shows a screen shot of ACK.
4.2 Named Entity Annotation Tool
For named entity annotation task, we chose to em-
ploy very simple annotation guidelines, to facili-
tate maximum efficiency and accuracy from native-
speaker annotators regardless of linguistic training.
We used an named entity (NE) annotation tool
called SimpleNET, which we previously developed
for the named entity annotation task for another
project. SimpleNET requires almost no training
in tool usage, and annotations can be made either
with the keyboard or the mouse. The NE anno-
tated text in the LCTL language packs was created
with this tool. This tool is written in Python us-
ing the QT GUI toolkit, which allows the display
58
of bidirectional text.
Figure 2: SimpleNET Annotation Tool
Figure 2 shows a screen shot of SimpleNET.
4.3 Named Entity Taggers and POS
Taggers
We created common development frameworks for
creating named entity taggers and part-of-speech
taggers for the LCTL languages. These frame-
works allowed us to create taggers for any new
language given enough properly-formatted training
data and test data. Included are core code written
in Java as well as data processing utilities written
in Perl and Python. The framework for creating
POS taggers was centered around the MALLET
toolkit (McCallum, 2002).3
4.4 Data Package Creation and
Distribution
As per LDC?s usual mechanisms for small corpora,
language packs were to be packaged as a tar gzip
(.tgz) file, and distributed to the REFLEX-LCTL
participating research sites. The distribution of the
completed languages packs were handled by our se-
cure web downloading system. Access instructions
were sent to the participating research sites, and
all downloads were logged for future reference.
5 Steps for Creating Each
Language Pack
5.1 Identifying Local Language Experts
and Native Speakers
An intermediate step between planning and cre-
ation was to identify and contact any available lo-
cal experts in the targeted languages, and recruit
additional native speakers to serve as annotators
and language informants. Annotators were not
necessarily linguists or other language experts, but
3We thank Partha Pratim Talukdar for providing
frameworks for creating taggers.
they were native speakers with reading and writing
proficiencies who received training as needed from
in-house language experts for creating our anno-
tated corpora, and helped to identify and evaluate
harvestable online resources.
Intensive recruiting efforts were conducted for
native speakers of each language. Our recruiting
strategy utilized such resources as online discussion
boards and student associations for those language
communities, and we were also able to capitalize
on the diversity of the student/staff body at the
University of Pennsylvania by recruiting through
posted signs on campus.
5.2 Identifying Available Language
Resources
The first step in creating each language pack was
to identify resources that are already available. To
this end we implemented a series of ?Harvest Fes-
tivals?; intensive sessions where our entire team,
along with native speaker informants, convened
to search the web for available resources. Avail-
able resources were immediately documented on
a shared and group editable internal wiki page.
By bringing together native speakers, linguists,
programmers, information managers and projects
managers in the same room, we were able to min-
imize communications latency, brainstorm as a
group, and quickly build upon each other?s efforts.
This approach was generally quite successful, es-
pecially for the text corpora and lexicons, and
brought us some of our most useful data.
5.3 Investigating Intellectual Property
Rights
As soon as Raw digital resources were identi-
fied, our local intellectual property rights special-
ist began investigation into their usability for the
REFLEX-LCTL language packs. It was necessary
to contact many individual data providers to ob-
tain an agreement, so the process was quite lengthy
and in some cases permission was not granted un-
til shortly before the package was scheduled for
release to the REFLEX community. Our general
practice was to process all likely candidate data
pools and remove data as necessary in later stages,
thus ensuring that IPR was not a bottleneck in
language pack creation. The exception to this was
for large data sources, where removal would have
significantly affected the quantity of data in the
deliverable.
5.4 Creating Basic Text Processing Tools
The next step was to create the language-specific
basic data processing tools, such as encoding con-
verter, sentence segmenter and tokenizer.
The goal for this project was to include whatever
encoding converters were needed to convert all of
59
the raw text and lexical resources collected or cre-
ated into the standard encoding selected for that
target language.
Dividing text into individual sentences is a nec-
essary first step for many processes including the
human translation that dominated much of our ef-
fort. Simple in principle, LCTL sentence segmen-
tation can prove tantalizingly complex. Our goal
was to produce a sentence segmenter that accepts
text in our standard encoding as input and outputs
segmented sentences in the same encoding.
Word segmentation, or tokenization, is also chal-
lenging for languages such as Thai. For Thai,
our approach was to utilize an existing opensource
word segmentation tool, and enhancing it by using
a larger word list than the provided one.
We designed the basic format conversion tools,
such as the text-to-LTF converter, to be able to
just plug in language-specific tokenizers and seg-
menters.
5.5 Creating Monolingual Text
The monolingual text corpora in the languages
packs were primarily created by identifying and
harvesting available resources from the Internet,
such as news, newsgroups and weblogs in the tar-
get language. Once the IPR expert determined
that we can use the resources for the REFLEX-
LCTL program, we harvested the document files ?
recent documents as archived documents. The har-
vested files were then analyzed and the files that
actually have usable contents, such as news arti-
cles and weblog postings were kept and converted
into the LCTL Text format. The formatting pro-
cess was typically done in the following steps: 1)
convert the harvested document or article in html
or other web format to a plain text format, strip-
ping html tags, advertisements, links and other
non-contents; 2) convert the plain text files into
UTF-8, 2) verify the contents with native speak-
ers, and if necessary, further remove non-contents,
or divide a file into multiple files; 3) convert the
plain text files into the LCTL Text format, apply-
ing sentence segmentation and tokenization. Each
document file is assigned a unique document ID.
Essential information about the document such as
the publication date was kept in the final files.
5.6 Creating Parallel Text
Each language pack contains at least 250,000 words
of parallel text. Part of this data was found re-
sources harvested from online resources, such as
bilingual news web site. The found parallel doc-
uments were converted into the LTF format, and
aligned at the sentence level, producing segment-
mapping tables between the LTF files in the LCTL
language and the LTF files in English.
The rest of this data was created by manually
translating documents in the LCTL language into
English, or documents in English into the LCTL
language. A subset of the monolingual text corpus
was selected for translation into English.
In addition, about 65,000 words of English
source text were selected as the ?Common English
Subset? for translation into each LCTL language.
Having the same set of parallel documents for all
languages will facilitate comparison between any
or all of the diverse LCTL languages. The Com-
mon Subset included : newswire text, weblogs, a
phrasebook and an elicitation corpus. The phrase-
book contained common phrases used in daily life,
such as ?I?m here?, and ?I have to go?. The
elicitation corpus, provided by Carnegie Mellon
University (Alvarez et al, 2006), included expres-
sions, such as ?male name 1 will write a book
for female name 1, where male name 1 and fe-
male name 1 are common names in the LCTL lan-
guage. The set of elicitation expressions is designed
to elicit lexical distinctions which differ across lan-
guages.
The manual translation tasks were outsourced
to translation agencies or independent translators.
Since the translators were instructed to translate
text which had already been sentence-segmented,
the creation of sentence-level mappings was trivial.
However, we found that it was important to create
a sentence-numbered template for the translators
to use, otherwise we were likely to receive trans-
lations where the source text sentence boundaries
were not respected.
5.7 Creating Lexicons
Bilingual lexicons are also an important resource
that can support a variety of human language tech-
nologies, such as machine translation and informa-
tion extraction. The goal for this resource was a
lexicon of at least 10,000 lemmas with glosses and
parts of speech for each language. For most of the
languages, we were able to identify existing lexi-
cons, either digital or printed, to consult with and
extract information for a subset of the lexical en-
tries; however, in all cases we needed to process
them substantially before they could be used effi-
ciently. We performed quality checking, normaliz-
ing, added parts of speech and glosses, added en-
tries and removed irrelevant entries.
5.8 Creating Annotated Corpora
A subset of the target language text in each lan-
guage pack received up to three types of annota-
tions: part-of-speech tags, morphological analysis,
and named entity tags. Named entity annotations
were created for all language packs.
Annotations were created by native speakers us-
ing the annotation tools discussed in section 4.
60
5.9 Creating Morphological Analyzers
To address the requirement to include some kind
of tool for morphological analysis in each language
pack, we created either a morphological analyzer
implementing hand-written rules or a stemmer us-
ing an unsupervised statistical approach, such as
the approach described in (Hammarstrom, 2006).
5.10 Creating Named Entity Taggers
We created a named entity tagger for each lan-
guage pack using our common development frame-
work for named entity taggers4.3. The tagger was
created using the named entity annotated text we
created for the language packs.
5.11 Creating Part-of-Speech Taggers
Similarly, we created a POS tagger for each lan-
guage pack using our common development frame-
work for POS taggers (See Section 4.3). We coor-
dinated the POS tag sets for the taggers and lexi-
cons.
5.12 Creating Name Transliterators
A transliterator that converts the language?s na-
tive script into Latin script is a desired resource.
For some languages, this is not a straightforward
task. For example, not all vowels are explicitly rep-
resented in Bengali script, and there can be mul-
tiple pronunciations possible for a given Bengali
character. Names, especially names foreign to the
target language exhibit a wide variety of spelling,
and in HLTs, make up a large percentage of the
out-of-vocabulary terms. We focused on creating
a transliterator to for romanization of names in the
LCTL languages. This resource was generally cre-
ated by the programming team with consultation
from native speakers.
5.13 Writing Grammatical Sketches
The grammatical sketches provide an outline of the
features of the written language, to provide the
language engineers with description of challenges
specific to the languages in creating language tech-
nologies. These sketches were written mainly by
senior linguists in our group, for readers who do
not necessarily have training in linguistics. The
format of these documents was either html or pdf.
6 Summary of Completed
Language Packs
Table 1 summarizes the contents of the 8 Asian
language packs .4 All of the language packs have
already been distributed to REFLEX-LCTL par-
ticipating research sites. The packs continue to
be used to develop and test language technologies.
For example, the Urdu pack was used to support a
4The numbers represent the number of tokens.
task in the 2006 NIST Open MT Evaluation cam-
paign (of Standards and Technology, 2009). Once
a language pack has been used for evaluation it will
be placed into queue for general release.
7 Conclusion
We have developed an efficient approach for creat-
ing basic text language resources for diverse lan-
guages. Our process integrated the efforts of soft-
ware programmers, native speakers, language spe-
cialists, and translation agencies to identify and
built on already available resources, and create new
resources as efficiently as possible.
Using our streamlined processes, we were able
to complete language packs for eight diverse Asian
languages. We hope that the completed resources
will provide valuable support for research and tech-
nology development for these languages.
We faced various challenges at the beginning of
the project which led us to revisions of our meth-
ods, and some of these challenges would surely be
encountered during a similar effort. We hope that
our approach as described here will be of service to
future endeavors in HLT development for under-
resourced languages.
References
Alison Alvarez, Lori S. Levin, Robert E. Fred-
erking, Simon Fung, and Donna Gates. 2006.
The MILE corpus for less commonly taught lan-
guages. In Proceedings of HLT-NAACL 2006.
ELDA. 2008. BLARK Resource/Modules
Matrix. From Evaluations and Language Re-
sources Distribution Agency (ELDA) web site
http://www.elda.org/blark/matrice res mod.php
, accessed on 2/23/2008.
Raymond G. Gordon, Jr., editor. 2005. Eth-
nologue: Languages of the World, Fifteenth
edition, Online version. SIL International.
http://www.ethnologue.com/.
Harald Hammarstrom, 2006. Poor Man?s Stem-
ming: Unsupervised Recognition of Same-Stem
Words. Springer Berlin / Heidelberg.
Andrew Kachites McCallum. 2002. MAL-
LET: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
National Institute of Standards and
Technology. 2009. NIST Open
Machine Translation Evaluation.
http://www.itl.nist.gov/iad/mig/tests/mt/,
accessed on June 7, 2009.
61
Urdu Thai Bengali Tamil Punjabi Tagalog Pashto Uzbek
Mono Text 14,804 39,700 2,640 1,112 13,739 774 5,958 790
Parallel Text (L ? E) 1,300 694 237 308 203 180 206
Parallel Text (Found) 947 1,496 243 230
Parallel Text (E ? L) 65 65 65 65 65 65 65 65
Lexicon 26 232 482 10 108 18 10 25
Encoding Converter Yes Yes Yes Yes Yes Yes Yes Yes
Sentence Segmenter Yes Yes Yes Yes Yes Yes Yes Yes
Word Segmenter Yes Yes Yes Yes Yes Yes Yes Yes
POS Tagger Yes Yes Yes Yes Yes Yes Yes Yes
POS Tagged Text 5 5 59
Morphological Analyzer Yes Yes Yes Yes Yes Yes Yes Yes
Morph-Tagged Text 11 144
NE Annotated Text 233 218 138 132 157 136 165 93
Named Entity Tagger Yes Yes Yes Yes Yes Yes Yes Yes
Name Transliterator Yes Yes Yes Yes Yes Yes Yes Yes
Descriptive Grammar Yes Yes Yes Yes Yes Yes Yes Yes
Table 1: Language Packs for Asian Languages (Data Volume in 1000 Words)
A DTD for LTF Files
<!ELEMENT LCTL_TEXT (DOC+) >
<!ATTLIST LCTL_TEXT lang CDATA #IMPLIED
source_file CDATA #IMPLIED
source_type CDATA #IMPLIED
author CDATA #IMPLIED
encoding CDATA #IMPLIED >
<!ELEMENT DOC (HEADLINE|DATELINE|AUTHORLINE|TEXT)+ >
<!ATTLIST DOC id ID #REQUIRED
lang CDATA #IMPLIED
>
<!ELEMENT HEADLINE (SEG+) >
<!ELEMENT DATELINE (#PCDATA) >
<!ELEMENT AUTHORLINE (#PCDATA) >
<!ELEMENT TEXT (P|SEG)+ >
<!ELEMENT P (SEG+) >
<!ELEMENT SEG (ORIGINAL_TEXT?, TOKEN*) >
<!ATTLIST SEG id ID #REQUIRED
start_token IDREF #IMPLIED
end_token IDREF #IMPLIED
start_char CDATA #IMPLIED
end_char CDATA #IMPLIED
>
<!ELEMENT ORIGINAL_TEXT (#PCDATA) >
<!ELEMENT TOKEN (#PCDATA) >
<!ATTLIST TOKEN id ID #REQUIRED
attach (LEFT|RIGHT|BOTH)
#IMPLIED
pos CDATA #IMPLIED
morph CDATA #IMPLIED
gloss CDATA #IMPLIED
start_char CDATA #IMPLIED
end_char CDATA #IMPLIED
>
62
