Hybrid Statistical and Structural Semantic Modeling for Thai Multi-
Stage Spoken Language Understanding 
 
Chai Wutiwiwatchai   and   Sadaoki Furui 
Department of Computer Science, Tokyo Institute of Technology 
2-12-1 Ookayama, Meguro-ku, Tokyo, 152-8552 Japan. 
{chai, furui}@furui.cs.titech.ac.jp 
 
Abstract 
This article proposes a hybrid statistical and 
structural semantic model for multi-stage 
spoken language understanding (SLU). The 
first stage of this SLU utilizes a weighted fi-
nite-state transducer (WFST)-based parser, 
which encodes the regular grammar of con-
cepts to be extracted. The proposed method 
improves the regular grammar model by in-
corporating a well-known n-gram semantic 
tagger. This hybrid model thus enhances the 
syntax of n-gram outputs while providing 
robustness against speech-recognition errors. 
With applications to a Thai hotel reservation 
domain, it is shown to outperform both indi-
vidual models at every stage of the SLU sys-
tem. Under the probabilistic WFST 
framework, the use of N-best hypotheses 
from the speech recognizer instead of the 1-
best can further improve performance requir-
ing only a small additional processing time. 
1 Introduction 
Automatic speech recognition (ASR) for Thai lan-
guage is still in the first stage, where Thai researchers 
in related fields have worked towards creating funda-
mental tools for language processing such as phono-
logical and morphological analyzers. Although Thai 
writing is an alphabetic system, a problem of writing 
without sentence markers or spaces between words has 
obstructed initiation of development of ASR. Pioneer-
ing a Thai spoken dialogue system has therefore be-
come a challenging task, where several unique 
components need to be developed specifically for a 
Thai system. 
Our prototype dialogue system, namely Thai Inter-
active Hotel Reservation Agent (TIRA), was created 
mainly by handcrafted rules. The first user evaluation 
(Wutiwiwatchai and Furui, 2003a) showed that the 
spoken language understanding (SLU) part of the sys-
tem proved the most problematic as it could not cover 
the variety of contents supplied by the users, especially 
when they talked in a mixed-initiative style. 
To rapidly improve performance, a trainable SLU 
model is preferable and it needs to be able to learn 
from a partially annotated corpus, where only essential 
keywords are given. This is particularly important for 
Thai where no large corpus is available. 
Recently, a novel multi-stage SLU model has been 
developed (Wutiwiwatchai and Furui, 2003b), which 
combines two different practices used for SLU-related 
tasks, robust semantic parsing and topic classification. 
The former paradigm was implemented in the concept 
extraction and concept-value recognition component, 
whereas the latter was applied for the goal identifica-
tion component. The concept extraction utilizes a set 
of weighted finite-state transducers (WFST) to encode 
possible word-syntax (or regular grammar) expressed 
for each concept. The concept WFST not only deter-
mines the existence of a concept in an input utterance, 
but also labels keywords used to construct its value in 
the concept-value recognition component. Given the 
extracted concepts, the goal of the utterance can be 
identified in the goal identification component using a 
generalized pattern classifier. 
This article reports an improvement of the concept 
extraction and concept-value recognition parts by con-
ducting a well-known statistical n-gram parser to com-
pensate for the concept expressions, which cannot be 
recognized by the ordinary concept WFST. The n-
gram modeling alone lacks structural information as it 
captures only up to n-word dependencies. Combining 
the statistical and structural model for SLU hence be-
comes a better alternative. Motivated by B?chet et al  
(2002), we propose a strategic way called logical n-
gram modeling, which combines the statistical n-gram 
with the existing regular grammar. In contrast to the 
regular-grammar approach, the probabilistic model 
allows the SLU to deal with ASR N-best hypotheses, 
resulting in an increment of the overall performance. 
Some related works are reviewed in the next sec-
tion, followed by a description of our multi-stage SLU 
model. Section 4 explains the proposed hybrid model. 
Section 5 shows the evaluation results with a conclu-
sion in section 6. 
2 Related Works 
In the technology of trainable or data-driven SLU, two 
different practices for different applications have been 
widely investigated. The first practice aims to tag the 
words (or group of words) in the utterance with se-
mantic labels, which are later converted to a certain 
format of semantic representation. To generate such a 
semantic frame, words in the utterance are usually 
aligned to a semantic tree by a parsing algorithm such 
as a probabilistic context free grammar or a recursive 
network whose nodes represent semantic symbols of 
the words and arcs consist of transition probabilities. 
During parsing, these probabilities are summed up, 
and used to determine the most likely parsed tree. 
Many understanding engines have been successfully 
implemented based on this paradigm (Seneff, 1992; 
Potamianos et al, 2000; Miller et al, 1994). A draw-
back of this method is, however, the requirement of a 
large, fully annotated corpus, i.e. a corpus with seman-
tic tags on every word, to ensure training reliability. 
The second practice has been utilized in applica-
tions such as call classification (Gorin et al, 1997). In 
this application, the understanding module aims to 
classify an input utterance to one of predefined user 
goals (if an utterance is supposed to have one goal) 
directly from the words contained in the utterance. 
This problem can be considered a simple pattern clas-
sification task. An advantage of this method is the 
need for training utterances tagged only with their 
goals, one for each utterance. However, another proc-
ess is required if one needs to obtain more detailed 
information. Our motivation for combining the two 
practices described above is that this allows the use of 
an only partially annotated corpus, while still allowing 
the system to capture sufficient information. The idea 
of combination has also been investigated in other 
works such as Wang et al (2002). 
Another issue related to this article is the combina-
tion of a statistical and rule-based approach for SLU, a 
system which is expected to improve the overall per-
formance over both individual approaches. The closest 
approach to our work was proposed by B?chet et al  
(2002), aiming to extract named-entities (NEs) from 
an input utterance. NE extraction is performed in two 
steps, detecting the NEs by a statistical tagger and ex-
tracting NE values using local models. Est?ve et al 
(2003) proposed a tighter coupling method that em-
beds conceptual structures into the ASR decoding 
network. Wang et al (2000), and Hacioglu and Ward 
(2001) proposed similar ideas for unified models that 
incorporated domain-specific context-free grammars 
(CFGs) into domain-independent n-gram models. The 
hybrid models thus improved the generalized ability of 
the CFG and specificity of the n-gram. With the exist-
ing regular grammar model in a weighted finite-state 
transducer (WFST) framework, we propose another 
strategy to incorporate the statistical n-gram model 
into the concept extraction and concept-value recogni-
tion components of our multi-stage SLU. 
3 Multi-Stage SLU 
In the design of our spoken dialogue system, the dia-
logue manager decides to respond to the user after 
perceiving the user goal. In some types of goal, infor-
mation items contained in the utterance are required 
for communication. For example the goal ?request for 
facilities? must come with the facilities the user is ask-
ing for, and the goal ?request for prerequisite keys? 
aims to have the user state the reserved date and the 
number of participants. Hence, the SLU module must 
be able to identify the goal and extract the required 
information items. 
We proposed a novel SLU model (Wutiwiwatchai 
and Furui, 2003b) that processes an input utterance in 
three stages, concept extraction, goal identification, 
and concept-value recognition. Figure 1 illustrates the 
overall architecture of the SLU model, in which its 
components are described in detail as follows: 
 
 
Figure 1. Overall architecture of the multi-stage SLU. 
3.1 Concept extraction 
The function of concept extraction is similar to that of 
other works, aiming to extract a set of concepts from 
an input utterance. However, our way to define a con-
cept is rather different. 
? A concept has a unique semantic meaning. 
? The order of concepts is not important. 
? Each type of concept occurs only once in an ut-
terance. 
? The semantic meaning of a concept can be inter-
preted from a sequence of words arbitrarily 
placed in the utterance (the sequence can overlap 
or cross each other). 
Examples of utterances and concepts contained in the 
utterances are shown in Table 1. A word sequence or 
 
 
Concept-value 
recognition 
Accepted  
substrings 
 
Goal Concept-values 
Concept extraction 
Concepts 
Word string 
Goal 
identification 
substring corresponding to the concept is presented in 
the form of a label sequence. The ??? and two-alphabet 
symbols such as ?fd? denote the words required to in-
dicate the concept. The two-alphabet symbols addi-
tionally specify keywords used for concept-value 
recognition. The ?-? is for other words not related to 
the concept. As defined above, a concept such as 
?reqprovide? (asking whether something is provided) is 
expressed by the substring ?there is ? right?, which 
contains two separated strings, ?there is? and ?right?. 
In the same utterance, another concept ?yesnoq? (ask-
ing by a yes-no question) also possesses the word 
?right?. We considered this method of definition to 
have more impact for presenting the meaning of con-
cepts, compared to what has been defined in other 
works. It must be noted that some concepts contain 
values such as the concept ?numperson? (the number of 
people), whereas some do not, such as the concept 
?yesnoq?. 
 
 
 
Figure 2. A portion of regular grammar WFST for the 
concept ?numperson? (the number of people). 
We implemented the concept extraction component 
by using weighted finite-state transducers (WFSTs). 
Similar to the implementation of salient grammar 
fragments in Gorin et al (1997), the possible word 
sequences expressed for a concept are encoded in a 
WFST, one for each type of concept. Figure 2 demon-
strates a portion of WFST for the concept ?numperson?. 
Each arc or transition of the WFST is labeled with an 
input word (or word class) followed after a colon by 
an output semantic label, and enclosed after a slash by 
a weight. A special symbol ?NIL? represents any word 
not included in the concept. The transitions, linking 
between the start and end node, characterize the ac-
ceptable word syntax. Weights of these transitions, 
except those containing ?NIL?, are assigned to be -1. 
The rest are assigned to have zero weights. The output 
labels indicate keywords as shown in Table 1. These 
labels will be used later by the concept-value recogni-
tion component. 
In the training step, each concept WFST was cre-
ated separately. The training utterances were tagged by 
marking just the words required by the concept. Then 
the WFST was constructed by: 
1. replacing the unmarked words in each training  
utterance by the symbols ?NIL?, 
2. making an individual FST for the preprocessed 
utterance, 
3. performing the union operation of all FSTs and 
determinizing the resulting FST, 
4. attaching the recursive-arcs of every word to 
the start and end node as illustrated in Fig. 2, 
5. assigning the weights to the transitions as  
described previously. 
In the parsing step, an input utterance is fed to 
every concept WFST in parallel. For each WFST, the 
words in the utterance that are not included in the 
WFST are replaced by the symbols ?NIL? and the pre-
processed word string is parsed by the WFST using the 
composition operation. By minimizing the cumulative 
weight, the longest accepted substring is chosen. A 
concept is considered to exist if at least one substring 
is accepted. Since this model is a kind of word-
grammar representation for a particular concept, we 
have called it the concept regular grammar or ?Reg? 
model in short. 
 
                               ?two  nights  from  the  sixth  of  July? 
Concept Keyword labels of accepted substring 
(1) reservedate 
    -        -          ?      ?       fd     ?    fm 
(2) numnight 
   nn      ?          -       -       -       -     - 
Goal inform_prerequisite-keys 
Label sequence 
 2:nn    2:?     1:?    1:?   1:fd  1:?  1:fm 
 
                               ?there   is    a   pool,  right?? 
Concept Keyword labels of accepted substring 
(1) reqprovide 
     ?      ?      -      -        ? 
(2) facility 
     -       -     ?      fc       -  
(3) yesnoq 
     -       -      -      -        ? 
Goal request_facility 
Label sequence 
   1:?   1:?   2:?  2:fc  1:?,3:? 
 
Table 1. Examples of defined goals, concepts and their 
corresponding substrings presented by keyword labels. 
3.2 Goal identification 
Having extracted the concepts, the goal of the utter-
ance can be identified. The goal in our case can be 
considered as a derivative of the dialogue act coupled 
with additional information. As the examples show in 
Table 1, the goal ?request_facility? means a request 
(dialogue act) for some facilities (additional informa-
tion). Since we observed in our largest corpus that 
only 1.1% were multiple-goal utterances, an utterance 
could be supposed to have only one goal. 
The goal identification task can be viewed as a 
simple pattern classification problem, where a goal is 
identified given an input vector of binary values indi-
cating the existence of predefined concepts. Our previ-
ous work (Wutiwiwatchai and Furui, 2003b) showed 
that this task could be efficiently achieved by the sim-
ple multi-layer perceptron type of artificial neural net-
work (ANN). 
DGT: np /-1 
I: ac /-1 DGT: nc /-1 
person: ? /-1 
NIL: ? /0 
S E 
friend: ac /-1 
I: ? /0 
NIL: ? /0 
DGT: ? /0 
? 
I :? /0 
NIL: ? /0 
DGT: ? /0 
? 
3.3 Concept-value recognition 
Recall again that some concepts contain values such as 
the concept ?numperson?, whose value is the number 
of people, whereas some concepts do not, such as the 
concept ?yesnoq?. Given an input utterance, the SLU 
module must be able to identify the goal and extract 
information items such as the reserved date, the num-
ber of people, the name of facility, etc. The concepts 
extracted in the first stage are not only used to identify 
the goal, but also strongly related to the described in-
formation items, that is, the values of concepts are 
actually the required information items. Hence, ex-
tracting the information items is to recognize the con-
cept values. 
Since the keywords within a concept have already 
been labeled by WFST composition in the concept 
extraction step, recognizing the concept-value is just a 
matter of converting the labeled keywords to a certain 
format. For sake of explanation, let?s consider the ut-
terance ?two nights from the sixth of July? in Table 1. 
After parsing by the ?reservedate? (the reserved date) 
concept WFST, the substring ?from the sixth of July? 
is accepted with the words ?sixth? and ?July? labeled 
by the symbols ?fd? and ?fm? respectively. These label 
symbols are specifically defined for each type of con-
cept and have their unique meanings, e.g. ?fd? for the 
check-in date, ?fm? for the check-in month, etc. The 
labeled keywords are then converted to a predefined 
format for the concept value. The value of ?reserve-
date? concept is in a form of <fy-fm-fd_ty-tm-td>, and 
thus the labeled keywords ?sixth(fd) July(fm)? is con-
verted to <04-07-06_ty-tm-td>. It must be noted that 
although the check-in year is not stated in the utterance, 
the concept-value recognition process under its knowl-
edge-base inherently assigns the value ?04? (the year 
2004) to the ?fy?. This process can greatly help in solv-
ing anaphoric expressions in natural conversation. Ta-
ble 2 gives more examples of substrings accepted and 
labeled by ?reservedate? WFST, and their correspond-
ing values. Currently, this conversion task is per-
formed by simple rules. 
 
Accepted substring Concept-value 
 
?sixth(fd) to eighth(td) of July(tm)? 
 
?check-in tomorrow(fd)? 
 
?until next Tuesday(td)? 
 
<04-07-06_04-07-08> 
 
<04-06-10_ty-tm-td> 
 
<fy-fm-fd_04-06-18> 
 
 
Table 2. Examples of substrings accepted by the ?re-
servedate? WFST with their corresponding values. 
4 Hybrid Statistical and Structural Se-
mantic Modeling 
Although the Reg model described in Sect. 3.1 has an 
ability to capture long-distant dependencies for seen 
grammar, it certainly fails to parse an unseen-grammar 
utterance, especially when it is distorted by speech 
recognition errors. This article thus presents an effort 
to improve concept extraction and concept-value 
recognition by incorporating a statistical approach. 
4.1 N-gram modeling 
We can view the concept extraction process as a se-
quence labeling task, where a label sequence L = (l1 ? 
lT) as shown in the ?Label sequence? lines of Table 1 
is determined given a word string W = (w1?wT). Each 
label, in the form of {c:l}, refers to the cth-concept 
with keyword label l. A word is allowed to be in mul-
tiple concepts, hence having multiple keyword labels 
such as {1:?,3: ?} as shown in the last line of Table 1. 
Finding the most probable sequence L is equivalent to 
maximizing the joint probability P(W,L), which can be 
simplified using n-gram modeling (n = 2 for bigram) 
as follows: 
?
=
??
==
T
t
tttt
LL
lwlwPLWPL
1
11 ),|,(maxarg),(maxarg~  
 (1) 
The described n-gram model, called ?Ngram? 
hereafter, can be implemented also by a WFST, whose 
weights are the smoothed n-gram probabilities. Parsing 
an utterance by the Ngram WFST is performed simply 
by applying the WFST composition in the same way 
as operated with the Reg model. 
4.2 Logical n-gram modeling 
Although the n-gram model can assign a likelihood 
score to any input utterance, it cannot distinguish be-
tween valid and invalid grammar structure. On the 
other hand, the regular grammar model can give se-
mantic tags to an utterance that is permitted by the 
grammar, but always rejects an ungrammatical utter-
ance. Thus, another probabilistic approach that inte-
grates the advantages of both models is optimum. 
Our proposed model, motivated mainly by (B?chet 
et al 2002), combines the statistical and structural 
models in two-pass processing. Firstly, the conven-
tional n-gram model is used to generate M-best hy-
potheses of label sequences given an input word string. 
The likelihood score of each hypothesis is then en-
hanced once its word-and-label syntax is permitted by 
the regular grammar model. By rescoring the M-best 
list using the modified scores, the syntactically valid 
sequence that has the highest n-gram probability is 
reordered to the top. Even if no label sequence is per-
mitted by the regular grammar, the hybrid model is 
still able to output the best sequence based on the 
original n-gram scores. Since the proposed model aims 
to enhance the logic of n-gram outputs, it is named the 
logical n-gram model. 
This idea can be implemented efficiently in the 
framework of WFST as depicted in Fig. 3. At first, the 
concept-specific Reg WFST is modified from the one 
shown in Fig. 2 by replacing the weight -1 by a vari-
able -?, which can be empirically adjusted to gain the 
best result. An unknown word string in the form of a 
finite state machine is parsed by the Ngram WFST, 
producing a WFST of M-best label-sequence hypothe-
ses. Concepts are detected in the top hypothesis. Then, 
the concept-value recognition process is applied for 
each detected concept separately. In the concept-value 
recognition process, the M-best WFST is intersected 
by the concept-specific Reg WFST. Rescoring the 
result offers a new WFST of P-best (P < M) hypothe-
ses with a score in logarithmic domain for each hy-
pothesis assigned by 
 
?
=
??
+=
T
t
ttttt lwlwPScore
1
11 )),|,((log ? , (2) 
where }0,{?? ?t . If ? is set to 0, the intersection op-
eration is just to filter out the hypotheses that violate 
the regular grammar, while the original scores from n-
gram model are left unaltered. If a larger ? is used, the 
hypothesis that contains a longer valid syntax is given 
a higher score. When no hypothesis in the M-best list 
is permitted by the grammar (P = 0), the top hypothe-
sis of the M-best list is outputted. It is noted that the 
strategy of eliminating unacceptable paths of n-gram 
due to syntactical violation has also successfully been 
used in a WFST-based speech recognition system 
(Szarvas and Furui, 2003). Hereafter, we will refer to 
the logical n-gram modeling as ?LNgram?. 
4.3 The use of ASR N-best hypotheses 
The probabilistic model allows the use of N-best hy-
potheses from the automatic speech recognition (ASR) 
engine. As described in Sect. 4.1, our Ngram semantic 
model produces a joint probability P(W,L), which in-
dicates the chance that the semantic-label sequence L 
occurs with the word hypothesis W. When the N-best 
word hypotheses generated from the ASR are fed into 
the Ngram semantic parser, the parsed scores are 
combined with the ASR likelihood scores in a log-
linear interpolation fashion (Klakow, 1998) as shown 
in Eq. 3. 
?? ?
??
?
1
,
),(),(maxarg~ LWPWAPL
NWL
 (3) 
where A is an acoustic speech signal, and P(A,W) is a 
product of an acoustic score P(A|W) and a language 
score P(W). ?N denotes the N-best list and ? is an in-
terpolation weight, which can be adjusted experimen-
tally to give the best result. This interpolation method 
can be easily implemented in a WFST framework 
compared to normal linear interpolation. 
An N-best list can be used in the LNgram using 
the same criterion as well. The only necessary precau-
tion is an appropriate size of M in the M-best seman-
tic-label list, which is rescored in the second pass to 
improve the concept-value result. 
 
 
Figure 5. Logical n-gram modeling. 
5 Evaluation and Discussion 
5.1 Corpora 
Collecting and annotating a corpus is an especially 
serious problem for language like Thai, where only 
few databases are available. To shorten the collection 
time, we created a specific web page simulating our 
expected conversational dialogues, and asked Thai 
native users to answer the dialogue questions by typ-
ing. As we asked the users to try answering the ques-
tions using spoken language, we could obtain a fairly 
good corpus for training the SLU. 
Currently, 5,869 typed-in utterances from 150 us-
ers have been completely annotated. To reduce the 
effort of manual annotation, we conducted a semi-
automatic annotation method. The prototype rule-
based SLU was used to roughly tag each utterance 
with a goal and concepts, which were then manually 
corrected. Words or phases that were relevant to the 
concept were marked automatically based on their 
frequencies and information mutual to the concept. 
Finally the tags were manually checked and the key-
words within each concept were additionally marked 
by the defined label symbols. 
All 5,869 utterances described above were used as 
a training set (TR) for the SLU system. We also col-
lected a set of speech utterances during an evaluation 
of our prototype dialogue system. It contained 1,101 
speech utterances from 96 dialogues. By balancing the 
 Semantic-label tagging 
by the Ngram model 
The top hypothesis 
M-best hypotheses 
 
Rescoring by each 
concept Reg model 
Extracted 
concepts 
 
Converting keyword seq. 
to concept values 
Concept values 
Concept-value 
recognition 
Word string 
Concept 
extraction 
Concept 
Reg models 
occurrence of goals, we reserved 500 utterances for a 
development set (DS), which was used for tuning pa-
rameters. The remaining 601 utterances were used for 
an evaluation set (ES). Table 3 shows the characteris-
tics of each data set. From the TR set, 75 types of con-
cepts and 42 types of goals were defined. The out-of-
goal and out-of-concept denote goals and concepts that 
are not defined in the TR set, and thus cannot be rec-
ognized by the trained SLU. Since concepts that con-
tain no value are not counted for concept-value 
evaluation, Table 3 also shows the number of concepts 
that contain values in the line ?# Concept-values?. 
 
Characteristic TR DS ES 
# Utterances 5,869 500 601 
# Words / utterance 7.3 6.2 5.8 
# Goal types 42 40 40 
# Concept types 75 58 57 
# Concept-value types 20 18 18 
# Concepts 10,041 791 949 
# Concept-values 6,365 366 439 
% Out-of-goal  5.2 5.3 
% Out-of-concept  2.8 3.3 
% Word accuracy  77.2 79.0 
 
Table 3. Characteristics of data sets 
5.2 Evaluation measures 
Four measures were used for evaluation: 
1. Word accuracy (WAcc) ? the standard measure 
for evaluating the ASR, 
2. Concept F-measure (ConF) ? the F-measure of 
detected concepts, 
3. Goal accuracy (GAcc) ? the number of 
utterances with correctly identified goals, 
divided by the total number of test utterances, 
4. Concept-value accuracy (CAcc) ? the number 
of concepts, whose values are correctly 
matched to their references, divided by the total 
number of concepts that contain values. 
5.3 The use of logical n-gram modeling 
The first experiment was to inspect improvement 
gained after conducting the statistical approaches for 
concept extraction and concept-value recognition. 
Only the 1-best word hypothesis from the ASR was 
experimented in this section. The AT&T generalized 
FSM library (Mohri et al, 1997) was used to construct 
and operate all WFSTs, and the SNNS toolkit (Zell et 
al., 1994) was used to create the ANN classifiers for 
the goal identification task. 
The baseline system utilized the Reg model for 
concept extraction and concept-value recognition, and 
the multi-layer perceptron ANN for goal identification. 
75 WFSTs corresponding to the number of defined 
concepts were created from the TR set. The ANN con-
sisted of a 75-node input layer, a 100-node hidden 
layer (Wutiwiwatchai and Furui, 2003b), and a 42-
node output layer equal to the number of goals to be 
identified. 
66
68
70
72
74
76
10 20 30 40 50 60 70 80 90 100
M -best
CA
c
c 
(%
)
 
Figure 4. CAcc results with respect to values of M in 
an oracle test for the DS set. 
58
59
60
61
62
63
64
0.
0
0.
1
0.
2
0.
3
0.
4
0.
5
0.
6
0.
7
0.
8
0.
9
1.
0
1.
1
1.
2
CA
cc
 
(%
)
 
          ? 
Figure 5. CAcc results with variation of ? for the DS 
set when M is set to 80. 
Recognition Orthography Measure 
Reg Ngram LNgram Reg LNgram 
ConF 76.5 88.6 78.9 91.4 
GAcc 71.4 76.0 81.2 83.5 
CAcc 65.1 52.4 67.2 75.7 76.8 
 
Table 4. Evaluation results for the ES set using the 
Reg, Ngram, and LNgram models. 
Another WFST was constructed for the n-gram 
semantic parser (n = 2 in our experiment), which was 
used for the Ngram model and the first pass of the 
LNgram model. Two parameters, M and ?, in the 
LNgram approach need to be adjusted. To determine 
an appropriate value of M, we plotted in an oracle 
mode the CAcc of the DS set with respect to M, as 
shown in Figure 4. According to the graph, an M of 80 
was considered optimum and set for the rest of the 
experiments. Figure 5 then shows the CAcc obtained 
for rescored M-best hypotheses when the weight ? as 
defined in Eq. 2 is varied. Here, the larger value of ? 
means to assign a higher score to the hypothesis that 
contains longer valid word-and-label syntax. Hence, 
we concluded by Fig. 5 that reordering the hypotheses, 
which contain longer valid syntaxes, could improve 
the CAcc significantly. Since the CAcc results become 
steady when the value of ? is greater than 0.7, a ? of 
1.0 is used henceforth to ensure the best performance. 
The overall evaluation results on the ES set are 
shown in Table 4, where M and ? in the LNgram 
model are set to 80 and 1.0 respectively. ?Recognition? 
denotes the experiments on automatic speech-
recognized utterances (at 79% WAcc), whereas ?Or-
thography? means their exact manual transcriptions. It 
is noted that the LNgram approach utilizes the same 
process of Ngram in its first pass, where the concepts 
are determined. Therefore, the ConF and GAcc results 
of both approaches are the same. 
According to the results, the Ngram tagger worked 
well for the concept extraction task as it increased the 
ConF by over 10%. The improvement mainly came 
from reduction of redundant concepts often accepted 
by the Reg model. The better extraction of concepts 
could give better goal identification accuracy reasona-
bly. However, as we expected, the conventional 
Ngram model itself had no syntactic information and 
thus often produced a confusing label sequence, espe-
cially for ill-formed utterances. A typical error oc-
curred for words that could be tagged with one of 
several semantic labels, such as the word ?MNT? (re-
ferring to the name of the month), which could be 
identified as ?check-in month? or ?check-out month?. 
These two alternatives could only be clarified by a 
context word, which sometimes located far from the 
word ?MNT?. This problem could be solved by using 
the Reg model. The Reg model, however, could not 
provide a label sequence to any out-of-syntax sentence. 
The LNgram as an integration of both models thus 
obviously outperformed the others. 
In conclusion, the LNgram model could improve 
the ConF, GAcc, and CAcc by 15.8%, 6.4%, and 3.2% 
relative to the baseline Reg model. Moreover, if we 
considered the orthography result an upperbound of 
the underlying model, the GAcc and CAcc results pro-
duced by the LNgram model are relatively closer to 
their upperbounds compared to the Reg model. This 
verifies robustness improvement of the proposed 
model against speech-recognition errors. 
5.4 The use of ASR N-best hypotheses 
To incorporate N-best hypotheses from the ASR to the 
LNgram model, we need to firstly determine an ap-
propriate value of N. An oracle test that measures 
WAcc and ConF for the DS set with variation of N is 
shown in Fig. 6. Although we can select a proper value 
of N by considering only the WAcc, we also examine 
the ConF to ensure that the selected N provides possi-
bility to improve the understanding performance as 
well. According to Fig. 6, the ConF highly correlates 
to the WAcc, and an N of 50 is considered optimum 
for our task. At this operating point, we plot another 
curve of ConF for the DS set with a variation of ?, the 
interpolation weight in Eq. 3, as shown in Fig. 7. The 
appropriate value of ? is 0.6, as the highest ConF is 
obtained at this point. The last parameter we need to 
adjust is the value of M. Although we have tuned the 
value of M for the case of 1-best word hypothesis, the 
appropriate value of M may change when the N-best 
hypotheses are used instead. However, in our trial, we 
found that the optimum value of M is again in the 
same range as that operated for the 1-best case. A 
probable reason is that rescoring the N-best word hy-
potheses by the Ngram model can reorder the good 
hypotheses to a certain upper portion of the N-best list, 
and thus rescoring in the second pass of the LNgram 
is independent to the value of N. Consequently, an M 
of 80 as that selected for the 1-best hypothesis is also 
used for the N-best case. 
75
80
85
90
95
100
1 11 21 31 41 51 61 71 81 91
N -best
%
WAcc
ConF
 
Figure 6. WAcc and ConF results with respect to val-
ues of N in an oracle test for the DS set. 
85
86
87
88
89
90
0.
0
0.
1
0.
2
0.
3
0.
4
0.
5
0.
6
0.
7
0.
8
0.
9
1.
0
Co
n
F 
(%
)
50-best
1-best
 
   ? 
Figure 7. ConF results with variation of ? for the DS 
set when N is set to 50. 
Given all tuned parameters, an evaluation on the 
ES set is carried out as shown in Fig. 8. With the Reg 
model as a baseline system, the use of N-best hypothe-
ses further improves the the ConF, GAcc, and CAcc 
by 0.9%, 0.6%, and 3.9% from the only 1-best, and 
hence reduces the gap between the speech-recognized 
test set and the orthography test set by 25%, 5.3%, and 
26% respectively. 
Finally, we would like to note that the proposed 
LNgram approach provided the significant advantage 
of a much smaller computational time compared to the 
original Reg approach. While the Reg model requires 
C times (C denotes the number of defined concepts) of 
WFST operations to determine concepts, the LNgram 
needs only D+1 times (D << C), where D is the num-
ber of concepts appearing in the top hypothesis pro-
duced by the n-gram semantic model. Moreover, under 
the framework of WFST, incorporating ASR N-best 
hypotheses required only a small increment of addi-
tional processing time compared to the use of 1-best. 
60
65
70
75
80
85
90
95
ConF GAcc CAcc
%
Reg
LNgram (1-best)
LNgram (50-best)
Orthgraphy
 
Figure 8. Comparative results for the ES set between 
the use of ASR 1-best and N-best (N = 50) hypotheses. 
6 Conclusion and Future Works 
Recently, a multi-stage spoken language understanding 
(SLU) approach has been proposed for the first Thai 
spoken dialogue system. This article reported an im-
provement on the SLU system by replacing the regular 
grammar-based semantic model by a hybrid n-gram 
and regular grammar approach, which not only cap-
tures long-distant dependencies of word syntax, but 
also provides robustness against speech-recognition 
errors. The proposed model, called logical n-gram 
modeling, obviously improved the performance in 
every SLU stage, while reducing the computational 
time compared to the original regular-grammar ap-
proach. Under the probabilistic WFST framework, the 
system was improved further by using N-best word-
hypotheses from the ASR, requiring only a small addi-
tional processing time compared to the use of 1-best. 
Further improvement of overall speech understand-
ing as well as a spoken dialogue system in the future 
can be expected by introducing dialogue-state depend-
ent modeling in the ASR and/or the SLU. A better way 
to utilize the first P-best goal hypotheses produced by 
the goal identifier instead of 1-best would also en-
hance the understanding performance. 
 
References 
B?chet, F., Gorin, A., Wright, J., and Tur, D. H. 2002. 
Named entity extraction from spontaneous speech in How 
May I Help You. Proc. ICSLP 2002, 597-600. 
Est?ve, Y., Raymond, C., B?chet, F., and De Mori, R. 2003. 
Conceptual decoding for spoken dialogue systems. Proc. 
Eurospeech 2003, 617-620. 
Gorin, A. L., Riccardi, G., and Wright, J. H. 1997. How May 
I Help You. Speech Communication, 23, 113-127. 
Hacioglu, K., and Ward, W. 2001. Dialog-context dependent 
language modeling combining n-grams and stochastic 
context-free grammars. Proc. ICASSP 2001, 537-540. 
Klakow, D. 1998. Log-linear interpolation of language mod-
els. Proc. ICSLP 1998, 1695-1699. 
Miller, S., Bobrow, R., Ingria, R., and Schwartz, R. 1994. 
Hidden understanding models of natural language. Proc. 
ACL 1994, 25-32. 
Mohri, M., Pereira, F., and Riley, M. 1997. General-purpose 
finite-state machine software tools. 
http://www.research.att.com/sw/tools/fsm, AT&T Labs ? 
Research. 
Potamianos, A., Kwang, H., and Kuo, J. 2000. Statistical 
recursive finite state machine parsing for speech under-
standing. Proc. ICSLP 2000, vol.3, 510-513. 
Seneff, S. 1992. TINA: A natural language system for spoken 
language applications. Computational Linguistics, 18(1), 
61-86. 
Szarvas, M. and Furui, S. Finite-state transducer based 
modeling of morphosyntax with applications to Hungar-
ian LVCSR. Proc. ICASSP 2003, 368-371. 
Wang, Y. Y., Mahajan, M., and Huang, X. 2000. A unified 
context-free grammar and n-gram model for spoken lan-
guage processing. Proc. ICASSP 2000, 1639-1642. 
Wang, Y. Y., Acero, A., Chelba, C., Frey, B., and Wong, L. 
2002. Combination of statistical and rule-based ap-
proaches for spoken language understanding. Proc. 
ICSLP 2002, 609-612. 
Wutiwiwatchai, C. and Furui, S. 2003a. Pioneering a Thai 
Language Spoken Dialogue System. Spring Meeting of 
Acoustic Society of Japan, 2-4-15, 87-88. 
Wutiwiwatchai, C., and Furui, S. 2003b. Combination of 
finite state automata and neural network for spoken lan-
guage understanding. Proc. EuroSpeech 2003, 2761-2764. 
Zell, A., Mamier, G., Vogt, M., Mach, N., Huebner, R., 
Herrmann, K. U., Doering, S., and Posselt, D. SNNS 
Stuttgart neural network simulator, user manual. Univer-
sity of Stuttgart. 
Speech-to-Speech Translation Activities in Thailand 
Chai Wutiwiwatchai, Thepchai Supnithi, Krit Kosawat 
Human Language Technology Laboratory 
National Electronics and Computer Technology Center 
112 Pahonyothin Rd., Klong-luang, Pathumthani 12120 Thailand 
{chai.wut, thepchai.sup, krit.kos}@nectec.or.th 
 
 
Abstract 
A speech-to-speech translation project 
(S2S) has been conducted since 2006 by 
the Human Language Technology labora-
tory at the National Electronics and Com-
puter Technology Center (NECTEC) in 
Thailand. During the past one year, there 
happened a lot of activities regarding tech-
nologies constituted for S2S, including 
automatic speech recognition (ASR), ma-
chine translation (MT), text-to-speech syn-
thesis (TTS), as well as technology for lan-
guage resource and fundamental tool de-
velopment. A developed prototype of Eng-
lish-to-Thai S2S has opened several re-
search issues, which has been taken into 
consideration. This article intensively re-
ports all major research and development 
activities and points out remaining issues 
for the rest two years of the project. 
1 Introduction 
Speech-to-speech translation (S2S) has been ex-
tensively researched since many years ago. Most of 
works were on some major languages such as 
translation among European languages, American 
English, Mandarin Chinese, and Japanese. There is 
no initiative of such research for the Thai language. 
In the National Electronics and Computer Tech-
nology Center (NECTEC), Thailand, there is a 
somewhat long history of research on Thai speech 
and natural language processing. Major technolo-
gies include Thai automatic speech recognition 
(ASR), Thai text-to-speech synthesis (TTS), Eng-
lish-Thai machine translation (MT), and language 
resource and fundamental tool development. These 
basic technologies are ready to seed for S2S re-
search. The S2S project has then been conducted in 
NECTEC since the end of 2006. 
The aim of the 3-year S2S project initiated by 
NECTEC is to build an English-Thai S2S service 
over the Internet for a travel domain, i.e. to be used 
by foreigners who journey in Thailand. In the first 
year, the baseline system combining the existing 
basic modules applied for the travel domain was 
developed. The prototype has opened several re-
search issues needed to be solved in the rest two 
years of the project. This article summarizes all 
significant activities regarding each basic technol-
ogy and reports remaining problems as well as the 
future plan to enhance the baseline system.
The rest of article is organized as follows. The 
four next sections describe in details activities 
conducted for ASR, MT, TTS, and language re-
sources and fundamental tools. Section 6 summa-
rizes the integration of S2S system and discusses 
on remaining research issues as well as on-going 
works. Section 7 concludes this article. 
2 Automatic Speech Recognition (ASR) 
Thai ASR research focused on two major topics. 
The first topic aimed to practice ASR in real envi-
ronments, whereas the second topic moved to-
wards large vocabulary continuous speech recogni-
tion (LVCSR) in rather spontaneous styles such as 
news broadcasting and telephone conversation. 
Following sub-sections give more details. 
2.1 Robust speech recognition 
To tackle the problem of noisy environments, 
acoustic model selection was adopted in our sys-
tem. A tree structure was constructed with each 
leaf node containing speaker-, noise-, and/or SNR-
specific acoustic model. The structure allowed ef-
ficient searching over a variety of speech environ-
ments. Similar to many robust ASR systems, the 
selected acoustic model was enhanced by adapting 
by the input speech using any adaptation algorithm 
such as MLLR or MAP. In our model, however, 
simulated-data adaptation was proposed (That-
phithakkul et al, 2006). The method synthesized 
an adaptation set by adding noise extracted from 
the input speech to a pre-recorded set of clean 
speech. A speech/non-speech detection module 
determined in the input speech the silence portions, 
which were assumed to be the environmental noise. 
This approach solved the problem of incorrect 
transcription in unsupervised adaptation and en-
hanced the adaptation performance by increasing 
the size of adaptation data. 
2.2 Large-vocabulary continuous speech rec-
ognition (LVCSR) 
During the last few years, researches on continuous 
speech recognition were based mainly on two da-
tabases, the NECTEC-ATR (Kasuriya et al, 
2003a) and the LOTUS (Kasuriya et al, 2003b). 
The former corpus was for general purposes, 
whereas the latter corpus was well designed for 
research on acoustic phonetics as well as research 
on 5,000-word dictation systems. A number of re-
search works were reported, starting by optimizing 
the Thai phoneme inventory (Kanokphara, 2003). 
Recently, research has moved closer to real and 
spontaneous speech. The first task collaborated 
with a Thai telephone service provider was to build 
a telephone conversation corpus (Cotsomrong et al, 
2007). To accelerate the corpus development, 
Thatphithakkul et al (2007) developed a speaker 
segmentation model which helped separating 
speech from two speakers being conversed. The 
model was based on the simple Hidden Markov 
model (HMM), which achieved over 70% accuracy.
Another on-going task is a collection of broadcast 
news video. The aim of the task is to explore the 
possibility to use the existing read-speech model to 
boot broadcast news transcription. More details 
will be given in Section 5. 
3 Machine Translation (MT) 
It was a long history of the NECTEC English-to-
Thai machine translation (MT) which has been 
publicly serviced online. The ?Parsit? 1  system 
modified from the engine developed by NEC, Ja-
pan, which was a rule-based MT (RBMT). Over 
900 parsed rules were coded by Thai linguists. The 
system recognized more than 70,000 lexical words 
and 120,000 meanings. 
 
 
 
Figure 1. Examples of using MICG to solve two 
major problems of parsing Thai, (a) coordination 
with gapping and (b) verb serialization. 
3.1 Thai-to-English MT 
Recently, there has been an effort to develop the 
first rule-based system for Thai-to-English MT. 
The task is much more difficult than the original 
English-to-Thai translation since the Thai word 
segmentation, sentence breaking, and grammar 
parser are all not complete. Coding rules for pars-
ing Thai is not trivial and the existing approach 
used to translate English to Thai cannot be applied 
counter wise. Last year, a novel rule-based ap-
proach appropriate for Thai was proposed 
(Boonkwan and Supnithi, 2007). The approach, 
called memory-inductive categorical grammar 
(MICG), was derived from the categorical gram-
mar (CG). The MICG introduced memorization 
and induction symbols to solve problems of ana-
lytic languages such as Thai as well as many spo-
                                                 
1
 Parsit MT, http://www.suparsit.com/ 
ken languages. In parsing Thai, there are two major 
problems, coordination with gapping and verb se-
rialization. Figure 1 shows examples of the two 
problems with the MICG solution, where the 
square symbol denotes the chunk to be memorized 
and the diamond symbol denotes the chunk to be 
induced. A missing text chunk can be induced by 
seeking for its associated memorized text chunk.  
3.2 TM and SMT 
In order to improve the performance of our transla-
tion service, we have adopted a translation memory 
(TM) module in which translation results corrected 
by users are stored and reused. Moreover, the ser-
vice system is capable to store translation results of 
individual users. A na?ve user can select from the 
list of translation results given by various users. 
Figure 3 captures the system interface.  
Due to powerful hardware today, research has 
turned to rely more on statistical approaches. This 
is also true for the machine translation issue. Sta-
tistical machine translation (SMT) has played an 
important role on modeling translation given a 
large amount of parallel text. In NECTEC, we also 
realize the benefit of SMT especially on its adapta-
bility and naturalness of translation results. How-
ever, a drawback of SMT compared to RBMT is 
that it works quite well on a limited domain, i.e. 
translating in a specific domain. This is actually 
suitable to the S2S engine which has been designed 
to work in only a travel domain. Therefore, in par-
allel to RBMT, SMT is being explored for limited 
domains. Two parallel text corpora have been con-
structed. The first one, collected by ATR under the 
Asian speech translation advanced research (A-
STAR)2 consortium, is a Thai incorporated version 
of the Basic travel expression (BTEC) corpus (Ki-
kui et al, 2003). This corpus will seed the devel-
opment of S2S in the travel domain. The second 
parallel corpus contains examples of parallel sen-
tences given in several Thai-English dictionaries.  
The latter corpus has been used for a general 
evaluation of Thai-English SMT. Details of both 
corpora will be given in the Section 5. 
4 Text-to-Speech Synthesis (TTS) 
Thai TTS research has begun since 2000. At pre-
sent, the system utilizes a corpus-based unit-
                                                 
2
 A-STAR consortium, http://www.slc.atr.jp/AStar/ 
selection technique. A well-constructed phoneti-
cally-balanced speech corpus, namely ?TSynC-1?, 
containing approximately 13 hours is embedded in 
the TTS engine, namely ?Vaja?3. Although the lat-
est version of Vaja achieved a fair speech quality, 
there are still a plenty of rooms to improve the sys-
tem. During the past few years, two major issues 
were considered; reducing the size of speech cor-
pus and improving unit selection by prosody in-
formation. Following sub-sections describe the 
detail of each issue. 
4.1 Corpus space reduction 
A major problem of corpus-based unit-selection 
TTS is the large size of speech corpus required to 
obtain high-quality, natural synthetic-speech. Scal-
ability and adaptability of such huge database be-
come a critical issue. We then need the most com-
pact speech corpus that still provides acceptable 
speech quality. An efficient way to reduce the size 
of corpus was recently proposed (Wutiwiwatchai et 
al., 2007). The method incorporated Thai phonetics 
knowledge in the design of phoneme/diphone in-
ventory. Two assumptions on diphone characteris-
tics were proved and used in the new design. One 
was to remove from the inventory the diphone 
whose coarticulation strength between adjacent 
phonemes was very weak. Normally, the corpus 
was designed to cover all tonal diphones in Thai. 
The second strategy to reduce the corpus was to 
ignore tonal levels of unvoiced phonemes. Ex-
periments showed approximately 30% reduction of 
the speech corpus with the quality of synthesized 
speech remained. 
4.2 Prosody-based naturalness improvement 
The baseline TTS system selected speech units by 
considering only phoneme and tone context. In the 
past few years, analyses and modeling Thai pro-
sodic features useful for TTS have been exten-
sively explored. The first issue was to detect 
phrasal units given an input text. After several ex-
periments (Tesprasit et al, 2003; Hansakunbun-
theung et al, 2005), we decided to develop a clas-
sification and decision tree (CART) for phrase 
break detection.  
The second issue was to model phoneme dura-
tion. Hansakunbuntheung et al (2003) compared 
several models to predict the phoneme duration. 
                                                 
3
 Vaja TTS, http://vaja.nectec.or.th/ 
Mainly, we found linear regression appropriate for 
our engine as its simplicity and efficiency. Both 
two prosody information were integrated in our 
Vaja TTS engine, which achieved a better synthe-
sis quality regarding subjective and objective 
evaluations (Rugchatjaroen et al, 2007). 
5 Language Resources and Tools 
A lot of research issues described in previous sec-
tions definitely requires the development and as-
sessment of speech and language corpora. At the 
same time, there have been attempts to enhance the 
existing language processing tools that are com-
monly used in a number of advanced applications. 
This section explains the activities on resource and 
tool development. 
5.1 Speech and text corpora 
Table 1 summarizes recent speech and text corpora 
developed in NECTEC. Speech corpora in NEC-
TEC have been continuously developed since 2000. 
The first official corpus under the collaboration 
with ATR was for general purpose (Kasuriya et al, 
2003a). The largest speech corpus, called LOTUS 
(Kasuriya et al, 2003b), was well-designed read 
speech in clean and office environments. It con-
tained both phonetically balanced utterances and 
news paper utterances covering 5,000 lexical 
words. The latter set was designed for research on 
Thai dictation systems. Several research works 
utilizing the LOTUS were reported as described in 
the Section 2.2. 
The last year was the first-year collaboration of 
NECTEC and a telephone service provider to de-
velop the first Thai telephone conversation speech 
corpus (Cotsomrong et al, 2007). The corpus has 
been used to enhance the ASR capability in dealing 
with various noisy telephone speeches. 
Regarding text corpora, as already mentioned in 
the Section 3, two parallel text corpora were de-
veloped. The first corpus was a Thai version of the 
Basic travel expression corpus (BTEC), which will 
be used to train a S2S system. The second corpus 
developed ourselves was a general domain. It will 
be used also in the SMT research. Another impor-
tant issue of corpus technology is to create golden 
standards for several Thai language processing top-
ics. Our last year attempts focused on two sets; a 
golden standard set for evaluating MT and a 
golden standard set for training and evaluating 
Thai word segmentation. Finally, the most basic 
but essential in all works is the dictionary. Within 
the last year, we have increased the number of 
word entries in our lexicon from 35,000 English-
to-Thai and 53,000 Thai-to-English entries to over 
70,000 entries both. This incremental dictionary 
will be very useful in sustaining improvement of 
many language processing applications. 
 
Table 1. Recent speech/text corpora in NECTEC. 
 
Corpus Purpose Details 
LOTUS 
 
Well-designed 
speech utterances for 
5,000-word dictation 
systems 
- 70 hours of pho-
netically balanced 
and 5,000-word 
coverage sets 
TSynC-1 Corpus-based unit-
selection Thai speech 
synthesis 
- 13 hours pros-
ody-tagged fluent 
speech 
Thai BTEC Parallel text and 
speech corpora for 
travel-domain S2S 
- 20,000 textual 
sentences and a 
small set of speech 
in travel domain 
Parallel text Pairs of Thai-English 
sample sentences 
from dictionaries 
used for SMT 
- 0.2M pairs of 
sentences 
NECTEC-
TRUE 
Telephone conversa-
tion speech for 
acoustic modeling 
- 10 hours conver-
sational speech in 
various telephone 
types 
5.2 Fundamental language tools 
Two major language tools have been substantially 
researched, word segmentation and letter-to-sound 
conversion. These basic tools are very useful in 
many applications such as ASR, MT, TTS, as well 
as Information retrieval (IR). 
Since Thai writing has no explicit word and sen-
tence boundary marker. The first issue on process-
ing Thai is to perform word segmentation. Our 
baseline morphological analyzer determined word 
boundaries and word part-of-speech (POS) simul-
taneously using a POS n-gram model and a prede-
fined lexicon. Recently, we have explored Thai 
named-entity (NE) recognition, which is expected 
to help alleviating the problem of incorrect word 
segmentation. Due to the difficulty of Thai word 
segmentation, we initiated a benchmark evaluation 
on Thai word segmentation, which will be held in 
2008. This will gather researchers who are inter-
ested in Thai language processing to consider the 
problem on a standard text corpus. 
The problem of incorrect word segmentation 
propagates to the letter-to-sound conversion (LTS) 
module which finds pronunciations on the word 
basis. Our original LTS algorithm was based on 
probabilistic generalized LR parser (PGLR). Re-
cently, we proposed a novel method to automati-
cally induce syllable patterns from a large text with 
no need for any preprocessing (Thangthai et al, 
2006). This approach largely helped alleviating the 
tedious work on text corpus annotation. 
Another important issue we took into account 
was an automatic approach to find pronunciations 
of English words using Thai phonology. The issue 
is particularly necessary in many languages where 
their local scripts are always mixed with English 
scripts. We proposed a new model that utilized 
both English graphemes and English phonemes, if 
found in an English pronunciation dictionary, to 
predict Thai phonemes of the word (Thangthai et 
al., 2007). 
6 Speech-to-Speech Translation (S2S) 
In parallel to the research and development of in-
dividual technology elements, some efforts have 
been on the development of Thai-English speech-
to-speech translation (S2S). Wutiwiwatchai (2007) 
already explained in details about the activities, 
which will be briefly reported in this section. 
As described briefly in the Introduction, the aim 
of our three-year S2S project is to develop an S2S 
engine in the travel domain, which will be given 
service over the Internet. In the last year, we de-
veloped a prototype English-to-Thai S2S engine, 
where major tasks turned to be the development of 
English ASR in the travel domain and the integra-
tion of three core engines, English ASR, English-
to-Thai RBMT, and Thai TTS. 
6.1 System development 
Our current prototype of English ASR adopted a 
well-known SPHINX toolkit, developed by Carne-
gie Mellon University. An American English 
acoustic model has been provided with the toolkit. 
An n-gram language model was trained by a small 
set of sentences in travel domain. The training text 
contains 210 patterns of sentences spanning over 
480 lexical words, all prepared by hands. Figure 2 
shows some examples of sentence pattern. 
 
 
Figure 2. Examples of sentence patterns for lan-
guage modeling (uppercases are word classes, 
bracket means repetition). 
 
In the return direction, a Thai ASR is required. 
Instead of using the SPHINX toolkit4, we built our 
own Thai ASR toolkit, which accepts an acoustic 
model in the Hidden Markov toolkit (HTK)5 for-
mat proposed by Cambridge University. The ?iS-
peech?6 toolkit that supports an n-gram language 
model is currently under developing.  
The English ASR, English-to-Thai RBMT, and 
Thai TTS were integrated simply by using the 1-
best result of ASR as an input of MT and generat-
ing a sound of the MT output by TTS. The proto-
type system, run on PC, utilizes a push-to-talk in-
terface so that errors made by ASR can be allevi-
ated.  
6.2 On-going works 
To enhance the acoustic and language models, a 
Thai speech corpus as well as a Thai-English paral-
lel corpus in the travel domain is constructing as 
mentioned in the Section 5.1, the Thai version of 
BTEC corpus. Each monolingual part of the paral-
lel text will be used to train a specific ASR lan-
guage model. 
For the MT module, we can use the parallel text 
to train a TM or SMT. We expect to combine the 
trained model with our existing rule-based model, 
which will be hopefully more effective than each 
individual model. Recently, we have developed a 
TM engine. It will be incorporated in the S2S en-
gine in this early stage. 
In the part of TTS, several issues have been re-
searched and integrated in the system. On-going 
works include incorporating a Thai intonation 
                                                 
4
 CMU SPHINX, http://cmusphinx.sourceforge.net/ 
5
 HTK, Cambridge University, http://htk.eng.cam.ac.uk/ 
6
 iSpeech ASR, http://www.nectec.or.th/rdi/ispeech/ 
model in unit-selection, improving the accuracy of 
Thai text segmentation, and learning for hidden 
Markov model (HMM) based speech synthesis, 
which will hopefully provide a good framework 
for compiling TTS on portable devices. 
7 Conclusion 
There have been a considerable amount of research 
and development issues carried out under the 
speech-to-speech translation project at NECTEC, 
Thailand. This article summarized and reported all 
significant works mainly in the last few years. In-
deed, research and development activities in each 
technology element, i.e. ASR, MT, and TTS have 
been sustained individually. The attempt to inte-
grate all systems forming an innovative technology 
of S2S has just been carried out for a year. There 
are many research and development topics left to 
explore. Major challenges include at least but not 
limited to the following issues: 
 The rapid development of Thai-specific ele-
ments such as robust Thai domain-specific 
ASR and MT 
 Migration of the existing written language 
translation to spoken language translation 
Recently, there have been some initiations of 
machine translation among Thai and other lan-
guages such as Javi, a minor language used in the 
southern part of Thailand and Mandarin Chinese. 
We expect that some technologies carried out in 
this S2S project will be helpful in porting to the 
other pairs of languages. 
Acknowledgement 
The authors would like to thank the ATR, Japan, in 
initiating the fruitful A-STAR consortium and in 
providing some resources and tools for our re-
search and development. 
References 
Boonkwan, P., Supnithi, T., 2008. Memory-inductive 
categorial grammar: an approach to gap resolution 
in analytic-language translation, To be presented in 
IJCNLP 2008. 
Cotsomrong, P., Saykham, K., Wutiwiwatchai, C., 
Sreratanaprapahd, S., Songwattana, K., 2007. A Thai 
spontaneous telephone speech corpus and its appli-
cations to speech recognition, O-COCOSDA 2007. 
Hansakunbuntheung, C., Tesprasit, V., Siricharoenchai, 
R., Sagisaka, Y., 2003. Analysis and modeling of syl-
lable duration for Thai speech synthesis, EU-
ROSPEECH 2003, pp. 93-96. 
Hansakunbuntheung, C., Thangthai, A., Wutiwiwatchai, 
C., Siricharoenchai, R., 2005. Learning methods and 
features for corpus-based phrase break prediction on 
Thai, EUROSPEECH 2005, pp. 1969-1972. 
Kanokphara, S., 2003. Syllable structure based phonetic 
units for context-dependent continuous Thai speech 
recognition, EUROSPEECH 2003, pp. 797-800. 
Kasuriya, S., Sornlertlamvanich, V., Cotsomrong, P., 
Jitsuhiro, T., Kikui, G., Sagisaka, Y., 2003a. NEC-
TEC-ATR Thai speech corpus, O-COCOSDA 2003. 
Kasuriya, S., Sornlertlamvanich, V., Cotsomrong, P., 
Kanokphara, S., Thatphithakkul, N., 2003b. Thai 
speech corpus for speech recognition, International 
Conference on Speech Databases and Assessments 
(Oriental-COCOSDA). 
Kikui, G., Sumita, E., Takezawa, T., Yamamoto, S., 
2003. Creating corpora for speech-to-speech transla-
tion, EUROSPEECH 2003. 
Tesprasit, V., Charoenpornsawat, P., Sornlertlamvanich, 
V., 2003. Learning phrase break detection in Thai 
text-to-speech, EUROSPEECH 2003, pp. 325-328. 
Rugchatjaroen, A., Thangthai, A., Saychum, S., That-
phithakkul, N., Wutiwiwatchai, C., 2007. Prosody-
based naturalness improvement in Thai unit-selection 
speech synthesis, ECTI-CON 2007, Thailand. 
Thangthai, A., Hansakunbuntheung, C., Siricharoenchai, 
R., Wutiwiwatchai, C., 2006. Automatic syllable-
pattern induction in statistical Thai text-to-phone 
transcription, INTERSPEECH 2006. 
Thangthai, A., Wutiwiwatchai, C., Ragchatjaroen, A., 
Saychum, S., 2007. A learning method for Thai pho-
netization of English words, INTERSPEECH 2007. 
Thatphithakkul, N., Kruatrachue, B., Wutiwiwatchai, C., 
Marukatat, S., Boonpiam, V., 2006. A simulated-data 
adaptation technique for robust speech recognition, 
INTERSPEECH 2006. 
Wutiwiwatchai, C., 2007. Toward Thai-English speech 
translation, International Symposium on Universal 
Communications (ISUC 2007), Japan. 
Wutiwiwatchai, C., Saychum, S., Rugchatjaroen, A., 
2007. An intensive design of a Thai speech synthesis 
corpus, To be presented in International Symposium 
on Natural Language Processing (SNLP 2007). 
 
The State of the Art in Thai Language Processing
Virach Sornlertlamvanich, Tanapong Potipiti, Chai Wutiwiwatchai and Pradit Mittrapiyanuruk
National Electronics and Computer Technology Center (NECTEC),
National Science and Technology Development Agency,  Ministry of Science and Technology Environment.
22nd Floor Gypsum Metropolitan Tower 539/2 Sriayudhya Rd. Rajthevi Bangkok 10400 Thailand.
Email: {virach, tanapong, chai}@nectec.or.th, pmittrap@notes.nectec.or.th
Abstract
This paper reviews the current state of tech-
nology and research progress in the Thai
language processing. It resumes the charac-
teristics of the Thai language and the ap-
proaches to overcome the difficulties in each
processing task.
1 Some Problematic Issues in the Thai
Processing
It is obvious that the most fundamental semantic
unit in a language is the word. Words are ex-
plicitly identified in those languages with word
boundaries. In Thai, there is no word boundary.
Thai words are implicitly recognized and in
many cases, they depend on the individual
judgement. This causes a lot of difficulties in the
Thai language processing. To illustrate the
problem, we employed a classic English exam-
ple.
The segmentation of  ? GODISNOWHERE?.
No. Segmentation Meaning
(1) God is now here. God is here.
(2) God is no where. God doesn?t exist.
(3) God is nowhere. God doesn?t exist.
With the different segmentations, (1) and (2)
have absolutely opposite meanings. (2) and 3
are ambiguous that nowhere is one word or two
words. And the difficulty becomes greatly ag-
gravated when unknown words exist.
As a tonal language, a phoneme with differ-
ent tone has different meaning. Many unique
approaches are introduced for both the tone gen-
eration in speech synthesis research and tone
recognition in speech recognition research.
These difficulties propagate to many levels in
the language processing area such as lexical ac-
quisition, information retrieval, machine trans-
lation, speech processing, etc. Furthermore the
similar problem also occurs in the levels of sen-
tence and paragraph.
2 Word and Sentence Segmentation
The first and most obvious problem to attack is
the problem of word identification and segmen-
tation. For the most part, the Thai language
processing relies on manually created dictionar-
ies, which have inconsistencies in defining word
units and limitation in the quantity.  [1] proposed
a word extraction algorithm employing C4.5
wit  some string features such as entropy and
mutual information. They reported a result of
85% in precision and 50% in recall measures.
For word segmentation, the longest matching,
maximal matching and probabilistic segmenta-
tion had been applied in the early research [2],
[3]. However, these approaches have some
limitations in dealing with unknown words.
More advanced techniques of word segmenta-
tion captured many language features such as
context words, parts of speech, collocations and
semantics [4], [5]. These reported about 95-99 %
of accuracy. For sentence segmentation, the tr-
gram model was adopted and yielded 85% of
accuracy [6].
3 Machine Translation
Currently, there is only one machine
translation system available to
the public, called ParSit (http://www.
links.nectec.or.th/services/parsit),   it is a service
of English-to-Thai webpage translation.  ParSiT
is a collaborative work of NECTEC, Thailand
and NEC, Japan. This system is based on an in-
terlingual approach MT and the translation accu-
racy is about 80%.  Other approaches such as
generate-and-repair [7] and sentence pattern
mapping have been also studied [8].
4 Language Resources
The only Thai text corpus available for research
use is the ORCHID corpus. ORCHID is a 9-MB
Thai part-of-speech tagged corpus initiated by
NECTEC, Thailand and Communications Re-
search Laboratory, Japan. ORCHID is available
at http://www.links.nectec.or.th /orchid.
5 Research in Thai OCR
Frequently used Thai characters are about 80
characters, including alphabets, vowels, tone
marks, special marks, and numerals. Thai writ-
ing are in 4 levels, without spaces between
words, and the problem of similarity among
many patterns has made research challenging.
Moreover, the use of English and Thai in general
Thai text creates many more patterns which
must be recognized by OCR.
For more than 10 years, there has been a con-
siderable growth in Thai OCR research,
especially for ?printed character? task. The early
proposed approaches focused on structural
matching and tended towards neural-network-
based algorithms with input for some special
characteristics of Thai characters e.g., curves,
heads of characters, and placements. At least 3
commercial products have been launched in-
cluding ?ArnThai? by NECTEC, which claims
to achieve 95% recognition performance on
clean input. Recent technical improvement of
ArnThai has been reported in [9]. Recently, fo-
cus has been changed to develop system that are
more robust with any unclean scanning input.
The approach of using more efficient features,
fuzzy algorithms, and document analysis is re-
quired in this step.
At the same time, ?Offline Thai handwritten
character recognition? task has been investigated
but is only in the research phase of isolated
characters. Almost all proposed engines were
neural network-based with several styles of in-
put features [10], [11]. There has been a small
amount of research on ?Online handwritten
character recognition?. One attempt was pro-
posed by [12], which was also neural network-
based with chain code input.
6 Thai Speech Technology
Regarding speech, Thai, like Chinese, is a tonal
language. The tonal perception is important to
the meaning of the speech. The research cur-
rently being done in speech technology can be
divided into 3 major fields: (1) speech analysis,
(2) speech recognition and (3) speech synthesis.
Most of the research in (1) done by the linguists
are on the basic study of Thai phonetics e.g.
[13].
In speech recognition, most of the current
research [14] focus on the recognition of isolated
words. To develop continuous speech recogni-
tion, a large-scale speech corpus is needed. Th
status of practical research on continuous speech
recognition is in its initial step with at least one
published paper [15]. In contrast to western
speech recognition, topics specifying tonal lan-
guages or tone recognition have been deeply
researched as seen in many papers e.g., [16].
For text-to-speech synthesis, processing the
idiosyncrasy of Thai text and h ling the tones
i terplaying with intonation are the topics that
make the TTS algorithm for the Thai language
different from others. In the research, the first
su cessful system was accomplished by [14] and
later by NECTEC [15]. Both systems employ
the same synthesis technique based on the con-
aten tion of demisyllable inventory units.
R ferences
[1] V. Sornlertlamvanich, T. Potipiti and T. Charoenporn. Auto-
matic Corpus-Based Thai Word Extraction with the C4.5
Learning Algorithm. In forthcoming Proceedings of COLING
2000.
[2] V. Sornlertlamvanich. Word Segmentation for Thai in Machine
Translation System Machine Translation. N ti al Electronics
and Computer Technology Center, Bangkok. pp. 50-56, 1993.
(in Thai).
[3] A. Kawtrakul, S. Kumtanode, T. Jamjunya nd A. Jewriyavech.
Lexibase Model for Writing Production Assistant System. In
Proceedings of the Symposium on Natural Language Processing
in Thailand, 1995.
[4] S. Meknavin, P. Charoenpornsawat and B. Kijsirikul. Featured
Based Thai Word Segmentation. In Proceedings of Natural
Language Processing Pacific Rim Symposium, pp. 41-46, 1997.
[5] A. Kawtrakul, C. Thumkanon, P. Varasarai and M. Sukta-
rachan. Autmatic Thai Unknown Word Recognition. I  Proceed-
ings of Natural Language Processing Pacific Rim Symposium,
pp. 341-347, 1997.
[6] P. Mitrapiyanurak and V. Sornlertlamvanich. The Automatic
Thai Sentence Extraction. In Proceedings of the Fourth Sympo-
sium on Natural Language Processing, pp. 23-28, May 2000.
[7] K. Naruedomkul and N. Cercone. Generate and Repair
Machine Translation. I  Proceedings of the Fourth Symposium
on Natural Language Processing, pp. 63-79, May 2000.
[8] K. Chancharoen and B. Sirinaowakul. English Thai Machine
Translation Using Sentence Pattern Mapping. In Proceedings of
the Fourth Symposium on Natural Language Processing, pp. 29-
36, May 2000.
[9] C. Tanprasert and T. Koanantakool. Thai OCR: A Neural Net-
work Application. I  Proceedings of IEEE Region Ten Confer-
ence, vol.1, pp.90-95, November 1996.
[10] I. Methasate, S. Jitapankul, K. Kiratiratanaphung and W.
Unsiam. Fuzzy Feature Extraction for Thai Handwritten Char-
acter Recognition. I  Proceedings of the Forth Symposium on
Natural Language Processing, pp.136-141, May 2000.
[11] P. Phokharatkul and C. Kimpan. Handwritten Thai Character
Recognition using Fourior Descriptors and Genetic Neural Net-
works. In Proceedings of the Fourth Symposium on Natural
Language Processing, pp.108-123, May 2000.
[12] S. Madarasmi and P. Lekhachaiworakul. Customizable Online
Thai-English Handwriting Recognition. In Proceedings of the
Forth Symposium on Natural Language Processing, pp.142-153,
May 2000.
[13] J. T. Gandour, S. Potisuk and S. Dechongkit. Tonal Coarticu-
lation in Thai, Journal of Phonetics, vol 22, pp.477-492, 1994.
[14] S. Luksaneeyanawin, et al A Thai Text-to-Speech System. In
Proceedings of Fourth NECTEC Conference, pp.65-78, 1992. (in
Thai).
[15] P. Mittrapiyanuruk, C. Hansakunbuntheung, V. Tesprasit and
V. Sornlertlamvanich. Improving Naturalness of Thai Text-to-
Speech Synthesis by Prosodic Rule. In forthcoming Proceedings
of ICSLP2000.
[16] S. Jitapunkul, S. Luksaneeyanawin, V. Ahkuputra, C. Wuti-
wiwatchai. Recent Advances of Thai Speech Recognition in
Thailand. In Proceedings of IEEE Asia-Pacific conference on
Circuits and  Systems,  pp.173-176, 1998.
Proceedings of the 2010 Named Entities Workshop, ACL 2010, pages 66?70,
Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational Linguistics
Syllable-based Thai-English Machine Transliteration
Chai Wutiwiwatchai  and  Ausdang Thangthai
National Electronics and Computer Technology Center
Pathumthani, Thailand
{chai.wutiwiwatchai,ausdang.thangthai}@nectec.or.th
Abstract
This article describes the first trial on bidirec-
tional Thai-English machine transliteration ap-
plied on the NEWS 2010 transliteration cor-
pus. The system relies on segmenting source-
language  words into syllable-like units,  find-
ing unit's pronunciations, consulting a syllable 
transliteration  table  to  form  target-language 
word hypotheses, and ranking the hypotheses 
by using syllable n-gram. The approach yields 
84.2% and 70.4% mean F-scores on English-
to-Thai  and  Thai-to-English  transliteration. 
Discussion  on  existing  problems  and  future 
solutions are addressed.
1 Introduction
Transliteration  aims  to  phonetically  transcribe 
text in source languages with text in target lan-
guages.  The  task  is  crucial  for  various  natural 
language  processing  research  and  applications 
such as machine translation, multilingual text-to-
speech synthesis and information retrieval. Most 
of  current  Thai  writings contain both Thai  and 
English scripts. Such English words when writ-
ten in Thai are mainly their translations. Without 
official  translation  forms,  transliterations  often 
take place.
Thai-English  machine  transliteration  and  re-
lated research have been investigated for many 
years.  Works  for  Thai  word  romanization  or 
Thai-to-English transliteration are such as Char-
oenporn  et  al.  (1999),  Aroonmanakun  and 
Rivepiboon (2004). Both works proposed statist-
ical  romanization models  based on the syllable 
unit.  Generating  Thai  scripts  of  English  words 
are mainly via automatic transcription of English 
words.  Aroonmanakun  (2005)  described  a 
chunk-based n-gram model where the chunk is a 
group of characters useful for mapping to Thai 
transcriptions. Thangthai et al (2007) proposed a 
method  for  generating  Thai  phonetic  transcrip-
tions of  English words for  use in  Thai/English 
text-to-speech  synthesis.  The  CART  learning 
machine was adopted to map English characters 
to  Thai  phonetics.  As  our  literature  review,  a 
general algorithm for bi-directional Thai-to-Eng-
lish  and  English-to-Thai  transliteration  has  not 
been investigated.
The NEWS machine transliteration shared task 
has just included Thai-English words as a part of 
its corpus in 2010, serving as a good source for 
algorithm benchmarking. In this article, a Thai-
English machine transliteration system is evalu-
ated on the NEWS 2010 corpus. The system was 
developed under intuitive concepts that translit-
eration  among  Thai-English  is  mostly  done on 
the basis of sound mimicking of syllable units. 
Therefore, the algorithm firstly segments the in-
put word in a source language into syllable-like 
units  and  finding  pronunciations  of  each  unit. 
The pronunciation in the form of phonetic scripts 
is used to find possible transliteration forms giv-
en a syllable translation table. The best result is 
determined by using syllable n-gram.
The  next  section  describes  more  details  of 
Thai-English  transliteration  problems  and  the 
Thai-English NEWS 2010 corpus. The detail of 
proposed  system is  given  in  Section  3  and  its 
evaluation is reported in Section 4. Section 5 dis-
cusses on existing problems and possible  solu-
tions.
2 Thai-English Transliteration
As  mentioned  in  the  Introduction,  the  current 
Thai writing often contains both Thai and Eng-
lish scripts especially for English words without 
compact  translations.  Many  times,  translitera-
tions  take  place  when  only  Thai  scripts  are 
needed. This is not only restricted to names but 
also  some  common  words  like  ?computer?, 
?physics?, etc.
The  Thai  Royal  Institute  (http://www.roy-
in.go.th) is authorized to issue official guidelines 
for Thai transcriptions of foreign words and also 
romanization of Thai words, which are respect-
ively equivalent to English-to-Thai and Thai-to-
English  transliteration.  Romanization  of  Thai 
words is based on sound transcription. Thai con-
66
sonant and vowel alphabets are defined to map to 
roman  alphabets.  Similarly,  English-to-Thai 
transliteration is  defined based on the  phonetic 
transcription of English words. However, in the 
latter case, an English phoneme could be mapped 
to  multiple  Thai  alphabets.  For  example,  the 
sound /k/ could be mapped to either ?? ?,  ?? ? , 
???, or ???. Moreover, the guideline reserves for 
transliterations generally used in the current writ-
ing and also transliterations appeared in the offi-
cial Royal Institute dictionaries, even such trans-
literations do not comply with the guideline.
Since the guidelines are quite flexible and it is 
also common that  lots  of Thai  people may not 
strictly follow the guidelines, ones can see many 
ways  of  transliteration  in  daily  used  text.  To 
solve this ambiguity, both the official guidelines 
and statistics of  usage must  be incorporated in 
the machine transliteration system.
The Thai-English part of NEWS 2010 corpus 
developed by the National Electronics and Com-
puter  Technology Center  (NECTEC)  composes 
of word pairs collected mainly from 3 sources; 
press from the Thai Royal  Institute, press from 
other sources, and the NEWS 2009 corpus. The 
first two sources, sharing about 40%  of the cor-
pus, mostly contain common English words of-
ten transliterated into Thai and the transliteration 
is  almost  restricted  to  the  Royal  Institute 
guidelines. The rest are English names selected 
from the NEWS 2009 corpus based on their fre-
quencies found by the Google search. Such Eng-
lish names were transliterated into Thai and re-
checked  by  linguists  using  the  Royal  Institute 
transliteration guideline.
3 Proposed Transliteration System
Our proposed model is similar to what proposed 
by Jiang et al (2009), which introduced transla-
tion among Chinese and English names based on 
syllable units and determined the best candidate 
using the  statistical  n-gram model.  The overall 
structure of our model is shown in Figure 1. 
3.1 Syllabification and letter-to-sound
An input word in the source language is first seg-
mented  into syllable-like  units.  It  is  noted that 
there are some cases where segmented units are 
not  really a  syllable.  For  examples,  ?S?  in  the 
word ?SPECTOR? might actually be pronounced 
as a single consonant without  vowel.  The Thai 
word ??????/s-a n-?:/ is unbreakable as the letter 
expressed for the first syllable /s-a/ is enclosed in 
the  letters  of  the  second  syllable  /n-?:/.  These 
cases are considered exceptional syllables.
Figure 1. The overall system architecture.
In the Thai-to-English system,  syllabification 
of Thai words is a part of a Thai letter-to-sound 
conversion  tool  provided  by  Thangthai  et  al. 
(2006). It is performed using context-free gram-
mar (CFG) rules created by Tarsaku et al (2001). 
The CFG rules produce syllable-sequence hypo-
theses,  which are  then  disambiguated  by using 
syllable  n-gram.  Simultaneously,  the  tool 
provides  the  phonetic  transcription  of  the  best 
syllable sequence by using a simple syllable-to-
phone mapping. Figure 1 shows an example of 
an input Thai word ?????????? ? which is segmen-
ted into 3 syllables ??|????|?????? and converted 
to the phonetic transcription defined for Thai ?s-
a|p-e-k|t-?:?.
In the English-to-Thai system, a simple syllab-
ification module of English words is created us-
ing the following rules.
1) Marking all vowel letters ?a, e, i, o, u?,
     e.g. L[o]m[o]c[a]t[i]v[e], J[a]nsp[o]rt
2) Using some rules, merging consonantal 
     letters surrounding each vowel to form
     basic syllables, 
     e.g. Lo|mo|ca|ti|ve, Jan|sport
3) Post-processing by merging the syllable 
     with ?e? vowel into its preceding syllable 
     e.g. Lo|mo|ca|tive,  and re-segmenting for 
     syllables without vowel letters, e.g. 
         mcdo|nald to mc|do|nald, sport to s|port
Letter-to-sound conversion of English words can 
actually be conducted by several public tools like 
Festival  (http://www.cstr.ed.ac.uk/projects/  fest-
ival/). However, the tool does not meet our re-
67
quirement as it could not output syllable bound-
aries of the phonetic sequence and finding such 
boundaries is not trivial. Instead, a tool for con-
verting English words to Thai phonetic transcrip-
tions developed by Thangthai et al (2007) is ad-
opted. In this tool, the CART learning machine is 
used to capture the relationship among alphabets 
and  English  phone  transcriptions  of  English 
words  and Thai phone transcriptions.  Since the 
Thai  phonetic transcription is defined based on 
the syllable structure, the syllable boundaries of 
phonetic transcriptions given by this tool can be 
obtained.
3.2 Syllable transliteration and disambigu-
ation
In  the  training  phase,  both  Thai  and  English 
words in pairs  are syllabified and converted to 
phonetic  transcriptions  using  the  methods  de-
scribed in the previous subsection. To reduce the 
effect of errors caused by automatic syllabifica-
tion,  only  word  pairs  having  equal  number  of 
syllables are kept for building a syllable translit-
eration table. The table consists of a list of syl-
lable phonetic transcriptions and its possible tex-
tual syllables in both languages. An n-gram mod-
el  of  textual  syllables  in each language is  also 
prepared from the training set.
In  the  testing  phase,  each  syllable  in  the 
source-language word is mapped to possible syl-
lables in the target language via its phonetic tran-
scription  using  the  syllable  transliteration  table 
described  above.  Since  each  syllable  could  be 
transliterated to multiple hypotheses, the best hy-
pothesis  can be determined by considering syl-
lable n-gram probabilities.
4 Experiments
The Thai-English part of NEWS 2010 were de-
ployed in our experiment. The training set com-
poses  of  24,501  word  pairs  and  two  test  sets, 
2,000 words for English-to-Thai and 1,994 words 
for Thai-to-English  are used for evaluation. All 
training words were syllable segmented and con-
verted to phonetic transcriptions using the tools 
described in the Section 3.1. Since the CFG rules 
could not completely cover all possible syllables 
in  Thai,  some  words  failed  from automatically 
generating  phonetic  transcriptions  were  filtered 
out. As mentioned also in the Section 3.1, only 
word pairs with equal number of segmented syl-
lables were kept for training. Finally, 16,705 out 
of 24,501 word pairs were reserved for building 
the syllable transliteration table and for training 
syllable 2-gram models.
Table 1 shows some statistics of syllables col-
lected  from the  training  word  pairs.  Since  the 
Thai-English  word  pairs  provided  in  NEWS 
2010  were  prepared  mainly  by  transliterating 
English words and names into Thai, it is hence 
reasonable that the number of distinct syllables 
in  Thai  is  considerably lower  than  in  English. 
Similarly, the other statistics like the numbers of 
homophones  per  syllable  phonetic-transcription 
are in the same manner.
Total no. of syllables 39,537
Avg. no. of syllables per word 2.4
No. of distinct syllables 4,367 (Thai)
6,307 (English)
No. of distinct syllable 
phonetic-transcriptions
1,869
Avg. no. of homophones per 
syllable phonetic-transcription
2.3 (Thai)
3.4 (English)
Max. no. of homophones per 
syllable phonetic-transcription
16 (Thai)
38 (English)
Table 2. Some statistics of syllables extracted 
from the training set.
As seen from the Table 1 that there could be 
up to 38 candidates of textual syllables given a 
syllable  phonetic  transcription.  To  avoid  the 
large search space of syllable combinations, only 
top-frequency  syllables  were  included  in  the 
search space. Table 2 shows transliteration res-
ults regarding 4 measures defined in the NEWS 
2010 shared task. Both experiments on English-
to-Thai  and  Thai-to-English  transliteration  are 
non-standard  tests  as  external  letter-to-sound 
conversion tools are incorporated. 
Measure Eng-to-Thai Thai-to-Eng
ACC in Top-1 0.247 0.093
Mean F-score 0.842 0.707
MRR 0.367 0.132
MAPref 0.247 0.093
Table 2. Transliteration results based on the 
NEWS 2010 measurement.
68
5 Analysis and Discussion
There are still some problematic issues regarding 
the transliteration format including hyphenation 
and case sensitivity in the test data. Ignoring both 
problems leads to 0.5% and 8.3% improvement 
on the English-to-Thai and Thai-to-English tests 
respectively. Figure 2 illustrates the distribution 
of test words and error words with respect to the 
word length in the unit  of syllables.  More than 
80% of test words are either 2 or 3 syllables. It 
can be roughly seen that the ratio of error words 
over  test  words  increases  with  respect  to  the 
length  of  words.  This  is  by  the  fact  that  the 
whole word will be considered incorrect even if 
only a syllable in the word is wrongly transliter-
ated. Out of 3,860 syllable units extracted from 
all error words, over 57% are correctly transliter-
ated.
Figure 2. The distribution of test words and error 
words with respect to the word length.
Another issue largely affecting the system per-
formance is as mentioned in the Section 2 that 
the Thai Royal Institute's guideline is somewhat 
flexible  for  multiple  ways  of  transliteration. 
However, the corpus used to train and test cur-
rently provides only one way of transliteration. 
Improving the corpus to cope with such translit-
eration  flexibility  is  needed.  In  developing  the 
Thai-English NEWS 2010 transliteration corpus, 
some  foreign  names  are  difficult  to  pronounce 
even by linguists. Errors in the corpus are then 
unavoidable and required further improvement.
Many algorithms could be conducted to help 
improve the system accuracy.  First,  the current 
system uses only syllable n-gram probabilities to 
determine  the  best  result  without  considering 
how  likely  the  target  syllable  is  close  to  the 
source syllable. For example, the source syllables 
?BIKE? and ?BYTE? are transliterated to Thai as 
??????and ?????? respectively. Both Thai translit-
erated syllables are pronounced in the same way 
as  /b-ai/.  It  can  be  seen  that  both  syllables 
?BIKE?  and  ?BYTE?  can  be  linked  to  both 
?? ? ?? ? and ?? ??? ? . Selecting the best syllable 
takes  only  the  syllable  n-gram  into  account 
without considering its right transliteration. Dir-
ect mapping between source and target syllables 
could  solve  this  problem but  leads  to  another 
problem of unseen syllables. A better way is to 
incorporate in the search space another score rep-
resenting the closeness of source and target syl-
lables.  As the example,  the syllable ?BIKE? is 
closer to ?????? than to ?????? as the letter ?K? is 
normally pronounced like ???  /k/, not ???  /th/. 
We have tried incorporating such knowledge by 
introducing  a  syllable  similarity  score  in  the 
search space. Given a pair of source and target 
syllables, the syllable similarity score is the num-
ber  of  consonants  having  the  same  sound like 
?K? and ??? divided by the total number of con-
sonants  in  the  syllable.  Unfortunately,  this  ap-
proach  could  not  yield  any  improvement  cur-
rently as many syllable pairs happened to have 
the same similarity score. A better definition of 
the score will be conducted in the future work.
6 Conclusion
The Thai-English part of NEWS 2010 translitera-
tion corpus was briefly described and its use in 
building  a  Thai-English  machine  transliteration 
system  was  reported.  The  system  is  based  on 
transliteration of syllable units extracted from the 
whole input word. Within the space of candidate 
transliterated syllables,  the  best  output  was de-
termined by using the statistical syllable n-gram 
model. There are many issues left for further im-
provement. First, possible transliterations of each 
word should be added to the corpus. Second, the 
system itself could be improved by e.g. incorpor-
ating better syllabification approaches, defining a 
better  syllable  similarity  score,  and  comparing 
with  other  potential  algorithms.  Finally,  as  the 
Thai-to-English part of the transliteration corpus 
is  actually  back-transliteration  of  English-to-
Thai, it is interesting to extend the corpus to cope 
with real-use Thai-to-English word pairs.
Acknowledgments
The authors would like to thank the Thai Royal 
Institute and Assoc. Prof. Dr. Wirote Aroonman-
akun  from the  Faculty  of  Arts,  Chulalongkorn 
University,  who help supply parts  of  the  Thai-
English NEWS 2010 transliteration corpus.
69
References 
Ausdang  Thangthai,  Chatchawarn  Hansakun-
buntheung,  Rungkarn  Siricharoenchai,  and  Chai 
Wutiwiwatchai.  2006.  Automatic  syllable-pattern 
induction  in  statistical  Thai  text-to-phone  tran-
scription,  In  Proc.  of  INTERSPEECH 2006,  pp. 
1344-1347.
Ausdang  Thangthai,  Chai  Wutiwiwatchai,  Anocha 
Ragchatjaroen, Sittipong Saychum. 2007. A learn-
ing  method  for  Thai  phonetization  of  English  
words,  In  Proc.  of  INTERSPEECH  2007,  pp. 
1777-1780.
Thatsanee  Charoenporn,  Ananlada  Chotimongkol, 
and  Virach  Sornlertlamvanich.  1999.  Automatic  
romanization  for  Thai,  In  Proc.  of  the  Oriental 
COCOSDA 1999, Taipei, Taiwan.
Wirote  Aroonmanakun  and  Wanchai  Rivepiboon. 
2004.  A unified model of Thai word segmentation  
and romanization, In Proc. of the 18th Pacific Asia 
Conference on Language, Information and Compu-
tation, Tokyo, Japan, pp. 205-214.
Wirote Aroonmanakun. 2005. A chunk-based n-gram 
English to Thai transliteration, In Proc. of the 6th 
Symposium on Natural Language Processing, Chi-
ang Rai, Thailand, pp. 37-42.
Xue Jiang,  Le Sun, Dakun Zhang. 2009.  A syllable-
based name transliteration system, In Proc. of  the 
2009  Named  Entities  Workshop,  ACL-IJCNLP 
2009, pp. 96?99.
70
