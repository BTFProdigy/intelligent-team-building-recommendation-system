Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 379?388,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
Topical Keyphrase Extraction from Twitter
Wayne Xin Zhao? Jing Jiang? Jing He? Yang Song? Palakorn Achananuparp?
Ee-Peng Lim? Xiaoming Li?
?School of Electronics Engineering and Computer Science, Peking University
?School of Information Systems, Singapore Management University
{batmanfly,peaceful.he,songyangmagic}@gmail.com,
{jingjiang,eplim,palakorna}@smu.edu.sg, lxm@pku.edu.cn
Abstract
Summarizing and analyzing Twitter content is
an important and challenging task. In this pa-
per, we propose to extract topical keyphrases
as one way to summarize Twitter. We propose
a context-sensitive topical PageRank method
for keyword ranking and a probabilistic scor-
ing function that considers both relevance and
interestingness of keyphrases for keyphrase
ranking. We evaluate our proposed methods
on a large Twitter data set. Experiments show
that these methods are very effective for topi-
cal keyphrase extraction.
1 Introduction
Twitter, a new microblogging website, has attracted
hundreds of millions of users who publish short
messages (a.k.a. tweets) on it. They either pub-
lish original tweets or retweet (i.e. forward) oth-
ers? tweets if they find them interesting. Twitter
has been shown to be useful in a number of appli-
cations, including tweets as social sensors of real-
time events (Sakaki et al, 2010), the sentiment pre-
diction power of Twitter (Tumasjan et al, 2010),
etc. However, current explorations are still in an
early stage and our understanding of Twitter content
still remains limited. How to automatically under-
stand, extract and summarize useful Twitter content
has therefore become an important and emergent re-
search topic.
In this paper, we propose to extract keyphrases
as a way to summarize Twitter content. Tradition-
ally, keyphrases are defined as a short list of terms to
summarize the topics of a document (Turney, 2000).
It can be used for various tasks such as document
summarization (Litvak and Last, 2008) and index-
ing (Li et al, 2004). While it appears natural to use
keyphrases to summarize Twitter content, compared
with traditional text collections, keyphrase extrac-
tion from Twitter is more challenging in at least two
aspects: 1) Tweets are much shorter than traditional
articles and not all tweets contain useful informa-
tion; 2) Topics tend to be more diverse in Twitter
than in formal articles such as news reports.
So far there is little work on keyword or keyphrase
extraction from Twitter. Wu et al (2010) proposed
to automatically generate personalized tags for Twit-
ter users. However, user-level tags may not be suit-
able to summarize the overall Twitter content within
a certain period and/or from a certain group of peo-
ple such as people in the same region. Existing work
on keyphrase extraction identifies keyphrases from
either individual documents or an entire text collec-
tion (Turney, 2000; Tomokiyo and Hurst, 2003).
These approaches are not immediately applicable
to Twitter because it does not make sense to ex-
tract keyphrases from a single tweet, and if we ex-
tract keyphrases from a whole tweet collection we
will mix a diverse range of topics together, which
makes it difficult for users to follow the extracted
keyphrases.
Therefore, in this paper, we propose to study the
novel problem of extracting topical keyphrases for
summarizing and analyzing Twitter content. In other
words, we extract and organize keyphrases by top-
ics learnt from Twitter. In our work, we follow the
standard three steps of keyphrase extraction, namely,
keyword ranking, candidate keyphrase generation
379
and keyphrase ranking. For keyword ranking, we
modify the Topical PageRank method proposed by
Liu et al (2010) by introducing topic-sensitive score
propagation. We find that topic-sensitive propaga-
tion can largely help boost the performance. For
keyphrase ranking, we propose a principled proba-
bilistic phrase ranking method, which can be flex-
ibly combined with any keyword ranking method
and candidate keyphrase generation method. Ex-
periments on a large Twitter data set show that
our proposed methods are very effective in topical
keyphrase extraction from Twitter. Interestingly, our
proposed keyphrase ranking method can incorporate
users? interests by modeling the retweet behavior.
We further examine what topics are suitable for in-
corporating users? interests for topical keyphrase ex-
traction.
To the best of our knowledge, our work is the
first to study how to extract keyphrases from mi-
croblogs. We perform a thorough analysis of the
proposed methods, which can be useful for future
work in this direction.
2 Related Work
Our work is related to unsupervised keyphrase ex-
traction. Graph-based ranking methods are the
state of the art in unsupervised keyphrase extrac-
tion. Mihalcea and Tarau (2004) proposed to use
TextRank, a modified PageRank algorithm to ex-
tract keyphrases. Based on the study by Mihalcea
and Tarau (2004), Liu et al (2010) proposed to de-
compose a traditional random walk into multiple
random walks specific to various topics. Language
modeling methods (Tomokiyo and Hurst, 2003) and
natural language processing techniques (Barker and
Cornacchia, 2000) have also been used for unsuper-
vised keyphrase extraction. Our keyword extraction
method is mainly based on the study by Liu et al
(2010). The difference is that we model the score
propagation with topic context, which can lower the
effect of noise, especially in microblogs.
Our work is also related to automatic topic label-
ing (Mei et al, 2007). We focus on extracting topical
keyphrases in microblogs, which has its own chal-
lenges. Our method can also be used to label topics
in other text collections.
Another line of relevant research is Twitter-
related text mining. The most relevant work is
by Wu et al (2010), who directly applied Tex-
tRank (Mihalcea and Tarau, 2004) to extract key-
words from tweets to tag users. Topic discovery
from Twitter is also related to our work (Ramage et
al., 2010), but we further extract keyphrases from
each topic for summarizing and analyzing Twitter
content.
3 Method
3.1 Preliminaries
Let U be a set of Twitter users. Let C =
{{du,m}
Mu
m=1}u?U be a collection of tweets gener-
ated by U , where Mu is the total number of tweets
generated by user u and du,m is the m-th tweet of
u. Let V be the vocabulary. du,m consists of a
sequence of words (wu,m,1, wu,m,2, . . . , wu,m,Nu,m)
where Nu,m is the number of words in du,m and
wu,m,n ? V (1 ? n ? Nu,m). We also assume
that there is a set of topics T over the collection C.
Given T and C, topical keyphrase extraction is to
discover a list of keyphrases for each topic t ? T .
Here each keyphrase is a sequence of words.
To extract keyphrases, we first identify topics
from the Twitter collection using topic models (Sec-
tion 3.2). Next for each topic, we run a topical
PageRank algorithm to rank keywords and then gen-
erate candidate keyphrases using the top ranked key-
words (Section 3.3). Finally, we use a probabilis-
tic model to rank the candidate keyphrases (Sec-
tion 3.4).
3.2 Topic discovery
We first describe how we discover the set of topics
T . Author-topic models have been shown to be ef-
fective for topic modeling of microblogs (Weng et
al., 2010; Hong and Davison, 2010). In Twit-
ter, we observe an important characteristic of tweets:
tweets are short and a single tweet tends to be about
a single topic. So we apply a modified author-topic
model called Twitter-LDA introduced by Zhao et al
(2011), which assumes a single topic assignment for
an entire tweet.
The model is based on the following assumptions.
There is a set of topics T in Twitter, each represented
by a word distribution. Each user has her topic inter-
ests modeled by a distribution over the topics. When
a user wants to write a tweet, she first chooses a topic
based on her topic distribution. Then she chooses a
380
1. Draw ?B ? Dir(?), pi ? Dir(?)
2. For each topic t ? T ,
(a) draw ?t ? Dir(?)
3. For each user u ? U ,
(a) draw ?u ? Dir(?)
(b) for each tweet du,m
i. draw zu,m ? Multi(?u)
ii. for each word wu,m,n
A. draw yu,m,n ? Bernoulli(pi)
B. draw wu,m,n ? Multi(?B) if
yu,m,n = 0 and wu,m,n ?
Multi(?zu,m) if yu,m,n = 1
Figure 1: The generation process of tweets.
bag of words one by one based on the chosen topic.
However, not all words in a tweet are closely re-
lated to the topic of that tweet; some are background
words commonly used in tweets on different topics.
Therefore, for each word in a tweet, the user first
decides whether it is a background word or a topic
word and then chooses the word from its respective
word distribution.
Formally, let ?t denote the word distribution for
topic t and ?B the word distribution for background
words. Let ?u denote the topic distribution of user
u. Let pi denote a Bernoulli distribution that gov-
erns the choice between background words and topic
words. The generation process of tweets is described
in Figure 1. Each multinomial distribution is gov-
erned by some symmetric Dirichlet distribution pa-
rameterized by ?, ? or ?.
3.3 Topical PageRank for Keyword Ranking
Topical PageRank was introduced by Liu et al
(2010) to identify keywords for future keyphrase
extraction. It runs topic-biased PageRank for each
topic separately and boosts those words with high
relevance to the corresponding topic. Formally, the
topic-specific PageRank scores can be defined as
follows:
Rt(wi) = ?
?
j:wj?wi
e(wj , wi)
O(wj)
Rt(wj)+ (1??)Pt(wi),
(1)
where Rt(w) is the topic-specific PageRank score
of word w in topic t, e(wj , wi) is the weight for the
edge (wj ? wi), O(wj) =
?
w? e(wj , w
?) and ?
is a damping factor ranging from 0 to 1. The topic-
specific preference value Pt(w) for each word w is
its random jumping probability with the constraint
that
?
w?V Pt(w) = 1 given topic t. A large Rt(?)
indicates a word is a good candidate keyword in
topic t. We denote this original version of the Topi-
cal PageRank as TPR.
However, the original TPR ignores the topic con-
text when setting the edge weights; the edge weight
is set by counting the number of co-occurrences of
the two words within a certain window size. Tak-
ing the topic of ?electronic products? as an exam-
ple, the word ?juice? may co-occur frequently with a
good keyword ?apple? for this topic because of Ap-
ple electronic products, so ?juice? may be ranked
high by this context-free co-occurrence edge weight
although it is not related to electronic products. In
other words, context-free propagation may cause the
scores to be off-topic.
So in this paper, we propose to use a topic context
sensitive PageRank method. Formally, we have
Rt(wi) = ?
?
j:wj?wi
et(wj , wi)
Ot(wj)
Rt(wj)+(1??)Pt(wi).
(2)
Here we compute the propagation from wj to wi in
the context of topic t, namely, the edge weight from
wj to wi is parameterized by t. In this paper, we
compute edge weight et(wj , wi) between two words
by counting the number of co-occurrences of these
two words in tweets assigned to topic t. We denote
this context-sensitive topical PageRank as cTPR.
After keyword ranking using cTPR or any other
method, we adopt a common candidate keyphrase
generation method proposed by Mihalcea and Tarau
(2004) as follows. We first select the top S keywords
for each topic, and then look for combinations of
these keywords that occur as frequent phrases in the
text collection. More details are given in Section 4.
3.4 Probabilistic Models for Topical Keyphrase
Ranking
With the candidate keyphrases, our next step is to
rank them. While a standard method is to simply
aggregate the scores of keywords inside a candidate
keyphrase as the score for the keyphrase, here we
propose a different probabilistic scoring function.
Our method is based on the following hypotheses
about good keyphrases given a topic:
381
Figure 2: Assumptions of variable dependencies.
Relevance: A good keyphrase should be closely re-
lated to the given topic and also discriminative. For
example, for the topic ?news,? ?president obama? is
a good keyphrase while ?math class? is not.
Interestingness: A good keyphrase should be inter-
esting and can attract users? attention. For example,
for the topic ?music,? ?justin bieber? is more inter-
esting than ?song player.?
Sometimes, there is a trade-off between these two
properties and a good keyphrase has to balance both.
Let R be a binary variable to denote relevance
where 1 is relevant and 0 is irrelevant. Let I be an-
other binary variable to denote interestingness where
1 is interesting and 0 is non-interesting. Let k denote
a candidate keyphrase. Following the probabilistic
relevance models in information retrieval (Lafferty
and Zhai, 2003), we propose to use P (R = 1, I =
1|t, k) to rank candidate keyphrases for topic t. We
have
P (R = 1, I = 1|t, k)
= P (R = 1|t, k)P (I = 1|t, k, R = 1)
= P (I = 1|t, k, R = 1)P (R = 1|t, k)
= P (I = 1|k)P (R = 1|t, k)
= P (I = 1|k)?
P (R = 1|t, k)
P (R = 1|t, k) + P (R = 0|t, k)
= P (I = 1|k)?
1
1 + P (R=0|t,k)P (R=1|t,k)
= P (I = 1|k)?
1
1 + P (R=0,k|t)P (R=1,k|t)
= P (I = 1|k)?
1
1 + P (R=0|t)P (R=1|t) ?
P (k|t,R=0)
P (k|t,R=1)
= P (I = 1|k)?
1
1 + P (R=0)P (R=1) ?
P (k|t,R=0)
P (k|t,R=1)
.
Here we have assumed that I is independent of t and
R given k, i.e. the interestingness of a keyphrase is
independent of the topic or whether the keyphrase is
relevant to the topic. We have also assumed that R
is independent of t when k is unknown, i.e. without
knowing the keyphrase, the relevance is independent
of the topic. Our assumptions can be depicted by
Figure 2.
We further define ? = P (R=0)P (R=1) . In general we
can assume that P (R = 0)  P (R = 1) because
there are much more non-relevant keyphrases than
relevant ones, that is, ?  1. In this case, we have
logP (R = 1, I = 1|t, k) (3)
= log
(
P (I = 1|k)?
1
1 + ? ? P (k|t,R=0)P (k|t,R=1)
)
? log
(
P (I = 1|k)?
P (k|t, R = 1)
P (k|t, R = 0)
?
1
?
)
= logP (I = 1|k) + log
P (k|t, R = 1)
P (k|t, R = 0)
? log ?.
We can see that the ranking score logP (R = 1, I =
1|t, k) can be decomposed into two components, a
relevance score log P (k|t,R=1)P (k|t,R=0) and an interestingness
score logP (I = 1|k). The last term log ? is a con-
stant and thus not relevant.
Estimating the relevance score
Let a keyphrase candidate k be a sequence of
words (w1, w2, . . . , wN ). Based on an independent
assumption of words given R and t, we have
log
P (k|t, R = 1)
P (k|t, R = 0)
= log
P (w1w2 . . . wN |t, R = 1)
P (w1w2 . . . wN |t, R = 0)
=
N?
n=1
log
P (wn|t, R = 1)
P (wn|t, R = 0)
. (4)
Given the topic model ?t previously learned for
topic t, we can set P (w|t, R = 1) to ?tw, i.e. the
probability of w under ?t. Following Griffiths and
Steyvers (2004), we estimate ?tw as
?tw =
#(Ct, w) + ?
#(Ct, ?) + ?|V|
. (5)
Here Ct denotes the collection of tweets assigned to
topic t, #(Ct, w) is the number of times w appears in
Ct, and #(Ct, ?) is the total number of words in Ct.
P (w|t, R = 0) can be estimated using a smoothed
background model.
P (w|R = 0, t) =
#(C, w) + ?
#(C, ?) + ?|V|
. (6)
382
Here #(C, ?) denotes the number of words in the
whole collection C, and #(C, w) denotes the number
of times w appears in the whole collection.
After plugging Equation (5) and Equation (6) into
Equation (4), we get the following formula for the
relevance score:
log
P (k|t, R = 1)
P (k|t, R = 0)
=
?
w?k
(
log
#(Ct, w) + ?
#(C, w) + ?
+ log
#(C, ?) + ?|V|
#(Ct, ?) + ?|V|
)
=
(?
w?k
log
#(Ct, w) + ?
#(C, w) + ?
)
+ |k|?, (7)
where ? = #(C,?)+?|V|#(Ct,?)+?|V| and |k| denotes the number
of words in k.
Estimating the interestingness score
To capture the interestingness of keyphrases, we
make use of the retweeting behavior in Twitter. We
use string matching with RT to determine whether
a tweet is an original posting or a retweet. If a
tweet is interesting, it tends to get retweeted mul-
tiple times. Retweeting is therefore a stronger indi-
cator of user interests than tweeting. We use retweet
ratio |ReTweetsk||Tweetsk| to estimate P (I = 1|k). To prevent
zero frequency, we use a modified add-one smooth-
ing method. Finally, we get
logP (I = 1|k) = log
|ReTweetsk|+ 1.0
|Tweetsk|+ lavg
. (8)
Here |ReTweetsk| and |Tweetsk| denote the num-
bers of retweets and tweets containing the keyphrase
k, respectively, and lavg is the average number of
tweets that a candidate keyphrase appears in.
Finally, we can plug Equation (7) and Equa-
tion (8) into Equation (3) and obtain the following
scoring function for ranking:
Scoret(k) = log
|ReTweetsk|+ 1.0
|Tweetsk|+ lavg
(9)
+
(
?
w?k
log
#(Ct, w) + ?
#(C, w) + ?
)
+ |k|?.
#user #tweet #term #token
13,307 1,300,300 50,506 11,868,910
Table 1: Some statistics of the data set.
Incorporating length preference
Our preliminary experiments with Equation (9)
show that this scoring function usually ranks longer
keyphrases higher than shorter ones. However, be-
cause our candidate keyphrase are extracted without
using any linguistic knowledge such as noun phrase
boundaries, longer candidate keyphrases tend to be
less meaningful as a phrase. Moreover, for our task
of using keyphrases to summarize Twitter, we hy-
pothesize that shorter keyphrases are preferred by
users as they are more compact. We would there-
fore like to incorporate some length preference.
Recall that Equation (9) is derived from P (R =
1, I = 1|t, k), but this probability does not allow
us to directly incorporate any length preference. We
further observe that Equation (9) tends to give longer
keyphrases higher scores mainly due to the term
|k|?. So here we heuristically incorporate our length
preference by removing |k|? from Equation (9), re-
sulting in the following final scoring function:
Scoret(k) = log
|ReTweetsk|+ 1.0
|Tweetsk|+ lavg
(10)
+
(
?
w?k
log
#(Ct, w) + ?
#(C, w) + ?
)
.
4 Experiments
4.1 Data Set and Preprocessing
We use a Twitter data set collected from Singapore
users for evaluation. We used Twitter REST API1
to facilitate the data collection. The majority of the
tweets collected were published in a 20-week period
from December 1, 2009 through April 18, 2010. We
removed common stopwords and words which ap-
peared in fewer than 10 tweets. We also removed all
users who had fewer than 5 tweets. Some statistics
of this data set after cleaning are shown in Table 1.
We ran Twitter-LDA with 500 iterations of Gibbs
sampling. After trying a few different numbers of
1http://apiwiki.twitter.com/w/page/22554663/REST-API-
Documentation
383
topics, we empirically set the number of topics to
30. We set ? to 50.0/|T | as Griffiths and Steyvers
(2004) suggested, but set ? to a smaller value of 0.01
and ? to 20. We chose these parameter settings be-
cause they generally gave coherent and meaningful
topics for our data set. We selected 10 topics that
cover a diverse range of content in Twitter for eval-
uation of topical keyphrase extraction. The top 10
words of these topics are shown in Table 2.
We also tried the standard LDA model and the
author-topic model on our data set and found that
our proposed topic model was better or at least com-
parable in terms of finding meaningful topics. In ad-
dition to generating meaningful topics, Twitter-LDA
is much more convenient in supporting the compu-
tation of tweet-level statistics (e.g. the number of
co-occurrences of two words in a specific topic) than
the standard LDA or the author-topic model because
Twitter-LDA assumes a single topic assignment for
an entire tweet.
4.2 Methods for Comparison
As we have described in Section 3.1, there are three
steps to generate keyphrases, namely, keyword rank-
ing, candidate keyphrase generation, and keyphrase
ranking. We have proposed a context-sensitive top-
ical PageRank method (cTPR) for the first step of
keyword ranking, and a probabilistic scoring func-
tion for the third step of keyphrase ranking. We now
describe the baseline methods we use to compare
with our proposed methods.
Keyword Ranking
We compare our cTPR method with the original
topical PageRank method (Equation (1)), which rep-
resents the state of the art. We refer to this baseline
as TPR.
For both TPR and cTPR, the damping factor is
empirically set to 0.1, which always gives the best
performance based on our preliminary experiments.
We use normalized P (t|w) to set Pt(w) because our
preliminary experiments showed that this was the
best among the three choices discussed by Liu et al
(2010). This finding is also consistent with what Liu
et al (2010) found.
In addition, we also use two other baselines for
comparison: (1) kwBL1: ranking by P (w|t) = ?tw.
(2) kwBL2: ranking by P (t|w) = P (t)?
t
w?
t? P (t
?)?t?w
.
Keyphrase Ranking
We use kpRelInt to denote our relevance and inter-
estingness based keyphrase ranking function P (R =
1, I = 1|t, k), i.e. Equation (10). ? and ? are em-
pirically set to 0.01 and 500. Usually ? can be set to
zero, but in our experiments we find that our rank-
ing method needs a more uniform estimation of the
background model. We use the following ranking
functions for comparison:
? kpBL1: Similar to what is used by Liu et al
(2010), we can rank candidate keyphrases by
?
w?k f(w), where f(w) is the score assigned
to word w by a keyword ranking method.
? kpBL2: We consider another baseline ranking
method by
?
w?k log f(w).
? kpRel: If we consider only relevance but
not interestingness, we can rank candidate
keyphrases by
?
w?k log
#(Ct,w)+?
#(C,w)+? .
4.3 Gold Standard Generation
Since there is no existing test collection for topi-
cal keyphrase extraction from Twitter, we manually
constructed our test collection. For each of the 10
selected topics, we ran all the methods to rank key-
words. For each method we selected the top 3000
keywords and searched all the combinations of these
words as phrases which have a frequency larger than
30. In order to achieve high phraseness, we first
computed the minimum value of pointwise mutual
information for all bigrams in one combination, and
we removed combinations having a value below a
threshold, which was empirically set to 2.135. Then
we merged all these candidate phrases. We did not
consider single-word phrases because we found that
it would include too many frequent words that might
not be useful for summaries.
We asked two judges to judge the quality of the
candidate keyphrases. The judges live in Singapore
and had used Twitter before. For each topic, the
judges were given the top topic words and a short
topic description. Web search was also available.
For each candidate keyphrase, we asked the judges
to score it as follows: 2 (relevant, meaningful and in-
formative), 1 (relevant but either too general or too
specific, or informal) and 0 (irrelevant or meaning-
less). Here in addition to relevance, the other two
criteria, namely, whether a phrase is meaningful and
informative, were studied by Tomokiyo and Hurst
384
T2 T4 T5 T10 T12 T13 T18 T20 T23 T25
eat twitter love singapore singapore hot iphone song study win
food tweet idol road #singapore rain google video school game
dinner blog adam mrt #business weather social youtube time team
lunch facebook watch sgreinfo #news cold media love homework match
eating internet april east health morning ipad songs tomorrow play
ice tweets hot park asia sun twitter bieber maths chelsea
chicken follow lambert room market good free music class world
cream msn awesome sqft world night app justin paper united
tea followers girl price prices raining apple feature math liverpool
hungry time american built bank air marketing twitter finish arsenal
Table 2: Top 10 Words of Sample Topics on our Singapore Twitter Dateset.
(2003). We then averaged the scores of the two
judges as the final scores. The Cohen?s Kappa co-
efficients of the 10 topics range from 0.45 to 0.80,
showing fair to good agreement2. We further dis-
carded all candidates with an average score less than
1. The number of the remaining keyphrases for each
topic ranges from 56 to 282.
4.4 Evaluation Metrics
Traditionally keyphrase extraction is evaluated using
precision and recall on all the extracted keyphrases.
We choose not to use these measures for the fol-
lowing reasons: (1) Traditional keyphrase extraction
works on single documents while we study topical
keyphrase extraction. The gold standard keyphrase
list for a single document is usually short and clean,
while for each Twitter topic there can be many
keyphrases, some are more relevant and interesting
than others. (2) Our extracted topical keyphrases are
meant for summarizing Twitter content, and they are
likely to be directly shown to the users. It is there-
fore more meaningful to focus on the quality of the
top-ranked keyphrases.
Inspired by the popular nDCG metric in informa-
tion retrieval (Ja?rvelin and Keka?la?inen, 2002), we
define the following normalized keyphrase quality
measure (nKQM) for a methodM:
nKQM@K =
1
|T |
?
t?T
?K
j=1
1
log2(j+1)
score(Mt,j)
IdealScore(K,t)
,
where T is the set of topics, Mt,j is the j-
th keyphrase generated by method M for topic
2We find that judgments on topics related to social me-
dia (e.g. T4) and daily life (e.g. T13) tend to have a higher
degree of disagreement.
t, score(?) is the average score from the two hu-
man judges, and IdealScore(K,t) is the normalization
factor?score of the top K keyphrases of topic t un-
der the ideal ranking. Intuitively, ifM returns more
good keyphrases in top ranks, its nKQM value will
be higher.
We also use mean average precision (MAP) to
measure the overall performance of keyphrase rank-
ing:
MAP =
1
|T |
?
t?T
1
NM,t
|Mt|?
j=1
NM,t,j
j
1(score(Mt,j) ? 1),
where 1(S) is an indicator function which returns
1 when S is true and 0 otherwise, NM,t,j denotes
the number of correct keyphrases among the top j
keyphrases returned byM for topic t, and NM,t de-
notes the total number of correct keyphrases of topic
t returned byM.
4.5 Experiment Results
Evaluation of keyword ranking methods
Since keyword ranking is the first step for
keyphrase extraction, we first compare our keyword
ranking method cTPR with other methods. For each
topic, we pooled the top 20 keywords ranked by all
four methods. We manually examined whether a
word is a good keyword or a noisy word based on
topic context. Then we computed the average num-
ber of noisy words in the 10 topics for each method.
As shown in Table 5, we can observe that cTPR per-
formed the best among the four methods.
Since our final goal is to extract topical
keyphrases, we further compare the performance
of cTPR and TPR when they are combined with a
keyphrase ranking algorithm. Here we use the two
385
Method nKQM@5 nKQM@10 nKQM@25 nKQM@50 MAP
kpBL1 TPR 0.5015 0.54331 0.5611 0.5715 0.5984
kwBL1 0.6026 0.5683 0.5579 0.5254 0.5984
kwBL2 0.5418 0.5652 0.6038 0.5896 0.6279
cTPR 0.6109 0.6218 0.6139 0.6062 0.6608
kpBL2 TPR 0.7294 0.7172 0.6921 0.6433 0.6379
kwBL1 0.7111 0.6614 0.6306 0.5829 0.5416
kwBL2 0.5418 0.5652 0.6038 0.5896 0.6545
cTPR 0.7491 0.7429 0.6930 0.6519 0.6688
Table 3: Comparisons of keyphrase extraction for cTPR and baselines.
Method nKQM@5 nKQM@10 nKQM@25 nKQM@50 MAP
cTPR+kpBL1 0.61095 0.62182 0.61389 0.60618 0.6608
cTPR+kpBL2 0.74913 0.74294 0.69303 0.65194 0.6688
cTPR+kpRel 0.75361 0.74926 0.69645 0.65065 0.6696
cTPR+kpRelInt 0.81061 0.75184 0.71422 0.66319 0.6694
Table 4: Comparisons of keyphrase extraction for different keyphrase ranking methods.
kwBL1 kwBL2 TPR cTPR
2 3 4.9 1.5
Table 5: Average number of noisy words among the top
20 keywords of the 10 topics.
baseline keyphrase ranking algorithms kpBL1 and
kpBL2. The comparison is shown in Table 3. We
can see that cTPR is consistently better than the three
other methods for both kpBL1 and kpBL2.
Evaluation of keyphrase ranking methods
In this section we compare keypharse ranking
methods. Previously we have shown that cTPR is
better than TPR, kwBL1 and kwBL2 for keyword
ranking. Therefore we use cTPR as the keyword
ranking method and examine the keyphrase rank-
ing method kpRelInt with kpBL1, kpBL2 and kpRel
when they are combined with cTPR. The results are
shown in Table 4. From the results we can see the
following: (1) Keyphrase ranking methods kpRelInt
and kpRel are more effective than kpBL1 and kpBL2,
especially when using the nKQM metric. (2) kpRe-
lInt is better than kpRel, especially for the nKQM
metric. Interestingly, we also see that for the nKQM
metric, kpBL1, which is the most commonly used
keyphrase ranking method, did not perform as well
as kpBL2, a modified version of kpBL1.
We also tested kpRelInt and kpRel on TPR, kwBL1
and kwBL2 and found that kpRelInt and kpRel are
consistently better than kpBL2 and kpBL1. Due to
space limit, we do not report all the results here.
These findings support our assumption that our pro-
posed keyphrase ranking method is effective.
The comparison between kpBL2 with kpBL1
shows that taking the product of keyword scores is
more effective than taking their sum. kpRel and
kpRelInt also use the product of keyword scores.
This may be because there is more noise in Twit-
ter than traditional documents. Common words (e.g.
?good?) and domain background words (e.g. ?Sin-
gapore?) tend to gain higher weights during keyword
ranking due to their high frequency, especially in
graph-based method, but we do not want such words
to contribute too much to keyphrase scores. Taking
the product of keyword scores is therefore more suit-
able here than taking their sum.
Further analysis of interestingness
As shown in Table 4, kpRelInt performs better
in terms of nKQM compared with kpRel. Here we
study why it worked better for keyphrase ranking.
The only difference between kpRel and kpRelInt is
that kpRelInt includes the factor of user interests. By
manually examining the top keyphrases, we find that
the topics ?Movie-TV? (T5), ?News? (T12), ?Music?
(T20) and ?Sports? (T25) particularly benefited from
kpRelInt compared with other topics. We find that
well-known named entities (e.g. celebrities, politi-
cal leaders, football clubs and big companies) and
significant events tend to be ranked higher by kpRe-
lInt than kpRel.
We then counted the numbers of entity and event
keyphrases for these four topics retrieved by differ-
ent methods, shown in Table 6 . We can see that
in these four topics, kpRelInt is consistently better
than kpRel in terms of the number of entity and event
keyphrases retrieved.
386
T2 T5 T10 T12 T20 T25
chicken rice adam lambert north east president obama justin bieber manchester united
ice cream jack neo rent blk magnitude earthquake music video champions league
fried chicken american idol east coast volcanic ash lady gaga football match
curry rice david archuleta east plaza prime minister taylor swift premier league
chicken porridge robert pattinson west coast iceland volcano demi lovato f1 grand prix
curry chicken alexander mcqueen bukit timah chile earthquake youtube channel tiger woods
beef noodles april fools street view goldman sachs miley cyrus grand slam(tennis)
chocolate cake harry potter orchard road coe prices telephone video liverpool fans
cheese fries april fool toa payoh haiti earthquake song lyrics final score
instant noodles andrew garcia marina bay #singapore #business joe jonas manchester derby
Table 7: Top 10 keyphrases of 6 topics from cTPR+kpRelInt.
Methods T5 T12 T20 T25
cTPR+kpRel 8 9 16 11
cTPR+kpRelInt 10 12 17 14
Table 6: Numbers of entity and event keyphrases re-
trieved by different methods within top 20.
On the other hand, we also find that for some
topics interestingness helped little or even hurt the
performance a little, e.g. for the topics ?Food? and
?Traffic.? We find that the keyphrases in these top-
ics are stable and change less over time. This may
suggest that we can modify our formula to handle
different topics different. We will explore this direc-
tion in our future work.
Parameter settings
We also examine how the parameters in our model
affect the performance.
?: We performed a search from 0.1 to 0.9 with a
step size of 0.1. We found ? = 0.1 was the optimal
parameter for cTPR and TPR. However, TPR is more
sensitive to ?. The performance went down quickly
with ? increasing.
?: We checked the overall performance with
? ? {400, 450, 500, 550, 600}. We found that ? =
500 ? 0.01|V| gave the best performance gener-
ally for cTPR. The performance difference is not
very significant between these different values of ?,
which indicates that the our method is robust.
4.6 Qualitative evaluation of cTPR+kpRelInt
We show the top 10 keyphrases discovered by
cTPR+kRelInt in Table 7. We can observe that these
keyphrases are clear, interesting and informative for
summarizing Twitter topics.
We hypothesize that the following applications
can benefit from the extracted keyphrases:
Automatic generation of realtime trendy phrases:
For exampoe, keyphrases in the topic ?Food? (T2)
can be used to help online restaurant reviews.
Event detection and topic tracking: In the topic
?News? top keyphrases can be used as candidate
trendy topics for event detection and topic tracking.
Automatic discovery of important named entities:
As discussed previously, our methods tend to rank
important named entities such as celebrities in high
ranks.
5 Conclusion
In this paper, we studied the novel problem of topical
keyphrase extraction for summarizing and analyzing
Twitter content. We proposed the context-sensitive
topical PageRank (cTPR) method for keyword rank-
ing. Experiments showed that cTPR is consistently
better than the original TPR and other baseline meth-
ods in terms of top keyword and keyphrase extrac-
tion. For keyphrase ranking, we proposed a prob-
abilistic ranking method, which models both rele-
vance and interestingness of keyphrases. In our ex-
periments, this method is shown to be very effec-
tive to boost the performance of keyphrase extrac-
tion for different kinds of keyword ranking methods.
In the future, we may consider how to incorporate
keyword scores into our keyphrase ranking method.
Note that we propose to rank keyphrases by a gen-
eral formula P (R = 1, I = 1|t, k) and we have made
some approximations based on reasonable assump-
tions. There should be other potential ways to esti-
mate P (R = 1, I = 1|t, k).
Acknowledgements
This work was done during Xin Zhao?s visit to the
Singapore Management University. Xin Zhao and
Xiaoming Li are partially supported by NSFC under
387
the grant No. 60933004, 61073082, 61050009 and
HGJ Grant No. 2011ZX01042-001-001.
References
Ken Barker and Nadia Cornacchia. 2000. Using noun
phrase heads to extract document keyphrases. In Pro-
ceedings of the 13th Biennial Conference of the Cana-
dian Society on Computational Studies of Intelligence:
Advances in Artificial Intelligence, pages 40?52.
Thomas L. Griffiths and Mark Steyvers. 2004. Find-
ing scientific topics. Proceedings of the National
Academy of Sciences of the United States of America,
101(Suppl. 1):5228?5235.
Liangjie Hong and Brian D. Davison. 2010. Empirical
study of topic modeling in Twitter. In Proceedings of
the First Workshop on Social Media Analytics.
Kalervo Ja?rvelin and Jaana Keka?la?inen. 2002. Cumu-
lated gain-based evaluation of ir techniques. ACM
Transactions on Information Systems, 20(4):422?446.
John Lafferty and Chengxiang Zhai. 2003. Probabilistic
relevance models based on document and query gener-
ation. Language Modeling and Information Retrieval,
13.
Quanzhi Li, Yi-Fang Wu, Razvan Bot, and Xin Chen.
2004. Incorporating document keyphrases in search
results. In Proceedings of the 10th Americas Confer-
ence on Information Systems.
Marina Litvak and Mark Last. 2008. Graph-based key-
word extraction for single-document summarization.
In Proceedings of the Workshop on Multi-source Mul-
tilingual Information Extraction and Summarization,
pages 17?24.
Zhiyuan Liu, Wenyi Huang, Yabin Zheng, and Maosong
Sun. 2010. Automatic keyphrase extraction via topic
decomposition. In Proceedings of the 2010 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 366?376.
Qiaozhu Mei, Xuehua Shen, and ChengXiang Zhai.
2007. Automatic labeling of multinomial topic mod-
els. In Proceedings of the 13th ACM SIGKDD Interna-
tional Conference on Knowledge Discovery and Data
Mining, pages 490?499.
R. Mihalcea and P. Tarau. 2004. TextRank: Bringing or-
der into texts. In Proceedings of the 2004 Conference
on Empirical Methods in Natural Language Process-
ing.
Daniel Ramage, Susan Dumais, and Dan Liebling. 2010.
Characterizing micorblogs with topic models. In Pro-
ceedings of the 4th International Conference on We-
blogs and Social Media.
Takeshi Sakaki, Makoto Okazaki, and Yutaka Matsuo.
2010. Earthquake shakes Twitter users: real-time
event detection by social sensors. In Proceedings of
the 19th International World Wide Web Conference.
Takashi Tomokiyo and Matthew Hurst. 2003. A lan-
guage model approach to keyphrase extraction. In
Proceedings of the ACL 2003 Workshop on Multi-
word Expressions: Analysis, Acquisition and Treat-
ment, pages 33?40.
Andranik Tumasjan, Timm O. Sprenger, Philipp G. Sand-
ner, and Isabell M. Welpe. 2010. Predicting elections
with Twitter: What 140 characters reveal about politi-
cal sentiment. In Proceedings of the 4th International
Conference on Weblogs and Social Media.
Peter Turney. 2000. Learning algorithms for keyphrase
extraction. Information Retrieval, (4):303?336.
Jianshu Weng, Ee-Peng Lim, Jing Jiang, and Qi He.
2010. TwitterRank: finding topic-sensitive influential
twitterers. In Proceedings of the third ACM Interna-
tional Conference on Web Search and Data Mining.
Wei Wu, Bin Zhang, and Mari Ostendorf. 2010. Au-
tomatic generation of personalized annotation tags for
twitter users. In Human Language Technologies: The
2010 Annual Conference of the North American Chap-
ter of the Association for Computational Linguistics,
pages 689?692.
Xin Zhao, Jing Jiang, Jianshu Weng, Jing He, Lim Ee-
Peng, Hongfei Yan, and Xiaoming Li. 2011. Compar-
ing Twitter and traditional media using topic models.
In Proceedings of the 33rd European Conference on
Information Retrieval.
388
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 536?544,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Finding Bursty Topics from Microblogs
Qiming Diao, Jing Jiang, Feida Zhu, Ee-Peng Lim
Living Analytics Research Centre
School of Information Systems
Singapore Management University
{qiming.diao.2010, jingjiang, fdzhu, eplim}@smu.edu.sg
Abstract
Microblogs such as Twitter reflect the general
public?s reactions to major events. Bursty top-
ics from microblogs reveal what events have
attracted the most online attention. Although
bursty event detection from text streams has
been studied before, previous work may not
be suitable for microblogs because compared
with other text streams such as news articles
and scientific publications, microblog posts
are particularly diverse and noisy. To find top-
ics that have bursty patterns on microblogs,
we propose a topic model that simultaneous-
ly captures two observations: (1) posts pub-
lished around the same time are more like-
ly to have the same topic, and (2) posts pub-
lished by the same user are more likely to have
the same topic. The former helps find event-
driven posts while the latter helps identify and
filter out ?personal? posts. Our experiments
on a large Twitter dataset show that there are
more meaningful and unique bursty topics in
the top-ranked results returned by our mod-
el than an LDA baseline and two degenerate
variations of our model. We also show some
case studies that demonstrate the importance
of considering both the temporal information
and users? personal interests for bursty topic
detection from microblogs.
1 Introduction
With the fast growth of Web 2.0, a vast amount of
user-generated content has accumulated on the so-
cial Web. In particular, microblogging sites such
as Twitter allow users to easily publish short in-
stant posts about any topic to be shared with the
general public. The textual content coupled with
the temporal patterns of these microblog posts pro-
vides important insight into the general public?s in-
terest. A sudden increase of topically similar posts
usually indicates a burst of interest in some event
that has happened offline (such as a product launch
or a natural disaster) or online (such as the spread
of a viral video). Finding bursty topics from mi-
croblogs therefore can help us identify the most pop-
ular events that have drawn the public?s attention. In
this paper, we study the problem of finding bursty
topics from a stream of microblog posts generated
by different users. We focus on retrospective detec-
tion, where the text stream within a certain period is
analyzed in its entirety.
Retrospective bursty event detection from tex-
t streams is not new (Kleinberg, 2002; Fung et al,
2005; Wang et al, 2007), but finding bursty topic-
s from microblog steams has not been well studied.
In his seminal work, Kleinberg (2002) proposed a s-
tate machine to model the arrival times of documents
in a stream in order to identify bursts. This model
has been widely used. However, this model assumes
that documents in the stream are all about a given
topic. In contrast, discovering interesting topics that
have drawn bursts of interest from a stream of top-
ically diverse microblog posts is itself a challenge.
To discover topics, we can certainly apply standard
topic models such as LDA (Blei et al, 2003), but
with standard LDA temporal information is lost dur-
ing topic discovery. For microblogs, where posts are
short and often event-driven, temporal information
can sometimes be critical in determining the topic of
a post. For example, typically a post containing the
536
word ?jobs? is likely to be about employment, but
right after October 5, 2011, a post containing ?jobs?
is more likely to be related to Steve Jobs? death. Es-
sentially, we expect that on microblogs, posts pub-
lished around the same time have a higher probabil-
ity to belong to the same topic.
To capture this intuition, one solution is to assume
that posts published within the same short time win-
dow follow the same topic distribution. Wang et
al. (2007) proposed a PLSA-based topic model that
exploits this idea to find correlated bursty patterns
across multiple text streams. However, their model
is not immediately applicable for our problem. First,
their model assumes multiple text streams where
word distributions for the same topic are different
on different streams. More importantly, their model
was applied to news articles and scientific publica-
tions, where most documents follow the global top-
ical trends. On microblogs, besides talking about
global popular events, users also often talk about
their daily lives and personal interests. In order to
detect global bursty events from microblog posts, it
is important to filter out these ?personal? posts.
In this paper, we propose a topic model designed
for finding bursty topics from microblogs. Our mod-
el is based on the following two assumptions: (1) If
a post is about a global event, it is likely to follow
a global topic distribution that is time-dependent.
(2) If a post is about a personal topic, it is likely
to follow a personal topic distribution that is more
or less stable over time. Separation of ?global? and
?personal? posts is done in an unsupervised manner
through hidden variables. Finally, we apply a state
machine to detect bursts from the discovered topics.
We evaluate our model on a large Twitter dataset.
We find that compared with bursty topics discovered
by standard LDA and by two degenerate variations
of our model, bursty topics discovered by our model
are more accurate and less redundant within the top-
ranked results. We also use some example bursty
topics to explain the advantages of our model.
2 Related Work
To find bursty patterns from data streams, Kleinberg
(2002) proposed a state machine to model the ar-
rival times of documents in a stream. Different states
generate time gaps according to exponential density
functions with different expected values, and bursty
intervals can be discovered from the underlying state
sequence. A similar approach by Ihler et al (2006)
models a sequence of count data using Poisson dis-
tributions. To apply these methods to find bursty
topics, the data stream used must represent a single
topic.
Fung et al (2005) proposed a method that iden-
tifies both topics and bursts from document stream-
s. The method first finds individual words that have
bursty patterns. It then finds groups of words that
tend to share bursty periods and co-occur in the same
documents to form topics. Weng and Lee (2011)
proposed a similar method that first characterizes the
temporal patterns of individual words using wavelet-
s and then groups words into topics. A major prob-
lem with these methods is that the word clustering
step can be expensive when the number of bursty
words is large. We find that the method by Fung
et al (2005) cannot be applied to our dataset be-
cause their word clustering algorithm does not scale
up. Weng and Lee (2011) applied word clustering
to only the top bursty words within a single day, and
subsequently their topics mostly consist of two or
three words. In contrast, our method is scalable and
each detected bursty topic is directly associated with
a word distribution and a set of tweets (see Table 3),
which makes it easier to interpret the topic.
Topic models provide a principled and elegan-
t way to discover hidden topics from large docu-
ment collections. Standard topic models do not con-
sider temporal information. A number of temporal
topic models have been proposed to consider topic
changes over time. Some of these models focus on
the change of topic composition, i.e. word distri-
butions, which is not relevant to bursty topic detec-
tion (Blei and Lafferty, 2006; Nallapati et al, 2007;
Wang et al, 2008). Some other work looks at the
temporal evolution of topics, but the focus is not on
bursty patterns (Wang and McCallum, 2006; Ahmed
and Xing, 2008; Masada et al, 2009; Ahmed and X-
ing, 2010; Hong et al, 2011).
The model proposed by Wang et al (2007) is the
most relevant to ours. But as we have pointed out
in Section 1, they do not need to handle the sep-
aration of ?personal? documents from event-driven
documents. As we will show later in our experi-
ments, for microblogs it is critical to model users?
537
personal interests in addition to global topical trend-
s.
To capture users? interests, Rosen-Zvi et al
(2004) expand topic distributions from document-
level to user-level in order to capture users? specif-
ic interests. But on microblogs, posts are short and
noisy, so Zhao et al (2011) further assume that each
post is assigned a single topic and some words can
be background words. However, these studies do not
aim to detect bursty patterns. Our work is novel in
that it combines users? interests and temporal infor-
mation to detect bursty topics.
3 Method
3.1 Preliminaries
We first introduce the notation used in this paper and
formally formulate our problem. We assume that
we have a stream of D microblog posts, denoted as
d1, d2, . . . , dD. Each post di is generated by a user
ui, where ui is an index between 1 and U , and U is
the total number of users. Each di is also associat-
ed with a discrete timestamp ti, where ti is an index
between 1 and T , and T is the total number of time
points we consider. Each di contains a bag of word-
s, denoted as {wi,1, wi,2, . . . , wi,Ni}, where wi,j is
an index between 1 and V , and V is the vocabulary
size. Ni is the number of words in di.
We define a bursty topic b as a word distri-
bution coupled with a bursty interval, denoted as
(?b, tbs, tbe), where ?b is a multinomial distribution
over the vocabulary, and tbs and tbe (1 ? tbs ? tbe ? T )
are the start and the end timestamps of the bursty in-
terval, respectively. Our task is to find meaningful
bursty topics from the input text stream.
Our method consists of a topic discovery step and
a burst detection step. At the topic discovery step,
we propose a topic model that considers both users?
topical interests and the global topic trends. Burst
detection is done through a standard state machine
method.
3.2 Our Topic Model
We assume that there are C (latent) topics in the text
stream, where each topic c has a word distribution
?c. Note that not every topic has a bursty interval.
On the other hand, a topic may have multiple bursty
intervals and hence leads to multiple bursty topics.
We also assume a background word distribution ?B
that captures common words. All posts are assumed
to be generated from some mixture of these C + 1
underlying topics.
In standard LDA, a document contains a mixture
of topics, represented by a topic distribution, and
each word has a hidden topic label. While this is a
reasonable assumption for long documents, for short
microblog posts, a single post is most likely to be
about a single topic. We therefore associate a single
hidden variable with each post to indicate its topic.
Similar idea of assigning a single topic to a short se-
quence of words has been used before (Gruber et al,
2007; Zhao et al, 2011). As we will see very soon,
this treatment also allows us to model topic distribu-
tions at time window level and user level.
As we have discussed in Section 1, an importan-
t observation we have is that when everything else
is equal, a pair of posts published around the same
time is more likely to be about the same topic than a
random pair of posts. To model this observation, we
assume that there is a global topic distribution ?t for
each time point t. Presumably ?t has a high prob-
ability for a topic that is popular in the microblog-
sphere at time t.
Unlike news articles from traditional media,
which are mostly about current affairs, an important
property of microblog posts is that many posts are
about users? personal encounters and interests rather
than global events. Since our focus is to find popular
global events, we need to separate out these ?person-
al? posts. To do this, an intuitive idea is to compare
a post with its publisher?s general topical interests
observed over a long time. If a post does not match
the user?s long term interests, it is more likely re-
lated to a global event. We therefore introduce a
time-independent topic distribution ?u for each us-
er to capture her long term topical interests.
We assume the following generation process for
all the posts in the stream. When user u publishes
a post at time point t, she first decides whether to
write about a global trendy topic or a personal top-
ic. If she chooses the former, she then selects a topic
according to ?t. Otherwise, she selects a topic ac-
cording to her own topic distribution ?u. With the
chosen topic, words in the post are generated from
the word distribution for that topic or from the back-
ground word distribution that captures white noise.
538
1. Draw ?B ? Dirichlet(?), ? ? Beta(?), ? ?
Beta(?)
2. For each time point t = 1, . . . , T
(a) draw ?t ? Dirichlet(?)
3. For each user u = 1, . . . , U
(a) draw ?u ? Dirichlet(?)
4. For each topic c = 1, . . . , C,
(a) draw ?c ? Dirichlet(?)
5. For each post i = 1, . . . , D,
(a) draw yi ? Bernoulli(?)
(b) draw zi ? Multinomial(?ui) if yi = 0 or
zi ? Multinomial(?ti) if yi = 1
(c) for each word j = 1, . . . , Ni
i. draw xi,j ? Bernoulli(?)
ii. draw wi,j ? Multinomial(?B) if
xi,j = 0 or wi,j ? Multinomial(?zi)
if xi,j = 1
Figure 2: The generation process for all posts.
We use ? to denote the probability of choosing to
talk about a global topic rather than a personal topic.
Formally, the generation process is summarized in
Figure 2. The model is also depicted in Figure 1(a).
There are two degenerate variations of our model
that we also consider in our experiments. The first
one is depicted in Figure 1(b). In this model, we only
consider the time-dependent topic distributions that
capture the global topical trends. This model can be
seen as a direct application of the model by Wang
et al (2007). The second one is depicted in Fig-
ure 1(c). In this model, we only consider the users?
personal interests but not the global topical trends,
and therefore temporal information is not used. We
refer to our complete model as TimeUserLDA, the
model in Figure 1(b) as TimeLDA and the model in
Figure 1(c) asUserLDA. We also consider a standard
LDA model in our experiments, where each word is
associated with a hidden topic.
Learning
We use collapsed Gibbs sampling to obtain sam-
ples of the hidden variable assignment and to esti-
mate the model parameters from these samples. Due
to space limit, we only show the derived Gibbs sam-
pling formulas as follows.
First, for the i-th post, we know its publisher ui
and timestamp ti. We can jointly sample yi and zi
based on the values of all other hidden variables. Let
us use y to denote the set of all hidden variables y
and y?i to denote all y except yi. We use similar
symbols for other variables. We then have
p(yi = p, zi = c|z?i,y?i,x,w) ?
Mpi(p) + ?
Mpi(?) + 2?
?
M l(c) + ?
M l(?) + C?
?
?V
v=1
?E(v)?1
k=0 (M c(v) + k + ?)
?E(?)?1
k=0 (M c(?) + k + V ?)
, (1)
where l = ui when p = 0 and l = ti when p =
1. Here every M is a counter. Mpi(0) is the number
of posts generated by personal interests, while Mpi(1)
is the number of posts coming from global topical
trends. Mpi(?) = M
pi
0 + Mpi1 . M
ui
(c) is the number of
posts by user ui and assigned to topic c, and Mui(?) is
the total number of posts by ui. M ti(c) is the number
of posts assigned to topic c at time point ti, and M ti(?)
is the total number of posts at ti. E(v) is the number
of times word v occurs in the i-th post and is labeled
as a topic word, while E(?) is the total number of
topic words in the i-th post. Here, topic words refer
to words whose latent variable x equals 1. M c(v) is
the number of times word v is assigned to topic c,
and M c(?) is the total number of words assigned to
topic c. All the counters M mentioned above are
calculated with the i-th post excluded.
We sample xi,j for each word wi,j in the i-th post
using
p(xi,j = q|y, z,x?{i,j},w)
?
M?(q) + ?
M?(?) + 2?
?
M l(wi,j) + ?
M l(?) + V ?
, (2)
where l = B when q = 0 and l = zi when q = 1.
M?(0) and M
?
(1) are counters to record the numbers
of words assigned to the background model and any
topic, respectively, andM?(?) = M
?
(0)+M
?
(1). M
B
(wi,j)
is the number of times word wi,j occurs as a back-
ground word. M zi(wi,j) counts the number of times
word wi,j is assigned to topic zi, and M zi(?) is the to-
tal number of words assigned to topic zi. Again, all
counters are calculated with the current word wi,j
excluded.
539
Figure 1: (a) Our topic model for burst detection. (b) A variation of our model where we only consider global topical
trends. (c) A variation of our model where we only consider users? personal topical interests.
3.3 Burst Detection
Just like standard LDA, our topic model itself finds a
set of topics represented by ?c but does not directly
generate bursty topics. To identify bursty topics, we
use the following mechanism, which is based on the
idea by Kleinberg (2002) and Ihler et al (2006). In
our experiments, when we compare different mod-
els, we also use the same burst detection mechanism
for other models.
We assume that after topic modeling, for each dis-
covered topic c, we can obtain a series of counts
(mc1,mc2, . . . ,mcT ) representing the intensity of the
topic at different time points. For LDA, these
are the numbers of words assigned to topic c.
For TimeUserLDA, these are the numbers of posts
which are in topic c and generated by the global top-
ic distribution ?ti , i.e whose hidden variable yi is 1.
For other models, these are the numbers of posts in
topic c.
We assume that these counts are generated by two
Poisson distributions corresponding to a bursty state
and a normal state, respectively. Let ?0 denote the
expected count for the normal state and ?1 for the
bursty state. Let vt denote the state for time point t,
where vt = 0 indicates the normal state and vt = 1
indicates the bursty state. The probability of observ-
ing a count of mct is as follows:
p(mct |vt = l) =
e??l?m
c
t
l
mct !
,
where l is either 0 or 1. The state sequence
(v0, v1, . . . , vT ) is a Markov chain with the follow-
ing transition probabilities:
p(vt = l|vt?1 = l) = ?l,
Method P@5 P@10 P@20 P@30
LDA 0.600 0.800 0.750 N/A
TimeLDA 0.800 0.700 0.600 0.633
UserLDA 0.800 0.700 0.850 0.833
TimeUserLDA 1.000 1.000 0.900 0.800
Table 1: Precision at K for the various models.
Method P@5 P@10 P@20 P@30
LDA 0.600 0.800 0.700 N/A
TimeLDA 0.400 0.500 0.500 0.567
UserLDA 0.800 0.500 0.500 0.600
TimeUserLDA 1.000 0.900 0.850 0.767
Table 2: Precision at K for the various models after we
remove redundant bursty topics.
where l is either 0 or 1.
?0 and ?1 are topic specific. In our experiments,
we set ?0 = 1T
?
t mct , that is, ?0 is the average
count over time. We set ?1 = 3?0. For transition
probabilities, we empirically set ?0 = 0.9 and ?1 =
0.6 for all topics.
We can use dynamic programming to uncover the
underlying state sequence for a series of counts. Fi-
nally, a burst is marked by a consecutive subse-
quence of bursty states.
4 Experiments
4.1 Data Set
We use a Twitter data set to evaluate our models.
The original data set contains 151,055 Twitter users
based in Singapore and their tweets. These Twitter
users were obtained by starting from a set of seed
Singapore users who are active online and tracing
540
Bursty Period Top Words Example Tweets Label
Nov 29 vote, big, awards, (1) why didnt 2ne1 win this time! Mnet Asian
bang, mama, win, (2) 2ne1. you deserved that urgh! Music Awards
2ne1, award, won (3) watching mama. whoohoo (MAMA)
Oct 5 ? Oct 8 steve, jobs, apple, (1) breaking: apple says steve jobs has passed away! Steve Jobs
iphone, rip, world, (2) google founders: steve jobs was an inspiration! death
changed, 4s, siri (3) apple 4 life thankyousteve
Nov 1 ? Nov 3 reservior, bedok, adlyn, (1) this adelyn totally disgust me. slap her mum? girl slapping
slap, found, body, queen of cine? joke please can. mom
mom, singapore, steven (2) she slapped her mum and boasted about it on fb
(3) adelyn lives in woodlands , later she slap me how?
Nov 5 reservior, bedok, adlyn, (1) bedok = bodies either drowned or killed. suicide near
slap, found, body, (2) another body found, in bedok reservoir? bedok reservoir
mom, singapore, steven (3) so many bodies found at bedok reservoir. alamak.
Oct 23 man, arsenal, united, (1) damn you man city! we will get you next time! football game
liverpool, chelsea, city, (2) wtf 90min goal!
goal, game, match (3) 6-1 to city. unbelievable.
Table 3: Top-5 bursty topics ranked by TimeUserLDA. The labels are manually given. The 3rd and the 4th bursty
topics come from the same topic but have different bursty periods.
Rank LDA UserLDA TimeLDA
1 Steve Jobs? death MAMA MAMA
2 MAMA football game MAMA
3 N/A #zamanprimaryschool MAMA
4 girl slapping mom N/A girl slapping mom
5 N/A iphone 4s N/A
Table 4: Top-5 bursty topics ranked by other models. N/A indicates a meaningless burst.
their follower/followee links by two hops. Because
this data set is huge, we randomly sampled 2892
users from this data set and extracted their tweets
between September 1 and November 30, 2011 (91
days in total). We use one day as our time window.
Therefore our timestamps range from 1 to 91. We
then removed stop words and words containing non-
standard characters. Tweets containing less than 3
words were also discarded. After preprocessing, we
obtained the final data set with 3,967,927 tweets and
24,280,638 tokens.
4.2 Ground Truth Generation
To compare our model with other alternative models,
we perform both quantitative and qualitative evalua-
tion. As we have explained in Section 3, each mod-
el gives us time series data for a number of topics,
and by applying a Poisson-based state machine, we
can obtain a set of bursty topics. For each method,
we rank the obtained bursty topics by the number
of tweets (or words in the case of the LDA model)
assigned to the topics and take the top-30 bursty top-
ics from each model. In the case of the LDA mod-
el, only 23 bursty topics were detected. We merged
these topics and asked two human judges to judge
their quality by assigning a score of either 0 or 1.
The judges are graduate students living in Singapore
and not involved in this project. The judges were
given the bursty period and 100 randomly selected
tweets for the given topic within that period for each
bursty topic. They can consult external resources to
help make judgment. A bursty topic was scored 1
if the 100 tweets coherently describe a bursty even-
t based on the human judge?s understanding. The
inter-annotator agreement score is 0.649 using Co-
hen?s kappa, showing substantial agreement. For
ground truth, we consider a bursty topic to be cor-
rect if both human judges have scored it 1. Since
some models gave redundant bursty topics, we al-
so asked one of the judges to identify unique bursty
541
topics from the ground truth bursty topics.
4.3 Evaluation
In this section, we show the quantitative evalua-
tion of the four models we consider, namely, LDA,
TimeLDA, UserLDA and TimeUserLDA. For each
model, we set the number of topics C to 80, ? to 50C
and ? to 0.01 after some preliminary experiments.
Each model was run for 500 iterations of Gibbs sam-
pling. We take 40 samples with a gap of 5 iterations
in the last 200 iterations to help us assign values to
all the hidden variables.
Table 1 shows the comparison between these
models in terms of the precision of the top-K result-
s. As we can see, our model outperforms all other
models for K <= 20. For K = 30, the UserLDA
model performs the best followed by our model.
As we have pointed out, some of the bursty topics
are redundant, i.e. they are about the same bursty
event. We therefore also calculated precision at K
for unique topics, where for redundant topics the one
ranked the highest is scored 1 and the other ones
are scored 0. The comparison of the performance
is shown in Table 2. As we can see, in this case,
our model outperforms other models with all K. We
will further discuss redundant bursty topics in the
next section.
4.4 Sample Results and Discussions
In this section, we show some sample results from
our experiments and discuss some case studies that
illustrate the advantages of our model.
First, we show the top-5 bursty topics discovered
by the TimeUserLDA model in Table 3. As we can
see, all these bursty topics are meaningful. Some of
these events are global major events such as Steve
Jobs? death, while some others are related to online
events such as the scandal of a girl boasting about
slapping her mother on Facebook. For comparison,
we also show the top-5 bursty topics discovered by
other models in Table 4. As we can see, some of
them are not meaningful events while some of them
are redundant.
Next, we show two case studies to demonstrate
the effectiveness of our model.
Effectiveness of Temporal Models: Both
TimeLDA and TimeUserLDA tend to group posts
published on the same day into the same topic. We
find that this can help separate bursty topics from
general ones. An example is the topic on the Circle
Line. The Circle Line is one of the subway lines of
Singapore?s mass transit system. There were a few
incidents of delays or breakdowns during the period
between September and November, 2011. We show
the time series data of the topic related to the Circle
Line of UserLDA, TimeLDA and TimeUserLDA in
Figure 3. As we can see, the UserLDA model de-
tects a much large volume of tweets related to this
topic. A close inspection tells us that the topic under
UserLDA is actually related to the subway systems
in Singapore in general, which include a few other
subway lines, and the Circle Line topic is merged
with this general topic. On the other hand, TimeL-
DA and TimeUserLDA are both able to separate the
Circle Line topic from the general subway topic be-
cause the Circle Line has several bursts. What is
shown in Figure 3 for TimeLDA and TimeUserLDA
is only the topic on the Circle Line, therefore the
volume is much smaller. We can see that TimeLDA
and TimeUserLDA show clearer bursty patterns than
UserLDA for this topic. The bursts around day 20,
day 44 and day 85 are all real events based on our
ground truth.
Effectiveness of User Models: We have stat-
ed that it is important to filter out users? ?person-
al? posts in order to find meaningful global events.
We find that our results also support this hypothesis.
Let us look at the example of the topic on the Mnet
Asian Music Awards, which is a major music award
show that is held by Mnet Media annually. In 2011,
this event took place in Singapore on November 29.
Because Korean pop music is very popular in Singa-
pore, many Twitter users often tweet about Korean
pop music bands and singers in general. All our top-
ic models give multiple topics related to Korean pop
music, and many of them have a burst on Novem-
ber 29, 2011. Under the TimeLDA and UserLDA
models, this leads to several redundant bursty top-
ics for the MAMA event ranked within the top-30.
For TimeUserLDA, however, although the MAMA
event is also ranked the top, there is no redundan-
t one within the top-30 results. We find that this is
because with TimeUserLDA, we can remove tweet-
s that are considered personal and therefore do not
contribute to bursty topic ranking. We show the top-
ic intensity of a topic about a Korean pop singer in
542
 0
 200
 400
 600
 800
 1000
 1200
 10  20  30  40  50  60  70  80  90
m
t
UserLDA
 0
 200
 400
 600
 800
 1000
 1200
 10  20  30  40  50  60  70  80  90
m
t
TimeLDA
 0
 200
 400
 600
 800
 1000
 1200
 10  20  30  40  50  60  70  80  90
m
t
TimeUserLDA
Figure 3: Topic intensity over time for the topic on the Circle Line.
 0
 1000
 2000
 3000
 4000
 5000
 6000
 7000
 10  20  30  40  50  60  70  80  90
m
t
UserLDA
 0
 1000
 2000
 3000
 4000
 5000
 6000
 7000
 10  20  30  40  50  60  70  80  90
m
t
TimeLDA
 0
 1000
 2000
 3000
 4000
 5000
 6000
 7000
 10  20  30  40  50  60  70  80  90
m
t
TimeUserLDA
Figure 4: Topic intensity over time for the topic about a Korean pop singer. The dotted curves show the topic on Steve
Jobs? death.
Figure 4. For reference, we also show the intensity
of the topic on Steve Jobs? death under each mod-
el. We can see that because this topic is related to
Korean pop music, it has a burst on day 90 (Novem-
ber 29). But if we consider the relative intensity of
this burst compared with Steve Jobs? death, under
TimeLDA and UserLDA, this topic is still strong but
under TimeUserLDA its intensity can almost be ig-
nored. This is why with TimeLDA and UserLDA
this topic leads to a redundant burst within the top-
30 results but with TimeUserLDA the burst is not
ranked high.
5 Conclusions
In this paper, we studied the problem of finding
bursty topics from the text streams on microblogs.
Because existing work on burst detection from tex-
t streams may not be suitable for microblogs, we
proposed a new topic model that considers both the
temporal information of microblog posts and user-
s? personal interests. We then applied a Poisson-
based state machine to identify bursty periods from
the topics discovered by our model. We compared
our model with standard LDA as well as two de-
generate variations of our model on a real Twitter
dataset. Our quantitative evaluation showed that our
model could more accurately detect unique bursty
topics among the top ranked results. We also used
two case studies to illustrate the effectiveness of the
temporal factor and the user factor of our model.
Our method currently can only detect bursty top-
ics in a retrospective and offline manner. A more in-
teresting and useful task is to detect realtime bursts
in an online fashion. This is one of the directions we
plan to study in the future. Another limitation of the
current method is that the number of topics is pre-
determined. We also plan to look into methods that
allow appearance and disappearance of topics along
the timeline, such as the model by Ahmed and Xing
(2010).
Acknowledgments
This research is supported by the Singapore Nation-
al Research Foundation under its International Re-
search Centre @ Singapore Funding Initiative and
administered by the IDM Programme Office. We
thank the reviewers for their valuable comments.
References
Amr Ahmed and Eric P. Xing. 2008. Dynamic non-
parametric mixture models and the recurrent Chinese
543
restaurant process: with applications to evolutionary
clustering. In Proceedings of the SIAM International
Conference on Data Mining, pages 219?230.
Amr Ahmed and Eric P. Xing. 2010. Timeline: A dy-
namic hierarchical Dirichlet process model for recov-
ering birth/death and evolution of topics in text stream.
In Proceedings of the 26th Conference on Uncertainty
in Artificial Intelligence, pages 20?29.
David M. Blei and John D. Lafferty. 2006. Dynamic
topic models. In Proceedings of the 23rd International
Conference on Machine Learning.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent Dirichlet alocation. Journal of Machine
Learning Research, 3:993?1022.
Gabriel Pui Cheong Fung, Jeffrey Xu Yu, Philip S. Yu,
and Hongjun Lu. 2005. Parameter free bursty events
detection in text streams. In Proceedings of the 31st
International Conference on Very Large Data Bases,
pages 181?192.
Amit Gruber, Michal Rosen-Zvi, and Yair Weiss. 2007.
Hidden topic Markov model. In Proceedings of the
International Conference on Artificial Intelligence and
Statistics.
Liangjie Hong, Byron Dom, Siva Gurumurthy, and
Kostas Tsioutsiouliklis. 2011. A time-dependent top-
ic model for multiple text streams. In Proceedings of
the 17th ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining, pages 832?
840.
Alexander Ihler, Jon Hutchins, and Padhraic Smyth.
2006. Adaptive event detection with time-varying
poisson processes. In Proceedings of the 12th
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, pages 207?216.
Jon Kleinberg. 2002. Bursty and hierarchical structure in
streams. In Proceedings of the 8th ACM SIGKDD In-
ternational Conference on Knowledge Discovery and
Data Mining, pages 91?101.
Tomonari Masada, Daiji Fukagawa, Atsuhiro Takasu,
Tsuyoshi Hamada, Yuichiro Shibata, and Kiyoshi
Oguri. 2009. Dynamic hyperparameter optimization
for bayesian topical trend analysis. In Proceedings of
the 18th ACM Conference on Information and knowl-
edge management, pages 1831?1834.
Ramesh M. Nallapati, Susan Ditmore, John D. Lafferty,
and Kin Ung. 2007. Multiscale topic tomography. In
Proceedings of the 13th ACM SIGKDD International
Conference on Knowledge Discovery and Data Min-
ing, pages 520?529.
Michal Rosen-Zvi, Thomas Griffiths, Mark Steyvers, and
Padhraic Smyth. 2004. The author-topic model for au-
thors and documents. In Proceedings of the 20th con-
ference on Uncertainty in artificial intelligence, pages
487?494.
Xuerui Wang and Andrew McCallum. 2006. Topics over
time: a non-Markov continuous-time model of topical
trends. In Proceedings of the 12th ACM SIGKDD In-
ternational Conference on Knowledge Discovery and
Data Mining, pages 424?433.
Xuanhui Wang, ChengXiang Zhai, Xiao Hu, and Richard
Sproat. 2007. Mining correlated bursty topic pattern-
s from coordinated text streams. In Proceedings of
the 13th ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining, pages 784?
793.
Chong Wang, David M. Blei, and David Heckerman.
2008. Continuous time dynamic topic models. In Pro-
ceedings of the 24th Conference on Uncertainty in Ar-
tificial Intelligence, pages 579?586.
Jianshu Weng and Francis Lee. 2011. Event detection in
Twitter. In Proceedings of the 5th International AAAI
Conference on Weblogs and Social Media.
Wayne Xin Zhao, Jing Jiang, Jianshu Weng, Jing He,
Ee-Peng Lim, Hongfei Yan, and Xiaoming Li. 2011.
Comparing twitter and traditional media using topic
models. In Proceedings of the 33rd European confer-
ence on Advances in information retrieval, pages 338?
349.
544
