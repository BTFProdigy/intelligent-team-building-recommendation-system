Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 481?488
Manchester, August 2008
Classifying What-type Questions by Head Noun Tagging 
Fangtao Li, Xian Zhang, Jinhui Yuan, Xiaoyan Zhu 
State Key Laboratory on Intelligent Technology and Systems 
Tsinghua National Laboratory for Information Science and Technology  
Department of Computer Sci. and Tech., Tsinghua University, Beijing 100084, China 
zxy-dcs@tsinghua.edu.cn 
 
Abstract 
Classifying what-type questions into 
proper semantic categories is found more 
challenging than classifying other types 
in question answering systems.  In this 
paper, we propose to classify what-type 
questions by head noun tagging. The ap-
proach highlights the role of head nouns 
as the category discriminator of what-
type questions. To reduce the semantic 
ambiguities of head noun, we integrate 
local syntactic feature, semantic feature 
and category dependency among adjacent 
nouns with Conditional Random Fields 
(CRFs). Experiments on standard ques-
tion classification data set show that the 
approach achieves state-of-the-art per-
formances. 
1 Introduction 
Question classification is a crucial component of 
modern question answering system. It classifies 
questions into several semantic categories which 
indicate the expected semantic type of answers to 
the questions. The semantic category helps to 
filter out irrelevant answer candidates, and de-
termine the answer selection strategies.   1 
The widely used question category criteria is a 
two-layered taxonomy developed by Li and Roth 
(2002) from UIUC. The hierarchy contains 6 
coarse classes and 50 fine classes as shown in 
Table 1. In this paper, we focus on fine-category 
classification. Each fine category will be denoted 
as ?Coarse:fine?, such as ?HUM:individual?. 
A what-type question is defined as the one 
whose question word is ?what?, ?which?, 
?name? or ?list?. It is a dominant type in ques-
tion answering system. Li and Roth (2006) find  
                                                 
? 2008. Licensed under the Creative Commons Attri-
bution-Noncommercial-Share Alike 3.0 Unported 
license (http://creativecommons.org/licenses/by-nc-
sa/3.0/). Some rights reserved. 
Coarse Fine 
ABBR abbreviation, expression 
DESC definition, description, manner, reason 
ENTY animal, body, color, creation, currency, dis-
ease/medicine, event, food, instrument, language, 
letter, other, plant, product, religion, sport, sub-
stance, symbol, technique, term, vehicle, word 
HUM description, group, individual, title 
LOC city, country, mountain, other, state 
NUM code, count, date, distance, money, order, other, 
percent, period, speed, temperature, size, weight 
Table 1.  Question Ontology 
that the distribution of what-type questions over 
the semantic classes is quite diverse, and they are 
more difficult to be classified than other types.  
Table 2 shows the classification accuracies of 
each question word in UIUC data set using Sup-
port Vector Machine (SVM) with unigram fea-
tures. What-type questions account for more than 
70 percent in the data set, but the classification 
accuracy of this type only achieves 75.50%. In 
this experiment, 90.53% (86 over 95) of the er-
rors are generated by what-type questions. Due to 
its challenge, this paper focuses on what-type 
question classification. 
 
 Total Wrong Accuracy 
What-type 351 86 75.50% 
Where 26 2 92.31% 
When 26 0 100.0% 
Who 47 3 93.62% 
How 46 4 91.30% 
Why 4 0 100.0% 
Total 500 95 81.00% 
Table 2.  Classification performance for each 
question words with unigram 
Head noun has been presented to  play an im-
portant role in classifying what-type questions 
(Metzler and Croft, 2005).  It refers to the noun 
reflecting the focus of a question, such as 
?flower? in the question ?What is Hawaii's state 
481
flower??. These nouns can effectively reduce the 
noise generated by other words. If the head noun 
?length? is identified from the question ?What is 
the length of the coastline of the state of 
Alaska??, this question can be easily classified 
into ?NUM:distance?. However, the above SVM 
misclassified this question into ?LOC:-state?, as 
the words ?state? and ?Alaska? confused the 
classifier. Considering another two questions 
expressed in (Zhang and Lee, 2002), ?Which 
university did the president graduate from?? and 
?Which president is a graduate of the Harvard 
University?, although they contain similar words, 
it is not difficult to distinguish them with the 
head nouns ?university? and ?president? respec-
tively. 
Nevertheless, a head noun may correspond to 
several semantic categories. In this situation, we 
need to incorporate the head noun context for 
disambiguation. The potentially useful context 
features include local syntactic features, semantic 
features and neighbor?s semantic category. Take 
the noun ?money? as an example, it possibly cor-
responds to two categories: ?NUM:money? and 
?ENTY:currency?. If there is an adjacent word 
falling into ?Loc:country? category, the ?money? 
tends to belong to ?ENTY:currency?. Otherwise, 
if the ?ENTY:product? or ?HUM:individual? 
surrounds it, the word ?money? may refer to 
?NUM:money?. 
Based on the above notions, we propose a new 
strategy to classify what-type questions by word 
tagging, and the selected head noun determines 
question category.  The question classification 
task is formulated into word sequence tagging 
problem. All the question words are divided into 
semantic words and non-semantic words. The 
semantic word expresses certain semantic cate-
gory, such as ?dog? corresponding to category 
?ENTITY:animal?, while ?have? corresponding 
to no category. The label for semantic words is 
one of the question categories, and ?O? is for 
non-semantic word. Here, we just consider the 
nouns as semantic words, others as non-semantic 
words. Each word in a question will be tagged as 
a label using Conditional Random Fields model, 
and the head noun?s label is chosen as the ques-
tion category.  
In conclusion, the CRFs based approach has 
two main steps: the first step is to tag all the 
words in questions using CRFs, and the second 
step is choosing the head noun?s label as the 
question category. It can use the head noun to 
eliminate the noisy words, and take advantages 
of CRFs model to integrate not only the syntactic 
and semantic features, but also the adjacent cate-
gories to tag head noun. 
The rest of this paper is organized as follows: 
Section 2 discusses related work. Section 3 in-
troduces the Condition Random Fields(CRFs) 
and the defined Long-Dependency CRFs 
(LDCRFs). Section 4 describes the features used 
in the LDCRFs. The head noun extraction me-
thod is presented in Section 5. We evaluate the 
proposed approach in Section 6. Section 7 con-
cludes this paper and discusses future work. 
2 Related works 
Question Answering Track was first introduced 
in the Text REtrieval Conference (TREC) in 
1999. Since then, question classification has been 
a popular topic in the research community of text 
mining. Simple question classification approach-
es usually employ hand-crafted rules (such as 
Prager et. al, 1999), which are effective for spe-
cific question taxonomy. However, laborious 
human effort is required to create these rules. 
Some other systems employed machine learn-
ing approaches to classify questions.  Li and 
Roth (2002) presented a hierarchical classifier 
based on the Sparse Network of Winnows (Snow) 
architecture. Tow classifiers were involved in 
this work: the first one classified questions into 
the coarse categories; and the other classified 
questions into fine categories. Several syntactic 
and semantic features, including semi-
automatically constructed class-specific relation-
al features, were extracted and compared in their 
experiments. The results showed that the hierar-
chical classifier was effective for question classi-
fication task.  
Metzler and Croft (2005) used prior know-
ledge about correlations between question words 
and types to train word-specific question classifi-
ers. They identified the question words firstly, 
and trained separate classifier for each question 
word. WordNet was used as semantic features to 
boost the classification performance. In this pa-
per, according to question word, all the questions 
are classifie into two categories: what-type ones 
and non-what-type one. 
Recent question classification methods have 
paid more attention on the syntactic structure of 
sentence. They used a parser to get the syntactic 
tree, and then took advantage of the structure 
information. Zhang and Lee (2002) proposed a 
tree kernel Support Vector Machine classifier 
and experiment results showed that syntactic in-
formation and tree kernel could solve this prob-
482
lem. Nguyen et al (2007) proposed a subtree 
mining method for question classification. They 
formulated question classification as tree catego-
ry determination, and maximum entropy and 
boosting model with subtree features were used. 
The experiment results showed that the subtree 
mining method can achieve a higher accuracy in 
question classification task.  
In this paper, we formulate the what-type 
question classification as word sequence tagging 
problem. The tagged label is either one of the 
question categories for nouns s or ?O? for other 
words. Since head noun can be the discriminator 
for a question, its tag is extracted as the question 
category in our work. A long-dependency Condi-
tional Random Fields Classifier is defined to tag 
question words with the features which not only 
include the syntactic and semantic features, but 
also the semantic categories? transition features. 
3 Conditional Random Fields 
Conditional Random Fields (CRFs) are a type of 
discriminative probabilistic model proposed for 
labeling sequential data (Lafferty et al 2001). Its 
definition is as follows:      
Definition: Let ( )G V E= ,  be a graph such that 
( )v v VY ?=Y , so that Y  is indexed by the vertices of G . 
Then ( ),X Y  is a conditional random field in case, when 
conditioned on X , the random variables vY  obey the Mar-
kov property with respect to the 
graph: ( )v wp Y Y w v| , , ? =X ( )v wp Y Y w v| , ,X ? , where w v?  
means that w  and v  are neighbors in G . 
   
The joint distribution over the label sequence Y  
given X  has the form  
1
( ) exp ( ) ( )
( ) i i i ie E i v V i
p t e e s v v
Z
? ?
? , ? ,
| = , | , + , | , ,? ?? ?? ?? ?Y X Y X Y XX
where ( )Z X  is a normalization factor, is  is a 
state feature function and it  is a transition fea-
ture function, i?  and i?  are the corresponding 
weights. 
 Here we assume the features are given, then 
the parameter estimation problem is to determine 
the parameters 1 2 1 2( , )? ? ? ? ?= , , , ,? ?  from 
training data. The inference problem is to find 
the most probable label sequence y?  for input 
sequence x . 
  In the training set, we label all the noun 
words with semantic question categories, and 
other words will be labeled by ?O?. We suppose 
that only adjacent noun words connect with each 
other, and there is no edge between noun and 
non-noun words, i.e., noun word and non-noun 
words may share neighbor?s state features, but 
they are not connected by an edge. A labeled ex-
ample is shown as ?What/O was/O 
Queen/HUM:individual Victria/HUM:individual 
?s/O title/HUM:title regarding/O India/LOC:-
country?. In this labeled sentence, only three 
edges connect four noun words: Queen, Victria, 
title and India.  
 
Figure 1. Long-Dependency CRFs, the dotted 
lines summarize many outgoing edges 
 
  With this assumption, we define a Long-
Dependency Conditional Random Fields 
(LDCRFs) model (see Figure 1). The long de-
pendency means that the target words may have 
no edge with its neighbors, but connect with oth-
er words at a long distance. It can be considered 
as a type of linear- chain CRFs.  Its parameter 
estimation problem and inference problem can be 
solved by the algorithm for chain-structure CRFs 
(Sutton and McCallum, 2007). 
4 Feature Sets 
One of the most attractive advantages  of CRFs is 
that they can integrate rich features, including 
not only state features, but also transition fea-
tures. In this section, we will introduce the syn-
tactic, semantic and transition features used in 
our sequence tagging approach. 
4.1 Syntactic Features 
The questions, which have similar syntactic style, 
intend to belong to the same category. Besides 
words, part-of-speech, chunker, parser informa-
tion and question length are used as syntactic 
features. 
All the words are lemmatized to root forms, 
and a window size (here is 4) is set to utilize the 
surrounding words. 
The part-of-speech (POS) tagging is com-
pleted by SS Tagger (Tsuruoka and Tsujii, 2005), 
with our own improvement. 
The noun phrase chunking (NP chunking) 
module uses the basic NP chunker software from 
483
(Ramshaw and Marcus, 1995) to recognize the 
noun phrases in the question.  
The importance of question syntactic structure 
is reported in (Zhang and Lee, 2002; Nguyen et 
al. 2007). They used complex machine learning 
method to capture the tree architecture. The 
LDCRFs based approach just selects parent node, 
relation with parent and governor for each target 
word generated from Minipar(Lin, 1999). 
The length of question is another important 
syntactic feature. In our experiment, a threshold 
is set to denote the length as ?high? or ?low?. 
4.2 Semantic Features 
Semantic features concern what words mean and 
how these meanings combine in sentence to form 
sentence meanings. Named Entity is a predefined 
semantic category for noun word. WordNet 
(Fellbaum, 1998) is a public semantic lexicon for 
English language, and it is used to get hypernym 
for noun word and synset for head verb which is 
the first notional verb in the sentence. 
   Named Entity: Named entity recognizer as-
signs a semantic category to the noun phrase. It 
is widely used to provide semantic information in 
text mining. In this paper, Stanford Named Entity 
Recognizer (Finkel et al 2005) is used to classify 
noun phrases into four semantic categories: 
PERSON, LOCATION, ORGANIZARION and 
MISC. 
Noun Hypernym: Hypernyms can be consi-
dered as semantic abstractions. It helps to narrow 
the gap between training set and testing set. For 
example, ?What is Maryland's state bird??, if we 
recursively find the bird?s hypernym ?animal?, 
which appeared in training set, this question can 
be easily classified. 
In training set, we try to select appropriate 
hypernyms for each category. An correct Word-
Net sense is first assigned for each polysemous 
noun, and then all its hypernyms are recursively 
extracted. The sense determination step is 
processed with the algorithm in (Pedersen et al 
2005). They disambiguate word sense by assign-
ing a target word the sense, which is most related 
to the senses of its neighboring words.  
Since the word sense disambiguation method 
has low performance, with F1-measure below 
50% reported in (Pedersen et al 2005), a feature 
selection method is used to extract the most dis-
criminative hypernyms. The hypernyms selection 
method is processed as follows: we first remove 
the low frequency hypernyms, and select the 
hypernyms using a chi-square method. The chi-
square value measures the lack of independence 
between a hypernym h and category jc . It is de-
fined as: 
2
2 ( ) ( )( , )
( ) ( ) ( ) ( )j
A B C D AD CB
h c
A C B D A B C D
? + + + ? ?= + ? + ? + ? +  
where A is the number of hypernym h, which 
belongs to category jc ; B is the number of  h out 
of jc ; C is the number of other hypernyms in jc ; 
D is the number of other hypernyms out of jc . 
We set a threshold to select the most discri-
minative hypernym set. Extracted examples are 
shown in Figure 2.  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 2. Examples of extracted hypernym  
 
It can be seen that these hypernyms are appro-
priate to describe the semantic meaning of the 
category.  They are expected to work as the 
class-specific relational features which are semi-
constructed by (Li and Roth, 2002). In our ap-
proach, we just use the noun?s minimum upper 
hypernym, existing in training set, as the feature. 
Head Verb Synset:  To avoid losing question 
verb information, we extract head verb, which is 
the first notional verb in a question, and expand 
it using WordNet synset as feature. The head 
verb extraction is based on the following simple 
rules: 
If the first word is ?name? or ?list?, the head 
verb will be denoted as this word. If the first verb 
following question word is ?do? or other aux-
iliary verb, the next verb is extracted as head 
verb. Otherwise the head verb is extracted as the 
first verb after question word. 
4.3 Transition Features 
State transition feature captures the contextual 
constraints among labels. We define it as  
( 1 ) ( )y yt e i i e e y y??, ?=< , + >, | , = | =< , > .Y X Y
ENTITY:animal: 
animal, carnivore, chordate, equine, 
horse, living_thing, vertebrate, mammal, 
odd-toed_ungulate, organism, placental  
ENTITY:food: 
alcohol, beer, beverage, brew, cereal, 
condiment, crop, drink, drug_of_abuse, 
flavorer, food, foodstuff, helping, indefi-
nite_quantity, ingredient, liquid, output, 
produce, small_indefinite_quantity, pro-
duction, solid, substance, vegetable 
484
Where e represents the edge between adjacent 
nouns. It captures adjacent categories as features 
to tag the target noun. Note that, for simplicity, 
the value of above feature is independent of the 
observations X.  
5 Head Noun Extraction 
After tagging all the words in a question, we will 
extract head noun and assign its tagged label to 
the question as the final question classification 
result. 
The head noun extraction is a simple heuristic 
method inspired by  (Metzler and Croft, 2005). 
We first run a POS tagger on each question, and 
post-process them to make sure that each sen-
tence has at least one noun word. Next, the first 
NP chunk after the question word is extracted by 
shallow parsing. The head noun is determined by 
the following heuristic rules: 
1. If the NP chunker is before the first verb, 
or the NP chunk is after the first verb but 
there is no possessive case after the NP 
chunker, we mark the rightmost word in 
the chunker as head noun. 
2. Otherwise, extract the next NP chunker 
and recursively process the above rules. 
Although this method may depend on the per-
formance of POS tagger and shallow parser, it 
achieves the accuracy of over 95% on the UIUC 
data set in our implementation. 
6 Experiments 
6.1 Experiment Settings 
Data Set: 
We evaluate the proposed approach on the 
UIUC data set (Li and Roth, 2002). 5500 ques-
tions are selected for training, and 500 questions 
are selected for testing. The classification catego-
ries have been introduced as question ontology   
in section 1. This paper only focuses on 50 fine 
classes. 
To train the LDCRFs, we manually labeled all 
the noun words with one of 50 fine categories.  
Other words are labeled with ?O?. One of the 
labeled examples is ?What/O was/O 
Queen/HUM:individual Victria/HUM:individual 
?s/O title/HUM:title regarding/O In-
dia/LOC:country?. Ten people labeled 3407 
what-type questions as training set. Each ques-
tion was independently annotated by two people 
and reviewed by the third. For words which have 
more than one category, the annotators selected 
the most salient one according to the context. For 
testing set, 351 what-type questions were se-
lected for experiments evaluation. 
Evaluation metric: 
Accuracy performance is widely used to evaluate 
question classification methods [Li and Roth, 
2002; Zhang and Lee, 2003, Melter and Croft, 
2004; Nguyen et al 2007].   
6.2 Approach Performance Evaluation 
 # Wrong Accuracy 
SVM 86 75.50% 
LDCRFs-
based 
80 77.20% 
Table 3. LDCRFS-based Approach V.S. SVM 
Table 3 shows the compared results between 
the proposed LDCRFs based approach and SVM 
with unigram feature. The LDCRFs based ap-
proach achieves accuracy of 77.20%, compared 
with 75.50% of SVM. Observing the detailed 
classification results, we conclude two advantag-
es of LDCRFs over SVMs. First LDCRFs based 
approach focuses on head noun to reduce the 
noise generated by other words. The question 
?What is the length of the coastline of the state of 
Alaska?? is misclassified as ?LOC:state? by 
SVM, whereas it is correctly classified by our 
approach. Second, LDCRFs based approach can 
utilize rich features, including not only state fea-
tures, but also transition features. With the new 
features involved, LDCRFs is expected to im-
prove classification performance. This unigram 
result is used as our baseline. The following ex-
periments are conducted to test the new feature 
contribution. 
Syntactic Features: 
In addition to words, four types of features, in-
cluding part-of-speech (POS), chunker, parser 
information (Parser), and question length 
(Length), are extracted as syntactic features. 
 
 Accuracy 
Unigram (U) 77.20% 
U+POS 78.35% 
U+Chunker 77.20% 
U+Parser 79.20% 
U+Length 77.49% 
Total Syn 80.06% 
Table 4. Syntactic Feature Performance 
From the syntactic feature results in Table 4, 
we can draw the following conclusions?  
(a). Among four types of syntactic features, pars-
485
er information contributes mostly. (Metzler and 
Croft, 2005) once claimed that it didn?t make 
improvement by just incorporating these infor-
mation as explicit feature, and they should be 
used implicitly via a tree structure. Without using 
the complex tree mining and representing tech-
nique, our LDCRFs-based approach just incorpo-
rates the word parent, relation with parent and 
word governor from Minipar as features. The 
experiments show that the parser information 
feature is able to capture the syntactic structure 
information, and it makes much improvement in 
this sequence tagging approach. 
(b) Question length makes small improvement. 
However, the chunker features make no im-
provement, consistent with the observation re-
ported by (Li and Roth, 2006). 
? The best accuracy (80.06%) is achieved by 
integrating all the syntactic features.  
Semantic Features: 
 
 Accuracy 
Unigram(U) 77.20% 
U+NE 77.20% 
U+HVSyn 78.63% 
U+NHype 78.35% 
Total Sem 80.06% 
Table 5. Semantic Feature Performance 
The semantic features include Named Entity 
(NE), Noun Hypernym (NHype) and Head Verb 
Synset (HVSyn).  
From Table 5 we can draw the following conclu-
sions: 
(a) NE makes no improvement in classification 
task. The reason is that the named entity recog-
nizer contains only four semantic categories. It is 
too coarse to distinguish 50 fined-categories. 
 (b) The LDCRFs-based approach just considers 
the noun words as semantic words. The head 
verb synsets (HVSyn) are imported as one of 
semantic features. The experiment results show 
that it is effective to incorporate the head verb as 
features, which achieves the best individual accu-
racy among semantic features. 
(c) Noun hypernyms (NHype) are the most im-
portant semantic features. They narrow the se-
mantic gap between training set and testing set. 
From Section 4.2, we can see that the selected 
noun hypernyms are appropriate for each catego-
ry. While, the experiment with NHype features 
doesn?t make considerable improvement as we 
previously thought. The reason may come from 
the fact that the word sense disambiguation me-
thod has low performance. A hypernym selection 
method is used in training set, but we didn?t 
tackle the error in testing set. Once the word 
sense disambiguation is wrong, it will not make 
improvement, but generate noise (see the discus-
sion examples in next section).  
(d) It is an interesting result that using all the se-
mantic features can achieve the same accuracy as 
the syntactic features (80.06%).  
Feature Combination: 
In this section, we carry out experiments to ex-
amine whether the performance can be boosted 
by integrating syntactic features and semantic 
features. Several results are shown in Table 6. 
The experiments show that: 
(a)  Parser Information and Head Verb Synset are 
both the most contributive features for syntactic 
set and semantic feature set. While the perfor-
mance with these two features can?t beat the per-
formance by combining Parser Information and 
Noun Hypernyms.  
 
 Accuracy 
U+POS+NE+HVSyn 80.91% 
U+Parser+NHype 81.77% 
U+Parser+HVSyn 80.91% 
U+POS+Length+NHype 80.63% 
Total 82.05% 
Table 6. Combined Feature Performance 
(b) The best result for classifying what-type 
questions with our approach is achieved by inte-
grating all the features. The accuracy is 82.05%, 
which is 18.7 percent error reduction (from 
22.08% to 17.95%) over unigram feature set. It 
shows that the features we extract are effectively 
used in our CRFs based approach. 
Transition Feature:  
Transition feature can capture the information 
between adjacent categories. It offers another 
semantic feature for LDCRFs-based approach.  
 
 No transition 
features  
With transi-
tion features 
Syn 79.20% 80.06% 
Sem 79.49% 80.06% 
Total 81.48% 82.05% 
Table 7. Transition Feature Performance 
The performances of all these three experi-
ment decline without the transition features. It 
shows that the dependency between adjacent se-
486
mantic categories contributes to the classifier 
performance.  
6.3 System Performance Comparison and 
Discussion  
In this section, the what-type questions and non-
what-type questions are combined to show the 
final result. Non-what-type questions are classi-
fied using SVM with unigrams as reported in 
Section 1, and what-type questions are classified 
by the LDCRFs based approach. The combined 
results are used to compare with the current 
question classification methods. 
 
Classifier Accuracy
Li?s Hierarchical method 84.20% 
Nguyen?s tree method 83.60% 
Metzler?s U+ WordNet method 82.20% 
LDCRFs-based with U+Parser 83.60% 
LDCRFs-based with U+NHype 83.00% 
LDCRFs-based with total features 85.60% 
Table 8. Comparison with related work 
Table 8 shows the accuracies of the LDCRFs 
based question classification approach with dif-
ferent feature sets, in comparison with the tree 
method (Nguyen et al 2007), the WordNet Me-
thod (Metzler and Croft, 2005) and the hierarchical 
method (Li and Roth, 2002). We can see the 
LDCRFs-based approach is effective: 
(a) Without formulating the syntactic structure as 
a tree, the LDCRFs-based approach still achieves 
accuracy 83.60% with unigram and parser infor-
mation, which is the same as Nguyen?s tree clas-
sifier.  
(b) Although the LDCRFs-based approach with 
unigrams and Noun Hypernyms generates 
noise as described in Section 6.2, it still out-
performs the Metzler?s method using WordNet 
and unigram features (83.00% v.s. 82.20%).  
(c) The experiment with total features achieves 
the accuracy of 85.60%. It outperforms Li?s Hie-
rarchical classifier, even they use semi-automatic 
constructed features.  
 
6.3.1 Analysis and Discussion  
Even the sequence tagging model achieves high 
accuracy performance, there still exists many 
problems. We use the matrix defined in Li and 
Roth (2002) to show the performance errors. The 
metric is defined as follows: 
*2 /( )ij i j i jD Err N N= +   
Where i jErr  is the number of questions in class 
i that are misclassified as belong to class j, Ni 
and Nj are the numbers of questions in class i and 
j respectively.  
From the matrix in Figure 3, we can see two 
major mistake pairs are ?ENTY:substance? and 
?ENTY:other?, ?ENTY:currency? and 
?NUM:money?. They really have similar mean-
ings, which confuses even human beings.  
 
 
Figure 3. The gray-scale map of Matrix D[n,n]. The 
gray scale of the small box in position [i,j] denotes 
D[i,j]. The larger Dij is, the darker the color is. 
 
Several factors influence the performance: 
(a) Head noun extraction error: This error is 
mainly caused by errors of POS tagger and shal-
low parser. For the wrong POS example 
?what/WP hemisphere/EX is/VBZ the/DT Phil-
ippines/NNPS in/IN ?/.?, ?Philippines? is ex-
tracted as head word. The result is misclassified 
into ?LOC:country?. For the shallow parser error 
example ?what/WP/B-NP is/VBZ/B-VP the/D 
T/BNP speed/NN/I-NP humminbirds/NNS /I-NP 
fly/V- BP/B-VP ?/./O?, ?hummingbirds? is ex-
tract as head word, rather than ?speed?. The 
question is misclassified into ?ENTY:animal?. 
(b) WordNet sense disambiguation errors: In 
question ?What is the highest dam in the U.S. ?? 
The real sense for dam is dam#1: a barrier con-
structed to contain the flow of water or to keep 
out the sea; while the disambiguation method 
determine the second sense as dam#2: a metric 
unit of length equal to ten meters.  
(c) Lack of head nouns: the CRFs based ap-
proach is sensitive to the Head Noun. If the ques-
tion doesn?t contain the head noun, it is difficult 
to produce the correct result, such as the question 
?What is done with worn or outdated flags?? In 
the future work, we will focus on the head noun 
absence problem. 
487
7 Conclusion 
In this paper, we propose a novel approach with 
Conditional Random Fields to classify what-type 
questions. We first use the CRFs model to label 
all the words in a question, and then choose the 
label of head noun as the question category.  As 
far as we know, this is the first trial to formulate 
question classification into word sequence tag-
ging problem. We believe that the model has two 
distinguished advantages: 
1. Extracting head noun can eliminate the noise 
generated by the non-head words 
2. The Conditional Random Fields model can 
integrate rich features, including not only the 
syntactic and semantic features, but also the 
transition features between labels.  
Experiments show that the LDCRFs-based ap-
proach can achieve comparable performance to 
those of the state-of-the-art question answering 
systems. With the addition of more features, the 
performance of the LDCRFs based approach can 
be expected to be further improved. 
Acknowledgement 
This work is supported by National Natural 
Science Foundation of China (60572084, 
60621062), Hi-tech Research and Development 
Program of China (2006AA02Z321), National 
Basic Research Program of China 
(2007CB311003).  Thank Shuang Lin and Jiao 
Li for revising this paper. Thanks for the review-
ers? comments. 
 References 
Christiane Fellbaum. 1998. WordNet: an Electronic 
Lexical Database. MIT Press. 
Prager, J., D. Radev, E. Brown, A. Coden, and V. 
Samn. 1999. `The use of predictive annotation for 
question answering in TREC'. In: Proceedings of 
the 8th Text Retrieval Conference (TREC-8). 
John Lafferty, Andrew McCallum, Fernando Pereira. 
2001. Conditional Random Fields: Probabilistic 
Models for Segmenting and Labeling Sequence Da-
ta. In Proceedings of ICML-2001. 
Li, X. and D. Roth. 2002. Learning question classifi-
ers. In Proceedings of the 19th International Confe-
rence on Compuatational Linguistics (COLING), 
pages 556?562. 
Zhang, D. and W. Lee. 2003. Question classification 
using support vector machines. In Proceedings of 
the 26th Annual International ACM SIGIR confe-
rence, pages 26?32 
Donald Metzler and W. Bruce Croft. 2004. Analysis 
of Statistical Question Classfication for Fact-based 
Questions. In Journal of Information Retrieval. 
Ted Pedersen, Satanjeev Banerjee, and Siddharth 
Patwardhan . 2005. Maximizing Semantic Related-
ness to Perform Word Sense Disambiguation. Uni-
versity of Minnesota Supercomputing Institute        
Research Report UMSI 2005/25, March. 
Xin Li, Dan Roth. 2006. Learning Question Classifi-
ers: The Role of Semantic Information. In Natural 
Language Engineering, 12(3):229-249 
Minh Le Nguyen, Thanh Tri Nguyen and Akira Shi-
mazu. 2007. Subtree Mining for Question Classifi-
cation Problem. In Proceedings of the 20th Interna-
tional Conference on Artificial Intelligence. Pages 
1695-1700. 
C. Sutton and A. McCallum. 2007. An introduction to 
conditional random fields for relational learning. 
In L. Getoor and B. Taskar (Eds.). Introduction to 
statistical relational learning. MIT Press. 
Y. Tsuruoka and J. Tsujii,. 2005. Bidirectional infe-
rence with the easiest-first strategy for tagging se-
quence data. In Proc. HLT/EMNLP?05, Vancouver, 
October, pp. 467-474. 
L. Ramshaw and M. Marcus. 1995. Text chunking 
using transformation-based learning, Proc. 3rd 
Workshop on Very Large Corpora, pp. 82?94. 
J.R. Finkel, T. Grenager and C. Manning. 2005. In-
corporating non-local information into information 
extraction systems by Gibbs sampling. Proc. 43rd 
Annual Meeting of ACL, pp. 363?370. 
D. Lin. 1999. MINIPAR: a minimalist parser. In Mar-
yland Linguistics Colloquium, University of Mary-
land, College Park. 
 
 
 
488
Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 737?745,
Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLP
Answering Opinion Questions with Random Walks on Graphs
Fangtao Li, Yang Tang, Minlie Huang, and Xiaoyan Zhu
State Key Laboratory on Intelligent Technology and Systems
Tsinghua National Laboratory for Information Science and Technology
Department of Computer Sci. and Tech., Tsinghua University, Beijing 100084, China
{fangtao06,tangyang9}@gmail.com,{aihuang,zxy-dcs}@tsinghua.edu.cn
Abstract
Opinion Question Answering (Opinion
QA), which aims to find the authors? sen-
timental opinions on a specific target, is
more challenging than traditional fact-
based question answering problems. To
extract the opinion oriented answers, we
need to consider both topic relevance and
opinion sentiment issues. Current solu-
tions to this problem are mostly ad-hoc
combinations of question topic informa-
tion and opinion information. In this pa-
per, we propose an Opinion PageRank
model and an Opinion HITS model to fully
explore the information from different re-
lations among questions and answers, an-
swers and answers, and topics and opin-
ions. By fully exploiting these relations,
the experiment results show that our pro-
posed algorithms outperform several state
of the art baselines on benchmark data set.
A gain of over 10% in F scores is achieved
as compared to many other systems.
1 Introduction
Question Answering (QA), which aims to pro-
vide answers to human-generated questions auto-
matically, is an important research area in natu-
ral language processing (NLP) and much progress
has been made on this topic in previous years.
However, the objective of most state-of-the-art QA
systems is to find answers to factual questions,
such as ?What is the longest river in the United
States?? and ?Who is Andrew Carnegie?? In fact,
rather than factual information, people would also
like to know about others? opinions, thoughts and
feelings toward some specific objects, people and
events. Some examples of these questions are:
?How is Bush?s decision not to ratify the Kyoto
Protocol looked upon by Japan and other US al-
lies??(Stoyanov et al, 2005) and ?Why do peo-
ple like Subway Sandwiches?? from TAC 2008
(Dang, 2008). Systems designed to deal with such
questions are called opinion QA systems. Re-
searchers (Stoyanov et al, 2005) have found that
opinion questions have very different character-
istics when compared with fact-based questions:
opinion questions are often much longer, more
likely to represent partial answers rather than com-
plete answers and vary much more widely. These
features make opinion QA a harder problem to
tackle than fact-based QA. Also as shown in (Stoy-
anov et al, 2005), directly applying previous sys-
tems designed for fact-based QA onto opinion QA
tasks would not achieve good performances.
Similar to other complex QA tasks (Chen et al,
2006; Cui et al, 2007), the problem of opinion QA
can be viewed as a sentence ranking problem. The
Opinion QA task needs to consider not only the
topic relevance of a sentence (to identify whether
this sentence matches the topic of the question)
but also the sentiment of a sentence (to identify
the opinion polarity of a sentence). Current solu-
tions to opinion QA tasks are generally in ad hoc
styles: the topic score and the opinion score are
usually separately calculated and then combined
via a linear combination (Varma et al, 2008) or
just filter out the candidate without matching the
question sentiment (Stoyanov et al, 2005). How-
ever, topic and opinion are not independent in re-
ality. The opinion words are closely associated
with their contexts. Another problem is that exist-
ing algorithms compute the score for each answer
candidate individually, in other words, they do not
consider the relations between answer candidates.
The quality of a answer candidate is not only de-
termined by the relevance to the question, but also
by other candidates. For example, the good an-
swer may be mentioned by many candidates.
In this paper, we propose two models to ad-
dress the above limitations of previous sentence
737
ranking models. We incorporate both the topic
relevance information and the opinion sentiment
information into our sentence ranking procedure.
Meanwhile, our sentence ranking models could
naturally consider the relationships between dif-
ferent answer candidates. More specifically, our
first model, called Opinion PageRank, incorpo-
rates opinion sentiment information into the graph
model as a condition. The second model, called
Opinion HITS model, considers the sentences as
authorities and both question topic information
and opinion sentiment information as hubs. The
experiment results on the TAC QA data set demon-
strate the effectiveness of the proposed Random
Walk based methods. Our proposed method per-
forms better than the best method in the TAC 2008
competition.
The rest of this paper is organized as follows:
Section 2 introduces some related works. We will
discuss our proposed models in Section 3. In Sec-
tion 4, we present an overview of our opinion QA
system. The experiment results are shown in Sec-
tion 5. Finally, Section 6 concludes this paper and
provides possible directions for future work.
2 Related Work
Few previous studies have been done on opin-
ion QA. To our best knowledge, (Stoyanov et
al., 2005) first created an opinion QA corpus
OpQA. They find that opinion QA is a more chal-
lenging task than factual question answering, and
they point out that traditional fact-based QA ap-
proaches may have difficulty on opinion QA tasks
if unchanged. (Somasundaran et al, 2007) argues
that making finer grained distinction of subjective
types (sentiment and arguing) further improves the
QA system. For non-English opinion QA, (Ku et
al., 2007) creates a Chinese opinion QA corpus.
They classify opinion questions into six types and
construct three components to retrieve opinion an-
swers. Relevant answers are further processed by
focus detection, opinion scope identification and
polarity detection. Some works on opinion min-
ing are motivated by opinion question answering.
(Yu and Hatzivassiloglou, 2003) discusses a nec-
essary component for an opinion question answer-
ing system: separating opinions from fact at both
the document and sentence level. (Soo-Min and
Hovy, 2005) addresses another important compo-
nent of opinion question answering: finding opin-
ion holders.
More recently, TAC 2008 QA track (evolved
from TREC) focuses on finding answers to opin-
ion questions (Dang, 2008). Opinion questions
retrieve sentences or passages as answers which
are relevant for both question topic and question
sentiment. Most TAC participants employ a strat-
egy of calculating two types of scores for answer
candidates, which are the topic score measure and
the opinion score measure (the opinion informa-
tion expressed in the answer candidate). How-
ever, most approaches simply combined these two
scores by a weighted sum, or removed candidates
that didn?t match the polarity of questions, in order
to extract the opinion answers.
Algorithms based on Markov Random Walk
have been proposed to solve different kinds of
ranking problems, most of which are inspired by
the PageRank algorithm (Page et al, 1998) and the
HITS algorithm (Kleinberg, 1999). These two al-
gorithms were initially applied to the task of Web
search and some of their variants have been proved
successful in a number of applications, including
fact-based QA and text summarization (Erkan and
Radev, 2004; Mihalcea and Tarau, 2004; Otter-
bacher et al, 2005; Wan and Yang, 2008). Gener-
ally, such models would first construct a directed
or undirected graph to represent the relationship
between sentences and then certain graph-based
ranking methods are applied on the graph to com-
pute the ranking score for each sentence. Sen-
tences with high scores are then added into the
answer set or the summary. However, to the best
of our knowledge, all previous Markov Random
Walk-based sentence ranking models only make
use of topic relevance information, i.e. whether
this sentence is relevant to the fact we are looking
for, thus they are limited to fact-based QA tasks.
To solve the opinion QA problems, we need to
consider both topic and sentiment in a non-trivial
manner.
3 Our Models for Opinion Sentence
Ranking
In this section, we formulate the opinion question
answering problem as a topic and sentiment based
sentence ranking task. In order to naturally inte-
grate the topic and opinion information into the
graph based sentence ranking framework, we pro-
pose two random walk based models for solving
the problem, i.e. an Opinion PageRank model and
an Opinion HITS model.
738
3.1 Opinion PageRank Model
In order to rank sentence for opinion question an-
swering, two aspects should be taken into account.
First, the answer candidate is relevant to the ques-
tion topic; second, the answer candidate is suitable
for question sentiment.
Considering Question Topic: We first intro-
duce how to incorporate the question topic into
the Markov Random Walk model, which is simi-
lar as the Topic-sensitive LexRank (Otterbacher et
al., 2005). Given the set Vs = {vi} containing all
the sentences to be ranked, we construct a graph
where each node represents a sentence and each
edge weight between sentence vi and sentence vj
is induced from sentence similarity measure as fol-
lows: p(i ? j) = f(i?j)
P|Vs|
k=1 f(i?k)
, where f(i ? j)
represents the similarity between sentence vi and
sentence vj , here is cosine similarity (Baeza-Yates
and Ribeiro-Neto, 1999). We define f(i ? i) = 0
to avoid self transition. Note that p(i ? j) is usu-
ally not equal to p(j ? i). We also compute the
similarity rel(vi|q) of a sentence vi to the question
topic q using the cosine measure. This relevance
score is then normalized as follows to make the
sum of all relevance values of the sentences equal
to 1: rel?(vi|q) = rel(vi|q)P|Vs|
k=1 rel(vk|q)
.
The saliency score Score(vi) for sentence vi
can be calculated by mixing topic relevance score
and scores of all other sentences linked with it as
follows: Score(vi) = ?
?
j 6=i Score(vj) ? p(j ?
i)+(1??)rel?(vi|q), where ? is the damping fac-
tor as in the PageRank algorithm.
The matrix form is: p? = ?M?T p? + (1 ?
?)~?, where p? = [Score(vi)]|Vs|?1 is the vec-
tor of saliency scores for the sentences; M? =
[p(i ? j)]|Vs|?|Vs| is the graph with each entry
corresponding to the transition probability; ~? =
[rel?(vi|q)]|Vs|?1 is the vector containing the rel-
evance scores of all the sentences to the ques-
tion. The above process can be considered as a
Markov chain by taking the sentences as the states
and the corresponding transition matrix is given by
A? = ?M?T + (1 ? ?)~e~?T .
Considering Topics and Sentiments To-
gether: In order to incorporate the opinion infor-
mation and topic information for opinion sentence
ranking in an unified framework, we propose an
Opinion PageRank model (Figure 1) based on a
two-layer link graph (Liu and Ma, 2005; Wan and
Yang, 2008). In our opinion PageRank model, the
Figure 1: Opinion PageRank
first layer contains all the sentiment words from a
lexicon to represent the opinion information, and
the second layer denotes the sentence relationship
in the topic sensitive Markov Random Walk model
discussed above. The dashed lines between these
two layers indicate the conditional influence be-
tween the opinion information and the sentences
to be ranked.
Formally, the new representation for the two-
layer graph is denoted as G? = ?Vs, Vo, Ess, Eso?,
where Vs = {vi} is the set of sentences and Vo =
{oj} is the set of sentiment words representing the
opinion information; Ess = {eij |vi, vj ? Vs}
corresponds to all links between sentences and
Eso = {eij |vi ? Vs, oj ? Vo} corresponds to
the opinion correlation between a sentence and
the sentiment words. For further discussions, we
let ?(oj) ? [0, 1] denote the sentiment strength
of word oj , and let ?(vi, oj) ? [0, 1] denote the
strength of the correlation between sentence vi and
word oj . We incorporate the two factors into the
transition probability from vi to vj and the new
transition probability p(i ? j|Op(vi),Op(vj)) is
defined as f(i?j|Op(vi),Op(vj ))
P|Vs|
k=1 f(i?k|Op(vi),Op(vk))
when
? f 6=
0, and defined as 0 otherwise, where Op(vi) is de-
noted as the opinion information of sentence vi,
and f(i ? j|Op(vi),Op(vj)) is the new similar-
ity score between two sentences vi and vj , condi-
tioned on the opinion information expressed by the
sentiment words they contain. We propose to com-
pute the conditional similarity score by linearly
combining the scores conditioned on the source
opinion (i.e. f(i ? j|Op(vi))) and the destina-
tion opinion (i.e. f(i ? j|Op(vj))) as follows:
f(i ? j|Op(vi),Op(vj))
= ? ? f(i ? j|Op(vi)) + (1? ?) ? f(i ? j|Op(vj))
= ? ?
X
ok?Op(vi)
f(i ? j) ? pi(ok) ? ?(ok, vi)
+ (1? ?) ?
X
o
k?
?Op(vj))
(i ? j) ? pi(ok? ) ? ?(ok? , vj) (1)
where ? ? [0, 1] is the combination weight con-
trolling the relative contributions from the source
739
opinion and the destination opinion. In this study,
for simplicity, we define ?(oj) as 1, if oj ex-
ists in the sentiment lexicon, otherwise 0. And
?(vi, oj) is described as an indicative function. In
other words, if word oj appears in the sentence vi,
?(vi, oj) is equal to 1. Otherwise, its value is 0.
Then the new row-normalized matrix M?? is de-
fined as follows: M??ij = p(i ? j|Op(i),Opj).
The final sentence score for Opinion PageR-
ank model is then denoted by: Score(vi) = ? ?
?
j 6=i Score(vj) ? M??ji + (1 ? ?) ? rel?(si|q).
The matrix form is: p? = ?M??T p? + (1 ? ?) ? ~?.
The final transition matrix is then denoted as:
A? = ?M??T +(1??)~e~?T and the sentence scores
are obtained by the principle eigenvector of the
new transition matrix A?.
3.2 Opinion HITS Model
The word?s sentiment score is fixed in Opinion
PageRank. This may encounter problem when
the sentiment score definition is not suitable for
the specific question. We propose another opin-
ion sentence ranking model based on the popular
graph ranking algorithm HITS (Kleinberg, 1999).
This model can dynamically learn the word senti-
ment score towards a specific question. HITS al-
gorithm distinguishes the hubs and authorities in
the objects. A hub object has links to many au-
thorities, and an authority object has high-quality
content and there are many hubs linking to it. The
hub scores and authority scores are computed in a
recursive way. Our proposed opinion HITS algo-
rithm contains three layers. The upper level con-
tains all the sentiment words from a lexicon, which
represent their opinion information. The lower
level contains all the words, which represent their
topic information. The middle level contains all
the opinion sentences to be ranked. We consider
both the opinion layer and topic layer as hubs and
the sentences as authorities. Figure 2 gives the bi-
partite graph representation, where the upper opin-
ion layer is merged with lower topic layer together
as the hubs, and the middle sentence layer is con-
sidered as the authority.
Formally, the representation for the bipartite
graph is denoted as G# = ?Vs, Vo, Vt, Eso, Est?,
where Vs = {vi} is the set of sentences. Vo =
{oj} is the set of all the sentiment words repre-
senting opinion information, Vt = {tj} is the set
of all the words representing topic information.
Eso = {eij |vi ? Vs, oj ? Vo} corresponds to the
Figure 2: Opinion HITS model
correlations between sentence and opinion words.
Each edge eij is associated with a weight owij de-
noting the strength of the relationship between the
sentence vi and the opinion word oj . The weight
owij is 1 if the sentence vi contains word oj , other-
wise 0. Est denotes the relationship between sen-
tence and topic word. Its weight twij is calculated
by tf ? idf (Otterbacher et al, 2005).
We define two matrixes O = (Oij)|Vs|?|Vo| and
T = (Tij)|Vs|?|Vt| as follows, for Oij = owij ,
and if sentence i contains word j, therefore owij
is assigned 1, otherwise owij is 0. Tij = twij =
tfj ? idfj (Otterbacher et al, 2005).
Our new opinion HITS model is different from
the basic HITS algorithm in two aspects. First,
we consider the topic relevance when computing
the sentence authority score based on the topic hub
level as follows: Authsen(vi) ?
?
twij>0 twij ?
topic score(j)?hubtopic(j), where topic score(j)
is empirically defined as 1, if the word j is in the
topic set (we will discuss in next section), and 0.1
otherwise.
Second, in our opinion HITS model, there are
two aspects to boost the sentence authority score:
we simultaneously consider both topic informa-
tion and opinion information as hubs.
The final scores for authority sentence, hub
topic and hub opinion in our opinion HITS model
are defined as:
Auth(n+1)sen (vi) = (2)
? ?
X
twij>0
twij ? topic score(j) ?Hub(n)topic(tj)
+ (1? ?) ?
X
owij>0
owij ?Hub(n)opinion(oj)
Hub(n+1)topic (ti) =
X
twki>0
twki ?Auth(n)sen(vi) (3)
Hub(n+1)opinion(oi) =
X
owki>0
owki ?Auth(n)sen(vi) (4)
740
Figure 3: Opinion Question Answering System
The matrix form is:
a(n+1) = ? ? T ? e ? tTs ? I ? h(n)t + (1 ? ?) ? O ? h(n)o (5)
h(n+1)t = T
T ? a(n) (6)
h(n+1)o = O
T ? a(n) (7)
where e is a |Vt|?1 vector with all elements equal
to 1 and I is a |Vt| ? |Vt| identity matrix, ts =
[topic score(j)]|Vt|?1 is the score vector for topic
words, a(n) = [Auth(n)sen(vi)]|Vs|?1 is the vector
authority scores for the sentence in the nth itera-
tion, and the same as h(n)t = [Hub
(n)
topic(tj)]|Vt|?1,
h(n)o = [Hub(n)opinion(tj)]|Vo|?1. In order to guaran-
tee the convergence of the iterative form, authority
score and hub score are normalized after each iter-
ation.
For computation of the final scores, the ini-
tial scores of all nodes, including sentences, topic
words and opinion words, are set to 1 and the
above iterative steps are used to compute the new
scores until convergence. Usually the convergence
of the iteration algorithm is achieved when the dif-
ference between the scores computed at two suc-
cessive iterations for any nodes falls below a given
threshold (10e-6 in this study). We use the au-
thority scores as the saliency scores in the Opin-
ion HITS model. The sentences are then ranked
by their saliency scores.
4 System Description
In this section, we introduce the opinion question
answering system based on the proposed graph
methods. Figure 3 shows five main modules:
Question Analysis: It mainly includes two
components. 1).Sentiment Classification: We
classify all opinion questions into two categories:
positive type or negative type. We extract several
types of features, including a set of pattern fea-
tures, and then design a classifier to identify sen-
timent polarity for each question (similar as (Yu
and Hatzivassiloglou, 2003)). 2).Topic Set Expan-
sion: The opinion question asks opinions about
a particular target. Semantic role labeling based
(Carreras and Marquez, 2005) and rule based tech-
niques can be employed to extract this target as
topic word. We also expand the topic word with
several external knowledge bases: Since all the en-
tity synonyms are redirected into the same page in
Wikipedia (Rodrigo et al, 2007), we collect these
redirection synonym words to expand topic set.
We also collect some related lists as topic words.
For example, given question ?What reasons did
people give for liking Ed Norton?s movies??, we
collect all the Norton?s movies from IMDB as this
question?s topic words.
Document Retrieval: The PRISE search en-
gine, supported by NIST (Dang, 2008), is em-
ployed to retrieve the documents with topic word.
Answer Candidate Extraction: We split re-
trieved documents into sentences, and extract sen-
tences containing topic words. In order to im-
prove recall, we carry out the following process to
handle the problem of coreference resolution: We
classify the topic word into four categories: male,
female, group and other. Several pronouns are de-
fined for each category, such as ?he?, ?him?, ?his?
for male category. If a sentence is determined to
contain the topic word, and its next sentence con-
tains the corresponding pronouns, then the next
sentence is also extracted as an answer candidate,
similar as (Chen et al, 2006).
Answer Ranking: The answer candidates
are ranked by our proposed Opinion PageRank
method or Opinion HITS method.
Answer Selection by Removing Redundancy:
We incrementally add the top ranked sentence into
the answer set, if its cosine similarity with ev-
ery extracted answer doesn?t exceed a predefined
threshold, until the number of selected sentence
(here is 40) is reached.
5 Experiments
5.1 Experiment Step
5.1.1 Dataset
We employ the dataset from the TAC 2008 QA
track. The task contains a total of 87 squishy
741
opinion questions.1 These questions have simple
forms, and can be easily divided into positive type
or negative type, for example ?Why do people like
Mythbusters?? and ?What were the specific ac-
tions or reasons given for a negative attitude to-
wards Mahmoud Ahmadinejad??. The initial topic
word for each question (called target in TAC) is
also provided. Since our work in this paper fo-
cuses on sentence ranking for opinion QA, these
characteristics of TAC data make it easy to pro-
cess question analysis. Answers for all questions
must be retrieved from the TREC Blog06 collec-
tion (Craig Macdonald and Iadh Ounis, 2006).
The collection is a large sample of the blog sphere,
crawled over an eleven-week period from Decem-
ber 6, 2005 until February 21, 2006. We retrieve
the top 50 documents for each question.
5.1.2 Evaluation Metrics
We adopt the evaluation metrics used in the TAC
squishy opinion QA task (Dang, 2008). The TAC
assessors create a list of acceptable information
nuggets for each question. Each nugget will be
assigned a normalized weight based on the num-
ber of assessors who judged it to be vital. We use
these nuggets and corresponding weights to assess
our approach. Three human assessors complete
the evaluation process. Every question is scored
using nugget recall (NR) and an approximation to
nugget precision (NP) based on length. The final
score will be calculated using F measure with TAC
official value ? = 3 (Dang, 2008). This means re-
call is 3 times as important as precision:
F (? = 3) =
(32 + 1) ?NP ?NR
32 ?NP + NR
where NP is the sum of weights of nuggets re-
turned in response over the total sum of weights
of all nuggets in nugget list, and NP = 1 ?
(length ? allowance)/(length) if length is no
less than allowance and 0 otherwise. Here
allowance = 100 ? (?nuggets returned) and
length equals to the number of non-white char-
acters in strings. We will use average F Score to
evaluate the performance for each system.
5.1.3 Baseline
The baseline combines the topic score and opinion
score with a linear weight for each answer candi-
date, similar to the previous ad-hoc algorithms:
final score = (1 ? ?) ? opinion score + ?? topic score
(8)
13 questions were dropped from the evaluation due to no
correct answers found in the corpus
The topic score is computed by the cosine sim-
ilarity between question topic words and answer
candidate. The opinion score is calculated using
the number of opinion words normalized by the
total number of words in candidate sentence.
5.2 Performance Evaluation
5.2.1 Performance on Sentimental Lexicons
Lexicon Neg Pos Description
Name Size Size
1 HowNet 2700 2009 English translation
of positive/negative
Chinese words
2 Senti- 4800 2290 Words with a positive
WordNet or negative score
above 0.6
3 Intersec- 640 518 Words appeared in
tion both 1 and 2
4 Union 6860 3781 Words appeared in
1 or 2
5 All 10228 10228 All words appeared
in 1 or 2 without
distinguishing pos
or neg
Table 1: Sentiment lexicon description
For lexicon-based opinion analysis, the selec-
tion of opinion thesaurus plays an important role
in the final performance. HowNet2 is a knowledge
database of the Chinese language, and provides an
online word list with tags of positive and negative
polarity. We use the English translation of those
sentiment words as the sentimental lexicon. Sen-
tiWordNet (Esuli and Sebastiani, 2006) is another
popular lexical resource for opinion mining. Ta-
ble 1 shows the detail information of our used sen-
timent lexicons. In our models, the positive opin-
ion words are used only for positive questions, and
negative opinion words just for negative questions.
We initially set parameter ? in Opinion PageRank
as 0 as (Liu and Ma, 2005), and other parameters
simply as 0.5, including ? in Opinion PageRank,
? in Opinion HITS, and ? in baseline. The exper-
iment results are shown in Figure 4.
We can make three conclusions from Figure 4:
1. Opinion PageRank and Opinion HITS are both
effective. The best results of Opinion PageRank
and Opinion HITS respectively achieve around
35.4% (0.199 vs 0.145), and 34.7% (0.195 vs
0.145) improvements in terms of F score over the
best baseline result. We believe this is because our
proposed models not only incorporate the topic in-
formation and opinion information, but also con-
2http://www.keenage.com/zhiwang/e zhiwang.html
742
0 15
0.2
0.25
HowNet SentiWordNet Intersection Union All
0
0.05
0.1
.
Baseline Opinion PageRank Opinion HITS
Figure 4: Sentiment Lexicon Performance
sider the relationship between different answers.
The experiment results demonstrate the effective-
ness of these relations. 2. Opinion PageRank and
Opinion HITS are comparable. Among five sen-
timental lexicons, Opinion PageRank achieves the
best results when using HowNet and Union lexi-
cons, and Opinion HITS achieves the best results
using the other three lexicons. This may be be-
cause when the sentiment lexicon is defined appro-
priately for the specific question set, the opinion
PageRank model performs better. While when the
sentiment lexicon is not suitable for these ques-
tions, the opinion HITS model may dynamically
learn a temporal sentiment lexicon and can yield
a satisfied performance. 3. Hownet achieves the
best overall performance among five sentiment
lexicons. In HowNet, English translations of the
Chinese sentiment words are annotated by non-
native speakers; hence most of them are common
and popular terms, which maybe more suitable for
the Blog environment (Zhang and Ye, 2008). We
will use HowNet as the sentiment thesaurus in the
following experiments.
In baseline, the parameter ? shows the relative
contributions for topic score and opinion score.
We vary ? from 0 to 1 with an interval of 0.1, and
find that the best baseline result 0.170 is achieved
when ?=0.1. This is because the topic informa-
tion has been considered during candidate extrac-
tion, the system considering more opinion infor-
mation (lower ?) achieves better. We will use this
best result as baseline score in following experi-
ments. Since F(3) score is more related with re-
call, F score and recall will be demonstrated. In
the next two sections, we will present the perfor-
mances of the parameters in each model. For sim-
plicity, we denote Opinion PageRank as PR, Opin-
ion HITS as HITS, baseline as Base, Recall as r, F
score as F.
0.22
0.24
0.26
PR_r PR_F Base_r Base_F
F(3)
0.12
0.14
0.16
0.18
0.2
0 0.2 0.4 0.6 0.8 1 
Figure 5: Opinion PageRank Performance with
varying parameter ? (? = 0.5)
0.22
0.24
0.26
PR_r PR_F Base_r Base_F
F(3)
0.12
0.14
0.16
0.18
0.2
0 0.2 0.4 0.6 0.8 1
 
Figure 6: Opinion PageRank Performance with
varying parameter ? (? = 0.2)
5.2.2 Opinion PageRank Performance
In Opinion PageRank model, the value ? com-
bines the source opinion and the destination opin-
ion. Figure 5 shows the experiment results on pa-
rameter ?. When we consider lower ?, the system
performs better. This demonstrates that the desti-
nation opinion score contributes more than source
opinion score in this task.
The value of ? is a trade-off between answer
reinforcement relation and topic relation to calcu-
late the scores of each node. For lower value of ?,
we give more importance to the relevance to the
question than the similarity with other sentences.
The experiment results are shown in Figure 6. The
best result is achieved when ? = 0.8. This fig-
ure also shows the importance of reinforcement
between answer candidates. If we don?t consider
the sentence similarity(? = 0), the performance
drops significantly.
5.2.3 Opinion HITS Performance
The parameter ? combines the opinion hub score
and topic hub score in the Opinion HITS model.
The higher ? is, the more contribution is given
743
0.22
0.24
0.26
HITS_r HITS_F Base_r Base_FF(3)
0.12
0.14
0.16
0.18
0.2
0 0.2 0.4 0.6 0.8 1 
Figure 7: Opinion HITS Performance with vary-
ing parameter ?
to topic hub level, while the less contribution is
given to opinion hub level. The experiment results
are shown in Figure 7. Similar to baseline param-
eter ?, since the answer candidates are extracted
based on topic information, the systems consider-
ing opinion information heavily (?=0.1 in base-
line, ?=0.2) perform best.
Opinion HITS model ranks the sentences by au-
thority scores. It can also rank the popular opin-
ion words and popular topic words from the topic
hub layer and opinion hub layer, towards a specific
question. Take the question 1024.3 ?What reasons
do people give for liking Zillow?? as an example,
its topic word is ?Zillow?, and its sentiment polar-
ity is positive. Based on the final hub scores, the
top 10 topic words and opinion words are shown
as Table 2.
Opinion real, like, accurate, rich, right, interesting,
Words better, easily, free, good
Topic zillow, estate, home, house, data, value,
Words site, information, market, worth
Table 2: Question-specific popular topic words
and opinion words generated by Opinion HITS
Zillow is a real estate site for users to see the
value of houses or homes. People like it because it
is easily used, accurate and sometimes free. From
the Table 2, we can see that the top topic words
are the most related with question topic, and the
top opinion words are question-specific sentiment
words, such as ?accurate?, ?easily?, ?free?, not
just general opinion words, like ?great?, ?excel-
lent? and ?good?.
5.2.4 Comparisons with TAC Systems
We are also interested in the performance compar-
ison with the systems in TAC QA 2008. From Ta-
ble 3, we can see Opinion PageRank and Opinion
System Precision Recall F(3)
OpPageRank 0.109 0.242 0.200
OpHITS 0.102 0.256 0.205
System 1 0.079 0.235 0.186
System 2 0.053 0.262 0.173
System 3 0.109 0.216 0.172
Table 3: Comparison results with TAC 2008 Three
Top Ranked Systems (system 1-3 demonstrate top
3 systems in TAC)
HITS respectively achieve around 10% improve-
ment compared with the best result in TAC 2008,
which demonstrates that our algorithm is indeed
performing much better than the state-of-the-art
opinion QA methods.
6 Conclusion and Future Works
In this paper, we proposed two graph based sen-
tence ranking methods for opinion question an-
swering. Our models, called Opinion PageRank
and Opinion HITS, could naturally incorporate
topic relevance information and the opinion senti-
ment information. Furthermore, the relationships
between different answer candidates can be con-
sidered. We demonstrate the usefulness of these
relations through our experiments. The experi-
ment results also show that our proposed methods
outperform TAC 2008 QA Task top ranked sys-
tems by about 10% in terms of F score.
Our random walk based graph methods inte-
grate topic information and sentiment information
in a unified framework. They are not limited to
the sentence ranking for opinion question answer-
ing. They can be used in general opinion docu-
ment search. Moreover, these models can be more
generalized to the ranking task with two types of
influencing factors.
Acknowledgments: Special thanks to Derek
Hao Hu and Qiang Yang for their valuable
comments and great help on paper prepara-
tion. We also thank Hongning Wang, Min
Zhang, Xiaojun Wan and the anonymous re-
viewers for their useful comments, and thank
Hoa Trang Dang for providing the TAC eval-
uation results. The work was supported by
973 project in China(2007CB311003), NSFC
project(60803075), Microsoft joint project ?Opin-
ion Summarization toward Opinion Search?, and
a grant from the International Development Re-
search Center, Canada.
744
References
Ricardo Baeza-Yates and Berthier Ribeiro-Neto. 1999.
Modern Information Retrieval. Addison Wesley,
May.
Xavier Carreras and Lluis Marquez. 2005. Introduc-
tion to the conll-2005 shared task: Semantic role la-
beling.
Yi Chen, Ming Zhou, and Shilong Wang. 2006.
Reranking answers for definitional qa using lan-
guage modeling. In ACL-CoLing, pages 1081?1088.
Hang Cui, Min-Yen Kan, and Tat-Seng Chua. 2007.
Soft pattern matching models for definitional ques-
tion answering. ACM Trans. Inf. Syst., 25(2):8.
Hoa Trang Dang. 2008. Overview of the tac
2008 opinion question answering and summariza-
tion tasks (draft). In TAC.
Gu?nes Erkan and Dragomir R. Radev. 2004. Lex-
pagerank: Prestige in multi-document text summa-
rization. In EMNLP.
Andrea Esuli and Fabrizio Sebastiani. 2006. Senti-
wordnet: A publicly available lexical resource for
opinion mining. In LREC.
Jon M. Kleinberg. 1999. Authoritative sources in a
hyperlinked environment. J. ACM, 46(5):604?632.
Lun-Wei Ku, Yu-Ting Liang, and Hsin-Hsi Chen.
2007. Question analysis and answer passage re-
trieval for opinion question answering systems. In
ROCLING.
Tie-Yan Liu and Wei-Ying Ma. 2005. Webpage im-
portance analysis using conditional markov random
walk. In Web Intelligence, pages 515?521.
Rada Mihalcea and Paul Tarau. 2004. Textrank:
Bringing order into texts. In EMNLP.
Jahna Otterbacher, Gu?nes Erkan, and Dragomir R.
Radev. 2005. Using random walks for question-
focused sentence retrieval. In HLT/EMNLP.
Lawrence Page, Sergey Brin, Rajeev Motwani, and
Terry Winograd. 1998. The pagerank citation rank-
ing: Bringing order to the web. Technical report,
Stanford University.
Swapna Somasundaran, Theresa Wilson, Janyce
Wiebe, and Veselin Stoyanov. 2007. Qa with at-
titude: Exploiting opinion type analysis for improv-
ing question answering in online discussions and the
news. In ICWSM.
Kim Soo-Min and Eduard Hovy. 2005. Identifying
opinion holders for question answering in opinion
texts. In AAAI 2005 Workshop.
Veselin Stoyanov, Claire Cardie, and Janyce Wiebe.
2005. Multi-perspective question answering using
the opqa corpus. In HLT/EMNLP.
Vasudeva Varma, Prasad Pingali, Rahul Katragadda,
and et al 2008. Iiit hyderabad at tac 2008. In Text
Analysis Conference.
X. Wan and J Yang. 2008. Multi-document summa-
rization using cluster-based link analysis. In SIGIR,
pages 299?306.
Hong Yu and Vasileios Hatzivassiloglou. 2003. To-
wards answering opinion questions: Separating facts
from opinions and identifying the polarity of opinion
sentences. In EMNLP.
Min Zhang and Xingyao Ye. 2008. A generation
model to unify topic relevance and lexicon-based
sentiment for opinion retrieval. In SIGIR, pages
411?418.
745
Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 653?661,
Beijing, August 2010
Structure-Aware Review Mining and Summarization
Fangtao Li1, Chao Han1, Minlie Huang1, Xiaoyan Zhu1,
Ying-Ju Xia2, Shu Zhang2 and Hao Yu2
1State Key Laboratory of Intelligent Technology and Systems?
1Tsinghua National Laboratory for Information Science and Technology?
1Department of Computer Science and Technology, Tsinghua University
2Fujitsu Research and Development Center
fangtao06@gmail.com; zxy_dcs@tsinghua.edu.cn
Abstract
In this paper, we focus on object feature 1
1 Introduction
based review summarization. Different from 
most of previous work with linguistic rules or 
statistical methods, we formulate the review
mining task as a joint structure tagging prob-
lem. We propose a new machine learning 
framework based on Conditional Random 
Fields (CRFs). It can employ rich features to 
jointly extract positive opinions, negative opi-
nions and object features for review sentences.
The linguistic structure can be naturally inte-
grated into model representation. Besides li-
near-chain structure, we also investigate con-
junction structure and syntactic tree structure
in this framework. Through extensive experi-
ments on movie review and product review 
data sets, we show that structure-aware mod-
els outperform many state-of-the-art ap-
proaches to review mining.
With the rapid expansion of e-commerce, people 
are more likely to express their opinions and 
hands-on experiences on products or services
they have purchased. These reviews are impor-
tant for both business organizations and personal 
costumers. Companies can decide on their strat-
egies for marketing and products improvement. 
Customers can make a better decision when pur-
1 Note that there are two meanings for word ?feature?. 
We use ?object feature? to represent the target entity,
which the opinion expressed on, and use ?feature? as
the input for machine learning methods.
chasing products or services. Unfortunately, 
reading through all customer reviews is difficult, 
especially for popular items, the number of re-
views can be up to hundreds or even thousands. 
Therefore, it is necessary to provide coherent 
and concise summaries for these reviews.
Figure 1. Feature based Review Summarization
Inspired by previous work (Hu and Liu, 2004; 
Jin and Ho, 2009), we aim to provide object fea-
ture based review summarization. Figure 1 
shows a summary example for movie ?Gone 
with the wind?. The object (movie) features, 
such as ?movie?, ?actor?, with their correspond-
ing positive opinions and negative opinions, are 
listed in a structured way. The opinions are 
ranked by their frequencies. This provides a con-
cise view for reviews. To accomplish this goal, 
we need to do three tasks:  1), extract all the ob-
ject features and opinions; 2), determine the sen-
timent polarities for opinions; 3), for each object 
feature, determine the relevant opinions, i.e. ob-
ject feature-opinion pairs.
For the first two tasks, most previous studies
employ linguistic rules or statistical methods (Hu 
and Liu, 2004; Popescu and Etzioni 2005). They 
mainly use unsupervised learning methods,
which lack an effective way to address infre-
quent object features and opinions. They are also
hard to incorporate rich overlapping features.
Gone With The Wind:
Movie:
     Positive: great, good, amazing, ? , breathtaking
     Negative: bad, boring, waste time, ? , mistake
Actor: 
     Positive: charming , brilliant , great, ? , smart 
     Negative: poor, fail, dirty, ? , lame
Music:
     Positive: great, beautiful, very good, ? , top
     Negative: annoying, noise, too long, ? , unnecessary 
    ? ?
653
Actually, there are many useful features, which 
have not been fully exploited for review mining.
Meanwhile, most of previous methods extract 
object features, opinions, and determine the po-
larities for opinions separately. In fact, the object 
features, positive opinions and negative opinions
correlate with each other. 
In this paper, we formulate the first two tasks,
i.e. object feature, opinion extraction and opi-
nion polarity detection, as a joint structure tag-
ging problem, and propose a new machine learn-
ing framework based on Conditional Random 
Fields (CRFs). For each sentence in reviews, we 
employ CRFs to jointly extract object features,
positive opinions and negative opinions, which 
appear in the review sentence. This framework
can naturally encode the linguistic structure. Be-
sides the neighbor context with linear-chain 
CRFs, we propose to use Skip-chain CRFs and 
Tree CRFs to utilize the conjunction structure
and syntactic tree structure. We also propose a
new unified model, Skip-Tree CRFs to integrate 
these structures. Here, ?structure-aware? refers 
to the output structure, which model the relation-
ship among output labels. This is significantly 
different from the previous input structure me-
thods, which consider the linguistic structure as 
heuristic rules (Ding and Liu, 2007) or input fea-
tures for classification (Wilson et al 2009). Our 
proposed framework has the following advan-
tages: First, it can employ rich features for re-
view mining. We will analyze the effect of fea-
tures for review mining in this framework.
Second, the framework can utilize the relation-
ship among object features, positive opinions 
and negative opinions. It jointly extracts these 
three types of expressions in a unified way.
Third, the linguistic structure information can be 
naturally integrated into model representation,
which provides more semantic dependency for 
output labels. Through extensive experiments on 
movie review and product review, we show our 
proposed framework is effective for review min-
ing.
The rest of this paper is organized as follows: 
In Section 2, we review related work. We de-
scribe our structure aware review mining me-
thods in Section 3. Section 4 demonstrates the 
process of summary generation. In Section 5, we 
present and discuss the experiment results. Sec-
tion 6 is the conclusion and future work.
2 Related Work
Object feature based review summary has been 
studied in several papers. Zhuang et al (2006) 
summarized movie reviews by extracting object 
feature keywords and opinion keywords. Object 
feature-opinion pairs were identified by using a 
dependency grammar graph. However, it used a
manually annotated list of keywords to recognize 
movie features and opinions, and thus the system 
capability is limited. Hu and Liu (2004) pro-
posed a statistical approach to capture object 
features using association rules. They only con-
sidered adjective as opinions, and the polarities 
of opinions are recognized with WordNet expan-
sion to manually selected opinion seeds. Popescu 
and Etzioni (2005) proposed a relaxation labe-
ling approach to utilize linguistic rules for opi-
nion polarity detection. However, most of these 
studies focus on unsupervised methods, which
are hard to integrate various features. Some stu-
dies (Breck et al 2007; Wilson et al 2009; Ko-
bayashi et al 2007) have used classification 
based methods to integrate various features. But 
these methods separately extract object features
and opinions, which ignore the correlation 
among output labels, i.e. object features and opi-
nions. Qiu et al (2009) exploit the relations of 
opinions and object features by adding some lin-
guistic rules. However, they didn?t care the opi-
nion polarity. Our framework can not only em-
ploy various features, but also exploit the corre-
lations among the three types of expressions, i.e.
object features, positive opinions, and negative 
opinions, in a unified framework. Recently, Jin 
and Ho (2009) propose to use Lexicalized HMM
for review mining. Lexicalized HMM is a va-
riant of HMM. It is a generative model, which is 
hard to integrate rich, overlapping features. It 
may encounter sparse data problem, especially 
when simultaneously integrating multiple fea-
tures. Our framework is based on Conditional 
Random Fields (CRFs). CRFs is a discriminative 
model, which can easily integrate various fea-
tures.
These are some studies on opinion mining with 
Conditional Random Fields. For example, with 
CRFs, Zhao et al(2008) and McDonald et al 
(2007) performed sentiment classification in sen-
tence and document level; Breck et al(2007) 
identified opinion expressions from newswire 
documents; Choi et al (2005) determined opi-
654
nion holders to opinions also from newswire da-
ta. None of previous work focuses on jointly ex-
tracting object features, positive opinions and 
negative opinions simultaneously from review 
data. More importantly, we also show how to 
encode the linguistic structure, such as conjunc-
tion structure and syntactic tree structure, into 
model representation in our framework. This is 
significantly different from most of previous 
studies, which consider the structure information 
as heuristic rules (Hu and Liu, 2004) or input 
features (Wilson et al 2009).
Recently, there are some studies on joint sen-
timent/topic extraction (Mei et al 2007; Titov 
and McDonald, 2008; Snyder and Barzilay, 
2007). These methods represent reviews as sev-
eral coarse-grained topics, which can be consi-
dered as clusters of object features. They are
hard to indentify the low-frequency object fea-
tures and opinions. While in this paper, we will 
extract all the present object features and corres-
ponding opinions with their polarities. Besides, 
the joint sentiment/topic methods are mainly
based on review document for topic extraction.
In our framework, we focus on sentence-level
review extraction.
3 Structure Aware Review Mining
3.1 Problem Definition
To produce review summaries, we need to first 
finish two tasks: identifying object features, opi-
nions, and determining the polarities for opi-
nions. In this paper, we formulate these two 
tasks as a joint structure tagging problem. We
first describe some related definitions:
Definition (Object Feature): is defined as whole 
target expression that the subjective expressions 
have been commented on. Object features can be 
products, services or their elements and proper-
ties, such as ?character?, ?movie?, ?director? for 
movie review, and ?battery?, ?battery life?,
?memory card? for product review.
Definition (Review Opinion): is defined as the 
whole subjective expression on object features.
For example, in sentence ?The camera is easy to 
use?, ?easy to use? is a review opinion. ?opinion? 
is used for short.
Definition (Opinion Polarity): is defined as the 
sentiment category for review opinion. In this 
paper, we consider two types of polarities: posi-
tive opinion and negative opinion. For example,
?easy to use? belongs to positive opinion.
For our review mining task, we need to 
represent three types of expressions: object fea-
tures, positive opinions, and negative opinions. 
These expressions may be words, or whole
phrases. We use BIO encoding for tag represen-
tation, where the non-opinion and neutral opi-
nion words are represented as ?O?. With Nega-
tion (N), which is only one word, such as ?not?,
?don?t?, as an independent tag, there are totally 8 
tags, as shown in Table 1. The following is an 
example to denote the tags:
The/O camera/FB comes/O with/O a/O piti-
ful/CB 32mb/FB compact/FI flash/FI card/FI ./O
FB Feature Beginning CB Negative Beginning
FI Feature Inside CI Negative Inside
PB Positive Beginning N Negation Word 
PI Positive Inside O Other 
Table 1. Basic Tag Set for Review Mining
3.2 Structure Aware Model
In this section, we describe how to encode dif-
ferent linguistic structure into model representa-
tion based on our CRFs framework.
3.2.1 Using Linear CRFs.
For each sentence in a review, our task is to ex-
tract all the object features, positive opinions and 
negative opinions. This task can be modeled as a 
classification problem. Traditional classification 
tools, e.g. Maximum Entropy model (Berger et 
al, 1996), can be employed, where each word or 
phrase will be treated as an instance. However, 
they independently consider each word or 
phrase, and ignore the dependency relationship 
among them.
Actually, the context information plays an im-
portant role for review mining. For example, 
given two continuous words with same part of 
speech, if the previous word is a positive opi-
nion, the next word is more likely a positive opi-
nion. Another example is that if the previous 
word is an adjective, and it is an opinion, the 
next noun word is more likely an object feature.
To this end, we formulate the review mining 
task as a joint structure tagging problem, and 
propose a general framework based on Condi-
tional Random Fields (CRFs) (Lafferty et al, 
2001) which are able to model the dependencies 
655
y1 yn-1y3y2 yn
x1 xn-1x3x2 xn
(a) Linear-chain  CRFs
y4
x1 xn-1x3x2 xnxn-2
?
x4
y1 yn-2y3 yn
y2 yn-1
(c) Tree-CRFs
y4
x1 xn-1x3x2 xnxn-2
?
x4
y1 yn-2y3 yn
y2 yn-1
(d) Skip-Tree CRFs
(b) Skip-chain  CRFs
Figure 2 CRFs models
between nodes. (See Section 3.2.5 for more 
about CRFs)
In this section, we propose to use linear-chain
CRFs to model the sequential dependencies be-
tween continuous words, as discussed above. It 
views each word in the sentence as a node, and 
adjacent nodes are connected by an edge. The 
graphical representation is shown in Figure 2(a).
Linear CRFs can make use of dependency rela-
tionship among adjacent words.
3.2.2 Leveraging Conjunction Structure
We observe that the conjunctions play important 
roles on review mining: If the words or phrases 
are connected by conjunction ?and?, they mostly 
belong to the same opinion polarity. If the words 
or phrases are connected by conjunction ?but?, 
they mostly belong to different opinion polarity,
as reported in (Hatzivassiloglou and McKeown,
1997; Ding and Liu, 2007). For example, ?This
phone has a very cool and useful feature ? the
speakerphone?, if we only detect ?cool?, it is 
hard to determine its opinion polarity. But if we 
see ?cool? is connected with ?useful? by con-
junction ?and?, we can easily acquire the polari-
ty of ?cool? as positive. This conjunction struc-
ture not only helps to determine the opinions, but 
also helps to recognize object features. For ex-
ample, ?I like the special effects and music in 
this movie?, with word ?music? and conjunction
?and?, we can easily detect that ?special effects? 
as an object feature.
To model the long distance dependency with 
conjunctions, we use Skip-chain CRFs model to 
detect object features and opinions. The graphi-
cal representation of a Skip-chain CRFs, given in 
Figure 2(b), consists of two types of edges: li-
near-edge (

to 

) and skip-edge (

to 

). 
The linear-edge is described as linear CRFs. The 
skip-edge is imported as follows:
We first identify the conjunctions in the re-
view sentence, with a collected conjunction set,
including ?and?, ?but?, ?or?, ?however?, ?al-
though? etc. For each conjunction, we extract its 
connected two text sequences. The nearest two 
words with same part of speech from the two 
text sequences are connected with the skip-edge. 
Here, we just consider the noun, adjective, and 
adverb. For example, in ?good pictures and 
beautiful music?, there are two skip-edges: one 
connects two adjective words ?good? and ?beau-
tiful?; the other connects two nouns ?pictures? 
and ?music?. We also employ the general senti-
ment lexicons, SentiWordNet (Esuli and Sebas-
tiani, 2006), to connect opinions. Two nearest 
opinion words, detected by sentiment lexicon,
from two sequences, will also be connected by 
skip-edge. If the nearest distance exceeds the 
threshold, this skip edge will be discarded. Here,
we consider the threshold as nine.
Skip-chain CRFs improve the performance of 
review mining, because it naturally encodes the 
conjunction structure into model representation 
with skip-edges.
3.2.3 Leveraging Syntactic Tree Structure
Besides the conjunction structure, the syntactic 
tree structure also helps for review mining. The
tree denotes the syntactic relationship among 
words. In a syntactic dependency representation, 
each node is a surface word. For example, the 
corresponding dependency tree (Klein and Man-
ning, 2003) for the sentence, ?I really like this 
long movie?, is shown in Figure 3.
y1 yn-1y3y2 yn
x1 xn-1x3x2 xn
656
like
longthis
really movieI
nsubj dobjadvmod
det amod
Figure 3. Syntactic Dependency Tree Representation
In linear-chain structure and skip-chain structure, 
?like? and ?movie? have no direct edge, but in 
syntactic tree, ?movie? is directly connected 
with ?like?, and their relationship ?dobj? is also 
included, which shows ?movie? is an objective 
of ?like?. It can provide deeper syntactic depen-
dencies for object features, positive opinions and 
negative opinions. Therefore, it is important to 
consider the syntactic structure in the review 
mining task. 
In this section, we propose to use Tree CRFs to
model the syntactic tree structure for review 
mining. The representation of a Tree CRFs is 
shown in Figure 2(c). The syntactic tree structure 
is encoded into our model representation. Each 
node is corresponding to a word in the depen-
dency tree. The edge is corresponding to depen-
dency tree edge. Tree CRFs can make use of de-
pendency relationship in syntactic tree structure
to boost the performance.
3.2.4 Integrating Conjunction Structure and 
Syntactic Tree Structure
Conjunction structure provides the semantic re-
lations correlated with conjunctions. Syntactic 
tree structure provides dependency relation in 
the syntactic tree. They represent different se-
mantic dependencies. It is interesting to consider 
these two dependencies in a unified model. We 
propose Skip-Tree CRFs, to combine these two 
structure information. The graphical representa-
tion of a Skip-Tree CRFs, given in Figure 2(d),
consists of two types of edges: tree edges and 
conjunction skip-edges. We hope to simulta-
neously model the dependency in conjunction 
structure and syntactic tree structure.
We also notice that there is a relationship 
?conj? in syntactic dependency tree. However, 
we find that it only connects two head words for 
a few coordinating conjunction, such as ?and", 
?or", ?but?. Our designed conjunction skip-edge
provides more information for joint structure 
tagging. We analyze more conjunctions to con-
nect not only two head words, but also the words 
with same part of speech. We also connect the 
words with sentiment lexicon. We will show that 
the skip-tree CRFs, which combine the two 
structures, is effective in the experiment section.
3.2.5 Conditional Random Fields
A CRFs is an undirected graphical model G of 
the conditional distribution (	|
). Y are the 
random variables over the labels of the nodes 
that are globally conditioned on X, which are the 
random variables of the observations. The condi-
tional probability is defined as: 
P
(
	 
|


)
=  
1
(
)
    



(, 	|, 
)
,
+   



(, 	|, 
)
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 483?491,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Learning to Link Entities with Knowledge Base
Zhicheng Zheng, Fangtao Li, Minlie Huang, Xiaoyan Zhu
State Key Laboratory of Intelligent Technology and Systems
Tsinghua National Laboratory for Information Science and Technology
Department of Computer Science and Technology, Tsinghua University, Beijing 100084, China
{zhengzc04,fangtao06}@gmail.com, {aihuang,zxy-dcs}@tsinghua.edu.cn
Abstract
This paper address the problem of entity link-
ing. Specifically, given an entity mentioned in
unstructured texts, the task is to link this entity
with an entry stored in the existing knowledge
base. This is an important task for informa-
tion extraction. It can serve as a convenient
gateway to encyclopedic information, and can
greatly improve the web users? experience.
Previous learning based solutions mainly fo-
cus on classification framework. However, it?s
more suitable to consider it as a ranking prob-
lem. In this paper, we propose a learning to
rank algorithm for entity linking. It effectively
utilizes the relationship information among
the candidates when ranking. The experi-
ment results on the TAC 20091 dataset demon-
strate the effectiveness of our proposed frame-
work. The proposed method achieves 18.5%
improvement in terms of accuracy over the
classification models for those entities which
have corresponding entries in the Knowledge
Base. The overall performance of the system
is also better than that of the state-of-the-art
methods.
1 Introduction
The entity linking task is to map a named-entity
mentioned in a text to a corresponding entry stored
in the existing Knowledge Base. The Knowledge
Base can be considered as an encyclopedia for en-
tities. It contains definitional, descriptive or rele-
vant information for each entity. We can acquire the
knowledge of entities by looking up the Knowledge
1http://www.nist.gov/tac/
Base. Wikipedia is an online encyclopedia, and now
it becomes one of the largest repositories of encyclo-
pedic knowledge. In this paper, we use Wikipedia as
our Knowledge Base.
Entity linking can be used to automatically aug-
ment text with links, which serve as a conve-
nient gateway to encyclopedic information, and can
greatly improve user experience. For example, Fig-
ure 1 shows news from BBC.com. When a user is
interested in ?Thierry Henry?, he can acquire more
detailed information by linking ?Thierry Henry? to
the corresponding entry in the Knowledge Base.
Figure 1: Entity linking example
Entity linking is also useful for some information
extraction (IE) applications. We can make use of
information stored in the Knowledge Base to assist
the IE problems. For example, to answer the ques-
tion ?When was the famous basketball player Jor-
dan born??, if the Knowledge Base contains the en-
483
tity of basketball player Michael Jordan and his in-
formation (such as infobox2 in Wikipedia), the cor-
rect answer ?February 17, 1963? can be easily re-
trieved.
Entity linking encounters the problem of entity
ambiguity. One entity may refer to several entries
in the Knowledge Base. For example, the entity
?Michael Jordan? can be linked to the basketball
player or the professor in UC Berkeley.
Previous solutions find that classification based
methods are effective for this task (Milne and Wit-
ten, 2008). These methods consider each candidate
entity independently, and estimate a probability that
the candidate entry corresponds to the target entity.
The candidate with the highest probability was cho-
sen as the target entity. In this way, it?s more like
a ranking problem rather than a classification prob-
lem. Learning to rank methods take into account the
relations between candidates, which is better than
considering them independently. Learning to rank
methods are popular in document information re-
trieval, but there are few studies on information ex-
traction. In this paper, we investigate the application
of learning to rank methods to the entity linking task.
And we compare several machine learning methods
for this task. We investigate the pairwise learning to
rank method, Ranking Perceptron (Shen and Joshi,
2005), and the listwise method, ListNet (Cao et al,
2007). Two classification methods, SVM and Per-
ceptron, are developed as our baselines. In com-
parison, learning to rank methods show significant
improvements over classification methods, and List-
Net achieves the best result. The best overall per-
formance is also achieved with our proposed frame-
work.
This paper is organized as follows. In the next
section we will briefly review the related work. We
present our framework for entity linking in section
3. We then describe in section 4 learning to rank
methods and features for entity linking. A top1 can-
didate validation module will be explained in section
5. Experiment results will be discussed in section 6.
Finally, we conclude the paper and discusses the fu-
ture work in section 7.
2Infoboxes are tables with semi-structured information in
some pages of Wikipedia
2 Related Work
There are a number of studies on named entity dis-
ambiguation, which is quite relevant to entity link-
ing. Bagga and Baldwin (1998) used a Bag of Words
(BOW) model to resolve ambiguities among people.
Mann and Yarowsky (2003) improved the perfor-
mance of personal names disambiguation by adding
biographic features. Fleischman (2004) trained a
Maximum Entropy model with Web Features, Over-
lap Features, and some other features to judge
whether two names refer to the same individual.
Pedersen (2005) developed features to represent the
context of an ambiguous name with the statistically
significant bigrams.
These methods determined to which entity a spe-
cific name refer by measuring the similarity between
the context of the specific name and the context of
the entities. They measured similarity with a BOW
model. Since the BOW model describes the con-
text as a term vector, the similarity is based on co-
occurrences. Although a term can be one word or
one phrase, it can?t capture various semantic rela-
tions. For example, ?Michael Jordan now is the boss
of Charlotte Bobcats? and ?Michael Jordan retired
from NBA?. The BOW model can?t describe the re-
lationship between Charlotte Bobcats and NBA. Ma-
lin and Airoldi (2005) proposed an alternative sim-
ilarity metric based on the probability of walking
from one ambiguous name to another in a random
walk within the social network constructed from all
documents. Minkov (2006) considered extended
similarity metrics for documents and other objects
embedded in graphs, facilitated via a lazy graph
walk, and used it to disambiguate names in email
documents. Bekkerman and McCallum (2005) dis-
ambiguated web appearances of people based on the
link structure of Web pages. These methods tried to
add background knowledge via social networks. So-
cial networks can capture the relatedness between
terms, so the problem of a BOW model can be
solved to some extent. Xianpei and Jun (2009) pro-
posed to use Wikipedia as the background knowl-
edge for disambiguation. By leveraging Wikipedia?s
semantic knowledge like social relatedness between
named entities and associative relatedness between
concepts, they can measure the similarity between
entities more accurately. Cucerzan (2007) and
484
Bunescu (2006) used Wikipedia?s category informa-
tion in the disambiguation process. Using different
background knowledge, researcher may find differ-
ent efficient features for disambiguation.
Hence researchers have proposed so many effi-
cient features for disambiguation. It is important to
integrate these features to improve the system per-
formance. Some researchers combine features by
manual rules or weights. However, it is not conve-
nient to directly use these rules or weights in another
data set. Some researchers also try to use machine
learning methods to combine the features. Milne and
Witten (2008) used typical classifiers such as Naive
Bayes, C4.5 and SVM to combine features. They
trained a two-class classifier to judge whether a can-
didate is a correct target. And then when they try
to do disambiguation for one query, each candidate
will be classified into the two classes: correct tar-
get or incorrect target. Finally the candidate answer
with the highest probability will be selected as the
target if there are more than one candidates classi-
fied as answers. They achieve great performance in
this way with three efficient features. The classifier
based methods can be easily used even the feature
set changed. However, as we proposed in Introduc-
tion, it?s not the best way for such work. We?ll detail
the learning to rank methods in the next section.
3 Framework for Entity Linking
Input%a%query
Output%the%
final answer%
Figure 2: The framework for entity linking
Entity linking is to align a named-entity men-
tioned in a text to a corresponding entry stored in
the existing Knowledge Base. We proposed a frame-
work to solve the ?entity linking? task. As illustrated
in Figure 2, when inputting a query which is an en-
tity mentioned in a text, the system will return the
target entry in Knowledge Base with four modules:
1. Query Processing. First, we try to correct the
spelling errors in the queries by using query
spelling correction supplied by Google. Sec-
ond, we expand the query in three ways: ex-
panding acronym queries from the text where
the entity is located, expanding queries with the
corresponding redirect pages of Wikipedia and
expanding queries by using the anchor text in
the pages from Wikipedia.
2. Candidates Generation. With the queries gen-
erated in the first step, the candidate genera-
tion module retrieves the candidates from the
Knowledge Base. The candidate generation
module also makes use of the disambiguation
pages in Wikipedia. If there is a disambigua-
tion page corresponding to the query, the linked
entities listed in the disambiguation page are
added to the candidate set.
3. Candidates Ranking. In the module, we rank all
the candidates with learning to rank methods.
4. Top1 Candidate Validation. To deal with those
queries without appropriate matching, we fi-
nally add a validation module to judge whether
the top one candidate is the target entry.
The detail information of ranking module and val-
idation module will be introduced in next two sec-
tions.
4 Learning to Rank Candidates
In this section we first introduce the learning to rank
methods, and then describe the features for ranking
methods.
4.1 Learning to rank methods
Learning to rank methods are popular in the area of
document retrieval. There are mainly two types of
learning to rank methods: pairwise and listwise. The
pairwise approach takes as instances object pairs in
a ranking list for a query in learning. In this way,
it transforms the ranking problem to the classifica-
tion problem. Each pair from ranking list is labeled
based on the relative position or with the score of
485
ranking objects. Then a classification model can be
trained on the labeled data and then be used for rank-
ing. The pairwise approach has advantages in that
the existing methodologies on classification can be
applied directly. The listwise approach takes can-
didate lists for a query as instances to train ranking
models. Then it trains a ranking function by min-
imizing a listwise loss function defined on the pre-
dicted list and the ground truth list.
To describe the learning to rank methods, we first
introduce some notations:
? Query set Q = {q(i)?i = 1 : m}.
? Each query q(i) is associated with a list of ob-
jects(in document retrieval, the objects should
be documents), d(i) = {d(i)j ?j = 1 : n(i)}.
? Each object list has a labeled score list y(i) =
{y(i)j ?j = 1 : n(i)} represents the relevance de-
gree between the objects and the query.
? Features vectors x(i)j from each query-object
pair, j = 1 : n(i).
? Ranking function f, for each x(i)j it outputs a
score f(x(i)j ). After the training phase, to rank
the objects, just use the ranking function f to
output the score list of the objects, and rank
them with the score list.
In the paper we will compare two different learn-
ing to rank approaches: Ranking Perceptron for pair-
wise and ListNet for listwise. A detailed introduc-
tion on Ranking Perceptron (Shen and Joshi, 2005)
and ListNet (Cao et al, 2007) is given.
4.1.1 Ranking Perceptron
Ranking Perceptron is a pairwise learning to rank
method. The score function f!(x(i)j ) is defined as
< !, x(i)j >.
For each pair (x(i)j1 , x
(i)
j2 ), f!(x
(i)
j1 ? x
(i)
j2 ) is com-
puted. With a given margin function g(x(i)j1 , x
(i)
j2 ) and
a positive rate  , if f!(x(i)j1 ? x
(i)
j2 ) ? g(x
(i)
j1 , x
(i)
j2 ) ,
an update is performed:
!t+1 = !t + (x(i)j1 ? x
(i)
j2 )g(x
(i)
j1 , x
(i)
j2 )
After iterating enough times, we can use the func-
tion f! to rank candidates.
4.1.2 ListNet
ListNet takes lists of objects as instances in learn-
ing. It uses a probabilistic method to calculate the
listwise loss function.
ListNet transforms into probability distributions
both the scores of the objects assigned by the ora-
cle ranking function and the real score of the objects
given by human.
Let  denote a permutation on the objects. In List-
Net alorithm, the probability of  with given scores
is defined as:
Ps() =
n
?
j=1
exp(s(j))
?n
k=j exp(s(k))
Then the top k probability of Gk(j1, j2, ..., jk) can
be calculated as:
Ps(Gk(j1, j2, ..., jk)) =
k
?
t=1
exp(sjt)
?l
l=t exp(sjl)
The ListNet uses a listwise loss function with
Cross Entropy as metric:
L(y(i), z(i)) = ?
?
?g?Gk
Py(i)(g)log(Pz(i) (g))
Denote as f! the ranking function based on
Neural Network model !. The gradient of
L(y(i), z(i)(f!)) with respect to parameter ! can be
calculated as:
?! = ?L(y
(i), z(i)(f!))
?!
= ?
?
?g?Gk
?Pz(i)(f!)(g)
?!
Py(i)(g)
Pz(i)(f!)(g)
In each iteration, the ! is updated with ? ??!
in a gradient descent way. Here  is the learning
rate.
To train a learning to rank model, the manually
evaluated score list for each query?s candidate list is
required. We assign 1 to the real target entity and 0
to the others.
486
4.2 Features for Ranking
In the section, we will introduce the features used
in the ranking module. For convenience, we define
some symbols first:
? Q represents a query, which contains a named
entity mentioned in a text. CSet represents the
candidate entries in Knowledge Base for the
query Q. C represents a candidate in CSet.
? Q?s nameString represents the name string of
Q. Q?s sourceText represents the source text of
Q. Q?s querySet represents the queries which
are expansions of Q?s nameString.
? C?s title represents the title of corresponding
Wikipedia article of C. C?s titleExpand repre-
sents the union set of the redirect set of C and
the anchor text set of C. C?s article represents
the Wikipedia article of C.
? C?s nameEntitySet represents the set of all
named entities in C?s article labeled by Stan-
ford NER (Finkel et al, 2005). Q?s nameEnti-
tySet represents the set of all named entities in
Q?s sourceText.
? C?s countrySet represents the set of all coun-
tries in C?s article, and we detect the countries
from text via a manual edited country list. Q?s
countrySet represents the set of all countries
in Q?s sourceText. C?s countrySetInTitle rep-
resents the set of countries exist in one of the
string s from C?s titleExpand.
? C?s citySetInTitle represents the set of all cities
exist in one of the string s from C?s titleExpand,
and we detect the cities from text via a manual
edited list of famous cities. Q?s citySet repre-
sents the set of all cities in Q?s sourceText.
? Q?s type represents the type of query Q. It?s la-
beled by Stanford NER. C?s type is manually
labeled already in the Knowledge Base.
The features that used in the ranking module can
be divided into 3 groups: Surface, Context and Spe-
cial. Each of these feature groups will be detailed
next.
4.2.1 Surface Features
The features in Surface group are used to measure
the similarity between the query string and candidate
entity?s name string.
? StrSimSurface. The feature value is the max-
imum similarity between the Q?s nameString
and each string s in the set C?s titleExpand. The
string similarity is measured with edit distance.
? ExactEqualSurface. The feature value is 1 if
there is a string s in set C?s titleExpand same as
the Q?s nameString, or the Candidate C is ex-
tracted from the disambiguation page. In other
case, the feature value is set to 0.
? StartWithQuery. The feature value is 1 if there
is a string s in set C?s titleExpand starting with
the Q?s nameString, and C?s ExactEqualSur-
face is not 1. In other case, the feature value
is set to 0.
? EndWithQuery. The feature value is 1 if there
is a string s in set C?s titleExpand ending with
the Q?s nameString, and C?s ExactEqualSur-
face is not 1. In other case, the feature value
is set to 0.
? StartInQuery. The feature value is 1 if there is a
string s in set C?s titleExpand that s is the prefix
of the Q?s nameString, and C?s ExactEqualSur-
face is not 1. In other case, the feature value is
set to 0.
? EndInQuery. The feature value is 1 if there is a
string s in set C?s titleExpand that s is the post-
fix of the Q?s nameString, and C?s ExactEqual-
Surface is not 1. In other case, the feature value
is set to 0.
? EqualWordNumSurface. The feature value is
the maximum number of same words between
the Q?s nameString and each string s in the set
C?s titleExpand.
? MissWordNumSurface. The feature value is
the minimum number of different words be-
tween the Q?s nameString and each string s in
the set C?s titleExpand.
487
4.2.2 Context Features
The features in Context group are used to measure
the context relevance between query and the candi-
date entity. We mainly consider the TF-IDF similar-
ity and named entity co-occurrence.
? TFSimContext. The feature value is the TF-
IDF similarity between the C?s article and Q?s
sourceText.
? TFSimRankContext. The feature value is the
inverted rank of C?s TFSimContext in the CSet.
? AllWordsInSource. The feature value is 1 if all
words in C?s title exist in Q?s sourceText, and
in other case, the feature value is set to 0.
? NENumMatch. The feature value is the num-
ber of of same named entities between C?s
nameEntitySet and Q?s nameEntitySet. Two
named entities are judged to be the same if and
only if the two named entities? strings are iden-
tical.
4.2.3 Special Features
Besides the features above, we also find that the
following features are quite significant in the entity
linking task: country names, city names and types
of queries and candidates.
? CountryInTextMatch. The feature value is the
number of same countries between C?s coun-
trySet and Q?s countrySet.
? CountryInTextMiss. The feature value is the
number of countries that exist in Q?s country-
Set but do not exist in C?s countrySet.
? CountryInTitleMatch. The feature value is the
number of same countries between C?s coun-
trySetInTitle and Q?s countrySet.
? CountryInTitleMiss. The feature value is the
number of countries that exist in C?s country-
SetInTitle but do not exist in Q?s countrySet.
? CityInTitleMatch. The feature value is the
number of same cities between C?s citySetInTi-
tle and Q?s citySet.
? TypeMatch. The feature value is 0 if C?s type is
not consistent with Q?s type, in other case, the
feature value is set to 1.
When ranking the candidates in CSet, the fea-
tures? value was normalized into [0, 1] to avoid noise
caused by large Integer value or small double value.
5 Top 1 Candidate Validation
To deal with those queries without target entities in
the Knowledge Base, we supply a Top 1 candidate
validation module. In the module, a two-class classi-
fier is used to judge whether the top one candidate is
the true target entity. The top one candidate selected
from the ranking module can be divided into two
classes: target and non-target, depending on whether
it?s the correct target link of the query. According
to the performance of classification, SVM is chosen
as the classifier (In practice, the libsvm package is
used) and the SVM classifier is trained on the entire
training set.
Most of the features used in the validation mod-
ule are the same as those in ranking module, such as
StrSimSurface, EqualWordNumSurface, MissWord-
NumSurface, TFSimContext, AllWordsInSource,
NENumMatch and TypeMatch. We also design
some other features, as follows:
? AllQueryWordsInWikiText. The feature value
is one if Q?s textRetrievalSet contains C, and
in other case the feature value is set to zero.
The case that Q?s textRetrievalSet contains C
means the candidate C?s article contains the Q?s
nameString.
? CountryInTextPer. The feature is the percent-
age of countries from Q?s countrySet exist in
C?s countrySet too. The feature can be seen as
a normalization of CountryInTextMatch/Miss
features in ranking module.
? ScoreOfRank. The feature value is the score
of the candidate given by the ranking module.
The ScoreOfRank takes many features in rank-
ing module into consideration, so only fewer
features of ranking module are used in the clas-
sifier.
488
6 Experiment and Analysis
6.1 Experiment Setting
Algorithm Accuracy Improvement
over SVM
ListNet 0.9045 +18.5%
Ranking Perceptron 0.8842 +15.8%
SVM 0.7636 -
Perceptron 0.7546 -1.2%
Table 1: Evaluation of different ranking algorithm
Entity linking is initiated as a task in this year?s
TAC-KBP3 track, so we use the data from this track.
The entity linking task in the KBP track is to map
an entity mentioned in a news text to the Knowl-
edge Base, which consist of articles from Wikipedia.
The KBP track gives a sample query set which con-
sists of 416 queries for developing. The test set con-
sists of 3904 queries. 2229 of these queries can?t
be mapped to Knowledge Base, for which the sys-
tems should return NIL links. The remaining 1675
queries all can be aligned to Knowledge Base. We
will firstly analyze the ranking methods with those
non-NIL queries, and then with an additional vali-
dation module, we train and test with all queries in-
cluding NIL queries.
As in the entity linking task of KBP track, the ac-
curacy is taken as
accuracy = #(correct answered queries)#(total queries)
6.2 Evaluation of Machine Learning Methods
in ranking
As mentioned in the section of related work, learn-
ing to rank methods in entity linking performs bet-
ter than the classification methods. To justify this,
some experiments are designed to evaluate the per-
formance of our ranking module when adopting dif-
ferent algorithms.
To evaluate the performance of the ranking mod-
ule, we use all the queries which can be aligned to a
target entry in the Knowledge Base. The training set
contains 285 valid queries and the test set contains
1675.
3http://apl.jhu.edu/ paulmac/kbp.html
Set Features in Set
Set1 Surface Features
Set2 Set1+TF-IDF Features
Set3 Set2+AllWordsInSource
Set4 Set3+NENumMatch
Set5 Set4+CountryInTitle Features
Set6 Set5+CountryInText Features
Set7 Set6+CityInTitleMatch
Set8 Set7+MatchType
Table 2: Feature Sets
Three algorithms are taken into comparison: List-
Net, Ranking Perceptron, and classifier based meth-
ods. The classifier based methods are trained by di-
viding the candidates into two classes: target and
non-target. Then, the candidates are ranked accord-
ing to their probability of being classified as target.
two different classifiers are tested here, SVM and
Perceptron.
!
0.5
0.55
0.6
0.65
0.7
0.75
0.8
0.85
0.9
0.95
Set1 Set2 Set3 Set4 Set5 Set6 Set7 Set8
Ac
cu
ra
cy
Feature)Set
ListNet
Ranking!Perceptron
Figure 3: Comparison of ListNet and Ranking Perceptron
As shown in Table 1, the two learning to rank
methods perform much better than the classification
based methods. The experiment results prove our
point that the learning to rank algorithms are more
suitable in this work. And the ListNet shows slight
improvement over Ranking Perceptron, but since the
improvement is not so significant, maybe it depends
on the feature set. To confirm this, we compare the
two algorithms with different features, as showed
in Table 2. In Figure 3, The ListNet outperforms
Ranking Perceptron with all feature sets except Set1,
which indicates that the listwise approach is more
suitable than the pairwise approach. The pairwise
approach suffers from two problems: first, the ob-
jective of learning is to minimize classification er-
489
Systems Accuracy of all queries Accuracy of non-NIL queries Accuracy of NIL queries
System1 0.8217 0.7654 0.8641
System2 0.8033 0.7725 0.8241
System3 0.7984 0.7063 0.8677
ListNet+SVM 0.8494 0.79 0.8941
Table 3: Evaluation of the overall performance, compared with KBP results (System 1-3 demonstrate the top three
ranked systems)
rors but not to minimize the errors in ranking; sec-
ond, the number of pairs generated from list varies
largely from list to list, which will result in a model
biased toward lists with more objects. The issues are
also discussed in (Y.B. Cao et al, 2006; Cao et al,
2007). And the listwise approach can fix the prob-
lems well.
As the feature sets are added incrementally, it can
be used for analyzing the importance of the features
to the ranking task. Although Surface Group only
takes into consideration the candidate?s title and the
query?s name string, its accuracy is still higher than
60%. This is because many queries have quite small
number of candidates, the target entry can be picked
out with the surface features only. The result shows
that after adding the TF-IDF similarity related fea-
tures, the accuracy increases significantly to 84.5%.
Although TF-IDF similarity is a simple way to mea-
sure the contextual similarity, it performs well in
practice. Another improvement is achieved when
adding the CountryInTitleMatch and CountryInTi-
tleMiss features. Since a number of queries in test
set need to disambiguate candidates with different
countries in their titles, the features about coun-
try in the candidates? title are quite useful to deal
with these queries. But it doesn?t mean that the
features mentioned above are the most important.
Because many features correlated with each other
quite closely, adding these features doesn?t lead to
remarkable improvement. The conclusion from the
results is that the Context Features significantly im-
prove the ranking performance and the Special Fea-
tures are also useful in the entity linking task.
6.3 Overall Performance Evaluation
We are also interested in overall performance with
the additional validation module. We use all the
3904 queries as the test set, including both NIL
and non-NIL queries. The top three results from
the KBP track (McNamee and Dang, 2009) are se-
lected as comparison. The evaluation result in Table
3 shows that our proposed framework outperforms
the best result in the KBP track, which demonstrates
the effectiveness of our methods.
7 Conclusions and Future Work
This paper demonstrates a framework of learning to
rank for linking entities with the Knowledge Base.
Experimenting with different ranking algorithms, it
shows that the learning to rank methods perform
much better than the classification methods in this
problem. ListNet achieves 18.5% improvement over
SVM, and Ranking Perceptron gets 15.8% improve-
ment over SVM. We also observe that the listwise
learning to rank methods are more suitable for this
problem than pairwise methods. We also add a vali-
dation module to deal with those entities which have
no corresponding entry in the Knowledge Base. We
also evaluate the proposed method on the whole data
set given by the KBP track, for which we add a bi-
nary SVM classification module to validate the top
one candidate. The result of experiment shows the
proposed strategy performs better than all the sys-
tems participated in the entity linking task.
In the future, we will try to develop more sophis-
ticated features in entity linking and design a typical
learning to rank method for the entity linking task.
Acknowledgments
This work was partly supported by the Chinese Nat-
ural Science Foundation under grant No.60973104
and No. 60803075, partly carried out with the aid
of a grant from the International Development Re-
search Center, Ottawa, Canada IRCI project from
the International Development.
490
References
Bagga and Baldwin. 1998. Entity-Based Cross-
Document Coreferencing Using the Vector Spcae
Model. in Proceedings of HLT/ACL.
Gideon S. Mann and David Yarowsky. 2003. Unsuper-
vised Personal Name Disambiguation. in Proceedings
of CONIL.
Michael Ben Fleishman. 2004. Multi-Document Person
Name Resolution. in Proceedings of ACL.
Ted Pedersen, Amruta Purandare and Anagha Kulkarni.
2005. Name Discrimination by Clustering Similar
Contexts. in Proceedings of CICLing.
B.Malin and E. Airoldi. 2005. A Network Analysis
Model for Disambiguation of Names in Lists. in Pro-
ceedings of CMOT.
Einat Minkov, William W. Cohen and Andrew Y. Ng.
2006. Contextual Search and Name Disambiguation
in Email Using Graph. in Proceedings of SIGIR.
Ron Bekkerman and Andrew McCallum. 2005. Disam-
biguating Web Appearances of People in a Social Net-
work. in Proceedings of WWW.
Xianpei Han and Jun Zhao. 2009. Named Entity Disam-
biguation by Leveraging Wikipedia Semantic Knowl-
edge. in Proceedings of CIKM.
David Milne and Ian H. Witten. 2008. Learning to Link
with Wikipedia. in Proceedings of CIKM.
Herbrich, R., Graepel, T. and Obermayer K. 1999. Sup-
port vector learning for ordinal regression. in Pro-
ceedings of ICANN.
Freund, Y., Iyer, R., Schapire, R. E. and Singer, Y. 1998.
An efficient boosting algorithm for combining prefer-
ences. in Proceedings of ICML.
Burges, C., Shaked, T., Renshaw, E., Lazier, A., Deeds,
M., Hamilton, N. and Hullender, G. 2005. Learning to
rank using gradient descent. in Proceedings of ICML.
Cao, Y. B., Xu, J., Liu, T. Y., Li, H., Huang, Y. L. and
Hon, H. W. 2006. Adapting ranking SVM to document
retrieval. in Proceedings of SIGIR.
Cao, Z., Qin, T., Liu, T. Y., Tsai, M. F. and Li, H. 2007.
Learning to rank: From pairwise approach to listwise
approach. in Proceedings of ICML.
Qin, T., Zhang, X.-D., Tsai, M.-F., Wang, D.-S., Liu,
T.Y., and Li, H. 2007. Query-level loss functions for
information retrieval. in Proceedings of Information
processing and management.
L. Shen and A. Joshi. 2005. Ranking and Reranking with
Perceptron. Machine Learning,60(1-3),pp. 73-96.
Silviu Cucerzan. 2007. Large-Scale Named Entity Dis-
ambiguation Based on Wikipedia Data. in Proceed-
ings of EMNLP-CoNLL.
Razvan Bunescu and Marius Pasca. 2006. Using En-
cyclopedic Knowledge for Named Entity Disambigua-
tion. in Proceedings of EACL.
Paul McNamee and Hoa Dang. 2009. Overview
of the TAC 2009 Knowledge Base Population Track
(DRAFT). in Proceedings of TAC.
Jenny Rose Finkel, Trond Grenager, and Christopher
Manning. 2005. Incorporating Non-local Information
into Information Extraction Systems by Gibbs Sam-
pling. Proceedings of the 43nd Annual Meeting of
the Association for Computational Linguistics (ACL
2005), pp. 363-370.
491
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 410?419,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
Cross-Domain Co-Extraction of Sentiment and Topic Lexicons
Fangtao Li?, Sinno Jialin Pan?, Ou Jin?, Qiang Yang? and Xiaoyan Zhu?
?Department of Computer Science and Technology, Tsinghua University, Beijing, China
?{fangtao06@gmail.com, zxy-dcs@tsinghua.edu.cn}
?Institute for Infocomm Research, Singapore
?jspan@i2r.a-star.edu.sg
?Hong Kong University of Science and Technology, Hong Kong, China
?{kingomiga@gmail.com, qyang@cse.ust.hk}
Abstract
Extracting sentiment and topic lexicons is im-
portant for opinion mining. Previous works
have showed that supervised learning methods
are superior for this task. However, the perfor-
mance of supervised methods highly relies on
manually labeled training data. In this paper,
we propose a domain adaptation framework
for sentiment- and topic- lexicon co-extraction
in a domain of interest where we do not re-
quire any labeled data, but have lots of labeled
data in another related domain. The frame-
work is twofold. In the first step, we gener-
ate a few high-confidence sentiment and topic
seeds in the target domain. In the second
step, we propose a novel Relational Adaptive
bootstraPping (RAP) algorithm to expand the
seeds in the target domain by exploiting the
labeled source domain data and the relation-
ships between topic and sentiment words. Ex-
perimental results show that our domain adap-
tation framework can extract precise lexicons
in the target domain without any annotation.
1 Introduction
In the past few years, opinion mining and senti-
ment analysis have attracted much attention in Natu-
ral Language Processing (NLP) and Information Re-
trieval (IR) (Pang and Lee, 2008; Liu, 2010). Senti-
ment lexicon construction and topic lexicon extrac-
tion are two fundamental subtasks for opinion min-
ing (Qiu et al, 2009). A sentiment lexicon is a list
of sentiment expressions, which are used to indicate
sentiment polarity (e.g., positive or negative). The
sentiment lexicon is domain dependent as users may
use different sentiment words to express their opin-
ion in different domains (e.g., different products). A
topic lexicon is a list of topic expressions, on which
the sentiment words are expressed. Extracting the
topic lexicon from a specific domain is important
because users not only care about the overall senti-
ment polarity of a review but also care about which
aspects are mentioned in review. Note that, similar
to sentiment lexicons, different domains may have
very different topic lexicons.
Recently, Jin and Ho (2009) and Li et al (2010a)
showed that supervised learning methods can
achieve state-of-the-art results for lexicon extrac-
tion. However, the performance of these meth-
ods highly relies on manually annotated training
data. In most cases, the labeling work may be time-
consuming and expensive. It is impossible to anno-
tate each domain of interest to build precise domain-
dependent lexicons. It is more desirable to automat-
ically construct precise lexicons in domains of inter-
est by transferring knowledge from other domains.
In this paper, we focus on the co-extraction task
of sentiment and topic lexicons in a target domain
where we do not have any labeled data, but have
plenty of labeled data in a source domain. Our
goal is to leverage the knowledge extracted from the
source domain to help lexicon co-extraction in the
target domain. To address this problem, we propose
a two-stage domain adaptation method. In the first
step, we build a bridge between the source and tar-
get domains by identifying some common sentiment
words as sentiment seeds in the target domain, such
as ?good?, ?bad?, ?nice?, etc. After that, we gener-
ate topic seeds in the target domain by mining some
general syntactic relation patterns between the sen-
timent and topic words from the source domain. In
the second step, we propose a Relational Adaptive
bootstraPping (RAP) algorithm to expand the seeds
in the target domain. Our proposed method can uti-
410
lize useful labeled data from the source domain as
well as exploit the relationships between the topic
and sentiment words to propagate information for
lexicon construction in the target domain. Experi-
mental results show that our proposed method is ef-
fective for cross-domain lexicon co-extraction.
In summary, we have three main contributions: 1)
We give a systematic study on cross-domain senti-
ment analysis in word level. While, most of previous
work focused on document level; 2) A new two-step
domain adaptation framework, with a novel RAP al-
gorithm for seed expansion, is proposed. 3) We con-
duct extensive evaluation, and the experimental re-
sults demonstrate the effectiveness of our methods.
2 Related Work
2.1 Sentiment or Topic Lexicon Extraction
Sentiment or topic lexicon extraction is to iden-
tify the sentiment or topic words from text. In the
past, many machine learning techniques have been
proposed for this task. Hu and Liu et al (2004)
proposed an association-rule-based method to ex-
tract topic words and a dictionary-based method to
identify sentiment words, independently. Wiebe et
al. (2004) and Rioff et al (2003) proposed to
identify subjective adjectives and nouns using word
clustering based on their distributional similarity.
Popescu and Etzioni (2005) proposed a relaxed la-
beling approach to utilize linguistic rules for opinion
polarity detection. Some researchers also proposed
to use topic modeling to identify implicit topics and
sentiment words (Mei et al, 2007; Titov and Mc-
Donald, 2008; Zhao et al, 2010; Li et al, 2010b),
where a topic is a cluster of words, which is differ-
ent from our fine-grained topic-word extraction.
Jin and Ho (2009) and Li et al (2010a) both pro-
posed to use supervised sequential labeling methods
for topic and opinion extraction. Experimental re-
sults showed that the supervised learning methods
can achieve state-of-the-art performance on lexicon
extraction. However, these methods need to manu-
ally annotate a lot of training data in each domain.
Recently, Qiu et al (2009) proposed a rule-based
semi-supervised learning methods for lexicon ex-
traction. However, their method requires to manu-
ally define some general syntactic rules among sen-
timent and topic words. In addition, it still requires
some annotated words in the target domain. In this
paper, we do not assume any predefined rules and
labeled data be available in the target domain.
2.2 Domain Adaptation
Domain adaptation aims at transferring knowledge
across domains where data distributions may be dif-
ferent (Pan and Yang, 2010). In the past few years,
domain adaptation techniques have been widely ap-
plied to various NLP tasks, such as part-of-speech
tagging (Ando and Zhang, 2005; Jiang and Zhai,
2007; Daume? III, 2007), named-entity recognition
and shallow parsing (Daume? III, 2007; Jiang and
Zhai, 2007; Wu et al, 2009). There are also
lots of studies for cross-domain sentiment analy-
sis (Blitzer et al, 2007; Tan et al, 2007; Li et al,
2009; Pan et al, 2010; Bollegala et al, 2011; He
et al, 2011; Glorot et al, 2011). However, most
of them focused on coarse-grained document-level
sentiment classification, which is different from our
fine-grained word-level extraction. Our work is sim-
ilar to Jakob and Gurevych (2010) which proposed a
Conditional Random Field (CRF) for cross-domain
topic word extraction. However, the performance
of their method highly depends on the manually de-
signed features. In our experiments, we compare our
method with theirs, and find that ours can achieve
much better results on cross-domain lexicon extrac-
tion. Note that our work is also different from a re-
cent work (Du et al, 2010), which focused on identi-
fying the polarity of adjective words by using cross-
domain knowledge. While we extract both topic and
sentiment words and allow non-adjective sentiment
words, which is more practical.
3 Cross-Domain Lexicon Co-Extraction
3.1 Problem Definition
Recall that, we focus on the setting where we have
no labeled data in the target domain, while we have
plenty of labeled data in the source domain. De-
note DS = {(wSi , ySi)}n1i=1 the source domain data,
where wSi represents a word in the source domain.
ySi ? Y is the corresponding label of wSi . Simi-
larly, we denote DT = {wTj}n2j=1 the target domain
data, where the input wTj is a word in the target do-
main. In lexicon extraction, Y ? {1, 2, 3}, where
yi = 1 denotes the corresponding word wi a sen-
timent word, yi = 2 denotes wi a topic word, and
yi = 3 denotes wi neither a sentiment nor topic
word. Our goal is to predict labels on DT to extract
topic and sentiment words for constructing topic and
411
sentiment lexicons, respectively.
3.2 Motivating Examples
In this section, we use some examples to introduce
the motivation behind our proposed method. Table 1
shows several reviews from two domains: movie and
camera. From the table, we can observe that there
are some common sentiment words across different
domains, such as ?great?, ?excellent? and ?amaz-
ing?. However, the topic words may be different.
For example, in the movie domain, topic words in-
clude ?movie? and ?script?. While in the camera do-
main, topic words include ?camera? and ?photos?.
Domain Review
camera
The camera is great.
it is a very amazing product.
i highly recommend this camera.
takes excellent photos.
photos had some artifacts and noise.
movie
This movie has good script, great
casting, excellent acting.
I love this movie.
Godfather was the most amazing movie.
The movie is excellent.
Table 1: Reviews in camera and movie domains. Bold-
faces are topic words and Italics are sentiment words.
Based on the observations, we can build a connec-
tion between the source and target domains by iden-
tifying the common sentiment words. Furthermore,
intuitively, there are some general syntactic relation-
ships or patterns between topic and sentiment words
across different domains. Therefore, if we can mine
the patterns from the source and target domain data,
then we are able to construct an indirect connection
between topic words across domains by using the
common sentiment words as a bridge, which makes
knowledge transfer across domains possible.
Figure 1 shows two dependency trees for the sen-
tence ?the camera is great? in the camera domain
and the sentence ?the movie is excellent? in the
movie domain, respectively. As can be observed, the
relationships between the topic and sentiment words
in the two sentences are the same. They both share
a ?TOPIC-nsubj-SENTIMENT? relation. Let the
camera domain be the source domain and the movie
domain be the target domain. If the word ?excel-
lent? is identified as a common sentiment word, and
the ?TOPIC-nsubj-SENTIMENT? relation extracted
from the camera domain is recognized as a common
syntactic pattern, then the word ?movie? can be pre-
dicted as a topic word in the movie domain with high
probability. After new topic words are extracted in
the movie domain, we can apply the same syntac-
tic pattern or other syntactic patterns to extract new
sentiment and topic words iteratively.
great
camera is
The
nsubj cop
det
(a) Camera domain.
excellent
movie is
The
nsubj cop
det
(b) Movie domain.
Figure 1: Examples of dependency tree structure.
More specifically, we use the shortest path be-
tween a topic word and a sentiment word in the cor-
responding dependency tree to denote the relation
between them. To get more general paths, we do
not take original words in the path into considera-
tion, but use their POS tags instead, such as ?NN?,
?VB?, ?JJ?, etc. As an example shown in Figure 2,
we can extract two paths or relationships between
topic and sentiment words from the dependency tree
of the sentence ?The movie has good script?: ?NN-
amod-JJ? from ?script? and ?good?, and ?NN-nsubj-
VB-dobj-NN-amod-JJ? from ?movie? and ?good?.
has(VB)
script(NN)
the(DT)
movie(NN)
good(JJ)
dobj nsubj
amod det
Figure 2: Example of pattern extraction.
In the following sections, we present the proposed
two-stage domain adaptation framework: 1) gener-
ating some sentiment and topic seeds in the target
domain; and 2) expanding the seeds in the target do-
main to construct sentiment and topic lexicons.
4 Seed Generation
Our basic idea is to first identify several common
sentiment words across domains as sentiment seeds.
Meanwhile, we mine some general patterns between
sentiment and topic words from the source domain.
Finally, we use the sentiment seeds and general pat-
terns to generate topic seeds in the target domain.
412
4.1 Sentiment Seed Generation
To identify common sentiment words across do-
mains, we extract all sentiment words from the
source domain as candidates. For each candidate,
we calculate its score based on the following metric:
S1(wi) = (pS(wi) + pT (wi)) e(?|pS(wi)?pT (wi)|), (1)
where pS(wi) and pT (wi) are the probabilities of the
word wi occurring in the source and target domains,
respectively. If a word wi has high S1 score, which
implies that the word wi occurs frequently and simi-
larly in both domains, then it can be considered as a
common sentiment word (Pan et al, 2010; Blitzer et
al., 2007). We select top r candidates with highest
S1 scores as sentiment seeds.
4.2 Topic Seed Generation
We extract all patterns between sentiment and topic
words in the source domain as candidates. For each
pattern candidate, we calculate its score based on a
metric defined in AutoSlog-TS (Riloff, 1996):
S2(Rj) = Acc(Rj)? log2(Freq(Rj)), (2)
where Acc(Rj) is the accuracy of the pattern Rj in
the source domain, and Freq(Rj) is the frequency
of the pattern Rj observed in target domain. This
metric aims to identify the patterns that are precise
in the source domain and observed frequently in the
target domain. We also select the top r patterns
with highest S2 scores. With the patterns and sen-
timent seeds, we extract topic-word candidates and
measure their scores based on a variant metric of
quadratic combination (Zhang and Ye, 2008):
S3(wk) =
?
Rj?A, wi?B
(S2(Rj)? S1(wi)) , (3)
where B is a set of sentiment seeds and A is a set of
patterns which the words wi and wk satisfy. We then
select the top r candidates as topic seeds.
5 Seed Expansion
After generating the topic and sentiment seeds, we
aim to expand them in the target domain to construct
topic and sentiment lexicons. In this section, we pro-
pose a new bootstrapping-based method to address
this problem.
Bootstrapping is the process of improving the per-
formance of a weak classifier by iteratively adding
training data and retraining the classifier. More
specifically, bootstrapping starts with a small set
of labeled ?seeds?, and iteratively adds unlabeled
data that are labeled by the classifier to the train-
ing set based on some selection criterion, and retrain
the classifier. Many bootstrapping-based algorithms
have been proposed to information extraction and
other NLP tasks (Blum and Mitchell, 1998; Riloff
and Jones, 1999; Jones et al, 1999; Wu et al, 2009).
One important issue in bootstrapping is how to
design a criterion to select unlabeled data to be
added to the training set iteratively. Our proposed
bootstrapping for cross-domain lexicon extraction
is based on the following two observations: 1) Al-
though the source and target domains are different,
part of source domain labeled data is still useful for
lexicon extraction in the target domain after some
adaptation; 2) The syntactic relationships among
sentiment and topic words can be used to expand the
seeds in the target domain for lexicon construction.
Based on the two observations, we propose a
new bootstrapping-based method named Relational
Adaptive bootstraPping (RAP), as summarized in
Algorithm 1, for expanding lexicons across do-
mains. In each iteration, we employ a cross-domain
classifier trained on the source domain lexicons and
the extracted target domain lexicons to predict the
labels of the target unlabeled data, and select top k2
predicted topic and sentiment words as candidates
based on confidence. With the extracted syntactic
patterns in the previous iterations, we construct a
bipartite graph between sentiment and topic words
on the extracted target domain lexicons and candi-
dates. After that, a graph-based score refinement al-
gorithm is performed on the graph, and the top k1
candidates are added to the extracted lexicons based
on the final scores. Accordingly, with the new ex-
tracted lexicons, we update the syntactic patterns in
each iteration. The details of RAP are presented in
the following sections.
5.1 Cross-Domain Classifier
In this paper, we employ Transfer AdaBoost (TrAd-
aBoost) (Dai et al, 2007) as the cross-domain learn-
ing algorithm in RAP. In TrAdaBoost, each word
wSi (or wTj ) is represented by a feature vector xSi
(or xTj ). A classifier trained on the source domain
data DS = {(xSi , ySi)} may perform poor on xTj
because of domain difference. The main idea of
TrAdaBoost is to re-weight the source domain data
based on a few of target domain labeled data, which
is referred to as seeds in our task. The re-weighting
413
aims to reduce the effect of the ?bad? source do-
main data while encourage the ?good? ones to get
a more precise classifier in target domain. In each
iteration of RAP, we train cross-domain classifiers
fTO and fTP for sentiment- and topic- word extrac-
tion using TrAdaBoost separately (taking sentiment
or topic words as positive instances). We use linear
Support Vector Machines (SVMs) as the base clas-
sifier in TrAdaBoost. For features to represent each
word, we use lexicon features, such as the previous,
current and next words, and POS tag features, such
as the previous, current and next words? POS tags.
Algorithm 1 Relational Adaptive bootstraPping
Require: Target domain data DT = DlT
?DuT , where DlT
consists of sentiment seeds B and topic seeds C and their
initial scores S1(wi), ?wi ? B and S3(wj), ?wj ? C, DuT
is the set of unlabeled target domain data; labeled source
domain data DS ; a cross-domain classifier; iteration num-
ber M and candidate selection number k1, k2.
Ensure: Expand C and B in the target domain.
1: Initialize a pattern set A = ?, S?1(wi) = S1(wi), wi ? B
and S?3(wj) = S3(wj), wj ? C. Consider all patterns
observed in the source domain as pattern candidates P .
2: for m = 1 . . .M do
3: Extract new pattern candidates to P with DlT in target
domain, update pattern score S?2(Rj), where Rj ? P ,
based on Eq. (4), and select the top k1 patterns to the
pattern set A.
4: Learn the cross-domain classifiers fTO and fTP for
sentiment- and topic- word extraction with DS
?DlT
separately. Predict the sentiment score hTfO (wTj ) and
topic score hTfP (wTj ) on D
u
T , and select k2 sentiment
words and topic words with highest scores as candidates.
5: Construct a bipartite graph between sentiment and topic
words on DlT and the k2 sentiment- and topic- word can-
didates, and calculate the normalized weights ?ij?s for
each edge of the graph.
6: Refine the scores S?1 and S?3 of the k2 sentiment and
topic word candidates using Eqs. (5) and (6) iteratively.
7: Select k1 new sentiment words and k1 new topic words
with the final scores, and add them to lexicons B and C.
Update S?1(wi) and S?3(wj) accordingly.
8: end for
9: return Expanded lexicons B and C.
5.2 Graph Construction
Based on the cross-domain classifiers fTO and fTP ,
we can predict the sentiment label score hTfO(wTi)
and topic label score hTfP (wTi) for the target domain
data wTi . According to all predicted values, we re-
spectively select top k2 new sentiment- and topic-
words as candidates. Together with the extracted
sentiment and topic lexicons in the target domain,
we build a bipartite graph among them as shown in
Figure 3. In the bipartite graph, one set of nodes
represents topic words, including new topic candi-
dates and words in the lexicon C, and the other set
of nodes represents sentiment words, including new
sentiment candidates and words in the lexicon B.
For a pair of sentiment and topic words wOTi and w
P
Tj ,
if there is a pattern Rj in the pattern set A that they
can satisfy, then there exists an edge eij between
them. Furthermore, each edge eij is associated with
a nonnegative weight ?ij , which is measured as fol-
lows, ?ij =
?
Rk?E S?2(Rk), where S?2 is the pattern
score. Similar to the metric defined in Eq. (3), the
pattern score is defined as:
S?2(Rj) =
?
{wi,wk}?E
(
S?1(wi)? S?3(wk)
)
, (4)
where E = {{wi, wj}|, wi ? B,wj ? C and
wi, wj satisfy Rj , Rj ? A}. Note that in the be-
ginning of each iteration, S?2 is updated based on the
new sentiment score S?1 and topic score S?3. We fur-
ther normalize ?ij by ??ij = ?ij/(
?
ij ?ij).
Topic words Sentiment words
music
movie
recommend
good
boring
script
NN-nsubj-VB-dobj-NN-amod-JJ
NN-amod-JJ
NN-nsubj-JJ
NN-amod-JJ
NN-d
obj-V
B
Figure 3: Topic and sentiment word graph.
5.3 Score Computation
We construct the bipartite graph to exploit the re-
lationships between sentiment and topic words to
propagate information for lexicon extraction. We
use the following reinforcement formulas to itera-
tively update the final sentiment score S?1(wTj ) and
topic score S?3(wTi), respectively:
S?1(wTj ) = ?
?
i
S?3(wTi)??ij + (1? ?)hTfO (wTj ), (5)
S?3(wTi) = ?
?
j
S?1(wTj )??ij + (1? ?)hTfP (wTi), (6)
where ? is a trade-off parameter between the pre-
dicted value by cross-domain classifier and the re-
inforcement scores from other nodes connected by
414
edge eij . Here ? is empirically set to be 0.5. With
Eqs. (5) and (6), the sentiment scores and topic
scores are iteratively refined until the state of the
graph trends to be stable. This can be considered
as an extension to the HITS algorithm(Kleinberg,
1999). Finally, we select k1 ? k2 sentiment and
topic words from the k2 candidates based on their
refined scores, and add them to the target domain
lexicons, respectively. We also update the sentiment
score S?1 and topic score S?3 for next iteration.
5.4 Special Cases
We now introduce two special cases of the RAP al-
gorithm. In Eqs. (5) and (6), if the parameter ? = 1,
then RAP only uses the relationships between sen-
timent and topic words with their patterns to propa-
gate label information in the target domain without
using the cross-domain classifier. We call this reduc-
tion relational bootstrapping. If ? = 0, then RAP
only utilizes useful source domain labeled data to as-
sist learning of the target domain classifier without
considering the relationships between sentiment and
topic words. We call this reduction adaptive boot-
strapping, which can be considered as a bootstrap-
ping version of TrAdaBoost. We also empirically
study these two special cases in experiments.
6 Experiments on Lexicon Evaluation
6.1 Data Set and Evaluation Criteria
We use the review dataset from (Li et al, 2010a),
which contains 500 movie and 601 product reviews,
for evaluation. The sentiment and topic words are
manually annotated. In this dataset, all types of
sentiment words are annotated instead of adjective
words only. For example, the verbs, such as ?like?,
?recommend?, and nouns, such as ?masterpiece?,
are also labeled as sentiment words. We construct
two cross-domain lexicon extraction tasks: ?prod-
uct vs. movie? and ?movie vs. product?, where the
word before ?vs.? corresponds with the source do-
main and the word after ?vs.? corresponds with the
target domain. We evaluate our methods in terms of
precision, recall and F-score (F1).
6.2 Baselines
The results of in-domain classifiers, which are
trained on plenty of target domain labeled data, can
be treated as upper-bounds. We denote iSVM and
iCRF the in-domain SVM and CRF classifiers in
experiments, and compare our proposed methods,
RAP, relational bootstrapping, and adaptive boot-
strapping, with the following baselines,
Unsupervised Method (Un) we implement a rule-
based method for lexicon extraction based on (Hu
and Liu, 2004), where adjective words that match
a rule is recognized as sentiment words, and nouns
that match a rule are recognized as topic words.
Semi-Supervised Method (Semi) we implement
the double propagation model proposed in (Qiu et
al., 2009). Since this method requires some target
domain labeled data, we manually label 30 senti-
ment words in the target domain.
Cross-Domain CRF (Cross-CRF) we implement
a cross-domain CRF algorithm proposed by (Jakob
and Gurevych, 2010).
TrAdaBoost We apply TrAdaBoost (Dai et al,
2007) on the source domain labeled data and the
generated seeds in the target domain to train a lexi-
con extractor.
6.3 Comparison Results
Comparison results on lexicon extraction are shown
in Table 2 and Table 3. From Table 2, we can ob-
serve that our proposed methods are effective for
sentiment lexicon extraction. The relational boot-
strapping method performs better than the unsuper-
vised method, TrAdaBoost and the cross-domain
CRF algorithm, and achieves comparable results
with the semi-supervised method. However, com-
pared to the semi-supervised method, our proposed
relational bootstrapping method does not require any
labeled data in the target domain. We can also ob-
serve that the adaptive bootstrapping method and the
RAP method perform much better than other meth-
ods in terms of F-score. The reason is that part of
the source domain labeled data may be useful for
learning the target classifier after reweighting. In
addition, we also observe that embedding the TrAd-
aBoost algorithm into a bootstrapping process can
further boost the performance of the classifier for
sentiment lexicon extraction.
Table 3 shows the comparison results on topic lex-
icon extraction. From the table, we can observe that
different from the sentiment lexicon extraction task,
the relational bootstrapping method performs better
than the adaptive bootstrapping method slightly. The
reason may be that for the sentiment lexicon extrac-
tion task, there exist some common sentiment words
415
product vs. movie movie vs. product
Prec. Rec. F1 Prec. Rec. F1
Un 0.82 0.31 0.45 0.74 0.23 0.35
Semi 0.71 0.44 0.54 0.62 0.45 0.52
Cross-CRF 0.69 0.40 0.51 0.65 0.34 0.45
Tradaboost 0.73 0.41 0.52 0.72 0.42 0.52
Adaptive 0.68 0.53 0.59 0.63 0.52 0.57
Relational 0.55 0.51 0.53 0.57 0.51 0.54
RAP 0.69 0.59 0.64 0.66 0.59 0.62
iSVM 0.82 0.60 0.70 0.80 0.61 0.68
iCRF 0.80 0.66 0.72 0.80 0.62 0.69
Table 2: Results on sentiment lexicon extraction. Num-
bers in boldface denote significant improvement.
product vs. movie movie vs. product
Prec. Rec. F1 Prec. Rec. F1
Un 0.41 0.32 0.36 0.53 0.35 0.41
Semi 0.54 0.59 0.56 0.75 0.50 0.60
Cross-CRF 0.70 0.23 0.34 0.80 0.24 0.37
Tradaboost 0.64 0.45 0.53 0.57 0.47 0.51
Adaptive 0.76 0.44 0.56 0.70 0.52 0.59
Relational 0.57 0.58 0.58 0.61 0.57 0.59
RAP 0.80 0.56 0.66 0.73 0.58 0.65
iSVM 0.83 0.73 0.78 0.85 0.70 0.77
iCRF 0.84 0.78 0.81 0.87 0.73 0.80
Table 3: Results on topic lexicon extraction. Numbers in
boldface denote significant improvement.
across domains, thus part of the labeled source do-
main data may be useful for the target learning task.
However, for the topic lexicon extraction task, the
topic words may be totally different, and as a result,
we may not be able to find useful source domain
labeled data to boost the performance for lexicon
extraction in the target domain. In this case, mu-
tual label propagation between sentiment and topic
words may be more reasonable for knowledge trans-
fer. RAP absorbs the advantages of the adaptive and
relational bootstrapping methods, thus can get the
best results in both lexicon extraction tasks.
We also observe that relational bootstrapping can
get better recall, but lower precision, compared to
adaptive bootstrapping. This is because relational
bootstrapping only utilizes the patterns to propagate
label information, which may cover more topic and
sentiment seeds, but include some noisy words. For
example, given two phases ?like the camera? and
?recommend the camera?, we can extract a pattern
?VB-dobj-NN?. However, by using this pattern and
the topic word ?camera?, we may extract ?take? as
a sentiment word from another phase ?take the cam-
era?, which is incorrect. The adaptive bootstrapping
method can utilize various features to make predic-
tions more precisely, which may have higher preci-
sion, but encounter the lower recall problem. For ex-
ample, ?flash? is not identified as a topic word in the
target product domain (camera domain). Our RAP
method can exploit both relationships between sen-
timent and topic words and part of labeled source
domain data for cross-domain lexicon extraction. It
can correctly identify the above two cases.
6.3.1 Parameter Sensitivity Study
In this section, we conduct experiments to study
the effect of different parameter settings. There are
several parameters in the framework: the number
of generated seeds r, the number of new candidates
k2 and the number of selections k in each iteration,
and the number of iterations M (? is empirically set
to 0.5 ). For the parameter k2, we just set it to a
large number (k2 = 100) such that have rich candi-
dates to build the bipartite graph. In the experiments
reported in the previous section, we set r = 20,
k1 = 10 and M = 50. Figures 4(a) and 4(b) show
the results under varying values of r in the ?product
vs. movie? task. Observe that for sentiment word
extraction, the results of the proposed methods are
not sensitive to the values of r. While for the topic
word extraction, the proposed methods perform well
when r falls in the range from 15 to 20.
5 10 15 20 25 30
0.45
0.5
0.55
0.6
0.65
0.7
Values of r
F?
sc
or
e
 
 
Relational
Adaptive
RAP
(a) Sentiment word extraction
5 10 15 20 25 30
0.45
0.5
0.55
0.6
0.65
0.7
Values of r
F?
sc
or
e
 
 
Relational
Adaptive
RAP
(b) Topic word extraction
Figure 4: Results on varying values of r.
0 10 20 30 40 50
0.4
0.45
0.5
0.55
0.6
0.65
Number of iterations
F?
sc
or
e
 
 
Relational
Adaptive
RAP
(a) Sentiment word extraction
0 10 20 30 40 50
0.4
0.45
0.5
0.55
0.6
0.65
0.7
Number of iterations
F?
sc
or
e
 
 
Relational
Adaptive
RAP
(b) Topic word extraction
Figure 5: Results on varying values of M .
We also test the sensitivity of the parameter k1
and find that the proposed methods work well and
robust when k1 falls in the range from 10 to 20.
416
Figures 5(a) and 5(b) show the results under vary-
ing numbers of iterations in the ?product vs. movie?
task. As we can see, our proposed methods converge
well when M ? 40.
7 Application: Sentiment Classification
To further verify the usefulness of the lexicons ex-
tracted by the RAP method, we apply the extracted
sentiment lexicon for sentiment classification.
7.1 Experiment Setting
Our work is motivated by the work of (Pang and
Lee, 2004), which only used subjective sentences
for document-level sentiment classification, instead
of using all sentences. In this experiment, we only
use sentiment related words as features to represent
opinion documents for classification, instead of us-
ing all words. Our goal is compare the sentiment
lexicon constructed by the RAP method with other
general lexicons on the impact of for sentiment clas-
sification. The general lexicons used for comparison
are described in Table 4.
We use the dataset from (Blitzer et al, 2007) for
sentiment classification. It contains a collection of
product reviews from Amazon.com. The reviews are
about four product domains: books, dvds, electron-
ics and kitchen appliance. In each domain, there are
1000 positive and 1000 negative reviews. To con-
struct domain specific sentiment lexicons, we apply
RAP on each product domain with the movie domain
described in Section 6.1 as the source domain. Fi-
nally, we use linear SVM as the classifier and the
classification accuracy as the evaluate criterion.
Lexicon Name Size Description
Senti-WordNet 6957 Words with a subjective score > 0.6
(Esuli and Sebastiani, 2006)
HowNet 4619 Eng. translation of subj. Chinese
words (Dong and Dong, 2006)
Subj. Clues 6878 Lexicons from (Wilson et al, 2005)
Table 4: Description of different lexicons.
7.2 Experimental Results
Experimental results on sentiment classification are
shown in Table 5, where we denote ?All? using all
unigram and bigram features instead of using sub-
jective words. As we can see that a classifier trained
with features constructed by our RAP method per-
formance best in all domains. Note that the num-
ber of features (sentiment words) constructed by our
method is much smaller than that of all unigram
and bigram features, which can reduce the classi-
fier training time dramatically. These promising re-
sults imply that our RAP can be applied for senti-
ment classification effectively and efficiently.
All Senti HowNet Subj. Clue Ours
dvd 82.55 79.80 80.57 80.93 84.05
book 80.71 76.22 78.22 79.48 81.65
electronic 84.43 82.42 83.05 83.22 86.71
kitchen 87.70 81.78 84.17 84.23 88.83
Table 5: Sentiment classification results (accuracy in %).
Numbers in boldface denotes significant improvement.
8 Conclusions
In this paper, we propose a two-stage framework for
co-extraction of sentiment and topic lexicons across
domains where we have no labeled data in the tar-
get domain but have plenty of labeled data in an-
other domain. In the first stage, we propose a sim-
ple strategy to generate a few high-quality sentiment
and topic seeds for the target domain. In the second
stage, we propose a novel Relational Adaptive boot-
straPping (RAP) method to expand the seeds, which
can exploit the relationships between topic and opin-
ion words, and make use of part of useful source do-
main labeled data for help. Extensive experimental
results show our proposed method can extract pre-
cise sentiment and topic lexicons from the target do-
main. Furthermore, the extracted sentiment lexicon
can be applied to sentiment classification effectively.
In the future work, besides the heterogeneous
relationships between topic and sentiment words,
we intend to investigate the homogeneous relation-
ships among topic words and those among sentiment
words (Qiu et al, 2009) to further boost the perfor-
mance of RAP method. Furthermore, in our frame-
work, we do not identify the polarity of the extracted
sentiment lexicon. We also plan to embed this com-
ponent into our unified framework. Finally, it is also
interesting to exploit multi-domain knowledge (Li
and Zong, 2008; Bollegala et al, 2011) for cross-
domain lexicon extraction.
9 Acknowledgement
This work was supported by the Chinese Natu-
ral Science Foundation No.60973104, National Key
Basic Research Program 2012CB316301, and Hong
Kong RGC GRF Projects 621010 and 621211.
417
References
Rie K. Ando and Tong Zhang. 2005. A framework for
learning predictive structures from multiple tasks and
unlabeled data. J. Mach. Learn. Res., 6:1817?1853.
John Blitzer, Mark Dredze, and Fernando Pereira. 2007.
Biographies, bollywood, boom-boxes and blenders:
Domain adaptation for sentiment classification. In
Proceedings of the 45th Annual Meeting of the Asso-
ciation of Computational Linguistics, pages 432?439,
Prague, Czech Republic. ACL.
Avrim Blum and Tom Mitchell. 1998. Combining la-
beled and unlabeled data with co-training. In Proceed-
ings of the 11th Annual Conference on Computational
Learning Theory, pages 92?100.
Danushka Bollegala, David Weir, and John Carroll.
2011. Using multiple sources to construct a sentiment
sensitive thesaurus for cross-domain sentiment clas-
sification. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguistics:
Human Language Technologies, pages 132?141, Port-
land, Oregon. ACL.
Wenyuan Dai, Qiang Yang, Guirong Xue, and Yong Yu.
2007. Boosting for transfer learning. In Proceed-
ings of the 24th International Conference on Machine
Learning, pages 193?200, Corvalis, Oregon, USA,
June. ACM.
Hal Daume? III. 2007. Frustratingly easy domain adapta-
tion. In Proceedings of the 45th Annual Meeting of the
Association of Computational Linguistics, pages 256?
263, Prague, Czech Republic. ACL.
Zhendong Dong and Qiang Dong, editors. 2006.
HOWNET and the computation of meaning. World
Scientific Publishers, Norwell, MA, USA.
Weifu Du, Songbo Tan, Xueqi Cheng, and Xiaochun
Yun. 2010. Adapting information bottleneck method
for automatic construction of domain-oriented senti-
ment lexicon. In Proceedings of the 3rd ACM inter-
national conference on Web search and data mining,
pages 111?120, New York, NY, USA. ACM.
Andrea Esuli and Fabrizio Sebastiani. 2006. SENTI-
WORDNET: A publicly available lexical resource for
opinion mining. In In Proceedings of the 5th Confer-
ence on Language Resources and Evaluation, pages
417?422.
Xavier Glorot, Antoine Bordes, and Yoshua Bengio.
2011. Domain adaptation for large-scale sentiment
classification: A deep learning approach. In Pro-
ceedings of the 28th International Conference on Ma-
chine Learning, pages 513?520, Bellevue, Washing-
ton, USA.
Yulan He, Chenghua Lin, and Harith Alani. 2011. Auto-
matically extracting polarity-bearing topics for cross-
domain sentiment classification. In Proceedings of the
49th Annual Meeting of the Association for Compu-
tational Linguistics: Human Language Technologies,
pages 123?131, Portland, Oregon. ACL.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the tenth
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, pages 168?177, Seat-
tle, WA, USA. ACM.
Niklas Jakob and Iryna Gurevych. 2010. Extracting
opinion targets in a single- and cross-domain setting
with conditional random fields. In Proceedings of
the 2010 Conference on Empirical Methods in Natural
Language Processing, pages 1035?1045, Cambridge,
Massachusetts, USA. ACL.
Jing Jiang and ChengXiang Zhai. 2007. Instance weight-
ing for domain adaptation in NLP. In Proceedings of
the 45th Annual Meeting of the Association of Com-
putational Linguistics, pages 264?271, Prague, Czech
Republic. ACL.
Wei Jin and Hung Hay Ho. 2009. A novel lexical-
ized HMM-based learning framework for web opinion
mining. In Proceedings of the 26th Annual Interna-
tional Conference on Machine Learning, pages 465?
472, Montreal, Quebec, Canada. ACM.
Rosie Jones, Andrew Mccallum, Kamal Nigam, and
Ellen Riloff. 1999. Bootstrapping for text learning
tasks. In In IJCAI-99 Workshop on Text Mining: Foun-
dations, Techniques and Applications, pages 52?63.
Jon M. Kleinberg. 1999. Authoritative sources in a hy-
perlinked environment. J. ACM, 46:604?632, Sept.
Shoushan Li and Chengqing Zong. 2008. Multi-domain
sentiment classification. In Proceedings of the 46th
Annual Meeting of the Association for Computational
Linguistics on Human Language Technologies: Short
Papers, pages 257?260, Columbus, Ohio, USA. ACL.
Tao Li, Vikas Sindhwani, Chris Ding, and Yi Zhang.
2009. Knowledge transformation for cross-domain
sentiment classification. In Proceedings of the 32nd
international ACM SIGIR conference on Research and
development in information retrieval, pages 716?717,
Boston, MA, USA. ACM.
Fangtao Li, Chao Han, Minlie Huang, Xiaoyan Zhu,
Ying-Ju Xia, Shu Zhang, and Hao Yu. 2010a.
Structure-aware review mining and summarization. In
Proceedings of the 23rd International Conference on
Computational Linguistics, pages 653?661, Beijing,
China.
Fangtao Li, Minlie Huang, and Xiaoyan Zhu. 2010b.
Sentiment analysis with global topics and local de-
pendency. In Proceedings of the Twenty-Fourth AAAI
Conference on Artificial Intelligence, Atlanta, Geor-
gia, USA. AAAI Press.
418
Bing Liu. 2010. Sentiment analysis and subjectivity.
Handbook of Natural Language Processing, Second
Edition.
Qiaozhu Mei, Xu Ling, Matthew Wondra, Hang Su, and
ChengXiang Zhai. 2007. Topic sentiment mixture:
modeling facets and opinions in weblogs. In Pro-
ceedings of the 16th international conference on World
Wide Web, pages 171?180, Banff, Alberta, Canada.
ACM.
Sinno Jialin Pan and Qiang Yang. 2010. A survey
on transfer learning. IEEE Trans. Knowl. Data Eng.,
22(10):1345?1359, Oct.
Sinno Jialin Pan, Xiaochuan Ni, Jian-Tao Sun, Qiang
Yang, and Chen Zheng. 2010. Cross-domain senti-
ment classification via spectral feature alignment. In
Proceedings of the 19th International Conference on
World Wide Web, pages 751?760, Raleigh, NC, USA,
Apr. ACM.
Bo Pang and Lillian Lee. 2004. A sentimental edu-
cation: sentiment analysis using subjectivity summa-
rization based on minimum cuts. In Proceedings of
the 42nd Annual Meeting on Association for Compu-
tational Linguistics, Barcelona, Spain. ACL.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and Trends in Infor-
mation Retrieval, 2(1-2):1?135.
Ana-Maria Popescu and Oren Etzioni. 2005. Extracting
product features and opinions from reviews. In Pro-
ceedings of Human Language Technology Conference
and Conference on Empirical Methods in Natural Lan-
guage Processing, pages 339?346, Vancouver, British
Columbia, Canada. ACL.
Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen. 2009.
Expanding domain sentiment lexicon through double
propagation. In Proceedings of the 21st international
jont conference on Artifical intelligence, pages 1199?
1204, Pasadena, California, USA. Morgan Kaufmann
Publishers Inc.
Ellen Riloff and Rosie Jones. 1999. Learning dictio-
naries for information extraction by multi-level boot-
strapping. In Proceedings of the 6th national con-
ference on Artificial intelligence, pages 474?479, Or-
lando, Florida, United States. AAAI.
Ellen Riloff, Janyce Wiebe, and Theresa Wilson. 2003.
Learning subjective nouns using extraction pattern
bootstrapping. In Proceedings of the 7th conference
on natural language learning, pages 25?32, Edmon-
ton, Canada. ACL.
Ellen Riloff. 1996. Automatically generating extrac-
tion patterns from untagged text. In Proceedings of
the Thirteenth National Conference on Artificial In-
telligence, pages 1044?1049, Portland, Oregon, USA.
AAAI Press/MIT Press.
Songbo Tan, Gaowei Wu, Huifeng Tang, and Xueqi
Cheng. 2007. A novel scheme for domain-transfer
problem in the context of sentiment analysis. In Pro-
ceedings of the 16th ACM conference on Conference
on information and knowledge management, pages
979?982, Lisbon, Portugal. ACM.
Ivan Titov and Ryan McDonald. 2008. A joint model of
text and aspect ratings for sentiment summarization.
In Proceedings of the 46th Annual Meeting of the As-
sociation of Computational Linguistics: Human Lan-
guage Technologies, pages 308?316, Columbus, Ohio,
USA. ACL.
Janyce Wiebe, Theresa Wilson, Rebecca Bruce, Matthew
Bell, and Melanie Martin. 2004. Learning subjective
language. Comput. Linguist., 30:277?308, Sept.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In Proceedings of the conference
on Human Language Technology and Empirical Meth-
ods in Natural Language Processing, pages 347?354,
Vancouver, British Columbia, Canada. ACL.
Dan Wu, Wee Sun Lee, Nan Ye, and Hai Leong Chieu.
2009. Domain adaptive bootstrapping for named en-
tity recognition. In Proceedings of the 2009 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing, pages 1523?1532, Singapore. ACL.
Min Zhang and Xingyao Ye. 2008. A generation
model to unify topic relevance and lexicon-based sen-
timent for opinion retrieval. In Proceedings of the
31st annual international ACM SIGIR conference on
Research and development in information retrieval,
pages 411?418, Singapore. ACM.
Wayne Xin Zhao, Jing Jiang, Hongfei Yan, and Xiaom-
ing Li. 2010. Jointly modeling aspects and opin-
ions with a MaxEnt-LDA hybrid. In Proceedings of
the 2010 Conference on Empirical Methods in Natural
Language Processing, pages 56?65, Cambridge, Mas-
sachusetts, USA. ACL.
419
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1723?1732,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Deceptive Answer Prediction with User Preference Graph
Fangtao Li?, Yang Gao?, Shuchang Zhou??, Xiance Si?, and Decheng Dai?
?Google Research, Mountain View
?State Key Laboratory of Computer Architecture, Institute of Computing Technology, CAS
{lifangtao,georgezhou,sxc,decheng}@google.com
?Department of Computer Science and Technology, Tsinghua University
gao young@163.com
Abstract
In Community question answering (QA)
sites, malicious users may provide decep-
tive answers to promote their products or
services. It is important to identify and fil-
ter out these deceptive answers. In this
paper, we first solve this problem with
the traditional supervised learning meth-
ods. Two kinds of features, including tex-
tual and contextual features, are investi-
gated for this task. We further propose
to exploit the user relationships to identify
the deceptive answers, based on the hy-
pothesis that similar users will have simi-
lar behaviors to post deceptive or authentic
answers. To measure the user similarity,
we propose a new user preference graph
based on the answer preference expressed
by users, such as ?helpful? voting and
?best answer? selection. The user prefer-
ence graph is incorporated into traditional
supervised learning framework with the
graph regularization technique. The ex-
periment results demonstrate that the user
preference graph can indeed help improve
the performance of deceptive answer pre-
diction.
1 Introduction
Currently, Community QA sites, such as Yahoo!
Answers1 and WikiAnswers2, have become one of
the most important information acquisition meth-
ods. In addition to the general-purpose web search
engines, the Community QA sites have emerged as
popular, and often effective, means of information
seeking on the web. By posting questions for other
participants to answer, users can obtain answers
to their specific questions. The Community QA
1http://answers.yahoo.com
2http://wiki.answers.com
sites are growing rapidly in popularity. Currently
there are hundreds of millions of answers and mil-
lions of questions accumulated on the Community
QA sites. These resources of past questions and
answers are proving to be a valuable knowledge
base. From the Community QA sites, users can di-
rectly get the answers to meet some specific infor-
mation need, rather than browse the list of returned
documents to find the answers. Hence, in recent
years, knowledge mining in Community QA sites
has become a popular topic in the field of artifi-
cial intelligence (Adamic et al, 2008; Wei et al,
2011).
However, some answers may be deceptive. In
the Community QA sites, there are millions of
users each day. As the answers can guide the
user?s behavior, some malicious users are moti-
vated to give deceptive answers to promote their
products or services. For example, if someone
asks for recommendations about restaurants in the
Community QA site, the malicious user may post a
deceptive answer to promote the target restaurant.
Indeed, because of lucrative financial rewards, in
several Community QA sites, some business own-
ers provide incentives for users to post deceptive
answers for product promotion.
There are at least two major problems that the
deceptive answers cause. On the user side, the
deceptive answers are misleading to users. If
the users rely on the deceptive answers, they will
make the wrong decisions. Or even worse, the pro-
moted link may lead to illegitimate products. On
the Community QA side, the deceptive answers
will hurt the health of the Community QA sites. A
Community QA site without control of deceptive
answers could only benefit spammers but could
not help askers at all. If the asker was cheated by
the provided answers, he will not trust and visit
this site again. Therefore, it is a fundamental task
to predict and filter out the deceptive answers.
In this paper, we propose to predict deceptive
1723
answer, which is defined as the answer, whose pur-
pose is not only to answer the question, but also
to promote the authors? self-interest. In the first
step, we consider the deceptive answer prediction
as a general binary-classification task. We extract
two types of features: one is textual features from
answer content, including unigram/bigram, URL,
phone number, email, and answer length; the other
is contextual features from the answer context, in-
cluding the relevance between answer and the cor-
responding question, the author of the answer, an-
swer evaluation from other users and duplication
with other answers. We further investigate the user
relationship for deceptive answer prediction. We
assume that similar users tend to have similar be-
haviors, i.e. posting deceptive answers or post-
ing authentic answers. To measure the user rela-
tionship, we propose a new user preference graph,
which is constructed based on the answer evalu-
ation expressed by users, such as ?helpful? vot-
ing and ?best answer? selection. The user prefer-
ence graph is incorporated into traditional super-
vised learning framework with graph regulariza-
tion, which can make answers, from users with
same preference, tend to have the same category
(deceptive or authentic). The experiment results
demonstrate that the user preference graph can fur-
ther help improve the performance for deceptive
answer prediction.
2 Related Work
In the past few years, it has become a popular task
to mine knowledge from the Community QA sites.
Various studies, including retrieving the accumu-
lated question-answer pairs to find the related an-
swer for a new question, finding the expert in a
specific domain, summarizing single or multiple
answers to provide a concise result, are conducted
in the Community QA sites (Jeon et al, 2005;
Adamic et al, 2008; Liu et al, 2008; Song et
al., 2008; Si et al, 2010a; Figueroa and Atkin-
son, 2011). However, an important issue which
has been neglected so far is the detection of decep-
tive answers. If the acquired question-answer cor-
pus contains many deceptive answers, it would be
meaningless to perform further knowledge mining
tasks. Therefore, as the first step, we need to pre-
dict and filter out the deceptive answers. Among
previous work, answer quality prediction (Song et
al., 2010; Harper et al, 2008; Shah and Pomer-
antz, 2010; Ishikawa et al, 2010) is most related to
the deceptive answer prediction task. But these are
still significant differences between two tasks. An-
swer quality prediction measures the overall qual-
ity of the answers, which refers to the accuracy,
readability, completeness of the answer. While
the deceptive answer prediction aims to predict if
the main purpose of the provided answer is only
to answer the specific question, or includes the
user?s self-interest to promote something. Some
of the previous work (Song et al, 2010; Ishikawa
et al, 2010; Bian et al, 2009) views the ?best
answer? as high quality answers, which are se-
lected by the askers in the Community QA sites.
However, the deceptive answer may be selected as
high-quality answer by the spammer, or because
the general users are mislead. Meanwhile, some
answers from non-native speakers may have lin-
guistic errors, which are low-quality answers, but
are still authentic answers. Our experiments also
show that answer quality prediction is much dif-
ferent from deceptive answer prediction.
Previous QA studies also analyze the user graph
to investigate the user relationship (Jurczyk and
Agichtein, 2007; Liu et al, 2011). They mainly
construct the user graph with asker-answerer rela-
tionship to estimate the expertise score in Commu-
nity QA sites. They assume the answerer is more
knowledgeable than the asker. However, we don?t
care which user is more knowledgeable, but are
more likely to know if two users are both spam-
mers or authentic users. In this paper, we pro-
pose a novel user preference graph based on their
preference towards the target answers. We assume
that the spammers may collaboratively promote
the target deceptive answers, while the authen-
tic users may generally promote the authentic an-
swers and demote the deceptive answers. The user
preference graph is constructed based on their an-
swer evaluation, such as ?helpful? voting or ?best
answer? selection.
3 Proposed Features
We first view the deceptive answer prediction as a
binary-classification problem. Two kinds of fea-
tures, including textual features and contextual
features, are described as follows:
3.1 Textual Features
We first aim to predict the deceptive answer by an-
alyzing the answer content. Several textual fea-
tures are extracted from the answer content:
3.1.1 Unigrams and Bigrams
The most common type of feature for text classi-
fication is the bag-of-word. We use an effective
1724
feature selection method ?2 (Yang and Pedersen,
1997) to select the top 200 unigrams and bigrams
as features. The top ten unigrams related to decep-
tive answers are shown on Table 1. We can see that
these words are related to the intent for promotion.
professional service advice address
site telephone therapy recommend
hospital expert
Table 1: Top 10 Deceptive Related Unigrams
3.1.2 URL Features
Some malicious users may promote their products
by linking a URL. We find that URL is good indi-
cator for deceptive answers. However, some URLs
may provide the references for the authentic an-
swers. For example, if you ask the weather in
mountain view, someone may just post the link
to ?http://www.weather.com/?. Therefore, besides
the existence of URL, we also use the following
URL features:
1). Length of the URLs: we observe that the
longer urls are more likely to be spam.
2). PageRank Score: We employ the PageRank
(Page et al, 1999) score of each URL as popularity
score.
3.1.3 Phone Numbers and Emails
There are a lot of contact information mentioned
in the Community QA sites, such as phone num-
bers and email addresses, which are very likely to
be deceptive, as good answers are found to be less
likely to refer to phone numbers or email addresses
than the malicious ones. We extract the number of
occurrences of email and phone numbers as fea-
tures.
3.1.4 Length
We have also observed some interesting patterns
about the length of answer. Deceptive ones tend
to be longer than authentic ones. This can be ex-
plained as the deceptive answers may be well pre-
pared to promote the target. We also employ the
number of words and sentences in the answer as
features.
3.2 Contextual Features
Besides the answer textual features, we further in-
vestigate various features from the context of the
target answer:
3.2.1 Question Answer Relevance
The main characteristic of answer in Community
QA site is that the answer is provided to answer
the corresponding question. We can use the corre-
sponding question as one of the context features by
measuring the relevance between the answer and
the question. We employ three different models
for Question-Answer relevance:
Vector Space Model
Each answer or question is viewed as a word
vector. Given a question q and the answer a, our
vector model uses weighted word counts(e.g.TF-
IDF) as well as the cosine similarity (q ? a) of
their word vectors as relevant function (Salton and
McGill, 1986). However, vector model only con-
sider the exact word match, which is a big prob-
lem, especially when the question and answer are
generally short compared to the document. For ex-
ample, Barack Obama and the president of the US
are the same person. But the vector model would
indicate them to be different. To remedy the word-
mismatch problem, we also look for the relevance
models in higher semantic levels.
Translation Model
A translation model is a mathematical model in
which the language translation is modeled in a sta-
tistical way. The probability of translating a source
sentence (as answer here) into target sentence (as
question here) is obtained by aligning the words
to maximize the product of all the word probabil-
ities. We train a translation model (Brown et al,
1990; Och and Ney, 2003) using the Community
QA data, with the question as the target language,
and the corresponding best answer as the source
language. With translation model, we can com-
pute the translation score for new question and an-
swer.
Topic Model
To reduce the false negatives of word mismatch
in vector model, we also use the topic models to
extend matching to semantic topic level. The topic
model, such as Latent Dirichlet Allocation (LDA)
(Blei et al, 2003), considers a collection of doc-
uments with K latent topics, where K is much
smaller than the number of words. In essence,
LDA maps information from the word dimen-
sion to a semantic topic dimension, to address the
shortcomings of the vector model.
3.2.2 User Profile Features
We extract several user?s activity statistics to con-
struct the user profile features, including the level
1725
of the user in the Community QA site, the number
of questions asked by this user, the number of an-
swers provided by this user, and the best answer
ratio of this user.
3.2.3 User Authority Score
Motivated by expert finding task (Jurczyk and
Agichtein, 2007; Si et al, 2010a; Li et al, 2011),
the second type of author related feature is author-
ity score, which denotes the expertise score of this
user. To compute the authority score, we first con-
struct a directed user graph with the user interac-
tions in the community. The nodes of the graph
represent users. An edge between two users in-
dicates a contribution from one user to the other.
Specifically, on a Q&A site, an edge from A to
B is established when user B answered a question
asked by A, which shows user B is more likely to
be an expert than A. The weight of an edge indi-
cates the number of interactions. We compute the
user?s authority score (AS) based on the link anal-
ysis algorithm PageRank:
AS(ui) =
1? d
N + d
?
uj?M(ui)
AS(uj)
L(uj)
(1)
where u1, . . . , uN are the users in the collection,
N is the total number of users, M(ui) is the set
of users whose answers are provided by user ui,
L(ui) is the number of users who answer ui?s
questions, d is a damping factor, which is set as
0.85. The authority score can be computed itera-
tively with random initial values.
3.2.4 Robot Features
The third type of author related feature is used for
detecting whether the author is a robot, which are
scripts crafted by malicious users to automatically
post answers. We observe that the distributions of
the answer-posting time are very different between
general user and robot. For example, some robots
may make posts continuously and mechanically,
hence the time increment may be smaller that hu-
man users who would need time to think and pro-
cess between two posts. Based on this observa-
tion, we design an time sequence feature for robot
detection. For each author, we can get a list of
time points to post answers, T = {t0, t1, ..., tn},
where ti is the time point when posting the ith an-
swer. We first convert the time sequence T to time
interval sequence ?T = {?t0,?t1, ...,?tn?1},
where ?ti = ti+1 ? ti. Based on the interval
sequences for all users, we then construct a ma-
trix Xm?b whose rows correspond to users and
columns correspond to interval histogram with
predefined range. We can use each row vector as
time sequence pattern to detect robot. To reduce
the noise and sparse problem, we use the dimen-
sion reduction techniques to extract the latent se-
mantic features with Singular Value Decomposi-
tion (SVD) (Deerwester et al, 1990; Kim et al,
2006).
3.2.5 Evaluation from Other Users
In the Community QA sites, other users can ex-
press their opinions or evaluations on the answer.
For example, the asker can choose one of the an-
swers as best answer. We use a bool feature to de-
note if this answer is selected as the best answer.
In addition, other users can label each answer as
?helpful? or ?not helpful?. We also use this helpful
evaluation by other users as the contextual feature,
which is defined as the ratio between the number
of ?helpful? votes and the number of total votes.
3.2.6 Duplication with Other Answers
The malicious user may post the pre-written prod-
uct promotion documents to many answers, or just
change the product name. We also compute the
similarity between different answers. If the two
answers are totally same, but the question is differ-
ent, these answer is potentially as a deceptive an-
swer. Here, we don?t want to measure the semantic
similarity between two answers, but just measure
if two answers are similar to the word level, there-
fore, we apply BleuScore (Papineni et al, 2002),
which is a standard metric in machine translation
for measuring the overlap between n-grams of two
text fragments r and c. The duplication score of
each answer is the maximum BleuScore compared
to all other answers.
4 Deceptive Answer Prediction with User
Preference Graph
Besides the textual and contextual features, we
also investigate the user relationship for decep-
tive answer prediction. We assume that similar
users tend to perform similar behaviors (posting
deceptive answers or posting authentic answers).
In this section, we first show how to compute the
user similarity (user preference graph construc-
tion), and then introduce how to employ the user
relationship for deceptive answer prediction.
4.1 User Preference Graph Construction
In this section, we propose a new user graph to de-
scribe the relationship among users. Figure 1 (a)
shows the general process in a question answering
1726
Question 
Answer1 
Answer2 
Best Answer 
u1 
u2 
u3 
u4 
u5 
u6 
(a) Question Answering (b) User Preference Relation (c) User Preference Graph
Figure 1: User Preference Graph Construction
thread. The asker, i.e. u1, asks a question. Then,
there will be several answers to answer this ques-
tion from other users, for example, answerers u2
and u3. After the answers are provides, users can
also vote each answer as ?helpful? or ?not help-
ful? to show their evaluation towards the answer .
For example, users u4, u5 vote the first answer as
?not helpful?, and user u6 votes the second answer
as ?helpful?. Finally, the asker will select one an-
swer as the best answer among all answers. For
example, the asker u1 selects the first answer as
the ?best answer?.
To mine the relationship among users, previous
studies mainly focus on the asker-answerer rela-
tionship (Jurczyk and Agichtein, 2007; Liu et al,
2011). They assume the answerer is more knowl-
edgeable than the asker. Based on this assump-
tion, they can extract the expert in the commu-
nity, as discussed in Section 3.2.3. However, we
don?t care which user is more knowledgeable, but
are more interested in whether two users are both
malicious users or authentic users. Here, we pro-
pose a new user graph based on the user prefer-
ence. The preference is defined based on the an-
swer evaluation. If two users show same pref-
erence towards the target answer, they will have
the user-preference relationship. We mainly use
two kinds of information: ?helpful? evaluation and
?best answer? selection. If two users give same
?helpful? or ?not helpful? to the target answer, we
view these two users have same user preference.
For example, user u4 and user u5 both give ?not
helpful? evaluation towards the first answer, we
can say that they have same user preference. Be-
sides the real ?helpful? evaluation, we also assume
the author of the answer gives the ?helpful? evalu-
ation to his or her own answer. Then if user u6 give
?helpful? evaluation to the second answer, we will
view user u6 has same preference as user u3, who
is the author of the second answer. We also can ex-
tract the user preference with ?best answer? selec-
tion. If the asker selects the ?best answer? among
all answers, we will view that the asker has same
preference as the author of the ?best answer?. For
example, we will view user u1 and user u2 have
same preference.
Based on the two above assumptions, we can
extract three user preference relationships (with
same preference) from the question answering ex-
ample in Figure 1 (a): u4 ? u5, u3 ? u6, u1 ? u2,
as shown in Figure1 (b). After extracting all user
preference relationships, we can construct the user
preference graph as shown in Figure 1 (c). Each
node represents a user. If two users have the user
preference relationship, there will be an edge be-
tween them. The edge weight is the number of
user preference relationships.
In the Community QA sites, the spammers
mainly promote their target products by promoting
the deceptive answers. The spammers can collab-
oratively make the deceptive answers look good,
by voting them as high-quality answer, or select-
ing them as ?best answer?. However, the authen-
tic users generally have their own judgements to
the good and bad answers. Therefore, the evalu-
ation towards the answer reflects the relationship
among users. Although there maybe noisy rela-
tionship, for example, an authentic user may be
cheated, and selects the deceptive answer as ?best
answer?, we hope the overall user preference rela-
tion can perform better results than previous user
interaction graph for this task.
1727
4.2 Incorporating User Preference Graph
To use the user graph, we can just compute the
feature value from the graph, and add it into the
supervised method as the features introduced in
Section 3. Here, we propose a new technique to
employ the user preference graph. We utilize the
graph regularizer (Zhang et al, 2006; Lu et al,
2010) to constrain the supervised parameter learn-
ing. We will introduce this technique based on
a commonly used model f(?), the linear weight
model, where the function value is determined by
linear combination of the input features:
f(xi) = wT ? xi =
?
k
wk ? xik (2)
where xi is a K dimension feature vector for the
ith answer, the parameter value wk captures the
effect of the kth feature in predicting the deceptive
answer. The best parameters w? can be found by
minimizing the following objective function:
?1(w) =
?
i
L(wTxi, yi) + ? ? |w|2F (3)
where L(wTxi, yi) is a loss function that mea-
sures discrepancy between the predicted label
wT ? xi and the true label yi, where yi ?
{+1,?1}. The common used loss functions in-
clude L(p, y) = (p?y)2 (least square), L(p, y) =
ln (1 + exp (?py)) (logistic regression). For sim-
plicity, here we use the least square loss function.
|w|2F =
?
k w2k is a regularization term defined
in terms of the Frobenius norm of the parameter
vector w and plays the role of penalizing overly
complex models in order to avoid fitting.
We want to incorporate the user preference re-
lationship into the supervised learning framework.
The hypothesis is that similar users tend to have
similar behaviors, i.e. posting deceptive answers
or authentic answers. Here, we employ the user
preference graph to denote the user relationship.
Based on this intuition, we propose to incorporate
the user graph into the linear weight model with
graph regularization. The new objective function
is changed as:
?2(w) =
?
i
L(wTxi, yi) + ? ? |w|2F +
?
?
ui,uj?Nu
?
x?Aui ,y?Auj
wui,uj (f(x)? f(y))2 (4)
where Nu is the set of neighboring user pairs in
user preference graph, i.e, the user pairs with same
preference. Aui is the set of all answers posted by
user ui. wui,uj is the weight of edge between ui
and uj in user preference graph. In the above ob-
jective function, we impose a user graph regular-
ization term
?
?
ui,uj?Nu
?
x?Aui ,y?Auj
wui,uj (f(x)? f(y))2
to minimize the answer authenticity difference
among users with same preference. This regu-
larization term smoothes the labels on the graph
structure, where adjacent users with same prefer-
ence tend to post answers with same label.
5 Experiments
5.1 Experiment Setting
5.1.1 Dataset Construction
In this paper, we employ the Confucius (Si et
al., 2010b) data to construct the deceptive an-
swer dataset. Confucius is a community question
answering site, developed by Google. We first
crawled about 10 million question threads within
a time range. Among these data, we further sam-
ple a small data set, and ask three trained annota-
tors to manually label the answer as deceptive or
not. If two or more people annotate the answer as
deceptive, we will extract this answer as a decep-
tive answer. In total, 12446 answers are marked
as deceptive answers. Similarly, we also manu-
ally annotate 12446 authentic answers. Finally,
we get 24892 answers with deceptive and authen-
tic labels as our dataset. With our labeled data,
we employ supervised methods to predict decep-
tive answers. We conduct 5-fold cross-validation
for experiments. The larger question threads data
is employed for feature learning, such as transla-
tion model, and topic model training.
5.1.2 Evaluation Metrics
The evaluation metrics are precision, recall and
F -score for authentic answer category and de-
ceptive answer category: precision = Sp?ScSp ,
recall = Sp?ScSc , and F = 2?precision?recallprecision+recall , where
Sc is the set of gold-standard positive instances for
the target category, Sp is the set of predicted re-
sults. We also use the accuracy as one metric,
which is computed as the number of answers pre-
dicted correctly, divided by the number of total an-
swers.
1728
Deceptive Answer Authentic Answer Overall
Prec. Rec. F-Score Prec. Rec. F-Score Acc.
Random 0.50 0.50 0.50 0.50 0.50 0.50 0.50
Unigram/Bigram (UB) 0.61 0.71 0.66 0.66 0.55 0.60 0.63
URL 0.93 0.26 0.40 0.57 0.98 0.72 0.62
Phone/Mail 0.94 0.15 0.25 0.53 0.99 0.70 0.57
Length 0.56 0.91 0.69 0.76 0.28 0.41 0.60
All Textual Features 0.64 0.67 0.66 0.66 0.63 0.64 0.65
QA Relevance 0.66 0.57 0.61 0.62 0.71 0.66 0.64
User Profile 0.62 0.53 0.57 0.59 0.67 0.63 0.60
User Authority 0.54 0.80 0.65 0.62 0.33 0.43 0.56
Robot 0.66 0.62 0.64 0.61 0.66 0.64 0.64
Answer Evaluation 0.55 0.53 0.54 0.55 0.57 0.56 0.55
Answer Duplication 0.69 0.71 0.70 0.70 0.68 0.69 0.69
All Contextual Feature 0.78 0.74 0.76 0.75 0.79 0.77 0.77
Textutal + Contextual 0.80 0.82 0.81 0.82 0.79 0.80 0.81
Table 2: Results With Textual and Contextual Features
5.2 Results with Textual and Contextual
Features
We tried several different classifiers, including
SVM, ME and the linear weight models with least
square and logistic regression. We find that they
can achieve similar results. For simplicity, the lin-
ear weight with least square is employed in our
experiment. Table 2 shows the experiment results.
For textual features, it achieves much better re-
sult with unigram/bigram features than the ran-
dom guess. This is very different from the an-
swer quality prediction task. The previous stud-
ies (Jeon et al, 2006; Song et al, 2010) find that
the word features can?t improve the performance
on answer quality prediction. However, from Ta-
ble 1, we can see that the word features can pro-
vide some weak signals for deceptive answer pre-
diction, for example, words ?recommend?, ?ad-
dress?, ?professional? express some kinds of pro-
motion intent. Besides unigram and bigram, the
most effective textual feature is URL. The phone
and email features perform similar results with
URL. The observation of length feature for decep-
tive answer prediction is very different from previ-
ous answer quality prediction. For answer quality
prediction, length is an effective feature, for exam-
ple, long-length provides very strong signals for
high-quality answer (Shah and Pomerantz, 2010;
Song et al, 2010). However, for deceptive answer
prediction, we find that the long answers are more
potential to be deceptive. This is because most of
deceptive answers are well prepared for product
promotion. They will write detailed answers to at-
tract user?s attention and promote their products.
Finally, with all textual features, the experiment
achieves the best result, 0.65 in accuracy.
For contextual features, we can see that, the
most effective contextual feature is answer dupli-
cation. The malicious users may copy the pre-
pared deceptive answers or just simply edit the tar-
get name to answer different questions. Question-
answer relevance and robot are the second most
useful single features for deceptive answer predic-
tion. The main characteristics of the Community
QA sites is to accumulate the answers for the tar-
get questions. Therefore, all the answers should be
relevant to the question. If the answer is not rel-
evant to the corresponding question, this answer
is more likely to be deceptive. Robot is one of
main sources for deceptive answers. It automat-
ically post the deceptive answers to target ques-
tions. Here, we formulate the time series as in-
terval sequence. The experiment result shows that
the robot indeed has his own posting behavior pat-
terns. The user profile feature also can contribute
a lot to deceptive answer prediction. Among the
user profile features, the user level in the Com-
munity QA site is a good indicator. The other
two contextual features, including user authority
and answer evaluation, provide limited improve-
ment. We find the following reasons: First, some
malicious users post answers to various questions
for product promotion, but don?t ask any question.
From Equation 1, when iteratively computing the
1729
Deceptive Answer Authentic Answer Overall
Prec. Rec. F-Score Prec. Rec. F-Score Acc.
Interaction Graph as Feature 0.80 0.82 0.81 0.82 0.79 0.80 0.81
Interaction Graph as Regularizer 0.80 0.83 0.82 0.82 0.80 0.81 0.82
Preference Graph as Feature 0.79 0.83 0.81 0.82 0.78 0.80 0.81
Preference Graph as Regularizer 0.83 0.86 0.85 0.85 0.83 0.84 0.85
Table 3: Results With User Preference Graph
final scores, the authority scores for these mali-
cious users will be accumulated to large values.
Therefore, it is hard to distinguish whether the
high authority score represents real expert or mali-
cious user. Second, the ?best answer? is not a good
signal for deceptive answer prediction. This may
be selected by malicious users, or the authentic
asker was misled, and chose the deceptive answer
as ?best answer?. This also demonstrates that the
deceptive answer prediction is very different from
the answer quality prediction. When combining
all the contextual features, it can achieve the over-
all accuracy 0.77, which is much better than the
textual features. Finally, with all the textual and
contextual features, we achieve the overall result,
0.81 in accuracy.
5.3 Results with User Preference Graph
Table 3 shows the results with user preference
graph. We compare with several baselines. Inter-
action graph is constructed by the asker-answerer
relationship introduced in Section 3.2.3. When
using the user graph as feature, we compute the
authority score for each user with PageRank as
shown in Equation 1. We also incorporating the
interaction graph with a regularizer as shown in
Equation 4. Note that we didn?t consider the edge
direction when using interaction graph as a regu-
larizer. From the table, we can see that when in-
corporating user preference graph as a feature, it
can?t achieve a better result than the interaction
graph. The reason is similar as the interaction
graph. The higher authority score may boosted
by other spammer, and can?t be a good indica-
tor to distinguish deceptive and authentic answers.
When we incorporate the user preference graph
as a regularizer, it can achieve about 4% further
improvement, which demonstrates that the user
evaluation towards answers, such as ?helpful? vot-
ing and ?best answer? selection, is a good signal
to generate user relationship for deceptive answer
prediction, and the graph regularization is an ef-
fective technique to incorporate the user prefer-
ence graph. We also analyze the parameter sen-
10?5 10?4 10?3 10?2 10?1 1000.76
0.78
0.8
0.82
0.84
0.86
0.88
Acc
urac
y
 
 General supervised methodInteraction Graph as RegularizerPreference Graph as Regularizer
Figure 2: Results with different values of ?
sitivity. ? is the tradeoff weight for graph regular-
ization term. Figure 2 shows the results with dif-
ferent values of ?. We can see that when ? ranges
from 10?4 ? 10?2, the deceptive answer predic-
tion can achieve best results.
6 Conclusions and Future Work
In this paper, we discuss the deceptive answer
prediction task in Community QA sites. With
the manually labeled data set, we first predict the
deceptive answers with traditional classification
method. Two types of features, including textual
features and contextual features, are extracted and
analyzed. We also introduce a new user prefer-
ence graph, constructed based on the user evalua-
tions towards the target answer, such as ?helpful?
voting and ?best answer? selection. A graph reg-
ularization method is proposed to incorporate the
user preference graph for deceptive answer predic-
tion. The experiments are conducted to discuss
the effects of different features. The experiment
results also show that the method with user pref-
erence graph can achieve more accurate results for
deceptive answer prediction.
In the future work, it is interesting to incorpo-
rate more features into deceptive answer predic-
tion. It is also important to predict the deceptive
question threads, which are posted and answered
both by malicious users for product promotion.
Malicious user group detection is also an impor-
tant task in the future.
1730
References
Lada A. Adamic, Jun Zhang, Eytan Bakshy, and
Mark S. Ackerman. 2008. Knowledge sharing
and yahoo answers: everyone knows something. In
Proceedings of the 17th international conference on
World Wide Web, WWW ?08, pages 665?674, New
York, NY, USA. ACM.
Jiang Bian, Yandong Liu, Ding Zhou, Eugene
Agichtein, and Hongyuan Zha. 2009. Learning to
recognize reliable users and content in social media
with coupled mutual reinforcement. In Proceedings
of the 18th international conference on World wide
web, WWW ?09, pages 51?60, NY, USA. ACM.
David M. Blei, Andrew Y. Ng, and Michael I. Jordan.
2003. Latent dirichlet alocation. J. Mach. Learn.
Res., 3:993?1022, March.
Peter F. Brown, John Cocke, Stephen A. Della Pietra,
Vincent J. Della Pietra, Fredrick Jelinek, John D.
Lafferty, Robert L. Mercer, and Paul S. Roossin.
1990. A statistical approach to machine translation.
Comput. Linguist., 16:79?85, June.
S. Deerwester, S.T. Dumais, G.W. Furnas, T.K. Lan-
dauer, and R. Harshman. 1990. Indexing by latent
semantic analysis. Journal of the American society
for information science, 41(6):391?407.
A. Figueroa and J. Atkinson. 2011. Maximum entropy
context models for ranking biographical answers to
open-domain definition questions. In Twenty-Fifth
AAAI Conference on Artificial Intelligence.
F. Maxwell Harper, Daphne Raban, Sheizaf Rafaeli,
and Joseph A. Konstan. 2008. Predictors of answer
quality in online q&a sites. In Proceedings of the
twenty-sixth annual SIGCHI conference on Human
factors in computing systems, CHI ?08, pages 865?
874, New York, NY, USA. ACM.
Daisuke Ishikawa, Tetsuya Sakai, and Noriko Kando,
2010. Overview of the NTCIR-8 Community QA Pi-
lot Task (Part I): The Test Collection and the Task,
pages 421?432. Number Part I.
Jiwoon Jeon, W. Bruce Croft, and Joon Ho Lee. 2005.
Finding similar questions in large question and an-
swer archives. In Proceedings of the 14th ACM
CIKM conference, 05, pages 84?90, NY, USA.
ACM.
J. Jeon, W.B. Croft, J.H. Lee, and S. Park. 2006. A
framework to predict the quality of answers with
non-textual features. In Proceedings of the 29th an-
nual international ACM SIGIR conference on Re-
search and development in information retrieval,
pages 228?235. ACM.
P. Jurczyk and E. Agichtein. 2007. Discovering au-
thorities in question answer communities by using
link analysis. In Proceedings of the sixteenth ACM
CIKM conference, pages 919?922. ACM.
H. Kim, P. Howland, and H. Park. 2006. Dimension
reduction in text classification with support vector
machines. Journal of Machine Learning Research,
6(1):37.
Fangtao Li, Minlie Huang, Yi Yang, and Xiaoyan Zhu.
2011. Learning to identify review spam. In Pro-
ceedings of the Twenty-Second international joint
conference on Artificial Intelligence-Volume Volume
Three, pages 2488?2493. AAAI Press.
Yuanjie Liu, Shasha Li, Yunbo Cao, Chin-Yew Lin,
Dingyi Han, and Yong Yu. 2008. Understand-
ing and summarizing answers in community-based
question answering services. In Proceedings of the
22nd International Conference on Computational
Linguistics - Volume 1, COLING ?08, pages 497?
504, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
Jing Liu, Young-In Song, and Chin-Yew Lin. 2011.
Competition-based user expertise score estimation.
In Proceedings of the 34th international ACM SI-
GIR conference on Research and development in In-
formation Retrieval, pages 425?434. ACM.
Yue Lu, Panayiotis Tsaparas, Alexandros Ntoulas, and
Livia Polanyi. 2010. Exploiting social context for
review quality prediction. In Proceedings of the
19th international conference on World wide web,
pages 691?700. ACM.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Comput. Linguist., 29:19?51, March.
Lawrence Page, Sergey Brin, Rajeev Motwani, and
Terry Winograd. 1999. The pagerank citation rank-
ing: Bringing order to the web. Technical Report
1999-66, Stanford InfoLab, November. SIDL-WP-
1999-0120.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of
the 40th Annual Meeting on Association for Com-
putational Linguistics, ACL ?02, pages 311?318,
Stroudsburg, PA, USA. ACL.
Gerard Salton and Michael J. McGill. 1986. Intro-
duction to Modern Information Retrieval. McGraw-
Hill, Inc., New York, NY, USA.
Chirag Shah and Jefferey Pomerantz. 2010. Evaluat-
ing and predicting answer quality in community qa.
In Proceedings of the 33rd international ACM SIGIR
conference on Research and development in infor-
mation retrieval, SIGIR ?10, pages 411?418, New
York, NY, USA. ACM.
X. Si, Z. Gyongyi, and E. Y. Chang. 2010a. Scal-
able mining of topic-dependent user reputation for
improving user generated content search quality. In
Google Technical Report.
1731
Xiance Si, Edward Y. Chang, Zolta?n Gyo?ngyi, and
Maosong Sun. 2010b. Confucius and its intelli-
gent disciples: integrating social with search. Proc.
VLDB Endow., 3:1505?1516, September.
Young-In Song, Chin-Yew Lin, Yunbo Cao, and Hae-
Chang Rim. 2008. Question utility: a novel static
ranking of question search. In Proceedings of the
23rd national conference on Artificial intelligence
- Volume 2, AAAI?08, pages 1231?1236. AAAI
Press.
Y.I. Song, J. Liu, T. Sakai, X.J. Wang, G. Feng, Y. Cao,
H. Suzuki, and C.Y. Lin. 2010. Microsoft research
asia with redmond at the ntcir-8 community qa pilot
task. In Proceedings of NTCIR.
Wei Wei, Gao Cong, Xiaoli Li, See-Kiong Ng, and
Guohui Li. 2011. Integrating community question
and answer archives. In AAAI.
Y. Yang and J.O. Pedersen. 1997. A compara-
tive study on feature selection in text categoriza-
tion. In MACHINE LEARNING-INTERNATIONAL
WORKSHOP THEN CONFERENCE-, pages 412?
420. MORGAN KAUFMANN PUBLISHERS.
Tong Zhang, Alexandrin Popescul, and Byron Dom.
2006. Linear prediction models with graph regu-
larization for web-page categorization. In Proceed-
ings of the 12th ACM SIGKDD international con-
ference on Knowledge discovery and data mining,
pages 821?826. ACM.
1732
Coling 2008: Proceedings of the 2nd workshop on Information Retrieval for Question Answering (IR4QA), pages 42?49
Manchester, UK. August 2008
Answer Validation by Information Distance Calculation
Fangtao Li, Xian Zhang, Xiaoyan Zhu
State Key Laboratory on Intelligent Technology and Systems
Tsinghua National Laboratory for Information Science and Technology
Department of Computer Science and Technology, Tsinghua University, Beijing 100084, China
zxy-dcs@mail.tsinghua.edu.cn
Abstract
In this paper,an information distance based
approach is proposed to perform answer
validation for question answering system.
To validate an answer candidate, the ap-
proach calculates the conditional informa-
tion distance between the question focus
and the candidate under certain condition
pattern set. Heuristic methods are de-
signed to extract question focus and gen-
erate proper condition patterns from ques-
tion. General search engines are employed
to estimate the Kolmogorov complexity,
hence the information distance. Experi-
mental results show that our approach is
stable and flexible, and outperforms tradi-
tional tfidf methods.
1 Introduction
Question answering(QA) system aims at finding
exact answers to a natural language question. In
order to correctly answer a question, several com-
ponents are implemented including question clas-
sification, passage retrieval, answer candidates
generation, answer validation etc. Answer Vali-
dation is to decide whether the candidate answers
are correct or not, or even to determine the accu-
rate confidence score to them. Most of QA systems
employ answer validation as the last step to iden-
tify the correct answer. If this component fails, it
is impossible to enable the question to be correctly
answered.
Automatic techniques for answer validation are
of great interest among question answering re-
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
search. With automatic answer validation, the
system will carry out different refinements of its
searching criteria to check the relevance of new
candidate answers. In addition, since most of
QA systems rely on complex architectures and the
evaluation of their performances requires a huge
amount of work, the automatic assessment of can-
didates with respect to a given question will speed
up both algorithm refinement and testing.
Currently, answer validation is mainly viewed
as a classification problem or ranking problem.
Different models, such as Support Vector Ma-
chine (Shen and Klakow, 2006) and Maximum En-
tropy Model (Ittycheriah et al, 2001), are used to
integrate sophisticated linguistic features to deter-
mine the correctness of candidates. The answer
validation exercise (Penas et al , 2007) aims at
developing systems able to decide whether the an-
swer is correct or not. They formulate answer val-
idation as a text entailment problem. These ap-
proaches are dependent on sophisticated linguis-
tic analysis of syntactic and semantic relations be-
tween question and candidates. It is quite expen-
sive to use deep analysis for automatic answer val-
idation, especially in large scale data set. Thus it
is appropriate to find an alternative solution to this
problem. Here, we just consider the English an-
swer validation task.
This paper proposes a novel approach based on
information retrieval on the Web. The answer val-
idation problem is reformulated as distance calcu-
lation from an answer candidate to a question. The
hypothesis is that, among all candidates, the cor-
rect answer has the smallest distance from ques-
tion. We employ conditional normalized min dis-
tance, which is based on Kolmogorov Complexity
theory (Li and Vitanyi, 1997), for this task. The
distance measures the relevance between question
42
focus and candidates conditioned on a surface pat-
tern set. For distance calculation, we first ex-
tract the question focus, and then a hierarchical
pattern set is automatically constructed as condi-
tion. Since Kolmogrov Complexity can be approx-
imated through frequency counts. Two types of
search engine ?Google? and ?Altavista? are used
to approximate the distance.
The paper is organized as follows: Section 2
describes related work. The fundamental Kol-
mogorov Complexity theory is introduced in Sec-
tion 3. Section 4 presents our proposed answer val-
idation method based on information retrieval. In
Section 5, we describe the experiments and discus-
sions. The paper is concluded in Section 6.
2 Related Work
Answer Validation is an emerging topic in Ques-
tion Answering, where open domain systems are
often required to rank huge amounts of answer
candidates. This task can be viewed as a classi-
fication problem or re-ranking problem.
Early question answering systems focused on
employing surface text patterns (Subbotin and
Subbotin, 2001) for answer validation. Xu et
al. (2003) identified that pattern-based approaches
got bad performances due to poor system recall.
Some researchers exploited machine learning tech-
niques with rich syntactic or semantic features to
measure the similarity between question and an-
swer. Ittycheriah et al (2001) used Maximum En-
tropy model to combine rich features and automat-
ically learn feature weights. These features in-
cluded query expansion features, focus features,
named entity features, dependency relation fea-
tures, pattern features et al Shen and Klakow
(2006) presented three methods, including feature
vector, string kernel and tree kernel, to represent
surface text features and parse tree features in Sup-
port Vector Machines. Ko et al (2007) pro-
posed a probabilistic graphical model to estimate
the probability of correctness for all candidate an-
swers. Four types of features were employed,
including knowledge-based features, data-driven
features, string distance feature and synonym fea-
tures.
Started in 2006, the annual Answer Validation
Exercise (Penas et al , 2007) aims to develop sys-
tems to decide if the answer to a question is correct
or not. The English answer validation task is refor-
mulated as a Text Entailment problem. The triplet,
including question, answer and supporting text, is
given. The system determines if the supporting
text can entail the hypothesis, which is a reformu-
lation from the question and answer. All partici-
pants used lexical processing, including lemmati-
zation and part-of speech tagging. Some systems
used first order logic representations, performed
semantic analysis and took the validation decision
with a theorem proof.
The above approaches should process deep syn-
tactic and semantic analysis for either questions or
candidate answers. The annotated linguistic re-
source is hard to acquire for the supervised clas-
sification problem. Another alternative solution
for answer validation is to exploit the redundancy
of large scale data. Eric et al (2007) devel-
oped AskMSR question answering system. They
focus on the Web as a gigantic data repository
with tremendous redundancy that can be exploited
to extract the correct answer. Lin (2007) im-
plemented another Web-based question answering
system, named ARANEA, which is used approxi-
mate tfidf method for answer validation.
3 Preliminaries
3.1 Kolmogorov complexity
Kolmogorov complexity , or algorithm entropy ,
K(x) of a string x is the length of the shortest bi-
nary program to compute x. It defines randomness
of an individual string. Kolmogorov complexity
has been widely accepted as an information theory
for individual objects parallel to that of Shannon?s
information theory which is defined on an ensem-
ble of objects. It has also found many applications
in computer science such as average case analysis
of algorithms (Li and Vitanyi, 1997). For a uni-
versal Turing machine U , the Kolmogorov com-
plexity of a binary string x condition to another
binary string y, K
U
(x|y), is the length of the short-
est (prefix-free) program for U that outputs x with
input y. It has been proved that for different uni-
versal Turing machine U
?
, for all x, y
K
U
(x|y) = K
U
?
(x|y) + C,
where the constant C depends only on U
?
. Thus we
simply write K
U
(x|y) as K(x|y). Define K(x) =
K(x|?), where ? is the empty string. For for-
mal definitions and a comprehensive study of Kol-
mogorov complexity, see (Li and Vitanyi, 1997).
43
3.2 Information Distance
Based on the Kolmogovov complexity theory, in-
formation distance (Bennett et al, 1998) is a uni-
versal distance metric, which has been success-
fully applied to many applications. The informa-
tion distance D(x, y) is defined as the length of
a shortest binary program which can compute x
given y as well as compute y from x. It has been
proved that , up to an additive logarithmic term,
D(x, y) = max{K(x|y),K(y|x)}. The normal-
ized version of D(x, y), called the normalized in-
formation distance(NID), is defined as
d
max
(x, y) =
max{K(x|y),K(y|x)}
max{K(x),K(y)}
(1)
Parallel to this, the min distance is proposed in
(Zhang et al , 2007), defined as
D
min
(x, y) = min{K(x|y),K(y|x)}. (2)
And the normalized version is
d
min
(x, y) =
min{K(x|y),K(y|x)}
min{K(x),K(y)}
. (3)
3.3 Conditional Information Distance
Conditional information distance is defined as
d
max
(x, y|c) =
max{K(x|y, c),K(y|x, c)}
max{K(x|c),K(y|c)}
, (4)
d
min
(x, y|c) =
min{K(x|y, c),K(y|x, c)}
min{K(x|c),K(y|c)}
. (5)
where c is given in both x to y and y to x compu-
tation.
The information distance is proved to be uni-
versal (Zhang et al , 2007), that is, if x and y
are ?close? under any distance measure, they are
?close? under the measure of information distance.
However, it is not clear yet how to find out such
?closeness? in traditional information distance the-
ory. Now the conditional information distance pro-
vides a possible solution.Figure 1 gives a more in-
terpretable explanation: the condition c could map
the original concepts x and y into different x
c
and
y
c
, thus the variant ?closeness? could be reflected
by the distance between x
c
and y
c
, as shown in
Figure1.
Figure 1: Conditional information distances under different
conditions c?s
The Kolmogorov complexity is non-
computable, that is, to use the information
distance measures, we must estimate the K(x)
first. There are traditionally two ways to do
this: (1) by compression (Li et al , 2001),
and (2) by frequency counting based on coding
theorem (Cilibrasi and Vitanyi, 2007). The second
approach is implemented in this paper.
4 Answer Validation with Information
Distance
Given a question q and a candidate answer c, the
answer validation task can be considered as deter-
mining the degree of relevance of c with respect
to q. The intuition of our approach is that the dis-
tance between question and the correct answer is
smaller than other candidates. Take the question
?What is the capital of the USA?? as an example,
among all candidates, the correct answer ?Wash-
ington? is closest to the question under some dis-
tance measure. Thus the answer validation prob-
lem is to determine a proper distance measure.
Fortunately, it has been proved that the informa-
tion distance (Bennett et al, 1998) is universal so
that the similarity between the question and the an-
swer can surely be discovered using this measure.
Direct calculation of the unconditional distance
is difficult and non-flexible. We find it possible
and convenient to estimate the conditional infor-
mation distance between question focus and the
answers, under certain context as the condition. As
explained previously, different conditions lead to
different distance. With the most proper condition
and the nearest distance, the best answer can be
identified out of previously determined candidates.
The conditional normalized min distance is em-
ployed for distance calculation, which is defined
44
Figure 2: Sample of conditional information distance calculation.
as:
d
min
(x, y|c)
=
K
(
c(x,y)
)
?max{K
(
c(x,?)
)
,K
(
c(?,y)
)
}
min{K
(
c(x,?)
)
,K
(
c(?,y)
)
}?K
(
c(?,?)
)
where x represents the answer candidates, y is
the question focus, and c is condition pattern. The
function c(x, y) will be described in the Distance
Calculation section.
Figure 2 shows the procedure of distance cal-
culation. Given a question and a set of candidates,
we calculate the min information distance between
question focus and candidates conditioned on sur-
face patterns. Obviously, in order to calculate in-
formation distance, there are three issues to be ad-
dressed:
1. Question Focus Extraction: since the question
answer distance is reformulated as the mea-
sure between question focus and answer con-
ditioned on the surface pattern, it is important
to extract some words or phrases as question
focus.
2. Condition Pattern Generation: Obviously, the
generation of the condition is the key part.
We have built a well revised algorithm, in
which proper conditions can be generated
from question sentence according to some
heuristic rules.
3. Distance Calculation: after question focus
and condition patterns are obtained, the last
step is calculating the conditional distance to
estimate the relevance between question and
answer candidates.
4.1 Question Focus Extraction
Most factoid questions refer to specific objects. A
question is asked to learn some knowledge for this
object from certain perspective. In our approach,
we take the key named entity or noun phrase, usu-
ally as the subject or the main object of the ques-
tion sentence as the reference object. Take the
question ?What city is Lake Washington by? as ex-
ample, the specific object is ?Lake Washington?.
The question focus is identified using some heuris-
tic rules as follows:
1. The question is processed by shallow parsing.
All the noun phrases(NP) are extracted as NP set.
2. All the named entities(NE) in the question are
extracted as NE set.
3. If only one same element is identified in both
NE and NP set, this element is considered as ques-
tion focus.
4. If step 3 fails, but two elements from NE and
NP set have overlap words, then choose the ele-
ment with more words as question focus.
5. If step 3 and 4 fail, choose the candidate,
which is nearest with verb phrase in dependency
tree, as question focus.
4.2 Condition Pattern Generation
A set of hierarchical patterns is automatically con-
structed for conditional min distance calculation.
4.2.1 Condition Pattern Construction
Several operations are defined for patterns con-
struction from the original question sentence. We
describe pattern set construction with a sam-
ple question ?What year was President Kennedy
killed??:
1. With linguistic analysis, the question is
split into pieces of tokens. These tokens in-
45
clude wh-word phrases, preposition phrases, noun
phrases, verb phrases, key verb, etc. The exam-
ple question is split into ?What year?(wh-word
phrase), ?was?(key verb) ?President Kennedy?
(noun phrases), ?killed?(verb phrase).
2. Replace the wh-word phrases with the candi-
date placeholder ?c?. Then the words ?What year?
is replaced with placeholder ?c?.
3. Replace the question focus with the focus
placeholder ?f?, and add this pattern to the pat-
tern set. The example question focus is identified
as ?President Kennedy?. It is replaced with place-
holder ?f?. The first pattern ??c? was ?f? killed??
is generated.
4. Voice Transformation: with morphology
techniques, verbs are expanded with all their tense
forms ( i.e. present, past tense and past participle).
The tokens? order is adjusted to transform between
active voice and passive voice. Both patterns are
added to the patterns set. For sample question,
the passive pattern is translated into active pattern,
??c? kill ?f??.
5. Preposition addition: for time and location
questions, the preposition (i.e. in, on and at) is
added before the candidate ?c?; Then the pattern
??c? was ?f? killed? is reformulated as ?(in |on)
?c? was ?f? killed?.
6. Tokens shift: preposition phrase token could
be shifted to the begin or the end of pattern, and
?key verb? must be shift before the ?verb phrase?.
Then the pattern ?(in |on) ?c? was ?f? killed? can
be reformulated as ??f? was killed (in |on) ?c??.
7. Definitional patterns: several heuristic pat-
terns, as introduced at (Hildebrandt et al , 2004),
are added into our final pattern sets, such as ??c?,
?f??.
By such heuristic rules, the original pattern set is
obtained from question sentence. The patterns are
initially enclosed in quotation marks, which means
exact matching. However, by eliminating these
quotations, or reducing the scope that they cover,
the matching is relaxed as words co-occurrence.
The patterns are expanded into different strict-level
patterns by adding or removing quotation marks
for each tokens or adjacent tokens combination.
Several condition pattern samples are shown in Ta-
ble 1
Table 1: Sample condition patterns, ? ?? ? denotes exact
match in web query.
? ? <f>(was | were) killed (in | on) <c>?
? ? (in | on) <c>, <f>(was | were) killed?
? ? (in | on) <c>? & ?<f>(was | were) killed?
? ? (in | on) <c>? & ?<f>? & ?(was | were) killed?
? in | on <c><f>(was | were) killed
Each operation introduced above is given a pre-
defined confidence coefficient(cc). Then the con-
fidence coefficient of a pattern is defined as the
multiplication of cc for all performed operations
to generate this pattern.
4.2.2 Condition Pattern Ranking
From the previous step, a set of condition pat-
terns and corresponding confidence coefficient are
obtained. Let p
i
denotes the ith pattern in the pat-
tern set, and cc
i
is the confidence coefficient for the
ith pattern. The confidence coefficient estimation
in previous section contains much noise. And the
patterns with similar confidence coefficient make
little difference. Therefore, the exact confidence
coefficient value is not directly used. We cluster
the patterns into different priority groups. C
j
de-
notes the pattern cluster with jth priority. Here,
the smaller j means higher priority. The condi-
tion patterns are ranked mainly based on confi-
dence coefficient and the number of double quo-
tation marks. The following algorithm shows each
step in detail:
Table 2: patterns ranking algorithm
Input patterns set C = {(p
i
, cc
i
)}
Algorithm
(1) Initialize C
j
= ?, j = 0
(2) if C is empty, end this algorithm
(3) Select (p
max
, cc
max
), where cc
max
?
cc
i
, (p
i
, cc
i
) ? C
(4) if C
j
is empty, add cc
max
into C
j
, jump to
(2)
(5) select the minimum confidence coefficient
(p
min
, cc
min
) from C
j
, compare it with
(p
max
, cc
max
). if the number of double
quotes(??) in p
min
is equal to the number in
p
max
, add p
max
into C
j
. otherwise, j =
j + 1, C
j
= {p
max
}.
(6) jump to (2) and repeat
4.3 Distance Calculation
Conditional min distance d
min
is used to mea-
sure the relevance between question and candidate.
From section 3, d
min
is not computable, but ap-
proximated by frequency counts based on the cod-
ing theory:
46
dmin
(x, y|c)
=
K
(
c(x,y)
)
?max{K
(
c(x,?)
)
,K
(
c(?,y)
)
}
min{K
(
c(x,?)
)
,K
(
c(?,y)
)
}?K
(
c(?,?)
)
=
log f
(
c(x,y)
)
?min{log f
(
c(x,?)
)
,log f
(
c(?,y)
)
}
max{log f
(
c(x,?)
)
,log f
(
c(?,y)
)
}?log f
(
c(?,?)
)
The function c(x, ?) means substituting ?c? in c
by answer candidate x and removing placeholder
?f? if any. Similar definition applies to c(y, ?),
c(x, y). For example, given pattern ??f? was in-
vented in ?c??, question focus ?the telegraph? and
a candidate ?1867?. c(x, ?) is ?was invented in
1867?. c(y, ?) is ?the telegraph was invented?, and
c(x, y) is ?the telegraph was invented in 1867?.
The frequency counts f(x) are estimated as the
number of returned pages by certain search en-
gine with respect to x . f(c(?, ?)) denote the to-
tal pages indexed in search engine. Two types of
search engines ?Google? and ?Altavista? are em-
ployed.
The patterns are selected in priority order to cal-
culate the information distance for each candidate.
5 Experiment and Discussion
5.1 Experiment Setup
Data set: The standard QA test collection (Lin
and Katz, 2006) is employed in our experiments. It
consists of 109 factoid questions, covering several
domains including history, geography, physics, bi-
ology, economics, fashion knowledge, and etc.. 20
candidates are prepared for each questions. All an-
swer candidates are first extracted by the imple-
mented question answering system. Then we re-
view the candidate set for each question. If the cor-
rect answer is not in this set, it is manually added
into the set.
Performance Metric: The top 1 answer precision
and mean reciprocal rank (MRR) are used for per-
formance evaluation.The top 1 answer means the
correct answer ranks first with our distance calcu-
lation method, and MRR =
1
n
?
?
i
(
1
rank
i
), in
which the
1
rank
i
is 1 if the correct answer occurs in
the first position; 0.5 if it firstly occurs in the sec-
ond position; 0.33 for the third, 0.25 for the fourth,
0.2 for the fifth and 0 if none of the first five an-
swers is correct.
The open source factoid QA system ARANEA
(downloaded from Jimmy Lin?s website in 2005)
is used for comparison, which implements an ap-
proximate tfidf algorithm for candidate scoring.
Both ARANEA and our proposed approaches use
the internet directly. Google is used as the search
engine for ARENEA, and our conditional normal-
ized min distance is calculated with Google and
Altavista respectively.
5.2 Experiment Results
The performances of our proposed approach and
ARANEA are shown in Table 3. For top 1 an-
swer precision, our conditional min distance cal-
culation method through Google achieves 69.7%,
and Altavista is 66.1%, which make 56.6%
(69.7% v.s.42.2% ) and 50.0% (66.1% v.s 42.2%)
improvement compared with ARENEA?s tfidf
method. Our proposed methods achieve 0.756 and
0.772 compared with ARENEA?s 0.581 for MRR
measure.
Table 3: Performance comparison, where d
min
(G) denotes
the distance calculation through ?Google?, d
min
(A) through
?Altavista?
tfidf d
min
(G) d
min
(A)
# of Top 1 46 72 69
% of Top 1 42.2 69.7 66.1
MRR 0.581 0.772 0.756
Table 4 shows some correct answer validation
examples. the Google Condition(GC) and the Al-
tavista Condition(AC) columns are the employed
condition patterns for distance calculation. For
question 1400, the conditional normalized google
min distance calculates the distance between ques-
tion focus ?the telegragh? and all 20 answer can-
didates. The minimum distance score is achieved
between ?the telegraph? and ?1837? with the con-
dition pattern ??f? was invented in ?c??. There-
fore, the candidate ?1837? is validated as the cor-
rect answer. Meanwhile, the minimum value for
conditional normalized altavista min distance is
achieved on the same condition.
These results demonstrate that the distance cal-
culation method provides a feasible solution for
answer validation.
In discussion section, we will study three ques-
tions:
1. What is the role of search engine?
2. What is the role of condition pattern?
3. What is the role of question focus?
47
Table 4: Question Examples in conditional information calculation through Google and Altavista. GC:Google Condition;
AC:Altavista Condition
ID Question GC AC Answer Question focus
1400 When was the telegraph
invented?
??y was in-
vented in ?s?
??y was
invented in
?x?
1837 the telegraph
1401 What is the democratic
party symbol?
??y is ?x? ??y is ?x? the don-
key
the democratic
party symbol
1411 What Spanish explorer
discovered the Missis-
sippi River?
??x discov-
ered ?y?
??x? ?dis-
covered?
??y?
Hernando
de Soto
the Mississippi
River
1412 Who is the governor of
Colorado?
??y is ?x? ??y, ?x? Gov. Bill
Ritter
the governor of
Colorado
1484 What college did Allen
Iverson attend?
??y attended
?x?
??x? ?did
?y?
Georgetown
Univer-
sity
Allen Iverson at-
tend
5.3 Discussions
5.3.1 Role of Search Engine
The rise of world-wide-web has enticed millions
of users to create billions of web pages. The re-
dundancy of web information is an important re-
source for question answering. Our Kolmogorov
Complexity based information distance is approx-
imated with query frequency obtained by search
engine. Two types of search engines ?Google? and
?Altavista? are employed in this paper. The num-
ber of top 1 correct answer is 72 through ?Google?
and 69 through ?Altavista?. There is little differ-
ence between two numbers, which shows that the
information distance based on Kolmogorov Com-
plexity is independent of special search engine.
The performance didn?t vary much with the change
of search engine. Actually, if the local data is ac-
cumulated large enough, the information distance
can be approximated without the internet. The
quality and size of data set affect the experiment
performance.
5.3.2 Role of Condition Pattern
Pattern set offers convenient and flexible condi-
tion for information distance calculation. In the
experiment, there are 61 questions correctly an-
swered by both Google and Altavista. 46 ques-
tions of them employ different patterns. Consider-
ing Question 1412, the condition pattern in Google
is ??c? is ?f??, while in Altavista, it is ??f?, ?c??.
However, the correct answer ?Gov. Bill Ritter? is
identified by both methods. The information dis-
tance is stable over specific condition patterns.
5.3.3 Role of Question Focus
Question focus is considered as the discrimina-
tor for the question. The distance between a ques-
tion and a candidate is reformulated as the distance
between question focus and candidate conditioned
on a set of surface patterns. The proposed ap-
proach may not properly extract the question fo-
cus, but the answers can be correctly identified
when the condition pattern becomes loose enough.
Take the question 1484 ?What college did Allen
Iverson attend?? as example, the verb ?attend? is
tagged as ?noun?, then question focus is mistak-
enly extracted as ?Allen Iverson attend?, instead of
the correct ?Allen Iverson?. The two conditional
information distance method still identify the cor-
rect answer ?Georgetown University?. Because
they both employed the looser condition patterns
???c?? ??f??? and ???c?? did ??f???.Therefore, our
proposed distance answer validation methods are
robust to the question focus selection component.
From the discussion above, it can be seen that
our algorithm is stable and robust, not depending
on the specific search engine, condition pattern,
and question focus.
6 Conclusions
We have presented a novel approach for answer
validation based on information distance. The an-
swer validation task is reformulated as distance
calculation between question focus and candidate
conditioned on a set of surface patterns. The ex-
periments show that our proposed answer valida-
tion method makes a great improvement compared
48
with ARANEA?s tfidf method. Furthermore, The
experiments show that our approach is stable and
robust, not depending on the specific search en-
gine, condition pattern, and question focus. In fu-
ture work, we will try to calculate information dis-
tance in the local constructed data set, and expand
this distance measure into other application fields.
Acknowledgement
This work is supported by National Natural Sci-
ence Foundation of China (60572084, 60621062),
Hi-tech Research and Development Program of
China (2006AA02Z321), National Basic Research
Program of China (2007CB311003).
References
Abraham Ittycheriah, Martin Franz, Wei-Jing Zhu, Ad-
wait Ratnaparkhi, and Richard J.Mammone. 2001.
Question answering using maximum entropy conm-
ponents. In Proceedings of the Second meeting of
the North American Chapter of the Association for
Computational Linguistics on Language tecnologies.
Anselmo Penas, A. Rodrigo, F. Verdejo. 2007.
Overview of the Answer Validation Exercise 2007.
Working Notes for the CLEF 2007 Workshop.
C.H. Bennett, P. Gacs, M. Li, P. Vit?anyi, W. Zurek..
1998. Information Distance. IEEE Trans. Inform.
Theory, 44:4, 1407?1423.
Eric Brill and Susan Dumais and Michele Banko. 2002.
An analysis of the AskMSR question-answering sys-
tem. EMNLP ?02: the ACL-02 conference on Em-
pirical methods in natural language processing.
Hildebrandt W., Katz B., and Lin J. 2004. Answer-
ing Definition Questions Using Multiple Knowledge
Sources. Proceedings of Human Language Technol-
ogy Conference. Boston, USA.
Jeongwoo Ko, Luo Si, Eric Nyberg. 2007. A Proba-
bilistic Graphical Model for Joint Answer Ranking
in Question Answering. In Proceedings of the 30th
annual international ACM SIGIR conference on Re-
search and development in information retrieval.
Jimmy Lin and Boris Katz. 2006. Building a reusable
test collection for question answering. J. Am. Soc.
Inf. Sci. Technol..
Jimmy Lin. 2007. An Exploration of the Principles Un-
derlying Redundancy-Based Factoid Question An-
swering. ACM Transactions on Information Sys-
tems, 27(2):1-55.
Jinxi Xu, Ana Licuanan and Ralph Weischedel. 2003.
Trec 2003 qa at bbn: Answering definitional ques-
tions. In Proceedings of the 12th Text REtrieval
Conference, Gaithersburgh, MD, USA.
Ming Li and Paul MB Vitanyi. 1997. An Introduc-
tion to Kolmogorov Complexity and Its Applications.
Working Notes for the CLEF 2007 Workshop.
M. Li, J. Badger, X. Chen, S. Kwong, P. Kearney,
H. Zhang.. 2001. An information-based sequence
distance and its application to whole mitochondrial
genome phylogeny. Bioinformatics, 17:2.
M. Subbotin and S. Subbotin. 2001. Patterns of Po-
tential Answer Expressions as Clues to the Right An-
swers. In TREC-10 Notebook papers. Gaithesburg,
MD.
R. Cilibrasi, P.M.B. Vit?anyi. 2007. An Exploration of
the Principles Underlying Redundancy-Based Fac-
toid Question Answering. EEE Trans. Knowledge
and Data Engineering, 19:3, 370?383.
Shen, Dan and Dietrich Klakow . 2006. Exploring cor-
relation of dependency relation paths for answer ex-
traction. In Proceedings of COLING-ACL, Sydney,
Australia.
Xian Zhang, Yu Hao, Xiaoyan Zhu, and Ming Li. 2007.
Information Distance from a Question to an Answer.
In Proceedings of the 13th ACM SIGKDD interna-
tional conference on Knowledge discovery and data
mining.
49
