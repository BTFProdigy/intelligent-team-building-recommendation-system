Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 241?249, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
UMCC_DLSI: Reinforcing a Ranking Algorithm with Sense 
Frequencies and Multidimensional Semantic Resources to solve 
Multilingual Word Sense Disambiguation 
 
Yoan Guti?rrez, Yenier 
Casta?eda, Andy Gonz?lez, 
Rainel Estrada, Dennys D. Piug, 
Jose I. Abreu, Roger P?rez 
Antonio Fern?ndez Orqu?n, 
Andr?s Montoyo, Rafael Mu?oz 
Franc Camara 
DI, University of Matanzas DLSI, University of Alicante Independent Consultant 
Matanzas, Cuba Alicante, Spain USA 
{yoan.gutierrez, 
yenier.castaneda, 
rainel.estrada, 
dennys.puig, jose.abreu, 
roger.perez}@umcc.cu, 
andy.gonzalez@infonet.umcc
.cu 
antonybr@yahoo.com, 
{montoyo,rafael}@dlsi.ua.
es 
info@franccamara.c
om 
 
Abstract 
This work introduces a new unsupervised 
approach to multilingual word sense 
disambiguation. Its main purpose is to 
automatically choose the intended sense 
(meaning) of a word in a particular context for 
different languages. It does so by selecting the 
correct Babel synset for the word and the 
various Wiki Page titles that mention the 
word. BabelNet contains all the output 
information that our system needs, in its Babel 
synset. Through Babel synset, we find all the 
possible Synsets for the word in WordNet. 
Using these Synsets, we apply the 
disambiguation method Ppr+Freq to find what 
we need. To facilitate the work with WordNet, 
we use the ISR-WN which offers the 
integration of different resources to WordNet. 
Our system, recognized as the best in the 
competition, obtains results around 69% of 
Recall. 
1 Introduction 
Word Sense Disambiguation (WSD) focuses on 
resolving the semantic ambiguity of a given word.  
This is an important task in Natural Language 
Processing (NLP) because in many applications, 
such as Automatic Translation, it is essential to 
know the exact meaning of a word in a given 
context. In order to solve semantic ambiguity, 
different systems have been developed. However, 
we can categorize them in two main groups: 
supervised and unsupervised systems. The 
supervised ones need large quantity of hand-tagged 
data in order to gather enough information to build 
rules, train systems, and so on. Unsupervised 
systems, on the other hand, do not need such a 
large amount of hand-tagged datasets. This means 
that, when there aren?t enough corpora to train the 
systems, an unsupervised system is a good option. 
A sub-task of WSD is Multilingual Word Sense 
Disambiguation (MWSD) (Navigli et al, 2013) 
that aims at resolving ambiguities in different 
languages. 
In a language, there are words that have only one 
sense (or meaning), but in other languages, the 
same words can have different senses. For 
example, ?patient? is a word that in English can be 
either a noun or an adjective, but in German, it 
only has one sense - ?viz? (a person that needs 
treatment). This shows that the information 
obtained by combining two languages can be more 
useful for WSD because the word senses in each 
language can complement each other. For it to be 
useful, MWSD needs a multilingual resource that 
contains different languages, such as BabelNet 
(Navigli and Ponzetto, 2010; 2012) and 
EuroWordNet (Vossen, 1998). 
241
As the preferred disambiguation method, we 
decided to use the Ppr+Freq (Personalized Page 
Rank combined with Frequencies of senses)  
(Guti?rrez, 2012) method because, among 
unsupervised systems, graph-based methods have 
obtained more promising results.  
It is worth mentioning the relevant approaches 
used by the scientific community to achieve 
promising results. One approach used is structural 
interconnections, such as Structural Semantic 
Interconnections (SSI), which create structural 
specifications of the possible senses for each word 
in a context (Navigli and Velardi, 2005). The other 
approaches used are ?Exploring the integration of 
WordNet? (Miller et al, 1990), FrameNet (Laparra 
et al, 2010) and those using Page-Rank such as 
(Sinha and Mihalcea, 2007) and (Agirre and Soroa, 
2009). 
The aforementioned types of graph based 
approaches have achieved relevant results in both 
the SensEval-2 and SensEval-3 competitions (see 
Table 1). 
Algorithm Recall 
TexRank (Mihalcea, 2005)  54.2% 
(Sinha and Mihalcea, 2007) 56.4% 
(Tsatsaronis et al, 2007) 49.2% 
Ppr (Agirre and Soroa, 2009) 58.6% 
Table 1. Relevant WSD approaches. Recall measure is 
calculated recalls using SensEval-2 (English All Word 
task) guidelines over. 
Experiments using SensEval-2 and SensEval-3 
corpora suggest that Ppr+Freq (Guti?rrez, 2012) 
can lead to better results by obtaining over 64% of 
Recall. Therefore we selected Ppr+Freq as the 
WSD method for our system. 
The key proposal for this work is an 
unsupervised algorithm for MWSD, which uses an 
unsupervised method, Ppr+Freq, for semantic 
disambiguation with resources like BabelNet (as 
sense inventory only) (Navigli and Ponzetto, 2010) 
and ISR-WN (as knowledge base) (Guti?rrez et al, 
2011a; 2010a). 
ISR-WN was selected as the default knowledge 
base because of previous NLP research, which 
included: (Fern?ndez et al, 2012; Guti?rrez et al, 
2010b; Guti?rrez et al, 2012; 2011b; 2011c; 
2011d), which achieved relevant results using ISR-
WN as their knowledge base. 
2 System architecture  
By using one of BabelNet (BN) features, our 
technique begins by looking for all the Babel 
synsets (Bs) linked to the lemma of each word in 
the sentence that we need to disambiguate.  
Through the Bs offsets, we can get its 
corresponding WordNet Synset (WNS), which 
would be retrieved from WordNet (WN) using the 
ISR-WN resource. As a result, for each lemma, we 
have a WordNet Synset List (WNSL) from which 
our Word Sense Disambiguation method obtains 
one WNS as the correct meaning. 
Our WSD method consists of applying a 
modification of the Personalizing PageRank (Ppr) 
algorithm (Agirre and Soroa, 2009), which 
involves the senses frequency. More specifically, 
the key proposal is known as Ppr+Freq (see 
Section 2.3).  
Given a set of WNSLs of WNSL, as words 
window, we applied the Synsets ranking method, 
Ppr+Freq, which ranks in a descending order, the 
Synsets of each lemma according to a calculated 
factor of relevance. The first Synset (WNS) of 
each WNSL (the most relevant) is established as 
the correct one and its associated Babel synset (Bs) 
is also tagged as correct. To determine the Wiki 
Page Titles (WK), we examine the WIKI 
(Wikipedia pages) and WIKIRED (Wikipedia 
pages redirections) in the correct Babel synset 
obtained. 
Figure 1 shows a general description of our 
system that is made up of the following steps: 
I. Obtaining lemmas  
II. Obtaing WN Synset of selected lemmas  
III. Applying Ppr+Freq method  
IV. Assigning Synset, Babel synset and Wiki 
page title 
Note that ISR-WN contains WN as its nucleus. 
This allows linking both resources, BabelNet and 
ISR-WN.
242
 
Figure 1. General process description taking as instance a sentence provided by the trial dataset. 
 
2.1 Obtaining lemmas  
For each input sentence, we extract the labeled 
lemmas. As an example, for the sentence, ?The 
struggle against the drug lords in Colombia will be 
a near thing,? the selected lemmas are: ?struggle,? 
?drug_lord,? ?Colombia?, and ?near_thing.? 
 
Figure 2. Obtaining synset of lemmas. 
 
2.2 Obtaing WN Synset of selected lemmas  
For each lemma obtained in the previous section, 
we look through BabelNet to recover the Bs that 
contains the lemma among its labels. When BSs 
are mapped to WN, we use the ISR-WN resource 
to find the corresponding Synset. Since a lemma 
can appear in a different Bs, it can be mapped with 
several WNS. Thus, we get a Synset list for each 
lemma in the sentence. In case the lemma does not 
have an associated Bs, its list would be empty. An 
example of this step is shown on Figure 2. 
2.3 Applying Ppr+Freq method 
In the above case, Ppr+Freq modifies the ?classic? 
Page Rank approach instead of assigning the same 
weight for each sense of WN in the disambiguation 
graph (??). 
The PageRank (Brin and Page, 1998) 
adaptation, Ppr , which was popularized by (Agirre 
IV . Assigning Synset, Babel Synset and Wiki page title
? The struggle against the drug lords in Colombia will be a near thing .?
struggle drug_lord Colombia near_thing
Wikipedia WordNet BabelNet
ISR-WN
WordNet
(WN)
SUMO
WN-Domain
WN-Affect
SemanticClass eXtended WN3.0
eXtended WN1.7
struggle%1:04:01:: drug_lord%1:18:00:: colombia%1:15:00:: near_thing%1:04:00::
bn:00009079n bn:00028876n bn:00020697n bn:00057109n
-- Drug_Lord Colombia --
I. Obtaing lemmas
II. Obtaining Synset of selected lemmas
III. Applying Ppr+Freq method
WN key
BS
WK
struggle
drug_lord Colombia
near_thing
struggle
bn:00074762n wn:00587514n
bn:00009079n wn:00739796n
bn:00009080n wn:00901980n
drug_lord bn:00028876n wn:09394468n
colombia
bn:00020697n wn:08196765n
bn:02051949n
bn:02530766n
near_thing bn:00057109n wn:00193543n
Sentence lemmas 
Babel synset 
WordNet synset 
243
and Soroa, 2009) in Word Sense Disambiguation 
thematic, and which has obtained relevant results, 
was an inspiration to us in our work. The main idea 
behind this algorithm is that, for each edge 
between ?i and ?j in graph ?, a vote is made from 
?i to ?j. As a result, the relevance of ?j is 
increased. 
On top of that, the vote strength from ? to ? 
depends on ???? relevance. The philosophy behind 
it is that, the more important the vertex is, the more 
strength the voter would have. Thus, PageRank is 
generated by applying a random walkthrough from 
the internal interconnection of ?, where the final 
relevance of ??  represents the random walkthrough 
probability over ?, and ending on ??. 
Ppr+Freq includes the existent semantic and 
frequency patterns of each sense of the word to 
disambiguate while finding a way to connect each 
one of these words in a knowledge base. 
The new graph-based approach of WSD 
generates a graph of disambiguated words for each 
input sentence. For that reason, it is necessary to 
classify the word senses according to the other 
words that compose the context. The general 
method is shown in Figure 3. This method is 
divided into three steps: 
I. Creation of a disambiguation graph 
II. Application of Ppr+Freq in the generated 
graph 
III. Selection of the correct answer 
Creation of a disambiguation graph: In the first 
step, a disambiguation graph is built by means of a 
Breath First Search (BFS) over the ?super? graph 
composed by all the resources integrated into ISR-
WN. The components involved in this process are: 
WordNet, SUMO (Zouaq et al, 2009) WordNet 
Domains (Magnini and Cavaglia, 2000) WordNet 
Affects (Strapparava and Valitutti, 2004) Semantic 
Classes (Izquierdo et al, 2007) and eXtended 
WordNet (XWN) relations (Moldovan and Rus, 
2001). This search aims to recover all senses 
(nodes), domain labels (from WordNet Domain 
and WordNet Affects), SUMO categories, and 
Semantic Classes labels through the shortest path 
between every pair of senses in the WNSL set 
associated with the input sentence. Using ISR-WN 
as the KB, through experimentation, we obtained 
the shortest paths with a length of five edges. For a 
better understanding of this process, see (Guti?rrez, 
2012). 
Application of Ppr+Freq in the generated 
graph: In the second step, we use the weighted 
Personalized PageRank. Here, all the vertices from 
vector ? in ?? are initialized with the value  
1
?
 ; 
where ? is the number of nodes in ??. On the 
other hand, the vertices that represent word senses 
in the analyzed sentence are not initialized with 
this value. Instead, they are initialized with values 
in the range [0?1], which are associated to their 
occurrence frequency in SemCor1 (Corpus and 
sense frequencies knowledge). In the last step, 
after applying the Ppr+Freq algorithm over ??, we 
get a representative vector which contains ISR-WN 
nodes in ?? sorted in a descending order by a 
ranking score computed by this algorithm. For a 
better description, see (Guti?rrez, 2012). 
Selection of the correct answer: As the correct 
sense, we take the highest ranked sense of each 
target word involved in this vector. Note that 
domain labels, SUMO categories, semantic class 
labels, and affect labels are ranked too. They could 
be used in the future to determine relevant 
conceptualizations that would be useful for text 
classification and more. 
In our system, we assume the following 
configuration: dumping factor ? = 0.85 and like in 
(Agirre and Soroa, 2009) we used 30 iterations. A 
detailed explanation about PageRank algorithm 
can be found in (Agirre and Soroa, 2009). 
Table 2 shows an example that analyzes the 
Synset for each word in the sentence and also 
shows how the higher ranked Synsets of the target 
words are selected as the correct ones. For a 
detailed explanation of Ppr+Freq, see (Guti?rrez, 
2012). 
2.4 Assigning Synset, Babel synset and Wiki 
Pages 
In this step, English is handled differently from 
other languages because WordNet Synsets are 
available only for English. The following sections 
explain how we proceed in each case. Once the 
Synsets list is obtained for each lemma in section 
2.3, selecting the correct answer for the lemma is 
all that?s left to do. 
                                                     
1 http://www.cse.unt.edu/~rada/downloads.html 
244
 
Figure 3. General process of WSD with Ppr+Freq. 
2.4.1 English 
Given a lemma, we go through its Synset list from 
beginning to end looking for the first Synset that 
contains a key2 for the lemma. If such Synset 
exists, it is designated as the Synset for the lemma. 
Otherwise, no Synset is assigned. 
As already explained, each Synset in the list is 
connected to a Bs. Therefore, the lemma linked 
with the correct WNS selected in the previous step, 
is chosen as the correct lemma. In case no Synsets 
were designated as the correct ones, we take the 
first Bs in BN, which contains the lemma among 
its labels.  
To determine the Wiki pages titles (WK) we 
examine the WIKIRED and WIKI labels in the 
correct Bs selected in the preceding step. This 
search is restricted only to labels corresponding to 
the analyzed language and discriminating upper 
and lower case letters. Table 2 shows some sample 
results of the WSD process. 
Lemma struggle drug_lord 
WNS 00739796n 09394468n 
WN key struggle%1:04:01:: drug_lord%1:18:00:: 
Bs bn:00009079n bn:00028876n 
WK - Drug_Lord 
Lemma colombia near_thing 
WNS 08196765n 00193543n 
WN key colombia%1:15:00:: near_thing%1:04:00:: 
Bs bn:00020697n bn:00057109n 
WK Colombia - 
Table 2 : Example of English Language. 
                                                     
2A sense_key is the best way to represent a sense in 
semantic tagging or other systems that refer to WordNet 
senses. sense_key?s are independent of WordNet sense 
numbers and synset_offset?s, which vary between versions of 
the database. 
2.4.2 Other languages  
For this scenario, we introduce a change in the first 
step discussed in the previous section. The reason 
is that the Synsets do not contain any keys in any 
other language than English. Thus, the correct 
Synset for the lemma is the first in the Synset list 
for the lemma obtained, as described, in section 
2.3. 
3 Results 
We tested three versions (runs) of the proposed 
approach and evaluated them through a trial 
dataset provided by Task123 of Semeval-2013 
using babelnet-1.0.1. Table 3 shows the result for 
each run. Note that the table results were 
calculated with the traditional WSD recall 
measure, being this measure which has ranked 
WSD systems on mostly Semeval competitions. 
On the other hand, note that our precision and 
recall results are different because the coverage is 
not 100%. See Table 5. 
 English French 
Runs WNS Bs WK Bs WK 
Run1 0.70 0.71 0.77 0.59 0.85 
Run2 0.70 0.71 0.78 0.60 0.85 
Run3 0.69 0.70 0.77 - - 
Table 3 : Results of runs with trial recall values. 
As can be noticed on Table 3, results of different 
versions do not have big differences, but in 
general, Run2 achieves the best results; it?s better 
                                                     
3 http://www.cs.york.ac.uk/semeval-2013/task12 
ISR-WN
footballer#1 | cried#9 | winning#3
footballer | cry | winning
Lemmas
?The footballer cried when winning?
Disambiguation
Graph
(0,9)
Footballer#1
(0,3)
cry#7
(0,4)
cry#9
(0,2)
cry#10
(0,2)
cry#11
(0,2)
cry#12
(0,2)
winning#1
(0,3)
winning#3
Creating GD
Ppr+Freq
Selecting senses
245
than Run1 in the WK with a 78% in English and 
Bs with 60% in French. The best results are in the 
WK in French with a value of 85%. 
Since we can choose to include different 
resources into ISR-WN, it is important to analyze 
how doing so would affect the results. Table 4 
shows comparative results for Run 2 of a trial 
dataset with BabelNet version 1.1.1. 
As can be observed in Table 4, the result does not 
have a significant change even though we used the 
ISR-WN with all resources.  
A better analysis of Ppr+Freq in, as it relates to 
the influence of each resource involved in ISR-WN 
(similar to Table 4 description) assessing 
SensEval-2 and SensEval-3 dataset, is shown in 
(Guti?rrez, 2012). There are different resource 
combinations showing that only XWN1.7 and all 
ISR-WN resources obtain the highest performance. 
Other analysis found in (Guti?rrez, 2012) evaluates 
the influence of adding the sense frequency for 
Ppr+Freq.  
By excluding the Factotum Domain, we obtain 
the best result in Bs 54% for French (only 1% 
more than the version used in the competition). 
The other results are equal, with a 69% in WNS, 
66% in Bs, 64% in WK for English, and 69% in 
WK for French. 
        English French 
WN Domains Sumo Affect Factotum 
Domain 
SemanticClass XWN3.0 XWN1.7 WNS Bs WK Bs WK 
X X X X X X X X 0.69 0.66 0.64 0.53 0.69 
X X  X X X X X 0.69 0.66 0.64 0.53 0.69 
X    X X X X 0.68 0.65 0.64 0.52 0.69 
X X X X  X X X 0.69 0.66 0.64 0.54 0.69 
X X X X  X  X 0.68 0.65 0.65 0.53 0.69 
Table 4. Influence of different resources that integrate ISR-WN in our technique. 
    Wikipedia BabelNet WordNet 
System Language Precision Recall F-score Precision Recall F-score Precision Recall F-score 
MFS DE 0.836 0.827 0.831 0.676 0.673 0.686 - - - 
  EN 0.86 0.753 0.803 0.665 0.665 0.656 0.63 0.63 0.63 
  ES 0.83 0.819 0.824 0.645 0.645 0.644 - - - 
  FR 0.698 0.691 0.694 0.455 0.452 0.501 - - - 
  IT 0.833 0.813 0.823 0.576 0.574 0.572 - - - 
Run1 DE 0.758 0.46 0.572 0.619 0.617 0.618 - - - 
  EN 0.619 0.484 0.543 0.677 0.677 0.677 0.639 0.635 0.637 
  ES 0.773 0.493 0.602 0.708 0.703 0.705 - - - 
  FR 0.817 0.48 0.605 0.608 0.603 0.605 - - - 
  IT 0.785 0.458 0.578 0.659 0.656 0.657 - - - 
Run2 DE 0.769 0.467 0.581 0.622 0.62 0.621 - - - 
  EN 0.62 0.487 0.546 0.685 0.685 0.685 0.649 0.645 0.647 
  ES 0.778 0.502 0.61 0.713 0.708 0.71 - - - 
  FR 0.815 0.478 0.603 0.608 0.603 0.605 - - - 
  IT 0.787 0.463 0.583 0.659 0.657 0.658 - - - 
Run3 EN 0.622 0.489 0.548 0.68 0.68 0.68 0.642 0.639 0.64 
Table 5. Results of Runs for Task12 of semeval-2013 using the test dataset. 
 
246
3.1 Run1 
In this Run, WNSLs consist of all the target words 
involved in each sentence. This run is applied at 
the sentence level. The results for the competition 
are shown in Table 5. For this Run, the best result 
was obtained for Spanish with a 70.3% in Bs and 
49.3% in WK of Recall. As we can see, for Run1 
the precision is high for Wikipedia disambiguation, 
obtaining for French the best result of the ranking. The 
low Recall in Wikipedia is due to the exact mismatching 
of labels between our system output and the gold 
standard. This fact, affects the rest of our runs. 
3.2 Run2 
In this Run, WNSLs consist of all the target words 
involved in each domain. We can obtain the target 
words because the training and test dataset contain 
the sentences grouped by topics.  For instance, for 
English, 13 WNSLs are established. This Run is 
applied at the corpora level. The results for the 
competition are shown in Table 5. It is important to 
emphasize that our best results ranked our 
algorithm as first place among all proposed 
approaches for the MWSD task. 
For this run, the best Recall was obtained for 
Spanish with a 70.8% in Bs and 50.2% in WK. 
This Run also has the best result of the three runs. 
For the English competition, it ended up with a 
64.5% in WNS, 68.5% in Bs, and 48.7% in WK. 
This Run obtained promising results, which took 
first place in the competition. It also had better 
results than that of the First Sense (Most Frequent 
Sense) baseline in Bs results for all languages, 
except for German. In Bs, it only obtained lower 
results in German with a 62% of Recall for our 
system and 67.3% for the First Sense baseline. 
3.3 Run3 
In this run, WNSLs consist of all the words 
included in each sentence. This run uses target 
words and non-target words of each sentence, as 
they are applied to the sentence level. The results 
for the competition are shown in Table 5.  
As we can see, the behavior of this run is similar 
to the previous runs. 
4 Conclusions and Future work  
The above results suggest that our proposal is a 
promising approach. It is also important to notice 
that a richer knowledgebase can be built by 
combining different resources such as BabelNet 
and ISR-WN, which can lead to an improvement 
of the results. Notwithstanding, our system has 
been recognized as the best in the competition, 
obtaining results around 70% of Recall. 
According to the Task12 results4, only the 
baseline Most Frequent Sense (MFS) could 
improve our scores in order to achieve better WK 
and German (DE) disambiguation. Therefore, we 
plan to review this point to figure out why we 
obtained better results in other categories, but not 
for this one. At the same time, further work will 
use the internal Babel network to run the Ppr+Freq 
method in an attempt to find a way to enrich the 
semantic network obtained for each target sentence 
to disambiguate. On top of that, we plan to 
compare Ppr (Agirre and Soroa, 2009) with 
Ppr+Freq using the Task12 dataset. 
Availability of our Resource 
In case researchers would like to use our resource, 
it is available at the GPLSI5 home page or by 
contacting us via email. 
Acknowledgments 
This research work has been partially funded by 
the Spanish Government through the project 
TEXT-MESS 2.0 (TIN2009-13391-C04), "An?lisis 
de Tendencias Mediante T?cnicas de Opini?n 
Sem?ntica" (TIN2012-38536-C03-03) and 
?T?cnicas de Deconstrucci?n en la Tecnolog?as del 
Lenguaje Humano? (TIN2012-31224); and by the 
Valencian Government through the project 
PROMETEO (PROMETEO/2009/199). 
References 
Agirre, E. and A. Soroa. Personalizing PageRank for 
Word Sense Disambiguation. Proceedings of the 12th 
conference of the European chapter of the 
Association for Computational Linguistics (EACL-
2009), Athens, Greece, 2009. 
                                                     
4 http://www.cs.york.ac.uk/semeval-
2013/task12/index.php?id=results 
5 http://gplsi.dlsi.ua.es/ 
247
Fern?ndez, A.; Y. Guti?rrez; H. D?vila; A. Ch?vez; A. 
Gonz?lez; R. Estrada; Y. Casta?eda; S. V?zquez; A. 
Montoyo and R. Mu?oz. UMCC_DLSI: 
Multidimensional Lexical-Semantic Textual 
Similarity. {*SEM 2012}: The First Joint Conference 
on Lexical and Computational Semantics -- Volume 
1: Proceedings of the main conference and the shared 
task, and Volume 2: Proceedings of the Sixth 
International Workshop on Semantic Evaluation 
{(SemEval 2012)}, Montreal, Canada, Association 
for Computational Linguistics, 2012. 608--616 p.  
Guti?rrez, Y. An?lisis Sem?ntico Multidimensional 
aplicado a la Desambiguaci?n del Lenguaje Natural. 
Departamento de Lenguajes y Sistemas Inform?ticos. 
Alicante, Alicante, 2012. 189. p. 
Guti?rrez, Y.; A. Fern?ndez; A. Montoyo and S. 
V?zquez. Integration of semantic resources based on 
WordNet. XXVI Congreso de la Sociedad Espa?ola 
para el Procesamiento del Lenguaje Natural, 
Universidad Polit?cnica de Valencia, Valencia, 
SEPLN 2010, 2010a. 161-168 p. 1135-5948. 
Guti?rrez, Y.; A. Fern?ndez; A. Montoyo and S. 
V?zquez. UMCC-DLSI: Integrative resource for 
disambiguation task. Proceedings of the 5th 
International Workshop on Semantic Evaluation, 
Uppsala, Sweden, Association for Computational 
Linguistics, 2010b. 427-432 p.  
Guti?rrez, Y.; A. Fern?ndez; A. Montoyo and S. 
V?zquez Enriching the Integration of Semantic 
Resources based on WordNet Procesamiento del 
Lenguaje Natural, 2011a, 47: 249-257. 
Guti?rrez, Y.; S. V?zquez and A. Montoyo. Improving 
WSD using ISR-WN with Relevant Semantic Trees 
and SemCor Senses Frequency. Proceedings of the 
International Conference Recent Advances in Natural 
Language Processing 2011, Hissar, Bulgaria, RANLP 
2011 Organising Committee, 2011b. 233--239 p.  
Guti?rrez, Y.; S. V?zquez and A. Montoyo. Sentiment 
Classification Using Semantic Features Extracted 
from WordNet-based Resources. Proceedings of the 
2nd Workshop on Computational Approaches to 
Subjectivity and Sentiment Analysis (WASSA 
2.011), Portland, Oregon., Association for 
Computational Linguistics, 2011c. 139--145 p.  
Guti?rrez, Y.; S. V?zquez and A. Montoyo. Word Sense 
Disambiguation: A Graph-Based Approach Using N-
Cliques Partitioning Technique. en:  Natural 
Language Processing and Information Systems. 
MU?OZ, R.;MONTOYO, A.et al Springer Berlin / 
Heidelberg, 2011d. 6716: 112-124.p.  
Guti?rrez, Y.; S. V?zquez and A. Montoyo. A graph-
Based Approach to WSD Using Relevant Semantic 
Trees and N-Cliques Model. CICLing 2012, New 
Delhi, India, 2012. 225-237 p.  
Izquierdo, R.; A. Su?rez and G. Rigau A Proposal of 
Automatic Selection of Coarse-grained Semantic 
Classes for WSD Procesamiento del Lenguaje 
Natural, 2007, 39: 189-196. 
Laparra, E.; G. Rigau and M. Cuadros. Exploring the 
integration of WordNet and FrameNet. Proceedings 
of the 5th Global WordNet Conference (GWC'10), 
Mumbai, India, 2010.  
Magnini, B. and G. Cavaglia. Integrating Subject Field 
Codes into WordNet. Proceedings of Third 
International Conference on Language Resources and 
Evaluation (LREC-2000), 2000. 1413--1418 p.  
Mihalcea, R. Unsupervised large-vocabulary word sense 
disambiguation with graph-based algorithms for 
sequence data labeling. Proceedings of HLT05, 
Morristown, NJ, USA., 2005.  
Miller, G. A.; R. Beckwith; C. Fellbaum; D. Gross and 
K. Miller. Five papers on WordNet. Princenton 
University, Cognositive Science Laboratory, 1990. 
Moldovan, D. I. and V. Rus Explaining Answers with 
Extended WordNet ACL, 2001. 
Navigli, R.; D. Jurgens and D. Vannella. SemEval-2013 
Task 12: Multilingual Word Sense Disambiguation. . 
Proceedings of the 7th International Workshop on 
Semantic Evaluation (SemEval 2013), in conjunction 
with the Second Joint Conference on Lexical and 
Computational Semantics (*SEM 2013), Atlanta, 
Georgia, 2013.  
Navigli, R. and S. P. Ponzetto. BabelNet: Building a 
Very Large Multilingual Semantic Network. 
Proceedings of the 48th Annual Meeting of the 
Association for Computational Linguistics, Uppsala, 
Sweden, Association for Computational Linguistics, 
2010. 216--225 p.  
Navigli, R. and S. P. Ponzetto BabelNet: The automatic 
construction, evaluation and application of a wide-
coverage multilingual semantic network Artif. Intell., 
2012, 193: 217-250. 
Navigli, R. and P. Velardi Structural Semantic 
Interconnections: A Knowledge-Based Approach to 
Word Sense Disambiguation IEEE Transactions on 
Pattern Analysis and Machine Intelligence, 2005, 
27(7): 1075-1086. 
Sinha, R. and R. Mihalcea. Unsupervised Graph-based 
Word Sense Disambiguation Using Measures of 
Word Semantic Similarity. Proceedings of the IEEE 
International Conference on Semantic Computing 
(ICSC 2007), Irvine, CA, 2007. 
248
Strapparava, C. and A. Valitutti. WordNet-Affect: an 
affective extension of WordNet. Proceedings of the 
4th International Conference on Language Resources 
and Evaluation (LREC 2004), Lisbon, 2004. 1083-
1086 p.  
Tsatsaronis, G.; M. Vazirgiannis and I. 
Androutsopoulos. Word sense disambiguation with 
spreading activation networks generated from 
thesauri. IJCAI, 2007.  
Vossen, P. EuroWordNet: A Multilingual Database with 
Lexical Semantic Networks.  Dordrecht, Kluwer 
Academic Publishers, 1998.  
Zouaq, A.; M. Gagnon and B. Ozell. A SUMO-based 
Semantic Analysis for Knowledge Extraction. 
Proceedings of the 4th Language & Technology 
Conference, Pozna?, Poland, 2009.  
 
 
249
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 443?449, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguistics
UMCC_DLSI-(SA): Using a ranking algorithm and informal features 
to solve Sentiment Analysis in Twitter 
Yoan Guti?rrez, Andy Gonz?lez, 
Roger P?rez, Jos? I. Abreu 
University of Matanzas, Cuba 
{yoan.gutierrez, roger.perez 
,jose.abreu}@umcc.cu, 
andy.gonzalez@infonet.umcc.cu 
Antonio Fern?ndez Orqu?n, 
Alejandro Mosquera, Andr?s 
Montoyo, Rafael Mu?oz 
University of Alicante, Spain 
antonybr@yahoo.com, 
{amosquera, montoyo, 
rafael}@dlsi.ua.es 
Franc Camara 
Independent Consultant 
USA 
info@franccamara
.com 
 
Abstract 
In this paper, we describe the development 
and performance of the supervised system 
UMCC_DLSI-(SA). This system uses corpora 
where phrases are annotated as Positive, 
Negative, Objective, and Neutral, to achieve 
new sentiment resources involving word 
dictionaries with their associated polarity. As 
a result, new sentiment inventories are 
obtained and applied in conjunction with 
detected informal patterns, to tackle the 
challenges posted in Task 2b of the Semeval-
2013 competition. Assessing the effectiveness 
of our application in sentiment classification, 
we obtained a 69% F-Measure for neutral and 
an average of 43% F-Measure for positive 
and negative using Tweets and SMS 
messages. 
1 Introduction 
Textual information has become one of the most 
important sources of data to extract useful and 
heterogeneous knowledge from. Texts can provide 
factual information, such as: descriptions, lists of 
characteristics, or even instructions to opinion-
based information, which would include reviews, 
emotions, or feelings. These facts have motivated 
dealing with the identification and extraction of 
opinions and sentiments in texts that require 
special attention.  
Many researchers, such as (Balahur et al, 2010; 
Hatzivassiloglou et al, 2000; Kim and Hovy, 
2006; Wiebe et al, 2005) and many others have 
been working on this and related areas. 
Related to assessment Sentiment Analysis (SA) 
systems, some international competitions have 
taken place. Some of those include: Semeval-2010 
(Task 18: Disambiguating Sentiment Ambiguous 
Adjectives 1 ) NTCIR (Multilingual Opinion 
Analysis Task (MOAT 2)) TASS 3  (Workshop on 
Sentiment Analysis at SEPLN workshop) and 
Semeval-2013 (Task 2 4  Sentiment Analysis in 
Twitter) (Kozareva et al, 2013). 
In this paper, we introduce a system for Task 2 
b) of the Semeval-2013 competition. 
1.1 Task 2 Description 
In participating in ?Task 2: Sentiment Analysis in 
Twitter? of Semeval-2013, the goal was to take a 
given message and its topic and classify whether it 
had a positive, negative, or neutral sentiment 
towards the topic. For messages conveying, both a 
positive and negative sentiment toward the topic, 
the stronger sentiment of the two would end up as 
the classification. Task 2 included two sub-tasks. 
Our team focused on Task 2 b), which provides 
two training corpora as described in Table 3, and 
two test corpora: 1) sms-test-input-B.tsv (with 
2094 SMS) and 2) twitter-test-input-B.tsv (with 
3813 Twit messages). 
The following section shows some background 
approaches. Subsequently, in section 3, we 
describe the UMCC_DLSI-(SA) system that was 
used in Task 2 b). Section 4 describes the 
assessment of the obtained resource from the 
Sentiment Classification task. Finally, the 
conclusion and future works are presented in 
section 5. 
2 Background 
The use of sentiment resources has proven to be a 
necessary step for training and evaluating  systems 
that implement sentiment analysis, which also 
                                                 
1 http://semeval2.fbk.eu/semeval2.php 
2 http://research.nii.ac.jp/ntcir/ntcir-ws8/meeting/ 
3 http://www.daedalus.es/TASS/ 
4http://www.cs.york.ac.uk/semeval-2013/task2/ 
443
include fine-grained opinion mining (Balahur, 
2011). 
In order to build sentiment resources, several 
studies have been conducted. One of the first is the 
relevant work by (Hu and Liu, 2004) using lexicon 
expansion techniques by adding synonymy and 
antonym relations provided by WordNet 
(Fellbaum, 1998; Miller et al, 1990) Another one 
is the research described by (Hu and Liu, 2004; 
Liu et al, 2005) which obtained an Opinion 
Lexicon compounded by a list of positive and 
negative opinion words or sentiment words for 
English (around 6800 words). 
A similar approach has been used for building 
WordNet-Affect (Strapparava and Valitutti, 2004) 
which expands six basic categories of emotion; 
thus, increasing the lexicon paths in WordNet. 
Nowadays, many sentiment and opinion 
messages are provided by Social Media. To deal 
with the informalities presented in these sources, it 
is necessary to have intermediary systems that 
improve the level of understanding of the 
messages. The following section offers a 
description of this phenomenon and a tool to track 
it. 
2.1 Text normalization 
Several informal features are present in opinions 
extracted from Social Media texts. Some research 
has been conducted in the field of lexical 
normalization for this kind of text. TENOR 
(Mosquera and Moreda, 2012) is a multilingual 
text normalization tool for Web 2.0 texts with an 
aim to transform noisy and informal words into 
their canonical form. That way, they can be easily 
processed by NLP tools and applications. TENOR 
works by identifying out-of-vocabulary (OOV) 
words such as slang, informal lexical variants, 
expressive lengthening, or contractions using a 
dictionary lookup and replacing them by matching 
formal candidates in a word lattice using phonetic 
and lexical edit distances. 
2.2 Construction of our own Sentiment 
Resource  
Having analyzed the examples of SA described in 
section 2, we proposed building our own sentiment 
resource (Guti?rrez et al, 2013) by adding lexical 
and informal patterns to obtain classifiers that can 
deal with Task 2b of Semeval-2013. We proposed 
the use of a method named RA-SR (using Ranking 
Algorithms to build Sentiment Resources) 
(Guti?rrez et al, 2013) to build sentiment word 
inventories based on senti-semantic evidence 
obtained after exploring text with annotated 
sentiment polarity information. Through this 
process, a graph-based algorithm is used to obtain 
auto-balanced values that characterize sentiment 
polarities, a well-known technique in Sentiment 
Analysis. This method consists of three key stages: 
(I) Building contextual word graphs; (II) Applying 
a ranking algorithm; and (III) Adjusting the 
sentiment polarity values. 
These stages are shown in the diagram in Figure 1, 
which the development of sentimental resources 
starts off by giving four corpora of annotated 
sentences (the first with neutral sentences, the 
second with objective sentences, the third with 
positive sentences, and the last with negative 
sentences). 
 
 
Figure 1. Resource walkthrough development 
process. 
2.3 Building contextual word graphs 
Initially, text preprocessing is performed by 
applying a Post-Tagging tool (using Freeling 
(Atserias et al, 2006) tool version 2.2 in this case) 
to convert all words to lemmas 5 . After that, all 
obtained lists of lemmas are sent to RA-SR, then 
divided into four groups: neutral, objective, 
positive, and negative candidates. As the first set 
                                                 
5 Lemma denotes canonic form of the words. 
Phr se 3
Phrase 2
W1 W2 W3 W4
W5 W3 W2
W3 W4 W5 W6
W1
W7
Phrase 1 Positve
Phrases
W5 W6 W8 W9
W8 W9 W7
W6 W9 W10 W11
W6
W1 W8
Negative
Phrases
Phrase 3
Phrase 2
Phrase 1
Positive 
Words
Negative 
Words
W1 W2 W3 W4
W5
W6 W7
W5
W6
W7
W8
W9
W10
W11
(I)
(II) Reinforcing words 
Weight = 1
(II) (II) 
(I)
W ight =1
W ight =1
Weight =1
Weight =1
W1 W2 W3 W4 W5 W6 W7 W8 W9 W10 W11
W1 W2 W3 W4 W5 W6 W7 W8 W9 W10 W11
(III) 
W1
Default Weight = 1/N Default Weight = 1/N
W1 W2 W3
W4W5
Phrase 3
Phrase 2
W1 W2 W3 W4
W5 W3 W2
W3 W1 W2 W4
W1
W5
Phrase 1 Neutral 
Phrases
W1 W6 W7 W8
W8 W7 W3
W6 W8 W7 W5
W5
W5 W2
Objective 
Phrases
Phrase 3
Phrase 2
Phrase 1
(II) 
W1 W2 W3 W4 W5 W6 W7 W8
(II) 
W1 W2 W3
W5
W6 W7
W8
Default Weight = 1/N
(I)(I)
Default Weight = 1/N
444
of results, four contextual graphs are 
obtained:  ????,   ???? , ????,  and ???? , where 
each graph includes the words/lemmas from the 
neutral, objective, positive and negative sentences 
respectively. These graphs are generated after 
connecting all words for each sentence into 
individual sets of annotated sentences in 
concordance with their annotations (??? , ??? , 
???, ??? ). 
Once the four graphs representing neutral, 
objective, positive and negative contexts are 
created, we proceed to assign weights to apply 
graph-based ranking techniques in order to auto-
balance the particular importance of each vertex ?? 
into ????, ????, ???? and ????. 
As the primary output of the graph-based ranking 
process, the positive, negative, neutral, and 
objective values are calculated using the PageRank 
algorithm and normalized with equation (1). For a 
better understanding of how the contextual graph 
was built see (Guti?rrez et al, 2013). 
2.4 Applying a ranking algorithm 
To apply a graph-based ranking process, it is 
necessary to assign weights to the vertices of the 
graph. Words involved into ????, ????, ???? 
and ???? take the default of 1/N as their weight 
to define the weight of ? vector, which is used in 
our proposed ranking algorithm. In the case where 
words are identified on the sentiment repositories 
(see Table 4) as positive or negative, in relation to 
their respective graph, a weight value of 1 (in a 
range [0?1] ) is assigned. ?  represents the 
maximum quantity of words in the current graph. 
After that, a graph-based ranking algorithm is 
applied in order to structurally raise the graph 
vertexes? voting power. Once the reinforcement 
values are applied, the proposed ranking algorithm 
is able to increase the significance of the words 
related to these empowered vertices. 
The PageRank (Brin and Page, 1998) 
adaptation, which was popularized by (Agirre and 
Soroa, 2009) in Word Sense Disambiguation 
thematic, and which has obtained relevant results, 
was an inspiration to us in our work. The main 
idea behind this algorithm is that, for each edge 
between ?i and ?j in graph ?, a vote is made from 
? i to ? j. As a result, the relevance of ? j is 
increased. 
On top of that, the vote strength from ?  to ? 
depends on ???? relevance. The philosophy behind 
it is that, the more important the vertex is, the 
more strength the voter would have. Thus, 
PageRank is generated by applying a random 
walkthrough from the internal interconnection of 
? , where the final relevance of ??  represents the 
random walkthrough probability over ? , and 
ending on ??.  
In our system, we apply the following 
configuration: dumping factor ? = 0.85 and, like 
in (Agirre and Soroa, 2009) we used 30 iterations. 
A detailed explanation about the PageRank 
algorithm can be found in (Agirre and Soroa, 
2009)  
After applying PageRank, in order to obtain 
standardized values for both graphs, we normalize 
the rank values by applying the equation (1), 
where ???(??) obtains the maximum rank value 
of ?? vector (rankings? vector). 
??? = ???/???(??) (1) 
2.5 Adjusting the sentiment polarity values 
After applying the PageRank algorithm on????, 
???? , ????  and ???? , having normalized their 
ranks, we proceed to obtain a final list of lemmas 
(named ?? ) while avoiding repeated elements. 
?? is represented by ???  lemmas, which would 
have, at that time, four assigned values: Neutral, 
Objective, Positive, and Negative, all of which 
correspond to a calculated rank obtained by the 
PageRank algorithm.  
At that point, for each lemma from ??,  the 
following equations are applied in order to select 
the definitive subjectivity polarity for each one: 
??? =  {
??? ? ??? ;  ??? > ???
0                ; ?????????
 (2) 
??? =  {
??? ? ??? ;  ??? > ???
0                ; ?????????
 (3) 
Where ???  is the Positive value and ???  the 
Negative value related to each lemma in ??. 
In order to standardize again the ???  and ??? 
values and making them more representative in a 
[0?1] scale, we proceed to apply a normalization 
process over the ??? and ??? values. 
From there, based on the objective features 
commented by (Baccianella et al, 2010), we 
assume the same premise to establish an 
alternative objective value of the lemmas. 
Equation (4) is used for that: 
?????? = 1 ? |??? ? ???| (4) 
Where ??????  represents the alternative 
objective value. 
445
As a result, each word obtained in the sentiment 
resource has an associated value of: positivity 
(??? , see equation (2)), negativity (??? , see 
equation (3)), objectivity(????_???,  obtained by 
PageRank over ????  and normalized with 
equation (1)), calculated-objectivity (??????, now 
cited as ???_???????? ) and neutrality (??? , 
obtained by PageRank over ???? and normalized 
with equation (1)). 
3  System Description 
The system takes annotated corpora as input from 
which two models are created. One model is 
created by using only the data provided at 
Semeval-2013 (Restricted Corpora, see Table 3), 
and the other by using extra data from other 
annotated corpora (Unrestricted Corpora, see 
Table 3). In all cases, the phrases are pre-
processed using Freeling 2.2 pos-tagger (Atserias 
et al, 2006) while a dataset copy is normalized 
using TENOR (described in section 2.1). 
The system starts by extracting two sets of 
features. The Core Features (see section 3.1) are 
the Sentiment Measures and are calculated for a 
standard and normalized phrase. The Support 
Features (see section 3.2) are based on regularities, 
observed in the training dataset, such as 
emoticons, uppercase words, and so on. 
The supervised models are created using Weka6 
and a Logistic classifier, both of which the system 
uses to predict the values of the test dataset. The 
selection of the classifier was made after analyzing 
several classifiers such as: Support Vector 
Machine, J48 and REPTree. Finally, the Logistic 
classifier proved to be the best by increasing the 
results around three perceptual points. 
The test data is preprocessed in the same way 
the previous corpora were. The same process of 
feature extraction is also applied. With the 
aforementioned features and the generated models, 
the system proceeds to classify the final values of 
Positivity, Negativity, and Neutrality.  
3.1 The Core Features 
The Core Features is a group of measures based on 
the resource created early (see section 2.2). The 
system takes a sentence preprocessed by Freeling 
2.2 and TENOR. For each lemma of the analyzed 
sentence, ??? , ??? , ???_???????? ,  ????_???, 
                                                 
6 http://www.cs.waikato.ac.nz/ 
and ???  are calculated by using the respective 
word values assigned in RA-SR. The obtained 
values correspond to the sum of the corresponding 
values for each intersecting word between the 
analyzed sentence (lemmas list) and the obtained 
resource by RA-SR. Lastly, the aforementioned 
attributes are normalized by dividing them by the 
number of words involved in this process. 
Other calculated attributes are: ???_????? , 
???_????? , ???_????????_????? , 
???_????_????? and ???_?????. These attributes 
count each involved iteration for each feature type 
( ??? , ??? , ????_??? , ??????  and ??? 
respectively, where the respective value may be 
greater than zero. 
Attributes ???  and cnn are calculated by 
counting the amount of lemmas in the phrases 
contained in the Sentiment Lexicons (Positive and 
Negative respectively).  
All of the 12 attributes described previously are 
computed for both, the original, and the 
normalized (using TENOR) phrase, totaling 24 
attributes. The Core features are described next.  
Feature Name Description 
??? 
Sum of respective value of each word. 
??? 
???_???????? 
????_??? 
??? 
???_????? 
Counts the words where its respective value 
is greater than zero 
???_????? 
???_????????_????? 
????_???_????? 
???_????? 
??? (to positive) Counts the words contained in the 
Sentiment Lexicons for their respective 
polarities. 
??? (to negative) 
Table 1. Core Features 
3.2 The Support Features 
The Support Features is a group of measures based 
on characteristics of the phrases, which may help 
with the definition on extreme cases. The emotPos 
and emotNeg values are the amount of Positive 
and Negative Emoticons found in the phrase. The 
exc and itr are the amount of exclamation and 
interrogation signs in the phrase. The following 
table shows the attributes that represent the 
support features: 
Feature Name Description 
??????? 
Counts the respective Emoticons 
??????? 
??? (exclamation marks (?!?)) 
Counts the respective marks 
??? (question marks (???)) 
?????_????? Counts the uppercase words 
?????_??? Sums the respective values of the 
Uppercase words ?????_??? 
?????_???_?????_??? (to Counts the Uppercase words 
446
positivity) contained in their respective 
Graph ?????_???_?????_???(to 
negativity) 
?????_???_?????_???? (to 
positivity) 
Counts the Uppercase words 
contained in the Sentiment 
Lexicons 7 for their respective 
polarity  
?????_???_?????_???? (to 
negativity) 
???????_????? Counts the words with repeated 
chars  
???????_??? Sums the respective values of the 
words with repeated chars ???????_??? 
???????_???_?????_???? (in 
negative lexical resource ) 
Counts the words with repeated 
chars contained in the respective 
lexical resource ???????_???_?????_???? (in 
positive lexical resource ) 
???????_???_?????_??? (in 
positive graph ) 
Counts the words with repeated 
chars contained in the respective 
graph ???????_???_?????_???  (in 
negative graph ) 
Table 2. The Support Features 
4 Evaluation 
In the construction of the sentiment resource, we 
used the annotated sentences provided by the 
corpora described in Table 3. The resources listed 
in Table 3 were selected to test the functionality of 
the words annotation proposal with subjectivity 
and objectivity. Note that the shadowed rows 
correspond to constrained runs corpora: tweeti-b-
sub.dist_out.tsv 8  (dist), b1_tweeti-objorneu-
b.dist_out.tsv 9  (objorneu), twitter-dev-input-
B.tsv10 (dev). 
The resources from Table 3 that include 
unconstrained runs corpora are: all the previously 
mentioned ones, Computational-intelligence11 (CI) 
and stno12 corpora. 
The used sentiment lexicons are from the 
WordNetAffect_Categories13 and opinion-words14 
files as shown in detail in Table 4. 
Some issues were taken into account throughout 
this process. For instance, after obtaining a 
contextual graph ?, factotum words are present in 
most of the involved sentences (i.e., verb ?to be?). 
This issue becomes very dangerous after applying 
the PageRank algorithm because the algorithm 
                                                 
7 Resources described in Table 4. 
8Semeval-2013 (Task 2. Sentiment Analysis in Twitter, 
subtask b). 
9Semeval-2013 (Task 2. Sentiment Analysis in Twitter, 
subtask b). 
10 http://www.cs.york.ac.uk/semeval-2013/task2/ 
11A sentimental corpus obtained applying techniques 
developed by GPLSI department. See 
(http://gplsi.dlsi.ua.es/gplsi11/allresourcespanel) 
12NTCIR Multilingual Opinion Analysis Task (MOAT) 
http://research.nii.ac.jp/ntcir/ntcir-ws8/meeting/ 
13 http://wndomains.fbk.eu/wnaffect.html 
14 http://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html 
strengthens the nodes possessing many linked 
elements. For that reason, the subtractions ??? ?
??? and ??? ? ??? are applied, where the most 
frequent words in all contexts obtain high values. 
The subtraction becomes a dumping factor.  
As an example, when we take the verb ?to be?, 
before applying equation (1), the verb achieves the 
highest values in each subjective context graph 
(????  and ????)  namely, 9.94 and 18.67 rank 
values respectively. These values, once equation 
(1) is applied, are normalized obtaining both 
??? =  1 and ??? =  1 in a range [0...1]. At the 
end, when the following steps are executed 
(Equations (2) and (3)), the verb ?to be? 
achieves ??? = 0 , ??? = 0  and 
therefore  ?????? = 1 . Through this example, it 
seems as though we subjectively discarded words 
that appear frequently in both contexts (Positive 
and Negative). 
Corpus N P O Neu 
Obj 
or Neu 
Unk T 
C UC 
dist 176 368 110 34 - - 688 X X 
objorneu 828 1972 788 1114 1045 - 5747 X X 
dev 340 575 - 739 - - 1654 X X 
CI 6982 6172 - - - - 13154  X 
stno15 1286 660 - 384 - 10000 12330  X 
T 9272 9172 898 1532 1045 10000 31919   
Table 3. Corpora used to apply RA-SR. Positive (P), 
Negative (N), Objective (Obj/O), Unknow (Unk), Total 
(T), Constrained (C), Unconstrained (UC). 
Sources P N T 
WordNet-Affects_Categories 
 (Strapparava and Valitutti, 2004) 
629 907 1536 
opinion-words  
(Hu and Liu, 2004; Liu et al, 2005) 
2006 4783 6789 
Total 2635 5690 8325 
Table 4. Sentiment Lexicons. Positive (P), Negative 
(N) and Total (T). 
   Precision (%) Recall (%) Total (%) 
 C Inc P  N  Neu P N Neu Prec Rec F1 
Run1 8032 1631 80,7 83,8 89,9 90,9 69,5 86,4 84,8 82,3 82,9 
Run2 19101 4671 82,2 77,3 89,4 80,7 81,9 82,3 83,0 81,6 80,4 
Table 5. Training dataset evaluation using cross-
validation (Logistic classifier (using 10 folds)). 
Constrained (Run1), Unconstrained (Run2), Correct(C), 
Incorrect (Inc). 
4.1 The training evaluation 
In order to assess the effectiveness of our trained 
classifiers, we performed some evaluation tests.  
Table 5 shows relevant results obtained after 
applying our system to an environment (specific 
domain). The best results were obtained with the 
                                                 
15 NTCIR Multilingual Opinion Analysis Task (MOAT) 
http://research.nii.ac.jp/ntcir/ntcir-ws8/meeting/ 
447
restricted corpus. The information used to increase 
the knowledge was not balanced or perhaps is of 
poor quality. 
4.2 The test evaluation 
The test dataset evaluation is shown in Table 6, 
where system results are compared with the best 
results in each case. We notice that the constrained 
run is better in almost every aspect. In the few 
cases where it was lower, there was a minimal 
difference. This suggests that the information used 
to increase our Sentiment Resource was 
unbalanced (high difference between quantity of 
tagged types of annotated phrases), or was of poor 
quality. By comparing these results with the ones 
obtained by our system on the test dataset, we 
notice that on the test dataset, the results fell in the 
middle of the effectiveness scores. After seeing 
these results (Table 5 and Table 6), we assumed 
that our system performance is better in a 
controlled environment (or specific domain). To 
make it more realistic, the system must be trained 
with a bigger and more balanced dataset. 
Table 6 shows the results obtained by our 
system while comparing them to the best results of 
Task 2b of Semeval-2013. In Table 5, we can see 
the difference between the best systems. They are 
the ones in bold and underlined as target results.  
These results have a difference of around 20 
percentage points. The grayed out ones correspond 
to our runs. 
      Precision (%) Recall (%) Total 
Runs C Inc P N Neu P N Neu Prec Rec F 1 
1_tw 2082 1731 60,9 46,5 52,8 49,8 41,4 64,1 53,4 51,8 49,3 
1_tw_cnd 2767 1046 81,4 69,7 67,7 66,7 60,4 82,6 72,9 69,9 69,0 
2_tw 2026 1787 58,0 42,2 42,2 52,2 43,9 57,4 47,4 51,2 49,0 
2_tw_ter 2565 1248 71,1 54,6 68,6 74,7 59,4 63,1 64,8 65,7 64,9 
1_sms 1232 862 43,9 46,1 69,5 55,9 31,7 68,9 53,2 52,2 43,4 
1_sms_cnd 1565 529 73,1 55,4 85,2 73,0 75,4 75,3 71,2 74,5 68,5 
2_sms 1023 1071 38,4 31,4 68,3 60,0 38,3 47,8 46,0 48,7 40,7 
2_sms_ava 1433 661 60,9 49,4 81,4 65,9 63,7 71,0 63,9 66,9 59,5 
Table 6. Test dataset evaluation using official scores. 
Corrects(C), Incorrect (Inc). 
Table 6 run descriptions are as follows:  
? UMCC_DLSI_(SA)-B-twitter-constrained 
(1_tw), 
? NRC-Canada-B-twitter-constrained 
(1_tw_cnd),  
? UMCC_DLSI_(SA)-B-twitter-unconstrained 
(2_tw), 
? teragram-B-twitter-unconstrained (2_tw_ter), 
? UMCC_DLSI_(SA)-B-SMS-constrained 
(1_sms), 
? NRC-Canada-B-SMS-constrained 
(1_sms_cnd), UMCC_DLSI_(SA)-B-SMS-
unconstrained (2_sms), 
? AVAYA-B-sms-unconstrained (2_sms_ava). 
As we can see in the training and testing 
evaluation tables, our training stage offered more 
relevant scores than the best scores in Task2b 
(Semaval-2013). This means that we need to 
identify the missed features between both datasets 
(training and testing). 
For that reason, we decided to check how many 
words our system (more concretely, our Sentiment 
Resource) missed. Table 7 shows that our system 
missed around 20% of the words present in the test 
dataset. 
 hits miss miss (%) 
twitter 23807 1591 6,26% 
sms 12416 2564 17,12% 
twitter nonrepeat   2426 863 26,24% 
sms norepeat 1269 322 20,24% 
Table 7. Quantity of words used by our system over 
the test dataset. 
5 Conclusion and further work 
Based on what we have presented, we can say that 
we could develop a system that would be able to 
solve the SA challenge with promising results. The 
presented system has demonstrated election 
performance on a specific domain (see Table 5) 
with results over 80%. Also, note that our system, 
through the SA process, automatically builds 
sentiment resources from annotated corpora.  
For future research, we plan to evaluate RA-SR 
on different corpora. On top of that, we also plan 
to deal with the number of neutral instances and 
finding more words to evaluate the obtained 
sentiment resource. 
Acknowledgments 
This research work has been partially funded by 
the Spanish Government through the project 
TEXT-MESS 2.0 (TIN2009-13391-C04), 
"An?lisis de Tendencias Mediante T?cnicas de 
Opini?n Sem?ntica" (TIN2012-38536-C03-03) 
and ?T?cnicas de Deconstrucci?n en la 
Tecnolog?as del Lenguaje Humano? (TIN2012-
31224); and by the Valencian Government through 
the project PROMETEO 
(PROMETEO/2009/199). 
448
References 
Agirre, E. and A. Soroa. Personalizing PageRank for 
Word Sense Disambiguation. Proceedings of the 
12th conference of the European chapter of the 
Association for Computational Linguistics (EACL-
2009), Athens, Greece, 2009.  
Atserias, J.; B. Casas; E. Comelles; M. Gonz?lez; L. 
Padr? and M. Padr?. FreeLing 1.3: Syntactic and 
semantic services in an opensource NLP library. 
Proceedings of LREC'06, Genoa, Italy, 2006.  
Baccianella, S.; A. Esuli and F. Sebastiani. 
SENTIWORDNET 3.0: An Enhanced Lexical 
Resource for Sentiment Analysis and Opinion 
Mining. 7th Language Resources and Evaluation 
Conference, Valletta, MALTA., 2010. 2200-2204 p.  
Balahur, A. Methods and Resources for Sentiment 
Analysis in Multilingual Documents of Different 
Text Types. Department of Software and Computing 
Systems. Alacant, Univeristy of Alacant, 2011. 299. 
p. 
Balahur, A.; E. Boldrini; A. Montoyo and P. Martinez-
Barco. The OpAL System at NTCIR 8 MOAT. 
Proceedings of NTCIR-8 Workshop Meeting, 
Tokyo, Japan., 2010. 241-245 p.  
Brin, S. and L. Page The anatomy of a large-scale 
hypertextual Web search engine Computer Networks 
and ISDN Systems, 1998, 30(1-7): 107-117. 
Fellbaum, C. WordNet. An Electronic Lexical 
Database.  University of Cambridge, 1998. p. The 
MIT Press.  
Guti?rrez, Y.; A. Gonz?lez; A. F. Orqu?n; A. Montoyo 
and R. Mu?oz. RA-SR: Using a ranking algorithm to 
automatically building resources for subjectivity 
analysis over annotated corpora. 4th Workshop on 
Computational Approaches to Subjectivity, 
Sentiment & Social Media Analysis (WASSA 2013), 
Atlanta, Georgia, 2013.  
Hatzivassiloglou; Vasileios and J. Wiebe. Effects of 
Adjective Orientation and Gradability on Sentence 
Subjectivity. International Conference on 
Computational Linguistics (COLING-2000), 2000.  
Hu, M. and B. Liu. Mining and Summarizing Customer 
Reviews. Proceedings of the ACM SIGKDD 
International Conference on Knowledge Discovery 
and Data Mining (KDD-2004), USA, 2004.  
Kim, S.-M. and E. Hovy. Extracting Opinions, Opinion 
Holders, and Topics Expressed in Online News 
Media Text. In Proceedings of workshop on 
sentiment and subjectivity in text at proceedings of 
the 21st international conference on computational 
linguistics/the 44th annual meeting of the association 
for computational linguistics (COLING/ACL 2006), 
Sydney, Australia, 2006. 1-8 p.  
Kozareva, Z.; P. Nakov; A. Ritter; S. Rosenthal; V. 
Stoyonov and T. Wilson. Sentiment Analysis in 
Twitter. in:  Proceedings of the 7th International 
Workshop on Semantic Evaluation. Association for 
Computation Linguistics, 2013. 
Liu, B.; M. Hu and J. Cheng. Opinion Observer: 
Analyzing and Comparing Opinions on the Web. 
Proceedings of the 14th International World Wide 
Web conference (WWW-2005), Japan, 2005.  
Miller, G. A.; R. Beckwith; C. Fellbaum; D. Gross and 
K. Miller. Five papers on WordNet. Princenton 
University, Cognositive Science Laboratory, 1990. 
Mosquera, A. and P. Moreda. TENOR: A Lexical 
Normalisation Tool for Spanish Web 2.0 Texts. in:  
Text, Speech and Dialogue - 15th International 
Conference (TSD 2012). Springer, 2012. 
Strapparava, C. and A. Valitutti. WordNet-Affect: an 
affective extension of WordNet. Proceedings of the 
4th International Conference on Language Resources 
and Evaluation (LREC 2004), Lisbon, 2004. 1083-
1086 p.  
Wiebe, J.; T. Wilson and C. Cardie. Annotating 
Expressions of Opinions and Emotions in Language. 
Kluwer Academic Publishers, Netherlands, 2005.  
 
  
 
449
Proceedings of the 4th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 94?99,
Atlanta, Georgia, 14 June 2013. c?2013 Association for Computational Linguistics
 
RA-SR: Using a ranking algorithm to automatically building resources 
for subjectivity analysis over annotated corpora 
Yoan Guti?rrez, Andy Gonz?lez 
University of Matanzas, Cuba 
yoan.gutierrez@umcc.cu, 
andy.gonzalez@infonet.umcc.cu 
Antonio Fern?ndez Orqu?n, Andr?s 
Montoyo, Rafael Mu?oz 
University of Alicante, Spain 
antonybr@yahoo.com, {montoyo, 
rafael}@dlsi.ua.es 
 
Abstract 
In this paper we propose a method that 
uses corpora where phrases are annotated 
as Positive, Negative, Objective and 
Neutral, to achieve new sentiment 
resources involving words dictionaries 
with their associated polarity. Our 
method was created to build sentiment 
words inventories based on senti-
semantic evidences obtained after 
exploring text with annotated sentiment 
polarity information. Through this 
process a graph-based algorithm is used 
to obtain auto-balanced values that 
characterize sentiment polarities well 
used on Sentiment Analysis tasks. To 
assessment effectiveness of the obtained 
resource, sentiment classification was 
made, achieving objective instances over 
80%. 
1 Introduction 
In recent years, textual information has become 
one of the most important sources of knowledge 
to extract useful data. Texts can provide factual 
information, such as: descriptions, lists of 
characteristics, or even instructions to opinion-
based information, which would include reviews, 
emotions or feelings. These facts have motivated 
dealing with the identification and extraction of 
opinions and sentiments in texts that require 
special attention. Among most widely used terms 
in Natural Language Processing, in concrete in 
Sentiment Analysis (SA) and Opinion Mining, is 
the subjectivity term proposed by (Wiebe, 1994). 
This author defines it as ?linguistic expression of 
somebody?s opinions, sentiments, emotions, 
evaluations, beliefs and speculations?. Another 
important aspect opposed to subjectivity is the 
objectivity, which constitute a fact expression 
(Balahur, 2011). Other interesting terms also 
proposed by (Wiebe et al, 2005) considers, 
private state, theses terms involve opinions, 
beliefs, thoughts, feelings, emotions, goals, 
evaluations and judgments.  
Many researchers such as (Balahur et al, 2010; 
Hatzivassiloglou et al, 2000; Kim and Hovy, 
2006; Wiebe et al, 2005) and many others have 
been working in this way and related areas. To 
build systems able to lead SA challenges it is 
necessary to achieve sentiment resources 
previously developed. These resources could be 
annotated corpora, affective semantic structures, 
and sentiment dictionaries.  
In this paper we propose a method that uses 
annotated corpora where phrases are annotated as 
Positive, Negative, Objective and Neutral, to 
achieve new resources for subjectivity analysis 
involving words dictionaries with their 
associated polarity.  
The next section shows different sentiment and 
affective resources and their main characteristics. 
After that, our proposal is developed in section 3. 
Section 4, present a new sentiment resource 
obtained after evaluating RA-SR over many 
corpora. Section 5 described the evaluation and 
analysis of the obtained resource, and also an 
assessment of the obtained resource in Sentiment 
Classification task. Finally, conclusion and 
further works are presented in section 6. 
2 Related work 
It is known that the use of sentiment resources 
has proven to be a necessary step for training and 
evaluation for systems implementing sentiment 
analysis, including also fine-grained opinion 
mining (Balahur, 2011). 
Different techniques have been used into 
product reviews to obtain lexicons of subjective 
words with their associated polarity. We can 
study the relevant research promoted by (Hu and 
Liu, 2004) which start with a set of seed 
adjectives (?good? and ?bad?) and reinforce the 
semantic knowledge applying a expanding the 
lexicon with synonymy and antonymy relations 
provided by WordNet (Miller et al, 1990). As 
result of Hu and Liu researches an Opinion 
Lexicon is obtained with around 6800 positive 
94
 and negative English words (Hu and Liu, 2004; 
Liu et al, 2005). 
A similar approach has been used in building 
WordNet-Affect (Strapparava and Valitutti, 
2004). In this case the building method starting 
from a larger of seed affective words set. These 
words are classified according to the six basic 
categories of emotion (joy, sadness, fear, 
surprise, anger and disgust), are also expanded 
increase the lexicon using paths in WordNet. 
Other widely used in SA has been 
SentiWordNet resource (Esuli and Sebastiani, 
2006)). The main idea that encouraged its 
construction has been that ?terms with similar 
glosses in WordNet tend to have similar 
polarity?. 
Another popular lexicon is MicroWNOp 
(Cerini et al, 2007). It contains opinion words 
with their associated polarity. It has been built on 
the basis of a set of terms extracted from the 
General Inquirer1 (Stone et al, 1996).  
The problem is that these resources do not 
consider the context in which the words appear. 
Some methods tried to overcome this critique 
and built sentiment lexicons using the local 
context of words. 
We can mentioned to (Pang et al, 2002) whom 
built a lexicon with associated polarity value, 
starting with a set of classified seed adjectives 
and using conjunctions (?and?) disjunctions 
(?or?, ?but?) to deduce orientation of new words 
in a corpus. 
(Turney, 2002) classifies words according to 
their polarity based on the idea that terms with 
similar orientation tend to co-occur in 
documents.  
On the contrary in (Balahur and Montoyo, 
2008b), is computed the polarity of new words 
using ?polarity anchors? (words whose polarity 
is known beforehand) and Normalized Google 
Distance (Cilibrasi and Vit?nyi, 2007) scores 
using as training examples opinion words 
extracted from ?pros and cons reviews? from the 
same domain. This research achieved the lexical 
resource Emotion Triggers (Balahur and 
Montoyo, 2008a). 
Another approach that uses the polarity of the 
local context for computing word polarity is the 
one presented by (Popescu and Etzioni, 2005), 
who use a weighting function of the words 
around the context to be classified. 
All described resources have been obtained 
manually or semi-automatically. Therefore, we 
                                                 
1
 http://www.wjh.harvard.edu/~inquirer/ 
focus our target in archiving automatically new 
sentiment resources supported over some of 
aforementioned resources. In particular, we will 
offer contributions related with methods to build 
sentiment lexicons using the local context of 
words. 
3 Our method 
We propose a method named RA-SR (using 
Ranking Algorithms to build Sentiment 
Resources) to build sentiment words inventories 
based on senti-semantic evidences obtained after 
exploring text with annotated sentiment polarity 
information. Through this process a graph-based 
algorithm is used to obtain auto-balanced values 
that characterize sentiment polarities widely used 
on Sentiment Analysis tasks. This method 
consists of three main stages: (I) Building 
contextual words graphs; (II) Applying ranking 
algorithm; and (III) Adjusting sentiment polarity 
values. 
 
Figure 1. Resource walkthrough development process. 
These stages are represented in the diagram of 
Figure 1, where the development process begins 
introducing two corpuses of annotated sentences 
with positive and negative sentences 
respectively. Initially, a preprocessing of the text 
is made applying Freeling pos-tagger (Atserias et 
al., 2006) version 2.2 to convert all words to 
lemmas2. After that, all lemmas lists obtained are 
introduced in RA-SR, divided in two groups (i.e. 
positive and negative candidates, ????  and ????).  
3.1 Building contextual words graphs 
Giving two sets of sentences (???? and ????) 
annotated as positive and negative respectively, 
where ????	 = [?????, ? , ?????]  and ????	 =[?????, ? , ?????]	  contains list ?  involving 
words lemmatized by Freeling 2.2 Pos-Tagger 
                                                 
2
 Lemma denotes canonic form of the words. 
Corpora
Phrase 3
Phrase 2
W1 W2 W3 W4
W5 W3 W2
W3 W4 W5 W6
W1
W7
Phrase 1 Positve
Phrases
W5 W6 W8 W9
W8 W9 W7
W6 W9 W10 W11
W6
W1 W8
Negative
Phrases
Phrase 3
Phrase 2
Phrase 1
Positive 
Words
Negative 
Words
W1 W2 W3 W4
W5
W6 W7
W5
W6
W7
W8
W9
W10
W11
(I)
(II) Reinforcing words 
Weight = 1
(II) (II) 
(I)
Weight =1
Weight =1
Weight =1
Weight =1
W1 W2 W3 W4 W5 W6 W7 W8 W9 W10 W11
W1 W2 W3 W4 W5 W6 W7 W8 W9 W10 W11
(III) 
W1
Default Weight = 1/N Default Weight = 1/N
95
 (Atserias et al, 2006), a process to build two 
lexical contextual graphs, ????  and ????  is 
applied. Those sentences are manually annotated 
as positive and negative respectively. These 
graphs involve lemmas from the positive and 
negative sentences respectively. 
A contextual graph ?  is defined as an 
undirected graph ? =	 (?, ?) , where ?  denotes 
the set of vertices and ? the set of edges. Given 
the list ?	 = [?1 	? ??]  a lemma graph is created 
establishing links among all lemmas of each 
sentence, where words involved allow to 
interconnect sentences ??  in ? . As a result 
word/lemma networks ????  and ????  are 
obtained, where ?	 = 	?	 = [?? 	? ??]	  and for 
every edge (?? , ??)?	? being ??, ???	?. Therefore, ?? and ??	 are the same. 
Then, having two graphs, we proceed to 
initialize weight to apply graph-based ranking 
techniques in order to auto-balance the particular 
importance of each ?? into ???? and ????. 
3.2 Applying ranking algorithm 
To apply a graph-based ranking process, it is 
necessary to assign weights to the vertices of the 
graph. Words involved into ???? and ???? take 
the default value 1/N as their weight to define 
the weight of ?  vector, which is used in our 
proposed ranking algorithm. In the case where 
words are identified on the sentiment repositories 
(see Table 2) as positive or negative, in relation 
to their respective graph, a weight value of 1 (in 
a range [0?1] ) is assigned. ?  represents the 
maximum quantity of words in the current graph. 
Thereafter, a graph-based ranking algorithm is 
applied in order to structurally raise the graph 
vertexes? voting power. Once the reinforcement 
values are applied, the proposed ranking 
algorithm is able to increase the significance of 
the words related to these empowered vertices. 
The PageRank (Brin and Page, 1998) 
adaptation, which was popularized by (Agirre 
and Soroa, 2009) in Word Sense Disambiguation 
thematic, and the one that has obtained relevant 
results, was an inspiration to us in this work. The 
main idea behind this algorithm is that, for each 
edge between ?i and ?j in graph ?, a vote is made 
from ?i to ?j. As a result, the relevance of ?j is 
increased. 
On top of that, the vote strength from ?  to ? 
depends on ????  relevance. The philosophy 
behind it is that, the more important the vertex is, 
the more strength the voter would have. Thus, 
PageRank is generated by applying a random 
walkthrough from the internal interconnection of 
?, where the final relevance of ??  represents the 
random walkthrough probability over ? , and 
ending on ??. 
In our system, we apply the following equation 
and configuration:  
 
??	 = 	????	 +	(1 ? ?)? (1) 
Where: ?	 is a probabilistic transition matrix 
?	?	? , being ??,?  = ???	  if a link from ? i to ? j 
exist, in other case zero is assigned; ? is a vector ?	?	1	with values previously described in this 
section; ?? is the probabilistic structural vector 
obtained after a random walkthrough to arrive to 
any vertex; ?	  is a dumping factor with value 
0.85, and like in (Agirre and Soroa, 2009) we 
used 30 iterations. 
A detailed explanation about the PageRank 
algorithm can be found in (Agirre and Soroa, 
2009). 
After applying PageRank, in order to obtain 
standardized values for both graphs, we 
normalize the rank values by applying the 
following equation: 
 
??? = ???/???(??) (2) 
Where ???(??)  obtains the maximum rank 
value of ?? vector. 
3.3 Adjusting sentiment polarity values 
After applying the PageRank algorithm on ???? 
and ???? , and having normalized their ranks, 
we proceed to obtain a final list of lemmas 
(named ?? ) while avoiding repeated elements. ??	is represented by ???  lemmas, which would 
have, at that time, two assigned values: Positive, 
and Negative, which correspond to a calculated 
rank obtained by the PageRank algorithm.  
At that point, for each lemma from ??,  the 
following equations are applied in order to select 
the definitive subjectivity polarity for each one: 
??? = 	 ???? ? ???	; 	??? > ???0																; ????????? ? (3) 
??? = 	 ???? ? ???	; 	??? > ???0																; ????????? ? (4) 
Where ??? is the Positive value and ??? the 
Negative value related to each lemma in ??. 
In order to standardize the ??? and ??? values 
again and making them more representative in a 
[0?1] scale, we proceed to apply a 
normalization process over the ???  and ??? 
values. 
Following and based on the objective features 
commented by (Baccianella et al, 2010), we 
assume their same premise to establish objective 
values of the lemmas. Equation (5) is used to this 
96
 proceeding, where ???  represent the objective 
value. ??? = 1 ? |??? ? ???| (5) 
4 Sentiment Resource obtained 
At the same time we have obtained a ?? where 
each word is represented by ???, ??? and ??? 
values, acquired automatically from annotated 
sentiment corpora. With our proposal we have 
been able to discover new sentiment words in 
concordance of contexts in which the words 
appear. Note that the new obtained resource 
involves all lemmas identified into the annotated 
corpora. ???, ???, and ??? are nominal values 
between range [0? 	1]. 
5 Evaluation 
In the construction of the sentiment resource we 
used the annotated sentences provided from 
corpora described on Table 1. Note that we only 
used the sentences annotated positively and 
negatively. The resources involved into this table 
were a selection made to prove the functionality 
of the words annotation proposal of subjectivity 
and objectivity. 
The sentiment lexicons used were provided 
from WordNetAffect_Categories 3 and opinion-
words4 files and shown in detail in Table 2. 
Corpus Neg Pos Obj Neu Obj 
or Neu Unknow Total 
computational-
intelligence5 6982 6172 - - - - 13154 
tweeti-b-
sub.dist_out.tsv6 176 368 110 34 - - 688 
b1_tweeti-
objorneu-
b.dist_out.tsv6 
828 1972 788 1114 1045 - 5747 
stno7 1286 660 
 
384 - 10000 12330 
Total 9272 9172 898 1532 1045 10000 31919 
Table 1. Corpora used to apply RA-SR. 
Sources Pos Neg Total 
WordNet-Affects_Categories 
(Strapparava and Valitutti, 2004) 
629 907 1536 
opinion-words (Hu and Liu, 2004; Liu 
et al, 2005) 
2006 4783 6789 
Total 2635 5690 8325 
Table 2. Sentiment Lexicons. 
Some issues were taking into account through 
this process. For example, after obtaining a 
                                                 
3
 http://wndomains.fbk.eu/wnaffect.html 
4
 http://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html 
5
 A sentimental corpus obtained applying techniques 
developed by GPLSI department. See 
(http://gplsi.dlsi.ua.es/gplsi11/allresourcespanel) 
6
 Train dataset of Semeval-2013 (Task 2. Sentiment 
Analysis in Twitter, subtask b.) 
7
 Test dataset of NTCIR Multilingual Opinion Analysis 
Task (MOAT) http://research.nii.ac.jp/ntcir/ntcir-
ws8/meeting/ 
contextual graph ? factotum words are present in 
mostly of the involved sentences (i.e. verb ?to 
be?). This aspect is very dangerous after 
applying PageRank algorithm, because this 
algorithm because this algorithm strengthens the 
nodes possessing many linked elements. For that 
reason, the subtractions ??? ? ???  and ??? ????  are applied, where the most frequently 
words in all contexts obtains high values and 
being the subtraction a damping factor.  
Following an example; when we take the verb 
?to be?, before applying equation (2), verb ?to 
be? archives the highest values into each context 
graph (????  and ???? ), 9.94 and 18.67 rank 
values respectively. These values, applying 
equation (2), are normalized obtaining both ???	 = 	1  and ???	 = 	1  in a range [0...1]. 
Finally, when the next steps are executed 
(Equations (3) and (4)) verb ?to be? 
achieves ???	 = 0 , ??? = 0  and 
therefore 	???	 = 1 . Through this example it 
seems as we subjectively discarded words that 
appear frequently in both contexts (Positive and 
Negative contexts). 
Using the corpora from Table 1 we obtain 
25792 sentimentally annotated lemmas with ???, ??? and ???  features. Of them 12420 positive 
and 11999 negative lemmas were discovered, , 
and 1373 words already derived from existing 
lexical resources. 
Another contribution has been the ??? , ??? 
and ???  scores assigned to words of lexical 
inventory, which were used to reinforce the 
contextual graphs in the building process. Those 
words in concordance to our scenario count 842 
Positives and 383 Negatives.  
5.1 Sentiment Resource Applied on 
Sentiment Analysis 
To know if our method offers resources that 
improve the SA state of the art, we propose a 
baseline supported on the sentiment dictionaries, 
and other method (Ranking Sentiment Resource 
(RSR)) supported over our obtained resource. 
The baseline consists on analyzing sentences 
applying Equation (6) and Equation (7). 
?????????? = ?????????????????	 (6) 
?????????? = ?????????????????	 (7) 
Where: ???????? is the total of positive words 
(aligned with the sentiment dictionaries) in the 
sentence; ????????  is the total of negative 
words (aligned with the sentiment dictionaries) 
97
 in the sentence; ?????????  is the total of 
words in the sentence.  
Using these measures over the analyzed 
sentences, for each sentence, we obtain two 
attributes, ?????????? and	??????????; and 
a third attribute (named Classification) 
corresponding to its classification. 
On the other hand, we propose RSR. This SA 
method uses in a different way the Equation (6) 
and Equation (7), and introduces Equation (8).  
?????????? = ?????????????????	 (8) 
Being ???????? the sum of Positive ranking 
values of the sentence words, aligned with the 
obtained resource (??); ????????  the sum of 
Negative ranking values of the sentence words, 
aligned with the obtained resource (?? ); and ???????  the sum of Objective ranking values 
of the sentence words, aligned with the obtained 
resource (??). 
In RSR method we proved with two approach, 
RSR (1/di) and RSR (1-(1/di)). The first approach 
is based on a resource developed using 
PageRank with  ??,? = 1/??   and the other 
approach is using ??,? = 1 ? (1/??) . Table 3 
shows experimentation results. 
The evaluation has been applied over a corpus 
provided by ?Task 2. Sentiment Analysis in 
Twitter, subtask b?, in particular tweeti-b-
sub.dist_out.tsv file. This corpus contains 597 
annotated phrases, of them Positives (314), 
Negatives (155), Objectives (98) or Neutrals 
(30). For our understanding this quantity of 
instances offers a representative perception of 
RA-SR contribution; however we will think to 
evaluate RA-SR over other corpora in further 
researches. 
 
C I R. Pos (%) 
R. Neg 
(%) 
R. Obj 
(%) 
R. 
Neu 
(%) 
Total 
P. 
(%) 
Total 
R. 
(%) 
Baseline 366 231 91.1 51.6 0.0 0.0 48.2 61.3 
RSR(1/di) 416 181 87.3 39.4 80.6 6.7% 67.8 69.7 
RSR(1-(1/di) 469 128 88.5 70.3 81.6 6.7% 76.8 78.6 
Table 3. Logistic function (Cross-validation 10 folds) 
over tweeti-b-sub.dist_out.tsv8 corpus (597 instances). 
Recall (R), Precision (P), Correct (C), Incorrect (I). 
As we can see the baseline only is able to 
dealing with negative and positive instances. Is 
important to remark that our proposal starting up 
knowing only the words used in baseline and is 
able to growing sentiment information to other 
words related to them. We can see this fact on 
                                                 
8
 Semeval-2013 (Task 2. Sentiment Analysis in Twitter, 
subtask b.) 
Table 3, RSR is able to classify objective 
instances over 80% of Recall and the baseline 
does not.  
Other relevant element is the recall difference 
between RSR (1/di) and RSR (1 ? (1/??) . 
Traditionally (1/??) result value has been 
assigned to ? in PageRank algorithm. We have 
demonstrated that in lexical contexts RSR (1-
(1/di)) approach offers a better performance of 
PageRank algorithm, showing recall differences 
around 10 perceptual points. 
6 Conclusion and further works 
As a conclusion we can say that our proposal is 
able to automatically increase sentiment 
information, obtaining 25792 sentimentally 
annotated lemmas with ??? , ???  and ??? 
features. Of them 12420 positive and 11999 
negative lemmas were discovered. 
In other hand, The RSR is capable to classify 
objective instances over 80% and negatives over 
70%. We cannot tackle efficiently neutral 
instances, perhaps it is due to the lack of neutral 
information in the sentiment resource we used. 
Also, it could be due to the low quantity of 
neutral instances in the evaluated corpus. 
In further research we will evaluate RA-SR 
over different corpora, and we are also going to 
deal with the number of neutral instances. 
The variant RSR (1 ? (1/??)  performs better 
than RSR(1/??) one. This demonstrates that in 
lexical contexts using PageRank with ??,? = 1 ?(1/??) offers a better performance. Other further 
work consists in exploring Social Medias to 
expand our retrieved sentiment resource 
obtaining real time evidences that occur in Web 
2.0. 
Acknowledgments 
This research work has been partially funded by 
the Spanish Government through the project 
TEXT-MESS 2.0 (TIN2009-13391-C04), 
"An?lisis de Tendencias Mediante T?cnicas de 
Opini?n Sem?ntica" (TIN2012-38536-C03-03) 
and ?T?cnicas de Deconstrucci?n en la 
Tecnolog?as del Lenguaje Humano? (TIN2012-
31224); and by the Valencian Government 
through the project PROMETEO 
(PROMETEO/2009/199). 
 
References 
Agirre, E. and A. Soroa. Personalizing PageRank for 
Word Sense Disambiguation. Proceedings of the 
12th conference of the European chapter of the 
98
 Association for Computational Linguistics (EACL-
2009), Athens, Greece, 2009. p.  
Atserias, J.; B. Casas; E. Comelles; M. Gonz?lez; L. 
Padr? and M. Padr?. FreeLing 1.3: Syntactic and 
semantic services in an opensource NLP library. 
Proceedings of LREC'06, Genoa, Italy, 2006. p.  
Baccianella, S.; A. Esuli and F. Sebastiani. 
SENTIWORDNET 3.0: An Enhanced Lexical 
Resource for Sentiment Analysis and Opinion 
Mining. 7th Language Resources and Evaluation 
Conference, Valletta, MALTA., 2010. 2200-2204 
p.  
Balahur, A. Methods and Resources for Sentiment 
Analysis in Multilingual Documents of Different 
Text Types. Department of Software and 
Computing Systems. Alacant, Univeristy of 
Alacant, 2011. 299. p. 
Balahur, A.; E. Boldrini; A. Montoyo and P. 
Martinez-Barco. The OpAL System at NTCIR 8 
MOAT. Proceedings of NTCIR-8 Workshop 
Meeting, Tokyo, Japan., 2010. 241-245 p.  
Balahur, A. and A. Montoyo. Applying a culture 
dependent emotion trigger database for text 
valence and emotion classification. Procesamiento 
del Lenguaje Natural, 2008a. p.  
Balahur, A. and A. Montoyo. Building a 
recommender system using community level social 
filtering. 5th International Workshop on Natural 
Language and Cognitive Science (NLPCS), 2008b. 
32-41 p.  
Brin, S. and L. Page The anatomy of a large-scale 
hypertextual Web search engine Computer 
Networks and ISDN Systems, 1998, 30(1-7): 107-
117. 
Cerini, S.; V. Compagnoni; A. Demontis; M. 
Formentelli and G. Gandini Language resources 
and linguistic theory: Typology, second language 
acquisition, English linguistics (Forthcoming), 
chapter Micro-WNOp: A gold standard for the 
evaluation of automatically compiled lexical 
resources for opinion mining., 2007. 
Cilibrasi, R. L. and P. M. B. Vit?nyi The Google 
Similarity Distance IEEE TRANSACTIONS ON 
KNOWLEDGE AND DATA ENGINEERING, 
2007, VOL. 19, NO 3. 
Esuli, A. and F. Sebastiani. SentiWordNet: A Publicly 
Available Lexical Resource for Opinion Mining. 
Fifth international conference on Languaje 
Resources and Evaluation Genoa - ITaly., 2006. 
417-422 p.  
Hatzivassiloglou; Vasileios and J. Wiebe. Effects of 
Adjective Orientation and Gradability on Sentence 
Subjectivity. International Conference on 
Computational Linguistics (COLING-2000), 2000. 
p.  
Hu, M. and B. Liu. Mining and Summarizing 
Customer Reviews. Proceedings of the ACM 
SIGKDD International Conference on Knowledge 
Discovery and Data Mining (KDD-2004), USA, 
2004. p.  
Kim, S.-M. and E. Hovy. Extracting Opinions, 
Opinion Holders, and Topics Expressed in Online 
News Media Text. In Proceedings of workshop on 
sentiment and subjectivity in text at proceedings of 
the 21st international conference on computational 
linguistics/the 44th annual meeting of the 
association for computational linguistics 
(COLING/ACL 2006), Sydney, Australia, 2006. 1-
8 p.  
Liu, B.; M. Hu and J. Cheng. Opinion Observer: 
Analyzing and Comparing Opinions on the Web. 
Proceedings of the 14th International World Wide 
Web conference (WWW-2005), Japan, 2005. p.  
Miller, G. A.; R. Beckwith; C. Fellbaum; D. Gross 
and K. Miller Introduction to WordNet: An On-line 
Lexical Database International Journal of 
Lexicography, 3(4):235-244., 1990. 
Pang, B.; L. Lee and S. Vaithyanathan. Thumbs up? 
Sentiment Classification using machine learning 
techniquies. EMNLP -02, the Conference on 
Empirical Methods in Natural Language 
Processing, USA, 2002. 79-86 p.  
Popescu, A. M. and O. Etzioni. Extracting product 
features and opinions from reviews. Proccedings of 
HLT-EMNLP, Canada, 2005. p.  
Stone, P.; D. C.Dumphy; M. S. Smith and D. M. 
Ogilvie The General Inquirer: A Computer 
Approach to Content Analysis The MIT Press, 
1996. 
Strapparava, C. and A. Valitutti. WordNet-Affect: an 
affective extension of WordNet. Proceedings of the 
4th International Conference on Language 
Resources and Evaluation (LREC 2004), Lisbon, 
2004. 1083-1086 p.  
Turney, P. D. Thumbs up or thumbs down? Semantic 
orientation applied to unsupervised classification 
of reviews. Proceeding 40th Annual Meeting of the 
Association for Computational Linguistic. ACL 
2002, USA, 2002. 417-424 p.  
Wiebe, J. Tracking point of view in narrative 
Computational Linguistic, 1994, 20(2): 233-287. 
Wiebe, J.; T. Wilson and C. Cardie. Annotating 
Expressions of Opinions and Emotions in 
Language. Kluwer Academic Publishers, 
Netherlands, 2005. p.  
99
