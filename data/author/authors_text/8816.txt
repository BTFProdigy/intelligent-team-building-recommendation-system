Proceedings of the ACL Interactive Poster and Demonstration Sessions,
pages 45?48, Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
Multimodal Generation in the COMIC Dialogue System
Mary Ellen Foster and Michael White
Institute for Communicating and Collaborative Systems
School of Informatics, University of Edinburgh
{M.E.Foster,Michael.White}@ed.ac.uk
Andrea Setzer and Roberta Catizone
Natural Language Processing Group
Department of Computer Science, University of Sheffield
{A.Setzer,R.Catizone}@dcs.shef.ac.uk
Abstract
We describe how context-sensitive, user-
tailored output is specified and produced
in the COMIC multimodal dialogue sys-
tem. At the conference, we will demon-
strate the user-adapted features of the dia-
logue manager and text planner.
1 Introduction
COMIC1 is an EU IST 5th Framework project com-
bining fundamental research on human-human inter-
action with advanced technology development for
multimodal conversational systems. The project
demonstrator system adds a dialogue interface to a
CAD-like application used in bathroom sales situa-
tions to help clients redesign their rooms. The input
to the system includes speech, handwriting, and pen
gestures; the output combines synthesised speech, a
talking head, and control of the underlying applica-
tion. Figure 1 shows screen shots of the COMIC
interface.
There are four main phases in the demonstra-
tor. First, the user specifies the shape of their
own bathroom, using a combination of speech in-
put, pen-gesture recognition and handwriting recog-
nition. Next, the user chooses a layout for the sani-
tary ware in the room. After that, the system guides
the user in browsing through a range of tiling op-
tions for the bathroom. Finally, the user is given a
1COnversational Multimodal Interaction with Computers;
http://www.hcrc.ed.ac.uk/comic/.
three-dimensional walkthrough of the finished bath-
room. We will focus on how context-sensitive, user-
tailored output is generated in the third, guided-
browsing phase of the interaction. Figure 2 shows
a typical user request and response from COMIC in
this phase. The pitch accents and multimodal ac-
tions are indicated; there is also facial emphasis cor-
responding to the accented words.
The primary goal of COMIC?s guided-browsing
phase is to help users become better informed about
the range of tiling options for their bathroom. In
this regard, it is similar to the web-based system
M-PIRO (Isard et al, 2003), which generates per-
sonalised descriptions of museum objects, and con-
trasts with task-oriented embodied dialogue systems
such as SmartKom (Wahlster, 2003). Since guided
browsing requires extended descriptions, in COMIC
we have placed greater emphasis on producing high-
quality adaptive output than have previous embodied
dialogue projects such as August (Gustafson et al,
1999) and Rea (Cassell et al, 1999). To generate
its adaptive output, COMIC uses information from
the dialogue history and the user model throughout
the generation process, as in FLIGHTS (Moore et
al., 2004); both systems build upon earlier work on
adaptive content planning (Carenini, 2000; Walker
et al, 2002). An experimental study (Foster and
White, 2005) has shown that this adaptation is per-
ceptible to users of COMIC.
2 Dialogue Management
The task of the Dialogue and Action Manager
(DAM) is to decide what the system will show and
say in response to user input. The input to the
45
(a) Bathroom-design application (b) Talking head
Figure 1: Components of the COMIC interface
User Tell me about this design [click on Alt Mettlach]
COMIC [Look at screen]
THIS DESIGN is in the CLASSIC style.
[circle tiles]
As you can see, the colours are DARK RED and OFF WHITE.
[point at tiles]
The tiles are from the ALT METTLACH collection by VILLEROY AND BOCH.
[point at design name]
Figure 2: Sample COMIC input and output
DAM consists of multiple scored hypotheses con-
taining high-level, modality-independent specifica-
tions of the user input; the output is a similar high-
level specification of the system action. The DAM
itself is modality-independent. For example, the in-
put in Figure 2 could equally well have been the user
simply pointing to a design on the screen, with no
speech at all. This would have resulted in the same
abstract DAM input, and thus in the same output: a
request to show and describe the given design.
The COMIC DAM (Catizone et al, 2003) is
a general-purpose dialogue manager which can
handle different dialogue management styles such
as system-driven, user-driven or mixed-initiative.
The general-purpose part of the DAM is a sim-
ple stack architecture with a control structure;
all the application-dependent information is stored
in a variation of Augmented Transition Networks
(ATNs) called Dialogue Action Forms (DAFs).
These DAFs represent general dialogue moves, as
well as sub-tasks or topics, and are pushed onto and
popped off of the stack as the dialogue proceeds.
When processing a user input, the control struc-
ture decides whether the DAM can stay within the
current topic (and thus the current DAF), or whether
a topic shift has occurred. In the latter case, a new
DAF is pushed onto the stack and executed. After
that topic has been exhausted, the DAM returns to
the previous topic automatically. The same princi-
ple holds for error handling, which is implemented
at different levels in our approach.
In the guided-browsing phase of the COMIC sys-
tem, the user may browse tiling designs by colour,
style or manufacturer, look at designs in detail, or
change the amount of border and decoration tiles.
The DAM uses the system ontology to retrieve de-
signs according to the chosen feature, and consults
the user model and dialogue history to narrow down
the resulting designs to a small set to be shown and
described to the user.
46
3 Presentation Planning
The COMIC fission module processes high-level
system-output specifications generated by the DAM.
For the example in Figure 2, the DAM output indi-
cates that the given tile design should be shown and
described, and that the description must mention the
style. The fission module fleshes out such specifica-
tions by selecting and structuring content, planning
the surface form of the text to realise that content,
choosing multimodal behaviours to accompany the
text, and controlling the output of the whole sched-
ule. In this section, we describe the planning pro-
cess; output coordination is dealt with in Section 6.
Full technical details of the fission module are given
in (Foster, 2005).
To create the textual content of a description, the
fission module proceeds as follows. First, it gath-
ers all of the properties of the specified design from
the system ontology. Next, it selects the properties
to include in the description, using information from
the dialogue history and the user model, along with
any properties specifically requested by the dialogue
manager. It then creates a structure for the selected
properties and creates logical forms as input for the
OpenCCG surface realiser. The logical forms may
include explicit alternatives in cases where there are
multiple ways of expressing a property; for exam-
ple, it could say either This design is in the classic
style or This design is classic. OpenCCG makes use
of statistical language models to choose among such
alternatives. This process is described in detail in
(Foster and White, 2004; Foster and White, 2005).
In addition to text, the output of COMIC
also incorporates multimodal behaviours including
prosodic specifications for the speech synthesiser
(pitch accents and boundary tones), facial behaviour
specifications (expressions and gaze shifts), and de-
ictic gestures at objects on the application screen us-
ing a simulated pointer. Pitch accents and bound-
ary tones are selected by the realiser based on the
context-sensitive information-structure annotations
(theme/rheme; marked/unmarked) included in the
logical forms. At the moment, the other multimodal
coarticulations are specified directly by the fission
module, but we are currently experimenting with
using the OpenCCG realiser?s language models to
choose them, using example-driven techniques.
4 Surface Realisation
Surface realisation in COMIC is performed by the
OpenCCG2 realiser, a practical, open-source realiser
based on Combinatory Categorial Grammar (CCG)
(Steedman, 2000b). It employs a novel ensemble of
methods for improving the efficiency of CCG reali-
sation, and in particular, makes integrated use of n-
gram scoring of possible realisations in its chart re-
alisation algorithm (White, 2004; White, 2005). The
n-gram scoring allows the realiser to work in ?any-
time? mode?able at any time to return the highest-
scoring complete realisation?and ensures that a
good realisation can be found reasonably quickly
even when the number of possibilities is exponen-
tial. This makes it particularly suited for use in an
interactive dialogue system such as COMIC.
In COMIC, the OpenCCG realiser uses factored
language models (Bilmes and Kirchhoff, 2003) over
words and multimodal coarticulations to select the
highest-scoring realisation licensed by the grammar
that satisfies the specification given by the fission
module. Steedman?s (Steedman, 2000a) theory of
information structure and intonation is used to con-
strain the choice of pitch accents and boundary tones
for the speech synthesiser.
5 Speech Synthesis
The COMIC speech-synthesis module is imple-
mented as a client to the Festival speech-synthesis
system.3 We take advantage of recent advances in
version 2 of Festival (Clark et al, 2004) by using
a custom-built unit-selection voice with support for
APML prosodic annotation (de Carolis et al, 2004).
Experiments have shown that synthesised speech
with contextually appropriate prosodic features can
be perceptibly more natural (Baker et al, 2004).
Because the fission module needs the timing in-
formation from the speech synthesiser to finalise the
schedules for the other modalities, the synthesiser
first prepares and stores the waveform for its input
text; the sound is then played at a later time, when
the fission module indicates that it is required.
2http://openccg.sourceforge.net/
3http://www.cstr.ed.ac.uk/projects/festival/
47
6 Output Coordination
In addition to planning the presentation content as
described earlier, the fission module also controls
the system output to ensure that all parts of the pre-
sentation are properly coordinated, using the tim-
ing information returned by the speech synthesiser
to create a full schedule for the turn to be generated.
As described in (Foster, 2005), the fission module
allows multiple segments to be prepared in advance,
even while the preceding segments are being played.
This serves to minimise the output delay, as there is
no need to wait until a whole turn is fully prepared
before output begins, and the time taken to speak the
earlier parts of the turn can also be used to prepare
the later parts.
7 Acknowledgements
This work was supported by the COMIC project
(IST-2001-32311). This paper describes only part
of the work done in the project; please see http://
www.hcrc.ed.ac.uk/comic/ for full details. We
thank the other members of COMIC for their col-
laboration during the course of the project.
References
Rachel Baker, Robert A.J. Clark, and Michael White.
2004. Synthesizing contextually appropriate intona-
tion in limited domains. In Proceedings of 5th ISCA
workshop on speech synthesis.
Jeff Bilmes and Katrin Kirchhoff. 2003. Factored lan-
guage models and general parallelized backoff. In
Proceedings of HLT-03.
Giuseppe Carenini. 2000. Generating and Evaluating
Evaluative Arguments. Ph.D. thesis, Intelligent Sys-
tems Program, University of Pittsburgh.
Justine Cassell, Timothy Bickmore, Mark Billinghurst,
Lee Campbell, Kenny Chang, Hannes Vilhja?lmsson,
and Hao Yan. 1999. Embodiment in conversational
interfaces: Rea. In Proceedings of CHI99.
Roberta Catizone, Andrea Setzer, and Yorick Wilks.
2003. Multimodal dialogue management in the
COMIC project. In Proceedings of EACL 2003 Work-
shop on Dialogue Systems: Interaction, adaptation,
and styles of management.
Robert A.J. Clark, Korin Richmond, and Simon King.
2004. Festival 2 ? build your own general purpose
unit selection speech synthesiser. In Proceedings of
5th ISCA workshop on speech synthesis.
Berardina de Carolis, Catherine Pelachaud, Isabella
Poggi, and Mark Steedman. 2004. APML, a
mark-up language for believable behaviour generation.
In H Prendinger, editor, Life-like Characters, Tools,
Affective Functions and Applications, pages 65?85.
Springer.
Mary Ellen Foster and Michael White. 2004. Tech-
niques for text planning with XSLT. In Proceedings
of NLPXML-2004.
Mary Ellen Foster and Michael White. 2005. Assessing
the impact of adaptive generation in the COMIC multi-
modal dialogue system. In Proceedings of IJCAI-2005
Workshop on Knowledge and Reasoning in Practical
Dialogue Systems. To appear.
Mary Ellen Foster. 2005. Interleaved planning and out-
put in the COMIC fission module. Submitted.
Joakim Gustafson, Nikolaj Lindberg, and Magnus Lun-
deberg. 1999. The August spoken dialogue system.
In Proceedings of Eurospeech 1999.
Amy Isard, Jon Oberlander, Ion Androtsopoulos, and
Colin Matheson. 2003. Speaking the users? lan-
guages. IEEE Intelligent Systems, 18(1):40?45.
Johanna Moore, Mary Ellen Foster, Oliver Lemon, and
Michael White. 2004. Generating tailored, compara-
tive descriptions in spoken dialogue. In Proceedings
of FLAIRS 2004.
Mark Steedman. 2000a. Information structure and
the syntax-phonology interface. Linguistic Inquiry,
31(4):649?689.
Mark Steedman. 2000b. The Syntactic Process. MIT
Press.
Wolfgang Wahlster. 2003. SmartKom: Symmetric mul-
timodality in an adaptive and reusable dialogue shell.
In Proceedings of the Human Computer Interaction
Status Conference 2003.
M.A. Walker, S. Whittaker, A. Stent, P. Maloor, J.D.
Moore, M. Johnston, and G. Vasireddy. 2002. Speech-
plans: Generating evaluative responses in spoken dia-
logue. In Proceedings of INLG 2002.
Michael White. 2004. Reining in CCG chart realization.
In Proceedings of INLG 2004.
Michael White. 2005. Efficient realization of coordinate
structures in Combinatory Categorial Grammar. Re-
search on Language and Computation. To appear.
48
Techniques for Text Planning with XSLT
Mary Ellen Foster and Michael White
Institute for Communicating and Collaborative Systems
School of Informatics, University of Edinburgh
Edinburgh EH8 9LW
{mef,mwhite}@inf.ed.ac.uk
Abstract
We describe an approach to text planning that uses
the XSLT template-processing engine to create log-
ical forms for an external surface realizer. Using a
realizer that can process logical forms with embed-
ded alternatives provides a substitute for backtrack-
ing in the text-planning process. This allows the text
planner to combine the strengths of the AI-planning
and template-based traditions in natural language
generation.
1 Introduction
In the traditional pipeline view of natural language
generation (Reiter and Dale, 2000), many steps in-
volve converting between increasingly specific tree
representations. As Wilcock (2001) points out,
this sort of tree-to-tree transformation is a task
to which XML?and particularly XSLT template
processing?is particularly suited.
In this paper, we describe how we plan text by
treating the XSLT processor as a top-down rule-
expanding planner that translates dialogue-manager
specifications into logical forms to be sent to the
OpenCCG text realizer (White and Baldridge, 2003;
White, 2004a; White, 2004b). XSLT is used to per-
form many text-planning tasks, including structur-
ing and aggregating the content, performing lexical
choice via the selection of logical-form templates,
and generating multiple alternative realizations for
messages where possible.
Using an external realizer at the end of the plan-
ning process provides two advantages. First, we
can use the realizer to deal with those aspects of
surface realization that are difficult to implement
in XSLT, but that the realizer is designed to han-
dle (e.g., syntactic agreement via unification). Sec-
ond, we take advantage of OpenCCG?s use of statis-
tical language models by sending multiple alterna-
tive logical forms to the realizer, and having it make
the final choice of surface form. Allowing the text
planner to produce multiple alternatives also obvi-
ates the need for backtracking, which is not some-
thing that is otherwise easily incorporated into the a
system based on XSLT processing.
We have implemented this approach in two di-
alogue systems. In this paper, we concentrate on
how text is planned in the COMIC multimodal di-
alogue system (den Os and Boves, 2003). Similar
techniques are also used in the FLIGHTS spoken-
dialogue system (Moore et al, 2004), which gener-
ates user-tailored descriptions and comparisons of
flight itineraries.
The rest of this paper is organized as follows:
Section 2 gives an overview of the COMIC dia-
logue system and the OpenCCG text realizer. Sec-
tion 3 then shows how the COMIC text planner
generates logical forms for the realizer from high-
level dialogue-manager specifications. Section 4
describes how the interface between the text planner
and the realizer allows us to send multiple alterna-
tive logical forms, and shows the advantages of this
approach. Section 5 discusses related work, while
Section 6 outlines the future plans for this work and
gives some conclusions.
2 Systems
2.1 COMIC
COMIC1 (den Os and Boves, 2003) is an ongo-
ing project investigating multimodal dialogue sys-
tems. The demonstrator adds a dialogue interface
to a CAD-like application used in bathroom sales
situations to help clients redesign their rooms. The
input to the system includes speech, handwriting,
and pen gestures; the output combines synthesized
speech, a ?talking head? avatar, and control of the
underlying application. Figure 1 shows screen shots
of the avatar and the bathroom-design application.
COMIC produces a variety of output, using its
full range of modalities. In this paper, we will
concentrate on the textual content of those turns in
which the system describes one or more options for
1COnversational Multimodal Interaction with Computers;
http://www.hcrc.ed.ac.uk/comic/.
(a) Avatar (b) Bathroom-design application
Figure 1: Components of the COMIC demonstrator
decorating the user?s bathroom, as in the following
description of a set of tiles:
(1) Here is a country design. It uses tiles from
Coem?s Armonie series. The tiles are terra-
cotta and beige, giving the room the feeling of
a Tuscan country home. There are floral motifs
on the decorative tiles.
2.2 OpenCCG
The OpenCCG realizer (White and Baldridge,
2003) is a practical, open-source realizer based on
Combinatory Categorial Grammar (CCG; Steed-
man, 2000). It employs a novel ensemble of meth-
ods for improving the efficiency of CCG realization,
and in particular, makes integrated use of n-gram
scoring of possible realizations in its chart realiza-
tion algorithm (White, 2004a; White, 2004b). The
n-gram scoring allows the realizer to work in ?any-
time? mode?able at any time to return the highest-
scoring complete realization?and ensures that a
good realization can be found reasonably quickly
even when the number of possibilities is exponen-
tial.
Like other realizers, the OpenCCG realizer is par-
tially responsible for determining word order and
inflection. For example, the realizer determines that
also should preferably follow the verb in There are
also floral motifs on the decorative tiles, whereas in
other cases it typically precedes the verb, as in It
also has abstract shapes. It also enforces subject-
verb agreement, e.g., between are and motifs, and
it and has, respectively. Less typically, in COMIC
and FLIGHTS, the OpenCCG realizer additionally
determines the type of pitch accents, and the type
and placement of boundary tones, based on the in-
formation structure of its input logical forms.
3 Text Planning in COMIC
Broadly speaking, text planning in COMIC follows
the standard pipeline model of natural language
generation (Reiter and Dale, 2000). The input to
the COMIC text planner, from the dialogue man-
ager, specifies the content of the description at a
high level; the output consists of logical forms for
the OpenCCG realizer.
The module is implemented in Java and uses
Apache Xalan2 to process the XSLT templates. The
initial implementation of the presentation-planning
module?of which the XSLT-based sentence plan-
ner described here is just a part?took approxi-
mately one month. After that, the module was de-
bugged and updated incrementally over a period
of several months, during which time additional
templates were created to support updates in the
OpenCCG grammar. The development process was
made easier by the ability to use OpenCCG to parse
a target sentence, and then base a template on the
resulting logical form.
The current presentation planner uses 14 tem-
plates for content structuring and aggregation (Sec-
tion 3.2), and just over 100 to build the logical forms
(Section 3.3). The tasks described here take lit-
tle time to perform (i.e., hundreds of milliseconds);
2http://xml.apache.org/xalan-j/
<rdf:Description rdf:about="#Tileset9">
<rdf:type>
<daml:Class rdf:about="#Tileset"/>
</rdf:type>
<comic:has_id>
<xsd:string xsd:value="9"/>
</comic:has_id>
<comic:has_commentary
rdf:resource="#Commentary9"/>
<comic:has_decoration>
<xsd:string xsd:value="floral-motifs"/>
</comic:has_decoration>
<comic:has_series>
<xsd:string xsd:value="Armonie"/>
</comic:has_series>
<comic:has_manufacturer>
<xsd:string xsd:value="Coem"/>
</comic:has_manufacturer>
<comic:has_colour rdf:resource="#Terracotta"/>
<comic:has_colour rdf:resource="#Beige"/>
<comic:has_style rdf:resource="#Country"/>
</rdf:Description>
Figure 2: Ontology properties of tileset 9
<object type="describe">
<slot name="has_object">
<object type="Tileset">
<slot name="has_id">
<value type="String">9</value>
</slot>
</object>
</slot>
<slot name="has_feature">
<value type="String">has_colour</value>
</slot>
</object>
Figure 3: Dialogue-manager specification
most of the module?s time is spent communicating
with other modules in the system.
3.1 Content Selection
The features of the available designs are stored
in the system ontology. This is represented in
DAML+OIL (soon to be OWL) and includes tile
properties such as style, colour, and decoration.
There is also canned-text commentary associated
with some features (e.g., the Tuscan country home
text in (1)). The ontology instance corresponding to
design (?tileset?) 9 is shown in Figure 2.
For a description like (1), the dialogue-manager
specifies only the tileset to be described, and option-
ally a set of features to include in the description.
Figure 3 shows a dialogue-manager message3 indi-
cating that tileset 9 should be described, and that the
description must include the colour.
To select the content of the description, we first
retrieve all of the features of the indicated design
3The object-slot-value syntax used here allows messages
containing ontology instances to be validated easily against an
XML schema.
from the ontology, using the Jena semantic web
framework.4 We then use the system dialogue his-
tory to filter the retrieved features by removing any
that have already been described to the user. Finally,
we add back to the set any features specifically re-
quested by the dialogue manager, even if they have
been included in a previous description.
3.2 Content Structuring
The result of content selection is an unordered set
of tileset features; this set is converted into a text
plan as follows. First, for each selected feature, a
message is created in XML that combines the in-
formation gathered from the ontology with infor-
mation from the system dialogue history. Figure 4
shows the messages corresponding to the colour
feature and to the associated canned-text commen-
tary. The dialogue-history information is included
in the same-as-last (i.e., whether this value is the
same as the corresponding value of the previous tile-
set) and already-said attributes.
The unordered set of messages is converted to an
ordered list using a small number of heuristics: for
example, features requested by the dialogue man-
ager are always put at the start of the list, while
canned-text commentary always goes immediately
after the feature to which it refers. These heuristics
provide a partial ordering, which is then converted
to a total ordering by breaking ties at random.
The next step is to aggregate the flat list of mes-
sages. In many NLG systems, aggregation is a task
that is done at the syntactic level; in COMIC, we
instead work at the conceptual level. Thanks to the
fact that we produce multiple alternative syntactic
structures (see Section 4), we can be confident that,
whatever the final set of messages, there will be
some syntactic structure available to realize them.
The aggregation is done using a set of XSLT tem-
plates that combine adjacent messages based on var-
ious criteria. For example, the template shown in
Figure 5 combines a feature-value message with
the associated canned-text commentary.5 Figure 6
shows the combined message that results when the
messages in Figure 4 are processed by this template.
The sentence boundaries in the final text are de-
termined by the content structure: each aggregated
message after aggregation corresponds to exactly
one sentence in the output.
4http://jena.sourceforge.net/
5This template is simplified; there are actually many more
tests, and aggregation is performed in several passes to allow
multi-level aggregation. The set namespace refers to a Java Set
instance that stores message IDs to avoid processing a message
twice.
<messages>
<msg id="t2-1-5" prop="has_colour" type="prop-has-val" same-as-last="false" already-said="false">
<slot name="object" value="tileset9"/>
<slot name="value" value="terracotta beige"/>
</msg>
<msg full-sentence="false" id="t2-1-6" prop="has_colour" type="canned-text">
<slot name="object" value="tileset9"/>
<slot name="value" value="give-tuscan-feeling"/>
</msg>
</messages>
Figure 4: Initial messages
<xsl:template match="messages">
<xsl:variable name="void" select="set:clear()"/>
<messages>
<xsl:for-each select="msg">
<xsl:variable name="next" select="following-sibling::msg[1]"/>
<xsl:choose>
<!-- Return nothing if we?ve already processed this message. -->
<xsl:when test="set:contains(@id)"/>
<!-- Add canned text to a sentence. -->
<xsl:when test="@prop=$next/@prop and @type=?prop-has-val?
and $next/@type=?canned-text? and not($next/@full-sentence=?true?)">
<msg type="same-prop-canned-text" id="{concat(@id, ?+?, $next/@id)}">
<slot name="prop"> <xsl:copy-of select="."/> </slot>
<slot name="text"> <xsl:copy-of select="$next"/> </slot>
</msg>
<xsl:variable name="void" select="set:add(string(@id))"/>
<xsl:variable name="void" select="set:add(string($next/@id))"/>
</xsl:when>
<!-- ... other tests ... -->
<!-- Nothing matched: just copy the message across. -->
<xsl:otherwise> <xsl:copy-of select="."/> </xsl:otherwise>
</xsl:choose>
</xsl:for-each>
</messages>
</xsl:template>
Figure 5: Aggregation template (simplified)
3.3 Sentence Planning
After the content of a description has been selected
and structured, the logical forms to send to the re-
alizer are created by applying further XSLT tem-
plates. Every such template matches a message
with particular properties, and produces a logical
form for the realizer, possibly combining the results
of other templates to produce its own final result.
XSLT modes are used to select different templates
in different target syntactic contexts.
Two sample templates are shown in Figure 7.
The first template produces the logical form for
a sentence (mode="s") describing the colours of
a tileset (e.g., The tiles are terracotta and beige).
The second template creates a logical form repre-
senting a commentary message as a verb phrase6
(mode="vp"), and then appends it as an elaboration
6Canned-text commentary is represented in the realizer lex-
icon as a multi-word verb.
to a sentence about the same property. When the
messages in Figure 6 are transformed by these tem-
plates, the result is the logical form shown in Fig-
ure 8, which corresponds to the sentence The tiles
are terracotta and beige, giving the room the feel-
ing of a Tuscan country home.
Referring expressions are generated based on the
number of mentions of the referent: the first refer-
ence gets a full NP (e.g., this design), while subse-
quent mentions are pronominalized.
The logical form created for each top-level mes-
sage is sent to the OpenCCG realizer, which then
generates and returns the corresponding surface
form. As described below, the logical forms may
incorporate alternatives, in which case the realizer
chooses the logical form to use.
4 Sending Alternatives to the Realizer
Many messages can be realized by several differ-
ent logical forms. For example, to inform the user
<messages>
<msg id="t2-1-5+t2-1-6" type="same-prop-canned-text">
<slot name="prop">
<msg id="t2-1-5" prop="has_colour" type="prop-has-val" same-as-last="false" already-said="false">
<slot name="object" value="tileset9"/>
<slot name="value" value="terracotta beige"/>
</msg>
</slot>
<slot name="text">
<msg full-sentence="false" id="t2-1-6" prop="has_colour" type="canned-text">
<slot name="object" value="tileset9"/>
<slot name="text" value="give-tuscan-feeling"/>
</msg>
</slot>
</msg>
</messages>
Figure 6: Combined messages
<xsl:template match="msg[@type=?prop-has-val? and @prop=?has_colour?]" mode="s">
<node pred="be" tense="pres">
<rel name="Arg"> <node pred="tile" det="the" num="pl"/> </rel>
<rel name="Prop"> <xsl:apply-templates select="slot[@name=?value?]" mode="np"/> </rel>
</node>
</xsl:template>
<xsl:template match="msg[@type=?same-prop-canned-text?]" mode="s">
<node pred="elab-rel">
<rel name="Core"> <xsl:apply-templates select="slot[@name=?prop?]/msg" mode="s"/> </rel>
<rel name="Trib"> <xsl:apply-templates select="slot[@name=?text?]/msg" mode="vp"/> </rel>
</node>
</xsl:template>
Figure 7: Sentence-planning templates (simplified)
that a particular design is in the country style, the
options include This design is in the country style
and This design is country. Often, the text planner
has no reason to prefer one alternative over another.
Rather than picking an arbitrary option within the
text planner (as did, e.g., van Deemter et al (1999)),
we instead defer the choice and send all of the valid
alternatives to the realizer, in a packed representa-
tion. This makes the implementation of the text
planner more straightforward. Figure 9 shows an
example of such a logical form, incorporating both
of the above options under a <one-of> element.
To process a logical form with embedded alterna-
tives, the COMIC realizer makes use of the same
n-gram language models that it uses to guide its
search for the realization of a single logical form.
Since OpenCCG cannot yet handle the realization
of logical forms with embedded alternatives directly
(though this capability is planned), in the current
system the packed alternatives are first multiplied
out into a list of top-level alternatives, whose order
is randomly shuffled. The realizer then computes
the best realization for each top-level alternative in
turn, keeping track of the overall best scoring com-
plete realization, until either the anytime time limit
is reached or the list is exhausted. To allow for some
free variation, a new realization?s score must exceed
the current best one by a certain threshold before it
is considered significantly better.
As a concrete example, consider the case where
the system must confirm that the user intends to re-
fer to a tileset with a specific feature. The feature
could be included in the logical form in two ways: it
could be attached directly to the design node (2?3),
or it could instead be included as a non-restrictive
modifier (4).
(2) Do you mean this country design?
(3) Do you mean this design by Coem?
(4) Do you mean this design, with tiles by Coem?
When the modifier can be placed before design, as
in (2), the directly-attached structure is acceptable.
However, for some features, the modifier can only
be placed after the modified noun, as in (3). In
these cases, the preferred structure is instead the
non-restrictive one in (4); this breaks the sentence
into two intonational phrases, which makes it eas-
ier to understand when it is output by the speech
synthesizer. This preference is implemented by in-
cluding only sentences of the preferred type when
<!-- The tiles are terracotta and beige, giving the room the feeling of a Tuscan country home. -->
<lf id="t2-1-5+t2-1-6">
<node mood="dcl" info="rh" pred="elab-rel" id="n7">
<rel name="Core">
<node tense="pres" id="n2" pred="be">
<rel name="Arg"> <node det="the" pred="tile" id="n1" num="pl"/> </rel>
<rel name="Prop">
<node id="n3" pred="and">
<rel name="List">
<node id="n4" kon="+" pred="terracotta"> <rel name="Of"> <node idref="n1"/> </rel> </node>
<node kon="+" id="n6" pred="beige"> <rel name="Of"> <node idref="n1"/> </rel> </node>
</rel>
</node>
</rel>
</node>
</rel>
<rel name="Trib">
<node id="n8" pred="give-tuscan-feeling"> <rel name="Arg"> <node idref="n1"/> </rel> </node>
</rel>
</node>
</lf>
Figure 8: Generated logical form
<lf id="t2-1-2">
<!-- This design is ... -->
<node tense="pres" mood="dcl" info="rh" pred="be" id="n13">
<rel name="Arg">
<node id="n1" num="sg" pred="design" kon="+">
<rel name="Det"> <node kon="+" pred="this" id="n18"/> </rel>
</node>
</rel>
<rel name="Prop">
<one-of>
<!-- ... in the country style. -->
<node pred="in" id="n14">
<rel name="Fig"> <node idref="n1"/> </rel>
<rel name="Ground">
<node num="sg" det="the" pred="style" id="n15">
<rel name="HasProp"> <node id="n16" kon="+" pred="country"/> </rel>
</node>
</rel>
</node>
<!-- ... country. -->
<node id="n20" kon="+" pred="country"> <rel name="Of"> <node idref="n1"/> </rel> </node>
</one-of>
</rel>
</node>
</lf>
Figure 9: Logical form containing alternatives
building the language model for OpenCCG. The re-
alizer will then give (4) a higher n-gram score than
(3), and will therefore choose the desired structure.
In addition to simplifying the implementation, re-
taining multiple alternatives through the planning
process also increases the robustness of the system,
and provides a substitute for backtracking. Particu-
larly during development, there may be times when
a required template simply does not exist; for ex-
ample, the second template in Figure 7 will fail if
the canned-text commentary cannot be realized as a
verb phrase. In such cases, the text planner prunes
out the failing possibilities before sending the set of
options to the realizer, using the template shown in
Figure 10.
5 Related Work
The work presented here continues in the tradi-
tion of several recent NLG systems that use what
could be called generalized template-based process-
ing. By generalized, we mean that, rather than ma-
nipulating flat strings with no underlying linguis-
tic representation, these systems instead work with
structured fragments, which are often processed re-
cursively. Other systems that fall into this cate-
gory include EXEMPLARS (White and Caldwell,
1998), D2S (van Deemter et al, 1999), Interact
<xsl:template match="one-of">
<!-- Recursive pruning step -->
<xsl:variable name="pruned-alts">
<xsl:for-each select="*">
<xsl:variable name="pruned-alt"> <xsl:apply-templates select="."/> </xsl:variable>
<xsl:if test="not(xalan:nodeset($pruned-alt)//fail)">
<xsl:copy-of select="$pruned-alt"/>
</xsl:if>
</xsl:for-each>
</xsl:variable>
<xsl:variable name="num-remaining" select="count(xalan:nodeset($pruned-alts)/*)"/>
<!-- Propagation step -->
<xsl:choose>
<!-- keep one-of when multiple alts succeed -->
<xsl:when test="$num-remaining &gt; 1"> <one-of> <xsl:copy-of select="$pruned-alts"/> </one-of> </xsl:when>
<!-- filter out one-of when just one choice remains -->
<xsl:when test="$num-remaining = 1"> <xsl:copy-of select="$pruned-alts"/> </xsl:when>
<!-- fail if none remain -->
<xsl:otherwise> <fail/> </xsl:otherwise>
</xsl:choose>
</xsl:template>
Figure 10: Failure-pruning template
(Wilcock, 2001; Wilcock, 2003), and SmartKom
(Becker, 2002).
The main novel contribution of the text-planning
approach described here is in its use of an exter-
nal realizer that processes logical forms with em-
bedded alternatives. This eliminates the need to
use a backtracking AI planner (Becker, 2002) or to
make arbitrary choices when multiple alternatives
are available (van Deemter et al, 1999). The real-
izer also uses a completely different algorithm than
the XSLT template processing?bottom-up, chart-
based search rather than top-down rule expansion?
which allows it to deal with those aspects of NLG
that are more easily addressed using this kind of
processing strategy.
Our approach to text planning draws from both
the AI-planning and the template-based traditions
in natural language generation. Most previous
NLG systems that use AI planners use them pri-
marily to do hierarchical decomposition of com-
municative goals; the work described here uses
XSLT to achieve the same end, with a substitute
for backtracking provided by the realizer?s support
for multiple alternatives. The system is nonetheless
equally based on (generalized) template processing.
This demonstrates that, rather than being in con-
flict, the two traditions actually have complemen-
tary strengths, which can usefully be combined in a
single system (contra Reiter, 1995; cf. van Deemter
et al, 1999).
6 Conclusions and Future Work
We have described a successful implementation of
the classic NLG pipeline that uses XSLT template
processing as a top-down rule-expanding planner.
Implementing the necessary steps using XSLT was
generally straightforward, and the ability to use off-
the shelf, well-tested and well-documented tools
such as Java and Xalan adds to the ease of imple-
mentation and robustness.
Our implementation creates logical forms for the
OpenCCG realizer; this allows the realizer to be
used for those parts of the generation process to
which XSLT is less well-suited. We also take ad-
vantage of the fact that the realizer uses statistical
language models in its search for a surface form
by generating logical forms with embedded alter-
natives, allowing the realizer to choose the one to
use. This both adds robustness to the system and
eliminates the need for backtracking within the text
planner.
The current implementation is fast and reliable: it
correctly processes all input from the dialogue man-
ager, and the time it takes to do so is relatively short
compared to that required by the other processing
and communication tasks in COMIC.
The entire COMIC demonstrator will shortly be
evaluated. As part of this evaluation, we plan to
measure users? recall of the information that the sys-
tem presents to them, where that information is gen-
erated at different levels of detail.
At the moment, the logical form for each mes-
sage is created in isolation. In future versions of
COMIC, we plan to use ideas from centering theory
to help ensure coherence by planning a coherent se-
quence of logical forms for a description. We will
implement this in a way similar to that described by
Kibble and Power (2000).
We will also incorporate a model of the user?s
preferences into a later version of COMIC. The
model will be used both to rank the options to be
presented to the user, and to generate user-tailored
descriptions of those options, as in FLIGHTS
(Moore et al, 2004).
Finally, we plan to extend the use of data-driven
techniques in the realizer, and to make use of these
techniques to help in choosing among alternatives
in the other COMIC output modalities.
Acknowledgements
Thanks to Jon Oberlander, Johanna Moore, and
the anonymous reviewers for helpful comments and
discussion. This work was supported in part by the
COMIC (IST-2001-32311) and FLIGHTS (EPSRC-
GR/R02450/01) projects.
References
Tilman Becker. 2002. Practical, template-based
natural language generation with TAG. In Pro-
ceedings of TAG+6.
Kees van Deemter, Emiel Krahmer, and Marie?t The-
une. 1999. Plan-based vs. template-based NLG:
a false opposition? In Proceedings of ?May I
speak freely?? workshop at KI-99.
Rodger Kibble and Richard Power. 2000. An inte-
grated framework for text planning and pronomi-
nalisation. In Proceedings of INLG 2000.
Johanna Moore, Mary Ellen Foster, Oliver Lemon,
and Michael White. 2004. Generating tailored,
comparative descriptions in spoken dialogue. In
Proceedings of FLAIRS 2004.
Els den Os and Lou Boves. 2003. Towards ambi-
ent intelligence: Multimodal computers that un-
derstand our intentions. In Proceedings of eChal-
lenges e-2003.
Ehud Reiter. 1995. NLG vs. templates. In Proceed-
ings of EWNLG-95.
Ehud Reiter and Robert Dale. 2000. Building Nat-
ural Language Generation Systems. Cambridge
University Press.
Mark Steedman. 2000. The Syntactic Process.
MIT Press.
Michael White. 2004a. Efficient realization of
coordinate structures in Combinatory Categorial
Grammar. Research on Language and Computa-
tion. To appear.
Michael White. 2004b. Reining in CCG chart re-
alization. In Proceedings of INLG 2004. To ap-
pear.
Michael White and Jason Baldridge. 2003. Adapt-
ing chart realization to CCG. In Proceedings of
EWNLG-03.
Michael White and Ted Caldwell. 1998. EXEM-
PLARS: A practical, extensible framework for
dynamic text generation. In Proceedings of INLG
1998.
Graham Wilcock. 2001. Pipelines, templates and
transformations: XML for natural language gen-
eration. In Proceedings of NLPXML-2001.
Graham Wilcock. 2003. Integrating natural lan-
guage generation with XML web technology. In
Proceedings of EACL-2003 Demo Sessions.
Designing an Extensible API for
Integrating Language Modeling and Realization
Michael White
School of Informatics
University of Edinburgh
Edinburgh EH8 9LW, UK
http://www.iccs.informatics.ed.ac.uk/~mwhite/
Abstract
We present an extensible API for inte-
grating language modeling and realiza-
tion, describing its design and efficient
implementation in the OpenCCG sur-
face realizer. With OpenCCG, language
models may be used to select realiza-
tions with preferred word orders, pro-
mote alignment with a conversational
partner, avoid repetitive language use,
and increase the speed of the best-first
anytime search. The API enables a vari-
ety of n-gram models to be easily com-
bined and used in conjunction with ap-
propriate edge pruning strategies. The
n-gram models may be of any order,
operate in reverse (?right-to-left?), and
selectively replace certain words with
their semantic classes. Factored lan-
guage models with generalized backoff
may also be employed, over words rep-
resented as bundles of factors such as
form, pitch accent, stem, part of speech,
supertag, and semantic class.
1 Introduction
The OpenCCG1 realizer (White and Baldridge,
2003; White, 2004a; White, 2004c) is an open
source surface realizer for Steedman?s (2000a;
2000b) Combinatory Categorial Grammar (CCG).
It is designed to be the first practical, reusable re-
alizer for CCG, and includes implementations of
1http://openccg.sourceforge.net
CCG?s unique accounts of coordination and infor-
mation structure?based prosody.
Like other surface realizers, the OpenCCG re-
alizer takes as input a logical form specifying the
propositional meaning of a sentence, and returns
one or more surface strings that express this mean-
ing according to the lexicon and grammar. A dis-
tinguishing feature of OpenCCG is that it imple-
ments a hybrid symbolic-statistical chart realiza-
tion algorithm that combines (1) a theoretically
grounded approach to syntax and semantic com-
position, with (2) the use of integrated language
models for making choices among the options
left open by the grammar (thereby reducing the
need for hand-crafted rules). In contrast, previous
chart realizers (Kay, 1996; Shemtov, 1997; Car-
roll et al, 1999; Moore, 2002) have not included
a statistical component, while previous statisti-
cal realizers (Knight and Hatzivassiloglou, 1995;
Langkilde, 2000; Bangalore and Rambow, 2000;
Langkilde-Geary, 2002; Oh and Rudnicky, 2002;
Ratnaparkhi, 2002) have employed less general
approaches to semantic representation and com-
position, and have not typically made use of fine-
grained logical forms that include specifications
of such information structural notions as theme,
rheme and focus.
In this paper, we present OpenCCG?s extensi-
ble API (application programming interface) for
integrating language modeling and realization, de-
scribing its design and efficient implementation in
Java. With OpenCCG, language models may be
used to select realizations with preferred word or-
ders (White, 2004c), promote alignment with a
conversational partner (Brockmann et al, 2005),
and avoid repetitive language use. In addition,
by integrating language model scoring into the
search, it also becomes possible to use more accu-
rate models to improve realization times, when the
realizer is run in anytime mode (White, 2004b).
To allow language models to be combined
in flexible ways?as well as to enable research
on how to best combine language modeling and
realization?OpenCCG?s design includes inter-
faces that allow user-defined functions to be used
for scoring partial realizations and for pruning
low-scoring ones during the search. The design
also includes classes for supporting a range of
language models and typical ways of combining
them. As we shall see, experience to date indi-
cates that the benefits of employing a highly gen-
eralized approach to scoring and pruning can be
enjoyed with little or no loss of performance.
The rest of this paper is organized as follows.
Section 2 gives an overview of the realizer archi-
tecture, highlighting the role of the interfaces for
plugging in custom scoring and pruning functions,
and illustrating how n-gram scoring affects accu-
racy and speed. Sections 3 and 4 present Open-
CCG?s classes for defining scoring and pruning
functions, respectively, giving examples of their
usage. Finally, Section 5 summarizes the design
and concludes with a discussion of future work.
2 Realizer Overview
The UML class diagram in Figure 1 shows the
high-level architecture of the OpenCCG realizer;
sample Java code for using the realizer appears in
Figure 2. A realizer instance is constructed with
a reference to a CCG grammar (which supports
both parsing and realization). The grammar?s lex-
icon has methods for looking up lexical items via
their surface forms (for parsing), or via the prin-
cipal predicates or relations in their semantics (for
realization). A grammar also has a set of hierar-
chically organized atomic types, which can serve
as the values of features in the syntactic categories,
or as ontological sorts for the discourse referents
in the logical forms (LFs).
Lexical lookup yields lexical signs. A sign pairs
a list of words with a category, which itself pairs
a syntactic category with a logical form. Lexical
signs are combined into derived signs using the
rules in the grammar?s rule group. Derived signs
maintain a derivation history, and their word lists
share structure with the word lists of their input
signs.
As mentioned in the introduction, for general-
ity, the realizer makes use of a configurable sign
scorer and pruning strategy. A sign scorer imple-
ments a function that returns a number between
0 and 1 for an input sign. For example, a stan-
dard trigram language model can be used to im-
plement a sign scorer, by returning the probability
of a sign?s words as its score. A pruning strat-
egy implements a method for determining which
edges to prune during the realizer?s search. The
input to the method is a ranked list of edges for
signs that have equivalent categories (but different
words); grouping edges in this way ensures that
pruning cannot ?break? the realizer, i.e. prevent it
from finding some grammatical derivation when
one exists. By default, an N-best pruning strategy
is employed, which keeps the N highest scoring in-
put edges, pruning the rest (where N is determined
by the current preference settings).
The realization algorithm is implemented by the
realize method. As in the chart realizers cited
earlier, the algorithm makes use of a chart and
an agenda to perform a bottom-up dynamic pro-
gramming search for signs whose LFs completely
cover the elementary predications in the input log-
ical form. See Figure 9 (Section 3.1) for a real-
ization trace; the algorithm?s details and a worked
example appear in (White, 2004a; White, 2004c).
The realize method returns the edge for the best
realization of the input LF, as determined by the
sign scorer. After a realization request, the N-best
complete edges?or more generally, all the edges
for complete realizations that survived pruning?
are also available from the chart.
The search for complete realizations proceeds
in one of two modes, anytime and two-stage
(packing/unpacking). In the anytime mode, a best-
first search is performed with a configurable time
limit (which may be a limit on how long to look
for a better realization, after the first complete one
is found). With this mode, the scores assigned by
the sign scorer determine the order of the edges
on the agenda, and thus have an impact on realiza-
tion speed. In the two-stage mode, a packed forest
 Realizer 
 +timeLimitMS: int 
 +Realizer(grammar: Grammar)  
 +realize(lf: LF): Edge  
 +getChart( ): Chart 
 Grammar 
 Chart 
 +bestEdge: Edge 
 +bestEdges( ): List<Edge> 
 ?interface?  
 SignScorer 
 +score(sign: Sign, complete: boolean): double 
 ?interface?  
 PruningStrategy 
 +pruneEdges(edges: List<Edge>): List<Edge> 
 Lexicon 
 +getSignsFromWord(...): Set<Sign>  
 +getSignsFromPred(...): Set<Sign>  
 +getSignsFromRel(...): Set<Sign> 
 RuleGroup 
 +applyRules(...): List<Sign> 
 Types 
 Edge 
 +score: double  
 +completeness: double 
 Sign 
 ?interface?  
 Category 
 +getLF( ): LF 
 Word  DerivationHistory 
 A realizer for a CCG grammar makes use of   
 a configurable sign scorer and pruning strategy.  
 The realize method takes a logical form (LF)   
 as input and returns the edge for the best   
 realization of that LF. 
 After a realization request, the N?best edges  
 are also available from the chart. 
 The lexico?grammar and signs   
 are the same for parsing and   
 realization. 
*
1..n 0..2
Figure 1: High-level architecture of the OpenCCG realizer
// load grammar , instantiate realizer
URL grammarURL = ...;
Grammar grammar = new Grammar(grammarURL );
Realizer realizer = new Realizer(grammar );
// configure realizer with trigram backoff model
// and 10-best pruning strategy
realizer.signScorer = new StandardNgramModel (3, "lm.3bo");
realizer.pruningStrategy = new NBestPruningStrategy (10);
// ... then , for each request:
// get LF from input XML
Document inputDoc = ...;
LF lf = realizer.getLfFromDoc(inputDoc );
// realize LF and get output words in XML
Edge bestEdge = realizer.realize(lf);
Document outputDoc = bestEdge.sign.getWordsInXml ();
// return output
... outputDoc ...;
Figure 2: Example realizer usage
of all possible realizations is created in the first
stage; then in the second stage, the packed repre-
sentation is unpacked in bottom-up fashion, with
scores assigned to the edge for each sign as it is
unpacked, much as in (Langkilde, 2000). In both
modes, the pruning strategy is invoked to deter-
mine whether to keep or prune newly constructed
edges. For single-best output, the anytime mode
can provide signficant time savings by cutting off
the search early; see (White, 2004c) for discus-
sion. For N-best output?especially when a com-
plete search (up to the edges that survive the prun-
ing strategy) is desirable?the two-stage mode can
be more efficient.
To illustrate how n-gram scoring can guide the
best-first anytime search towards preferred real-
izations and reduce realization times, we repro-
duce in Table 1 and Figures 3 through 5 the cross-
validation tests reported in (White, 2004b). In
these tests, we measured the realizer?s accuracy
and speed, under a variety of configurations, on
the regression test suites for two small but linguis-
tically rich grammars: the English grammar for
the COMIC2 dialogue system?the core of which
is shared with the FLIGHTS system (Moore et al,
2004)?and the Worldcup grammar discussed in
2http://www.hcrc.ed.ac.uk/comic/
(Baldridge, 2002). Table 1 gives the sizes of the
test suites. Using these two test suites, we timed
how long it took on a 2.2 GHz Linux PC to realize
each logical form under each realizer configura-
tion. To measure accuracy, we counted the num-
ber of times the best scoring realization exactly
matched the target, and also computed a modified
version of the Bleu n-gram precision metric (Pap-
ineni et al, 2001) employed in machine translation
evaluation, using 1- to 4-grams, with the longer
n-grams given more weight (cf. Section 3.4). To
rank candidate realizations, we used standard n-
gram backoff models of orders 2 through 6, with
semantic class replacement, as described in Sec-
tion 3.1. For smoothing, we used Ristad?s nat-
ural discounting (Ristad, 1995), a parameter-free
method that seems to work well with relatively
small amounts of data.
To gauge how the amount of training data af-
fects performance, we ran cross-validation tests
with increasing numbers of folds, with 25 as the
maximum number of folds. We also compared the
realization results using the n-gram scorers with
two baselines and one topline (oracle method).
The first baseline assigns all strings a uniform
score of zero, and adds new edges to the end of the
agenda, corresponding to breadth-first search. The
LF/target Unique up
pairs to SC Mean Min Max Mean Min Max
COMIC 549 219 13.1 6 34 8.4 2 20
Worldcup 276 138 9.2 4 18 6.8 3 13
Input nodesLength
Table 1: Test suite sizes.
Folds Baseline 1 Baseline 2 N2 N3 N4 N5 N6 Topline1.04 459.464 370.889 460.454 450.548 450.843 458.592 460.228 147.8181.1 459.464 370.889 406.743 365.058 360.705 370.678 372.869 147.8181.2 459.464 370.889 366.007 305.226 306.098 314.825 308.131 147.8181.33 459.464 370.889 349.035 279.426 266.956 274.539 275.559 147.8181.5 459.464 370.889 342.078 272.148 256.851 260.654 262.124 147.8182 459.464 370.889 320.536 238.563 217.597 222.8 221.709 147.8183 459.464 370.889 311.634 234.106 210.492 211.816 212.674 147.8185 459.464 370.889 307.725 225.306 202.372 203.661 201.568 147.81810 459.464 370.889 302.233 223.579 199.632 200.148 198.929 147.81825 459.464 370.889 302.286 220.612 197.918 199.415 196.361 147.818
COMIC: First
0
100
200
300
400
500
1.04 1.1 1.2 1.33 1.5 2 3 5 10 25
Num Folds
Tim
e (m
s)
Baseline 1
Baseline 2
N2
N3
N4
N5
N6
Topline
Folds Baseline 1 Baseline 2 N2 N3 N4 N5 N6 Topline1.04 151.819 115.243 159.754 156.134 158.022 160.493 160.38 49.1271.1 151.819 115.243 158.638 152.946 154.062 155.007 154.71 49.1271.2 151.819 115.243 145.496 130.123 131.138 130.167 131.674 49.1271.33 151.819 115.243 138.312 119.837 119.029 119.754 119.815 49.1271.5 151.819 115.243 136.243 121.29 120.185 120.757 121.083 49.1272 151.819 115.243 128.822 102.221 98.844 97.772 97.54 49.1273 151.819 115.243 124.475 97.652 92.931 92.373 93.493 49.1275 151.819 115.243 122.656 94.051 90.649 89.957 90.627 49.12710 151.819 115.243 125.087 92.471 88.446 87.96 88.928 49.12725 151.819 115.243 121.076 92.623 88.043 86.149 87.293 49.127
Worldcup: First
0
40
80
120
160
200
1.04 1.1 1.2 1.33 1.5 2 3 5 10 25
Num Folds
Tim
e (m
s)
Baseline 1
Baseline 2
N2
N3
N4
N5
N6
Topline
Figure 3: Mean time (in ms.) until first realization is found using n-grams of different orders and Ristad?s
natural discounting (N), for cross-validation tests with increasing numbers of folds.
Folds Baseline 1 Baseline 2 N2 N3 N4 N5 N6 Topline1.04 241 41 524 512 513 513 513 5491.1 241 41 493 477 469 469 469 5491.2 241 41 542 542 542 542 542 5491.33 241 41 538 541 541 541 541 5491.5 241 41 548 542 542 542 542 5492 241 41 549 547 547 547 547 5493 241 41 549 548 548 548 548 5495 241 41 549 548 548 548 548 54910 241 41 549 548 548 548 548 54925 241 41 549 548 548 548 548 549
COMIC: Exact
0
100
200
300
400
500
600
1.04 1.1 1.2 1.33 1.5 2 3 5 10 25
Num Folds
Tim
e (
ms
)
Baseline 1
Baseline 2
N2
N3
N4
N5
N6
Topline
Folds Baseline 1 Baseline 2 N2 N3 N4 N5 N6 Topline1.04 86 70 112 114 114 114 114 2761.1 86 70 148 156 154 157 157 2761.2 86 70 177 173 174 180 177 2761.33 86 70 187 198 199 203 201 2761.5 86 70 200 210 211 211 213 2762 86 70 210 227 225 231 232 2763 86 70 221 239 238 242 245 2765 86 70 222 245 241 244 247 27610 86 70 224 242 235 246 246 27625 86 70 228 248 238 250 252 276
Worldcup: Exact
0
50
100
150
200
250
300
1.04 1.1 1.2 1.33 1.5 2 3 5 10 25
Num Folds
Tim
e (
ms
)
Baseline 1
Baseline 2
N2
N3
N4
N5
N6
Topline
Figure 4: Number of realizations exactly matching target using n-grams of different orders.
Folds Baseline 1 Baseline 2 N2 N3 N4 N5 N6 Topline1.04 0.754 .379 0.974 0.963 0.963 0.963 0.963 11.1 0.754 0.379 0.967 0.953 0.942 0.942 0.942 11.2 0.754 0.379 0.996 0.996 0.996 0.996 0.996 11.33 0.754 0.379 0.992 0.995 0.995 0.995 0.995 11.5 0.754 0.379 1 0.996 0.996 0.996 0.996 12 0.754 0.379 1 0.999 0.999 0.999 0.999 13 0.754 0.379 1 0.999 0.999 0.999 0.999 15 0.754 0.379 1 0.999 0.999 0.999 0.999 110 0.754 0.379 1 0.999 0.999 0.999 0.999 125 0.754 0.379 1 0.999 0.999 0.999 0.999 1
COMIC: Score
0
0.2
0.4
0.6
0.8
1
1.04 1.1 1.2 1.33 1.5 2 3 5 10 25
Num Folds
Tim
e (m
s)
Baseline 1
Baseline 2
N2
N3
N4
N5
N6
Topline
Folds Baseline 1 Baseline 2 N2 N3 N4 N5 N6 Topline1.04 0.6 .538 0.646 0.658 0.656 0.656 0.656 1.1 0.6 0.538 0.766 0.767 0.764 0.768 0.768 1.2 0.6 0.538 0.806 0.794 0.8 0.804 0.803 11.33 0.6 0.538 0.833 0.849 0.852 0.859 0.856 1.5 0.6 0.538 0.87 0.877 0.873 0.873 0.877 12 0.6 0.538 0.875 0.888 0.886 0.894 0.895 13 0.6 0.538 0.901 0.923 0.92 0.924 0.924 15 0.6 0.538 0.9 0.93 0.925 0.927 0.931 110 0.6 0.538 0.905 0.93 0.915 0.928 0.922 125 0.6 0.538 0.915 0.938 0.917 0.932 0.935 1
Worldcup: Score
0
0.2
0.4
0.6
0.8
1
1.04 1.1 1.2 1.33 1.5 2 3 5 10 25
Num Folds
Tim
e (m
s)
Baseline 1
Baseline 2
N2
N3
N4
N5
N6
Topline
Figure 5: Modified BLEU scores using n-grams of different orders.
second baseline uses the same scorer, but adds new
edges at the front of the agenda, corresponding to
depth-first search. The topline uses the modified
Bleu score, computing n-gram precision against
just the target string. With this setup, Figures 3-
5 show how initial realization times decrease and
accuracy increases when longer n-grams are em-
ployed. Figure 3 shows that trigrams offer a sub-
stantial speedup over bigrams, while n-grams of
orders 4-6 offer a small further improvement. Fig-
ures 4 and 5 show that with the COMIC test suite,
all n-gram orders work well, while with the World-
cup test suite, n-grams of orders 3-6 offer some
improvement over bigrams.
To conclude this section, we note that together
with OpenCCG?s other efficiency methods, n-
gram scoring has helped to achieve realization
times adequate for interactive use in both the
COMIC and FLIGHTS dialogue systems, along
with very high quality. Estimates indicate that
n-gram scoring typically accounts for only 2-5%
of the time until the best realization is found,
while it can more than double realization speed by
accurately guiding the best-first anytime search.
This experience suggests that more complex scor-
ing models can more than pay for themselves,
efficiency-wise, if they yield significantly more ac-
curate preference orders on edges.
3 Classes for Scoring Signs
The classes for implementing sign scorers appear
in Figure 6. In the diagram, classes for n-gram
scoring appear towards the bottom, while classes
for combining scorers appear on the left, and the
class for avoiding repetition appears on the right.
3.1 Standard N-gram Models
The StandardNgramModel class can load stan-
dard n-gram backoff models for scoring, as shown
earlier in Figure 2. Such models can be con-
structed with the SRILM toolkit (Stolcke, 2002),
which we have found to be very useful; in princi-
ple, other toolkits could be used instead, as long as
their output could be converted into the same file
formats. Since the SRILM toolkit has more re-
strictive licensing conditions than those of Open-
CCG?s LGPL license, OpenCCG includes its own
classes for scoring with n-gram models, in order to
avoid any necessary runtime dependencies on the
SRILM toolkit.
The n-gram tables are efficiently stored in a trie
data structure (as in the SRILM toolkit), thereby
avoiding any arbitrary limit on the n-gram order.
To save memory and speed up equality tests, each
string is interned (replaced with a canonical in-
stance) at load time, which accomplishes the same
purpose as replacing the strings with integers, but
without the need to maintain a separate mapping
from integers back to strings. For better gener-
alization, certain words may be dynamically re-
placed with the names of their semantic classes
when looking up n-gram probabilities. Words are
assigned to semantic classes in the lexicon, and the
semantic classes to use in this way may be config-
ured at the grammar level. Note that (Oh and Rud-
nicky, 2002) and (Ratnaparkhi, 2002) make simi-
lar use of semantic classes in n-gram scoring, by
deferring the instantiation of classes (such as de-
parture city) until the end of the generation pro-
cess; our approach accomplishes the same goal in
a slightly more flexible way, in that it also allows
the specific word to be examined by other scoring
models, if desired.
As discussed in (White, 2004c), with dialogue
systems like COMIC n-gram models can do an
excellent job of placing underconstrained adjec-
tival and adverbial modifiers?as well as bound-
ary tones?without resorting to the more com-
plex methods investigated for adjective ordering in
(Shaw and Hatzivassiloglou, 1999; Malouf, 2000).
For instance, in examples like those in (1), they
correctly select the preferred positions for here and
also (as well as for the boundary tones), with re-
spect to the verbal head and sister dependents:
(1) a. HereL+H? LH% we have a design in
the classicH? style LL% .
b. ThisL+H? design LH% hereL+H? LH%
is alsoH? classic LL% .
We have also found that it can be useful to
use reverse (or ?right-to-left?) models, as they can
help to place adverbs like though, as in (2):
(2) The tiles are alsoH? from the JazzH? series
though LL% .
 ?interface?  
 SignScorer 
 +score(sign: Sign, complete: boolean): double 
 +nullScorer: SignScorer 
 SignScorerInterpolation 
 #weights: double[ ] 
 SignScorerProduct 
 RepetitionScorer 
 +penalty: double 
 +updateContext(sign: Sign) 
 +resetContext( )  
 +ageContext( ) 
 Returns zero for all signs, thereby  
 providing no distinguishing information. 
 NgramScorer 
 #order: int 
 #reverse: boolean  
 #useSemClasses: boolean  
 #wordsToScore: List<Word> 
 #prepareToScoreWords( )  
 #score( ): double  
 #logProbFromNgram(i: int, order: int): float 
 ?interface?  
 NgramFilter 
 +filterOut(words: List<Word>): boolean 
 AAnFilter 
 +addException(w1: String, w2: String) 
 LinearNgramScorerCombo 
 #weights: double[ ] 
 #logProbFromNgram(i: int, order: int): float 
 Returns a score that is linear in   
 log space with the number of  
 repeated items times the penalty. 
 StandardNgramModel 
 +StandardNgramModel(order: int, filename:String) 
 #prepareToScoreWords( )  
 #logProbFromNgram(i: int, order: int): float 
 NgramPrecisionModel 
 +NgramPrecisionModel(targets: String[ ], order: int)  
 #prepareToScoreWords( )  
 #score( ): double 
 FactoredNgramModel 
 +FactoredNgramModel(child: String, parents: String[ ], filename: String)  
 #prepareToScoreWords( )  
 #logProbFromNgram(i: int, order: int): float 
 FactoredNgramModelFamily 
 +FactoredNgramModelFamily(filename: String) 
 #prepareToScoreWords( )  
 #logProbFromNgram(i: int, order: int): float 
 Utility classes for combining   
 scorers multiplicatively or  
 via linear interpolation. 
 Utility class for interpolating  
 n?gram models at the word level. 
 N?gram models can be of any order,  
 can reverse the words, and  
 can replace certain words with  
 their semantic classes. 
 Returns a modified version of the   
 BLEU score used in MT evaluation. 
 Filters bigrams with the wrong  
 choice of a or an given   
 the initial letter of the following   
 word, with configurable exceptions. 
 Factored n?gram models return the probability of the  
 child factor of the current word given a sequence of  
 parent factors. Multiple models can be organized  
 into families. 
2..n
*2..n
1..n
Figure 6: Classes for scoring signs
In principle, the forward and reverse probabilities
should be the same?as they are both derived via
the chain rule from the same joint probability of
the words in the sequence?but we have found that
with sparse data the estimates can differ substan-
tially. In particular, since though typically appears
at the end of a variety of clauses, its right context
is much more predictable than its left context, and
thus reverse models yield more accurate estimates
of its likelihood of appearing clause-finally. To il-
lustrate, Figures 7 and 8 show the forward and re-
verse trigram probabilities for two competing real-
izations of (2) in a 2-fold cross-validation test (i.e.
with models trained on the half of the test suite
not including this example). With the forward tri-
gram model, since though has not been observed
following series, and since series is a frequently
occurring word, the penalty for backing off to the
unigram probability for though is high, and thus
the probability is quite low. The medial placement
(following alsoH?) also yields a low probability,
but not as low as the clause-final one, and thus
the forward model ends up preferring the medial
placement, which is quite awkward. By contrast,
the reverse model yields a very clear preference
for the clause-final position of though, and for this
reason interpolating the forward and reverse mod-
els (see Section 3.3) also yields the desired prefer-
ence order.
Figure 9 shows a trace of realizing (2) with such
an interpolated model. In the trace, the interpo-
lated model is loaded by the class MyEvenScorer.
The input LF appears at the top. It is flattened
into a list of elementary predications, so that cov-
erage of these predications can be tracked using bit
vectors. The LF chunks ensure that the subtrees
under h1 and s1 are realized as independent sub-
problems; cf. (White, 2004a) for discussion. The
edges produced by lexical lookup and instantiation
appear next, under the heading Initial Edges,
with only the edges for alsoH? and though shown
in the figure. For each edge, the coverage percent-
age and score (here a probability) appear first, fol-
lowed by the word(s) and the coverage vector, then
the syntactic category (with features suppressed),
and finally any active LF chunks. The edges added
to the chart appear (unsorted) under the heading
All Edges. As this trace shows, in the best-first
search, high probability phrases such as the tiles
are alsoH? can be added to the chart before low-
frequency words such as though have even left
the agenda. The first complete realization, cor-
responding to (2), also turns out to be the best
one here. As noted in the figure, complete realiza-
tions are scored with sentence delimiters, which?
by changing the contexts of the initial and final
words?can result in a complete realization hav-
ing a higher probability than its input partial real-
izations (see next section for discussion). One way
to achieve more monotonic scores?and thus more
efficient search, in principle?could be to include
sentence delimiters in the grammar; we leave this
question for future work.
3.2 N-gram Scorers
The StandardNgramModel class is implemented
as a subclass of the base class NgramScorer.
All NgramScorer instances may have any num-
ber of NgramFilter instances, whose filter-
Out methods are invoked prior to n-gram scoring;
if any of these methods return true, a score of zero
is immediately returned. The AAnFilter provides
one concrete implementation of the NgramFilter
interface, and returns true if it finds a bigram con-
sisting of a followed by a vowel-inital word, or an
followed by a consonant-initial word, subject to a
configurable set of exceptions that can be culled
from bigram counts. We have found that such n-
gram filters can be more efficient, and more reli-
able, than relying on n-gram scores alone; in par-
ticular, with a/an, since the unigram probability
for a tends to be much higher than that of an, with
unseen words beginning with a vowel, there may
not be a clear preference for the bigram beginning
with an.
The base class NgramScorer implements the
bulk of the score method, using an abstract log-
ProbFromNgram method for subclass-specific cal-
culation of the log probabilities (with backoff) for
individual n-grams. The score method also in-
vokes the prepareToScoreWords method, in or-
der to allow for subclass-specific pre-processing
of the words in the given sign. With Standard-
NgramModel, this method is used to extract the
word forms or semantic classes into a list of strings
to score. It also appends any pitch accents to the
the tiles are also_H* from the SERIES_H* series though LL% .
p( the | <s> ) = [2gram] 0.0999418 [ -1.00025 ]
p( tiles | the ...) = [3gram] 0.781102 [ -0.107292 ]
p( are | tiles ...) = [3gram] 0.484184 [ -0.31499 ]
p( also_H* | are ...) = [3gram] 0.255259 [ -0.593018 ]
p( from | also_H* ...) = [3gram] 0.0649038 [ -1.18773 ]
p( the | from ...) = [3gram] 0.5 [ -0.30103 ]
p( SERIES_H* | the ...) = [3gram] 0.713421 [ -0.146654 ]
p( series | SERIES_H* ...) = [3gram] 0.486827 [ -0.312626 ]
p( though | series ...) = [1gram] 1.58885e-06 [ -5.79892 ]
p( LL% | though ...) = [2gram] 0.416667 [ -0.380211 ]
p( . | LL% ...) = [3gram] 0.75 [ -0.124939 ]
p( </s> | . ...) = [3gram] 0.999977 [ -1.00831e-05 ]
1 sentences, 11 words, 0 OOVs
0 zeroprobs, logprob= -10.2677 ppl= 7.17198 ppl1= 8.57876
the tiles are also_H* though from the SERIES_H* series LL% .
p( the | <s> ) = [2gram] 0.0999418 [ -1.00025 ]
p( tiles | the ...) = [3gram] 0.781102 [ -0.107292 ]
p( are | tiles ...) = [3gram] 0.484184 [ -0.31499 ]
p( also_H* | are ...) = [3gram] 0.255259 [ -0.593018 ]
p( though | also_H* ...) = [1gram] 1.11549e-05 [ -4.95254 ]
p( from | though ...) = [1gram] 0.00805451 [ -2.09396 ]
p( the | from ...) = [2gram] 0.509864 [ -0.292545 ]
p( SERIES_H* | the ...) = [3gram] 0.713421 [ -0.146654 ]
p( series | SERIES_H* ...) = [3gram] 0.486827 [ -0.312626 ]
p( LL% | series ...) = [3gram] 0.997543 [ -0.00106838 ]
p( . | LL% ...) = [3gram] 0.733867 [ -0.134383 ]
p( </s> | . ...) = [3gram] 0.999977 [ -1.00831e-05 ]
1 sentences, 11 words, 0 OOVs
0 zeroprobs, logprob= -9.94934 ppl= 6.74701 ppl1= 8.02574
Figure 7: Forward probabilities for two placements of though (COMIC test suite, 2-fold cross validation)
the tiles are also_H* from the SERIES_H* series though LL% .
p( . | <s> ) = [2gram] 0.842366 [ -0.0744994 ]
p( LL% | . ...) = [3gram] 0.99653 [ -0.00150975 ]
p( though | LL% ...) = [3gram] 0.00677446 [ -2.16913 ]
p( series | though ...) = [1gram] 0.00410806 [ -2.38636 ]
p( SERIES_H* | series ...) = [2gram] 0.733867 [ -0.134383 ]
p( the | SERIES_H* ...) = [3gram] 0.744485 [ -0.128144 ]
p( from | the ...) = [3gram] 0.765013 [ -0.116331 ]
p( also_H* | from ...) = [3gram] 0.0216188 [ -1.66517 ]
p( are | also_H* ...) = [3gram] 0.5 [ -0.30103 ]
p( tiles | are ...) = [3gram] 0.432079 [ -0.364437 ]
p( the | tiles ...) = [3gram] 0.9462 [ -0.0240173 ]
p( </s> | the ...) = [3gram] 0.618626 [ -0.208572 ]
1 sentences, 11 words, 0 OOVs
0 zeroprobs, logprob= -7.57358 ppl= 4.27692 ppl1= 4.88098
the tiles are also_H* though from the SERIES_H* series LL% .
p( . | <s> ) = [2gram] 0.842366 [ -0.0744994 ]
p( LL% | . ...) = [3gram] 0.99653 [ -0.00150975 ]
p( series | LL% ...) = [3gram] 0.0948425 [ -1.023 ]
p( SERIES_H* | series ...) = [3gram] 0.733867 [ -0.134383 ]
p( the | SERIES_H* ...) = [3gram] 0.744485 [ -0.128144 ]
p( from | the ...) = [3gram] 0.765013 [ -0.116331 ]
p( though | from ...) = [1gram] 3.50735e-08 [ -7.45502 ]
p( also_H* | though ...) = [1gram] 0.00784775 [ -2.10525 ]
p( are | also_H* ...) = [2gram] 0.2291 [ -0.639975 ]
p( tiles | are ...) = [3gram] 0.432079 [ -0.364437 ]
p( the | tiles ...) = [3gram] 0.9462 [ -0.0240173 ]
p( </s> | the ...) = [3gram] 0.618626 [ -0.208572 ]
1 sentences, 11 words, 0 OOVs
0 zeroprobs, logprob= -12.2751 ppl= 10.5421 ppl1= 13.0594
Figure 8: Reverse probabilities for two placements of though (COMIC test suite, 2-fold cross validation)
Input LF:
@b1:state(be ^ <info>rh ^ <mood>dcl ^ <tense>pres ^ <owner>s ^ <kon>- ^
<Arg>(t1:phys-obj ^ tile ^ <det>the ^ <num>pl ^ <info>rh ^ <owner>s ^ <kon>-) ^
<Prop>(h1:proposition ^ has-rel ^ <info>rh ^ <owner>s ^ <kon>- ^
<Of>t1:phys-obj ^
<Source>(s1:abstraction ^ series ^ <det>the ^ <num>sg ^ <info>rh ^ <owner>s ^ <kon>- ^
<HasProp>(j1:series ^ Jazz ^ <kon>+ ^ <info>rh ^ <owner>s))) ^
<HasProp>(a1:proposition ^ also ^ <kon>+ ^ <info>rh ^ <owner>s) ^
<HasProp>(t2:proposition ^ though ^ <info>rh ^ <owner>s ^ <kon>-))
Instantiating scorer from class: MyEvenScorer
Preds:
ep[0]: @a1:proposition(also)
ep[1]: @a1:proposition(<info>rh)
ep[2]: @a1:proposition(<kon>+)
ep[3]: @a1:proposition(<owner>s)
ep[4]: @b1:state(be)
ep[5]: @b1:state(<info>rh)
...
LF chunks:
chunk[0]: {14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30}
chunk[1]: {20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30}
Initial Edges:
{0.12} [0.011] also_H* {0, 1, 2, 3, 12} :- s\.s
{0.12} [0.011] also_H* {0, 1, 2, 3, 12} :- s\np/^(s\np)
{0.12} [0.011] also_H* {0, 1, 2, 3, 12} :- s/^s
...
{0.12} [6E-4] though {13, 37, 38, 39, 40} :- s\np/^(s\np)
{0.12} [6E-4] though {13, 37, 38, 39, 40} :- s\.s
{0.12} [6E-4] though {13, 37, 38, 39, 40} :- s/^s
...
Uninstantiated Semantically Null Edges:
{0.00} [0.073] LL% {} :- s$1\*(s$1)
{0.00} [0.011] L {} :- s$1\*(s$1)
All Edges:
{0.02} [0.059] . {7} :- sent\*s
{0.02} [0.059] . {7} :- sent\*(s\np)
{0.02} [0.052] the {25} :- np/^n < 0 1 >
{0.02} [0.052] the {32} :- np/^n
{0.12} [0.032] tiles {31, 33, 34, 35, 36} :- n
{0.02} [0.018] from {19} :- n\n/<np < 0 >
{0.15} [0.018] from {14, 15, 16, 17, 18, 19} :- s\!np/<np < 0 >
{0.17} [0.017] is {4, 5, 6, 8, 9, 10, 11} :- s\np/(s\!np)
...
{0.15} [0.009] the tiles {31, 32, 33, 34, 35, 36} :- np
{0.15} [0.009] the tiles {31, 32, 33, 34, 35, 36} :- s/@i(s\@inp)
{0.15} [0.009] the tiles {31, 32, 33, 34, 35, 36} :- s$1\@i(s$1/@inp)
...
{0.44} [0.001] the tiles are also_H* {0, 1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 12, 31, 32, 33, 34, 35, 36} :- s/(s\!np)
...
{0.12} [6E-4] though {13, 37, 38, 39, 40} :- s\np/^(s\np)
{0.12} [6E-4] though {13, 37, 38, 39, 40} :- s\.s
{0.12} [6E-4] though {13, 37, 38, 39, 40} :- s/^s
...
{0.85} [1.32E-5] the tiles are also_H* from the Jazz_H* series {...} :- s
{0.85} [1.32E-5] the tiles are also_H* from the Jazz_H* series LL% {...} :- s
...
{0.24} [2.64E-6] also_H* though {0, 1, 2, 3, 12, 13, 37, 38, 39, 40} :- s\np/^(s\np)
{0.24} [2.64E-6] also_H* though {0, 1, 2, 3, 12, 13, 37, 38, 39, 40} :- s\.s
...
{0.88} [5.44E-8] the tiles are from the Jazz_H* series though LL% . {...} :- sent
...
{0.85} [4.85E-8] the tiles are from the Jazz_H* series also_H* {...} :- s
...
{0.88} [3.14E-9] the tiles also_H* are from the Jazz_H* series LL% . {...} :- sent
...
{0.98} [2.96E-9] the tiles are also_H* from the Jazz_H* series though {...} :- s
...
{0.98} [1.51E-9] the tiles are also_H* from the Jazz_H* series though LL% {...} :- s
{1.00} [1.34E-8] the tiles are also_H* from the Jazz_H* series though LL% . {...} :- sent
****** first complete realization; scored with <s> and </s> tags ******
...
{0.24} [1.44E-9] also_H* though L {...} :- s\np/^(s\np)
...
{0.56} [2E-10] though the tiles are also_H* LL% {...} :- s/(s\!np)
...
Complete Edges (sorted):
{1.00} [1.34E-8] the tiles are also_H* from the Jazz_H* series though LL% . {...} :- sent
{1.00} [1.33E-8] the tiles are also_H* from the Jazz_H* series LL% though LL% . {...} :- sent
...
Figure 9: Realizer trace for example (2) with interpolated model
word forms or semantic classes, effectively treat-
ing them as integral parts of the words.
Since the realizer builds up partial realizations
bottom-up rather than left-to-right, it only adds
start of sentence (and end of sentence) tags with
complete realizations. As a consequence, the
words with less than a full n? 1 words of history
are scored with appropriate sub-models. For ex-
ample, the first word of a phrase is scored with
a unigram sub-model, without imposing backoff
penalties.
Another consequence of bottom-up realization
is that both the left- and right-contexts may change
when forming new signs from a given input sign.
Consequently, it is often not possible (even in prin-
ciple) to use the score of an input sign directly in
computing the score of a new result sign. If one
could make assumptions about how the score of an
input sign has been computed?e.g., by a bigram
model?one could determine the score of the re-
sult sign from the scores of the input signs together
with an adjustment for the word(s) whose context
has changed. However, our general approach to
sign scoring precludes making such assumptions.
Nevertheless, it is still possible to improve the effi-
ciency of n-gram scoring by caching the log prob-
ability of a sign?s words, and then looking up that
log probability when the sign is used as the first
input sign in creating a new combined sign?thus
retaining the same left context?and only recom-
puting the log probabilities for the words of any in-
put signs past the first one. (With reverse models,
the sign must be the last sign in the combination.)
In principle, the derivation history could be con-
sulted further to narrow down the words whose n-
gram probabilities must be recomputed to the min-
imum possible, though NgramScorer only imple-
ments a single-step lookup at present.3 Finally,
note that a Java WeakHashMap is used to imple-
ment the cache, in order to avoid an undesirable
buildup of entries across realization requests.
3.3 Interpolation
Scoring models may be linearly interpolated in
two ways. Sign scorers of any variety may be
3Informal experiments indicate that caching log probabil-
ities in this way can yield an overall reduction in best-first
realization times of 2-3% on average.
combined using the SignScorerInterpolation
class. For example, Figure 10 shows how forward
and reverse n-gram models may be interpolated.
With n-gram models of the same direction, it is
also possible to linearly interpolate models at the
word level, using the LinearNgramScorerCombo
class. Word-level interpolation makes it easier to
use cache models created with maximum likeli-
hood estimation, as word-level interpolation with
a base model avoids problems with zero probabil-
ities in the cache model. As discussed in (Brock-
mann et al, 2005), cache models can be used to
promote alignment with a conversational partner,
by constructing a cache model from the bigrams
in the partner?s previous turn, and interpolating it
with a base model.4 Figure 11 shows one way to
create such an interpolated model.
3.4 N-gram Precision Models
The NgramPrecisionModel subclass of Ngram-
Scorer computes a modified version of the Bleu
score used in MT evaluation (Papineni et al,
2001). Its constructor takes as input an array of
target strings?from which it extracts the n-gram
sequences to use in computing the n-gram preci-
sion score?and the desired order. Unlike with
the Bleu score, rank order centroid weights (rather
than the geometric mean) are used to combine
scores of different orders, which avoids problems
with scoring partial realizations which have no n-
gram matches of the target order. For simplicity,
the score also does not include the Bleu score?s
bells and whistles to make cheating on length dif-
ficult.
We have found n-gram precision models to be
very useful for regression testing the grammar, as
an n-gram precision model created just from the
target string nearly always leads the realizer to
choose that exact string as its preferred realiza-
tion. Such models can also be useful for evaluating
the success of different scoring models in a cross-
validation setup, though with high quality output,
manual inspection is usually necessary to deter-
mine the importance of any differences between
4At present, such cache models must be constructed with
a call to the SRILM toolkit; it would not be difficult to add
OpenCCG support for constructing them though, since these
models do not require smoothing.
// configure realizer with 4-gram forward and reverse backoff models ,
// interpolated with equal weight
NgramScorer forwardModel = new StandardNgramModel (4, "lm.4bo");
NgramScorer reverseModel = new StandardNgramModel (4, "lm -r.4bo");
reverseModel.setReverse(true);
realizer.signScorer = new SignScorerInterpolation(
new SignScorer [] { forwardModel , reverseModel }
);
Figure 10: Example interpolated n-gram model
// configure realizer with 4-gram backoff base model ,
// interpolated at the word level with a bigram maximum -likelihood
// cache model , with more weight given to the base model
NgramScorer baseModel = new StandardNgramModel (4, "lm.4bo");
NgramScorer cacheModel = new StandardNgramModel (2, "lm -cache.mle");
realizer.signScorer = new LinearNgramScorerCombo(
new SignScorer [] { baseModel , cacheModel },
new double [] { 0.6, 0.4 }
);
Figure 11: Example word-level interpolation of a cache model
the preferred realization and the target string.
3.5 Factored Language Models
A factored language model (Bilmes and Kirch-
hoff, 2003) is a new kind of language model that
treats words as bundles of factors. To support
scoring with such models, OpenCCG represents
words as objects with a surface form, pitch accent,
stem, part of speech, supertag, and semantic class.
Words may also have any number of further at-
tributes, such as associated gesture classes, in or-
der to handle in a general way elements like pitch
accents that are ?coarticulated? with words.
To represent words efficiently, and to speed up
equality tests, all attribute values are interned, and
the Word objects themselves are interned via a fac-
tory method. Note that in Java, it is straightfor-
ward to intern objects other than strings by em-
ploying a WeakHashMap to map from an object
key to a weak reference to itself as the canonical
instance. (Using a weak reference avoids accu-
mulating interned objects that would otherwise be
garbage collected.)
With the SRILM toolkit, factored language
models can be constructed that support general-
ized parallel backoff : that is, backoff order is
not restricted to just dropping the most temporally
distant word first, but rather may be specified as
a path through the set of contextual parent vari-
ables; additionally, parallel backoff paths may be
specified, with the possibility of combining these
paths dynamically in various ways. In OpenCCG,
the FactoredNgramModel class supports scoring
with factored language models that employ gen-
eralized backoff, though parallel backoff is not
yet supported, as it remains somewhat unclear
whether the added complexity of parallel backoff
is worth the implementation effort. Typically, sev-
eral related factored language models are specified
in a single file and loaded by a FactoredNgram-
ModelFamily, which can multiplicatively score
models for different child variables, and include
different sub-models for the same child variable.
To illustrate, let us consider a simplified version
of the factored language model family used in the
COMIC realizer. This model computes the proba-
bility of the current word given the preceding ones
according to the formula shown in (3), where a
word consists of the factors word (W), pitch accent
(A), gesture class (GC), and gesture instance (GI),
plus the other standard factors which the model ig-
nores:
(3)
P(?W,A,GC,GI? | ?W,A,GC,GI??1 . . .) ?
P(W |W?1W?2A?1A?2) ?
P(GC |W ) ?
P(GI |GC)
In (3), the probability of the current word is ap-
proximated by the probability of the current word
form given the preceding two word forms and pre-
ceding two pitch accents, multiplied by the proba-
bility of the current gesture class given the current
word form, and by the probability of the current
gesture instance given the current gesture class.
Note that in the COMIC grammar, the choice of
pitch accent is entirely rule governed, so the cur-
rent pitch accent is not scored separately in the
model. However, the preceding pitch accents are
taken into account in predicting the current word
form, as perplexity experiments have suggested
that they do provide additional information be-
yond that provided by the previous word forms.
The specification file for this model appears in
Figure 12. The format of the file is a restricted
form of the files used by the SRILM toolkit to
build factored language models. The file specifies
four models, where the first, third and fourth mod-
els correspond to those in (3). With the first model,
since the previous words are typically more infor-
mative than the previous pitch accents, the backoff
order specifies that the most distant accent, A(-2),
should be dropped first, followed by the previous
accent, A(-1), then the most distant word, W(-2),
and finally the previous word, W(-1). The sec-
ond model is considered a sub-model of the first?
since it likewise predicts the current word?to be
used when there is only one word of context avail-
able (i.e. with bigrams). Note that when scoring
a bigram, the second model will take the previous
pitch accent into account, whereas the first model
would not. For documentation of the file format as
it is used in the SRILM toolkit, see (Kirchhoff et
al., 2002).
Like StandardNgramModel, the Factored-
NgramModel class stores its n-gram tables in a trie
data structure, except that it stores an interned fac-
tor key (i.e. a factor name and value pair, or just a
string, in the case of the word form) at each node,
rather than a simple string. During scoring, the
logProbFromNgram method determines the log
probability (with backoff) of a given n-gram by
extracting the appropriate sequence of factor keys,
and using them to compute the log probability as
with standard n-gram models. The Factored-
NgramModelFamily class computes log probabil-
ities by delegating to its component factored n-
gram models (choosing appropriate sub-models,
when appropriate) and summing the results.
3.6 Avoiding Repetition
While cache models appear to be a promising av-
enue to promote lexical and syntactic alignment
with a conversational partner, a different mech-
anism appears to be called for to avoid ?self-
alignment??that is, to avoid the repetitive use of
words and phrases. As a means to experiment
with avoiding repetition, OpenCCG includes the
RepetitionScorer class. This class makes use
of a configurable penalty plus a set of methods for
dynamically managing the context. It returns a
score of 10?cr?p, where cr is the count of repeated
items, and p is the penalty. Note that this formula
returns 1 if there are no repeated items, and returns
a score that is linear in log space with the number
of repeated items otherwise.
A repetition scorer can be combined multiplica-
tively with an n-gram model, in order to discount
realizations that repeat items from the recent con-
text. Figure 13 shows such a combination, to-
gether with the operations for updating the con-
text. By default, open class stems are the consid-
ered the relevant items over which to count rep-
etitions, though this behavior can be specialized
by subclassing RepetitionScorer and overrid-
ing the updateItems method. Note that in count-
ing repetitions, full counts are given to items in the
previous words or recent context, while fractional
counts are given to older items; the exact details
may likewise be changed in a subclass, by over-
riding the repeatedItems method.
4 Pruning Strategies
The classes for defining edge pruning strategies
appear in Figure 14. As mentioned in Section 2,
an N-best pruning strategy is employed by default,
where N is determined by the current preference
settings. It is also possible to define custom strate-
gies. To support the definition of a certain kind
## Simplified COMIC realizer FLM spec file
## Trigram Word model based on previous words and accents, dropping accents first,
## with bigram sub-model;
## Unigram Gesture Class model based on current word; and
## Unigram Gesture Instance model based on current gesture class
4
## 3gram with A
W : 4 W(-1) W(-2) A(-1) A(-2) w_w1w2a1a2.count w_w1w2a1a2.lm 5
W1,W2,A1,A2 A2 ndiscount gtmin 1
W1,W2,A1 A1 ndiscount gtmin 1
W1,W2 W2 ndiscount gtmin 1
W1 W1 ndiscount gtmin 1
0 0 ndiscount gtmin 1
## bigram with A
W : 2 W(-1) A(-1) w_w1a1.count w_w1a1.lm 3
W1,A1 A1 ndiscount gtmin 1
W1 W1 ndiscount gtmin 1
0 0 ndiscount gtmin 1
## Gesture class depends on current word
GC : 1 W(0) gc_w0.count gc_w0.lm 2
W0 W0 ndiscount gtmin 1
0 0 ndiscount gtmin 1
## Gesture instance depends only on class
GI : 1 GC(0) gi_gc0.count gi_gc0.lm 2
GC0 GC0 ndiscount gtmin 1
0 0
Figure 12: Example factored language model family specification
// set up n-gram scorer and repetition scorer
String lmfile = "ngrams/combined.flm";
boolean semClasses = true;
NgramScorer ngramScorer = new FactoredNgramModelFamily(lmfile , semClasses );
ngramScorer.addFilter(new AAnFilter ());
RepetitionScorer repetitionScorer = new RepetitionScorer ();
// combine n-gram scorer with repetition scorer
realizer.signScorer = new SignScorerProduct(
new SignScorer [] { ngramScorer , repetitionScorer }
);
// ... then , after each realization request ,
Edge bestEdge = realizer.realize(lf);
// ... update repetition context for next realization:
repetitionScorer.ageContext ();
repetitionScorer.updateContext(bestEdge.getSign ());
Figure 13: Example combination of an n-gram scorer and a repetition scorer
of custom strategy, the abstract class Diversity-
PruningStrategy provides an N-best pruning
strategy that promotes diversity in the edges that
are kept, according to the equivalence relation
established by the abstract notCompellingly-
Different method. In particular, in order to de-
termine which edges to keep, a diversity pruning
strategy clusters the edges into a ranked list of
equivalence classes, which are sequentially sam-
pled until the limit N is reached. If the single-
BestPerGroup flag is set, then a maximum of one
edge per equivalence class is retained.
As an example, the COMIC realizer?s diversity
pruning strategy appears in Figure 15. The idea
behind this strategy is to avoid having the N-best
lists become full of signs whose words differ only
in the exact gesture instance associated with one
or more of the words. With this strategy, if two
signs differ in just this way, the edge for the lower-
scoring sign will be considered ?not compellingly
different? and pruned from the N-best list, mak-
ing way for other edges whose signs exhibit more
interesting differences.
OpenCCG also provides a concrete subclass
of DiversityPruningStrategy named Ngram-
DiversityPruningStrategy, which general-
izes the approach to pruning described in (Langk-
ilde, 2000). With this class, two signs are consid-
ered not compellingly different if they share the
same n?1 initial and final words, where n is the
n-gram order. When one is interested in single-
best output, an n-gram diversity pruning strategy
can increase efficiency while guaranteeing no loss
in quality?as long as the reduction in the search
space outweighs the extra time necessary to check
for the same initial and final words?since any
words in between an input sign?s n?1 initial and
final ones cannot affect the n-gram score of a
new sign formed from the input sign. However,
when N-best outputs are desired, or when repeti-
tion scoring is employed, it is less clear whether
it makes sense to use an n-gram diversity pruning
strategy; for this reason, a simple N-best strategy
remains the default option.
5 Conclusions and Future Work
In this paper, we have presented OpenCCG?s ex-
tensible API for efficiently integrating language
modeling and realization, in order to select realiza-
tions with preferred word orders, promote align-
ment with a conversational partner, avoid repeti-
tive language use, and increase the speed of the
best-first anytime search. As we have shown,
the design enables a variety of n-gram models
to be easily combined and used in conjunction
with appropriate edge pruning strategies. The n-
gram models may be of any order, operate in re-
verse (?right-to-left?), and selectively replace cer-
tain words with their semantic classes. Factored
language models with generalized backoff may
also be employed, over words represented as bun-
dles of factors such as form, pitch accent, stem,
part of speech, supertag, and semantic class.
In future work, we plan to further explore
how to best employ factored language models; in
particular, inspired by (Bangalore and Rambow,
2000), we plan to examine whether factored lan-
guage models using supertags can provide an ef-
fective way to combine syntactic and lexical prob-
abilities. We also plan to implement the capabil-
ity to use one-of alternations in the input logical
forms (Foster and White, 2004), in order to more
efficiently defer lexical choice decisions to the lan-
guage models.
Acknowledgements
Thanks to Jason Baldridge, Carsten Brockmann,
Mary Ellen Foster, Philipp Koehn, Geert-Jan Krui-
jff, Johanna Moore, Jon Oberlander, Miles Os-
borne, Mark Steedman, Sebastian Varges and the
anonymous reviewers for helpful discussion.
    ?interface?  
 PruningStrategy 
 +pruneEdges(edges: List<Edge>): List<Edge> 
 NBestPruningStrategy 
 #CAT_PRUNE_VAL: int 
 DiversityPruningStrategy 
 +singleBestPerGroup: boolean 
 +notCompellinglyDifferent(sign1: Sign, sign2: Sign): boolean 
 NgramDiversityPruningStrategy 
 #order: int 
 +notCompellinglyDifferent(sign1: Sign, sign2: Sign): boolean 
 Returns the edges pruned from   
 the given ones, which always have   
 equivalent categories and are   
 sorted by score. 
 Keeps only the n?best edges. 
 Prunes edges that are not  
 compellingly different. 
 Defines edges to be not compellingly different   
 when the n?1 initial and final words are the same   
 (where n is the order). 
Figure 14: Classes for defining pruning strategies
// configure realizer with gesture diversity pruner
realizer.pruningStrategy = new DiversityPruningStrategy () {
/**
* Returns true iff the given signs are not compellingly different;
* in particular , returns true iff the words differ only in their
* gesture instances. */
public boolean notCompellinglyDifferent(Sign sign1 , Sign sign2) {
List words1 = sign1.getWords (); List words2 = sign2.getWords ();
if (words1.size() != words2.size ()) return false;
for (int i = 0; i < words1.size (); i++) {
Word w1 = (Word) words1.get(i); Word w2 = (Word) words2.get(i);
if (w1 == w2) continue;
if (w1.getForm () != w2.getForm ()) return false;
if (w1.getPitchAccent () != w2.getPitchAccent ()) return false;
if (w1.getVal("GC") != w2.getVal("GC")) return false;
// nb: assuming that they differ in the val of GI at this point
}
return true;
}
};
Figure 15: Example diversity pruning strategy
References
Jason Baldridge. 2002. Lexically Specified Deriva-
tional Control in Combinatory Categorial Gram-
mar. Ph.D. thesis, School of Informatics, University
of Edinburgh.
Srinivas Bangalore and Owen Rambow. 2000. Ex-
ploiting a probabilistic hierarchical model for gen-
eration. In Proc. COLING-00.
Jeff Bilmes and Katrin Kirchhoff. 2003. Factored lan-
guage models and general parallelized backoff. In
Proc. HLT-03.
Carsten Brockmann, Amy Isard, Jon Oberlander, and
Michael White. 2005. Variable alignment in affec-
tive dialogue. In Proc. UM-05 Workshop on Affec-
tive Dialogue Systems. To appear.
John Carroll, Ann Copestake, Dan Flickinger, and Vic-
tor Poznan?ski. 1999. An efficient chart generator
for (semi-) lexicalist grammars. In Proc. EWNLG-
99.
Mary Ellen Foster and Michael White. 2004. Tech-
niques for Text Planning with XSLT. In Proc. 4th
NLPXML Workshop.
Martin Kay. 1996. Chart generation. In Proc. ACL-96.
Katrin Kirchhoff, Jeff Bilmes, Sourin Das, Nico-
lae Duta, Melissa Egan, Gang Ji, Feng He, John
Henderson, Daben Liu, Mohamed Noamany, Pat
Schone, Richard Schwartz, and Dimitra Vergyri.
2002. Novel Approaches to Arabic Speech Recogni-
tion: Report from the 2002 Johns-Hopkins Summer
Workshop.
Kevin Knight and Vasileios Hatzivassiloglou. 1995.
Two-level, many-paths generation. In Proc. ACL-
95.
Irene Langkilde-Geary. 2002. An empirical verifi-
cation of coverage and correctness for a general-
purpose sentence generator. In Proc. INLG-02.
Irene Langkilde. 2000. Forest-based statistical sen-
tence generation. In Proc. NAACL-00.
Robert Malouf. 2000. The order of prenominal adjec-
tives in natural language generation. In Proc. ACL-
00.
Johanna Moore, Mary Ellen Foster, Oliver Lemon, and
Michael White. 2004. Generating tailored, com-
parative descriptions in spoken dialogue. In Proc.
FLAIRS-04.
Robert C. Moore. 2002. A complete, efficient
sentence-realization algorithm for unification gram-
mar. In Proc. INLG-02.
Alice H. Oh and Alexander I. Rudnicky. 2002.
Stochastic natural language generation for spoken
dialog systems. Computer, Speech & Language,
16(3/4):387?407.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2001. Bleu: a Method for Automatic
Evaluation of Machine Translation. Technical Re-
port RC22176, IBM.
Adwait Ratnaparkhi. 2002. Trainable approaches to
surface natural language generation and their appli-
cation to conversational dialog systems. Computer,
Speech & Language, 16(3/4):435?455.
Eric S. Ristad. 1995. A Natural Law of Succession.
Technical Report CS-TR-495-95, Princeton Univ.
James Shaw and Vasileios Hatzivassiloglou. 1999. Or-
dering among premodifiers. In Proc. ACL-99.
Hadar Shemtov. 1997. Ambiguity Management in Nat-
ural Language Generation. Ph.D. thesis, Stanford
University.
Mark Steedman. 2000a. Information structure and
the syntax-phonology interface. Linguistic Inquiry,
31(4):649?689.
Mark Steedman. 2000b. The Syntactic Process. MIT
Press.
Andreas Stolcke. 2002. SRILM ? An extensible lan-
guage modeling toolkit. In Proc. ICSLP-02.
Michael White and Jason Baldridge. 2003. Adapting
Chart Realization to CCG. In Proc. EWNLG-03.
Michael White. 2004a. Efficient Realization of Coor-
dinate Structures in Combinatory Categorial Gram-
mar. Research on Language and Computation. To
appear.
Michael White. 2004b. Experiments with multimodal
output in human-machine interaction. IST Project
COMIC Public Deliverable 7.4.
Michael White. 2004c. Reining in CCG Chart Real-
ization. In Proc. INLG-04.
Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 410?419,
Singapore, 6-7 August 2009.
c
?2009 ACL and AFNLP
Perceptron Reranking for CCG Realization
Michael White and Rajakrishnan Rajkumar
Department of Linguistics
The Ohio State University
Columbus, OH, USA
{mwhite,raja}@ling.osu.edu
Abstract
This paper shows that discriminative
reranking with an averaged perceptron
model yields substantial improvements in
realization quality with CCG. The paper
confirms the utility of including language
model log probabilities as features in the
model, which prior work on discrimina-
tive training with log linear models for
HPSG realization had called into question.
The perceptron model allows the combina-
tion of multiple n-gram models to be opti-
mized and then augmented with both syn-
tactic features and discriminative n-gram
features. The full model yields a state-
of-the-art BLEU score of 0.8506 on Sec-
tion 23 of the CCGbank, to our knowledge
the best score reported to date using a re-
versible, corpus-engineered grammar.
1 Introduction
In this paper, we show how discriminative train-
ing with averaged perceptron models (Collins,
2002) can be used to substantially improve surface
realization with Combinatory Categorial Gram-
mar (Steedman, 2000, CCG). Velldal and Oepen
(2005) and Nakanishi et al (2005) have shown that
discriminative training with log-linear (maximum
entropy) models is effective in realization rank-
ing with Head-Driven Phrase Structure Grammar
(Pollard and Sag, 1994, HPSG). Here we show
that averaged perceptron models also perform well
for realization ranking with CCG. Averaged per-
ceptron models are very simple, just requiring a
decoder and a simple update function, yet despite
their simplicity they have been shown to achieve
state-of-the-art results in Treebank and CCG pars-
ing (Huang, 2008; Clark and Curran, 2007a) as
well as on other NLP tasks.
Along the way, we address the question of
whether it is beneficial to incorporate n-gram log
probabilities as baseline features in a discrimina-
tively trained realization ranking model. On a lim-
ited domain corpus, Velldal & Oepen found that
including the n-gram log probability of each can-
didate realization as a feature in their log-linear
model yielded a substantial boost in ranking per-
formance; on the Penn Treebank (PTB), however,
Nakanishi et al found that including an n-gram log
prob feature in their model was of no benefit (with
the use of bigrams instead of 4-grams suggested as
a possible explanation). With these mixed results,
the utility of n-gram baseline features for PTB-
scale discriminative realization ranking has been
unclear. In our particular setting, the question is:
Do n-gram log prob features improve performance
in broad coverage realization ranking with CCG,
where factored language models over words, part-
of-speech tags and supertags have previously been
employed (White et al, 2007; Espinosa et al,
2008)?
We answer this question in the affirmative, con-
firming the results of Velldal & Oepen, despite
the differences in corpus size and kind of lan-
guage model. We show that including n-gram log
prob features in the perceptron model is highly
beneficial, as the discriminative models we tested
without these features performed worse than the
generative baseline. These findings are in line
with Collins & Roark?s (2004) results with incre-
mental parsing with perceptrons, where it is sug-
gested that a generative baseline feature provides
the perceptron algorithm with a much better start-
ing point for learning. We also show that discrim-
inative training allows the combination of multi-
ple n-gram models to be optimized, and that the
best model augments the n-gram log prob fea-
tures with both syntactic features and discrimina-
tive n-gram features. The full model yields a state-
of-the-art BLEU (Papineni et al, 2002) score of
0.8506 on Section 23 of the CCGbank, which is
to our knowledge the best score reported to date
410
using a reversible, corpus-engineered grammar.
The paper is organized as follows. Section 2 re-
views previous work on broad coverage realization
with OpenCCG. Section 3 describes our approach
to realization reranking with averaged perceptron
models. Section 4 presents our evaluation of the
perceptron models, comparing the results of dif-
ferent feature sets. Section 5 compares our results
to those obtained by related systems and discusses
the difficulties of cross-system comparisons. Fi-
nally, Section 6 concludes with a summary and
discussion of future directions for research.
2 Background
2.1 Surface Realization with CCG
CCG (Steedman, 2000) is a unification-based cat-
egorial grammar formalism which is defined al-
most entirely in terms of lexical entries that encode
sub-categorization information as well as syntactic
feature information (e.g. number and agreement).
Complementing function application as the stan-
dard means of combining a head with its argu-
ment, type-raising and composition support trans-
parent analyses for a wide range of phenomena,
including right-node raising and long distance de-
pendencies. An example syntactic derivation ap-
pears in Figure 1, with a long-distance depen-
dency between point and make. Semantic com-
position happens in parallel with syntactic compo-
sition, which makes it attractive for generation.
OpenCCG is a parsing/generation library which
works by combining lexical categories for words
using CCG rules and multi-modal extensions on
rules (Baldridge, 2002) to produce derivations.
Surface realization is the process by which logical
forms are transduced to strings. OpenCCG uses
a hybrid symbolic-statistical chart realizer (White,
2006) which takes logical forms as input and pro-
duces sentences by using CCG combinators to
combine signs. Edges are grouped into equiva-
lence classes when they have the same syntactic
category and cover the same parts of the input log-
ical form. Alternative realizations are ranked us-
ing integrated n-gram or perceptron scoring, and
pruning takes place within equivalence classes of
edges. To more robustly support broad coverage
surface realization, OpenCCG greedily assembles
fragments in the event that the realizer fails to find
a complete realization.
To illustrate the input to OpenCCG, consider
the semantic dependency graph in Figure 2. In
aa1
heh3
he h2
<Det>
<Arg0> <Arg1>
<TENSE>pres
<NUM>sg
<Arg0>
w1 want.01
m1
<Arg1>
<GenRel>
<Arg1>
<TENSE>pres
p1point
h1have.03
make.03
<Arg0>
s[b]\np/np
np/n
np
n
s[dcl]\np/np
s[dcl]\np/(s[to]\np)
np
Figure 2: Semantic dependency graph from the
CCGbank for He has a point he wants to make
[. . . ], along with gold-standard supertags (cate-
gory labels)
the graph, each node has a lexical predication
(e.g. make.03) and a set of semantic features
(e.g. ?NUM?sg); nodes are connected via depen-
dency relations (e.g. ?ARG0?). (Gold-standard su-
pertags, or category labels, are also shown; see
Section 2.4 for their role in hypertagging.) In-
ternally, such graphs are represented using Hy-
brid Logic Dependency Semantics (HLDS), a
dependency-based approach to representing lin-
guistic meaning (Baldridge and Kruijff, 2002). In
HLDS, each semantic head (corresponding to a
node in the graph) is associated with a nominal
that identifies its discourse referent, and relations
between heads and their dependents are modeled
as modal relations.
2.2 Realization from an Enhanced CCGbank
Our starting point is an enhanced version of the
CCGbank (Hockenmaier and Steedman, 2007)?a
corpus of CCG derivations derived from the Penn
Treebank?with Propbank (Palmer et al, 2005)
roles projected onto it (Boxwell and White, 2008).
To engineer a grammar from this corpus suitable
for realization with OpenCCG, the derivations are
first revised to reflect the lexicalized treatment
of coordination and punctuation assumed by the
multi-modal version of CCG that is implemented
in OpenCCG (White and Rajkumar, 2008). Fur-
ther changes are necessary to support semantic de-
pendencies rather than surface syntactic ones; in
411
He has a point he wants to make
np s
dcl
\np/np np/n n np s
dcl
\np/(s
to
\np) s
to
\np/(s
b
\np) s
b
\np/np
> >T >B
np s/(s\np) s
to
\np/np
>B
s
dcl
\np/np
>B
s
dcl
/np
np\np
<
np
>
s
dcl
\np
<
s
dcl
Figure 1: Syntactic derivation from the CCGbank for He has a point he wants to make [. . . ]
particular, the features and unification constraints
in the categories related to semantically empty
function words such complementizers, infinitival-
to, expletive subjects, and case-marking preposi-
tions are adjusted to reflect their purely syntactic
status.
In the second step, a grammar is extracted from
the converted CCGbank and augmented with log-
ical forms. Categories and unary type chang-
ing rules (corresponding to zero morphemes) are
sorted by frequency and extracted if they meet the
specified frequency thresholds. A separate trans-
formation then uses a few dozen generalized tem-
plates to add logical forms to the categories, in a
fashion reminiscent of (Bos, 2005). As shown in
Figure 2, numbered semantic roles are taken from
PropBank when available, and more specific rela-
tions are introduced in the categories for closed-
class items such as determiners.
After logical form insertion, the extracted and
augmented grammar is loaded and used to parse
the sentences in the CCGbank according to the
gold-standard derivation. If the derivation can
be successfully followed, the parse yields a log-
ical form which is saved along with the corpus
sentence in order to later test the realizer. Cur-
rently, the algorithm succeeds in creating logical
forms for 98.85% of the sentences in the develop-
ment section (Sect. 00) of the converted CCGbank,
and 97.06% of the sentences in the test section
(Sect. 23). Of these, 95.99% of the development
LFs are semantic dependency graphs with a sin-
gle root, while 95.81% of the test LFs have a sin-
gle root. The remaining cases, with multiple roots,
are missing one or more dependencies required to
form a fully connected graph. Such missing de-
pendencies usually reflect remaining inadequacies
in the logical form templates.
An error analysis of OpenCCG output by Ra-
jkumar et al (2009) recently revealed that out of
2331 named entities (NEs) annotated by the BBN
corpus (Weischedel and Brunstein, 2005), 238
were not realized correctly. For example, multi-
word NPs like Texas Instruments Japan Ltd. were
realized as Japan Texas Instruments Ltd. Accord-
ingly, inspired by Hogan et al?s (2007)?s Experi-
ment 1, Rajkumar et al used the BBN corpus NE
annotation to collapse certain classes of NEs. But
unlike Hogan et al?s experiment where all the NEs
annotated by the BBN corpus were collapsed, Ra-
jkumar et al chose to collapse into single tokens
only NEs whose exact form can be reasonably ex-
pected to be specified in the input to the realizer.
For example, while some quantificational or com-
paratives phrases like more than $ 10,000 are an-
notated as MONEY in the BBN corpus, Rajkumar
et al only collapse $ 10,000 into an atomic unit,
with more than handled compositionally accord-
ing to the semantics assigned to it by the gram-
mar. Thus, after transferring the BBN annotations
to the CCGbank corpus, Rajkumar et al (partially)
collapsed NEs which are CCGbank constituents
according to the following rules: (1) completely
collapse the PERSON, ORGANIZATION, GPE,
WORK OF ART major class type entitites; (2) ig-
nore phrases like three decades later, which are
annotated as DATE entities; and (3) collapse all
phrases with POS tags CD or NNP(S) or lexical
items % or $, ensuring that all prototypical named
entities are collapsed.
It is worth noting that improvements in our
corpus-based grammar engineering process?
including a more precise treatment of punctuation,
better named entity handling and the addition of
catch-all logical form templates?have resulted in
a 13.5 BLEU point improvement in our baseline
realization scores on Section 00 of the CCGbank,
from a score of 0.6567 in (Espinosa et al, 2008)
to 0.7917 in (Rajkumar et al, 2009), contribut-
ing greatly to the state-of-the-art results reported
412
in Section 4. A further 4.5 point improvement is
obtained from the use of named entity classes in
language modeling and hypertagging (Rajkumar
et al, 2009), as described next, and from our per-
ceptron reranking model, described in Section 3.
2.3 Factored Language Models
As in (White et al, 2007; Rajkumar et al, 2009),
we use factored language models (Bilmes and
Kirchhoff, 2003) over words, part-of-speech tags
and supertags
1
to score partial and complete real-
izations. The trigram models were created using
the SRILM toolkit (Stolcke, 2002) on the standard
training sections (02?21) of the CCGbank, with
sentence-initial words (other than proper names)
uncapitalized. While these models are consider-
ably smaller than the ones used in (Langkilde-
Geary, 2002; Velldal and Oepen, 2005), the train-
ing data does have the advantage of being in the
same domain and genre. The models employ in-
terpolated Kneser-Ney smoothing with the default
frequency cutoffs. The best performing model
interpolates three component models using rank-
order centroid weights: (1) a word trigram model;
(2) a word model with semantic classes replac-
ing named entities; and (3) a trigram model that
chains a POS model with a supertag model, where
the POS model (P ) conditions on the previous two
POS tags, and the supertag model (S) conditions
on the previous two POS tags as well as the current
one, as shown below:
p
PS
(
~
F
i
|
~
F
i?1
i?2
) = p(P
i
| P
i?1
i?2
)p(S
i
| P
i
i?2
) (1)
Training data for the semantic class?replaced
model was created by replacing (collapsed) words
with their NE classes, in order to address data spar-
sity issues caused by rare words in the same se-
mantic class. For example, the Section 00 sen-
tence Pierre Vinken , 61 years old , will join the
board as a nonexecutive director Nov. 29 . be-
comes PERSON , DATE:AGE DATE:AGE old ,
will join the ORG DESC:OTHER as a nonexecu-
tive PER DESC DATE:DATE DATE:DATE . Dur-
ing realization, word forms are generated, but are
then replaced by their semantic classes for scoring
using the semantic class?replaced model, similar
to Oh and Rudnicky (2002).
Note that the use of supertags in the factored
language model to score possible realizations is
1
With CCG, supertags (Bangalore and Joshi, 1999) are
lexical categories considered as fine-grained syntactic labels.
distinct from the prediction of supertags for lexical
category assignment: the former takes the words
in the local context into account (as in supertag-
ging for parsing), while the latter takes features of
the logical form into account. This latter process
we call hypertagging, to which we now turn.
2.4 Hypertagging
A crucial component of the OpenCCG realizer is
the hypertagger (Espinosa et al, 2008), or su-
pertagger for surface realization, which uses a
maximum entropy model to assign the most likely
lexical categories to the predicates in the input log-
ical form, thereby greatly constraining the real-
izer?s search space.
2
Figure 2 shows gold-standard
supertags for the lexical predicates in the graph;
such category labels are predicted by the hyper-
tagger at run-time. As in recent work on using
supertagging in parsing, the hypertagger operates
in a multitagging paradigm (Curran et al, 2006),
where a variable number of predictions are made
per input predicate. Instead of basing category as-
signment on linear word and POS context, how-
ever, the hypertagger predicts lexical categories
based on contexts within a directed graph structure
representing the logical form (LF) of the sentence
to be realized. The hypertagger generalizes Ban-
galore and Rambow?s (2000) method of using su-
pertags in generation by using maximum entropy
models with a larger local context.
During realization, the hypertagger returns a ?-
best list of supertags in order of decreasing prob-
ability. Increasing the number of categories re-
turned clearly increases the likelihood that the
most-correct supertag is among them, but at a cor-
responding cost in chart size. Accordingly, the hy-
pertagger begins with a highly restrictive value for
?, and backs off to progressively less-restrictive
values if no complete realization can be found us-
ing the set of supertags returned. Clark and Curran
(2007b) have shown this iterative relaxation strat-
egy to be highly effective in CCG parsing.
3 Perceptron Reranking
As Collins (2002) observes, perceptron training
involves a simple, on-line algorithm, with few it-
erations typically required to achieve good perfor-
mance. Moreover, averaged perceptrons?which
2
The approach has been dubbed hypertagging since it op-
erates at a level ?above? the syntax, moving from semantic
representations to syntactic categories.
413
Input: training examples (x
i
, y
i
)
Initialization: set ? = 0, or use optional input
model
Algorithm:
for t = 1 . . . T , i = 1 . . . N
z
i
= argmax
y?GEN(x
i
)
?(x
i
, y) ? ?
if z
i
6= y
i
? = ? + ?(x
i
, y
i
) ? ?(x
i
, z
i
)
Output: ? =
?
T
t=1
?
N
i=1
?
ti
/TN
Figure 3: Averaged perceptron training algorithm
approximate voted perceptrons, a maximum-
margin method with attractive theoretical
properties?seem to work remarkably well in
practice, while adding little further complexity.
Additionally, since features only take on non-
zero values when they appear in training items
requiring updates, perceptrons integrate feature
selection with, and often produce quite small
models, especially when starting with a good
baseline.
The generic averaged perceptron training algo-
rithm appears in Figure 3. In our case, the algo-
rithm trains a model for reranking the n-best real-
izations generated using our existing factored lan-
guage model for scoring, with the oracle-best re-
alization considered the correct answer. Accord-
ingly, the input to the algorithm is a list of pairs
(x
i
, y
i
), where x
i
is a logical form, GEN(x
i
) are
the n-best realizations for x
i
, and y
i
is the oracle-
best member of GEN(x
i
). The oracle-best realiza-
tion is determined using a 4-gram precision metric
(approximating BLEU) against the reference sen-
tence.
We have followed Huang (2008) in using
oracle-best targets for training, rather than gold
standard ones, in order to better approximate test
conditions during training. However, following
Clark & Curran (2007a), during training we seed
the realizer with the gold-standard supertags, aug-
menting the hypertagger?s ?-best list, in order to
ensure that the n-best realizations are generally of
high quality; consequently, the gold standard real-
ization (i.e., the corpus sentence) usually appears
in the n-best list.
3
In addition, we use a hyper-
tagger trained on all the training data, to improve
hypertagger performance, while excluding the cur-
3
As in Clark & Curran?s approach, we use a single ? value
during training, rather than iteratively loosening the ? value;
the chosen ? value determines the size of the discrimation
space.
rent training section (in jack-knifed fashion) from
the word-based parts of the language model, in or-
der to make the language model scores more re-
alistic. It remains for future work to determine
whether using a different compromise between en-
suring high-quality training data and remaining
faithful to the test conditions would yield better
results.
Since realization of the n-best lists for train-
ing is the most time-consuming part of the pro-
cess, in our current implementation we perform
this step once, generating event files along the way
containing feature vectors for each candidate real-
ization. The event files are used to calculate the
frequency distribution for the features, and mini-
mum cutoffs are chosen to trim the feature alpha-
bet to a reasonable size. Training then takes place
by iterating over the event files, ignoring features
that do not appear in the alphabet. As Figure 3
indicates, training consists of calculating the top-
ranked realization according to the current model
?, and performing an update when the top-ranked
realization does not match the oracle-best realiza-
tion. Updates to the model add the feature vec-
tor ?(x
i
, y
i
) for the missed oracle-best realiza-
tion, and subtract the feature vector ?(x
i
, z
i
) for
the mistakenly top-ranked realization. The final
model averages the models across the T iterations
over the training data, andN test cases within each
iteration.
Note that while training the perceptron model
involves n-best reranking, realization with the re-
sulting model can be viewed as forest rescoring,
since scoring of all partial realizations is integrated
into the realizer?s beam search. In future work, we
intend to investigate saving the realizer?s packed
charts, rather than event files, and integrating the
unpacking of the charts with the perceptron train-
ing algorithm.
The features we employ in our perceptron mod-
els are of three kinds. First, as in the log-linear
models of Velldal & Oepen and Nakanishi et al,
we incorporate the log probability of the candidate
realization?s word sequence according to our fac-
tored language model as a single feature in the per-
ceptron model. Since our language model linearly
interpolates three component models, we also in-
clude the log prob from each component language
model as a feature, so that the combination of
these components can be optimized.
Second, we include syntactic features in our
414
Feature Type Example
LexCat + Word s/s/np + before
LexCat + POS s/s/np + IN
Rule s
dcl
? np s
dcl
\np
Rule + Word s
dcl
? np s
dcl
\np + bought
Rule + POS s
dcl
? np s
dcl
\np + VBD
Word-Word ?company, s
dcl
? np s
dcl
\np, bought?
Word-POS ?company, s
dcl
? np s
dcl
\np, VBD?
POS-Word ?NN, s
dcl
? np s
dcl
\np, bought?
Word + ?
w
?bought, s
dcl
? np s
dcl
\np? + d
w
POS + ?
w
?VBD, s
dcl
? np s
dcl
\np? + d
w
Word + ?
p
?bought, s
dcl
? np s
dcl
\np? + d
p
POS + ?
p
?VBD, s
dcl
? np s
dcl
\np? + d
p
Word + ?
v
?bought, s
dcl
? np s
dcl
\np? + d
v
POS + ?
v
?VBD, s
dcl
? np s
dcl
\np? + d
v
Table 1: Basic and dependency features from
Clark & Curran?s (2007b) normal form model;
distances are in intervening words, punctuation
marks and verbs, and are capped at 3, 3 and 2,
respectively
model by implementing Clark & Curran?s (2007b)
normal form model in OpenCCG.
4
The features of
this model are listed in Table 1; they are integer-
valued, representing counts of occurrences in a
derivation. These syntactic features are quite com-
parable to the dominance-oriented features in the
union of the Velldal & Oepen and Nakanishi et
al. models, except that our feature set does not
include grandparenting, which has been found to
have limited utility in CCG parsing. Our syntac-
tic features also include ones that measure the dis-
tance between headwords in terms of intervening
words, punctuation marks or verbs; these features
generalize the ones in Nakanishi et al?s model.
Note that in contrast to parsing, in realization dis-
tance features are non-local, since different partial
realizations in the same equivalence class typically
differ in word order; as we are working in a rerank-
ing paradigm though, the non-local nature of these
features is unproblematic.
Third, we include discriminative n-gram fea-
tures in our model, following Roark et al?s (2004)
approach to discriminative n-gram modeling for
speech recognition. By discriminative n-gram fea-
tures, we mean features counting the occurrences
of each n-gram that is scored by our factored lan-
guage model, rather than a feature whose value is
the log prob determined by the language model.
As Roark et al note, discriminative training with
n-gram features has the potential to learn to nega-
4
We have omitted Clark & Curran?s root features, since
the category we use for the full stop ensures that it must ap-
pear at the root of any complete derivation.
Model #Alph-feats #Feats Acc Time
full-model 2402173 576176 96.40% 08:53
lp-ngram 1127437 342025 94.52% 05:19
lp-syn 1274740 291728 85.03% 05:57
Table 2: Perceptron Training Details?number of
features in the alphabet, number of features in the
model, training accuracy and training time (hours)
for 10 iterations on a single commodity server
tively weight n-grams that appear in some of the
GEN(x
i
) candidates, but which never appear in
the naturally occurring corpus used to train a stan-
dard, generative language model. Since our fac-
tored language model considers words, semantic
classes, part-of-speech tags and supertags, our n-
gram features represent a considerable generaliza-
tion of the sequence-oriented features in Velldal
& Oepen?s model, which never contain more than
one word and do not include semantic classes.
4 Evaluation
4.1 Experimental Conditions
For the experiments reported below, we used a
lexico-grammar extracted from Sections 02?21 of
our enhanced CCGbank, a hypertagging model in-
corporating named entity class features, and a tri-
gram factored language model over words, named
entity classes, part-of-speech tags and supertags,
as described in the preceding section. BLEU
scores were calculated after removing the under-
scores between collapsed NEs.
Events were generated for each training section
separately. As already noted, the hypertagger and
POS/supertag language model was trained on all
the training sections, while separate word-based
models were trained excluding each of the train-
ing sections in turn. Event files for 26530 training
sentences with complete realizations were gener-
ated in 7 hours and 16 minutes on a cluster us-
ing one commodity server per section, with an av-
erage n-best list size of 18.2. Perceptron models
were trained on single machines; details for three
of the models appear in Table 2. The complete set
of models is listed in Table 3.
4.2 Results
Realization results on the development section are
given in Table 4. As the first block of rows af-
ter the baseline shows, of the models incorporating
a single kind of feature, only the one with the n-
gram log prob features beats the baseline BLEU
415
Model Description
baseline-w3 No perceptron (3g wd only)
baseline No perceptron
syn-only-nodist All syntactic features except distance
ngram-only Just ngram features
syn-only Just syntactic features
lp-only Just log prob features
lp-ngram Log prob + Ngram features
lp-syn Log prob + Syntactic features
full-model Log prob + Ngram +Syntactic features
Table 3: Legend for Experimental Conditions
score, with the other models falling well below
the baseline (though faring better than the trigram-
word LM baseline). This result confirms the im-
portance of including n-gram log prob features in
discriminative realization ranking models, in line
with Velldal & Oepen?s findings, and contra those
of Nakanishi et al, even though it was Nakanishi
et al who experimented with the Penn Treebank
corpus, while Velldal &Oepen?s experiments were
on a much smaller, limited domain corpus. The
second block of rows shows that both the discrim-
inative n-gram features and the syntactic features
provide a substantial boost when used with the n-
gram log prob features, with the syntactic features
yielding a more than 3 BLEU point gain. The
final row shows that the full model works best,
though the boosts provided by the syntactic and
discriminative n-gram features are clearly not in-
dependent. The BLEU point trends are mirrored in
the percentage of exact match realizations, which
goes up by more than 10% from the baseline. The
percentage of complete (i.e., non-fragmentary) re-
alizations, however, goes down; we expect that
this is due to the time taken up by our current
naive method of feature extraction, which does not
cache the features calculated for partial realiza-
tions. Realization results on the standard test sec-
tion appear in Table 5, confirming the gains made
by the full model over the baseline.
5
We calculated statistical significance for the
main results on the development section using
bootstrap random sampling.
6
After re-sampling
1000 times, significance was calculated using a
paired t-test (999 d.f.). The results indicated that
lp-only exceeded the baseline, lp-ngram and lp-
5
Note that the baseline for Section 23 uses 4-grams and a
filter for balanced punctuation (White and Rajkumar, 2008),
unlike the other reported configurations, which would explain
the somewhat smaller increase seen with this section.
6
Scripts for running these tests are available at
http://projectile.sv.cmu.edu/research/
public/tools/bootStrap/tutorial.htm
Model %Exact %Compl. BLEU Time
baseline-w3 26.00 83.15 0.7646 1.8
baseline 29.00 83.28 0.7963 2.0
syn-only-nodist 26.02 82.69 0.7754 3.2
ngram-only 27.67 82.95 0.7777 3.0
syn-only 28.34 82.74 0.7838 3.4
lp-only 32.01 83.02 0.8009 2.1
lp-ngram 36.31 80.47 0.8183 3.1
lp-syn 39.47 79.74 0.8323 3.5
full-model 40.11 79.63 0.8373 3.6
Table 4: Section 00 Results (98.9% coverage)?
percentage of exact match and grammatically
complete realizations, BLEU scores and average
times, in seconds
Model %Exact %Complete BLEU
baseline 33.74 85.04 0.8173
full-model 40.45 83.88 0.8506
Table 5: Section 23 Results (97.06% coverage)
syn exceeded lp-only, and the full model exceeded
lp-syn, with p < 0.0001 in each case.
4.3 Examples
Table 6 presents four examples where the full
model improves upon the baseline. Example sen-
tence wsj 0020.10 in Table 6 is a case where the
perceptron successfully weights the component
ngram models, as the lp-ngram model and those
that build on it get it right. Note that here, the mod-
ifier ordering in small video-viewing is not speci-
fied in the logical form and either ordering is pos-
sible syntactically. In wsj 0024.2, number agree-
ment between the conjoined subject noun phrase
and verb is obtained only with the full model. This
suggests that the full model is more robust to cases
where the grammar is insufficiently precise (num-
ber agreement is enforced by the grammar in only
the simplest cases). Example wsj 0034.9 corrects
a VP ordering mismatch, where the corpus sen-
tence is clearly preferred to the one where into
oblivion is shifted to the end. Finally, wsj 0047.13
corrects an animacy mismatch on the wh-pronoun,
in large part due to the high negative weight as-
signed to the discriminative n-gram feature PER-
SON , which. Note that the full model still dif-
fers from the original sentence in its placement of
the adverb reportedly, choosing the arguably more
natural position following the auxiliary.
4.4 Comparison to Other Systems
Table 7 lists our results in the context of those re-
ported for other systems on PTB Section 23. The
416
Ref-wsj 0020.10 that measure could compel Taipei ?s growing number of small video-viewing parlors to pay ...
baseline,syn-only,ngram-only that measure could compel Taipei ?s growing number of video-viewing small parlors to ...
lp-only, lp-ngram, full-model that measure could compel Taipei ?s growing number of small video-viewing parlors to ...
Ref-wsj 0024.2 Esso Australia Ltd. , a unit of new york-based Exxon Corp. , and Broken Hill Pty. operate the fields ...
all except full-model Esso Australia Ltd. , a unit of new york-based Exxon Corp. , and Broken Hill Pty. operates the fields ...
full-model Esso Australia Ltd. , a unit of new york-based Exxon Corp. , and Broken Hill Pty. operate the fields ...
Ref-wsj 0034.9 they fell into oblivion after the 1929 crash .
baseline, lp-ngram they fell after the 1929 crash into oblivion .
lp-only, ngram-only, syn-only, full-model they fell into oblivion after the 1929 crash .
Ref-wsj 0047.13 Antonio Novello , whom Mr. Bush nominated to serve as surgeon general , reportedly has assured . . .
baseline,baseline-w3, lp-syn, lp-only Antonio Novello , which Mr. Bush nominated to serve as surgeon general , has reportedly assured . . .
full-model, lp-ngram, syn-only, ngram-syn Antonio Novello , whom Mr. Bush nominated to serve as surgeon general , has reportedly assured . . .
Table 6: Examples of realized output
System Coverage BLEU %Exact
Callaway (05) 98.5% 0.9321 57.5
OpenCCG (09) 97.1% 0.8506 40.5
Ringger et al (04) 100.0% 0.836 35.7
Langkilde-Geary (02) 83% 0.757 28.2
Guo et al (08) 100.0% 0.7440 19.8
Hogan et al (07) ?100.0% 0.6882
OpenCCG (08) 96.0% 0.6701 16.0
Nakanishi et al (05) 90.8% 0.7733
Table 7: PTB Section 23 BLEU scores and exact
match percentages in the NLG literature (Nakan-
ishi et al?s results are for sentences of length 20 or
less)
most similar systems to ours are those of Nakan-
ishi et al (2005) and Hogan et al (2007), as they
both involve chart realizers for reversible gram-
mars engineered from the Penn Treebank. While
direct comparisons across systems cannot really
be made when inputs vary in their semantic depth
and specificity, we observe that our all-sentences
BLEU score of 0.8506 exceeds that of Hogan et
al., who report a top score of 0.6882 (though with
coverage near 100%), and also surpasses Nakan-
ishi et al?s score of 0.7733, despite their results be-
ing limited to sentences of length 20 or less (with
91% coverage). Velldal & Oepen?s (2005) system
is also closely related, as noted in the introduc-
tion, but as their experiments are on a limited do-
main corpus, their results cannot be compared at
all meaningfully.
5 Related Work and Discussion
As alluded to above, realization systems cannot be
easily compared, even on the same corpus, when
their inputs are not the same. This point is dra-
matically illustrated in Langkilde-Geary?s (2002)
system, where a BLEU score of 0.514 is reported
for minimally specified inputs on PTB Section 23,
while a score of 0.757 is reported for the ?Per-
mute, no dir? case (which perhaps most closely
resembles our inputs), and a score of 0.924 is re-
ported for the most fully specified inputs; note,
however, that in the latter case word order is deter-
mined by sibling order in the inputs, an assump-
tion not commonly made. As another example,
Guo et al (2008) report a competitive result of
0.7440 (with 100% coverage) using a dependency-
based approach; however, their inputs, like those
of Hogan et al, include more surface syntactic in-
formation than ours, as they specify case-marking
prepositions, wh-pronouns and complementizers.
In a recent experiment to assess the impact of
input specificity, we found that including pred-
icates for all prepositions in our logical forms
boosted our baseline results by more than 3 BLEU
points, with complete realizations found in more
than 90% of the test cases, indicating that generat-
ing from a more surfacy input is indeed an easier
task than generating from a deeper representation.
Given the current lack of consensus on realizer in-
put specificity, we believe it is important to keep
in mind that within-system comparisons (such as
those in the preceding section) are the ones that
should be given the most credence.
Returning to our cross-system comparison, it is
perhaps surprising that Callaway (2005) reports
the best PTB BLEU score to date, 0.9321, with
98.5% coverage, using a purely symbolic, hand-
crafted grammar augmented to handle the most
frequent coverage issues for the PTB. While Call-
away?s inputs are unordered, word order is often
determined by positional features (e.g. front) or
by the type of modification (e.g. describer vs.
qualifier), and parts-of-speech are included
for lexical items. Additionally, in contrast to our
approach, Callaway makes use of a generation-
only grammar, rather than a reversible one, and his
approach is less well-suited to producing n-best
417
outputs. Nevertheless, his high scores do suggest
the potential for precise grammar engineering to
improve realization quality.
While we have yet to perform a thorough er-
ror analysis, our impression is that although the
current set of syntactic features substantially im-
proves clausal constituent ordering, a variety of
disfluent cases remain. More thorough inves-
tigations of features for constituent ordering in
English have been performed by Ringger et al
(2004), Filippova and Strube (2009) and Zhong
and Stent (2009), all of whom develop classifiers
for determining linear order. In future work, we
plan to investigate whether features inspired by
these approaches can be usefully integrated into
our perceptron reranker.
Also related to the present work is discrimina-
tive training in syntax-based MT (Turian et al,
2007; Watanabe et al, 2007; Blunsom et al, 2008;
Chiang et al, 2009). Not surprisingly, since MT is
a harder problem than surface realization, syntax-
based MT systems have made use of less precise
grammars and more impoverished (target-side)
feature sets than those tackling realization rank-
ing. With progress on discriminative training with
large numbers of features in syntax-based MT, the
features found to be useful for high-quality sur-
face realization may become increasingly relevant
for MT as well.
6 Conclusions
In this paper, we have shown how discriminative
reranking with an averaged perceptron model can
be used to achieve substantial improvements in re-
alization quality with CCG. Using a comprehen-
sive feature set, we have also confirmed the util-
ity of including language model log probabilities
as features in the model, which prior work on
discriminative training with log linear models for
HPSG realization had called into question. The
perceptron model allows the combination of mul-
tiple n-gram models to be optimized and then aug-
mented with both syntactic features and discrim-
inative n-gram features, inspired by related work
in discriminative parsing and language modeling
for speech recognition. The full model yields a
state-of-the-art BLEU score of 0.8506 on Section
23 of the CCGbank, to our knowledge the best
score reported to date using a reversible, corpus-
engineered grammar, despite our use of deeper,
less specific inputs. Finally, the perceptron model
paves the way for exploring the utility of richer
feature spaces in statistical realization, including
the use of linguistically-motivated and non-local
features, a topic which we plan to investigate in
future work.
Acknowledgements
This work was supported in part by NSF grant IIS-
0812297 and by an allocation of computing time
from the Ohio Supercomputer Center. Our thanks
also to the OSU Clippers group and the anony-
mous reviewers for helpful comments and discus-
sion.
References
Jason Baldridge and Geert-Jan Kruijff. 2002. Cou-
pling CCG and Hybrid Logic Dependency Seman-
tics. In Proc. ACL-02.
Jason Baldridge. 2002. Lexically Specified Deriva-
tional Control in Combinatory Categorial Gram-
mar. Ph.D. thesis, University of Edinburgh.
Srinivas Bangalore and Aravind K. Joshi. 1999. Su-
pertagging: An Approach to Almost Parsing. Com-
putational Linguistics, 25(2):237?265.
Srinivas Bangalore and Owen Rambow. 2000. Ex-
ploiting a probabilistic hierarchical model for gener-
ation. In Proc. COLING-00.
Jeff Bilmes and Katrin Kirchhoff. 2003. Factored lan-
guage models and general parallelized backoff. In
Proc. HLT-03.
Phil Blunsom, Trevor Cohn, and Miles Osborne. 2008.
A discriminative latent variable model for statistical
machine translation. In Proc. ACL-08: HLT.
Johan Bos. 2005. Towards wide-coverage semantic
interpretation. In Proc. IWCS-6.
Stephen Boxwell andMichaelWhite. 2008. Projecting
Propbank roles onto the CCGbank. In Proc. LREC-
08.
Charles Callaway. 2005. The types and distributions of
errors in a wide coverage surface realizer evaluation.
In Proceedings of the 10th European Workshop on
Natural Language Generation.
David Chiang, Kevin Knight, and Wei Wang. 2009.
11,001 new features for statistical machine transla-
tion. In Proc. NAACL HLT 2009.
Stephen Clark and James Curran. 2007a. Perceptron
training for a wide-coverage lexicalized-grammar
parser. In ACL 2007 Workshop on Deep Linguistic
Processing.
418
Stephen Clark and James R. Curran. 2007b. Wide-
Coverage Efficient Statistical Parsing with CCG and
Log-Linear Models. Computational Linguistics,
33(4):493?552.
Michael Collins and Brian Roark. 2004. Incremen-
tal parsing with the perceptron algorithm. In Proc.
ACL-04.
Michael Collins. 2002. Discriminative training meth-
ods for hidden Markov models: theory and ex-
periments with perceptron algorithms. In Proc.
EMNLP-02.
James R. Curran, Stephen Clark, and David Vadas.
2006. Multi-tagging for lexicalized-grammar pars-
ing. In Proc. COLING/ACL-06.
Dominic Espinosa, Michael White, and Dennis Mehay.
2008. Hypertagging: Supertagging for surface real-
ization with CCG. In Proc. ACL-08: HLT.
Katja Filippova and Michael Strube. 2009. Tree lin-
earization in English: Improving language model
based approaches. In Proc. NAACL HLT 2009 Short
Papers.
Yuqing Guo, Josef van Genabith, and Haifeng Wang.
2008. Dependency-based n-gram models for
general purpose sentence realisation. In Proc.
COLING-08.
Julia Hockenmaier and Mark Steedman. 2007. CCG-
bank: A Corpus of CCG Derivations and Depen-
dency Structures Extracted from the Penn Treebank.
Computational Linguistics, 33(3):355?396.
Deirdre Hogan, Conor Cafferkey, Aoife Cahill, and
Josef van Genabith. 2007. Exploiting multi-word
units in history-based probabilistic generation. In
Proc. EMNLP-CoNLL.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proc. ACL-08:
HLT.
Irene Langkilde-Geary. 2002. An empirical verifi-
cation of coverage and correctness for a general-
purpose sentence generator. In Proc. INLG-02.
Hiroko Nakanishi, Yusuke Miyao, and Jun?ichi Tsujii.
2005. Probabilistic methods for disambiguation of
an HPSG-based chart generator. In Proc. IWPT-05.
Alice H. Oh and Alexander I. Rudnicky. 2002.
Stochastic natural language generation for spoken
dialog systems. Computer, Speech & Language,
16(3/4):387?407.
Martha Palmer, Dan Gildea, and Paul Kingsbury. 2005.
The proposition bank: A corpus annotated with se-
mantic roles. Computational Linguistics, 31(1).
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
of the 40th Annual Meeting of the Association for
Computational Linguistics (ACL), Philadelphia, PA.
Carl Pollard and Ivan Sag. 1994. Head-Driven Phrase
Structure Grammar. University Of Chicago Press.
Rajakrishnan Rajkumar, Michael White, and Dominic
Espinosa. 2009. Exploiting named entity classes in
CCG surface realization. In Proc. NAACL HLT 2009
Short Papers.
Eric Ringger, Michael Gamon, Robert C. Moore,
David Rojas, Martine Smets, and Simon Corston-
Oliver. 2004. Linguistically informed statistical
models of constituent structure for ordering in sen-
tence realization. In Proc. COLING-04.
Brian Roark, Murat Saraclar, Michael Collins, and
Mark Johnson. 2004. Discriminative language
modeling with conditional random fields and the
perceptron algorithm. In Proc. ACL-04.
Mark Steedman. 2000. The syntactic process. MIT
Press, Cambridge, MA, USA.
Andreas Stolcke. 2002. SRILM ? An extensible lan-
guage modeling toolkit. In Proc. ICSLP-02.
Joseph Turian, Benjamin Wellington, and I. Dan
Melamed. 2007. Scalable discriminative learn-
ing for natural language parsing and translation. In
Proc. NIPS 19.
Erik Velldal and Stephan Oepen. 2005. Maximum en-
tropy models for realization ranking. In Proc. MT
Summit X.
Taro Watanabe, Jun Suzuki, Hajime Tsukada, and
Hideki Isozaki. 2007. Online large-margin training
for statistical machine translation. In Proc. EMNLP-
CoNLL-07.
RalphWeischedel and Ada Brunstein. 2005. BBN pro-
noun coreference and entity type corpus. Technical
report, BBN.
Michael White and Rajakrishnan Rajkumar. 2008.
A more precise analysis of punctuation for broad-
coverage surface realization with CCG. In Proc.
of the Workshop on Grammar Engineering Across
Frameworks (GEAF08).
Michael White, Rajakrishnan Rajkumar, and Scott
Martin. 2007. Towards broad coverage surface re-
alization with CCG. In Proc. of the Workshop on
Using Corpora for NLG: Language Generation and
Machine Translation (UCNLG+MT).
Michael White. 2006. Efficient Realization of Coor-
dinate Structures in Combinatory Categorial Gram-
mar. Research on Language and Computation,
4(1):39?75.
Huayan Zhong and Amanda Stent. 2009. Determining
the position of adverbial phrases in English. In Proc.
NAACL HLT 2009 Short Papers.
419
Proceedings of NAACL HLT 2009: Short Papers, pages 161?164,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Exploiting Named Entity Classes in CCG Surface Realization
Rajakrishnan Rajkumar Michael White
Department of Linguistics
The Ohio State University
Columbus, OH, USA
{raja,mwhite,espinosa}@ling.osu.edu
Dominic Espinosa
Abstract
This paper describes how named entity (NE)
classes can be used to improve broad cover-
age surface realization with the OpenCCG re-
alizer. Our experiments indicate that collaps-
ing certain multi-word NEs and interpolating
a language model where NEs are replaced by
their class labels yields the largest quality in-
crease, with 4-grams adding a small additional
boost. Substantial further benefit is obtained
by including class information in the hyper-
tagging (supertagging for realization) compo-
nent of the system, yielding a state-of-the-
art BLEU score of 0.8173 on Section 23 of
the CCGbank. A targeted manual evaluation
confirms that the BLEU score increase corre-
sponds to a significant rise in fluency.
1 Introduction
Hogan et al (2007) have recently shown that better
handling of named entities (NEs) in broad coverage
surface realization with LFG can lead to substan-
tial improvements in BLEU scores. In this paper,
we confirm that better NE handling can likewise im-
prove broad coverage surface realization with CCG,
even when employing a more restrictive notion of
named entities that better matches traditional real-
ization practice. Going beyond Hogan et al (2007),
we additionally show that NE classes can be used
to improve realization quality through better lan-
guage models and better hypertagging (supertagging
for realization) models, yielding a state-of-the-art
BLEU score of 0.8173 on Section 23 of the CCG-
bank.
A question addressed neither by Hogan et al
nor anyone else working on broad coverage surface
realization recently is whether reported increases
in BLEU scores actually correspond to observable
improvements in quality. We view this situation
as problematic, not only because Callison-Burch
et al (2006) have shown that BLEU does not al-
ways rank competing systems in accord with hu-
man judgments, but also because surface realiza-
tion scores are typically much higher than those in
MT?where BLEU?s performance has been repeat-
edly assessed?even when using just one reference.
Thus, in this paper, we present a targeted manual
evaluation confirming that our BLEU score increase
corresponds to a significant rise in fluency, a practice
we encourage others to adopt.
2 CCG Surface Realization
CCG (Steedman, 2000) is a unification-based cat-
egorial grammar formalism defined almost en-
tirely in terms of lexical entries that encode sub-
categorization as well as syntactic features (e.g.
number and agreement). OpenCCG is a pars-
ing/generation library which includes a hybrid
symbolic-statistical chart realizer (White, 2006). A
vital component of the realizer is the hypertagger
(Espinosa et al, 2008), which predicts lexical cat-
egory assignments using a maxent model trained on
contexts within a directed graph structure represent-
ing the logical form (LF) input; features and rela-
tions in the graph as well as parent child relation-
ships are the main features used to train the model.
The realizer takes as input an LF description (see
Figure 1 of Espinosa et al, 2008), but here we also
161
use LFs with class information on some elementary
predications (e.g. @x:MONEY($ 10,000)). Chart re-
alization proceeds in iterative beta-best fashion, with
a progressively wider hypertagger beam width. If no
complete realization is found within the time limit,
fragments are greedily assembled. Alternative real-
izations are ranked using integrated n-gram scoring;
n-gram models help in choosing word order and, to
a lesser extent, making lexical choices.
3 Collapsing Named Entities
An error analysis of the OpenCCG baseline output
reveals that out of 2331 NEs annotated by the BBN
corpus, 238 are not realized correctly. For exam-
ple, multi-word NPs like Texas Instruments Japan
Ltd. are realized as Japan Texas Instruments Ltd..
Inspired by Hogan et al?s (2007)?s Experiment 1,
we decided to use the BBN corpus NE annotation
(Weischedel and Brunstein, 2005) to collapse cer-
tain classes of NEs. But unlike their experiment
where all the NEs annotated by the BBN corpus are
collapsed, we chose to collapse into single tokens
only NEs whose exact form can be reasonably ex-
pected to be specified in the input to the realizer.
For example, while some quantificational or com-
paratives phrases like more than $ 10,000 are anno-
tated as MONEY in the BBN corpus, in our view
only $ 10,000 should be collapsed into an atomic
unit, with more than handled compositionally ac-
cording to the semantics assigned to it by the gram-
mar. Thus, after transferring the BBN annotations to
the CCGbank corpus, we (partially) collapsed NEs
which are CCGbank constituents according to the
following rules: (1) completely collapse the PER-
SON, ORGANIZATION, GPE, WORK OF ART
major class type entitites; (2) ignore phrases like
three decades later, which are annotated as DATE
entities; and (3) collapse all phrases with POS tags
CD or NNP(S) or lexical items % or $, ensuring that
all prototypical named entities are collapsed.
4 Exploiting NE Classes
Going beyond Hogan et al (2007) and collaps-
ing experiments, we also experiment with NE
classes in language models and hypertagging mod-
els. BBN annotates both major types and subtypes
(DATE:AGE, DATE:DATE etc). For all our experi-
ments, we use both of these.
4.1 Class replaced n-gram models
For both the original CCGbank as well as the col-
lapsed corpus, we created language model training
data with semantic classes replacing actual words,
in order to address data sparsity issues caused by
rare words in the same semantic class. For exam-
ple, in the collapsed corpus, the Section 00 sen-
tence Pierre Vinken , 61 years old , will join the
board as a nonexecutive director Nov. 29 . be-
comes PERSON , DATE:AGE DATE:AGE old ,
will join the ORG DESC:OTHER as a nonexecutive
PER DESC DATE:DATE DATE:DATE . During re-
alization, word forms are generated, but are then re-
placed by their semantic classes and scored using
the semantic class replaced n-gram model, similar
to (Oh and Rudnicky, 2002). As the specific words
may still matter, the class replaced model is interpo-
lated at the word level with an ordinary, word-based
language model, as well as with a factored language
model over POS tags and supertags.
4.2 Class features in hypertagging
We also experimented with a hypertagging model
trained over the collapsed corpus, where the seman-
tic classes of the elementary lexical predications,
along with the class features of their adjacent nodes,
are added as features.
5 Evaluation
5.1 Hypertagger evaluation
As Table 2 indicates, the hypertagging model does
worse in terms of per-logical predication accuracy
& per-whole-graph accuracy on the collapsed cor-
pus. To some extent this is not surprising, as collaps-
ing eliminates many easy tagging cases; however, a
full explanation is still under investigation. Note that
class information does improve performance some-
what on the collapsed corpus.
5.2 Realizer evaluation
For a both the original CCGbank and the col-
lapsed corpus, we extracted a section 02?21 lexico-
grammars and used it to derive LFs for the devel-
opment and test sections. We used the language
models in Table 1 to score realizations and for the
162
Condition Expansion
LM baseline-LM: word 3g+ pos 3g*stag 3g
HT baseline Hypertagger
LM4 LM with 4g word
LMC LM with class-rep model interpolated
LM4C LM with both
HTC HT with classes on nodes as extra feats
Table 1: Legend for Experimental Conditions
Corpus Condition Tags/pred Pred Graph
Uncollapsed HT 1.0 93.56% 39.14%
HT 1.5 98.28% 78.06%
Partly HT 1.0 92.22% 35.04%
Collapsed HTC 1.0 92.89% 38.31%
HT 1.5 97.87% 73.14%
HTC 1.5 98.02% 75.30%
Table 2: Hypertagger testing on Section 00 of the uncol-
lapsed corpus (1896 LFs & 38104 predicates) & partially
collapsed corpus (1895 LFs & 35370 predicates)
collapsed corpus, we also tried a class-based hyper-
tagging model. Hypertagger ?-values were set for
each corpus and for each hypertagging model such
that the predicted tags per pred was the same at each
level. BLEU scores were calculated after removing
the underscores between collapsed NEs.
5.3 Results
Our baseline results are much better than those pre-
viously reported with OpenCCG in large part due to
improved grammar engineering efforts and bug fix-
ing. Table 3 shows development set results which
indicate that collapsing appears to improve realiza-
tion on the whole, as evidenced by the small increase
in BLEU scores. The class-replaced word model
provides a big boost on the collapsed corpus, from
0.7917 to 0.7993, much more than 4-grams. Adding
semantic classes to the hypertagger improves its ac-
curacy and gives us another half BLEU point in-
crease. Standard test set results, reported in Table 4,
confirm the overall increase, from 0.7940 to 0.8173.
In analyzing the Section 00 results, we found that
with the collapsed corpus, NE errors were reduced
from 238 to 99, which explains why the BLEU
score increases despite the drop in exact matches and
grammatically complete realizations from the base-
line. A semi-automatic analysis reveals that most
of the corrections involve proper names that are no
longer mangled. Correct adjective ordering is also
achieved in some cases; for example, Dutch publish-
Corpus Condition %Exact %Complete BLEU
Uncollapsed LM+HT 29.27 84.02 0.7900
(98.6% LM4+HT 29.14 83.61 0.7899
coverage) LMC+HT 30.64 83.70 0.7937
LM4C+HT 30.85 83.65 0.7946
Partly collapsed LM+HT 28.28 82.48 0.7917
(98.6% LM4+HT 28.68 82.54 0.7929
coverage) LMC+HT 30.74 82.33 0.7993
LM4C+HT 31.06 82.33 0.7995
LM4C+HTC 32.01 83.17 0.8042
Table 3: Section 00 blind testing results
Condition %Exact %Complete BLEU
LM+HT 29.38 82.53 0.7940
LM4C+HTC 33.74 85.04 0.8173
Table 4: Section 23 results: LM+HT baseline on origi-
nal corpus (97.8% coverage), LM4C+HTC best case on
collapsed corpus (94.8% coverage)
ing group is enforced by the class-replaced models,
while all the other models realize this as publishing
Dutch group. Additionally, the class-replaced model
sometimes helps with animacy marking on relative
pronouns, as in Mr. Otero , who . . . instead of Mr.
Otero , which . . . . (Note that our input LFs do not
directly specify the choice of function words such
as case-marking prepositions, relative pronouns and
complementizers, and thus class-based scoring can
help to select the correct surface word form.)
5.4 Targeted manual evaluation
While the language models employing NE classes
certainly improve some examples, others are made
worse, and some are just changed to different, but
equally acceptable paraphrases. For this reason, we
carried out a targeted manual evaluation to confirm
the BLEU results.
5.4.1 Procedure
Along the lines of (Callison-Burch et al, 2006),
two native speakers (two of the authors) provided
ratings for a random sample of 49 realizations that
differed between the baseline and best conditions on
the collapsed corpus. Note that the selection pro-
cedure excludes exact matches and thus focuses on
sentences whose realization quality may be lower
on average than in an arbitrary sample. Sentences
were rated in the context of the preceding sentence
(if any) for both fluency and adequacy in compari-
son to the original sentence. The judges were not
163
LEU scoreB
 2
 2.5
 3
 3.5
 4
 4.5
 5
 0.66  0.68  0.7  0.72  0.74  0.76  0.78  0.8
AdequacyFluency
Baseline
BestH
um
an 
Sco
re
Figure 1: BLEU scores plotted against human judge-
ments of fluency and adequacy
aware of the condition (best/baseline) while doing
the rating. Ratings of the two judges were averaged
for each item.
5.4.2 Results
In the human evaluation, the best system?s mean
scores were 4.4 for adequacy and 3.61 for fluency,
compared with the baseline?s scores of 4.35 and 3.36
respectively. Figure 1 shows these results including
the standard error for each measurement, with the
BLEU scores for this specific test set. The sample
size was sufficient to show that the increase in flu-
ency from 3.36 to 3.61 represented a significant dif-
ference (paired t-test, 1-tailed, p = 0.015), while the
adequacy scores did not differ significantly.
5.4.3 Brief comparison to related systems
While direct comparisons cannot really be made
when inputs vary in their semantic depth and speci-
ficity, we observe that our all-sentences BLEU score
of 0.8173 exceeds that of Hogan et al (2007), who
report a top score of 0.6882 (though with coverage
near 100%). Nakanishi et al (2005) and Langkilde-
Geary (2002) report scores of 0.7733 and 0.7570, re-
spectively, though the former is limited to sentences
of length 20 or less, and the latter?s coverage is much
lower.
6 Conclusion and Future Work
In this paper, we have shown how named entity
classes can be used to improve the OpenCCG re-
alizer?s language models and hypertagging models,
helping to achieve a state-of-the-art BLEU score of
0.8173 on CCGbank Section 23. We have also con-
firmed the increase in quality through a targeted
manual evaluation, a practice we encourage others
working on surface realization to adopt. In future
work, we plan to investigate the unexpected drop in
hypertagger performance on our NE-collapsed cor-
pus, which we conjecture may be resolved by taking
advantage of Vadas and Curran?s (2008) corrections
to the CCGbank?s NP structures.
7 Acknowledgements
This work was supported in part by NSF IIS-
0812297 and by an allocation of computing time
from the Ohio Supercomputer Center. Our thanks
also to Josef Van Genabith, the OSU Clippers group
and the anonymous reviewers for helpful comments
and discussion.
References
Chris Callison-Burch, Miles Osborne, and Philipp
Koehn. 2006. Re-evaluating the role of BLEU in ma-
chine translation research. In Proc. EACL.
Dominic Espinosa, Michael White, and Dennis Mehay.
2008. Hypertagging: Supertagging for surface real-
ization with CCG. In Proc. ACL-08:HLT.
Deirdre Hogan, Conor Cafferkey, Aoife Cahill, and Josef
van Genabith. 2007. Exploiting multi-word units
in history-based probabilistic generation. In Proc.
EMNLP-CoNLL.
Irene Langkilde-Geary. 2002. An empirical verification
of coverage and correctness for a general-purpose sen-
tence generator. In Proc. INLG-02.
Hiroko Nakanishi, Yusuke Miyao, and Jun?ichi Tsujii.
2005. Probabilistic methods for disambiguation of an
HPSG-based chart generator. In Proc. IWPT-05.
Alice H. Oh and Alexander I. Rudnicky. 2002. Stochas-
tic natural language generation for spoken dialog sys-
tems. Computer, Speech & Language, 16(3/4):387?
407.
Mark Steedman. 2000. The syntactic process. MIT
Press, Cambridge, MA, USA.
David Vadas and James R. Curran. 2008. Parsing noun
phrase structure with CCG. In Proc. ACL-08:HLT.
Ralph Weischedel and Ada Brunstein. 2005. BBN pro-
noun coreference and entity type corpus. Technical
report, BBN.
Michael White. 2006. Efficient Realization of Coordi-
nate Structures in Combinatory Categorial Grammar.
Research on Language and Computation, 4(1):39?75.
164
Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 1113?1120,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Learning to Say It Well:
Reranking Realizations by Predicted Synthesis Quality
Crystal Nakatsu and Michael White
Department of Linguistics
The Ohio State University
Columbus, OH 43210 USA
fcnakatsu,mwhiteg@ling.ohio-state.edu
Abstract
This paper presents a method for adapting
a language generator to the strengths and
weaknesses of a synthetic voice, thereby
improving the naturalness of synthetic
speech in a spoken language dialogue sys-
tem. The method trains a discriminative
reranker to select paraphrases that are pre-
dicted to sound natural when synthesized.
The ranker is trained on realizer and syn-
thesizer features in supervised fashion, us-
ing human judgements of synthetic voice
quality on a sample of the paraphrases rep-
resentative of the generator?s capability.
Results from a cross-validation study indi-
cate that discriminative paraphrase rerank-
ing can achieve substantial improvements
in naturalness on average, ameliorating the
problem of highly variable synthesis qual-
ity typically encountered with today?s unit
selection synthesizers.
1 Introduction
Unit selection synthesis1?a technique which con-
catenates segments of natural speech selected from
a database?has been found to be capable of pro-
ducing high quality synthetic speech, especially
for utterances that are similar to the speech in the
database in terms of style, delivery, and coverage
(Black and Lenzo, 2001). In particular, in the lim-
ited domain of a spoken language dialogue sys-
tem, it is possible to achieve highly natural synthe-
sis with a purpose-built voice (Black and Lenzo,
2000). However, it can be difficult to develop
1See e.g. (Hunt and Black, 1996; Black and Taylor, 1997;
Beutnagel et al, 1999).
a synthetic voice for a dialogue system that pro-
duces natural speech completely reliably, and thus
in practice output quality can be quite variable.
Two important factors in this regard are the label-
ing process for the speech database and the direc-
tion of the dialogue system?s further development,
after the voice has been built: when labels are as-
signed fully automatically to the recorded speech,
label boundaries may be inaccurate, leading to un-
natural sounding joins in speech output; and when
further system development leads to the genera-
tion of utterances that are less like those in the
recording script, such utterances must be synthe-
sized using smaller units with more joins between
them, which can lead to a considerable dropoff in
quality.
As suggested by Bulyko and Ostendorf (2002),
one avenue for improving synthesis quality in a di-
alogue system is to have the system choose what
to say in part by taking into account what is likely
to sound natural when synthesized. The idea is
to take advantage of the generator?s periphrastic
ability:2 given a set of generated paraphrases that
suitably express the desired content in the dialogue
context, the system can select the specific para-
phrase to use as its response according to the pre-
dicted quality of the speech synthesized for that
paraphrase. In this way, if there are significant
differences in the predicted synthesis quality for
the various paraphrases?and if these predictions
are generally borne out?then, by selecting para-
phrases with high predicted synthesis quality, the
dialogue system (as a whole) can more reliably
produce natural sounding speech.
In this paper, we present an application of dis-
2See e.g. (Iordanskaja et al, 1991; Langkilde and Knight,
1998; Barzilay and McKeown, 2001; Pang et al, 2003) for
discussion of paraphrase in generation.
1113
criminative reranking to the task of adapting a lan-
guage generator to the strengths and weaknesses
of a particular synthetic voice. Our method in-
volves training a reranker to select paraphrases
that are predicted to sound natural when synthe-
sized, from the N-best realizations produced by
the generator. The ranker is trained in super-
vised fashion, using human judgements of syn-
thetic voice quality on a representative sample of
the paraphrases. In principle, the method can be
employed with any speech synthesizer. Addition-
ally, when features derived from the synthesizer?s
unit selection search can be made available, fur-
ther quality improvements become possible.
The paper is organized as follows. In Section 2,
we review previous work on integrating choice in
language generation and speech synthesis, and on
learning discriminative rerankers for generation.
In Section 3, we present our method. In Section 4,
we describe a cross-validation study whose results
indicate that discriminative paraphrase reranking
can achieve substantial improvements in natural-
ness on average. Finally, in Section 5, we con-
clude with a summary and a discussion of future
work.
2 Previous Work
Most previous work on integrating language gen-
eration and synthesis, e.g. (Davis and Hirschberg,
1988; Prevost and Steedman, 1994; Hitzeman et
al., 1998; Pan et al, 2002), has focused on how
to use the information present in the language
generation component in order to specify contex-
tually appropriate intonation for the speech syn-
thesizer to target. For example, syntactic struc-
ture, information structure and dialogue context
have all been argued to play a role in improving
prosody prediction, compared to unrestricted text-
to-speech synthesis. While this topic remains an
important area of research, our focus is instead
on a different opportunity that arises in a dialogue
system, namely, the possibility of choosing the ex-
act wording and prosody of a response according
to how natural it is likely to sound when synthe-
sized.
To our knowledge, Bulyko and Ostendorf
(2002) were the first to propose allowing the
choice of wording and prosody to be jointly deter-
mined by the language generator and speech syn-
thesizer. In their approach, a template-based gen-
erator passes a prosodically annotated word net-
work to the speech synthesizer, rather than a single
text string (or prosodically annotated text string).
To perform the unit selection search on this ex-
panded input efficiently, they employ weighted
finite-state transducers, where each step of net-
work expansion is then followed by minimiza-
tion. The weights are determined by concatena-
tion (join) costs, relative frequencies (negative log
probabilities) of the word sequences, and prosodic
prediction costs, for cases where the prosody is
not determined by the templates. In a perception
experiment, they demonstrated that by expand-
ing the space of candidate responses, their system
achieved higher quality speech output.
Following (Bulyko and Ostendorf, 2002), Stone
et al (2004) developed a method for jointly de-
termining wording, speech and gesture. In their
approach, a template-based generator produces
a word lattice with intonational phrase breaks.
A unit selection algorithm then searches for a
low-cost way of realizing a path through this
lattice that combines captured motion samples
with recorded speech samples to create coherent
phrases, blending segments of speech and mo-
tion together phrase-by-phrase into extended ut-
terances. Video demonstrations indicate that natu-
ral and highly expressive results can be achieved,
though no human evaluations are reported.
In an alternative approach, Pan and Weng
(2002) proposed integrating instance-based real-
ization and synthesis. In their framework, sen-
tence structure, wording, prosody and speech
waveforms from a domain-specific corpus are si-
multaneously reused. To do so, they add prosodic
and acoustic costs to the insertion, deletion and
replacement costs used for instance-based surface
realization. Their contribution focuses on how to
design an appropriate speech corpus to facilitate
an integrated approach to instance-based realiza-
tion and synthesis, and does not report evaluation
results.
A drawback of these approaches to integrating
choice in language generation and synthesis is that
they cannot be used with most existing speech syn-
thesizers, which do not accept (annotated) word
lattices as input. In contrast, the approach we in-
troduce here can be employed with any speech
synthesizer in principle. All that is required is
that the language generator be capable of produc-
ing N-best outputs; that is, the generator must be
able to construct a set of suitable paraphrases ex-
1114
pressing the desired content, from which the top
N realizations can be selected for reranking ac-
cording to their predicted synthesis quality. Once
the realizations have been reranked, the top scor-
ing realization can be sent to the synthesizer as
usual. Alternatively, when features derived from
the synthesizer?s unit selection search can be made
available?and if the time demands of the dia-
logue system permit?several of the top scoring
reranked realizations can be sent to the synthe-
sizer, and the resulting utterances can be rescored
with the extended feature set.
Our reranking approach has been inspired by
previous work on reranking in parsing and gen-
eration, especially (Collins, 2000) and (Walker et
al., 2002). As in Walker et al?s (2002) method for
training a sentence plan ranker, we use our gen-
erator to produce a representative sample of para-
phrases and then solicit human judgements of their
naturalness to use as data for training the ranker.
This method is attractive when there is no suit-
able corpus of naturally occurring dialogues avail-
able for training purposes, as is often the case for
systems that engage in human-computer dialogues
that differ substantially from human-human ones.
The primary difference between Walker et al?s
work and ours is that theirs examines the impact
on text quality of sentence planning decisions such
as aggregation, whereas ours focuses on the im-
pact of the lexical and syntactic choice at the sur-
face realization level on speech synthesis quality,
according to the strengths and weaknesses of a
particular synthetic voice.
3 Reranking Realizations by Predicted
Synthesis Quality
3.1 Generating Alternatives
Our experiments with integrating language gener-
ation and synthesis have been carried out in the
context of the COMIC3 multimodal dialogue sys-
tem (den Os and Boves, 2003). The COMIC sys-
tem adds a dialogue interface to a CAD-like ap-
plication used in sales situations to help clients re-
design their bathrooms. The input to the system
includes speech, handwriting, and pen gestures;
the output combines synthesized speech, an ani-
mated talking head, deictic gestures at on-screen
objects, and direct control of the underlying appli-
cation.
3COnversational Multimodal Interaction with Computers,
http://www.hcrc.ed.ac.uk/comic/.
Drawing on the materials used in (Foster and
White, 2005) to evaluate adaptive generation in
COMIC, we selected a sample of 104 sentences
from 38 different output turns across three dia-
logues. For each sentence in the set, a variant was
included that expressed the same content adapted
to a different user model or adapted to a differ-
ent dialogue history. For example, a description
of a certain design?s colour scheme for one user
might be phrased as As you can see, the tiles have
a blue and green colour scheme, whereas a vari-
ant expression of the same content for a different
user could be Although the tiles have a blue colour
scheme, the design does also feature green, if the
user disprefers blue.
In COMIC, the sentence planner uses XSLT to
generate disjunctive logical forms (LFs), which
specify a range of possible paraphrases in a nested
free-choice form (Foster and White, 2004). Such
disjunctive LFs can be efficiently realized us-
ing the OpenCCG realizer (White, 2004; White,
2006b; White, 2006a). Note that for the experi-
ments reported here, we manually augmented the
disjunctive LFs for the 104 sentences in our sam-
ple to make greater use of the periphrastic capa-
bilities of the COMIC grammar; it remains for fu-
ture work to augment the COMIC sentence plan-
ner produce these more richly disjunctive LFs au-
tomatically.
OpenCCG includes an extensible API for inte-
grating language modeling and realization. To se-
lect preferred word orders, from among all those
allowed by the grammar for the input LF, we used
a backoff trigram model trained on approximately
750 example target sentences, where certain words
were replaced with their semantic classes (e.g.
MANUFACTURER, COLOUR) for better general-
ization. For each of the 104 sentences in our sam-
ple, we performed 25-best realization from the dis-
junctive LF, and then randomly selected up to 12
different realizations to include in our experiments
based on a simulated coin flip for each realization,
starting with the top-scoring one. We used this
procedure to sample from a larger portion of the
N-best realizations, while keeping the sample size
manageable.
Figure 1 shows an example of 12 paraphrases
for a sentence chosen for inclusion in our sample.
Note that the realizations include words with pitch
accent annotations as well as boundary tones as
separate, punctuation-like words. Generally the
1115
 this
H
design
H
uses tiles from Villeroy and Boch
H
?s Funny Day
H
collection LL% .
 this
H
design
H
is based on the Funny Day
H
collec-
tion by Villeroy and Boch
H
LL% .
 this
H
design
H
is based on Funny Day
H
LL% , by
Villeroy and Boch
H
LL% .
 this
H
design
H
draws from the Funny Day
H
collec-
tion by Villeroy and Boch
H
LL% .
 this
H
one draws from Funny Day
H
LL% , by
Villeroy and Boch
H
LL% .
 here
L+H
LH% we have a design that is based on
the Funny Day
H
collection by Villeroy and Boch
H
LL% .
 this
H
design
H
draws from Villeroy and Boch
H
?s
Funny Day
H
series LL% .
 here is a design that draws from Funny Day
H
LL% ,
by Villeroy and Boch
H
LL% .
 this
H
one draws from Villeroy and Boch
H
?s
Funny Day
H
collection LL% .
 this
H
draws from the Funny Day
H
collection by
Villeroy and Boch
H
LL% .
 this
H
one draws from the Funny Day
H
collection by
Villeroy and Boch
H
LL% .
 here is a design that draws from Villeroy and Boch
H
?s Funny Day
H
collection LL% .
Figure 1: Example of sampled periphrastic alter-
natives for a sentence.
quality of the sampled paraphrases is very high,
only occasionally including dispreferred word or-
ders such as We here have a design in the family
style, where here is in medial position rather than
fronted.4
3.2 Synthesizing Utterances
For synthesis, OpenCCG?s output realizations are
converted to APML,5 a markup language which
allows pitch accents and boundary tones to be
specified, and then passed to the Festival speech
synthesis system (Taylor et al, 1998; Clark et al,
2004). Festival uses the prosodic markup in the
text analysis phase of synthesis in place of the
structures that it would otherwise have to predict
from the text. The synthesiser then uses the con-
text provided by the markup to enforce the selec-
4In other examples medial position is preferred, e.g. This
design here is in the family style.
5Affective Presentation Markup Language; see
http://www.cstr.ed.ac.uk/projects/
festival/apml.html.
tion of suitable units from the database.
A custom synthetic voice for the COMIC sys-
tem was developed, as follows. First, a domain-
specific recording script was prepared by select-
ing about 150 sentences from the larger set of tar-
get sentences used to train the system?s n-gram
model. The sentences were greedily selected with
the goals of ensuring that (i) all words (including
proper names) in the target sentences appeared at
least once in the record script, and (ii) all bigrams
at the level of semantic classes (e.g. MANUFAC-
TURER, COLOUR) were covered as well. For the
cross-validation study reported in the next section,
we also built a trigram model on the words in the
domain-specific recording script, without replac-
ing any words with semantic classes, so that we
could examine whether the more frequent occur-
rence of the specific words and phrases in this part
of the script is predictive of synthesis quality.
The domain-specific script was augmented with
a set of 600 newspaper sentences selected for di-
phone coverage. The newspaper sentences make
it possible for the voice to synthesize words out-
side of the domain-specific script, though not
necessarily with the same quality. Once these
scripts were in place, an amateur voice talent was
recorded reading the sentences in the scripts dur-
ing two recording sessions. Finally, after the
speech files were semi-automatically segmented
into individual sentences, the speech database was
constructed, using fully automatic labeling.
We have found that the utterances synthesized
with the COMIC voice vary considerably in their
naturalness, due to two main factors. First, the
system underwent further development after the
voice was built, leading to the addition of a va-
riety of new phrases to the system?s repertoire, as
well as many extra proper names (and their pro-
nunciations); since these names and phrases usu-
ally require going outside of the domain-specific
part of the speech database, they often (though not
always) exhibit a considerable dropoff in synthe-
sis quality.6 And second, the boundaries of the au-
tomatically assigned unit labels were not always
accurate, leading to problems with unnatural joins
and reduced intelligibility. To improve the reliabil-
ity of the COMIC voice, we could have recorded
more speech, or manually corrected label bound-
6Note that in the current version of the system, proper
names are always required parts of the output, and thus the
discriminative reranker cannot learn to simply choose para-
phrases that leave out problematic names.
1116
aries; the goal of this paper is to examine whether
the naturalness of a dialogue system?s output can
be improved in a less labor-intensive way.
3.3 Rating Synthesis Quality
To obtain data for training our realization reranker,
we solicited judgements of the naturalness of the
synthesized speech produced by Festival for the
utterances in our sample COMIC corpus. Two
judges (the first two authors) provided judgements
on a 1?7 point scale, with higher scores represent-
ing more natural synthesis. Ratings were gathered
using WebExp2,7 with the periphrastic alternatives
for each sentence presented as a group in a ran-
domized order. Note that for practical reasons,
the utterances were presented out of the dialogue
context, though both judges were familiar with the
kinds of dialogues that the COMIC system is ca-
pable of.
Though the numbers on the seven point scale
were not assigned labels, they were roughly taken
to be ?horrible,? ?poor,? ?fair,? ?ok,? ?good,? ?very
good? and ?perfect.? The average assigned rating
across all utterances was 4.05 (?ok?), with a stan-
dard deviation of 1.56. The correlation between
the two judges? ratings was 0.45, with one judge?s
ratings consistently higher than the other?s.
Some common problems noted by the judges
included slurred words, especially the sometimes
sounding like ther or even their; clipped words,
such as has shortened at times to the point of
sounding like is, or though clipped to unintelligi-
bility; unnatural phrasing or emphasis, e.g. occa-
sional pauses before a possessive ?s, or words such
as style sounding emphasized when they should
be deaccented; unnatural rate changes; ?choppy?
speech from poor joins; and some unintelligible
proper names.
3.4 Ranking
While Collins (2000) and Walker et al (2002)
develop their rankers using the RankBoost algo-
rithm (Freund et al, 1998), we have instead cho-
sen to use Joachims? (2002) method of formu-
lating ranking tasks as Support Vector Machine
(SVM) constraint optimization problems.8 This
choice has been motivated primarily by conve-
nience, as Joachims? SVMlight package is easy to
7http://www.hcrc.ed.ac.uk/web exp/
8See (Barzilay and Lapata, 2005) for another application
of SVM ranking in generation, namely to the task of ranking
alternative text orderings for local coherence.
use; we leave it for future work to compare the
performance of RankBoost and SVMlight on our
ranking task.
The ranker takes as input a set of paraphrases
that express the desired content of each sentence,
optionally together with synthesized utterances
for each paraphrase. The output is a ranking of
the paraphrases according to the predicted natu-
ralness of their corresponding synthesized utter-
ances. Ranking is more appropriate than classifi-
cation for our purposes, as naturalnesss is a graded
assessment rather than a categorical one.
To encode the ranking task as an SVM con-
straint optimization problem, each paraphrase j
of a sentence i is represented by a feature vector
(s
ij
) = hf
1
(s
ij
); : : : ; f
m
(s
ij
)i, where m is the
number of features. In the training data, the fea-
ture vectors are paired with the average value of
their corresponding human judgements of natural-
ness. From this data, ordered pairs of paraphrases
(s
ij
; s
ik
) are derived, where s
ij
has a higher nat-
uralness rating than s
ik
. The constraint optimiza-
tion problem is then to derive a parameter vector
~w that yields a ranking score function ~w  (s
ij
)
which minimizes the number of pairwise rank-
ing violations. Ideally, for every ordered pair
(s
ij
; s
ik
), we would have ~w (s
ij
) > ~w (s
ik
);
in practice, it is often impossible or intractable to
find such a parameter vector, and thus slack vari-
ables are introduced that allow for training errors.
A parameter to the algorithm controls the trade-off
between ranking margin and training error.
In testing, the ranker?s accuracy can be deter-
mined by comparing the ranking scores for ev-
ery ordered pair (s
ij
; s
ik
) in the test data, and
determining whether the actual preferences are
borne out by the predicted preference, i.e. whether
~w  (s
ij
) > ~w  (s
ik
) as desired. Note that
the ranking scores, unlike the original ratings, do
not have any meaning in the absolute sense; their
import is only to order alternative paraphrases by
their predicted naturalness.
In our ranking experiments, we have used
SVMlight with all parameters set to their default
values.
3.5 Features
Table 1 shows the feature sets we have investigated
for reranking, distinguished by the availability of
the features and the need for discriminative train-
ing. The first row shows the feature sets that are
1117
Table 1: Feature sets for reranking.
Discriminative
Availability no yes
Realizer NGRAMS WORDS
Synthesizer COSTS ALL
available to the realizer. There are two n-gram
models that can be used to directly rank alterna-
tive realizations: NGRAM-1, the language model
used in COMIC, and NGRAM-2, the language
model derived from the domain-specific recording
script; for feature values, the negative logarithms
are used. There are also two WORDS feature
sets (shown in the second column): WORDS-BI,
which includes NGRAMS plus a feature for every
possible unigram and bigram, where the value of
the feature is the count of the unigram or bigram
in a given realization; and WORDS-TRI, which
includes all the features in WORDS-BI, plus a
feature for every possible trigram. The second
row shows the feature sets that require informa-
tion from the synthesizer. The COSTS feature set
includes NGRAMS plus the total join and target
costs from the unit selection search. Note that a
weighted sum of these costs could be used to di-
rectly rerank realizations, in much the same way
as relative frequencies and concatenation costs are
used in (Bulyko and Ostendorf, 2002); in our
experiments, we let SVMlight determine how to
weight these costs. Finally, there are two ALL fea-
ture sets: ALL-BI includes NGRAMS, WORDS-
BI and COSTS, plus features for every possi-
ble phone and diphone, and features for every
specific unit in the database; ALL-TRI includes
NGRAMS, WORDS-TRI, COSTS, and a feature
for every phone, diphone and triphone, as well as
specific units in the database. As with WORDS,
the value of a feature is the count of that feature in
a given synthesized utterance.
4 Cross-Validation Study
To train and test our ranker on our feature sets,
we partitioned the corpus into 10 folds and per-
formed 10-fold cross-validation. For each fold,
90% of the examples were used for training the
ranker and the remaining unseen 10% were used
for testing. The folds were created by randomly
choosing from among the sentence groups, result-
ing in all of the paraphrases for a given sentence
occurring in the same fold, and each occurring ex-
Table 2: Comparison of results for differing fea-
ture sets, topline and baseline.
Features Mean Score SD Accuracy (%)
BEST 5.38 1.11 100.0
WORDS-TRI 4.95 1.24 77.3
ALL-BI 4.95 1.24 77.9
ALL-TRI 4.90 1.25 78.0
WORDS-BI 4.86 1.28 76.8
COSTS 4.69 1.27 68.2
NGRAM-2 4.34 1.38 56.2
NGRAM-1 4.30 1.29 53.3
RANDOM 4.11 1.22 50.0
actly once in the testing set as a whole.
We evaluated the performance of our ranker
by determining the average score of the best
ranked paraphrase for each sentence, under each
of the following feature combinations: NGRAM-
1, NGRAM-2, COSTS, WORDS-BI, WORDS-
TRI, ALL-BI, and ALL-TRI. Note that since we
used the human ratings to calculate the score of
the highest ranked utterance, the score of the high-
est ranked utterance cannot be higher than that
of the highest human-rated utterance. Therefore,
we effectively set the human ratings as the topline
(BEST). For the baseline, we randomly chose an
utterance from among the alternatives, and used
its associated score. In 15 tests generating the ran-
dom scores, our average scores ranged from 3.88?
4.18. We report the median score of 4.11 as the
average for the baseline, along with the mean of
the topline and each of the feature subsets, in Ta-
ble 2.
We also report the ordering accuracy of each
feature set used by the ranker in Table 2. As men-
tioned in Section 3.4, the ordering accuracy of the
ranker using a given feature set is determined by
c=N , where c is the number of correctly ordered
pairs (of each paraphrase, not just the top ranked
one) produced by the ranker, and N is the total
number of human-ranked ordered pairs.
As Table 2 indicates, the mean of BEST is 5.38,
whereas our ranker using WORDS-TRI features
achieves a mean score of 4.95. This is a difference
of 0.42 on a seven point scale, or only a 6% dif-
ference. The ordering accuracy of WORDS-TRI
is 77.3%.
We also measured the improvement of our
ranker with each feature set over the random base-
line as a percentage of the maximum possible
gain (which would be to reproduce the human
topline). The results appear in Figure 2. As the
1118
010203040506070
NGR
AM-1
NGR
AM-2
COS
TS
WOR
DS-B
IA
LL-T
RI
ALL-
BI
WOR
DS-T
RI
Figure 2: Improvement as a percentage of the
maximum possible gain over the random baseline.
figure indicates, the maximum possible gain our
ranker achieves over the baseline is 66% (using the
WORDS-TRI or ALL-BI feature set) . By com-
parison, NGRAM-1 and NGRAM-2 achieve less
than 20% of the possible gain.
To verify our main hypothesis that our ranker
would significantly outperform the baselines,
we computed paired one-tailed t-tests between
WORDS-TRI and RANDOM (t = 2:4, p <
8:9x10
 13), and WORDS-TRI and NGRAM-1
(t = 1:4, p < 4:5x10 8). Both differences were
highly significant. We also performed seven post-
hoc comparisons using two-tailed t-tests, as we
did not have an a priori expectation as to which
feature set would work better. Using the Bonfer-
roni adjustment for multiple comparisons, the p-
value required to achieve an overall level of signif-
icance of 0.05 is 0.007. In the first post-hoc test,
we found a significant difference between BEST
and WORDS-TRI (t = 8:0,p < 1:86x10 12),
indicating that there is room for improvement of
our ranker. However, in considering the top scor-
ing feature sets, we did not find a significant dif-
ference between WORDS-TRI and WORDS-BI
(t = 2:3, p < 0:022), from which we infer that the
difference among all of WORDS-TRI, ALL-BI,
ALL-TRI and WORDS-BI is not significant also.
This suggests that the synthesizer features have
no substantial impact on our ranker, as we would
expect ALL-TRI to be significantly higher than
WORDS-TRI if so. However, since COSTS does
significantly improve upon NGRAM2 (t = 3:5,
p < 0:001), there is some value to the use of syn-
thesizer features in the absence of WORDS. We
also looked at the comparison for the WORDS
models and COSTS. While WORDS-BI did not
perform significantly better than COSTS ( t =
2:3, p < 0:025), the added trigrams in WORDS-
TRI did improve ranker performance significantly
over COSTS (t = 3:7, p < 3:29x10 4). Since
COSTS ranks realizations in the much the same
way as (Bulyko and Ostendorf, 2002), the fact that
WORDS-TRI outperforms COSTS indicates that
our discriminative reranking method can signifi-
cantly improve upon their non-discriminative ap-
proach.
5 Conclusions
In this paper, we have presented a method for
adapting a language generator to the strengths
and weaknesses of a particular synthetic voice by
training a discriminative reranker to select para-
phrases that are predicted to sound natural when
synthesized. In contrast to previous work on
this topic, our method can be employed with any
speech synthesizer in principle, so long as fea-
tures derived from the synthesizer?s unit selec-
tion search can be made available. In a case
study with the COMIC dialogue system, we have
demonstrated substantial improvements in the nat-
uralness of the resulting synthetic speech, achiev-
ing two-thirds of the maximum possible gain, and
raising the average rating from ?ok? to ?good.? We
have also shown that in this study, our discrimina-
tive method significantly outperforms an approach
that performs selection based solely on corpus fre-
quencies together with target and join costs.
In future work, we intend to verify the results
of our cross-validation study in a perception ex-
periment with na??ve subjects. We also plan to in-
vestigate whether additional features derived from
the synthesizer can better detect unnatural pauses
or changes in speech rate, as well as F0 contours
that fail to exhibit the targeting accenting pattern.
Finally, we plan to examine whether gains in qual-
ity can be achieved with an off-the-shelf, general
purpose voice that are similar to those we have ob-
served using COMIC?s limited domain voice.
Acknowledgements
We thank Mary Ellen Foster, Eric Fosler-Lussier
and the anonymous reviewers for helpful com-
ments and discussion.
References
Regina Barzilay and Mirella Lapata. 2005. Modeling
local coherence: An entity-based approach. In Pro-
1119
ceedings of the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics, Ann Arbor.
Regina Barzilay and Kathleen McKeown. 2001. Ex-
tracting paraphrases from a parallel corpus. In Proc.
ACL/EACL.
M. Beutnagel, A. Conkie, J. Schroeter, Y. Stylianou,
and A. Syrdal. 1999. The AT&T Next-Gen TTS
system. In Joint Meeting of ASA, EAA, and DAGA.
Alan Black and Kevin Lenzo. 2000. Limited domain
synthesis. In Proceedings of ICSLP2000, Beijing,
China.
Alan Black and Kevin Lenzo. 2001. Optimal data
selection for unit selection synthesis. In 4th ISCA
Speech Synthesis Workshop, Pitlochry, Scotland.
Alan Black and Paul Taylor. 1997. Automatically clus-
tering similar units for unit selection in speech syn-
thesis. In Eurospeech ?97.
Ivan Bulyko and Mari Ostendorf. 2002. Efficient in-
tegrated response generation from multiple targets
using weighted finite state transducers. Computer
Speech and Language, 16:533?550.
Robert A.J. Clark, Korin Richmond, and Simon King.
2004. Festival 2 ? build your own general pur-
pose unit selection speech synthesiser. In 5th ISCA
Speech Synthesis Workshop, pages 173?178, Pitts-
burgh, PA.
Michael Collins. 2000. Discriminative reranking for
natural language parsing. In Proc. ICML.
James Raymond Davis and Julia Hirschberg. 1988.
Assigning intonational features in synthesized spo-
ken directions. In Proc. ACL.
Els den Os and Lou Boves. 2003. Towards ambient
intelligence: Multimodal computers that understand
our intentions. In Proc. eChallenges-03.
Mary Ellen Foster and Michael White. 2004. Tech-
niques for Text Planning with XSLT. In Proc. 4th
NLPXML Workshop.
Mary Ellen Foster and Michael White. 2005. As-
sessing the impact of adaptive generation in the
COMIC multimodal dialogue system. In Proc.
IJCAI-05 Workshop on Knowledge and Representa-
tion in Practical Dialogue Systems.
Y. Freund, R. Iyer, R. E. Schapire, and Y. Singer. 1998.
An efficient boosting algorithm for combining pref-
erences. In Machine Learning: Proc. of the Fif-
teenth International Conference.
Janet Hitzeman, Alan W. Black, Chris Mellish, Jon
Oberlander, and Paul Taylor. 1998. On the use of
automatically generated discourse-level information
in a concept-to-speech synthesis system. In Proc.
ICSLP-98.
A. Hunt and A. Black. 1996. Unit selection in a
concatenative speech synthesis system using a large
speech database. In Proc. ICASSP-96, Atlanta,
Georgia.
Lidija Iordanskaja, Richard Kittredge, and Alain
Polgu?ere. 1991. Lexical selection and paraphrase
in a meaning-text generation model. In Ce?cile L.
Paris, William R. Swartout, and William C. Mann,
editors, Natural Language Generation in Artificial
Intelligence and Computational Linguistics, pages
293?312. Kluwer.
Thorsten Joachims. 2002. Optimizing search engines
using clickthrough data. In Proc. KDD.
Irene Langkilde and Kevin Knight. 1998. Generation
that exploits corpus-based statistical knowledge. In
Proc. COLING-ACL.
Shimei Pan and Wubin Weng. 2002. Designing a
speech corpus for instance-based spoken language
generation. In Proc. of the International Natural
Language Generation Conference (INLG-02).
Shimei Pan, Kathleen McKeown, and Julia Hirschberg.
2002. Exploring features from natural language
generation for prosody modeling. Computer Speech
and Language, 16:457?490.
Bo Pang, Kevin Knight, and Daniel Marcu. 2003.
Syntax-based alignment of multiple translations:
Extracting paraphrases and generating new sen-
tences. In Proc. HLT/NAACL.
Scott Prevost and Mark Steedman. 1994. Specify-
ing intonation from context for speech synthesis.
Speech Communication, 15:139?153.
Matthew Stone, Doug DeCarlo, Insuk Oh, Christian
Rodriguez, Adrian Stere, Alyssa Lees, and Chris
Bregler. 2004. Speaking with hands: Creating ani-
mated conversational characters from recordings of
human performance. ACM Transactions on Graph-
ics (SIGGRAPH), 23(3).
P. Taylor, A. Black, and R. Caley. 1998. The architec-
ture of the the Festival speech synthesis system. In
Third International Workshop on Speech Synthesis,
Sydney, Australia.
Marilyn A. Walker, Owen C. Rambow, and Monica Ro-
gati. 2002. Training a sentence planner for spo-
ken dialogue using boosting. Computer Speech and
Language, 16:409?433.
Michael White. 2004. Reining in CCG Chart Realiza-
tion. In Proc. INLG-04.
Michael White. 2006a. CCG chart realization from
disjunctive logical forms. In Proc. INLG-06. To ap-
pear.
Michael White. 2006b. Efficient Realization of Coor-
dinate Structures in Combinatory Categorial Gram-
mar. Research on Language & Computation, on-
line first, March.
1120
Proceedings of ACL-08: HLT, pages 183?191,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
Hypertagging: Supertagging for Surface Realization with CCG
Dominic Espinosa and Michael White and Dennis Mehay
Department of Linguistics
The Ohio State University
Columbus, OH, USA
{espinosa,mwhite,mehay}@ling.osu.edu
Abstract
In lexicalized grammatical formalisms, it is
possible to separate lexical category assign-
ment from the combinatory processes that
make use of such categories, such as pars-
ing and realization. We adapt techniques
from supertagging ? a relatively recent tech-
nique that performs complex lexical tagging
before full parsing (Bangalore and Joshi,
1999; Clark, 2002) ? for chart realization
in OpenCCG, an open-source NLP toolkit for
CCG. We call this approach hypertagging, as
it operates at a level ?above? the syntax, tag-
ging semantic representations with syntactic
lexical categories. Our results demonstrate
that a hypertagger-informed chart realizer can
achieve substantial improvements in realiza-
tion speed (being approximately twice as fast)
with superior realization quality.
1 Introduction
In lexicalized grammatical formalisms such as Lex-
icalized Tree Adjoining Grammar (Schabes et al,
1988, LTAG), Combinatory Categorial Grammar
(Steedman, 2000, CCG) and Head-Driven Phrase-
Structure Grammar (Pollard and Sag, 1994, HPSG),
it is possible to separate lexical category assign-
ment ? the assignment of informative syntactic cat-
egories to linguistic objects such as words or lex-
ical predicates ? from the combinatory processes
that make use of such categories ? such as pars-
ing and surface realization. One way of performing
lexical assignment is simply to hypothesize all pos-
sible lexical categories and then search for the best
combination thereof, as in the CCG parser in (Hock-
enmaier, 2003) or the chart realizer in (Carroll and
Oepen, 2005). A relatively recent technique for lex-
ical category assignment is supertagging (Bangalore
and Joshi, 1999), a preprocessing step to parsing that
assigns likely categories based on word and part-of-
speech (POS) contextual information. Supertagging
was dubbed ?almost parsing? by these authors, be-
cause an oracle supertagger left relatively little work
for their parser, while speeding up parse times con-
siderably. Supertagging has been more recently ex-
tended to a multitagging paradigm in CCG (Clark,
2002; Curran et al, 2006), leading to extremely ef-
ficient parsing with state-of-the-art dependency re-
covery (Clark and Curran, 2007).
We have adapted this multitagging approach to
lexical category assignment for realization using the
CCG-based natural language toolkit OpenCCG.1 In-
stead of basing category assignment on linear word
and POS context, however, we predict lexical cat-
egories based on contexts within a directed graph
structure representing the logical form (LF) of a
proposition to be realized. Assigned categories are
instantiated in OpenCCG?s chart realizer where, to-
gether with a treebank-derived syntactic grammar
(Hockenmaier and Steedman, 2007) and a factored
language model (Bilmes and Kirchhoff, 2003), they
constrain the English word-strings that are chosen to
express the LF. We have dubbed this approach hy-
pertagging, as it operates at a level ?above? the syn-
tax, moving from semantic representations to syn-
tactic categories.
We evaluate this hypertagger in two ways: first,
1http://openccg.sourceforge.net.
183
we evaluate it as a tagger, where the hypertagger
achieves high single-best (93.6%) and multitagging
labelling accuracies (95.8?99.4% with category per
lexical predication ratios ranging from 1.1 to 3.9).2
Second, we compare a hypertagger-augmented ver-
sion of OpenCCG?s chart realizer with the pre-
existing chart realizer (White et al, 2007) that sim-
ply instantiates the chart with all possible CCG cat-
egories (subject to frequency cutoffs) for each in-
put LF predicate. The hypertagger-seeded realizer
runs approximately twice as fast as the pre-existing
OpenCCG realizer and finds a larger number of
complete realizations, resorting less to chart frag-
ment assembly in order to produce an output within
a 15 second per-sentence time limit. Moreover, the
overall BLEU (Papineni et al, 2002) and METEOR
(Lavie and Agarwal, 2007) scores, as well as num-
bers of exact string matches (as measured against to
the original sentences in the CCGbank) are higher
for the hypertagger-seeded realizer than for the pre-
existing realizer.
This paper is structured as follows: Section 2 pro-
vides background on chart realization in OpenCCG
using a corpus-derived grammar. Section 3 de-
scribes our hypertagging approach and how it is in-
tegrated into the realizer. Section 4 describes our
results, followed by related work in Section 5 and
our conclusions in Section 6.
2 Background
2.1 Surface Realization with OpenCCG
The OpenCCG surface realizer is based on Steed-
man?s (2000) version of CCG elaborated with
Baldridge and Kruijff?s multi-modal extensions for
lexically specified derivation control (Baldridge,
2002; Baldridge and Kruijff, 2003) and hybrid
logic dependency semantics (Baldridge and Kruijff,
2002). OpenCCG implements a symbolic-statistical
chart realization algorithm (Kay, 1996; Carroll et al,
1999; White, 2006b) combining (1) a theoretically
grounded approach to syntax and semantic composi-
tion with (2) factored language models (Bilmes and
Kirchhoff, 2003) for making choices among the op-
tions left open by the grammar.
In OpenCCG, the search for complete realizations
2Note that the multitagger is ?correct? if the correct tag is
anywhere in the multitag set.
he h2
aa1
heh3
<Det>
<Arg0> <Arg1>
<TENSE>pres
<NUM>sg
<Arg0>
w1 want.01
m1
<Arg1>
<GenRel>
<Arg1>
<TENSE>pres
p1point
h1have.03
make.03
Figure 1: Semantic dependency graph from the CCGbank
for He has a point he wants to make [. . . ]
makes use of n-gram language models over words
represented as vectors of factors, including surface
form, part of speech, supertag and semantic class.
The search proceeds in one of two modes, anytime
or two-stage (packing/unpacking). In the anytime
mode, a best-first search is performed with a con-
figurable time limit: the scores assigned by the n-
gram model determine the order of the edges on
the agenda, and thus have an impact on realization
speed. In the two-stage mode, a packed forest of
all possible realizations is created in the first stage;
in the second stage, the packed representation is un-
packed in bottom-up fashion, with scores assigned
to the edge for each sign as it is unpacked, much
as in (Langkilde, 2000). Edges are grouped into
equivalence classes when they have the same syn-
tactic category and cover the same parts of the in-
put logical form. Pruning takes place within equiv-
alence classes of edges. Additionally, to realize a
wide range of paraphrases, OpenCCG implements
an algorithm for efficiently generating from disjunc-
tive logical forms (White, 2006a).
To illustrate the input to OpenCCG, consider the
semantic dependency graph in Figure 1, which is
taken from section 00 of a Propbank-enhanced ver-
sion of the CCGbank (Boxwell and White, 2008).
In the graph, each node has a lexical predica-
tion (e.g. make.03) and a set of semantic features
(e.g. ?NUM?sg); nodes are connected via depen-
dency relations (e.g. ?ARG0?). Internally, such
184
graphs are represented using Hybrid Logic Depen-
dency Semantics (HLDS), a dependency-based ap-
proach to representing linguistic meaning developed
by Baldridge and Kruijff (2002). In HLDS, hy-
brid logic (Blackburn, 2000) terms are used to de-
scribe dependency graphs. These graphs have been
suggested as representations for discourse structure,
and have their own underlying semantics (White,
2006b).
To more robustly support broad coverage surface
realization, OpenCCG has recently been enhanced
to greedily assemble fragments in the event that the
realizer fails to find a complete realization. The frag-
ment assembly algorithm begins with the edge for
the best partial realization, i.e. the one that covers
the most elementary predications in the input logi-
cal form, with ties broken according to the n-gram
score. (Larger fragments are preferred under the
assumption that they are more likely to be gram-
matical.) Next, the chart and agenda are greedily
searched for the best edge whose semantic coverage
is disjoint from those selected so far; this process re-
peats until no further edges can be added to the set
of selected fragments. In the final step, these frag-
ments are concatenated, again in a greedy fashion,
this time according to the n-gram score of the con-
catenated edges: starting with the original best edge,
the fragment whose concatenation on the left or right
side yields the highest score is chosen as the one to
concatenate next, until all the fragments have been
concatenated into a single output.
2.2 Realization from an Enhanced CCGbank
White et al (2007) describe an ongoing effort to en-
gineer a grammar from the CCGbank (Hockenmaier
and Steedman, 2007) ? a corpus of CCG deriva-
tions derived from the Penn Treebank ? suitable for
realization with OpenCCG. This process involves
converting the corpus to reflect more precise anal-
yses, where feasible, and adding semantic represen-
tations to the lexical categories. In the first step, the
derivations in the CCGbank are revised to reflect the
desired syntactic derivations. Changes to the deriva-
tions are necessary to reflect the lexicalized treat-
ment of coordination and punctuation assumed by
the multi-modal version of CCG that is implemented
in OpenCCG. Further changes are necessary to sup-
port semantic dependencies rather than surface syn-
tactic ones; in particular, the features and unifica-
tion constraints in the categories related to semanti-
cally empty function words such complementizers,
infinitival-to, expletive subjects, and case-marking
prepositions are adjusted to reflect their purely syn-
tactic status.
In the second step, a grammar is extracted from
the converted CCGbank and augmented with logi-
cal forms. Categories and unary type changing rules
(corresponding to zero morphemes) are sorted by
frequency and extracted if they meet the specified
frequency thresholds.
A separate transformation then uses around two
dozen generalized templates to add logical forms
to the categories, in a fashion reminiscent of (Bos,
2005). The effect of this transformation is illustrated
below. Example (1) shows how numbered seman-
tic roles, taken from PropBank (Palmer et al, 2005)
when available, are added to the category of an ac-
tive voice, past tense transitive verb, where *pred*
is a placeholder for the lexical predicate; examples
(2) and (3) show how more specific relations are in-
troduced in the category for determiners and the cat-
egory for the possessive ?s, respectively.
(1) s1 :dcl\np2/np3 =?
s1 :dcl,x1\np2 :x2/np3 :x3 : @x1(*pred* ?
?TENSE?pres ? ?ARG0?x2 ? ?ARG1?x3)
(2) np1/n1 =?
np1 :x1/n1 :x1 : @x1(?DET?(d ? *pred*))
(3) np1/n1\np2 =?
np1 :x1/n1 :x1\np2 :x2 : @x1(?GENOWN?x2)
After logical form insertion, the extracted and
augmented grammar is loaded and used to parse the
sentences in the CCGbank according to the gold-
standard derivation. If the derivation can be success-
fully followed, the parse yields a logical form which
is saved along with the corpus sentence in order to
later test the realizer. The algorithm for following
corpus derivations attempts to continue processing if
it encounters a blocked derivation due to sentence-
internal punctuation. While punctuation has been
partially reanalyzed to use lexical categories, many
problem cases remain due to the CCGbank?s re-
liance on punctuation-specific binary rules that are
not supported in OpenCCG.
185
Currently, the algorithm succeeds in creating log-
ical forms for 97.7% of the sentences in the devel-
opment section (Sect. 00) of the converted CCG-
bank, and 96.1% of the sentences in the test section
(Sect. 23). Of these, 76.6% of the development log-
ical forms are semantic dependency graphs with a
single root, while 76.7% of the test logical forms
have a single root. The remaining cases, with multi-
ple roots, are missing one or more dependencies re-
quired to form a fully connected graph. These miss-
ing dependencies usually reflect inadequacies in the
current logical form templates.
2.3 Factored Language Models
Following White et al (2007), we use factored tri-
gram models over words, part-of-speech tags and
supertags to score partial and complete realiza-
tions. The language models were created using the
SRILM toolkit (Stolcke, 2002) on the standard train-
ing sections (2?21) of the CCGbank, with sentence-
initial words (other than proper names) uncapital-
ized. While these models are considerably smaller
than the ones used in (Langkilde-Geary, 2002; Vell-
dal and Oepen, 2005), the training data does have
the advantage of being in the same domain and
genre (using larger n-gram models remains for fu-
ture investigation). The models employ interpolated
Kneser-Ney smoothing with the default frequency
cutoffs. The best performing model interpolates a
word trigrammodel with a trigrammodel that chains
a POS model with a supertag model, where the POS
model conditions on the previous two POS tags, and
the supertag model conditions on the previous two
POS tags as well as the current one.
Note that the use of supertags in the factored lan-
guage model to score possible realizations is distinct
from the prediction of supertags for lexical category
assignment: the former takes the words in the local
context into account (as in supertagging for parsing),
while the latter takes features of the logical form into
account. It is this latter process which we call hyper-
tagging, and to which we now turn.
3 The Approach
3.1 Lexical Smoothing and Search Errors
In White et al?s (2007) initial investigation of scal-
ing up OpenCCG for broad coverage realization,
test set grammar complete
oracle / best
dev (00) dev 49.1% / 47.8%
train 37.5% / 22.6%
Table 1: Percentage of complete realizations using an or-
acle n-gram model versus the best performing factored
language model.
all categories observed more often than a thresh-
old frequency were instantiated for lexical predi-
cates; for unseen words, a simple smoothing strategy
based on the part of speech was employed, assign-
ing the most frequent categories for the POS. This
approach turned out to suffer from a large number
of search errors, where the realizer failed to find a
complete realization before timing out even in cases
where the grammar supported one. To confirm that
search errors had become a significant issue, White
et al compared the percentage of complete realiza-
tions (versus fragmentary ones) with their top scor-
ing model against an oracle model that uses a simpli-
fied BLEU score based on the target string, which is
useful for regression testing as it guides the best-first
search to the reference sentence. The comparison
involved both a medium-sized (non-blind) grammar
derived from the development section and a large
grammar derived from the training sections (the lat-
ter with slightly higher thresholds). As shown in
Table 1, with the large grammar derived from the
training sections, many fewer complete realizations
are found (before timing out) using the factored lan-
guage model than are possible, as indicated by the
results of using the oracle model. By contrast, the
difference is small with the medium-sized grammar
derived from the development section. This result is
not surprising when one considers that a large num-
ber of common words are observed to have many
possible categories.
In the next section, we show that a supertag-
ger for CCG realization, or hypertagger, can reduce
the problem of search errors by focusing the search
space on the most likely lexical categories.
3.2 Maximum Entropy Hypertagging
As supertagging for parsing involves studying a
given input word and its local context, the concep-
186
tual equivalent for a lexical predicate in the LF is to
study a given node and its local graph structure. Our
implementation makes use of three general types of
features: lexicalized features, which are simply the
names of the parent and child elementary predica-
tion nodes, graph structural features, such as the
total number of edges emanating from a node, the
number of argument and non-argument dependents,
and the names of the relations of the dependent
nodes to the parent node, and syntactico-semantic
attributes of nodes, such as the tense and number.
For example, in the HLDS graph shown in Figure 1,
the node representing want has two dependents, and
the relational type of make with respect to want is
ARG1.
Clark (2002) notes in his parsing experiments that
the POS tags of the surrounding words are highly in-
formative. As discussed below, a significant gain in
hypertagging accuracy resulted from including fea-
tures sensitive to the POS tags of a node?s parent, the
node itself, and all of its arguments and modifiers.
Predicting these tags requires the use of a separate
POS tagger, which operates in a manner similar to
the hypertagger itself, though exploiting a slightly
different set of features (e.g., including features cor-
responding to the four-character prefixes and suf-
fixes of rare logical predication names). Follow-
ing the (word) supertagging experiments of (Cur-
ran et al, 2006) we assigned potentially multiple
POS tags to each elementary predication. The POS
tags assigned are all those that are some factor ?
of the highest ranked tag,3 giving an average of 1.1
POS tags per elementary predication. The values of
the corresponding feature functions are the POS tag
probabilities according to the POS tagger. At this
ambiguity level, the POS tagger is correct ? 92% of
the time.
Features for the hypertagger were extracted from
semantic dependency graphs extracted from sections
2 through 21 of the CCGbank. In total, 37,168
dependency graphs were derived from the corpus,
yielding 468,628 feature parameters.
The resulting contextual features and gold-
standard supertag for each predication were then
used to train a maximum entropy classifier model.
3I.e., all tags t whose probabilities p(t) ? ? ? p?, where p?
is the highest ranked tag?s probability.
Maximum entropy models describe a set of proba-
bility distributions of the form:
p(o | x) =
1
Z(x)
? exp
( n?
i=1
?ifi(o, x)
)
where o is an outcome, x is a context, the fi are
feature functions, the ?i are the respective weights
of the feature functions, and Z(x) is a normalizing
sum over all competing outcomes. More concretely,
given an elementary predication labeled want (as in
Figure 1), a feature function over this node could be:
f(o, x) =
{ 1, if o is (s[dcl]\np)/(s[adj]\np) and
number of LF dependents(x) = 2
0, otherwise.
We used Zhang Le?s maximum entropy toolkit4
for training the hypertagging model, which uses an
implementation of Limited-memory BFGS, an ap-
proximate quasi-Newton optimization method from
the numerical optimization literature (Liu and No-
cedal, 1989). Using L-BFGS allowed us to include
continuous feature function values where appropri-
ate (e.g., the probabilities of automatically-assigned
POS tags). We trained each hypertagging model to
275 iterations and our POS tagging model to 400 it-
erations. We used no feature frequency cut-offs, but
rather employed Gaussian priors with global vari-
ances of 100 and 75, respectively, for the hypertag-
ging and POS tagging models.
3.3 Iterative ?-Best Realization
During realization, the hypertagger serves to prob-
abilistically filter the categories assigned to an ele-
mentary predication, as well as to propose categories
for rare or unseen predicates. Given a predication,
the tagger returns a ?-best list of supertags in order
of decreasing probability. Increasing the number of
categories returned clearly increases the likelihood
that the most-correct supertag is among them, but at
a corresponding cost in chart size. Accordingly, the
hypertagger begins with a highly restrictive value for
?, and backs off to progressively less-restrictive val-
ues if no complete realization could be found using
the set of supertags returned. The search is restarted
4http://homepages.inf.ed.ac.uk/s0450736/
maxent toolkit.html.
187
Table 2: Hypertagger accuracy on Sections 00 and 23.
Results (in percentages) are for per-logical-predication
(PR) and per-whole-graph (GRPH) tagging accurcies.
Difference between best-only and baselines (b.l.) is sig-
nificant (p < 2 ? 10?16) by McNemar?s ?2 test.
Sect00 Sect23
? TagsPred PR GRPH PR GRPH
b.l. 1 1 68.7 1.8 68.7 2.3
b.l. 2 2 84.3 9.9 84.4 10.9
1.0 1 93.6 40.4 93.6 38.2
0.16 1.1 95.8 55.7 96.2 56.8
0.05 1.2 96.6 63.8 97.3 66.0
0.0058 1.5 97.9 74.8 98.3 76.9
1.75e-3 1.8 98.4 78.9 98.7 81.8
6.25e-4 2.2 98.7 82.5 99.0 84.3
1.25e-4 3.2 99.0 85.7 99.3 88.5
5.8e-5 3.9 99.1 87.2 99.4 89.9
from scratch with the next ? value, though in prin-
ciple the same chart could be expanded. The iter-
ative, ?-best search for a complete realization uses
the realizer?s packing mode, which can more quickly
determine whether a complete realization is possi-
ble. If the halfway point of the overall time limit
is reached with no complete realization, the search
switches to best-first mode, ultimately assembling
fragments if no complete realization can be found
during the remaining time.
4 Results and Discussion
Several experiments were performed in training and
applying the hypertagger. Three different models
were created using 1) non-lexicalized features only,
2) all features excluding POS tags, 3) all, 3) all
features except syntactico-semantic attributes such
as tense and number and 4) all features available.
Models trained on these feature subsets were tested
against one another on Section 00, and then the best
performing model was run on both Section 00 and
23.
4.1 Feature Ablation Testing
The the whole feature set was found in feature abla-
tion testing on the development set to outperform all
other feature subsets significantly (p < 2.2 ? 10?16).
These results listed in Table 3. As we can see, taking
Table 3: Hypertagger feature ablation testing results on
Section 00. The full feature set outperforms all others sig-
nificantly (p < 2.2 ? 10?16). Results for per-predication
(PR) and per-whole-graph (GRPH) tagging percentage
accuracies are listed. (Key: no-POS=no POS features;
no-attr=no syntactico-semantic attributes such as tense
and number; non-lex=non-lexicalized features only (no
predication names).
FEATURESET PR GRPH
full 93.6 40.37
no-POS 91.3 29.5
no-attr 91.8 31.2
non-lex 91.5 28.7
away any one class of features leads to drop in per-
predication tagging accuracy of at least 1.8% and a
drop per-whole-graph accuracy of at least 9.2%. As
expected from previous work in supertagging (for
parsing), POS features resulted in a large improve-
ment in overall accuracy (1.8%). Although the POS
tagger by itself is only 92% accurate (as a multi-
tagger of 1.1 POSword average ambiguity) ? well be-
low the state-of-the-art for the tagging of words ?
its predictions are still quite valuable to the hyper-
tagger.
4.2 Best Model Hypertagger Accuracy
The results for the full feature set on Sections 00
and 23 are outlined in Table 2. Included in this
table are accuracy data for a baseline dummy tag-
ger which simply assigns the most-frequently-seen
tag(s) for a given predication and backs off to the
overall most frequent tag(s) when confronted with
an unseen predication. The development set (00)
was used to tune the ? parameter to obtain reason-
able hypertag ambiguity levels; the model was not
otherwise tuned to it. The hypertagger achieves high
per-predication and whole-graph accuracies even at
small ambiguity levels.
4.3 Realizer Performance
Tables 4 and 5 show how the hypertagger improves
realization performance on the development and test
sections of the CCGbank. As Table 4 indicates, us-
ing the hypertagger in an iterative beta-best fash-
ion more than doubles the number of grammati-
cally complete realizations found within the time
188
Table 5: Realization quality metrics exact match, BLEU and METEOR, on complete realizations only and overall,
with and without hypertagger, on Sections 00 and 23.
Sec- Hyper- Complete Overall
tion tagger BLEU METEOR Exact BLEU METEOR
00 with 0.8137 0.9153 15.3% 0.6567 0.8494
w/o 0.6864 0.8585 11.3% 0.5902 0.8209
23 with 0.8149 0.9162 16.0% 0.6701 0.8557
w/o 0.6910 0.8606 12.3% 0.6022 0.8273
Table 4: Percentage of grammatically complete realiza-
tions, runtimes for complete realizations and overall run-
times, with and without hypertagger, on Sections 00 and
23.
Sec- Hyper- Percent Complete Overall
tion tagger Complete Time Time
00 with 47.4% 1.2s 4.5s
w/o 22.6% 8.7s 9.5s
23 with 48.5% 1.2s 4.4s
w/o 23.5% 8.9s 9.6s
limit; on the development set, this improvement eli-
mates more than the number of known search errors
(cf. Table 1). Additionally, by reducing the search
space, the hypertagger cuts overall realization times
by more than half, and in the cases where complete
realizations are found, realization times are reduced
by a factor of four, down to 1.2 seconds per sentence
on a desktop Linux PC.
Table 5 shows that increasing the number of com-
plete realizations also yields improved BLEU and
METEOR scores, as well as more exact matches. In
particular, the hypertagger makes possible a more
than 6-point improvement in the overall BLEU score
on both the development and test sections, and a
more than 12-point improvement on the sentences
with complete realizations.
As the effort to engineer a grammar suitable for
realization from the CCGbank proceeds in paral-
lel to our work on hypertagging, we expect the
hypertagger-seeded realizer to continue to improve,
since a more complete and precise extracted gram-
mar should enable more complete realizations to be
found, and richer semantic representations should
simplify the hypertagging task. Even with the cur-
rent incomplete set of semantic templates, the hy-
pertagger brings realizer performance roughly up to
state-of-the-art levels, as our overall test set BLEU
score (0.6701) slightly exceeds that of Cahill and
van Genabith (2006), though at a coverage of 96%
instead of 98%. We caution, however, that it remains
unclear how meaningful it is to directly compare
these scores when the realizer inputs vary consider-
ably in their specificity, as Langkilde-Geary?s (2002)
experiments dramatically illustrate.
5 Related Work
Our approach follows Langkilde-Geary (2002) and
Callaway (2003) in aiming to leverage the Penn
Treebank to develop a broad-coverage surface re-
alizer for English. However, while these earlier,
generation-only approaches made use of converters
for transforming the outputs of Treebank parsers to
inputs for realization, our approach instead employs
a shared bidirectional grammar, so that the input to
realization is guaranteed to be the same logical form
constructed by the parser. In this regard, our ap-
proach is more similar to the ones pursued more re-
cently by Carroll, Oepen and Velldal (2005; 2005;
2006), Nakanishi et al (2005) and Cahill and van
Genabith (2006) with HPSG and LFG grammars.
While we consider our approach to be the first to
employ a supertagger for realization, or hypertagger,
the approach is clearly reminiscent of the LTAG tree
models of Srinivas and Rambow (2000). The main
difference between the approaches is that ours con-
sists of a multitagging step followed by the bottom-
up construction of a realization chart, while theirs
involves the top-down selection of the single most
likely supertag for each node that is grammatically
189
compatible with the parent node, with the proba-
bility conditioned only on the child nodes. Note
that although their approach does involve a subse-
quent lattice construction step, it requires making
non-standard assumptions about the TAG; in con-
trast, ours follows the chart realization tradition of
working with the same operations of grammatical
combination as in parsing, including a well-defined
notion of semantic composition. Additionally, as
our tagger employs maximum entropy modeling, it
is able to take into account a greater variety of con-
textual features, including those derived from parent
nodes.
In comparison to other recent chart realization ap-
proaches, Nakanishi et al?s is similar to ours in that
it employs an iterative beam search, dynamically
changing the beam size in order to cope with the
large search space. However, their log-linear selec-
tion models have been adapted from ones used in
parsing, and do not condition choices based on fea-
tures of the input semantics to the same extent. In
particular, while they employ a baseline maximum
likelihood model that conditions the probability of
a lexical entry upon its predicate argument struc-
ture (PAS) ? that is, the set of elementary predi-
cations introduced by the lexical item ? this prob-
ability does not take into account other elements of
the local context, including parents and modifiers,
and their lexical predicates. Similarly, Cahill and
van Genabith condition the probability of their lex-
ical rules on the set of feature-value pairs linked to
the RHS of the rule, but do not take into account any
additional context. Since their probabilistic mod-
els involve independence assumptions like those in
a PCFG, and since they do not employ n-grams for
scoring alternative realizations, their approach only
keeps the single most likely edge in an equivalence
class, rather than packing them into a forest. Car-
roll, Oepen and Velldal?s approach is like Nakanishi
et al?s in that they adapt log-linear parsing models
to the realization task; however, they employ manu-
ally written grammars on much smaller corpora, and
perhaps for this reason they have not faced the need
to employ an iterative beam search.
6 Conclusion
We have introduced a novel type of supertagger,
which we have dubbed a hypertagger, that assigns
CCG category labels to elementary predications in
a structured semantic representation with high accu-
racy at several levels of tagging ambiguity in a fash-
ion reminiscent of (Bangalore and Rambow, 2000).
To our knowledge, we are the first to report tag-
ging results in the semantic-to-syntactic direction.
We have also shown that, by integrating this hy-
pertagger with a broad-coverage CCG chart real-
izer, considerably faster realization times are possi-
ble (approximately twice as fast as compared with
a realizer that performs simple lexical look-ups)
with higher BLEU, METEOR and exact string match
scores. Moreover, the hypertagger-augmented real-
izer finds more than twice the number of complete
realizations, and further analysis revealed that the
realization quality (as per modified BLEU and ME-
TEOR) is higher in the cases when the realizer finds
a complete realization. This suggests that further
improvements to the hypertagger will lead to more
complete realizations, hence more high-quality re-
alizations. Finally, further efforts to engineer a
grammar suitable for realization from the CCGbank
should provide richer feature sets, which, as our fea-
ture ablation study suggests, are useful for boosting
hypertagging performance, hence for finding better
and more complete realizations.
Acknowledgements
The authors thank the anonymous reviewers, Chris
Brew, Detmar Meurers and Eric Fosler-Lussier for
helpful comments and discussion.
References
Jason Baldridge and Geert-Jan Kruijff. 2002. Coupling
CCG and Hybrid Logic Dependency Semantics. In
Proc. ACL-02.
Jason Baldridge and Geert-Jan Kruijff. 2003. Multi-
Modal Combinatory Categorial Grammar. In Proc.
ACL-03.
Jason Baldridge. 2002. Lexically Specified Derivational
Control in Combinatory Categorial Grammar. Ph.D.
thesis, School of Informatics, University of Edinburgh.
Srinivas Bangalore and Aravind K. Joshi. 1999. Su-
190
pertagging: An Approach to Almost Parsing. Com-
putational Linguistics, 25(2):237?265.
Srinivas Bangalore and Owen Rambow. 2000. Exploit-
ing a probabilistic hierarchical model for generation.
In Proce. COLING-00.
Jeff Bilmes and Katrin Kirchhoff. 2003. Factored lan-
guage models and general parallelized backoff. In
Proc. HLT-03.
Patrick Blackburn. 2000. Representation, reasoning, and
relational structures: a hybrid logic manifesto. Logic
Journal of the IGPL, 8(3):339?625.
Johan Bos. 2005. Towards wide-coverage semantic in-
terpretation. In Proc. IWCS-6.
Stephen Boxwell and Michael White. 2008. Projecting
Propbank roles onto the CCGbank. In Proc. LREC-08.
To appear.
Aoife Cahill and Josef van Genabith. 2006. Robust
PCFG-based generation using automatically acquired
LFG approximations. In Proc. COLING-ACL ?06.
Charles Callaway. 2003. Evaluating coverage for large
symbolic NLG grammars. In Proc. IJCAI-03.
John Carroll and Stefan Oepen. 2005. High efficiency
realization for a wide-coverage unification grammar.
In Proc. IJCNLP-05.
John Carroll, Ann Copestake, Dan Flickinger, and Vic-
tor Poznan?ski. 1999. An efficient chart generator for
(semi-) lexicalist grammars. In Proc. ENLG-99.
Stephen Clark and James Curran. 2007. Wide-coverage
efficient statistical parsing with CCG and log-linear
models. Computational Linguistics, 33(4).
Stephen Clark. 2002. Supertagging for combinatory
categorial grammar. In Proceedings of the 6th Inter-
national Workshop on Tree Adjoining Grammars and
Related Frameworks (TAG+6), pages 19?24, Venice,
Italy.
James R. Curran, Stephen Clark, and David Vadas.
2006. Multi-tagging for lexicalized-grammar pars-
ing. In Proceedings of the Joint Conference of the
International Committee on Computational Linguis-
tics and the Association for Computational Linguis-
tics (COLING/ACL-06), pages 697?704, Sydney, Aus-
tralia.
Julia Hockenmaier and Mark Steedman. 2007. CCG-
bank: A Corpus of CCG Derivations and Dependency
Structures Extracted from the Penn Treebank. Com-
putational Linguistics, 33(3):355?396.
Julia Hockenmaier. 2003. Data and Models for Sta-
tistical Parsing with Combinatory Categorial Gram-
mar. Ph.D. thesis, University of Edinburgh, Edin-
burgh, Scotland.
Martin Kay. 1996. Chart generation. In Proc. ACL-96.
Irene Langkilde-Geary. 2002. An empirical verification
of coverage and correctness for a general-purpose sen-
tence generator. In Proc. INLG-02.
Irene Langkilde. 2000. Forest-based statistical sentence
generation. In Proc. NAACL-00.
Alon Lavie and Abhaya Agarwal. 2007. METEOR: An
automatic metric for MT evaluation with high levels
of correlation with human judgments. In Proceedings
of Workshop on Statistical Machine Translation at the
45th Annual Meeting of the Association of Computa-
tional Linguistics (ACL-2007), Prague.
D C Liu and Jorge Nocedal. 1989. On the limited mem-
ory method for large scale optimization. Mathematical
Programming B, 45(3).
Hiroko Nakanishi, Yusuke Miyao, and Jun?ichi Tsujii.
2005. Probabilistic methods for disambiguation of an
HPSG-based chart generator. In Proc. IWPT-05.
Martha Palmer, Dan Gildea, and Paul Kingsbury. 2005.
The proposition bank: A corpus annotated with se-
mantic roles. Computational Linguistics, 31(1).
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting of the Association for Computa-
tional Linguistics (ACL), Philadelphia, PA.
Carl J Pollard and Ivan A Sag. 1994. Head-Driven
Phrase Structure Grammar. University Of Chicago
Press.
Yves Schabes, Anne Abeille?, and Aravind K. Joshi.
1988. Parsing strategies with ?lexicalized? grammars:
Application to tree adjoining grammars. In Proceed-
ings of the 12th International Conference on Compu-
tational Linguistics (COLING-88), Budapest.
Mark Steedman. 2000. The Syntactic Process. MIT
Press, Cambridge, Massachusetts, USA.
Andreas Stolcke. 2002. SRILM ? An extensible lan-
guage modeling toolkit. In Proc. ICSLP-02.
Erik Velldal and Stephan Oepen. 2005. Maximum en-
tropy models for realization ranking. In Proc. MT
Summit X.
Erik Velldal and Stephan Oepen. 2006. Statistical rank-
ing in tactical generation. In Proceedings of the 2006
Conference on Empirical Methods in Natural Lan-
guage Processing, Sydney, Australia, July.
Michael White, Rajakrishnan Rajkumar, and Scott Mar-
tin. 2007. Towards broad coverage surface realiza-
tion with CCG. In Proc. of the Workshop on Using
Corpora for NLG: Language Generation and Machine
Translation (UCNLG+MT).
Michael White. 2006a. CCG chart realization from dis-
junctive inputs. In Proceedings, INLG 2006.
Michael White. 2006b. Efficient realization of coordi-
nate structures in Combinatory Categorial Grammar.
Research on Language and Computation, 4(1):39?75.
191
Proceedings of the Fourth International Natural Language Generation Conference, pages 12?19,
Sydney, July 2006. c?2006 Association for Computational Linguistics
CCG Chart Realization from Disjunctive Inputs
Michael White
Department of Linguistics
The Ohio State University
Columbus, OH 43210 USA
http://www.ling.ohio-state.edu/?mwhite/
Abstract
This paper presents a novel algorithm
for efficiently generating paraphrases from
disjunctive logical forms. The algorithm
is couched in the framework of Combina-
tory Categorial Grammar (CCG) and has
been implemented as an extension to the
OpenCCG surface realizer. The algorithm
makes use of packed representations sim-
ilar to those initially proposed by Shem-
tov (1997), generalizing the approach in a
more straightforward way than in the algo-
rithm ultimately adopted therein.
1 Introduction
In recent years, the generate-and-select paradigm
of natural language generation has attracted in-
creasing attention, particularly for the task of sur-
face realization. In this paradigm, symbolic meth-
ods are used to generate a space of possible phras-
ings, and statistical methods are used to select
one or more outputs from this space. To spec-
ify the desired paraphrase space, one may either
provide an input logical form that underspecifies
certain realization choices, or include explicit dis-
junctions in the input LF (or both). Our experi-
ence suggests that disjunctive LFs are an impor-
tant capability, especially as one seeks to make
grammars reusable across applications, and to em-
ploy domain-specific, sentence-level paraphrases
(Barzilay and Lee, 2003).
Prominent examples of surface realizers in
the generate-and-select paradigm include Nitro-
gen/Halogen (Langkilde, 2000; Langkilde-Geary,
2002) and Fergus (Bangalore and Rambow, 2000).
More recently, generate-and-select realizers in the
chart realization tradition (Kay, 1996) have ap-
peared, including the OpenCCG (White, 2004)
and LinGO (Carroll and Oepen, 2005) realizers.
Chart realizers make it possible to use the same
reversible grammar for both parsing and realiza-
tion, and employ well-defined methods of seman-
tic composition to construct semantic representa-
tions that can properly represent the scope of log-
ical operators.
In the chart realization tradition, previous work
has not generally supported disjunctive logical
forms, with (Shemtov, 1997) as the only published
exception (to the author?s knowledge). Arguably,
part of the reason that disjunctive LFs have not
yet been embraced more broadly by those work-
ing on chart realization is that Shemtov?s solution,
while ingenious, is dauntingly complex. Look-
ing beyond chart realizers, both Nitrogen/Halogen
and Fergus support some forms of disjunctive in-
put; however, in comparison to Shemtov?s inputs,
theirs are less expressive, in that they do not al-
low disjunctions across different levels of the input
structure.
As an alternative to Shemtov?s method, this pa-
per presents a chart realization algorithm for gen-
erating paraphrases from disjunctive logical forms
that is more straightforward to implement, to-
gether with an initial case study of the algorithm?s
efficiency. As discussed in Section 5, the algo-
rithm makes use of packed representations similar
to those initially proposed by Shemtov, generaliz-
ing the approach in a way that avoids the prob-
lems that led Shemtov to reject his preliminary
method. The algorithm is couched in the frame-
work of Steedman?s (2000) Combinatory Catego-
rial Grammar (CCG) and has been implemented
as an extension to the OpenCCG surface realizer.
Though the algorithm is well suited to CCG, it is
expected to be applicable to other constraint-based
grammatical frameworks as well.
12
be<TENSE>pres,<MOOD>dcl e 
<ARG> <PROP>
based_on <DET>the,<NUM>sgdesign d p 
<SOURCE> <ARTIFACT> collection<DET>the,<NUM>sg c 
<HASPROP> <CREATOR>Funny_Day f v Villeroy_and_Boch
 
(a) Semantic dependency graph for The design (is|?s)
based on the Funny Day collection by Villeroy and
Boch.
be<TENSE>pres,<MOOD>dcl e 
<ARG> <PROP>
based_on <DET>the,<NUM>sgdesign d p 
<SOURCE> <ARTIFACT> series<NUM>sg c 
<HASPROP> <GENOWNER>Funny_Day f v Villeroy_and_Boch
 
(b) Semantic dependency graph for The design (is|?s)
based on Villeroy and Boch?s Funny Day series.
be<TENSE>pres,<MOOD>dcl e 
<ARG> <PROP>
based_on <DET>the,<NUM>sgdesign d p 
<SOURCE> <ARTIFACT> collection|series(<DET>the)?,<NUM>sg c 
<HASPROP> <GENOWNER>Funny_Day f v Villeroy_and_Boch
<CREATOR>
 
(c) Disjunctive semantic dependency graph covering (a)-
(b), i.e. The design (is|?s) based on (the Funny Day
(collection|series) by Villeroy and Boch | Villeroy and
Boch?s Funny Day (collection|series)).
Figure 1: Example semantic dependency graphs
from the COMIC dialogue system.
@e(be ? ?TENSE?pres ? ?MOOD?dcl ?
?ARG?(d ? design ? ?DET?the ? ?NUM?sg) ?
?PROP?(p ? based on ?
?ARTIFACT?d ?
?SOURCE?(c ? collection ? ?DET?the ? ?NUM?sg ?
?HASPROP?(f ? Funny Day) ?
?CREATOR?(v ? V&B))))
(a)
...
@e(be ? ?TENSE?pres ? ?MOOD?dcl ?
?ARG?(d ? design ? ?DET?the ? ?NUM?sg) ?
?PROP?(p ? based on ?
?ARTIFACT?d ?
?SOURCE?(c ? ?NUM?sg ? (?DET?the)? ?
(collection ? series) ?
?HASPROP?(f ? Funny Day) ?
(?CREATOR?v ? ?GENOWNER?v ))))
? @v(Villeroy and Boch)
(c)
Figure 2: HLDS for examples in Figure 1.
2 Disjunctive Logical Forms
As an illustration of disjunctive logical forms,
consider the semantic dependency graphs in Fig-
ure 1, which are taken from the COMIC1 mul-
timodal dialogue system.2 Graphs such as these
constitute the input to the OpenCCG realizer.
Each node has a lexical predication (e.g. design)
and a set of semantic features (e.g. ?NUM?sg);
nodes are connected via dependency relations (e.g.
?ARTIFACT?).
Given the lexical categories in the COMIC
grammar, the graphs in Figure 1(a) and (b) fully
specify their respective realizations, with the ex-
ception of the choice of the full or contracted
form of the copula. To generalize over these al-
ternatives, the disjunctive graph in (c) may be
employed. This graph allows a free choice be-
tween the domain synonyms collection and se-
ries, as indicated by the vertical bar between
their respective predications. The graph also al-
lows a free choice between the ?CREATOR? and
?GENOWNER? relations?lexicalized via by and
the possessive, respectively?connecting the head
c (collection or series) with the dependent v (for
1http://www.hcrc.ed.ac.uk/comic/
2To simplify the exposition, the features specifying infor-
mation structure and deictic gestures have been omitted, as
have the semantic sorts of the discourse referents.
13
@e(see ? ?ARG0?(m ? man) ? ?ARG1?(g ? girl) ?
@o(on??ARG1?(h?hill))?@w(with??ARG1?(t? telescope))?
((?MOD?o ? @h(?MOD?w)) ?
(@g(?MOD?o) ? (@g(?MOD?w) ? @h(?MOD?w))) ?
(?MOD?w ? (?MOD?o ? @g(?MOD?o)))))
Figure 3: Disjunctive LF for 5-way ambiguity in
A man saw a girl on the hill with a telescope.
Villeroy and Boch); this choice is indicated by an
arc between the two dependency relations. Finally,
the determiner feature (?DET?the) on c is indicated
as optional, via the question mark.
It is worth pausing at this point to observe
that in designing the COMIC grammar, the differ-
ences between (a) and (b) could perhaps have been
collapsed. However, such a move would make
it more difficult to reuse the grammar in other
applications?and indeed, the core of the gram-
mar is shared with the FLIGHTS system (Moore et
al., 2004)?as it would presuppose that these para-
phrases should always available in the same con-
texts. An example of a sentence-level paraphrase,
whose context of applicability is more clearly lim-
ited, appears in (1):
(1) (This design | This one | This) (is|?s) (clas-
sic | in the classic style) | Here we have
a (classic design | design in the classic
style).
This example shows some of the phrasings that
may be used in COMIC to describe the style of
a design that has not been discussed previously.
The example includes a top-level disjunction be-
tween the use of a deictic NP this design | this one
| this (with an accompanying pointing gesture) fol-
lowed by the copula, or the use of the phrase here
we have to introduce the design. While these al-
ternatives can function as paraphrases in this con-
text, it is difficult to see how one might specify
them in a single underspecified (and application-
neutral) logical form.
Graphs such as those in Figure 1 are repre-
sented internally using Hybrid Logic Dependency
Semantics (HLDS), as in Figure 2. HLDS is a
dependency-based approach to representing lin-
guistic meaning developed by Baldridge and Krui-
jff (2002). In HLDS, hybrid logic (Blackburn,
2000) terms3 are used to describe dependency
3Hybrid logic extends modal logic with nominals, a new
sort of basic formula that explicitly names states/nodes. Like
propositions, nominals are first-class citizens of the object
graphs. These graphs have been suggested as rep-
resentations for discourse structure, and have their
own underlying semantics (White, 2006).
In HLDS, as can be seen in Figure 2(a), each
semantic head is associated with a nominal that
identifies its discourse referent, and heads are con-
nected to their dependents via dependency re-
lations, which are modeled as modal relations.
Modal relations are also used to represent seman-
tic features. In (c), two new operators are in-
troduced to represent periphrastic alternatives and
optional parts of the meaning, namely ? and (?)?,
for exclusive-or and optionality, respectively. To
indicate that a nominal represents a reference to a
node that is considered a shared part of multiple
alternatives, the nominal is annotated with a box,
as exemplified by v . As will be discussed in Sec-
tion 3.1, this notion of shared references is needed
during the logical form flattening stage of the al-
gorithm in order to determine which elementary
predications are part of each alternative.
As mentioned earlier, disjunctive LFs may con-
tain alternations that are not at the same level.
To illustrate, Figure 3 shows the representation
(minus semantic features) for the 5-way ambigu-
ity in A man saw a girl on the hill with a tele-
scope (Shemtov, 1997, p. 45); in the figure, the
nominal o (for on) can be a dependent of e (for
see) or g (for girl), for example. As Shemtov ex-
plains, such packed representations can be useful
in machine translation for generating ambiguity-
preserving target language sentences. In a straight
generation context, disjunctions that span levels
enable one to compactly represent alternatives that
differ in their head-dependent assumptions; for in-
stance, to express contrast, one might employ the
coordinate conjunction but as the sentence head,
or the subordinate conjunction although as a de-
pendent of the main clause head.
3 The Algorithm
As with the other chart realizers cited in the in-
troduction, the OpenCCG realizer makes use of a
chart and an agenda to perform a bottom-up dy-
namic programming search for signs whose LFs
language, and thus formulas can be formed using propo-
sitions, nominals, and standard boolean operators. They
may also employ the satisfaction operator, @. A formula
@i(p??F?(j?q)) indicates that the formulas p and ?F?(j?q)
hold at the state named by i, and that the state j, where q
holds, is reachable via the modal relation F; equivalently, it
states that node i is labeled by p, and that node j, labeled by
q, is reachable from i via an arc labeled F.
14
completely cover the elementary predications in
the input logical form. The search for complete
realizations proceeds in one of two modes, any-
time or two-stage packing/unpacking. This sec-
tion focuses on how the two-stage mode has been
extended to efficiently generate paraphrases from
disjunctive logical forms.
3.1 LF Flattening
In a preprocessing stage, the input logical form
is flattened to an array of elementary predications
(EPs), one for each lexical predication, semantic
feature or dependency relation. When the input
LF contains no exclusive-or or optionality oper-
ators, the list of EPs, when conjoined, yields a
graph description that is equivalent to the origi-
nal one. With disjunctive logical forms, however,
more needs to be said. Our strategy is to keep track
of the elementary predications that make up the al-
ternatives and optional parts of the LF, as specified
by the exclusive-or or optionality operators, and
use these to enforce constraints on the elementary
predications that may appear in any given realiza-
tion. These constraints ensure that only combina-
tions of EPs that describe a graph that is also de-
scribed by the original LF are allowed.
To illustrate, the results of flattening the LF in
Figure 2(c) are given below:
(2) 0: @e(be), 1: @e(?TENSE?pres),
2: @e(?MOOD?dcl), 3: @e(?ARG?d),
4: @d(design), 5: @d(?DET?the),
6: @d(?NUM?sg),
7: @e(?PROP?p), 8: @p(based on),
9: @p(?ARTIFACT?d), 10: @p(?SOURCE?c),
11: @c(?NUM?sg), 12: @c(?DET?the),
13: @c(collection), 14: @c(series),
15: @c(?HASPROP?f), 16: @f (Funny Day),
17: @c(?CREATOR?v), 18: @c(?GENOWNER?v),
19: @v(Villeroy and Boch)
(3) alt0,0 = {13}; alt0,1 = {14}
alt1,0 = {17, 19}; alt1,1 = {18, 19}
opt0 = {12}
In (2), the EPs are shown together with their array
positions. Since the EPs are tracked positionally, it
is possible to use bit vectors to represent the alter-
natives and optional parts of the LF. In (3), the first
line shows the bit vectors4 for the choice between
collection (EP 13) and series (EP 14), as alterna-
tives 0 and 1 in alternative group 0. On the sec-
4Only the positive bits are shown, via their indices.
ond line, the bit vectors for the ?CREATOR? (EP
17) and ?GENOWNER? (EP 18) alternatives ap-
pear; note that both of these options also involve
the shared EP 19. The bit vector for the optional
determiner (EP 12) is shown on the third line.
The constraint associated with each group of al-
ternatives is that in order to be valid, a collection
of EPs must not intersect with the non-overlapping
parts of more than one alternative. For example,
for the second group of alternatives in (3), a valid
collection could include EPs 17 and 19, or EPs 18
and 19, but it could not include EPs 17 and 18 to-
gether.
Flattening an LF to obtain the array of EPs,
as in (2), just requires a relatively straightforward
traversal of the HLDS formula. Obtaining the al-
ternatives and optional parts of the LF is a bit
more involved. To do so, during the traversal,
the exclusive-or and optionality operators are han-
dled by introducing a new alternative group or op-
tional part, and then keeping track of which ele-
mentary predications fall under each alternative or
under the optional part. Subsequently, the alterna-
tives and optional parts are recursively propagated
through any nominals marked as shared, collecting
any further EPs that turn up along the way.5 For
example, with the second alternative group (sec-
ond line) of (3), the initial traversal creates EPs
17 and 18 under alts alt1,0 and alt1,1, respectively.
Since EPs 17 and 18 both include a nominal de-
pendent v marked as shared in Figure 2(c), both
alternatives are propagated through this reference,
and thus EP 19 ends up as part of both alt1,0 and
alt1,1. Determining which EPs have shared mem-
bership in multiple alternatives is essential for ac-
curately tracking an edge?s coverage of the input
LF, a topic which will be considered next.
3.2 Edges
In the OpenCCG realizer, an edge is a data struc-
ture that wraps a CCG sign, which itself consists
of a word sequence paired with a category (syn-
tactic category plus logical form). An edge has
bit vectors to record its coverage of the input LF
and its indices, i.e. syntactically available nomi-
nals. In packing mode, a representative edge also
maintains a list of alternative edges whose signs
have equivalent categories (but different word se-
quences), so that a representative edge may effec-
5Though space precludes discussion, it is worth noting
that the same propagation of membership applies to the LF
chunks described in (White, 2006).
15
tively stand in for the others during chart construc-
tion.
To handle disjunctive inputs, an edge addition-
ally maintains a list of active (i.e., partially com-
pleted) LF alternatives. It also makes use of a
revised notion of input coverage and a revised
equivalence relation. As in Shemtov?s (1997, Sec-
tion 3.3.2) preliminary algorithm, an edge is con-
sidered to cover an entire disjunction (alternative
group) if it covers all the EPs of one of its alter-
natives. With optional parts of an LF, an edge that
does not cover any EPs in the optional part can be
extended to a new edge (using the same sign) that
is additionally considered to cover all the EPs in
the optional part. In this way, an edge can be de-
fined to be complete with respect to the input LF
if it covers all its EPs. For example, an edge for
the sentence in Figure 1(b) would be considered
complete, since (i) it would cover all the EPs in
(2) except for 12, 13 and 17; (ii) 12 is optional;
(iii) 14 completes alt0,1, and thus counts as cover-
ing 13, the other EP in the group; and (iv) 18 and
19 complete alt1,1, and thus count as covering EP
17.
As Shemtov points out, this extended notion
of input coverage provides an appropriate way to
form edge equivalence classes, as it can gather
edges together that realize different alternatives in
the same group. Thus, in OpenCCG, edge equiva-
lence classes have been modified to include edges
with the same syntactic category and coverage bit
vector, but different word sequences and/or logical
forms (as the latter varies according to which al-
ternative is realized). The appropriate equivalence
checks are efficiently carried out using a hash map
with a custom hash function and equals method.
3.3 Lexical Instantiation
Once the input LF has been flattened, and the alter-
natives and optional parts have been identified, the
next step is to access and instantiate lexical items.
For each elementary predication, all lexical items
indexed by the EP?s lexical predicate or relation
are retrieved from the lexicon.6 Each such lexi-
cal item is then instantiated against the input EPs,
starting with the one that triggered its retrieval,
and incrementally extending successful instantia-
tions until all the lexical item?s EPs have been in-
stantiated (otherwise failing). The lexical instanti-
6See (White, 2004; White, 2006) for discussion of how
semantically null lexical items and unary type changing rules
are handled.
ation routine returns all instantiations that satisfy
the alternative exclusion constraints. Associated
with each instantiation is a bit vector that encodes
the coverage of the input EPs. From each bit vec-
tor, the active (partially completed) LF alternatives
are determined, and the bit vector is updated to in-
clude the EPs in any completed disjunctions. Fi-
nally, edges are created for the instantiated lexical
items, which include the active alternatives and the
updated coverage vector.
Continuing with example (2)-(3), the selected
lexical edges in (4) below illustrate how lexical in-
stantiation interacts with disjunctions:
(4) a. {11,13,14} collection ` nc :
@c(collection) ? @c(?NUM?sg)
b. {11,13,14} series ` nc :
@c(series) ? @c(?NUM?sg)
c. {17} alt1,0 by ` nc\nc/npv :
@c(?CREATOR?v)
d. {18} alt1,1 ?s ` npc/nc\npv :
@c(?GENOWNER?v)
e. {19} alt1,0; alt1,1 Villeroy and Boch ` npv
: @v (V&B)
The nouns in (a) and (b) complete alt0,0 and alt0,1,
respectively, and thus they each count as cover-
ing EPs 11, 13 and 14. In (c) and (d), by and ?s
partially cover alt1,0 and alt1,1, respectively, and
thus these alternatives are active for their respec-
tive edges. In (e), V&B partially covers both alt1,0
and alt1,1, and thus both alternatives are active.
3.4 Derivation
Following lexical instantiation, the lexical edges
are added to the agenda, as is usual practice with
chart algorithms, and the main loop is initiated.
During each iteration of the main loop, an edge
is moved from the agenda to the chart. If the edge
is in the same equivalence class as an edge already
in the chart, it is added as an alternative to the ex-
isting representative edge. Otherwise, it is com-
bined with all applicable edges in the chart (via the
grammar?s combinatory rules), as well as with the
grammar?s unary rules, where any newly created
edges are added to the agenda. The loop termi-
nates when no edges remain on the agenda.
Before edge combinations are attempted, a
number of constraints are checked, as detailed in
(White, 2006). In particular, the edges? coverage
bit vectors are required to not intersect, which en-
sures that they cover disjoint parts of the input LF.
Since the coverage vectors are updated to cover all
the EPs in a disjunction when one of the alterna-
tives is completed, this check also ensures that the
16
1. {8-10} based on ` sp\npd/npc
2. {12} the ` npc/nc
3. {15, 16} Funny Day ` nc/nc
4. {11, 13, 14} collection ` nc
{11, 13, 14} series ` nc
5. {17} alt1,0 by ` nc\nc/npv
6. {18} alt1,1 ?s ` npc/nc\npv
7. {19} alt1,0; alt1,1 Villeroy and Boch ` npv
8. {11, 13-16} FD [collection] ` nc (3 4 >)
9. {17-19} by V&B ` nc\nc (5 7 >)
10. {17-19} V&B ?s ` npc/nc (7 6 <)
11. {11, 13-19} FD [coll.] by V&B ` nc (8 9 <)
12. {11, 13-19} V&B ?s FD [coll.] ` npc (10 8 >)
13. {11-19} the FD [coll.] by V&B ` npc (2 11 >)
{11-19} V&B ?s FD [coll.] ` npc (12 optC)
14. {8-19} b. on [the FD [coll.] . . . ] ` sp\npd (1 13 >)
Figure 4: Part of realization chart for Figure 1(c).
exclusion constraints for the disjunction continue
to be enforced. Thus, for example, no attempt will
be made to combine the edges for collection and
series in (4a) and (4b), since they both express EP
11 and since they contribute to different alterna-
tives in group 0.
To enforce the constraints associated with active
alternatives, a compatibility check is made to en-
sure that if the input edges have active alternatives
in the same group, the intersection of these alter-
natives is non-empty. To illustrate, consider the
edges for by and the possessive ?s in (4c) and (4d).
Since these edges have different alternatives active
within group 1, the compatibility check fails, and
thus their combination is not attempted. By con-
trast, the edge for Villeroy and Boch in (4e) will
pass the compatibility check with both (4c) and
(4d), as it shares an active alternative in common
with each of these. When two edges succeed in
combining, a new edge is constructed from the re-
sulting sign by taking the union of the coverage bit
vectors, determining the active alternatives, and
updating the coverage vector to include the EPs
in any completed disjunctions.
When the grammar?s unary rules are applied to
an edge, an operation is also invoked for creat-
ing an edge (for the same sign) with one or more
optional parts marked as completed. This oper-
ation is invoked when it would complete the in-
put LF, complete an alternative, or complete an LF
chunk.7 A constraint on its application is that the
optional parts must be wholly missing from the in-
put edge; additionally, in the case of completing an
alternative or LF chunk, the optional parts must be
part of the alternative or chunk in question.
Figure 4 demonstrates how the lexical edges in
(4) are combined in the chart.8 These lexical edges
appear on lines 4-7. Note that the edge for series
is added as an alternative edge to the one for col-
lection, which acts as a representative for both; to
highlight its role as a representative, collection is
shown in square brackets from line 8 onwards. At
the end of each line, the derivation of each (non-
lexical) edge is shown in parentheses, in terms of
its input edges and combinatory rule. On line 13,
observe that the NP using the possessive is added
as an alternative to the one using the by-phrase;
the possessive version becomes part of the same
equivalence class when the optional determiner is
marked as covered, via the optional part comple-
tion operation.
3.5 Unpacking
Once chart construction has finished, the complete
realizations are recursively unpacked bottom-up in
a way that generalizes the approach of (Langkilde,
2000). Unpacking proceeds by multiplying out the
alternative edges stored with the representative in-
put edges; filtering out any duplicate edges result-
ing from spurious ambiguities; scoring the new
edges with the scoring method configured via the
API; and pruning the results with the configured
pruning strategy. Note that since there is no need
for checking grammatical or other constraints dur-
ing the unpacking stage, new edges can be quickly
and cheaply constructed using structure sharing.
To briefly illustrate the process, consider how
the Funny Day collection edge in line 8 of Fig-
ure 4 is unpacked. While the Funny Day input
edge has no alternative edges, the collection input
edge has the series edge as an alternative, and thus
a new Funny Day series edge will be created and
scored; as long as the pruning strategy keeps more
than the single-best option, this edge will be added
as an alternative, and both combinations will be
propagated upwards through the edges in lines 11
7LF chunks serve to avoid propagating semantically in-
complete phrases; see (White, 2006) for discussion.
8To save space, the figure only shows part of the normal
form derivation, and the logical forms for the categories have
been suppressed.
17
10-best two-stage 1-best anytime
time edges time edges
disjunctive 1.1 602 0.5 281
sequential 5.6 3550 4.1 2854
Table 1: Comparison of average run times (in sec-
onds) and edges created vs. sequential realization
and 12.
4 Case Study
To examine the potential of the algorithm to effi-
ciently generate paraphrases, this section presents
a case study of its run times versus sequential real-
ization of the equivalent top-level LF alternatives
in disjunctive normal form. The study used the
COMIC grammar, a small but not trivial grammar
that suffices for the purposes of the system. In
this grammar, there are relatively few categories
per lexeme on average, but the boundary tone cat-
egories engender a great deal of non-determinism.
With other grammars, run times can be expected
to vary.
In anticipation of the present work, Foster and
White (2004) generated disjunctive logical forms
during sentence planning, then (as a stopgap mea-
sure) multiplied out the disjunctions and sequen-
tially realized the top-level alternatives until an
overall time limit was reached. Taking the pre-
vious logical forms as a starting point, 104 sen-
tences from the evaluation in (Foster and White,
2005) were selected, and their LFs were manu-
ally augmented to cover a greater range of para-
phrases allowed by the grammar.9 To obtain the
corresponding top-level LF alternatives, 100-best
realization was performed, and the unique LFs
appearing in the top 100 realizations were gath-
ered; on average, there were 29 such unique LFs.
We then compared the present algorithm?s per-
formance against sequential realization in produc-
ing 10-best outputs and single-best outputs. In
the 10-best case, we used the two-stage pack-
ing/unpacking mode; for the single-best case, we
used the anytime mode with 3-best pruning. With
both cases, the run times include scoring with a
trigram language model, and were measured on a
2.8GHz Linux PC. Realization quality was not as-
sessed as part of the study, though manual inspec-
tion indicated that it was very high.
Table 1 shows the results of the comparison.
9Extending the COMIC sentence planner to produce these
augmented LFs is left for future work.
The average run times of the present algorithm,
with disjunctive LFs as input, appear on the first
line, along with the average number of edges cre-
ated; on the second line are the average aggregate
run times and num. edges created of sequentially
realizing the top-level alternatives (not including
the time taken to produce these alternatives). As
can be seen, realization from disjunctive inputs
yields a 5-fold and 8-fold speedup over the se-
quential approach in the two cases, with corre-
sponding reductions in the number of edges cre-
ated. Additionally, the run times appear to be ad-
equate for use in interactive dialogue systems (es-
pecially in the anytime, single-best case).
5 Comparison to Shemtov (1997)
The present approach differs from Shemtov?s in
two main ways. First, since Shemtov developed
his approach with the task of ambiguity preserv-
ing translation in mind, he framed the problem as
one of generating from ambiguous semantic rep-
resentations, such as one might find in a parse
chart with unresolved ambiguities. Consequently,
he devised a method for converting the meanings
in a packed parse chart into an encoding where
each fact (here, EP) appears exactly once, together
with an indication of the meaning alternatives it
belongs to, expressed as propositional formulas.
While this contexted facts encoding may be suit-
able for MT, it is not very convenient as an input
representation for systems which generate from
non-linguistic data, as the formulas representing
the contexts only make sense in reference to a
parse chart. By contrast, the present approach
takes as input disjunctive logical forms that should
be reasonably intuitive to construct in dialogue
systems or other NLG applications, since they are
straightforwardly related to their non-disjunctive
counterparts.
The second way in which the approach differs
concerns the relative simplicity of the algorithms
ultimately adopted. As part of his preliminary al-
gorithm (Shemtov, 1997, Section 3.3.2), Shemtov
proposed the extended use of coverage bit vectors
that we embraced in Section 3.2. He then de-
veloped a refined version to handle disjunctions
with intersecting predicates. However, he con-
cluded that this refined version was arc-consistent
but not path-consistent (p. 65, fn. 10), given that it
checked combinations of contexted facts pairwise,
without keeping track of which alternations such
18
combinations were committed to. By contrast, the
present approach does not suffer from this defect,
because it checks the alternative exclusion con-
straints on all of a lexical edge?s EPs at once (us-
ing bit vectors for both edge coverage and alter-
native membership), and also ensures that the ac-
tive alternatives are compatible before combining
edges during derivations. Shemtov does not ap-
pear to have considered a solution along the lines
proposed here; instead, he went on to develop a
sound but considerably more complex algorithm
(his Section 3.4), where an edge?s coverage bit
vector is replaced with a contexted coverage array
(an array of boolean conditions). With these ar-
rays, it is no longer easy to group edges into equiv-
alence classes, and thus during chart construc-
tion Shemtov is forced to group together edges
which are not derivationally equivalent. Conse-
quently, to prevent overgeneration, his algorithm
has to solve during the enumeration phase a sys-
tem of constraints (potentially exponential in size)
formed from the conditions in the contexted cov-
erage arrays?a process which is far from straight-
forward.
6 Conclusions
This paper has presented a new chart realization
algorithm for efficiently generating surface real-
izations from disjunctive logical forms, and has
argued that the approach represents an improve-
ment over that of (Shemtov, 1997) in terms of both
usability and simplicity. The algorithm has been
implemented as an extension to the OpenCCG hy-
brid symbolic/statistical realizer, and has recently
been employed to generate n-best realization lists
for reranking according to their predicted synthe-
sis quality (Nakatsu and White, 2006), as well as
to generate dialogues exhibiting individuality and
alignment(Brockmann et al, 2005; Isard et al,
2005). An initial case study has shown that the
algorithm works many times faster than sequential
realization, with run times suitable for use in dia-
logue systems; a more comprehensive study of the
algorithm?s efficiency is planned for future work.
Acknowledgements
The author thanks Mary Ellen Foster, Amy Isard,
Johanna Moore, Mark Steedman and the anony-
mous reviewers for helpful feedback and discus-
sion, and the University of Edinburgh?s Institute
for Communicating and Collaborative Systems for
partially supporting this work.
References
Jason Baldridge and Geert-Jan Kruijff. 2002. Coupling CCG
and Hybrid Logic Dependency Semantics. In Proc. ACL-
02.
Srinivas Bangalore and Owen Rambow. 2000. Exploiting a
probabilistic hierarchical model for generation. In Proc.
COLING-00.
Regina Barzilay and Lillian Lee. 2003. Learning to
paraphrase: An unsupervised approach using multiple-
sequence alignment. In Proc. of NAACL-HLT.
Patrick Blackburn. 2000. Representation, reasoning, and re-
lational structures: a hybrid logic manifesto. Logic Jour-
nal of the IGPL, 8(3):339?625.
Carsten Brockmann, Amy Isard, Jon Oberlander, and
Michael White. 2005. Modelling alignment for affec-
tive dialogue. In Proc. UM-05 Workshop on Adapting the
Interaction Style to Affective Factors.
John Carroll and Stefan Oepen. 2005. High efficiency real-
ization for a wide-coverage unification grammar. In Proc.
IJCNLP-05.
Mary Ellen Foster and Michael White. 2004. Techniques for
Text Planning with XSLT. In Proc. 4th NLPXML Work-
shop.
Mary Ellen Foster and Michael White. 2005. Assessing the
impact of adaptive generation in the COMIC multimodal
dialogue system. In Proc. IJCAI-05 Workshop on Knowl-
edge and Representation in Practical Dialogue Systems.
Amy Isard, Carsten Brockmann, and Jon Oberlander. 2005.
Individuality and alignment in generated dialogues. In
Proc. INLG-06. To appear.
Martin Kay. 1996. Chart generation. In Proc. ACL-96.
Irene Langkilde-Geary. 2002. An empirical verification of
coverage and correctness for a general-purpose sentence
generator. In Proc. INLG-02.
Irene Langkilde. 2000. Forest-based statistical sentence gen-
eration. In Proc. NAACL-00.
Johanna Moore, Mary Ellen Foster, Oliver Lemon, and
Michael White. 2004. Generating tailored, comparative
descriptions in spoken dialogue. In Proc. FLAIRS-04.
Crystal Nakatsu and Michael White. 2006. Learning to say it
well: Reranking realizations by predicted synthesis qual-
ity. In Proc. of COLING-ACL-06. To appear.
Hadar Shemtov. 1997. Ambiguity Management in Natural
Language Generation. Ph.D. thesis, Stanford University.
Mark Steedman. 2000. The Syntactic Process. MIT Press.
Michael White. 2004. Reining in CCG Chart Realization. In
Proc. INLG-04.
Michael White. 2006. Efficient Realization of Coordinate
Structures in Combinatory Categorial Grammar. Research
on Language & Computation, on-line first, March.
19
Proceedings of the NAACL HLT Workshop on Software Engineering, Testing, and Quality Assurance for Natural Language Processing, pages 45?46,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Grammar Engineering for CCG using Ant and XSLT?
Scott Martin, Rajakrishnan Rajkumar, and Michael White
Ohio State University
Department of Linguistics
{scott,raja,mwhite}@ling.ohio-state.edu
Overview
Corpus conversion and grammar extraction have
traditionally been portrayed as tasks that are
performed once and never again revisited (Burke
et al, 2004). We report the successful imple-
mentation of an approach to these tasks that
facilitates the improvement of grammar engi-
neering as an evolving process. Taking the
standard version of the CCGbank (Hocken-
maier and Steedman, 2007) as input, our sys-
tem then introduces greater depth of linguis-
tic insight by augmenting it with attributes
the original corpus lacks: Propbank roles and
head lexicalization for case-marking preposi-
tions (Boxwell and White, 2008), derivational
re-structuring for punctuation analysis (White
and Rajkumar, 2008), named entity annotation
and lemmatization. Our implementation ap-
plies successive XSLT transforms controlled by
Apache Ant (http://ant.apache.org/) to an
XML translation of this corpus, finally produc-
ing an OpenCCG grammar (http://openccg.
sourceforge.net/). This design is beneficial
to grammar engineering both because of XSLT?s
unique suitability to performing arbitrary trans-
formations of XML trees and the fine-grained
control that Ant provides. The resulting system
enables state-of-the-art BLEU scores for surface
realization on section 23 of the CCGbank.
1 Design
Rather than transforming the corpus, it would
be simple to introduce several of the corpus aug-
?This work was supported in part by NSF grant no.
IIS-0812297.
mentations that we make (e.g. punctuation re-
structuring) during grammar extraction. How-
ever, machine learning applications (e.g., real-
ization ranking) benefit when the corpus and
extracted grammar are consistent. A case in
point: annotating the corpus with named en-
tities, then using n-gram models with words re-
placed by their class labels to score realization.
Accordingly, our pipeline design starts by gen-
erating an XML version of the CCGbank us-
ing JavaCC (http://javacc.dev.java.net/)
from the original corpus. Next, conversion and
extraction transforms are applied to create a
converted corpus (also in XML) and extracted
grammar (in OpenCCG format).
We refactored our original design to separate
the grammar engineering task into several con-
figurable processes using Ant tasks. This sim-
plifies process management, speeds experiment
iterations, and facilitates the comparison of dif-
ferent grammar engineering strategies.
2 Implementation
It seemed natural to implement our pipeline pro-
cedure in XSLT since both OpenCCG grammars
and our CCGbank translation are represented in
XML. Aside from its inherent attributes, XSLT
requires no re-compilation as a result of being an
interpreted language. Also, because both con-
version and extraction use a series of transforms
in a chain, each required sub-step can be split
into as many XSLT transforms as desired.
Both the conversion and extraction steps
were implemented by extending Ant with cus-
tom tasks as configuring Ant tasks requires no
45
source editing or compilation. Ant is partic-
ularly well-suited to this process because, like
OpenCCG (whose libraries are used in the ex-
traction phase), it is written in Java. Our sys-
tem also employs the Ant-provided javacc task,
invoking the JavaCC parser to translate the
CCGbank to XML. This approach is preferable
to a direct Java implementation because it keeps
source code and configuration separate, allowing
for more rapid grammar engineering iterations.
Our particular implementation harnesses
Ant?s built-in FileSet (for specification of
groups of corpus files) and FileList (for re-
use of series of XSLT transforms) data types.
The first of our extension tasks, convert, encap-
sulates the conversion process while the second
task, extract, implements the grammar extrac-
tion procedure for a previously-converted cor-
pus.
3 Experimental Impact
Our conversion process currently supports var-
ious experiments by including only specified
transforms. We gain the ability to cre-
ate corpora with various combinations of at-
tributes, among them punctuation annotation,
semantic class information, and named entities
(lack of space precludes inclusion of examples
here; see http://www.ling.ohio-state.edu/
~scott/publications/grammareng/). In ad-
dition to extracting grammars, the extraction
task employs a constrained parser to create log-
ical forms (LFs) for surface realization and ex-
tracts SRILM training data for realization scor-
ing. This task also enables feature extraction
from LF graphs for training during supertagging
for realization (Espinosa et al, 2008).
Our design supports comprehensive experi-
mentation and has helped facilitate recent ef-
forts to investigate factors impacting surface re-
alization, such as semantic classes and named
entities. Our initial results reported in (White et
al., 2007) record 69.7% single-rooted LFs with a
BLEU score of 0.5768. But current figures stand
at 95.8% single-rooted LFs and a state-of-the
art BLEU score of 0.8506 on section 23 of the
CCGbank. (Fragmentary LFs result when at
least one semantic dependency is missing from
the LF graph.) In achieving these results, im-
provements in the grammar engineering process
have been at least as important as improvements
in the statistical models.
4 Conclusions and Future Work
We designed and implemented a system that fa-
cilitates the process of grammar engineering by
separating conversion and extraction steps into
a pipeline of XSLT transforms. Our Ant imple-
mentation is highly configurable and has posi-
tive effects on our grammar engineering efforts,
including increased process control and a short-
ened testing cycle for different grammar engi-
neering approaches. Future work will focus on
increasing the number of single-rooted LFs and
integrating this system with OpenCCG.
References
[Boxwell and White2008] Stephen Boxwell and
Michael White. 2008. Projecting Propbank roles
onto the CCGbank. In Proc. LREC-08.
[Burke et al2004] Michael Burke, Aoife Cahill,
Mairead Mccarthy, Ruth O?Donovan, Josef
Genabith, and Andy Way. 2004. Evaluating
automatic LFG F-structure annotation for the
Penn-II treebank. Research on Language and
Computation, 2:523?547, December.
[Espinosa et al2008] Dominic Espinosa, Michael
White, and Dennis Mehay. 2008. Hypertagging:
Supertagging for surface realization with CCG.
In Proc. ACL-08: HLT.
[Hockenmaier and Steedman2007] Julia Hockenmaier
and Mark Steedman. 2007. CCGbank: A Corpus
of CCG Derivations and Dependency Structures
Extracted from the Penn Treebank. Computa-
tional Linguistics, 33(3):355?396.
[White and Rajkumar2008] Michael White and Ra-
jakrishnan Rajkumar. 2008. A more precise
analysis of punctuation for broad-coverage sur-
face realization with CCG. In Proc. of the Work-
shop on Grammar Engineering Across Frame-
works (GEAF08).
[White et al2007] Michael White, Rajakrishnan Ra-
jkumar, and Scott Martin. 2007. Towards broad
coverage surface realization with CCG. In Proc.
of the Workshop on Using Corpora for NLG: Lan-
guage Generation and Machine Translation (UC-
NLG+MT).
46
Multidocument Summarization via Information Extraction 
Michael White and Tanya Korelsky 
CoGenTex, Inc. 
Ithaca, NY 
mike,tanya@cogentex.com 
Claire Cardie, Vincent Ng, David Pierce, and 
Kiri Wagstaff 
Department of Computer Science 
Cornell University, Ithaca, NY 
cardie,yung,pierce,wkiri@cs.cornell.edu 
 
ABSTRACT 
We present and evaluate the initial version of RIPTIDES, a 
system that combines information extraction, extraction-based 
summarization, and natural language generation to support user-
directed multidocument summarization. 
1. INTRODUCTION 
Although recent years has seen increased and successful research 
efforts in the areas of single-document summarization, 
multidocument summarization, and information extraction, very 
few investigations have explored the potential of merging 
summarization and information extraction techniques.  This paper 
presents and evaluates the initial version of RIPTIDES, a system 
that combines information extraction (IE), extraction-based 
summarization, and natural language generation to support user-
directed multidocument summarization.  (RIPTIDES stands for 
RapIdly Portable Translingual Information extraction and 
interactive multiDocumEnt Summarization.)  Following [10], we 
hypothesize that IE-supported summarization will enable the 
generation of more accurate and targeted summaries in specific 
domains than is possible with current domain-independent 
techniques.  
In the sections below, we describe the initial implementation and 
evaluation of the RIPTIDES IE-supported summarization system.  
We conclude with a brief discussion of related and ongoing work. 
2. SYSTEM DESIGN 
Figure 1 depicts the IE-supported summarization system. The 
system first requires that the user select (1) a set of documents in 
which to search for information, and (2) one or more scenario 
templates (extraction domains) to activate. The user optionally 
provides filters and preferences on the scenario template slots, 
specifying what information s/he wants to be reported in the 
summary. RIPTIDES next applies its Information Extraction 
subsystem to generate a database of extracted events for the 
selected domain and then invokes the Summarizer to generate a 
natural language summary of the extracted information subject to 
the user?s constraints. In the subsections below, we describe the 
IE system and the Summarizer in turn. 
2.1 IE System 
The domain for the initial IE-supported summarization system and 
its evaluation is natural disasters.  Very briefly, a top-level natural 
disasters scenario template contains: document-level information 
(e.g. docno, date-time); zero or more agent elements denoting 
each person, group, and organization in the text; and zero or 
more disaster elements.  Agent elements encode standard 
information for named entities (e.g. name, position, geo-political 
unit).  For the most part, disaster elements also contain standard 
event-related fields (e.g. type, number, date, time, location, 
damage sub-elements).  
The final product of the RIPTIDES system, however, is not a set 
of scenario templates, but a user-directed multidocument 
summary.  This difference in goals influences a number of 
template design issues.  First, disaster elements must distinguish 
different reports or views of the same event from multiple sources.  
As a result, the system creates a separate disaster event for each 
such account.  Disaster elements should also include the reporting 
agent, date, time, and location whenever possible.  In addition, 
damage elements (i.e. human and physical effects) are best 
grouped according to the reporting event.  Finally, a slight 
broadening of the IE task was necessary in that extracted text was 
not constrained to noun phrases.  In particular, adjectival and 
adverbial phrases that encode reporter confidence, and sentences 
and clauses denoting relief effort progress appear beneficial for 
creating informed summaries.  Figure 2 shows the scenario 
template for one of 25 texts tracking the 1998 earthquake in 
Afghanistan (TDT2 Topic 89).  The texts were also manually 
annotated for noun phrase coreference; any phrase involved in a 
coreference relation appears underlined in the running text. 
The RIPTIDES system for the most part employs a traditional IE 
architecture [4].  In addition, we use an in-house implementation 
of the TIPSTER architecture [8] to manage all linguistic 
annotations.  A preprocessor first finds sentences and tokens.  For 
syntactic analysis, we currently use the Charniak [5] parser, which 
creates Penn Treebank-style parses [9] rather than the partial 
parses used in most IE systems.  Output from the parser is 
converted automatically into TIPSTER parse and part-of-speech 
annotations, which are added to the set of linguistic annotations 
for the document.  The extraction phase of the system identifies 
domain-specific relations among relevant entities in the text.  It 
relies on Autoslog-XML, an XSLT implementation of the 
Autoslog-TS system [12], to acquire extraction patterns.  
Autoslog-XML is a weakly supervised learning system that 
requires two sets of texts for training ? one set comprises texts 
relevant to the domain of interest and the other, texts not relevant 
 
 
 
to the domain.  Based on these and a small set of extraction 
pattern templates, the system finds a ranked list of possible 
extraction patterns, which a user then annotates with the 
appropriate extraction label (e.g. victim). Once acquired, the 
patterns are applied to new documents to extract slot fillers for the 
domain.  Selectional restrictions on allowable slot fillers are 
implemented using WordNet [6] and BBN?s Identifinder [3] 
named entity component.  In the current version of the system, no 
coreference resolution is attempted; instead, we rely on a very 
simple set of heuristics to guide the creation of output templates.  
The disaster scenario templates extracted for each text are 
provided as input to the summarization component along with all 
linguistic annotations accrued in the IE phase.  No relief slots are 
included in the output at present, since there was insufficient 
annotated data to train a reliable sentence categorizer. 
2.2 The Summarizer 
In order to include relief and other potentially relevant 
information not currently found in the scenario templates, the 
Summarizer extracts selected sentences from the input articles and 
adds them to the summaries generated from the scenario 
templates.  The extracted sentences are listed under the heading 
Selected News Excerpts, as shown in the two sample summaries 
appearing in Figures 3 and 4, and discussed further in Section 
2.2.5 below. 
2.2.1 Summarization Stages 
The Summarizer produces each summary in three main stages.  In 
the first stage, the output templates are merged into an event-
oriented structure, while keeping track of source information.  The 
merge operation currently relies on simple heuristics to group 
extracted facts that are comparable; for example, during this phase 
damage reports are grouped according to whether they pertain to 
the event as a whole, or instead to damage in the same particular 
location.  Heuristics are also used in this stage to determine the 
most relevant damage reports, taking into account specificity, 
recency and news source.  Towards the same objective but using a 
more surface-oriented means, simple word-overlap clustering is 
used to group sentences from different documents into clusters 
that are likely to report similar content.  In the second stage, a 
base importance score is first assigned to each slot/sentence based 
on a combination of document position, document recency and 
group/cluster membership.  The base importance scores are then 
adjusted according to user-specified preferences and matching 
scenario
 templates
A powerful earthquake struck Afghanistan on May 
30 at 11:25? 
Damage 
VOA (06/02/1998) estimated that 5,000 were killed 
by the earthquake, whereas AP (APW, 06/02/1998) 
instead reported ? 
Relief Status 
CNN (06/02/1998): Food, water, medicine 
and other supplies have started to arrive.  
[?] 
NLG of
summary
content
selection
multi-document
template
merging
text collection
IE
System
user information
need
event-oriented
structure
event-oriented
structure with slot
importance scores
summary
Summarizer
slot   filler
slot   filler
slot   filler
slot   filler
...   
slot   filler
slot   filler
slot   filler
slot   filler
...   
slot   filler
slot   filler
slot   filler
slot   filler
...   
slot   filler
slot   filler
slot   filler
slot   filler
...   
Figure 1.  RIPTIDES System Design 
criteria.  The adjusted scores are used to select the most important 
slots/sentences to include in the summary, subject to the user-
specified word limit.  In the third and final stage, the summary is 
generated from the resulting content pool using a combination of 
top-down, schema-like text building rules and surface-oriented 
revisions.  The extracted sentences are simply listed in document 
order, grouped into blocks of adjacent sentences. 
2.2.2 Specificity of Numeric Estimates 
In order to intelligently merge and summarize scenario templates, 
we found it necessary to explicitly handle numeric estimates of 
varying specificity.  While we did find specific numbers (such as 
3,000) in some damage estimates, we also found cases with no 
number phrase at all (e.g. entire villages).  In between these 
extremes, we found vague estimates (thousands) and ranges of 
numbers (anywhere from 2,000 to 5,000).  We also found phrases 
that cannot be easily compared (more than half the region?s 
residents). 
To merge related damage information, we first calculate the 
numeric specificity of the estimate as one of the values NONE, 
VAGUE, RANGE, SPECIFIC, or INCOMPARABLE, based on the presence 
of a small set of trigger words and phrases (e.g. several, as many 
as, from ? to).  Next, we identify the most specific current 
estimates by news source, where a later estimate is considered to 
update an earlier estimate if it is at least as specific.  Finally, we 
determine two types of derived information units, namely (1) the 
minimum and maximum estimates across the news sources, and 
(2) any intermediate estimates that are lower than the maximum 
estimate.1   
In the content determination stage, scores are assigned to the 
derived information units based on the maximum score of the 
underlying units.  In the summary generation stage, a handful of 
text planning rules are used to organize the text for these derived 
units, highlighting agreement and disagreement across sources. 
2.2.3 Improving the Coherence of Extracted 
Sentences 
In our initial attempt to include extracted sentences, we simply 
chose the top ranking sentences that would fit within the word 
limit, subject to the constraint that no more than one sentence per 
cluster could be chosen, in order to help avoid redundancy.  We 
found that this approach often yielded summaries with very poor 
coherence, as many of the included sentences were difficult to 
make sense of in isolation.   
To improve the coherence of the extracted sentences, we have 
experimented with trying to boost coherence by favoring 
sentences in the context of the highest-ranking sentences over 
those with lower ranking scores, following the hypothesis that it is 
better to cover fewer topics in more depth than to change topics 
excessively.  In particular, we assign a score to a set of sentences 
by summing the base scores plus increasing coherence boosts for 
adjacent sentences, sentences that precede ones with an initial 
                                                                
1
 Less specific estimates such as ?hundreds? are considered lower 
than more specific numbers such as ?5000? when they are lower 
by more than a factor of 10. 
 Document no.: ABC19980530.1830.0342  
Date/time: 05/30/1998 18:35:42.49  
Disaster Type: earthquake  
?location: Afghanistan  
?date: today  
?magnitude: 6.9  
?magnitude-confidence: high  
?epicenter: a remote part of the country  
?damage:  
               human-effect:  
                   victim: Thousands of people  
                   number: Thousands  
                  outcome: dead  
                  confidence: medium  
                  confidence-marker: feared  
               physical-effect:  
                  object: entire villages  
                  outcome: damaged  
                  confidence: medium  
                  confidence-marker: Details now hard to 
                                                come by / reports say  
PAKISTAN MAY BE PREPARING 
FOR ANOTHER TEST  
Thousands of people are feared dead following... (voice-
over) ...a powerful earthquake that hit Afghanistan today. 
The quake registered 6.9 on the Richter scale, centered in 
a remote part of the country. (on camera) Details now 
hard to come by, but reports say entire villages were 
buried by the quake.  
 
Figure 2.  Example scenario template for the natural disasters domain 
Earthquake strikes quake-devastated villages in 
northern Afghanistan 
A earthquake struck quake-devastated villages in northern 
Afghanistan Saturday. The earthquake had a magnitude of 6.9 
on the Richter scale on the Richter scale. 
Damage 
Estimates of the death toll varied. CNN (06/02/1998) provided 
the highest estimate of 4,000 dead, whereas ABC 
(06/01/1998) gave the lowest estimate of 140 dead. 
In capital: Estimates of the number injured varied. 
Selected News Excerpts 
CNN (06/01/98):  
Thousands are dead and thousands more are still missing. Red 
cross officials say the first priority is the injured. Getting 
medicine to them is difficult due to the remoteness of the 
villages affected by the quake.  
PRI (06/01/98):  
We spoke to the head of the international red cross there, Bob 
McCaro on a satellite phone link. He says it?s difficult to 
know the full extent of the damage because the region is so 
remote. There?s very little infrastructure.  
PRI (06/01/98):  
Bob McCaro is the head of the international red cross in the 
neighboring country of Pakistan. He?s been speaking to us 
from there on the line.  
APW (06/02/98):  
The United Nations, the Red Cross and other agencies have 
three borrowed helicopters to deliver medical aid.  
Figure 4.  200 word summary of actual IE output, with 
emphasis on Red Cross 
pronoun, and sentences that preceded ones with strongly 
connecting discourse markers such as however, nevertheless, etc.  
We have also softened the constraint on multiple sampling from 
the same cluster, making use of a redundancy penalty in such 
cases.  We then perform a randomized local search for a good set 
of sentences according to these scoring criteria.  
2.2.4 Implementation 
The Summarizer is implemented using the Apache 
implementation of XSLT [1] and CoGenTex?s Exemplars 
Framework [13].  The Apache XSLT implementation has 
provided a convenient way to rapidly develop a prototype 
implementation of the first two processing stages using a series of 
XML transformations.  In the first step of the third summary 
generation stage, the text building component of the Exemplars 
Framework constructs a ?rough draft? of the summary text.  In 
this rough draft version, XML markup is used to partially encode 
the rhetorical, referential, semantic and morpho-syntactic structure 
of the text.  In the second generation step, the Exemplars text 
polishing component makes use of this markup to trigger surface-
Earthquake strikes Afghanistan 
A powerful earthquake struck Afghanistan last Saturday at 
11:25. The earthquake was centered in a remote part of the 
country and had a magnitude of 6.9 on the Richter scale. 
Damage 
Estimates of the death toll varied. VOA (06/02/1998) 
provided the highest estimate of 5,000 dead. CNN 
(05/31/1998) and CNN (06/02/1998) supplied lower estimates 
of 3,000 and up to 4,000 dead, whereas APW (06/02/1998) 
gave the lowest estimate of anywhere from 2,000 to 5,000 
dead. People were injured, while thousands more were 
missing. Thousands were homeless. 
Quake-devastated villages were damaged. Estimates of the 
number of villages destroyed varied. CNN (05/31/1998) 
provided the highest estimate of 50 destroyed, whereas VOA 
(06/04/1998) gave the lowest estimate of at least 25 destroyed. 
In Afghanistan, thousands of people were killed. 
Further Details 
Heavy after shocks shook northern afghanistan. More homes 
were destroyed. More villages were damaged. 
Landslides or mud slides hit the area. 
Another massive quake struck the same region three months 
earlier. Some 2,300 victims were injured. 
Selected News Excerpts 
ABC (05/30/98):  
PAKISTAN MAY BE PREPARING FOR ANOTHER TEST 
Thousands of people are feared dead following...  
ABC (06/01/98):  
RESCUE WORKERS CHALLENGED IN AFGHANISTAN 
There has been serious death and devastation overseas. In 
Afghanistan...  
CNN (06/02/98):  
Food, water, medicine and other supplies have started to 
arrive. But a U.N. relief coordinator says it?s a "scenario from 
hell".  
Figure 3.  200 word summary of simulated IE output, with 
emphasis on damage 
oriented revision rules that smooth the text into a more polished 
form.  A distinguishing feature of our text polishing approach is 
the use of a bootstrapping tool to partially automate the 
acquisition of application-specific revision rules from examples. 
2.2.5 Sample Summaries 
Figures 3 and 4 show two sample summaries that were included in 
our evaluation (see Section 3 for details).  The summary in Figure 
3 was generated from simulated output of the IE system, with 
preference given to damage information; the summary in Figure 4 
was generated from the actual output of the current IE system, 
with preference given to information including the words Red 
Cross.   
While the summary in Figure 3 does a reasonable job of reporting 
the various current estimates of the death toll, the estimates of the 
death toll shown in Figure 4 are less accurate, because the IE 
system failed to extract some reports, and the Summarizer failed 
to correctly merge others.  In particular, note that the lowest 
estimate of 140 dead attributed to ABC is actually a report about 
the number of school children killed in a particular town.  Since 
no location was given for this estimate by the IE system, the 
Summarizer?s simple heuristic for localized damaged reports ? 
namely, to consider a damage report to be localized if a location is 
given that is not in the same sentence as the initial disaster 
description ? did not work here.  The summary in Figure 3 also 
suffered from some problems with merging:  the inclusion of a 
paragraph about thousands killed in Afghanistan is due to an 
incorrect classification of this report as a localized one (owing to 
an error in sentence boundary detection), and the discussion of the 
number of villages damaged should have included a report of at 
least 80 towns or villages damaged. 
Besides the problems related to slot extraction and merging 
mentioned above, the summaries shown in Figures 3 and 4 suffer 
from relatively poor fluency.  In particular, the summaries could 
benefit from better use of descriptive terms from the original 
articles, as well as better methods of sentence combination and 
rhetorical structuring.  Nevertheless, as will be discussed further 
in Section 4, we suggest that the summaries show the potential for 
our techniques to intelligently combine information from many 
articles on the same natural disaster. 
3. EVALUATION AND INITIAL RESULTS 
To evaluate the initial version of the IE-supported summarization 
system, we used Topic 89 from the TDT2 collection ? 25 texts 
on the 1998 Afghanistan earthquake. Each document was 
annotated manually with the natural disaster scenario templates 
that comprise the desired output of the IE system. In addition, 
treebank-style syntactic structure annotations were added 
automatically using the Charniak parser.  Finally, MUC-style 
noun phrase coreference annotations were supplied manually.  All 
annotations are in XML.  The manual and automatic annotations 
were automatically merged, leading to inaccurate annotation 
extents in some cases.  
Next, the Topic 89 texts were split into a development corpus and 
a test corpus.  The development corpus was used to build the 
summarization system; the evaluation summaries were generated 
from the test corpus.  We report on three different variants of the 
RIPTIDES system here: in the first variant (RIPTIDES-SIM1), an 
earlier version of the Summarizer uses the simulated output of the 
IE system as its input, including the relief annotations; in the 
second variant (RIPTIDES-SIM2), the current version of the 
Summarizer uses the simulated output of the IE system, without 
the relief annotations; and in the third variant (RIPTIDES-IE), the 
Summarizer uses the actual output of the IE system as its input.2   
Summaries generated by the RIPTIDES variants were compared 
to a Baseline system consisting of a simple, sentence-extraction 
multidocument summarizer relying only on document position, 
recency, and word overlap clustering.  (As explained in the 
previous section, we have found that word overlap clustering 
provides a bare bones way to help determine what information is 
repeated in multiple articles, thereby indicating importance to the 
document set as a whole, as well as to help reduce redundancy in 
the resulting summaries.)  In addition, the RIPTIDES and 
Baseline system summaries were compared against the summaries 
of two human authors.  All of the summaries were graded with 
respect to content, organization, and readability on an A-F scale 
by three graduate students, all of whom were unfamiliar with this 
project.  Note that the grades for RIPTIDES-SIM1, the Baseline 
system, and the two human authors were assigned during a first 
evaluation in October, 2000, whereas the grades for RIPTIDES-
SIM2 and RIPTIDES-IE were assigned by the same graders in an 
update to this evaluation in April, 2001. 
Each system and author was asked to generate four summaries of 
different lengths and emphases: (1) a 100-word summary of the 
May 30 and May 31 articles; (2) a 400-word summary of all test 
articles, emphasizing specific, factual information; (3) a 200-word 
summary of all test articles, focusing on the damage caused by the 
quake, and excluding information about relief efforts, and (4) a 
200-word summary of all test articles, focusing on the relief 
efforts, and highlighting the Red Cross?s role in these efforts.  
The results are shown in Tables 1 and 2.  Table 1 provides the 
overall grade for each system or author averaged across all graders 
and summaries, where each assigned grade has first been 
converted to a number (with A=4.0 and F=0.0) and the average 
converted back to a letter grade.  Table 2 shows the mean and 
standard deviations of the overall, content, organization, and 
readability scores for the RIPTIDES and the Baseline systems 
averaged across all graders and summaries.  Where the differences 
vs. the Baseline system are significant according to the t-test, the 
p-values are shown. 
Given the amount of development effort that has gone into the 
system to date, we were not surprised that the RIPTIDES variants 
fared poorly when compared against the manually written 
summaries, with RIPTIDES-SIM2 receiving an average grade of 
C, vs. A- and B+ for the human authors.  Nevertheless, we were 
pleased to find that RIPTIDES-SIM2 scored a full grade ahead of 
the Baseline summarizer, which received a D, and that 
                                                                
2
 Note that since the summarizers for the second and third variants 
did not have access to the relief sentence categorizations, we 
decided to exclude from their input the two articles (one 
training, one test) classified by TDT2 Topic 89 as only 
containing brief mentions of the event of interest, as otherwise 
they would have no means of excluding the largely irrelevant 
material in these documents. 
RIPTIDES-IE managed a slightly higher grade of D+, despite the 
immature state of the IE system.  As Table 2 shows, the 
differences in the overall scores were significant for all three 
RIPTIDES variants, as were the scores for organization and 
readability, though not for content in the cases of RIPTIDES-
SIM1 and RIPTIDES-IE. 
4. RELATED AND ONGOING WORK 
The RIPTIDES system is most similar to the SUMMONS system 
of Radev and McKeown [10], which summarized the results of 
MUC-4 IE systems in the terrorism domain.  As a pioneering 
effort, the SUMMONS system was the first to suggest the 
potential of combining IE with NLG in a summarization system, 
though no evaluation was performed.  In comparison to 
SUMMONS, RIPTIDES appears to be designed to more 
completely summarize larger input document sets, since it focuses 
more on finding the most relevant current information, and since 
it includes extracted sentences to round out the summaries.  
Another important difference is that SUMMONS sidestepped the 
problem of comparing reported numbers of varying specificity 
(e.g. several thousand vs. anywhere from 2000 to 5000 vs. up to 
4000 vs. 5000), whereas we have implemented rules for doing so.  
Finally, we have begun to address some of the difficult issues that 
arise in merging information from multiple documents into a 
coherent event-oriented view, though considerable challenges 
remain to be addressed in this area. 
The sentence extraction part of the RIPTIDES system is similar to 
the domain-independent multidocument summarizers of Goldstein 
et al [7] and Radev et al [11] in the way it clusters sentences 
across documents to help determine which sentences are central to 
the collection, as well as to reduce redundancy amongst sentences 
included in the summary.  It is simpler than these systems insofar 
as it does not make use of comparisons to the centroid of the 
document set.  As pointed out in [2], it is difficult in general for 
multidocument summarizers to produce coherent summaries, 
since it is less straightforward to rely on the order of sentences in 
the underlying documents than in the case of single-document 
summarization.  Having also noted this problem, we have focused 
our efforts in this area on attempting to balance coherence and 
informativeness in selecting sets of sentences to include in the 
summary. 
In ongoing work, we are investigating techniques for improving 
merging accuracy and summary fluency in the context of 
summarizing the more than 150 news articles we have collected 
from the web about each of the recent earthquakes in Central 
America and India (January, 2001).  We also plan to investigate 
using tables and hypertext drill-down as a means to help the user 
verify the accuracy of the summarized information. 
By perusing the web collections mentioned above, we can see that 
trying to manually extricate the latest damage estimates from 150+ 
news articles from multiple sources on the same natural disaster 
would be very tedious.  Although estimates do usually converge, 
they often change rapidly at first, and then are gradually dropped 
from later articles, and thus simply looking at the latest article is 
not satisfactory.  While significant challenges remain, we suggest 
that our initial system development and evaluation shows that our 
approach has the potential to accurately summarize damage 
estimates, as well as identify other key story items using shallower 
techniques, and thereby help alleviate information overload in 
specific domains. 
5. ACKNOWLEDGMENTS 
We thank Daryl McCullough for implementing the coherence 
boosting randomized local search, and we thank Ted Caldwell, 
Daryl McCullough, Corien Bakermans, Elizabeth Conrey, 
Purnima Menon and Betsy Vick for their participation as authors 
and graders.  This work has been partially supported by DARPA 
TIDES contract no. N66001-00-C-8009. 
6. REFERENCES 
[1] The Apache XML Project.  2001.  ?Xalan Java.?  
http://xml.apache.org/. 
Table 1 
Baseline RIPTIDES-SIM1 RIPTIDES-SIM2 RIPTIDES-IE Person 1 Person 2 
D C/C- C D+ A- B+ 
 
 
Table 2 
 Baseline RIPTIDES-SIM1 RIPTIDES-SIM2 RIPTIDES-IE 
Overall 0.96 +/- 0.37 1.86 +/- 0.56 (p=.005) 2.1 +/- 0.59 (p=.005) 1.21 +/- 0.46 (p=.05) 
Content 1.44 +/- 1.0 1.78 +/- 0.68 2.2 +/- 0.65 (p=.005) 1.18 +/- 0.6 
Organization 0.64 +/- 0.46 2.48 +/- 0.56 (p=.005) 2.08 +/- 0.77 (p=.005) 1.08 +/- 0.65 (p=.05) 
Readability 0.75 +/- 0.6 1.58 +/- 0.61 (p=.005) 2.05 +/- 0.65 (p=.005) 1.18 +/- 0.62 (p=.05) 
 
[2] Barzilay, R., Elhadad, N. and McKeown, K.  2001.  
?Sentence Ordering in Multidocument Summarization.?  In 
Proceedings of HLT 2001. 
[3] Bikel, D., Schwartz, R. and Weischedel, R.  1999.  ?An 
Algorithm that Learns What's in a Name.?  Machine 
Learning 34:1-3, 211-231. 
[4] Cardie, C. 1997. ?Empirical Methods in Information 
Extraction.?  AI Magazine 18(4): 65-79. 
[5] Charniak, E.  1999.  ?A maximum-entropy-inspired parser.?  
Brown University Technical Report CS99-12. 
[6] Fellbaum, C.  1998.  WordNet: An Electronic Lexical 
Database.  MIT Press, Cambridge, MA. 
[7] Goldstein, J., Mittal, V., Carbonell, J. and Kantrowitz, M.  
2000.  ?Multi-document summarization by sentence 
extraction.?  In Proceedings of the ANLP/NAACL Workshop 
on Automatic Summarization, Seattle, WA. 
[8] Grishman, R.  1996.  ?TIPSTER Architecture Design 
Document Version 2.2.?  DARPA, available at 
http://www.tipster.org/. 
[9] Marcus, M., Marcinkiewicz, M. and Santorini, B.  1993.  
?Building a Large, Annotated Corpus of English: The Penn 
Treebank.?  Computational Linguistics 19:2, 313-330. 
[10] Radev, D. R. and McKeown, K. R.  1998. ?Generating 
natural language summaries from multiple on-line sources.?  
Computational Linguistics 24(3):469-500. 
[11] Radev, D. R., Jing, H. and Budzikowska, M.  2000.  
?Summarization of multiple documents: clustering, sentence 
extraction, and evaluation.?  In Proceedings of the 
ANLP/NAACL Workshop on Summarization, Seattle, WA. 
[12] Riloff, E.  1996.  ?Automatically Generating Extraction 
Patterns from Untagged Text.?  In Proceedings of the 
Thirteenth National Conference on Artificial Intelligence, 
Portland, OR, 1044-1049.  AAAI Press / MIT Press. 
[13] White, M. and Caldwell, T.  1998.  ?EXEMPLARS: A 
Practical, Extensible Framework for Dynamic Text 
Generation.?  In Proceedings of the Ninth International 
Workshop on Natural Language Generation, Niagara-on-
the-Lake, Canada, 266-275. 
 
 
Towards Translingual Information Access 
using Portable Information Extraction 
Michael White, Claire Cardie, Chung-hye Han, Nari Kim, # 
Benoit Lavoie, Martha Palmer, Owen Rainbow,* Juntae Yoon 
CoGenTex, Inc. 
Ithaca, NY, USA 
\[mike,benoit.owen\] 
@cogentex.com 
Institute for Research in 
Cognitive Science 
University of Pennsylvania 
Philadelphia, PA, USA 
chunghye@babel, ling. upenn, edu 
\[ nari, mpalmer, j tyoon } 
@linc. cis.upenn.edu 
Dept. of Computer Science 
Cornell University 
Ithaca, NY, USA 
cardie@cs, cornell, edu 
Abstract 
We report on a small study undertaken to 
demonstrate the feasibility of combining 
portable information extraction with MT in 
order to support translingual information 
access. After describing the proposed 
system's usage scenario and system design, 
we describe our investigation of transferring 
information extraction techniques developed 
for English to Korean. We conclude with a 
brief discussion of related MT issues we plan 
to investigate in future work. 
1 Introduction 
In this paper, we report on a small study 
undertaken to demonstrate the feasibility of 
combining portable information extraction with 
MT in order to support ranslingual information 
access. The goal of our proposed system is to 
better enable analysts to perform information 
filtering tasks on foreign language documents. 
This effort was funded by a SBIR Phase I award 
from the U.S. Army Research Lab, and will be 
pursued further under the DARPA TIDES 
initiative. 
Information extraction (IE) systems are 
designed to extract specific types of information 
from natural language texts. In order to achieve 
acceptable accuracy, IE systems need to be 
tuned for a given topic domain. Since this 
domain tuning can be labor intensive, recent IE 
research has focused on developing learning 
algorithms for training IE system components 
(cf. Cardie, 1997, for a survey). To date, 
however, little work has been done on IE 
systems for languages other than English 
(though cf. MUC-5, 1994, and MUC-7, 1998, 
for Japanese IE systems); and, to our knowledge, 
none of the available techniques for the core task 
of learning information extraction patterns have 
been extended or evaluated for multilingual 
information extraction (though again cf. MUC-7, 
1998, where the use of learning techniques for 
the IE subtasks of named entity recognition and 
coreference r solution are described). 
Given this situation, the primary objective of 
our study was to demonstrate he feasibility of 
using portable--i.e., easily trainable--IE 
technology on Korean documents, focusing on 
techniques for learning information extraction 
patterns. Secondary objectives of the study were 
to elaborate the analyst scenario and system 
design. 
2 Analyst Scenario 
Figure 1 illustrates how an intelligence analyst 
might use the proposed system: 
? The analyst selects one or more Korean 
documents in which to search for 
information (this step not shown). 
# Current affiliation: Konan Technology, Inc., Korea, nari@konantech.co.kr 
* Current affiliation: A'IT Labs-Research, Florham Park, NJ, USA, rambow@research.att.com 
31 
Ouery  
Find Report 
Event: Nest !!lg ........... 
sourcn:l . . . . . .  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  ~ ~ 
sate :  I ................. ' ......................... ' ................. i 
Locat Ion: I~u.'~h K..e~.e..a.; ..................................... j I~ 
Part clpant : I .................................................................. i 
Iseun:i~'North Korea" AND "missiles" i 
I 
Response  to  Ouery  
The reports Indicate 2 meetings held In South Korea on the 
issues of North Korea anti missiles: 
Sources Translated Extracts 
Joon,ap~l A ~ ~  
. . . .  I ,4 ~t ln ,  g ~# ,#~=1# o,1 Apf J l  ~ sLYout tP~ I10t 
,!nerF, orea j /ine? ~t~wn Saoul end Tokyo for  the 
Noes / - -  t~Q I ela~gen?? d~/tu~tlons ~uc/t eg Alottl I  Kofgm'~ 
Trans la t ion  o f  Korean  Source  Repor t  
\[Joongang Dally\] 
Korean. Japanese H in i s ters  Discuss NK Po l i cy  
The tmo ministers ~9rsed that any further launching of a 
missile by North Korean would undermine the security of 
~Northeast Asia and the Korea, the United States and Japan 
should take Joint steps against the North Korean missile 
threat. 
}-long requested that Koeura cork to normalize Japan's 
relations with North Korea. rather than cutting channels 
of dialogue bet#men the two countries. 
Koeura said that i f  North Korea continues Its missile 
testing, the Japanese government will definitely stop 
making contributions to KEDO. 
The tee ministers also tentatively agreed that J~anese 
primo minister Kslzo Obuchl should make a state visit  to 
Korea on or around Nerch 20. 
Korean  Source  Repor t  
E t -~ "~I -D lX i '~  ~oo ~ Cll~o" 
oj_a, xd~. ~ ~.\]Ol D IXF~ ~F ~,~FI,,t ~'-9-, ~.~OF ~t~l.~. ~t l  
ud~Otl ~l . : , r t}  ~\]l~i/ ~ol~.-E.II .?-INto ?,,toiSF.~. ~t.-Ol-~ 8-.~01 
~XlI~II= = ~ZISH LDFPI~_ ~C.F~ uH~C3 ~-~-  ~.1-~..~ OF-..It~ 
~01 ~cF.  
x~.~ ~.~OI ~l,.Lt~ EH~=  ~S lO I  ...~CI.~ ~.~_o~ ~It~,/~F 
a~_tOI LO~O KILL= ~0~OPj ~-~/~1 )H~F ~dXl~ 8~9F 
Figure 1 
The analyst selects one or more scenario 
template, to activate in the query. Each 
scenario template corresponds to a specific 
type of event. Available scenario templates 
might include troop movements, acts of 
violence, meetings and negotiathms, 
protests, etc. In Figure 1, the selected event 
is of type meeting (understood broadly). 
The analyst fills in the available slots of the 
selected scenario template in order to restrict 
the search to the information considered to 
be relevant. In Figure 1, the values specified 
in the scenario template indicate that the 
information to f'md is about meetings having 
as location South Korea and as issue North 
Korea and missiles. The analyst also 
32  
specifies what information s/he wants to be 
reported when information matching the 
query is found. In Figure 1, the selected 
boxes under the Report column indicate that 
all information found satisfying the query 
should be reported except for the meeting 
participants. 1 
? Once the analyst submits the query for 
evaluation, the system searches the input 
documents for information matching the 
query. As a result, a hypertext document is 
generated describing the information 
matching the query as well as the source of 
this information. Note that the query 
contains English keywords that are 
automatically translated into Korean prior to 
matching. The extracted information is 
presented in English after being translated 
from Korean. In Figure 1, the generated 
hypertext response indicates two documents 
in the input set that matched the query 
totally or in part. Each summary in the 
response includes just the translations of the 
extracted information that the analyst 
requested to be reported. 
? For each document extract matching the 
analyst query, the analyst can obtain a 
complete machine translation of the Korean 
document where the match was found, and 
where the matched information is 
highlighted. Working with a human 
translator, the analyst can also verify the 
accuracy of the reported information by 
accessing the documents in their original 
language. 
3 System Design 
Figure 2 shows the high-level design of the 
system. It consists of the following components: 
? The User Interface. The browser-based 
interface is for entering queries and 
displaying the resulting presentations. 
? The Portable Information Extractor (PIE) 
component. The PIE component uses the 
While in this example the exclusion of participant 
information in the resulting report is rather artificial, 
in general a scenario template may contain many 
different ypes of information, not all of which are 
likely to interest an analyst at once. 
Extraction Pattem Library - -  which 
contains the set of extraction patterns 
learned in the lab, one set per scenario 
template - -  to extract specific types of 
information from the input Korean 
documents, once parsed. 
? The Ranker component. This component 
ranks the extracted information returned by 
the PIE component according to how well it 
matches the keyword restrictions in the 
query. The MT component's English-to- 
Korean Transfer Lexicon is used to map the 
English keywords to corresponding Korean 
ones. When the match falls below a user- 
? configurable threshold, the extracted 
information is filtered out. 
? The MT component. The MT component 
(cf. Lavoie et al, 2000) translates the 
extracted Korean phrases or sentences into 
corresponding English ones. 
? The Presentation Generator component. 
This component generates well-organized, 
easy-to-read hypertext presentations by 
organizing and formatting the ranked 
extracted information. It uses existing NLG 
components, including the Exemplars text 
planning framework (White and Caldwell, 
1998) and the RealPro syntactic realizer 
(Lavoie and Rainbow, 1997). 
In our feasibility study, the majority of the effort 
went towards developing the PIE component, 
described in the next section. This component 
was implemented in a general way, i.e. in a way 
that we would expect to work beyond the 
specific training/test corpus described below. In 
contrast, we only implemented initial versions of 
the User Interface, Ranker and Presentation 
Generator components, in order to demonstrate 
the system concept; that is, these initial versions 
were only intended.to work with our training/test 
corpus, and will require considerable further 
development prior to reaching operational status. 
For the MT component, we used an early 
version of the lexical transfer-based system 
currently under development in an ongoing 
SBIR Phase II project (cf. Nasr et al, 1997; 
Palmer et al, 1998; Lavoie et al, 2000), though 
with a limited lexicon specifically for translating 
the slot fillers in our training/test corpus. 
33 
Korean Documents 
Parser 
Tagged l 
Korean Documents ( LexiconK?rean 1
~ Syntactic . . . . . .  Eaglish Grammar Structure (English) RealPro 
English Lexicon / ' S~'ntactic Realizer Sentence (English) 
t Parsed Document ~ ::i~i?~'~vii~i? ' .~:Qi~I~:i~-'-iL \[:!::ili:::.:: ~t r~.  :::::::::::::::::::::::: 
Extracted Information \[ 
(Korean) 
Ordered Extracted 
Information(Korean) 
Parsed Document \] Machine "lYanslation I ( 
~l Component (MT) 
Ordered Extracted 
Information (English) 
User Input Data Presentation (E glish) 
Information Extraction 
Query (English) 1 
i : rla0 Inf0rntauonl 
English-Korean 7 
Transfer Lexicon J
Korean-English 
Transfer Lexicon ) 
T 
Miiiiii ii 
? 
Presentation (English) 
End user Document Processing Knowledge base 
component component 
D (C)OTS component 
\[\]Component created in Phase I 
\[\]Component created or improved in Phase II 
Figure 2 
4 Portable Information Extraction 
4.1 Scenario Template and Training/Fest 
Corpus 
For our Phase I feasibility demonstration, we 
chose a minimal scenario template for meeting 
and negotiation events consisting of one or more 
participant slots plus optional date and location 
slots. 2 We then gathered a small corpus of thirty 
articles by searching for articles containing 
"North Korea" and one or more of about 15 
keywords. The first two sentences (with a few 
exceptions) were then annotated with the slots to 
be extracted, leading to a total of 51 sentences 
containing 47 scenario templates and 89 total 
2 In the end, we did not use the 'issue' slot shown in 
Figure 1, as it contained more complex Idlers than 
those that ypically have been handled in IE systems. 
correct slots. Note that in a couple of cases 
more than one template was given for a single 
long sentence. 
When compared to the MUC scenario 
template task, our extraction task was 
considerably simpler, for the following reasons: 
* The answer keys only contained information 
that could be found within a single sentence, 
i.e. the answer keys did not require merging 
information across entences. 
? The answer keys did not require anaphoric 
references to be resolved, and we did not 
deal with conjuncts eparately. 
? We did not attempt o normalize dates or 
remove appositives from NPs. 
4.2 Extraction Pattern Learning 
For our feasibility study, we chose to follow the 
AutoSlog (Lehnert et al, 1992; Riloff, 1993) 
approach to extraction pattern acquisition. In 
this approach, extraction patterns are acquired 
34 
i. E: 
K: 
<target-np>=<subject> <active voice verb> 
<participant> MET 
<target-np>=<subject> <active voice verb> 
<John-i> MANNASSTA 
<John-nom>'MET 
2. E: 
K: 
<target-np>=<subject> <verb> <infinitive> 
<participant> agreed to MEET 
<target-np>=<subject> <verbl-ki- lo> <verb2> 
<John-un> MANNA-ki- lo hapuyhayssta 
<John-nom> MEET-ki- lo agreed 
(-ki: nominalization ending, -io: an adverbial postposition) 
Figure 3 
via a one-shot general-to-specific learning 
algorithm designed specifically for the 
information extraction task. 3 The learning 
algorithm is straightforward and depends only 
on the existence of a (partial) parser and a small 
set of general inguistic patterns that direct the 
creation of specific patterns. As a training 
corpus, it requires a set of texts with noun 
phrases annotated with the slot type to be 
extracted. 
To adapt the AutoSlog approach to Korean, 
we first devised Korean equivalents of the 
English patterns, two of which are shown in 
Figure 3. It turned out that for our corpus, we 
could collapse some of these patterns, though 
some new ones were also needed. In the end we 
used just nine generic patterns. 
Important issues that arose in adapting the 
approach were (1) greater flexibility in word 
order and heavier reliance on morphological 
cues in Korean, and (2) the predominance of 
light verbs (verbs with little semantic ontent of 
their own) and aspectual verbs in the chosen 
domain. We discuss these issues in the next two 
sections. 
4.3 Korean Parser 
We used Yoon's hybrid statistical Korean parser 
(Yoon et al, 1997, 1999; Yoon, 1999) to process 
the input sentences prior to extraction. The 
parser incorporates a POS tagger and 
3 For TIDES, we plan to use more sophisticated 
learning algorithms, as well as active learning 
techniques, such as those described in Thompson et 
al. (1999). 
morphological nalyzer and yields a dependency 
representation as its output? The use of a 
dependency representation e abled us to handle 
the greater flexibility in word order in Korean. 
To facilitate pattern matching, we wrote a 
simple program to convert he parser's output o 
XML form. During the XML conversion, two 
simple heuristics were applied, one to recover 
implicit subjects, and another to correct a 
recurring misanalysis of noun compounds. 
4.4 Trigger Word Filtering and 
Generalization 
In the newswire corpus we looked at, meeting 
events were rarely described with the verb 
'mannata' ('to meet'). Instead, they were 
usually described with a noun that stands for 
'meeting' and a light or aspectual verb, for 
example, 'hoyuy-lul kacta' ('to have a meeting') 
or 'hoyuy-lul machita' ('to finish a meeting'). 
In order to acquire extraction patterns that made 
appropriate use of such collocations, we decided 
to go beyond the AutoSlog approach and 
explicitly group trigger words (such as 'hoyuy') 
into classes, and to likewise group any 
collocations, such as those involving light verbs 
or aspectual verbs. To fmd collocations for the 
trigger words, we reviewed a Korean lexical co- 
occurrence base which was constructed from a 
corpus of 40 million words (Yoon et al, 1997). 
We then used the resulting specification to filter 
the learned patterns to just those containing the 
4 Overall dependency precision is reported to be 
89.4% (Yoon, 1999). 
35 
. - !  
trigger words or trigger word collocations, as 
well as to generalize the patterns to the word 
class level. Because the number of tr:igger 
words is small, this specification can be done 
quickly, and soon pays off in terms of time 
saved in manually filtering the learned patterns. 
4.5 Results 
In testing our approach, we obtained overall 
results of 79% recall and 67% precision in a 
hold-one-out cross validation test. In a cross 
validation test, one repeatedly divides a corpus 
into different raining and test sets, averaging the 
results; in the hold-one-out version, the system 
is tested on a held-out example after being 
trained on the rest. In the IE setting, the recall 
measure is the number of correct slots found 
divided by the total number of correct slots, 
while the precision measure is the number of 
correct slots found divided by the total number 
of slots found. 
While direct comparisons with the MUC 
conference results cannot be made for the 
reasons we gave above, we nevertheless 
consider these results quite promising, as these 
scores exceed the best scores reported at MUC-6 
on the scenario template task. 5 
Table 1: Hold-One-Out Cross Validation 
Slots Recall Precision 
All 79% 67% 
Participant 75% 84% 
Date/Location 86% 54% 
Table2: Hold-One-OutCross Validat~n 
wi~outGeneralizafion 
Slots Recall Precision 
All 61% 64% 
Participant 57% 81% 
Date/Location 67% 52% 
A breakdown by slot is shown in Table 1. We 
may note that precision is low for date and 
location slots because we used a simplistic 
sentence-level merge, rather than dependencies. 
To measure the impact of our approach to 
generalization, we may compare the results in 
5 
http://www.nist.gov/itl/div894/894.02/related_project 
s/tipster/muc.htm 
Table 1 with those shown in Table 2, where 
generalization is not used. As can be seen, the 
generalization step adds substantially to overall 
recall. 
To illustrate the effect of generalization, 
consider the pattern to extract he subject NP of 
the light verb 'kac (hold)' when paired with an 
object NP headed by the noun 'hyepsang 
(negotiation)'. Since this pattern only occurs 
once in our corpus, the slot is not successfully 
extracted in the cross-validation test without 
generalization. However, since this example 
does fall under the more generalized pattern of 
extracting the subject NP of a verb in the light 
verb class when paired with an object NP 
headed by a noun the 'hoytam-hyepsang' class, 
the slot is successfully extracted in the cross- 
validation test using the generalized patterns. 
Cases like these are the source of the 18% boost 
in recall of participant slots, from 57% to 75%. 
5 Discussion 
Our feasibility study has focused our attention 
on several questions concerning the interaction 
of IE and MT, which we hope to pursue under 
the DARPA TIDES initiative. One question is 
the extent o which slot filler translation is more 
practicable than general-purpose MT; one would 
expect to achieve much higher quality on slot 
fillers, as they are typically relatively brief noun 
phrases, and instantiation of a slot implies a 
degree of semantic lassification. On the other 
hand, one might find that higher quality is 
required in order to take translated phrases out 
of their original context. Another question is 
how to automate the construction of bilingual 
lexicons. An important issue here will be how 
to combine information from different sources, 
given that automatically acquired lexical 
information is apt to be less reliable, though 
domain-specific. 
Acknowledgements 
Our thanks go to Richard Kittredge and Tanya 
Korelsky for helpful comments and advice. This 
work was supported by ARL contract DAAD 17- 
99-C-0005. 
36 
References 
Cardie, C. (1997). Empirical Methods in Information 
Extraction. AI Magazine 18(4):65-79. 
Lavoie, B. and Rambow, O. (1997). RealPro - -  A 
fast, portable sentence realizer. In Proceedings of 
the Conference on Applied Natural Language 
Processing (ANLP'97), Washington, DC. 
Lavoie, B., Korelsky, T., and Rambow, O. (2000). A 
Framework for MT and Multilingual NLG Systems 
Based on Uniform Lexico-Structural Processing. 
To appear in Proceedings of the Sixth Conference 
on Applied Natural Language Processing (ANLP- 
2000), Seattle, WA. 
Lehnert, W., Cardie, C., Fisher, D., McCarthy, J., 
Riloff, E., and Soderland, S. (1992). University of 
Massachusetts: Description of the CIRCUS system 
as used in MUC-4. In Proceedings of the Fourth 
Message Understanding Conference (MUC-4), 
pages 282-288, San Mateo, CA. Morgan 
Kaufmann. 
MUC-5 (1994). Proceedings of the Fifth Message 
Understanding Conference (MUC-5). Morgan 
Kaufmann, San Mateo, CA. 
MUC-7 (1998). Proceedings of the Seventh Message 
Understanding Conference (MUC-7). Morgan 
Kaufmann, San Francisco, CA. 
Nasr, A., Rambow, O., Palmer, M., and Rosenzweig, 
J. (1997). Enriching lexical transfer with cross- 
linguistic semantic features. In Proceedings of the 
lnterlingua Workshop at the MT Summit, San 
Diego, CA. 
Palmer, M., Rambow, O., and Nasr, A. (1998). 
Rapid prototyping of domain-specific machine 
translation systems. In Machine Translation and 
the Information Soup - Proceedings of the Third 
Conference of the Association for Machine 
Translation in the Americas AMTA'98, Springer 
Verlag (Lecture Notes in Artificial Intelligence No. 
1529), Berlin. 
Riloff, E. (1993). Automatically constructing a
dictionary for information exlxaction tasks. In 
Proceedings of the Eleventh National Conference 
on Artificial Intelligence, pages 811-816, 
Washington, DC. AAAI Press / MIT Press. 
Thompson, C. A., Califf, M. E., and Mooney, R. J. 
(1999). Active learning for natural language 
parsing and information extraction. In Proceedings 
of the Sixteenth International Machine Learning 
Conference (1CML-99), Bled, Slovenia. 
White, M. and Caldwell, T. (1998). EXEMPLARS: A 
practical, extensible framework for dynamic text 
generation. In Proceedings of the 8th International 
Workshop on Natural Language Generation, 
Niagara-on-the-Lake, Ontario. 
Yoon, J. (1999). Efficient dependency parsing based 
on three types of chunking and lexical association. 
Submitted. 
Yoon, J., Choi, K.-S., and Song, M. (1999). Three 
types of chunking in Korean and dependency 
analysis based on lexical association. In 
Proceedings of lCCPOL. 
Yoon, J., Kim, S., and Song, M. (1997). New parsing 
method using global association table. In 
Proceedings of the 5th International Workshop on 
Parsing Technology. 
37 
Inducing Lexico-Structural Transfer Rules from Parsed Bi-texts
Benoit Lavoie, Michael White, and Tanya Korelsky
CoGenTex, Inc.
840 Hanshaw Road
Ithaca, NY 14850, USA
benoit,mike,tanya@cogentex.com
Abstract
This paper describes a novel approach
to inducing lexico-structural transfer
rules from parsed bi-texts using syn-
tactic pattern matching, statistical co-
occurrence and error-driven filtering.
We present initial evaluation results and
discuss future directions.
1 Introduction
This paper describes a novel approach to inducing
transfer rules from syntactic parses of bi-texts and
available bilingual dictionaries. The approach
consists of inducing transfer rules using the four
major steps described in more detail below: (i)
aligning the nodes of the parses; (ii) generating
candidate rules from these alignments; (iii) order-
ing candidate rules by co- occurrence; and (iv) ap-
plying error-driven filtering to select the final set
of rules.
Our approach is based on lexico-structural
transfer (Nasr et. al., 1997), and extends recent
work reported in (Han et al, 2000) about Korean
to English transfer in particular. Whereas Han et
al. focus on high quality domain-specific transla-
tion using handcrafted transfer rules, in this work
we instead focus on automating the acquisition of
such rules.
Our approach can be considered a generaliza-
tion of syntactic approaches to example-based
machine translation (EBMT) such as (Nagao,
1984; Sato and Nagao, 1990; Maruyama and
Watanabe, 1992). While such approaches use
syntactic transfer examples during the actual
transfer of source parses, our approach instead
uses syntactic transfer examples to induce general
transfer rules that can be compiled into a transfer
dictionary for use in the actual translation process.
Our approach is similar to the recent work of
(Meyers et al, 1998) where transfer rules are also
derived after aligning the source and target nodes
of corresponding parses. However, it also differs
from (Meyers et al, 1998) in several important
points. The first difference concerns the content
of parses and the resulting transfer rules; in (Mey-
ers et al, 1998), parses contain only lexical labels
and syntactic roles (as arc labels), while our ap-
proach uses parses containing lexical labels, syn-
tactic roles, and any other syntactic information
provided by parsers (tense, number, person, etc.).
The second difference concerns the node align-
ment; in (Meyers et al, 1998), the alignment of
source and target nodes is designed in a way that
preserves node dominancy in the source and tar-
get parses, while our approach does not have such
restriction. One of the reasons for this difference
is due to the different language pairs under study;
(Meyers et al, 1998) deals with two languages
that are closely related syntactically (Spanish and
English) while we are dealing with languages that
syntactically are quite divergent, Korean and En-
glish (Dorr, 1994). The third difference is in the
process of identification of transfer rules candi-
dates; in (Meyers et al, 1998), the identification
is done by using the exact tree fragments in the
source and target parse that are delimited by the
alignment, while we use all source and target tree
sub-patterns matching a subset of the parse fea-
tures that satisfy a customizable set of alignment
constraints and attribute constraints. The fourth
third difference is in the level of abstraction of
transfer rules candidates; in (Meyers et al, 1998),
the source and target patterns of each transfer rule
are fully lexicalized (except possibly the terminal
nodes), while in our approach the nodes of trans-
fer rules do not have to be lexicalized.
Section 2 describes our approach to trans-
fer rules induction and its integration with data
preparation and evaluation. Section 3 describes
the data preparation process and resulting data.
Section 4 describes the transfer induction process
in detail. Section 5 describes the results of our ini-
tial evaluation. Finally, Section 6 concludes with
a discussion of future directions.
2 Overall Approach
In its most general form, our approach to transfer
rules induction includes three different processes,
data preparation, transfer rule induction and eval-
uation. An overview of each process is provided
below; further details are provided in subsequent
sections.
The data preparation process creates the fol-
lowing resources from the bi-texts:
? A training set and a test set of source and
target parses for the bi-texts, post-processed
into a syntactic dependency representation.
? A baseline transfer dictionary, which may in-
clude (depending upon availability) lexical
transfer rules extracted from the bi-texts us-
ing statistical methods, lexical transfer rules
from existing bilingual dictionaries, and/or
handcrafted lexico-structural transfer rules.
The transfer induction process induces lexico-
structural transfer rules from the training set of
corresponding source and target parses that, when
added to the baseline transfer dictionary, produce
transferred parses that are closer to the corre-
sponding target parses. The transfer induction
process has the following steps:
? Nodes of the corresponding source and tar-
get parses are aligned using the baseline
transfer dictionary and some heuristics based
on the similarity of part-of-speech and syn-
tactic context.
? Transfer rule candidates are generated based
on the sub-patterns that contain the corre-
sponding aligned nodes in the source and tar-
get parses.
? The transfer rule candidates are ordered
based on their likelihood ratios.
? The transfer rule candidates are filtered, one
at a time, in the order of the likelihood ra-
tios, by removing those rule candidates that
do not produce an overall improvement in
the accuracy of the transferred parses.
The evaluation process has the following steps:
? Both the baseline transfer dictionary and the
induced transfer dictionary (i.e., the baseline
transfer dictionary augmented with the in-
duced transfer rules) are applied to the test
set in order to produce two sets of transferred
parses, the baseline set and the (hopefully)
improved induced set. For each set, the dif-
ferences between the transferred parses and
target parses are measured, and the improve-
ment in tree accuracy is calculated.
? After performing syntactic realization on the
baseline set and the induced set of trans-
ferred parses, the differences between the
resulting translated strings and the target
strings are measured, and the improvement
in string accuracy is calculated.
? For a subset of the translated strings, human
judgments of accuracy and grammaticality
are gathered, and the correlations between
the manual and automatic scores are calcu-
lated, in order to assess the meaningfulness
of the automatic measures.
3 Data Preparation
3.1 Parsing the Bi-texts
In our experiments to date, we have used a cor-
pus consisting of a Korean dialog of 4183 sen-
tences and their English human translations. We
ran off-the-shelf parsers on each half of the cor-
pus, namely the Korean parser developed by Yoon
et al (1997) and the English parser developed by
Collins (1997). Neither parser was trained on our
corpus.
We automatically converted the phrase struc-
ture output of the Collins parser into the syntac-
tic dependency representation used by our syn-
tactic realizer, RealPro (Lavoie and Rambow,
1997). This representation is based on the deep-
syntactic structures (DSyntS) of Meaning-Text
Theory (Mel?c?uk, 1988). The important features
of a DSyntS are as follows:
? a DSyntS is an unordered tree with labeled
nodes and labeled arcs;
? a DSyntS is lexicalized, meaning that the
nodes are labeled with lexemes (uninflected
words) from the target language;
? a DSyntS is a dependency structure and not a
phrase- structure structure: there are no non-
terminal nodes, and all nodes are labeled
with lexemes;
? a DSyntS is a syntactic representation,
meaning that the arcs of the tree are la-
beled with syntactic relations such as SUB-
JECT (represented in DSyntSs as I), rather
than conceptual or semantic relations such as
AGENT;
? a DSyntS is a deep syntactic representation,
meaning that only meaning-bearing lexemes
are represented, and not function words.
Since the output of the Yoon parser is quite sim-
ilar, with the exception of its treatment of syn-
tactic relations, we have used its output as is.
The DSyntS representations for two correspond-
ing Korean1 and English sentences are illustrated
in Figure 1.
In examining the outputs of the two parsers
on our corpus, we found that about half of the
parse pairs contained incorrect dependency as-
signments, incomplete lemmatization or incom-
plete parses. To reduce the impact of such pars-
ing errors in our initial experiments, we have pri-
marily focused on a higher quality subset of 1763
sentence pairs that were selected according to the
following criteria:
? Parse pairs where the source or target parse
contained more than 10 nodes were rejected,
1Korean is represented in romanized format in this paper.
(S1) {i} {Ci-To-Reul} {Ta-Si} {Po-Ra}.
this + map-accusative + again + look-imp
(D1) {po} [class=vbma ente={ra}] (
s1 {ci-to} [class=nnin2 ppca={reul}] (
s1 {i} [class=ande]
)
s1 {ta-si} [class=adco2]
)
(S2) Look at the map again.
(D2) look [class=verb mood=imp] (
attr at [class=preposition] (
ii map [class=common_noun article=def]
)
attr again [class=adverb]
)
Figure 1: Syntactic dependency representations
for corresponding Korean and English sentences
since these usually contained more parse er-
rors than smaller parses.
? Parse pairs where the source or target parse
contained non-final punctuation were re-
jected; this criterion was based on our ob-
servation that in most such cases, the source
or target parses contained only a fragment
of the original sentence content (i.e., one or
both parsers only parsed what was on one
side of an intra-sentential punctuation mark).
We divided this higher quality subset into train-
ing and test sets by randomly choosing 50% of
the 1763 higher quality parse pairs (described in
Section 3.1) for inclusion in the training set, re-
serving the remaining 50% for the test set. The
average numbers of parse nodes in the training set
and test set were respectively 6.91 and 6.11 nodes.
3.2 Creating the Baseline Transfer
Dictionary
In the general case, any available bilingual dic-
tionaries can be combined to create the base-
line transfer dictionary. These dictionaries may
include lexical transfer dictionaries extracted
from the bi-texts using statistical methods, exist-
ing bilingual dictionaries, or handcrafted lexico-
structural transfer dictionaries. If probabilistic in-
formation is not already associated with the lexi-
cal entries, log likelihood ratios can be computed
and added to these entries based on the occur-
rences of these lexical items in the parse pairs.
In our initial experiments, we decided to focus
on the scenario where the baseline transfer dic-
@KOREAN:
{po} [class=vbma] (
s1 $X [ppca={reul}]
)
@ENGLISH:
look [class=verb] (
attr at [class=preposition] (
ii $X
)
)
@-2xLOG_LIKELIHOOD: 12.77
Figure 2: Transfer rule for English lexicalization
and preposition insertion
@KOREAN:
$X [class=vbma ente={ra}]
@ENGLISH:
$X [class=verb mood=imp]
@-2xLOG_LIKELIHOOD: 33.37
Figure 3: Transfer rule for imperative forms
tionary is created from lexical transfer entries ex-
tracted from the bi-texts using statistical methods.
To simulate this scenario, we created our baseline
transfer dictionary by taking the lexico-syntactic
transfer dictionary developed by Han et al (2000)
for this corpus and removing the (more general)
rules that were not fully lexicalized. Starting with
this purely lexical baseline transfer dictionary en-
abled us to examine whether these more general
rules could be discovered through induction.
4 Transfer Rule Induction
The induced lexico-structural transfer rules are
represented in a formalism similar to the one de-
scribed in Nasr et al (1997), and extended to also
include log likelihood ratios. Figures 2 and 3
illustrate two entry samples that can be used to
transfer a Korean syntactic representation for ci-
to-reul po-ra to an English syntactic representa-
tion for look at the map. The first rule lexicalizes
the English predicate and inserts the correspond-
ing preposition while the second rule inserts the
English imperative attribute. This formalism uses
notation similar to the syntactic dependency nota-
tion shown in Figure 1, augmented with variable
arguments prefixed with $ characters.
4.1 Aligning the Parse Nodes
To align the nodes in the source and target parse
trees, we devised a new dynamic programming
alignment algorithm that performs a top-down,
bidirectional beam search for the least cost map-
ping between these nodes. The algorithm is pa-
rameterized by the costs of (1) aligning two nodes
whose lexemes are not found in the baseline trans-
fer dictionary; (2) aligning two nodes with dif-
fering parts of speech; (3) deleting or inserting a
node in the source or target tree; and (4) aligning
two nodes whose relative locations differ.
To determine an appropriate part of speech cost
measure, we first extracted a small set of parse
pairs that could be reliably aligned using lexical
matching alone, and then based the cost measure
on the co-occurrence counts of the observed parts
of speech pairings. The remaining costs were set
by hand.
As a result of the alignment process, alignment
id attributes (aid) are added to the nodes of the
parse pairs. Some nodes may be in alignment
with no other node, such as English prepositions
not found in the Korean DSyntS.
4.2 Generating Rule Candidates
Candidate transfer rules are generated using three
data sources:
? the training set of aligned source and target
parses resulting from the alignment process;
? a set of alignment constraints which identify
the subtrees of interest in the aligned source
and target parses (Section 4.2.1);
? a set of attribute constraints which determine
what parts of the aligned subtrees to include
in the transfer rule candidates? source and
target patterns (Section 4.2.2).
The alignment and attribute constraints are nec-
essary to keep the set of candidate transfer rules
manageable in size.
4.2.1 Alignment constraints
Figure 4 shows an example alignment constraint.
This constraint, which matches the structural pat-
terns of the transfer rule illustrated in Figure 2,
uses the aid alignment attribute to indicate that
@KOREAN:
$X1 [aid=$1] (
$R1 $X2 [aid=$2]
)
@ENGLISH:
$Y1 [aid=$1] (
$R2 $Y2 (
$R3 $Y3 [aid=$2]
)
)
Figure 4: Alignment constraint
in a Korean and English parse pair, any source
and target sub-trees matching this alignment con-
straint (where $X1 and $Y1 are aligned or have
the same attribute aid values and where $X2 and
$Y3 are aligned) can be used as a point of depar-
ture for generating transfer rule candidates. We
suggest that alignment constraints such as this one
can be used to define most of the possible syntac-
tic divergences between languages (Dorr, 1994),
and that only a handful of them are necessary for
two given languages (we have identified 11 gen-
eral alignment constraints necessary for Korean to
English transfer so far).
4.2.2 Attribute constraints
Attribute constraints are used to limit the space
of possible transfer rule candidates that can be
generated from the sub-trees satisfying the align-
ment constraints. Candidate transfer rules must
satisfy all of the attribute constraints. Attribute
constraints can be divided into two types:
? independent attribute constraints, whose
scope covers only one part of a candidate
transfer rule and which are the same for the
source and target parts;
? concurrent attribute constraints, whose
scope extends to both the source and target
parts of a candidate transfer rule.
The examples of an independent attribute con-
straint and of a concurrent attribute constraint are
given in Figure 5 and Figure 6 respectively. As
with the alignment constraints, we suggest that a
relatively small number of attribute constraints is
necessary to generate most of the desired rules for
a given language pair.
Each node of a candidate transfer rule must have its relation
attribute (relationship with its governor) specified if it is an
internal node, otherwise this relation must not be specified:
e.g.
 $X1 ( $R $X2 )
Figure 5: Independent attribute constraint
In a candidate transfer rule, inclusion of the lexemes of two
aligned nodes must be done concurrently:
e.g.
$X [aid=$1]
and
$Y [aid=$1]
e.g.
 [aid=$1]
and
 [aid=$1]
Figure 6: Concurrent attribute constraint
4.3 Ordering Rule Candidates
In the next step, transfer rule candidates are or-
dered as follows: first, by their log likelihood ra-
tios (Manning and Schutze, 1999: 172-175); sec-
ond, any transfer rule candidates with the same
log likelihood ratio are ordered by their speci-
ficity.
4.3.1 Rule ordering by log likelihood ratio
We calculate the log likelihood ratio, log ?, ap-
plied to a transfer rule candidate as indicated in
Figure 7. Note that log ? is a negative value,
and following (Manning and Schutze, 1999), we
assign -2 log ? to the transfer rule. Note also
that in the definitions of C1, C2, and C12 we are
currently only considering one occurrence or co-
occurrence of the source and/or target patterns per
parse pair, while in general there could be more
than one; in our initial experiments these defini-
tions have sufficed.
4.3.2 Rule ordering by specificity
If two or more candidate transfer rules have the
same log likelihood ratio, ties are broken by a
specificity heuristic, with the result that more gen-
eral rules are ordered ahead of more specific ones.
The specificity of a rule is defined to be the fol-
lowing sum: the number of attributes found in
the source and target patterns, plus 1 for each for
log ? =
logL(C12, C1, p) + logL(C2 ? C12, N ? C1, p)
? logL(C12, C1, p1)? logL(C2?C12, N ?C1, p2)
where, not counting attributes aid,
? C1 = number of source parses containing at least one
occurrence of C?s source pattern
? C2 = number of target parses containing at least one
occurrence of C?s target pattern
? C12 = number of source and target parse pairs contain-
ing at least one co-occurrence of C?s source pattern
and C?s target pattern satisfying the alignment con-
straints
? N = number of source and target parse pairs
? P = C2/N ;
? P1 = C12/C1;
? P2 = (C2 ? C12)/(N ? C1);
? L(k, n, x) = xk(1? x)n?k
Figure 7: Log likelihood ratios for transfer rule
candidates
each lexeme attribute and for each dependency re-
lationship. In our initial experiments, this simple
heuristic has been satisfactory.
4.4 Filtering Rule Candidates
Once the candidate transfer rules have been or-
dered, error-driven filtering is used to select those
that yield improvements over the baseline trans-
fer dictionary. The algorithm works as follows.
First, in the initialization step, the set of accepted
transfer rules is set to just those appearing in the
baseline transfer dictionary, and the current er-
ror rate is established by applying these transfer
rules to all the source structures and calculating
the overall difference between the resulting trans-
ferred structures and the target parses. Then, in a
single pass through the ordered list of candidates,
each transfer rule candidate is tested to see if it
reduces the error rate. During each iteration, the
candidate transfer rule is provisionally added to
the current set of accepted rules and the updated
set is applied to all the source structures. If the
overall difference between the transferred struc-
tures and the target parses is lower than the cur-
rent error rate, then the candidate is accepted and
@KOREAN:
{po} [class=vbma ente={ra}] (
s1 $X [ppca={reul}]
)
@ENGLISH:
look [class=verb mood=imp] (
attr at [class=preposition] (
ii $X
)
)
@-2xLOG_LIKELIHOOD: 11.40
Figure 8: Transfer rule for English imperative
with lexicalization and preposition insertion
the current error rate is updated; otherwise, the
candidate is rejected and removed from the cur-
rent set.
4.5 Discussion of Induced Rules
Experimentation with the training set of 882 parse
pairs described in Section 3.1 produced 12467
source and target sub-tree pairs using the align-
ment constraints, from which 20569 transfer rules
candidate were generated and 7565 were accepted
after filtering. We expect that the number of
accepted rules per parse pair will decrease with
larger training sets, though this remains to be ver-
ified.
The rule illustrated in Figure 3 was accepted as
the 65th best transfer rule with a log likelihood
ratio of 33.37, and the rule illustrated in Figure 2
was accepted as the 189th best transfer rule can-
didate with a log likelihood ratio of 12.77. An ex-
ample of a candidate transfer rule that was not ac-
cepted is the one that combines the features of the
two rules mentioned above, illustrated in Figure 8.
This transfer rule candidate had a lower log like-
lihood ratio of 11.40; consequently, it is only con-
sidered after the two rules mentioned above, and
since it provides no further improvement upon
these two rules, it is filtered out.
In an informal inspection of the top 100 ac-
cepted transfer rules, we found that most of them
appear to be fairly general rules that would nor-
mally be found in a general syntactic-based trans-
fer dictionary. In looking at the remaining rules,
we found that the rules tended to become increas-
ingly corpus-specific.
5 Initial Evaluation
5.1 Results
In an initial evaluation of our approach, we ap-
plied both the baseline transfer dictionary and
the induced transfer dictionary (i.e., the baseline
transfer dictionary augmented with the transfer
rules induced from the training set) to the test half
of the 1763 higher quality parse pairs described in
Section 3.1, in order to produce two sets of trans-
ferred parses, the baseline set and the induced set.
For each set, we then calculated tree accuracy re-
call and precision measures as follows:
Tree accuracy recall The tree accuracy recall
for a transferred parse and a correspond-
ing target parse is determined the by C/Rq,
where C is the total number of features (at-
tributes, lexemes and dependency relation-
ships) that are found in both the nodes of
the transferred parse and in the correspond-
ing nodes in the target parse, and Rq is the
total number of features found in the nodes
of the target parse. The correspondence be-
tween the nodes of the transferred parse and
the nodes of the target parse is determined
with alignment information obtained using
the technique described in Section 4.1.
Tree accuracy precision The tree accuracy pre-
cision for a transferred parse and a corre-
sponding target parse is determined the by
C/Rt, where C is the total number of fea-
tures (attributes, lexemes and dependency
relationships) that are found in both the
nodes of the transferred parse and in the cor-
responding nodes in the target parse, and Rt
is the total number of features found in the
nodes of the transferred parse.
Table 1 shows the tree accuracy results, where
the f-score is equally weighted between recall and
precision. The results illustrated in Table 1 indi-
cate that the transferred parses obtained using in-
duction were moderately more similar to the tar-
get parses than the transferred parses obtained us-
ing the baseline transfer, with about 15 percent
improvement in the f-score.
Recall Precision F-Score
Baseline 37.77 46.81 41.18
Induction 55.35 58.20 55.82
Table 1: Tree accuracy results
5.2 Discussion
At the time of writing, the improvements in tree
accuracy do not yet appear to yield apprecia-
ble improvements in realization results. While
our syntactic realizer, RealPro, does produce rea-
sonable surface strings from the target depen-
dency trees, despite occasional errors in parsing
the target strings and converting the phrase struc-
ture trees to dependency trees, it appears that the
tree accuracy levels for the transferred parses will
need to be higher on average before the improve-
ments in tree accuracy become consistently visi-
ble in the realization results. At present, the fol-
lowing three problems represent the most impor-
tant obstacles we have identified to achieving bet-
ter end-to-end results:
? Since many of the test sentences require
transfer rules for which there are no similar
cases in the set of training sentences, it ap-
pears that the relatively small size of our cor-
pus is a significant barrier to better results.
? Some performance problems with the cur-
rent implementation have forced us to make
use of a perhaps overly strict set of alignment
and attribute constraints. With an improved
implementation, it may be possible to find
more valuable rules from the same training
data.
? A more refined treatment of rule conflicts is
needed in order to allow multiple rules to
access overlapping contexts, while avoiding
the introduction of multiple translations of
the same content in certain cases.
6 Conclusion and Future Directions
In this paper we have presented a novel approach
to transfer rule induction based on syntactic pat-
tern co-occurrence in parsed bi-texts. In an initial
evaluation on a relatively small corpus, we have
shown that the induced syntactic transfer rules
from Korean to English lead to a modest increase
in the accuracy of transferred parses when com-
pared to the target parses. In future work, we
hope to demonstrate that a combination of consid-
ering a larger set of transfer rule candiates, refin-
ing our treatment of rule conflicts, and making use
of more training data will lead to further improve-
ments in tree accuracy, and, following syntactic
realization, will yield to significant improvements
in end-to-end results.
Acknowledgements
We thank Richard Kittredge for helpful discus-
sion, Daryl McCullough and Ted Caldwell for
their help with evaluation, and Chung-hye Han,
Martha Palmer, Joseph Rosenzweig and Fei Xia
for their assistance with the handcrafted Korean-
English transfer dictionary and the conversion of
phrase structure parses to syntactic dependency
representations. This work has been partially sup-
ported by DARPA TIDES contract no. N66001-
00-C-8009.
References
Michael Collins. 1997. Three generative, lexicalised
models for statistical parsing. In Proceedings of the
35th Meeting of the Association for Computational
Linguistics (ACL?97), Madrid, Spain.
Bonnie Dorr. 1994. Machine translation divergences:
A formal description and proposed solution. Com-
putational Linguistics, 20(4):597?635.
C. Han, B. Lavoie, M. Palmer, O. Rambow, R. Kit-
tredge, T. Korelsky, N. Kim, and M. Kim. 2000.
Handling structural divergences and recovering
dropped arguments in a Korean-English machine
translation system. In Proceedings of the Fourth
Conference on Machine Translation in the Ameri-
cas (AMTA?00), Misin Del Sol, Mexico.
Benoit Lavoie and Owen Rambow. 1997. RealPro ?
a fast, portable sentence realizer. In Proceedings of
the Conference on Applied Natural Language Pro-
cessing (ANLP?97), Washington, DC.
C. D. Manning and H. Schutze. 1999. Foundations
of Statistical Natural Language Processing. MIT
Press.
H. Maruyama and H. Watanabe. 1992. Tree cover
search algorithm for example-based translation. In
Proceedings of the Fourth International Conference
on Theoretical and Methodological Issues in Ma-
chine Translation (TMI?92), pages 173?184.
Y. Matsumoto, H. Hishimoto, and T. Utsuro. 1993.
Structural matching of parallel texts. In Proceed-
ings of the 31st Annual Meetings of the Association
for Computational Linguistics (ACL?93), pages 23?
30.
Igor Mel?c?uk. 1988. Dependency Syntax. State Uni-
versity of New York Press, Albany, NY.
A. Meyers, R. Yangarber, R. Grishman, C. Macleod,
and A. Moreno-Sandoval. 1998. Deriving transfer
rules from dominance-preserving alignments. In
Proceedings of COLING-ACL?98, pages 843?847.
Makoto Nagao. 1984. A framework of a mechan-
ical translation between Japenese and English by
analogy principle. In A. Elithorn and R. Banerji,
editors, Artificial and Human Intelligence. NATO
Publications.
Alexis Nasr, Owen Rambow, Martha Palmer, and
Joseph Rosenzweig. 1997. Enriching lexical trans-
fer with cross-linguistic semantic features. In Pro-
ceedings of the Interlingua Workshop at the MT
Summit, San Diego, California.
S. Sato and M. Nagao. 1990. Toward memory-
based translation. In Proceedings of the 13th Inter-
national Conference on Computational Linguistics
(COLING?90), pages 247?252.
Fei Xia and Martha Palmer. 2001. Converting depen-
dency structures to phrase structures. In Notes of
the First Human Language Technology Conference,
San Diego, California.
J. Yoon, S. Kim, and M. Song. 1997. New parsing
method using global association table. In Proceed-
ings of the 5th International Workshop on Parsing
Technology.
Selecting Sentences for Multidocument Summaries using
Randomized Local Search
Michael White
CoGenTex, Inc.
840 Hanshaw Road
Ithaca, NY 14850, USA
mike@cogentex.com
Claire Cardie
Dept. of Computer Science
Cornell University
Ithaca, NY 14850, USA
cardie@cs.cornell.edu
Abstract
We present and evaluate a randomized local
search procedure for selecting sentences to in-
clude in a multidocument summary. The search
favors the inclusion of adjacent sentences while
penalizing the selection of repetitive material,
in order to improve intelligibility without un-
duly affecting informativeness. Sentence simi-
larity is determined using both surface-oriented
measures and semantic groups obtained from
merging the output templates of an information
extraction subsystem. In a comparative evalu-
ation against two DUC-like baselines and three
simpler versions of our system, we found that
our randomized local search method provided
substantial improvements in both content and
intelligibility, while the use of the IE groups also
appeared to contribute a small further improve-
ment in content.
1 Introduction
Improving the intellibility of multidocument
summaries remains a significant challenge.
While most previous approaches to multidoc-
ument summarization have addressed the prob-
lem of reducing repetition, less attention has
been paid to problems of coherence and co-
hesion. In a typical extractive system (e.g.
Goldstein et al (2000)), sentences are selected
for inclusion in the summary one at a time,
with later choices sensitive to their similarity
to earlier ones; the selected sentences are then
ordered either chronologically or by relevance.
The resulting summaries often jump incoher-
ently from topic to topic, and contain broken
cohesive links, such as dangling anaphors or un-
met presuppositions.
Barzilay et al (2001) present an improved
method of ordering sentences in the context
of MultiGen, a multidocument summarizer
that identifies sets of similar sentences, termed
themes, and reformulates their common phrases
as new text. In their approach, topically related
themes are identified and kept together in the
resulting summary, in order to help improve co-
hesion and reduce topic switching.
In this paper, we pursue a related but simpler
idea in an extractive context, namely to favor
the selection of blocks of adjacent sentences in
constructing a multidocument summary. Here,
the challenge is to improve intelligibility with-
out unduly sacrificing informativeness; for ex-
ample, selecting the beginning of the most re-
cent article in a document set will usually pro-
duce a highly intelligible text, but one that is
not very representative of the document set as
a whole.
To manage this tradeoff, we have developed a
randomized local search procedure (cf. Selman
and Kautz (1994)) to select the highest ranking
set of sentences for the summary, where the in-
clusion of adjacent sentences is favored and the
selection of repetitive material is penalized. The
method involves greedily searching for the best
combination of sentences to swap in and out of
the current summary until no more improve-
ments are possible; noise strategies include oc-
casionally adding a sentence to the current sum-
mary, regardless of its score, and restarting the
local search from random starting points for a
fixed number of iterations. In determining sen-
tence similarity, we have used surface-oriented
similarity measures obtained from Columbia?s
SimFinder tool (Hatzivassiloglou et al, 2001),
as well as semantic groups obtained from merg-
ing the output templates of an information ex-
traction (IE) subsystem.
In related work, Marcu (2001) describes an
approach to balancing informativeness and in-
telligibility that also involves searching through
        Philadelphia, July 2002, pp. 9-18.  Association for Computational Linguistics.
          Proceedings of the Workshop on Automatic Summarization (including DUC 2002),
sets of sentences to select. In contrast to
our approach, Marcu employs a beam search
through possible summaries of progressively
greater length, which seems less amenable to
an anytime formulation; this may be an im-
portant practical consideration, since Marcu re-
ports search times in hours, whereas we have
found that less than a minute of searching is
usually effective. In other related work, Lin
and Hovy (2002) suggest pairing extracted sen-
tences with their corresponding lead sentences;
we have not directly compared our search-based
approach to Lin and Hovy?s simpler method.
In order to evaluate our approach, we com-
pared 200-word summaries generated by our
system to those of two baselines that are similar
to those used in DUC 2001 (Harman, 2001), and
to three simpler versions of the system, where
a simple marginal relevance selection procedure
was used instead of the selection search, and/or
the IE groups were ignored. In general, we
found that our randomized local search method
provided substantial improvements in both con-
tent and intelligibility over the DUC-like base-
lines and the simplest variant of our system,
which used marginal relevance selection and no
IE groups (with the exception that the last arti-
cle baseline was always ranked first in intelligi-
bility). The use of the IE groups also appeared
to contribute a small further improvement in
content when used with our selection search.
We discuss these results in greater detail in the
final section of the paper.
2 System Description
We have implemented our randomized local
search method for sentence selection as part
of the RIPTIDES (White et al, 2001) sys-
tem. RIPTIDES combines information extrac-
tion (IE) in the domain of natural disasters and
multidocument summarization to produce hy-
pertext summaries. The hypertext summaries
include a high-level textual overview; tables of
all comparable numeric estimates, organized to
highlight discrepancies; and targeted access to
supporting information from the original arti-
cles. In White et al (2002), we showed that the
hypertext summaries can help to identify dis-
repancies in numeric estimates, and provide a
significantly more complete picture of the avail-
able information than the latest article. The
next subsection walks through a sample hyper-
text summary; it is followed by descriptions of
the IE and Summarizer system components.
2.1 Example
Figure 1 shows a textual overview of the first
dozen or so articles in a corpus of news arti-
cles gathered from the web during the first week
after the January 2001 earthquake in Central
America. Clicking on the magnifying glass icon
brings up the original article in the right frame,
with the extracted sentences highlighted.
The index to the hypertext summary appears
in the left frame of figure 1. Links to the
overview and to the lead sentences of the arti-
cles are followed by links to tables that display
the base level extraction slots for the main event
(here, an earthquake) including its description,
date, location, epicenter and magnitude. Access
to the overall damage estimates appears next,
with separate tables for types of human effects
(e.g. dead, missing) and for object types (e.g.
villages, bridges, houses) with physical effects.
Figure 2 shows the extracted estimates of the
overall death toll. In order to help identify dis-
crepancies, the high and low current estimates
are shown at the top, followed by other cur-
rent estimates and then all extracted estimates.
Heuristics are used to determine which esti-
mates to consider current, taking into account
the source (either news source or attributed
source), specificity (e.g. hundreds vs. at least
200) and confidence level, as indicated by the
presence of hedge words such as perhaps or as-
sumed. The tables also provide links to the orig-
inal articles, allowing the user to quickly and
directly determine the accuracy of any estimate
in the table.
2.2 IE System
The IE system combines existing language tech-
nology components (Bikel et al, 1997; Char-
niak, 1999; Day et al, 1997; Fellbaum, 1998) in
a traditional IE architecture (Cardie, 1997; Gr-
ishman, 1996). Unique features of the system
include a weakly supervised extraction pattern-
learning component, Autoslog-XML, which is
based on Autoslog-TS (Riloff, 1996), but op-
erates in an XML framework and acquires pat-
terns for extracting text elements beyond noun
phrases (e.g. verb groups, adjectives, adverbs,
and single-noun modifiers). In addition, a
Figure 1: Hypertext Summary Overview
Figure 2: Tables of Death Toll Estimates
heuristic-based clustering algorithm organizes
the extracted concepts into output templates
specifically designed to support multi-document
summarization: the IE system, for example, dis-
tinguishes different reports or views of the same
event from multiple sources (White et al, 2001).
Output templates from the IE system for each
text to be covered in the multi-document sum-
mary are provided as input to the summariza-
tion component along with all linguistic anno-
tations accrued in the IE phase.
2.3 Summarizer
The Summarizer operates in three main stages.
In the first stage, the IE output templates are
merged into an event-oriented structure where
comparable facts are semantically grouped. To-
wards the same objective, surface-oriented clus-
tering is used to group sentences from different
documents into clusters that are likely to report
similar content. In the second stage, importance
scores are assigned to the sentences based on the
following indicators: position in document, doc-
ument recency, presence of quotes, average sen-
tence overlap, headline overlap, size of cluster
(if any), size of semantic groups (if any), speci-
ficity of numeric estimates, and whether these
estimates are deemed current. In the third and
final stage, the hypertext summary is generated
from the resulting content pool. Further details
on each stage follow in the paragraphs below;
see White et al (2002) for a more complete de-
scription.
In the analysis stage, we use Columbia?s
SimFinder tool (Hatzivassiloglou et al, 2001) to
obtain surface-oriented similarity measures and
clusters for the sentences in the input articles.
To obtain potentially more accurate partitions
using the IE output, we semantically merge the
extracted slots into comparable groups, i.e. ones
whose members can be examined for discrepan-
cies. This requires distinguishing (i) different
types of damage; (ii) overall damage estimates
vs. those that pertain to a specific locale; and
(iii) damage due to related events, such as previ-
ous quakes in the same area. During this stage,
we also analyze the numeric estimates for speci-
ficity and confidence level, and determine which
estimates to consider current.
In the scoring stage, SimFinder?s similarity
measures and clusters are combined with the
semantic groupings obtained from merging the
IE templates in order to score the input sen-
tences. The scoring of the clusters and seman-
tic groups is based on their size, and the scores
are combined at the sentence level by includ-
ing the score of all semantic groups that con-
tain a phrase extracted from a given sentence.
More precisely, the scores are assigned in three
phases, according to a set of hand-tuned pa-
rameter weights. First, a base score is assigned
to each sentence according to a weighted sum
of the position in document, document recency,
presence of quotes, average sentence overlap,
and headline overlap. The average sentence
overlap is the average of all pairwise sentence
similarity measures; we have found this measure
to be a useful counterpart to sentence position
in reliably identifying salient sentences, with the
other factors playing a lesser role. In the second
scoring phase, the clusters and semantic groups
are assigned a score according to the sum of the
base sentence scores. After normalization in the
third scoring phase, the weighted cluster and
group scores are used to boost the base scores,
thereby favoring sentences from the more im-
portant clusters and semantic groups. Finally,
a small boost is applied for currenten and more
specific numeric estimates.
In the generation stage, the overview is con-
structed by selecting a set of sentences in a
context-sensitive fashion, and then ordering the
blocks of adjacent sentences according to their
importance scores. The summarization scoring
model begins with the sum of the scores for
the candidate sentences, which is then adjusted
to penalize the inclusion of multiple sentences
from the same cluster or semantic group, or sen-
tences whose similarity measure is above a cer-
tain threshold, and to favor the inclusion of ad-
jacent sentences from the same article, in order
to boost intelligibility. A larger bonus is applied
when including a sentence that begins with an
initial pronoun as well as the previous one, and
an even bigger bonus is added when including
a sentence that begins with a strong rhetorical
marker (e.g. however) as well as its predecessor;
corresponding penalties are also used when the
preceding sentence is missing, or when a short
sentence appears without an adjacent one.
To select the sentences for the overview ac-
cording to this scoring model, we use an itera-
tive randomized local search procedure inspired
by Selman and Kautz (1994). Two noise strate-
gies are employed to lessen the problem of lo-
cal maxima in the search space: (i) the local
search is restarted from random starting points,
for a fixed number of iterations, and (ii) during
each local search iteration, greedy steps are in-
terleaved with random steps, where a sentence
is added regardless of its score. In the first local
search iteration, the initial sentence collection
consists of the highest scoring sentences up to
the word limit. In subsequent iterations, the ini-
tial collection is composed of randomly selected
sentences, weighted according to their scores,
up to the word limit. During each local search
iteration, a random step or a greedy step (cho-
sen at random) is repeatedly performed until a
greedy step fails to improve upon the current
collection of sentences. In each greedy step, one
sentence is chosen to add to the collection, and
zero or more (typically one) sentences are cho-
sen to remove from the collection, such that the
word limit is still met, and this combination of
sentences represents the best swap available ac-
cording to the scoring model. After the prede-
termined number of iterations, the best combi-
nation of sentences found during the search is
output; note that the algorithm could easily be
formulated in an anytime fashion as well. From
a practical perspective, we have found that 10
iterations often suffices to find a reasonable col-
lection of sentences, taking well under a minute
on a desktop PC.
Once the overview sentences have been se-
lected, the hypertext summary is generated as a
collection of HTML files, using a series of XSLT
transformations.
2.4 Training and Tuning
For the evaluation below, the IE system was
trained on 12 of 25 texts from topic 89 of the
TDT2 corpus, a set of newswires that describe
the May 1998 earthquake in Afganistan. It
achieves 42% recall and 61% precision when
evaluated on the remaining 13 topic 89 texts.
The parameter settings of the Summarizer were
chosen by hand using the complete TDT2 topic
89 document set as input.
3 Evaluation Method and Results
To select the inputs for the evaluation, we took
five subsets of the articles from TDT2 topic
89 ? all the articles up to the end of days 1
through 5 after the quake. We chose to use
TDT2 topic 89 so that we could assess the im-
pact of the IE quality on the results, given that
we had previously created manual IE annota-
tions for these articles (White et al, 2001).1
1Although our decision to use subsets of TDT2 topic
89 as inputs meant that our training/tuning and test
data overlapped, we do not believe that this choice overly
compromises our results, since ? as will be discussed in
this section and the next ? the impact of the IE groups
For each input document set, we ran the RIP-
TIDES system to produce overview summaries
of 200 words or less. For comparison purposes,
we also ran two baselines, similar to those used
in DUC 2001 (Harman, 2001), and three sim-
pler versions of the system, for a total of six
summary types:
Last The first N sentences of the latest article
in the document set, up to the word limit.
Leads The lead sentences from the latest arti-
cles in the document set, up to the word
limit, listed in chronological order.
MR The top ranking sentences selected accord-
ing to their thresholded marginal relevance,
up to the word limit, listed in chronological
order, using RIPTIDES to score the sen-
tences, except with the IE groups zeroed
out.
MR+IE The MR summarization method, but
with the IE groups included for the RIP-
TIDES sentence scorer.
Search The RIPTIDES overview, except with
the IE groups zeroed out for sentence scor-
ing.
Search+IE The RIPTIDES overview.
The marginal relevance systems (MR and
MR+IE) used a simple selection mechanism
which does not involve search, inspired by the
maximal marginal relevance (MMR) approach
(Goldstein et al, 2000). This selection mech-
anism begins by selecting the top ranking sen-
tence for inclusion, then determines whether to
include the second ranking sentence depending
on whether it is sufficiently dissimilar from the
first one, based on comparing the SimFinder
similarity measure against a hard threshold
(0.85), and likewise for lower ranked sentences,
comparing them against all sentences included
so far, up to the word limit. The selected sen-
tences are then gathered into blocks of adjacent
sentences, and ordered chronologically.2
turned out to be small, while with our selection search,
we ran into a couple of problems on the test data that
did not show up in tuning the parameter settings.
2In trying out the MR systems on all the articles in
TDT2 topic 89, we found chronological ordering to usu-
ally be more coherent than importance ordering.
Content Rank, Simulated IE
Last Leads MR MR+IE Search Search+IE
Day 1 5, 6 5, 5 4, 4 3, 3 1, 1 1, 1
Day 2 5, 4 6, 6 1, 4 4, 3 2, 1 1, 2
Day 3 6, 6 3, 3 3, 3 5, 5 1, 1 1, 1
Day 4 6, 5 3, 6 3, 4 3, 2 2, 2 1, 1
Day 5 6, 6 2, 5 2, 4 2, 2 2, 2 1, 1
Average 5.5 ?0.7 4.4 ?1.5 3.2 ?1.0 3.1 ?1.2 1.6 ?0.7 1.1 ?0.3
Table 1: Content Rankings on TDT2 Topic 89, using Simulated IE. The scores for the two judges
at each time point are separated by commas.
Intelligibility Rank, Simulated IE
Last Leads MR MR+IE Search Search+IE
Day 1 1, 1 5, 2 6, 6 3, 2 2, 4 3, 5
Day 2 1, 1 5, 5 6, 5 4, 4 2, 2 2, 3
Day 3 1, 1 6, 5 5, 6 3, 4 3, 1 2, 1
Day 4 1, 1 6, 6 5, 5 3, 3 3, 4 2, 2
Day 5 1, 1 4, 6 4, 5 4, 3 2, 4 2, 2
Average 1 ?0 5 ?1.2 5.3 ?0.7 3.3 ?0.7 2.7 ?1.1 2.4 ?1.1
Table 2: Intelligibility Rankings on TDT2 Topic 89, using Simulated IE.
Content Rank, Actual IE
Last Leads MR MR+IE Search Search+IE
Day 1 5, 6 5, 5 4, 4 3, 3 2, 1 1, 1
Day 2 5, 4 6, 6 1, 4 3, 3 3, 2 1, 1
Day 3 6, 6 1, 2 1, 2 5, 5 3, 1 3, 2
Day 4 6, 5 4, 6 1, 2 5, 4 1, 1 1, 2
Day 5 6, 6 2, 5 4, 2 4, 4 2, 2 1, 1
Average 5.5 ?0.7 4.2 ?1.9 2.5 ?1.4 3.9 ?0.9 1.8 ?0.8 1.4 ?0.7
Table 3: Content Rankings on TDT2 Topic 89, using Actual IE.
Intelligibility Rank, Actual IE
Last Leads MR MR+IE Search Search+IE
Day 1 1, 1 5, 2 6, 6 3, 2 2, 4 3, 5
Day 2 1, 1 5, 5 6, 5 4, 4 2, 2 2, 2
Day 3 1, 1 6, 4 5, 6 2, 3 2, 2 2, 4
Day 4 1, 1 6, 6 4, 4 4, 3 3, 2 2, 4
Day 5 1, 1 4, 6 4, 5 4, 2 3, 4 2, 3
Average 1 ?0 4.9 ?1.3 5.1 ?0.9 3.1 ?0.9 2.6 ?0.8 2.9 ?1.1
Table 4: Intelligibility Rankings on TDT2 Topic 89, using Actual IE.
TDT2 Topic 89, Simulated IE
1
1.5
2
2.5
3
3.5
4
4.5
5
5.5
6
Las
t
Lea
ds MR
MR
+IE
Sea
rch
Sea
rch
+IE
System
Av
g 
Ra
nk Content
Intelligibility
Figure 3: Average System Rank for Content and Intelligibility on TDT2 Topic 89, using Simulated
IE. The ranks are averaged across two judges and five time points; manual IE annotations were
used with the MR+IE and Search+IE systems.
TDT2 Topic 89, Actual IE
11.522.533.544.555.56
Las
t
Lea
ds MR
MR
+IE
Sea
rch
Sea
rch
+IE
System
Av
g 
Ra
nk Content
Intelligibility
Figure 4: Average System Rank for Content and Intelligibility on TDT2 Topic 89, using Actual
IE. The ranks are averaged across two judges and five time points; actual IE annotations were used
with the MR+IE and Search+IE systems, making all systems fully automatic.
For each of the five time points, we ran the
six systems on two versions of the input docu-
ment sets, one with the manual IE annotations
(simulated IE) and one with the automatic IE
annotations (actual IE). Note that with each of
the first three systems, the output did not differ
from one version of the input to the other, since
these systems did not depend on the IE anno-
tations and did not involve randomized search.
Next, for each document set, we had two judges3
rank the summaries from best to worst, with
3The authors were the judges.
Significant Differences in System Versions
Last Leads MR MR+IE Search Search+IE
Last Int(Sim,Act) Con(Sim,Act) Con(Sim,Act) Con(Sim,Act) Con(Sim,Act)
Int(Sim,Act) Int(Sim,Act) Int(Sim,Act) Int(Sim,Act)
Leads - Int(Sim,Act) Con(Sim,Act) Con(Sim,Act)
Int(Sim,Act) Int(Sim,Act)
MR Int(Sim,Act) Con(Sim) Con(Sim)
Int(Sim,Act) Int(Sim,Act)
MR+IE Con(Act) Con(Sim,Act)
Search -
Table 5: Significant Differences in System Versions based on Pairwise t-Tests. Table entries indicate
statistically significant differences (at the 95% confidence level) in mean Content and Intelligibility
rank on TDT2 Topic 89 texts using Simulated or Actual output from the IE system.
ties allowed, in two categories, content and in-
telligibility. In the case of ties, the tied systems
shared the appropriate ranking; for example, if
two summaries tied for the best content, each
received a rank of 1, with the next best sum-
mary receiving a rank of 3 (cf. Olympic pairs
skating).
The charts in figures 3 and 4 show the sys-
tem rank for content and intelligibility for the
simulated IE and actual IE versions of the doc-
ument sets, respectively, averaged across the
two judges and five time points. Tables 1
through 4 list all the judgements together with
their means and standard deviations.
In general, we found that Search and
Search+IE provided substantial improvements
in both content and intelligibility over Last,
Leads and MR, with the exception that
Last was always ranked first in intelligibility.
Search+IE also appeared to show a small fur-
ther improvement in content.
Determining the significance of the improve-
ments is somewhat complex, due to the small
number of data points and the use of multiple
comparisons. To judge the significance levels,
we calculated pairwise t-tests for all the means
listed in tables 1 through 4, and applied the
Bonferroni adjustment, which is a conservative
way to perform multiple comparisons where the
total chance of error is spread across all com-
parisons.4
4With the total ? equal to 0.05, the Bonferroni ad-
justment provides a 95% confidence level that all the
pairwise judgements are correct. In our case, a total ?
of 0.05 corresponds to an individual ? of 0.0033, which
is difficult to exceed with a small number of data points.
Statisitically significant differences in system
performance at the 95% confidence level appear
in table 5. Turning first to the content rankings,
with the simulated IE output, we found that
both Search and Search+IE scored significantly
higher than Last, Leads and MR. While the dif-
ference between Search and Search+IE was not
significant, only Search+IE achieved a signifi-
cantly higher average rank than MR+IE. With
the actual IE output, Search and Search+IE
again scored significantly higher than Last and
Leads and, although these two systems did not
show a significant improvement over MR, both
systems did improve significantly over Leads
and MR+IE, in contrast to MR.
Turning now to the intelligibility rankings,
with both the simulated and actual IE and, we
found that Search and Search+IE improved sig-
nificantly over Leads and MR. The difference
between Search and Search+IE was not signifi-
cant. Surprisingly, MR+IE scored significantly
higher than MR, and not significantly worse
than Search and Search+IE.
4 Discussion
We were pleased with the substantial improve-
ments in both content and intelligibility that
our randomized local search method provided
over the DUC-like baselines and the simplest
variant of our system, the one using marginal
relevance selection and no IE groups (with the
exception that the last article baseline was al-
ways ranked first in intelligibility). We did not
expect to find that the selection search would
yield substantial improvements over marginal
relevance selection in the content rankings, since
the search method was designed to improve in-
telligibility without unduly affecting content.
At the same time though, we were somewhat
disappointed that the use of the IE groups ap-
peared to only contribute a small further im-
provement in content when used with our selec-
tion search.
It is not entirely clear why our selection
search method led to improvements in the con-
tent rankings when compared to the marginal
relevance variants. One possibility is that the
randomized local search was able to find sen-
tences with greater information density. An-
other possibility is that the use of a hard thresh-
old by the marginal relevance variants led to
some poor sentence selections fairly far down on
the list of ranked sentences; the marginal rele-
vance selection may have worked better had we
used a smaller threshold, or if we had re-ranked
the sentences following each selection according
to a redundancy penalty, rather than simply us-
ing a threshold.
It is also not clear why the IE groups did
not help more with content selection. It may
well be that a more elaborate evaluation, involv-
ing more systems and judgements, would indeed
show that the IE groups yielded significant im-
provements in content rankings. On the other
hand, our results may indicate that shallow ex-
tractive techniques can pick up much the same
information as IE techniques, at least for the
purpose of selecting sentences for generic extrac-
tive summaries. Note that for purposes of dis-
crepancy detection, the ability of IE techniques
to more completely extract relevant phrases is
clearly demonstrated in White et al (2002).
On the intelligibility side, we were surprised
to find that the IE groups led to improvements
in intelligibility when used with marginal rele-
vance selection. One likely explanation for this
improvement is that this system variant jumped
around less from topic to topic than its counter-
part that did not make use of the IE info.
Another question is why the selection search
did not yield further improvements in intelli-
bility. One reason is that the search method
always selected sentences up to the word limit,
even when this yielded highly repetitive sum-
maries ? as was the case with the first two test
sets, which only contained a handful of articles.
Another reason is that the search routine was
prone to selecting a couple of sentences from an
article that was largely off topic, only containing
a brief mention of the quake.
These deficiencies point to possible improve-
ments in the search method: informativeness
could perhaps be balanced with conciseness by
deselecting sentences that do not improve the
overall score; and off-topic sentences could per-
haps be avoided by taking into account the cen-
trality of the document in the sentence scores.
More speculatively, it would be interesting to
extend the approach to work with sub-sentential
units, and to make use of a greater variety of
inter-sentential cohesive links.
Acknowledgments
This work was supported in part by DARPA
TIDES contract N66001-00-C-8009 and NSF
Grants 0081334 and 0074896. We thank
Tanya Korelsky, Daryl McCullough, Vincent
Ng, David Pierce and Kiri Wagstaff for their
help with earlier versions of the system.
References
Regina Barzilay, Noemie Elhadad, and Kath-
leen McKeown. 2001. Sentence Ordering in
Multidocument Summarization. In Proceed-
ings of the First International Conference on
Human Language Technology Research, San
Diego, CA.
D. Bikel, S. Miller, R. Schwartz, and
R. Weischedel. 1997. Nymble: A High-
Performance Learning Name-Finder. In Pro-
ceedings of the Fifth Conference on Applied
Natural Language Processing, pages 194?201,
San Francisco, CA. Morgan Kaufmann.
C. Cardie. 1997. Empirical Methods in Infor-
mation Extraction. AI Magazine, 18(4):65?
79.
Eugene Charniak. 1999. A maximum-entropy-
inspired parser. Technical Report CS99-12,
Brown University.
D. Day, J. Aberdeen, L. Hirschman,
R. Kozierok, P. Robinson, and M. Vi-
lain. 1997. Mixed-Initiative Development
of Language Processing Systems. In Pro-
ceedings of the Fifth Conference on Applied
Natural Language Processing. Association for
Computational Linguistics.
C. Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press, Cambridge,
MA.
J. Goldstein, V. Mittal, J. Carbonell, and
M. Kantrowitz. 2000. Multi-document sum-
marization by sentence extraction. In Pro-
ceedings of the ANLP/NAACL Workshop on
Automatic Summarization, Seattle, WA.
R. Grishman. 1996. TIPSTER Archi-
tecture Design Document Version 2.2.
Technical report, DARPA. Available at
http://www.tipster.org/.
Donna Harman. 2001. Proceedings of the 2001
Document Understanding Conference (DUC-
2001). NIST.
Vasileios Hatzivassiloglou, Judith L. Klavans,
Melissa L. Holcombe, Regina Barzilay, Min-
Yen Kan, and Kathleen R. McKeown. 2001.
Simfinder: A flexible clustering tool for sum-
marization. In Proceedings of the NAACL
2001 Workshop on Automatic Summariza-
tion, Pittsburgh, PA.
Chin-Yew Lin and Eduard Hovy. 2002. Au-
tomated Multi-document Summarization in
NeATS. In Proceedings of the Second In-
ternational Conference on Human Language
Technology Research, San Diego, CA. To ap-
pear.
Daniel Marcu. 2001. Discourse-Based Sum-
marization in DUC-2001. In Proceedings of
the 2001 Document Understanding Confer-
ence (DUC-2001).
E. Riloff. 1996. Automatically Generating Ex-
traction Patterns from Untagged Text. In
Proceedings of the Thirteenth National Con-
ference on Artificial Intelligence, pages 1044?
1049, Portland, OR. AAAI Press / MIT
Press.
Bart Selman and Henry Kautz. 1994. Noise
Strategies for Improving Local Search. In
Proceedings of AAAI-94.
Michael White, Tanya Korelsky, Claire Cardie,
Vincent Ng, David Pierce, and Kiri Wagstaff.
2001. Multidocument Summarization via In-
formation Extraction. In Proceedings of the
First International Conference on Human
Language Technology Research, San Diego,
CA.
Michael White, Claire Cardie, Vincent Ng, and
Daryl McCullough. 2002. Detecting Discrep-
ancies in Numeric Estimates Using Multidoc-
ument Hypertext Summaries. In Proceedings
of the Second International Conference on
Human Language Technology Research, San
Diego, CA. To appear.
Learning Domain-Specific Transfer Rules:
An Experiment with Korean to English Translation
Benoit Lavoie, Michael White, and Tanya Korelsky
CoGenTex, Inc.
840 Hanshaw Road
Ithaca, NY 14850, USA
benoit,mike,tanya@cogentex.com
Abstract
We describe the design of an MT system that em-
ploys transfer rules induced from parsed bitexts
and present evaluation results. The system learns
lexico-structural transfer rules using syntactic pat-
tern matching, statistical co-occurrence and error-
driven filtering. In an experiment with domain-
specific Korean to English translation, the approach
yielded substantial improvements over three base-
line systems.
1 Introduction
In this paper, we describe the design of an MT
system that employs transfer rules induced from
parsed bitexts and present evaluation results for Ko-
rean to English translation. Our approach is based
on lexico-structural transfer (Nasr et. al., 1997),
and extends recent work reported in (Han et al,
2000) about Korean to English transfer in particular.
Whereas Han et al focus on high quality domain-
specific translation using handcrafted transfer rules,
in this work we instead focus on automating the ac-
quisition of such rules.
The proposed approach is inspired by example-
based machine translation (EBMT; Nagao, 1984;
Sato and Nagao, 1990; Maruyama and Watanabe,
1992) and is similar to the recent works of (Mey-
ers et al, 1998) and (Richardson et al, 2001) where
transfer rules are also derived after aligning the
source and target nodes of corresponding parses.
However, while (Meyers et al, 1998) and (Richard-
son et al, 2001) only consider parses and rules
with lexical labels and syntactic roles, our approach
uses parses containing any syntactic information
provided by parsers (lexical labels, syntactic roles,
tense, number, person, etc.), and derives rules con-
sisting of any source and target tree sub-patterns
matching a subset of the parse features. A more de-
tailed description of the differences can be found in
(Lavoie et. al., 2001).
2 Overall Runtime System Design
Our Korean to English MT runtime system relies on
the following off-the-shelf software components:
Korean parser For parsing, we used the wide cov-
erage syntactic dependency parser for Korean
developed by (Yoon et al, 1997). The parser
was not trained on our corpus.
Transfer component For transfer of the Korean
parses to English structures, we used the same
lexico-structural transfer framework as (Lavoie
et al, 2000).
Realizer For surface realization of the transferred
English syntactic structures, we used the Re-
alPro English realizer (Lavoie and Rambow,
1997).
The training of the system is described in the next
two sections.
3 Data Preparation
3.1 Parses for the Bitexts
In our experiments, we used a parallel corpus de-
rived from bilingual training manuals provided by
the U.S. Defense Language Institute. The corpus
consists of a Korean dialog of 4,183 sentences about
battle scenario message traffic and their English hu-
man translations.
The parses for the Korean sentences were ob-
tained using Yoon?s parser, as in the runtime sys-
tem. The parses for the English human transla-
(S1) {i} {Ci-To-Reul} {Ta-Si} {Po-Ra}.
this + map-accusative + again + look-imp
(D1) {po} [class=vbma ente={ra}] (
s1 {ci-to} [class=nnin2 ppca={reul}] (
s1 {i} [class=ande] )
s1 {ta-si} [class=adco2] )
(S2) Look at the map again.
(D2) look [class=verb mood=imp] (
attr at [class=preposition] (
ii map [class=common_noun article=def] )
attr again [class=adverb] )
Figure 1: Syntactic dependency representations for
corresponding Korean and English sentences
Korean English
Avg. sentence size 9.08 13.26
Avg. parse size 8.96 10.77
Table 1: Average sizes for sentences and parses in
corpus
tions were derived from an English Tree Bank de-
veloped in (Han et al, 2000). To enable the sur-
face realization of the English parses via RealPro,
we automatically converted the phrase structures
of the English Tree Bank into deep-syntactic de-
pendency structures (DSyntSs) of the Meaning-Text
Theory (MTT) (Mel?c?uk, 1988) using Xia?s con-
verter (Xia and Palmer, 2001) and our own conver-
sion grammars. The realization results of the re-
sulting DSyntSs for our training corpus yielded a
unigram and bigram accuracy (f-score) of approxi-
mately 95% and 90%, respectively.
A DSyntS is an unordered tree where all nodes
are meaning-bearing and lexicalized. Since the out-
put of the Yoon parser is quite similar, we have used
its output as is. The syntactic dependency represen-
tations for two corresponding Korean1 and English
sentences are shown in Figure 1.
3.2 Training and Test Sets of Parse Pairs
The average sentence lengths (in words) and parse
sizes (in nodes) for the 4,183 Korean and English
sentences in our corpus are given in Table 1.
In examining the Korean parses, we found that
many of the larger parses, especially those contain-
ing intra-sentential punctuation, had incorrect de-
pendency assignments, incomplete lemmatization
or were incomplete parses. In examining the En-
glish converted parses, we found that many of
1Korean is represented in romanized format in this paper.
Korean English
Avg. sentence size 6.43 9.36
Avg. parse size 6.43 7.34
Table 2: Average sizes for sentences and parses in
training set
Korean English
Avg. sentence size 7.04 10.58
Avg. parse size 7.02 8.56
Table 3: Average sizes for sentences and parses in
test set
the parses containing intra-sentential punctuation
marks other than commas had incorrect dependency
assignments, due to the limitations of our conver-
sion grammars. Consequently, in our experiments
we have primarily focused on a higher quality sub-
set of 1,483 sentence pairs, automatically selected
by eliminating from the corpus all parse pairs where
one of the parses contained more than 11 content
nodes or involved problematic intra-sentential punc-
tuation.
We divided this higher quality subset into training
and test sets. For the test set, we randomly selected
50 parse pairs containing at least 5 nodes each. For
the training set, we used the remaining 1,433 parse
pairs. The average sentence lengths and parse sizes
for the training and test sets are represented in Ta-
bles 2 and 3.
3.3 Creating the Baseline Transfer Dictionary
In our system, transfer dictionaries contain Ko-
rean to English lexico-structural transfer rules de-
fined using the formalism described in (Nasr et.
al., 1997), extended to include log likelihood ra-
tios (Manning and Schutze, 1999: 172-175). Sam-
ple transfer rules are illustrated in Section 4. The
simplest transfer rules consist of direct lexical map-
pings, while the most complex may contain source
and target syntactic patterns composed of multiple
nodes defined with lexical and/or syntactic features.
Each transfer rule is assigned a log likelihood ratio
calculated using the training parse set.
To create the baseline transfer dictionary for our
experiments, we had three bilingual dictionary re-
sources at our disposal:
A corpus-based handcrafted dictionary: This
dictionary was manually assembled by (Han et
al., 2000) for the same corpus used here. Note,
Korean English
Lexical coverage 92.18% 90.17%
Table 4: Concurrent lexical coverage of training set
by baseline dictionary
however, that it was developed for different
parse representations, and with an emphasis
primarily on the lexical coverage of the source
parses, rather than the source and target parse
pairs.
A corpus-based extracted dictionary: This dic-
tionary was automatically created from our
corpus by the RALI group from the University
of Montreal. Since the extraction heuristics
did not handle the rich morphological suffixes
of Korean, the extraction results contained
inflected words rather than lexemes.
A wide coverage dictionary: This dictionary of
70,300 entries was created by Systran, without
regard to our corpus.
We processed and combined these resources as
follows:
 First, we replaced the inflected words with un-
inflected lexemes using Yoon?s morphological
analyzer and a wide coverage English morpho-
logical database (Karp and Schabes, 1992).
 Second, we merged all morphologically an-
alyzed entries after removing all non-lexical
features, since these features generally did not
match those found in the parses.
 Third, we matched the resulting transfer dictio-
nary entries with the training parse set, in order
to determine for each entry all possible part-of-
speech instantiations and dependency relation-
ships. For each distinct instantiation, we cal-
culated a log likelihood ratio.
 Finally, we created a baseline dictionary us-
ing the instantiated rules whose source patterns
had the best log likelihood ratios.
Table 4 illustrates the concurrent lexical coverage
of the training set using the resulting baseline dictio-
nary, i.e. the percentage of nodes covered by rules
whose source and target patterns both match. Note
that since the baseline dictionary contained some
noise, we allowed induced rules to override ones in
the baseline dictionary where applicable.
@KOREAN:
{po} [class=vbma] (
s1 $X [ppca={reul}] )
@ENGLISH:
look [class=verb] (
attr at [class=preposition] (
ii $X ))
@-2xLOG_LIKELIHOOD: 12.77
Figure 2: Transfer rule for English lexicalization
and preposition insertion
4 Transfer Rule Induction
The transfer rule induction process has the follow-
ing steps described below (additional details can
also be found in (Lavoie et. al., 2001)):
 Nodes of the corresponding source and target
parses are aligned using the baseline transfer
dictionary and some heuristics based on the
similarity of part-of-speech and syntactic con-
text.
 Transfer rule candidates are generated based
on the sub-patterns that contain the corre-
sponding aligned nodes in the source and target
parses.
 The transfer rule candidates are ordered based
on their likelihood ratios.
 The transfer rule candidates are filtered, one at
a time, in the order of the likelihood ratios, by
removing those rule candidates that do not pro-
duce an overall improvement in the accuracy of
the transferred parses.
Figures 2 and 3 show two sample induced rules.
The rule formalism uses notation similar to the
syntactic dependency notation shown in Figure 1,
augmented with variable arguments prefixed with
$ characters. These two lexico-structural rules can
be used to transfer a Korean syntactic representation
for ci-to-reul po-ra to an English syntactic represen-
tation for look at the map. The first rule lexicalizes
the English predicate and inserts the corresponding
preposition while the second rule inserts the English
imperative attribute.
4.1 Aligning the Parse Nodes
To align the nodes in the source and target parse
trees, we devised a new dynamic programming
@KOREAN:
$X [class=vbma ente={ra}]
@ENGLISH:
$X [class=verb mood=imp]
@-2xLOG_LIKELIHOOD: 33.37
Figure 3: Transfer rule for imperative forms
alignment algorithm that performs a top-down, bidi-
rectional beam search for the least cost mapping be-
tween these nodes. The algorithm is parameterized
by the costs of (1) aligning two nodes whose lex-
emes are not found in the baseline transfer dictio-
nary; (2) aligning two nodes with differing parts of
speech; (3) deleting or inserting a node in the source
or target tree; and (4) aligning two nodes whose rel-
ative locations differ.
To determine an appropriate part of speech cost
measure, we first extracted a small set of parse pairs
that could be reliably aligned using lexical matching
alone, and then based the cost measure on the co-
occurrence counts of the observed parts of speech
pairings. The remaining costs were set by hand.
As a result of the alignment process, alignment id
attributes (aid) are added to the nodes of the parse
pairs. Some nodes may be in alignment with no
other node, such as English prepositions not found
in the Korean DSyntS.
4.2 Generating Rule Candidates
Candidate transfer rules are generated by extracting
source and target tree sub-patterns from the aligned
parse pairs using the two set of constraints described
below.
4.2.1 Alignment constraints
Figure 4 shows an example alignment constraint.
This constraint, which matches the structural pat-
terns of the transfer rule illustrated in Figure 2, uses
the aid alignment attribute to indicate that in a Ko-
rean and English parse pair, any source and target
sub-trees matching this alignment constraint (where
$X1 and $Y1 are aligned, i.e. have the same aid at-
tribute values, and where $X2 and $Y3 are aligned)
can be used as a point of departure for generat-
ing transfer rule candidates. We suggest that align-
ment constraints such as this one can be used to de-
fine most of the possible syntactic divergences be-
tween languages (Dorr, 1994), and that only a hand-
ful of them are necessary for two given languages
(we have identified 11 general alignment constraints
@KOREAN:
$X1 [aid=$1] (
$R1 $X2 [aid=$2] )
@ENGLISH:
$Y1 [aid=$1] (
$R2 $Y2 (
$R3 $Y3 [aid=$2] ) )
Figure 4: Alignment constraint
Each node of a candidate transfer rule must have
its relation attribute (relationship with its governor)
specified if it is an internal node, otherwise this relation
must not be specified:
e.g.
 $X1 ( $R $X2 )
Figure 5: Independent attribute constraint
necessary for Korean to English transfer so far).
4.2.2 Attribute constraints
Attribute constraints are used to limit the space of
possible transfer rule candidates that can be gen-
erated from the sub-trees satisfying the alignment
constraints. Candidate transfer rules must satisfy all
of the attribute constraints. Attribute constraints can
be divided into two types:
 independent attribute constraints, whose scope
covers only one part of a candidate transfer rule
and which are the same for the source and tar-
get parts;
 concurrent attribute constraints, whose scope
extends to both the source and target parts of a
candidate transfer rule.
Figures 5 and 6 give examples of an indepen-
dent attribute constraint and of a concurrent attribute
constraint. As with the alignment constraints, we
suggest that a relatively small number of attribute
constraints is necessary to generate most of the de-
sired rules for a given language pair.
4.3 Ordering Rule Candidates
In the next step, transfer rule candidates are ordered
as follows. First, they are sorted by their decreasing
log likelihood ratios. Second, if two or more can-
didate transfer rules have the same log likelihood
ratio, ties are broken by a specificity heuristic, with
the result that more general rules are ordered ahead
In a candidate transfer rule, inclusion of the lexemes of
two aligned nodes must be done concurrently:
e.g.
$X [aid=$1]
and
$Y [aid=$1]
e.g.
 [aid=$1]
and
 [aid=$1]
Figure 6: Concurrent attribute constraint
of more specific ones. The specificity of a rule is
defined to be the following sum: the number of at-
tributes found in the source and target patterns, plus
1 for each for each lexeme attribute and for each de-
pendency relationship. In our initial experiments,
this simple heuristic has been satisfactory.
4.4 Filtering Rule Candidates
Once the candidate transfer rules have been ordered,
error-driven filtering is used to select those that yield
improvements over the baseline transfer dictionary.
The algorithm works as follows. First, in the initial-
ization step, the set of accepted transfer rules is set
to just those appearing in the baseline transfer dic-
tionary. The current error rate is also established, by
applying these transfer rules to all the source struc-
tures and calculating the overall difference between
the resulting transferred structures and the target
parses, using a tree accuracy recall and precision
measure (determined by comparing the features and
dependency relationships in the transferred parses
and corresponding target parses). Then, in a sin-
gle pass through the ordered list of candidates, each
transfer rule candidate is tested to see if it reduces
the error rate. During each iteration, the candidate
transfer rule is provisionally added to the current set
of accepted rules and the updated set is applied to
all the source structures. If the overall difference be-
tween the transferred structures and the target parses
is lower than the current error rate, then the can-
didate is accepted and the current error rate is up-
dated; otherwise, the candidate is rejected and re-
moved from the current set.
4.5 Discussion of Induced Rules
In our experiments, the alignment constraints
yielded 22,881 source and target sub-tree pairs from
the training set of 1,433 parse pairs. Using the at-
@KOREAN:
{po} [class=vbma ente={ra}] (
s1 $X [ppca={reul}] )
@ENGLISH:
look [class=verb mood=imp] (
attr at [class=preposition] (
ii $X ) )
@-2xLOG_LIKELIHOOD: 11.40
Figure 7: Transfer rule for English imperative with
lexicalization and preposition insertion
tribute constraints, an initial list of 801,674 trans-
fer rule candidates was then generated from these
sub-tree pairs. The initial list was subsequently re-
duced to 32,877 unique transfer rule candidates by
removing duplicates and by eliminating candidates
that had the same source pattern as another candi-
date with a better log likelihood ratio. After filter-
ing, 2,133 of these transfer rule candidates were ac-
cepted. We expect that the number of accepted rules
per parse pair will decrease with larger training sets,
though this remains to be verified.
The rule illustrated in Figure 3 was accepted as
the 65th best transfer rule with a log likelihood ra-
tio of 33.37, and the rule illustrated in Figure 2 was
accepted as the 189th best transfer rule candidate
with a log likelihood ratio of 12.77. An example
of a candidate transfer rule that was not accepted is
the one that combines the features of the two rules
mentioned above, illustrated in Figure 7. This trans-
fer rule candidate had a lower log likelihood ratio of
11.40; consequently, it is only considered after the
two rules mentioned above, and since it provides no
further improvement upon these two rules, it is fil-
tered out.
In an informal inspection of the top 100 accepted
transfer rules, we found that most of them appear
to be fairly general rules that would normally be
found in a general syntactic-based transfer dictio-
nary. In looking at the remaining rules, we found
that the rules tended to become increasingly corpus-
specific.
The induction results were obtained using a Java
implementation of the induction component. Mi-
crosoft SQL Server was used to count and dedupli-
cate the rule candidates. The data preparation and
induction processes took about 12 hours on a 300
MHz PC with 256 MB RAM.
5 Evaluation
5.1 Systems Compared
Babelfish As a first baseline, we used Babelfish
from Systran, a commercial large coverage MT
system supporting Korean to English transla-
tion. This system was not trained on our cor-
pus.
GIZA++/RW As a second baseline, we used an
off-the-shelf statistical MT system, consisting
of the ISI ReWrite Decoder (Germann et al,
2001) together with a translation model pro-
duced by GIZA++ (Och and Ney, 2000) and
a language model produced by the CMU Sta-
tistical Language Modeling Toolkit (Clarkson
and Rosenfeld, 1997). This system was trained
on our corpus only.
Lex Only As a third baseline, we used our system
with the baseline transfer dictionary as the sole
transfer resource.
Lex+Induced We compared the three baseline sys-
tems against our complete system, using the
baseline transfer dictionary augmented with
the induced transfer rules.
We ran each of the four systems on the test set
of 50 Korean sentences described in Section 3.2
and compared the resulting translations using the
automatic evaluation and the human evaluation de-
scribed below.
5.2 Automatic Evaluation Results
For the automatic evaluation, we used the Bleu met-
ric from IBM (Papineni et al, 2001). The Bleu
metric combines several modified N-gram precision
measures (N = 1 to 4), and uses brevity penalties to
penalize translations that are shorter than the refer-
ence sentences.
Table 5 shows the Bleu N-gram precision scores
for each of the four systems. Our system
(Lex+Induced) had better precision scores than each
of the baseline systems, except in the case of 4-
grams, where it slightly trailed Babelfish. The sta-
tistical baseline system (GIZA++/RW) performed
poorly, as might have been expected given the small
amount of training data.
Table 6 shows the Bleu overall precision scores.
Our system (Lex+Induced) improved substantially
over both the Lex Only and Babelfish baseline sys-
tems. The score for the statistical baseline system
(GIZA++/RW) is not meaningful, due to the ab-
sence of 3-gram and 4-gram matches.
System 1-g Prec 2-g Prec 3-g Prec 4-g Prec
Babelfish 0.3814 0.1207 0.0467 0.0193
GIZA++/RW 0.1894 0.0173 0.0 0.0
Lex Only 0.4234 0.1252 0.0450 0.0145
Lex+Induced 0.4725 0.1618 0.0577 0.0185
Table 5: Bleu N-gram precision scores
System Bleu Score
Babelfish 0.0802
GIZA++/RW NA
Lex Only 0.0767
Lex+Induced 0.0950
Table 6: Bleu overall precision scores
5.3 Human Evaluation Results
For the human evaluation, we asked two English
native speakers to rank the quality of the transla-
tion results produced by the Babelfish, Lex Only
and Lex+Induced, with preference given to fidelity
over fluency. (The translation results of the statisti-
cal system were not yet available when the evalua-
tion was performed.) A rank of 1 was assigned to
the best translation, a rank of two to the second best
and a rank of 3 to the third, with ties allowed.
Table 7 shows the pairwise comparisons of the
three systems. The top section indicates that the
Babelfish and Lex Only baseline systems are es-
sentially tied, with neither system preferred more
frequently than the other. In contrast, the middle
and bottom sections show that our system improves
substantially over both baseline systems; most strik-
ingly, our system (Lex+Induced) was preferred al-
most 20% more frequently than the Babelfish base-
line (46% to 27%, with ties 27% of the time).
System Pair Comparison Result
Babelfish better than Lex Only 37%
Lex Only better than Babelfish 36%
Babelfish same as Lex Only 27%
Babelfish better than Lex+Induced 27%
Lex+Induced better than Babelfish 46%
Babelfish same as Lex+Induced 27%
Lex Only better than Lex+Induced 18%
Lex+Induced better than Lex Only 41%
Lex Only same as Lex+Induced 41%
Table 7: Human evaluation results
6 Conclusion and Future Work
In this paper we have described the design of an MT
system based on lexico-structural transfer rules in-
duced from parsed bitexts. In a small scale exper-
iment with Korean to English translation, we have
demonstrated a substantial improvement over three
baseline systems, including a nearly 20% improve-
ment in the preference rate for our system over Ba-
belfish (which was not trained on our corpus). Al-
though our experimentation was aimed at Korean
to English translation, we believe that our approach
can be readily applied to other language pairs.
It remains for future work to explore how well
the approach would fare with a much larger train-
ing corpus. One foreseeable problem concerns the
treatment of lengthy training sentences: since the
number of transfer rule candidates generated grows
exponentially with the size of the parse tree pairs,
refinements will be necessary in order to make use
of complex sentences; one option might be to auto-
matically chunk longer sentences into smaller units.
Acknowledgements
We thank Richard Kittredge for helpful discussion, Daryl Mc-
Cullough and Ted Caldwell for their help with evaluation and
Fei Xia for her assistance with the automatic conversion of
the phrase structure parses to syntactic dependency represen-
tations. We also thank Chung-hye Han, Chulwoo Park, Martha
Palmer, and Joseph Rosenzweig for the handcrafted Korean-
English transfer dictionary, and Graham Russell for the corpus-
based extracted transfer dictionary. This work has been par-
tially supported by DARPA TIDES contract no. N66001-00-C-
8009.
References
Philip Clarkson and Ronald Rosenfeld. 1997. Statistical Lan-
guage Modeling Using the CMU-Cambridge Toolkit. In
Proceedings of Eurospeech?97.
Bonnie Dorr. 1994. Machine translation divergences: A for-
mal description and proposed solution. Computational Lin-
guistics, 20(4):597?635.
Ulrich Germann, Michael Jahr, Kevin Knight, Daniel Marcu
and Kenji Yamada. 2001. Fast Decoding and Optimal De-
coding for Machine Translation. In Proceedings of ACL?01,
Toulouse, France.
Chung hye Han, Benoit Lavoie, Martha Palmer, Owen
Rambow, Richard Kittredge, Tanya Korelsky, Nari Kim,
and Myunghee Kim. 2000. Handling Structural Diver-
gences and Recovering Dropped Arguments in a Korean-
English Machine Translation System. In Proceedings of the
Fourth Conference on Machine Translation in the Americas
(AMTA?00), Mision Del Sol, Mexico.
Daniel Karp and Yves Schabes. 1992. A Wide Coverage
Public Domain Morphological Analyzer for English. In
Proceedings of the Fifteenth International Conference on
Computational Linguistics (COLING?92), pages 950?955,
Nantes, France.
Benoit Lavoie and Owen Rambow. 1997. RealPro ? A Fast,
Portable Sentence Realizer. In Proceedings of the Confer-
ence on Applied Natural Language Processing (ANLP?97),
Washington, DC.
Benoit Lavoie, Richard Kittredge, Tanya Korelsky, and Owen
Rambow. 2000. A framework for MT and multilingual
NLG systems based on uniform lexico-structural process-
ing. In Proceedings of ANLP/NAACL 2000, Seattle, Wash-
ington.
Benoit Lavoie, Michael White, and Tanya Korelsky. 2001. In-
ducing Lexico-Structural Transfer Rules from Parsed Bi-
texts. In Proceedings of the ACL 2001 Workshop on
Data-driven Machine Translation, pages 17?24, Toulouse,
France.
Christopher D. Manning and Hinrich Schutze. 1999. Foun-
dations of Statistical Natural Language Processing. MIT
Press.
H. Maruyama and H. Watanabe. 1992. Tree cover Search
Algorithm for Example-Based Translation. In Proceedings
of the Fourth International Conference on Theoretical and
Methodological Issues in Machine Translation (TMI?92),
pages 173?184.
Yuji Matsumoto, Hiroyuki Hishimoto, and Takehito Utsuro.
1993. Structural Matching of Parallel Texts. In Proceedings
of the 31st Annual Meetings of the Association for Compu-
tational Linguistics (ACL?93), pages 23?30.
Igor Mel?c?uk. 1988. Dependency Syntax. State University of
New York Press, Albany, NY.
Adam Meyers, Roman Yangarber, Ralph Grishman, Catherine
Macleod, and Antonio Moreno-Sandoval. 1998. Deriving
Transfer Rules from Dominance-Preserving Alignments. In
Proceedings of COLING-ACL?98, pages 843?847.
Makoto Nagao. 1984. A framework of a mechanical transla-
tion between Japenese and English by analogy principle. In
A. Elithorn and R. Banerji, editors, Artificial and Human
Intelligence. NATO Publications.
Alexis Nasr, Owen Rambow, Martha Palmer, and Joseph
Rosenzweig. 1997. Enriching lexical transfer with cross-
linguistic semantic features. In Proceedings of the Interlin-
gua Workshop at the MT Summit, San Diego, California.
Franz Josef Och and Hermann Ney. 2000. Improved Statistical
Alignment Models. In Proceedings of ACL?00, pages 440?
447, Hongkong, China.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
Zhu. 2001. Bleu: a Method for Automatic Evaluation of
Machine Translation. In IBM technical report, RC22176.
Stephen D. Richardson, William B. Dolan, Arul Menezes, and
Monica Corston-Oliver. 2001. Overcoming the Customiza-
tion Bottleneck using Example-based MT. In Proceedings
of the ACL 2001 Workshop on Data-driven Machine Trans-
lation, pages 9?16, Toulouse, France.
Satoshi Sato and Makoto Nagao. 1990. Toward Memory-
based Translation. In Proceedings of the 13th International
Conference on Computational Linguistics (COLING?90),
pages 247?252.
Fei Xia and Martha Palmer. 2001. Converting Dependency
Structures to Phrase Structures. In Notes of the First Human
Language Technology Conference, San Diego, California.
Juntae Yoon, Seonho Kim, and Mansuk Song. 1997. New
parsing method using global association table. In Proceed-
ings of the 5th International Workshop on Parsing Technol-
ogy.
Coling 2010: Poster Volume, pages 1032?1040,
Beijing, August 2010
Designing Agreement Features for Realization Ranking
Rajakrishnan Rajkumar and Michael White
Department of Linguistics
The Ohio State University
{raja,mwhite}@ling.osu.edu
Abstract
This paper shows that incorporating lin-
guistically motivated features to ensure
correct animacy and number agreement in
an averaged perceptron ranking model for
CCG realization helps improve a state-of-
the-art baseline even further. Tradition-
ally, these features have been modelled us-
ing hard constraints in the grammar. How-
ever, given the graded nature of grammat-
icality judgements in the case of animacy
we argue a case for the use of a statisti-
cal model to rank competing preferences.
Though subject-verb agreement is gener-
ally viewed to be syntactic in nature, a pe-
rusal of relevant examples discussed in the
theoretical linguistics literature (Kathol,
1999; Pollard and Sag, 1994) points to-
ward the heterogeneous nature of English
agreement. Compared to writing gram-
mar rules, our method is more robust and
allows incorporating information from di-
verse sources in realization. We also show
that the perceptron model can reduce bal-
anced punctuation errors that would other-
wise require a post-filter. The full model
yields significant improvements in BLEU
scores on Section 23 of the CCGbank and
makes many fewer agreement errors.
1 Introduction
In recent years a variety of statistical models for
realization ranking that take syntax into account
have been proposed, including generative mod-
els (Bangalore and Rambow, 2000; Cahill and
van Genabith, 2006; Hogan et al, 2007; Guo et
al., 2008), maximum entropy models (Velldal and
Oepen, 2005; Nakanishi et al, 2005) and averaged
perceptron models (White and Rajkumar, 2009).
To our knowledge, however, none of these mod-
els have included features specifically designed to
handle grammatical agreement, an important task
in surface realization. In this paper, we show that
incorporating linguistically motivated features to
ensure correct animacy and verbal agreement in
an averaged perceptron ranking model for CCG
realization helps improve a state-of-the-art base-
line even further. We also demonstrate the utility
of such an approach in ensuring the correct pre-
sentation of balanced punctuation marks.
Traditionally, grammatical agreement phenom-
ena have been modelled using hard constraints
in the grammar. Taking into consideration the
range of acceptable variation in the case of ani-
macy agreement and facts about the variety of fac-
tors contributing to number agreement, the ques-
tion arises: tackle agreement through grammar
engineering, or via a ranking model? In our
experience, trying to add number and animacy
agreement constraints to a grammar induced from
the CCGbank (Hockenmaier and Steedman, 2007)
turned out to be surprisingly difficult, as hard con-
straints often ended up breaking examples that
were working without such constraints, due to ex-
ceptions, sub-regularities and acceptable variation
in the data. With sufficient effort, it is conceiv-
able that an approach incorporating hard agree-
ment constraints could be refined to underspec-
ify cases where variation is acceptable, but even
so, one would want a ranking model to capture
preferences in these cases, which might vary de-
pending on genre, dialect or domain. Given that
1032
a ranking model is desirable in any event, we in-
vestigate here the extent to which agreement phe-
nomena can be more robustly and simply handled
using a ranking model alone, with no hard con-
straints in the grammar.
We also show here that the perceptron model
can reduce balanced punctuation errors that would
otherwise require a post-filter. As White and Ra-
jkumar (2008) discuss, in CCG it is not feasible
to use features in the grammar to ensure that bal-
anced punctuation (e.g. paired commas for NP ap-
positives) is used in all and only the appropriate
places, given the word-order flexibility that cross-
ing composition allows. While a post-filter is a
reasonably effective solution, it can be prone to
search errors and does not allow balanced punctu-
ation choices to interact with other choices made
by the ranking model.
The starting point for our work is a CCG re-
alization ranking model that incorporates Clark &
Curran?s (2007) normal-form syntactic model, de-
veloped for parsing, along with a variety of n-
gram models. Although this syntactic model plays
an important role in achieving top BLEU scores
for a reversible, corpus-engineered grammar, an
error analysis nevertheless revealed that many er-
rors in relative pronoun animacy agreement and
subject-verb number agreement remain with this
model. In this paper, we show that features specif-
ically designed to better handle these agreement
phenomena can be incorporated into a realization
ranking model that makes many fewer agreement
errors, while also yielding significant improve-
ments in BLEU scores on Section 23 of the CCG-
bank. These features make use of existing corpus
annotations ? specifically, PTB function tags and
BBN named entity classes (Weischedel and Brun-
stein, 2005) ? and thus they are relatively easy to
implement.
1.1 The Graded Nature of Animacy
Agreement
To illustrate the variation that can be found with
animacy agreement phenomena, consider first an-
imacy agreement with relative pronouns. In En-
glish, an inanimate noun can be modified by a rel-
ative clause introduced by that or which, while an
animate noun combines with who(m). With some
nouns though ? such as team, group, squad, etc.
? animacy status is uncertain, and these can be
found with all the three relative pronouns (who,
which and that). Google counts suggest that all
three choices are almost equally acceptable, as the
examples below illustrate:
(1) The groups who protested against plans to
remove asbestos from the nuclear subma-
rine base at Faslane claimed victory when
it was announced the government intends
to dispose of the waste on site. (The Glas-
gow Herald; Jun 25, 2010)
(2) Mr. Dorsch says the HIAA is work-
ing on a proposal to establish a privately
funded reinsurance mechanism to help
cover small groups that ca n?t get insur-
ance without excluding certain employees
. (WSJ0518.35)
1.2 The Heterogeneous Nature of Number
Agreement
Subject-verb agreement can be described as a con-
straint where the verb agrees with the subject in
terms of agreement features (number and person).
Agreement has often been considered to be a syn-
tactic phenomenon and grammar implementations
generally use syntactic features to enforce agree-
ment constraints (e.g. Velldal and Oepen, 2005).
However a closer look at our data and a survey
of the theoretical linguistics literature points to-
ward a more heterogeneous conception of English
agreement. Purely syntactic accounts are prob-
lematic when the following examples are consid-
ered:
(3) Five miles is a long distance to walk.
(Kim, 2004)
(4) King prawns cooked in chili salt and pep-
per was very much better, a simple dish
succulently executed. (Kim, 2004)
(5) ? I think it will shake confidence one more
time , and a lot of this business is based on
client confidence . ? (WSJ1866.10)
(6) It ?s interesting to find that a lot of the ex-
pensive wines are n?t always walking out
the door . (WSJ0071.53)
1033
In Example (3) above, the subject and deter-
miner are plural while the verb is singular. In
(4), the singular verb agrees with the dish, rather
than with individual prawns. Measure nouns such
as lot, ton, etc. exhibit singular agreement with
the determiner a, but varying agreement with the
verb depending on the head noun of the measure
noun?s of -complement. As is also well known,
British and American English differ in subject-
verb agreement with collective nouns. Kathol
(1999) proposes an explanation where agreement
is determined by the semantic properties of the
noun rather than by its morphological properties.
This accounts for all the cases above. In the light
of this explanation, specifying agreement features
in the logical form for realization could perhaps
solve the problem. However, the semantic view
of agreement is not completely convincing due to
counterexamples like the following discussed in
the literature (reported in Kim (2004)):
(7) Suppose you meet someone and they are
totally full of themselves
(8) Those scissors are missing.
In Example (7), the pronoun they used in a
generic sense is linked to the singular antecedent
someone, but its plural feature triggers plural
agreement with the verb. Example (8) illustrates a
situation where the subject scissors is arguably se-
mantically singular, but exhibits plural morphol-
ogy and plural syntactic agreement with both the
determiner as well as the verb. Thus this suggests
that English has a set of heterogeneous agree-
ment patterns rather than purely syntactic or se-
mantic ones. This is also reflected in the pro-
posal for a hybrid agreement system for English
(Kim, 2004), where the morphology tightly in-
teracts with the system of syntax, semantics, or
even pragmatics to account for agreement phe-
nomena. Our machine learning-based approach
approximates the insights discussed in the theoret-
ical linguistics literature. Writing grammar rules
to get these facts right proved to be surprisingly
difficult (e.g. discerning the actual nominal head
contributing agreement feature in cases like areas
of the factory were/*was vs. a lot of wines are/*is)
and required a list of measure nouns and parti-
tive quantifiers. We investigate here the extent
to which a machine learning?based approach is a
simpler, practical alternative for acquiring the rel-
evant generalizations from the data by combining
information from various information sources.
The paper is structured as follows. Section 2
provides CCG background. Section 3 describes
the features we have designed for animacy and
number agreement as well as for balanced punc-
tuation. Section 4 presents our evaluation of the
impact of these features in averaged perceptron re-
alization ranking models, tabulating specific kinds
of errors in the CCGbank development section as
well as overall automatic metric scores on Sec-
tion 23. Section 5 compares our results to those
obtained with related systems. Finally, Section 6
concludes with a summary of the paper?s contri-
butions.
2 Background
2.1 Surface Realization with Combinatory
Categorial Grammar (CCG)
CCG (Steedman, 2000) is a unification-based cat-
egorial grammar formalism which is defined al-
most entirely in terms of lexical entries that en-
code sub-categorization information as well as
syntactic feature information (e.g. number and
agreement). Complementing function application
as the standard means of combining a head with its
argument, type-raising and composition support
transparent analyses for a wide range of phenom-
ena, including right-node raising and long dis-
tance dependencies. An example syntactic deriva-
tion appears in Figure 1, with a long-distance
dependency between point and make. Seman-
tic composition happens in parallel with syntactic
composition, which makes it attractive for gener-
ation.
OpenCCG is a parsing/generation library which
works by combining lexical categories for words
using CCG rules and multi-modal extensions on
rules (Baldridge, 2002) to produce derivations.
Conceptually these extensions are on lexical cate-
gories. Surface realization is the process by which
logical forms are transduced to strings. OpenCCG
uses a hybrid symbolic-statistical chart realizer
(White, 2006) which takes logical forms as in-
put and produces sentences by using CCG com-
1034
He has a point he wants to make
np sdcl\np/np np/n n np sdcl\np/(sto\np) sto\np/(sb\np) sb\np/np
> >T >B
np s/(s\np) sto\np/np
>B
sdcl\np/np
>B
sdcl/np
np\np
<np
>
sdcl\np
<sdcl
Figure 1: Syntactic derivation from the CCGbank for He has a point he wants to make [. . . ]
aa1
heh3
he h2
<Det>
<Arg0> <Arg1>
<TENSE>pres
<NUM>sg
<Arg0>
w1 want.01
m1
<Arg1>
<GenRel>
<Arg1>
<TENSE>pres
p1point
h1have.03
make.03
<Arg0>
s[b]\np/np
np/n
np
n
s[dcl]\np/np
s[dcl]\np/(s[to]\np)
np
Figure 2: Semantic dependency graph from the
CCGbank for He has a point he wants to make
[. . . ], along with gold-standard supertags (cate-
gory labels)
binators to combine signs. Edges are grouped
into equivalence classes when they have the same
syntactic category and cover the same parts of
the input logical form. Alternative realizations
are ranked using integrated n-gram or perceptron
scoring, and pruning takes place within equiva-
lence classes of edges. To more robustly support
broad coverage surface realization, OpenCCG
greedily assembles fragments in the event that the
realizer fails to find a complete realization.
To illustrate the input to OpenCCG, consider
the semantic dependency graph in Figure 2. In
the graph, each node has a lexical predication
(e.g. make.03) and a set of semantic features
(e.g. ?NUM?sg); nodes are connected via depen-
dency relations (e.g. ?ARG0?). (Gold-standard su-
pertags, or category labels, are also shown; see
Section 2.2 for their role in hypertagging.) In-
ternally, such graphs are represented using Hy-
brid Logic Dependency Semantics (HLDS), a
dependency-based approach to representing lin-
guistic meaning (Baldridge and Kruijff, 2002). In
HLDS, each semantic head (corresponding to a
node in the graph) is associated with a nominal
that identifies its discourse referent, and relations
between heads and their dependents are modeled
as modal relations.
For our experiments, we use an enhanced ver-
sion of the CCGbank (Hockenmaier and Steed-
man, 2007)?a corpus of CCG derivations derived
from the Penn Treebank?with Propbank (Palmer
et al, 2005) roles projected onto it (Boxwell and
White, 2008). Additionally, certain multi-word
NEs were collapsed using underscores so that they
are treated as atomic entities in the input to the
realizer. To engineer a grammar from this cor-
pus suitable for realization with OpenCCG, the
derivations are first revised to reflect the lexical-
ized treatment of coordination and punctuation as-
sumed by the multi-modal version of CCG that is
implemented in OpenCCG (White and Rajkumar,
2008). Further changes are necessary to support
semantic dependencies rather than surface syntac-
tic ones; in particular, the features and unifica-
tion constraints in the categories related to seman-
tically empty function words such complemen-
tizers, infinitival-to, expletive subjects, and case-
marking prepositions are adjusted to reflect their
purely syntactic status.
1035
2.2 Hypertagging
A crucial component of the OpenCCG realizer is
the hypertagger (Espinosa et al, 2008), or su-
pertagger for surface realization, which uses a
maximum entropy model to assign the most likely
lexical categories to the predicates in the input
logical form, thereby greatly constraining the real-
izer?s search space.1 Category label prediction is
done at run-time and is based on contexts within
the directed graph structure as shown in Figure 2,
instead of basing category assignment on linear
word and POS context as in the parsing case.
3 Feature Design
The features we employ in our baseline perceptron
ranking model are of three kinds. First, as in the
log-linear models of Velldal & Oepen and Nakan-
ishi et al, we incorporate the log probability of the
candidate realization?s word sequence according
to our linearly interpolated language models as a
single feature in the perceptron model. Since our
language model linearly interpolates three com-
ponent models, we also include the log prob from
each component language model as a feature so
that the combination of these components can be
optimized. Second, we include syntactic features
in our model by implementing Clark & Curran?s
(2007) normal form model in OpenCCG. The fea-
tures of this model are listed in Table 1; they
are integer-valued, representing counts of occur-
rences in a derivation. Third, we include dis-
criminative n-gram features (Roark et al, 2004),
which count the occurrences of each n-gram that
is scored by our factored language model, rather
than a feature whose value is the log probability
determined by the language model. Table 2 de-
picts the new animacy, agreement and punctuation
features being introduced as part of this work. The
next two sections describe these features in more
detail.
3.1 Animacy and Number Agreement
Underspecification as to the choice of pronoun in
the input leads to competing realizations involv-
ing the relative pronouns who, that, which etc. The
1The approach has been dubbed hypertagging since it op-
erates at a level ?above? the syntax, moving from semantic
representations to syntactic categories.
Feature Type Example
LexCat + Word s/s/np + before
LexCat + POS s/s/np + IN
Rule sdcl ? np sdcl\np
Rule + Word sdcl ? np sdcl\np + bought
Rule + POS sdcl ? np sdcl\np + VBD
Word-Word ?company, sdcl ? np sdcl\np, bought?
Word-POS ?company, sdcl ? np sdcl\np, VBD?
POS-Word ?NN, sdcl ? np sdcl\np, bought?
Word + ?w ?bought, sdcl ? np sdcl\np? + dw
POS + ?w ?VBD, sdcl ? np sdcl\np? + dw
Word + ?p ?bought, sdcl ? np sdcl\np? + dp
POS + ?p ?VBD, sdcl ? np sdcl\np? + dp
Word + ?v ?bought, sdcl ? np sdcl\np? + dv
POS + ?v ?VBD, sdcl ? np sdcl\np? + dv
Table 1: Baseline features: Basic and dependency
features from Clark & Curran?s (2007) normal
form model; distances are in intervening words,
punctuation marks and verbs, and are capped at 3,
3 and 2, respectively
Feature Example
Animacy features
Noun Stem + Wh-pronoun researcher + who
Noun Class + Wh-pronoun PER DESC + who
Number features
Noun + Verb people + are
NounPOS + Verb NNS + are
Noun + VerbPOS people + VBP
NounPOS + VerbPOS NNS + VBP
Noun of + Verb lot of + are
Noun of + VerbPOS lot of + VBP
NounPOS of + Verb NN of + are
NounPOS of + VerbPOS NN of + VBP
Noun of + of-complementPOS + VerbPOS lot of + NN + VBZ
NounPOS of + of-complementPOS + VerbPOS NN of + NN + VBZ
Noun of + of-complementPOS + Verb lot of + NN + is
NounPOS of + of-complementPOS + Verb NN of + NN + is
Punctuation feature
Balanced Punctuation Indicator $unbalPunct=1
Table 2: New features introduced
existing ranking models (n-gram models as well
as perceptron) often allow the top-ranked output
to have the relative pronoun that associated with
animate nouns. The existing normal form model
uses the word forms as well as part-of-speech tag
based features. Though this is useful for associ-
ating proper nouns (tagged NNP or NNPS) with
who, for other nouns (as in consumers who vs.
consumers that/which), the model often prefers
the infelicitous pronoun. So here we designed fea-
tures which also took into account the named en-
tity class of the head noun as well as the stem of
the head noun. These features aid the discrimi-
native n-gram features (PERSON, which has high
negative weight). As the results section discusses,
1036
NE classes like PER DESC contribute substan-
tially towards animacy preferences.
For number agreement, we designed three
classes of features (c.f. Number Agr row in Table
2). Each of these classes results in 4 features. Dur-
ing feature extraction, subjects of the verbs tagged
VBZ and VBP and verbs was, were were iden-
tified using the PTB NP-SBJ function tag anno-
tation projected on to the appropriate arguments
of lexical categories of verbs. The first class
of features encoded all possible combinations of
subject-verb word forms and parts of speech tags.
In the case of NPs involving of-complements like
a lot of ... (Examples 5 and 6), feature classes 2
and 3 were extracted (class 1 was excluded). Class
2 features encode the fact that the syntactic head
has an associated of-complement, while class 3
features also include the part of speech tag of the
complement. In the case of conjunct/disjunct VPs
and subject NPs, the feature specifically looked
at the parts of speech of both the NPs/VPs form-
ing the conjunct/disjunct. The motivation behind
such a design was to glean syntactic and semantic
generalizations from the data. During feature ex-
traction, from each derivation, counts of animacy
and agreement features were obtained.
3.2 Balanced Punctuation
A complex issue that arises in the design of bi-
directional grammars is ensuring the proper pre-
sentation of punctuation. Among other things, this
involves the task of ensuring the correct realiza-
tion of commas introducing noun phrase apposi-
tives.
(9) John, CEO of ABC, loves Mary.
(10) * John, CEO of ABC loves Mary.
(11) Mary loves John, CEO of ABC.
(12) * Mary loves John, CEO of ABC,.
(13) Mary loves John, CEO of ABC, madly.
(14) * Mary loves John, CEO of ABC madly.
As of now, n-gram models rule out examples
like 12 above. All the other unacceptable ex-
amples are ruled out using a post-filter on real-
ized derivations. As described in White and Ra-
jkumar (2008), the need for the filter arises be-
cause a feature-based approach appears to be in-
adequate for dealing with the class of examples
presented above in CCG. This approach involves
the incorporation of syntactic features for punctu-
ation into atomic categories so that certain combi-
nations are blocked. To ensure proper appositive
balancing sentence finally, the rightmost element
in the sentence should transmit a relevant fea-
ture to the clause level, which the sentence-final
period can then check for the presence of right-
edge punctuation. However, the feature schema
does not constrain cases of balanced punctuation
in cases involving crossing composition and ex-
traction. However, in this paper we explore a sta-
tistical approach to ensure proper balancing of NP
apposition commas. The first step in this solution
is the introduction of a feature in the grammar
which indicates balanced vs. unbalanced marks.
We modified the result categories of unbalanced
appositive commas and dashes to include a fea-
ture marking unbalanced punctuation, as follows:
(15) , ` np?1?unbal=comma\?np?1?/?np?2?
Then, during feature extraction, derivations
were examined to detect categories such as
npunbal=comma , and checked to make sure this NP
is followed by another punctuation mark in the
string such as a full stop. The feature indicates the
presence or absence of unbalanced punctuation in
the derivation.
4 Evaluation
4.1 Experimental Conditions
For the experiments reported below, we used a
lexico-grammar extracted from Sections 02?21 of
our enhanced CCGbank with collapsed NEs, a
hypertagging model incorporating named entity
class features, and a trigram factored language
model over words, named entity classes, part-of-
speech tags and supertags. Perceptron training
events were generated for each training section
separately. The hypertagger and POS/supertag
language model were trained on all the training
sections, while separate word-based models were
trained excluding each of the training sections in
turn. Event files for 26530 training sentences with
complete realizations were generated, with an av-
erage n-best list size of 18.2. The complete set of
models is listed in Table 3.
1037
Model Description
full-model All the feats from models below
agr-punct Baseline Feats + Punct + Num-Agr
wh-punct Baseline Feats + Punct + Animacy-Agr
baseline-punct Baseline Feats + Punct
baseline Log prob + n-gram +Syntactic features
Table 3: Legend for experimental conditions
4.2 Results
Realization results on the development and test
sections are given in Table 4. For the develop-
ment section, in terms of both exact matches and
BLEU scores, the model with all the three features
discussed above (agreement, animacy and punc-
tuation) performs better than the baseline which
does not have any of these features. However, us-
ing these criteria, the best performing model is ac-
tually the model which has agreement and punc-
tuation features. The model containing all the
features does better than the punctuation-feature
only model, but performs slightly worse than the
agreement-punctuation model. Section 23, the
test section, confirms that the model with all the
features performs better than the baseline model.
We calculated statistical significance for the main
results using bootstrap random sampling.2 Af-
ter re-sampling 1000 times, significance was cal-
culated using a paired t-test (999 d.f.). The re-
sults indicated that the model with all the fea-
tures in it (full-model) exceeded the baseline with
p < 0.0001 . However, exact matches and
BLEU scores do not necessarily reflect the extent
to which important grammatical flaws have been
reduced. So to judge the effectiveness of the new
features, we computed the percentage of errors of
each type that were present in the best Section 00
realization selected by each of these models. Also
note that our baseline results differ slightly from
the corresponding results reported in White and
Rajkumar (2009) in spite of using the same feature
set because quotes were introduced into the cor-
pus on which these experiments were conducted.
Previous results were based on the original CCG-
bank text where quotation marks are absent.
Table 6 reports results of the error analysis. It
2Scripts for running these tests are available at
http://projectile.sv.cmu.edu/research/
public/tools/bootStrap/tutorial.htm
Section Model %Exact %Compl. BLEU
00 baseline 38.18 82.47 0.8341
baseline-punct 37.97 82.47 0.8340
wh-punct 38.93 82.53 0.8360
full-model 40.47 82.53 0.8403
agr-punct 40.84 82.53 0.8414
23 baseline 38.98 83.39 0.8442
full-model 40.09 83.35 0.8446
Table 4: Results (98.9% coverage)?percentage
of exact match and grammatically complete real-
izations and BLEU scores
Model METEOR TERP
baseline 0.9819 0.0939
baseline-punct 0.9819 0.0939
wh-punct 0.9827 0.0923
agr-punct 0.9821 0.0902
full-model 0.9826 0.0909
Table 5: Section 00 METEOR and TERP scores
can be seen that the punctuation-feature is effec-
tive in reducing the number of sentences with un-
balanced punctuation marks. Similarly, the full
model has fewer animacy mismatches and just
about the same number of errors of the other two
types, though it performs slightly worse than the
agreement-only model in terms of BLEU scores
and exact matches. We also manually examined
the remaining cases of animacy agreement errors
in the output of the full model here. Of the remain-
ing 18 errors, 14 were acceptable paraphrases in-
volving object relative clauses (eg. wsj 0083.40 ...
the business that/? a company can generate). We
also provide METEOR and TERP scores for these
models (Table 5). In recently completed work on
the creation of a human-rated paraphrase corpus
to evaluate NLG systems, our analyses showed
that BLEU, METEOR and TERP scores correlate
moderately with human judgments of adequacy
and fluency, and that the most reliable system-
level comparisons can be made only by looking
at all three metrics.
4.3 Examples
Table 7 presents four examples where the
full model differs from the baseline. Example
wsj 0003.8 illustrates an example where the NE
tag PER DESC for researchers helps the percep-
tron model enforce the correct animacy agree-
ment, while the two baseline models prefer the
1038
Ref-wsj 0003.8 full,agr,wh neither Lorillard nor the researchers who studied the workers were aware of any research on
smokers of the Kent cigarettes
baseline,baseline-punct neither Lorillard nor the researchers that studied the workers were aware of any research on
smokers of the Kent cigarettes .
Ref-wsj 0003.18 agr-punct, full the plant , which is owned by Hollingsworth & Vose Co. , was under contract with lorillard to make the cigarette filters .
baselines, wh the plant , which is owned by Hollingsworth & Vose Co. , were under contract with lorillard to make the cigarette filters .
Ref-wsj 0018.6 agr-punct, full model while many of the risks were anticipated when minneapolis-based Cray Research first announced the spinoff ...
agr-punct, full while many of the risks were anticipated when minneapolis-based Cray Research first announced the spinoff ...
baselines while many of the risks was anticipated when minneapolis-based Cray Research announced the spinoff ...
Ref-wsj 0070.4 agr-punct, full Giant Group is led by three Rally ?s directors , Burt Sugarman , James M. Trotter III and William E. Trotter II that last
month indicated that they hold a 42.5 % stake in Rally ?s and plan to seek a majority of seats on ...
all others Giant Group is led by three Rally ?s directors , Burt Sugarman , James M. Trotter III and William E. Trotter II that last
month indicated that they holds a 42.5 % stake in Rally ?s and plans to seek a majority of seats on ...
Ref-wsj 0047.5 ... the ban wo n?t stop privately funded tissue-transplant research or federally funded fetal-tissue research
that does n?t involve transplants .
agr, full ... the ban wo n?t stop tissue-transplant privately funded research or federally funded fetal-tissue research
that does n?t involve transplants .
baselines, wh ... the ban wo n?t stop tissue-transplant privately funded research or federally funded fetal-tissue research
that do n?t involve transplants .
Table 7: Examples of realized output
Model #Punct-Errs %Agr-Errs %WH-Errs
baseline 39 11.05 22.44
baseline-punct 0 10.79 20.77
wh-punct 11 10.87 13.53
agr-punct 8 4.0 21.84
full-model 10 4.31 15.53
Table 6: Error analysis of Section 00 complete re-
alizations (total of 1554 agreement cases; total of
207 WH-pronoun cases)
that realization. Example wsj 0003.18 illustrates
an instance of simple subject-verb agreement be-
ing enforced by the models containing the agree-
ment features. Example wsj 0070.4 presents a
more complex situation where a single subject
has to agree with both verbs in a conjoined verb
phrase. The last example in Table 7 shows the
case of a NP subject which is a disjunction of two
individual NPs. In both these cases, while the
baseline models do not enforce the correct choice,
the models with the agreement features do get this
right. This is because our agreement features are
sensitive to the properties of both NP and VP con-
juncts/disjuncts. In addition, most of the realiza-
tions involving of -complements are also ranked
correctly. In the final example sentence provided
(i.e. wsj 0018.6), the models with the agreement
features are able to enforce the correct the agree-
ment constraints in the phrase many of the risks
were in contrast to the baseline models.
5 Conclusion
In this paper, we have shown for the first time
that incorporating linguistically motivated fea-
tures to ensure correct animacy and number agree-
ment in a statistical realization ranking model
yields significant improvements over a state-of-
the-art baseline. While agreement has tradition-
ally been modelled using hard constraints in the
grammar, we have argued that using a statistical
ranking model is a simpler and more robust ap-
proach that is capable of learning competing pref-
erences and cases of acceptable variation. Our
approach also approximates insights about agree-
ment which have been discussed in the theoret-
ical linguistics literature. We have also shown
how a targeted error analysis can reveal substan-
tial reductions in agreement errors, whose impact
on quality no doubt exceeds what is suggested
by the small BLEU score increases. As future
work, we also plan to learn such patterns from
large amounts of unlabelled data and use models
learned thus to rank paraphrases.
Acknowledgements
This work was supported in part by NSF grant IIS-
0812297 and by an allocation of computing time
from the Ohio Supercomputer Center. Our thanks
also to Robert Levine and the anonymous review-
ers for helpful comments and discussion.
1039
References
Baldridge, Jason and Geert-Jan Kruijff. 2002. Cou-
pling CCG and Hybrid Logic Dependency Seman-
tics. In Proc. ACL-02.
Baldridge, Jason. 2002. Lexically Specified Deriva-
tional Control in Combinatory Categorial Gram-
mar. Ph.D. thesis, University of Edinburgh.
Bangalore, Srinivas and Owen Rambow. 2000. Ex-
ploiting a probabilistic hierarchical model for gen-
eration. In Proc. COLING-00.
Boxwell, Stephen and Michael White. 2008. Project-
ing Propbank roles onto the CCGbank. In Proc.
LREC-08.
Cahill, Aoife and Josef van Genabith. 2006. Ro-
bust PCFG-based generation using automatically
acquired LFG approximations. In Proc. COLING-
ACL ?06.
Clark, Stephen and James R. Curran. 2007. Wide-
Coverage Efficient Statistical Parsing with CCG and
Log-Linear Models. Computational Linguistics,
33(4):493?552.
Espinosa, Dominic, Michael White, and Dennis
Mehay. 2008. Hypertagging: Supertagging for sur-
face realization with CCG. In Proc. ACL-08: HLT.
Guo, Yuqing, Josef van Genabith, and Haifeng Wang.
2008. Dependency-based n-gram models for
general purpose sentence realisation. In Proc.
COLING-08.
Hockenmaier, Julia and Mark Steedman. 2007. CCG-
bank: A Corpus of CCG Derivations and Depen-
dency Structures Extracted from the Penn Treebank.
Computational Linguistics, 33(3):355?396.
Hogan, Deirdre, Conor Cafferkey, Aoife Cahill, and
Josef van Genabith. 2007. Exploiting multi-word
units in history-based probabilistic generation. In
Proc. EMNLP-CoNLL.
Kathol, Andreas. 1999. Agreement and the
Syntax-Morphology Interface in HPSG. In Levine,
Robert D. and Georgia M. Green, editors, Studies
in Contemporary Phrase Structure Grammar, pages
223?274. Cambridge University Press, Cambridge.
Kim, Jong-Bok. 2004. Hybrid Agreement in English.
Linguistics, 42(6):1105?1128.
Nakanishi, Hiroko, Yusuke Miyao, and Jun?ichi Tsujii.
2005. Probabilistic methods for disambiguation of
an HPSG-based chart generator. In Proc. IWPT-05.
Palmer, Martha, Dan Gildea, and Paul Kingsbury.
2005. The proposition bank: A corpus annotated
with semantic roles. Computational Linguistics,
31(1).
Pollard, Carl and Ivan Sag. 1994. Head-Driven
Phrase Structure Grammar. University Of Chicago
Press.
Roark, Brian, Murat Saraclar, Michael Collins, and
Mark Johnson. 2004. Discriminative language
modeling with conditional random fields and the
perceptron algorithm. In Proc. ACL-04.
Steedman, Mark. 2000. The syntactic process. MIT
Press, Cambridge, MA, USA.
Velldal, Erik and Stephan Oepen. 2005. Maximum
entropy models for realization ranking. In Proc. MT
Summit X.
Weischedel, Ralph and Ada Brunstein. 2005. BBN
pronoun coreference and entity type corpus. Tech-
nical report, BBN.
White, Michael and Rajakrishnan Rajkumar. 2008.
A more precise analysis of punctuation for broad-
coverage surface realization with CCG. In Proc.
of the Workshop on Grammar Engineering Across
Frameworks (GEAF08).
White, Michael and Rajakrishnan Rajkumar. 2009.
Perceptron reranking for CCG realization. In Pro-
ceedings of the 2009 Conference on Empirical
Methods in Natural Language Processing, pages
410?419, Singapore, August. Association for Com-
putational Linguistics.
White, Michael. 2006. Efficient Realization of Coor-
dinate Structures in Combinatory Categorial Gram-
mar. Research on Language and Computation,
4(1):39?75.
1040
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 564?574,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
Further Meta-Evaluation of Broad-Coverage Surface Realization
Dominic Espinosa and Rajakrishnan Rajkumar and Michael White and Shoshana Berleant
Department of Linguistics
The Ohio State University
Columbus, Ohio, USA
{espinosa,raja,mwhite,berleant}@ling.ohio-state.edu
Abstract
We present the first evaluation of the utility of
automatic evaluation metrics on surface real-
izations of Penn Treebank data. Using outputs
of the OpenCCG and XLE realizers, along
with ranked WordNet synonym substitutions,
we collected a corpus of generated surface re-
alizations. These outputs were then rated and
post-edited by human annotators. We eval-
uated the realizations using seven automatic
metrics, and analyzed correlations obtained
between the human judgments and the auto-
matic scores. In contrast to previous NLG
meta-evaluations, we find that several of the
metrics correlate moderately well with human
judgments of both adequacy and fluency, with
the TER family performing best overall. We
also find that all of the metrics correctly pre-
dict more than half of the significant system-
level differences, though none are correct in
all cases. We conclude with a discussion of the
implications for the utility of such metrics in
evaluating generation in the presence of varia-
tion. A further result of our research is a cor-
pus of post-edited realizations, which will be
made available to the research community.
1 Introduction and Background
In building surface-realization systems for natural
language generation, there is a need for reliable
automated metrics to evaluate the output. Unlike
in parsing, where there is usually a single gold-
standard parse for a sentence, in surface realization
there are usually many grammatically-acceptable
ways to express the same concept. This parallels
the task of evaluating machine-translation (MT) sys-
tems: for a given segment in the source language,
there are usually several acceptable translations into
the target language. As human evaluation of trans-
lation quality is time-consuming and expensive, a
number of automated metrics have been developed
to evaluate the quality of MT outputs. In this study,
we investigate whether the metrics developed for
MT evaluation tasks can be used to reliably evaluate
the outputs of surface realizers, and which of these
metrics are best suited to this task.
A number of surface realizers have been devel-
oped using the Penn Treebank (PTB), and BLEU
scores are often reported in the evaluations of these
systems. But how useful is BLEU in this con-
text? The original BLEU study (Papineni et al,
2001) scored MT outputs, which are of generally
lower quality than grammar-based surface realiza-
tions. Furthermore, even for MT systems, the
usefulness of BLEU has been called into question
(Callison-Burch et al, 2006). BLEU is designed to
work with multiple reference sentences, but in tree-
bank realization, there is only a single reference sen-
tence available for comparison.
A few other studies have investigated the use of
such metrics in evaluating the output of NLG sys-
tems, notably (Reiter and Belz, 2009) and (Stent et
al., 2005). The former examined the performance of
BLEU and ROUGE with computer-generated weather
reports, finding a moderate correlation with human
fluency judgments. The latter study applied sev-
eral MT metrics to paraphrase data from Barzilay
and Lee?s corpus-based system (Barzilay and Lee,
2003), and found moderate correlations with human
adequacy judgments, but little correlation with flu-
ency judgments. Cahill (2009) examined the perfor-
mance of six MT metrics (including BLEU) in evalu-
ating the output of a LFG-based surface realizer for
564
German, also finding only weak correlations with
the human judgments.
To study the usefulness of evaluation metrics such
as BLEU on the output of grammar-based surface
realizers used with the PTB, we assembled a cor-
pus of surface realizations from three different re-
alizers operating on Section 00 of the PTB. Two
human judges evaluated the adequacy and fluency
of each of the realizations with respect to the ref-
erence sentence. The realizations were then scored
with a number of automated evaluation metrics de-
veloped for machine translation. In order to investi-
gate the correlation of targeted metrics with human
evaluations, and gather other acceptable realizations
for future evaluations, the judges manually repaired
each unacceptable realization during the rating task.
In contrast to previous NLG meta-evaluations, we
found that several of the metrics correlate moder-
ately well with human judgments of both adequacy
and fluency, with the TER family performing best.
However, when looking at statistically significant
system-level differences in human judgments, we
found that some of the metrics get some of the rank-
ings correct, but none get them all correct, with dif-
ferent metrics making different ranking errors. This
suggests that multiple metrics should be routinely
consulted when comparing realizer systems.
Overall, our methodology is similar to that of
previous MT meta-evaluations, in that we collected
human judgments of system outputs, and com-
pared these scores with those assigned by auto-
matic metrics. A recent alternative approach to para-
phrase evaluation is ParaMetric (Callison-Burch et
al., 2008); however, it requires a corpus of annotated
(aligned) paraphrases (which does not yet exist for
PTB data), and is arguably focused more on para-
phrase analysis than paraphrase generation.
The plan of the paper is as follows: Section 2 dis-
cusses the preparation of the corpus of surface real-
izations. Section 3 describes the human evaluation
task and the automated metrics applied. Sections 4
and 5 present and discuss the results of these evalua-
tions. We conclude with some general observations
about automatic evaluation of surface realizers, and
some directions for further research.
2 Data Preparation
We collected realizations of the sentences in Sec-
tion 00 of the WSJ corpus from the following three
sources:
1. OpenCCG, a CCG-based chart realizer (White,
2006)
2. The XLE Generator, a LFG-based system de-
veloped by Xerox PARC (Crouch et al, 2008)
3. WordNet synonym substitutions, to investigate
how differences in lexical choice compare to
grammar-based variation.1
Although all three systems used Section 00 of
the PTB, they were applied with various parame-
ters (e.g., language models, multiple-output versus
single-output) and on different input structures. Ac-
cordingly, our study does not compare OpenCCG to
XLE, or either of these to the WordNet system.
2.1 OpenCCG realizations
OpenCCG is an open source parsing/realization
library with multimodal extensions to CCG
(Baldridge, 2002). The OpenCCG chart realizer
takes logical forms as input and produces strings
by combining signs for lexical items. Alternative
realizations are scored using integrated n-gram
and perceptron models. For robustness, fragments
are greedily assembled when necessary. Realiza-
tions were generated from 1,895 gold standard
logical forms, created by constrained parsing of
development-section derivations. The following
OpenCCG models (which differ essentially in the
way the output is ranked) were used:
1. Baseline 1: Output ranked by a trigram word
model
2. Baseline 2: Output ranked using three language
models (3-gram words + 3-gram words with
named entity class replacement + factored lan-
guage model of words, POS tags and CCG su-
pertags)
1Not strictly surface realizations, since they do not involve
an abstract input specification, but for simplicity we refer to
them as realizations throughout.
565
3. Baseline 3: Perceptron with syntax features and
the three LMs mentioned above
4. Perceptron full-model: n-best realizations
ranked using perceptron with syntax features
and the three n-gram models, as well as dis-
criminative n-grams
The perceptron model was trained on sections 02-
21 of the CCGbank, while a grammar extracted from
section 00-21 was used for realization. In addition,
oracle supertags were inserted into the chart during
realization. The purpose of such a non-blind test-
ing strategy was to evaluate the quality of the output
produced by the statistical ranking models in isola-
tion, rather than focusing on grammar coverage, and
avoid the problems associated with lexical smooth-
ing, i.e. lexical categories in the development sec-
tion not being present in the training section.
To enrich the variation in the generated realiza-
tions, dative-alternation was enforced during real-
ization by ensuring alternate lexical categories of the
verb in question, as in the following example:
(1) the executives gave [the chefs] [a stand-
ing ovation]
(2) the executives gave [a standing ovation]
[to the chefs]
2.2 XLE realizations
The corpus of realizations generated by the XLE
system contained 42,527 surface realizations of ap-
proximately 1,421 section 00 sentences (an aver-
age of 30 per sentence), initially unranked. The
LFG f-structures used as input to the XLE genera-
tor were derived from automatic parses, as described
in (Riezler et al, 2002). The realizations were
first tokenized using Penn Treebank conventions,
then ranked using perplexities calculated from the
same trigram word model used with OpenCCG. For
each sentence, the top 4 realizations were selected.
The XLE generator provides an interesting point
of comparison to OpenCCG as it uses a manually-
developed grammar with inputs that are less abstract
but potentially noisier, as they are derived from au-
tomatic parses rather than gold-standard ones.
2.3 WordNet synonymizer
To produce an additional source of variation, the
nouns and verbs of the sentences in section 00 of
the PTB were replaced with all of their WordNet
synonyms. Verb forms were generated using verb
stems, part-of-speech tags, and the morphg tool.2
These substituted outputs were then filtered using
the n-gram data which Google Inc. has made avail-
able.3 Those without any 5-gram matches centered
on the substituted word (or 3-gram matches, in the
case of short sentences) were eliminated.
3 Evaluation
From the data sources described in the previous sec-
tion, a corpus of realizations to be evaluated by the
human judges was constructed by randomly choos-
ing 305 sentences from section 00, then selecting
surface realizations of these sentences using the fol-
lowing algorithm:
1. Add OpenCCG?s best-scored realization.
2. Add other OpenCCG realizations until all four
models are represented, to a maximum of 4.
3. Add up to 4 realizations from either the XLE
system or the WordNet pool, chosen randomly.
The intent was to give reasonable coverage of all
realizer systems discussed in Section 2 without over-
loading the human judges. ?System? here means
any instantiation that emits surface realizations, in-
cluding various configurations of OpenCCG (using
different language models or ranking systems), and
these can be multiple-output, such as an n-best list,
or single-output (best-only, worst-only, etc.). Ac-
cordingly, more realizations were selected from the
OpenCCG realizer because 5 different systems were
being represented. Realizations were chosen ran-
domly, rather than according to sentence types or
other criteria, in order to produce a representative
sample of the corpus. In total, 2,114 realizations
were selected for evaluation.
2http://www.informatics.sussex.ac.uk/
research/groups/nlp/carroll/morph.html
3http://www.ldc.upenn.edu/Catalog/docs/
LDC2006T13/readme.txt
566
3.1 Human judgments
Two human judges evaluated each surface realiza-
tion on two criteria: adequacy, which represents the
extent to which the output conveys all and only the
meaning of the reference sentence; and fluency, the
extent to which it is grammatically acceptable. The
realizations were presented to the judges in sets con-
taining a reference sentence and the 1-8 outputs se-
lected for that sentence. To aid in the evaluation of
adequacy, one sentence each of leading and trailing
context were displayed. Judges used the guidelines
given in Figure 1, based on the scales developed
by the NIST Machine Translation Evaluation Work-
shop.
In addition to rating each realization on the two
five-point scales, each judge also repaired each out-
put which he or she did not judge to be fully ade-
quate and fluent. An example is shown in Figure 2.
These repairs resulted in new reference sentences for
a substantial number of sentences. These repaired
realizations were later used to calculate targeted ver-
sions of the evaluation metrics, i.e., using the re-
paired sentence as the reference sentence. Although
targeted metrics are not fully automatic, they are of
interest because they allow the evaluation algorithm
to focus on what is actually wrong with the input,
rather than all textual differences. Notably, targeted
TER (HTER) has been shown to be more consistent
with human judgments than human annotators are
with one another (Snover et al, 2006).
3.2 Automatic evaluation
The realizations were also evaluated using seven au-
tomatic metrics:
? IBM?s BLEU, which scores a hypothesis by
counting n-gram matches with the reference
sentence (Papineni et al, 2001), with smooth-
ing as described in (Lin and Och, 2004)
? The NIST n-gram evaluation metric, similar to
BLEU, but rewarding rarer n-gram matches, and
using a different length penalty
? METEOR, which measures the harmonic mean
of unigram precision and recall, with a higher
weight for recall (Banerjee and Lavie, 2005)
? TER (Translation Edit Rate), a measure of the
number of edits required to transform a hy-
pothesis sentence into the reference sentence
(Snover et al, 2006)
? TERP, an augmented version of TER which
performs phrasal substitutions, stemming, and
checks for synonyms, among other improve-
ments (Snover et al, 2009)
? TERPA, an instantiation of TERP with edit
weights optimized for correlation with ade-
quacy in MT evaluations
? GTM (General Text Matcher), a generaliza-
tion of the F-measure that rewards contiguous
matching spans (Turian et al, 2003)
Additionally, targeted versions of BLEU, ME-
TEOR, TER, and GTM were computed by using the
human-repaired outputs as the reference set. The
human repair was different from the reference sen-
tence in 193 cases (about 9% of the total), and we
expected this to result in better scores and correla-
tions with the human judgments overall.
4 Results
4.1 Human judgments
Table 1 summarizes the dataset, as well as the mean
adequacy and fluency scores garnered from the hu-
man evaluation. Overall adequacy and fluency judg-
ments were high (4.16, 3.63) for the realizer sys-
tems on average, and the best-rated realizer systems
achieved mean fluency scores above 4.
4.2 Inter-annotator agreement
Inter-annotator agreement was measured using the
?-coefficient, which is commonly used to measure
the extent to which annotators agree in category
judgment tasks. ? is defined as P (A)?P (E)1?P (E) , where
P (A) is the observed agreement between annota-
tors and P (E) is the probability of agreement due
to chance (Carletta, 1996). Chance agreement for
this data is calculated by the method discussed in
Carletta?s squib. However, in previous work in
MT meta-evaluation, Callison-Burch et al (2007),
assume the less strict criterion of uniform chance
agreement, i.e. 15 for a five-point scale. They also
567
Score Adequacy Fluency
5 All the meaning of the reference Perfectly grammatical
4 Most of the meaning Awkward or non-native; punctuation errors
3 Much of the meaning Agreement errors or minor syntactic problems
2 Meaning substantially different Major syntactic problems, such as missing words
1 Meaning completely different Completely ungrammatical
Figure 1: Rating scale and guidelines
Ref. It wasn?t clear how NL and Mr. Simmons would respond if Georgia Gulf spurns them again
Realiz. It weren?t clear how NL and Mr. Simmons would respond if Georgia Gulf again spurns them
Repair It wasn?t clear how NL and Mr. Simmons would respond if Georgia Gulf again spurns them
Figure 2: Example of repair
introduce the notion of ?relative? ?, which measures
how often two or more judges agreed that A > B,
A = B, or A < B for two outputs A and B, irre-
spective of the specific values given on the five-point
scale; here, uniform chance agreement is taken to be
1
3 . We report both absolute and relative ? in Table 2,
using actual chance agreement rather than uniform
chance agreement.
The ? scores of 0.60 for adequacy and 0.63 for flu-
ency across the entire dataset represent ?substantial?
agreement, according to the guidelines discussed in
(Landis and Koch, 1977), better than is typically
reported for machine translation evaluation tasks;
for example, Callison-Burch et al (2007) reported
?fair? agreement, with ? = 0.281 for fluency and
? = 0.307 for adequacy (relative). Assuming the
uniform chance agreement that the previously cited
work adopts, our inter-annotator agreements (both
absolute and relative) are still higher. This is likely
due to the generally high quality of the realizations
evaluated, leading to easier judgments.
4.3 Correlation with automatic evaluation
To determine how well the automatic evaluation
methods described in Section 3 correlate with the
human judgments, we averaged the human judg-
ments for adequacy and fluency, respectively, for
each of the rated realizations, and then computed
both Pearson?s correlation coefficient and Spear-
man?s rank correlation coefficient between these
scores and each of the metrics. Spearman?s corre-
lation makes fewer assumptions about the distribu-
tion of the data, but may not reflect a linear rela-
tionship that is actually present. Both are frequently
reported in the literature. Due to space constraints,
we show only Spearman?s correlation, although the
TER family scored slightly better on Pearson?s coef-
ficient, relatively.
The results for Spearman?s correlation are given
in Table 3. Additionally, the average scores for ad-
equacy and fluency were themselves averaged into
a single score, following (Snover et al, 2009), and
the Spearman?s correlation of each of the automatic
metrics with these scores are given in Table 4. All
reported correlations are significant at p < 0.001.
4.4 Bootstrap sampling of correlations
For each of the sub-corpora shown in Table 1, we
computed confidence intervals for the correlations
between adequacy and fluency human scores with
selected automatic metrics (BLEU, HBLEU, TER,
TERP, and HTER) as described in (Koenh, 2004). We
sampled each sub-corpus 1000 times with replace-
ment, and calculated correlations between the rank-
ings induced by the human scores and those induced
by the metrics for each reference sentence. We then
used these coefficients to estimate the confidence in-
terval, after excluding the top 25 and bottom 25 co-
efficients, following (Lin and Och, 2004). The re-
sults of this for the BLEU metric are shown in Table
5. We determined which correlations lay within the
95% confidence interval of the best performing met-
ric in each row of Table Table 3; these figures are
italicized.
568
5 Discussion
5.1 Human judgments of systems
The results for the four OpenCCG perceptron mod-
els mostly confirm those reported in (White and Ra-
jkumar, 2009), with one exception: the B-3 model
was below B-2, though the P-B (perceptron-best)
model still scored highest. This may have been due
to differences in the testing scenario. None of the
differences in adequacy scores among the individ-
ual systems are significant, with the exception of the
WordNet system. In this case, the lack of word-
sense disambiguation for the substituted words re-
sults in a poor overall adequacy score (e.g., wage
floor ? wage story). Conversely, it scores highest
for fluency, as substituting a noun or verb with a syn-
onym does not usually introduce ungrammaticality.
5.2 Correlations of human judgments with MT
metrics
Of the non-human-targeted metrics evaluated, BLEU
and TER/TERP demonstrate the highest correla-
tions with the human judgments of fluency (r =
0.62, 0.64). The TER family of evaluation metrics
have been observed to perform very well in MT-
evaluation tasks, and although the data evaluated
here differs from typical MT data in some impor-
tant ways, the correlation of TERP with the human
judgments is substantial. In contrast with previous
MT evaluations where TERP performs considerably
better than TER, these scored close to equal on our
data, possibly because TERP?s stem, synonym, and
paraphrase matching are less useful when most of
the variation is syntactic.
The correlations with BLEU and METEOR are
lower than those reported in (Callison-Burch et al,
2007); in that study, BLEU achieved adequacy and
fluency correlations of 0.690 and 0.722, respec-
tively, and METEOR achieved 0.701 and 0.719. The
correlations for these metrics might be expected to
be lower for our data, since overall quality is higher,
making the metrics? task more difficult as the out-
puts involve subtler differences between acceptable
and unacceptable variation.
The human-targeted metrics (represented by the
prefixed H in the data tables) correlated even more
strongly with the human judgments, compared to the
non-targeted versions. HTER demonstrated the best
correlation with realizer fluency (r = 0.75).
For several kinds of acceptable variation involv-
ing the rearrangement of constituents (such as da-
tive shift), TERP gives a more reasonable score than
BLEU, due to its ability to directly evaluate phrasal
shifts. The following realization was rated 4.5 for
fluency, and was more correctly ranked by TERP
than BLEU:
(3) Ref: The deal also gave Mitsui access to
a high-tech medical product.
(4) Realiz.: The deal also gave access to a
high-tech medical product to Mitsui.
For each reference sentence, we compared the
ranking of its realizations induced from the human
scores to the ranking induced from the TERP score,
and counted the rank errors by the latter, infor-
mally categorizing them by error type (see Table
7). In the 50 sentences with the highest numbers of
rank errors, 17 were affected by punctuation differ-
ences, typically involving variation in comma place-
ment. Human fluency judgments of outputs with
only punctuation problems were generally high, and
many realizations with commas inserted or removed
were rated fully fluent by the annotators. However,
TERP penalizes such insertions or deletions. Agree-
ment errors are another frequent source of rank-
ing errors for TERP. The human judges tended to
harshly penalize sentences with number-agreement
or tense errors, whereas TERP applies only a single
substitution penalty for each such error. We expect
that with suitable optimization of edit weights to
avoid over-penalizing punctuation shifts and under-
penalizing agreement errors, TERP would exhibit an
even stronger correlation with human fluency judg-
ments.
None of the evaluation metrics can distinguish an
acceptable movement of a word or constituent from
an unacceptable movement, with only one reference
sentence. A substantial source of error for both
TERP and BLEU is variation in adverbial placement,
as shown in (7).
Similar errors are seen with prepositional phrases
and some commonly-occurring temporal adverbs,
which typically admit a number of variations in
placement. Another important example of accept-
able variation which these metrics do not generally
rank correctly is dative alternation:
569
(7)
Ref. We need to clarify what exactly is wrong with it.
Realiz. Flu. TERP BLEU
We need to clarify exactly what is wrong with it. 5 0.1 0.5555
We need to clarify exactly what ?s wrong with it. 5 0.2 0.4046
We need to clarify what , exactly , is wrong with it. 5 0.2 0.5452
We need to clarify what is wrong with it exactly. 4.5 0.1 0.6756
We need to clarify what exactly , is wrong with it. 4 0.1 0.7017
We need to clarify what , exactly is wrong with it. 4 0.1 0.7017
We needs to clarify exactly what is wrong with it. 3 0.103 0.346
(5) Ref. When test booklets were passed
out 48 hours ahead of time, she says she
copied questions in the social studies sec-
tion and gave the answers to students.
(6) Realiz. When test booklets were passed
out 48 hours ahead of time , she says she
copied questions in the social studies sec-
tion and gave students the answers.
The correlations of each of the metrics with the
human judgments of fluency for the realizer systems
indicate at least a moderate relationship, in contrast
with the results reported in (Stent et al, 2005) for
paraphrase data, which found an inverse correlation
for fluency, and (Cahill, 2009) for the output of a sur-
face realizer for German, which found only a weak
correlation. However, the former study employed
a corpus-based paraphrase generation system rather
than grammar-driven surface realizers, and the re-
sulting paraphrases exhibited much broader varia-
tion. In Cahill?s study, the outputs of the realizer
were almost always grammatically correct, and the
automated evaluation metrics were ranking marked-
ness instead of grammatical acceptability.
5.3 System-level comparisons
In order to investigate the efficacy of the metrics
in ranking different realizer systems, or competing
realizations from the same system generated using
different ranking models, we considered seven dif-
ferent ?systems? from the whole dataset of realiza-
tions. These consisted of five OpenCCG-based re-
alizations (the best realization from three baseline
models, and the best and the worst realization from
the full perceptron model), and two XLE-based sys-
tems (the best and the worst realization, after rank-
ing the outputs of the XLE realizer with an n-gram
model). The mean of the combined adequacy and
fluency scores of each of these seven systems was
compared with that of every other system, result-
ing in 21 pairwise comparisons. Then Tukey?s HSD
test was performed to determine the systems which
differed significantly in terms of the average ade-
quacy and fluency rating they received.4 The test
revealed five pairwise comparisons where the scores
were significantly different.
Subsequently, for each of these systems, an over-
all system-level score for each of the MT metrics
was calculated. For the five pairwise comparisons
where the adequacy-fluency group means differed
significantly, we checked whether the metric ranked
the systems correctly. Table 8 shows the results of
a pairwise comparison between the ranking induced
by each evaluation metric, and the ranking induced
by the human judgments. Five of the seven non-
targeted metrics correctly rank more than half of the
systems. NIST, METEOR, and GTM get the most
comparisons right, but neither NIST nor GTM cor-
rectly rank the OpenCCG-baseline model 1 with re-
spect to the XLE-best model. TER and TERP get two
of the five comparisons correct, and they incorrectly
rank two of the five OpenCCG model comparisons,
as well as the comparison between the XLE-worst
and OpenCCG-best systems.
For the targeted metrics, HNIST is correct for all
five comparisons, while neither HBLEU nor HME-
TEOR correctly rank all the OpenCCG models. On
the other hand, HTER and HGTM incorrectly rank the
XLE-best system versus OpenCCG-based models.
In summary, some of the metrics get some of the
rankings correct, but none of the non-targeted met-
rics get al of them correct. Moreover, different met-
rics make different ranking errors. This argues for
4This particular test was chosen since it corrects for multiple
post-hoc analyses conducted on the same data-set.
570
the use of multiple metrics in comparing realizer
systems.
6 Conclusion
Our study suggests that although the task of evalu-
ating the output from realizer systems differs from
the task of evaluating machine translations, the au-
tomatic metrics used to evaluate MT outputs deliver
moderate correlations with combined human fluency
and adequacy scores when used on surface realiza-
tions. We also found that the MT-evaluation met-
rics are useful in evaluating different versions of the
same realizer system (e.g., the various OpenCCG re-
alization ranking models), and finding cases where
a system is performing poorly. As in MT-evaluation
tasks, human-targeted metrics have the highest cor-
relations with human judgments overall. These re-
sults suggest that the MT-evaluation metrics are use-
ful for developing surface realizers. However, the
correlations are lower than those reported for MT
data, suggesting that they should be used with cau-
tion, especially for cross-system evaluation, where
consulting multiple metrics may yield more reliable
comparisons. In our study, the targeted version of
TERP correlated most strongly with human judg-
ments of fluency.
In future work, the performance of the TER family
of metrics on this data might be improved by opti-
mizing the edit weights used in computing its scores,
so as to avoid over-penalizing punctuation move-
ments or under-penalizing agreement errors, both
of which were significant sources of ranking errors.
Multiple reference sentences may also help mitigate
these problems, and the corpus of human-repaired
realizations that has resulted from our study is a step
in this direction, as it provides multiple references
for some cases. We expect the corpus to also prove
useful for feature engineering and error analysis in
developing better realization models.5
Acknowledgements
We thank Aoife Cahill and Tracy King for providing
us with the output of the XLE generator. We also
thank Chris Callison-Burch and the anonymous re-
viewers for their helpful comments and suggestions.
5The corpus can be downloaded from http://www.
ling.ohio-state.edu/?mwhite/data/emnlp10/.
This material is based upon work supported by
the National Science Foundation under Grant No.
0812297.
References
Jason Baldridge. 2002. Lexically Specified Derivational
Control in Combinatory Categorial Grammar. Ph.D.
thesis, University of Edinburgh.
S. Banerjee and A. Lavie. 2005. METEOR: An auto-
matic metric for MT evaluation with improved corre-
lation with human judgments. In Proceedings of the
ACL Workshop on Intrinsic and Extrinsic Evaluation
Measures for Machine Translation and/or Summariza-
tion, pages 65?72.
R. Barzilay and L. Lee. 2003. Learning to paraphrase:
An unsupervised approach using multiple-sequence
alignment. In proceedings of HLT-NAACL, volume
2003, pages 16?23.
Aoife Cahill. 2009. Correlating human and automatic
evaluation of a german surface realiser. In Proceed-
ings of the ACL-IJCNLP 2009 Conference Short Pa-
pers, pages 97?100, Suntec, Singapore, August. Asso-
ciation for Computational Linguistics.
C. Callison-Burch, M. Osborne, and P. Koehn. 2006. Re-
evaluating the role of BLEU in machine translation re-
search. In Proceedings of EACL, volume 2006, pages
249?256.
Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,
Christof Monz, and Josh Schroeder. 2007. (meta-)
evaluation of machine translation. In StatMT ?07: Pro-
ceedings of the Second Workshop on Statistical Ma-
chine Translation, pages 136?158, Morristown, NJ,
USA. Association for Computational Linguistics.
C. Callison-Burch, T. Cohn, and M. Lapata. 2008. Para-
metric: An automatic evaluation metric for paraphras-
ing. In Proceedings of the 22nd International Con-
ference on Computational Linguistics-Volume 1, pages
97?104. Association for Computational Linguistics.
J. Carletta. 1996. Assessing agreement on classification
tasks: the kappa statistic. Computational linguistics,
22(2):249?254.
Dick Crouch, Mary Dalrymple, Ron Kaplan, Tracy King,
John Maxwell, and Paula Newman. 2008. Xle docu-
mentation. Technical report, Palo Alto Research Cen-
ter.
Philip Koenh. 2004. Statistical significance tests for ma-
chine translation evaluation. In Proceedings of the
2009 Conference on Empirical Methods in Natural
Language Processing.
J.R. Landis and G.G. Koch. 1977. The measurement of
observer agreement for categorical data. Biometrics,
33(1):159?174.
571
Chin-Yew Lin and Franz Josef Och. 2004. Orange: a
method for evaluating automatic evaluation metrics for
machine translation. In COLING ?04: Proceedings
of the 20th international conference on Computational
Linguistics, page 501, Morristown, NJ, USA. Associ-
ation for Computational Linguistics.
K. Papineni, S. Roukos, T. Ward, and W. Zhu. 2001.
Bleu: a method for automatic evaluation of machine
translation. Technical report, IBM Research.
E. Reiter and A. Belz. 2009. An investigation into the
validity of some metrics for automatically evaluating
natural language generation systems. Computational
Linguistics, 35(4):529?558.
Stefan Riezler, Tracy H. King, Ronald M. Kaplan,
Richard Crouch, John T. III Maxwell, and Mark John-
son. 2002. Parsing the wall street journal using
a lexical-functional grammar and discriminative esti-
mation techniques. In Proceedings of 40th Annual
Meeting of the Association for Computational Lin-
guistics, pages 271?278, Philadelphia, Pennsylvania,
USA, July. Association for Computational Linguistics.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea
Micciulla, and John Makhoul. 2006. A study of trans-
lation edit rate with targeted human annotation. In In
Proceedings of Association for Machine Translation in
the Americas, pages 223?231.
M. Snover, N. Madnani, B.J. Dorr, and R. Schwartz.
2009. Fluency, adequacy, or HTER?: exploring dif-
ferent human judgments with a tunable MT metric.
In Proceedings of the Fourth Workshop on Statistical
Machine Translation, pages 259?268. Association for
Computational Linguistics.
Amanda Stent, Matthew Marge, and Mohit Singhai.
2005. Evaluating evaluation methods for generation in
the presence of variation. In Proceedings of CICLing.
J.P. Turian, L. Shen, and I.D. Melamed. 2003. Evalua-
tion of machine translation and its evaluation. recall
(C? R), 100:2.
Michael White and Rajakrishnan Rajkumar. 2009. Per-
ceptron reranking for CCG realization. In Proceedings
of the 2009 Conference on Empirical Methods in Nat-
ural Language Processing, pages 410?419, Singapore,
August. Association for Computational Linguistics.
Michael White. 2006. Efficient Realization of Coordi-
nate Structures in Combinatory Categorial Grammar.
Research on Language and Computation, 4(1):39?75.
572
Type System #Refs #Paraphrases Average Paraphrases/Ref #Exact Matches Adq Flu
Single output OpenCCG Baseline 1 296 296 1.0 72 4.17 3.65
OpenCCG Baseline 2 296 296 1.0 82 4.34 3.94
OpenCCG Baseline 3 296 296 1.0 76 4.31 3.86
OpenCCG Perceptron Best 296 1.0 1.0 112 4.37 4.09
OpenCCG Perceptron Worst 117 117 1.0 5 4.34 3.36
XLE Best 154 154 1.0 24 4.41 4.07
XLE Worst 157 157 1.0 13 4.08 3.73
Multiple output OpenCCG-Perceptron All 296 767 2.6 158 4.45 3.91
OpenCCG All 296 1131 3.8 162 4.20 3.61
XLE All 174 557 3.2 54 4.17 3.81
Wordnet Subsitutions 162 486 3.0 0 3.66 4.71
Realizer All 296 1628 5.0 169 4.16 3.63
All 296 2114 7.1 169 4.05 3.88
Table 1: Descriptive statistics
System Adq Flu
p(A) p(E) ? p(A) p(E) ?
OpenCCG-Abs 0.73 0.47 0.48 0.70 0.24 0.61
OpenCCG-Rel 0.76 0.47 0.54 0.76 0.34 0.64
XLE-Abs 0.68 0.42 0.44 0.69 0.27 0.58
XLE-Rel 0.73 0.45 0.50 0.69 0.37 0.50
Wordnet-Abs 0.57 0.25 0.43 0.77 0.66 0.33
Wordnet-Rel 0.74 0.34 0.61 0.73 0.60 0.33
Realizer-Abs 0.70 0.44 0.47 0.69 0.24 0.59
Realizer-Rel 0.74 0.41 0.56 0.73 0.33 0.60
All-Abs 0.67 0.38 0.47 0.71 0.29 0.59
All-Rel 0.74 0.36 0.60 0.75 0.34 0.63
Table 2: Corpora-wise inter-annotator agreement (absolute and relative ? values shown)
Sys N B M G TP TA T HT HN HB HM HG
OpenCCG-Adq 0.27 0.39 0.35 0.18 0.39 0.34 0.4 0.43 0.3 0.43 0.43 0.23
OpenCCG-Flu 0.49 0.55 0.4 0.42 0.6 0.46 0.6 0.72 0.58 0.69 0.57 0.53
XLE-Adq 0.52 0.51 0.55 0.31 0.5 0.5 0.5 0.52 0.47 0.51 0.61 0.4
XLE-Flu 0.56 0.56 0.48 0.37 0.55 0.5 0.55 0.61 0.54 0.61 0.51 0.51
Wordnet-Adq 0.17 0.14 0.24 0.15 0.37 0.26 0.22 0.64 0.52 0.56 0.32 0.6
Wordnet-Flu 0.26 0.21 0.24 0.24 0.22 0.27 0.26 0.34 0.32 0.34 0.3 0.34
Realizer-Adq 0.47 0.6 0.57 0.42 0.59 0.57 0.6 0.62 0.49 0.62 0.65 0.48
Realizer-Flu 0.51 0.62 0.52 0.5 0.63 0.53 0.64 0.75 0.59 0.73 0.65 0.63
All-Adq 0.37 0.37 0.33 0.32 0.42 0.31 0.43 0.53 0.44 0.48 0.44 0.45
All-Flu 0.21 0.62 0.51 0.32 0.61 0.55 0.6 0.7 0.33 0.71 0.62 0.48
Table 3: Spearman?s correlations among NIST (N), BLEU (B), METEOR (M), GTM (G), TERp (TP), TERpa (TA),
TER (T), human variants (HN, HB, HM, HT, HG) and human judgments (-Adq: adequacy and -Flu: Fluency); Scores
which fall within the 95 %CI of the best are italicized.
Sys N B M G TP TA T HT HN HB HM HG
OpenCCG 0.49 0.57 0.42 0.4 0.61 0.46 0.62 0.73 0.58 0.7 0.59 0.51
XLE 0.63 0.64 0.59 0.39 0.62 0.58 0.63 0.69 0.6 0.68 0.63 0.54
Wordnet 0.21 0.14 0.21 0.19 0.38 0.25 0.23 0.65 0.56 0.57 0.31 0.63
Realizer 0.55 0.68 0.57 0.5 0.68 0.58 0.69 0.78 0.61 0.77 0.7 0.63
All 0.34 0.58 0.47 0.38 0.61 0.48 0.61 0.75 0.48 0.73 0.61 0.58
Table 4: Spearman?s correlations among NIST (N), BLEU (B), METEOR (M), GTM (G), TERp (TP), TERpa (TA),
TER (T), human variants (HN, HB, HM, HT, HG) and human judgments (combined adequacy and fluency scores)
573
System Adq Flu
Sp 95%L 95%U Sp 95%L 95%U
Realizer 0.60 0.58 0.63 0.62 0.59 0.65
XLE 0.51 0.47 0.56 0.56 0.51 0.61
OpenCCG 0.39 0.35 0.42 0.55 0.52 0.59
All 0.37 0.34 0.4 0.62 0.6 0.64
Wordnet 0.14 0.06 0.21 0.21 0.13 0.28
Table 5: Spearman?s correlation analysis (bootstrap sampling) of the BLEU scores of various systems with human
adequacy and fluency scores
Sys HJ N B M G TP TA T HT HN HB HM HG HJ1-HJ2
OpenCCG HJ-1 0.44 0.52 0.39 0.36 0.56 0.43 0.58 0.75 0.58 0.72 0.62 0.52 0.76
HJ-2 0.5 0.58 0.43 0.4 0.62 0.46 0.63 0.7 0.55 0.68 0.56 0.49
XLE HJ-1 0.6 0.6 0.55 0.37 0.57 0.55 0.58 0.69 0.63 0.68 0.64 0.54 0.75
HJ-2 0.6 0.6 0.56 0.39 0.6 0.55 0.61 0.64 0.54 0.61 0.57 0.51
Wordnet HJ-1 0.2 0.18 0.26 0.16 0.37 0.28 0.24 0.7 0.59 0.64 0.35 0.65 0.72
HJ-2 0.25 0.16 0.23 0.19 0.37 0.25 0.25 0.59 0.52 0.51 0.32 0.56
Realizer HJ-1 0.51 0.65 0.56 0.49 0.64 0.56 0.66 0.8 0.62 0.78 0.72 0.64 0.82
HJ-2 0.55 0.68 0.56 0.5 0.67 0.57 0.68 0.74 0.58 0.73 0.66 0.6
All HJ-1 0.32 0.53 0.45 0.37 0.57 0.44 0.57 0.77 0.5 0.74 0.62 0.59 0.79
HJ-2 0.35 0.58 0.46 0.37 0.61 0.47 0.6 0.71 0.44 0.69 0.57 0.54
Table 6: Spearman?s correlations of NIST (N), BLEU (B), METEOR (M), GTM (G), TERp (TP), TERpa (TA), human
variants (HT, HN, HB, HM, HG), and individual human judgments (combined adq. and flu. scores)
Factor Count
Punctuation 17
Adverbial shift 16
Agreement 14
Other shifts 8
Conjunct rearrangement 8
Complementizer ins/del 5
PP shift 4
Table 7: Factors influencing TERP ranking errors for 50 worst-ranked realization groups
Metric Score Errors
nist 4 C1-XB
bleu 3 XB-PW C1-XB
meteor 4 XW-PB
ter 2 PW-PB XW-PB C1-PB
terp 2 PW-PB XW-PB C1-PB
terpa 3 XW-PB C1-PB
gtm 4 C1-XB
hnist 5
hbleu 3 PW-PB XW-PB
hmeteor 2 PW-PB XW-PB C1-PB
hter 3 XB-PW C1-XB
hgtm 3 XB-PW C1-XB
Table 8: Metric-wise ranking performance in terms of agreement with a ranking induced by combined adequacy and
fluency scores; each metric gets a score out of 5 (i.e. number of system-level comparisons that emerged significant as
per the Tukey?s HSD test)
Legend: Perceptron Best (PB); Perceptron Worst (PW); XLE Best (XB); XLE Worst (XW); OpenCCG baseline mod-
els 1 to 3 (C1 ... C3)
574
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 244?255, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Minimal Dependency Length in Realization Ranking
Michael White and Rajakrishnan Rajkumar
Department of Linguistics
The Ohio State University
Columbus, OH, USA
{mwhite,raja}@ling.osu.edu
Abstract
Comprehension and corpus studies have found
that the tendency to minimize dependency
length has a strong influence on constituent or-
dering choices. In this paper, we investigate
dependency length minimization in the con-
text of discriminative realization ranking, fo-
cusing on its potential to eliminate egregious
ordering errors as well as better match the dis-
tributional characteristics of sentence order-
ings in news text. We find that with a state-
of-the-art, comprehensive realization rank-
ing model, dependency length minimization
yields statistically significant improvements
in BLEU scores and significantly reduces
the number of heavy/light ordering errors.
Through distributional analyses, we also show
that with simpler ranking models, dependency
length minimization can go overboard, too of-
ten sacrificing canonical word order to shorten
dependencies, while richer models manage to
better counterbalance the dependency length
minimization preference against (sometimes)
competing canonical word order preferences.
1 Introduction
In this paper, we show that for the constituent or-
dering problem in surface realization, incorporating
insights from the minimal dependency length the-
ory of language production (Temperley, 2007) into a
discriminative realization ranking model yields sig-
nificant improvements upon a state-of-the-art base-
line. We demonstrate empirically using OpenCCG,
our CCG-based (Steedman, 2000) surface realiza-
tion system, the utility of a global feature encoding
the total dependency length of a given derivation.
Although other works in the realization literature
have used phrase length or head-dependent distances
in their models (Filippova and Strube, 2009; Velldal
and Oepen, 2005; White and Rajkumar, 2009, i.a.),
to the best of our knowledge, this paper is the first
to use insights from the minimal dependency length
theory directly and study their effects, both qualita-
tively and quantitatively.
The impetus for this paper was the discovery
that despite incorporating a sophisticated syntac-
tic model borrowed from the parsing literature?
including features with head-dependent distances at
various scales?White & Rajkumar?s (2009) real-
ization ranking model still often performed poorly
on weight-related decisions such as when to em-
ploy heavy-NP shift. Table 1 illustrates this point.
In wsj 0034.9, the full model (incorporating numer-
ous syntactic features) succeeds in reproducing the
reference sentence, which is clearly preferable to
the rather awkward variant selected by the base-
line model (using various n-gram models). How-
ever, in wsj 0013.16, the full model fails to shift the
temporal modifier for now next to the phrasal verb
turned down, leaving it at the end of its very long
verb phrase where it is highly ambiguous (with mul-
tiple intervening attachment sites). Conversely, in
wsj 0044.3, the full model shifts before next to the
verb, despite the NP cheating being very light, yield-
ing a very confusing ordering given that before is
meant to be intransitive.
The syntactic features in White & Rajku-
mar?s (2009) realization ranking model are taken
from Clark & Curran?s (2007) normal form model
244
wsj 0034.9 they fell into oblivion after the 1929 crash .
FULL [same]
BASELINE they fell after the 1929 crash into oblivion .
wsj 0013.16 separately , the Federal Energy Regulatory Commission [V P turned down for now [NP a request
by Northeast [V P seeking approval of [NP its possible purchase of PS of New Hampshire]]]] .
FULL separately , the Federal Energy Regulatory Commission [V P turned down [NP a request by North-
east [V P seeking approval of [NP its possible purchase of PS of New Hampshire]]] for now] .
wsj 0044.3 she had seen cheating before , but these notes were uncanny .
FULL she had seen before cheating , but these notes were uncanny .
Table 1: Examples of OpenCCG output with White & Rajkumar?s (2009) models?the first represents a successful
case, the latter two egregious ordering errors
(Table 3; see Section 3). In this model, head-
dependenct distances are considered in conjunc-
tion with lexicalized and unlexicalized CCG deriva-
tion steps, thereby appearing in numerous features.
As such, the model takes into account the inter-
action of dependency length with derivation steps,
but in essence does not consider the main ef-
fect of dependency length itself. In this light,
our investigation of dependency length minimiza-
tion can be viewed as examining the question of
whether realization ranking models can be made
more accurate?and in particular, avoid egregious
ordering errors?by incorporating a feature to ac-
count for the main effect of dependency length.
It is important to observe at this point that de-
pendency length minimization is more of a prefer-
ence than an optimization objective, which must be
balanced against other order preferences at times.
A closer reading of Temperley?s (2007) study re-
veals that dependency length can sometimes run
counter to many canonical word order choices. A
case in point is the class of examples involving
pre-modifying adjunct sequences that precede both
the subject and the verb. Assuming that their par-
ent head is the main verb of the sentence, a long-
short sequence would minimize overall dependency
length. However, in 613 examples found in the Penn
Treebank, the average length of the first adjunct was
3.15 words while the second adjunct was 3.48 words
long, thus reflecting a short-long pattern, as illus-
trated in the Temperley p.c. example in Table 2.
Apart from these, Hawkins (2001) shows that argu-
ments are generally located closer to the verb than
adjuncts. Gildea and Temperley (2007) also suggest
that adverb placement might involve cases which go
against dependency length minimization. An exam-
ination of 295 legitimate long-short post-verbal con-
stituent orders (counter to dependency length) from
Section 00 of the Penn Treebank revealed that tem-
poral adverb phrases are often involved in long-short
orders, as shown in wsj 0075.13 in Table 2. In our
setup, the preference to minimize dependency length
can be balanced by features capturing preferences
for alternate choices (e.g. the argument-adjunct dis-
tinction in our dependency ordering model, Table 4).
Via distributional analyses, we show that while sim-
pler realization ranking models can go overboard
in minimizing dependency length, richer models
largely succeed in overcoming this issue, while still
taking advantage of dependency length minimiza-
tion to avoid egregious ordering errors.
2 Background
2.1 Minimal Dependency Length
Comprehension and corpus studies (Gibson, 1998;
Gibson, 2000; Temperley, 2007) point to the ten-
dency of production and comprehension systems to
adhere to principles of dependency length minimiza-
tion. The idea of dependency length minimization
is based on Gibson?s (1998) Dependency Locality
Theory (DLT) of comprehension, which predicts
that longer dependencies are more difficult to pro-
cess. DLT predictions have been further validated
using comprehension studies involving eye-tracking
corpora (Demberg and Keller, 2008). DLT metrics
also correlate reasonably well with activation de-
cay over time expressed in computational models of
245
Temperley (p.c.) [In 1976], [as a film student at the Purchase campus of the State University of New York], Mr.
Lane, shot ...
wsj 0075.13 The Treasury also said it plans to sell [$ 10 billion] [in 36-day cash management bills] [on
Thursday].
Table 2: Counter-examples to dependency length minimization
comprehension (Lewis et al2006; Lewis and Va-
sishth, 2005).
Extending these ideas from comprehension, Tem-
perley (2007) poses the question: Does language
production reflect a preference for shorter dependen-
cies as well so as to facilitate comprehension? By
means of a study of Penn Treebank data, Temperley
shows that English sentences do display a tendency
to minimize the sum of all their head-dependent
distances as illustrated by a variety of construc-
tions. Further, Gildea and Temperley (2007) report
that random linearizations have higher dependency
lengths compared to actual English, while an ?opti-
mal? algorithm (from the perspective of dependency
length minimization), which places dependents on
either sides of a head in order of increasing length,
is closer to actual English. Tily (2010) also applies
insights from the above cited papers to show that
dependency length constitutes a significant pressure
towards language change. For head-final languages
(e.g., Japanese), dependency length minimization
results in the ?long-short? constituent ordering in
language production (Yamashita and Chang, 2001).
More generally, Hawkins?s (1994; 2000) processing
domains, dependency length minimization and end-
weight effects in constituent ordering (Wasow and
Arnold, 2003) are all very closely related. The de-
pendency length hypothesis goes beyond the predic-
tions made by Hawkins? Minimize Domains princi-
ple in the case of English clauses with three post-
verbal adjuncts: Gibson?s DLT correctly predicts
that the first constituent tends to be shorter than the
second, while Hawkins? approach does not make
predictions about the relative orders of the first two
constituents.
However, it would be very reductive to consider
dependency length minimization as the sole factor
in language production. In fact, a large body of
prior work discusses a variety of other factors in-
volved in language production. These other prefer-
ences are either correlated with dependency length
or can override the minimal dependency length pref-
erence. Complexity (Wasow, 2002; Wasow and
Arnold, 2003), animacy (Snider and Zaenen, 2006;
Branigan et al2008), information status consid-
erations (Wasow and Arnold, 2003; Arnold et al
2000), the argument-adjunct distinction (Hawkins,
2001) and lexical bias (Wasow and Arnold, 2003;
Bresnan et al2007) are a few prominent factors.
More recently, Anttila et al2010) argued that the
principle of end weight can be revised by calculat-
ing weight in prosodic terms to provide more ex-
planatory power. As Temperley (2007) suggests,
a satisactory model should combine insights from
multiple approaches, a theme which we investigate
in this work by means of a rich feature set adapted
from the parsing and realization literature. Our fea-
ture design has been inspired by the conclusions of
the above-cited works pertaining to the role of de-
pendency length minimization in syntactic choice
in conjuction with other factors influencing con-
stituent order. However, going beyond Temper-
ley?s corpus study, we confirm the utility of incor-
porating a feature for minimizing dependency length
into machine-learned models with hundreds of thou-
sands of features found to be useful in previous pars-
ing and realization work, and investigate the extent
to which these features can counterbalance a de-
pendency length minimization preference in cases
where canonical word order considerations should
prevail.
2.2 Surface Realization with Combinatory
Categorial Grammar (CCG)
We provide here a brief overview of CCG and the
OpenCCG realizer; for further details, see the works
cited below.
CCG (Steedman, 2000) is a unification-based
categorial grammar formalism defined almost en-
tirely in terms of lexical entries that encode sub-
246
Feature Type Example
LexCat + Word s/s/np + before
LexCat + POS s/s/np + IN
Rule sdcl ? np sdcl\np
Rule + Word sdcl ? np sdcl\np + bought
Rule + POS sdcl ? np sdcl\np + VBD
Word-Word ?company, sdcl ? np sdcl\np, bought?
Word-POS ?company, sdcl ? np sdcl\np, VBD?
POS-Word ?NN, sdcl ? np sdcl\np, bought?
Word + ?w ?bought, sdcl ? np sdcl\np? + dw
POS + ?w ?VBD, sdcl ? np sdcl\np? + dw
Word + ?p ?bought, sdcl ? np sdcl\np? + dp
POS + ?p ?VBD, sdcl ? np sdcl\np? + dp
Word + ?v ?bought, sdcl ? np sdcl\np? + dv
POS + ?v ?VBD, sdcl ? np sdcl\np? + dv
Table 3: Basic and dependency features from Clark &
Curran?s (2007) normal form model; distances are in in-
tervening words, punctuation marks and verbs, and are
capped at 3, 3 and 2, respectively
categorization as well as syntactic features (e.g.
number and agreement). OpenCCG is a pars-
ing/generation library which includes a hybrid
symbolic-statistical chart realizer (White, 2006;
White and Rajkumar, 2009). The input to the
OpenCCG realizer is a semantic graph, where each
node has a lexical predication and a set of seman-
tic features; nodes are connected via dependency re-
lations. Internally, such graphs are represented us-
ing Hybrid Logic Dependency Semantics (HLDS),
a dependency-based approach to representing lin-
guistic meaning (Baldridge and Kruijff, 2002). Al-
ternative realizations are ranked using integrated n-
gram or averaged perceptron scoring models. In the
experiments reported below, the inputs are derived
from the gold standard derivations in the CCGbank
(Hockenmaier and Steedman, 2007), and the outputs
are the highest-scoring realizations found during the
realizer?s chart-based search.1
3 Feature Design
In the realm of paraphrasing using tree lineariza-
tion, Kempen and Harbusch (2004) explore features
which have later been appropriated into classifica-
tion approaches for surface realization (Filippova
and Strube, 2007). Prominent features include in-
1The realizer can also be run using inputs derived from
OpenCCG?s parser, though informal experiments suggest that
parse errors tend to decrease generation quality.
formation status, animacy and phrase length. In the
case of ranking models for surface realization, by far
the most comprehensive experiments involving lin-
guistically motivated features are reported in work
of Cahill for German realization ranking (Cahill et
al., 2007; Cahill and Riester, 2009). Apart from
language model and Lexical Functional Grammar
(LFG) c-structure and f -structure based features,
Cahill also designed and incorporated features mod-
eling information status considerations.
The feature sets explored in this paper ex-
tend those in previous work on realization ranking
with OpenCCG using averaged perceptron models
(White and Rajkumar, 2009; Rajkumar et al2009;
Rajkumar and White, 2010) to include more com-
prehensive ordering features. The feature classes
are listed below, where DEPLEN, HOCKENMAIER
and DEPORD are novel, and the rest are as in ear-
lier OpenCCG models. The inclusion of the DE-
PORD features is intended to yield a model with a
similarly rich set of ordering features as Cahill and
Forster?s (2009) realization ranking model for Ger-
man. Except where otherwise indicated, features are
integer-valued, representing counts of occurrences
in a derivation.
DEPLEN The total of the length between all se-
mantic heads and dependents for a realization,
where length is in intervening words2 exclud-
ing punctuation. For length purposes, collapsed
named entities were counted as a single word in
the experiments reported here.
NGRAMS The log probabilities of the word se-
quence scored using three different n-gram
models: a trigram word model, a trigram
word model with named entity classes replac-
ing words, and a trigram model over POS tags
and supertags.
HOCKENMAIER As an extra component of the
generative baseline, the log probability of the
derivation according to (a reimplementation
2We also experimented with two other definitions of depen-
dency length described in the literature, namely (1) counting
only nouns and verbs to approximate counting by discourse ref-
erents (Gibson, 1998) and (2) omitting function words to ap-
proximate prosodic weight (Anttila et al2010); however, re-
alization ranking accuracy was slightly worse than counting all
non-punctuation words.
247
Feature Type Example
HeadBroadPos + Rel + Precedes + HeadWord + DepWord ?VB, Arg0, dep, wants, he?
. . . + HeadWord + DepPOS ?VB, Arg0, dep, wants, PRP?
. . . + HeadPOS + DepWord ?VB, Arg0, dep, VBZ, he?
. . . + HeadWord + DepPOS ?VB, Arg0, dep, VBZ, PRP?
HeadBroadPos + Side + DepWord1 + DepWord2 ?NN, left, an, important?
. . . + DepWord1 + DepPOS2 ?NN, left, an, JJ?
. . . + DepPOS1 + DepWord2 ?NN, left, DT, important?
. . . + DepPOS1 + DepPOS2 ?NN, left, DT, JJ?
. . . + Rel1 + Rel2 ?NN, left, Det, Mod?
Table 4: Basic head-dependent and sibling dependent ordering features
of) Hockenmaier?s (2003) generative syntactic
model.
DISCRIMINATIVE NGRAMS Sequences from each
of the n-gram models in the perceptron model.
AGREEMENT Features for subject-verb and ani-
macy agreement as well as balanced punctua-
tion.
C&C NF BASE The features from Clark & Cur-
ran?s (2007) normal form model, listed in Ta-
ble 3, minus the distance features.
C&C NF DISTANCE The distance features from
the C&C normal form model, where the dis-
tance between a head and its dependent is mea-
sured in intervening words, punctuation marks
or verbs; caps of 3, 3 and 2 (resp.) on the
distances have the effect of binning longer dis-
tances.
DEPORD Several classes of features for ordering
heads and dependents as well as sibling depen-
dents on the same side of the head. The ba-
sic features?using words, POS tags and de-
pendency relations, grouped by the broad POS
tag of the head?are shown in Table 4. There
are also similar features using words and a
word class (instead of words and POS tags),
where the class is either the named entity class,
COLOR for color words, PRO for pronouns,
one of 60-odd suffixes culled from the web, or
HYPHEN or CAP for hyphenated or capital-
ized words. Additionally, there are features for
detecting definiteness of an NP or PP (where
the definiteness value is used in place of the
POS tag).
Model # Alph Feats # Model Feats
GLOBAL 4 4
DEPLEN-GLOBAL 5 5
DEPORD-NONF 790,887 269,249
DEPORD-NODIST 1,035,915 365,287
DEPLEN-NODIST 1,035,916 366,094
DEPORD-NF 1,173,815 431,226
DEPLEN 1,173,816 428,775
Table 6: Model sizes?number of features in alphabet for
each model (satisfying count cutoff of 5) along with num-
ber active in model after 5 training epochs
4 Evaluation
4.1 Experimental Conditions
We followed the averaged perceptron training proce-
dure of White and Rajkumar (2009) with a couple of
updates. First, as noted earlier, we used a reimple-
mentation of Hockenmaier?s (2003) generative syn-
tactic model as an extra component of our genera-
tive baseline; and second, only five epochs of train-
ing were used, which was found to work as well as
using additional epochs on the development set. As
in the earlier work, the models were trained on the
standard training sections (02?21) of an enhanced
version of the CCGbank, using a lexico-grammar
extracted from these sections.
The models tested in the experiments reported be-
low are summarized in Table 5. The three groups
of models are designed to test the impact of the
dependency length feature when added to feature
sets of increasing complexity. In more detail,
the GLOBAL and DEPLEN-GLOBAL models contain
dense features on entire derivations; their values
are the log probabilities of the three n-gram mod-
248
Model Dep Ngram Hocken- Discr Agree- C&C NF C&C NF Dep
Len Mods maier Ngrams ment Base Dist Ord
GLOBAL N Y Y N N N N N
DEPLEN-GLOBAL Y Y Y N N N N N
DEPORD-NONF N Y Y Y Y N N Y
DEPORD-NODIST N Y Y Y Y Y N Y
DEPLEN-NODIST Y Y Y Y Y Y N Y
DEPORD-NF N Y Y Y Y Y Y Y
DEPLEN Y Y Y Y Y Y Y Y
Table 5: Legend for experimental conditions
els used in the earlier work along with the Hock-
enmaier model (and the dependency length feature,
in DEPLEN-GLOBAL). The second group is cen-
tered on DEPORD-NODIST, which contains all fea-
tures except the dependency length feature and the
distance features in Clark & Curran?s normal form
model, which may indirectly capture some depen-
dency length minimization preferences. In addition
to DEPLEN-NODIST?where the dependency length
feature is added?this group also contains DEPORD-
NONF, which is designed to test (as a side compari-
son) whether the Clark & Curran normal form base
features are still useful even when used in conjunc-
tion with the new dependency ordering features. In
the final group, DEPORD-NF contains all the features
examined in this paper except the dependency length
feature, while DEPLEN contains all the features in-
cluding the dependency length feature. Note that the
learned weight of the total dependency length fea-
ture was negative in each case, as expected.
Table 6 shows the sizes of the various models. For
each model, the alphabet?whose size increases to
over a million features?is the set of applicable fea-
tures found to have discriminative value in at least 5
training examples; from these, a subset are made ac-
tive (i.e., take on a non-zero weight) through percep-
tron updates when the feature value differs between
the model-best and oracle-best realization.
4.2 BLEU Results
Following the usual practice in the realization rank-
ing, we first evaluate our results quantitatively us-
ing exact matches and BLEU (Papineni et al2002),
a corpus similarity metric developed for MT evalu-
ation. Realization results for the development and
Model % Exact BLEU Signif
Sect 00
GLOBAL 33.03 0.8292 -
DEPLEN-GLOBAL 34.73 0.8345 ***
DEPORD-NONF 42.33 0.8534 **
DEPORD-NODIST 43.12 0.8560 -
DEPLEN-NODIST 43.87 0.8587 ***
DEPORD-NF 43.44 0.8590 -
DEPLEN 44.56 0.8610 **
Sect 23
GLOBAL 34.75 0.8302 -
DEPLEN-GLOBAL 34.70 0.8330 ***
DEPORD-NODIST 41.42 0.8561 -
DEPLEN-NODIST 42.95 0.8603 ***
DEPORD-NF 41.32 0.8577 -
DEPLEN 42.05 0.8596 **
Table 7: Development (Section 00) & test (Section 23)
set results?exact match percentage and BLEU scores,
along with statistical significance of BLEU compared to
the unmarked model in each group (* = p < 0.1, ** =
p < 0.05, *** = p < 0.01); significant within-group
winners (at p < 0.05) are shown in bold
test sections appear in Table 7. For all three model
groups, the dependency length feature yields signif-
icant increases in BLEU scores, even in compar-
ison to the model (DEPORD-NF) containing Clark
& Curran?s distance features in addition to the new
dependency ordering features (as well as all other
features but total dependency length). The second
group additionally shows that the Clark & Curran
normal form base features do indeed have a signif-
icant impact on BLEU scores even when used with
249
Model % DL % DL DL Signif
Lower Greater Mean
GOLD n.a. n.a. 41.02 -
GLOBAL 17.23 21.59 42.40 ***
DEPLEN-GLOBAL 24.37 12.81 40.29 ***
DEPORD-NONF 15.76 19.34 42.34 ***
DEPORD-NODIST 14.58 19.06 42.03 ***
DEPLEN-NODIST 17.75 14.82 40.87 n.s.
DEPORD-NF 14.96 17.65 41.58 ***
DEPLEN 16.28 14.78 40.97 n.s.
Table 8: Dependency length compared to corpus?
percentage of realizations with dependency length less
than and greater than gold standard, along with mean
dependency length, whose significance is tested against
gold; 1671 development set (Section 00) complete real-
izations analyzed
the new dependency ordering model, as DEPORD-
NONF is significantly worse than DEPORD-NODIST
(the impact of the distance features is evident in the
increases from the second group to the third group).
As with the dev set, the dependency length feature
yielded a significant increase in BLEU scores for
each comparison on the test set al
For each group, the statistical significance of the
difference in BLEU scores between a model and the
unmarked model (-) is determined by bootstrap re-
sampling (Koehn, 2004).3 Note that although the
differences in BLEU scores are small, they end
up being statistically significant because the mod-
els frequently yield the same top scoring realiza-
tion, and reliably deliver improvements in the cases
where they differ. In particular, note that DEPLEN
and DEPORD-NF agree on the best realization 81%
of the time, while DEPLEN-NODIST and DEPORD-
NODIST have 78.1% agreement, and DEPLEN-
GLOBAL and GLOBAL show 77.4% agreement; by
comparison, DEPORD-NODIST and GLOBAL only
agree on the best realization 51.1% of the time.
4.3 Detailed Analyses
The effect of the dependency length feature on the
distribution of dependency lengths is illustrated in
Table 8. The table shows the mean of the total de-
pendency length of each realized derivation com-
3Kudos to Kevin Gimpel for making his resampling
scripts available from http://www.ark.cs.cmu.edu/
MT/paired_bootstrap_v13a.tar.gz.
Model % Short % Long % Eq % Single
/ Long / Short Constit
GOLD 25.25 4.87 4.08 65.79
GLOBAL 23.15 7.86 3.94 65.04
DEPLEN-GLOBAL 24.58 5.57 4.09 65.76
DEPORD-NONF 23.13 6.61 4.03 66.23
DEPORD-NODIST 23.38 6.52 3.94 66.15
DEPLEN-NODIST 24.03 5.38 4.01 66.58
DEPORD-NF 23.74 5.92 3.96 66.40
DEPLEN 24.36 5.36 4.07 66.21
Table 9: Distribution of various kinds of post-verbal con-
stituents in the development set (Section 00); 4692 gold
cases considered
pared to the corresponding gold standard derivation,
as well as the number of derivations with greater and
lower dependency length. According to paired t-
tests, the mean dependency lengths for the DEPLEN-
NODIST and DEPLEN models do not differ signifi-
cantly from the gold standard. In contrast, the mean
dependency length of all the models that do not in-
clude the dependency length feature does differ sig-
nificantly (p < 0.001) from the gold standard. Ad-
ditionally, all these models have more realizations
with dependency length greater than the gold stan-
dard, in comparison to the dependency length min-
imizing models; this shows the efficacy of the de-
pendency length feature in approximating the gold
standard. Interestingly, the DEPLEN-GLOBAL model
significantly undershoots the gold standard on mean
dependency length, and has the most skewed dis-
tribution of sentences with greater vs. lesser depen-
dency length than the gold standard.
Apart from studying dependency length directly,
we also looked at one of the attested effects of de-
pendency length minimization, viz. the tendency to
prefer short-long post-verbal constituents in produc-
tion (Temperley, 2007). The relative lengths of ad-
jacent post-verbal constituents were computed and
their distribution is shown in Table 9. While cal-
culating length, punctuation marks were excluded.
Four kinds of constituents were found in the post-
verbal domain. For every verb, apart from single
constituents and equal length constituents, short-
long and long-short sequences were also observed.
Table 9 demonstrates that for both the gold standard
corpus as well as the realizer models, short-long
constituents were more frequent than long-short or
equal length constituents. This follows the trend re-
250
Model % Light % Heavy Signif
/ Heavy / Light
GOLD 8.60 0.36 -
GLOBAL 7.73 2.02 ***
DEPLEN-GLOBAL 8.35 0.75 **
DEPORD-NONF 7.98 1.15 ***
DEPORD-NODIST 8.04 1.12 ***
DEPLEN-NODIST 8.23 0.45 n.s.
DEPORD-NF 8.26 0.71 **
DEPLEN 8.36 0.51 n.s.
Table 10: Distribution of heavy unequal constituents
(length difference > 5) in Section 00; 4692 gold cases
considered and significance tested against the gold stan-
dard using a ?-square test
ported by previous corpus studies of English (Tem-
perley, 2007; Wasow and Arnold, 2003). The figures
reported here show the tendency of the DEPLEN*
models to be closer to the gold standard than the
other models, especially in the case of short-long
constituents.
We also performed an analysis of relative con-
stituent lengths focusing on light-heavy and heavy-
light cases; specifically, we examined unequal
length constituent sequences where the length dif-
ference of the constituents was greater than 5, and
the shorter constituent was under 5 words. Table 10
shows the results. Using a ?-square test, the distri-
bution of heavy unequal length constituent counts in
the DEPLEN-NODIST and DEPLEN models does not
significantly differ from that of the gold standard. In
contrast, for all the other models, the counts do dif-
fer significantly from the gold standard.
4.4 Examples
Table 11 shows examples of how the dependency
length feature (DEPLEN) affects the output even in
comparison to a model (DEPORD) with a rich set
of discriminative syntactic and dependency order-
ing features, but no features directly targeting rel-
ative weight. In wsj 0015.7, the dependency length
model produces an exact match, while the DEPORD
model fails to shift the short temporal adverbial next
year next to the verb, leaving a confusingly repeti-
tive this year next year at the end of the sentence.
In wsj 0020.1, the dependency length model pro-
duces a nearly exact match with just an equally ac-
ceptable inversion of closely watching. By contrast,
the DEPORD model mistakenly shifts the direct ob-
ject South Korea, Taiwan and Saudia Arabia to the
end of the sentence where it is difficult to under-
stand following two very long intervening phrases.
In wsj 0021.8, both models mysteriously put not in
front of the auxiliary and leave out the complemen-
tizer, but DEPORD also mistakenly leaves before at
the end of the verb phrase where it is again apt to
be interpreted as modifying the preceding verb. In
wsj 0075.13, both models put the temporal modi-
fier on Thursday in its canonical VP-final position,
despite this order running counter to dependency
length minimization. Finally, wsj 0014.2 shows a
case where DEPORD is nearly an exact match (except
for a missing comma), but the dependency length
model fronts the PP on the 12-member board, where
it is grammatical but rather marked (and not moti-
vated in the discourse context).
4.5 Interim Discussion
The experiments show a consistent positive effect of
the dependency length feature in improving BLEU
scores and achieving a better match with the corpus
distributions of dependency length and short/long
constituent orders. The results in Table 10 are partic-
ulary encouraging, as they show that minimizing de-
pendency length reduces the number of realizations
in which a heavy constituent precedes a light one
down to essentially the level of the corpus, thereby
eliminating many realizations that can be expected
to have egregious errors like those shown in Ta-
ble 11.
Intriguingly, there is some evidence that a nega-
tively weighted total dependency length feature can
go too far in minimizing dependency length, in the
absence of other informative features to counterbal-
ance it. In particular, the DEPLEN-GLOBAL model in
Table 8 has significantly lower dependency length
than the corpus, but in the richer models with dis-
criminative synactic and dependency ordering fea-
tures, there are no significant differences. It may still
be though that additional features are necessary to
counteract the tendency towards dependency length
minimization, for example to ensure that initial con-
stituents play their intended role in establishing and
continuing topics in discourse, as also observed in
Table 11.
251
wsj 0015.7 the exact amount of the refund will be determined next year based on actual collections made until
Dec. 31 of this year .
DEPLEN [same]
DEPORD the exact amount of the refund will be determined based on actual collections made until Dec. 31
of this year next year .
wsj 0020.1 the U.S. , claiming some success in its trade diplomacy , removed South Korea , Taiwan and Saudi
Arabia from a list of countries it is closely watching for allegedly failing to honor U.S. patents ,
copyrights and other intellectual-property rights .
DEPLEN the U.S. claiming some success in its trade diplomacy , removed South Korea , Taiwan and Saudi
Arabia from a list of countries it is watching closely for allegedly failing to honor U.S. patents ,
copyrights and other intellectual-property rights .
DEPORD the U.S. removed from a list of countries it is watching closely for allegedly failing to honor U.S.
patents , copyrights and other intellectual-property rights , claiming some success in its trade diplo-
macy , South Korea , Taiwan and Saudi Arabia .
wsj 0021.8 but he has not said before that the country wants half the debt forgiven .
DEPLEN but he not has said before ? the country wants half the debt forgiven .
DEPORD but he not has said ? the country wants half the debt forgiven before .
wsj 0075.13 The Treasury also said it plans to sell [$ 10 billion] [in 36-day cash management bills] [on Thurs-
day].
DEPLEN [same]
DEPORD [same]
wsj 0014.2 they succeed Daniel M. Rexinger , retired Circuit City executive vice president , and Robert R.
Glauber , U.S. Treasury undersecretary , on the 12-member board .
DEPORD they succeed Daniel M. Rexinger , retired Circuit City executive vice president , and Robert R.
Glauber , U.S. Treasury undersecretary ? on the 12-member board .
DEPLEN on the 12-member board they succeed Daniel M. Rexinger , retired Circuit City executive vice
president , and Robert R. Glauber , U.S. Treasury undersecretary .
Table 11: Examples of realized output for full models with and without the dependency length feature
4.6 Targeted Human Evaluation
To determine whether heavy-light ordering differ-
ences often represent ordering errors (including
egregious ones), rather than simply representing ac-
ceptable variation, we conducted a targeted human
evaluation on examples of this kind. Specifically,
for each of the DEPLEN* models and their corre-
sponding models without the dependency length fea-
ture, we chose the 25 sentences from the develop-
ment section whose realizations exhibited the great-
est difference in dependency length between sibling
constituents appearing in opposite orders, and asked
two judges (not the authors) to choose which of the
two realizations best expressed the meaning of the
reference sentence in a grammatical and fluent way,
with the choice forced (2AFC). Table 12 shows the
results. Agreement between the judges was high,
Model % Preferred % Agr Signif
GLOBAL 22 - -
DEPLEN-GLOBAL 78 84 ***
DEPORD-NODIST 24 - -
DEPLEN-NODIST 76 92 ***
DEPORD-NF 26 - -
DEPLEN 74 96 ***
Table 12: Targeted human evaluation?percentage of re-
alizations preferred by two human judges in a 2AFC test
among the 25 development set sentences with the great-
est differences in dependency length, with a binomial test
for significance
252
with only one disagreement on the realizations from
the DEPLEN and DEPORD-NF models (involving an
acceptable paraphrase in our judgment), and only
four disagreements on the DEPLEN-GLOBAL and
GLOBAL realizations. Pooling the judgments, the
preference for the DEPLEN* models was well above
the chance level of 50% according to a binomial test
(p < 0.001 in each case). Inspecting the data our-
selves, we found that many of the items did indeed
involve egregious ordering errors that the DEPLEN*
models managed to avoid.
5 Related Work
As noted in the introduction, to the best of our
knowledge this paper is the first to examine the im-
pact of dependency length minimization on realiza-
tion ranking. While there have been quite a few
papers to date reporting results on Penn Treebank
data, since the various systems make different as-
sumptions regarding the specificity of their inputs,
all but the most broad-brushed comparisons remain
impossible at present, and thus detailed studies such
as the present one can only be made within the con-
text of different models for the same system. Some
progress on this issue has been made in the con-
text of the Generation Challenges Surface Realiza-
tion Shared Task (Belz et al2011), but it remains
to be seen to what extent fair cross-system compar-
isons using common inputs can be achieved.
For (very) rough comparison purposes, Table 13
lists our results in the context of those reported for
various other systems on PTB Section 23. As the
table shows, the OpenCCG scores are quite com-
petitive, exceeded only by Callaway?s (2005) ex-
tensively hand-crafted system as well as Bohnet et
al.?s (2011) system on shared task shallow inputs
(-S), which performs much better than their sys-
tem on deep inputs (-D) that more closely resemble
OpenCCG?s.
6 Conclusions
In this paper, we have investigated dependency
length minimization in the context of realization
ranking, focusing on its potential to eliminate egre-
gious ordering errors as well as better match the dis-
tributional characteristics of sentence orderings in
news text. When added to a state-of-the-art, com-
System Coverage BLEU % Exact
Callaway (05) 98.5% 0.9321 57.5
Bohnet et al (11) 100% 0.8911
OpenCCG (12) 97.1% 0.8596 42.1
OpenCCG (09) 97.1% 0.8506 40.5
Ringger et al04) 100% 0.836 35.7
Bohnet et al (11) 100% 0.7943
Langkilde-Geary (02) 83% 0.757 28.2
Guo et al08) 100% 0.7440 19.8
Hogan et al07) ?100% 0.6882
OpenCCG (08) 96.0% 0.6701 16.0
Nakanishi et al05) 90.8% 0.7733
Table 13: PTB Section 23 BLEU scores and exact match
percentages in the NLG literature (Nakanishi et al re-
sults are for sentences of length 20 or less)
prehensive realization ranking model, we showed
that including a dense, global feature for minimiz-
ing total dependency length yields statistically sig-
nificant improvements in BLEU scores and signif-
icantly reduces the number of heavy-light ordering
errors. Going beyond the BLEU metric, we also
conducted a targeted human evaluation to confirm
the utility of the dependency length feature in mod-
els of varying richness. Interestingly, even with the
richest model, in some cases we found that the de-
pendency length feature still appears to go too far in
minimizing dependency length, suggesting that fur-
ther counter-balancing features?especially ones for
the sentence-initial position (Filippova and Strube,
2009)?warrant investigation in future work.
Acknowledgments
This work was supported in part by NSF grants no.
IIS-1143635 and IIS-0812297. We thank the anony-
mous reviewers for helpful comments and discus-
sion, and Scott Martin and Dennis Mehay for their
participation in the targeted human evaluation.
253
References
Arto Anttila, Matthew Adams, and Mike Speriosu. 2010.
The role of prosody in the English dative alternation.
Language and Cognitive Processes.
Jennifer E. Arnold, Thomas Wasow, Anthony Losongco,
and Ryan Ginstrom. 2000. Heaviness vs. newness:
The effects of structural complexity and discourse sta-
tus on constituent ordering. Language, 76:28?55.
Jason Baldridge and Geert-Jan Kruijff. 2002. Coupling
CCG and Hybrid Logic Dependency Semantics. In
Proc. ACL-02.
Anja Belz, Mike White, Dominic Espinosa, Eric Kow,
Deirdre Hogan, and Amanda Stent. 2011. The
first surface realisation shared task: Overview and
evaluation results. In Proceedings of the Genera-
tion Challenges Session at the 13th European Work-
shop on Natural Language Generation, pages 217?
226, Nancy, France, September. Association for Com-
putational Linguistics.
Bernd Bohnet, Simon Mille, Beno??t Favre, and Leo Wan-
ner. 2011. <stumaba >: From deep representation to
surface. In Proceedings of the Generation Challenges
Session at the 13th European Workshop on Natural
Language Generation, pages 232?235, Nancy, France,
September. Association for Computational Linguis-
tics.
H Branigan, M Pickering, and M Tanaka. 2008. Con-
tributions of animacy to grammatical function assign-
ment and word order during production. Lingua,
118(2):172?189.
Joan Bresnan, Anna Cueni, Tatiana Nikitina, and R. Har-
ald Baayen. 2007. Predicting the Dative Alternation.
Cognitive Foundations of Interpretation, pages 69?94.
Aoife Cahill and Arndt Riester. 2009. Incorporating in-
formation status into generation ranking. In Proceed-
ings of, ACL-IJCNLP ?09, pages 817?825, Morris-
town, NJ, USA. Association for Computational Lin-
guistics.
Aoife Cahill, Martin Forst, and Christian Rohrer. 2007.
Designing features for parse disambiguation and real-
isation ranking. In Miriam Butt and Tracy Holloway
King, editors, Proceedings of the 12th International
Lexical Functional Grammar Conference, pages 128?
147. CSLI Publications, Stanford.
Charles Callaway. 2005. The types and distributions
of errors in a wide coverage surface realizer evalua-
tion. In Proceedings of the 10th European Workshop
on Natural Language Generation.
Stephen Clark and James R. Curran. 2007. Wide-
Coverage Efficient Statistical Parsing with CCG and
Log-Linear Models. Computational Linguistics,
33(4):493?552.
Vera Demberg and Frank Keller. 2008. Data from eye-
tracking corpora as evidence for theories of syntactic
processing complexity. Cognition, 109(2):193?210.
Katja Filippova and Michael Strube. 2007. Generating
constituent order in German clauses. In ACL 2007,
Proceedings of the 45th Annual Meeting of the As-
sociation for Computational Linguistics, June 23-30,
2007, Prague, Czech Republic. The Association for
Computer Linguistics.
Katja Filippova and Michael Strube. 2009. Tree lin-
earization in English: Improving language model
based approaches. In Proceedings of Human Lan-
guage Technologies: The 2009 Annual Conference of
the North American Chapter of the Association for
Computational Linguistics, Companion Volume: Short
Papers, pages 225?228, Boulder, Colorado, June. As-
sociation for Computational Linguistics.
Edward Gibson. 1998. Linguistic complexity: Locality
of syntactic dependencies. Cognition, 68:1?76.
Edward Gibson. 2000. Dependency locality theory:
A distance-based theory of linguistic complexity. In
Alec Marantz, Yasushi Miyashita, and Wayne O?Neil,
editors, Image, Language, brain: Papers from the First
Mind Articulation Project Symposium. MIT Press,
Cambridge, MA.
Daniel Gildea and David Temperley. 2007. Optimizing
grammars for minimum dependency length. In Pro-
ceedings of the 45th Annual Meeting of the Association
of Computational Linguistics, pages 184?191, Prague,
Czech Republic, June. Association for Computational
Linguistics.
Yuqing Guo, Josef van Genabith, and Haifeng Wang.
2008. Dependency-based n-gram models for general
purpose sentence realisation. In Proc. COLING-08.
John A. Hawkins. 1994. A Performance Theory of Order
and Constituency. Cambridge University Press, New
York.
John A. Hawkins. 2000. The relative order of
prepositional phrases in English: Going beyond
manner-place-time. Language Variation and Change,
11(03):231?266.
John A. Hawkins. 2001. Why are categories adjacent?
Journal of Linguistics, 37:1?34.
Julia Hockenmaier and Mark Steedman. 2007. CCG-
bank: A Corpus of CCG Derivations and Dependency
Structures Extracted from the Penn Treebank. Com-
putational Linguistics, 33(3):355?396.
Julia Hockenmaier. 2003. Data and models for statis-
tical parsing with Combinatory Categorial Grammar.
Ph.D. thesis, University of Edinburgh.
Deirdre Hogan, Conor Cafferkey, Aoife Cahill, and Josef
van Genabith. 2007. Exploiting multi-word units
in history-based probabilistic generation. In Proc.
EMNLP-CoNLL.
254
Gerard Kempen and Karin Harbusch. 2004. Generat-
ing natural word orders in a semi-free word order lan-
guage: Treebank-based linearization preferences for
German. In Alexander F. Gelbukh, editor, CICLing,
volume 2945 of Lecture Notes in Computer Science,
pages 350?354. Springer.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Dekang Lin and
Dekai Wu, editors, Proceedings of EMNLP 2004,
pages 388?395, Barcelona, Spain, July. Association
for Computational Linguistics.
Irene Langkilde-Geary. 2002. An empirical verification
of coverage and correctness for a general-purpose sen-
tence generator. In Proc. INLG-02.
R. L. Lewis and S. Vasishth. 2005. An activation-based
model of sentence processing as skilled memory re-
trieval. Cognitive Science, 29:1?45, May.
Richard L. Lewis, Shravan Vasishth, and Julie Van Dyke.
2006. Computational principles of working memory
in sentence comprehension. Trends in Cognitive Sci-
ences, 10(10):447?454.
Hiroko Nakanishi, Yusuke Miyao, and Jun?ichi Tsujii.
2005. Probabilistic methods for disambiguation of an
HPSG-based chart generator. In Proc. IWPT-05.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proc. ACL-02.
Rajakrishnan Rajkumar and Michael White. 2010. De-
signing agreement features for realization ranking.
In Coling 2010: Posters, pages 1032?1040, Beijing,
China, August. Coling 2010 Organizing Committee.
Rajakrishnan Rajkumar, Michael White, and Dominic
Espinosa. 2009. Exploiting named entity classes in
CCG surface realization. In Proceedings of Human
Language Technologies: The 2009 Annual Conference
of the North American Chapter of the Association for
Computational Linguistics, Companion Volume: Short
Papers, pages 161?164, Boulder, Colorado, June. As-
sociation for Computational Linguistics.
Eric Ringger, Michael Gamon, Robert C. Moore, David
Rojas, Martine Smets, and Simon Corston-Oliver.
2004. Linguistically informed statistical models of
constituent structure for ordering in sentence realiza-
tion. In Proc. COLING-04.
Neal Snider and Annie Zaenen. 2006. Animacy and syn-
tactic structure: Fronted NPs in English. In M. Butt,
M. Dalrymple, and T.H. King, editors, Intelligent Lin-
guistic Architectures: Variations on Themes by Ronald
M. Kaplan. CSLI Publications, Stanford.
Mark Steedman. 2000. The Syntactic Process. MIT
Press.
David Temperley. 2007. Minimization of dependency
length in written English. Cognition, 105(2):300 ?
333.
Harry Tily. 2010. The Role of Processing Complexity
in Word Order Variation and Change. Ph.D. thesis,
Stanford University.
Erik Velldal and Stefan Oepen. 2005. Maximum entropy
models for realization ranking. In Proc. MT-Summit
X.
Thomas Wasow and Jennifer Arnold. 2003. Post-verbal
Constituent Ordering in English. Mouton.
Tom Wasow. 2002. Postverbal Behavior. CSLI Publica-
tions, Stanford.
Michael White and Rajakrishnan Rajkumar. 2009. Per-
ceptron reranking for CCG realization. In Proceedings
of the 2009 Conference on Empirical Methods in Nat-
ural Language Processing, pages 410?419, Singapore,
August. Association for Computational Linguistics.
Michael White. 2006. Efficient Realization of Coordi-
nate Structures in Combinatory Categorial Grammar.
Research on Language & Computation, 4(1):39?75.
Hiroko Yamashita and Franklin Chang. 2001. ?Long
before short? preference in the production of a head-
final language. Cognition, 81.
255
Generating Tailored, Comparative
Descriptions with Contextually
Appropriate Intonation
Michael White?
The Ohio State University
Robert A. J. Clark??
University of Edinburgh
Johanna D. Moore?
University of Edinburgh
Generating responses that take user preferences into account requires adaptation at all levels of
the generation process. This article describes a multi-level approach to presenting user-tailored
information in spoken dialogues which brings together for the first time multi-attribute decision
models, strategic content planning, surface realization that incorporates prosody prediction, and
unit selection synthesis that takes the resulting prosodic structure into account. The system
selects the most important options to mention and the attributes that are most relevant to
choosing between them, based on the user model. Multiple options are selected when each offers a
compelling trade-off. To convey these trade-offs, the system employs a novel presentation strategy
which straightforwardly lends itself to the determination of information structure, as well as the
contents of referring expressions. During surface realization, the prosodic structure is derived
from the information structure using Combinatory Categorial Grammar in a way that allows
phrase boundaries to be determined in a flexible, data-driven fashion. This approach to choosing
pitch accents and edge tones is shown to yield prosodic structures with significantly higher
acceptability than baseline prosody prediction models in an expert evaluation. These prosodic
structures are then shown to enable perceptibly more natural synthesis using a unit selection
voice that aims to produce the target tunes, in comparison to two baseline synthetic voices. An
expert evaluation and f0 analysis confirm the superiority of the generator-driven intonation and
its contribution to listeners? ratings.
1. Introduction
In an evaluation of nine spoken dialogue information systems, developed as part of the
DARPA Communicator program, the information presentation phase of the dialogues
? 1712 Neil Ave., Columbus, OH 43210, USA. Web: http://www.ling.ohio-state.edu/~mwhite/.
?? 10 Crichton Street, Edinburgh, Scotland EH8 1AB, UK. Web:
http://www.cstr.ed.ac.uk/ssi/people/robert.html.
? 10 Crichton Street, Edinburgh, Scotland EH8 1AB, UK. Web: http://www.hcrc.ed.ac.uk/~jmoore/.
Submission received: 31 January 2008; revised submission received: 19 May 2009; accepted for publication:
24 September 2009.
? 2010 Association for Computational Linguistics
Computational Linguistics Volume 36, Number 2
Figure 1
Typical information presentation phase of a Communicator dialogue.
was found to be the primary contributor to dialogue duration (Walker, Passonneau,
and Boland 2001). During this phase, the typical system sequentially presents the set of
options that match the user?s constraints, as shown in Figure 1. The user can then navi-
gate through these options and refine them by offering new constraints. When multiple
options are returned, this process can be exacting, leading to reduced user satisfaction.
As Walker et al (2004) observe, having to access the set of available options sequen-
tially makes it hard for the user to remember information relevant to making a decision.
To reduce user memory load, we need alternative strategies for sequential presentation.
In particular, we require better algorithms for:
1. selecting the most relevant subset of options to mention, as well as the
attributes that are most relevant to choosing among them; and
2. determining how to organize and express the descriptions of the selected
options and attributes, in ways that are both easy to understand and
memorable.1
In this article, we describe how we have addressed these points in the FLIGHTS2
system, reviewing and extending the description given in Moore et al (2004). FLIGHTS
follows previous work (Carberry, Chu-Carroll, and Elzer 1999; Carenini and Moore
2000; Walker et al 2002) in applying decision-theoretic models of user preferences to
the generation of tailored descriptions of the most relevant available options. Multi-
attribute decision theory provides a detailed account of howmodels of user preferences
can be used in decision making (Edwards and Barron 1994). Such preference models
have been shown to enable systems to present information in ways that are concise and
1 An issue we do not address in this article is whether a multimodal system would be more effective than a
voice-only one. We believe that these needs, and in particular the need to express information with
contextually appropriate prosody, are also highly relevant for multimodal systems. We also note that
there is still strong demand for voice-oriented systems for eyes-busy use and for the blind.
2 FLIGHTS stands for Fancy Linguistically Informed Generation of Highly Tailored Speech.
160
White, Clark, and Moore Generating Tailored Descriptions with Appropriate Intonation
tailored to the user?s interests (Carenini and Moore 2001; Walker et al 2004; Carenini
and Moore 2006). Decision-theoretic models have also been commercially deployed in
Web systems.3
To present multiple options, we introduce a novel strategy where the best option
(with respect to the user model) is presented first, followed by the most compelling
remaining options, in terms of trade-offs between attributes that are important to the
user. (Multiple options are selected only when each offers a compelling trade-off.) An
important property of this strategy is that it naturally lends itself to the determination
of information structure, as well as the contents of referring expressions. Thus, to help
make the trade-offs among the selected options clear to the user, FLIGHTS (1) groups
attributes that are positively and negatively valued for the user, (2) chooses referring ex-
pressions that highlight the salient distinguishing attributes, (3) determines information
structure and prosodic structure that express contrasts intelligibly, and (4) synthesizes
utterances with a unit selection voice that takes the prosodic structure into account.
As such, FLIGHTS goes beyond previous systems in adapting its output according to
user preferences at all levels of the generation process, not just at the levels of content
selection and text planning.
Our approach to generating contextually appropriate intonation follows Prevost
(1995) and Steedman (2000a) in using Combinatory Categorial Grammar (CCG) to
convey the information structure of sentences via pitch accents and edge tones. To
adapt Prevost and Steedman?s approach to FLIGHTS, we operationalize the information
structural notion of theme to correspond to implicit questions that necessarily arise
in presenting the trade-offs among options. We also refine the way in which prosodic
structure is derived from information structure by allowing for a more flexible, one-to-
many mapping between themes or rhemes and intonational phrases, where the final
choice of the type and placement of edge tones is determined by n-gram models. To
investigate the impact of information structural grammatical constraints in our hybrid
rule-based, data-driven approach, we compare realizer outputs with those of baseline
n-gram models, and show that the realizer yields target prosodic structures with signif-
icantly higher acceptability than the baseline models in an expert evaluation.
The prosodic structure derived during surface realization is passed as prosodic
markup to the speech synthesizer. The synthesizer uses this prosodic markup in the
text analysis phase of synthesis in place of the structures that it would otherwise have
to predict from the text. The synthesizer then uses the context provided by the markup
to enforce the selection of suitable units from the database. To verify that the prosodic
markup yields improvements in the quality of synthetic speech, we present an exper-
iment which shows that listeners perceive a unit selection voice that aims to produce
the target prosodic structures as significantly more natural than either of two baseline
unit selection voices that do not use the markup. We also present an expert evaluation
and f0 analysis which confirm the superiority of the generator-driven intonation and its
contribution to listeners? ratings.
The remainder of this article is structured as follows. Section 2 presents our ap-
proach to natural language generation (NLG) in the information presentation phase
of a FLIGHTS dialogue, including how multi-attribute decision models are used in
content selection; how rhetorical and information structure are determined during dis-
course planning; how lexical choice and referring expressions are handled in sentence
planning; how prosodic structures are derived in surface realization; and how these
3 See http://www.cogentex.com/solutions/recommender/index.shtml, for example.
161
Computational Linguistics Volume 36, Number 2
Figure 2
Tailored descriptions of the available flights for three different user models.
prosodic structures compare to those predicted by baseline n-gram models in an expert
evaluation. Section 3 describes how the prosodic structures are used in the unit selection
voice employed in the present study, and compares this voice to the baseline ones
used in our perception experiment. Section 4 provides the methods and results of the
perception experiment itself, along with the expert prosody evaluation and f0 analysis.
Section 5 compares our approach to related work. Finally, Section 6 concludes with a
summary and discussion of remaining issues.
2. NLG in FLIGHTS
2.1 Tailoring Flight Descriptions
To illustrate how decision-theoretic models of user preferences can be used to tailor
descriptions of the available options at many points in the generation process, let us
consider the following three hypothetical users of the FLIGHTS system:
Student (S) A student who cares most about price, all else being equal.
Frequent Flyer (FF) A business traveler who prefers business class, but cares most
about building up frequent-flyer miles on KLM.
Business Class (BC) Another business traveler who prefers KLM, but wants, above all,
to travel in business class.
Suppose that each user is interested in flying from Edinburgh to Brussels on a
certain day, and would like to arrive by five o?clock in the afternoon. FLIGHTS begins
the dialogue by gathering the details necessary to query the database for possible flights.
Next, it uses the preferences encoded in the usermodel to select the highest ranked flight
for each user, as well as those flights that offer interesting trade-offs. These flights are
then described to the user, as shown in Figure 2.4
For the student (S), the BMI flight is rated most highly, because it is a fairly in-
expensive, direct flight that arrives near the desired time. The Ryanair flight is also
4 The set of available flights was obtained by ?screen scraping? data from several online sources. The
hypothetical user preferences were chosen with an eye towards making the example interesting. Actual
user preferences are specified as part of registering to use the FLIGHTS system.
162
White, Clark, and Moore Generating Tailored Descriptions with Appropriate Intonation
mentioned as a possibility, as it has the best price; it ends up ranked lower overall than
the BMI flight though, because it requires a connection and arrives well in advance of
the desired arrival time. For the KLM frequent flyer (FF), life is a bit more complicated:
A KLM flight with a good arrival time is offered as the top choice, even though it is a
connecting flight with no availability in business class. As alternatives, the direct flight
on BMI (with no business class availability) and the British Airways flight with seats
available in business class (but requiring a connection) are described. Finally, for the
must-have-business-class traveler (BC), the British Airways flight with business class
available is presented first, despite its requiring a connection; the direct flight on BMI is
offered as another possibility.
Although user preferences have an immediately apparent impact on content se-
lection and ordering, they also have more subtle effects on many aspects of how the
selected content is organized and expressed, as explained subsequently.
Referring expressions: Rather than always referring to the available flights in the same
way, flights of interest are instead described using the attributes most relevant to
the user: for example, direct flight, cheapest flight, KLM flight.
Aggregation: For conciseness, multiple attributes may be given in a single sentence,
subject to the constraint that attributes whose values are positive (or negative)
for the user should be kept together. For example, in There?s a KLM flight arriving
Brussels at four fifty p.m., but business class is not available and you?d need to connect in
Amsterdam, the values of the attributes airline and arrival-time are considered
good, and thus are grouped together to contrast with the values of the attributes
fare-class and number-of-legs, which are considered bad.
Scalar terms: Scalar modifiers like good, as in good price, and just, as in just fifty pounds,
are chosen to characterize an attribute?s value to the user relative to values of the
same attribute for other options.
Discourse cues: Attributes with negative values for the user are acknowledged using
discourse cues, such as but and though. Interesting trade-offs can also be signaled
using cues such as if -conditionals.
Information structure and prosody: Compelling trade-offs are always indicated via
prosodic phrasing and emphasis, as in (There ARE seats in business class)theme (on
the British Airways flight)rheme (that arrives at four twenty p.m.)rheme, where the di-
vision of the sentence into theme and rheme phrases is shown informally using
parentheses, and contrastive emphasis (on ARE) is shown using small caps. (See
Section 2.6.2 for details of how emphasis and phrasing are realized by pitch
accents and edge tones.)
2.2 Architecture
The architecture of the FLIGHTS generator appears in Figure 3. OAA (Martin, Cheyer,
and Moran 1999) serves as a communications hub, with the following agents responsi-
ble for specific tasks: DIPPER (Bos et al 2003) for dialogue management; a Java agent
that implements an additivemulti-attribute value function (AMVF), a decision-theoretic
model of the user?s preferences (Carenini and Moore 2000, 2006), for user modeling;
OPlan (Currie and Tate 1991) for content planning; Xalan XSLT5 and OpenCCG (White
5 http://xml.apache.org/xalan-j/.
163
Computational Linguistics Volume 36, Number 2
Figure 3
FLIGHTS generation architecture.
2004, 2006a, 2006b) for sentence planning and surface realization; and Festival (Taylor,
Black, and Caley 1998) for speech synthesis. The user modeling, content planning, sen-
tence planning, and surface realization agents are described in the ensuing subsections.
FLIGHTS follows a typical pipeline architecture (Reiter and Dale 2000) for NLG.
The NLG subsystem takes as input an abstract communicative goal from the dialogue
manager. In the information presentation phase of the dialogue, this goal is to describe
the available flights that best meet the user?s constraints and preferences. Given a
communicative goal, the content planner selects and arranges the information to convey
by applying the plan operators that implement its presentation strategy. In so doing,
it makes use of three further knowledge sources: the user model, the domain model,
and the dialogue history. Next, the content plan is sent to the sentence planner, which
uses XSLT templates to perform aggregation, lexicalization, and referring expression
generation. The output of sentence planning is a sequence of logical forms (LFs). The
use of LF templates represents a practical and flexible way to deal with the interaction
of decisions made at the sentence planning level, and further blurs the traditional
distinction between template-based and ?real? NLG that van Deemter, Krahmer, and
Theune (2005) have called into question. Each LF is realized as a sentence using a CCG
lexico-grammar (Steedman 2000a, 2000b). Note that in contrast to the generation archi-
tectures of, for example, Pan, McKeown, and Hirschberg (2002) and Walker, Rambow,
and Rogati (2002), the prosodic structure of the sentence is determined as an integral
part of surface realization, rather than in a separate prosody prediction component.
The prosodic structure is passed to the Festival speech synthesizer using Affective
Presentation Markup Language (de Carolis et al 2004; Steedman 2004), or APML, an
XML markup language for the annotation of affect, information structure, and prosody.
Festival uses the Tones and Break Indices (Silverman et al 1992), or ToBI,6 pitch
accents and edge tones?specified as APML annotations?in determining utterance
phrasing and intonation, and employs a custom synthetic voice to produce the system
utterances.
6 See http://www.ling.ohio-state.edu/?tobi/ for an introduction to ToBI and links to on-line
resources.
164
White, Clark, and Moore Generating Tailored Descriptions with Appropriate Intonation
2.3 User Modeling
FLIGHTS uses an additive multi-attribute value function (AMVF) to represent the
user?s preferences, as in the GEA real estate recommendation system (Carenini and
Moore 2000, 2006) and the MATCH restaurant recommendation system (Walker et al
2004). Decision-theoretic models of this kind are based on the notion that, if anything
is valued, it is valued for multiple reasons, where the relative importance of different
reasons may vary among users.
The first step is to identify good flights for a particular origin, destination, and
arrival or departure time. The following attributes contribute to this objective: arrival-
time, departure-time, number-of-legs, total-travel-time, price, airline, fare-
class, and layover-airport. As in MATCH, these attributes are arranged into a
one-level tree.
The second step is to define a value function for each attribute. A value function
maps from the features of a flight to a number between 0 and 1, representing the value
of that flight for that attribute, where 0 is the worst and 1 is the best. For example,
the function for total-travel-time computes the difference in minutes between the
flight?s arrival and departure times, and then multiplies the result by a scaling factor to
obtain an evaluation between 0 and 1. The functions for the airline, layover-airport,
and fare-class attributes make use of user-specified preferred or dispreferred values
for that attribute. In the current version of these functions, a preferred value is given a
score of 0.8, a dispreferred value 0.2, and all other values 0.5.7
The structure and weights of the user model represent a user?s dispositional biases
about flight selection. Situational features are incorporated in two ways. The requested
origin and destination are used as a filter when selecting the set of available options
by querying the database. In contrast, the requested arrival or departure time?if
specified?is used in the corresponding attribute?s evaluation function to give a higher
score to flights that are closer to the specified time. If an arrival or departure time is not
specified, the corresponding attribute is disabled in the user model.
As in previous work, the overall evaluation of an option is computed as the
weighted sum of its evaluation on each attribute. That is, if f represents the option being
evaluated,N is the total number of attributes, and wi and vi are, respectively, the weight
and the value for attribute i, then the evaluation v( f ) of option f is computed as follows:
v( f ) =
N
?
i=1
wivi( f )
To create a user model for a specific user, two types of information are required. The
user must rank the attributes in order of importance, and he or she must also specify
any preferred or dispreferred attribute values for the airline, layover-airport, and
fare-class attributes. In FLIGHTS, we also allow users to specify a partial ordering of
the rankings, so that several attributes can be given equal importance when registering
to use the system. Figure 4 shows the user models for the student (S), frequent-flyer (FF),
and business-class (BC) users discussed earlier; because no departure time is specified
in the sample query, departure-time is not included in these examples.
7 In informal experiments, we did not find the system to be particularly sensitive to the exact values used
in the attribute functions when selecting content.
165
Computational Linguistics Volume 36, Number 2
Figure 4
Sample user models.
Based on the user?s ranking of the attributes, weights are assigned to each attribute.
As in previous work, we use Rank Order Centroid (ROC) weights (Edwards and Barron
1994). This allows weights to be assigned based on rankings, guaranteeing that the sum
will be 1. The nth ROC weight wRn of N total weights is computed as follows:
wRn =
1
N
N
?
i=n
1
i
We extend these initial weights to the partial-ordering case as follows. If attributes
i . . . j all have the same ranking, then the weight of each will be the mean of the relevant
ROC weights; that is,
(
j
?
k=i
wRk )/( j? i+ 1)
As a concrete example, if there is a single highest-ranked attribute followed by a three-
way tie for second, then w1 = w
R
1 , and w2 = w3 = w4 =
1
3 (w
R
2 + w
R
3 + w
R
4 ).
2.4 Content Planning
2.4.1 Content Selection. Once a specific user model has been created, the AMVF can be
used to select a set of flights to describe for that user, and to determine the features of
those flights that should be included in the descriptions. We use a novel strategy that
combines features of the Compare and Recommend strategies of Walker et al (2004),
refining them with an enhanced method of selecting options to mention. In brief, the
idea behind the strategy is to select the top-ranked flight, along with any other highly
ranked flights that offer a compelling trade-off?that is, a better value (for the user)
on one of its attributes. As we shall see in Section 2.4.2, by guaranteeing that any
option beyond the first one offers such a trade-off, our strategy lends itself naturally
to the determination of information structure and the contents of referring expressions
identifying the option. By contrast, Walker et al?s Recommend strategy only presents
a single option, and their Compare strategy does not present options in a ranked order
that facilitates making trade-offs salient. In addition, we may observe that with our
strategy, the user model need only contain a rough approximation of the user?s true
166
White, Clark, and Moore Generating Tailored Descriptions with Appropriate Intonation
Figure 5
Algorithm for selecting the options to describe.
preferences in order for it to do its job of helping to identify good flights for the user
to consider.8 A similar observation underlies the candidate/critique model of Linden,
Hanks, and Lesh?s (1997) Web-based system.
Selecting the Options to Describe. In determining whether an option is worth mention-
ing, we make use of two measures. Firstly, we use the z-score of each option; this mea-
sures how far the evaluation v( f ) of an option f is from the mean evaluation. Formally,
it is defined using the mean (?V) and standard deviation (?V) of all evaluations, as
follows:
z( f ) = (v( f )? ?V )/?V
We also make use of the compellingnessmeasure described by Carenini andMoore
(2000, 2006), who provide a formal definition. Informally, the compellingness of an
attribute measures its strength in contributing to the overall difference between the
evaluation of two options, all other things being equal. For options f, g, and threshold
value kc, we define the set comp( f, g, kc) as the set of attributes that have a higher score
for f than for g, and for which the compellingness is above kc.
The set Sel of options to describe is constructed as follows. First, we include the top-
ranked option. Next, for all of the other options whose z-score is above a threshold kz,
we check whether there is an attribute of that option that offers a compelling trade-off
over the already selected options; if so, we add that option to the set.9 This algorithm is
presented in Figure 5.
For the BC user model, for example, this algorithm proceeds as follows. First, it
selects the top-ranked flight: a connecting flight on British Airways with availability
in business class. The next-highest-ranked flight is a morning flight, which does not
have any attributes that are compellingly better than those of the top choice, and is
therefore skipped. However, the third option presents an interesting trade-off: even
though business class is not available, it is a direct flight, so it is also included. None
of the other options above the threshold present any interesting trade-offs, so only those
two flights are included.
8 Of course, if the user model could be relied upon to contain perfect information, the system could always
just recommend a single best flight; however, because we do not expect our models to capture a user?s
preferences perfectly, we have designed the system to let the user weigh the alternatives when there
appear to be interesting trade-offs among the available options.
9 The requirement that an option offer a compelling trade-off is similar to the exclusion of dominated
solutions in Linden, Hanks, and Lesh (1997).
167
Computational Linguistics Volume 36, Number 2
The selected flights for the other sample user models show similar trade-offs, as
mentioned in the discussion of Figure 2. For FF, the selected flights are a connecting,
economy-class flight on the preferred airline; a direct, economy-class flight on a neutral
airline; and a connecting, business-class flight on a neutral airline. For S, the top choices
are a reasonably cheap direct flight that arrives near the desired time, and an even
cheaper, connecting flight that arrives much earlier in the day.
Selecting the Attributes to Include. When selecting the attributes to include in the de-
scription, we make use of an additional measure, s-compellingness. Informally, the
s-compellingness of an attribute represents the contribution of that attribute to the
evaluation of a single option; again, the formal definition is given by Carenini and
Moore (2000, 2006). Note that an attribute may be s-compelling in either a positive or a
negative way. For an option f and threshold kc, we define the set s-comp( f, kc) as the set
of attributes whose s-compellingness for f is greater than kc.
The set Atts of attributes is constructed in two steps. First, we add the most com-
pelling attributes of the top choice. Next, we add all attributes that represent a trade-off
between any two of the selected options; that is, attributes that are compellingly better
for one option than for another. The algorithm appears in Figure 6.
For the BC user model, the s-compelling attributes of the top choice are arrival-
time and fare-class; the latter is also a compelling advantage of this flight over the
second option. The advantage of the second option over the first is that it is direct, so
number-of-legs is also included. A similar process on the other user models results
in price, arrival-time, and number-of-legs being selected for S, and arrival-time,
fare-class, airline, and number-of-legs for FF.
2.4.2 Planning Texts with Information Structure. Based on the information returned by the
content selection process, together with further information from the user model and
the current dialogue context, the content planning agent develops a plan for presenting
the available options. A distinguishing feature of the resulting content plans is that
they contain specifications of the information structure of sentences (Steedman 2000a),
including sentence theme (roughly, the topic the sentence addresses) and sentence
rheme (roughly, the new contribution on a topic).
Steedman (2000a) characterizes the notions of theme and rheme more formally by
stating that a theme presupposes a rheme alternative set, in the sense of Rooth (1992),
while a rheme restricts this set. Because Steedman does not fully formalize the discourse
update semantics of sentence themes, we have chosen to operationalize themes in
FLIGHTS in one particular way, namely, as corresponding to implicit questions that
arise in the context of describing the available options. Our reasoning is as follows.
First, we note that a set of alternative answers corresponds formally to the meaning of a
question. Next, we observe that whenever a flight option presents a compelling trade-off
Figure 6
Algorithm for selecting the attributes to include.
168
White, Clark, and Moore Generating Tailored Descriptions with Appropriate Intonation
for a particular attribute, it at least partially addresses the question of whether there are
any flights that have a desirable value for that attribute; moreover, whenever a flight
is presented that has a less-than-optimal value for an attribute, its mention implicitly
raises the question of whether any flights are available with a better value for that
attribute. Finally, we conclude that by specifying and realizing content as thematic, the
system can help the user understand why a flight option is being presented, since the
theme (by virtue of its presupposition) identifies what implicit question?that is, what
trade-off?the option is addressing.
In all, the content planner?s presentation strategy performs the following functions:
 marking the status of items as definite/indefinite and predications as
theme/rheme for information structure;
 determining contrast between options and attributes, or between groups of
options and attributes;
 grouping and ordering of similar options and attributes (e.g., presenting the
top scoring option first vs. last);
 choosing the contents of referring expressions (e.g., referring to a particular
option by airline); and
 decomposing strategies into basic dialogue acts and hierarchically organized
rhetorical speech acts.
The presentation strategy is specified via a small set (about 40) of content planning
operators. These operators present the selected flights as an ordered sequence of op-
tions, starting with the best one. Each flight is suggested and then further described. As
part of suggesting a flight, it is identified by its most compelling attribute, according to
the user model. (Recall that any selected flight options beyond the highest ranked one
must offer a compelling trade-off.) Flights are additionally identified by their airline,
which we deemed sufficiently significant to warrant special treatment (otherwise the
strategy is domain-independent).
The information for the first flight is presented as all rheme, as no direct link is
made to the preceding query. Each subsequent, alternative flight is presented with its
most compelling attribute in the theme, and the remaining attributes in the rheme. For
instance, consider the student example (S) in Figure 2. After presenting the BMI flight?
which has a good price, is direct, and arrives near the desired time?the question arises
whether there are any cheaper alternatives. Because the second option has price as its
most compelling attribute?and given that it is the least expensive flight available?it is
identified as the CHEAPEST flight, with this phrase forming the theme of the utterance.
As another illustration, consider the business-class traveler example (BC) in Figure 2.
After presenting the British Airways flight, which has availability in business class
but is not direct, the question arises whether there are any direct flights available. The
presentation of the second option addresses this implicit question, introducing the BMI
flight with the theme phrase There is a DIRECT flight.
The output of the content planner is derived from the hierarchical goal structure
produced during planning. Figure 7 shows the resulting content plan for the student
example. Note that, as part of suggesting the second flight option in the sequence (f2),
subgoals are introduced that identify the option by informing the user that the option
has type flight and that the option has the attribute cheapest, where this attribute is
the most compelling one for the user. Both subgoals are marked as part of the theme
169
Computational Linguistics Volume 36, Number 2
Figure 7
Content plan for student example (S).
(rheme marking is the default); the subgoal for the option type is also marked for
definiteness (indefinite is the default), as the cheapest attribute uniquely identifies the
flight. The remaining information for the second flight is presented in terms of a contrast
between its positive and negative attributes, as determined by the user model.
The way in which our presentation strategy uses theme phrases to connect alterna-
tive flight suggestions to implicit questions is related to Prevost?s (1995) use of theme
phrases to link system answers to immediately preceding user questions. It is also
170
White, Clark, and Moore Generating Tailored Descriptions with Appropriate Intonation
Figure 8
Semantic dependency graph produced by the sentence planner for (The CHEAPEST flight)theme (is
on RYANAIR)rheme.
related to Kruijff-Korbayova? et al?s (2003) use of theme phrases to link utterances with
questions under discussion (Ginzburg 1996; Roberts 1996) in an information-state based
dialogue system. An interesting challenge that remains for future work is to determine
to what extent our presentation strategy can be generalized to handle theme/rheme
partitioning, both for explicit questions across turns as well for implicit questions within
turns.
2.5 Sentence Planning
The sentence planning agent uses the Xalan XSLT processor to transform the output of
the content planner into a sequence of LFs that can be realized by the OpenCCG agent.
It is intended to be a relatively straightforward component, as the content planner has
been designed to implement the most important high-level generation choices. Its pri-
mary responsibility is to lexicalize the basic speech acts in the content plan?which may
appear in referring expressions?alongwith the rhetorical speech acts that connect them
together. When alternative lexicalizations are available, all possibilities are included in
a packed structure (Foster and White 2004; White 2006a). The sentence planner is also
responsible for adding discourse markers such as also and but, adding pronouns, and
choosing sentence boundaries. It additionally implements a small number of rhetorical
restructuring operations for enhanced fluency.
The sentence planner makes use of approximately 50 XSLT templates to recursively
transform content plans into logical forms. An example logical form that results from
applying these templates to the content plan shown in Figure 7 appears in Figure 8 (with
alternative lexicalizations suppressed). As described further in Section 2.6.1, the logical
forms produced by the sentence planner are semantic dependency structures,10 which
make use of an info feature to encode theme/rheme partitioning, and a kon feature to
implement Steedman?s (2006) notion of kontrast (Vallduv?? and Vilkuna 1998). Following
Steedman, kontrast is assigned to the interpretations of words which contribute to
distinguishing the theme or rheme of the utterance from other alternatives that the
context makes available.
In order to trigger the inclusion of context-sensitive discourse markers such as also
and either, the sentence planner compares the inform acts for consecutive flight options
10 The nodes of the graph are typically labelled by lexical predicates. An exception here is the has-rel
predicate, which allows the predicative complement on Ryanair to introduce the ?AIRLINE? role in a way
that is similar to the dependency structure for the Ryanair flight.
171
Computational Linguistics Volume 36, Number 2
to see whether acts with the same type have the same value.11 These same checks can
also trigger de-accenting. For example, when two consecutive flights have no seats in
business class, the second one can be described using the phrase it has NO AVAILABILITY
in business class either, where business class has been de-accented.
The development of the XSLT templates wasmade considerably easier by the ability
to invoke OpenCCG to parse a target sentence and then use the resulting logical form
as the basis of a template. (See Section 3 for a description of how the target sentences in
the FLIGHTS voice script were developed.) Using LF templates, rather than templates
at the string level, makes it simpler to uniformly handle discourse markers such as
also and either, which have different preferred positions within a clause, depending in
part on which verb they modify. LF templates also simplify the treatment of subject?
verb agreement. Additionally, by employing LF templates with a single theme/rheme
partition, it becomes possible to underspecify whether the theme and rheme will be
realized by one intonational phrase each or by multiple phrases (see Section 2.6.2). At
the same time, certain aspects of the LF templates can be left alone, when there is no
need for further analysis and generalization.
As noted earlier, the use of LF templates further blurs the traditional distinction
between template-based and ?real? NLG that vanDeemter, Krahmer, and Theune (2005)
have called into question. In the case of referring expressions especially, LF templates in
FLIGHTS represent a practical and flexible way to deal with the interaction of decisions
made at the sentence planning level, as the speech acts identifying flight options are
considered together with the other basic and rhetorical speech acts in the applicability
conditions for the templates that structure clauses. In this way, options can be identified
not only in definite NPs, such as the CHEAPEST flight, but also in there-existentials and
conditionals, such as there is a DIRECT flight on BMI that . . . or if you prefer to fly DIRECT,
there?s a BMI flight that . . . . We may further observe that the traditional approach to gen-
erating referring expressions (Reiter and Dale 2000), where a distinguishing description
for an entity is constructed during sentence planning without regard to a user model,
would not fit in our architecture, where the user model drives the selection of a referring
expression?s content at the content planning level.
Although the use of LF templates in XSLT represents a practical approach to
handling sentence planning tasks that were not the focus of our research, it is not
one that promotes reuse, and thus it is worth noting which aspects of our sentence
planner would pose challenges for a more declarative and general treatment. The bulk
of the templates concern domain-specific lexicalization and are straightforward; given
the way these were developed from the results of OpenCCG parsing, it is conceiv-
able that this process could be largely automated from example input?output pairs.
The templates for adding pronouns and discourse markers require more expertise
but remain reasonably straightforward; the templates for rhetorical restructuring and
choosing sentence boundaries, in contrast, are fairly intricate. In principle, satisfactory
results might be obtained using a more general set of options for handling pronouns,
discourse markers, and sentence boundaries, together with an overgenerate-and-rank
methodology; we leave this possibility as a topic for future research.
2.6 Surface Realization
2.6.1 Chart Realization with OpenCCG. For surface realization, we use the OpenCCG open
source realizer (White 2004, 2006a, 2006b). A distinguishing feature of OpenCCG is that
11 In future work, these checks could be extended to apply across turns as well.
172
White, Clark, and Moore Generating Tailored Descriptions with Appropriate Intonation
it implements a hybrid symbolic-statistical chart realization algorithm that combines
(1) a theoretically grounded approach to syntax and semantic composition, with (2) the
use of integrated language models for making choices among the options left open by
the grammar. In so doing, it brings together the symbolic chart realization (Kay 1996;
Shemtov 1997; Carroll et al 1999; Moore 2002) and statistical realization (Knight and
Hatzivassiloglou 1995; Langkilde 2000; Bangalore and Rambow 2000; Langkilde-Geary
2002; Oh and Rudnicky 2002; Ratnaparkhi 2002) traditions. Another recent approach to
combining these traditions appears in Carroll and Oepen (2005), where parse selection
techniques are incorporated into an HPSG realizer.
Like other realizers, the OpenCCG realizer is partially responsible for determining
word order and inflection. For example, the realizer determines that also should prefer-
ably follow the verb in There is also a very cheap flight on Air France, whereas in other cases
it typically precedes the verb, as in I also have a flight that leaves London at 3:45 p.m. It also
enforces subject?verb agreement, for example, between is and flight, or between are and
seats. Less typically, in FLIGHTS and in the COMIC12 system, the OpenCCG realizer
additionally determines the prosodic structure, in terms of the type and placement of
pitch accents and edge tones, based on the information structure of its input logical
forms. Although OpenCCG?s algorithmic details have been described in the works cited
above, details of how prosodic structure can be determined from information structure
in OpenCCG appear for the first time in this article.
The grammars used in the FLIGHTS and COMIC systems have been manually
written with the aim of achieving very high quality. However, to streamline grammar
development, the grammar has been allowed to overgenerate in areas where rules are
difficult to write and where n-gram models can be reliable; in particular, it does not
sufficiently constrain modifier order, which in the case of adverb placement especially
can lead to a large number of possible orderings. Additionally, it allows for a one-to-
many mapping from themes or rhemes to edge tones, yielding many variants that differ
only in boundary type or placement. As will be explained subsequently, we consider
this more flexible, data-driven approach to phrasing to be better suited to the needs of
generation than would be a more direct implementation of Steedman?s (2000a) theory,
which would require all phrasing choices to be made at the sentence planning level.
We have built a language model for the system from the FLIGHTS speech corpus
described in Section 3.1. To enhance generalization, named entities and scalar adjectives
have been replacedwith the names of their semantic classes (such as TIME, DATE, CITY,
AIRLINE, etc.), as is often done in limited domain systems. Note that in the corpus and
the model, pitch accents are treated as integral parts of words, whereas edge tones and
punctuation marks appear as separate words. The language model is a 4-gram back-off
model built with the SRI language modeling toolkit (Stolcke 2002), keeping all 1-counts
and using Ristad?s (1995) natural discounting law for smoothing. OpenCCG has its own
implementation for run-time scoring; in FLIGHTS, the model additionally incorporates
an a/an-filter, which assigns a score of zero to sequences containing a followed by a
vowel, or an followed by a consonant, subject to exceptions culled from bigram counts.
2.6.2 Deriving Prosody. CCG is a unification-based categorial framework that is both
linguistically and computationally attractive. We provide here a brief overview of CCG;
an extensive introduction appears in Steedman (2000b).
12 http://www.hcrc.ed.ac.uk/comic/.
173
Computational Linguistics Volume 36, Number 2
Figure 9
A simple CCG derivation.
Figure 10
A CCG derivation with non-standard constituents.
A grammar in CCG is defined almost entirely in terms of the entries in the lexicon,
which are (possibly complex) categories bearing standard feature information (such as
verb form, agreement, etc.) and subcategorization information. CCG has a small set of
rules which can be used to combine categories in derivations. The two most basic rules
are forward (>) and backward (<) function application. These rules are illustrated in
Figure 9, which shows the derivation of a simple copular sentence (with no prosodic
information).13 In the figure, the noun and proper name receive atomic categories with
labels n and np, respectively. The remaining words receive functional categories, such as
sdcl\np/(sadj\np) for the verb is; this category seeks a predicative adjective (sadj\np) to its
right and an np to its left, and returns the category sdcl, for a declarative sentence. Note
that dcl and adj are values for the form feature; other features, such as those for number
and case, have been suppressed in the figure (as has the feature label, form).
CCG also employs further rules based on the composition (B), type raising (T),
and substitution (S) combinators of combinatory logic. These rules add an element
of associativity to the grammar, making possible multiple derivations with the same
semantics. They are crucial for building the ?non-standard? constituents that are the
hallmark of categorial grammars, and which are essential for CCG?s handling of coor-
dination, extraction, intonation, and other phenomena. For example, Figure 10 shows
how the category for there can be type-raised and composed with the category for is
in order to derive the constituent there is a direct flight (sdcl/(sadj\np)), which cuts across
the VP in traditional syntax. Because there is a direct flight corresponds to the theme
phrase, and on BMI to the rheme phrase, in the first clause of the second sentence in
13 These derivations are shown from the parsing perspective, starting with an ordered sequence of words at
the top. During realization, it is the semantics rather than the word sequence which is given; the word
sequence is determined during the search for a derivation that covers the input semantics. See the
discussion of Figure 11 (later in this section) for a discussion of the semantic representations used in
OpenCCG.
174
White, Clark, and Moore Generating Tailored Descriptions with Appropriate Intonation
Figure 11
A derivation of theme and rheme phrases.
the business-class (BC) example in Figure 2, the derivation in Figure 10 also shows how
CCG?s flexible notion of constituency allows the intonation structure and information
structure to coincide, where the intonation coincides with surface structure, and the
information structure is built compositionally from the constituent analysis.
In Steedman (2000a), theme and rheme tunes are characterized by distinct patterns
of pitch accents and edge tones. The notation for pitch accents and edge tones is taken
from Pierrehumbert (1980) and ToBI (Silverman et al 1992). We have implemented a re-
duced version of Steedman?s theory in OpenCCG, where theme tunes generally consist
of one or more L+H* pitch accents followed by a L-H% compound edge tone at the end
of the phrase,14 and rheme tunes consist of one or more H* pitch accents followed by
a L- or L-L% boundary at phrase end. Additionally, yes/no questions typically receive
a H-H% final boundary, and L-H% boundaries are often used as continuation rises to
mark the end of a non-final conjoined phrase.
The key implementation idea in Steedman?s (2000a) approach is to use features on
the syntactic categories to enforce information structural phrasing constraints?that is,
to ensure that the intonational phrases are consistent with the theme/rheme partition
in the semantics. For example, if there is a direct flight corresponds to the theme and on
BMI the rheme, then the syntactic features ensure that the intonation brackets the clause
as (there is a direct flight) (on BMI), rather than (there) (is a direct flight on BMI) or (there
is) (a direct flight on BMI), etc. To illustrate, Figure 11 shows, again from the parsing
perspective, how theme and rheme phrases are derived in OpenCCG for the subject
NP and VP of Figure 9, respectively. (To save space, pitch accents and edge tones will
henceforth be written using subscripts and string elements, rather than appearing on a
separate tonal tier.) In the figure, the category for cheapest has the info feature on each
14 To reduce ambiguity in the grammar, unmarked themes?that is, constituents which appear to be
thematic but which are not grouped intonationally into a separate phrase?are assumed to be
incorporated as background parts of the rheme, as suggested in Calhoun et al (2005).
175
Computational Linguistics Volume 36, Number 2
atomic category set to th(eme), and Ryanair has its info feature set to rh(eme), due to
the presence of the L+H* and H* accents, respectively. The remaining words have no
accents, and thus their categories have a variable (M, for EME) as the value of the info
feature on each atomic category.15 All the words also have a variable (O) as the value of
the owner feature, discussed subsequently, on each atomic category. As words combine
into phrases, the info variables serve to propagate the theme or rheme status of a
phrase; thus, the phrase the cheapestL+H? flight has category npth,O, whereas is on RyanairH?
ends up with category sdcl,rh,O\nprh,O. Because these two phrases have incompatible info
values, they cannot combine before being ?promoted? to intonational phrases by their
respective edge tones. In this way, the constraint that phrasing choices respect the
theme/rheme partition is enforced.
The edge tones allow complete intermediate or intonational phrases to combine by
changing the value of the info feature to phr(ase) in the result category.16 Note that
the argument categories of the edge tones do not select for a particular value of the
info feature; instead, L-H% has sh$1 as its argument category, whereas L-L% has ss$1,
where h(earer) and s(peaker) are the respective values of the owner feature.17 Combination
with an edge tone unifies the owner features throughout the phrase. In a prototypical
theme phrase, the hearer is the owner, whereas in a rheme phrase, the speaker is the
owner.
Like other compositional grammatical frameworks, CCG allows logical forms to
be built in parallel with the derivational process. Traditionally, the ?-calculus has been
used to express semantic interpretations, but OpenCCG instead makes use of a more
flexible representational framework, Hybrid Logic Dependency Semantics (Baldridge
and Kruijff 2002; Kruijff 2003), or HLDS. In HLDS, hybrid logic (Blackburn 2000) terms
are used to describe semantic dependency graphs, such as the one seen earlier in
Figure 8. As discussed in White (2006b), HLDS is well suited to the realization task, in
that it enables an approach to semantic construction that ensures semantic monotonicity,
simplifies equality tests, and avoids copying in coordinate constructions.
To illustrate, four lexical items from Figure 11 appear in Example (1). The three
words are derived by a lexical rule which adds pitch accents and information structure
features to the base forms. The format of the entries is lexeme  category, where the
category is itself a pair in the format syntax : logical form. The logical form is a conjunction
of elementary predications (EPs), which come in three varieties: lexical predications,
such as @Ebe; semantic features, such as @E?TENSE?pres; and dependency relations,
such as @E?ARG?X.
(1) a. cheapestL+H?  nX,th,O/nX,th,O : @X?HASPROP?P ?@Pcheapest ?
@P?INFO?th ?@P?OWNER?O ?@P?KON?+
b. RyanairH?  npX,rh,O : @XRyanair ?
@X?INFO?rh ?@X?OWNER?O ?@X?KON?+
15 In practice, variables likeM are replaced with ?fresh? variables (such asM 1) during lexical lookup, so
that theM variables are distinct for different words.
16 The $ variables range over a (possibly empty) stack of arguments, allowing the edge tones to employ a
single s$1\s$1 category to combine with s/(s\np), s\np, and so on. As indicated in the figure, the new
value of the info feature (phr) is automatically distributed throughout the arguments of the result
category.
17 See Steedman (2000a) for a discussion of this notion of ?ownership,? or responsibility.
176
White, Clark, and Moore Generating Tailored Descriptions with Appropriate Intonation
c. is  sE,dcl,M,O\npX,M,O/(sP,adj,M,O\npX,M,O) : @Ebe ?
@E?TENSE?pres ?@E?ARG?X ?@E?PROP?P ?
@E?INFO?M ?@E?OWNER?O ?@E?KON??
d. L-H%  sphr$1\sh$1
In these entries, the indices (or nominals, in hybrid logic terms) in the logical forms,
such as X, P, and E, correspond to nodes in the semantic graph structure, and are
linked to the syntactic categories via the index feature. Similarly, the syntactic features
info and owner have associated ?INFO? and ?OWNER? features in the semantics. As
discussed previously, in derivations the values of the info and owner features are
propagated throughout an intonational phrase, which has the effect of propagating the
values of the ?INFO? and ?OWNER? semantic features to every node in the dependency
graph corresponding to the phrase. In this way, a distributed representation of the
theme/rheme partition is encoded, in a fashion reminiscent of Kruijff?s (2003) approach
to representing topic?focus articulation using hybrid logic. By contrast, the ?KON? fea-
ture (cf. Section 2.5) is a purely local one, and thus appears only in the semantics. Note
that because the edge tones do not add any elementary predications, one or more edge
tones?and thus one or more intonational phrases?may be used to derive the same
theme or rheme in the logical form.
To simplify the semantic representations the sentence planner must produce, Open-
CCG includes default rules that (where applicable) propagate the value of the ?INFO?
feature to subtrees in the logical form, set the ?OWNER? feature to its prototypical value,
and set the value of the ?KON? feature to false. When applied to the logical form for
the semantic dependency graph in Figure 8, the rules yield the HLDS term in Exam-
ple (2). After this logical form is flattened to a conjunction of EPs, lexical instantiation
looks up relevant lexical entries, such as those in Example (1), and instantiates the
variables to match those in the EPs. The chart-based search for complete realizations
proceeds from these instantiated lexical entries.
(2) @e(be ? ?TENSE?pres ? ?INFO?rh ? ?OWNER?s ? ?KON?? ?
?ARG?( f ? flight) ? ?DET?the ? ?NUM?sg ? ?INFO?th ? ?OWNER?h ? ?KON?? ?
?HASPROP?(c ? cheapest ? ?INFO?th ? ?OWNER?h ? ?KON?+)) ?
?PROP?(p ? has-rel ? ?INFO?rh ? ?OWNER?s ? ?KON?? ?
?OF?f ?
?AIRLINE?(r ? Ryanair ? ?INFO?rh ? ?OWNER?s ? ?KON?+)))
Given our focus in FLIGHTS on using intonation to express contrasts intelligibly,
we have chosen to employ hard constraints in the OpenCCG grammar on the choice
of pitch accents and on the use of edge tones to separate theme and rheme phrases.
However, the grammar only partially constrains the type of edge tone (as explained
previously), and allows the theme and rheme to be expressed by one or more intona-
tional phrases each; consequently, the final choice of the type and placement of edge
tones is determined by the n-gram model. To illustrate, consider Example (3), which
shows how the frequent flyer sentence seen in Figure 2 is divided into four intonational
phrases. Other possibilities (among many) allowed by the grammar include leaving out
the L-L% boundary between flight and arriving, which would yield a phrase that?s likely
to be perceived as too long, or adding a L- or L-L% boundary between there?s and a,
which would yield two unnecessarily short phrases.
(3) There?s a KLMH? flight L-L% arriving BrusselsH? at fourH? fiftyH? p.m.H? L-L%,
but businessH? classH? is notH? availableH? L-H% and you?d need to connectH? in
AmsterdamH? L-L%.
177
Computational Linguistics Volume 36, Number 2
To allow for this flexible mapping between themes and rhemes and one or more into-
national phrases, we take advantage of our distributed approach to representing the
theme/rheme partition, where edge tones mark the ends of intonational phrases with-
out introducing their own elementary predications. As an alternative, we could have
associated a theme or rheme predication with the edge tones, which would be more
in line with Steedman?s (2000a) approach. However, doing so would make it necessary
to include one such predication per phrase in the logical forms, thereby anticipating
the desired number of output theme and rheme phrases in the realizer?s input. Given
that the naturalness of intonational phrasing decisions can depend on surface features
like phrase length, we consider the distributed approach to representing theme/rheme
status to be better suited to the needs of generation.
2.7 Comparison to Baseline Prosody Prediction Models
As noted in Section 2.2, spoken language dialogue systems often include a separate
prosody prediction component (Pan,McKeown, andHirschberg 2002;Walker, Rambow,
and Rogati 2002), rather than determining prosodic structure as an integral part of
surface realization, as we do here. Although it is beyond the scope of this article to
compare our approach to a full-blown, machine learning?based prosody prediction
model, we do present in this section an expert evaluation that shows that our approach
outperforms strong baseline n-gram models. In particular, we show that the informa-
tion structural constraints in the grammar play an important role in producing target
prosodic boundaries, and that these boundary choices are preferred to those made by
an n-gram model in isolation.
According to Pan, McKeown, and Hirschberg (2002, page 472), word-based n-gram
models can be surpringly good: ?The word itself also proves to be a good predictor for
both accent and break index prediction. . . . Since the performance of this [word] model
is the best among all the [single-feature] accent prediction models investigated, it seems
to suggest that for a CTS [Concept-to-Speech] application created for a specific domain,
features like word can be quite effective in prosody prediction.? Indeed, although their
best accent prediction model exceeded the word-based one, the difference did not reach
statistical significance (page 485). Additionally, word predictability, measured by the
log probability of bigrams and trigrams, was found to significantly correlate with pitch
accent decisions (pages 482?483), and contributed to their best machine-learned models
for accent presence and boundary presence.18
As baseline n-gram models, we trained 1- to 4-gram models for predicting accents
and boundaries using the FLIGHTS speech corpus (Section 3.1), the same corpus used to
train the realizer?s language model. The baseline n-gram models are factored language
models (Bilmes and Kirchhoff 2003), with words, accents, and boundaries as factors.
The accent models have accent as the child variable and 1?4 words as parent variables,
startingwith the current word, and including up to three previous words. The boundary
models are analogous, with boundary as the child variable. With these models, each
maximum likelihood prediction of pitch accent or edge tone is independent of all other
choices, so there is no need to perform a best-path search. The majority baseline predicts
no accent and no edge tone for each word.
18 Note that in our setting, it would be impossible to exactly replicate Pan et al?s models, in which syntactic
boundaries play an important role, as CCG does not have a rigid notion of syntactic boundary.
178
White, Clark, and Moore Generating Tailored Descriptions with Appropriate Intonation
Table 1
Baseline pitch accent and boundary prediction accuracy against target tunes.
Accuracy
Majority 1-gram 2-gram 3-gram 4-gram N
Accent Presence 73.3% 98.0% 98.6% 97.4% 97.4% 344
Accent Type 73.3% 96.5% 97.4% 96.2% 96.8% 344
Boundary Presence 81.1% 91.9% 94.5% 95.4% 95.1% 344
Boundary Type 81.1% 91.0% 92.7% 93.0% 93.9% 344
We tested the realizer and the baseline n-gram models on the 31 sentences used to
synthesize the stimuli in the perception experiment described in Section 4 (see Figure 13
for an example mini-dialogue). None of the test sentences appear verbatim in the
FLIGHTS speech corpus. The test sentences contain target prosodic structures intended
to be appropriate for the discourse context. Given these structures, we can quantify
the accuracy with which the realizer is able to reproduce the pitch accent and edge
tone choices in the target sentences, and compare it to the accuracy with which n-gram
models predict these choices using maximum likelihood. Note that the target prosodic
structures may not represent the only natural choices, motivating the need for the expert
evaluation described further subsequently.
Although the realizer is capable of generating the target prosodic structure of each
test sentence exactly, the test sentence (with its target prosody) is not always the top-
ranked realization of the corresponding logical form, which may differ in word order
or choice of function words. Thus, to compare the realizer?s choices against the target
accents and boundaries, we generated n-best lists of realizations that included the target
realization, and compared this realization to others in the n-best list with the same
words in the same order (ignoring pitch accents and edge tones). In each case, the target
realization was ranked higher than all other realizations with the same word sequence,
and so we may conclude that the realizer reproduces the target accent and boundary
choices in the test sentences with 100% accuracy.
The accuracy with which the baseline n-gram models reproduce the target tunes
is shown in Table 1. As the test sentences are very similar to those in the FLIGHTS
speech corpus, the accent model performs remarkably well, with the bigram model
reproducing the exact accent type (including no accent) in 97.4% of the cases, and
agreeing on the choice of whether to accent the word at all in 98.6% of the cases. The
boundary model also performs well, though substantially worse than the realizer, with
the 4-gram model reproducing the boundary type (including no boundary) in 93.9% of
the cases, and agreeing on boundary presence in 95.1% of the cases.19
Inspired byMarsi?s (2004) work on evaluating optionality in prosody prediction, we
asked an expert ToBI annotator, who was unfamiliar with the experimental hypotheses
under investigation, to indicate for each test sentence the range of contextually appro-
priate tunes by providing all the pitch accents and edge tones that would be acceptable
19 Because the boundary models sometimes failed to include a boundary before a comma or full stop,
default L-L% boundaries were added in these cases.
179
Computational Linguistics Volume 36, Number 2
Table 2
Examples comparing target tunes to baseline prosody prediction models, with expert corrections
(Items 07-2 and 06-1).
(a) target the only directL+H? flight L-H% leaves at 5:10H? L-L% .
edits none
n-grams the only directH? flight leaves at 5:10H? L-L% .
edits the only directL+H? flight L- leaves at 5:10H? L-L% .
(b) target there ?s a directH? flight on British AirwaysH? with a goodH? price L-L% .
edits there ?s a directH? flight on British AirwaysH? L- with a goodH? price L-L% .
n-grams there ?s a directH? flight on British AirwaysH? L-L% with a goodL+H?
price L-L% .
edits none
for each word.20 However, our annotator found this task to be too difficult, in part
because of the difficulty of coming up with all possible acceptable tunes, and in part
because of dependencies between the choices made for each word. For this reason,
we instead chose to follow the approach taken with the Human Translation Error
Rate (HTER) post-edit metric in MT (Snover et al 2006), and asked our annotator to
indicate, for the target tune and the n-gram baseline tune, which accents and boundaries
would need to change in order to yield a tune appropriate for the context. For the
n-gram baseline tune, we used the choices of the bigram accent model and the 4-gram
boundary model.
Examples of how the target tunes (and realizer choices) compare to those of the
baseline accent and boundary prediction models appear in Table 2, along with the
corrections provided by our expert ToBI annotator.21 In Table 2(a), the target tune,
which contains the theme phrase the only directL+H? flight L-H%, was considered fully
acceptable. By contrast, with the tune of the n-gram models, the H* accent on directwas
not considered acceptable for the context, and at least a minor phrase boundary was
considered necessary to delimit the theme phrase. In Table 2(b), we have an example
where the target tunewas not considered fully acceptable: Although the target consisted
of a single, all-rheme intonational phrase, with no intermediate phrases, our annotator
indicated that at least a minor phrase break was necessary after British Airways. The
n-gram models assigned a major phrase break at this point, which was also considered
acceptable. Note also that the n-gram models had a L+H* accent on good, in contrast to
the target tune?s H*, but both choices were considered acceptable.
Table 3 shows the number of accent and boundary corrections for all 31 test sen-
tences at different levels of specificity. Overall, there were just 10 accent or boundary
corrections for the target tunes, versus 24 for those of the baseline models, out of 688
total accent and boundary choices, a significant difference (p = 0.01, Fisher?s Exact Test
[FET], 1-tailed). With the accents, there were fewer corrections for the target tunes,
but not many in either case, and the difference was not significant. Of course, with a
20 We thank one of the anonymous reviewers for this suggestion. Note that in Marsi?s study, having one
annotator indicate optionality was found to be a reasonable approximation of deriving optionality from
multiple annotations.
21 During realization, multiwords are treated as single words, such as British Airways and 5:10 (five ten a.m.).
Accents on multiwords are distributed to the individual words before the output is sent to the speech
synthesizer.
180
White, Clark, and Moore Generating Tailored Descriptions with Appropriate Intonation
Table 3
Number of prosodic corrections of different types in all utterances and theme utterances for the
target tunes and the ones selected by n-gram models. Items in bold are significantly different at
p = 0.05 or less by a one-tailed Fisher?s Exact Test.
All Utts Theme Utts
Target n-grams Target n-grams
Total corrections 10 24 3 11
Accents 4 7 2 5
Presence 3 4 2 3
Boundaries 6 17 1 6
Presence 2 13 0 4
Major 0 9 0 3
larger sample size, or with test sentences that are less similar to those in the corpus,
a significant difference could arise. With the boundaries, out of 344 choices, the target
tunes had six corrections, only two of which involved the presence of a boundary, and
none of which involved a missing major boundary; by contrast, the n-gram baseline
had 17, 13, and 9 such corrections, respectively, a significant difference in each case
(p = 0.02, p = 0.003, p = 0.002, respectively, FET). In the subset of 12 sentences involving
theme phrases, where the intonation is more marked than in the all-rheme sentences,
the target tunes again had significantly fewer corrections overall (3 vs. 11 corrections
out of 220 total choices; p = 0.03, FET), and the difference in boundary and boundary
presence corrections approached significance (p = 0.06 in each case, FET).
Having shown that the information structural constraints in the grammar play an
important role in producing target realizations with contextually appropriate prosodic
structures?in particular, in making acceptable boundary choices, where the choice is
not deterministically rule-governed?we now briefly demonstrate that the realizer?s
n-grammodel (see Section 2.6.1) has an important role to play as well. Table 4 compares
the realizer?s output on the test sentences against its output with the language model
disabled, in which case an arbitrary choice is effectively made among those outputs
allowed by the grammar. Not surprisingly, given that the grammar has been allowed
to overgenerate, the realizer produces far more exact matches and far higher BLEU
(Papineni et al 2001) scores with its language model than without. Looking at the dif-
ferences between the realizer?s highest scoring outputs and the target realizations, the
differences largely appear to represent cases of acceptable variation. The most frequent
difference concerns whether an auxiliary or copular verb is contracted or not, where
either choice seems reasonable. Most of the other differences represent minor variations
in word order, such as directH? Air FranceH? flight vs. directH? flight on Air FranceH?. By
Table 4
Impact of language model on realizer choice.
Exact Match BLEU
Realizer 61.3% 0.9505
No LM 0.0% 0.6293
181
Computational Linguistics Volume 36, Number 2
contrast, many of the outputs chosen with no language model scoring contain unde-
sirable variation in word order or phrasing; for example: Air FranceH? directH? flight
instead of directH? flight on Air FranceH?, and you L- though would need L- to connectH? in
AmsterdamH? L- instead of you?d need to connectH? in AmsterdamH? though L-L%.
2.8 Interim Summary
In this section, we have introduced a presentation strategy for highlighting the most
compelling trade-offs for the user that straightforwardly lends itself to the determina-
tion of information structure, which then drives the choice of prosodic structure during
surface realization with CCG. We have also shown how phrase boundaries can be
determined in a flexible, data-driven fashion, while respecting the constraints imposed
by the grammar. An expert evaluation demonstrated that the approach yields prosodic
structures with significantly higher acceptability than strong n-gram baseline prosody
prediction models. In the next two sections, we show how the generator-driven prosody
can be used to produce perceptibly more natural synthetic speech.
3. Unit Selection Synthesis with Prosodic Markup
In this section we describe the unit selection voices that we employed in our perception
experiment (Section 4). Three voices in total were used in the evaluation: GEN, ALL,
and APML. Each was a state-of-the-art unit selection voice using the Festival Multisyn
speech synthesis engine (Clark, Richmond, and King 2007). The GEN voice, used as
a baseline, is a general-purpose unit selection voice. The ALL voice is a voice built
using the same data as the GEN voice but with the addition of the data from the
FLIGHTS speech corpus described in Section 3.1. The APML voice augments the in-
domain data in the ALL voice with prosodic markup, which is then used at run-time by
the synthesizer in conjunction with marked-up input to guide the selection of units.
To provide in-domain data for the ALL and APML voices, we needed to record
a suitable data set from the original speaker used for the GEN voice. We describe the
process of constructing this speech corpus for FLIGHTS next, in Section 3.1. Then, in
Section 3.2, we describe the unit selection voices in detail, alongwith how the in-domain
data was used in building them.
3.1 The FLIGHTS Speech Corpus
The FLIGHTS speech corpus was intended to have a version of each word that needs to
be spoken by the system, recorded in the context in which it will be spoken. This context
can be thought of as a three-word window centered around the given word, together
with the word?s target pitch accent and edge tone, if any. This would theoretically
provide more than sufficient speech data for a full limited domain voice, and a voice
using this data would be guaranteed to have a unit sequence for any sentence generated
by FLIGHTS where each individual unit is a tight context match for the target unit.
Because the system was still limited in its generation capabilities at the time of
recording the voice data for FLIGHTS, we developed a recording script by combining
the generated data that was available with additional utterances that we anticipated
the finished system would generate. To do so, we assembled a set of around 50 utter-
ance templates that describe flight availability?with slots to be filled by times, dates,
amounts, airlines, airports, cities, and flight details?to which we added approximately
two hundred individual or single-slot utterances, which account for the introductions,
182
White, Clark, and Moore Generating Tailored Descriptions with Appropriate Intonation
questions, confirmations, and other responses that the systemmakes. We thenmade use
of an algorithm designed by Baker (2003) to iterate through the filler combinations for
each utterance template to provide a suitable recording script.22
To illustrate, Example (4) shows an example utterance template, along with two
utterances in the recording script created from this template. The example demonstrates
that having multiple slots in a template makes it possible to reduce the number of sen-
tences that need to be recorded to cover all possible combinations of fillers. With (b) and
(c) recorded as illustrated to fill the template in (a), we would then have the recorded
data to mix and match the slot fillers with two different values each to synthesize eight
different sentences. It is also often possible to use the fillers for a slot in one utterance
as the fillers for a slot in another utterance, if the phrase structure is sufficiently similar.
A more complex case is shown in Example (5), involving adjacent slots. In this case, it
is not sufficient to just record one example of each possible filler, as the context around
that filler is changed by the presence of other slots; instead, combinations of fillers must
be considered (Baker 2003).
(4) a. It arrives at ?TIME?H? L-H% and costs just ?AMOUNT?H? L-L%, but it requires
a connectionH? in ?CITY?H? L-L%.
b. It arrives at sixH? p.m.H? L-H% and costs just fiftyH? poundsH? L-L%, but it
requires a connectionH? in ParisH? L-L%.
c. It arrives at nineH? a.m.H? L-H% and costs just eightyH? poundsH? L-L%, but it
requires a connectionH? in PisaH? L-L%.
(5) a. There are ?NUM?H? ?MOD?H? flights L-L% from ?CITY?H? to ?CITY?H?
today L-L%.
b. There are twoH? earlierH? flights L-L% from BordeauxH? to AmsterdamH?
today L-L%.
The recording script presented similar utterances in blocks. It made use of small
caps to indicate intended word-level emphasis and punctuation to partially indicate
desired phrasing, as the speaker was not familiar with ToBI annotation. The intended
dialogue context for the utterances was discussed with our speaker, but the recordings
did not consist of complete dialogues, as using complete dialogues would have been too
time consuming. The recording took place during two afternoon sessions and yielded
approximately two hours of recorded speech. The resulting speech database consisted
of 1,237 utterances, the bulk of which was derived from the utterances that describe
flight availability.
The recordings were automatically split into sentence-length utterances, each of
which had a generated APML file with the pitch accents and edge tones predicted for
the utterance. The prosodic structures were based on intuition, as there was no corpus
of human?human dialogues sufficiently similar to what the system produces that could
have been used to inform prosodic choice.
The recordings were then manually checked against the predicted APML, by listen-
ing to the audio and looking at the f0 contours to see if the pitch accents and edge
tones matched the predicted ones. In the interest of consistency, changes were only
22 The target prosody for each utterance template was based on the developers? intuitions, with input from
Mark Steedman. As one of the anonymous reviewers observed, an alternative approach might have been
to conduct an elicitation study in a Wizard-of-Oz setup to determine target tunes. This strategy may have
yielded more natural prosodic variation, but perhaps at the expense of employing less distinctive tunes.
For this project, it was not practical to take the extra time required to conduct such an elicitation study.
183
Computational Linguistics Volume 36, Number 2
made to the APML files when there were clear cases of the speaker diverging from the
predicted phrasing or emphasis; nevertheless, the changes did involve mismatches in
both the presence and the type of the accents and boundaries. As an example of the kind
of changes that were made, in Example (4), we had predicted that the speaker would
use a L-H% boundary (a continuation rise) prior to the word but, however she instead
consistently used a low boundary tone in this location. Consequently, we changed all
the APML files for sentences with this pattern to include a L-L% compound edge tone
at that point. Further details of this data cleanup process are given in Rocha (2004).
3.2 The Synthetic Voices
Ideally, we would like to build a general purpose unit selection voice where we can
fully specify the intonation in terms of ToBI accents and boundaries and then have the
synthesizer generate this for us. This, however, is currently not a fully viable option for
the reasons discussed subsequently, so instead we built a number of different voices
using the unit selection technique to investigate how working towards a voice with
full intonation control would function. There are a number of ways in which prosodic
generation for unit selection can be approached (Aylett 2005; van Santen et al 2005;
Clark and King 2006), with most systems designed to produce best general prosody.
This contrasts with the framework chosen here, which is designed to make maximum
use of in-domain data, in an attempt to produce a system that realizes prosody as well
as is possible in that domain. This can be considered an upper bound for what more
general systems could achieve under optimal conditions.
The ideal way to use intonation within the unit selection framework requires three
components: the database of speech, which must be fully annotated for intonation;
the predicted intonation for a target being synthesized; and a target cost that ensures
suitable candidates are found to match the target. Each of these requirements prove to
be difficult to achieve successfully.
Manually labeling a large speech database accurately and consistently with intona-
tion labels is both difficult and expensive, whereas automatic labeling techniques tend
to produce mediocre results at best. Producing accent labeling for the flight information
script is somewhat easier because of the limited number of templates that the script is
based upon, and once the templates are labeled, intonation for individual sentences can
be derived. To build a general purpose unit selection voice for the FLIGHTS domain,
we would want to combine the FLIGHTS data with speech data designed to provide
more general coverage. Providing accent labeling for the extra, non-FLIGHTS, data is
the main problem here.
Predicting intonation for a target utterance is generally a difficult task, and can
only really be done well when the domain of the speech synthesis is constrained. For
example, a number of statistical modeling techniques may be able to provide adequate
accent prediction for a system to read the news or forecast the weather, because the sen-
tence structure is usually limited to a sequence of simple statements. The task becomes
difficult when the sentence structure is more variable, for example in dialogue where a
number of different question forms may exist along with contrastive statements, and so
forth. In the FLIGHTS domain we can side-step the prediction problem by providing a
specification for the intonation of a sentence as part of language generation.
In one sense, defining a target-cost component to direct the search towards finding
suitable intonation is not difficult, as a simple penalty for not matching the target
intonation suffices. However, standard unit selection techniques only ever take into
184
White, Clark, and Moore Generating Tailored Descriptions with Appropriate Intonation
account local effects, and no provision is made to ensure a suitable global intonation
contour.
These problems, and a number of technical issues relating to the use of markup,
automatic segment alignment, and the voice building process, prohibit us from building
a general purpose unit selection voice where we can specify the required intonation.
One of the questions that this study is attempting to answer is whether it is worth trying
to resolve these issues to build such a system in the future. To address this question
we present a number of voices designed to investigate the issues of producing natural
speech synthesis in the FLIGHTS domain.
3.2.1 The GEN Voice. The GEN voice uses a database of approximately 2,000 sentences
of read newspaper speech. The Festival Multisyn unit selection engine selects diphone
units from the database by minimizing a combination of a target cost and a join cost.
The target cost scores the linguistic context of the diphone in terms of stress, position
of the diphone in the current word, syllable and phrase, phonetic context, and part of
speech. The join cost scores the continuity between selected units in terms of spectrum,
energy, and f0. There is no explicit modeling of prosody in this system.
The GEN voice was used as a baseline. There is nothing specific to the FLIGHTS
domain associated with any part of this voice, and the quality of the resulting synthesis
is comparable with a typical unit selection speech synthesiser.
3.2.2 The ALL voice. The ALL voice was created by augmenting the GEN voice with
the speech data recorded as part of the FLIGHTS speech corpus described herein. The
motivation here is to attempt to provide a system which would have the best possible
quality that a general purpose unit selection synthesiser could have when working in
this domain. The additional data increases the availability of units in the exact contexts
that would be required by the FLIGHTS system. Having examples of airport and airline
names in the database, for example, increases the likelihood that when these words
are synthesized there are appropriate, often consecutive, units available to synthesize
them. Naturalness is improved both by the better context match and by the need for
fewer joins in constructing these words and the utterances they are used in.
3.2.3 The APML voice. The APML voice is different from the ALL voice in that it is
designed to take APML input from the FLIGHTS system rather than text input. The
APML voice comprises the same speech data as the ALL voice, but also includes
the APML annotation for the FLIGHTS part of the corpora. The target cost for the
synthesizer is augmented with a prosodic component which penalizes any mismatch
between supplied APML input and the APML specification associated with a particular
unit. As the read newspaper component of this voice does not have accompanying
APML markup, and because the voice is required to work for text input (although this
capability is not used here), the target cost penalizes (1) the synthesis of an APML-
specified target with a unit that does not have accompanying APML markup; (2) the
synthesis of a target without APML specification with APML-specified units, and (3)
synthesis where there is an APML mismatch between the target and candidate. It is
important that these prosodic mismatches are only discouraged, rather than forbidden,
to allow the system to synthesize out of the original domain if required.
As work on the FLIGHTS system progressed, we found numerous cases where the
speech corpus failed to anticipate all the possible outputs of the system. For example,
although we included an utterance template for conveying price in a conjoined verb
phrase, as in Example (4), we did not include a template for conveying price in a
185
Computational Linguistics Volume 36, Number 2
single independent clause, as in It costs just fifty pounds. Had the system been further
along in its development when we recorded the speech data, we could have pursued a
strategy of selecting utterances to record from generated outputs, in order to maximize
some coverage criterion.23 In either case, though, we consider it difficult to develop
a recording script that will completely cover all three-word sequences that a system
will ever generate, especially if further development is considered. For this reason, we
believe it to be essential to have a strategy to handle the cases where generation needs
go beyond the original plan. The use of an augmented general purpose unit selection
system?rather than, for example, a strictly limited domain system?means that there
is no difficulty in synthesizing extra material as needed, although there is the risk that it
will not sound quite as good as that which is closer in context to the original in-domain
recordings. For an APML voice where the input is ?new? APML, if there are no suitable
units within the APML-annotated section of the corpus, units will be chosen from the
main portion of the corpus. There will be a penalty in terms of target cost for doing so,
but the best sequence will still be found.
4. Synthesis Evaluation
In this section, we describe a perceptual experiment which was carried out to deter-
mine whether the prosodic structures generated by the FLlGHTS system actually result
in improved naturalness in speech synthesis. We also describe an evaluation of the
prosody in the synthesized speech that makes use of an expert annotator?s assessments
of the acceptability of the perceived tunes for the given context, and an evaluation that
examines objective differences in the f0 contours of the theme phrases in the synthesized
speech. The three types of evaluation (subject ratings, expert annotation, and objective
measures) all show the APML voice outperforming the ALL voice, both on the complete
set of test utterances as well as on the subset containing theme phrases. Additionally, the
three types of evaluation support each other in that differences in ratings of particular
items correspond to differences in acceptability annotations and to differences in the
objective measures, as will be explained in the following.
4.1 Perception Experiment
4.1.1 Methodology. Our experimental hypothesis was that listeners would prefer the
APML voice, used with contextually appropriate intonation markup, over the ALL and
GEN control voices. We further hypothesized that the preference would be larger for
the utterances containing theme phrases, where the intonation is more marked than it
is in the all-rheme utterances.
Subjects were presented with mini-dialogues consisting of a summary of the user?s
request in text and three versions of the system?s response, one for each of the three
voices, as shown in Figure 12. System utterances were presented side-by-side, with
each system turn comprising two to four utterances, and each voice labeled as A, B,
or C. The label assignments were balanced across the mini-dialogues so that each voice
appeared an equal number of times labeled as A, B, or C, and the mini-dialogues were
presented in an individually randomized order. Subjects were asked to assign ratings
to each version of each utterance on a 1?7 scale, with 7 corresponding to ?completely
natural? and 1 corresponding to ?completely unnatural.? Ratings were gathered on-line
23 For example, in developing a custom voice for the COMIC system, we selected from generated utterances
in order to maximize bigram coverage with high priority named entities.
186
White, Clark, and Moore Generating Tailored Descriptions with Appropriate Intonation
Figure 12
Screenshot of webexp2 interface for gathering listener ratings.
using webexp2.24 Subjects were allowed to play the sound files any number of times in
any order, but were required to assign ratings to all the utterances before proceeding to
the next screen. In assigning their ratings, subjects were instructed to pay attention to
the context given by the summary of the user?s request, keeping the following questions
in mind:
 Does the utterance make it clear how well the flight (or flights) in question
meet the user?s needs?
 Are words emphasised in a way that highlights the trade-offs among the
different options?
 For the second and subsequent utterances, is emphasis used in a way that
makes sense given the previous system utterances?
 Is the utterance clear and easy to understand, or garbled and difficult to
understand?
Twelve mini-dialogues were used as stimuli, comprising 31 utterances in total. The
dialogues were constructed so as to contain a representative range of theme phrases,
with each mini-dialogue containing one utterance with a theme phrase. An example
dialogue, with target tunes for the APML voice, appears in Figure 13; the second system
utterance contains a theme phrase. The complete set of stimuli, including sound files, is
available on-line25 from the first author?s web page.
Fourteen native English speaking subjects participated in the study. The subjects
were all from the UK or USA and had no known hearing deficits. For participating in
the study, subjects were entered into a prize drawing.
24 http://www.webexp.info/.
25 http://www.ling.ohio-state.edu/?mwhite/flights-stimuli/.
187
Computational Linguistics Volume 36, Number 2
Figure 13
Example mini-dialogue (Dialogue 03).
4.1.2 Results. As shown in Figure 14, the APML voice received higher ratings than the
ALL voice, and both the APML and ALL voices scored much higher than the GEN
voice. Overall, the APML voice?s average rating surpassed that of the ALL voice by
0.77; its score of 5.83 was close to 6 on the rating scale, corresponding to ?mostly
natural,? while the ALL voice?s score was 5.06, just above 5 on the scale, corresponding
to ?somewhat natural.? The difference between the two voices was highly significant
(paired t-test, t433 = 10.20, p < 0.001). On the theme utterances, the difference between
the APML and ALL voices was even larger, at 0.91. With the APML voice, there was
no significant difference between the average ratings of the theme utterances and those
without theme phrases. In contrast, the ALL voice showed a trend towards the theme
utterances scoring worse than the remaining ones (t-test, 1-tailed, t432 = ?1.39, p =
0.08), with the average of the theme utterances 0.19 lower than that of the all-rheme
utterances. The GEN voice did considerably worse (0.48) on the theme utterances
(t-test, 1-tailed, t432 = ?3.06, p = 0.001).
4.2 Expert Prosody Evaluation
4.2.1 Methodology. To more directly examine whether the APML voice yielded more
contextually appropriate prosody than the ALL voice, we asked an expert ToBI anno-
tator, who was unfamiliar with the experimental hypotheses under investigation, to
annotate the perceived tunes in the synthesized stimuli for these two voices. In cases
of uncertainty, multiple annotations were given. The stimuli from the ALL voice were
always presented first, so that listening to the tune from the APML item would not bias
our annotator against the corresponding ALL item. Because there is not necessarily a
unique tune that is acceptable for the context, we also asked our annotator to indicate
the closest acceptable tune by indicating which accents and boundaries would need to
Figure 14
Mean ratings for each voice. Theme utterances are the subset containing a theme phrase. (Error
bars show standard errors.)
188
White, Clark, and Moore Generating Tailored Descriptions with Appropriate Intonation
Figure 15
Example annotation and f0 showing tune corrections (Item 05-2).
change in order to yield a tune appropriate for the context, in much the same fashion
as in our prosody prediction evaluation (Section 2.7).26 We then counted the number of
accent or boundary corrections at different levels of specificity.
An example annotation with tune corrections appears in Figure 15. This utterance
is the second one in Dialogue 05, where the user prefers to fly BMI; accordingly, the
utterance contains the theme phrase (with target tune) the only BMIL+H? flight L-H%.
With the APML version of the utterance, the annotator perceived a L+H* accent on BMI
and a L-H% boundary on de-accented flight, as desired. Additionally, the annotator
perceived a H* accent on only, which was not part of the target tune. The tune was
nevertheless considered completely acceptable, as the Edits tier is identical to the Tones
tier. By contrast, with the ALL version of the utterance, BMI was less prominent than
only and flight, and accordingly it received no pitch accent, whereas only and flight
received L+H* and H* accents, respectively; in addition, a H- boundary was annotated
on only, and a boundary that was uncertain between L- and L-L% was annotated on
26 The target tunes were not presented together with the synthesized stimuli, to avoid influencing the tunes
perceived by our expert.
189
Computational Linguistics Volume 36, Number 2
Table 5
Number of prosodic corrections of different types in all utterances and theme utterances for the
two voices. Items in bold are significantly different at p = 0.05 or less by a one-tailed Fisher?s
Exact Test.
All Utts Theme Utts
APML ALL APML ALL
Total corrections 22 49 11 26
Accents 10 33 4 16
Presence 6 20 2 8
L+H* 4 13 2 8
Boundaries 12 16 7 10
Presence 7 6 3 4
flight. This tune for the theme phrase was not considered acceptable for the context: As
the Edits tier shows, the lack of an accent on BMI, and the presence of an accent on
flight, was corrected by our annotator, as was the H- minor phrase boundary on only.
This choice makes sense for the context, as BMI is what distinguishes this option from
the one suggested in the first utterance, while flight is given information at this point in
the dialogue. Indeed, it is difficult to come up with an interpretation of onlyL+H? BMI,
with no accent on BMI, though this tune might make sense if the question of whether
the flight was code-shared with another airline was a salient one in the context.
4.2.2 Results. The results of the expert prosody evaluation appear in Table 5. Across all 31
utterances, there were 49 accent or boundary corrections for the ALL voice, versus just
22 for the APML voice, out of 688 total accent and boundary choices, a highly significant
difference (p < 0.001, Fisher?s Exact Test, 1-tailed). With the 12 theme utterances, there
were 26 corrections for the ALL voice, versus 11 for the APML voice, out of 220 total
choices (p = 0.008, FET). Looking at the pitch accents, the difference in the number
of accent corrections was significant in each case (p < 0.001, FET, and p = 0.004, FET,
respectively), as was the number of corrections involving the presence or absence of a
pitch accent (p = 0.004, FET, and p = 0.05, FET, respectively) and the number involving
L+H* accents (p = 0.02, FET, and p = 0.05, FET, respectively).27 Looking at the bound-
aries, we may observe that with the ALL voice, the majority of corrections were with
accents, whereas with the APML voice, the majority were with boundaries. Although
the APML voice had fewer boundary corrections, the difference was not significant.
As noted in the discussion of Figure 15, the perceived tunes with the APML voice
did not always exactly match the target tunes, sometimes in ways that our expert
annotator considered acceptable. More commonly, however, mismatches between the
perceived and target tunes were not judged to be acceptable. In fact, of the 22 correc-
tions for the APML voice, 19 would have been acceptable had the target tune been
successfully achieved; that is, 19 of the 22 corrections changed a perceived accent or
boundary to the one in the target tune. In 12 of these 19 cases, our annotator perceived
an accent or boundary where the target tune had none. There were also five other cases
27 A correction was counted as involving the presence or absence of a pitch accent if the word was
perceived as having an accent when it was deemed that it should have none, or was perceived as having
no accent when it was deemed that it should have one; L+H* accent and boundary presence corrections
were counted in the same fashion.
190
White, Clark, and Moore Generating Tailored Descriptions with Appropriate Intonation
Figure 16
Mean ratings of items with 0?4 prosodic corrections for the APML and ALL voices combined.
(Error bars show standard errors.)
where the target was a L-L% boundary, but a rising boundary was perceived instead.
The remaining cases involved accent type mismatches.
To examine the relationship between the number of corrections indicated by our
expert ToBI annotator and the ratings in the perception experiment, we grouped the
APML and ALL utterances by the number of corrections and calculated the mean
ratings for each group. The results appear in Figure 16, which shows the expected
relationship between mean ratings and the number of corrections, with ratings go-
ing down as the number of corrections go up. The pattern is less clear with two to
four corrections, where there are fewer tokens. One-tailed t-tests show that utterances
with zero corrections were rated significantly higher than utterances with one to four
corrections (t39 = 2.96, t35 = 5.14, t30 = 3.03, t28 = 5.35, respectively; p < 0.01 in each
case); utterances with one correction were also significantly higher than those with four
corrections (t17 = 2.30, p = 0.02).
We also examined the relationship between the listener ratings and the number
of errors noted by our expert annotator through a multiple regression analysis. The
regressors were the number of accent or boundary presence errors and the number of
remaining accent or boundary errors, involving only a difference in type. The regression
equation was Rating = 5.85? 0.41? PresenceErrors? 0.28? TypeOnlyErrors, which ac-
counted for 32% of the variance and was highly significant (F2,59 = 15.3, p < 0.001). As
expected, ratings were negatively related to accent and boundary errors, with presence
errors showing a greater impact than type-only errors. Both the effect of presence errors
(t59 = ?4.34, p < 0.001) and the effect of type-only errors (t59 = ?2.65, p = 0.01) were
significant. Because both kinds of errors had a significant impact on ratings, the results
suggest that it is worthwhile to make use of fine-grained ToBI categories where feasible,
as we discuss further in Section 6.
4.3 Objective Measures
4.3.1 Methodology. In addition to the expert prosody evaluation, we also measured the
f0 values and durations of the adjectives and nouns in the theme phrases, to see whether
they differed significantly between the APML and ALL voices. We also measured the
drop in f0 between the adjective, which should receive a contrastive pitch accent, and
the noun, which should be deaccented. Note that the f0 drop is a potentially more fine-
grained measure than pitch accent corrections, because the accents could be considered
acceptable even though the tune is less distinct in some cases.
An example in which this occurs appears in Figure 17, which shows the second
utterance of Dialogue 03, seen earlier in Figure 13, for the APML and ALL voices. Here,
191
Computational Linguistics Volume 36, Number 2
Figure 17
Example annotation and f0 showing difference in f0 drop in the theme phrases (Item 03-2).
the APML version was annotated with the target accents, namely L+H* for Lufthansa
and none for flight, whereas the ALL version was annotated with a L+H* !H* pattern,
whichwas also considered acceptable. However, with the APML version the pitch drops
from an f0 value of 293 Hz to 177 Hz, whereas with the ALL version the pitch only drops
from 260 Hz to 209 Hz. As a result, the theme tune is much less distinctive in the ALL
version, in a way that could impact the ease with which the theme phrase?s discourse
function is identified.
4.3.2 Results. Figure 18 shows the mean f0 values across all 12 theme phrases for the
adjective, which should receive contrastive emphasis, and the head noun, which should
be reduced. As the figure shows, the pattern seen in Figure 17 was borne out on the
whole in the stimuli with theme phrases. In particular, the theme phrases for the APML
voice had an f0 drop of 90 Hz on average from the adjective to the noun, whereas the
ALL voice only had an f0 drop of 50 Hz, a significant difference (t-test, 1-tailed, t22 =
2.60, p = 0.008). The durations of the adjectives and nouns, however, did not show a
significant difference. In addition to the f0 drop, a significant difference was also found
with the f0 max on the adjective and the f0 min on the noun (t-tests, 1-tailed, t28 = 1.70,
p = 0.05 and t22 = ?1.86, p = 0.04, respectively). Finally, a relatively high correlation (r =
0.78) was also found between the difference in f0 drop and the difference in the ratings
of the theme utterances for the two voices (t10 = 3.94, p = 0.001), suggesting that the less
192
White, Clark, and Moore Generating Tailored Descriptions with Appropriate Intonation
distinctive theme tunes produced by the ALL voice were perceived as less natural than
the ones produced by the APML voice.
4.4 Discussion
The perception experiment confirmed our hypothesis that listeners would prefer the
APML voice, used with contextually appropriate intonation markup, over the ALL
and GEN control voices. Both on the complete set of utterances, as well as the subset
containing theme phrases, the APML voice was rated substantially higher on average
than the ALL voice, and much higher than the GEN voice. Note that the ALL voice was
also rated much higher on average than the GEN voice?which is not surprising, given
that it has access to the same limited domain data as the APML voice?showing that
the ALL voice serves as a tough baseline to beat. The expert prosody evaluation and the
objective measures also confirmed the superiority of the APML voice, in particular on
the theme utterances.
We also found evidence in favor of our hypothesis that the preference for the APML
voice would be larger for the utterances containing theme phrases?where the intona-
tion is more marked than it is in the all-rheme utterances?as the difference between
the mean ratings of the APML and ALL voices was larger on the theme utterances than
on the remaining ones. Additionally, with the APML voice, there was no significant
difference between the mean ratings of the theme utterances and those without theme
phrases, whereas the ALL voice showed a trend towards the theme utterances scoring
worse than the all-rheme ones, and the GEN voice clearly did considerably worse on
the theme utterances. One small surprise was that with the ALL voice, the difference
between the mean ratings of the theme and all-rheme utterances did not reach the
standard level of statistical significance. Of course, it could be that with a larger sample
size, a more significant difference would be found. However, it is undoubtedly the case
that the ALL voice dropped off less on the theme utterances than did the GEN voice,
and the reason is almost certainly that the limited domain data has good coverage of
the theme phrases, and thus the ALL voice often does reasonably well on the theme
utterances even without explicit prosodic control. What is perhaps more remarkable
is that the ALL voice did not do better on the all-rheme utterances, as can be seen in
the number of expert corrections listed in Table 5 for all the utterances, which go well
beyond those in the theme utterance subset. That the ALL voice had more than double
Figure 18
Mean f0 values for emphasized and reduced words in theme phrases for the APML and ALL
voices. (Error bars show standard errors.)
193
Computational Linguistics Volume 36, Number 2
the number of corrections as the APML voice on both the complete set of utterances
and the subset containing theme phrases shows that the prosodic specifications were
important throughout.
Finally, we may observe that although the APML voice had fewer boundary correc-
tions than did the ALL voice, the difference did not reach significance, suggesting that
there is room for improvement in how boundaries are handled in the APML voice. In
particular, this result suggests that edge tones, and intermediate phrase boundaries in
particular, should affect the selection of units non-locally, as theoretically their effect on
the pitch contour spreads back to the last pitch accent. In fact, it maywell be that because
the speech synthesis system only models prosodic effects locally, essentially at the
syllable level, and does not take utterance-level structures such as tune into account, a
ceiling has been reached for both accent and phrasing performance. Representing global
prosodic structures to ensure prosodic coherence will be one of the major challenges for
future generations of speech synthesis systems.
5. Related Work
The FLIGHTS system combines and extends earlier approaches to user-tailored genera-
tion in spoken dialogue. A distinguishing feature of FLIGHTS is that it adapts its output
according to user preferences at all levels of the generation process, from the selection
of content to linguistic realization and the prosody targeted in speech synthesis.
Themost similar system to ours is MATCH (Walker et al 2004), which employs sim-
pler content planning strategies and does not explicitly point out the trade-offs among
options. MATCH also uses simple templates for realization, and does not attempt to
control intonation. Carenini and Moore?s (2000, 2006) system is also closely related, but
it does not make comparisons, and generates text rather than speech. Carberry et al?s
(1999) system likewise employs additive decision models in recommending courses,
though their focus is on dynamically acquiring amodel of the student?s preferences, and
the system is limited to recommending a single option considered better than the user?s
current one. In addition, the system only addresses the problem of selecting positive
attributes to justify the recommendation, and does not plan and prosodically realize the
positive and negative attributes of multiple suggested options.
These systems all employ a user model to select a small set of good options, and to
identify the attributes that justify their desirability, in order to present a summary, com-
parison, or recommendation to the user. Evaluation showed that tailoring recommen-
dations and comparisons to the user increased argument effectiveness and improved
user satisfaction (Walker et al 2004). Thus, the user-model (UM-) based approach is an
appropriate strategy for spoken dialogue systems when there are a small number of
options to present, either because the number of options is limited or because users can
supply sufficient constraints to winnow down a large set before querying the database
of options.
Other researchers have argued that it is important to allow users to browse the data
for a number of reasons: (1) if there are many options that share attribute values, they
will be very close in score when ranked using the UM-based approach; (2) users may
not be able to provide constraints until they hear more information about the space of
options; and (3) the UM-based approach does not give users an overview of the option
space, and this may reduce their confidence that they have been told about the best
option(s) (Demberg and Moore 2006).
Polifroni, Chung, and Seneff (2003) proposed a ?summarize and refine? (SR) ap-
proach, in which the system structures a large number of options into a small number
194
White, Clark, and Moore Generating Tailored Descriptions with Appropriate Intonation
of clusters that share attributes. The system then summarizes the clusters based on their
attributes, implicitly prompting the user to provide additional constraints. The system
produces summaries such as
I have found 983 restaurants. Most of them are located in Boston and Cambridge. There are 32
choices for cuisine. I also have information about price range.
which help the user get an overview of the option space. Chung (2004) extended this
approach by proposing a constraint relaxation strategy for coping with queries that
are too restrictive to be satisfied by any option. Pon-Barry, Weng, and Varges (2006)
found that fewer dialogue turns were necessary when the system proactively suggested
refinements and relaxations.
However, as argued in Demberg and Moore (2006), there are several limitations
to the SR approach. First, many turns may be required during the refinement process.
Second, if there is no optimal solution, exploration of trade-offs is difficult. Finally, the
attributes on which the data has been clustered may be irrelevant for the specific user.
Demberg and Moore (2006) subsequently developed the user-model-based summarize
and refine approach (UMSR) to combine the benefits of the UM and SR approaches, by
integrating user modeling with automated clustering. When there are more than a small
number of relevant options, the UMSR approach builds a cluster-based tree structure
which orders the options to allow for stepwise refinement. The effectiveness of the tree
structure, which directs the dialogue flow, is optimized by taking the user?s preferences
into account. Trade-offs between alternative options are presented explicitly to give
the user a better overview of the option space and lead the user to a more informed
choice. To give the user confidence that they are being presented with all relevant
options, a brief account of the remaining (irrelevant) options is also provided. Results of
a laboratory experiment comparing the SR andUMSR approaches demonstrated that (1)
participants preferredUMSR, (2) UMSR presentations are as easy to understand as those
of SR, (3) UMSR increases overall user satisfaction, (4) UMSR significantly improves the
user?s overview of the available options, and (5) UMSR increases users? confidence in
having heard about all relevant options. Although the UMSR approach has not been
implemented in the FLIGHTS system, it could be used when there are a large number
of available options to winnow them down to a handful of relevant ones, which would
then be compared following the approach described in this article.
As regards our work on intonation, as stated in the introduction, Prevost?s (1995)
generator has directly informed our approach to information structure and prosody; his
system does not make use of quantitative user models though, and only describes single
options. Theune (2002) likewise follows Prevost?s approach in her system, refining the
way contrast is determined in assigning pitch accents. Theune et al (2001) show that a
system employing syntactic templates and a rule-based prosody assignment algorithm
leads to more natural synthesis (of Dutch); unlike FLIGHTS though, their D2S system
does not employ a user preference model or a notion of theme phrase, and does not
distinguish different types of pitch accents. Also closely related is Kruijff-Korbayova?
et al?s (2003) information-state based dialogue system, in which the authors explore a
similar approach to using information structure across dialogue turns; however, their
system does not make use of a user model, and employs template-based realization
with much simpler sentence structures. Kruijff-Korbayova? et al likewise present an
evaluation indicating that the contextual appropriateness of spoken output (in Ger-
man) improves when intonation is assigned on the basis of information structure. In
comparison with our evaluation, theirs examines improvements over a general purpose
195
Computational Linguistics Volume 36, Number 2
text-to-speech voice with default intonation, rather than a limited domain voice, which
provides a much higher baseline in terms of the naturalness of the resulting intonation.
Less closely related is most work on machine-learning approaches to prosody
prediction in text-to-speech (TTS) systems (Hirschberg 1993; Hirschberg and Prieto
1996; Taylor and Black 1998; Dusterhoff, Black, and Taylor 1999; Brenier et al 2006)
and concept-to-speech (CTS) systems (Hitzeman et al 1998, 1999; Pan, McKeown, and
Hirschberg 2002). These approaches have typically aimed to develop generic models
of prosody prediction, by training classifiers for accents and boundaries that make
use of a considerable variety of features. For example, in predicting accent placement
(but not type), Hitzeman et al?s CTS system makes use of rhetorical relations such as
list and contrast, along with the reference type of NPs and whether they represent
first mentions of an entity in the discourse. In Pan et al?s more comprehensive study,
their system predicts accent placement (but not type), break indices, and edge tones
based on features extracted from the SURGE realizer (Elhadad 1993), deep semantic
and discourse features, including semantic type, semantic abnormality and given/new
status, and surface features, such as part of speech andword informativeness. However,
neither of these CTS approaches makes use of the theme/rheme distinction, or the
notion of kontrast that stems from Rooth?s (1992) work on alternative sets, both of
which are crucial to Steedman?s (2000a) theory of how information structure constrains
prosodic choices. More recently, Brenier et al have shown that the ratio of accented to
unaccented tokens of a word in spontaneous speech is a surprisingly effective feature in
predicting pitch accents; they also argued that using information status and contrast is
unlikely to improve upon prominence prediction based only on surface features, since
thesemanually labeled features did not yield substantial improvements in their decision
tree models. Again, however, their approach does not make use of the theme/rheme
distinction, and does not attempt to predict pitch accent type or edge tones; in addition,
they report frequent errors on auxiliaries and negatives (e.g., no), which we have found
to be important for highlighting trade-offs prosodically.
In contrast to these approaches, we have emphasized the generation and synthesis
of sharply distinctive theme and rheme tunes in the limited domain of a dialogue
system, using a hybrid rule-based and data-driven approach. In particular, whereas
Hitzeman et al (1998, 1999) and Pan, McKeown, and Hirschberg (2002) make use of
individual classifiers for prosodic realization decisions?with no means of tying the
decisions of these classifiers together?our approach instead uses rules and constraints
in the grammar to specify a space of possible realizations, through which the realizer
searches to find a sequence of words, pitch accents, and edge tones that maximizes the
probability assigned by an n-gram model for the domain.
In an approach that is more similar in spirit to ours, Bulyko and Ostendorf (2002)
likewise aim to reproduce distinctive intonational patterns in a limited domain. How-
ever, unlike our approach, theirs makes use of simple templates for generating para-
phrases, as their focus is on how deferring the final choice of wording and prosodic
realization to their synthesizer enables them to achieve more natural sounding synthetic
speech. Following on the work described in this article, Nakatsu and White (2006)
present a discriminative approach to realization ranking based on predicted synthesis
quality that is directly compatible with the FLIGHTS system.
Turning to our synthesis evaluation, we note that debate over the standardization of
speech synthesis evaluation continues, with the Blizzard Challenge (Black and Tokuda
2005; Fraser and King 2007) proving to be a useful forum for discussing and performing
evaluation across different synthesis platforms. Mayo, Clark, and King (2005) have pro-
posed to evaluate speech synthesis evaluation from a perceptual viewpoint to discover
196
White, Clark, and Moore Generating Tailored Descriptions with Appropriate Intonation
exactly what subjects pay attention to, in order to ensure that evaluation actually is
evaluating what we think it is. The authors found through the use of multidimensional
scaling (MDS) techniques that, when asked to make judgments on the naturalness of
synthetic speech, subjects made judgments relating to a number of different dimensions,
including both segmental quality and prosody. Subsequently, Clark et al (2007) found
correlations between results in MDS spaces and standard mean opinion score (MOS)
tests, but as the MOS tests did not correspond to single dimensions in the MDS space,
they suggested that it may be possible to design more informative evaluations by
asking subjects to specifically rate each factor of interest (e.g., prosody), where each
factor relates to one dimension in the MDS space. However, as no specific method is
suggested to guarantee reliable prosodic judgments from naive listeners, we have left
this question for future research, opting instead to augment the listener ratings gathered
in our perception experiment with an expert prosody evaluation and an f0 analysis of
the theme phrases.
6. Conclusions and Discussion
In this article, we have described an approach to presenting user-tailored information
in spoken dialogues which for the first time brings together multi-attribute decision
models, strategic content planning, surface realization which incorporates prosodic fea-
tures, and unit selection synthesis that takes the resulting prosodicmarkup into account.
Based on the user model, the system selects the most important subset of the available
options to mention and the attributes that are most relevant to choosing between
them. To convey these trade-offs, the system employs a novel presentation strategy
which makes it straightforward to determine information structure and the contents of
referring expressions. During surface realization with OpenCCG, the prosodic structure
is derived from the information structure in a way that allows phrase boundaries to be
determined in a flexible, data-driven fashion, andwith significantly higher acceptability
than baseline prosody prediction models in an expert evaluation. We hypothesize that
the resulting descriptions are both memorable and easy for users to understand. As a
step towards verifying this hypothesis, we have presented an experiment which shows
that listeners perceive a unit selection voice that makes use of the prosodic markup
as significantly more natural than either of two baseline synthetic voices. Through
an expert evaluation and f0 analysis, we have also confirmed the superiority of the
generator-driven intonation and its contribution to listeners? ratings. In future work, we
intend to examine the impact of our generation and synthesis methods on memorability
or other task-oriented measures.
The present study provides evidence that it is worthwhile to investigate methods
of developing general purpose synthesizers that accept prosodic specifications in their
input. The main reason that we did not use such a synthesizer in our evaluation is that
in order to build a general purpose Festival APML voice, suitable APMLmarkupwould
be required for the 2,000?3,000 utterances that make up the core unit selection database.
As these utterances are outside of the FLIGHTS domain (and thus not generated by
the NLG system), it would not be possible with current technology to provide accurate
APML markup for these utterances. Given the difficulty of automatically annotating
general texts with APML, it may be worth considering a simplified version of the
markup for the database. For example, a system which marks the location of pri-
mary phrasal stress, other pitch accents, and a simple categorization of overall tune
(wh-question, yes/no-question, statement, etc.) could be used to annotate the speech
database. This could be achieved with an accent detector and a very simple parser to
197
Computational Linguistics Volume 36, Number 2
determine tune type. The APML specification on the input could easily be mapped to
information equivalent to the database annotation by simple rules. Reasonable quality
synthesis could then be achieved without the database being fully parsed and anno-
tated. Additionally, if there is a portion of in-domain data in the database where full
annotation is available, it could be used directly when those units are searched.
An interesting unresolved question is the extent to which more generic prosody
prediction models, along the lines of Hitzeman et al (1999) and Pan, McKeown, and
Hirschberg (2002)?whichmake no use of such information structural notions as theme,
rheme, and kontrast (cf. Section 2.5)?could be trained to produce tunes as distinctive
as those we have targeted. We suspect that such models would have trouble doing so,
given data sparsity issues and the fact that machine-learned classification models tend
to discover general trends, rather than aiming to reproduce aspects of specific examples,
which may contain important but rare events. At the same time, it remains for us to
investigate whether our hybrid rule-based and data-driven approach can be generalized
to be as flexible andwidely applicable as these machine-learnedmodels, while retaining
its ability to express contrasts intelligibly. In so doing, we expect information structural
constraints in the grammar to continue to play an important role.
Acknowledgments
We thank Rachel Baker, Steve Conway, Mary
Ellen Foster, Kallirroi Georgila, Oliver
Lemon, Colin Matheson, Neide Franca
Rocha, and Mark Steedman for their
contributions to the FLIGHTS system; Eric
Fosler-Lussier and Craige Roberts for helpful
discussion; and Julie McGory for providing
the expert ToBI annotations. We also thank
the anonymous reviewers for helpful
comments and suggestions. This work was
supported in part by EPSRC grant
GR/R02450/01 and an Arts & Humanities
Innovation grant from The Ohio State
University.
References
Aylett, Matthew. 2005. Merging data driven
and rule based prosodic models for unit
selection TTS. In 5th ISCA Speech Synthesis
Workshop, pages 55?59, Pittsburg, PA.
Baker, Rachel Elizabeth. 2003. Using unit
selection to synthesise contextually
appropriate intonation in limited domain
synthesis. Master?s thesis, Department of
Linguistics, University of Edinburgh.
Baldridge, Jason and Geert-Jan Kruijff. 2002.
Coupling CCG and Hybrid Logic
Dependency Semantics. In Proceedings of
40th Annual Meeting of the Association for
Computational Linguistics, pages 319?326,
Philadelphia, PA.
Bangalore, Srinivas and Owen Rambow.
2000. Exploiting a probabilistic hierarchical
model for generation. In Proceedings of
COLING-00, pages 42?48, Saarbrucken,
Germany.
Bilmes, Jeff and Katrin Kirchhoff. 2003.
Factored language models and general
parallelized backoff. In Proceedings of
HLT-03, pages 4?6, Edmonton, Canada.
Black, Alan W. and Keiichi Tokuda. 2005.
The blizzard challenge?2005: Evaluating
corpus-based speech synthesis on
common datasets. In Interspeech 2005,
pages 77?80, Lisbon.
Blackburn, Patrick. 2000. Representation,
reasoning, and relational structures: A
hybrid logic manifesto. Logic Journal of the
IGPL, 8(3):339?625.
Bos, Johan, Ewan Klein, Oliver Lemon,
and Tetsushi Oka. 2003. DIPPER:
Description and formalisation of an
information-state update dialogue system
architecture. In 4th SIGdial Workshop on
Discourse and Dialogue, pages 115?124,
Sapporo.
Brenier, Jason, Ani Nenkova, Anubha
Kothari, Laura Whitton, David Beaver,
and Dan Jurafsky. 2006. The (non)utility
of linguistic features for predicting
prominence in spontaneous speech.
IEEE/ACL 2006 Workshop on Spoken
Language Technology, pages 54?57, Palm
Beach, Aruba.
Bulyko, Ivan and Mari Ostendorf. 2002.
Efficient integrated response generation
from multiple targets using weighted finite
state transducers. Computer Speech and
Language, 16:533?550.
Calhoun, Sasha, Malvina Nissim, Mark
Steedman, and Jason Brenier. 2005. A
framework for annotating information
structure in discourse. Proceedings of the
ACL-05 Workshop on Frontiers in Corpus
198
White, Clark, and Moore Generating Tailored Descriptions with Appropriate Intonation
Annotation II: Pie in the Sky, pages 45?52,
Ann Arbor, Michigan.
Carberry, Sandra, Jennifer Chu-Carroll, and
Stephanie Elzer. 1999. Constructing and
utilizing a model of user preferences in
collaborative consultation dialogues.
Computational Intelligence Journal,
15(3):185?217.
Carenini, Giuseppe and Johanna D. Moore.
2000. A strategy for generating evaluative
arguments. In Proceedings of INLG-00,
pages 47?54, Mitzpe Ramon.
Carenini, Giuseppe and Johanna D. Moore.
2001. An empirical study of the influence
of user tailoring on evaluative argument
effectiveness. In Proceedings of IJCAI-01,
pages 1307?1314, Seattle, WA.
Carenini, Giuseppe and Johanna D. Moore.
2006. Generating and evaluating
evaluative arguments. Artificial Intelligence,
170:925?952.
Carroll, John, Ann Copestake, Dan
Flickinger, and Victor Poznan?ski. 1999.
An efficient chart generator for (semi-)
lexicalist grammars. In Proceedings of
EWNLG-99, pages 86?95, Toulouse, France.
Carroll, John and Stefan Oepen. 2005. High
efficiency realization for a wide-coverage
unification grammar. In Proceedings of
IJCNLP-05, pages 165?176, Jeju Island,
Korea.
Chung, Grace. 2004. Developing a flexible
spoken dialog system using simulation.
In Proceedings of ACL ?04, pages 63?70,
Barcelona.
Clark, Robert A. J. and Simon King. 2006.
Joint prosodic and segmental unit selection
speech synthesis. In Proceedings of
Interspeech 2006, pages 1312?1315,
Pittsburgh, PA.
Clark, Robert A. J., Monika Podsiadlo, Mark
Fraser, Catherine Mayo, and Simon King.
2007. Statistical analysis of the Blizzard
Challenge 2007 listening test results. In
Proceedings of Blizzard Workshop (in
Proceedings of the 6th ISCA Workshop on
Speech Synthesis), pages 1?6, Bonn,
Germany.
Clark, Robert A. J., Korin Richmond, and
Simon King. 2007. Multisyn: Open-domain
unit selection for the Festival speech
synthesis system. Speech Communication,
49(4):317?330.
Currie, K. and A. Tate. 1991. O-Plan: The
open planning architecture. Artificial
Intelligence, 52:49?86.
de Carolis, Bernadina, Catherine Pelachaud,
Isabella Poggi, and Mark Steedman. 2004.
APML, a Mark-up Language for
Believable Behavior Generation. In
H. Prendinger and M. Ishizuka, editors,
Life-like Characters. Tools, Affective Functions
and Applications. Springer, Berlin,
pages 65?85.
Demberg, Vera and Johanna D. Moore. 2006.
Information presentation in spoken
dialogue systems. In Proceedings of the 11th
Conference of the European Chapter of the
Association for Computational Linguistics
(EACL-06), pages 5?72, Trento, Italy.
Dusterhoff, Kurt E., Alan W. Black, and
Paul A. Taylor. 1999. Using decision trees
within the tilt intonation model to
predict f0 contours. In Eurospeech-99,
pages 1627?1630, Budapest, Hungary.
Edwards, W. and F. H. Barron. 1994.
SMARTS and SMARTER: Improved
simple methods for multiattribute utility
measurement. Organizational Behavior and
Human Decision Processes, 60:306?325.
Elhadad, Michael. 1993. Using Argumentation
to Control Lexical Choice: A Functional
Unification Implementation. Ph.D. thesis,
Columbia University.
Foster, Mary Ellen and Michael White. 2004.
Techniques for Text Planning with XSLT.
In Proceedings of the 4th NLPXML Workshop,
pages 1?8, Barcelona, Spain.
Fraser, Mark and Simon King. 2007. The
Blizzard Challenge 2007. In Proceedings of
Blizzard Workshop (in Proceedings of the
6th ISCA Workshop on Speech Synthesis),
pages 7?12, Bonn, Germany.
Ginzburg, Jonathan. 1996. Interrogatives:
Questions, facts and dialogue. In
Shalom Lappin, editor, The Handbook of
Contemporary Semantic Theory. Blackwell,
Oxford, pages 385?422.
Hirschberg, Julia. 1993. Pitch accent in
context: Predicting intonational
prominence from text. Artificial Intelligence,
63:305?340.
Hirschberg, Julia and Pilar Prieto. 1996.
Training intonational phrasing rules
automatically for English and Spanish
text-to-speech. Speech Communication,
18:281?290.
Hitzeman, Janet, Alan W. Black, Chris
Mellish, Jon Oberlander, Massimo Poesio,
and Paul Taylor. 1999. An annotation
scheme for concept-to-speech synthesis.
In Proceedings of EWNLG-99, pages 59?66,
Toulouse.
Hitzeman, Janet, Alan W. Black, Chris
Mellish, Jon Oberlander, and Paul Taylor.
1998. On the use of automatically
generated discourse-level information in a
concept-to-speech synthesis system. In
199
Computational Linguistics Volume 36, Number 2
Proceedings of ICSLP-98, pages 2763?2768,
Sydney, Australia.
Kay, Martin. 1996. Chart generation. In
Proceedings of ACL-96, pages 200?204,
Santa Cruz, USA.
Knight, Kevin and Vasileios
Hatzivassiloglou. 1995. Two-level,
many-paths generation. In Proceedings of
ACL-95, pages 252?260, Cambridge, MA.
Kruijff, Geert-Jan M. 2003. Binding across
boundaries. In Geert-Jan M. Kruijff and
Richard T. Oehrle, editors, Resource-
Sensitivity in Binding and Anaphora. Kluwer
Academic Publishers, pages 123?158,
Dordrecht, The Netherlands.
Kruijff-Korbayova?, Ivana, Stina Ericsson,
Kepa J. Rodr??guez, and Elena Karagjosova.
2003. Producing contextually appropriate
intonation in an information-state based
dialogue system. In Proceedings of EACL-93,
pages 227?234, Budapest, Hungary.
Langkilde, Irene. 2000. Forest-based
statistical sentence generation. In
Proceedings of NAACL-00, pages 170?177,
Seattle, Washington.
Langkilde-Geary, Irene. 2002. An empirical
verification of coverage and correctness
for a general-purpose sentence generator.
In Proceedings of INLG-02, pages 17?24,
New York, NY.
Linden, Greg, Steve Hanks, and Neal Lesh.
1997. Interactive assessment of user
preference models: The automated
travel assistant. In Proceedings of User
Modeling ?97, pages 67?78, Chia Laguna,
Sardinia, Italy.
Marsi, Erwin. 2004. Optionality in evaluating
prosody prediction. In Proceedings of the
5th ISCA Speech Synthesis Workshop,
pages 13?18, Pittsburg, PA.
Martin, D. L., A. J. Cheyer, and D. B. Moran.
1999. The Open Agent Architecture: A
framework for building distributed
software systems. Applied Artificial
Intelligence, 13(1):91?128.
Mayo, C., R. A. J. Clark, and S. King. 2005.
Multidimensional scaling of listener
responses to synthetic speech. In
Interspeech 2005, pages 1725?1728, Lisbon.
Moore, Johanna, Mary Ellen Foster, Oliver
Lemon, and Michael White. 2004.
Generating tailored, comparative
descriptions in spoken dialogue. In
Proceedings of FLAIRS-04, pages 917?922,
Miami Beach, USA.
Moore, Robert C. 2002. A complete, efficient
sentence-realization algorithm for
unification grammar. In Proceedings of
INLG-02, pages 41?48, New York, NY.
Nakatsu, Crystal and Michael White. 2006.
Learning to say it well: Reranking
realizations by predicted synthesis quality.
In Proceedings of COLING-ACL ?06,
pages 1113?1120, Sydney, Australia.
Oh, Alice H. and Alexander I. Rudnicky.
2002. Stochastic natural language
generation for spoken dialog systems.
Computer, Speech & Language,
16(3/4):387?407.
Pan, Shimei, Kathleen McKeown, and Julia
Hirschberg. 2002. Exploring features from
natural language generation for prosody
modeling. Computer Speech and Language,
16:457?490.
Papineni, Kishore, Salim Roukos, Todd
Ward, and Wei-Jing Zhu. 2001. Bleu: A
method for Automatic Evaluation of
Machine Translation. Technical Report
RC22176, IBM.
Pierrehumbert, Janet. 1980. The Phonology
and Phonetics of English Intonation. Ph.D.
thesis, MIT.
Polifroni, Joseph, Grace Chung, and
Stephanie Seneff. 2003. Towards
automatic generation of mixed-initiative
dialogue systems fromWeb content.
In Proceedings of Eurospeech ?03,
pages 193?196, Geneva.
Pon-Barry, Heather, Fuliang Weng, and
Sebastian Varges. 2006. Evaluation of
content presentation strategies for an
in-car spoken dialogue system. In
Proceedings of Interspeech 2006,
pages 1930?1933, Pittsburgh, PA.
Prevost, Scott. 1995. A Semantics of Contrast
and Information Structure for Specifying
Intonation in Spoken Language Generation.
Ph.D. thesis, University of Pennsylvania.
Ratnaparkhi, Adwait. 2002. Trainable
approaches to surface natural language
generation and their application to
conversational dialog systems. Computer,
Speech & Language, 16(3/4):435?455.
Reiter, Ehud and Robert Dale. 2000. Building
Natural Language Generation Systems.
Cambridge University Press, Cambridge.
Ristad, Eric S.?1995. A Natural Law of
Succession. Technical Report
CS-TR-495-95, Princeton University.
Roberts, Craige. 1996. Information
structure: Towards an integrated
formal theory of pragmatics. Ohio State
University Working Papers in Linguistics,
49:91?136.
Rocha, Neide Franca. 2004. Evaluating
prosodic markup in a spoken dialogue
system. Master?s thesis, Department of
Linguistics, University of Edinburgh.
200
White, Clark, and Moore Generating Tailored Descriptions with Appropriate Intonation
Rooth, Mats. 1992. A theory of focus
interpretation. Natural Language Semantics,
1:75?116.
Shemtov, Hadar. 1997. Ambiguity Management
in Natural Language Generation. Ph.D.
thesis, Stanford University.
Silverman, K., M. Beckman, J. Pitrelli,
M. Ostendorf, C. Wightman, P. Price,
J. Pierrehumbert, and J. Hirschberg. 1992.
ToBI: A standard for labeling English
prosody. Proceedings of ICSLP92, 2:867?870,
Banff, Canada.
Snover, Matthew, Bonnie Dorr, Richard
Schwartz, Linnea Micciulla, and John
Makhoul. 2006. A study of translation edit
rate with targeted human annotation. In
Proceedings of the Association for Machine
Translation in the Americas (AMTA-06),
pages 223?231, Cambridge, MA.
Steedman, Mark. 2000a. Information
structure and the syntax-phonology
interface. Linguistic Inquiry,
31(4):649?689.
Steedman, Mark. 2000b. The Syntactic Process.
MIT Press, Cambridge, MA.
Steedman, Mark. 2004. Using APML to
specify intonation. Magicster Project
Deliverable 2.5. University of Edinburgh.
Available at http://www.ltg.ed.ac.uk/
magicster/deliverables/annex2.5/
apml-howto.pdf
Steedman, Mark. 2006. Information-
structural semantics for English
intonation. In Chungmin Lee, Matt
Gordon, and Daniel Bu?ring, editors,
Topic and Focus: Cross-Linguistic
Perspectives on Meaning and Intonation.
Springer, Dordrecht, pages 245?264.
Stolcke, Andreas. 2002. SRILM? An
extensible language modeling toolkit. In
Proceedings of ICSLP-02, pages 901?904,
Denver, Colorado.
Taylor, Paul and Alan Black. 1998. Assigning
phrase breaks from part-of-speech
sequences. Computer Speech and Language,
12:99?117.
Taylor, P., A. Black, and R. Caley. 1998. The
architecture of the the Festival speech
synthesis system. In Third International
Workshop on Speech Synthesis,
pages 147?151, Sydney.
Theune, Marie?t. 2002. Contrast in
concept-to-speech generation. Computer
Speech and Language, 16:491?531.
Theune, Marie?t, Esther Klabbers, Jan-Roelof
de Pijper, Emiel Krahmer, and Jan Odijk.
2001. From data to speech: A general
approach. Natural Language Engineering,
7(1):47?86.
Vallduv??, Enric and Maria Vilkuna. 1998. On
rheme and kontrast. In Peter Culicover
and Louise McNally, editors, Syntax and
Semantics, Vol. 29: The Limits of Syntax.
Academic Press, San Diego, CA,
pages 79?108.
van Deemter, Kees, Emiel Krahmer, and
Marie?t Theune. 2005. Real versus
template-based natural language
generation: A false opposition?
Computational Linguistics, 31(1):15?23.
van Santen, Jan, Alexander Kain, Esther
Klabbers, and Taniya Mishra. 2005.
Synthesis of prosody using multi-level
unit sequences. Speech Communication,
46(3?4):365?375.
Walker, M. A., S. Whittaker, A. Stent,
P. Maloor, J. D. Moore, M. Johnston, and
G. Vasireddy. 2002. Speech-plans:
Generating evaluative responses in spoken
dialogue. In Proceedings of INLG ?02,
pages 73?80, New York, NY.
Walker, M. A., S. J. Whittaker, A. Stent,
P. Maloor, J. D. Moore, M. Johnston, and
G. Vasireddy. 2004. Generation and
evaluation of user-tailored responses in
multimodal dialogue. Cognitive Science,
28:811?840.
Walker, Marilyn A., Rebecca Passonneau,
and Julie E. Boland. 2001. Quantitative
and qualitative evaluation of DARPA
Communicator spoken dialogue systems.
In Proceedings of ACL-01, pages 515?522,
Toulouse, France.
Walker, Marilyn A., Owen C. Rambow, and
Monica Rogati. 2002. Training a sentence
planner for spoken dialogue using
boosting. Computer Speech and Language,
16:409?433.
White, Michael. 2004. Reining in CCG Chart
Realization. In Proceedings of INLG-04,
pages 182?191, Brockenhurst, UK.
White, Michael. 2006a. CCG chart realization
from disjunctive inputs. In Proceedings of
INLG-06, pages 12?19, Sydney, Australia.
White, Michael. 2006b. Efficient realization of
coordinate structures in Combinatory
Categorial Grammar. Research on Language
& Computation, 4(1):39?75.
201

Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 413?423,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
That?s Not What I Meant!
Using Parsers to Avoid Structural Ambiguities in Generated Text
Manjuan Duan and Michael White
Department of Linguistics
The Ohio State University
Columbus, OH 43210, USA
{duan,mwhite}@ling.osu.edu
Abstract
We investigate whether parsers can be
used for self-monitoring in surface real-
ization in order to avoid egregious errors
involving ?vicious? ambiguities, namely
those where the intended interpretation
fails to be considerably more likely than
alternative ones. Using parse accuracy
in a simple reranking strategy for self-
monitoring, we find that with a state-
of-the-art averaged perceptron realization
ranking model, BLEU scores cannot be
improved with any of the well-known
Treebank parsers we tested, since these
parsers too often make errors that human
readers would be unlikely to make. How-
ever, by using an SVM ranker to combine
the realizer?s model score together with
features from multiple parsers, including
ones designed to make the ranker more ro-
bust to parsing mistakes, we show that sig-
nificant increases in BLEU scores can be
achieved. Moreover, via a targeted man-
ual analysis, we demonstrate that the SVM
reranker frequently manages to avoid vi-
cious ambiguities, while its ranking errors
tend to affect fluency much more often
than adequacy.
1 Introduction
Rajkumar & White (2011; 2012) have recently
shown that some rather egregious surface realiza-
tion errors?in the sense that the reader would
likely end up with the wrong interpretation?can
be avoided by making use of features inspired by
psycholinguistics research together with an other-
wise state-of-the-art averaged perceptron realiza-
tion ranking model (White and Rajkumar, 2009),
as reviewed in the next section. However, one is
apt to wonder: could one use a parser to check
whether the intended interpretation is easy to re-
cover, either as an alternative or to catch additional
mistakes? Doing so would be tantamount to self-
monitoring in Levelt?s (1989) model of language
production.
Neumann & van Noord (1992) pursued the idea
of self-monitoring for generation in early work
with reversible grammars. As Neumann & van
Noord observed, a simple, brute-force way to gen-
erate unambiguous sentences is to enumerate pos-
sible realizations of an input logical form, then
to parse each realization to see how many inter-
pretations it has, keeping only those that have
a single reading; they then went on to devise a
more efficient method of using self-monitoring to
avoid generating ambiguous sentences, targeted to
the ambiguous portion of the output. We might
question, however, whether it is really possible
to avoid ambiguity entirely in the general case,
since Abney (1996) and others have argued that
nearly every sentence is potentially ambiguous,
though we (as human comprehenders) may not
notice the ambiguities if they are unlikely. Tak-
ing up this issue, Khan et al (2008)?building on
Chantree et al?s (2006) approach to identifying
?innocuous? ambiguities?conducted several ex-
periments to test whether ambiguity could be bal-
anced against length or fluency in the context of
generating referring expressions involving coordi-
nate structures. Though Khan et al?s study was
limited to this one kind of structural ambiguity,
they do observe that generating the brief variants
when the intended interpretation is clear instanti-
ates Van Deemter?s (2004) general strategy of only
avoiding vicious ambiguities?that is, ambigui-
ties where the intended interpretation fails to be
considerably more likely than any other distractor
interpretations?rather than trying to avoid all am-
biguities.
In this paper, we investigate whether Neumann
& van Noord?s brute-force strategy for avoid-
413
ing ambiguities in surface realization can be up-
dated to only avoid vicious ambiguities, extend-
ing (and revising) Van Deemter?s general strategy
to all kinds of structural ambiguity, not just the
one investigated by Khan et al To do so?in a
nutshell?we enumerate an n-best list of realiza-
tions and rerank them if necessary to avoid vicious
ambiguities, as determined by one or more auto-
matic parsers. A potential obstacle, of course, is
that automatic parsers may not be sufficiently rep-
resentative of human readers, insofar as errors that
a parser makes may not be problematic for human
comprehension; moreover, parsers are rarely suc-
cessful in fully recovering the intended interpreta-
tion for sentences of moderate length, even with
carefully edited news text. Consequently, we ex-
amine two reranking strategies, one a simple base-
line approach and the other using an SVM reranker
(Joachims, 2002).
Our simple reranking strategy for self-
monitoring is to rerank the realizer?s n-best list
by parse accuracy, preserving the original order in
case of ties. In this way, if there is a realization in
the n-best list that can be parsed more accurately
than the top-ranked realization?even if the
intended interpretation cannot be recovered with
100% accuracy?it will become the preferred
output of the combined realization-with-self-
monitoring system. With this simple reranking
strategy and each of three different Treebank
parsers, we find that it is possible to improve
BLEU scores on Penn Treebank development data
with White & Rajkumar?s (2011; 2012) baseline
generative model, but not with their averaged
perceptron model. In inspecting the results of
reranking with this strategy, we observe that while
it does sometimes succeed in avoiding egregious
errors involving vicious ambiguities, common
parsing mistakes such as PP-attachment errors
lead to unnecessarily sacrificing conciseness or
fluency in order to avoid ambiguities that would be
easily tolerated by human readers. Therefore, to
develop a more nuanced self-monitoring reranker
that is more robust to such parsing mistakes, we
trained an SVM using dependency precision and
recall features for all three parses, their n-best
parsing results, and per-label precision and recall
for each type of dependency, together with the
realizer?s normalized perceptron model score as
a feature. With the SVM reranker, we obtain a
significant improvement in BLEU scores over
White & Rajkumar?s averaged perceptron model
on both development and test data. Additionally,
in a targeted manual analysis, we find that in cases
where the SVM reranker improves the BLEU
score, improvements to fluency and adequacy are
roughly balanced, while in cases where the BLEU
score goes down, it is mostly fluency that is made
worse (with reranking yielding an acceptable
paraphrase roughly one third of the time in both
cases).
The paper is structured as follows. In Sec-
tion 2, we review the realization ranking mod-
els that serve as a starting point for the paper.
In Section 3, we report on our experiments with
the simple reranking strategy, including a discus-
sion of the ways in which this method typically
fails. In Section 4, we describe how we trained an
SVM reranker and report our results using BLEU
scores (Papineni et al, 2002). In Section 5, we
present a targeted manual analysis of the devel-
opment set sentences with the greatest change in
BLEU scores, discussing both successes and er-
rors. In Section 6, we briefly review related work
on broad coverage surface realization. Finally, in
Section 7, we sum up and discuss opportunities for
future work in this direction.
2 Background
We use the OpenCCG
1
surface realizer for the ex-
periments reported in this paper. The OpenCCG
realizer generates surface strings for input seman-
tic dependency graphs (or logical forms) using a
chart-based algorithm (White, 2006) for Combi-
natory Categorial Grammar (Steedman, 2000) to-
gether with a ?hypertagger? for probabilistically
assigning lexical categories to lexical predicates
in the input (Espinosa et al, 2008). An exam-
ple input appears in Figure 1. In the figure,
nodes correspond to discourse referents labeled
with lexical predicates, and dependency relations
between nodes encode argument structure (gold
standard CCG lexical categories are also shown);
note that semantically empty function words such
as infinitival-to are missing. The grammar is ex-
tracted from a version of the CCGbank (Hocken-
maier and Steedman, 2007) enhanced for realiza-
tion; the enhancements include: better analyses of
punctuation (White and Rajkumar, 2008); less er-
ror prone handling of named entities (Rajkumar et
al., 2009); re-inserting quotes into the CCGbank;
1
http://openccg.sf.net
414
aa1
heh3
he h2
<Det>
<Arg0> <Arg1>
<TENSE>pres
<NUM>sg
<Arg0>
w1 want.01
m1
<Arg1>
<GenRel>
<Arg1>
<TENSE>pres
p1point
h1have.03
make.03
<Arg0>
s[b]\np/np
np/n
np
n
s[dcl]\np/np
s[dcl]\np/(s[to]\np)
np
Figure 1: Example OpenCCG semantic depen-
dency input for he has a point he wants to make,
with gold standard lexical categories for each node
and assignment of consistent semantic roles across
diathesis alternations (Boxwell and White, 2008),
using PropBank (Palmer et al, 2005).
To select preferred outputs from the chart, we
use White & Rajkumar?s (2009; 2012) realization
ranking model, recently augmented with a large-
scale 5-gram model based on the Gigaword cor-
pus. The ranking model makes choices addressing
all three interrelated sub-tasks traditionally con-
sidered part of the surface realization task in natu-
ral language generation research (Reiter and Dale,
2000; Reiter, 2010): inflecting lemmas with gram-
matical word forms, inserting function words and
linearizing the words in a grammatical and natu-
ral order. The model takes as its starting point two
probabilistic models of syntax that have been de-
veloped for CCG parsing, Hockenmaier & Steed-
man?s (2002) generative model and Clark & Cur-
ran?s (2007) normal-form model. Using the aver-
aged perceptron algorithm (Collins, 2002), White
& Rajkumar (2009) trained a structured predic-
tion ranking model to combine these existing syn-
tactic models with several n-gram language mod-
els. This model improved upon the state-of-the-art
in terms of automatic evaluation scores on held-
out test data, but nevertheless an error analysis re-
vealed a surprising number of word order, func-
tion word and inflection errors. For each kind of
error, subsequent work investigated the utility of
employing more linguistically motivated features
to improve the ranking model.
To improve word ordering decisions, White &
Rajkumar (2012) demonstrated that incorporat-
ing a feature into the ranker inspired by Gib-
son?s (2000) dependency locality theory can de-
liver statistically significant improvements in au-
tomatic evaluation scores, better match the distri-
butional characteristics of sentence orderings, and
significantly reduce the number of serious order-
ing errors (some involving vicious ambiguities) as
confirmed by a targeted human evaluation. Sup-
porting Gibson?s theory, comprehension and cor-
pus studies have found that the tendency to min-
imize dependency length has a strong influence
on constituent ordering choices; see Temperley
(2007) and Gildea and Temperley (2010) for an
overview.
Table 1 shows examples from White and Rajku-
mar (2012) of how the dependency length feature
(DEPLEN) affects the OpenCCG realizer?s output
even in comparison to a model (DEPORD) with
a rich set of discriminative syntactic and depen-
dency ordering features, but no features directly
targeting relative weight. In wsj 0015.7, the de-
pendency length model produces an exact match,
while the DEPORD model fails to shift the short
temporal adverbial next year next to the verb, leav-
ing a confusingly repetitive this year next year at
the end of the sentence. Note how shifting next
year from its canonical VP-final position to appear
next to the verb shortens its dependency length
considerably, while barely lengthening the depen-
dency to based on; at the same time, it avoids
ambiguity in what next year is modifying. In
wsj 0020.1 we see the reverse case: the depen-
dency length model produces a nearly exact match
with just an equally acceptable inversion of closely
watching, keeping the direct object in its canoni-
cal position. By contrast, the DEPORD model mis-
takenly shifts the direct object South Korea, Tai-
wan and Saudia Arabia to the end of the sentence
where it is difficult to understand following two
very long intervening phrases.
With function words, Rajkumar and White
(2011) showed that they could improve upon the
earlier model?s predictions for when to employ
that-complementizers using features inspired by
Jaeger?s (2010) work on using the principle of
uniform information density, which holds that
human language use tends to keep information
density relatively constant in order to optimize
communicative efficiency. In news text, com-
415
wsj 0015.7 the exact amount of the refund will be determined next year based on actual collections made
until Dec. 31 of this year .
DEPLEN [same]
DEPORD the exact amount of the refund will be determined based on actual collections made until Dec.
31 of this year next year .
wsj 0020.1 the U.S. , claiming some success in its trade diplomacy , removed South Korea , Taiwan and
Saudi Arabia from a list of countries it is closely watching for allegedly failing to honor U.S.
patents , copyrights and other intellectual-property rights .
DEPLEN the U.S. claiming some success in its trade diplomacy , removed South Korea , Taiwan and
Saudi Arabia from a list of countries it is watching closely for allegedly failing to honor U.S.
patents , copyrights and other intellectual-property rights .
DEPORD the U.S. removed from a list of countries it is watching closely for allegedly failing to honor U.S.
patents , copyrights and other intellectual-property rights , claiming some success in its trade
diplomacy , South Korea , Taiwan and Saudi Arabia .
Table 1: Examples of realized output for full models with and without the dependency length feature
(White and Rajkumar, 2012)
plementizers are left out two times out of three,
but in some cases the presence of that is cru-
cial to the interpretation. Generally, inserting a
complementizer makes the onset of a complement
clause more predictable, and thus less informa-
tion dense, thereby avoiding a potential spike in
information density that is associated with com-
prehension difficulty. Rajkumar & White?s exper-
iments confirmed the efficacy of the features based
on Jaeger?s work, including information density?
based features, in a local classification model.
2
Their experiments also showed that the improve-
ments in prediction accuracy apply to cases in
which the presence of a that-complementizer ar-
guably makes a substantial difference to fluency
or intelligiblity. For example, in (1), the pres-
ence of that avoids a local ambiguity, helping the
reader to understand that for the second month in
a row modifies the reporting of the shortage; with-
out that, it is very easy to mis-parse the sentence
as having for the second month in a row modifying
the saying event.
(1) He said that/?? for the second month in a row,
food processors reported a shortage of nonfat
dry milk. (PTB WSJ0036.61)
Finally, to reduce the number of subject-verb
agreement errors, Rajkumar and White (2010) ex-
tended the earlier model with features enabling it
to make correct verb form choices in sentences
involving complex coordinate constructions and
2
Note that the features from the local classification model
for that-complementizer choice have not yet been incorpo-
rated into OpenCCG?s global realization ranking model, and
thus do not inform the baseline realization choices in this
work.
with expressions such as a lot of where the correct
choice is not determined solely by the head noun.
They also improved animacy agreement with rela-
tivizers, reducing the number of errors where that
or which was chosen to modify an animate noun
rather than who or whom (and vice-versa), while
also allowing both choices where corpus evidence
was mixed.
3 Simple Reranking
3.1 Methods
We ran two OpenCCG surface realization models
on the CCGbank dev set (derived from Section 00
of the Penn Treebank) and obtained n-best (n =
10) realizations. The first one is the baseline gen-
erative model (hereafter, generative model) used
in training the averaged perceptron model. This
model ranks realizations using the product of the
Hockenmaier syntax model, n-gram models over
words, POS tags and supertags in the training sec-
tions of the CCGbank, and the large-scale 5-gram
model from Gigaword. The second one is the
averaged perceptron model (hereafter, perceptron
model), which uses all the features reviewed in
Section 2. In order to experiment with multiple
parsers, we used the Stanford dependencies (de
Marneffe et al, 2006), obtaining gold dependen-
cies from the gold-standard PTB parses and auto-
matic dependencies from the automatic parses of
each realization. Using dependencies allowed us
to measure parse accuracy independently of word
order. We chose the Berkeley parser (Petrov et
al., 2006), Brown parser (Charniak and Johnson,
2005) and Stanford parser (Klein and Manning,
2003) to parse the realizations generated by the
416
Berkeley Brown Stanford
No reranking 87.93 87.93 87.93
Labeled 87.77 87.87 87.12
Unlabeled 87.90 87.97 86.97
Table 2: Devset BLEU scores for simple ranking
on top of n-best perceptron model realizations
That?s Not What I Meant!
Using Parsers to Avoid Structural Ambiguities in Generated Text
Manjuan Duan and Michael White
Department of Linguistics
The Ohio State University
Columbus, OH 43210, USA
{duan,mwhite}@ling.osu.edu
. . . is propelling the region toward economic integration
aux
dobj
prep
pobj
(a) gold dependency
. . . is propelling toward economic integration the region
aux
pobj
prep
dobj
(b) simple ranker
. . . is propelling the region toward economic integration
aux
dobj
prep
pobj
(c) perceptron best
Figure 1: Example parsing mistake in PP-
attachment (wsj 0043.1)
Abstract
We investigate . . .
Figure 2: xa ple parsing istake in PP-
attach ent ( sj 0043.1)
two realization models and calculated precision,
recall and F
1
of the dependencies for each realiza-
tion by comparing them with the gold dependen-
cies. We then ranked the realizations by their F
1
score of parse accuracy, keeping the original rank-
ing in case of ties. We also tried using unlabeled
(and unordered) dependencies, in order to possi-
bly make better use of parses that were close to
being correct. In this setting, as long as the right
pair of tokens occur in a dependency relation, it
was counted as a correctly recovered dependency.
3.2 Results
Simple ranking with the Berkeley parser of the
generative model?s n-best realizations raised the
BLEU score from 85.55 to 86.07, well below
the averaged perceptron model?s BLEU score of
87.93. However, as shown in Table 2, none of the
parsers yielded significant improvements on the
top of the perceptron model.
Inspecting the results of simple ranking re-
vealed that while simple ranking did success-
fully avoid vicious ambiguities in some cases,
parser mistakes with PP-attachments, noun-noun
compounds and coordinate structures too often
blocked the gold realization from emerging on top.
To illustrate, Figure 2 shows an example with a
PP-attachment mistake. In the figure, the key gold
dependencies of the reference sentence are shown
in (a), the dependencies of the realization selected
by the simple ranker are shown in (b), and the de-
pendencies of the realization selected by the per-
ceptron ranker (same as gold) appear in (c), with
the parsing mistake indicated by the dashed line.
The simple ranker ends up choosing (b) as the best
realization because it has the most accurate parse
compared to the reference sentence, given the mis-
take with (c).
Other common parse errors are illustrated in
Figure 3. Here, (b) ends up getting chosen by the
simple ranker as the realization with the most ac-
curate parse given the failures in (c), where the ad-
ditional technology, personnel training is mistak-
enly analyzed as one noun phrase, a reading un-
likely to be considered by human readers.
In sum, although simple ranking helps to avoid
vicious ambiguity in some cases, the overall re-
sults of simple ranking are no better than the per-
ceptron model (according to BLEU, at least), as
parse failures that are not reflective of human in-
tepretive tendencies too often lead the ranker to
choose dispreferred realizations. As such, we turn
now to a more nuanced model for combining the
results of multiple parsers in a way that is less sen-
sitive to such parsing mistakes, while also letting
the perceptron model have a say in the final rank-
ing.
4 Reranking with SVMs
4.1 Methods
Since different parsers make different errors, we
conjectured that dependencies in the intersection
of the output of multiple parsers may be more re-
liable and thus may more reliably reflect human
comprehension preferences. Similarly, we conjec-
tured that large differences in the realizer?s percep-
tron model score may more reliably reflect human
fluency preferences than small ones, and thus we
combined this score with features for parser accu-
racy in an SVM ranker. Additionally, given that
parsers may more reliably recover some kinds of
dependencies than others, we included features for
each dependency type, so that the SVM ranker
might learn how to weight them appropriately.
Finally, since the differences among the n-best
parses reflect the least certain parsing decisions,
417
the additional technology, personnel training and promotional efforts
det
amod
nn
conj
cc
conj
amod
(a) gold dependency
the additional technology, training personnel and promotional efforts
det
amod
nn
conj
cc
conj
amod
(b) simple ranker
the additional technology, personnel training and promotional efforts
det
amod
nn
dep
cc
conj
amod
(c) perceptron best
Figure 2: Example parsing mistakes in a noun-noun compound and a coordinate structure (wsj 0085.45)
Figure 3: xa ple parsing istakes in a noun-noun co pound and a coordinate structure ( sj 0085.45)
and thus ones that may require more common
sense inference that is easy for humans but not
machines, we conjectured that including features
from the n-best parses may help to better match
human performance. In more detail, we made use
of the following feature classes for each candidate
realization:
perceptron model score the score from the real-
izer?s model, normalized to [0,1] for the real-
izations in the n-best list
precision and recall labeled and unlabeled preci-
sion and recall for each parser?s best parse
per-label precision and recall (dep) precision
and recall for each type of dependency
obtained from each parser?s best parse (using
zero if not defined for lack of predicted or
gold dependencies with a given label)
n-best precision and recall (nbest) labeled and
unlabeled precision and recall for each
parser?s top five parses, along with the same
features for the most accurate of these parses
In training, we used the BLEU scores of each
realization compared with its reference sentence
to establish a preference order over pairs of candi-
date realizations, assuming that the original corpus
sentences are generally better than related alterna-
tives, and that BLEU can somewhat reliably pre-
dict human preference judgments.
We trained the SVM ranker (Joachims, 2002)
with a linear kernel and chose the hyper-parameter
c, which tunes the trade-off between training error
and margin, with 6-fold cross-validation on the de-
vset. We trained different models to investigate the
contribution made by different parsers and differ-
ent types of features, with the perceptron model
score included as a feature in all models. For each
parser, we trained a model with its overall preci-
sion and recall features, as shown at the top of Ta-
ble 3. Then we combined these three models to get
a new model (Bkl+Brw+St in the table) . Next,
to this combined model we separately added (i)
the per-label precision and recall features from all
the parsers (BBS+dep), and (ii) the n-best features
from the parsers (BBS+nbest). The full model
(BBS+dep+nbest) includes all the features listed
above. Finally, since the Berkeley parser yielded
the best results on its own, we also tested mod-
els using all the feature classes but only using this
parser by itself.
4.2 Results
Table 3 shows the results of different SVM rank-
ing models on the devset. We calculated signifi-
cance using paired bootstrap resampling (Koehn,
2004).
3
Both the per-label precision & recall fea-
3
Kudos to Kevin Gimpel for making his implementa-
tion available: http://www.ark.cs.cmu.edu/MT/
paired_bootstrap_v13a.tar.gz
418
BLEU sig.
perceptron baseline 87.93 ?
Berkeley 88.45 *
Brown 88.34
Stanford 88.18
Bkl+Brw+St 88.44 *
BBS+dep 88.63 **
BBS+nbest 88.60 **
BBS+dep+nbest 88.73 **
Bkl+dep 88.63 **
Bkl+nbest 88.48 *
Bkl +dep+nbest 88.68 **
Table 3: Devset results of SVM ranking on top
of perceptron model. Significance codes: ?? for
p < 0.05, ? for p < 0.1.
BLEU sig.
perceptron baseline 86.94 ?
BBS+dep+nbest 87.64 **
Table 4: Final test results of SVM ranking on top
of perceptron model. Significance codes: ?? for
p < 0.05, ? for p < 0.1.
tures and the n-best parse features contributed to
achieving a significant improvement compared to
the perceptron model. Somewhat surprisingly, the
Berkeley parser did as well as all three parsers us-
ing just the overall precision and recall features,
but not quite as well using all features. The com-
plete model, BBS+dep+nbest, achieved a BLEU
score of 88.73, significantly improving upon the
perceptron model (p < 0.02). We then confirmed
this result on the final test set, Section 23 of the
CCGbank, as shown in Table 4 (p < 0.02 as well).
5 Analysis and Discussion
5.1 Targeted Manual Analysis
In order to gain a better understanding of the suc-
cesses and failures of our SVM ranker, we present
here a targeted manual analysis of the develop-
ment set sentences with the greatest change in
BLEU scores, carried out by the second author
(a native speaker). In this analysis, we consider
whether the reranked realization improves upon
or detracts from realization quality?in terms of
adequacy, fluency, both or neither?along with
a linguistic categorization of the differences be-
tween the reranked realization and the original
top-ranked realization according to the averaged
perceptron model. Unlike the broad-based and ob-
jective evaluation in terms of BLEU scores pre-
sented above, this analysis is narrowly targeted
and subjective, though the interested reader is in-
vited to review the complete set of analyzed ex-
amples that accompany the paper as a supplement.
We leave a more broad-based human evaluation by
naive subjects for future work.
Table 5 shows the results of the analysis, both
overall and for the most frequent categories of
changes. Of the 50 sentences where the BLEU
score went up the most, 15 showed an improve-
ment in adequacy (i.e., in conveying the intended
meaning), 22 showed an improvement in fluency
(with 3 cases also improving adequacy), and 16
yielded no discernible change in fluency or ade-
quacy. By contrast, with the 50 sentences where
the BLEU score went down the most, adequacy
was only affected 4 times, though fluency was af-
fected 32 times, and 15 remained essentially un-
changed.
4
The table also shows that differences
in the order of VP constituents usually led to a
change in adequacy or fluency, as did ordering
changes within NPs, with noun-noun compounds
and named entities as the most frequent subcate-
gories of NP-ordering changes. Of the cases where
adequacy and fluency were not affected, contrac-
tions and subject-verb inversions were the most
frequent differences.
Examples of the changes yielded by the SVM
ranker appear in Table 6. With wsj 0036.54,
the averaged perceptron model selects a realiza-
tion that regrettably (though amusingly) swaps
purchasing and more than 250?yielding a sen-
tence that suggests that the executives have been
purchased!?while the SVM ranker succeeds in
ranking the original sentence above all competing
realizations. With wsj 0088.25, self-monitoring
with the SVM ranker yields a realization nearly
identical to the original except for an extra comma,
where it is clear that in public modifies do this;
by contrast, in the perceptron-best realization, in
public mistakenly appears to modify be disclosed.
With wsj 0041.18, the SVM ranker unfortunately
prefers a realization where presumably seems to
modify shows rather than of two politicians as
4
The difference in the distribution of adequacy change,
fluency change and no change counts between the two condi-
tions is highly significant statistically (?
2
= 9.3, df = 2, p <
0.01). In this comparison, items where both fluency and ade-
quacy were affected were counted as adequacy cases.
419
?adq ?flu =eq ?vpord ?npord ?nn ?ne =vpord =sbjinv =cntrc
BLEU wins 15 22 16 10 9 7 3 4 - 11
BLEU losses 4 32 15 8 13 5 5 4 7 -
Table 5: Manual analysis of devset sentences where the SVM ranker achieved the greatest in-
crease/decrease in BLEU scores (50 each of wins/losses) compared to the averaged perceptron baseline
model in terms of positive or negative changes in adequacy (?adq), fluency (?flu) or neither (=eq);
changes in VP ordering (?vpord), NP ordering (?npord), noun-noun compound ordering (?nn) and
named entities (?ne); and neither positive nor negative changes in VP ordering (=vpord), subject-
inversion (=sbjinv) and contractions (=cntrc). In all but one case (counted as =eq here), the BLEU
wins saw positive changes and the BLEU losses saw negative changes.
wsj 0036.54 the purchasing managers ? report is based on data provided by more than 250 purchasing executives .
SVM RANKER [same]
PERCEP BEST the purchasing managers ? report is based on data provided by purchasing more than 250 executives .
wsj 0088.25 Markey said we could have done this in public because so little sensitive information was disclosed ,
the aide said .
SVM RANKER Markey said , we could have done this in public because so little sensitive information was disclosed ,
the aide said .
PERCEP BEST Markey said , we could have done this because so little sensitive information was disclosed in public ,
the aide said .
wsj 0041.18 the screen shows two distorted , unrecognizable photos , presumably of two politicians .
SVM RANKER the screen shows two distorted , unrecognizable photos presumably , of two politicians .
PERCEP BEST [same as original]
wsj 0044.111 ? I was dumbfounded ? , Mrs. Ward recalls .
SVM RANKER ? I was dumbfounded ? , recalls Mrs. Ward .
PERCEP BEST [same as original]
Table 6: Examples of devset sentences where the SVM ranker improved adequacy (top), made it worse
(middle) or left it the same (bottom)
in the original, which the averaged perceptron
model prefers. Finally, wsj 0044.111 is an exam-
ple where a subject-inversion makes no difference
to adequacy or fluency.
5.2 Discussion
The BLEU evaluation and targeted manual analy-
sis together show that the SVM ranker increases
the similarity to the original corpus of realizations
produced with self-monitoring, often in ways that
are crucial for the intended meaning to be apparent
to human readers.
A limitation of the experiments reported in this
paper is that OpenCCG?s input semantic depen-
dency graphs are not the same as the Stanford de-
pendencies used with the Treebank parsers, and
thus we have had to rely on the gold parses in
the PTB to derive gold dependencies for measur-
ing accuracy of parser dependency recovery. In a
realistic application scenario, however, we would
need to measure parser accuracy relative to the re-
alizer?s input. We initially tried using OpenCCG?s
parser in a simple ranking approach, but found that
it did not improve upon the averaged perceptron
model, like the three parsers used subsequently.
Given that with the more refined SVM ranker, the
Berkeley parser worked nearly as well as all three
parsers together using the complete feature set,
the prospects for future work on a more realistic
scenario using the OpenCCG parser in an SVM
ranker for self-monitoring now appear much more
promising, either using OpenCCG?s reimplemen-
tation of Hockenmaier & Steedman?s generative
CCG model, or using the Berkeley parser trained
on OpenCCG?s enhanced version of the CCG-
bank, along the lines of Fowler and Penn (2010).
6 Related Work
Approaches to surface realization have been de-
veloped for LFG, HPSG, and TAG, in addition
to CCG, and recently statistical dependency-based
approaches have been developed as well; see the
report from the first surface realization shared
420
task (Belz et al, 2010; Belz et al, 2011) for an
overview. To our knowledge, however, a com-
prehensive investigation of avoiding vicious struc-
tural ambiguities with broad coverage statistical
parsers has not been previously explored. As
our SVM ranking model does not make use of
CCG-specific features, we would expect our self-
monitoring method to be equally applicable to re-
alizers using other frameworks.
7 Conclusion
In this paper, we have shown that while using
parse accuracy in a simple reranking strategy for
self-monitoring fails to improve BLEU scores
over a state-of-the-art averaged perceptron realiza-
tion ranking model, it is possible to significantly
increase BLEU scores using an SVM ranker that
combines the realizer?s model score together with
features from multiple parsers, including ones de-
signed to make the ranker more robust to parsing
mistakes that human readers would be unlikely to
make. Additionally, via a targeted manual analy-
sis, we showed that the SVM reranker frequently
manages to avoid egregious errors involving ?vi-
cious? ambiguities, of the kind that would mislead
human readers as to the intended meaning.
As noted in Reiter?s (2010) survey, many NLG
systems use surface realizers as off-the-shelf com-
ponents. In this paper, we have focused on
broad coverage surface realization using widely-
available PTB data?where there are many sen-
tences of varying complexity with gold-standard
annotations?following the common assumption
that experiments with broad coverage realization
are (or eventually will be) relevant for NLG ap-
plications. Of course, the kinds of ambiguity that
can be problematic in news text may or may not be
the same as the ones encountered in particular ap-
plications. Moreover, for certain applications (e.g.
ones with medical or legal implications), it may be
better to err on the side of ambiguity avoidance,
even at some expense to fluency, thereby requir-
ing training data reflecting the desired trade-off to
adapt the methods described here. We leave these
application-centered issues for investigation in fu-
ture work.
The current approach is primarily suitable for
offline use, for example in report generation where
there are no real-time interaction demands. In fu-
ture work, we also plan to investigate ways that
self-monitoring might be implemented more effi-
ciently as a combined process, rather than running
independent parsers as a post-process following
realization.
Acknowledgments
We thank Mark Johnson, Micha Elsner, the OSU
Clippers Group and the anonymous reviewers for
helpful comments and discussion. This work was
supported in part by NSF grants IIS-1143635 and
IIS-1319318.
References
S. Abney. 1996. Statistical methods and linguistics. In
Judith Klavans and Philip Resnik, editors, The bal-
ancing act: Combining symbolic and statistical ap-
proaches to language, pages 1?26. MIT Press, Cam-
bridge, MA.
Anja Belz, Mike White, Josef van Genabith, Deirdre
Hogan, and Amanda Stent. 2010. Finding common
ground: Towards a surface realisation shared task.
In Proceedings of INLG-10, Generation Challenges,
pages 267?272.
Anja Belz, Michael White, Dominic Espinosa, Eric
Kow, Deirdre Hogan, and Amanda Stent. 2011. The
first surface realisation shared task: Overview and
evaluation results. In Proceedings of the Generation
Challenges Session at the 13th European Workshop
on Natural Language Generation, pages 217?226,
Nancy, France, September. Association for Compu-
tational Linguistics.
Stephen Boxwell and Michael White. 2008. Projecting
Propbank roles onto the CCGbank. In Proc. LREC-
08.
F. Chantree, B. Nuseibeh, A. De Roeck, and A. Willis.
2006. Identifying nocuous ambiguities in natural
language requirements. In Requirements Engineer-
ing, 14th IEEE International Conference, pages 59?
68. IEEE.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and maxent discriminative
reranking. In Proceedings of ACL, pages 173?180,
Ann Arbor, Michigan. Association for Computa-
tional Linguistics.
Stephen Clark and James R. Curran. 2007. Wide-
Coverage Efficient Statistical Parsing with CCG and
Log-Linear Models. Computational Linguistics,
33(4):493?552.
Michael Collins. 2002. Discriminative training meth-
ods for hidden Markov models: theory and ex-
periments with perceptron algorithms. In Proc.
EMNLP-02.
421
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher Manning. 2006. Generating typed de-
pendency parses from phrase structure parses. In
Proceedings of LREC.
Dominic Espinosa, Michael White, and Dennis Mehay.
2008. Hypertagging: Supertagging for surface real-
ization with CCG. In Proceedings of ACL-08: HLT,
pages 183?191, Columbus, Ohio, June. Association
for Computational Linguistics.
Timothy A. D. Fowler and Gerald Penn. 2010. Ac-
curate context-free parsing with Combinatory Cat-
egorial Grammar. In Proceedings of the 48th An-
nual Meeting of the Association for Computational
Linguistics, pages 335?344, Uppsala, Sweden, July.
Association for Computational Linguistics.
Edward Gibson. 2000. Dependency locality theory:
A distance-based theory of linguistic complexity.
In Alec Marantz, Yasushi Miyashita, and Wayne
O?Neil, editors, Image, Language, brain: Papers
from the First Mind Articulation Project Symposium.
MIT Press, Cambridge, MA.
Daniel Gildea and David Temperley. 2010. Do gram-
mars minimize dependency length? Cognitive Sci-
ence, 34(2):286?310.
Julia Hockenmaier and Mark Steedman. 2002. Gen-
erative models for statistical parsing with Combina-
tory Categorial Grammar. In Proc. ACL-02.
Julia Hockenmaier and Mark Steedman. 2007. CCG-
bank: A Corpus of CCG Derivations and Depen-
dency Structures Extracted from the Penn Treebank.
Computational Linguistics, 33(3):355?396.
T. Florian Jaeger. 2010. Redundancy and reduction:
Speakers manage information density. Cognitive
Psychology, 61(1):23?62, August.
Thorsten Joachims. 2002. Optimizing search engines
using clickthrough data. In Proc. KDD.
I.H. Khan, K. Van Deemter, and G. Ritchie. 2008.
Generation of referring expressions: Managing
structural ambiguities. In Proceedings of the 22nd
International Conference on Computational Lin-
guistics, pages 433?440. Association for Computa-
tional Linguistics.
Dan Klein and Christopher Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of the 41st
Meeting of the Association for Computational Lin-
guistics, pages 423?430.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Dekang Lin and
Dekai Wu, editors, Proceedings of EMNLP 2004,
pages 388?395, Barcelona, Spain, July. Association
for Computational Linguistics.
Willem J. M. Levelt. 1989. Speaking: From Intention
to Articulation. MIT Press.
G?unter Neumann and Gertjan van Noord. 1992. Self-
monitoring with reversible grammars. In Proceed-
ings of the 14th conference on Computational lin-
guistics - Volume 2, COLING ?92, pages 700?706,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Martha Palmer, Dan Gildea, and Paul Kingsbury. 2005.
The proposition bank: A corpus annotated with se-
mantic roles. Computational Linguistics, 31(1).
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proc. ACL-02.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and
interpretable tree annotation. In Proceedings of
COLING-ACL.
Rajakrishnan Rajkumar and Michael White. 2010. De-
signing agreement features for realization ranking.
In Proc. Coling 2010: Posters, pages 1032?1040,
Beijing, China, August.
Rajakrishnan Rajkumar and Michael White. 2011.
Linguistically motivated complementizer choice in
surface realization. In Proceedings of the UC-
NLG+Eval: Language Generation and Evaluation
Workshop, pages 39?44, Edinburgh, Scotland, July.
Association for Computational Linguistics.
Rajakrishnan Rajkumar, Michael White, and Dominic
Espinosa. 2009. Exploiting named entity classes in
CCG surface realization. In Proc. NAACL HLT 2009
Short Papers.
Ehud Reiter and Robert Dale. 2000. Building natu-
ral generation systems. Studies in Natural Language
Processing. Cambridge University Press.
Ehud Reiter. 2010. Natural language generation. In
Alexander Clark, Chris Fox, and Shalom Lappin,
editors, The Handbook of Computational Linguistics
and Natural Language Processing (Blackwell Hand-
books in Linguistics), Blackwell Handbooks in Lin-
guistics, chapter 20. Wiley-Blackwell, 1 edition.
Mark Steedman. 2000. The syntactic process. MIT
Press, Cambridge, MA, USA.
David Temperley. 2007. Minimization of dependency
length in written English. Cognition, 105(2):300?
333.
K. Van Deemter. 2004. Towards a probabilistic version
of bidirectional OT syntax and semantics. Journal of
Semantics, 21(3):251?280.
Michael White and Rajakrishnan Rajkumar. 2008.
A more precise analysis of punctuation for broad-
coverage surface realization with CCG. In Coling
2008: Proceedings of the workshop on Grammar
Engineering Across Frameworks, pages 17?24.
422
Michael White and Rajakrishnan Rajkumar. 2009.
Perceptron reranking for CCG realization. In Pro-
ceedings of the 2009 Conference on Empirical Meth-
ods in Natural Language Processing, pages 410?
419, Singapore, August. Association for Computa-
tional Linguistics.
Michael White and Rajakrishnan Rajkumar. 2012.
Minimal dependency length in realization ranking.
In Proceedings of the 2012 Joint Conference on
Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning,
pages 244?255, Jeju Island, Korea, July. Association
for Computational Linguistics.
Michael White. 2006. Efficient Realization of
Coordinate Structures in Combinatory Categorial
Grammar. Research on Language & Computation,
4(1):39?75.
423
Coling 2008: Proceedings of the workshop on Grammar Engineering Across Frameworks, pages 17?24
Manchester, August 2008
A More Precise Analysis of Punctuation for
Broad-Coverage Surface Realization with CCG
Michael White and Rajakrishnan Rajkumar
Department of Linguistics
The Ohio State University
Columbus, OH, USA
{mwhite,raja}@ling.osu.edu
Abstract
This paper describes a more precise anal-
ysis of punctuation for a bi-directional,
broad coverage English grammar extracted
from the CCGbank (Hockenmaier and
Steedman, 2007). We discuss various ap-
proaches which have been proposed in
the literature to constrain overgeneration
with punctuation, and illustrate how as-
pects of Briscoe?s (1994) influential ap-
proach, which relies on syntactic features
to constrain the appearance of balanced
and unbalanced commas and dashes to ap-
propriate sentential contexts, is unattrac-
tive for CCG. As an interim solution
to constrain overgeneration, we propose
a rule-based filter which bars illicit se-
quences of punctuation and cases of im-
properly unbalanced apposition. Using
the OpenCCG toolkit, we demonstrate
that our punctuation-augmented grammar
yields substantial increases in surface re-
alization coverage and quality, helping to
achieve state-of-the-art BLEU scores.
1 Introduction
In his pioneering monograph, Nunberg (1990) ar-
gues that punctuation is a systematic module of the
grammar of written text and is governed by princi-
ples and constraints like other sub-systems such as
syntax or phonology. Since then, others including
Briscoe (1994) and Doran (1998) have explored
ways of including rules and representations for
punctuation marks in broad coverage grammars. In
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
computational systems, punctuation provides dis-
ambiguation cues which can help parsers arrive at
the correct parse. From a natural language gener-
ation standpoint, text without punctuation can be
difficult to comprehend, or even misleading.
In this paper, we describe a more precise analy-
sis of punctuation for a bi-directional, broad cover-
age English grammar extracted from the CCGbank
(Hockenmaier and Steedman, 2007). In contrast to
previous work, which has been primarily oriented
towards parsing, our goal has been to develop an
analysis of punctuation that is well suited for both
parsing and surface realization. In addition, while
Briscoe and Doran have simply included punctu-
ation rules in their manually written grammars,
our approach has been to revise the CCGbank it-
self with punctuation categories and more precise
linguistic analyses, and then to extract a grammar
from the enhanced corpus.
In developing our analysis, we illustrate how as-
pects of Briscoe?s (1994) approach, which relies
on syntactic features to constrain the appearance
of balanced and unbalanced commas and dashes to
appropriate sentential contexts, is unattractive for
CCG, with its more flexible handling of word or-
der. Consequently, as an interim solution, we have
chosen to identify and filter undesirable configu-
rations when scoring alternative realizations. We
also point to other ways in which punctuation con-
straints could be incorporated into the grammar,
for exploration in future work.
Using the OpenCCG toolkit, we demonstrate
that our punctuation-enhanced grammar yields
substantial increases in surface realization quality,
helping to achieve state-of-the-art BLEU scores.
We use non-blind testing to evaluate the efficacy
of the grammar, and blind-testing to evaluate its
performance on unseen data. The baseline models
17
are (1) a grammar which has lexicalized punctu-
ation categories only for conjunction and apposi-
tion, and (2) one which has punctuation categories
corresponding to the existing treatment of punctua-
tion in the corpus. Non-blind testing results shown
a nearly 9-point increase in BLEU scores com-
pared to the best baseline model using oracle n-
grams, as well as a 40% increase in exact matches.
Blind testing results show a more than 5.5-point
increase in BLEU scores, contributing to an all-
sentences score of 0.7323 on Section 23 with over
96% coverage.
2 Background
CCG (Steedman, 2000) is a unification-based cat-
egorial grammar formalism which is defined al-
most entirely in terms of lexical entries that encode
sub-categorization information as well as syntactic
feature information (e.g. number and agreement).
Complementing function application as the stan-
dard means of combining a head with its argument,
type-raising and composition support transparent
analyses for a wide range of phenomena, includ-
ing right-node raising and long distance dependen-
cies. Semantic composition happens in parallel
with syntactic composition, which makes it attrac-
tive for generation.
OpenCCG is a parsing/generation library which
works by combining lexical categories for words
using CCG rules and multi-modal extensions on
rules (Baldridge, 2002) to produce derivations.
Surface realization is the process by which logical
forms are transduced to strings. OpenCCG uses
a hybrid symbolic-statistical chart realizer (White,
2006) which takes logical forms as input and pro-
duces sentences by using CCG combinators to
combine signs. Alternative realizations are ranked
using integrated n-gram scoring.
To illustrate the input to OpenCCG, consider
the semantic dependency graph in Figure 1. In
the graph, each node has a lexical predication
(e.g. make.03) and a set of semantic features (e.g.
?NUM?sg); nodes are connected via dependency
relations (e.g. ?ARG0?). Internally, such graphs
are represented using Hybrid Logic Dependency
Semantics (HLDS), a dependency-based approach
to representing linguistic meaning (Baldridge and
Kruijff, 2002). In HLDS, each semantic head (cor-
responding to a node in the graph) is associated
with a nominal that identifies its discourse referent,
and relations between heads and their dependents
he h2
aa1
heh3
<Det>
<Arg0> <Arg1>
<TENSE>pres
<NUM>sg
<Arg0>
w1 want.01
m1
<Arg1>
<GenRel>
<Arg1>
<TENSE>pres
p1point
h1have.03
make.03
<Arg0>
Figure 1: Semantic dependency graph from the
CCGbank for He has a point he wants to make
[. . . ]
are modeled as modal relations.
3 The need for an OpenCCG analysis of
punctuation
The linguistic analysis aims to make a broad cover-
age OpenCCG grammar extracted from the CCG-
bank (White et al, 2007) more precise by adding
lexicalized punctuation categories to deal with
constructions involving punctuation. The origi-
nal CCGbank corpus does not have lexical cate-
gories for punctuation; instead, punctuation marks
carry categories derived from their part of speech
tags and form part of a binary rule. It is as-
sumed that there are no dependencies between
words and punctuation marks and that the re-
sult of punctuation rules is the same as the non-
punctuation category. OpenCCG does not support
non-combinatory binary rules, as they can be re-
placed by equivalent lexicalized categories with
application-only slashes. For example, a binary
rule of the form , s ? s can be replaced by the
equivalent category s
?1?
/
?
s
?1?
for the comma. In
fact, this would work reasonably well for parsing,
but is inadequate for generation. To illustrate, con-
sider (1):
(1) Despite recent declines in yields, in-
vestors continue to pour cash into
money funds. (wsj 0004.10)
A comma category like the one shown above
would end up overgenerating, as sentences and
18
sentential complements would be generated with
a comma preceding them. Also, the result of the
above function application rule could act as its own
argument, producing a string of commas. More
generally, binary rules miss out on many linguis-
tic generalizations, such as the presence of manda-
tory balancing marks in sentence-medial comma
or dash adjuncts.
The literature discusses various means to ad-
dress the issue of overgeneration: absorption rules
(Nunberg, 1990), syntactic features (Doran, 1998)
and (Briscoe, 1994) and semantic features (White,
2006). Section 5 explains these approaches in de-
tail, and considers a possible system of syntactic
features for a multi-modal CCG grammar imple-
mentation. We show how such a system is inade-
quate to constrain all possible cases of overgener-
ation, motivating our decision to employ semantic
features in our bi-directional grammar.
4 Integrating an analysis of punctuation
into the grammar
As our starting point, we used an XML repre-
sentation of an enhanced version of the CCGbank
with Propbank roles projected onto it (Boxwell and
White, 2008). Contexts and constructions in which
punctuation marks occur were isolated and the cor-
pus was then restructured by inserting new cate-
gories and modified derivations using XSL trans-
forms. In many cases this also involved modify-
ing the gold standard derivations substantially and
adding semantic representations to syntactic cat-
egories using logical form templates. Currently,
the algorithm succeeds in creating logical forms
for 98.01% of the sentences in the development
section (Sect. 00) of the converted CCGbank, and
96.46% of the sentences in the test section (Sect.
23). Of these, 92.10% of the development LFs
are semantic dependency graphs with a single root,
while 92.12% of the test LFs have a single root.
The remaining cases, with multiple roots, are miss-
ing one or more dependencies required to form a
fully connected graph. These missing dependen-
cies usually reflect inadequacies in the current log-
ical form templates. In Section 00, 89 punctuation
categories were created (66 commas, 14 dashes
and 3 each for the rest) out of 54 classes of binary
rules (37 comma, 8 dash, 3 apiece of colon, paren-
thesis and dots). Three high frequency comma cat-
egories are explained below.
4.1 Sentential Adjuncts
The comma in example (1) has been analysed
as selecting a sentential modifier to its left,
Despite recent declines in yields, to result in a
sentential modifier which then selects the rest of
the sentence. This results in the following lexical
category and semantics for the comma category:
(2) , ` s
?1?ind=X1 ,mod=M
/s
?1?
\
?
(s
?1?
/s
?1?
)
: @
M
(?EMPH-INTRO?+)
Syntactic categories and their semantics are linked
by index variables in the feature structures of cat-
egories. Index variables for semantic heads (e.g.
X1) are conventionally named X plus the number
of the feature structure. To support modifier modi-
fiers, as in (2), semantic heads of modifiers are also
made available through a modifier index feature,
with a variable conventionally named M .
1
Here,
the effect of combining the comma with the phrase
headed by despite is to add the ?EMPH-INTRO?+
feature to the despite-phrase?s semantics. Follow-
ing (Bayraktar et al, 1998), this feature indicates
that the comma has the discourse function of em-
phasizing an introductory clause or phrase. Dur-
ing realization, the feature triggers the look-up of
the category in (2), and prevents the re-application
of the category to its own output (as the feature
should only be realized once).
The category in (2) illustrates our approach,
which is to assign to every punctuation mark (other
than balancing marks) a category whose LF in-
cludes a feature or relation which represents its
discourse semantic function in broad-brush terms
such as emphasis, elaboration and apposition.
4.2 Verbs of reported speech
In (3), the comma which follows Neverthless and
sets off the phrase headed by said has the category
in (4):
(3) Nevertheless, said Brenda Malizia Ne-
gus, editor of Money Fund Report,
yields may blip up again before they
blip down because of recent rises in
short-term interest rates. (wsj 0004.8)
(4) , ` s
?2?
/s
?2?
/
?
punct[, ]/
?
(s
?1?dcl
\s
?2?dcl
)
: @
X2
(?ELABREL? ?X1)
1
A limited form of default unification is used in the im-
plementation to keep multiple modifiers from conflicting. As
the names of index variables are entirely predictable, they are
suppressed in the remainder of the paper.
19
In the genre of newswire text, this construction
occurs frequently with verbs of reported speech.
The CCGbank derivation of (3) assigns the cate-
gory s
?1?dcl
\s
?2?dcl
to the phrase headed by said,
the same category that is used when the phrase
follows the missing sentential complement. The
comma category in (4) selects for this category
and a balancing comma and then converts it to
a pre-sentential modifier, s
?2?
/s
?2?
. Semantically,
an elaboration relation is added between the main
clause and the reported speech phrase.
Category (4) overgenerates to some extent in
that it will allow a comma at the beginning of the
sentence. To prevent this, an alternative would be
to make the comma explicitly select for lexical ma-
terial to its left (in this case for the category of Nev-
erthless). Another possibility would be to follow
Doran (1998) in analyzing the above construction
by using the verb itself to select for the comma.
However, since our method involves changing the
gold standard derivations, and since making the
verb select extra commas or having the comma se-
lect leftward material would entail substantial fur-
ther changes to the derivations, we have opted to
go with (4), balancing adequacy and convenience.
4.3 NP appositives
Neither the Penn Tree Bank nor the CCGbank
distinguishes between NP appositives and NP
conjunctions. We wrote a set of simple heuristic
rules to enforce this distinction, which is vital
to generation. Appositives can occur sentence
medially or finally. The conventions of writing
mandate that sentence medial appositives should
be balanced?i.e., the appositive NP should
be surrounded by commas or dashes on both
sides?while sentence final appositives should
be unbalanced?i.e., they should only have one
preceding comma or dash. The categories and
semantics for unbalanced and balanced appositive
commas are, respectively:
(5) a. , ` np
?1?
\np
?1?
/
?
np
?3?
: @
X1
(?APPOSREL? ?X3)
b. , ` np
?1?
\np
?1?
/
?
punct[, ]/
?
np
?3?
: @
X1
(?APPOSREL? ?X3)
Here, the unbalanced appositive has a category
where the comma selects as argument the apposi-
tive NP and converts it to a nominal modifier. For
balanced appositives, the comma selects the ap-
positive NP and the balancing comma to form a
nominal modifier (examples are given in the next
section).
5 Constraining overgeneration in
bi-directional grammars
A complex issue that arises in the design of bi-
directional grammars is ensuring the proper pre-
sentation of punctuation. Among other things, this
involves the task of ensuring the correct realization
of commas introducing noun phrase appositives?
in our case, choosing when to use (5a) vs. (5b). In
this section, we consider and ultimately reject a so-
lution that follows Briscoe (1994) in using syntac-
tic features. As an alternative, interim solution, we
then describe a rule-based filter which bars illicit
punctuation sequences and improperly unbalanced
apposition. The paradigm below helps illustrate
the issues:
(6) John, CEO of ABC, loves Mary.
(7) * John, CEO of ABC loves Mary.
(8) Mary loves John, CEO of ABC.
(9) * Mary loves John, CEO of ABC,.
(10) Mary loves John, CEO of ABC, madly.
(11) * Mary loves John, CEO of ABC madly.
5.1 Absorption vs. syntactic features
Nunberg (1990) argues that text adjuncts intro-
duced by punctuation marks have an underlying
representation where these adjuncts have marks on
either side. They attain their surface form when
a set of presentation rules are applied. This ap-
proach ensures that all sentence medial cases like
(6) and (10) above are generated correctly, while
unacceptable examples (7) and (11) would not be
generated at all. Example (8) would at first be
generated as (9): to deal with such sentences,
where two points happen to coincide, Nunberg
posits an implicit point which is absorbed by the
adjacent point. Absorption occurs according to
the ?strength? of the two points. Strength is de-
termined according to the Point Absorption Hi-
erarchy, which ranks commas lower than dashes,
semi-colons, colons and periods. As White (1995)
observes, from a generation-only perspective, it
makes sense to generate text adjuncts which are
always balanced and post-process the output to
delete lower ranked points, as absorption uses rel-
atively simple rules that operate independently of
20
the hierarchy of the constituents. However, us-
ing this approach for parsing would involve a pre-
processing step which inserts commas into possi-
ble edges of possible constituents, as described in
(Forst and Kaplan, 2006). To avoid this consider-
able complication, Briscoe (1994) has argued for
developing declarative approaches involving syn-
tactic features, with no deletions or insertions of
punctuation marks.
5.2 Features for punctuation in CCG?
Unfortunately, the feature-based approach appears
to be inadequate for dealing with the class of ex-
amples presented above in CCG. This approach in-
volves the incorporation of syntactic features for
punctuation into atomic categories so that certain
combinations are blocked. To ensure proper ap-
positive balancing sentence finally, the rightmost
element in the sentence should transmit a relevant
feature to the clause level, which the sentence-final
period can then check for the presence of right-
edge punctuation. Possible categories for a tran-
sitive verb and the full stop appear below:
(12) loves ` s
?1?bal=BAL,end=PE
\np
?2?bal=+
/np
?3?bal=BAL,end=PE
(13) . ` sent\
?
s
end=nil
Here the feature variables BAL and PE of the right-
most argument of the verb would unify with the
corresponding result category feature values to re-
alize the main clauses of (8) and (9) with the fol-
lowing feature values:
(14) Mary loves John, CEO of ABC `
s
?1?bal=?,end=nil
(15) Mary loves John, CEO of ABC, `
s
?1?bal=+,end=comma
Thus, in (15), the sentence-final period would not
combine with s
?1?bal=+,end=comma
and the deriva-
tion would be blocked.
2
5.2.1 Issue: Extraction cases
The solution sketched above is not adequate to
deal with extraction involving ditransitive verbs in
cases like (16) and (17):
2
It is worth noting than an n-gram scorer would highly
disprefer example (9), as a comma period sequence would not
be attested in the training data. However, an n-gram model
cannot be relied upon to eliminate examples like (11), which
would likely be favored as they are shorter than their balanced
counterparts.
(16) Mary loves a book that John gave Bill,
his brother.
(17) * Mary loves a book that John gave Bill,
his brother,.
As Figure 2 shows, an unacceptable case like (17)
is not blocked. Even when the sentence final NP is
balanced, the end=comma value is not propagated
to the root level. This is because the end feature
for the relative clause should depend on the first
(indirect) object of gave, rather than the second
(direct) object as in a full ditransitive clause. A
possible solution would be to introduce more fea-
tures which record the presence of punctuation in
the leftward and rightward arguments of complex
categories; this would be rather baroque, however.
5.2.2 Issue: Crossing composition
Another issue is how crossing composition, used
with adverbs in heavy NP shift contructions, inter-
acts with appositives, as in the following examples:
(18) Mary loves madly John, CEO of ABC.
(19) * Mary loves madly John, CEO of ABC,.
For examples (10) and (11), which do not involve
crossing composition, the category for the adverb
should be the one in (20):
(20) madly ` s
?1?end=nil
\np
?2?
\
(s
?1?bal=+
\np
?2?
)
Here the bal=+ feature on the argument of the ad-
verb madly ensures that the direct object of the
verb is balanced, as in (10); otherwise, the deriva-
tion fails, as in (11). Irrespective of the value
of the end feature of the argument, the result of
the adverb has the feature end=nil as the post-
modifier is lexical material which occurs after the
VP. With crossing composition, however, category
(20) would licence an erroneous derivation for ex-
ample (19), as the end=nil feature on the result of
the adverb category would prevent the percolation
of the end feature at the edge of the phrase to the
clausal root, as Figure 3 shows.
To block such derivations, one might consider
giving the adverb another category for use with
crossing composition:
(21) madly ` s
?1?
\np
?2?
\
?
(s
?1?
\np
?2?
)
The use of the non-associative, permutative modal-
ity ? on the main slash allows the crossing com-
position rule to be applied, and feature inheritance
21
that John gave Bill, his brother,
(n
end=PE
\n)/(s
end=PE
/np) np s
end=PE
\np/np
end=PE
/np np
end=comma
>T >
s/(s\np) s
end=PE
\np/np
end=PE
>B
s
end=PE
/np
end=PE
>
n
end=PE
\n
Figure 2: Object extraction
Mary loves madly John, CEO, .
np s
end=PE
\np/np
end=PE
s 1
end=nil
\np 1\(s 1
bal=+
\np 1) np
bal=+,end=comma
sent\
?
s
end=nil
<B
?
s
end=nil
\np/np
end=PE
>
s
end=nil
\np
<
s
end=nil
<
sent
Figure 3: Crossing composition
ensures that the end feature from the verb loves
is also copied over. Thus, in example (19), the
punctuation at the edge of the phrase would be
percolated to the clausal root, where the sentence-
final period would block the derivation. However,
in the slash modality inheritance hierarchy pro-
posed by Baldridge (2002), the ? modality inher-
its the properties of function application. Conse-
quently, this category could also lead to the erro-
neous derivation of example (11). In such a deriva-
tion, category (21) will not require the direct ob-
ject to have a balanced appositive; meanwhile, the
end=nil feature on the direct object will propagate
to the clausal root, where it will happily combine
with the category for the full stop. Finally, having
two distinct categories for the adverb would off-
set the advantage of multi-modal categorial gram-
mar in dealing with word order variation, where it
is possible to use one category in situations where
otherwise several categories would be required.
5.3 A rule-based filter to constrain
overgeneration
For the reasons discussed in the preceding section,
we decided not to use syntactic features to con-
strain overgeneration. Instead, we have employed
semantic features in the logical form together with
a rule-based filter, as an interim solution. Dur-
ing realization, the generated output is examined
and fragments where two marks appear in a row
are eliminated. Additionally, to handle improp-
erly unbalanced punctuation, we modified the re-
sult categories of unbalanced appositive commas
and dashes to include a feature marking unbal-
anced punctuation, as follows:
(22) , ` np
?1?unbal=comma
\
?
np
?1?
/
?
np
?2?
Then, during realization, a filter on derivations
looks for categories such as np
unbal=comma
, and
checks to make sure this NP is followed by a an-
other punctuation mark in the string. We report on
the effects of the filter in our results section.
6 Evaluation
We extracted a grammar from the restructured cor-
pus and created testbeds of logical forms under the
following conditions:
1. Baseline 1: A CCGbank version which has no
lexicalized categories corresponding to any
of the punctuation marks except sentence fi-
nal marks and commas which conjoin ele-
ments or introduce NP appositives. Conjunc-
tion and apposition are frequent in the corpus
and if excluded, logical forms for many sen-
tences are not produced, weakening the base-
line considerably.
2. Baseline 2: A CCGbank version where
all punctuation marks (except conjunc-
tion/apposition commas and sentence-final
marks, which have proper categories) have
lexicalized MMCCG categories with no se-
mantics, corresponding to binary rules in the
original CCGbank.
3. The CCGbank augmented with punctuation
categories.
22
Testing was done under four conditions:
1. Non-blind testing with oracle n-gram scoring.
This condition tests the grammar most di-
rectly, as it avoids the issue of lexical smooth-
ing and keeps the combinatorial search man-
ageable. A grammar extracted from the de-
velopment section (Section 00) of the CCG-
bank was applied to the LF testbed of that
section, using oracle n-gram scoring (along
with FLMs, see next) to generate the sen-
tences back. For each logical form, the gener-
ated output sentence was compared with the
actual gold standard sentence corresponding
to that logical form.
2. Blind testing with factored language mod-
els (FLM) and lexical smoothing, following
(White et al, 2007). Blind testing naturally
provides a more realistic test of performance
on unseen data. Here logical forms of Sec-
tions 00 and 23 were created using gram-
mars of those sections respectively and then
a grammar was extracted from the standard
training sections (02-21). This grammar was
used to generate from the LFs of the develop-
ment and test sections; for space reasons, we
only report the results on the test section.
3. Blind testing with hypertagging. Hypertag-
ging (Espinosa et al, 2008) is supertagging
for surface realization; it improves realizer
speed and coverage with large grammars by
predicting lexical category assignments with
a maximum entropy model.
4. The punctuation-enhanced grammars were
tested in the three conditions above with and
without the balanced punctuation filter.
7 Results
Non-blind testing results in Table 1 indicate that
both exact match figures as well BLEU scores in-
crease substantially in comparison to the baselines
when a punctuation augmented grammar is used.
The difference is especially notable when oracle
n-gram scoring is used. The punctuation filter im-
proves performance as exact matches increase by
1.66% and BLEU scores also show a slight in-
crease. Complete realizations are slightly worse
for the augmented grammar than Baseline 1, but
the coverage of the baseline grammar is lower.
Table 1: Non-blind testing on Section 00 (Gram-
mar coverage: Baseline 1, 95.8%; Baseline 2,
95.03%; Punct grammar, 98.0%)
N-grams Grammar Exact Complete BLEU
Oracle Baseline 1 35.8% 86.2% 0.8613
Baseline 2 39.10% 53.58% 0.8053
Punct 75.9% 85.3% 0.9503
FLM w/o Baseline 1 17.7% 83.0% 0.7293
filter Baseline 2 5.72% 4.18% 0.4470
Punct 29.7% 80.6% 0.7984
FLM w/ filt. Punct 31.3% 80.6% 0.8062
Table 2: Blind testing on Section 23 with FLM
(Grammar coverage: Baseline 1, 94.8%; Base-
line 2, 95.06%; Punct grammar, 96.5%)
Hyp., Filt. Grammar Exact Complete BLEU
no, w/o Baseline 1 11.1% 46.4% 0.6297
Baseline 2 2.97% 3.97% 0.3104
Punct 18.0% 43.2% 0.6815
no, w/ Punct 19.3% 43.3% 0.6868
yes, w/o Punct 20.4% 61.5% 0.7270
yes, w/ Punct 21.6% 61.5% 0.7323
Blind testing results shown in Table 2 also demon-
strate that the augmented grammar does better than
the baseline in terms of BLEU scores and ex-
act matches, with the hypertagger further boosting
BLEU scores and the number of complete realiza-
tions. The use of the filter yields a further 1.2?
1.3% increase in exact match figures as well as a
half a BLEU point improvement; a planned col-
lection of human judgments may reveal that these
improvements are more meaningful than the scores
would indicate.
Baseline 2, which models all punctuation, per-
forms very badly with FLM scoring though it does
better than the minimal punctuation Baseline 1
with oracle scoring. The main reason for this is
that, without any semantic or syntactic features to
constrain punctuation categories, they tend to re-
apply to their own output, clogging up the chart.
This results in a low number of complete realiza-
tions as well as exact matches.
While direct comparisons cannot really be made
across grammar frameworks, as inputs vary in
their semantic depth and specificity, we observe
that our all-sentences BLEU score of 0.7323 ex-
ceeds that of Hogan et al (2007), who report a
top score of 0.6882 including special treatment of
multi-word units (though their coverage is near
100%). Nakanishi et al (2005) and Langkilde-
23
Geary (2002) report scores several points higher,
though the former is limited to sentences of length
20 or less, and the latter?s coverage is much lower.
8 Conclusion
We have shown that incorporating a more pre-
cise analysis of punctuation into a broad-coverage
reversible grammar extracted from the CCGbank
yields substantial increases in the number of ex-
act matches and BLEU scores when performing
surface realization with OpenCCG, contributing to
state-of-the-art results. Our discussion has also
highlighted the inadequacy of using syntactic fea-
tures to control punctuation placement in CCG,
leading us to develop a filter to ensure appro-
priately balanced commas and dashes. In fu-
ture work, we plan to investigate a more satisfac-
tory grammatical treatment involving constraints
in independent orthographic derivations, perhaps
along the lines of the autonomous prosodic deriva-
tions which Steedman and Prevost (1994) discuss.
An evaluation of parsing side performance is also
planned.
Acknowledgments
We thank the anonymous reviewers, Detmar Meur-
ers and the Clippers and Synners groups at OSU
for helpful comments and discussion.
References
Baldridge, Jason and Geert-Jan Kruijff. 2002. Cou-
pling CCG and Hybrid Logic Dependency Seman-
tics. In Proc. ACL-02.
Baldridge, Jason. 2002. Lexically Specified Deriva-
tional Control in Combinatory Categorial Grammar.
Ph.D. thesis, University of Edinburgh.
Bayraktar, Murat, Bilge Say, and Varol Akman. 1998.
An Analysis of English Punctuation: The Special
Case of Comma. International Journal of Corpus
Linguistics, 3(1):33?58.
Boxwell, Stephen and Michael White. 2008. Pro-
jecting Propbank roles onto the CCGbank. In Proc.
LREC-08. To appear.
Briscoe, Ted. 1994. Parsing (with) punctuation. Tech-
nical report, Xerox, Grenoble, France.
Doran, Christine. 1998. Incorporating Punctuation
into the Sentence Grammar: A Lexicalized Tree Ad-
joining Grammar Perspective. Ph.D. thesis, Univer-
sity of Pennsylvania.
Espinosa, Dominic, Michael White, and Dennis Mehay.
2008. Hypertagging: Supertagging for surface real-
ization with CCG. In Proc. ACL-08:HLT. To appear.
Forst, Martin and Ronald M. Kaplan. 2006. The im-
portance of precise tokenizing for deep grammars.
In Proc. LREC-06.
Hockenmaier, Julia and Mark Steedman. 2007. CCG-
bank: A Corpus of CCG Derivations and Depen-
dency Structures Extracted from the Penn Treebank.
Computational Linguistics, 33(3):355?396.
Hogan, Deirdre, Conor Cafferkey, Aoife Cahill, and
Josef van Genabith. 2007. Exploiting multi-word
units in history-based probabilistic generation. In
Proc. EMNLP-CoNLL-07.
Langkilde-Geary, Irene. 2002. An empirical veri-
fication of coverage and correctness for a general-
purpose sentence generator. In Proc. INLG-02.
Nakanishi, Hiroko, Yusuke Miyao, and Jun?ichi Tsujii.
2005. Probabilistic methods for disambiguation of
an HPSG-based chart generator. In Proc. IWPT-05.
Nunberg, Geoffrey. 1990. The Linguistics of Punctua-
tion. CSLI Publications, Stanford, CA.
Steedman, Mark and S. Prevost. 1994. Specifying in-
tonation from context for speech synthesis. Speech
Communication, 15(1?2):139?153.
Steedman, Mark. 2000. The syntactic process. MIT
Press, Cambridge, MA, USA.
White, Michael, Rajakrishnan Rajkumar, and Scott
Martin. 2007. Towards broad coverage surface re-
alization with CCG. In Proc. of the Workshop on
Using Corpora for NLG: Language Generation and
Machine Translation (UCNLG+MT).
White, Michael. 1995. Presenting punctuation. In Pro-
ceedings of the Fifth European Workshop on Natural
Language Generation, pages 107?125.
White, Michael. 2006. Efficient realization of coordi-
nate structures in combinatory categorial grammar.
Research on Language and Computation, 4(1):39?
75.
24
Workshop on Monolingual Text-To-Text Generation, pages 74?83,
Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 74?83,
Portland, Oregon, 24 June 2011. c?2011 Association for Computational Linguistics
Creating Disjunctive Logical Forms from Aligned Sentences for
Grammar-Based Paraphrase Generation
Scott Martin and Michael White
Department of Linguistics
The Ohio State University
Columbus, Ohio, USA
{scott,mwhite}@ling.ohio-state.edu
Abstract
We present a method of creating disjunctive
logical forms (DLFs) from aligned sentences
for grammar-based paraphrase generation us-
ing the OpenCCG broad coverage surface re-
alizer. The method takes as input word-level
alignments of two sentences that are para-
phrases and projects these alignments onto the
logical forms that result from automatically
parsing these sentences. The projected align-
ments are then converted into phrasal edits
for producing DLFs in both directions, where
the disjunctions represent alternative choices
at the level of semantic dependencies. The re-
sulting DLFs are fed into the OpenCCG re-
alizer for n-best realization, using a pruning
strategy that encourages lexical diversity. Af-
ter merging, the approach yields an n-best list
of paraphrases that contain grammatical alter-
natives to each original sentence, as well as
paraphrases that mix and match content from
the pair. A preliminary error analysis suggests
that the approach could benefit from taking the
word order in the original sentences into ac-
count. We conclude with a discussion of plans
for future work, highlighting the method?s po-
tential use in enhancing automatic MT evalu-
ation.
1 Introduction
In this paper, we present our initial steps towards
merging the grammar-based and data-driven para-
phrasing traditions, highlighting the potential of
our approach to enhance the automatic evaluation
of machine translation (MT). Kauchak and Barzi-
lay (2006) have shown that creating synthetic ref-
erence sentences by substituting synonyms from
Wordnet into the original reference sentences can
increase the number of exact word matches with
an MT system?s output and yield significant im-
provements in correlations of BLEU (Papineni et
al., 2002) scores with human judgments of trans-
lation adequacy. Madnani (2010) has also shown
that statistical machine translation technique can be
employed in a monolingual setting, together with
paraphrases acquired using Bannard and Callison-
Burch?s (2005) pivot method, in order to enhance
the tuning phase of training an MT system by aug-
menting a reference translation with automatic para-
phrases. Earlier, Barzilay and Lee (2003) and Pang
et al (2003) developed approaches to aligning mul-
tiple reference translations in order to extract para-
phrases and generate new sentences. By starting
with reference sentences from multiple human trans-
lators, these data-driven methods are able to capture
subtle, highly-context sensitive word and phrase al-
ternatives. However, the methods are not particu-
larly adept at capturing variation in word order or
the use of function words that follow from general
principles of grammar. By contrast, grammar-based
paraphrasing methods in the natural language gen-
eration tradition (Iordanskaja et al, 1991; Elhadad
et al, 1997; Langkilde and Knight, 1998; Stede,
1999; Langkilde-Geary, 2002; Velldal et al, 2004;
Gardent and Kow, 2005; Hogan et al, 2008) have
the potential to produce many such grammatical al-
ternatives: in particular, by parsing a reference sen-
tence to a representation that can be used as the in-
put to a surface realizer, grammar-based paraphrases
can be generated if the realizer supports n-best out-
put. To our knowledge though, methods of using a
grammar-based surface realizer together with multi-
ple aligned reference sentences to produce synthetic
74
Source Liu Lefei says that [in the long term] , in terms of asset alocation, overseas in-
vestment should occupy a certain proportion of [an insurance company?s overall
allocation] .
Reference Liu Lefei said that in terms of capital allocation , outbound investment should make
up a certain ratio of [overall allocations for insurance companies] [in the long run]
.
Paraphrase Liu Lefei says that [in the long run], in terms of capital allocation, overseas invest-
ment should occupy the certain ratio of an [insurance company?s overall allocation]
Table 1: Zhao et al?s (2009) similarity example, with italics added to show word-level substitutions, and square
brackets added to show phrase location or construction mismatches. Here, the source sentence (itself a reference
translation) has been paraphrased to be more like the reference sentence.
references have not been investigated.1
As an illustration of the need to combine gram-
matical paraphrasing with data-driven paraphrasing,
consider the example that Zhao et al (2009) use
to illustrate the application of their paraphrasing
method to similarity detection, shown in Table 1.
Zhao et al make use of a large paraphrase table,
similar to the phrase tables used in statistical MT, in
order to construct paraphrase candidates. (Like the-
sauri or WordNet, such resources are complemen-
tary to the ones we make use of here.) To test their
system?s ability to paraphrase reference sentences in
service of MT evaluation, they attempt to paraphrase
one reference translation to make it more similar to
another reference translation; thus, in Table 1, the
source sentence (itself a reference translation) has
been paraphrased to be more like the (other) refer-
ence sentence. As indicated by italics, their sys-
tem has successfully paraphrased term, asset and
proportion as run, capital and ratio, respectively
(though the certain seems to have been mistakenly
substituted for a certain). However, their system
is not capable of generating a paraphrase with in
the long run at the end of the sentence, nor can it
rephrase insurance company?s overall allocation as
overall allocations for insurance companies, which
would seem to require access to more general gram-
matical knowledge.
To combine grammar-based paraphrasing with
lexical and phrasal alternatives gleaned from mul-
tiple reference sentences, our approach takes advan-
1The task is not unrelated to sentence fusion in multidoc-
ument summarization (Barzilay and McKeown, 2005), except
there the goal is to produce a single, shorter sentence from mul-
tiple related input sentences.
tage of the OpenCCG realizer?s ability to generate
from disjunctive logical forms (DLFs), i.e. packed
semantic dependency graphs (White, 2004; White,
2006a; White, 2006b; Nakatsu and White, 2006; Es-
pinosa et al, 2008; White and Rajkumar, 2009). In
principle, semantic dependency graphs offer a better
starting point for paraphrasing than the syntax trees
employed by Pang et. al, as paraphrases can gener-
ally be expected to be more similar at the level of
unordered semantic dependencies than at the level
of syntax trees. Our method starts with word-level
alignments of two sentences that are paraphrases,
since the approach can be used with any alignment
method from the MT (Och and Ney, 2003; Haghighi
et al, 2009, for example) or textual inference (Mac-
Cartney et al, 2008, inter alia) literature in princi-
ple. The alignments are projected onto the logical
forms that result from automatically parsing these
sentences. The projected alignments are then con-
verted into phrasal edits for producing DLFs in both
directions, where the disjunctions represent alterna-
tive choices at the level of semantic dependencies.
The resulting DLFs are fed into the OpenCCG re-
alizer for n-best realization. In order to enhance
the variety of word and phrase choices in the n-best
lists, a pruning strategy is used that encourages lex-
ical diversity. After merging, the approach yields
an n-best list of paraphrases that contain grammat-
ical alternatives to each original sentence, as well
as paraphrases that mix and match content from the
pair.
The rest of the paper is organized as follows. Sec-
tion 2 provides background on surface realization
with OpenCCG and DLFs. Section 3 describes our
75
method of creating DLFs from aligned paraphrases.
Finally, Section 4 characterizes the recurring errors
and concludes with a discussion of related and future
work.
2 Surface Realization with OpenCCG
OpenCCG is an open source Java library for pars-
ing and realization using Baldridge?s multimodal
extensions to CCG (Steedman, 2000; Baldridge,
2002). In the chart realization tradition (Kay, 1996),
the OpenCCG realizer takes logical forms as input
and produces strings by combining signs for lexical
items. Alternative realizations are scored using in-
tegrated n-gram and perceptron models (White and
Rajkumar, 2009), where the latter includes syntac-
tic features from Clark and Curran?s (2007) normal
form model as well as discriminative n-gram fea-
tures (Roark et al, 2004). Hypertagging (Espinosa
et al, 2008), or supertagging for surface realiza-
tion, makes it practical to work with broad coverage
grammars. For parsing, an implementation of Hock-
enmaier and Steedman?s (2002) generative model is
used to select the best parse. The grammar is auto-
matically extracted from a version of the CCGbank
(Hockenmaier and Steedman, 2007) with Propbank
(Palmer et al, 2005) roles projected onto it (Boxwell
and White, 2008).
A distinctive feature of OpenCCG is the ability
to generate from disjunctive logical forms (White,
2006a). This capability has many benefits, such
as enabling the selection of realizations according
to predicted synthesis quality (Nakatsu and White,
2006), and avoiding repetition in the output of a dia-
logue system (Foster and White, 2007). Disjunctive
inputs make it possible to exert fine-grained control
over the specified paraphrase space. In the chart re-
alization tradition, previous work has not generally
supported disjunctive logical forms, with Shemtov?s
(Shemtov, 1997) more complex approach as the only
published exception.
An example disjunctive input from the COMIC
system appears in Figure 1(c).2 Semantic de-
pendency graphs such as these?represented in-
ternally in Hybrid Logic Dependency Semantics
2To simplify the exposition, the features specifying informa-
tion structure and deictic gestures have been omitted, as have
the semantic sorts of the discourse referents.
be<TENSE>pres,<MOOD>dcl e 
<ARG> <PROP>
based_on <DET>the,<NUM>sgdesign d p 
<SOURCE> <ARTIFACT> collection<DET>the,<NUM>sg c 
<HASPROP> <CREATOR>Funny_Day f v Villeroy_and_Boch
 
(a) Semantic dependency graph for The design (is|?s)
based on the Funny Day collection by Villeroy and
Boch.
be<TENSE>pres,<MOOD>dcl e 
<ARG> <PROP>
based_on <DET>the,<NUM>sgdesign d p 
<SOURCE> <ARTIFACT> series<NUM>sg c 
<HASPROP> <GENOWNER >Funny_Day f v Villeroy_and_Boch
 
(b) Semantic dependency graph for The design (is|?s)
based on Villeroy and Boch?s Funny Day series.
be<TENSE>pres,<MOOD>dcl e 
<ARG> <PROP>
based_on <DET>the,<NUM>sgdesign d p 
<SOURCE> <ARTIFACT> collection|series (< DET>the) ? ,<NUM>sg c 
<HASPROP> <GENOWNER >Funny_Day f v Villeroy_and_Boch
<CREATOR>
 
(c) Disjunctive semantic dependency graph covering (a)-
(b), i.e. The design (is|?s) based on (the Funny Day
(collection|series) by Villeroy and Boch | Villeroy and
Boch?s Funny Day (collection|series)).
Figure 1: Example semantic dependency graphs
from the COMIC dialogue system.
@e(be ? ?TENSE?pres ? ?MOOD?dcl ?
?ARG?(d ? design ? ?DET?the ? ?NUM?sg) ?
?PROP?(p ? based on ?
?ARTIFACT?d ?
?SOURCE?(c ? collection ? ?DET?the ? ?NUM?sg ?
?HASPROP?(f ? Funny Day) ?
?CREATOR?(v ? V&B))))
(a)
...
@e(be ? ?TENSE?pres ? ?MOOD?dcl ?
?ARG?(d ? design ? ?DET?the ? ?NUM?sg) ?
?PROP?(p ? based on ?
?ARTIFACT?d ?
?SOURCE?(c ? ?NUM?sg ? (?DET?the)? ?
(collection ? series) ?
?HASPROP?(f ? Funny Day) ?
(?CREATOR?v ? ?GENOWNER?v ))))
? @v(Villeroy and Boch)
(c)
Figure 2: HLDS for examples in Figure 1.
2 Disjunctive Logical Forms
As an illustration of disjunctive logical forms,
consider the semantic dependency graphs in Fig-
ure 1, which are taken from the COMIC1 mul-
timodal dialogue system.2 Graphs such as these
constitute the input to the OpenCCG realizer.
Each node has a lexical predication (e.g. design)
and a set of semantic features (e.g. ?NUM?sg);
nodes are connected via dependency relations (e.g.
?ARTIFACT?).
Given the lexical categories in the COMIC
grammar, the graphs in Figure 1(a) and (b) fully
specify their respective realizations, with the ex-
ception of the choice of the full or contracted
form of the copula. To generalize over these al-
ternatives, the disjunctive graph in (c) may be
employed. This graph allows a free choice be-
tween the domain synonyms collection and se-
ries, as indicated by the vertical bar between
their respective predications. The graph also al-
lows a free choice between the ?CREATOR? and
?GENOWNER? relations?lexicalized via by and
the possessive, respectively?connecting the head
c (collection or series) with the dependent v (for
1http://www.hcrc.ed.ac.uk/comic/
2To simplify the exposition, the features specifying infor-
mation structure and deictic gestures have been omitted, as
have the semantic sorts of the discourse referents.
be<TENSE>pres,<MOOD>dcl e 
<ARG> <PROP>
based_on <DET>the,<NUM>sgdesign d p 
<SOURCE> <ARTIFACT> collection<DET>the,<NUM>sg c 
<HASPROP> <CREATOR>Funny_Day f v Villeroy_and_Boch
 
(a) Semantic dependency graph for The design (is|?s)
based on the Funny Day collection by Villeroy and
Boch.
be<TENSE>pres,<MOOD>dcl e 
<ARG> <PROP>
based_on <DET>the,<NUM>sgdesign d p 
<SOURCE> <ARTIFACT> series<NUM>sg c 
<HASPROP> <GENOWNER >Funny_Day f v Villeroy_and_Boch
 
(b) Semantic dependency graph for The design (is|?s)
based on Villeroy and Boch?s Funny Day series.
be<TENSE>pres,<MOOD>dcl e 
<ARG> <PROP>
based_on <DET>the,<NUM>sgdesign d p 
<SOURCE> <ARTIFACT> collection|series (< DET>the) ? ,<NUM>sg c 
<HASPROP> <GENOWNER >Funny_Day f v Villeroy_and_Boch
<CREATOR>
 
(c) Disjunctive semantic dependency graph covering (a)-
(b), i.e. The design (is|?s) based on (the Funny Day
(collection|series) by Villeroy and Boch | Villeroy and
Boch?s Funny Day (collection|series)).
Figure 1: Example semantic dependency graphs
from the COMIC dialogue system.
@e(be ? ?TENSE?pres ? ?MOOD?dcl ?
?ARG?(d ? design ? ?DET?the ? ?NUM?sg) ?
?PROP?(p ? based on ?
?ARTIFACT?d ?
?SOURCE?(c ? collection ? ?DET?the ? ?NUM?sg ?
?HASPROP?(f ? Funny Day) ?
?CREATOR?(v ? V&B))))
(a)
...
@e(be ? ?TENSE?pres ? ?MOOD?dcl ?
?ARG?(d ? design ? ?DET?the ? ?NUM?sg) ?
?PROP?(p ? based on ?
?ARTIFACT?d ?
?SOURCE?(c ? ?NUM?sg ? (?DET?the)? ?
(collection ? series) ?
?HASPROP?(f ? Funny Day) ?
(?CREATOR?v ? ?GENOWNER?v ))))
? @v(Villeroy and Boch)
(c)
Figure 2: HLDS for examples in Figure 1.
2 Disjunctive Logical Forms
As an illustration f disjunctive logical forms,
consider the semantic dependency graphs in Fig-
ure 1, which are taken from the COMIC1 mul-
timodal dialogue system.2 Graphs such as these
constitute the input to the OpenCCG realizer.
Each node has a lexical predication (e.g. design)
and a set of semantic features (e.g. ?NUM?sg);
nodes are connected via dependency relations (e.g.
?ARTIFACT?).
Given the lexical categories in the COMIC
grammar, the graphs in Figure 1(a) and (b) fully
specify their respective realizations, with the ex-
ception of the choice of the full or contracted
form of the copula. To generalize over these al-
ternatives, the disjunctive graph in (c) may be
employed. This graph allows a free choice be-
tween the domain synonyms collection and se-
ries, as indicated by the vertical bar between
their respective predications. The graph also al-
lows a free choice between the ?CREATOR? and
?GENOWNER? relations?lexicalized via by and
the possessive, respectively?connecting the head
c (collection or series) with the dependent v (for
1http://www.hcrc.ed.ac.uk/comic/
2To simplify the exposition, the features specifying infor-
mation structure and deictic gestures have been omitted, as
have the semantic sorts of the discourse referents.
be<TENSE>pres,<MOOD>dcl e 
<ARG> <PROP>
based_on <DET>the,<NUM>sgdesign d p 
<SOURCE> <ARTIFACT> collection<DET>the,<NUM>sg c 
<HASPROP> <CREATORFunny_Day f v Villeroy_and_Boch
 
(a) Semantic dependency graph for The design (is|?s)
based on the Funny Day collection by Villeroy and
Boch.
be<TENSE>pres,<MOOD>dcl e 
<ARG> <PROP>
based_on <DET>the,<NUM>sgdesign d p 
<SOURCE> <ARTIFACT> series<NUM>sg c 
<HASPROP> <GE OWNER >Funny_Day f v Villeroy_and_Boch
 
(b) Semantic dependency graph for The design (is|?s)
based on Villeroy and Boch?s Funny Day series.
be<TENSE>pres,<MOOD>dcl e 
<ARG> <PROP>
based_on <DET>the,<NUM>sgdesign d p 
<SOURCE> <ARTIFACT> collection|series (< DET>the) ? ,<NUM>sg c 
<HASPROP> <GENOWNER >Funny_Day f v Villeroy_and_Boch
<CREATOR>
 
(c) Disjunctive semantic dependency graph covering (a)-
(b), i.e. The design (is|?s) based on (the Funny Day
(collection|series) by Vill roy and Boch | Villeroy and
Boch?s Funny Day (collection| ries)).
Figure 1: Example semantic dependency graphs
from the COMIC dialogue system.
@e(be ? ?TENSE?pres ? ?MOOD?dcl ?
?ARG?(d ? design ? ?DET?the ? ?NUM?sg) ?
?PROP?(p ? based on ?
?ARTIFACT?d ?
?SOURCE?(c ? collection ? ?DET?the ? ?NUM?sg ?
?HASPROP?(f ? Funny Day) ?
?CREATOR?(v ? V&B))))
(a)
...
@e(be ? ?TENSE?pres ? ?MOOD?dcl ?
?ARG?(d ? design ? ?DET?the ? ?NUM?sg) ?
?PROP?(p ? based on ?
?ARTIFACT?d ?
?SOURCE?(c ? ?NUM?sg ? (?DET?the)? ?
(collection ? series) ?
?HASPROP?(f ? Funny Day) ?
(?CREATOR?v ? ?GENOWNER?v ))))
? @v(Villeroy and Boch)
(c)
Figure 2: HLDS for examples in Figure 1.
2 Disjunctive Logical Forms
As an illustration of disjunctive logical forms,
consider the semantic dependency graphs in Fig-
ure 1, which are taken from the COMIC1 mul-
timodal dialogue system.2 Graphs such as these
titute the input to the OpenCCG realizer.
Each node has a lexical predication (e.g. design)
and a set of semantic features (e.g. ?NUM?sg);
n des ar connected via dep ndency relations (e.g.
?ARTIFACT?).
Given the l xical at gori in the COMIC
grammar, the graphs n Figure 1(a) and (b) fully
specify their respective realizations, with the ex-
ception of the choice of the full or contracted
for of the copula. To generalize over these al-
ternatives, the disjunctive graph in (c) may be
empl yed. This graph allows a free choice be-
tween the domain synonyms collection and se-
ries, as indicated by the vertical bar between
their respective predications. The graph also al-
lows a free choice between the ?CREATOR? and
?GENOWNER? relations?lexicalized via by and
the possessive, respectively?connecting the head
c (collection or series) with the dependent v (for
1http://www.hcrc.ed.ac.uk/comic/
2To simplify the exposition, the features specifying infor-
mati n structure and deictic gestures hav been omitted, as
have the semantic sorts of the discourse referents.
Figure 1: Two imi ar logical forms from the COMIC
system as semantic ependency graphs, together with a
disjunctive logical form representing their combination
as a packed semantic dependency graph.
76
(Baldridge and Kruijff, 2002; White, 2006b), or
HLDS?constitute the input to the OpenCCG re-
alizer.3 This graph allows a free choice between
the domain synonyms collection and series, as in-
dicated by the vertical bar between their respec-
tive predications. The graph also allows a free
choice between the ?CREATOR? and ?GENOWNER?
relations?lexicalized via by and the possessive,
respectively?connecting the head c (collection or
series) with the dependent v (for Villeroy and Boch);
this choice is indicated by an arc between the two
dependency relations. Finally, the determiner fea-
ture (?DET?the) on c is indicated as optional, via the
question mark. Note that as an alternative, the deter-
miner feature could have been included in the dis-
junction with the ?CREATOR? relation (though this
would have been harder to show graphically); how-
ever, it is not necessary to do so, as constraints in the
lexicalized grammar will ensure that the determiner
is not generated together with the possessive.
3 Constructing DLFs from Aligned
Paraphrases
To develop our approach, we use the gold-standard
alignments in Cohn et al?s (2008) paraphrase cor-
pus. This corpus is constructed from three monolin-
gual sentence-aligned paraphrase subcorpora from
differing text genres, with word-level alignments
provided by two human annotators. We parse each
corpus sentence pair using the OpenCCG parser to
yield a logical form (LF) as a semantic dependency
graph with the gold-standard alignments projected
onto the LF pair. Disjunctive LFs are then con-
structed by inspecting the graph structure of each LF
in comparison with the other. Here, an alignment is
represented simply as a pair ?n1, n2? where n1 is a
node in the first LF and n2 a node in the second LF.
As Cohn et al?s corpus contains some block align-
ments, there are cases where a single node is aligned
3To be precise, the HLDS logical forms are descriptions of
semantic dependency graphs, which in turn can be interpreted
model theoretically via translation to Discourse Representation
Theory (Kamp and Reyle, 1993), as White (2006b) explains. A
disjunctive logical form is thus a description of a set of seman-
tic dependency graphs. (As the LFs derived using CCGbank
grammars do not represent quantifier scope properly, it would
be more accurate to call them quasi-LFs; as this issue does not
appear to impact the realization or DLF creation algorithms,
however, we have employed the simpler term.)
to multiple nodes in the other sentence of the para-
phrase.
A semantic dependency is represented as graph
G = ?N,E?, where N = nodes(G) is the set of
nodes in G and E = edges(G) is the set of edges
in G. An edge e is a labeled dependency between
nodes, with source(e) denoting the source node,
target(e) the target node, and label(e) the relation
e represents. For n, n? ? nodes(G) members of the
set of nodes for some graph G, n? ? parents(n) if
and only if there is an edge e ? edges(G) with n? =
source(e) and n = target(e). The set ancestors(n)
models the transitive closure of the ?parent-of? re-
lation: a ? ancestors(n) if and only if there is
some p ? parents(n) such that either a = p or
a ? ancestors(p). Nodes in a graph additionally
bear associated predicates and semantic features that
are derived during the parsing process.
3.1 The Algorithm
As a preprocessing step, we first characterize the dif-
ference between two LFs as a set of edit operations
via MAKEEDITS(g1, g2, alignments), as detailed
in Algorithm 1. An insert results when the second
graph contains an unaligned subgraph. Similarly, an
unaligned subgraph in the first LF is characterized
by a delete operation. For both inserts and deletes,
only the head of the inserted or deleted subgraph is
represented as an edit in order to reflect the fact that
these operations can encompass entire subgraphs. A
substitution occurs when a subgraph in the first LF
is aligned to one or more subgraphs in the second LF.
The case where subgraphs are block aligned corre-
sponds to a multi-word phrasal substitution (for ex-
ample, the substitution of Goldman for The US in-
vestment bank in paraphrase (2), below). The DLF
generation process is then driven by these edit oper-
ations.
DLFs are created for each sentence by DIS-
JUNCTIVIZE(g1, g2, alignments) and DISJUNC-
TIVIZE(g2, g1, alignments), respectively, where
g1 is the first sentence?s LF and g2 the LF of the
second (see Algorithm 2). The DLF construction
process takes as inputs a pair of dependency graphs
?g1, g2? and a set of word-level alignments from
Cohn et al?s (2008) paraphrase corpus projected
onto the graphs. This process creates a DLF by
merging or making optional material from the sec-
77
Algorithm 1 Preprocesses a pair of aligned LFs representing a paraphrase into edit operations.
1: procedure MAKEEDITS(g1, g2, alignments)
2: for all i ? {n ? nodes(g2) | ??x.?x, n? ? alignments} do . inserts
3: if ??p.p ? parents(i) ? ??x.?x, p? ? alignments then
4: insert(i)
5: for all d ? {n ? nodes(g1) | ??y.?n, y? ? alignments} do . deletes
6: if ??p.p ? parents(d) ? ??y.?p, y? ? alignments then
7: delete(d)
8: for all s ? nodes(g1) do . substitutions
9: if ?y.?s, y? ? alignments ? ??z.z ? parents(y) ? ?s, z? ? alignments then
10: substitution(s, y)
Algorithm 2 Constructs a disjunctive LF from an aligned paraphrase.
1: procedure DISJUNCTIVIZE(g1, g2, alignments)
2: MAKEEDITS(g1, g2, alignments)
3: for all i ? {n ? nodes(g2) | insert(n)} do
4: for all p ? {e ? edges(g2) | i = target(e)} do
5: for all ?n1, n2? ? {?x, y? ? alignments | y = source(p)} do
6: option(n1, p)
7: for all d ? {n ? nodes(g1) | delete(n)} do
8: for all p ? {e ? edges(g1) | d = target(e)} do
9: option(source(p), p)
10: for all s ? {n ? nodes(g1) | ?y.substitution(n, y)} do
11: for all p ? parents(s) do
12: choice(p, {e ? edges(g2) | substitution(s, target(e)) ? ?p, source(e)? ? alignments})
78
ond LF into the first LF.
As Algorithm 2 describes, first the inserts (line 3)
and deletes (line 7) are handled. In the case of in-
serts, for each node i in the second LF that is the
head of an inserted subgraph, we find every n2 that
is the source of an edge p whose target is i. The
edge p is added as an option for each node n1 in the
first LF that is aligned to n2. The process for deletes
is similar, modulo direction reversal. We find every
edge p whose target is d, where d is the head of an
unaligned subgraph in the first sentence, and make p
an option for the parent node source(p). With both
inserts and deletes, the intuitive idea is that an un-
aligned subgraph should be treated as an optional
dependency from its parent.
The following corpus sentence pair demonstrates
the handling of inserts/deletes:
(1) a. Justices said that the constitution allows
the government to administer drugs only
in limited circumstances.
b. In a 6-3 ruling, the justices said such
anti-psychotic drugs can be used only in
limited circumstances.
In the DLF constructed for (1a), the node represent-
ing the word drugs has two alternate children that
are not present in the first sentence itself (i.e., are in-
serted), such and anti-psychotic, both of which are in
the modifier relation to drugs. This happens because
drugs is aligned to the word drugs in (1b), which has
the modifier child nodes. The second sentence also
contains the insertion In a 6-3 ruling. This entire
subgraph is represented as an optional modifier of
said. Finally, the determiner the is inserted before
justices in the second sentence. This determiner is
also represented as an optional edge from justices.
Figure 2 shows the portion of the DLF reflecting the
optional modifier In a 6-3 ruling and optional deter-
miner the.
For substitutions (line 10), we consider each
subgraph-heading node s in the first LF that is sub-
stituted for some node y in the second LF that is
also a subgraph head. Then for each parent p of s,
the choices for p are contained in the set of edges
whose source is aligned to p and whose target is a
substitution for s. The intuition is that for each node
p in the first LF with an aligned subgraph c, there is a
disjunction between c and the child subgraphs of the
e say?TENSE?past
iin
rruling
aa x 6-3
j justices
t the
?MOD?
?ARG1?
?DET?
?MOD?
?ARG0?
?DET?
Figure 2: Disjunctive LF subgraph for the alternation (In
a 6-3 ruling)? (the)? justices said . . . in paraphrase (1).
The dotted lines represent optional edges, and some se-
mantic features are suppressed for readability.
node that p is aligned to in the second LF. For effi-
ciency, in the special case of substitutions involving
single nodes rather than entire subgraphs, only the
semantic predicates are disjoined.4
To demonstrate, consider the following corpus
sentence pair involving a phrasal substitution:
(2) a. The US investment bank said: we be-
lieve the long-term prospects for the en-
ergy sector in the UK remain attractive.
b. We believe the long-term prospects for
the energy sector in the UK remain at-
tractive, Goldman said.
In this paraphrase, the subtree The US investment
bank in (2a) is aligned to the single word Gold-
man in (2b), but their predicates are obviously differ-
ent. The constructed DLF contains a choice between
Goldman and The US investment bank as the subject
of said. Figure 3 illustrates the relevant subgraph of
the DLF constructed from the Goldman paraphrase
with a choice between subjects (?ARG0?). This dis-
junction arises because said in the first sentence is
aligned to said in the second, and The US investment
bank is the subject of said in the first while Gold-
man is its subject in the second. Note that, since
the substitution is a phrasal (block-aligned) one, the
constructed DLF forces a choice between Goldman
and the entire subgraph headed by bank, not between
4We leave certain more complex cases, e.g. multiple nodes
with aligned children, for future work.
79
e say?TENSE?past
ggoldman b bank
tthe
u us
i inv.
?ARG0? ?ARG0?
?DET?
?MOD?
?MOD?
Figure 3: Disjunctive LF subgraph for the alternation
(Goldman | The US investment bank) said . . . in para-
phrase (2). The arc represents the two choice edges for
the ?ARG0? relation from say. Certain semantic dependen-
cies are omitted, and the word investment is abbreviated
to save space.
Goldman and each of bank?s dependents (the, US,
and investment).
4 Discussion and Future Work
With a broad coverage grammar, we have found that
most of the realization alternatives in an n-best list
tend to reuse the same lexical choices, with the dif-
ferences mostly consisting of alternate word orders
or use of function words or punctuation. Accord-
ingly, in order to enhance the variety of word and
phrase choices in the n-best lists, we have taken
advantage of the API-level support for plugging in
custom pruning strategies and developed a custom
strategy that encourages lexical diversity. This strat-
egy groups realizations that share the same open
class stems into equivalence classes, where new
equivalence classes are favored over new alterna-
tives within the same equivalence class in filling up
the n-best list.
Using this lexical diversity pruning strategy, an
example of the paraphrases generated after DLF cre-
ation appears in Table 2. In the example, the girl
and brianna are successfully alternated, as are her
mother?s and the (bedroom). The example also in-
cludes a reasonable Heavy-NP shift, with into the
bedroom appearing before the NP list. Without the
lexical diversity pruning strategy, the phrase her
mother?s does not find its way into the n-best list.
The paraphrases also include a mistaken change in
tense from had to has and a mysterious inclusion of
including. Interestingly though, these mistakes fol-
low in the n-best list alternatives that are otherwise
the same, suggesting that a final pruning of the list
may make it possible to keep only generally good
paraphrases. (Note that the appositive 33 in the sec-
ond reference sentence also has been dropped, most
likely since the pruning strategy does not include
numbers in the set of content words at present.)
Although we have not yet formally evaluated the
paraphrases, we can already characterize some re-
curring errors. Named entities are an issue since
we have not incorporated a named entity recognizer;
thus, the realizer is apt to generate O. Charles Prince
instead of Charles O. Prince, for example. Worse,
medical examiner ?s spokeswoman ellen borakove
is realized both correctly and as medical examiner
?s ellen spokeswoman borakove. Naturally, there
are also paraphrasing errors that stem from parser
errors. Certainly with named entities, though per-
haps also with parser errors, we plan to investigate
whether we can take advantage of the word order in
the reference sentence in order to reduce the number
of mistakes. Here, we plan to investigate whether
a feature measuring similarity in word order to the
original can be balanced against the averaged per-
ceptron model score in a way that allows new para-
phrases to be generated while sticking to the orig-
inal order in cases of uncertainty. Initial experi-
ments with adding to the perceptron model score
an n-gram precision score (approximating BLEU)
with an appropriate weight indicate that realizations
including the correct word order in names such as
Charles O. Prince can be pushed to the top of the
n-best list, though it remains to be verified that the
weight for the similarity score can be adequately
tuned with held-out data. Incorporating a measure of
similarity to the original reference sentences into re-
alization ranking is a form of what Madnani (2010)
calls a self-paraphrase bias, though a different one
than his method of adjusting the probability mass as-
signed to the original.
In future work, we plan to evaluate the gener-
ated paraphrases both intrinsically and extrinsically
in combination with MT evaluation metrics. With
the intrinsic evaluation, we expect to examine the
impact of parser and alignment errors on the para-
phrases, and the extent to which these can be miti-
gated by a self-paraphrase bias, along with the im-
pact of the lexical diversity pruning strategy on the
80
Reference 1 lee said brianna had dragged food , toys and other things into the bedroom .
Realizations lee said the girl had dragged food , toys and other things into the bedroom .
lee said brianna had dragged food , toys and other things into the bedroom .
lee said , the girl had dragged [into the bedroom] food , toys and other things .
lee said the girl has dragged into the bedroom food , toys and other things .
lee said , brianna had dragged into the bedroom food , toys and other things .
lee said the girl had dragged food , toys and other things into her mother ?s bedroom .
lee said , the girl had dragged into her mother ?s bedroom food , toys and other things .
lee said brianna had dragged food , toys and other things into her mother ?s bedroom .
lee said the girl had dragged food , toys and other things into including the bedroom .
lee said , brianna had dragged into her mother ?s bedroom food , toys and other things .
Reference 2 lee , 33 , said the girl had dragged the food , toys and other things into her mother ?s bedroom .
Realizations lee said the girl had dragged [into the bedroom] the food , toys and other things .
lee said , the girl had dragged into the bedroom the food , toys and other things .
lee said the girl has dragged into the bedroom the food , toys and other things .
lee said brianna had dragged the food , toys and other things into the bedroom .
lee said , brianna had dragged into the bedroom the food , toys and other things .
lee said the girl had dragged the food , toys and other things into her mother ?s bedroom .
lee said brianna had dragged into her mother ?s bedroom the food , toys and other things .
lee said , the girl had dragged into her mother ?s bedroom the food , toys and other things .
lee said brianna had dragged the food , toys and other things into her mother ?s bedroom .
lee said the girl had dragged the food , toys and other things into including the bedroom .
Table 2: Example n-best realizations starting from each reference sentence. Alternative phrasings from the other
member of the pair are shown in italics the first time, and alternative phrase locations are shown in square brackets.
Mistakes are underlined, and suppressed after the first occurrence in the list.
number of acceptable paraphrases in the n-best list.
With the extrinsic evaluation, we plan to investi-
gate whether n-best paraphrase generation using the
methods described here can be used to augment a
set of reference translations in such a way as to in-
crease the correlation of automatic metrics with hu-
man judgments. As Madnani observes, generated
paraphrases of reference translations may be either
untargeted or targeted to specific MT hypotheses.
In the case of targeted paraphrases, the generated
paraphrases then approximate the process by which
automatic translations are evaluated using HTER
(Snover et al, 2006), with a human in the loop, as
the closest acceptable paraphrase of a reference sen-
tence should correspond to the version of the MT
hypothesis with minimal changes to make it accept-
able. While in principle we might similarly acquire
paraphrase rules using the pivot method, as in Mad-
nani?s approach, such rules would be quite noisy, as
it is a difficult problem to characterize the contexts
in which words or phrases can be acceptably substi-
tuted. Thus, our immediate focus will be on gen-
erating synthetic references with high precision, re-
lying on grammatical alternations plus contextually
acceptable alternatives present in multiple reference
translations, given that metrics such as METEOR
(Banerjee and Lavie, 2005) and TERp (Snover et al,
2010) can now employ paraphrase matching as part
of their scoring, complementing what can be done
with our methods. To the extent that we can main-
tain high precision in generating synthetic reference
sentences, we may expect the correlations between
automatic metric scores and human judgments to
improve as the task of the metrics becomes simpler.
Acknowledgements
This work was supported in part by NSF grant num-
ber IIS-0812297. We are also grateful to Trevor
Cohn for help with the paraphrase data.
References
Jason Baldridge and Geert-Jan Kruijff. 2002. Coupling
CCG and Hybrid Logic Dependency Semantics. In
Proc. ACL-02.
Jason Baldridge. 2002. Lexically Specified Derivational
81
Control in Combinatory Categorial Grammar. Ph.D.
thesis, School of Informatics, University of Edinburgh.
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An Automatic Metric for MT Evaluation with Im-
proved Correlation with Human Judgments. In Proc.
of the ACL-05 Workshop on Intrinsic and Extrinsic
Evaluation Measures for Machine Translation and/or
Summarization.
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Proc.
ACL-05, pages 597?604.
Regina Barzilay and Lillian Lee. 2003. Learning to
paraphrase: An unsupervised approach using multiple-
sequence alignment. In Proc. of NAACL-HLT.
Regina Barzilay and Kathleen McKeown. 2005. Sen-
tence fusion for multidocument news summarization.
Computational Linguistics, 31(3):297?327.
Stephen Boxwell and Michael White. 2008. Projecting
Propbank roles onto the CCGbank. In Proc. LREC-08.
Stephen Clark and James R. Curran. 2007. Wide-
Coverage Efficient Statistical Parsing with CCG and
Log-Linear Models. Computational Linguistics,
33(4):493?552.
Trevor Cohn, Chris Callison-Burch, and Mirella Lapata.
2008. Constructing corpora for the development and
evaluation of paraphrase systems. Computational Lin-
guistics, 34(4):597?614.
M. Elhadad, J. Robin, and K. McKeown. 1997. Floating
constraints in lexical choice. Computational Linguis-
tics, 23(2):195?239.
Dominic Espinosa, Michael White, and Dennis Mehay.
2008. Hypertagging: Supertagging for surface real-
ization with CCG. In Proceedings of ACL-08: HLT,
pages 183?191, Columbus, Ohio, June. Association
for Computational Linguistics.
Mary Ellen Foster and Michael White. 2007. Avoiding
repetition in generated text. In Proceedings of the 11th
European Workshop on Natural Language Generation
(ENLG 2007).
Claire Gardent and Eric Kow. 2005. Generating and se-
lecting grammatical paraphrases. In Proc. ENLG-05.
Aria Haghighi, John Blitzer, John DeNero, and Dan
Klein. 2009. Better word alignments with supervised
ITG models. In Proceedings of ACL, pages 923?931,
Suntec, Singapore, August. Association for Computa-
tional Linguistics.
Julia Hockenmaier and Mark Steedman. 2002. Gener-
ative models for statistical parsing with Combinatory
Categorial Grammar. In Proc. ACL-02.
Julia Hockenmaier and Mark Steedman. 2007. CCG-
bank: A Corpus of CCG Derivations and Dependency
Structures Extracted from the Penn Treebank. Com-
putational Linguistics, 33(3):355?396.
Deirdre Hogan, Jennifer Foster, Joachim Wagner, and
Josef van Genabith. 2008. Parser-based retraining
for domain adaptation of probabilistic generators. In
Proc. INLG-08.
Lidija Iordanskaja, Richard Kittredge, and Alain
Polgu?ere. 1991. Lexical selection and paraphrase
in a meaning-text generation model. In Ce?cile L.
Paris, William R. Swartout, and William C. Mann, edi-
tors, Natural Language Generation in Artificial Intelli-
gence and Computational Linguistics, pages 293?312.
Kluwer.
Hans Kamp and Uwe Reyle. 1993. From Discourse to
Logic. Kluwer.
David Kauchak and Regina Barzilay. 2006. Paraphras-
ing for automatic evaluation. In Proceedings of HLT-
NAACL.
Martin Kay. 1996. Chart generation. In Proc. ACL-96.
Irene Langkilde and Kevin Knight. 1998. The practical
value of n-grams in generation. In Proc. INLG-98.
Irene Langkilde-Geary. 2002. An empirical verification
of coverage and correctness for a general-purpose sen-
tence generator. In Proc. INLG-02.
Bill MacCartney, Michel Galley, and Christopher D.
Manning. 2008. A phrase-based alignment model for
natural language inference. In Proceedings of the 2008
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 802?811, Honolulu, Hawaii,
October. Association for Computational Linguistics.
Nitin Madnani. 2010. The Circle of Meaning: From
Translation to Paraphrasing and Back. Ph.D. thesis,
Department of Computer Science, University of Mary-
land College Park.
Crystal Nakatsu and Michael White. 2006. Learning to
say it well: Reranking realizations by predicted syn-
thesis quality. In Proc. COLING-ACL-06.
Franz Josef Och and Hermann Ney. 2003. A system-
atic comparison of various statistical alignment mod-
els. Computational Linguistics, 1(29):19?52.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated corpus of
semantic roles. Computational Linguistics, 31(1):71?
106.
Bo Pang, Kevin Knight, and Daniel Marcu. 2003.
Syntax-based alignment of multiple translations: Ex-
tracting paraphrases and generating new sentences. In
Proc. HLT/NAACL.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic eval-
uation of machine translation. In Proc. ACL-02.
Brian Roark, Murat Saraclar, Michael Collins, and Mark
Johnson. 2004. Discriminative language modeling
with conditional random fields and the perceptron al-
gorithm. In Proc. ACL-04.
82
Hadar Shemtov. 1997. Ambiguity Management in Natu-
ral Language Generation. Ph.D. thesis, Stanford Uni-
versity.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proc. of the Association for Machine Translation in
the Americas (AMTA-06).
Matthew Snover, Nitin Madnani, Bonnie Dorr, and
Richard Schwartz. 2010. TER-plus: Paraphrase,
semantic, and alignment enhancements to translation
edit rate. Machine Translation, 23:117?127.
M. Stede. 1999. Lexical Semantics and Knowledge Rep-
resentation in Multilingual Text Generation. Kluwer
Academic Publishers.
Mark Steedman. 2000. The Syntactic Process. MIT
Press.
Erik Velldal, Stephan Oepen, and Dan Flickinger. 2004.
Paraphrasing treebanks for stochastic realization rank-
ing. In Proceedings of the 3rd Workshop on Treebanks
and Linguistic Theories.
Michael White and Rajakrishnan Rajkumar. 2009. Per-
ceptron reranking for CCG realization. In Proceedings
of the 2009 Conference on Empirical Methods in Nat-
ural Language Processing, pages 410?419, Singapore,
August. Association for Computational Linguistics.
Michael White. 2004. Reining in CCG Chart Realiza-
tion. In Proc. INLG-04.
Michael White. 2006a. CCG chart realization from dis-
junctive logical forms. In Proc. INLG-06.
Michael White. 2006b. Efficient Realization of Coordi-
nate Structures in Combinatory Categorial Grammar.
Research on Language & Computation, 4(1):39?75.
Shiqi Zhao, Xiang Lan, Ting Liu, and Sheng Li. 2009.
Application-driven statistical paraphrase generation.
In Proceedings of the Joint Conference of the 47th
Annual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing of
the AFNLP, pages 834?842, Suntec, Singapore, Au-
gust. Association for Computational Linguistics.
83
Proceedings of the UCNLG+Eval: Language Generation and Evaluation Workshop, pages 39?44,
Edinburgh, Scotland, UK, July 31, 2011. c?2011 Association for Computational Linguistics
Linguistically Motivated Complementizer Choice in Surface Realization
Rajakrishnan Rajkumar and Michael White
Department of Linguistics
The Ohio State University
Columbus, OH, USA
{raja,mwhite}@ling.osu.edu
Abstract
This paper shows that using linguis-
tically motivated features for English
that-complementizer choice in an averaged
perceptron model for classification can
improve upon the prediction accuracy of a
state-of-the-art realization ranking model.
We report results on a binary classification
task for predicting the presence/absence of a
that-complementizer using features adapted
from Jaeger?s (2010) investigation of the
uniform information density principle in the
context of that-mentioning. Our experiments
confirm the efficacy of the features based
on Jaeger?s work, including information
density?based features. The experiments also
show that the improvements in prediction
accuracy apply to cases in which the presence
of a that-complementizer arguably makes a
substantial difference to fluency or intelli-
giblity. Our ultimate goal is to improve the
performance of a ranking model for surface
realization, and to this end we conclude with
a discussion of how we plan to combine the
local complementizer-choice features with
those in the global ranking model.
1 Introduction
Johnson (2009) observes that in developing statis-
tical parsing models, ?shotgun? features ? that is,
myriad scattershot features that pay attention to su-
perficial aspects of structure ? tend to be remark-
ably useful, while features based on linguistic the-
ory seem to be of more questionable utility, with
the most basic linguistic insights tending to have the
greatest impact.1 Johnson also notes that feature
design is perhaps the most important but least un-
derstood aspect of statistical parsing, and thus the
disappointing impact of linguistic theory on pars-
ing models is of real consequence. In this paper,
by contrast, we show that in the context of sur-
face realization, using linguistically motivated fea-
tures for English that-complementizer choice can
improve upon the prediction accuracy of a state-of-
the-art realization ranking model, arguably in ways
that make a substantial difference to fluency and in-
telligiblity.2 In particular, we report results on a bi-
nary classification task for predicting the presence
or absence of a that-complementizer using features
adapted from Jaeger?s (2010) investigation of the
uniform information density principle in the con-
text of that-mentioning. This information-theoretic
principle predicts that language production is af-
fected by a preference to distribute information uni-
formly across the linguistic signal. In Jaeger?s study,
uniform information density emerges as an impor-
tant predictor of speakers? syntactic reduction pref-
erences even when taking a sizeable variety of con-
trols based on competing hypotheses into account.
Our experiments confirm the efficacy of the fea-
tures based on Jaeger?s work, including information
density?based features.
1The term ?shotgun? feature appears in the slides for
Johnson?s talk (http://www.cog.brown.edu/?mj/
papers/johnson-eacl09-workshop.pdf), rather
than in the paper itself.
2For German surface realization, Cahill and Riester (2009)
show that incorporating information status features based on
the linguistics literature improves performance on realization
ranking.
39
That-complementizers are optional words that in-
troduce sentential complements in English. In the
Penn Treebank, they are left out roughly two-thirds
of the time, thereby enhancing conciseness. This
follows the low complementizer rates reported in
previous work (Tagliamonte and Smith, 2005; Ca-
coullos and Walker, 2009). While some surface re-
alizers, such as FUF/SURGE (Elhadad, 1991), have
made use of input features to control the choice of
whether to include a that-complementizer, for many
applications the decision seems best left to the real-
izer, since multiple surface syntactic factors appear
to govern the choice, rather than semantic ones. In
our experiments, we use the OpenCCG3 surface re-
alizer with logical form inputs underspecified for the
presence of that in complement clauses. While in
many cases, adding or removing that results in an
acceptable paraphrase, in the following example, the
absence of that in (2) introduces a local ambiguity,
which the original Penn Treebank sentence avoids
by including the complementizer.
(1) He said that for the second month in a
row, food processors reported a shortage
of nonfat dry milk. (WSJ0036.61)
(2) ? He said for the second month in a row,
food processors reported a shortage of
nonfat dry milk.
The starting point for this paper is White and Ra-
jkumar?s (2009) realization ranking model, a state-
of-the-art model employing shotgun features ga-
lore. An error analysis of this model, performed
by comparing CCGbank Section 00 realized deriva-
tions with their corresponding gold standard deriva-
tions, revealed that out of a total of 543 that-
complementizer cases, the realized output did not
match the gold standard choice 82 times (see Table 3
in Section 5 for details). Most of these mismatches
involved cases where a clause originally containing
a that-complementizer was realized in reduced form,
with no that. This under-prediction of that-inclusion
is not surprising, since the realization ranking model
makes use of baseline n-gram model features, and
n-gram models are known to have a built-in bias for
strings with fewer words.
3openccg.sf.net
We report here on experiments comparing this
global model to ones that employ local features
specifically designed for that-choice in complement
clauses. As a prelude to incorporating these fea-
tures into a model for realization ranking, we study
the efficacy of these features in isolation by means
of a binary classification task to predict the pres-
ence/absence of that in complement clauses. In
a global realization ranking setting, the impact of
these phenomenon-specific features might be less
evident, as they would interact with other features
for lexical selection and ordering choices that the
ranker makes. Note that a comprehensive ranking
model is desirable, since linear ordering and that-
complementizer choices may interact. For exam-
ple, Hawkins (2003) reports examples where explic-
itly marked phrases can occur either close to or far
from their heads as in (3) and (4), whereas zero-
marked phrases are only rarely attested at some dis-
tance from their heads and prefer adjacency, as (5)
and (6) show.
(3) I realized [that he had done it] with sad-
ness in my heart.
(4) I realized with sadness in my heart [that
he had done it].
(5) I realized [he had done it] with sadness in
my heart.
(6) ? I realized with sadness in my heart [he
had done it].
2 Background
CCG (Steedman, 2000) is a unification-based cat-
egorial grammar formalism defined almost en-
tirely in terms of lexical entries that encode sub-
categorization as well as syntactic features (e.g.
number and agreement). OpenCCG is a pars-
ing/generation library which includes a hybrid
symbolic-statistical chart realizer (White, 2006).
The chart realizer takes as input logical forms rep-
resented internally using Hybrid Logic Dependency
Semantics (HLDS), a dependency-based approach
to representing linguistic meaning (Baldridge and
Kruijff, 2002). To illustrate the input to OpenCCG,
consider the semantic dependency graph in Figure 1.
In the graph, each node has a lexical predication
(e.g. make.03) and a set of semantic features (e.g.
40
aa1
heh3
he h2
<Det>
<Arg0> <Arg1>
<TENSE>pres
<NUM>sg
<Arg0>
w1 want.01
m1
<Arg1>
<GenRel>
<Arg1>
<TENSE>pres
p1point
h1have.03
make.03
<Arg0>
s[b]\np/np
np/n
np
n
s[dcl]\np/np
s[dcl]\np/(s[to]\np)
np
Figure 1: Semantic dependency graph from the CCGbank
for He has a point he wants to make [. . . ], along with
gold-standard supertags (category labels)
?NUM?sg); nodes are connected via dependency re-
lations (e.g. ?ARG0?). In HLDS, each semantic head
(corresponding to a node in the graph) is associated
with a nominal that identifies its discourse referent,
and relations between heads and their dependents
are modeled as modal relations. We extract HLDS-
based quasi logical form graphs from the CCG-
bank and semantically empty function words such as
complementizers, infinitival-to, expletive subjects,
and case-marking prepositions are adjusted to reflect
their purely syntactic status. Alternative realizations
are ranked using an averaged perceptron model de-
scribed in the next section.
3 Feature Design
White and Rajkumar?s (2009) realization ranking
model serves as the baseline for this paper. It is
a global, averaged perceptron ranking model using
three kinds of features: (1) the log probability of the
candidate realization?s word sequence according to
three linearly interpolated language models (as well
as a feature for each component model), much as
in the log-linear models of Velldal & Oepen (2005)
and Nakanishi et al (2005); (2) integer-valued syn-
tactic features, representing counts of occurrences in
a derivation, from Clark & Curran?s (2007) normal
form model; and (3) discriminative n-gram features
(Roark et al, 2004), which count the occurrences of
each n-gram in the word sequence.
Table 1 shows the new complementizer-choice
features investigated in this paper. The example fea-
tures mentioned in the table are taken from the two
complement clause (CC) forms (with-that CC vs.
that-less CC) of the sentence below:
(7) The finding probably will support those
who argue [ that/? the U.S. should regu-
late the class of asbestos including croci-
dolite more stringently than the common
kind of asbestos, chrysotile, found in most
schools and other buildings], Dr. Talcott
said. (WSJ0003.19)
The first class of features, dependency length and
position of CC, have been adapted from the related
control features in Jaeger?s (2010) study. For the
above example, the position of the matrix verb with
respect to the start of the sentence (feature name
mvInd and having the value 7.0), the distance be-
tween the matrix verb and the onset of the CC (fea-
ture name mvCCDist with the value 1.0) and fi-
nally the length of the CC (feature ccLen with value
of 29.0 for the that-CC and 28.0 for the that-less
CC) are encoded as features. The second class of
features includes various properties of the matrix
verb viz. POS tag, form, stem and supertag (fea-
ture names mv Pos, mvStem, mvForm, mvSt, respec-
tively). These features were motivated by the fact
that Jaeger controls for the per-verb bias of this con-
struction, as attested in the earlier literature. The
third class of features are related to information den-
sity. Jaeger (2010) estimates information density at
the CC onset by using matrix verb subcategorization
frequency. In our case, more like the n-gram fea-
tures employed by Levy and Jaeger (2007), we used
log probabilities from two existing n-gram models,
viz. a trigram word model and trigram word model
with semantic class replacement. For each CC, two
features (one per language model) were extracted by
calculating the average of the log probs of individual
words from the beginning of the complement clause.
In the that-CC version of the example above, lo-
cal CC-features having the prefix $uidCCMean were
calculated by averaging the individual log probs of
the 3 words that the U.S. to get feature values of
-0.8353556 and -2.0460036 per language model (see
41
Feature Example for that-CCs Example for that-less CCs
Dependency length and position of CC
Position of matrix verb thatCC:mvInd 7.0 noThatCC:mvInd 7.0
Dist between matrix verb & CC thatCC:mvCCDist 1.0 noThatCC:mvCCDist 1.0
Length of CC thatCC:ccLen 29.0 noThatCC:ccLen 28.0
Matrix verb features
POS-tag thatCC:mvPos:VBP 1.0 noThatCC:mvPos:VBP 1.0
Stem thatCC:mvStem:argue 1.0 noThatCC:mvStem:argue 1.0
Form thatCC:mvForm:argue 1.0 noThatCC:mvForm:argue 1.0
CCG supertag thatCC:mvSt:s[dcl]\np/s[em] 1.0 noThatCC:mvSt:s[dcl]\np/s[dcl] 1.0
uniform information density (UID)
Average n-gram log probs thatCC:$uidCCMean1 -0.8353556 noThatCC:$uidCCMean1 -2.5177214
of first 2 words of that-less CCs thatCC:$uidCCMean2 -2.0460036 noThatCC:$uidCCMean2 -3.6464245
or first 3 words of that-CCs
Table 1: New features introduced (the prefix of each feature encodes the type of CC; subsequent parts supply the
feature name)
last part of Table 1). In the that-less CC version,
$uidCCMean features were calculated by averaging
the log probs of the first two words in the comple-
ment clause, i.e. the U.S.
4 Classification Experiment
To train a local classification model to predict the
presence of that in complement clauses, we used
an averaged perceptron ranking model with the
complementizer-specific features listed in Table 1
to rank alternate with-that vs. that-less CC choices.
For each CC classification instance in CCGbank
Sections 02?21, the derivation of the competing al-
ternate choice was created; i.e., in the case of a that-
CC, the corresponding that-less CC was created and
vice versa. Table 2 illustrates classification results
on Sections 00 (development) using models contain-
ing different feature sets & Section 23 (final test) for
the best-performing classification and ranking mod-
els. For both the development as well as test sec-
tions, the local classification model performed sig-
nificantly better than the global realization ranking
model according to McNemar?s ?2 test (p = 0.005,
two-tailed). Feature ablation tests on the develop-
ment data (Section 00) revealed that removing the
information density features resulted in a loss of ac-
curacy of around 1.8%.
5 Discussion
As noted in the introduction, in many cases, adding
or removing that to/from the corpus sentence results
in an acceptable paraphrase, while in other cases
the presence of that appears to make a substantial
Model Features % 00 % 23
Most Frequent Baseline 68.7 66.8
Global Realization Ranking 78.45 77.0
Local That-Classification
Only UID feats 74.77
Table 1 features except UID ones 81.4
Both feature sets above 83.24 83.02
Table 2: Classification accuracy results (Section 00 has
170/543 that-CCs; Section 23 has 192/579 that-CCs)
Construction %that % that / %Accuracy
Gold Classification Ranking
Gerundive (26) 53.8 61.5 / 92.3 26.9 / 57.7
Be-verb (21) 71.4 95.2 / 66.7 47.6 / 57.1
Non-adjacent CCs (53) 49.1 54.7 / 67.9 30.2 / 66.0
Total (543) 31.3 29.3 / 83.2 21.9 / 78.5
Table 3: Section 00 construction-wise that-CC propor-
tions and model accuracies (total CC counts given in
brackets alongside labels); gold standard obviously has
100% accuracy; models are local that-classification and
White and Rajkumar?s (2009) global realization ranking
model
difference to intelligibility or fluency. In order to
better understand the effect of the complementizer-
specific features, we examined three construction
types in the development data, viz. non-adjacent
complement clauses, gerundive matrix verbs and a
host of sub-cases involving a matrix be-verb (wh-
clefts, be+adjective etc.), where the presence of that
seemed to make the most difference. The results are
provided in Table 3. As is evident, the global realiza-
tion ranking model under-proposes the that-choice,
most likely due to the preference of n-gram mod-
els towards fewer words, while the local classifica-
42
WSJ0049.64 Observing [that/?? the judge has never exhibited any bias or prejudice], Mr. Murray concluded that he would be impartial
in any case involving a homosexual or prostitute as a victim.
WSJ0020.16 ? what this tells us is [that/?? U.S. trade law is working] ?, he said .
WSJ0010.5 The idea, of course: to prove to 125 corporate decision makers [that/?? the buckle on the Rust Belt is n?t so rusty after all ,
that it ?s a good place for a company to expand].
WSJ0044.118 Editorials in the Greenville newspaper allowed [that/?? Mrs. Yeargin was wrong], but also said the case showed how testing
was being overused.
WSJ0060.7 Viacom denies [?/?that it ?s using pressure tactics].
WSJ0018.4 The documents also said [that/?? although the 64-year-old Mr. Cray has been working on the project for more than six years ,
the Cray-3 machine is at least another year away from a fully operational prototype].
Table 4: Examples from model comparison
tion model is closer to the gold standard in terms of
that-choice proportions. For all the three construc-
tion types as well as overall, classifier performance
was better than ranker performance. The difference
in performance between the local classification and
global ranking models in the case of gerundive ma-
trix verbs is statistically significant according to the
McNemar?s ?2 test (Bonferroni corrected, two tailed
p = 0.001). The performance difference was not
significant with the other two constructions, how-
ever, using only the cases in Section 00.
Table 4 lists relevant examples where the classi-
fication model?s that-choice prediction matched the
gold standard while a competing model?s predic-
tion did not. Example WSJ0049.64 is one such
instance of classifier success involving a gerun-
dive matrix verb (in contrast to the realization
ranking model), Example WSJ0020.16 exemplifies
success with a wh-cleft construction and Exam-
ple WSJ0010.5 contains a non-adjacent CC. Apart
from these construction-based analyses, examples
like WSJ0044.118 indicate that the classification
model prefers the that-CC choice in cases that sub-
stantially improve intelligiblity, as here the overt
complementizer helps to avoid a local syntactic am-
biguity where the NP in allowed NP is unlikely to be
interpreted as the start of an S.
Finally, we also studied the effect of the uniform
information density features by comparing the full
classification model to a model without the UID
features. The full classification model exhibited a
trend towards significantly outperforming the ab-
lated model (McNemar?s p = 0.10, 2-tailed); more
test data would be needed to establish significance
conclusively. Examples are shown at the bottom of
Table 4. In WSJ0060.7, the full classification model
predicted a that-less clause (matching the gold stan-
dard), while the ablated classification model pre-
dicted a clause with that. In all such examples ex-
cept one, the information density features helped the
classification model avoid predicting that-inclusion
when not necessary. Example WSJ0018.4 is the
only instance where the best classification model
differed in predicting the that-choice.
6 Conclusions and Future Work
In this paper, we have shown that using linguistically
motivated features for English that-complementizer
choice in a local classifier can improve upon the
prediction accuracy of a state-of-the-art global re-
alization ranking model employing myriad shotgun
features, confirming the efficacy of features based
on Jaeger?s (2010) investigation of the uniform in-
formation density principle in the context of that-
mentioning. Since that-complementizer choice in-
teracts with other realization decisions, in future
work we plan to investigate incorporating these fea-
tures into the global realization ranking model. This
move will require binning the real-valued features,
as multiple complement clauses can appear in a sin-
gle sentence. Should feature-level integration prove
ineffective, we also plan to investigate alternative ar-
chitectures, such as using the local classifier outputs
as features in the global model.
Acknowledgements
This work was supported in part by NSF IIS-
0812297 and by an allocation of computing time
from the Ohio Supercomputer Center. Our thanks
also to Florian Jaeger, William Schuler, Peter Culi-
cover and the anonymous reviewers for helpful com-
ments and discussion.
43
References
Jason Baldridge and Geert-Jan Kruijff. 2002. Coupling
CCG and Hybrid Logic Dependency Semantics. In
Proc. ACL-02.
Rena Torres Cacoullos and James A. Walker. 2009. On
the persistence of grammar in discourse formulas: A
variationist study of ?that?. Linguistics, 47(1):1?43.
Stephen Clark and James R. Curran. 2007. Wide-
Coverage Efficient Statistical Parsing with CCG and
Log-Linear Models. Computational Linguistics,
33(4):493?552.
Michael Elhadad. 1991. FUF: The universal unifier user
manual version 5.0. Technical Report CUCS-038-91,
Dept. of Computer Science, Columbia University.
John A. Hawkins. 2003. Why are zero-marked phrases
close to their heads? In Gu?nter Rohdenburg and Britta
Mondorf, editors, Determinants of Grammatical Vari-
ation in English, Topics in English Linguistics 43. De
Gruyter Mouton, Berlin.
T. Florian Jaeger. 2010. Redundancy and reduction:
Speakers manage information density. Cognitive Psy-
chology, 61(1):23?62, August.
Mark Johnson. 2009. How the statistical revolution
changes (computational) linguistics. In Proceedings of
the EACL 2009 Workshop on the Interaction between
Linguistics and Computational Linguistics: Virtuous,
Vicious or Vacuous?, pages 3?11, Athens, Greece,
March. Association for Computational Linguistics.
Roger Levy and T. Florian Jaeger. 2007. Speakers opti-
mize information density through syntactic reduction.
Advances in Neural Information Processing Systems,
19:849.
Hiroko Nakanishi, Yusuke Miyao, and Jun?ichi Tsujii.
2005. Probabilistic methods for disambiguation of an
HPSG-based chart generator. In Proc. IWPT-05.
Brian Roark, Murat Saraclar, Michael Collins, and Mark
Johnson. 2004. Discriminative language modeling
with conditional random fields and the perceptron al-
gorithm. In Proc. ACL-04.
Mark Steedman. 2000. The syntactic process. MIT
Press, Cambridge, MA, USA.
S. Tagliamonte and J. Smith. 2005. No momentary
fancy! the zero ?complementizer? in English dialects.
English Language and Linguistics, 9(2):289?309.
Erik Velldal and Stephan Oepen. 2005. Maximum en-
tropy models for realization ranking. In Proc. MT
Summit X.
Michael White and Rajakrishnan Rajkumar. 2009. Per-
ceptron reranking for CCG realization. In Proceedings
of the 2009 Conference on Empirical Methods in Nat-
ural Language Processing, pages 410?419, Singapore,
August. Association for Computational Linguistics.
Michael White. 2006. Efficient Realization of Coordi-
nate Structures in Combinatory Categorial Grammar.
Research on Language and Computation, 4(1):39?75.
44
INLG 2012 Proceedings of the 7th International Natural Language Generation Conference, pages 136?140,
Utica, May 2012. c?2012 Association for Computational Linguistics
The Surface Realisation Task: Recent Developments and Future Plans
Anja Belz
Computing, Engineering and Maths
University of Brighton
Brighton BN1 4GJ, UK
a.s.belz@brighton.ac.uk
Bernd Bohnet
Institute for Natural Language Processing
University of Stuttgart
70174 Stuttgart
bohnet@ims.uni-stuttgart.de
Simon Mille, Leo Wanner
Information and Communication Technologies
Pompeu Fabra University
08018 Barcelona
<firstname>.<lastname>@upf.edu
Michael White
Department of Linguistics
Ohio State University
Columbus, OH, 43210, US
mwhite@ling.osu.edu
Abstract
The Surface Realisation Shared Task was first
run in 2011. Two common-ground input rep-
resentations were developed and for the first
time several independently developed surface
realisers produced realisations from the same
shared inputs. However, the input representa-
tions had several shortcomings which we have
been aiming to address in the time since. This
paper reports on our work to date on improv-
ing the input representations and on our plans
for the next edition of the SR Task. We also
briefly summarise other related developments
in NLG shared tasks and outline how the dif-
ferent ideas may be usefully brought together
in the future.
1 Introduction
The Surface Realisation (SR) Task was introduced
as a new shared task at Generation Challenges 2011
(Belz et al, 2011). Our aim in developing the SR
Task was to make it possible, for the first time, to
directly compare different, independently developed
surface realisers by developing a ?common-ground?
representation that could be used by all participat-
ing systems as input. In fact, we created two dif-
ferent input representations, one shallow, one deep,
in order to enable more teams to participate. Corre-
spondingly, there were two tracks in SR?11: In the
Shallow Track, the task was to map from shallow
syntax-level input representations to realisations; in
the Deep Track, the task was to map from deep
semantics-level input representations to realisations.
By the time teams submitted their system outputs,
it had become clear that the inputs required by some
types of surface realisers were more easily derived
from the common-ground representation than the in-
puts required by other types. There were other re-
spects in which the representations were not ideal,
e.g. the deep representations retained too many syn-
tactic elements as stopgaps where no deeper infor-
mation had been available. It was clear that the in-
put representations had to be improved for the next
edition of the SR Task. In this paper, we report on
our work in this direction so far and relate it to some
new shared task proposals which have been devel-
oped in part as a response to the above difficulties.
We discuss how these developments might usefully
be integrated, and outline plans for SR?13, the next
edition of the SR Task.
2 SR?11
The SR?11 input representations were created by
post-processing the CoNLL 2008 Shared Task
data (Surdeanu et al, 2008), for the preparation of
which selected sections of the WSJ Treebank were
converted to syntactic dependencies with the Pen-
nconverter (Johansson and Nugues, 2007). The
resulting dependency bank was then merged with
Nombank (Meyers et al, 2004) and Propbank
(Palmer et al, 2005). Named entity information
from the BBN Entity Type corpus was also incorpo-
rated. The SR?11 shallow representation was based
on the Pennconverter dependencies, while the deep
representation was derived from the merged Nom-
bank, Propbank and syntactic dependencies in a pro-
136
cess similar to the graph completion algorithm out-
lined by Bohnet (2010).
Five teams submitted a total of six systems to
SR?11 which we evaluated automatically using a
range of intrinsic metrics. In addition, systems were
assessed by human judges in terms of Clarity, Read-
ability and Meaning Similarity.
The four top-performing systems were all statis-
tical dependency realisers that do not make use of
an explicit, pre-existing grammar. By design, statis-
tical dependency realisers are robust and relatively
easy to adapt to new kinds of dependency inputs
which made them well suited to the SR?11 Task. In
contrast, there were only two systems that employed
a grammar, either hand-crafted or treebank-derived,
and these did not produce competitive results. Both
teams reported substantial difficulties in converting
the common ground inputs into the ?native? inputs
required by their systems.
The SR?11 results report pointed towards two
kinds of possible improvements: (i) introducing (ad-
ditional) tasks where performance would not depend
to the same extent on the relation between common-
ground and native inputs, e.g. a text-to-text shared
task on sentential paraphrasing; and (ii) improving
the representations themselves. In the remainder of
this paper we report on developments in both these
directions.
3 Towards SR?13
As outlined above, the first SR Shared Task turned
up some interesting representational issues that re-
quired some in-depth investigation. In the end, it
was this fact that led to the decision to postpone
the 2nd SR Shared Task until 2013 in order to al-
low enough time to address these issues properly. In
this section, we describe our plans for SR?13 to the
extent to which they have progressed.
3.1 Task definition
As in the first SR task, the participating teams will
be provided with annotated corpora consisting of
common-ground input representations and their cor-
responding outputs. Two kinds of input will be of-
fered: deep representations and surface representa-
tions. The deep input representations will be se-
mantic graphs; the surface representations syntactic
trees. Both will be derived from the Penn Treebank.
The task will consist in the generation of a text start-
ing from either of the input representations.
3.2 Changes to the input representations
During the working group discussions which fol-
lowed SR?11, it became apparent that the CoNLL
syntactic dependency trees overlaid with Prop-
bank/Nombank relations had turned out to be inade-
quate in various respects for the purpose of deriving
a suitable semantic representation. For instance:
? Governed prepositions are not distinguished
from semantically loaded prepositions in the
CoNLL annotation. In SR?11, only strongly
governed prepositions such as give something
TO someone were removed, but in many cases
the meaning of a preposition which introduces
an argument (of a verb, a noun, an adjective
or an adverb) clearly depends on the predicate:
believe IN something, account FOR some-
thing, etc. In those cases, too, the preposition
should be removed from the semantic annota-
tion, since the relisers have to be able to intro-
duce non-semantic features un-aided. On the
contrary, semantically loaded governed prepo-
sitions such as live IN a flat/ON a roof/NEXT
TO the main street etc. should be retained in
the annotation. These prepositions all receive
argumental arcs in PropBank/NomBank, so it
is not easy to distinguish between them. One
possibility would be to target a restricted list of
prepositions which are void of meaning most of
the time, and remove those prepositions when
they introduce arguments.
? The annotation of relative pronouns did not
survive the conversion of the original Penn
Treebank to the CoNLL format unscathed: the
antecedent of the relative pronoun is sometimes
lost or the relative pronoun is not annotated,
predominantly because the predicate which the
relative pronoun is an argument of was not con-
sidered to be a predicate by annotators, as in
the degree TO WHICH companies are irritated.
However, in the original constituency annota-
tion, the traces allow for retrieving antecedents
and semantic governors, hence using this orig-
137
inal annotation could be useful in order to get a
clean annotation of such phenomena.
Agreement has been reached on a range of other is-
sues, although the feasibility of implementing the
corresponding changes might have to be further
evaluated:
? Coordinations should be annotated in the se-
mantic representation with the conjunction as
the head of all the conjuncts. This treatment
would allow e.g. an adequate representation of
sharing of dependents among the conjuncts.
? The inversion of ?modifier? arcs and the intro-
duction of meta-semantemes would avoid an-
ticipating syntactic decisions such as the direc-
tion of non-argumental syntactic edges, and al-
low for connecting unconnected parts of the se-
mantic structures.
? In order to keep the scope of various phenom-
ena intact after inverting non-argumental edges,
we should explicitly mark the scope of e.g.
negations, quantifiers, quotation marks etc. as
attribute values on the nodes.
? Control arcs should be removed from the se-
mantic representation since they do not provide
information relevant at that level.
? Named entities will be further specified adding
a reduced set of named entity types from the
BBN annotations.
Finally, we will perform automatic and manual qual-
ity checks in order to ensure that the proposed
changes are adequately introduced in the annotation.
3.3 Evaluation
We will once again follow the main data set divi-
sions of the CoNLL?08 data (training set = WSJ Sec-
tions 02?21; development set = Section 24; test set =
Section 23), with the proviso that we have removed
300 randomly selected sentences from the develop-
ment set for use in human evaluations. Of these, we
used 100 sentences in SR?11 and will use a different
100 in SR?13.
Evaluation criteria identified as important for
evaluation of surface realisation output in previous
work include Adequacy (preservation of meaning),
Fluency (grammaticality/idiomaticity), Clarity, Hu-
manlikeness and Task Effectiveness. We will aim to
evaluate system outputs submitted by SR?13 partic-
ipants in terms of most of these criteria, using both
automatic and human-assessed methods.
As in SR?11, the automatic evaluation metrics (as-
sessing Humanlikeness) will be BLEU, NIST, TER
and possibly METEOR. We will apply text normal-
isation to system outputs before scoring them with
the automatic metrics. For n-best ranked system
outputs, we will again compute a single score for all
outputs by computing their weighted sum of their
individual scores, where a weight is assigned to a
system output in inverse proportion to its rank. For
a subset of the test data we may obtain additional al-
ternative realisations via Mechanical Turk for use in
the automatic evaluations.
We are planning to expand the range of human-
assessed evaluation experiments (assessing Ade-
quacy, Fluency and Clarity) to the following meth-
ods:
1. Preference Judgement Experiment (C2, C3):
Collect preference judgements using an exist-
ing evaluation interface (Kow and Belz, 2012)
and directly recruited evaluators. We will
present sentences in the context of a chunk of
5 consecutive sentences to the evaluators, and
ask for separate judgements for Clarity, Flu-
ency and Meaning Similarity.
2. HTER (Snover et al, 2006): In this evaluation
method, human evaluators are asked to post-
edit the output of a system, and the edits are
then categorised and counted. Crucial to this
evaluation method is the construction of clear
instructions for evaluators and the categorisa-
tion of edits. We will categorise edits as relat-
ing to Meaning Similarity, Fluency and/or Clar-
ity; we will also consider further subcategorisa-
tions.
We will once again provide evaluation scripts to par-
ticipants so they can perform automatic evaluations
on the development data. These scores serve two
purposes. Firstly, development data scores must be
included in participants? reports. Secondly, partici-
138
pants may wish to use the evaluation scripts in de-
veloping and tuning their systems.
We will report per-system results separately for
the automatic metrics (4 sets of results), and for the
human-assessed measures (2 sets of results). For
each set of results, we will report single-best and
n-best results. For single-best results, we may fur-
thermore report results both with and without miss-
ing outputs. We will rank systems, and report sig-
nificance of pairwise differences using bootstrap re-
sampling where necessary (Koehn, 2004; Zhang and
Vogel, 2010). We will separately report correlation
between human and automatic metrics, and between
different automatic metrics.
3.4 Assessing different aspects of realisation
separately
In addition, we will consider measuring different as-
pects of the realisation performance of participating
systems (syntax, word order, morphology) since a
system can perform well on one and badly on an-
other. For instance, a system might perform well
on morphological realisation while it has poor re-
sults on linearisation. We would like to capture this
fact. This may involve asking participating teams to
submit intermediate representations or identifiers to
identify the reference words. This more fine-grained
approach should help us to obtain a more precise
picture of the state of affairs in the field and could
help to reveal the respective strengths of different
surface realisers more clearly.
4 Related Developments
4.1 Syntactic Paraphrase Ranking
The new shared task on syntactic paraphrase ranking
described elsewhere in this volume (White, 2012) is
intended to run as a follow-on to the main surface
realisation shared task. Taking advantage of the hu-
man judgements collected to evaluate the surface re-
alisations produced by competing systems, the task
is to automatically rank the realisations that differ
from the reference sentence in a way that agrees with
the human judgements as often as possible. The task
is designed to appeal to developers of surface real-
isation systems as well as machine translation eval-
uation metrics. For surface realisation systems, the
task sidesteps the thorny issue of converting inputs
to a common representation. Developers of reali-
sation systems that can generate and optionally rank
multiple outputs for a given input will be encouraged
to participate in the task, which will test the system?s
ability to produce acceptable paraphrases and/or to
rank competing realisations. For MT evaluation
metrics, the task provides a challenging framework
for advancing automatic evaluation, as many of the
paraphrases are expected to be of high quality, dif-
fering only in subtle syntactic choices.
4.2 Content Selection Challenge
The new shared task on content selection has been
put forward (Bouayad-Agha et al, 2012) to initi-
ate work on content selection from a common, stan-
dardised semantic-web format input, and thus pro-
vide the context for an objective assessment of dif-
ferent content selection strategies. The task con-
sists in selecting the contents communicated in ref-
erence biographies of celebrities from a large vol-
ume of RDF-triples. The selected triples will be
evaluated against a gold triple selection set using
standard quality assessment metrics.
The task can be considered complementary to the
surface realisation shared task in that it contributes
to the medium-term goal of setting up a task that
covers all stages of the generation pipeline. In fu-
ture challenges, it can be explored to what extent and
how the output content plans can be mapped onto
semantic representations that serve as input to the
surface realisers.
5 Plans
We are currently working on the new improved
common-ground input representation scheme and
converting the data to the new scheme.
The provisional schedule for SR?13 looks as
follows:
Announcement and call for expres-
sions of interest:
6 July 2012
Preliminary registration and release
of description of new representations:
27 July 2012
Release of data and documentation: 2 Nov 2012
System Submission Deadline: 10 May 2013
Evaluation Period: 10 May?
10 Jul 2013
Provisional dates for results session: 8?9 Aug 2013
139
6 Conclusion
For a large number of NLP applications (among
them, e.g., text generation proper, summarisation,
question answering, and dialogue), surface realisa-
tion (SR) is a key technology. Unfortunately, so
far in nearly all of these applications, idiosyncratic,
custom-made SR implementations prevail. How-
ever, a look over the fence at the language analy-
sis side shows that the broad use of standard de-
pendency treebanks and semantically annotated re-
sources such as PropBank and NomBank that were
created especially with parsing in mind led to stan-
dardised high-quality off-the-shelf parser implemen-
tations. It seems clear that in order to advance the
field of surface realisation, the generation commu-
nity also needs adequate resources on which large-
scale experiments can be run in search of the surface
realiser with the best performance, a surface realiser
which is commonly accepted, follows general trans-
parent principles and is thus usable as plug-in in the
majority of applications.
The SR Shared Task aims to contribute to this
goal. On the one hand, it will lead to the creation
of NLG-suitable resources in that it will convert
the PropBank into a more semantic and more com-
pletely annotated resource. On the other hand, it will
offer a forum for the presentation and evaluation of
various approaches to SR and thus help us to search
for the best solution to the SR task with the greatest
potential to become a widely accepted off-the-shelf
tool.
Acknowledgments
We gratefully acknowledge the contributions to dis-
cussions and development of ideas made by the
other members of the SR working group: Miguel
Ballesteros, Johan Bos, Aoife Cahill, Josef van Gen-
abith, Pablo Gerva?s, Deirdre Hogan and Amanda
Stent.
References
Anja Belz, Michael White, Dominic Espinosa, Deidre
Hogan, Eric Kow, and Amanda Stent. 2011. The
first surface realisation shared task: Overview and
evaluation results. In Proceedings of the 13th Eu-
ropean Workshop on Natural Language Generation
(ENLG?11), pages 217?226. Association for Compu-
tational Linguistics.
Bernd Bohnet, Leo Wanner, Simon Mille, and Alicia
Burga. 2010. Broad coverage multilingual deep sen-
tence generation with a stochastic multi-level realizer.
In Proceedings of the 23rd International Conference
on Computational Linguistics, Beijing, China.
Nadjet Bouayad-Agha, Gerard Casamayor, Leo Wanner,
and Chris Mellish. 2012. Content selection from
semantic web data. In Proceedings of the 7th In-
ternational Natural Language Generation Conference
(INLG?12).
Richard Johansson and Pierre Nugues. 2007. Extended
constituent-to-dependency conversion for English. In
Joakim Nivre, Heiki-Jaan Kaalep, Kadri Muischnek,
and Mare Koit, editors, Proceedings of NODALIDA
2007, pages 105?112, Tartu, Estonia.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Dekang Lin and
Dekai Wu, editors, Proceedings of EMNLP 2004,
pages 388?395, Barcelona, Spain, July. Association
for Computational Linguistics.
Eric Kow and Anja Belz. 2012. LG-Eval: A toolkit
for creating online language evaluation experiments.
In Proceedings of the 8th International Conference on
Language Resources and Evaluation (LREC?12).
Adam Meyers, Ruth Reeves, Catherine Macleod, Rachel
Szekely, Veronika Zielinska, Brian Young, and Ralph
Grishman. 2004. The NomBank project: An interim
report. In NAACL/HLT Workshop Frontiers in Corpus
Annotation.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The Proposition Bank: A corpus annotated with
semantic roles. In Computational Linguistics Journal,
pages 71?105.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea
Micciulla, and John Makhoul. 2006. A study of trans-
lation edit rate with targeted human annotation. In In
Proceedings of Association for Machine Translation in
the Americas, pages 223?231.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Llu??s Ma`rquez, and Joakim Nivre. 2008. The
CoNLL 2008 shared task on joint parsing of syntac-
tic and semantic dependencies. In Proceedings of the
Twelfth Conference on Computational Natural Lan-
guage Learning (CoNLL?08), Manchester, UK.
Michael White. 2012. Shared task proposal: Syntac-
tic paraphrase ranking. In Proceedings of the 7th In-
ternational Natural Language Generation Conference
(INLG?12).
Ying Zhang and Stephan Vogel. 2010. Significance tests
of automatic machine translation evaluation metrics.
Machine Translation, 24:51?65.
140
INLG 2012 Proceedings of the 7th International Natural Language Generation Conference, pages 150?153,
Utica, May 2012. c?2012 Association for Computational Linguistics
Shared Task Proposal: Syntactic Paraphrase Ranking
Michael White
Department of Linguistics
The Ohio State University
Columbus, OH 43210 USA
mwhite@ling.ohio-state.edu
Abstract
We describe a new shared task on syntac-
tic paraphrase ranking that is intended to run
in conjunction with the main surface real-
ization shared task. Taking advantage of
the human judgments collected to evaluate
the surface realizations produced by com-
peting systems, the task is to automatically
rank these realizations?viewed as syntactic
paraphrases?in a way that agrees with the hu-
man judgments as often as possible. The task
is designed to appeal to developers of surface
realization systems as well as machine transla-
tion evaluation metrics: for surface realization
systems, the task sidesteps the thorny issue of
converting inputs to a common representation;
for MT evaluation metrics, the task provides
a challenging framework for advancing auto-
matic evaluation, as many of the paraphrases
are expected to be of high quality, differing
only in subtle syntactic choices.
1 Introduction
For the first surface realization shared task, the orga-
nizers considered running a follow-on task for evalu-
ating automatic evaluation metrics?along the lines
of similar meta-evaluations carried out for machine
translation in recent years?though it was deferred
for lack of time. For the second surface realiza-
tion shared task, we propose to generalize this met-
rics meta-evaluation task to also usefully encom-
pass realization ranking, where the various realiza-
tions generated for a given input in the main task
are viewed as syntactic paraphrases of the original
corpus sentence. The syntactic paraphrasing shared
task comprises three tracks, described in the next
section; in each case, the task is to automatically re-
produce the relative preference judgments gathered
during the human evaluation of the surface realiza-
tion main task. As explained further below, develop-
ers of realization systems that can generate and op-
tionally rank multiple outputs for a given input will
be encouraged to participate in the task, which will
test the system?s ability to produce acceptable para-
phrases and/or to rank competing realizations.
The objectives of the shared task are as follows:
broaden participation We expect developers of
automatic quality metrics in the MT commu-
nity to be interested in the proposed task, which
is anticipated to be both more focused (with
lexical choice largely excluded) and more chal-
lenging than in the MT case, given the gener-
ally high level of quality in realization results:
as realization quality increases, the metrics?
task becomes more difficult, since the para-
phrases of a given sentence often involve sub-
tle differences between acceptable and unac-
ceptable variation. In an earlier study of the
utility of automatic metrics with Penn Tree-
bank (PTB) surface realization data (Espinosa
et al, 2010), we observed moderate correla-
tions between the most popular metrics and hu-
man judgments, though lower than the levels
seen with MT data.
promote reuse of human judgments The task is
intended to test the effectiveness of realization
ranking models in a way that reuses human
judgments, making it possible to carry out re-
150
Track Reference PTB PTB
Sentence Gold Auto
Realization Ranking N Y N
Hybrid Y Y N
Metrics Meta-Eval Y N Y
Table 1: Additional inputs for the three realization tracks
producible system comparisons.
mitigate input conversion issues Realizer evalua-
tions have typically focused on single-best out-
puts, where the depth and specificity of sys-
tem inputs has a large impact on quality, mak-
ing comparative evaluation difficult. While the
surface realization shared task seeks to address
this issue by developing common ground input
representations, to date it has proved to be dif-
ficult to adapt existing systems to work with
these inputs. By focusing on ranking para-
phrases that are distinct from the reference sen-
tence, the proposed task may provide a way to
mitigate these issues, as discussed below.
2 Three Tracks: From Realization
Ranking to Metrics Meta-Evaluation
We propose three tracks for the task, going from
pure realization ranking to metrics meta-evaluation,
with a hybrid case in the middle. For all three tracks,
the input is a set of pairs of syntactic paraphrases
(distinct from the reference sentence), and the output
is the preferred member of each pair, where the goal
is to match the human judgments of relative prefer-
ence. The tracks differ in the additional inputs that
systems may use in determining which member of
each pair is preferred (see Table 1). In the realiza-
tion ranking track, the task is to rank order the para-
phrases for a given sentence, without having access
to the reference sentence, using a realization rank-
ing model. To do so, each system is allowed to use
its own ?native? inputs derived from the Penn Tree-
bank and PTB-based resources. To the extent that
a system?s statistical ranking model can be used to
assign a score to any possible realization, the rank-
ing task can be accomplished by simply ranking the
realizations by model score. As such, following this
strategy, the task is one of analysis by synthesis.
For non-statistical realizers, or ones that cannot
assign a score to any possible realization, there is
an alternative strategy available, namely to auto-
matically approximate HTER. Snover et al (2006)
demonstrate that the human-targeted translation edit
rate (HTER) represents a reliable and easily inter-
pretable method of evaluating MT output. With this
method, a human annotator produces a targeted ref-
erence sentence which is as close as possible to the
MT hypothesis while being fully acceptable; from
the targeted reference, the TER score then repre-
sents a normalized post-edit score, which has been
shown to correlate with human ratings at least as
well as more complex competing metrics. As Mad-
nani (2010) points out, generated paraphrases of
the reference sentence can be used to approximate
HTER scoring, as the closest acceptable paraphrase
of a reference sentence should correspond to the ver-
sion of the MT hypothesis with minimal changes to
make it acceptable. Indeed, in the limit, it should
be possible to use a system that can enumerate all
and only the acceptable paraphrases of a reference
sentence to fully implement HTER scoring.
Naturally, it is possible to combine the analysis-
by-synthesis and approximating HTER strategies.
One particularly simple way to do so is to (1) use
an n-best list of realizations with normalized scores,
(2) find the realization with the minimum TER score
for each paraphrase to rank, then (3) combine the re-
alizer?s model score with the TER score, e.g. just by
subtraction (weights for the combination could also
be optimized using machine learning).
Regarding the issue of whether fair comparisons
can be made when each system is allowed to use its
own PTB-derived ?native? input, note that it is un-
clear whether using shallow, specific inputs is neces-
sarily advantageous for ranking a range of possible
realizations, all distinct from the reference sentence:
in the limit, a realizer input that completely speci-
fies the reference sentence (and no other variants) is
of no help at all, as in this case the approximating
HTER strategy reduces to just doing TER scoring
against the reference sentence.
Turning now to the metrics meta-evaluation track,
here the the task is to rank order a set of realizations
for a given sentence, starting with the reference sen-
tence and nothing else. In principle, it should be
possible to use any MT metric for this task off-the-
151
shelf. It should also be possible for realization sys-
tems to participate in this track, if they can be paired
with a parser that produces inputs for the realizer, or
a parser whose outputs can be converted to realizer
inputs. To do so, strategies employed in the realiza-
tion ranking track can be combined with ones that
make use of the reference sentence.
Finally, between these two tracks is a hybrid track,
where one is allowed to substitute automatic parses
with gold parses. This track can be viewed as pro-
viding a way to estimate an upper bound on ap-
proaches that pay attention to how well a sentence
expresses an intended meaning, while also arguably
representing the most sensible way to automatically
evaluate outputs in a data-to-text setting, where in-
tended meanings can be reliably represented.
3 Pilot Experiments
In this section, we present two pilot experiments in-
tended to demonstrate the feasibility of the task. The
experiments use the human judgments collected in
Espinosa et al?s (2010) study, which consist of ade-
quacy and fluency ratings from two judges for a va-
riety of realizations for PTB Section 00. The real-
izations in the corpus were generated using several
OpenCCG realization ranking models (White and
Rajkumar, 2009) and using the XLE symbolic re-
alizer with subsequent n-gram ranking (paraphrases
involving WordNet substitutions were excluded).
For comparison purposes, three well-known met-
rics (BLEU, METEOR and TER) were tested, along
with three OpenCCG ranking models: (I) a gen-
erative baseline model, incorporating three n-gram
models as well as Hockenmaier?s (2003) genera-
tive model; (II) a model additionally incorporating
a slew of discriminative features, extending White
& Rajkumar?s model with dependency ordering fea-
tures; and (III) a model adding one additional fea-
ture for minimizing dependency length. Note that
Models II and III are very similar, usually yielding
the same single-best output, though occasionally dif-
fering in important ways; by contrast, both models
represent a substantial refinement of Model I.
The two experiments investigate different strate-
gies for approaching the hybrid task. The first exper-
iment investigates the approximating-HTER strat-
egy (with an analysis-by-synthesis component) us-
ing a 20-best list. For simplicity, edit rate (edit dis-
tance normalized by the number of words in the ref-
erence sentence) was used to find the realization in
the 20-best list that was closest to the paraphrase to
be ranked. The score for the paraphrase was then
calculated by normalizing the realizer model score
for the closest realization (linearly interpolating us-
ing the min and max scores across all 20-best lists),
subtracting the edit rate, and adding in the met-
ric score, for each of BLEU, METEOR and TER.1
Since edit rate is less reliable than TER, as it overly
penalizes phrasal shifts, the metric score was used
alone in cases where the edit rate exceeded 0.5.
The results of the first experiment appear in Ta-
ble 2. Human judgments were combined by av-
eraging the summed adequacy and fluency ratings
from each judge. Excluding exact match realiza-
tions, 2838 pairs of realizations with distinct com-
bined scores (from approximately 250 sentences)
were used to judge ranking accuracy. Here, BLEU
substantially outperforms METEOR and TER, and
combining Models I-III with BLEU does not yield
significant differences in ranking accuracy. Note,
however, that using TER scores rather than edit rate,
and optimizing the way the model scores are com-
bined with the TER score and BLEU score, could
perhaps yield significant improvements. With ME-
TEOR and TER, combining the model score, edit
rate and metric score in the simplest way does yield
highly significant improvements. With the ME-
TEOR combination, Model II achieves a highly sig-
nificant improvement over Model I, though in other
cases, only trends are observed across models.
The second experiment investigates the analysis-
by-synthesis strategy more directly. Here, the re-
alizer?s search was guided to reproduce each para-
phrase where possible, with model scores then cal-
culated where an exact match could be achieved.
The results appear in Table 3 for 474 pairs with dif-
fering combined human judgments. The first col-
umn shows the ranking accuracy using the model
scores by themselves; the subsequent columns com-
pare the accuracy using BLEU, METEOR and TER
against using the model score added to the metric
score. Here we see from the first column that Model
II substantially outperforms Model I, showing the
1TER scores were inverted for consistency.
152
BLEU Model+BLEU METEOR Model+METEOR TER Model+TER
Model I 71.2 70.2 58.6 65.4 (***) 59.7 68.7 (***)
Model II - 70.8 - 66.7 (***? ? ?) - 69.4 (***?)
Model III - 71.3 (?) - 67.1 (***) - 69.9 (***)
Table 2: Pairwise accuracy percentage on reproducing human judgments of relative adequacy plus fluency of syntactic
paraphrases, using n-best realizations from three OpenCCG ranking models and minimum edit rate in combination
with MT metrics (significance: * for p < 0.1, ** for p < 0.05, *** for p < 0.01 in comparison to MT metric, using
McNemar?s test; similarly for number of daggers in comparison to model in previous row)
Model BLEU Model+BLEU METEOR Model+METEOR TER Model+TER
Model I 62.2 67.7 73.0 (***) 49.2 65.4 (***) 50.6 73.8 (***)
Model II 67.1 (? ? ?) - 72.2 (***) - 68.6 (***??) - 74.9 (***)
Model III 66.2 - 72.6 (***) - 68.8 (***) - 75.1 (***)
Table 3: Pairwise accuracy percentage on reproducing human judgments of relative adequacy plus fluency of syntactic
paraphrases, using exact targeted realizations from three OpenCCG ranking models and minimum edit rate in com-
bination with MT metrics (significance: * for p < 0.1, ** for p < 0.05, *** for p < 0.01 in comparison to MT metric,
using McNemar?s test; similarly for number of daggers in comparison to model in previous row)
ability of the ranking task to discriminate among
models of varying sophistication, though the model
differences are largely washed out when the model
scores are combined with metric scores. In the sub-
sequent columns, we see that METEOR and TER
are only performing at chance (50%) on these par-
ticular ranking cases, while adding the model scores
and metric scores does much better, with Model III
plus TER performing the best overall, as might have
been expected. Even with BLEU, which performs
decently on its own, adding in the model scores
achieves substantial (and highly significant) gains.
4 Task Organization
The proposed syntactic paraphrase ranking task is
intended to be run as a straightforward extension of
the main surface realization shared task. For devel-
opment and training purposes, the human judgments
collected for the first surface realization shared task
will be made available; the data from Espinosa et
al.?s study is already publicly available as well. For
test data, the human judgments collected for eval-
uation during the second surface realization shared
task will be used. Ideally enough systems will enter
the main task to enable many pairwise comparisons
per sentence, and enough judges can be employed
to allow majority preferences to be used as the gold
standard. As baselines for the metrics meta-eval and
hybrid tracks, the BLEU, NIST, METEOR and TER
metrics will be run by the organizers. Time permit-
ting, a baseline system that works with n-best real-
ization scores will also be made available, so that
any developer of a realization system that can pro-
duce n-best outputs can easily participate.
Acknowledgments
This work was supported in part by NSF grant no.
IIS-1143635. Thanks go to the anonymous review-
ers for helpful comments and discussion.
References
Dominic Espinosa, Rajakrishnan Rajkumar, Michael
White, and Shoshana Berleant. 2010. Further meta-
evaluation of broad-coverage surface realization. In
Proc. of EMNLP-10, pages 564?574.
Julia Hockenmaier. 2003. Data and models for statis-
tical parsing with Combinatory Categorial Grammar.
Ph.D. thesis, University of Edinburgh.
Nitin Madnani. 2010. The Circle of Meaning: From
Translation to Paraphrasing and Back. Ph.D. thesis,
University of Maryland.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A study of
translation edit rate with targeted human annotation.
In Proc. of AMTA-06, pages 223?231.
Michael White and Rajakrishnan Rajkumar. 2009. Per-
ceptron reranking for CCG realization. In Proc. of
EMNLP-09, pages 410?419.
153
Proceedings of the 14th European Workshop on Natural Language Generation, pages 30?39,
Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational Linguistics
Enhancing the Expression of Contrast
in the SPaRKy Restaurant Corpus
David M. Howcroft and Crystal Nakatsu and Michael White
Department of Linguistics
The Ohio State University
Columbus, OH 43210, USA
{howcroft,cnakatsu,mwhite}@ling.osu.edu
Abstract
We show that Nakatsu & White?s (2010)
proposed enhancements to the SPaRKy
Restaurant Corpus (SRC; Walker et al,
2007) for better expressing contrast do in-
deed make it possible to generate better
texts, including ones that make effective
and varied use of contrastive connectives
and discourse adverbials. After first pre-
senting a validation experiment for natu-
ralness ratings of SRC texts gathered using
Amazon?s Mechanical Turk, we present
an initial experiment suggesting that such
ratings can be used to train a realization
ranker that enables higher-rated texts to be
selected when the ranker is trained on a
sample of generated restaurant recommen-
dations with the contrast enhancements
than without them. We conclude with a
discussion of possible ways of improving
the ranker in future work.
1 Introduction
To lessen the need for handcrafting in developing
generation systems, Walker et al (2007) extended
the overgenerate-and-rank methodology (Langk-
ilde and Knight, 1998; Mellish et al, 1998; Walker
et al, 2002; Nakatsu and White, 2006) to complex
information presentation tasks involving variation
in rhetorical structure. They illustrated their ap-
proach by developing SPaRKy (Sentence Planning
with Rhetorical Knowledge), a sentence planner
for generating restaurant recommendations and
comparisons in the context of the MATCH (Mul-
timodal Access To City Help) system (Walker et
al., 2004), and showed that SPaRKY can produce
texts comparable to those of MATCH?s template-
based generator.
Despite the evident importance of expressing
contrast clearly in making comparisons among
restaurants, Nakatsu (2008) surprisingly found
that most of the examples involving contrastive
connectives in the SPaRKy Restaurant Corpus
(SRC) received low ratings by the human judges.
Even though the low ratings were not necessarily
directly attributable to the use of a contrastive con-
nective in many cases, Nakatsu conjectured that
the large proportion of low-rated examples con-
taining contrastive connectives would make it dif-
ficult to train a ranker to learn to use contrastive
connectives effectively without augmenting the
corpus with better examples of contrast. Sub-
sequently, Nakatsu and White (2010) proposed
a set of enhancements to the SRC intended to
better express contrast?including ones employ-
ing multiple connectives in the same clause that
are problematic for RST (Mann and Thompson,
1988)?and showed how they could be generated
with Discourse Combinatory Categorial Grammar
(DCCG), an extension of CCG (Steedman, 2000)
designed to enable multi-sentence grammar-based
generation. However, Nakatsu and White did not
evaluate empirically whether these contrast en-
hancements were successful.
In this paper, we show that Nakatsu &
White?s (2010) proposed SRC contrast enhance-
ments do indeed make it possible to generate bet-
ter texts: in particular, we present an initial ex-
periment that shows that the oracle best restau-
rant recommendations including the contrast en-
hancements have significantly higher human rat-
ings for naturalness than comparable texts without
these enhancements, and which suggests that even
a basic n-gram ranker trained on the enhanced
recommendations can select texts with higher rat-
ings. The paper is structured as follows. In
Section 2, we review Nakatsu & White?s pro-
posed enhancements to the SRC for better express-
ing contrast?including the use of structural con-
nectives together with discourse adverbials?and
how they can be generated with DCCG. In Sec-
30
tion 3, we first present a validation experiment
showing that naturalness ratings gathered on Ama-
zon?s Mechanical Turk (AMT) are comparable to
those for the same texts in the original SRC; then,
we present our method of generating and selecting
a sample of new restaurant recommendation texts
with and without the contrast enhancements for
rating on AMT. In Section 4, we describe how we
trained discriminative n-gram rankers using cross
validation on the gathered ratings. In Section 5,
we present the oracle and cross validation results
in terms of mean scores of the top-ranked text. In
Section 6, we analyze how the individual contrast
enhancements affected the naturalness ratings and
discuss issues that may be still hampering natu-
ralness. Finally, in Section 7, we conclude with
a summary and a discussion of possible ways of
creating improved rankers in future work.
2 Enhancing Contrast with Discourse
Combinatory Categorial Grammar
Figure 1 (Nakatsu, 2008) shows examples from
the SRC where some of the SPaRKy realizations
are clearly more natural than others. In Nakatsu?s
experiments, she found that the use of contrastive
connectives was negatively correlated with human
ratings, and that an n-gram ranker learned to dis-
prefer texts containing these connectives. In an-
alyzing these unexpected results, Nakatsu noted
two factors that appeared to hamper the natural-
ness of the contrastive connective usage. First,
consistent with Grote et al?s (1995) observation
that however and on the other hand (unlike but and
while) signal that the clause they attach to is the
more important one, we might expect realizations
to be preferred when these connectives appear
with the more desirable of the contrasted qualities.
Such preferences do indeed appear to be present
in the SRC: for example, in Figure 1, alts 8 &
13?where the better property is ordered second?
are rated highly, while alts 7 & 11?where the
better property is ordered first?are rated poorly.
Nakatsu further observed that in human-authored
comparisons, when the second clause expresses
the lesser property, it is often qualified by only
or just; consistent with this observation, alts 7 &
11 do seem to improve with the inclusion of these
modifiers.
The second factor noted by Nakatsu that may
contribute to the awkwardness of however and on
the other hand is that both of these connectives
seem to be rather ?grand? for the rather simple
contrasts in Figure 1, and may sound more natu-
ral when used with heavier arguments.
Based on these observations, Nakatsu and
White (2010) proposed a set of enhancements to
the SRC, all of which are exemplified in Figure 2.
1
The enhancements include (i) optional summary
statements that give an overall assessment of each
restaurant based on the average of their property
values, thereby allowing contrasts to be expressed
over larger text spans; (ii) adverbial modifiers
only, just and merely to express a lesser value of
a given property than one mentioned earlier;
2
(iii)
the modifers also and too to signal the repetition
of the same value for a given property (Strieg-
nitz, 2004); and (iv) contrastive connectives for
different properties of the same restaurant, exem-
plified here by the contrast between decent decor
and mediocre food quality for Bienvenue.
In the text plan in Figure 2, <1>?<4> cor-
respond to the propositions in the original SRC
text plan and (1?)?(2?) are the new summary-level
propositions. Following Webber et al (2003),
Nakatsu and White (2010) take only, merely, just,
also, and too to be discourse adverbials, whose
discourse relations are allowed to cut across the
primary tree structure established by the other re-
lations in the figure. Note that in addition to go-
ing beyond RST?s limitation to tree-structured dis-
courses, the example also contains clauses em-
ploying multiple discourse connectives, where one
is a structural connective (such as however or
while) and the other is a discourse adverbial.
To realize such texts, Nakatsu & White intro-
duce Discourse Combinatory Categorial Grammar
(DCCG), an extension of CCG (Steedman, 2000)
to the discourse level. DCCG follows Discourse
Lexicalized Tree Adjoining Grammar (Webber,
2004) in providing a lexicalized treatment of struc-
tural connectives and discourse adverbials, but dif-
fers in doing so in a single CCG, rather than sep-
arate sentence-level and discourse-level grammars
whose interaction is not straightforward. As such,
DCCG requires no changes to the OpenCCG real-
izer (White, 2006b; White, 2006a; White and Ra-
1
In the text, words intended to help indicate similarities
and contrasts are italicized. Note that we have added overall
and on the whole to the summary statements to better indicate
their summarizing role.
2
The second value must be a less extreme one on the same
side of the scale; in principle, it could be merely poor rather
than horrible, but such low attribute values did not occur in
the corpus.
31
Strategy Alt # Rating Rank Realization
3 3 7 Sonia Rose has very good decor but Bienvenue has decent decor.
7 1 16 Sonia Rose has very good decor. On the other hand, Bienvenue has decent decor.
8 4.5 13 Bienvenue has decent decor. Sonia Rose, on the other hand, has very good decor.
C2 10 4.5 5 Bienvenue has decent decor but Sonia Rose has very good decor.
11 1 12 Sonia Rose has very good decor. However, Bienvenue has decent decor.
13 5 14 Bienvenue has decent decor. However, Sonia Rose has very good decor.
14 5 3 Sonia Rose has very good decor while Bienvenue has decent decor.
15 4 4 Bienvenue has decent decor while Sonia Rose has very good decor.
17 1 15 Bienvenue?s price is 35 dollars. Sonia Rose?s price, however, is 51 dollars. Bienvenue has decent decor.
However, Sonia Rose has very good decor.
Figure 1: Some alternative [Alt] realizations of SPaRKy sentence plans from a COMPARE-2 [C2] plan, with averaged
human ratings [Rating] (5 = highest rating) and ranks assigned by the n-gram ranker [Rank] (1 = top ranked).
tion, the SPaRKy sentence plan generator adds the
INFER relation to assertions whose relations were
not specified by the content planner.
During the sentence planning phase, SPaRKy or-
ders the clauses and combines them using randomly
selected clause-combining operations. During this
process, a clause-combining operation may insert 1
of 7 connectives according to the RST relation that
holds between two discourse units (i.e. inserting
since or because for a JUSTIFY relation; and, how-
ever, on the other hand, while, or but for a CON-
TRAST relation; or and for an INFER relation).
After each sentence plan is generated, it is real-
ized by the RealPro surface realizer and the result-
ing realization is rated by two judges on a scale of
1-5, where 5 is highly preferred. These ratings are
then averaged, producing a range of 9 possible rat-
ings from {1, 1.5, ..., 5}.
2.2 Ratings/Connectives Correlation
From the ratings of the examples in Figure 1, we
can see that some of the SPaRKy sentence plan re-
alizations seem more natural than others. Upon fur-
ther analysis, we noticed that utterances containing
many contrastive connectives seemed less preferred
than those with fewer or no contrastive connectives.
To quantify this observation, we calculated the av-
erage number of connectives (ave
c
i
) used per real-
ization with rating i, using ave
c
i
= Total
c
i
/N
r
i
,
where Total
c
i
is the total number of connectives in
realizations with rating i, and N
r
i
is the number of
realizations with rating i.
We use Pearson?s r to calculate each correlation
(in each case, df = 7). For both COMPARE strategies
(represented in Figure 2(a) and 2(b)), we find a sig-
nificant negative correlation for the average number
of connectives used in realizations with a given rat-
ing (C2: r =  0.97, p < 0.01; and C3: r =  0.93,
p < 0.01). These correlations indicate that judges?
ratings decreased as the average frequency of the
connectives increased.
Further analysis of the individual correlations
used in the comparative strategies show that there is
a significant negative correlation for however (C2:
r =  0.91, p < 0.01; and C3: r =  0.86,
p < 0.01) and on the other hand (C2: r =  0.89,
p < 0.01; and C3: r =  0.84, p < 0.01) in both
COMPARE strategies. In addition, in COMPARE-3,
the frequencies of while and but are also signifi-
cantly and strongly negatively correlated with the
judges? ratings (r =  0.86, p < 0.01 and r =
 0.90, p < 0.01, respectively), though there is no
such correlation between the use of these connec-
tives and their ratings in COMPARE-2.
Added together, all the contrastive connectives
show strong, significant negative correlations be-
tween their average frequencies and judges? ratings
for both comparative strategies (C2: r =  0.93,
p < 0.01; C3:r =  0.88, p < 0.01).
Interestingly, unlike in the COMPARE strategies,
there is a positive correlation (r = 0.73, p > 0.05)
between the judges? ratings and the average fre-
quency of all connectives used in the RECOMMEND
strategy (see Figure 2(c)). Since this strategy only
uses and, since, and because and does not utilize any
contrastive connectives, this gives further evidence
that only contrastive connectives are dispreferred.
2.3 N-gram Ranker and Features
To acertain whether these contrastive connectives
are being learned by the ranker, we re-implemented
the n-gram ranker using SVM-light (Joachims,
77
Figure 1: Some alternative (Alt) realizatio s of SPaRKy se tence plans from a COMPARE2 (C2) plan,
with averaged human ratings (Rating; 5 = highest rating) and ranks (Rank; 1 = top ranked) assigned by
an n-gram ranker (Nakatsu, 2008)
Generating with Discourse Combinatory Categorial Grammar / 53
contrast
g
g
g
g
g
g
g
g
g
g
g
g
g
g
g
g
g
W
W
W
W
W
W
W
W
W
W
W
W
W
W
W
W
W
evidence
l
l
l
l
l
l
l
l
l
L
L
L
L
L
L
L
evidence
l
l
l
l
l
l
l
l
l
L
L
L
L
L
L
L
nucleus:(1?)
assert-summary
(mod:good)
infer
o
o
o
o
o
o
o
O
O
O
O
O
O
O
O
nucleus:(2?)
assert-summary
(mod:mediocre)
{infer|contrast}
o
o
o
o
o
o
o
o
O
O
O
O
O
O
O
O
nucleus:<1>
assert-com-decor
(mod:decent)
nucleus:<3>
assert-com-food-
quality (mod:very good)
YY
nucleus:<2>
assert-com-decor
(mod:decent)
nucleus:<4>
assert-com-food-
quality (mod:mediocre)
bb bb
merely
additive
merely
FIGURE 31 SPaRKy tp-tree altered with new relations and summary
statements, corresponding to Example 50.
in bold font. Lastly, the connectives also and only, which represent the
additional relatio s, additive and merely, respectively, are indicated
in small caps.
(50) (1?): Soni Ros is a good restaurant.
<1>: It has decent decor and
<3>: very good food quality.
(2?): However, Bienvenue is just a mediocre restaurant.
<2>: While it also has decent decor,
<4>: it only has mediocre food quality.
7 Related Work
In terms of its discourse theoretical basis, DCCG is most closely re-
lated to D-LTAG. In general, as Webber (2006) observes, discourse
grammars vary in their theoretical style, from wholly based on de-
pendency relations (e.g. Halliday and Hasan 1976) to adherence to a
completely constituent-based model (e.g. Rhetorical Structure Theory
[RST], Mann and Thompson 1988; Linguistic Discourse Model, Polanyi
1988, Polanyi and van den Berg 1996). Dependency-based discourse
theories are advantageous because they allow discourse relations to ex-
ist between non-adjacent discourse units, lifting restrictions on which
clauses can serve as discourse arguments of a given relation. Compu-
(1?): Sonia Rose is a good restaurant overall.
<1>: It has decent decor and
<3>: very good food quality.
(2?): However, Bienvenue is just a mediocre
restaurant on the whole.
<2>: While it also has decent decor,
<4>: it only has mediocre food quality.
Figure 2: Modified SPaRKy text plan for text with new relations and summary statements intended to
enhance contrast (Nakatsu and White, 2010)
32
Generating with Discourse Combinatory Categorial Grammar / 25
ot1h, A. however, B. otoh, C. however, D.
ts
ot1h
ts
however
ts
otoh
ts
however
TC TC
ts
CUE
\
?
ts
CUE
ts
CUE
\
?
ts
CUE
< <
ts
ot1h
ts
otoh
TC
ts
nil
/
?
ts
otoh
>
ts
nil
TC
turn
nil
FIGURE 16 A DCCG derivation of nested contrast relations
Returning now to the intrasentential conjunctions that express con-
trast, their categories remain the same as in the preceding section, ex-
cept for the addition of the requirement that they combine with clauses
having nil values for the cue feature:
(39) a. {while, but } ` s
e,nil
\
?
s
e
1
,nil
\
?
punc
,
/
?
s
e
2
,nil
:
@
e
(contrast-rel ^ hArg1ie
1
^ hArg2ie
2
)
b. while ` s
e,nil
/
?
s
e
2
,nil
/
?
punc
,
/
?
s
e
1
,nil
:
@
e
(contrast-rel ^ hArg1ie
1
^ hArg2ie
2
)
Since these categories do not need to look outside the sentence to find
both of their discourse arguments, they do not change the cue values
of their result categories.
To conclude this section, we address the question of whether it is a
necessary move to employ unary type-changing rules in order to handle
intersentential discourse connectives in CCG. As noted in the preceding
section, the lexicalized categories for connectives o?ered therein suggest
that there is no problem in principle with devising a purely lexicalized
approach to discourse connectives; accordingly, the cue threading ap-
proach presented in this section appears to yield grammars with cov-
erage equivalent to purely lexicalized alternatives. Nevertheless, as we
have seen, the purely lexicalized approach leads to a proliferation of lex-
ical category ambiguity, and while lexical rules might be employed to
systematically assign the necessary lexical categories, the cue threading
approach is clearly more economical. Similar considerations led Hock-
enmaier and Steedman (2002, 2007) to make extensive use of type-
changing rules in their broad coverage grammar of English, indicating
that such rules have an important role to play in practical grammars.
Hockenmaier and Steedman further argued that the formal power of
the system is una?ected as long as (i) only a finite number of unary
rules are employed and (ii) the rules are designed so that they cannot
recursively apply to their own output, as is the case here.
Figure 3: DCCG derivation of nested contrast re-
lations (Nakatsu and White, 2010)
26 / LiLT volume 4, issue 1 September 2010
it also as poor decor
np s
CUE
\np/
?
(s
CUE
\np) s
nil
\np
nom
>
s
nil
\np
nom
<
s
nil
FIGURE 17 A DCCG derivation of a clause including the discourse
adverbial also.
5.3 Discourse Adverbials and Anaphora Resolution
Unlike structural connectives, which find their discourse arguments via
cue threading, discourse adverbials find one argument syntactically,
and the other through anaphora resolution. To illustrate how DCCG
accomplishes this, consider (1) from Section 2, repeated below:
(1) b
1
: Bienven e is a mediocre restaurant.
h
1
: It has poor decor and mediocre food quality.
b
3
: However, Sonia Rose is a good restaurant.
h
2
: While it also has poor decor,
h
3
: it has excellent food quality.
As illustrated by the derivation of the clause for h
2
in Figure 17, the pre-
verbal modifier category for also in (40c) below takes a VP category
s
e,CUE
\np as its argument and returns a VP category as its result,
adding an additive relation to the semantics.
(40) a. also ` s
e,CUE
/
?
s
e,CUE
/
?
punc
,
:
@
e
(hModi(a ^ additive-rel ^ hArg1ie
1
))
b. also ` s
e,CUE
\
?
s
e,CUE
\
?
punc
,
:
@
e
(hModi(a ^ additive-rel ^ hArg1ie
1
))
c. also ` s
e,CUE
\np/
?
(s
e,CUE
\np) :
@
e
(hModi(a ^ additive-rel ^ hArg1ie
1
))
Since discourse adverbials such as also do not necessarily find their dis-
course arguments in structurally adjacent text segments, they do not
use cue threading. Instead, the cue value on discourse adverbials is left
underspecfied, as seen in all the lexical entries for also in (40). These
underspecified values then unify with the cue value of the input cat-
egory, threading any undischarged structural connectives through. In
this way, a discourse adverbial and a structural connective can appear
on the same clause (e.g. However, Bienvenue also has good decor).
In our example, the underspecified cue value of the argument cate-
gory in (40c) is unified with the nil cue value from the input category
Figure 4: DCCG derivation of a clause with
the discourse adverbial also (Nakatsu and White,
2010)
jkumar, 2009) in order to generate texts that vary
in size fro single sen ences to entire paragraphs.
In DCCG, the technique of cue threa i g is
used to all w structural connectives?including
paired ones such as on the one hand . . . on the
other hand?to project beyond the sentence level,
while allowing no more than one to be active at
a time. In this way, structural connectives can
be nested, as sketched in Figu e 3, but cannot
cross. In the figure, the value of the cue featur for
each text segment (ts) is shown (where ot1h and
otoh abbreviate on the one hand and on the other
h nd); th se cue values can be propagated through
a derivation, allowing the discourse relations to
project, but must be discharged (to nil) in a com-
plete d rivation, thereby nsuring that the intended
discourse relations are actually realized. By con-
trast, discourse adverbials introduce their relations
anaphorically and are transparent to cue thread-
ing, as sketched in Figure 4, making u of typical
adverb categories syntactically. See Nakatsu and
White (2010) for further details.
3 Crowd Sourcing Ratings
To collect human judgements from a diverse group
of speakers of US English, we used Amazon?s
Mechanical Turk service (AMT) to run two ex-
periments. In the first experiment, subjects rated
the naturalness of 174 passages used in Walker et
al.?s (2007) study. As detailed in Section 5, this
validation experiment confirmed that the judge-
ments collected on AMT correlate with those of
the raters in Walker et al?s (2007) study. Our sec-
ond experiment collected ratings on 300 passages
realized with modifications for better contrast ex-
pression (WITHMODS) and 300 passages without
these modifications (NOMODS), both realized us-
ing OpenCCG. While this does not admit a direct
comparison to the realizations produced by Walker
et al (2007), this controls for differences between
the generators other than the variable of interest:
the contrastive enhancements. In addition to these
materials, five passages from the SRC were seen
by all subjects to control for anomalous subject be-
havior.
3.1 Survey Format
Each survey used demographic questions to de-
termine the native speaker status of the subject.
Instructions for completing comprehension ques-
tions and rating realizations followed the demo-
graphic questions.
3
Each subject saw fifteen stim-
uli, each consisting of a sample user query and the
target passage as in Figure 5. After reading the
stimulus, the subject answered a yes-or-no com-
prehension question (see ?3.2). Finally the subject
rated the naturalness of the passage on a seven-
point Likert scale ranging from very unnatural to
very natural. At the survey?s conclusion, the sub-
ject could offer free-form feedback, explain their
r sponses, or ask questions of the researchers. The
average completion time across all experiments
was about ten minutes.
Passage selection is detailed in ?3.3 and ?3.4.
3.2 Quality Control
We used three strategies to filter out low-quality
responses from AMT subjects.
Comprehension Questions A template-based
yes-or-no question (exemplified in Figure 5) fol-
lowed each passage. Subjects who answered less
than 75% of these questions correctly were re-
jected and not paid, in accordance with the pro-
tocol approved by our human subjects review
board. Responses from three subjects were ex-
cluded from analysis on this basis.
Uniform Ratings When a subject gave the
same rating for all passages in a given survey (and
in disagreement with other subjects), we took this
to mean that the subject was paying attention only
3
These materials, along with the generated passages
and their ratings are available at http://www.ling.
ohio-state.edu/?mwhite/data/enlg13/.
33
Figure 5: Sample survey stimulus and comprehension question
Method # subjects excluded
Comprehension Questions 3
Uniform Answers 1
SAME5 0
Native Speaker Status 2
Table 1: Number of subjects excluded based on
quality control measures or native language.
to the comprehension questions that ensured pay-
ment. Only one subject was excluded on this ba-
sis, though they were still paid for answering the
comprehension questions correctly.
SAME5 Passages Five passages were chosen
from the original SRC realizations for which the
original ratings (from Walker et al 2007) were
identical for both judges. The passages were se-
lected such that the first and third authors of this
paper agreed with the general valence and rela-
tive rankings of the passages. That is, we took
two unambiguously bad realizations, two unam-
biguously good realizations, and one realization
near the middle of the spectrum to represent a gold
standard for rating to compare subjects against. If
any subject?s ratings on these five passages were
clear outliers, we could remove that subject?s data
for anomalous behavior, but this measure proved
unnecessary for the subjects in the present study.
3.3 Validating AMT
Data Selection In this experiment, we sampled
174 of the 1757 realizations from the SRC rated
by subjects A and B in Walker et al?s (2007) ex-
periment.
The SRC realizations were divided randomly
into two groups. Within one group, realizations
were labelled by subject A?s rating for that real-
ization. Subject B?s rating was used for the other
group. Taking the poles of the rating scale and its
midpoint, the realizations were further partitioned
into six sets: realizations rated 1, 3, and 5 by sub-
ject A and realizations rated 1, 3, and 5 by subject
B. This division of the data ensured that the re-
alizations used would cover the full spectrum of
ratings while being representative of the SRC rat-
ings with respect to, e.g., inter-annotator ratings
correlations.
From each of these six sets, we chose 10 COM-
PARE2, 10 COMPARE3, and 10 RECOMMEND re-
alizations,
4
each of these groups representing a
different realization task in the SRC. The COM-
PARE2 and COMPARE3 tasks involved the com-
parison of two restaurants or three or more restau-
rants, respectively. In the RECOMMEND context,
the sytem had to generate a recommendation for a
single restaurant.
Subject Demographics Thirty-six subjects re-
sponded to this survey initially, but one was re-
jected based on a failure to answer the compre-
hension questions and data from another had to be
excluded for non-native speaker status. Two addi-
tional subjects were recruited to replace their data.
This resulted in a subject pool with a mean age
(std. dev.) of 34.67 (9.35) years. Twenty-four sub-
jects identified as female and twelve identified as
male. Each subject received $2.50 for the survey,
estimated to take approximately 20 minutes.
3.4 Rating OpenCCG Realizations
Data Selection We selected 15 content plans
(CPs) from the SRC where the use of the con-
trastive modifiers was licensed: five COMPARE2,
five COMPARE3, and five RECOMMEND CPs.
Each of the 112 textplans (TPs) that produced
4
Except that subject A used the rating ?5? less than subject
B. To compensate, we used as many 5-point ratings as were
available from subject A and then filled in the remainder of
the 10 slots with realizations rated ?4?. We mirrored these
selections in the data from subject B for consistency.
34
the SRC realizations for these CPs was then pre-
processed for realization in OpenCCG both with
contrast enhancements (WITHMODS) and without
them (NOMODS).
Both structural choices and ordering choices
are encoded in these TPs.
5
Structural choices in-
clude decisions about how to group the restau-
rant properties to be expressed, such as deciding
whether to describe one restaurant in its entirety
and then the other (i.e. a serial structure) or al-
ternating between one restaurant and the other, di-
rectly contrasting particular attributes (i.e. a back-
and-forth structure). Ordering choices fixed the
order of presentation of restaurant attributes in se-
rial plans and the order of presentation of attribute
contrasts in back-and-forth plans. As discussed in
?6, there turn out to be interesting interactions be-
tween these aggregation choices and the contrast
enhancements, interactions which we did not ex-
plore directly in this experiment.
Processing each TP produced a different LF for
each possible combination of aggregation choices
and contrastive modifications, resulting in approx-
imately 41k logical forms (LFs) for the TPs WITH-
MODS and 88k LFs for the TPs with NOMODS.
6
Each realization received two language model
(LM) scores, one based on the semantic classes
used during realization (LM
SC
) and one based on
the Gigaword corpus (LM
GW
). LM
SC
used a tri-
gram model over modified texts based on the SRC
where specific entities (e.g. restaurant names like
Caffe Buon Gusto) were replaced with their se-
mantic class (e.g. RESTAURANT). The LM scores
were normalized by CP, such that the scores for a
given CP summed to 1 in each LM. These were
then linearly combined with weights slightly pre-
ferring the LM
SC
score to produce a combined
LM score for each realization.
Sampling then proceeded without replacement,
weighted by the combined LM score for each real-
ization. For the NOMODS sample, 20 realizations
were chosen this way, but, in the WITHMODS sam-
ple, a series of regular expression filters were used
to ensure adequate representation of the modifica-
tions in the surveys. These filters selected (without
5
This differs from Walker et al (2007), wherein reorder-
ings were allowed in mapping from tp-trees to sp-trees and
d-trees.
6
In future work we will explore a probabilistic rather than
exhaustive mapping algorithm to produce only LFs that are
more likely to result in more fluent realizations?not unlike
the weighted aggregation done by Walker et al?s (2007) sen-
tence plan generator.
replacement) 10 realizations such that every con-
trastive modification licensed by a particular CP
was represented, leaving 10 realizations to be se-
lected by weighted sampling without replacement.
This process resulted in 300 passages in each of
the two conditions (WITHMODS, NOMODS): 20
realizations for each of the 15 CPs. Each survey
included 5 realizations WITHMODS paired by CP
with 5 realizations with NOMODS as well as the
SAME5 realizations. As noted earlier, pairing real-
izations in this way helps to control for differences
in the variety of aggregation choices and surface
realizations used in the SRC as opposed to our
SRC-inspired grammar for OpenCCG.
Subject Demographics Sixty-eight subjects
responded to these 180 surveys initially. Subjects
were allowed to complete up to six distinct sur-
veys. One subject?s data was excluded for non-
native status and another?s was excluded on the
basis of uniform ratings (as detailed in ?3.2). To
compensate for the eight surveys completed by
these subjects and ten surveys mistakenly admin-
istered in draft format, we recollected data for 18
of the 180 surveys. This resulted in a final pool of
80 subjects with an average (std. dev.) age 37.15
(13.5) years. Forty identified as female, thirty-
nine identified as male, and one identified as non-
gendered.
Because subjects in the validation study com-
pleted the survey in about 10 minutes on average
with a standard deviation of about 5 minutes, we
scaled the pay to $2.00 per survey in this experi-
ment. Since subjects could participate in this ex-
periment multiple times, they could receive up to
$12.00 for their contribution.
4 Training a Text Ranker
To perform the ranking, we trained a basic n-
gram ranker using SVM
light
in preference ranking
mode.
7
We used the average ratings obtained in ?3
as target value.
The feature set was composed of 2 types of fea-
tures. The first feature type are the two language
model scores from ?3.4, LM
SC
and LM
GW
. The
second feature type consisted of n-gram counts.
We indexed the unigrams and bigrams in each cor-
pus and used each as a feature whose value was the
number of times it appeared in a given realization.
We trained the ranker on, and extracted n-gram
7
SVM
light
is an implementation of support vector ma-
chines by (Joachims, 2002).
35
12
3
4
5
6
7
1 2 3 4 5Average rating from Walker et al (2007)
Ave
rag
e ra
ting
 fro
m A
MT
 su
bjec
ts
Figure 6: Average ratings from our experiment
and Walker et al (2007), accompanied by a line
of best fit. Jitter (0.1) applied to each point mini-
mizes overlap.
features from, 3 different corpora drawn from the
data selection in ?3.4. The first corpus contains
299 selections WITHMODS (1 selection was dis-
carded for only being rated once), the second cor-
pus contains 300 selections with NOMODS, and
the third corpus contains BOTH of the first two cor-
pora combined.
To train and test the ranker, we performed 15-
fold cross-validation on each corpus. Within each
training fold, we had 14 training examples, corre-
sponding to 14 CPs. Each training example con-
sisted of all of a given CP?s realizations and their
ratings. After training, the realizations for the re-
maining CP were ranked.
In order to evaluate the ranker, we used the
TopRank metric (Walker et al, 2007). For each
of the ranked CP realization sets, we extracted the
target values (i.e. the average rating given by sub-
jects) of the highest ranked realization. We then
averaged the target scores of all of the top-ranked
realizations across the 15 training folds to produce
the Top Rank metric. The oracle best score is
the score of the highest rated realization, as de-
termined by the average score assigned to that re-
alization by the subjects.
5 Results
Validation Figure 6 shows the correlation be-
tween the average ratings of our subjects on AMT
and the average ratings assigned by subjects A and
B in Walker et al (2007). This correlation was
0.31 (p < 0.01, Kendall?s tau), while the corre-
lation between subjects A and B was only 0.28
BOTH WITHMODS NOMODS
human 6.61 (0.28) 6.46 (0.43) 6.49 (0.26)
bigram 6.00 (0.58) 5.62 (0.83) 5.51 (1.02)
Table 2: TopRank scores and standard deviations
for the oracle (human) & bigram (bigram) ranks.
(p < 0.01, Kendall?s tau). On this basis we con-
clude that using AMT workers as subjects to rate
sentences for their naturalness is at least as rea-
sonable as having two expert annotators labelling
realizations for their overall quality.
SAME5 Comparison There was no signifi-
cant difference (p = 0.16, using Welch?s t-test)
between the scores given to the SAME5 stimuli
in the two experiments,
8
indicating that subjects
used the rating scale similarly in both experiments.
The mean ratings for the rest of the validation re-
alizations was 5.31 (1.43) and the mean for the
OpenCCG-based realizations in the ranking exper-
iment was 4.96 (1.51), which is significantly lower
according to Welch?s t-test (p < 0.01). This high-
lights the underlying differences between the two
generation systems, validating our choice to use
OpenCCG for both the WITHMODS and NOMODS
realizations to better examine the impact of the
contrast enhancements.
Ranking Table 2 reports the oracle results,
along with our ranker?s results, using the TopRank
metric. Most indicative of the benefit of the con-
trastive enhancements is the performance of the
oracle score for the BOTH (6.61) condition com-
pared to the NOMODS condition (6.49), which is
significantly higher according to a paired t-test
(p = 0.01).
We also found that the bigram ranker with the
averaged raw ratings was better at predicting the
top rank of the combined (BOTH) corpus (6.00 vs.
oracle-best of 6.61) than either of the other two,
and better on the WITHMODS condition (5.62)
than on the NOMODS condition (5.51). However,
a two-tailed t-test revealed that the difference was
not quite signficant between BOTH and NOMODS
at the conventional level (p = 0.06), though the
p-value did meet the 0.1 threshold sometimes em-
ployed in small-scale experiments. The perfor-
mance of the different rankers, as compared to the
oracle scores, can be seen in Figure 7.
These preliminary results with a simple ranker
8
Validation experiment mean (std. dev.) 4.89 (1.79) ver-
sus 5.10 (1.75) in the ranking experiment.
36
34
5
6
7
human bigramMethod
Top
Ran
k
Corpus
both
withMods
noMods
Figure 7: TopRank scores for each of the rankers
with standard error bars.
are promising, motivating future work on improv-
ing the ranker in addition to enlarging the dataset.
6 Discussion
To assess the impact of the enhancement options,
we performed a linear regression between the
contrast-related patterns we used for data selec-
tion and the normalized ratings, with scikit-learn?s
implementation of the Bayesian Ridge method of
regularizing weights.
9
In looking at examples,
we found that the number of discourse adverbials
appeared to be a factor, so we then added these
counts as features. The coefficients and corpus
counts appear in Table 3. The results show that
the discourse adverbials were effective some of
the time, especially when used sparingly and in
conjunction with while. The ?heavier? contrastive
connectives however and on the one/other hand
were dispreferred, perhaps in part because they
ended up appearing too often with small, single-
restaurant contrasts, as there were relatively few
examples of summary statements, most of which
were somewhat disfluent due to a medial choice
for overall / on the whole.
Table 4 shows examples that illustrate both suc-
cesses and remaining issues. At the top, two pairs
of examples are given where the normalized av-
erage ratings are higher with the inclusion of just
and only, and where the rating drops off greatly
when however is used with a lesser value and no
adverbial of this kind, as expected. At the bottom,
the first example shows one instance where the use
of multiple adverbials is dispreferred. A possible
9http://scikit-learn.org/stable/
modules/linear_model.html
pattern coeff count
| disc advb | = 1 0.23 102
while 0.19 38
also has 0.13 47
has . . . too 0.12 39
has only 0.09 43
while . . .disc advb 0.09 16
contrastive . . . overall 0.07 8
has just 0.04 46
however . . .disc advb 0.03 4
but -0.03 20
, however , -0.05 10
only has -0.06 30
has merely -0.11 46
on the whole -0.14 33
just has -0.16 29
merely has -0.16 8
| disc advb | = 2 -0.18 32
. however , -0.21 64
on the other hand -0.21 40
| disc advb | >= 3 -0.27 50
overall -0.29 34
on the one hand -0.36 22
Table 3: Coefficients of linear regression between
contrast-related patterns and normalized ratings,
along with pattern counts, where disc adv is one
of just, only, merely, also, too and contrastive is
one of while, however, on the one/other hand
factor here may be that in addition to there being
several similar adverbials in a row, they all involve
long-distance antecedents, which may be difficult
to process. Finally, the last example shows a real-
ization that receives a relatively high rating despite
the use of two adverbials; note, however, that since
this passage uses a back-and-forth text plan, the
antecedents of the adverbials are all very local.
10
Turning to the survey feedback, many subjects
provided insightful comments regarding the task.
The most frequent comment pointed out that our
comprehension questions sometimes precipitated
a false implicature: when asked if a restaurant had
decent decor, subjects commented that they felt
that answering ?no? meant implying that it had
terrible decor. Similar problems occurred when a
restaurant had, e.g., very good decor and the sub-
jects were asked if it had good decor. Despite oc-
casional deviations from our intended exact-match
interpretation of these questions, no subjects were
excluded for scoring too low as a result of this.
10
As one reviewer points out, there?s also an interaction be-
tween how attributes are aggregated and the ability to express
contrast. For example, contrasting the attributes for which a
restaurant scores highly with those for which it scores poorly
requires the aggregation of attributes with like valence, as in
?This restaurant has superb decor and very good service but
only mediocre food quality.? Our future work on aggregation
will explore this interaction as well.
37
Strategy Mods? Rating Realization
C2 Y 1.13 Da Andrea?s price is 28 dollars. Gene?s?s price is 33 dollars. Da Andrea has very good
food quality while Gene?s has just good food quality.
C2 N 0.73 Da Andrea?s price is 28 dollars. Gene?s?s price is 33 dollars. Da Andrea has very good
food quality while Gene?s has good food quality.
C2 Y 1.04 Da Andrea?s price is 28 dollars. Gene?s?s price is 33 dollars. Da Andrea has very good
food quality. However, Gene?s has only good food quality.
C2 N -0.63 Da Andrea?s price is 28 dollars. Gene?s?s price is 33 dollars. Da Andrea has very good
food quality. However, Gene?s has good food quality.
C3 Y -1.85 Daniel and Jo Jo offer exceptional value among the selected restaurants. Daniel, on the
whole, is a superb restaurant. Daniel?s price is 82 dollars. Daniel has superb decor. It has
superb service and superb food quality. Jo Jo, overall, is an excellent restaurant. Jo Jo?s
price is 59 dollars. Jo Jo just has very good decor. It just has excellent service. It has
merely excellent food quality.
C2 Y 1.12 Japonica?s price is 37 dollars while Dojo?s price is 14 dollars. Japonica has excellent food
quality while Dojo has merely decent food quality. Japonica has decent decor. Dojo has
only mediocre decor.
Table 4: Examples illustrating successful and problematic contrast enhancements
In order to elicit rankings at a variety of points
on the naturalness scale, our selection included a
number of realizations with lower quality over-
all, which subjects picked up on. For example,
one subject commented that, ?Repeatedly using
the name of each restaurant over and over in sim-
ple sentences make[s] almost all of these excerpts
sound horrifyingly awkward,? while another ob-
served, ?The constant [use] of more sentences, in-
stead of using conjunction words . . . makes it seem
as if the system is rambling and lost in though[t]
process.?
Several subjects also pointed out that it would
be more natural to discuss the cost of an average
meal at a restaurant than to state that a restau-
rant?s price is some particular number of dollars.
Though these domain-specific lexical preferences
are tangential to the focus of this paper, they sug-
gest that exploring options to expand the range
of realizations for more naturally expressing these
properties might be a fruitful direction for future
work.
In addition to expressing an explicit prefer-
ence for serial rather than back-and-forth text-
plans, subjects also commented that higher level
contrastive adverbials like however work better
when they are used sparingly at a high level, rein-
forcing the findings in our regressions. We also re-
ceived suggestions for future work improving the
expression of contrast: some subjects suggested
that using better and worse to make explicit com-
parisons between restaurants would improve the
naturalness, and one subject suggested explicitly
stating which restaurant is (say) the cheapest as in
White et al (2010).
7 Conclusions and Future Work
In this paper, we have shown using ratings gath-
ered on AMT that Nakatsu & White?s (2010) pro-
posed enhancements to the SPaRKy Restaurant
Corpus (Walker et al, 2007) for better express-
ing contrast do indeed make it possible to generate
better texts, and an initial experiment suggested
that even a basic n-gram ranker can do so automat-
ically. A regression analysis further revealed that
while using a few discourse adverbials sparingly
was effective, using too many discourse adverbials
had a negative impact, with antecedent distance
potentially an important factor. In future work, we
plan to improve upon this basic n-gram ranker to
take these observations into account and validate
these initial findings on a larger dataset. In the pro-
cess we will explore the interaction between con-
trast expression and aggregation and seek to bet-
ter model the felicity conditions for ?weighty? top
level adverbials such as however.
Acknowledgments
This work was supported in part by NSF grant
IIS-1143635. Special thanks to the anonymous
reviewers, the Clippers computational linguistics
discussion group at Ohio State, and to Mark Dras,
Francois Lareau, and Yasaman Motazedi at Mac-
quarie University.
References
Brigitte Grote, Nils Lenke, and Manfred Stede. 1995.
Ma(r)king concessions in English and German. In
Proc. of the Fifth European Workshop on Natural
Language Generation.
38
Thorsten Joachims. 2002. Optimizing search engines
using clickthrough data. In Proc. KDD.
Irene Langkilde and Kevin Knight. 1998. The practi-
cal value of n-grams in generation. In Proc. INLG-
98.
William C. Mann and Sandra A. Thompson. 1988.
Rhetorical Structure Theory: Towards a functional
theory of text organization. TEXT, 8(3):243?281.
Chris Mellish, Alistair Knott, Jon Oberlander, and
Mick O?Donnell. 1998. Experiments using stochas-
tic search for text planning. In Proc. INLG-98.
Crystal Nakatsu and Michael White. 2006. Learning
to say it well: Reranking realizations by predicted
synthesis quality. In Proceedings of the 21st In-
ternational Conference on Computational Linguis-
tics and 44th Annual Meeting of the Association for
Computational Linguistics, pages 1113?1120, Syd-
ney, Australia, July. Association for Computational
Linguistics.
Crystal Nakatsu and Michael White. 2010. Generat-
ing with discourse combinatory categorial grammar.
Linguistic Issues in Language Technology, 4(1):1?
62.
Crystal Nakatsu. 2008. Learning contrastive connec-
tives in sentence realization ranking. In Proceedings
of the 9th SIGdial Workshop on Discourse and Dia-
logue, pages 76?79, Columbus, Ohio, June. Associ-
ation for Computational Linguistics.
Mark Steedman. 2000. The Syntactic Process. MIT
Press.
Kristina Striegnitz. 2004. Generating Anaphoric Ex-
pressions ? Contextual Inference in Sentence Plan-
ning. Ph.D. thesis, University of Saalandes & Uni-
versit de Nancy.
Marilyn A. Walker, Owen C. Rambow, and Monica Ro-
gati. 2002. Training a sentence planner for spo-
ken dialogue using boosting. Computer Speech and
Language, 16:409?433.
M. A. Walker, S. J. Whittaker, A. Stent, P. Mal-
oor, J. D. Moore, M. Johnston, and G Vasireddy.
2004. Generation and evaluation of user tailored re-
sponses in multimodal dialogue. Cognitive Science,
28(5):811?840.
M. Walker, A. Stent, F. Mairesse, and Rashmi Prasad.
2007. Individual and domain adaptation in sentence
planning for dialogue. Journal of Artificial Intelli-
gence Research (JAIR), 30:413?456.
Bonnie Webber, Matthew Stone, Aravind Joshi, and
Alistair Knott. 2003. Anaphora and discourse struc-
ture. Computational Linguistics, 29(4).
Bonnie Webber. 2004. D-LTAG: Extending lex-
icalized TAG to discourse. Cognitive Science,
28(5):751?779.
Michael White and Rajakrishnan Rajkumar. 2009.
Perceptron reranking for CCG realization. In Pro-
ceedings of the 2009 Conference on Empirical Meth-
ods in Natural Language Processing, pages 410?
419, Singapore, August. Association for Computa-
tional Linguistics.
Michael White, Robert A. J. Clark, and Johanna D.
Moore. 2010. Generating tailored, comparative de-
scriptions with contextually appropriate intonation.
Computational Linguistics, 36(2):159?201.
Michael White. 2006a. CCG chart realization from
disjunctive logical forms. In Proc. INLG-06.
Michael White. 2006b. Efficient Realization of Coor-
dinate Structures in Combinatory Categorial Gram-
mar. Research on Language and Computation,
4(1):39?75, June.
39
Proceedings of the 8th International Natural Language Generation Conference, pages 147?151,
Philadelphia, Pennsylvania, 19-21 June 2014. c?2014 Association for Computational Linguistics
Towards Surface Realization with CCGs Induced from Dependencies
Michael White
Department of Linguistics
The Ohio State University
Columbus, OH 43210, USA
mwhite@ling.osu.edu
Abstract
We present a novel algorithm for inducing
Combinatory Categorial Grammars from
dependency treebanks, along with initial
experiments showing that it can be used
to achieve competitive realization results
using an enhanced version of the surface
realization shared task data.
1 Introduction
In the first surface realization shared task (Belz
et al., 2011), no grammar-based systems achieved
competitive results, as input conversion turned
out to be more difficult than anticipated. Since
then, Narayan & Gardent (2012) have shown that
grammar-based systems can be substantially im-
proved with error mining techniques. In this pa-
per, inspired by recent work on converting depen-
dency treebanks (Ambati et al., 2013) and seman-
tic parsing (Kwiatkowksi et al., 2010; Artzi and
Zettlemoyer, 2013) with Combinatory Categorial
Grammar (CCG), we pursue the alternative strat-
egy of inducing a CCG from an enhanced version
of the shared task dependencies, with initial exper-
iments showing even better results.
A silver lining of the failure of grammar-based
systems in the shared task is that it revealed some
problems with the data. In particular, it became
evident that in cases where a constituent is an-
notated with multiple roles in the Penn Treebank
(PTB), the partial nature of Propbank annotation
and the restriction to syntactic dependency trees
meant that information was lost between the sur-
face and deep representations, leading grammar-
based systems to fail for good reason. For ex-
ample, Figure 1 shows that with free object rel-
atives, only one of the two roles played by how
much manufacturing strength is captured in the
deep representation, making it difficult to linearize
this phrase correctly. By contrast, Figure 2 (top)
	
	
	

	
 

	



