Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 927?936,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
A new Approach to Improving Multilingual Summarization using a
Genetic Algorithm
Marina Litvak
Ben-Gurion University
of the Negev
Beer Sheva, Israel
litvakm@bgu.ac.il
Mark Last
Ben-Gurion University
of the Negev
Beer Sheva, Israel
mlast@bgu.ac.il
Menahem Friedman
Ben-Gurion University
of the Negev
Beer Sheva, Israel
fmenahem@bgu.ac.il
Abstract
Automated summarization methods can
be defined as ?language-independent,? if
they are not based on any language-
specific knowledge. Such methods can
be used for multilingual summarization
defined by Mani (2001) as ?processing
several languages, with summary in the
same language as input.? In this pa-
per, we introduce MUSE, a language-
independent approach for extractive sum-
marization based on the linear optimiza-
tion of several sentence ranking measures
using a genetic algorithm. We tested our
methodology on two languages?English
and Hebrew?and evaluated its perfor-
mance with ROUGE-1 Recall vs. state-
of-the-art extractive summarization ap-
proaches. Our results show that MUSE
performs better than the best known multi-
lingual approach (TextRank1) in both lan-
guages. Moreover, our experimental re-
sults on a bilingual (English and Hebrew)
document collection suggest that MUSE
does not need to be retrained on each lan-
guage and the same model can be used
across at least two different languages.
1 Introduction
Document summaries should use a minimum
number of words to express a document?s main
ideas. As such, high quality summaries can sig-
nificantly reduce the information overload many
professionals in a variety of fields must contend
1We evaluated several summarizers?SUMMA, MEAD,
Microsoft Word Autosummarize and TextRank?on the DUC
2002 corpus. Our results show that TextRank performed
best. In addition, TextRank can be considered language-
independent as long as it does not perform any morphological
analysis.
with on a daily basis (Filippova et al, 2009), as-
sist in the automated classification and filtering of
documents, and increase search engines precision.
Automated summarization methods can
use different levels of linguistic analysis:
morphological, syntactic, semantic and dis-
course/pragmatic (Mani, 2001). Although the
summary quality is expected to improve when
a summarization technique includes language
specific knowledge, the inclusion of that knowl-
edge impedes the use of the summarizer on
multiple languages. Only systems that perform
equally well on different languages without
language-specific knowledge (including linguistic
analysis) can be considered language-independent
summarizers.
The publication of information on the Internet
in an ever-increasing variety of languages 2 dic-
tates the importance of developing multilingual
summarization approaches. There is a particu-
lar need for language-independent statistical tech-
niques that can be readily applied to text in any
language without depending on language-specific
linguistic tools. In the absence of such techniques,
the only alternative to language-independent sum-
marization would be the labor-intensive transla-
tion of the entire document into a common lan-
guage.
Here we introduce MUSE (MUltilingual Sen-
tence Extractor), a new approach to multilingual
single-document extractive summarization where
summarization is considered as an optimization or
a search problem. We use a Genetic Algorithm
(GA) to find an optimal weighted linear combina-
tion of 31 statistical sentence scoring methods that
are all language-independent and are based on ei-
ther a vector or a graph representation of a docu-
ment, where both representations are based on a
2 Gulli and Signorini (2005) used Web searches in 75 dif-
ferent languages to estimate the size of the Web as of the end
of January 2005.
927
word segmentation.
We have evaluated our approach on two mono-
lingual corpora of English and Hebrew documents
and, additionally, on one bilingual corpora com-
prising English and Hebrew documents. Our eval-
uation experiments sought to
- Compare the GA-based approach for single-
document extractive summarization (MUSE) to
the best known sentence scoring methods.
- Determine whether the same weighting model is
applicable across two different languages.
This paper is organized as follows. The next
section describes the related work in statistical
extractive summarization. Section 3 introduces
MUSE, the GA-based approach to multilingual
single-document extractive summarization. Sec-
tion 4 presents our experimental results on mono-
lingual and bilingual corpora. Our conclusions
and suggestions for future work comprise the fi-
nal section.
2 Related Work
Extractive summarization is aimed at the selec-
tion of a subset of the most relevant fragments
from a source text into the summary. The frag-
ments can be paragraphs (Salton et al, 1997), sen-
tences (Luhn, 1958), keyphrases (Turney, 2000)
or keywords (Litvak and Last, 2008). Statisti-
cal methods for calculating the relevance score
of each fragment can be categorized into sev-
eral classes: cue-based (Edmundson, 1969), key-
word- or frequency-based (Luhn, 1958; Edmund-
son, 1969; Neto et al, 2000; Steinberger and
Jezek, 2004; Kallel et al, 2004; Vanderwende et
al., 2007), title-based (Edmundson, 1969; Teufel
and Moens, 1997), position-based (Baxendale,
1958; Edmundson, 1969; Lin and Hovy, 1997;
Satoshi et al, 2001) and length-based (Satoshi et
al., 2001).
Considered the first work on sentence scoring
for automated text summarization, Luhn (1958)
based the significance factor of a sentence on the
frequency and the relative positions of signifi-
cant words within a sentence. Edmundson (1969)
tested different linear combinations of four sen-
tence ranking scoring methods?cue, key, title and
position?to identify that which performed best
on a training corpus. Linear combinations of sev-
eral statistical sentence ranking methods were also
applied in the MEAD (Radev et al, 2001) and
SUMMA (Saggion et al, 2003) approaches, both
of which use the vector space model for text repre-
sentation and a set of predefined or user-specified
weights for a combination of position, frequency,
title, and centroid-based (MEAD) features. Gold-
stein et al (1999) integrated linguistic and statisti-
cal features. In none of these works, however, did
the researchers attempt to find the optimal weights
for the best linear combination.
Information retrieval and machine learning
techniques were integrated to determine sentence
importance (Kupiec et al, 1995; Wong et al,
2008). Gong and Liu (2001) and Steinberger and
Jezek (2004) used singular value decomposition
(SVD) to generate extracts. Ishikawa et al (2002)
combined conventional sentence extraction and a
trainable classifier based on support vector ma-
chines.
Some authors reduced the summarization pro-
cess to an optimization or a search problem. Has-
sel and Sjobergh (2006) used a standard hill-
climbing algorithm to build summaries that max-
imize the score for the total impact of the sum-
mary. A summary consists of first sentences from
the document was used as a starting point for the
search, and all neighbours (summaries that can
be created by simply removing one sentence and
adding another) were examined, looking for a bet-
ter summary.
Kallel et al (2004) and Liu et al (2006b)
used genetic algorithms (GAs), which are known
as prominent search and optimization meth-
ods (Goldberg, 1989), to find sets of sentences that
maximize summary quality metrics, starting from
a random selection of sentences as the initial pop-
ulation. In this capacity, however, the high com-
putational complexity of GAs is a disadvantage.
To choose the best summary, multiple candidates
should be generated and evaluated for each docu-
ment (or document cluster).
Following a different approach, Turney (2000)
used a GA to learn an optimized set of parame-
ters for a keyword extractor embedded in the Ex-
tractor tool.3 Ora?san et al (2000) enhanced the
preference-based anaphora resolution algorithms
by using a GA to find an optimal set of values for
the outcomes of 14 indicators and apply the opti-
mal combination of values from data on one text
to a different text. With such approach, training
may be the only time-consuming phase in the op-
eration.
3http://www.extractor.com/
928
Today, graph-based text representations are be-
coming increasingly popular, due to their abil-
ity to enrich the document model with syntactic
and semantic relations. Salton et al (1997) were
among the first to make an attempt at using graph-
based ranking methods in single document ex-
tractive summarization, generating similarity links
between document paragraphs and using degree
scores in order to extract the important paragraphs
from the text. Erkan and Radev (2004) and Mi-
halcea (2005) introduced algorithms for unsuper-
vised extractive summarization that rely on the
application of iterative graph-based ranking algo-
rithms, such as PageRank (Brin and Page, 1998)
and HITS (Kleinberg, 1999). Their methods rep-
resent a document as a graph of sentences inter-
connected by similarity relations. Various sim-
ilarity functions can be applied: cosine similar-
ity as in (Erkan and Radev, 2004), simple over-
lap as in (Mihalcea, 2005), or other functions.
Edges representing the similarity relations can be
weighted (Mihalcea, 2005) or unweighted (Erkan
and Radev, 2004): two sentences are connected if
their similarity is above some predefined threshold
value.
3 MUSE ? MUltilingual Sentence
Extractor
In this paper we propose a learning approach
to language-independent extractive summariza-
tion where the best set of weights for a linear com-
bination of sentence scoring methods is found by
a genetic algorithm trained on a collection of doc-
ument summaries. The weighting vector thus ob-
tained is used for sentence scoring in future sum-
marizations. Since most sentence scoring methods
have a linear computational complexity, only the
training phase of our approach is time-consuming.
3.1 Sentence scoring methods
Our work is aimed at identifying the best linear
combination of the 31 sentence scoring methods
listed in Table 1. Each method description in-
cludes a reference to the original work where the
method was proposed for extractive summariza-
tion. Methods proposed in this paper are denoted
by new. Formulas incorporate the following nota-
tion: a sentence is denoted by S, a text document
by D, the total number of words in S by N , the to-
tal number of sentences in D by n, the sequential
number of S in D by i, and the in-document term
frequency of the term t by tf(t). In the LUHN
method, Wi and Ni are the number of keywords
and the total number of words in the ith cluster, re-
spectively, such that clusters are portions of a sen-
tence bracketed by keywords, i.e., frequent, non-
common words.4
Figure 1 demonstrates the taxonomy of the
methods listed in Table 1. Methods that require
pre-defined threshold values are marked with a
cross and listed in Table 2 together with the aver-
age threshold values obtained after method eval-
uation on English and Hebrew corpora. Each
method was evaluated on both corpora, with dif-
ferent threshold t ? [0, 1] (only numbers with one
decimal digit were considered). Threshold val-
ues resulted in the best ROUGE-1 scores, were
selected. A threshold of 1 means that all terms
are considered, while a value of 0 means that
only terms with the highest rank (tf, degree, or
pagerank) are considered. The methods are di-
vided into three main categories?structure-, vec-
tor-, and graph-based?according to the text rep-
resentation model, and each category is divided
into sub-categories.
Section 3.3 describes our application of a GA to
the summarization task.
Table 2: Selected thresholds for threshold-based
scoring methods
Method Threshold
LUHN 0.9
LUHN DEG 0.9
LUHN PR 0.0
KEY [0.8, 1.0]
KEY DEG [0.8, 1.0]
KEY PR [0.1, 1.0]
COV 0.9
COV DEG [0.7, 0.9]
COV PR 0.1
3.2 Text representation models
The vector-based scoring methods listed in Ta-
ble 1 use tf or tf-idf term weights to evaluate
sentence importance. In contrast, representation
used by the graph-based methods (except for Tex-
tRank) is based on the word-based graph represen-
tation models described in (Schenker et al, 2004).
Schenker et al (2005) showed that such graph
representations can outperform the vector space
model on several document categorization tasks.
In the graph representation used by us in this work
4Luhn?s experiments suggest an optimal limit of 4 or 5
non-significant words between keywords.
929
Table 1: Sentence scoring metrics
Name Description Source
POS F Closeness to the beginning of the document: 1i (Edmundson, 1969)
POS L Closeness to the end of the document: i (Baxendale, 1958)
POS B Closeness to the borders of the document: max( 1i , 1n?i+1 ) (Lin and Hovy, 1997)
LEN W Number of words in the sentence (Satoshi et al, 2001)
LEN CH Number of characters in the sentence5
LUHN maxi?{clusters(S)}{CSi}, CSi = W
2
i
Ni (Luhn, 1958)
KEY Sum of the keywords frequencies:
?
t?{Keywords(S)} tf(t) (Edmundson, 1969)
COV Ratio of keywords number (Coverage): |Keywords(S)||Keywords(D)| (Liu et al, 2006a)
TF Average term frequency for all sentence words:
?
t?S
tf(t)
N (Vanderwende et al, 2007)
TFISF
?
t?S tf(t) ? isf(t), isf(t) = 1 ?
log(n(t))
log(n) , (Neto et al, 2000)
n(t) is the number of sentences containing t
SVD Length of a sentence vector in ?2 ? V T after computing Singular Value (Steinberger and Jezek, 2004)
Decomposition of a term by sentences matrix A = U?V T
TITLE O Overlap similarity6 to the title: sim(S, T ) = |S?T |min{|S|,|T |} (Edmundson, 1969)
TITLE J Jaccard similarity to the title: sim(S, T ) = |S?T ||S?T |
TITLE C Cosine similarity to the title: sim(~S, ~T ) = cos(~S, ~T ) = ~S?~T|~S|?|~T |
D COV O Overlap similarity to the document complement new
sim(S,D ? S) = |S?T |min{|S|,|D?S|}
D COV J Jaccard similarity to the document complement sim(S,D ? S) = |S?T ||S?D?S|
D COV C Cosine similarity to the document complement cos(~S, ~D ? S) = ~S? ~D?S|~S|?| ~D?S|
LUHN DEG Graph-based extensions of LUHN, KEY and COV measures respectively.
KEY DEG Node degree is used instead of a word frequency: words are considered
COV DEG significant if they are represented by nodes having a degree higher
than a predefined threshold
DEG Average degree for all sentence nodes:
?
i?{words(S)}
Degi
N
GRASE Frequent sentences from bushy paths are selected. Each sentence in the bushy
path gets a domination score that is the number of edges with its label in the
path normalized by the sentence length. The relevance score for a sentence
is calculated as a sum of its domination scores over all paths.
LUHN PR Graph-based extensions of LUHN, KEY and COV measures respectively.
KEY PR Node PageRank score is used instead of a word frequency: words are considered
COV PR significant if they are represented by nodes having a PageRank score higher
than a predefined threshold
PR Average PageRank for all sentence nodes:
?
t?S
PR(t)
N
TITLE E O Overlap-based edge matching between title and sentence graphs
TITLE E J Jaccard-based edge matching between title and sentence graphs
D COV E O Overlap-based edge matching between sentence and a document complement
graphs
D COV E J Jaccard-based edge matching between sentence and a document complement
graphs
ML TR Multilingual version of TextRank without morphological analysis: (Mihalcea, 2005)
Sentence score equals to PageRank (Brin and Page, 1998) rank of its node:
WS(Vi) = (1 ? d) + d ?
?
Vj?In(Vi)
wji
?
Vk?Out(Vj)
wjk
WS(Vj)
nodes represent unique terms (distinct words) and
edges represent order-relationships between two
terms. There is a directed edge from A to B if an A
term immediately precedes the B term in any sen-
tence of the document. We label each edge with
the IDs of sentences that contain both words in the
specified order.
3.3 Optimization?learning the best linear
combination
We found the best linear combination of the meth-
ods listed in Table 1 using a Genetic Algorithm
(GA). GAs are categorized as global search heuris-
tics. Figure 2 shows a simplified GA flowchart.
A typical genetic algorithm requires (1) a genetic
representation of the solution domain, and (2) a
fitness function to evaluate the solution domain.
We represent the solution as a vector of weights
930
Language-independent sentence
scoringmethods
Structure-
based
Vector-
based
Graph-
based
Position Length Frequency Similarity Degree SimilarityPagerank
Title Document
POS_F
POS_L
POS_B
LEN_W
LEN_CH
LUHN
KEY
COV
TF
TFIISF
SVD
TITLE_O
TITLE_J
TITLE_C
D_COV_O*
D_COV_J*
D_COV_C*
LUHN_DEG*
KEY_DEG*
COV_DEG*
DEG*
GRASE*
LUHN_PR*
KEY_PR*
COV_PR*
PR*
ML_TR
Title Document
TITLE_E_O*
TITLE_E_J*
D_COV_E_O*
D_COV_E_J*
Figure 1: Taxonomy of language-independent sentence scoring methods
Selection
Mating
Crossover
Mutation
Terminate?
Best
gene
yes
no
Initialization
Re
pr
od
uc
tio
n
Figure 2: Simplified flowchart of a Genetic Algo-
rithm
for a linear combination of sentence scoring
methods?real-valued numbers in the unlimited
range normalized in such a way that they sum up
to 1. The vector size is fixed and it equals to the
number of methods used in the combination.
Defined over the genetic representation, the fit-
ness function measures the quality of the repre-
sented solution. We use ROUGE-1 Recall (Lin
and Hovy, 2003) as a fitness function for mea-
suring summarization quality, which is maximized
during the optimization procedure.
Below we describe each phase of the optimiza-
tion procedure in detail.
Initialization GA will explore only a small part
of the search space, if the population is too small,
whereas it slows down if there are too many solu-
tions. We start from N = 500 randomly gener-
ated genes/solutions as an initial population, that
empirically was proven as a good choice. Each
gene is represented by a weighting vector vi =
w1, . . . , wD having a fixed number of D ? 31 ele-
ments. All elements are generated from a standard
normal distribution, with ? = 0 and ?2 = 1, and
normalized to sum up to 1. For this solution rep-
resentation, a negative weight, if it occurs, can be
considered as a ?penalty? for the associated met-
ric.
Selection During each successive generation, a
proportion of the existing population is selected to
breed a new generation. We use a truncation se-
lection method that rates the fitness of each so-
lution and selects the best fifth (100 out of 500)
of the individual solutions, i.e., getting the maxi-
mal ROUGE value. In such manner, we discard
?bad? solutions and prevent them from reproduc-
tion. Also, we use elitism?method that prevents
losing the best found solution in the population by
copying it to the next generation.
Reproduction In this stage, new
genes/solutions are introduced into the popu-
lation, i.e., new points in the search space are
explored. These new solutions are generated
from those selected through the following genetic
operators: mating, crossover, and mutation.
In mating, a pair of ?parent? solutions is ran-
domly selected, and a new solution is created us-
ing crossover and mutation, that are the most im-
portant part of a genetic algorithm. The GA per-
formance is influenced mainly by these two opera-
tors. New parents are selected for each new child,
and the process continues until a new population
of solutions of appropriate size N is generated.
Crossover is performed under the assumption
931
that new solutions can be improved by re-using
the good parts of old solutions. However it is
good to keep some part of population from one
generation to the next. Our crossover operator in-
cludes a probability (80%) that a new and different
offspring solution will be generated by calculat-
ing the weighted average of two ?parent? vectors
according to (Vignaux and Michalewicz, 1991).
Formally, a new vector v will be created from
two vectors v1 and v2 according to the formula
v = ? ? v1 + (1? ?) ? v2 (we set ? = 0.5). There
is a probability of 20% that the offspring will be a
duplicate of one of its parents.
Mutation in GAs functions both to preserve the
existing diversity and to introduce new variation.
It is aimed at preventing GA from falling into lo-
cal extreme, but it should not be applied too often,
because then GA will in fact change to random
search. Our mutation operator includes a proba-
bility (3%) that an arbitrary weight in a vector will
be changed by a uniformly randomized factor in
the range of [?0.3, 0.3] from its original value.
Termination The generational process is re-
peated until a termination condition?a plateau of
solution/combination fitness such that successive
iterations no longer produce better results?has
been reached. The minimal improvement in our
experiments was set to ? = 1.0E ? 21.
4 Experiments
4.1 Overview
The MUSE summarization approach was eval-
uated using a comparative experiment on two
monolingual corpora of English and Hebrew texts
and on a bilingual corpus of texts in both lan-
guages. We intentionally chose English and He-
brew, which belong to distinct language families
(Indo-European and Semitic languages, respect-
fully), to ensure that the results of our evaluation
would be widely generalizable. The specific goals
of the experiment are to:
- Evaluate the optimal sentence scoring models in-
duced from the corpora of summarized documents
in two different languages.
- Compare the performance of the GA-based mul-
tilingual summarization method proposed in this
work to the state-of-the-art approaches.
- Compare method performance on both lan-
guages.
- Determine whether the same sentence scoring
model can be efficiently used for extractive sum-
marization across two different languages.
4.2 Text preprocessing
Crucial to extractive summarization, proper sen-
tence segmentation contributes to the quality of
summarization results. For English sentences,
we used the sentence splitter provided with the
MEAD summarizer (Radev et al, 2001). A sim-
ple splitter that can split the text at periods, excla-
mation points, or question marks was used for the
Hebrew text.7
4.3 Experiment design
The English text material we used in our experi-
ments comprised the corpus of summarized doc-
uments available to the single document summa-
rization task at the Document Understanding Con-
ference, 2002 (DUC, 2002). This benchmark
dataset contains 533 news articles, each accompa-
nied by two to three human-generated abstracts of
approximately 100 words each.
For the Hebrew language, however, to the best
of our knowledge, no summarization benchmarks
exist. To generate a corpus of summarized Hebrew
texts, therefore, we set up an experiment where
human assessors were given 50 news articles of
250 to 830 words each from the Website of the
Haaretz newspaper.8 All assessors were provided
with the Tool Assisting Human Assessors (TAHA)
software tool9 that enables sentences to be easily
selected and stored for later inclusion in the doc-
ument extract. In total, 70 undergraduate students
from the Department of Information Systems En-
gineering, Ben Gurion University of the Negev
participated in the experiment. Each student par-
ticipant was randomly assigned ten different doc-
uments and instructed to (1) spend at least five
minutes on each document, (2) ignore dialogs and
quotations, (3) read the whole document before
beginning sentence extraction, (4) ignore redun-
dant, repetitive, and overly detailed information,
and (5) remain within the minimal and maximal
summary length constraints (95 and 100 words, re-
spectively). Summaries were assessed for quality
by comparing each student?s summary to those of
all the other students using the ROUGE evalua-
7Although the same set of splitting rules may be used for
many different languages, separate splitters were used for En-
glish and Hebrew because the MEAD splitter tool is restricted
to European languages.
8http://www.haaretz.co.il
9TAHA can be provided upon request
932
tion toolkit adapted to Hebrew10 and the ROUGE-
1 metric (Lin and Hovy, 2003). We filtered all the
summaries produced by assessors that received av-
erage ROUGE score below 0.5, i. e. agreed with
the rest of assessors in less than 50% of cases.
Finally, our corpus of summarized Hebrew texts
was compiled from the summaries of about 60%
of the most consistent assessors, with an aver-
age of seven extracts per single document11. The
ROUGE scores of the selected assessors are dis-
tributed between 50 and 57 percents.
The third, bilingual, experimental corpus was
assembled from documents in both languages.
4.4 Experimental Results
We evaluated English and Hebrew summaries us-
ing ROUGE-1, 2, 3, 4, L, SU and W metrics, de-
scribed in (2004). In agreement with Lin?s (2004)
conclusion, our results for the different metrics
were not statistically distinguishable. However,
ROUGE-1 showed the largest variation across the
methods. In the following comparisons, all results
are presented in terms of the ROUGE-1 Recall
metric.
We estimated the ROUGE metric using 10-fold
cross validation. The results of training and testing
comprise the average ROUGE values obtained for
English, Hebrew, and bilingual corpora (Table 3).
Since we experimented with a different number of
English and Hebrew documents (533 and 50, re-
spectively), we have created 10 balanced bilingual
corpora, each with the same number of English
and Hebrew documents, by combining approxi-
mately 50 randomly selected English documents
with all 50 Hebrew documents. Each corpus was
then subjected to 10-fold cross validation, and the
average results for training and testing were calcu-
lated.
We compared our approach (1) with a
multilingual version of TextRank (denoted by
ML TR) (Mihalcea, 2005) as the best known
multilingual summarizer, (2) with Microsoft
Word?s Autosummarize function12 (denoted by
MS SUM) as a widely used commercial summa-
10The regular expressions specifying ?word? were adapted
to Hebrew alphabet. The same toolkit was used for sum-
maries evaluation on Hebrew corpus.
11Dataset is available at http://www.cs.bgu.ac.
il/
?
litvakm/research/
12We reported the following bug to Microsoft: Microsoft
Word?s Document.Autosummarize Method returns different
results from the output of the AutoSummarize Dialog Box.
In our experiments, the Method results were used.
rizer, and (3) with the best single scoring method
in each corpus. As a baseline, we compiled sum-
maries created from the initial sentences (denoted
by POS F). Table 4 shows the comparative re-
sults (ROUGE mean values) for English, Hebrew,
and bilingual corpora, with the best summarizers
on top. Pairwise comparisons between summa-
rizers indicated that all methods (except POS F
and ML TR in the English and bilingual corpora
and D COV J and POS F in the Hebrew corpus)
were significantly different at the 95% confidence
level. MUSE performed significantly better than
TextRank in all three corpora and better than the
best single methods COV DEG in English and
D COV J in Hebrew corpora respectively.
Two sets of features?the full set of 31 sen-
tence scoring metrics and the 10 best bilingual
metrics determined in our previous work13 using
a clustering analysis of the methods results on
both corpora?were tested on the bilingual corpus.
The experimental results show that the optimized
combination of the 10 best metrics is not signif-
icantly distinguishable from the best single met-
ric in the multilingual corpus ? COV DEG. The
difference between the combination of all 31 met-
rics and COV DEG is significant only with a one-
tailed p-value of 0.0798 (considered not very sig-
nificant). Both combinations significantly outper-
formed all the other summarizers that were com-
pared. Table 4 contains the results of MUSE-
trained weights for all 31 metrics.
Our experiments showed that the removal of
highly-correlated metrics (the metric with the
lower ROUGE value out of each pair of highly-
correlated metrics) from the linear combination
slightly improved summarization quality, but the
improvement was not statistically significant. Dis-
carding bottom ranked features (up to 50%), also,
did not affect the results significantly.
Table 5 shows the best vectors generated from
training MUSE on all the documents in the En-
glish, Hebrew, and multilingual (one of 10 bal-
anced) corpora and their ROUGE training scores
and number of GA iterations.
While the optimal values of the weights are ex-
pected to be nonnegative, among the actual re-
sults are some negative values. Although there
is no simple explanation for this outcome, it may
be related to a well-known phenomenon from Nu-
merical Analysis called over-relaxation (Friedman
13submitted to publication
933
and Kandel, 1994). For example, Laplace equa-
tion ?xx + ?yy = 0 is iteratively solved over a
grid of points as follows: At each grid point let
?(n), ?(n) denote the nth iteration as calculated
from the differential equation and its modified fi-
nal value, respectively. The final value is chosen
as ??(n) + (1 ? ?)?(n?1). While the sum of the
two weights is obviously 1, the optimal value of ?,
which minimizes the number of iterations needed
for convergence, usually satisfies 1 < ? < 2
(i.e., the second weight 1? ? is negative) and ap-
proaches 2 the finer the grid gets. Though some-
what unexpected, this surprising result can be rig-
orously proved (Varga, 1962).
Table 3: Results of 10-fold cross validation
ENG HEB MULT
Train 0.4483 0.5993 0.5205
Test 0.4461 0.5936 0.5027
Table 4: Summarization performance. Mean
ROUGE-1
Metric ENG HEB MULT
MUSE 0.4461 0.5921 0.4633
COV DEG 0.4363 0.5679 0.4588
D COV J 0.4251 0.5748 0.4512
POS F 0.4190 0.5678 0.4440
ML TR 0.4138 0.5190 0.4288
MS SUM 0.3097 0.4114 0.3184
Assuming efficient implementation, most met-
rics have a linear computational complexity rela-
tive to the total number of words in a document
- O(n). As a result, MUSE total computation
time, given a trained model, is also linear (at fac-
tor of the number of metrics in a combination).
The training time is proportional to the number of
GA iterations multiplied by the number of indi-
viduals in a population times the fitness evaluation
(ROUGE) time. On average, in our experiments
the GA performed 5? 6 iterations?selection and
reproduction?before reaching convergence.
5 Conclusions and future work
In this paper we introduced MUSE, a new, GA-
based approach to multilingual extractive sum-
marization. We evaluated the proposed method-
ology on two languages from different language
families: English and Hebrew. The experimen-
tal results showed that MUSE significantly out-
performed TextRank, the best known language-
Table 5: Induced weights for the best linear com-
bination of scoring metrics
Metric ENG HEB MULT
COV DEG 8.490 0.171 0.697
KEY DEG 15.774 0.218 -2.108
KEY 4.734 0.471 0.346
COV PR -4.349 0.241 -0.462
COV 10.016 -0.112 0.865
D COV C -9.499 -0.163 1.112
D COV J 11.337 0.710 2.814
KEY PR 0.757 0.029 -0.326
LUHN DEG 6.970 0.211 0.113
POS F 6.875 0.490 0.255
LEN CH 1.333 -0.002 0.214
LUHN -2.253 -0.060 0.411
LUHN PR 1.878 -0.273 -2.335
LEN W -13.204 -0.006 1.596
ML TR 8.493 0.340 1.549
TITLE E J -5.551 -0.060 -1.210
TITLE E O -21.833 0.074 -1.537
D COV E J 1.629 0.302 0.196
D COV O 5.531 -0.475 0.431
TFISF -0.333 -0.503 0.232
DEG 3.584 -0.218 0.059
D COV E O 8.557 -0.130 -1.071
PR 5.891 -0.639 1.793
TITLE J -7.551 0.071 1.445
TF 0.810 0.202 -0.650
TITLE O -11.996 0.179 -0.634
SVD -0.557 0.137 0.384
TITLE C 5.536 -0.029 0.933
POS B -5.350 0.347 1.074
GRASE -2.197 -0.116 -1.655
POS L -22.521 -0.408 -3.531
Score 0.4549 0.6019 0.526
Iterations 10 6 7
independent approach, in both Hebrew and En-
glish using either monolingual or bilingual cor-
pora. Moreover, our results suggest that the same
weighting model is applicable across multiple lan-
guages. In future work, one may:
- Evaluate MUSE on additional languages and lan-
guage families.
- Incorporate threshold values for threshold-based
methods (Table 2) into the GA-based optimization
procedure.
- Improve performance of similarity-based metrics
in the multilingual domain.
- Apply additional optimization techniques like
Evolution Strategy (Beyer and Schwefel, 2002),
which is known to perform well in a real-valued
search space.
- Extend the search for the best summary to the
problem of multi-object optimization, combining
several summary quality metrics.
934
Acknowledgments
We are grateful to Michael Elhadad and Galina
Volk from Ben-Gurion University for providing
the ROUGE toolkit adapted to the Hebrew alpha-
bet, and to Slava Kisilevich from the University
of Konstanz for the technical support in evaluation
experiments.
References
P. B. Baxendale. 1958. Machine-made index for tech-
nical literaturean experiment. IBM Journal of Re-
search and Development, 2(4):354?361.
H.-G. Beyer and H.-P. Schwefel. 2002. Evolution
strategies: A comprehensive introduction. Journal
Natural Computing, 1(1):3?52.
S. Brin and L. Page. 1998. The anatomy of a large-
scale hypertextual web search engine. Computer
networks and ISDN systems, 30(1-7):107?117.
DUC. 2002. Document understanding conference.
http://duc.nist.gov.
H. P. Edmundson. 1969. New methods in automatic
extracting. ACM, 16(2).
G. Erkan and D. R. Radev. 2004. Lexrank: Graph-
based lexical centrality as salience in text summa-
rization. Journal of Artificial Intelligence Research,
22:457?479.
K. Filippova, M. Surdeanu, M. Ciaramita, and
H. Zaragoza. 2009. Company-oriented extractive
summarization of financial news. In Proceedings
of the 12th Conference of the European Chapter
of the Association for Computational Linguistics,
pages 246?254.
M. Friedman and A. Kandel. 1994. Fundamentals of
Computer Numerical Analysis. CRC Press.
D. E. Goldberg. 1989. Genetic algorithms in search,
optimization and machine learning. Addison-
Wesley.
J. Goldstein, M. Kantrowitz, V. Mittal, and J. Car-
bonell. 1999. Summarizing text documents: Sen-
tence selection and evaluation metrics. In Proceed-
ings of the 22nd Annual International ACM SIGIR
Conference on Research and Development in Infor-
mation Retrieval, pages 121?128.
Y. Gong and X. Liu. 2001. Generic text summarization
using relevance measure and latent semantic analy-
sis. In Proceedings of the 24th ACM SIGIR confer-
ence on Research and development in information
retrieval, pages 19?25.
A. Gulli and A. Signorini. 2005. The indexable web is
more than 11.5 billion pages. http://www.cs.
uiowa.edu/
?
asignori/web-size/.
M. Hassel and J. Sjobergh. 2006. Towards holistic
summarization: Selecting summaries, not sentences.
In Proceedings of Language Resources and Evalua-
tion.
K. Ishikawa, S-I. ANDO, S-I. Doi, and A. Okumura.
2002. Trainable automatic text summarization using
segmentation of sentence. In Proceedings of 2002
NTCIR 3 TSC workshop.
F. J. Kallel, M. Jaoua, L. B. Hadrich, and A. Ben
Hamadou. 2004. Summarization at laris labora-
tory. In Proceedings of the Document Understand-
ing Conference.
J.M. Kleinberg. 1999. Authoritative sources in a
hyperlinked environment. Journal of the ACM
(JACM), 46(5):604?632.
J. Kupiec, J. Pedersen, and F Chen. 1995. A trainable
document summarizer. In Proceedings of the 18th
annual international ACM SIGIR conference, pages
68?73.
C.Y. Lin and E. Hovy. 1997. Identifying topics by po-
sition. In Proceedings of the fifth conference on Ap-
plied natural language processing, pages 283?290.
Chin-Yew Lin and Eduard Hovy. 2003. Auto-
matic evaluation of summaries using n-gram co-
occurrence statistics. In NAACL ?03: Proceedings of
the 2003 Conference of the North American Chapter
of the Association for Computational Linguistics on
Human Language Technology, pages 71?78.
Chin-Yew Lin. 2004. Rouge: A package for auto-
matic evaluation of summaries. In Proceedings of
the Workshop on Text Summarization Branches Out
(WAS 2004), pages 25?26.
M. Litvak and M. Last. 2008. Graph-based keyword
extraction for single-document summarization. In
Proceedings of the Workshop on Multi-source Multi-
lingual Information Extraction and Summarization,
pages 17?24.
D. Liu, Y. He, D. Ji, and H. Yang. 2006a. Genetic al-
gorithm based multi-document summarization. Lec-
ture Notes in Computer Science, 4099:1140.
D. Liu, Y. Wang, C. Liu, and Z. Wang. 2006b. Mul-
tiple documents summarization based on genetic
algorithm. Lecture Notes in Computer Science,
4223:355.
H. P. Luhn. 1958. The automatic creation of literature
abstracts. IBM Journal of Research and Develop-
ment, 2:159?165.
Inderjeet Mani. 2001. Automatic Summarization. Nat-
ural Language Processing, John Benjamins Publish-
ing Company.
Rada Mihalcea. 2005. Language independent extrac-
tive summarization. In AAAI?05: Proceedings of the
20th national conference on Artificial intelligence,
pages 1688?1689.
935
J.L. Neto, A.D. Santos, C.A.A. Kaestner, and A.A. Fre-
itas. 2000. Generating text summaries through the
relative importance of topics. Lecture Notes in Com-
puter Science, pages 300?309.
Constantin Ora?san, Richard Evans, and Ruslan Mitkov.
2000. Enhancing preference-based anaphora res-
olution with genetic algorithms. In Dimitris
Christodoulakis, editor, Proceedings of the Second
International Conference on Natural Language Pro-
cessing, volume 1835, pages 185 ? 195, Patras,
Greece, June 2? 4. Springer.
Dragomir Radev, Sasha Blair-Goldensohn, and Zhu
Zhang. 2001. Experiments in single and multidoc-
ument summarization using mead. First Document
Understanding Conference.
Horacio Saggion, Kalina Bontcheva, and Hamish Cun-
ningham. 2003. Robust generic and query-based
summarisation. In EACL ?03: Proceedings of the
tenth conference on European chapter of the Associ-
ation for Computational Linguistics.
G. Salton, A. Singhal, M. Mitra, and C. Buckley. 1997.
Automatic text structuring and summarization. In-
formation Processing and Management, 33(2):193?
207.
C. N. Satoshi, S. Satoshi, M. Murata, K. Uchimoto,
M. Utiyama, and H. Isahara. 2001. Sentence ex-
traction system assembling multiple evidence. In
Proceedings of 2nd NTCIR Workshop, pages 319?
324.
A. Schenker, H. Bunke, M. Last, and A. Kandel. 2004.
Classification of web documents using graph match-
ing. International Journal of Pattern Recognition
and Artificial Intelligence, 18(3):475?496.
A. Schenker, H. Bunke, M. Last, and A. Kandel. 2005.
Graph-theoretic techniques for web content mining.
J. Steinberger and K. Jezek. 2004. Text summarization
and singular value decomposition. Lecture Notes in
Computer Science, pages 245?254.
S. Teufel and M. Moens. 1997. Sentence extraction as
a classification task. In Proceedings of the Workshop
on Intelligent Scalable Summarization, ACL/EACL
Conference, pages 58?65.
Peter D. Turney. 2000. Learning algorithms
for keyphrase extraction. Information Retrieval,
2(4):303?336.
L. Vanderwende, H. Suzuki, C. Brockett, and
A. Nenkova. 2007. Beyond sumbasic: Task-
focused summarization with sentence simplification
and lexical expansion. Information processing and
management, 43(6):1606?1618.
R.S. Varga. 1962. Matrix Iterative Methods. Prentice-
Hall.
G. A. Vignaux and Z. Michalewicz. 1991. A ge-
netic algorithm for the linear transportation problem.
IEEE Transactions on Systems, Man and Cybernet-
ics, 21:445?452.
K.F. Wong, M. Wu, and W. Li. 2008. Extractive sum-
marization using supervised and semi-supervised
learning. In Proceedings of the 22nd International
Conference on Computational Linguistics-Volume 1,
pages 985?992.
936
Coling 2008: Proceedings of the workshop on Multi-source Multilingual Information Extraction and Summarization, pages 17?24
Manchester, August 2008
Graph-Based Keyword Extraction for Single-Document Summarization
Marina Litvak
Department of
Information System Engineering
Ben-Gurion University of the Negev
Beer-Sheva 84105, Israel
litvakm@bgu.ac.il
Mark Last
Department of
Information System Engineering
Ben-Gurion University of the Negev
Beer-Sheva 84105, Israel
mlast@bgu.ac.il
Abstract
In this paper, we introduce and compare
between two novel approaches, supervised
and unsupervised, for identifying the key-
words to be used in extractive summa-
rization of text documents. Both our ap-
proaches are based on the graph-based
syntactic representation of text and web
documents, which enhances the traditional
vector-space model by taking into account
some structural document features. In the
supervised approach, we train classifica-
tion algorithms on a summarized collec-
tion of documents with the purpose of
inducing a keyword identification model.
In the unsupervised approach, we run the
HITS algorithm on document graphs under
the assumption that the top-ranked nodes
should represent the document keywords.
Our experiments on a collection of bench-
mark summaries show that given a set of
summarized training documents, the su-
pervised classification provides the highest
keyword identification accuracy, while the
highest F-measure is reached with a sim-
ple degree-based ranking. In addition, it is
sufficient to perform only the first iteration
of HITS rather than running it to its con-
vergence.
1 Introduction
Document summarization is aimed at all types of
electronic documents including HTML files with
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
the purpose of generating the summary - main doc-
ument information expressed in ?a few words?.
In this paper, we introduce and compare be-
tween two approaches: supervised and unsuper-
vised, for the cross-lingual keyword extraction to
be used as the first step in extractive summarization
of text documents. Thus, according to our problem
statement, the keyword is a word presenting in the
document summary.
The supervised learning approach for keywords
extraction was first suggested in (Turney, 2000),
where parametrized heuristic rules were combined
with a genetic algorithm into a system - GenEx -
that automatically identified keywords in a docu-
ment.
For both our approaches, we utilize a graph-
based representation for text documents. Such rep-
resentations may vary from very simple, syntactic
ones like words connected by edges representing
co-occurrence relation (Mihalcea and Tarau, 2004)
to more complex ones like concepts connected by
semantic relations (Leskovec et al, 2004). The
main advantage of a syntactic representation is its
language independency, while the semantic graphs
representation provide new characteristics of text
such as its captured semantic structure that it-
self can serve as a document surrogate and pro-
vide means for document navigation. Authors of
(Leskovec et al, 2004) reduce the problem of sum-
marization to acquiring machine learning models
for mapping between the document graph and the
graph of a summary. Using deep linguistic anal-
ysis, they extract sub-structures (subjectpredica-
teobject triples) from document semantic graphs in
order to get a summary. Contrary to (Leskovec et
al., 2004), both our approaches work with a syn-
tactic representation that does not require almost
any language-specific linguistic processing. In
17
this paper, we perform experiments with directed
graphs, where the nodes stand for words/phrases
and the edges represent syntactic relationships be-
tween them, meaning
?
followed by? (Schenker et
al., 2005).
Some of the most successful approaches to ex-
tractive summarization utilize supervised learn-
ing algorithms that are trained on collections of
?ground truth? summaries built for a relatively
large number of documents (Mani and Maybury,
1999). However, in spite of the reasonable perfor-
mance of such algorithms they cannot be adapted
to new languages or domains without training
on each new type of data. Our first approach
also utilizes classification algorithms, but, thanks
to the language-independent graph representation
of documents, it can be applied to various lan-
guages and domains without any modifications of
the graph construction procedure (except for the
technical upgrade of implementation for multi-
lingual processing of text, like reading Unicode or
language-specific encodings, etc.) (Markov et al,
2007; Last and Markov, 2005). Of course, as a su-
pervised approach it requires high-quality training
labeled data.
Our second approach uses a technique that does
not require any training data. To extract the sum-
mary keywords, we apply a ranking algorithm
called HITS (Kleinberg, 1999) to directed graphs
representing source documents. Authors of (Mi-
halcea and Tarau, 2004) applied the PageRank al-
gorithm (Brin and Page, 1998) for keyword extrac-
tion using a simpler graph representation (undi-
rected unweighted graphs), and show that their re-
sults compare favorably with results on established
benchmarks of manually assigned keywords. (Mi-
halcea and Tarau, 2004) are also using the HITS
algorithm for automatic sentence extraction from
documents represented by graphs built from sen-
tences connected by similarity relationships. Since
we work with directed graphs, HITS is the most
appropriate algorithm for our task as it takes into
account both in-degree and out-degree of nodes.
We show in our experiments that running HITS till
convergence is not necessary, and initial weights
that we get after the first iteration of algorithm
are good enough for rank-based extraction of sum-
mary keywords. Another important conclusion
that was infered from our experimental results is
that, given the training data in the form of anno-
tated syntactic graphs, supervised classification is
the most accurate option for identifying the salient
nodes in a document graph, while a simple degree-
based ranking provides the highest F-measure.
2 Document representation
Currently, we use the ?simple? graph representa-
tion defined in (Schenker et al, 2005) that holds
unlabeled edges representing order-relationship
between the the words represented by nodes. The
stemming and stopword removal operations of ba-
sic text preprocessing are done before graph build-
ing. Only a single vertex for each distinct word
is created even if it appears more than once in
the text. Thus each vertex label in the graph is
unique. If a word a immediately precedes a word
b in the same sentence somewhere in the docu-
ment, then there is a directed edge from the ver-
tex corresponding to term a to the vertex corre-
sponding to term b. Sentence terminating punctu-
ation marks (periods, question marks, and excla-
mation points) are taken by us into account and
an edge is not created when these are present be-
tween two words. This definition of graph edges
is slightly different from co-occurrence relations
used in (Mihalcea and Tarau, 2004) for building
undirected document graphs, where the order of
word occurrence is ignored and the size of the co-
occurrence window is varied between 2 and 10.
Sections defined for HTML documents are: title,
which contains the text related to the document?s
title and any provided keywords (meta-data) and
text, which comprises any of the readable text in
the document. This simple representation can be
extended to many different variations like a se-
mantic graph where nodes stand for concepts and
edges represent semantic relations between them
or a more detailed syntactic graph where edges and
nodes are labeled by significant information like
frequency, location, similarity, distance, etc. The
syntactic graph-based representations were shown
in (Schenker et al, 2005) to outperform the clas-
sical vector-space model on several clustering and
classification tasks. We choose the ?simple? repre-
sentation as a representation that saves processing
time and memory resources as well as gives nearly
the best results for the two above text mining tasks.
3 Keywords extraction
In this paper, we deal with the first stage of extrac-
tive summarization where the most salient words
(?keywords?) are extracted in order to generate a
18
summary. Since each distinct word in a text is rep-
resented by a node in the document graph, the key-
words extraction problem is reduced to the salient
nodes extraction in graphs.
3.1 The Supervised approach
In this approach, we try to identify the salient
nodes of document graphs by training a classifi-
cation algorithm on a repository of summarized
documents such as (DUC, 2002) with the purpose
of inducing a keyword identification model. Each
node of every document graph belongs to one of
two classes: YES if the corresponding word is in-
cluded in the document extractive summary and
NO otherwise. We consider the graph-based fea-
tures (e.g., degree) characterizing graph structure
as well as statistic-based features (Nobata et al,
2001) characterizing text content represented by a
node. The complete list of features, along with
their formal definitions, is provided below:
? In Degree - number of incoming edges
? Out Degree - number of outcoming edges
? Degree - total number of edges
? Frequency - term frequency of word repre-
sented by node
1
? Frequent words distribution ? {0, 1},
equals to 1 iff Frequency?threshold
2
? Location Score - calculates an average of lo-
cation scores between all sentences
3
contain-
ing the word N represented by node (denote
these sentences as S(N)):
Score (N) =
?
S
i
?S(N)
Score (S
i
)
|S (N)|
? Tfidf Score - calculates the tf-idf
score (Salton, 1975) of the word repre-
sented by node
4
.
1
The term frequency (TF) is the number of times the word
appears in a document divided by the number of total words
in the document.
2
In our experiment the threshold is set to 0.05
3
There are many variants for calculating sentence location
score (Nobata et al, 2001). In this paper, we calculate it as an
reciprocal of the sentence location in text: Score (S
i
) =
1
i
4
There are many different formulas used to calculate tfidf.
We use the next formula:
tf
tf+1
log
2
|D|
df
, where tf - term fre-
quency (as defined above), |D| - total number of documents in
the corpus, df - number of documents where the term appears.
? Headline Score ? {0, 1}, equals to 1 iff doc-
ument headline contains word represented by
node.
3.2 The Unsupervised approach
Ranking algorithms, such as Kleinberg?s HITS
algorithm (Kleinberg, 1999) or Google?s PageR-
ank (Brin and Page, 1998) have been elaborated
and used in Web-link analysis for the purpose of
optimizating the search performance on the Web.
These algorithms recursively assign a numerical
weight to each element of a hyperlinked set of doc-
uments, determining how important each page is.
A hyperlink to a page counts as a vote of support.
A page that is linked to by many important pages
(with high rank) receives a high rank itself. A
similar idea can be applied to lexical or seman-
tic graphs extracted from text documents, in or-
der to extract the most significant blocks (words,
phrases, sentences, etc.) for the summary (Mi-
halcea and Tarau, 2004; Mihalcea, 2004). In this
paper, we apply the HITS algorithm to document
graphs and evaluate its performance on automatic
unsupervised text unit extraction in the context of
the text summarization task. The HITS algorithm
distinguishes between ?authorities? (pages with a
large number of incoming links) and ?hubs? (pages
with a large number of outgoing links). For each
node, HITS produces two sets of scores - an ?au-
thority? score, and a ?hub? score:
HITS
A
(V
i
) =
?
V
j
?In(V
i
)
HITS
H
(V
j
) (1)
HITS
H
(V
i
) =
?
V
j
?Out(V
i
)
HITS
A
(V
j
) (2)
For the total rank (H) calculation we used the
following four functions:
1. rank equals to the authority score
H (V
i
) = HITS
A
(V
i
)
2. rank equals to the hub score
H (V
i
) = HITS
H
(V
i
)
3. rank equals to the average between two scores
H (V
i
) = avg {HITS
A
(V
i
) ,HITS
H
(V
i
)}
4. rank equals to the maximum between two
scores
H (V
i
) = max {HITS
A
(V
i
) ,HITS
H
(V
i
)}
19
average merit rank feature
0.192 +- 0.005 1 Frequent words distribution
0.029 +- 0 2 In Degree
0.029 +- 0 3 Out Degree
0.025 +- 0 4 Frequency
0.025 +- 0 5 Degree
0.017 +- 0 6 Headline Score
0.015 +- 0 7 Location Score
0.015 +- 0.001 8 Tfidf Score
Table 1: Feature selection results according to GainRatio value
0.8
24
0.8
26
0.8
28
0.8
30
0.8
32
0.8
34
0.8
36
0.8
38
0.8
40
0.8
42
8
7
6
5
4
3
2
1
siz
e o
f th
e fe
atu
re s
et
accuracy
NB
C a
ccu
rac
y
MR
 low
er b
oun
d
MR
 up
per
 bo
und
Figure 1: Accuracy for Na??veBayes classifier (NBC) and Majority Rule (MR)
4 Experimental results
All experiments have been performed on the
collection of summarized news articles pro-
vided by the Document Understanding Conference
2002 (DUC, 2002). This collection contains 566
English texts along with 2-3 summaries per doc-
ument on average. The size
5
of syntactic graphs
extracted from these texts is 196 on average, vary-
ing from 62 to 876.
4.1 Supervised approach
We utilized several classification algorithms im-
plemented in Weka?s software (Witten and Frank,
2005) : J48 (known as C4.5), SMO (Support Vec-
tor Machine) and Na??veBayes for building binary
classification models (a word belongs to summary
/ does not belong to the summary). For the training
we built dataset with two classes: YES for nodes
belonging to at least one summary of the docu-
5
We define the size of a graph as the number of its vertices.
ment, and NO for those that do not belong to any
summary. The accuracy of the default (majority)
rule over all nodes is equal to the percentage of
non-salient nodes (83.17%). For better classifica-
tion results we examined the importance of each
one of the features, described in Section 3.1 using
automated feature selection. Table 1 presents the
average GainRatio
6
values (?merits?) and the aver-
age rank of the features calculated from the DUC
2002 document collection, based on 10-fold cross
validation.
As expected, the results of J48 and SMO (these
algorithms perform feature selection while build-
ing the model) did not vary on different feature
sets, while Na??veBayes gave the best accuracy on
the reduced set. Figure 1 demonstrates the accu-
racy variations of Na??veBayes classifier on the dif-
ferent feature sets relative to the confidence inter-
6
Gain Ratio(A) =
Information Gain(A)
Intrinsic Info(A)
, where
Intrinsic Info(A) = ?
?
x
N
x
N
log
[
N
x
N
]
20
00.10.20.30.40.50.60.70.80.91
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
False
 Posi
tives
T rue P ositiv es
Figure 2: Sample ROC curve for one of the DUC?02 documents
Ranking function Degree vectors Converged vectors
Authority 0.625 0.600
Hub 0.620 0.601
Avg(Authority, Hub) 0.651 0.622
Max(Authority, Hub) 0.651 0.624
Table 2: Average AUC for each rank calculating function
val for the majority rule accuracy according to the
normal approximation of the binomial distribution
with ? = 0.05. Table 3 presents classification
results for supervised algorithms (for Na??veBayes
the results shown on the top 2 features) based on
10-fold cross validation as well as results of unsu-
pervised learning.
4.2 Unsupervised approach
We have studied the following research questions:
1. Is it possible to induce some classification
model based on HITS scores?
2. Is it necessary to run HITS until convergence?
In order to answer these questions we performed
the following two experiments:
1. In the first one, we run HITS only one it-
eration. Note, that the ranks resulted from
the first iteration are just in-degree and out-
degree scores for each node in graph, and
may be easily computed without even starting
HITS
7
.
7
Initially, both authority and hub vectors (a and h respec-
tively) are set to u = (1, 1, . . . , 1). At each iteration HITS
sets an authority vector to a = A
T
h, and the hub vector to
h = Aa, where A is an adjacency matrix of a graph. So, after
the first iteration, a = A
T
u and h = Au, that are the vec-
tors containing in-degree and out-degree scores for nodes in a
graph respectively.
2. In the second experiment we run HITS until
convergence
8
(different number of steps for
different graphs) and compare the results with
the results of the first experiment.
After each experiment we sorted the nodes of
each graph by rank for each function (see the rank
calculating functions described in Section 3.2).
After the sorting we built an ROC (Receiver Op-
erating Characteristic) curve for each one of the
graphs. Figure 2 demonstrates a sample ROC
curve for one of the documents from DUC 2002
collection.
In order to compare between ranking functions
(see Section 3.2) we calculated the average of AUC
(Area Under Curve) for the 566 ROC curves for
each function. Table 2 presents the average AUC
results for the four functions. According to these
results, functions that take into account both scores
(average and maximum between two scores) are
optimal. We use the average function for compar-
ing and reporting the following results. Also, we
can see that degree vectors give better AUC results
8
There are many techniques to evaluate the convergence
achievement. We say that convergence is achieved when for
any vertex i in the graph the difference between the scores
computed at two successive iterations falls below a given
threshold:
|
x
k+1
i
?x
k
i
|
x
k
i
< 10
?3
(Kamvar, 2003; Mihalcea and
Tarau, 2004)
21
05101520253035
1
29
57
85
113
141
169
197
225
253
281
309
337
365
393
421
449
477
505
533
561
589
numbe
r o
f wo
rds
cum ul ativ e A U C
degr
ee-r
anke
d w
ord
s
HITS-ran
ked 
wo
rds
Figure 3: Cumulative AUC curves for degree and converged vectors
Method Accuracy TP FP Precision Recall F-Measure
Classification J48 0.847 0.203 0.022 0.648 0.203 0.309
Na??veBayes 0.839 0.099 0.011 0.648 0.099 0.172
SMO 0.839 0.053 0.002 0.867 0.053 0.100
Degree-based N = 10 0.813 0.186 0.031 0.602 0.186 0.282
Ranking N = 20 0.799 0.296 0.080 0.480 0.296 0.362
N = 30 0.772 0.377 0.138 0.409 0.377 0.388
N = 40 0.739 0.440 0.200 0.360 0.440 0.392
Table 3: Results for each supervised and unsupervised method
than converged ones.
In order to compare between the degree-based
vectors and the converged ones we calculated
the precision curves
9
for each graph in both ex-
periments. Then for each ranking method the
curve representing an average cumulative AUC
over the 566 precision curves was calculated. Fig-
ure 3 demonstrates the difference between result-
ing curves. As we can conclude from this chart,
the degree-based vectors have a slight advantage
over the converged ones. The ?optimum? point
where the average AUC is maximum for both
methods is 111 words with the average AUC of
28.4 for degree-based words and 33 for HITS-
ranked words. That does not have much signifi-
cance because each document has a different ?op-
timum? point.
9
For each number of top ranked words the percentage of
positive words (belonging to summary) is shown.
Finally, we compared the results of unsuper-
vised method against the supervised one. For this
purpose, we consider unsupervised model based
on extracting top N ranked words for four differ-
ent values of N : 10, 20, 30 and 40. Table 3 rep-
resents the values for such commonly used met-
rics as: Accuracy, True Positive Rate, False Posi-
tive Rate, Precision, Recall and F-Measure respec-
tively for each one of the tested methods. The op-
timal values are signed in bold.
Despite the relatively poor accuracy perfor-
mance of both approaches, the precision and re-
call results for the unsupervised methods show
that the classification model, where we choose
the top most ranked words, definitely succeeds
compared to the similar keyword extraction meth-
ods. (Leskovec et al, 2004) that is about ?logical
triples? extraction rather than single keyword ex-
traction, presents results on DUC 2002 data, which
are similar to ours in terms of the F-measure (40%
22
against 39%) though our method requires much
less linguistic pre-processing and uses a much
smaller feature set (466 features against 8). (Mi-
halcea and Tarau, 2004) includes a more similar
task to ours (single keyword extraction) though
the definition of a keyword is different (?keywords
manually assigned by the indexers? against the
?summary keywords?) and a different dataset (In-
spec) was used for results presentation.
5 Conclusions
In this paper we have proposed and evaluated two
graph-based approaches: supervised and unsuper-
vised, for the cross-lingual keyword extraction to
be used in extractive summarization of text docu-
ments. The empirical results suggest the follow-
ing. When a large labeled training set of summa-
rized documents is available, the supervised classi-
fication is the most accurate option for identifying
the salient keywords in a document graph. When
there is no high-quality training set of significant
size, it is recommended to use the unsupervised
method based on the node degree ranking, which
also provides a higher F-measure than the super-
vised approach. The intuition behind this conclu-
sion is very simple: most words that are highly
?interconnected? with other words in text (except
stop-words) should contribute to the summary. Ac-
cording to our experimental results, we can extract
up to 15 words with an average precision above
50%. Running HITS to its convergence is redun-
dant, since it does not improve the initial results of
the degree ranking.
6 Future work
The next stage of our extractive summarization
methodology is generation of larger units from the
selected keywords. At each step, we are going
to reduce document graphs to contain larger units
(subgraphs) as nodes and apply some ranking al-
gorithms to the reduced graphs. This algorithm is
iterative, where graph reduction steps are repeated
until maximal subgraph size is exceeded or another
constraint is met. Also, we plan to work on the su-
pervised classification of sub-graphs, where many
graph-based features will be extracted and evalu-
ated.
In the future, we also intend to evaluate our
method on additional graph representations of doc-
uments, especially on the concept-based represen-
tation where the graphs are built from the con-
cepts fused from the texts. Once completed, the
graph-based summarization methodology will be
compared to previously developed state-of-the-
art summarization methods and tools. All ex-
periments will include collections of English and
non-English documents to demonstrate the cross-
linguality of our approach.
References
S. Brin and L. Page. 1998. The anatomy of a large-
scale hypertextual Web search engine. Computer
Networks and ISDN Systems, 30:1?7.
Document Understanding Documents 2002
[http://www-nlpir.nist.gov/projects/duc/index.html]
Sepandar D. Kamvar, Taher H. Haveliwala, and Gene
H. Golub. Adaptive methods for the computation of
pagerank. Technical report, Stanford University.
Kleinberg, J.M. 1999. Authoritative sources in a
hyperlinked environment. Journal of the ACM,
46(5):604-632.
Last, M. and Markov A. 2005. Identification of terror-
ist web sites with cross-lingual classiffication tools.
In Last, M. and Kandel, A. (Editors), Fighting Terror
in Cyberspace. World Scientific, Series in Machine
Perception and Artificial Intelligence, 65:117?143.
Leskovec, J., Grobelnik, M. and Milic-Frayling, N.
2004. Learning Semantic Graph Mapping for
Document Summarization. In Proceedings of
ECML/PKDD-2004 Workshop on Knowledge Dis-
covery and Ontologies.
Mani, I. and Maybury, M.T. 1999. Advances in Auto-
matic Text Summarization. MIT Press, Cambridge,
MA.
Markov A., Last, M. and Kandel, A. 2007. Fast
Categorization of Web Documents Represented by
Graphs. Advances in Web Mining and Web Usage
Analysis - 8th International Workshop on Knowl-
edge Discovery on the Web, WEBKDD 2006, Re-
vised Papers, O. Nasraoui, et al (Eds). Springer
Lecture Notes in Computer Science 4811:56?71.
Mihalcea R. 2004. Graph-based ranking algorithms
for sentence extraction, applied to text summariza-
tion. In Proceedings of the 42nd Annual Meeting
of the Association for Computational Lingusitics,
Barcelona, Spain.
Mihalcea and P. Tarau. 2004. TextRank - bringing or-
der into texts. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing,
Barcelona, Spain.
Martin F. Porter. 1980. An algorithm for suffix strip-
ping. Program, 14(3):130137, July.
23
Nobata, C., Sekine, S., Murata, M., Uchimoto, K.,
Utiyama, M. and Isahara, H. 2001. Sentence extrac-
tion system assembling multiple evidence. In Pro-
ceedings of the Second NTCIR Workshop Meeting,
5?213?218.
Salton, G., Wong, A. and Yang, C. S. 1975. A Vector
Space Model for Automatic Indexing Communica-
tions of the ACM, 18(11):613-620.
Schenker, A., Bunke, H., Last, M., Kandel, A. 2005.
Graph-Theoretic Techniques for Web Content Min-
ing, volume 62. World Scientific, Series in Machine
Perception and Artificial Intelligence.
Peter D. Turney. 2000. Learning Algorithms
for Keyphrase Extraction. Information Retrieval,
2(4):303?336.
Ian H. Witten and Eibe Frank 2005. Data Mining:
Practical machine learning tools and techniques,
2nd Edition, Morgan Kaufmann, San Francisco.
24
Proceedings of the 4th International Workshop on Cross Lingual Information Access at COLING 2010, pages 61?69,
Beijing, August 2010
Towards multi-lingual summarization: A comparative analysis of
sentence extraction methods on English and Hebrew corpora
Marina Litvak and Hagay Lipman and Assaf Ben Gur and Mark Last
Ben Gurion University of the Negev
{litvakm, lipmanh, bengura, mlast}@bgu.ac.il
Slava Kisilevich and Daniel Keim
University of Konstanz
slaks@dbvis.inf.uni-konstanz.de
Daniel.Keim@uni-konstanz.de
Abstract
The trend toward the growing multi-
linguality of the Internet requires text
summarization techniques that work
equally well in multiple languages. Only
some of the automated summarization
methods proposed in the literature, how-
ever, can be defined as ?language-
independent?, as they are not based on
any morphological analysis of the sum-
marized text. In this paper, we per-
form an in-depth comparative analysis of
language-independent sentence scoring
methods for extractive single-document
summarization. We evaluate 15 pub-
lished summarization methods proposed
in the literature and 16 methods intro-
duced in (Litvak et al, 2010). The eval-
uation is performed on English and He-
brew corpora. The results suggest that
the performance ranking of the com-
pared methods is quite similar in both
languages. The top ten bilingual scoring
methods include six methods introduced
in (Litvak et al, 2010).
1 Introduction
Automatically generated summaries can signif-
icantly reduce the information overload on pro-
fessionals in a variety of fields, could prove ben-
eficial for the automated classification and fil-
tering of documents, the search for information
over the Internet and applications that utilize
large textual databases.
Document summarization methodologies in-
clude statistic-based, using either the classic vec-
tor space model or a graph representation, and
semantic-based, using ontologies and language-
specific knowledge (Mani & Maybury, 1999).
Although the use of language-specific knowl-
edge can potentially improve the quality of auto-
mated summaries generated in a particular lan-
guage, its language specificity ultimately re-
stricts the use of such a summarizer to a sin-
gle language. Only systems that perform equally
well on different languages in the absence of any
language-specific knowledge can be considered
language-independent summarizers.
As the number of languages used on the In-
ternet increases continiously (there are at least
75 different languages according to a estimate
performed by A. Gulli and A. Signorini1 in the
end of January 2005), there is a growing need
for language-independent statistical summariza-
tion techniques that can be readily applied to text
in any language without using language-specific
morphological tools.
In this work, we perform an in-depth com-
parative analysis of 16 methods for language-
independent extractive summarization intro-
duced in (Litvak et al, 2010) that utilize ei-
ther vector or graph-based representations of text
documents computed from word segmentation
and 15 state-of-the art language-independent
scoring methods. The main goal of the eval-
uation experiments, which focused on English
and Hebrew corpora, is to find the most efficient
language-independent sentence scoring methods
1http://www.cs.uiowa.edu/ asignori/web-size/
61
in terms of summarization accuracy and com-
putational complexity across two different lan-
guages.
This paper is organized as follows. The
next section describes related work in extrac-
tive summarization. Section 3 reviews the evalu-
ated language-independent sentence scoring ap-
proaches. Section 4 contains our experimental
results on English and Hebrew corpora. The last
section comprises conclusions and future work.
2 Related Work
Extractive summarization is aimed at the selec-
tion of a subset of the most relevant fragments,
which can be paragraphs, sentences, keyphrases,
or keywords from a given source text. The ex-
tractive summarization process usually involves
ranking, such that each fragment of a summa-
rized text gets a relevance score, and extraction,
during which the top-ranked fragments are ex-
tracted and arranged in a summary in the same
order they appeared in the original text. Statisti-
cal methods for calculating the relevance score
of each fragment can rely on such informa-
tion as: fragment position inside the document,
its length, whether it contains keywords or title
words.
Research by Luhn (1958), in which the sig-
nificance factor of a sentence is based on the
frequency and the relative position of significant
words within that sentence, is considered the first
on automated text summarization. Luhn?s work
was followed shortly thereafter by that of Ed-
mundson (1969) and some time later by stud-
ies from Radev et al (2001) and Saggion et al
(2003), all of who applied linear combinations
of multiple statistical methods to rank sentences
using the vector space model as a text representa-
tion. In (Litvak et al, 2010) we improve the sum-
marization quality by identifying the best linear
combination of the metrics evaluated in this pa-
per.
Several information retrieval and machine
learning techniques have been proposed for de-
termining sentence importance (Kupiec et al,
1995; Wong et al, 2008). Gong and Liu (2001)
and Steinberger and Jezek (2004) showed that
singular value decomposition (SVD) can be ap-
plied to generate extracts.
Among text representation models, graph-
based text representations have gained popular-
ity in automated summarization, as they enable
the model to be enriched with syntactic and se-
mantic relations. Salton et al (1997) were
among the first to attempt graph-based ranking
methods for single document extractive summa-
rization by generating similarity links between
document paragraphs. The important paragraphs
of a text were extracted using degree scores.
Erkan and Radev (2004) and Mihalcea (2005) in-
troduced approaches for unsupervised extractive
summarization that rely on the application of it-
erative graph based ranking algorithms. In their
approaches, each document is represented as a
graph of sentences interconnected by similarity
relations.
3 Language-Independent Scoring
Methods for Sentence Extraction
Various language dependent and independent
sentence scoring methods have been introduced
in the literature. We selected the 15 most promi-
nent language independent methods for evalua-
tion. Most of them can be categorized as fre-
quency, position, length, or title-based, and they
utilize vector representation. TextRank (ML TR)
is the only method that is based on graph repre-
sentation, but there are also position and length-
based methods that calculate scores using the
overall structure of a document. We have also
considered 16 methods proposed in (Litvak et al,
2010), including 13 based on the graph-theoretic
representation (Section 3.1).
Figure 1 (Litvak et al, 2010) shows the taxon-
omy of the 31 methods considered in our work.
All methods introduced in (Litvak et al, 2010)
are denoted by an asterisk (*). Methods requir-
ing a threshold value t ? [0, 1] that specifies the
portion of the top rated terms considered signifi-
cant are marked by a cross in Figure 1 and listed
in Table 1 along with the optimal average thresh-
old values obtained after evaluating the methods
62
Table 1: Selected thresholds for threshold-based
scoring methods
Method Threshold
LUHN 0.9
LUHN DEG 0.9
LUHN PR 0.0
KEY [0.8, 1.0]
KEY DEG [0.8, 1.0]
KEY PR [0.1, 1.0]
COV 0.9
COV DEG [0.7, 0.9]
COV PR 0.1
on English and Hebrew documents (Litvak et al,
2010).
The methods are divided into three main cat-
egories: structure-, vector-, and graph-based
methods, and each category also contains an
internal taxonomy. Sections 3.2, 3.3, and
3.4 present structure-, vector-, and graph-based
methods, respectively. With each description, a
reference to the original work where the method
was proposed for extractive summarization is in-
cluded. We denote sentence by S and text docu-
ment by D.
3.1 Text Representation Models
The vector-based scoring methods listed below
use tf or tf-idf term weights to evaluate sen-
tence importance while that used by the graph-
based methods (except for TextRank) is based
on the word-based graph representation model
presented in Schenker et al (2004). We repre-
sent each document by a directed, labeled, un-
weighted graph in which nodes represent unique
terms (distinct normalized words) and edges rep-
resent order-relationships between two terms.
Each edge is labeled with the IDs of sentences
that contain both words in the specified order.
3.2 Structure-based Scoring Methods
In this section, we describe the existing
structure-based methods for multilingual sen-
tence scoring. These methods do not require any
text representation and are based on its structure.
? Position (Baxendale, 1958):
POS L Closeness to the end of the document:
score(Si) = i, where i is a sequential number of
a sentence in a document;
POS F Closeness to the beginning of the docu-
ment: score(Si) = 1i ;
POS B Closeness to the borders of the docu-
ment: score(Si) = max(1i , 1n?i+1), where n isthe total number of sentences in D.
? Length (Satoshi et al, 2001):
LEN W Number of words in a sentence;
LEN CH Number of characters in a sentence.
3.3 Vector-based Scoring Methods
In this section, we describe the vector-based
methods for multilingual sentence scoring, that
are based on the vector space model for text rep-
resentation.
? Frequency-based:
LUHN (Luhn, 1958)
score(S) = maxci?{clusters(S)}{csi}, where
clusters are portions of a sentence brack-
eted by keywords2 and csi = |keywords(ci)|
2
|ci| .KEY (Edmundson, 1969) Sum of the keyword
frequencies: score(S) = ?i?{keywords(S)} tfi,
where tfi is term in-document frequency of
keyword i.
COV (Kallel et al, 2004) Ratio of keyword
numbers (Coverage): score(S) = |keywords(S)||keywords(D)|TF (Vanderwende et al, 2007) Average term
frequency for all sentence words:
score(S) =
?
i?{words(S)} tfi
|S| .TFISF (Neto et al, 2000) Average term
frequency inverted sentence frequency
for all sentence words: score(S) =?
i?{words(S)} tfi ? isfi,
where isfi = 1? log(ni)log(n) , where n is the numberof sentences in a document and ni is the number
of sentences containing word i.
SVD (Steinberger & Jezek, 2004) score(S)
is equal to the length of a sentence vector
in ?2V T after computing the Singular Value
Decomposition of a term by sentence matrix
A = U?V T
? Title (Edmundson, 1969) similarity3 to the
title, score(S) = sim(S, T ):
TITLE O using overlap similarity: |S?T |min{|S|,|T |}
TITLE J using Jaccard similarity: |S?T ||S?T |
2Luhn?s experiments suggest an optimal limit of 4 or 5
non-significant words between keywords.
3Due to multilingual focus of our work, exact word
matching was used in all similarity-based methods.
63
Multilingual sentencescoringmethods
Structure-based Vector-based Graph-based
Position Length Frequency Similarity Degree SimilarityPagerank
Title DocumentPOS_FPOS_LPOS_B
LEN_WLEN_CH
LUHN?KEY ?COV ?TFTFIISFSVD
TITLE_OTITLE_JTITLE_C
D_COV_O*D_COV_J*D_COV_C*
LUHN_DEG ? *KEY_DEG ? *COV_DEG ? *DEG*GRASE*
LUHN_PR ? *KEY_PR ? *COV_PR ? *PR*ML_TR
Title Document
TITLE_E_O*TITLE_E_J* D_COV_E_O*D_COV_E_J*
Figure 1: Taxonomy of statistical language-independent sentence scoring methods (Litvak et al,
2010)
TITLE C using cosine similarity:
sim(~S, ~T ) = cos(~S, ~T ) = ~S?~T|~S|?|~T |
? Document Coverage (Litvak et al, 2010).
These methods score a sentence according to
its similarity to the rest of the sentences in
the document (D ? S) based on the following
intuition: the more document content is covered
by a sentence, the more important the sentence is
to a summary. Redundant sentences containing
repetitive information are removed using a
similarity filter. score(S) = sim(S,D ? S):
D COV O using Overlap similarity:
|S?T |
min{|S|,|D?S|}
D COV J using Jaccard similarity: |S?T ||S?D?S|D COV C using Cosine similarity:
cos(~S, ~D ? S) = ~S? ~D?S|~S|?| ~D?S|
3.4 Graph-based Scoring Methods
In this section, we describe the methods for mul-
tilingual sentence scoring using the graph text
representation based on sentence (ML TR) or
word (all except ML TR) segmentation.
ML TR Multilingual version of Tex-
tRank (Mihalcea, 2005) without morphological
analysis. Each document is represented as a
directed graph of nodes that stand for sen-
tences interconnected by similarity (overlap)
relationship. To each edge connecting two
vertices the weight is assigned and equal to
the similarity value between the corresponding
sentences. We used backward links, as it was
the most successful according to the reported
results in (Mihalcea, 2005). score(S) is equal
to PageRank (Brin & Page, 1998) of its node,
according to the formula adapted to the weights
assigned to edges.
? Degree-based (Litvak et al, 2010):4
LUHN DEG A graph-based extension of the
LUHN measure, in which a node degree is
used instead of a word frequency: words are
considered significant if they are represented
by nodes of a higher degree than a predefined
threshold (see Table 1).
KEY DEG Graph-based extension of KEY
measure.
COV DEG Graph-based extension of COV
measure.
DEG Average degree for all sentence nodes:
score(S) =
?
i?{words(S)}Degi
|S| .GRASE(GRaph-based Automated Sentence
Extractor) Modification of Salton?s algo-
rithm (Salton et al, 1997) using the graph
4All proposed here degree-based methods, except for
GRASE, use undirected graphs and degree of nodes as a
predictive feature. The methods based on the directed word
graphs and distinguishing between in- and out-links were
outperformed in our preliminary experiments by the undi-
rected approach.
64
representation defined in Section 3.1 above.
In our graph representation, all sentences are
represented by paths, completely or partially.
To identify the relevant sentences, we search
for the bushy paths and extract from them the
sentences that appear the most frequently. Each
sentence in the bushy path gets a domination
score that is the number of edges with its label
in the path normalized by the sentence length.
The relevance score for a sentence is calculated
as a sum of its domination scores over all paths.
? PageRank-based:5
LUHN PR A graph-based extension of the
LUHN measure in which the node PageRank
value is used instead of the word frequency:
keywords are those words represented by nodes
with a PageRank score higher than a predefined
threshold (see Table 1).
KEY PR Graph-based extension of KEY mea-
sure.
COV PR Graph-based extension of COV mea-
sure.
PR Average PageRank for all sentence nodes:
score(S) =
?
i?{words(S)} PRi
|S| .? Similarity-based. Edge matching techniques
similar to those of Nastase and Szpakowicz
(2006) are used. Edge matching is an alternative
approach to measure the similarity between
graphs based on the number of common edges:
TITLE E O Graph-based extension of TI-
TLE O ? Overlap-based edge matching between
title and sentence graphs.
TITLE E J Graph-based extension of TITLE J
? Jaccard-based edge matching between title
and sentence graphs.
D COV E O Graph-based extension of
D COV O ? Overlap-based edge matching
between sentence and document complement
(the rest of a document sentences) graphs.
D COV E J Graph-based extension of
D COV J ? Jaccard-based edge matching
5Using undirected word graphs with PageRank does not
make sense, since for an undirected graph a node pagerank
score is known to be proportional to its degree. Revers-
ing links will result in hub scores instead authority. The
methods distinguishing between authority and hub scores
were outperformed in our preliminary experiments by the
degree-based approach.
between sentence and document complement
graphs.
4 Experiments
4.1 Overview
The quality of the above-mentioned sentence
ranking methods was evaluated through a com-
parative experiment on corpora of English and
Hebrew texts. These two languages, which
belong to different language families (Indo-
European and Semitic languages, respectively),
were intentionally chosen for this experiment to
increase the generality of our evaluation. The
main difference between these languages, is that
Hebrew morphology allows morphemes to be
combined systematically into complex word-
forms. In different contexts, the same morpheme
can appear as a separate word-form, while in oth-
ers it appears agglutinated as a suffix or prefix to
another word-form (Adler, 2009).
The goals of the experiment were as follows:
- To evaluate the performance of different ap-
proaches for extractive single-document summa-
rization using graph and vector representations.
- To compare the quality of the multilingual sum-
marization methods proposed in our previous
work (Litvak et al, 2010) to the state-of-the-art
approaches.
- To identify sentence ranking methods that work
equally well on both languages.
4.2 Text Preprocessing
Extractive summarization relies critically on
proper sentence segmentation to insure the qual-
ity of the summarization results. We used a sen-
tence splitter provided with the MEAD summa-
rizer (Radev et al, 2001) for English and a sim-
ple splitter for Hebrew splitting the text at every
period, exclamation point, or question mark.6
4.3 Experimental Data
For English texts, we used the corpus of sum-
marized documents provided for the single doc-
6Although the same set of splitting rules may be used
for both languages, separate splitters were used since the
MEAD splitter is restricted to European languages.
65
ument summarization task at the Document
Understanding Conference 2002 (DUC, 2002).
This benchmark dataset contains 533 news arti-
cles, each of which is at least ten sentences long
and has two to three human-generated abstracts
of approximately 100 words apiece.
However, to the best of our knowledge, no
summarization benchmarks exist for the Hebrew
language texts. To collect summarized texts in
Hebrew, we set up an experiment7 in which 50
news articles of 250 to 830 words each from the
Haaretz8 newspaper internet site were summa-
rized by human assessors by extracting the most
salient sentences. In total, 70 undergraduate stu-
dents from the Department of Information Sys-
tems Engineering, Ben Gurion University of the
Negev participated in the experiment. Ten doc-
uments were randomly assigned to each of the
70 study participants who were instructed (1)
To dedicate at least five minutes to each doc-
ument, (2) To ignore dialogs and citations, (3)
To read the whole document before starting sen-
tence extraction, (4) To ignore redundant, repet-
itive, or overly detailed information, (5) To obey
the minimal and maximal summary constraints
of 95 and 100 words, respectively. Summaries
were assessed for quality by procedure described
in (Litvak et al, 2010).
4.4 Experimental Results
We evaluated English and Hebrew summaries
using the ROUGE-1, 2, 3, 4, L, SU and W met-
rics9, described in Lin (2004). Our results were
not statistically distinguishable and matched the
conclusion of Lin (2004). However, because
ROUGE-1 showed the largest variation across
the methods, all results in the following com-
parisons are presented in terms of ROUGE-1
metric. Similar to the approach described
in Dang (2006), we performed multiple com-
parisons between the sentence scoring methods.
The Friedman test was used to reject the null hy-
7The software enabling easy selection and storage of
sentences to be included in the document extract, can be
provided upon request.
8http://www.haaretz.co.il
9ROUGE toolkit was adapted to Hebrew by specifying
?token? using Hebrew alphabet
Table 2: English: Multiple comparisons of sen-
tence ranking approaches using the Bonferroni-
Dunn test of ROUGE-1 Recall
Approach ROUGE-1
COV DEG? 0.436 A
KEY DEG? 0.433 A B
KEY 0.429 A B C
COV PR? 0.428 A B C D
COV 0.428 A B C D
D COV C? 0.428 A B C D
D COV J? 0.425 B C D E
KEY PR? 0.424 B C D E
LUHN DEG? 0.422 C D E F
POS F 0.419 E F G
LEN CH 0.418 C D E F G
LUHN 0.418 D E F G
LUHN PR? 0.418 E F G H
LEN W 0.416 D E F G H
ML TR 0.414 E F G H
TITLE E J? 0.413 F G H I
TITLE E O? 0.413 F G H I
D COV E J? 0.410 F G H I
D COV O? 0.405 G H I J
TFISF 0.405 G H I J
DEG? 0.403 G H I J
D COV E O? 0.401 H I J K
PR? 0.400 G H I J K
TITLE J 0.399 I J K
TF 0.397 I J K
TITLE O 0.396 J K
SVD 0.395 I J K
TITLE C 0.395 J K
POS B 0.392 K L
GRASE? 0.372 L
POS L 0.339 M
pothesis (all methods perform the same) at the
0.0001 significance level, after which we ran the
Bonferroni-Dunn test (Demsar, 2006) for pair-
wise comparisons. Tables 2 and 3 show the re-
sults of multiple comparisons and are arranged
in descending order with the best approaches
on top. Methods not sharing any common let-
ter were significantly different at the 95% confi-
dence level.
The Pearson correlation between methods
ranking in English and Hebrew was 0.775, which
was larger than zero at a significance level of
0.0001. In other words, most of the methods
were ranked in nearly the same relative positions
in both corpora, and the top ranked methods per-
formed equally well in both languages. The dif-
ferences in ranking were caused by morphologi-
cal differences between two languages.
To determine which approaches performed
best in both languages, we analyzed the cluster-
ing results of the methods in both corpora and
found the intersection of the top clusters from
the two clustering results. For each language,
a document-method matrix of ROUGE scores
was created with methods represented by vec-
tors of their ROUGE scores for each document
in a corpora. Since most scores are not normally
66
Table 3: Hebrew: Multiple comparisons of sen-
tence ranking approaches using the Bonferroni-
Dunn test of ROUGE-1 Recall
Approach ROUGE-1
D COV J? 0.574 A
KEY 0.570 A B
COV DEG? 0.568 A B
POS F 0.567 A B
COV 0.567 A B
TITLE J 0.567 A B
POS B 0.565 A B
LUHN PR? 0.560 A B C
LUHN DEG? 0.560 A B C
D COV E J? 0.559 A B C
LUHN 0.559 A B C
TITLE E J? 0.556 A B C
TITLE E O? 0.556 A B C
KEY DEG? 0.555 A B C
LEN W 0.555 A B C
LEN CH 0.553 A B C
KEY PR? 0.546 A B C
COV PR? 0.546 A B C
TITLE O 0.545 A B C
D COV C? 0.543 A B C
TITLE C 0.541 A B C
ML TR 0.519 A B C D
TFISF 0.514 A B C D
D COV E O? 0.498 A B C D
SVD 0.498 A B C D
D COV O? 0.466 B C D
TF 0.427 C D E
DEG? 0.399 D E F
PR? 0.331 E F
GRASE? 0.243 F
POS L 0.237 F
Table 4: English: Correlation between sentence
ranking approaches using Pearson
Approach Correlated With
POS F (LUHN PR, 0.973), (TITLE E J, 0.902), (TITLE E O, 0.902)
TITLE O (TITLE J, 0.950)
LEN W (LEN CH, 0.909)
KEY PR (COV PR, 0.944)
TITLE E O (TITLE E J, 0.997)
distributed, we chose the K-means algorithm,
which does not assume normal distribution of
data, for clustering. We ran the algorithm with
different numbers of clusters (2 ? K ? 10),
and for each K, we measured two parameters:
the minimal distance between neighboring clus-
ters in the clustered data for each language and
the level of similarity between the clustering re-
sults for the two languages. For both param-
eters, we used the regular Euclidean distance.
For K ? 6, the clusters were highly similar
for each language, and the distance between En-
glish and Hebrew clustering data was maximal.
Based on the obtained results, we left results
only for 2 ? K ? 5 for each corpus. Then,
we ordered the clusters by the average ROUGE
score of each cluster?s instances (methods) and
identified the methods appearing in the top clus-
ters for all K values in both corpora. Table 6
shows the resulting top ten scoring methods with
their rank in each corpus. Six methods intro-
Table 5: Hebrew: Correlation between sentence
ranking approaches using Pearson
Approach Correlated With
KEY (KEY DEG, 0.930)
COV (D COV J, 0.911)
POS F (POS B, 0.945), (LUHN DEG, 0.959), (LUHN PR, 0.958)
POS B (LUHN DEG, 0.927), (LUHN PR, 0.925)
TITLE O (TITLE E J, 0.920), (TITLE E O, 0.920)
TITLE J (TITLE E J, 0.942), (TITLE E O, 0.942)
LEN W (LEN CH, 0.954), (KEY PR, 0.912)
LEN CH (KEY PR, 0.936), (KEY DEG, 0.915), (COV DEG, 0.901)
LUHN DEG (LUHN PR, 0.998)
KEY DEG (COV DEG, 0.904)
Table 6: Ranking of the best bilingual scores
Scoring Rank in Rank in Text
method English corpus Hebrew corpus Representation
KEY 3 2 vector
COV 4 4 vector
KEY DEG 2 10 graph
COV DEG 1 3 graph
KEY PR 6 12 graph
COV PR 4 12 graph
D COV C 4 14 vector
D COV J 5 1 vector
LEN W 10 10 structure
LEN CH 9 11 structure
duced in this paper, such as Document Cover-
age (D COV C/J) and graph adaptations of Cov-
erage (COV DEG/PR) and Key (KEY DEG/PR),
are among these top ten bilingual methods.
Neither vector- nor graph-based text represen-
tation models, however, can claim ultimate supe-
riority, as methods based on both models promi-
nently in the top-evaluated cluster. Moreover,
highly-correlated methods (see Tables 4 and 5
for highly-correlated pairs of methods in English
and Hebrew corpora, respectively) appear in the
same cluster in most cases. As a result, some
pairs from among the top ten methods are highly-
correlated in at least one language, and only one
from each pair can be considered. For example,
LEN W and LEN CH have high correlation coef-
ficients (0.909 and 0.954 in English and Hebrew,
respectively). Since LEN CH is more appropri-
ate for multilingual processing due to variations
in the rules of tokenization between languages
(e.g., English vs. German), it may be considered
a preferable multilingual metric.
In terms of summarization quality and com-
putational complexity, all scoring functions pre-
sented in Table 6 can be considered to perform
equally well for bilingual extractive summariza-
tion. Assuming their efficient implementation,
all methods have a linear computational com-
plexity, O(n), relative to the total number of
words in a document. KEY PR and COV PR re-
67
quire additional O(c(|E|+|V |)) time for running
PageRank, where c is the number of iterations it
needs to converge, |E| is the number of edges,
and |V | is the number of nodes (distinct words)
in a document graph. Since neither |E| nor |V | in
our graph representation can be as large as n, the
total computation time for KEY PR and COV PR
metrics is also linear relative to the document
size.
In terms of implementation complexity,
LEN W and LEN CH are simpliest, since they
even do not require any preprocessing and repre-
sentation building; KEY and COV require key-
words identification; D COV C, and D COV J
require vector space model building; KEY DEG
and COV DEG need graphs building (order of
words); whereas KEY PR and COV PR, in ad-
dition, require PageRank implementation.
5 Conclusion and Future Research
In this paper, we conducted in-depth, compar-
ative evaluations of 31 existing (16 of which
are mostly graph-based modifications of exist-
ing state-of-the-art methods, introduced in (Lit-
vak et al, 2010)) scoring methods10 using En-
glish and Hebrew language texts.
The experimental results suggest that the rel-
ative ranking of methods performance is quite
similar in both languages. We identified meth-
ods that performed significantly better in only
one of the languages and those that performed
equally well in both languages. Moreover, al-
though vector and graph-based approaches were
among the top ranked methods for bilingual ap-
plication, no text representation model presented
itself as markedly superior to the other.
Our future research will extend the evaluations
of language-independent sentence ranking met-
rics to a range of other languages such as Ger-
man, Arabic, Greek, and Russian. We will adapt
similarity-based metrics to multilingual applica-
tion by implementing them via n-gram matching
instead of exact word matching. We will fur-
ther improve the summarization quality by ap-
10We will provide the code for our summarizer upon re-
quest.
plying machine learning on described features.
We will use additional techniques for summary
evaluation and study the impact of morpholog-
ical analysis on the top ranked bilingual scores
using part-of-speech (POS) tagging11, anaphora
resolution, named entity recognition, and taking
word sense into account.
Acknowledgments
We are grateful to Michael Elhadad and Galina
Volk for providing the ROUGE toolkit adapted
to Hebrew alphabet.
References
Adler, M. (2009). Hebrew morphological
disambiguation: An unsupervised stochas-
tic word-based approach. Dissertation.
http://www.cs.bgu.ac.il/ adlerm/dat/thesis.pdf.
Baxendale, P. (1958). Machine-made index for
technical literature-an experiment. IBM Jour-
nal of Research and Development, 2, 354?361.
Brin, S., & Page, L. (1998). The anatomy of
a large-scale hypertextual web search engine.
Computer networks and ISDN systems, 30,
107?117.
Dang, H. T. (2006). Overview of DUC 2006.
Proceedings of the Document Understanding
Conference.
Demsar, J. (2006). Statistical comparisons of
classifiers over multiple data sets. Journal of
Machine Learning Research, 7, 1?30.
DUC (2002). Document understanding confer-
ence. http://duc.nist.gov.
Edmundson, H. P. (1969). New methods in auto-
matic extracting. J. ACM, 16.
Erkan, G., & Radev, D. R. (2004). LexRank:
Graph-based lexical centrality as salience in
text summarization. Journal of Artificial In-
telligence Research, 22, 457?479.
11Our experiments have shown that syntactic filters,
which select only lexical units of a certain part of speech,
do not significantly improve the performance of the evalu-
ated bilingual scoring methods.
68
Gong, Y., & Liu, X. (2001). Generic text summa-
rization using relevance measure and latent se-
mantic analysis. Proceedings of the 24th ACM
SIGIR conference on Research and develop-
ment in information retrieval (pp. 19?25).
Kallel, F. J., Jaoua, M., Hadrich, L. B., &
Hamadou, A. B. (2004). Summarization at
LARIS laboratory. Proceedings of the Doc-
ument Understanding Conference.
Kupiec, J., Pedersen, J., & Chen, F. (1995). A
trainable document summarizer. Proceedings
of the 18th annual international ACM SIGIR
conference (pp. 68?73).
Lin, C.-Y. (2004). ROUGE: A package for au-
tomatic evaluation of summaries. Proceedings
of the ACL?04 Workshop: Text Summarization
Branches Out (pp. 74?81).
Litvak, M., Last, M., & Friedman, M. (2010). A
new approach to improving multilingual sum-
marization using a genetic algorithm. Pro-
ceedings of the Association for Computational
Linguistics (ACL) 2010. Uppsala, Sweden.
Luhn, H. P. (1958). The automatic creation of
literature abstracts. IBM Journal of Research
and Development, 2, 159?165.
Mani, I., & Maybury, M. (1999). Advances in
automatic text summarization.
Mihalcea, R. (2005). Language independent ex-
tractive summarization. AAAI?05: Proceed-
ings of the 20th national conference on Artifi-
cial intelligence (pp. 1688?1689).
Nastase, V., & Szpakowicz, S. (2006). A study
of two graph algorithms in topic-driven sum-
marization. Proceedings of the Workshop
on Graph-based Algorithms for Natural Lan-
guage.
Neto, J., Santos, A., Kaestner, C., & Freitas, A.
(2000). Generating text summaries through
the relative importance of topics. Lecture
Notes in Computer Science, 300?309.
Radev, D., Blair-Goldensohn, S., & Zhang, Z.
(2001). Experiments in single and multidocu-
ment summarization using MEAD. First Doc-
ument Understanding Conference.
Saggion, H., Bontcheva, K., & Cunningham, H.
(2003). Robust generic and query-based sum-
marisation. EACL ?03: Proceedings of the
tenth conference on European chapter of the
Association for Computational Linguistics.
Salton, G., Singhal, A., Mitra, M., & Buckley, C.
(1997). Automatic text structuring and sum-
marization. Information Processing and Man-
agement, 33, 193?207.
Satoshi, C. N., Satoshi, S., Murata, M., Uchi-
moto, K., Utiyama, M., & Isahara, H. (2001).
Sentence extraction system assembling mul-
tiple evidence. Proceedings of 2nd NTCIR
Workshop (pp. 319?324).
Schenker, A., Bunke, H., Last, M., & Kandel,
A. (2004). Classification of web documents
using graph matching. International Journal
of Pattern Recognition and Artificial Intelli-
gence, 18, 475?496.
Steinberger, J., & Jezek, K. (2004). Text sum-
marization and singular value decomposition.
Lecture Notes in Computer Science, 245?254.
Vanderwende, L., Suzuki, H., Brockett, C., &
Nenkova, A. (2007). Beyond SumBasic: Task-
focused summarization with sentence simplifi-
cation and lexical expansion. Information pro-
cessing and management, 43, 1606?1618.
Wong, K., Wu, M., & Li, W. (2008). Ex-
tractive summarization using supervised and
semi-supervised learning. Proceedings of the
22nd International Conference on Computa-
tional Linguistics (pp. 985?992).
69
Proceedings of the MultiLing 2013 Workshop on Multilingual Multi-document Summarization, pages 77?81,
Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational Linguistics
Multilingual Single-Document Summarization with MUSE
Marina Litvak
Department of Software Engineering
Shamoon College of Engineering
Beer Sheva, Israel
marinal@sce.ac.il
Mark Last
Department of Information Systems
Engineering, Ben Gurion University
Beer Sheva, Israel
mlast@bgu.ac.il
Abstract
MUltilingual Sentence Extractor (MUSE)
is aimed at multilingual single-document
summarization. MUSE implements a
supervised language-independent summa-
rization approach based on optimization of
multiple sentence ranking methods using a
Genetic Algorithm. The main advantage
of MUSE is its language-independency
? it is using statistical sentence features,
which can be calculated for sentences in
any language.
In our previous work, the performance of
MUSE was found to be significantly bet-
ter than the best known state-of-the-art
extractive summarization approaches and
tools in three different languages: English,
Hebrew, and Arabic. Moreover, our ex-
perimental results in the cross-lingual do-
main suggest that MUSE does not need to
be retrained on a summarization corpus in
each new language, and the same weight-
ing model can be used across several lan-
guages (Last and Litvak, 2012).
MUSE participated in the MultiLing 2013
single document summarization task on
three languages: English, Hebrew and
Arabic. Due to a very limited time that
was given to the participants to run their
systems on the MultiLing 2013 data, the
results submitted to evaluation were ob-
tained by summarizing the documents us-
ing models pre-trained on different cor-
pora. As such, no training has been per-
formed on the MultiLing 2013 corpus.
1 MUltilingual Sentence Extractor
(MUSE): Overview
1.1 Methodology
MUSE implements a supervised learning ap-
proach to language-independent extractive sum-
marization where the best set of weights for a lin-
ear combination of sentence scoring methods is
found by a genetic algorithm trained on a col-
lection of documents and their summaries. The
weighting vector thus obtained is used for sen-
tence scoring in future summarizations. Since
most sentence scoring methods have a linear com-
putational complexity, only the training phase of
our approach is time-consuming.
Using MUSE, the user can choose the subset of
totally 31 sentence metrics that will be included
in the linear combination. The available metrics
are based on various text representation models
and are language-independent since they do not
rely on any language-specific knowledge. Fig-
ure 1 demonstrates the taxonomy of all 31 met-
rics. We divided them into three main categories?
structure-, vector-, and graph-based?according
to their text representation model, where each sub-
category contains group of metrics using the same
scoring method.
A detailed description of sentence metrics used
by MUSE can be found in (Last and Litvak, 2012).
The best linear combination of the metrics de-
picted in Figure 1 can be found using a Genetic
Algorithm (GA). GAs are categorized as global
search heuristics. Figure 2 shows a simplified GA
flowchart.
A typical genetic algorithm requires (1) a ge-
netic representation of the solution domain, (2) a
fitness function to evaluate the solution domain,
and (3) some basic parameter settings like selec-
tion and reproduction rules.
We represent each solution as a vector of
weights for a linear combination of sentence scor-
77
Multilingual sentence
scoringmetrics
Structure-
based
Vector-
based
Graph-
based
Position Length Frequency Similarity Degree SimilarityPagerank
Title Document
POS_F
POS_L
POS_B
LEN_W
LEN_CH
LUHN?
KEY ?
COV ?
TF
TFIISF
SVD
TITLE_O
TITLE_J
TITLE_C
D_COV_O
D_COV_J
D_COV_C
LUHN_DEG ?
KEY_DEG ?
COV_DEG ?
DEG
GRASE
LUHN_PR ?
KEY_PR ?
COV_PR ?
PR
ML_TR
Title Document
TITLE_E_O
TITLE_E_J
D_COV_E_O
D_COV_E_J
Figure 1: Taxonomy of language-independent sentence scoring metrics (Litvak et al, 2010b)
Selection
Mating
Crossover
Mutation
Terminate?
Best
gene
yes
no
Initialization
R
ep
ro
du
ct
io
n
Figure 2: Simplified flowchart of GA
ing metrics?real-valued numbers in the unlimited
range normalized in such a way that they sum up
to 1. The vector size is fixed and it equals to the
number of metrics used in the combination.
Defined over the genetic representation, the fit-
ness function measures the quality of the rep-
resented solution. We can use ROUGE-1 and
ROUGE-2, Recall (Lin and Hovy, 2003) as a
fitness functions for measuring summarization
quality?similarity with gold standard summaries,
which should be maximized during the train-
ing (optimization procedure). We use an anno-
tated corpus of summarized documents, where
each document is accompanied by several human-
generated summaries?abstracts or extracts, as a
training set.
The reader is referred to (Litvak et al, 2010b)
for a detailed description of the optimization pro-
Algorithm 1 Step 1: Training
Require: Gold Standard - a corpus of summarized docu-
ments D, N chosen metrics
Ensure: A weighted model W - vector of weights for each
of N metrics
Step 1.1: Compute M - sentence-score matrix
for all d ? D do
Let R1, R2, and R3 are d representations
for all sentences s ? d do
Calculate N metrics using R1, R2, and R3
Add metrics row for s into M
end for
end for
Step 1.2: Compute a vector W of metrics weights
Run a Genetic Algorithm on M , given D:
Initialize a population P
repeat
for all solution g ? P do
Generate a summary a
Evaluate a by ROUGE on summaries of D
end for
Select the best solutions G
P - a new population generated by G
until convergence - no better solutions are found
return a vector W of weights - output of a GA
cedure implemented by MUSE.
Algorithms 1 and 2 contain the pseudo-code for
two independent phases of MUSE: training and
summarization, respectively. Assuming efficient
implementation, all metrics have a linear compu-
tational complexity relative to the total number of
words in a document - O(n). As a result, the
summary extraction time, given a trained model,
is also linear (in the number of metrics in a com-
bination). The training time is proportional to the
number of GA iterations multiplied by the num-
ber of individuals in a population times the fitness
evaluation (ROUGE) time. On average, in our ex-
periments the GA performed 5 ? 6 iterations?
78
Algorithm 2 Step 2: Summarizing a new docu-
ment
Require: A document d, maximal summary length L, a
trained weighted model W
Ensure: A set of n sentences, which were top-ranked by the
algorithm as the most important.
Step 2.1: Compute a score of each sentence
Let R1, R2, and R3 are d representations
for all sentense s ? d do
Calculate N metrics using R1, R2, and R3
Calculate a score as a linear combination according to
W
end for
Step 2.2: Compile the document summary
Let S = ? be a summary of d
repeat
get the top ranked sentence si
S = S
?
si
until S exceeds max length L
return S
selection and reproduction?before reaching con-
vergence.
1.2 Architecture
The current version of MUSE tool can be ap-
plied only to text documents or textual content of
HTML pages. It consists of two main modules:
the training module activated in offline, and the
real-time summarization module. Both modules
utilize two different representations of documents
described in (Litvak et al, 2010b): vector- and
graph-based. The preprocessing module is respon-
sible for constructing each representation, and it is
embedded in both modules.
The training module receives as input a corpus
of documents, each accompanied by one or several
gold-standard summaries?abstracts or extracts?
compiled by human assessors. The set of docu-
ments may be either monolingual or multilingual
and their summaries have to be in the same lan-
guage as the original text. The training module
applies a genetic algorithm to a document-feature
matrix of precomputed sentence scores with the
purpose of finding the best linear combination of
features using any ROUGE metric as a fitness
function. ROUGE-1 Recall is used as a default
unless specified otherwise by the end-user. The
output/model of the training module is a vector
of weights for user-specified sentence ranking fea-
tures. In the current version of the tool, the user
can choose from 31 vector-based and graph-based
features. The recommendation for the best 10 fea-
tures can be found in (Litvak et al, 2010a).
The summarization module performs summa-
rization of input text/texts in real time. Each sen-
tence of an input text obtains a relevance score ac-
cording to the trained model, and the top ranked
sentences are extracted to the summary in their
original order. The length of resulting summaries
is limited by a user-specified value (maximum
number of words, maximum number of sentences
or a compression ratio). Being activated in real-
time, the summarization module is expected to use
the model trained on the same language as in-
put texts. However, if such model is not avail-
able (no annotated corpus in the text language),
the user can choose one of the following options:
(1) a model trained on some other language/corpus
(in (Litvak et al, 2010b) we show that the same
model can be efficiently used across different lan-
guages), or (2) user-specified weights for each
sentence feature (from 31 provided in the system)
in the linear combination.
The preprocessing module performs the follow-
ing tasks: (1) sentence segmentation, (2) word
segmentation, (3) vector space model construction
using tf and/or tf-idf weights, (4) a word-based
graph representation construction, (5) a sentence-
based graph representation construction, and (6)
document metadata construction, including such
information like frequency (tf and tf-idf) for each
unique term, its location inside the document, etc.
The outputs of this submodule are: sentence seg-
mented text (SST), vector space model (VSM), the
document graphs, and the metadata stored in the
xml files. Steps (1) and (2) are performed by the
text processor submodule, which consists of three
elements: filter, reader and sentence segmenter.
The filter works on the Unicode character level
and performs such operations like identification of
characters, digits, punctuations and normalization
(optional for some languages). The reader invokes
the filter, constructs word chunks from the input
stream and identifies the following states: words,
special characters, white spaces, numbers, URL
links and punctuation marks. The sentence seg-
menter invokes reader and divides the input space
into sentences. By implementing different filters,
the reader can work either with a specific language
(taking into account its intricacies) or with docu-
ments written in arbitrary language (in this case,
a general filtering according to UTF-8 encoding is
performed).
Figure 3 shows the general architecture of the
MUSE system.
79
Scoring 
and 
Ranking
Scoring 
GA
User-
specified 
parameters 
and settings
Summaries
Text 
documents
Preprocessing
Summarized 
documents
TR
A
IN
IN
G
SU
M
M
A
RI
ZA
TI
O
N
ROUGE
Preprocessing
Document 
Representation 
Models
Document-
Feature
Scores Matrix
Document 
Representation 
Models
Weighting 
Model
Figure 3: MUSE architecture
2 Training of MUSE
Since a very limited time was given to partici-
pants to run their summarizers on the MultiLing
2013 dataset, we did not perform training on a new
data. The models obtained from training MUSE
on monolingual corpora of English, Hebrew, and
Arabic texts in 2011 (Last and Litvak, 2012), have
been used for summarization in three languages.
Both ROUGE-1 and ROUGE-2 have beed used
for building the models. In the current settings,
ROUGE-1-based models were utilized.
The English text material used in the ex-
periments comprised the corpus of summa-
rized documents available for the summarization
task at the Document Understanding Conference
2002 (DUC, 2002). This benchmark dataset con-
tains 533 news articles, each accompanied by two
to three human-generated abstracts of approxi-
mately 100 words each.
For the Arabic language, we used a corpus com-
piled from 90 news articles. Each article was sum-
marized by three native Arabic speakers select-
ing the most important sentences into an extractive
summary of approximately 100 words each.
For the Hebrew language, we used a corpus
where 120 news articles of 250 to 830 words are
summarized by five human assessors each.
The documents from all corpora have a title as
the first sentence.
ROUGE-1 and ROUGE-2 metrics (Lin, 2004)
have been used as a fitness function during the
training of MUSE. The same metrics have been
used for evaluation of generated summaries in
three languages. In order to use the ROUGE
toolkit on Hebrew and Arabic, it was adapted to
these languages by specifying the regular expres-
sions for a single ?word? using Hebrew and Arabic
-1.000 -0.500 0.000 0.500 1.000 
COV 
COV_DEG 
COV_PR 
DEG 
D_COV_C 
D_COV_E_J 
D_COV_E_O 
D_COV_J 
D_COV_O 
GRASE 
KEY 
KEY_DEG 
KEY_PR 
LEN_CH 
LEN_W 
LUHN 
LUHN_DEG 
LUHN_PR 
POS_B 
POS_F 
POS_L 
PR 
SVD 
TF 
TFISF 
TITLE_C 
TITLE_E_J 
TITLE_E_O 
TITLE_J 
TITLE_O 
ML_TR 
ENG HEB ARAB 
-1.000 -0.500 0.000 0.500 1.000 
COV 
COV_DEG 
COV_PR 
DEG 
D_COV_C 
D_COV_E_J 
D_COV_E_O 
D_COV_J 
D_COV_O 
GRASE 
KEY 
KEY_DEG 
KEY_PR 
LEN_CH 
LEN_W 
LUHN 
LUHN_DEG 
LUHN_PR 
POS_B 
POS_F 
POS_L 
PR 
SVD 
TF 
TFISF 
TITLE_C 
TITLE_E_J 
TITLE_E_O 
TITLE_J 
TITLE_O 
ML_TR 
ENG HEB ARAB 
Figure 4: Models trained on monolingual corpora:
ROUGE-1 (left) and ROUGE-2 (right)
characters.
Figure 4 present models learned by MUSE on
different monolingual corpora using ROUGE-1
and ROUGE-2, respectively. The actual results in
the trained models include some negative values.
The evaluation results of MUSE on three
monolingual corpora using 10-fold cross valida-
tion showed its significant superiority over Tex-
tRank (Mihalcea, 2005), the best known language-
independent unsupervised approach.
3 Experimental Results
According to the results of automated evaluation
in MultiLing 2013 (N-gram graph methods: Au-
toSummENG, MeMoG, NPowER), MUSE took
fourth place in English corpus (out of 7 systems),
third place in Hebrew (out of 5 summarizers), and
the first place in Arabic (out of 6 participants).
We believe, that training MUSE on the original
data and using correct titles1 (by parsing xml doc-
uments) may significantly improve its results.
1Due to the time constraints of the single-document sum-
marization task, we used a simple txt format of summarized
documents in the published dataset, where the title is not sep-
arated from the first sentence by punctuation marks.
80
References
DUC. 2002. Document Understanding Conference.
http://duc.nist.gov.
M. Last and M. Litvak. 2012. Cross-lingual training
of summarization systems using annotated corpora
in a foreign language. Information Retrieval, pages
1?28, September.
Chin-Yew Lin and Eduard Hovy. 2003. Auto-
matic evaluation of summaries using N-gram co-
occurrence statistics. In NAACL ?03: Proceedings of
the 2003 Conference of the North American Chapter
of the Association for Computational Linguistics on
Human Language Technology, pages 71?78.
Chin-Yew Lin. 2004. ROUGE: A Package for Auto-
matic Evaluation of summaries. In Proceedings of
the Workshop on Text Summarization Branches Out
(WAS 2004), pages 25?26.
M. Litvak, S. Kisilevich, D. Keim, H. Lipman, A. Ben-
Gur, and M. Last. 2010a. Towards language-
independent summarization: A comparative analysis
of sentence extraction methods on english and he-
brew corpora. In Proceedings of the CLIA/COLING
2010.
Marina Litvak, Mark Last, and Menahem Friedman.
2010b. A new approach to improving multilingual
summarization using a Genetic Algorithm. In ACL
?10: Proceedings of the 48th Annual Meeting of the
Association for Computational Linguistics, pages
927?936.
Rada Mihalcea. 2005. Language independent extrac-
tive summarization. In AAAI?05: Proceedings of the
20th National Conference on Artificial Intelligence,
pages 1688?1689.
81
