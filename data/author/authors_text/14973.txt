Event Detection and Summarization in Weblogs with Temporal Collocations 
Chun-Yuan Teng and Hsin-Hsi Chen 
Department of Computer Science and Information Engineering 
National Taiwan University 
Taipei, Taiwan 
{r93019, hhchen}@csie.ntu.edu.tw 
Abstract 
 
This paper deals with the relationship between weblog content and time. With the proposed temporal mutual information, we analyze 
the collocations in time dimension, and the interesting collocations related to special events. The temporal mutual information is 
employed to observe the strength of term-to-term associations over time. An event detection algorithm identifies the collocations that 
may cause an event in a specific timestamp. An event summarization algorithm retrieves a set of collocations which describe an event. 
We compare our approach with the approach without considering the time interval. The experimental results demonstrate that the 
temporal collocations capture the real world semantics and real world events over time. 
 
1. 
2. 
Introduction 
Compared with traditional media such as online news 
and enterprise websites, weblogs have several unique 
characteristics, e.g., containing abundant life experiences 
and public opinions toward different topics, highly 
sensitive to the events occurring in the real world, and 
associated with the personal information of bloggers. 
Some works have been proposed to leverage these 
characteristics, e.g., the study of the relationship between 
the content and bloggers? profiles (Adamic & Glance, 
2005; Burger & Henderson, 2006; Teng & Chen, 2006), 
and content and real events (Glance, Hurst & Tornkiyo, 
2004; Kim, 2005; Thelwall, 2006; Thompson, 2003). 
In this paper, we will use temporal collocation to 
model the term-to-term association over time.  In the past, 
some useful collocation models (Manning & Sch?tze, 
1999) have been proposed such as mean and variance, 
hypothesis test, mutual information, etc. Some works 
analyze the weblogs from the aspect of time like the 
dynamics of weblogs in time and location (Mei, et al, 
2006), the weblog posting behavior (Doran, Griffith & 
Henderson, 2006; Hurst, 2006), the topic extraction (Oka, 
Abe & Kato, 2006), etc. The impacts of events on social 
media are also discussed, e.g., the change of weblogs after 
London attack (Thelwall, 2006), the relationship between 
the warblog and weblogs (Kim, 2005; Thompson, 2003), 
etc. 
This paper is organized as follows. Section 2 defines 
temporal collocation to model the strength of term-to-term 
associations over time.  Section 3 introduces an event 
detection algorithm to detect the events in weblogs, and 
an event summarization algorithm to extract the 
description of an event in a specific time with temporal 
collocations. Section 4 shows and discusses the 
experimental results.  Section 5 concludes the remarks. 
Temporal Collocations 
We derive the temporal collocations from Shannon?s 
mutual information (Manning & Sch?tze, 1999) which is 
defined as follows (Definition 1). 
Definition 1 (Mutual Information) The mutual 
information of two terms x and y is defined as: 
)()(
),(log),(),(
yPxP
yxPyxPyxI =  
where P(x,y) is the co-occurrence probability of x and y, 
and P(x) and P(y) denote the occurrence probability of x 
and y, respectively. 
Following the definition of mutual information, we 
derive the temporal mutual information modeling the 
term-to-term association over time, and the definition is 
given as follows.  
 Definition 2 (Temporal Mutual Information) Given 
a timestamp t and a pair of terms x and y, the temporal 
mutual information of x and y in t is defined as: 
)|()|(
)|,(log)|,()|,(
tyPtxP
tyxPtyxPtyxI =
where P(x,y|t) is the probability of co-occurrence of terms 
x and y in timestamp t, P(x|t) and P(y|t) denote the 
probability of occurrences of x and y in timestamp t, 
respectively. 
To measure the change of mutual information in time 
dimension, we define the change of temporal mutual 
information as follows. 
Definition 3 (Change of Temporal Mutual 
Information) Given time interval [t1, t2], the change of 
temporal mutual information is defined as: 
12
12
21
)|,()|,(),,,(
tt
tyxItyxIttyxC ?
?=  
where C(x,y,t1,t2) is the change of temporal mutual 
information of terms x and y in time interval [t1, t2], I(x,y| 
t1) and I(x,y| t2) are the temporal mutual information in 
time t1 and t2, respectively. 
3. Event Detection 
Event detection aims to identify the collocations 
resulting in events and then retrieve the description of 
events. Figure 1 sketches an example of event detection. 
The weblog is parsed into a set of collocations. All 
collocations are processed and monitored to identify the 
plausible events.  Here, a regular event ?Mother?s day? 
and an irregular event ?Typhoon Chanchu? are detected.  
The event ?Typhoon Chanchu? is described by the words  
 
 
 
 
 
 
 
 
 
 
 
 
Figure 1: An Example of Event Detection
?Typhoon?, ?Chanchu?, ?2k?, ?Eye?, ?Path? and 
?chinaphillippine?.  
The architecture of an event detection system includes 
a preprocessing phase for parsing the weblogs and 
retrieving the collocations; an event detection phase 
detecting the unusual peak of the change of temporal 
mutual information and identifying the set of collocations 
which may result in an event in a specific time duration; 
and an event summarization phase extracting the 
collocations related to the seed collocations found in a 
specific time duration. 
The most important part in the preprocessing phase is 
collocation extraction. We retrieve the collocations from 
the sentences in blog posts. The candidates are two terms 
within a window size. Due to the size of candidates, we 
have to identify the set of tracking terms for further 
analysis. In this paper, those candidates containing 
stopwords or with low change of temporal mutual 
information are removed. 
In the event detection phase, we detect events by 
using the peak of temporal mutual information in time 
dimension.  However, the regular pattern of temporal 
mutual information may cause problems to our detection. 
Therefore, we remove the regular pattern by seasonal 
index, and then detect the plausible events by measuring 
the unusual peak of temporal mutual information. 
If a topic is suddenly discussed, the relationship 
between the related terms will become higher. Two 
alternatives including change of temporal mutual 
information and relative change of temporal mutual 
information are employed to detect unusual events. Given 
timestamps t1 and t2 with temporal mutual information 
MI1 and MI2, the change of temporal mutual information 
is calculated by (MI2-MI1). The relative change of 
temporal mutual information is calculated by (MI2-
MI1)/MI1. 
For each plausible event, there is a seed collocation, 
e.g., ?Typhoon Chanchu?. In the event description 
retrieval phase, we try to select the collocations with the 
highest mutual information with the word w in a seed 
collocation. They will form a collocation network for the 
event.  Initially, the seed collocation is placed into the 
network.  When a new collocation is added, we compute 
the mutual information of the multiword collocations by 
the following formula, where n is the number of 
collocations in the network up to now. 
?= n iMInInformatioMutualMultiwo  
If the multiword mutual information is lower than a 
threshold, the algorithm stops and returns the words in the 
collocation network as a description of the event.  Figure 
2 sketches an example.  The collocations ?Chanchu?s 
path?, ?Typhoon eye?, and ?Chanchu affects? are added 
into the network in sequence based on their MI. 
We have two alternatives to add the collocations to 
the event description. The first method adds the 
collocations which have the highest mutual information 
as discussed above. In contrast, the second method adds 
the collocations which have the highest product of mutual 
information and change of temporal mutual information. 
 
 
 
 
 
 
Figure 2: An Example of Collocation network 
4. 
4.1. 
Experiments and Discussions 
Temporal Mutual Information versus 
Mutual Information 
In the experiments, we adopt the ICWSM weblog data 
set (Teng & Chen, 2007; ICWSM, 2007). This data set 
collected from May 1, 2006 through May 20, 2006 is 
about 20 GB. Without loss of generality, we use the 
English weblog of 2,734,518 articles for analysis. 
To evaluate the effectiveness of time information, we 
made the experiments based on mutual information 
(Definition 1) and temporal mutual information 
(Definition 2). The former called the incremental 
approach measures the mutual information at each time 
point based on all available temporal information at that 
time. The latter called the interval-based approach 
considers the temporal mutual information in different 
time stamps.  Figures 3 and 4 show the comparisons 
between interval-based approach and incremental 
approach, respectively, in the event of Da Vinci Code.   
We find that ?Tom Hanks? has higher change of 
temporal mutual information compared to ?Da Vinci 
Code?. Compared to the incremental approach in Figure 4, 
the interval-based approach can reflect the exact release 
date of ?Da Vinci Code.? 
 rd
=i 1 4.2. Evaluation of Event Detection 
We consider the events of May 2006 listed in 
wikipedia1 as gold standard. On the one hand, the events 
posted in wikipedia are not always complete, so that we 
adopt recall rate as our evaluation metric.  On the other 
hand, the events specified in wikipedia are not always 
discussed in weblogs.  Thus, we search the contents of 
blog post to verify if the events were touched on in our 
blog corpus. Before evaluation, we remove the events 
listed in wikipedia, but not referenced in the weblogs. 
 
 
 
 
 
 
 
 
 
 
 
Figure 3: Interval-based Approach in Da Vinci Code  
 
 
 
 
 
 
 
 
Figure 4: Incremental Approach in Da Vinci Code 
gure 5 sketches the idea of evaluation.  The left side 
of t s figure shows the collocations detected by our event 
dete tion system, and the right side shows the events 
liste  in wikipedia.  After matching these two lists, we 
can find that the first three listed events were correctly 
identified by our system.  Only the event ?Nepal Civil 
War? was listed, but not found. Thus, the recall rate is 
75% in this case. 
 
 
 
 
 
 
 
Figure 5: Evaluation of Event Detection Phase 
As discussed in Section 3, we adopt change of 
temporal mutual information, and relative change of 
temporal mutual information to detect the peak. In Figure 
6, we compare the two methods to detect the events in 
weblogs. The relative change of temporal mutual 
information achieves better performance than the change 
of temporal mutual information. 
                                                     
1 http://en.wikipedia.org/wiki/May_2006 
Table 1 and Table 2 list the top 20 collocations based 
on these two approaches, respectively. The results of the 
first approach show that some collocations are related to 
the feelings such as ?fell left? and time such as ?Saturday 
night?. In contrast, the results of the second approach 
show more interesting collocations related to the news 
events at that time, such as terrorists ?zacarias 
moussaoui? and ?paramod mahajan.? These two persons 
were killed in May 3. Besides, ?Geena Davis? got the 
golden award in May 3. That explains why the 
collocations detected by relative change of temporal 
mutual information are better than those detected by 
change of temporal mutual information. 
-20
-15
-10
-5
0
5
10
1 3 5 7 9 11 13 15 17 19
Time (day)
M
ut
ua
l i
nf
or
m
at
io
n
Da-Vinci Tom Hanks
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 6: Performance of Event Detection Phase 
-15
-10
-5
0
5
10
1 3 5 7 9 11 13 15 17 19
Time (day)
M
ut
ua
l i
nf
or
m
at
io
n
Da-Vinci Tom Hanks
Collocations CMI Collocations CMI 
May 03 9276.08 Current music 1842.67
Illegal immigrants 5833.17 Hate studying 1722.32
Feel left 5411.57 Stephen Colbert 1709.59
Saturday night 4155.29 Thursday night 1678.78
Past weekend 2405.32 Can?t believe 1533.33
White house 2208.89 Feel asleep 1428.18
Red sox 2208.43 Ice cream 1373.23
Album tool 2120.30 Oh god 1369.52
Sunday morning 2006.78 Illegalimmigration 1368.12
16.56
f 
CMI
32.50
31.63
29.09
28.45
28.34
28.13Sunday night 1992.37 Pretty cool 13
Table 1: Top 20 collocations with highest change o
temporal mutual information 
Collocations CMI Collocations 
casinos online 618.36 Diet sodas 
zacarias moussaoui 154.68 Ving rhames 
Tsunami warning 107.93 Stock picks 
Conspirator zacarias 71.62 Happy hump 
Artist formerly 57.04 Wong kan 
Federal  
Jury 
41.78 Sixapartcom 
movabletype Wed 3 39.20 Aaron echolls 27.48
Pramod mahajan 35.41 Phnom penh 25.78
BBC  
Version 
35.21 Livejournal 
sixapartcom 
23.83  Fi
hi
c
dGeena davis 33.64 George yeo 20.34
Table 2: Top 20 collocations with highest relative change 
of mutual information 
4.3. Evaluation of Event Summarization 
As discussed in Section 3, we have two methods to 
include collocations to the event description. Method 1 
employs the highest mutual information, and Method 2 
utilizes the highest product of mutual information and 
change of temporal mutual information. Figure 7 shows 
the performance of Method 1 and Method 2. We can see 
that the performance of Method 2 is better than that of 
Method 1 in most cases. 
 
 
 
 
 
 
 
 
 
 
 
 
Figure 7: Overall Performance of Event Summarization 
The results of event summarization by Method 2 are 
shown in Figure 8. Typhoon Chanchu appeared in the 
Pacific Ocean on May 10, 2006, passed through 
Philippine and China and resulted in disasters in these 
areas on May 13 and 18, 2006.  The appearance of the 
typhoon Chanchu cannot be found from the events listed 
in wikipedia on May 10.  However, we can identify the 
appearance of typhoon Chanchu from the description of 
the typhoon appearance such as ?typhoon named? and 
?Typhoon eye.  In addition, the typhoon Chanchu?s path 
can also be inferred from the retrieved collocations such 
as ?Philippine China? and ?near China?. The response of 
bloggers such as ?unexpected typhoon? and ?8 typhoons? 
is also extracted.   
 
 
 
 
 
 
 
 
 
 
Figure 8: Event Summarization for Typhoon Chanchu 
5. Concluding Remarks 
This paper introduces temporal mutual information to 
capture term-term association over time in weblogs. The 
extracted collocation with unusual peak which is in terms 
of relative change of temporal mutual information is 
selected to represent an event.  We collect those 
collocations with the highest product of mutual 
information and change of temporal mutual information 
to summarize the specific event.  The experiments on 
ICWSM weblog data set and evaluation with wikipedia 
event lists at the same period as weblogs demonstrate the 
feasibility of the proposed temporal collocation model 
and event detection algorithms. 
Currently, we do not consider user groups and 
locations. This methodology will be extended to model 
the collocations over time and location, and the 
relationship between the user-preferred usage of 
collocations and the profile of users. 
Acknowledgments 
Research of this paper was partially supported by 
National Science Council, Taiwan (NSC96-2628-E-002-
240-MY3) and Excellent Research Projects of National 
Taiwan University (96R0062-AE00-02). 
References 
Adamic, L.A., Glance, N. (2005). The Political 
Blogosphere and the 2004 U.S. Election: Divided 
They Blog. In: Proceedings of the 3rd International 
Workshop on Link Discovery, pp. 36--43. 
Burger, J.D., Henderson J.C. (2006). An Exploration of 
Observable Features Related to Blogger Age. In: 
Proceedings of AAAI 2006 Spring Symposium on 
Computational Approaches to Analysing Weblogs, pp. 
15--20. 
Doran, C., Griffith, J., Henderson, J. (2006). Highlights 
from 12 Months of Blogs. In: Proceedings of AAAI 
2006 Spring Symposium on Computational 
Approaches to Analysing Weblogs, pp. 30--33. 
Glance, N., Hurst, M., Tornkiyo, T. (2004). Blogpulse: 
Automated Trend Discovery for Weblogs. In: 
Proceedings of WWW 2004 Workshop on the 
Weblogging Ecosystem: Aggregation, Analysis, and 
Dynamics. 
Hurst, M. (2006). 24 Hours in the Blogosphere. In: 
Proceedings of AAAI 2006 Spring Symposium on 
Computational Approaches to Analysing Weblogs, pp. 
73--77. 
ICWSM (2007). http://www.icwsm.org/data.html 
Kim, J.H. (2005). Blog as an Oppositional Medium? A 
Semantic Network Analysis on the Iraq War Blogs. In: 
Internet Research 6.0: Internet Generations. 
 
Manning, C.D., Sch?tze, H. (1999). Foundations of 
Statistical Natural Language Processing, The MIT 
Press, London England. 
Mei, Q., Liu, C., Su, H., Zhai, C. (2006). A Probabilistic 
Approach to Spatiotemporal Theme Pattern Mining on 
Weblogs. In: Proceedings of the 15th International 
Conference on World Wide Web, Edinburgh, Scotland, 
pp. 533--542. 
Oka, M., Abe, H., Kato, K. (2006). Extracting Topics 
from Weblogs Through Frequency Segments. In: 
Proceedings of WWW 2006 Annual Workshop on the 
Weblogging Ecosystem: Aggregation, Analysis, and 
Dynamics. 
Teng, C.Y., Chen, H.H. (2006). Detection of Bloggers? 
Interest: Using Textual, Temporal, and Interactive 
Features. In: Proceeding of IEEE/WIC/ACM 
International Conference on Web Intelligence, pp. 
366--369. 
Teng, C.Y., Chen, H.H. (2007). Analyzing Temporal 
Collocations in Weblogs. In: Proceeding of 
International Conference on Weblogs and Social 
Media, 303--304. 
Thelwall, M. (2006). Blogs During the London Attacks: 
Top Information Sources and Topics. In: Proceedings 
of 3rd Annual Workshop on the Weblogging 
Ecosystem: Aggregation, Analysis and Dynamics. 
Thompson, G. (2003). Weblogs, Warblogs, the Public 
Sphere, and Bubbles. Transformations, 7(2). 
Proceedings of the Workshop on Embodied Language Processing, pages 33?40,
Prague, Czech Republic, June 28, 2007. c?2007 Association for Computational Linguistics
A ?person? in the interface: effects on user perceptions of              
multibiometrics  
?lvaro Hern?ndez, Beatriz L?pez, David D?az, 
Rub?n Fern?ndez, Luis Hern?ndez 
GAPS, Signal, Systems and Radiocommunications 
Department 
Universidad Polit?cnica de Madrid                          
Ciudad Universitaria s/n, 28040 Madrid, Spain 
alvaro@gaps.ssr.upm.es 
Javier Caminero 
 
Multilinguism & Speech Technology 
Group 
Telef?nica I+D 
Emilio Vargas,6, 28043,Madrid,Spain 
fjcg@tid.es 
 
 
 
 
Abstract 
In this paper we explore the possibilities 
that conversational agent technology offers 
for the improvement of the quality of hu-
man-machine interaction in a concrete area 
of application: the multimodal biometric 
authentication system. Our approach looks 
at the user perception effects related to the 
system interface rather than to the perform-
ance of the biometric technology itself. For 
this purpose we have created a multibio-
metric user test environment with two dif-
ferent interfaces or interaction metaphors: 
one with an embodied conversational agent 
and the other with on-screen text messages 
only. We present the results of an explora-
tory experiment that reveals interesting ef-
fects, related to the presence of a conversa-
tional agent, on the user?s perception of pa-
rameters such as privacy, ease of use, inva-
siveness or system security. 
1 Introduction 
The term biometrics, in Information Technology, 
refers to an array of techniques to identify people 
based on one or more unique behavioural or 
physiological characteristics. The techniques them-
selves have improved considerably over the past 
few decades, in terms of performance 
and reliability, with reported error rates at levels 
that indicate a reasonable level of technological 
maturity (Wayman et al, 2005). But in order to be 
truly useful the technology has to be acceptable to 
people in each of its areas of application. It is 
widely recognised (BioSec, 2004) that to achieve 
this goal a user-centred understanding much deeper 
than that which we have today is needed, and one 
which encompasses the important problem of in-
teraction with the interface. These, of course, are 
basic goals of the more general field of Human-
Computer Interaction, added to which are more 
specific issues regarding security (Sasse, 2004).  
As regards application interface technology, 
ever more realistic animated characters or embod-
ied conversational agents (ECAs) are being gradu-
ally introduced in the hope that they will enhance 
the users? experience and enrich the interaction. 
Some applications of ECAs promise to bring us 
closer to achieving universal usability. For in-
stance, they can be used to communicate with 
hearing impaired people through sign language 
(Huenerfauth, 2005) or lip-reading (Beskow et al, 
2004). Furthermore, language and the appearance, 
style, gesture repertoire and attitude of the charac-
ter can be tuned to each application?s context, to 
user preferences, and more importantly to take into 
account cultural particularities. 
The effects of animated characters on users and 
on the dynamics of user-system interaction are still 
unclear, as is the question of how to use them in 
order to maximize the benefits desired. However, 
the literature does report significant improvements 
in users? perception of the system and their interac-
tion with it when the interface includes an ani-
mated character (Moundridou and Virvou, 2001; 
Mori et al, 2003; Van Mulken et al, 1998). 
33
In what way and to what extent are the percep-
tions of users affected by the presence of an ani-
mated character in the system interface? And how 
does this affect users? opinion and acceptance of a 
biometric authentication system? We designed an 
experiment to learn a bit more about these impor-
tant usability questions. Expanding on previous 
studies of factors that impact on the usability of a 
biometric authentication system, the present paper 
reports the differences we have found in the sub-
jective perceptions of users interacting with our 
biometric authentication system through interfaces 
offering two different forms of assistance: informa-
tion and assistance in the form of text shown on-
screen, and given by a talking animated character. 
In the following section we review a variety of 
social and user perception parameters identified in 
the literature as being potentially affected by an 
ECA. In section 3 we describe our user test frame-
work and we show our results in section 4. 
2 Background 
According to Nass et al (1994) human?machine 
interaction is fundamentally social. This has clear 
implications for user interface design. The user?s 
view of how the system works doesn?t always cor-
respond to the actual way the technology works, 
but, rather, it depends on the user?s preconceptions, 
on the interaction process itself and on mental 
models that are influenced by the system interface. 
Introducing an ECA in the interface can have a 
visual impact on the user that can affect her per-
ception of the system as a whole. Ruttkay et al 
(2002) compile a number of user parameters (such 
as trust, ease of use, effectiveness, and personal 
taste) that have been shown in the literature to be 
affected by the presence of an ECA. 
Basically, there are two lines of work related to 
the effects of ECAs on the users? perception of a 
system. On one hand, the so called ?persona ef-
fect,? associated with the presence of the ECA, and 
on the other, effects connected with the character-
istics or qualities a specific ECA might have. 
2.1 The persona effect 
People seem to like and enjoy using systems with 
ECAs more than without them, they tend to find 
systems easier to use and tasks easier to accom-
plish, and they also feel more motivated and find 
learning easier (both learning to use the system and 
learning about a particular subject in the case of 
teaching applications), even though their perform-
ance is in fact roughly the same as that of users 
interacting without the ECA: Some authors specu-
late that objective performance improvements be-
yond user perceptions will be achieved in the long-
run. For instance, Moundridou and Virvou (2001) 
believe that the increased motivation of students 
using a tutor application with an animated charac-
ter may enhance their learning capacity in the long-
term. 
Animated characters can even help contain user 
stress and frustration caused by difficulties during 
interaction with the system (Mori et al, 2003), and 
as a result they may improve the efficiency of the 
interaction over that of a text-only system (Hone et 
al., 2003). An interesting point is that many of 
these psychological effects are observed as a re-
sponse to the mere presence of the animated char-
acter, without it providing any obvious cues or ex-
pression to help the user: people?s perceptions 
have also been found to be affected by an ECA?s 
behaviour. The phenomenon has been called ?Per-
sona Effect? (Lester et al, 1997). Later research 
(Van Mulken et al, 1998) has shown that the mere 
presence of an ECA can make tasks seem easier 
and more enjoyable to the user. Furthermore, an 
ECA showing greater empathic emotion towards 
the user improves the latter?s overall impression of 
the system and perception of ease of use (Brave et 
al., 2005; Mori et al, 2003). 
The presence of a human-like character can also 
have potential dangers such as the system anthro-
pomorphisation effect that may lead to users hav-
ing unrealistic expectations that are frustrated by 
actual interaction, as Walker et al (1994) points 
out, concluding that a human face in an interface 
can help attract the user?s attention and increase 
her level of motivation. At the same time, how-
ever, it can create high expectations about the in-
telligence of the system, which can lead to frustra-
tion if they are then not met. 
2.2 ECA feature-related effects 
Some authors have studied how the attitude dis-
played by the ECA, for instance regarding its pro-
activity and reactivity (Xiao et. al, 2004), may in-
duce in the user certain responses such as a sense 
of ease of use, system usefulness, frustration or 
sluggishness in task execution. Indeed, it has been 
shown that an affective and empathic attitude on 
34
the part of the ECA can have a very positive effect 
on the user?s perception of the interaction, lower-
ing the level of frustration (Hone et al, 2003; Mori 
et al, 2003) and improving the user?s opinion of 
the system (Brave et. al 2005). 
Another line of research deals with the gestures 
and nonverbal behaviour of the ECA. A good ges-
tural repertoire may promote in the user a percep-
tion of naturalness of interaction with the system 
and system socialness (see, e.g., Cassell and Bick-
more, 2000). 
The physical appearance of the ECA has also 
been seen to have an influence on the user. For 
instance, Leenheer (2006) has studied the effect of 
the colour of the clothing on the ECA, and Hone 
(2006) shows that a female character reduces user 
frustration levels better than a male one. Hone also 
points out that the actual efficiency of the interac-
tion may depend on the ECAs characteristics. 
Dehn and Van Mulken (2000) suggest that the 
great variability of results in the literature may be 
due not only to the different features of the ECAs 
across the studies, but also to the different areas of 
application in which the ECAs were used. In this 
paper we present a study of the influence of an 
ECA in a specific application domain: biometric 
authentication. First we identify the user percep-
tion parameters that we have considered may be 
affected by the ECA. Then we describe our ex-
ploratory test to examine the persona effect. We 
have left the observation of the effects of the 
physical, attitudinal and gestural features of the 
ECA for future experiments. 
3 Test design 
We created a multibiometric authentication test 
platform with two user interfaces, one with an 
ECA guiding the user through the steps of the re-
quired tasks, the other with the same information 
provided only through text displayed on the screen. 
We asked the users to carry out two general tasks: 
a) to try to access the system acting as impostors, 
and b) to enrol using their own biometric traits and 
then authenticate their real identity. 
3.1 System architecture 
The test platform architecture simulates a scenario 
in which a user has to securely access restricted 
information stored on a remote server across an IP 
network (Internet or Intranet). In order to access 
such information the user?s identity must be au-
thenticated on the basis of two biometric traits 
(hence our characterisation of the system as multi-
biometric). The user may choose the two modes 
she wishes to authenticate her identity with from 
among the following four: fingerprint, signature, 
voice and iris pattern. 
The specific technologies used for each biomet-
ric mode were: 
? Fingerprint: Sensor: Precise 100 digital 
fingerprint reader. Software: ?Precise Java? 
by Precise Biometrics. (Precise Biometrics, 
2007). 
? Signature: Sensor: Wacom Intuous2 A6 
digitizing tablet (WACOM, 2007). Soft-
ware: CiC iSign verification software (CIC, 
2007). 
? Voice: Sensor: standard microphone. 
Software: speech and speaker recognition by 
Nuance Communications (Nuance, 2007). 
? Iris: Sensor: Panasonic Autenticam BM-
100ET iris video camera (Panasonic, 2007). 
Software: ?Private ID? recognition algo-
rithms by Iridian (Iridian Technologies, 
2007).  
3.2 User interface 
We have created a web interface (using Java App-
let technology) with five flaps; one to access the 
general instructions of use, and one for each of the 
four biometric modes (in left to right order: finger-
print, signature, voice and iris). Below is a biomet-
ric trait visualisation area and a text message bar 
through which (in addition to the ECA) the system 
guides the user throughout the interaction. 
In addition, we divided the test users into two 
groups to which we presented two different inter-
action ?metaphors?: 
? ECA Metaphor: An ECA is permanently 
present on the right side of the screen to as-
sist the user by giving her general instruc-
tions and guiding her through the steps of 
the interaction. The ECA gives no informa-
tion regarding the details of each particular 
biometric mode. The ECA has been created 
and integrated into our application using the 
technology provided by Haptek (Haptek, 
2007). The ECA uses free Spanish Text-To-
Speech (TTS) software (Lernout and Haus-
35
pie, 2007) to speak to the user. Figure 1 
shows the interface with the ECA. 
? TEXT Metaphor: The user is only guided 
through text messages. 
Note: In the ECA metaphor the text message bar 
remains active, serving as subtitles to what the 
ECA says. The messages read by the ECA are ex-
actly the same as those given in text form in both 
metaphores. 
 
 
Figure 1: User interface for the multibiometric au-
thentication system. 
3.3 Description of the tests 
We designed the tests following the recommenda-
tions issued by the International Biometric Group 
(IBG, 2006). We worked with a sample of 20 us-
ers, half of which interacted with the ECA meta-
phor and the other half with the TEXT metaphor. 
The users carried out the following tasks distrib-
uted in two separate sessions (on different days): 
? On the first day an experimenter trained 
each participant in the use of each biometric 
mode. The training is specific for each mode 
and results in the creation of a biometric 
trait pattern for each user. After creating the 
user models the impostor tests were carried 
out. We allowed the users to consult the 
biometric traits (i.e., fingerprint, signature, 
voice sample and picture of the iris) of four 
people (2 females and 2 males), and we 
asked them to choose one of them in each of 
five impersonation attempts. In order to ac-
cess the system (in this case as impostors) 
users had to successfully mimic any two 
biometric traits of the same person. The sys-
tem returned the result of the attempt (suc-
cess or failure) at the end of the verification 
process. After taking all of the 5 attempts 
the users were directed to a web question-
naire to rate the ease of use, sense of secu-
rity and preference of each of the biometric 
modes, and to give an overall score for the 
system. 
? The second day the users were asked to au-
thenticate their own identity. The task was 
to successfully access the system three times 
in a maximum of 6 attempts. Just as in the 
impostor attempts, users had to enter two of 
their biometric traits in succession, after 
which they were informed of the system?s 
decision to accept or reject them. In case of 
failure in either of the two chosen modes, 
the system didn?t inform the users of which 
mode failed. At the end of this second ses-
sion the users completed another web ques-
tionnaire to give us their evaluation of sys-
tem privacy and an overall score of merit for 
the system, and for each biometric mode 
they rated pleasantness, ease of use and 
preference. In addition, those users who in-
teracted with the ECA metaphor were asked 
to rate the usefulness and pleasantness of the 
ECA. 
In addition to the questionnaire information we 
collected user-system interaction efficiency data 
such as number of failures, verification times and 
so on. However, in this paper we focus primarily 
on the users? impressions. To summarise, the pa-
rameters we have analysed are Preference, Secu-
rity, Ease-of-use, Pleasantness and Privacy, all 
measured on 7-point Likert scales. 
4 Results 
We carried out a series of two sample t-tests on the 
two groups of users (ECA Metaphor and TEXT 
Metaphor) and examined the influence of the ECA 
on the subjective parameters of the interaction. For 
each of the tests we propose a null hypothesis, HO, 
and an alternative hypothesis, H1. We have chosen 
the 5% (p=0.05) significance level to reject the null 
hypothesis. (The questionnaire values were nor-
malised to values between ?3 and 3 for statistical 
processing.) 
36
4.1 Comparative analysis of the ECA y 
TEXT metaphors 
Our general working hypothesis is that interaction 
with the ECA interface will be more pleasant for 
the user, which will result in a higher opinion of 
the system. We specify this in a series of hypothe-
ses for each of the perception parameters we intro-
duced in the previous section: 
 
Hypothesis 1:  
HO: ECA and TEXT Metaphor users rate the 
ease-of-use of the biometric modes equally. 
H1: ECA Metaphor users rate the ease-of-use of 
the biometric modes significantly higher than 
TEXT Metaphor users. 
The average ease-of-use score for the ECA 
Metaphor is: ?ECA = 1,30; and for the TEXT Meta-
phor: ?TEXT = 0.65. The two sample t-test showed 
that the difference was statistically significant 
(t(74)=1.94; p=0.028). Therefore we may accept 
the alternative hypothesis that the ECA increases 
the user?s perception of ease-of-use of biometric 
technology.  
 
Hypothesis 2:  
HO: ECA and TEXT Metaphor users rate the 
pleasantness of the biometric modes equally. 
H1: ECA Metaphor users rate the pleasantness 
of the biometric modes significantly higher than 
TEXT Metaphor users. 
The average pleasantness score for the ECA 
Metaphor is: ?ECA = 1.98; and for the TEXT Meta-
phor: ?TEXT = 1.20; The two sample t-test showed 
that the difference was statistically significant 
(t(77)=2.32; p=0.011). Therefore we may accept 
the alternative hypothesis that the ECA increases 
the pleasantness of the interaction with the biomet-
ric modes.   
 
Hypothesis 3:  
HO: ECA and TEXT Metaphor users rate the 
privacy of the system equally. 
H1: ECA Metaphor users rate the privacy of the 
system significantly higher than TEXT Metaphor 
users. 
The two sample t-test showed no statistically 
significant difference. We are therefore unable to 
reject the null hypothesis. Instead we propose the 
opposite alternative hypothesis:  
 
Hypothesis 3.1:  
H1: ECA Metaphor users rate the privacy of the 
system significantly higher than TEXT Metaphor 
users. 
The average score for the perception of privacy 
for the ECA Metaphor is ?ECA=-1.20; and for the 
TEXT Metaphor: ?TEXT=-0.60. The two sample t-
test showed that the difference was statistically 
significant (t(67)=-3.42 ; p=0.001). Thus we accept 
in this case the alternative hypothesis that users? 
perception of privacy is lower with the ECA Meta-
phor than with the TEXT Metaphor. This result 
might lend support to Zajonc?s (1965) suggestion 
that the presence of a character may enhance 
arousal or user sensitivity, which might explain 
why the user might feel uneasy letting the agent 
have her personal biometric traits. 
 
Hypothesis 4:  
HO: ECA and TEXT Metaphor users rate their 
perception of security of the biometric modes 
equally. 
H1: ECA Metaphor users? trust in the security 
of the biometric modes is higher than in the case 
of the TEXT Metaphor users. 
We obtained no statistically significant results, 
so we reverse the alternative hypothesis: 
 
Hypothesis 4.1:  
H1: ECA Metaphor users? trust in the security 
of the biometric modes is lower than in the case of 
the TEXT Metaphor users. 
Once more, our results were not statistically sig-
nificant. Therefore we cannot infer any relation-
ship between the presence of an ECA and users? 
sense security of a biometric system. 
     
Hypothesis 5:  
HO: Interaction with the ECA Metaphor and 
with the TEXT Metaphor is equally efficient. 
H1: Interaction with the ECA Metaphor is more 
efficient that interaction with the TEXT Metaphor. 
The objective parameter categories compared 
were speed (verification times and reaction times) 
and efficiency (number of verification failures, 
false matches and false rejections). We found no 
statistically significant differences between the 
averages of any of these variables across the two 
metaphors. Therefore we cannot determine any 
influence of the ECA on the actual efficiency of 
the interaction. 
37
The fact that our system is multibiometric ?in 
that it requires simultaneous verification of two 
from among four possible biometric traits? affects 
the complexity of the verification process (Ubuek, 
2003). We now look at the effect our ECA had on 
the users? perception of the cognitive demand and 
of the need for the extra security our multibiomet-
ric system is supposed to provide: 
 
Hypothesis 6:  
HO: ECA and TEXT Metaphor users feel 
equally about the need to require two biometric 
modes for identity verification to ensure security. 
H1: ECA Metaphor users feel that the require-
ment of two biometric modes for verification en-
hances security to a greater extent than in the case 
of the TEXT Metaphor users. 
The average score for the perceived need for the 
enhanced security provided by multibiometrics is, 
for the ECA Metaphor: ?ECA= 2.8; and for the 
TEXT Metaphor: ?TEXT=2.1. The two sample t-test 
showed that the difference was statistically signifi-
cant (t(12)=2.28 ; p=0.021). Therefore we may 
confirm the alternative hypothesis. 
We found no statistically significant differences 
between the two metaphors regarding the users? 
perception of the extra cognitive demand of multi-
biometrics. 
Table 1 summarises our results. 
 
EFFECTS ON THE 
USER 
ECA Metaphor (vs. TEXT 
Metaphor) 
Greater ease-of-use 
Greater pleasantness Subjective impressions 
of users Less privacy 
User behaviour 
throughout the interaction 
with the system 
We didn?t reach definitive 
conclusions 
Improvement in task 
execution 
We didn?t reach definitive 
conclusions 
Impressions regarding 
multibiometrics Enhanced security 
Table 1: Comparative results 
5 Conclusions and future lines of re-
search 
Some of the most serious obstacles to widespread 
use that biometic technology is facing are related 
to user interaction and acceptance. We believe the 
results presented in this paper open interesting new 
lines of research. We found that the presence of an 
ECA (persona effect) makes users experience in-
teraction as easier and more pleasant. Regarding 
sense of security, our results are in line with other 
studies on ECAs. The increased pleasantness of 
use of the biometric modes could help overcome 
users? reluctance to accept biometric systems. On 
the other hand, the presence of the ECA could have 
a negative affect by enhancing the users? percep-
tion of encroachment on their privacy. 
We believe it may be possible to increase the 
level of users? perceived privacy and user trust by 
adopting strategies such as allowing the user to 
personalise the appearance and even the behaviour 
of the avatar, as Xiao et al (2007) suggest. Giving 
the ECA greater and more natural communication 
skills (e.g., small talk, specific gestures, etc.) and a 
more empathic attitude (in line with ideas in the 
area of affective computing) could have further 
positive effects. 
We may mention the inclusion of ECAs on mul-
tibiometric systems as another interesting specific 
line of research, given the enhancement in the us-
ers? perception of the security of such systems 
compared to the same without ECA. 
6 Acknowledgements 
This study has been possible thanks to the support 
grant received from the TEC2006-13170-C02-02 
project of the Spanish Plan Nacional de I+D and 
the 04-AEC0620-000046/06 (?Recognition of fa-
cial and speech patterns for safe multimodal ser-
vices in mobile communications?) project by Tele-
f?nica, funded by the Comunidad Autonoma de 
Madrid. 
7 References 
Jonas Beskow, Inger Karlsson, Jo Kewley and Giam-
piero Salvi, 2004. SYFACE - A Talking Head Tele-
phone for the Hearing-impaired. In Computers help-
ing people with special needs 1178-1186.  
Biosec: Biometry and Security, 2004. Deliverable D6.3: 
Report on results of first phase usability testing and 
guidelines for developers. Available at: 
http://www.europeanbiometrics.info/images/resource
s/73_471_file.pdf (Accessed: 2007, March) 
Scott Brave, Clifford Nass, and Kevin Hutchinson, 
2005. Computers that care: investigating the effects 
of orientation of emotion exhibited by an embodied 
38
computer agent. In International Journal of Human 
Computer Studies, vol. 62, pp. 161-178. 
Justine Cassell and Tim Bickmore, 2000. External 
manifestations of trustworthiness in the interface. In 
Communications of the ACM, vol. 43, pp. 50-56. 
CIC, 2007. Communication Intelligence Corporation, 
?iSign for Java,? 
http://www.cic.com/products/isign/#iSignJava      
(Accessed: 2007, March) 
Doris M. Dehn and Sussane Van Mulken, 2000. The 
impact of animated interface agents: a review of em-
pirical research. In International Journal of Human-
Computer Studies, vol. 52, pp. 1-22. 
Haptek, 2007. http://www.haptek.com (Accessed: 2007, 
March) 
Kate Hone, Farah Akhtar and Martin Saffu, 2003. Affec-
tive agents to reduce user frustration: the role of 
agent embodiment. In Proceedings of Human-
Computer Interaction (HCI2003), Bath, UK, 2003. 
Kate Hone, 2006. Empathic agents to reduce user frus-
tration: The effects of varying agent characteristics. 
In  Interacting with Computers, vol. 18, pp. 227-245. 
Matt Huenerfauth, 2005. American Sign Language Gen-
eration: Multimodal LG with Multiple Linguistic 
Channels. In Proceedings of the ACL Student Re-
search Workshop (ACL 2005), pp. 37?42. 
IBG, 2006. International Biometric Group, 2006. Com-
parative Biometric Testing Available at: 
http://www.biometricgroup.com/reports/public/comp
arative_biometric_testing.html (Accessed: 2007, 
March) 
Iridian Technologies, 2007. Private ID. 
http://www.iridiantech.com/products.php?page=1 
(Accessed: 2007, March) 
Rinze Leenheer, 2006. Should ECAs ?dress to im-
press??, 4th Twente Student Conference on IT, 2006. 
James C. Lester, Sharolyn A. Converse, Susan E. 
Kahler, S. Todd Barlow, Brian A. Stone, and Rav-
inder S. Bhogal, 1997. The persona effect: affective 
impact of animated pedagogical agents.In Proceed-
ings of the SIGCHI conference on Human factors in 
computing systems, pp. 359-366. 
Lernout and Hauspie, 2007. 
http://www.microsoft.com/msagent/downloads/user.a
sp (Accessed: 2007, March) 
Junichiro Mori, Helmut Prendinger and Mitsuru Ishi-
zuka, 2003. Evaluation of an Embodied Conversa-
tional Agent with Affective Behavior. In Proceedings 
of the AAMAS03 Workshop on Embodied Conver-
sational Characters as Individuals , Melbourne, Aus-
tralia. 
Maria Moundridou and Maria Virvou, 2001. Evaluating 
the Impact of Interface Agents in an Intelligent Tu-
toring Systems Authoring Tool. In Proceedings of the 
Panhellenic Conference with International participa-
tion in Human-Computer interaction. 
Clifford Nass, Jonathan Steuer, and Ellen R. Tauber, 
1994. Computers are social actors. In Proceedings of 
the SIGCHI Conference on Human Factors in Com-
puting Systems: Celebrating interdependence. CHI 
'94. ACM Press, New York, NY, 72-78. 
Nuance, 2007. Nuance Communications Inc.  
http://www.nuance.com (Accessed: 2007, March) 
Panasonic, 2007. http://www.panasonic.com (Accessed: 
2007, March) 
Precise Biometrics, 2007. 
http://www.precisebiometrics.com/ (Accessed: 2007, 
March) 
Zs?fia Ruttkay, Claire Dormann and Han Noot, 2002. 
Evaluating ECAs - What and How?. In Proceedings 
of AAMAS 2002 Workshop on Embodied Conversa-
tional Agents -- Let's Specify and Evaluate Them!, 
Bologna, Italy. 
Angela Sasse, 2004. Usability and trust in information 
systems. Cyber Trust & Crime Prevention Project. 
University College London. 
Susanne Van Mulken, Elisabeth Andre, and Jochen 
Muller, 1998. The Persona Effect: How substantial is 
it?. In Proceedings of the ACM CHI 1998 Confer-
ence , pp. 53-66. Los Angeles, CA 
WACOM, 2007. http://www.wacom.com   (Accessed: 
2007, March) 
Janet H. Walker, Lee Sproull and R. Subramani, 1994.  
Using a human face in an interface. In Proceedings 
of the SIGCHI conference on Human factors in com-
puting systems: celebrating interdependence, pp. 85-
91. 
James Wayman, Anil K. Jain, Davide Maltoni and  
Maio Daio, 2005. Biometric Systems: Technology, 
Design and Performance Evaluation, Springer. 
Jun Xiao, John Stasko and Richard Catrambone, 2004. 
An Empirical Study of the Effect of Agent Compe-
tence on User Performance and Perception. In Pro-
ceedings of the Third International Joint Conference 
on Autonomous Agents and Multiagent Systems-
Volume 1, pp. 178-185. 
Jun Xiao, John Stasko and Richard Catrambone, 2007. 
The Role of Choice and Customization on Users' In-
39
teraction with Embodied Conversational Agents: Ef-
fects on Perception and Performance, Proceedings of 
CHI 2007, San Jose, CA. 
Robert B. Zajonc, 1965. Social Facilitation, Science, 
149, pp. 269-274. 
40
Proceedings of the Workshop on Embodied Language Processing, pages 67?74,
Prague, Czech Republic, June 28, 2007. c?2007 Association for Computational Linguistics
Design and validation of ECA gestures to improve                   
dialogue system robustness 
Beatriz L?pez, ?lvaro Hern?ndez, David D?az, 
Rub?n Fern?ndez, Luis Hern?ndez 
GAPS, Signal, Systems and Radiocommunications 
Department 
Universidad Polit?cnica de Madrid                          
Ciudad Universitaria s/n, 28040 Madrid, Spain 
alvaro@gaps.ssr.upm.es 
Doroteo Torre 
 
ATVS, Escuela Polit?cnica Superior 
Universidad Aut?noma de Madrid   
Ciudad Universitaria de Cantoblanco, 
28049 Madrid, Spain 
Doroteo.torre@uam.es 
 
 
 
 
Abstract 
In this paper we present validation tests 
that we have carried out on gestures that 
we have designed for an embodied conver-
sational agent (ECAs), to assess their 
soundness with a view to applying said 
gestures in a forthcoming experiment to 
explore the possibilities ECAs can offer to 
overcome typical robustness problems in 
spoken language dialogue systems 
(SLDSs). The paper is divided into two 
parts: First we carry our a literature review 
to acquire a sense of the extent to which 
ECAs can help overcome user frustration 
during human-machine interaction. Then 
we associate tentative, yet specific, ECA 
gestural behaviour with each of the main 
dialogue stages, with special emphasis on 
problem situations. In the second part we 
describe the tests we have carried out to 
validate our ECA?s gestural repertoire. The 
results obtained show that users generally 
understand and naturally accept the ges-
tures, to a reasonable degree. This encour-
ages us to proceed with the next stage of 
research: evaluating the gestural strategy in 
real dialogue situations with the aim of 
learning about how to favour a more effi-
cient and pleasant dialogue flow for the us-
ers.  
1 Introduction 
Spoken language dialogue systems and embodied 
conversational agents are being introduced in a 
rapidly increasing number of Human-Computer 
Interaction (HCI) applications. The technologies 
involved in SLDSs (speech recognition, dialogue 
design, etc.) are mature enough to allow the crea-
tion of trustworthy applications. However, robust-
ness problems still arise in concrete limited dia-
logue systems because there are many error 
sources that may cause the system to perform 
poorly. A common example is that users tend to 
repeat their previous utterance with some frustra-
tion when error recovery mechanisms come into 
play, which does not help the recognition process, 
and as a result using the system seems slow and 
unnatural (Boyce, 1999). 
At the same time, embodied conversational 
agents (ECAs) are gaining prominence in HCI sys-
tems, since they make for more user-friendly ap-
plications while increasing communication effec-
tiveness. There are many studies on the effects ?
from psychological to efficiency in goal achieve-
ment? ECAs have on users of a variety of applica-
tions, see Bickmore et al (2004) and Brave et al 
(2005), but still very few (Bell and Gustafson,  
2003) on the impact of ECAs in directed dialogue 
situations where robustness is a problem.  
Our research explores the potential of ECAs to 
assist in, or resolve, certain difficult dialogue situa-
tions that have been identified by various leading 
authors in the field (Cassell and Thorisson, 1999; 
Cassell and Stone, 1999), as well as a few we our-
67
selves suggest. After identifying the problematic 
situations of the dialogue we suggest a gestural 
strategy for the ECA to respond to such problem 
situations. Then we propose an experimental 
framework, for forthcoming tests, to study the po-
tential benefits of adding nonverbal communica-
tion in complex dialogue situations. In the study 
we present here we focus on preliminary validation 
of our gestural repertoire through user tests. We 
conclude by presenting our results and suggesting 
the direction our research will take from this point.   
2 How ECA technology can improve in-
teraction with SLDSs 
There are many nonverbal elements of communi-
cation in everyday life that are important because 
they convey a considerable amount of information 
and qualify the spoken message, sometimes even 
to the extent that what is meant is actually the op-
posite of what is said (Krauss et al, 1996). ECAs 
offer the possibility to combine several communi-
cation modes such as speech and gestures, making 
it possible, in theory, to create interfaces with 
which human-machine interaction is much more 
natural and comfortable. In fact, they are already 
being employed to improve interaction (Massaro et 
al., 2000). 
These are some common situations with SLDSs 
in which an ECA could have a positive effect: 
Efficient turn management: The body language 
and expressiveness of agents are important not 
only to reinforce the spoken message, but also to 
regulate the flow of the dialogue, as Cassell points 
out (in Bickmore et al, 2004). 
Improving error recovery: The process of rec-
ognition error recovery usually leads to a certain 
degree of user frustration (see Oviatt and VanGent, 
1996). Indeed, it is common, once an error occurs, 
to enter into an error spiral in which the system is 
trying to recover, the user gets ever more frustrated, 
and this frustration interferes in the recognition 
process making the situation worse (Oviatt et al, 
1998). ECAs may help reduce frustration, and by 
doing so make error recovery more effective (Hone, 
2005). 
Correct understanding of the state of the dia-
logue: Sometimes the user doesn?t know whether 
or not things are going normally (Oviatt, 1994). 
This sometimes leads the dialogue to error states 
that could be avoided. The expressive capacity of 
ECAs could be used to reflect with greater clarity 
the state the system takes the dialogue to be in. 
3 Suggesting ECA behaviour for each 
dialogue situation 
A variety of studies have been carried out on be-
havioural strategies for embodied conversational 
agents (Poggi, 2001; Cassell et al, 2000; Cassell et 
al., 2001; Chovil, 1992; Kendon, 1990), which deal 
with behaviour in hypothetical situations and in 
terms of the informational goals of each particular 
interaction (be it human-human or human-
machine). We direct our attention to the overall 
dialogue systems dynamics, focussing specifically 
on typical robustness problems and how to favour 
smooth sailing through the different stages of the 
dialogue. We draw from existing research under-
taken to try to understand the effects different ges-
tures displayed by ECAs have on people, and we 
apply this knowledge to a real dialogue system. In 
Table 1 we show the basic set of gestures we are 
using as a starting point. They are based mainly on 
descriptions in Bickmore (et al, 2004) and Cassell 
(et al, 2000), and on recommendations in Cassell 
and Thorisson (1999), Cassell (et al, 2001), Chovil 
(1992), Kendon (1990) and San-Segundo (et al, 
2001), to which we have added a few suggestions 
of our own.  
 
Dialogue stage 
ECA behaviour  
(movements, gestures and other cues) 
Initiation  
(welcoming the 
user)  
1. Welcome message: look at the camera, 
smile, wave hand 
2. Explanation of the task: zoom in 
3. Zoom out, lights dim 
Give turn 
 
Look directly at the user, raise eyebrows.   
Camera zooms out. Lights dim. 
Take turn Look directly at the user, raise hands into ges-
ture space. Camera zooms in. Light gets 
brighter. 
Wait Slight leaning back, one arm crossed and the 
other touching the cheek shift of body weight 
Help 
 
Beat gesture with the hands. Change of posture 
Error recovery 
with correction 
Lean towards the camera, beat gesture 
Confirmation 
(high  
confidence) 
Nod, smile, eyes fully open 
Confirmation 
(low  
confidence) 
Slight leaning of the head to one side, stop 
smiling, mildly squint 
Table 1: Gesture repertoire for the main dialogue 
stages 
 
68
3.1 Initiation 
The inclusion of an ECA at this stage ?humanises? 
the system (Oviatt and Adams, 2000). This is a 
problem, first because once a user has such high 
expectations the system can only end up disap-
pointing her, and secondly because the user will 
tend to use more natural (and thus complex) com-
munication, which the system is unable to handle, 
and the experience will ultimately be frustrating. 
On the other hand, especially in the case of new 
users, contact with a dialoguing animated character 
may have the effect that the user?s level of atten-
tion to the actual information that is being given is 
reduced (Schaumburg, 2001; Catrambone, 2002). 
Thus the goal is to present a human-like interface 
that is, at the same time, less striking and thus less 
distracting at first contact, and one that clearly 
?sets the rules? of the interaction and makes sure 
that the user keeps it framed within the capability 
of the system. 
We have designed a welcome gesture for our 
ECA based on the recommendations in Kendon 
(1990), to test whether or not it fosters a sense of 
ease in the user and helps her concentrate on the 
task at hand. Playing with the zoom, the size and 
the position of the ECA on the screen may also 
prove to be useful to frame the communication bet-
ter (see Table 1). 
3.2 Turn Management 
Turn management involves two basic actions: 
taking turn and giving turn. Again, in Table 1 we 
show the corresponding ECA gestures we will start 
testing with. Note that apart from the ECA gestures, 
we also play with zoom and light intensity: when 
it?s the ECA?s turn to speak the camera zooms-in 
slightly and the light becomes brighter, and when 
it?s the user?s turn the camera zooms out and the 
lights dim. The idea is that, hopefully, the user will 
associate each camera shot and level of light inten-
sity with each of the turn modes, and so know 
when she is expected to speak. 
The following are some typical examples of 
problem situations together with further considera-
tions about ECA behaviour that could help avoid 
or recover from them: 
? The user tries to interrupt at a point at 
which the barge-in feature is not active. If 
this happens the system does not process 
what the user has said, and when the system 
finally returns to listening mode there is si-
lence from both parts: the system expects 
input from the user, and the user expects an 
answer. Often both finally break the silence 
at the same time and the cycle begins again, 
or, if the system caught part of the user?s ut-
terance, a recognition error will most likely 
occur and the system will fall into a recogni-
tion error recovery subdialogue that the user 
does not expect. To help avoid such faulty 
events the ECAs demeanour should indicate 
as clearly as possible that the user is not be-
ing listened to at that particular moment. 
Speaking while looking away, perhaps at 
some object, and absence of attention cues 
(such as nodding) are possible ways to show 
that the user is not expected to interrupt the 
ECA. Since our present dialogue system 
produces fairly short utterances for the ECA, 
we are somewhat limited as to the active 
strategies to build into the ECA?s behaviour. 
However, there are at least three cues the 
user could read to realise that the system 
didn?t listen to what she said. The first is the 
fact that the system carries on speaking, ig-
noring the user?s utterance. Second, at the 
end of the system?s turn the ECA will per-
form a specific give-turn gesture. And third, 
after giving the turn the ECA will remain 
still and silent for a few seconds before per-
forming a waiting gesture (leaning back 
slightly with her arms crossed, shifting the 
body weight from one leg to another; see 
Table 1). In addition, if the user still remains 
silent after yet another brief waiting period 
the system will offer help. It will be interest-
ing to see at which point users realise that 
the system didn?t register their utterance. 
? A similar situation occurs if the Voice Ac-
tivity Detector (VAD) fails and the system 
doesn?t capture the user?s entire utterance, 
or when the user simply doesn?t say any-
thing when she is expected to (?no input?). 
Again, both system and user end up waiting 
for each other to say something. And again, 
the strategy we use is to have the ECA dis-
play a waiting posture. 
? It can also happen that the user doesn?t 
speak but the VAD ?thinks? she did, per-
haps after detecting some background noise 
69
(a ?phantom input?). The dialogue system?s 
reaction to something the user didn?t say can 
cause surprise and confusion in the user. 
Here the visible reactions of an ECA might 
help the user understand what has happened 
and allow her to steer the dialogue back on 
track. 
3.3 Recognition Confidence Scheme 
Once the user utterance has been recognised, in-
formation confirmation strategies are commonly 
used in dialogue systems. Different strategies are 
taken depending on the level of confidence in the 
correctness of the user locution as captured by the 
speech recognition unit (San-Segundo et al, 2001). 
Our scheme is as follows: 
? High confidence: if recognition confidence 
is high enough to safely assume that no error 
has occurred, the dialogue strategy is made 
more fluent, with no confirmations being 
sought by the system. 
? Intermediate confidence: the result is re-
garded as uncertain and the system tries im-
plicit confirmation (by including the uncer-
tain piece of information in a question about 
something else.) This, combined with a 
mixed initiative approach, allows the user to 
correct the system if an error did occur. 
? Low confidence: in this case recognition 
has probably failed. When this happens the 
dialogue switches to a more guided strategy, 
with explicit confirmation of the collected 
information and no mixed initiative. The 
user?s reply may confirm that the system 
understood correctly, in which case the dia-
logue continues to flow normally, or, on the 
other hand, it may show that there was a 
recognition error. In this case an error re-
covery mechanism begins. 
In addition to the dialogue strategies, ECAs 
could also be used to reflect in their manner the 
level of confidence that the system has understood 
the user, in accordance with the confirmation dia-
logue strategies. While the user speaks, our ECA 
will, if the recognition confidence level is high, 
nod her head (Cassell et al, 2000), smile and have 
her eyes fully open to give the user feedback that 
everything is going well and the system is under-
standing. If, on the other hand, confidence is low, 
in order to make it clearer to the user that there 
might be some problem with recognition and that 
extra care should be taken, an option might be for 
the ECA to gesture in such a way as to show that 
she isn?t quite sure she?s understood but is making 
an effort to. We have attempted to create this effect 
by having the ECA lean her head slightly to one 
side, stop smiling and mildly squint. Our goal, 
once again, is to find out whether these cues do 
indeed help users realise what the situation is. This 
is especially important if it helps to avoid the well-
known problem of falling into error spirals when a 
recognition error occurs in a spoken dialogue sys-
tem (Bulyko et al, 2005). In the case of intermedi-
ate recognition confidence followed by a mixed 
initiative strategy involving implicit confirmation, 
specific gestures could also be envisaged. We have 
chosen not to include specific gestures for these 
situations in our first trials, however, so as not to 
obscure our observations for the high and low con-
fidence cases. A neutral stance for the intermediate 
confidence level should be a useful reference 
against which to compare the other two cases. 
3.4 Recognition Problems 
We will consider those situations in which the sys-
tem finds the user?s utterance incomprehensible 
(no-match situations) and those in which the sys-
tem gets the user?s message wrong (recognitions 
errors). When a no-match occurs there are two 
ways in which an ECA can be useful. First, what 
the character should say must be carefully pon-
dered to ensure that the user is aware that the sys-
tem didn?t understand what she said and that the 
immediate objective is to solve this particular 
problem. This knowledge can make the user more 
patient with the system and tolerate better the un-
expected lengthening of the interaction (Goldberg, 
2003). Second, the ECAs manner should try to 
keep the user in a positive attitude. A common 
problem in no-match and error recovery situations 
is that the user becomes irritated or hyperarticu-
lates in an attempt to make herself understood, 
which in fact increases the probability of yet an-
other no-match or a recognition error. This we 
should obviously try to avoid. 
The ECA behaviour strategy we will test in no-
match situations is to have the character lean to-
wards the camera and raise her eyebrows (the idea 
being to convey a sense of surprise coupled with 
friendly interest). We have based our gesture on 
70
one given in (Fagerberg et al, 2003). If the user 
points out to the system that there has been a rec-
ognition error in a way that gives the correct in-
formation at the same time, then the ECA will con-
firm the corrected information with special empha-
sis in speech and gesture. For this purpose we have 
designed a beat gesture with both hands (see Table 
1).  
3.5 Help offers and request 
It will be interesting to see whether the fact that 
help is offered by an animated character (the ECA) 
is regarded by users to be more user-friendly than 
otherwise. If users feel more comfortable with the 
ECA, perhaps they will show greater initiative in 
requesting help from the system; and when it is 
offered by the system (when a problem situation 
occurs), the presence of a friendly ECA might help 
control user frustration. While the ECA is giving 
the requested information, she will perform a beat 
gesture with both hands for emphasis, and she will 
also change posture. The idea is to see whether this 
captures the interest of the user, makes her more 
confident and the experience more pleasant or, on 
the contrary, it distracts the user and makes help 
delivery less effective. 
 
Figure 1 illustrates a dialogue sequence includ-
ing the association between the different dialogue 
strategies and the ECA gesture sequences after a 
user?s utterance. 
4 Experimental set up 
Gestures and nonverbal communication are cul-
ture-dependent. This is an important fact to take 
into account because a single gesture might be in-
terpreted in different ways depending on the user?s 
culture (Kleinsmith et al, 2006). Therefore, a nec-
essary step prior to the evaluation of the various 
hypotheses put forward in the previous section is to 
test the gestures we have implemented for our 
ECA, within the framework designed for our study. 
This implies validating the gestures for Spanish 
users, since we have based them on studies within 
the Anglo-Saxon culture. 
4.1 Procedure 
For the purpose of testing the gesture repertoire 
developed for our ECA we have conceived an 
evaluation environment that simulates a realistic 
mobile videotelephony application that allows us-
ers to remotely check the state (e.g., on/off) of sev-
eral household devices (lights, heating, etc.). Our 
dialogue system incorporates mixed initiative, er-
ror recovery subdialogues, context-dependent help 
and the production of guided or flexible dialogues 
according to the confidence levels of the speech 
recogniser. Our environment uses Nuance Com-
munications? speech recognition technology 
(www.nuance.com). The ECA character has been 
designed by Haptek (www.haptek.com). 
During the gesture validation tests users didn?t 
interact directly with the dialogue system. We first 
asked the users to watch a system simulator (a 
video recording of a user interacting with the sys-
tem), so that they could see the ECA performing 
the gestures in the context of a real dialogue. 
After watching the simulation the users were 
asked to fill out a questionnaire. The questionnaire 
allowed users to view isolated clips of each
 
 
Figure 1: Dialogue strategies and related gesture sequence 
71
 
of the dialogue gestures (the eight that had ap-
peared in the video). To each gesture clip were as-
sociated questions basically covering the following 
three aspects:  
? Gesture interpretation: Users are asked to 
interpret each gesture, choosing one from 
among several given options (the same op-
tions for all gestures). The aim is to see 
whether the meaning and intention of each 
gesture are clear. In addition users told us 
whether they thought they had seen the ges-
ture in the previous dialogue sample. 
? Gesture design: Do users think the gesture 
is well made and does it look natural? To 
answer this question we asked users to rate 
the quality, expressiveness and clarity of the 
ECAs gesture (on a 9-point Likert scale). 
? User expectations: Users rated how useful 
they thought each gesture was (on a 9-point 
Likert scale). The idea is to juxtapose the 
utility function of the gestures in the users? 
mental model to our own when we designed 
them, and evaluate the similarity. In addition 
we collected suggestions as to how the users 
thought the gestures could be improved.  
4.2 Results  
We recruited 17 test users (most of them students 
between 20 and 25 years of age) for our trial. The 
results concerning the three previously mentioned 
aspects are shown in Table 2. In the case of the 
gesture interpretation, we present the percentage 
of the users who interpreted each gesture ?cor-
rectly? (i.e., as we had intended when we designed 
them). Depending on this percentage we label each 
gesture as ?Good?, ?Average?, or ?Bad?. For each 
of the parameters for gesture design and user ex-
pectations we give the mean and the standard de-
viation of the Likert scale scores. We label the av-
erage scores as ?Low? (Likert score between 1 and 
3), Medium (4-6) or ?High? (7-9).  
We now discuss the results separately for each 
of the dimensions: 
Regarding user expectations, the values for each 
gesture are High except for two of them, valued as 
Medium. These two gestures are the welcome ges-
ture and the gesture for offering help. In the case of 
the welcome gesture, users probably believe the 
beginning of the dialogue is already well enough 
defined when the ECA starts to speak. If so, users 
might see an element of redundancy in the wel-
come gesture, lowering its perceived utility in the 
dialogue process. On the other hand, the help ges-
ture utility might be valued lower than the rest be-
cause many users didn?t seem to understand its 
purpose (the clarity of the Help gesture was the 
least valued of all, ?=5.117). Nevertheless, the 
general user impressions of the utility of the evalu-
ated gesture repertoire fairly high. 
In relation to gesture design, we can see that, 
overall, the marks for quality and expressiveness 
are high. This implies our gesture design is, on the 
whole, adequate. Regarding the clarity of the ges-
tures, three of them are valued as Medium. These 
are the gestures expressing Give Turn, Error Re-
covery and Help offer. This could be related to the 
prevailing opinion among users that there are a few 
confusing gestures, although they are better under-
stood in the context of the application, when you 
listen to what the ECA says.   
Only half of the gestures were properly inter-
preted by the users. Those that weren?t (Give Turn, 
Take Turn, Error Recovery and the Help gesture) 
are, we realize, the subtlest in the repertoire, so we 
asked ourselves if there could be relation between 
a bad interpretation of the gesture and the whether 
that user didn?t remember seeing the gesture in the 
dialogue. In Figure 2 we show the number of users 
who claimed they hadn?t seen the ECA gestures 
during the dialogue sample. The coloured bars rep-
resent the overall accuracy in the interpretation of 
the gesture. We may observe that the gestures that 
a larger number of users hadn?t seen in the dia-
logue, and therefore, hadn?t an image of in proper 
context, tended also to be considered more unclear.  
We may conclude that some gestures need to be 
evaluated in context. In any case, and in spite of 
the uncertainty we have found regarding the inter-
pretation of certain gestures, we believe the posi-
tive evaluation by the users for the expressiveness 
and the quality of the gestures justifies us in vali-
dating our gestural repertoire for the next research 
stage where we will evaluate how well our ECA 
gestures function under real interaction conditions 
(taking into account objective data related to dia-
logue efficiency). 
72
 Table 2:  Results of the gesture validation tests. 
 
 
Figure 2: Interpretation vs. ?visibility? of the ges-
tures. 
 
5 Conclusions and future lines of work 
In this article we have identified a range of prob-
lem situations that may arise in dialogue systems, 
and defined various strategies for using an ECA to 
improve user-machine interaction throughout the 
whole dialogue. We have developed an experimen-
tal set up for a user validation of ECA gestures in 
the dialogue system and have obtained quantitative 
results and user opinions to improve the design of 
the gestures. The results of this validation allow us 
to be in a position to begin testing our dialogue 
system and evaluate our ECA gestures in the con-
text of a real dialogue. 
In future experiments we will attempt to go one 
step further and analyse how empathic emotions vs. 
self-oriented behaviour (see Brave et al, 2005) 
may affect the resolution of a variety of dialogue 
situations. To this end we plan to design ECA pro-
totypes that incorporate specific emotions, hoping 
to learn how best to connect empathically with the 
user, and what effects this may have on dialogue 
dynamics and the overall user perception of the 
system. 
References 
Linda Bell and Joakim Gustafson, 2003. Child and 
Adult SpeakerAdaptation during Error Resolution in 
a Publicly Available Spoken Dialogue System. Pro-
ceedings of Eurospeech 03, Geneve, Schweiz. 
Timothy W. Bickmore, Justine Cassell, Jan van Kup-
pevelt, Laila Dybkjaer and Niels Ole Bernsen,  2004. 
(atural, Intelligent and Effective Interaction with 
Multimodal Dialogue Systems, chapter Social Dia-
logue with Embodied Conversational Agents. Kluwer 
Academic. 
Susan J. Boyce, 1999. Spoken natural language dia-
logue systems: user interface issues for the future. In 
Human Factors and Voice Interactive Systems. D. 
Gardner-Bonneau Ed. Norwell, Massachusetts, Klu-
wer Academic Publishers: 37-62. 
Scott Brave, Clifford Nass, Kevin Hutchinson, 2005. 
Computers that care: investigating the effects of ori-
 INTERPRETATION DESIGN EXPECTATIONS 
 Good Interpretation (%) Quality Clarity Expressiveness Usefulness 
G1 
Wellcome 
88.23 
Good 
7.117 (0.927) 
High 
7.588 (1.277) 
High 
6.764 (1.147) 
High 
5.647  (2.119) 
Medium 
G2 
Give Turn 
35.29 
Average 
6.647 (1.057) 
High 
5.823 (1.333) 
Medium 
6.470  (1.007) 
High 
6.588 (1.543) 
High 
G3 
Take Turn 
23.53 
Bad 
7.117 (1.166) 
High 
6.705 (1.447) 
High 
6.941 (1.444) 
High 
6.647 (1.271) 
High 
G4 
Wait 
82.35 
Good 
7.058 (1.088) 
High 
7.176 (1.185) 
High 
7.176 (0.727) 
High 
6.588 (1.622) 
High 
G5 
Confirmation  
(Low confidence) 
76.47 
Good 
8.294 (0.587) 
High 
8.058 (1.028) 
High 
8.058 (1.028) 
High 
7.941 (1.028) 
High 
G6 
Confirmation (High 
confidence) 
94.11 
Good 
7.529 (1.124) 
High 
7.529 (1.124) 
High 
7.705(1.263) 
High 
7.588 (1.175) 
High 
G7 
Error Recovery 
41.17 
Average 
6.941 (1.088) 
High 
5.588 (2.032) 
Medium 
6.529 (1.462) 
High 
6.058 (1.390) 
High 
G8 
Help 
35.29 
Average 
6.823 (1.185) 
High 
5.117 (1.932) 
Medium 
6.058(1.560) 
High 
5.529 (1.771) 
Medium 
73
entation of emotion exhibited by an embodied com-
puter agent. Int. J. Human-Computer Studies, Nr. 62, 
Issue 2, pp. 161-178. 
Ivan Bulyko, Katrin Kirchhoff, Mari Ostendorf, Julie 
Goldberg, 2005 Error correction detection and re-
sponse generation in a spoken dialogue system. 
Speech Communication 45, 271-288. 
Justine Cassell, Kristinn R. Thorisson, 1999. The power 
of a nod and aglance: envelope vs. emotional feed-
back in animated conversational agents. Applied Ar-
tificial Intelligence, vol.13, pp.519-538. 
Justine Cassell and Matthew Stone, 1999. Living Hand 
to Mouth: Psychological Theories about Speech and 
Gesture in Interactive Dialogue Systems. Proceed-
ings of the AAAI 1999 Fall Symposium on Psycho-
logical Models of Communication in Collaborative 
Systems, pp. 34-42. November 5-7, North Falmouth, 
MA, 1999. 
Justine Cassell, Timothy W. Bickmore, Hannes 
Vilhj?lmsson and Hao Yan, 2000. More than just a 
pretty face: affordances of embodiment. In Proceed-
ings of the 5th international Conference on intelligent 
User interfaces. 
Justine Cassell, Yukiko I. Nakano, Timothy W. Bick-
more, Candace L. Sidner and  Charles Rich, 2001. 
(on-verbal cues for discourse structure. In Proceed-
ings of the 39th Annual Meeting on Association For 
Computational Linguistics. 
Richard Catrambone, 2002 Anthropomorphic agents as 
a user interface paradigm: Experimental findings 
and a framework for research. In: Proceedings of the 
24th Annual Conference of the Cognitive Science 
Society (pp. 166-171), Fairfax, VA, August. 
Nicole Chovil, 1992. Discourse-Oriented Facial Dis-
plays in Conversation. Research on Language and 
Social Interaction, 25, 163-194. 
Petra Fagerberg, Anna St?hl, Kristina H??k, 2003. De-
signing Gestures for Afective Input: an Analysis of 
Shape, Effort and Valence. In Proceedings of Mobile 
Ubiquitious and Multimedia, Norrk?ping, Sweden. 
Julie Goldberg, Mari Ostendorf, Katrin Kirchhoff, 2003. 
The impact of response wording in error correction 
subdialogs, In EHSD-2003, 101-106. 
Kate Hone, 2005. Animated Agents to reduce user frus-
tration, in The 19th British HCI Group Annual Con-
ference, Edinburgh, UK. 
Adam Kendon, 1990. Conducting interaction: patterns 
of behaviour in focused encounters, Cambridge Uni-
versity Press. 
Andrea Kleinsmith, P. Ravindra De Silva, Nadia Bian-
chi-Berthouze, 2006 Cross-cultural differences in 
recognizing affect from body posture Interacting with 
computers 10  1371-1389 
Robert M. Krauss, Yihsiu Chen and Purnima Chawla, 
1996 (onverbal behavior and nonverbal communica-
tion: What do conversational hand gestures tell us? 
In M. Zanna (Ed.), Advances in experimental social 
psychology (pp. 389 450).San Diego, CA: Academic 
Press. 
Dominic W. Massaro, Michael M. Cohen, Jonas 
Beskow and Ronald A. Cole,  2000.Developing and 
evaluating conversational agents. In Embodied Con-
versational Agents MIT Press, Cambridge, MA, 287-
318. 
Sharon Oviatt. 1994. Interface techniques for minimiz-
ing disfluent input to spoken language systems. In 
Proc. CHI'94 (pp. 205-210) Boston, ACM Press, 
1994 
Sharon Oviatt and Robert VanGent, 1996, Error resolu-
tion duringmultimodal humancomputer interaction. 
Proc. International Conference on Spoken Language 
Processing, 1 204-207. 
Sharon Oviatt, Margaret MacEachern, and Gina-Anne 
Levow, G.,1998. Predicting hyperarticulate speech 
during human-computer error resolution. Speech 
Communication, vol.24, 2, 1-23. 
Sharon Oviatt, and Bridget Adams, 2000. Designing 
and evaluating conversational interfaces with ani-
mated characters. Embodied conversational agents, 
MIT Press: 319-345. 
Isabella Poggi, 2001. How to decide which gesture to 
make according to our goals and our contextual 
knowledge. Paper presented at Gesture Workshop 
2001 London 18th-20th April, 2001 
Ruben San-Segundo, Juan M. Montero, Javier Ferreiros, 
Ricardo C?rdoba, Jose M. Pardo, 2001 Designing 
Confirmation Mechanisms and Error Recover Tech-
niques in a Railway Information System for Spanish. 
SIGDIAL. Septiembre 1-2,  Aalborg (Dinamarca). 
Heike Schaumburg, 2001. Computers as tools or as 
social actors?the users' perspective on anthropomor-
phic agents.International Journal of Cooperative In-
formation Systems.10, 1, 2, 217-234. 
 
74
227
228
229
230
