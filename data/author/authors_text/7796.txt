Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 161?168,
Sydney, July 2006. c?2006 Association for Computational Linguistics
PCFGs with Syntactic and Prosodic Indicators of Speech Repairs
John Halea Izhak Shafranb Lisa Yungc
Bonnie Dorrd Mary Harperde Anna Krasnyanskayaf Matthew Leaseg
Yang Liuh Brian Roarki Matthew Snoverd Robin Stewartj
a Michigan State University; b,c Johns Hopkins University; d University of Maryland, College Park; e Purdue University
f UCLA; g Brown University; h University of Texas at Dallas; i Oregon Health & Sciences University; j Williams College
Abstract
A grammatical method of combining two
kinds of speech repair cues is presented.
One cue, prosodic disjuncture, is detected
by a decision tree-based ensemble clas-
sifier that uses acoustic cues to identify
where normal prosody seems to be inter-
rupted (Lickley, 1996). The other cue,
syntactic parallelism, codifies the expec-
tation that repairs continue a syntactic
category that was left unfinished in the
reparandum (Levelt, 1983). The two cues
are combined in a Treebank PCFG whose
states are split using a few simple tree
transformations. Parsing performance on
the Switchboard and Fisher corpora sug-
gests that these two cues help to locate
speech repairs in a synergistic way.
1 Introduction
Speech repairs, as in example (1), are one kind
of disfluent element that complicates any sort
of syntax-sensitive processing of conversational
speech.
(1) and [ the first kind of invasion of ] the first
type of privacy seemed invaded to me
The problem is that the bracketed reparan-
dum region (following the terminology of Shriberg
(1994)) is approximately repeated as the speaker
The authors are very grateful for Eugene Charniak?s help
adapting his parser. We also thank the Center for Language
and Speech processing at Johns Hopkins for hosting the sum-
mer workshop where much of this work was done. This
material is based upon work supported by the National Sci-
ence Foundation (NSF) under Grant No. 0121285. Any opin-
ions, findings and conclusions or recommendations expressed
in this material are those of the authors and do not necessarily
reflect the views of the NSF.
?repairs? what he or she has already uttered.
This extra material renders the entire utterance
ungrammatical?the string would not be gener-
ated by a correct grammar of fluent English. In
particular, attractive tools for natural language
understanding systems, such as Treebank gram-
mars for written corpora, naturally lack appropri-
ate rules for analyzing these constructions.
One possible response to this mismatch be-
tween grammatical resources and the brute facts
of disfluent speech is to make one look more
like the other, for the purpose of parsing. In
this separate-processing approach, reparanda are
located through a variety of acoustic, lexical or
string-based techniques, then excised before sub-
mission to a parser (Stolcke and Shriberg, 1996;
Heeman and Allen, 1999; Spilker et al, 2000;
Johnson and Charniak, 2004). The resulting
parse tree then has the reparandum re-attached in
a standardized way (Charniak and Johnson, 2001).
An alternative strategy, adopted in this paper, is
to use the same grammar to model fluent speech,
disfluent speech, and their interleaving.
Such an integrated approach can use syntac-
tic properties of the reparandum itself. For in-
stance, in example (1) the reparandum is an
unfinished noun phrase, the repair a finished
noun phrase. This sort of phrasal correspon-
dence, while not absolute, is strong in conver-
sational speech, and cannot be exploited on the
separate-processing approach. Section 3 applies
metarules (Weischedel and Sondheimer, 1983;
McKelvie, 1998a; Core and Schubert, 1999) in
recognizing these correspondences using standard
context-free grammars.
At the same time as it defies parsing, con-
versational speech offers the possibility of lever-
aging prosodic cues to speech repairs. Sec-
161
Figure 1: The pause between two or s and the glottalization at the end of the first makes it easy for a
listener to identify the repair.
tion 2 describes a classifier that learns to label
prosodic breaks suggesting upcoming disfluency.
These marks can be propagated up into parse
trees and used in a probabilistic context-free gram-
mar (PCFG) whose states are systematically split
to encode the additional information.
Section 4 reports results on Switchboard (God-
frey et al, 1992) and Fisher EARS RT04F data,
suggesting these two features can bring about in-
dependent improvements in speech repair detec-
tion. Section 5 suggests underlying linguistic and
statistical reasons for these improvements. Sec-
tion 6 compares the proposed grammatical method
to other related work, including state of the art
separate-processing approaches. Section 7 con-
cludes by indicating a way that string- and tree-
based approaches to reparandum identification
could be combined.
2 Prosodic disjuncture
Everyday experience as well as acoustic anal-
ysis suggests that the syntactic interruption in
speech repairs is typically accompanied by a
change in prosody (Nakatani and Hirschberg,
1994; Shriberg, 1994). For instance, the spectro-
gram corresponding to example (2), shown in Fig-
ure 1,
(2) the jehovah?s witness or [ or ] mormons or
someone
reveals a noticeable pause between the occurrence
of the two ors, and an unexpected glottalization at
the end of the first one. Both kinds of cues have
been advanced as explanations for human listen-
ers? ability to identify the reparandum even before
the repair occurs.
Retaining only the second explanation, Lickley
(1996) proposes that there is no ?edit signal? per se
but that repair is cued by the absence of smooth
formant transitions and lack of normal juncture
phenomena.
One way to capture this notion in the syntax
is to enhance the input with a special disjunc-
ture symbol. This symbol can then be propa-
gated in the grammar, as illustrated in Figure 2.
This work uses a suffix ?+ to encode the percep-
tion of abnormal prosody after a word, along with
phrasal -BRK tags to decorate the path upwards to
reparandum constituents labeled EDITED. Such
NP
NP EDITED CC NP
NP NNP CC?BRK or NNPS
DT NNP POS witness
the jehovah ?s
or~+ mormons
Figure 2: Propagating BRK, the evidence of dis-
fluent juncture, from acoustics to syntax.
disjuncture symbols are identified in the ToBI la-
beling scheme as break indices (Price et al, 1991;
Silverman et al, 1992).
The availability of a corpus annotated with
ToBI labels makes it possible to design a break
index classifier via supervised training. The cor-
pus is a subset of the Switchboard corpus, con-
sisting of sixty-four telephone conversations man-
ually annotated by an experienced linguist accord-
ing to a simplified ToBI labeling scheme (Osten-
dorf et al, 2001). In ToBI, degree of disjuncture
is indicated by integer values from 0 to 4, where
a value of 0 corresponds to clitic and 4 to a major
phrase break. In addition, a suffix p denotes per-
ceptually disfluent events reflecting, for example,
162
hesitation or planning. In conversational speech
the intermediate levels occur infrequently and the
break indices can be broadly categorized into three
groups, namely, 1, 4 and p as in Wong et al
(2005).
A classifier was developed to predict three
break indices at each word boundary based on
variations in pitch, duration and energy asso-
ciated with word, syllable or sub-syllabic con-
stituents (Shriberg et al, 2005; Sonmez et al,
1998). To compute these features, phone-level
time-alignments were obtained from an automatic
speech recognition system. The duration of these
phonological constituents were derived from the
ASR alignment, while energy and pitch were com-
puted every 10ms with snack, a public-domain
sound toolkit (Sjlander, 2001). The duration, en-
ergy, and pitch were post-processed according to
stylization procedures outlined in Sonmez et al
(1998) and normalized to account for variability
across speakers.
Since the input vector can have missing val-
ues such as the absence of pitch during unvoiced
sound, only decision tree based classifiers were
investigated. Decision trees can handle missing
features gracefully. By choosing different com-
binations of splitting and stopping criteria, an
ensemble of decision trees was built using the
publicly-available IND package (Buntine, 1992).
These individual classifiers were then combined
into ensemble-based classifiers.
Several classifiers were investigated for detect-
ing break indices. On ten-fold cross-validation,
a bagging-based classifier (Breiman, 1996) pre-
dicted prosodic breaks with an accuracy of 83.12%
while chance was 67.66%. This compares favor-
ably with the performance of the supervised classi-
fiers on a similar task in Wong et al (2005). Ran-
dom forests and hidden Markov models provide
marginal improvements at considerable computa-
tional cost (Harper et al, 2005).
For speech repair, the focus is on detecting dis-
fluent breaks. The precision and recall trade-off
on its detection can be adjusted using a thresh-
old on the posterior probability of predicting ?p?,
as shown in Figure 3.
In essence, the large number of acoustic and
prosodic features related to disfluency are encoded
via the ToBI label ?p?, and provided as additional
observations to the PCFG. This is unlike previous
work on incorporating prosodic information (Gre-
00.10.20.30.40.50.6 0
0.1
0.2
0.3
0.4
0.5
0.6
Probability of Miss
Probab
ility of 
False 
Alarm
Figure 3: DET curve for detecting disfluent breaks
from acoustics.
gory et al, 2004; Lease et al, 2005; Kahn et al,
2005) as described further in Section 6.
3 Syntactic parallelism
The other striking property of speech repairs is
their parallel character: subsequent repair regions
?line up? with preceding reparandum regions. This
property can be harnessed to better estimate the
length of the reparandum by considering paral-
lelism from the perspective of syntax. For in-
stance, in Figure 4(a) the unfinished reparandum
noun phrase is repaired by another noun phrase ?
the syntactic categories are parallel.
3.1 Levelt?s WFR and Conjunction
The idea that the reparandum is syntactically par-
allel to the repair can be traced back to Levelt
(1983). Examining a corpus of Dutch picture de-
scriptions, Levelt proposes a bi-conditional well-
formedness rule for repairs (WFR) that relates the
structure of repairs to the structure of conjunc-
tions. The WFR conceptualizes repairs as the con-
junction of an unfinished reparandum string (?)
with a properly finished repair (?). Its original
formulation, repeated here, ignores optional inter-
regna like ?er? or ?I mean.?
Well-formedness rule for repairs (WFR) A re-
pair ???? is well-formed if and only if there
is a string ? such that the string ??? and? ??
is well-formed, where ? is a completion of
the constituent directly dominating the last
element of ?. (and is to be deleted if that
last element is itself a sentence connective)
In other words, the string ? is a prefix of a phrase
whose completion, ??if it were present?would
163
render the whole phrase ?? grammatically con-
joinable with the repair ?. In example (1) ? is the
string ?the first kind of invasion of?, ? is ?the first
type of privacy? and ? is probably the single word
?privacy.?
This kind of conjoinability typically requires
the syntactic categories of the conjuncts to be the
same (Chomsky, 1957, 36). That is, a rule schema
such as (2) where X is a syntactic category, is pre-
ferred over one where X is not constrained to be
the same on either side of the conjunction.
X ? X Conj X (2)
If, as schema (2) suggests, conjunction does fa-
vor like-categories, and, as Levelt suggests, well-
formed repairs are conjoinable with finished ver-
sions of their reparanda, then the syntactic cate-
gories of repairs ought to match the syntactic cat-
egories of (finished versions of) reparanda.
3.2 A WFR for grammars
Levelt?s WFR imposes two requirements on a
grammar
? distinguishing a separate category of ?unfin-
ished? phrases
? identifying a syntactic category for reparanda
Both requirements can be met by adapting Tree-
bank grammars to mirror the analysis of McK-
elvie1 (1998a; 1998b). McKelvie derives phrase
structure rules for speech repairs from fluent rules
by adding a new feature called abort that can
take values true and false. For a given gram-
mar rule of the form
A ? B C
a metarule creates other rules of the form
A [abort = Q] ?
B [abort = false] C [abort = Q]
where Q is a propositional variable. These rules
say, in effect, that the constituent A is aborted just
in case the last daughter C is aborted. Rules that
don?t involve a constant value for Q ensure that the
same value appears on parents and children. The
1McKelvie?s metarule approach declaratively expresses
Hindle?s (1983) Stack Editor and Category Copy Editor rules.
This classic work effectively states the WFR as a program for
the Fidditch deterministic parser.
WFR is then implemented by rule schemas such
as (3)
X ? X [abort = true] (AFF) X (3)
that permit the optional interregnum AFF to con-
join an unfinished X-phrase (the reparandum) with
a finished X-phrase (the repair) that comes after it.
3.3 A WFR for Treebanks
McKelvie?s formulation of Levelt?s WFR can be
applied to Treebanks by systematically recoding
the annotations to indicate which phrases are un-
finished and to distinguish matching from non-
matching repairs.
3.3.1 Unfinished phrases
Some Treebanks already mark unfinished
phrases. For instance, the Penn Treebank pol-
icy (Marcus et al, 1993; Marcus et al, 1994) is
to annotate the lowest node that is unfinished with
an -UNF tag as in Figure 4(a).
It is straightforward to propagate this mark up-
wards in the tree from wherever it is annotated to
the nearest enclosing EDITED node, just as -BRK
is propagated upwards from disjuncture marks on
individual words. This percolation simulates the
action of McKelvie?s [abort = true]. The re-
sulting PCFG is one in which distributions on
phrase structure rules with ?missing? daughters are
segregated from distributions on ?complete? rules.
3.4 Reparanda categories
The other key element of Levelt?s WFR is the
idea of conjunction of elements that are in some
sense the same. In the Penn Treebank annota-
tion scheme, reparanda always receive the label
EDITED. This means that the syntactic category
of the reparandum is hidden from any rule which
could favor matching it with that of the repair.
Adding an additional mark on this EDITED node
(a kind of daughter annotation) rectifies the situ-
ation, as depicted in Figure 4(b), which adds the
notation -childNP to a tree in which the unfin-
ished tags have been propagated upwards. This
allows a Treebank PCFG to represent the general-
ization that speech repairs tend to respect syntactic
category.
4 Results
Three kinds of experiments examined the effec-
tiveness of syntactic and prosodic indicators of
164
SCC EDITED NP
and NP NP
NP PP
DT JJ NN IN NP
the first kind of NP PP?UNF
NN IN
invasion of
DT JJ NN
the first type
(a) The lowest unfinished node is given.
S
CC EDITED?childNP NP
and NP?UNF NP
NP PP?UNF
DT JJ NN IN NP?UNF
the first kind of NP PP?UNF
NN IN
invasion of
DT JJ NN
the first type
(b) -UNF propagated, daughter-annotated Switchboard tree
Figure 4: Input (a) and output (b) of tree transformations.
speech repairs. The first two use the CYK algo-
rithm to find the most likely parse tree on a gram-
mar read-off from example trees annotated as in
Figures 2 and 4. The third experiment measures
the benefit from syntactic indicators alone in Char-
niak?s lexicalized parser (Charniak, 2000). The ta-
bles in subsections 4.1, 4.2, and 4.3 summarize
the accuracy of output parse trees on two mea-
sures. One is the standard Parseval F-measure,
which tracks the precision and recall for all labeled
constituents as compared to a gold-standard parse.
The other measure, EDIT-finding F, restricts con-
sideration to just constituents that are reparanda. It
measures the per-word performance identifying a
word as dominated by EDITED or not. As in pre-
vious studies, reference transcripts were used in all
cases. A check (
?
) indicates an experiment where
prosodic breaks where automatically inferred by
the classifier described in section 2, whereas in the
(?) rows no prosodic information was used.
4.1 CYK on Fisher
Table 1 summarizes the accuracy of a stan-
dard CYK parser on the newly-treebanked
Fisher corpus (LDC2005E15) of phone conver-
sations, collected as part of the DARPA EARS
program. The parser was trained on the entire
Switchboard corpus (ca. 107K utterances) then
tested on the 5368-utterance ?dev2? subset of the
Fisher data. This test set was tagged using MX-
POST (Ratnaparkhi, 1996) which was itself trained
on Switchboard. Finally, as described in section 2
these tags were augmented with a special prosodic
break symbol if the decision tree rated the proba-
bility a ToBI ?p? symbol higher than the threshold
value of 0.75.
A
nn
ot
at
io
n
Br
ea
k
in
de
x
Pa
rs
ev
a
lF
ED
IT
F
none
? 66.54 22.9?
66.08 26.1
daughter annotation ? 66.41 29.4? 65.81 31.6
-UNF propagation ? 67.06 31.5? 66.45 34.8
both ? 69.21 40.2? 67.02 40.6
Table 1: Improvement on Fisher, MXPOSTed tags.
The Fisher results in Table 1 show that syntac-
tic and prosodic indicators provide different kinds
of benefits that combine in an additive way. Pre-
sumably because of state-splitting, improvement
in EDIT-finding comes at the cost of a small decre-
ment in overall parsing performance.
4.2 CYK on Switchboard
Table 2 presents the results of similar experi-
ments on the Switchboard corpus following the
165
train/dev/test partition of Charniak and Johnson
(2001). In these experiments, the parser was given
correct part-of-speech tags as input.
A
nn
ot
at
io
n
Br
ea
k
in
de
x
Pa
rs
ev
a
lF
ED
IT
F
none
? 70.92 18.2?
69.98 22.5
daughter annotation ? 71.13 25.0? 70.06 25.5
-UNF propagation ? 71.71 31.1? 70.36 30.0
both ? 71.16 41.7? 71.05 36.2
Table 2: Improvement on Switchboard, gold tags.
The Switchboard results demonstrate independent
improvement from the syntactic annotations. The
prosodic annotation helps on its own and in com-
bination with the daughter annotation that imple-
ments Levelt?s WFR.
4.3 Lexicalized parser
Finally, Table 3 reports the performance of Char-
niak?s non-reranking, lexicalized parser on the
Switchboard corpus, using the same test/dev/train
partition.
Annotation Parseval F EDIT F
baseline 83.86 57.6
daughter annotation 80.85 67.2
-UNF propagation 81.68 64.7
both 80.16 70.0
flattened EDITED 82.13 64.4
Table 3: Charniak as an improved EDIT-finder.
Since Charniak?s parser does its own tagging,
this experiment did not examine the utility of
prosodic disjuncture marks. However, the com-
bination of daughter annotation and -UNF prop-
agation does lead to a better grammar-based
reparandum-finder than parsers trained on flat-
tened EDITED regions. More broadly, the re-
sults suggest that Levelt?s WFR is synergistic with
the kind of head-to-head lexical dependencies that
Charniak?s parser uses.
5 Discussion
The pattern of improvement in tables 1, 2, and
3 from none or baseline rows where no syntac-
tic parallelism or break index information is used,
to subsequent rows where it is used, suggest why
these techniques work. Unfinished-category an-
notation improves performance by preventing the
grammar of unfinished constituents from being
polluted by the grammar of finished constituents.
Such purification is independent of the fact that
rules with daughters labeled EDITED-childXP
tend to also mention categories labeled XP fur-
ther to the right (or NP and VP, when XP starts
with S). This preference for syntactic parallelism
can be triggered either by externally-suggested
ToBI break indices or grammar rules annotated
with -UNF. The prediction of a disfluent break
could be further improved by POS features and N-
gram language model scores (Spilker et al, 2001;
Liu, 2004).
6 Related Work
There have been relatively few attempts to harness
prosodic cues in parsing. In a spoken language
system for VERBMOBIL task, Batliner and col-
leagues (2001) utilize prosodic cues to dramati-
cally reduce lexical analyses of disfluencies in a
end-to-end real-time system. They tackle speech
repair by a cascade of two stages ? identification of
potential interruption points using prosodic cues
with 90% recall and many false alarms, and the
lexical analyses of their neighborhood. Their ap-
proach, however, does not exploit the synergy be-
tween prosodic and syntactic features in speech re-
pair. In Gregory et al (2004), over 100 real-valued
acoustic and prosodic features were quantized into
a heuristically selected set of discrete symbols,
which were then treated as pseudo-punctuation in
a PCFG, assuming that prosodic cues function like
punctuation. The resulting grammar suffered from
data sparsity and failed to provide any benefits.
Maximum entropy based models have been more
successful in utilizing prosodic cues. For instance,
in Lease et al (2005), interruption point probabil-
ities, predicted by prosodic classifiers, were quan-
tized and introduced as features into a speech re-
pair model along with a variety of TAG and PCFG
features. Towards a clearer picture of the inter-
action with syntax and prosody, this work uses
ToBI to capture prosodic cues. Such a method is
analogous to Kahn et al (2005) but in a genera-
tive framework.
The TAG-based model of Johnson and Charniak
(2004) is a separate-processing approach that rep-
166
resents the state of the art in reparandum-finding.
Johnson and Charniak explicitly model the
crossed dependencies between individual words
in the reparandum and repair regions, intersect-
ing this sequence model with a parser-derived lan-
guage model for fluent speech. This second step
improves on Stolcke and Shriberg (1996) and Hee-
man and Allen (1999) and outperforms the specific
grammar-based reparandum-finders tested in sec-
tion 4. However, because of separate-processing
the TAG channel model?s analyses do not reflect
the syntactic structure of the sentence being ana-
lyzed, and thus that particular TAG-based model
cannot make use of properties that depend on the
phrase structure of the reparandum region. This
includes the syntactic category parallelism dis-
cussed in section 3 but also predicate-argument
structure. If edit hypotheses were augmented to
mention particular tree nodes where the reparan-
dum should be attached, such syntactic paral-
lelism constraints could be exploited in the rerank-
ing framework of Johnson et al (2004).
The approach in section 3 is more closely re-
lated to that of Core and Schubert (1999) who
also use metarules to allow a parser to switch from
speaker to speaker as users interrupt one another.
They describe their metarule facility as a modi-
fication of chart parsing that involves copying of
specific arcs just in case specific conditions arise.
That approach uses a combination of longest-first
heuristics and thresholds rather than a complete
probabilistic model such as a PCFG.
Section 3?s PCFG approach can also be viewed
as a declarative generalization of Roark?s (2004)
EDIT-CHILD function. This function helps an
incremental parser decide upon particular tree-
drawing actions in syntactically-parallel contexts
like speech repairs. Whereas Roark conditions the
expansion of the first constituent of the repair upon
the corresponding first constituent of the reparan-
dum, in the PCFG approach there exists a separate
rule (and thus a separate probability) for each al-
ternative sequence of reparandum constituents.
7 Conclusion
Conventional PCFGs can improve their detection
of speech repairs by incorporating Lickley?s hy-
pothesis about interrupted prosody and by im-
plementing Levelt?s well-formedness rule. These
benefits are additive.
The strengths of these simple tree-based tech-
niques should be combinable with sophisticated
string-based (Johnson and Charniak, 2004; Liu,
2004; Zhang and Weng, 2005) approaches by
applying the methods of Wieling et al (2005)
for constraining parses by externally-suggested
brackets.
References
L. Breiman. 1996. Bagging predictors. Machine
Learning, 24(2):123?140.
W. Buntine. 1992. Tree classication software. In Tech-
nology 2002: The Third National Technology Trans-
fer Conference and Exposition, Baltimore.
E. Charniak and M. Johnson. 2001. Edit detection
and parsing for transcribed speech. In Proceedings
of the 2nd Meeting of the North American Chap-
ter of the Association for Computational Linguistics,
pages 118?126.
E. Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of NAACL-00, pages 132?
139.
N. Chomsky. 1957. Syntactic Structures. Anua Lin-
guarum Series Minor 4, Series Volume 4. Mouton
de Gruyter, The Hague.
M. G. Core and L. K. Schubert. 1999. A syntactic
framework for speech repairs and other disruptions.
In Proceedings of the 37th Annual Meeting of the As-
sociation for Computational Linguistics, pages 413?
420.
J. J. Godfrey, E. C. Holliman, and J. McDaniel. 1992.
SWITCHBOARD: Telephone speech corpus for re-
search and development. In Proceedings of ICASSP,
volume I, pages 517?520, San Francisco.
M. Gregory, M. Johnson, and E. Charniak. 2004.
Sentence-internal prosody does not help parsing the
way punctuation does. In Proceedings of North
American Association for Computational Linguis-
tics.
M. Harper, B. Dorr, J. Hale, B. Roark, I. Shafran,
M. Lease, Y. Liu, M. Snover, and L. Yung. 2005.
Parsing and spoken structural event detection. In
2005 Johns Hopkins Summer Workshop Final Re-
port.
P. A. Heeman and J. F. Allen. 1999. Speech repairs,
intonational phrases and discourse markers: model-
ing speakers? utterances in spoken dialog. Compu-
tational Linguistics, 25(4):527?571.
D. Hindle. 1983. Deterministic parsing of syntactic
non-fluencies. In Proceedings of the ACL.
M. Johnson and E. Charniak. 2004. A TAG-based
noisy channel model of speech repairs. In Proceed-
ings of ACL, pages 33?39.
167
M. Johnson, E. Charniak, and M. Lease. 2004. An im-
proved model for recognizing disfluencies in conver-
sational speech. In Proceedings of Rich Transcrip-
tion Workshop.
J. G. Kahn, M. Lease, E. Charniak, M. Johnson, and
M. Ostendorf. 2005. Effective use of prosody in
parsing conversational speech. In Proceedings of
Human Language Technology Conference and Con-
ference on Empirical Methods in Natural Language
Processing, pages 233?240.
M. Lease, E. Charniak, and M. Johnson. 2005. Pars-
ing and its applications for conversational speech. In
Proceedings of ICASSP.
W. J. M. Levelt. 1983. Monitoring and self-repair in
speech. Cognitive Science, 14:41?104.
R. J. Lickley. 1996. Juncture cues to disfluency. In
Proceedings the International Conference on Speech
and Language Processing.
Y. Liu. 2004. Structural Event Detection for Rich
Transcription of Speech. Ph.D. thesis, Purdue Uni-
versity.
M. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993. Building a large annotated corpus of En-
glish: The Penn Treebank. Computational Linguis-
tics, 19(2):313?330.
M. Marcus, G. Kim, M. A. Marcinkiewicz, R. MacIn-
tyre, A. Bies, M. Ferguson, K. Katz, and B. Schas-
berger. 1994. The Penn Treebank: Annotating
Predicate Argument Structure. In Proceedings of
the 1994 ARPA Human Language Technology Work-
shop.
D. McKelvie. 1998a. SDP ? Spoken Dialog Parser.
ESRC project on Robust Parsing and Part-of-speech
Tagging of Transcribed Speech Corpora, May.
D. McKelvie. 1998b. The syntax of disfluency in spon-
taneous spoken language. ESRC project on Robust
Parsing and Part-of-speech Tagging of Transcribed
Speech Corpora, May.
C. Nakatani and J. Hirschberg. 1994. A corpus-based
study of repair cues in spontaneous speech. Journal
of the Acoustical Society of America, 95(3):1603?
1616, March.
M. Ostendorf, I. Shafran, S. Shattuck-Hufnagel,
L. Carmichael, and W. Byrne. 2001. A prosodically
labelled database of spontaneous speech. In Proc.
ISCA Tutorial and Research Workshop on Prosody
in Speech Recognition and Understanding, pages
119?121.
P. Price, M. Ostendorf, S. Shattuck-Hufnagel, and
C. Fong. 1991. The use of prosody in syntactic
disambiguation. Journal of the Acoustic Society of
America, 90:2956?2970.
A. Ratnaparkhi. 1996. A maximum entropy part-of-
speech tagger. In Proceedings of Empirical Methods
in Natural Language Processing Conference, pages
133?141.
B. Roark. 2004. Robust garden path parsing. Natural
Language Engineering, 10(1):1?24.
E. Shriberg, L. Ferrer, S. Kajarekar, A. Venkataraman,
and A. Stolcke. 2005. Modeling prosodic feature
sequences for speaker recognition. Speech Commu-
nication, 46(3-4):455?472.
E. Shriberg. 1994. Preliminaries to a Theory of Speech
Disfluencies. Ph.D. thesis, UC Berkeley.
H. F. Silverman, M. Beckman, J. Pitrelli, M. Ostendorf,
C. Wightman, P. Price, J. Pierrehumbert, and J. Hir-
shberg. 1992. ToBI: A standard for labeling English
prosody. In Proceedings of ICSLP, volume 2, pages
867?870.
K. Sjlander, 2001. The Snack sound visualization mod-
ule. Royal Institute of Technology in Stockholm.
http://www.speech.kth.se/SNACK.
K. Sonmez, E. Shriberg, L. Heck, and M. Weintraub.
1998. Modeling dynamic prosodic variation for
speaker verification. In Proceedings of ICSLP, vol-
ume 7, pages 3189?3192.
Jo?rg Spilker, Martin Klarner, and Gu?nther Go?rz. 2000.
Processing self-corrections in a speech-to-speech
system. In Wolfgang Wahlster, editor, Verbmobil:
Foundations of speech-to-speech translation, pages
131?140. Springer-Verlag, Berlin.
J. Spilker, A. Batliner, and E. No?th. 2001. How to
repair speech repairs in an end-to-end system. In
R. Lickley and L. Shriberg, editors, Proc. of ISCA
Workshop on Disfluency in Spontaneous Speech,
pages 73?76.
A. Stolcke and E. Shriberg. 1996. Statistical language
modeling for speech disfluencies. In Proceedings
of the IEEE International Conference on Acoustics,
Speech and Signal Processing, pages 405?408, At-
lanta, GA.
R. M. Weischedel and N. K. Sondheimer. 1983.
Meta-rules as a basis for processing ill-formed in-
put. American Journal of Computational Linguis-
tics, 9(3-4):161?177.
M. Wieling, M-J. Nederhof, and G. van Noord. 2005.
Parsing partially bracketed input. Talk presented at
Computational Linguistics in the Netherlands.
D. Wong, M. Ostendorf, and J. G. Kahn. 2005. Us-
ing weakly supervised learning to improve prosody
labeling. Technical Report UWEETR-2005-0003,
University of Washington Electrical Engineering
Dept.
Q. Zhang and F. Weng. 2005. Exploring features for
identifying edited regions in disfluent sentences. In
Proceedings of the Nineth International Workshop
on Parsing Technologies, pages 179?185.
168
Proceedings of the Analyzing Conversations in Text and Speech (ACTS) Workshop at HLT-NAACL 2006, pages 8?14,
New York City, New York, June 2006. c?2006 Association for Computational Linguistics
Off-Topic Detection in Conversational Telephone Speech
Robin Stewart and Andrea Danyluk
Department of Computer Science
Williams College
Williamstown, MA 01267
{06rss 2, andrea}@cs.williams.edu
Yang Liu
Department of Computer Science
UT Dallas
Richardson, TX 75080
yangl@hlt.utdallas.edu
Abstract
In a context where information retrieval is
extended to spoken ?documents? includ-
ing conversations, it will be important to
provide users with the ability to seek in-
formational content, rather than socially
motivated small talk that appears in many
conversational sources. In this paper we
present a preliminary study aimed at auto-
matically identifying ?irrelevance? in the
domain of telephone conversations. We
apply a standard machine learning algo-
rithm to build a classifier that detects off-
topic sections with better-than-chance ac-
curacy and that begins to provide insight
into the relative importance of features for
identifying utterances as on topic or not.
1 Introduction
There is a growing need to index, search, summa-
rize and otherwise process the increasing amount of
available broadcast news, broadcast conversations,
meetings, class lectures, and telephone conversa-
tions. While it is clear that users have wide ranging
goals in the context of information retrieval, we as-
sume that some will seek only credible information
about a specific topic and will not be interested in the
socially-motivated utterances which appear through-
out most conversational sources. For these users,
a search for information about weather should not
return conversations containing small talk such as
?Nice weather we?ve been having.?
In this paper we investigate one approach for auto-
matically identifying ?irrelevance? in the domain of
telephone conversations. Our initial data consist of
conversations in which each utterance is labeled as
being on topic or not. We apply inductive classifier
learning algorithms to identify useful features and
build classifiers to automatically label utterances.
We begin in Section 2 by hypothesizing features
that might be useful for the identification of irrel-
evant regions, as indicated by research on the lin-
guistics of conversational speech and, in particular,
small talk. Next we present our data and discuss our
annotation methodology. We follow this with a de-
scription of the complete set of features and machine
learning algorithms investigated. Section 6 presents
our results, including a comparison of the learned
classifiers and an analysis of the relative utility of
various features.
2 Linguistics of Conversational Speech
Cheepen (Cheepen, 1988) posits that speakers have
two primary goals in conversation: interactional
goals in which interpersonal motives such as social
rank and trust are primary; and transactional goals
which focus on communicating useful information
or getting a job done. In a context where conversa-
tions are indexed and searched for information, we
assume in this paper that users will be interested
in the communicated information, rather than the
way in which participants interact. Therefore, we
assume that utterances with primarily transactional
purposes will be most important, while interactional
utterances can be ignored.
Greetings and partings are the most predictable
8
type of interactional speech. They consistently ap-
pear at the beginning and end of conversations and
follow a fairly formulaic pattern of content (Laver,
1981). Thus we hypothesize that: Utterances near
the beginning or end of conversations are less likely
to be relevant.
Cheepen also defines speech-in-action regions
to be segments of conversation that are related to
the present physical world or the activity of chat-
ting, e.g. ?What lovely weather.? or ?It is so nice
to see you.? Since these regions mainly involve
participants identifying their shared social situation,
they are not likely to contain transactional content.
Further, since speech-in-action segments are distin-
guished by their focus on the present, we hypoth-
esize that: Utterances with present tense verbs are
less likely to be relevant.
Finally, small talk that is not intended to demar-
cate social hierarchy tends to be abbreviated, e.g.
?Nice day? (Laver, 1981). From this we hypothe-
size that: Utterances lacking common helper words
such as ?it?, ?there?, and forms of ?to be? are less
likely to be relevant.
3 Related Work
Three areas of related work in natural language pro-
cessing have been particularly informative for our
research.
First, speech act theory states that with each ut-
terance, a conversant is committing an action, such
as questioning, critiquing, or stating a fact. This is
quite similar to the notion of transactional and inter-
actional goals. However, speech acts are generally
focused on the lower level of breaking apart utter-
ances and understanding their purpose, whereas we
are concerned here with a coarser-grained notion of
relevance. Work closer to ours is that of Bates et
al. (Bates et al, 2005), who define meeting acts for
recorded meetings. Of their tags, commentary is
most similar to our notion of relevance.
Second, there has been research on generating
small talk in order to establish rapport between an
automatic system and human user (Bickmore and
Cassell, 2000). Our work complements this by po-
tentially detecting off-topic speech from the human
user as an indication that the system should also re-
spond with interactional language.
Label Utterance
S 2: [LAUGH] Hi.
S 2: How nice to meet you.
S 1: It is nice to meet you too.
M 2: We have a wonderful topic.
M 1: Yeah.
M 1: It?s not too bad. [LAUGH]
T 2: Oh, I ? I am one hundred percent in
favor of, uh, computers in the classroom.
T 2: I think they?re a marvelous tool,
educational tool.
Table 1: A conversation fragment with annotations:
(S)mall Talk, (M)etaconversation, and On-(T)opic.
The two speakers are identified as ?1? and ?2?.
Third, off-topic detection can be viewed as a seg-
mentation of conversation into relevant and irrele-
vant parts. Thus our work has many similarities to
topic segmentation systems, which incorporate cue
words that indicate an abrupt change in topic (e.g.
?so anyway...?), as well as long term variations in
word occurrence statistics (Hearst, 1997; Reynar,
1999; Beeferman et al, 1999, e.g.). Our approach
uses previous and subsequent sentences to approxi-
mate these ideas, but might benefit from a more ex-
plicitly segmentation-based strategy.
4 Data
In our work we use human-transcribed conversa-
tions from the Fisher data (LDC, 2004). In each con-
versation, participants have been given a topic to dis-
cuss for ten minutes. Despite this, participants often
talk about subjects that are not at all related to the as-
signed topic. Therefore, a convenient way to define
irrelevance in conversations in this domain is seg-
ments which do not contribute to understanding the
assigned topic. This very natural definition makes
the domain a good one for initial study; however,
the idea can be readily extended to other domains.
For example, broadcast debates, class lectures, and
meetings usually have specific topics of discussion.
The primary transactional goal of participants in
the telephone conversations is to discuss the as-
signed topic. Since this goal directly involves the
act of discussion itself, it is not surprising that par-
ticipants often talk about the current conversation or
9
the choice of topic. There are enough such segments
that we assign them a special region type: Metacon-
versation. The purely irrelevant segments we call
Small Talk, and the remaining segments are defined
as On-Topic. We define utterances as segments of
speech that are delineated by periods and/or speaker
changes. An annotated excerpt is shown in Table 1.
For the experiments described in this paper, we
selected 20 conversations: 4 from each of the topics
?computers in education?, ?bioterrorism?, ?terror-
ism?, ?pets?, and ?censorship?. These topics were
chosen randomly from the 40 topics in the Fisher
corpus, with the constraint that we wanted to include
topics that could be a part of normal small talk (such
as ?pets?) as well as topics which seem farther re-
moved from small talk (such as ?censorship?).
Our selected data set consists of slightly more
than 5,000 utterances. We had 2-3 human annota-
tors label the utterances in each conversation, choos-
ing from the 3 labels Metaconversation, Small Talk,
and On-Topic. On average, pairs of annotators
agreed with each other on 86% of utterances. The
main source of annotator disagreement was between
Small Talk and On-Topic regions; in most cases this
resulted from differences in opinion of when exactly
the conversation had drifted too far from the topic to
be relevant.
For the 14% of utterances with mismatched la-
bels, we chose the label that would be ?safest? in the
information retrieval context where small talk might
get discarded. If any of the annotators thought a
given utterance was On-Topic, we kept it On-Topic.
If there was a disagreement between Metaconver-
sation and Small Talk, we used Metaconversation.
Thus, a Small Talk label was only placed if all anno-
tators agreed on it.
5 Experimental Setup
5.1 Features
As indicated in Section 1, we apply machine learn-
ing algorithms to utterances extracted from tele-
phone conversations in order to learn classifiers for
Small Talk, Metaconversation, and On-Topic. We
represent utterances as feature vectors, basing our
selection of features on both linguistic insights and
earlier text classification work. As described in Sec-
tion 2, work on the linguistics of conversational
Small Talk Metaconv. On-Topic
hi topic ,
. i ?
?s it you
yeah this that
? dollars the
hello so and
oh is know
?m what a
in was wouldn
my about to
but talk like
name for his
how me they
we okay of
texas do ?t
there phone he
well ah uh
from times um
are really put
here one just
Table 2: The top 20 tokens for distinguishing each
category, as ranked by the feature quality measure
(Lewis and Gale, 1994).
speech (Cheepen, 1988; Laver, 1981) implies that
the following features might be indicative of small
talk: (1) position in the conversation, (2) the use of
present-tense verbs, and (3) a lack of common helper
words such as ?it?, ?there?, and forms of ?to be?.
To model the effect of proximity to the beginning
of the conversation, we attach to each utterance a
feature that describes its approximate position in the
conversation. We do not include a feature for prox-
imity to the end of the conversation because our tran-
scriptions include only the first ten minutes of each
recorded conversation.
In order to include features describing verb tense,
we use Brill?s part-of-speech tagger (Brill, 1992) .
Each part of speech (POS) is taken to be a feature,
whose value is a count of the number of occurrences
in the given utterance.
To account for the words, we use a bag of words
model with counts for each word. We normalize
words from the human transcripts by converting ev-
erything to lower case and tokenizing contractions
10
Features Values
n word tokens for each word, # occurrences
standard POS tags as in Penn Treebank for each tag, # occurrences
line number in conversation 0-4, 5-9, 10-19, 20-49, >49
utterance type statement, question, fragment
utterance length (number of words) 1, 2, ..., 20, >20
number of laughs laugh count
n word tokens in previous 5 utterances for each word, total # occurrences in 5 previous
tags from POS tagger, previous 5 for each tag, total # occurrences in 5 previous
number of words, previous 5 total from 5 previous
number of laughs, previous 5 total from 5 previous
n word tokens, subsequent 5 utterances for each word, total # occ in 5 subsequent
tags from POS tagger, subsequent 5 for each tag, total # occurrences in 5 subsequent
number of words, subsequent 5 total from 5 subsequent
number of laughs, subsequent 5 total from 5 subsequent
Table 3: Summary of features that describe each utterance.
and punctuation. We rank the utility of words ac-
cording to the feature quality measure presented in
(Lewis and Gale, 1994) because it was devised for
the task of classifying similarly short fragments of
text (news headlines), rather than long documents.
We then consider the top n tokens as features, vary-
ing the number in different experiments. Table 2
shows the most useful tokens for distinguishing be-
tween the three categories according to this metric.
Additionally, we include as features the utterance
type (statement, question, or fragment), number of
words in the utterance, and number of laughs in the
utterance.
Because utterances are long enough to classify in-
dividually but too short to classify reliably, we not
only consider features of the current utterance, but
also those of previous and subsequent utterances.
More specifically, summed features are calculated
for the five preceding utterances and for the five sub-
sequent utterances. The number five was chosen em-
pirically.
It is important to note that there is some overlap
in features. For instance, the token ??? can be ex-
tracted as one of the n word tokens by Lewis and
Gale?s feature quality measure; it is also tagged by
the POS tagger; and it is indicative of the utterance
type, which is encoded as a separate feature as well.
However, redundant features do not make up a sig-
nificant percentage of the overall feature set.
Finally, we note that the conversation topic is not
taken to be a feature, as we cannot assume that con-
versations in general will have such labels. The
complete list of features, along with their possible
values, is summarized in Table 3.
5.2 Experiments
We applied several classifier learning algorithms to
our data: Naive Bayes, Support Vector Machines
(SVMs), 1-nearest neighbor, and the C4.5 decision
tree learning algorithm. We used the implementa-
tions in the Weka package of machine learning al-
gorithms (Witten and Frank, 2005), running the al-
gorithms with default settings. In each case, we per-
formed 4-fold cross-validation, training on sets con-
sisting of three of the conversations in each topic
(15 conversations total) and testing on sets of the re-
maining 1 from each topic (5 total). Average train-
ing set size was approximately 3800 utterances, of
which about 700 were Small Talk and 350 Metacon-
versation. The average test set size was 1270.
6 Results
6.1 Performance of a Learned Classifier
We evaluated the results of our experiments ac-
cording to three criteria: accuracy, error cost, and
plausibility of the annotations produced. In all
11
Algorithm % Accuracy Cohen?s Kappa
SVM 76.6 0.44
C4.5 68.8 0.26
k-NN 64.1 0.20
Naive Bayes 58.9 0.27
Table 4: Classification accuracy and Cohen?s Kappa
statistic for each of the machine learning algorithms
we tried, using all features at the 100-words level.
25 50 75 100 125 150 175
74
76
78
80
82
84
number of words used as features
0 200
acc
ura
cy 
(%)
72
86 Inter-annotator agreement
Baseline
Line numbers only
all features
Part of speech tags only
Figure 1: Classification results using SVMs with
varying numbers of words.
cases our best results were obtained with the SVM.
When evaluated on accuracy, the SVM models were
the only ones that exceeded a baseline accuracy of
72.8%, which is the average percentage of On-Topic
utterances in our data set. Table 4 displays the nu-
merical results using each of the machine learning
algorithms.
Figure 1 shows the average accuracy obtained
with an SVM classifier using all features described
in Section 5.1 except part-of-speech features (for
reasons discussed below), and varying the number
of words considered. While the best results were ob-
tained at the 100-words level, all classifiers demon-
strated significant improvement in accuracy over the
baseline. The average standard deviation over the 4
cross-validation runs of the results shown is 6 per-
centage points.
From a practical perspective, accuracy alone is
S M T <? classified as
55% 7% 38% Small Talk
21% 37% 42% Metaconv.
8% 3% 89% On Topic
Table 5: Confusion matrix for 100-word SVM clas-
sifier.
not an appropriate metric for evaluating our re-
sults. If the goal is to eliminate Small Talk regions
from conversations, mislabeling On-Topic regions
as Small Talk potentially results in the elimination
of useful material. Table 5 shows a confusion ma-
trix for an SVM classifier trained on a data set at the
100-word level. We can see that the classifier is con-
servative, identifying 55% of the Small Talk, but in-
correctly labeling On-Topic utterances as Small Talk
only 8% of the time.
Finally, we analyzed (by hand) the test data anno-
tated by the classifiers. We found that, in general,
the SVM classifiers annotated the conversations in a
manner similar to the human annotators, transition-
ing from one label to another relatively infrequently
as illustrated in Table 1. This is in contrast to the
1-nearest neighbor classifiers, which tended to an-
notate in a far more ?jumpy? style.
6.2 Relative Utility of Features
Several of the features we used to describe our
training and test examples were selected due to the
claims of researchers such as Laver and Cheepen.
We were interested in determining the relative con-
tributions of these various linguistically-motivated
features to our learned classifiers. Figure 1 and Table
6 report some of our findings. Using proximity to the
beginning of the conversation (?line numbers?) as a
sole feature, the SVM classifier achieved an accu-
racy of 75.6%. This clearly verifies the hypothesis
that utterances near the beginning of the conversa-
tion have different properties than those that follow.
On the contrary, when we used only POS tags
to train the SVM classifier, it achieved an accuracy
that falls exactly at the baseline. Moreover, remov-
ing POS features from the SVM classifier improved
results (Table 6). This may indicate that detect-
ing off-topic categories will require focusing on the
words rather than the grammar of utterances. On
12
Condition Accuracy Kappa
All features 76.6 0.44
No word features 75.0 0.19
No line numbers 76.9 0.44
No POS features 77.8 0.46
No utterance type, length, 76.9 0.45
or # laughs
No previous/next info 76.3 0.21
Only word features 77.9 0.46
Only line numbers 75.6 0.16
Only POS features 72.8 0.00
Only utterance type, length, 74.1 0.09
and # laughs
Table 6: Percent accuracy and Cohen?s Kappa statis-
tic for the SVM at the 100-words level when features
were left out or put in individually.
the other hand, part of speech information is im-
plicit in the words (for example, an occurrence of
?are? also indicates a present tense verb), so perhaps
labeling POS tags does not add any new informa-
tion. It is also possible that some other detection
approach and/or richer syntactic information (such
as parse trees) would be beneficial.
Finally, the words with the highest feature qual-
ity measure (Table 2) clearly refute most of the third
linguistic prediction. Helper words like ?it?, ?there?,
and ?the? appear roughly evenly in each region type.
Moreover, all of the verbs in the top 20 Small Talk
list are forms of ?to be? (some of them contracted
as in ?I?m?), while no ?to be? words appear in the
list for On-Topic. This is further evidence that dif-
ferentiating off-topic speech depends deeply on the
meaning of the words rather than on some more eas-
ily extracted feature.
7 Future Work
There are many ways in which we plan to expand
upon this preliminary study. We are currently in the
process of annotating more data and including ad-
ditional conversation topics. Other future work in-
cludes:
? applying topic segmentation approaches to our
data and comparing the results to those we have
obtained so far;
? investigating alternate approaches for detecting
Small Talk regions, such as smoothing with a
Hidden Markov Model;
? using semi-supervised and active learning tech-
niques to better utilize the large amount of un-
labeled data;
? running the experiments with automatically
generated (speech recognized) transcriptions,
rather than the human-generated transcriptions
that we have used to date. Our expectation is
that such transcripts will contain more noise
and thus pose new challenges;
? including prosodic information in the feature
set.
Acknowledgements
The authors would like to thank Mary Harper, Brian
Roark, Jeremy Kahn, Rebecca Bates, and Joe Cruz
for providing invaluable advice and data. We would
also like to thank the student volunteers at Williams
who helped annotate the conversation transcripts,
as well as the 2005 Johns Hopkins CLSP summer
workshop, where this research idea was formulated.
References
Rebecca Bates, Patrick Menning, Elizabeth Willingham,
and Chad Kuyper. 2005. Meeting Acts: A Labeling
System for Group Interaction in Meetings. August.
Doug Beeferman, Adam Berger, and John Lafferty.
1999. Statistical Models for Text Segmentation. Ma-
chine Learning.
Timothy Bickmore and Justine Cassell. 2000. How
about this weather?: Social Dialogue with Embodied
Conversational Agents. AAAI Fall Symposium on So-
cially Intelligent Agents.
Eric Brill. 1992. A simple rule-based part of speech tag-
ger. Proc. of the Third Conference on Applied NLP.
Christine Cheepen. 1988. The Predictability of Informal
Conversation. Pinter Publishers, London.
Marti A. Hearst. 1997. TextTiling: Segmenting Text into
Multiparagraph Subtopics Passages. Computational
Linguistics, 23(1):33?64.
John Laver, 1981. Conversational routine, chapter Lin-
guistic routines and politeness in greeting and parting,
pages 289?304. Mouton, The Hague.
13
LDC. 2004. Fisher english training speech part 1, tran-
scripts. http://www.ldc.upenn.edu/Catalog/Catalog
Entry.jsp?catalogId=LDC2004T19.
David D. Lewis and William A. Gale. 1994. A sequential
algorithm for training text classifiers. Proc. of SIGIR.
Jeffrey C. Reynar. 1999. Statistical models for topic seg-
mentation. Proceedings of the 37th Annual Meeting
of the ACL.
Ian H. Witten and Eibe Frank. 2005. Data Mining: Prac-
tical Machine Learning Tools and Techniques, Second
Edition. Morgan Kaufmann, San Francisco.
14
