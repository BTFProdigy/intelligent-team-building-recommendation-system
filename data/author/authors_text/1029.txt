A Model of Competence for Corpus-Based Machine Translation 
Michael Carl 
Institut fiir Angewandte Informationsforschung, 
Martin-Luther-Strafle 14, 
66111 Saarbrficken, Germany, 
carl@iai.uni-sb.de 
Abst rac t  
In this paper I claborate a model of colnpetencc 
for corpus-based machine translation (CBMT) along 
the lines of the representations u ed in the transla- 
tion system. Representations in CBMT-systems can 
be rich or austere, molecular or holistic and they can 
be fine-grained or coarse-grained. The paper shows 
that different CBMT architectures are required de- 
pendent on whether a better translation quality or 
a broader coverage is preferred according to Boitct 
(1999)'s formula: "Coverage * Quality = K". 
1 In t roduct ion  
In the machine translation (MT) literature, it has 
often been argued that translations of natural lan- 
guage texts are valid if and only if the source lan- 
guage text and the target language text have the 
same meaning cf. e.g. (Nagao, 1989). If we assume 
that MT systems produce meaningflfl translations 
to a certain extent, wc must assmne that such sys- 
tems have a notion of the source text meaning to a 
similar extent. Hence, the translation algorithm to- 
gether with the data it uses encode a formal model of 
meaning. Despite 50 years of intense research, there 
is no existing system that could map arbitrary input 
texts onto meaning-equivalent output texts. How is 
that possible? 
According to (Dummett, 1975) a theory of mean- 
lug is a theory of understanding: having a theory 
of meaning means that one has a theory of under- 
standing. In linguistic research, texts are described 
on a number of levels and dimensions each contribut- 
ing to its understanding and hence to its memfing. 
l~raditionally, the main focus has been on semantic 
aspects. In this research it is assumed that know- 
ing the propositional structure of a text means to 
understand it. Under the same premise, research in 
M.q? has focused on semantic aspects assmning that 
texts have the same meaning if they are semantically 
equivalent. 
Recent research in corpus-based MT has differ- 
ent premisses. Corpus-Based Machine Translation 
(CBMT) systems make use of a set of reference 
translations on which the translation of a new text 
is based. In CBMT-systems, it is assumed that 
the reference translations given to the system in a 
training phase have equivalence meanings. Accord- 
ing to their intelligence, these systems try to fig- 
urc out of what the meaning invariance consists in 
the reference text and learn an appropriate source 
language/target language mapping mechanism. A 
translation can only be generated if an appropriate 
example translation is available in the reference text. 
An interesting question in CBMT systems is thus: 
what theory of meaning should the learning pro- 
cess implement in order to generate an appropriate 
understanding of the source text such that it can 
be mapped iuto a meaning equivalent arget text? 
Dulmnett (Dummett, 1975) suggests a distinction 
of theories of meaning along the following lines: 
* In a rich theory of meaning, the knowledge of 
the concepts is achieved by knowing the features 
of these concepts. An ausle'ce theory merely re- 
lies upon simple recognition of the shape of the 
concepts. A rich theory can justify the use of a 
concept by means of the characteristic features 
of that concept, whereas an austere theory can 
justify the use of a concept merely by enmner- 
ating all occurrences of the use of that concept. 
. A moh'.euIar theory of meaning derives the 
understanding of an expression from a finite 
number of axioms. A holistic theory, in con- 
trast, derives the understanding of an expres- 
sion through its distinction from all other ex- 
pressions in that language. A molecular theory, 
therefore, provides criteria to associate a cer- 
tain meaning to a sentence and can explain the 
concepts used in the language. In a holistic the- 
ory nothing is specified about the knowledge of 
the language other than in global constraints 
related to the language as a whole. 
In addition, the granularity of concepts eems cru- 
cial for CBMT implementations. 
* A fine-grained theory of meaning derives con- 
cepts from single morphemes or separable words 
of the language, whereas in a coar~'e-qrained 
997 
theory of meaning, concepts are obtained from 
morpheme clusters. In a fine-grained theory of 
meaning, complex concepts can be created by 
hierarchical composition of their components, 
whereas in a coarse-grained theory of meaning, 
complex meanings can only be achieved through 
a concatenation of concept sequences. 
The next three sections discuss the dichotomies of 
theories of nleaning, rich 'vs. auz~ere, molecular vs. 
holis*ic and coarse-grained vs. fine-grained where a 
few CBMT systems are classified according to the 
terminology introduced. This leads to a model of 
competence for CBMT. It appears that translation 
systems can either be designed to have a broad cov- 
erage or a high quali@. 
2 R ich  vs .  Austere  CBMT 
A common characteristic of all CBMT systems is 
that the understanding of the translation task is de- 
rived fronl the understanding of the reference trans- 
lations. The inferred translation knowledge is used 
in the translation phase to generate new transla- 
tions. 
Collins (1998) distinguishes between Memory- 
Based MT, i.e. menlory heavy, linguistic light and 
Example-Based MT i.e. memory light and linguistic 
heavy. While the former systems implement an aus- 
tere theory of meaning, the latter make use of rich 
representations. 
The most superficial theory of understanding 
is implenlented in purely menlory-based MT ap- 
proaches where learning takes place only by extend- 
ing the reference text. No abstraction or generaliza- 
tion of the reference xamples takes place. 
Translation Memories (TMs) are such purely 
memory based MT-systems. A TM e.g. TRADOS's 
Translator's Workbench (Heyn, 1996), and STAR's 
TRANSIT calculates the graphenfic similarity of the 
input text and the source side of the reference trans- 
lations and return the target string of the nlost sim- 
ilar translation examples as output. TMs make use 
of a set of reference translation examples and a (k- 
nn) retrieval algorithm. They iulplement an austere 
theory of nleaning because they cannot justify the 
use of a word other than by looking up all contexts 
in which the word occurs. They can, however, enu- 
merate all occurrences of a word in the reference 
text. 
The TM distributed by ZERES (Zer, 1997) follows 
a richer approach. The reference translations and 
the input sentence to be translated are lemmatized 
and part-of-speech tagged. The source language sen- 
tence is nlapped against the reference translations 
on a surface string level, on a lemma level and on 
a part-of-speech level. Those example translations 
which show greatest similarity to the input sentence 
with respect to the three levels of description are 
returned as the best available translation. 
Example Based Machine Translation (EBMT) 
systems (Sato and Nagao, 1990; Collins, 1998; 
Gilvenir and Cicekli, 1998; Carl, 1999; Brown, 1997) 
are richer systems. Translation examples are stored 
as feature and tree structures. Translation tenlplates 
are generated which contain - SOuletinles weighted 
- connections in those positions where the source 
language and the target language equivalences are 
strong. In the translation phase, a multi-layered 
mapping from the source language into the target 
language takes place on the level of templates and 
on the level of fillers. 
The ReVerb EBMT system (Collins, 1998) per- 
forms sub-sentential chunking and seeks to link con- 
stituents with the same function in the source and 
the target language. A source language subject is 
translated as a target language subject and a source 
language object as a target language object. In case 
there is no appropriate translation template avail- 
able, single words can be replaced as well, at the 
expense of translation quality. 
The EBMT approach described in (Giivenir and 
Cicekli, 1998) makes use of morphological knowl- 
edge and relies on word stems as a basis for trans- 
lation. Translation templates are generalized fronl 
aligned sentences by substituting differences in sen- 
tence pairs with variables aud leaving the identical 
substrings unsubstituted. An iterative application 
of this nlethod generates translation examples and 
translation templates which serve as the basis for 
an example based MT system. An understanding 
consists of extraction of compositionally translatable 
substriugs and the generation of translation tem- 
plates. 
A similar approach is followed in EDGAR (Carl, 
1999). Sentences are morphologically analyzed and 
translation templates are decorated with features. 
Fillers in translation template slots are constrained 
to unify with these features. In addition to this, 
a shallow linguistic formalism is used to percolate 
features in derivation trees. 
Sato and Nagao (1990) proposed still richer repre- 
sentations where syntactically analyzed phrases and 
sentences are stored in a database. In the translation 
phase, most similar derivation trees are retrieved 
from the database and a target language deriva- 
tion tree is conlposed fronl the translated parts. By 
means of a thesaurus emantically similar lexical 
items may be exchanged in the derivation trees. 
Statistics based MT (SBMT) approaches imple- 
ment austere theories of lncaning. For instance, in 
Brown et al (1990) a couple of models are pre- 
sented starting with simple stochastic translation 
models getting incrementally more complex and rich 
by introducing more random variables. No linguistic 
998 
Richness of Representation Richness of Representation Granularity of Representation 
sere 
syrt 
mot  
gra  
? 1 8e?T~ 
o2, 3 ? G syn 
$4 TY~Or 
"5 *r "S,9 gra 
molecular nfixed holistic 
Atomicity of Representation 
Ol 
O2,3 06 
04 
08 o7 09,5 
word phrase sentence 
Granularity of Representation 
sent. 
phras, 
word  
O4,5 $9 
o6,7 
qb 1 o2,3 e8 
molecular mixed holistic 
Atonricity of Representation 
el: Sato and Nagao (1990) 
o4: ZERES Zer (1997) 
or: Brown (1997) 
? 2: EDGAR Carl (1999) 
? 5: TRADOS Heyn (1996) 
? s: Brown et al (1990) 
? 3: GEBMT Gfivenir and Cicekli (1998) 
? 6: ReVerb Collins (1998) 
? 9: McLean (1992) 
\]figure 1: Atomicity, Granularity and Richness of CBMT 
analyses are taken into account in these approaches. 
However, in further research the authors plan to 
integrate linguistic knowledge such as inflectional 
analysis of verbs, nouns and adjectives. 
McLean (McLean, 1992) has proposed an austere 
approach where lie uses neural networks (NN) to 
translate surface strings from English to French. His 
approach functions similar to TM where the NN is 
u,;ed to classify the sequences of surface word forms 
according to the examples given in the reference 
translations. On a small set of examples hc shows 
that NN can successfully be applied for MT. 
3 Molecu lar  vs.  Ho l i s t i c  CBMT 
As discussed in the previous section, all CBMT sys- 
tems make use of sonle text dimensions in order to 
map a source language text into the target language. 
TMs, for instance, rely on the set of graphenfical 
symbols i.e. the ASCII set. Richer systems use lcx- 
ical, morphological, syntactic and/or semantic de- 
scriptions. The degree to which the set of descrip- 
tions is independent from the reference translations 
determines the molecularity of the theory. The more 
the descriptions are learned from and thus depend 
on the reference translations the more the system 
becomes holistic. Learning descriptions from refer- 
cnce translations makes the system more robust and 
easy to adjust to a new text domain. 
SBMT approaches e.g. (Brown et al, 1990) have 
a purely holistic view on languages. Every sentence 
of one language is considered to bca  possible trans- 
lation of any sentence in the other language. No 
account is given for the equivalence of the source 
language meaning and the target language meaning 
other than by means of global considerations con- 
cenfing frequencies of occurrence in the reference 
text. In order to compute the most probable trails- 
lations, each pair of items of the source language and 
the target language is associated with a certain prob- 
ability. This prior probability is derived from the 
reference text. In the translation phase, several tar- 
get language sequences are considered and the one 
with the highest posterior probability is then taken 
to be the translation of the source language string. 
Similarly, neural network based CBMT systems 
(McLean, 1992) are holistic approaches. The train- 
ing of the weights and the nfinimization of the classi- 
fication error relies oil the reference text as a whole. 
Temptations to extract rules from the trained neu- 
ral networks eek to isolate and make explicit aspects 
on how the net successfully classifies new sequences. 
The training process, however, remains holistic. 
TMs implement the molecular CBMT approach as 
they rely on a static distance metric which is inde- 
pendent from the size and content of the case base. 
TMs are molecular because they rely on a fixed and 
limited set of graphic symbols. Adding further ex- 
ample translations to the data base does not increase 
the set of the graphic symbols nor does it modify the 
distance metric. Learning capacities in TMs are triv- 
ial as their only way to learn is through extension of 
the example base. 
The translation templates generated by Giivenir 
and Cicekli (1998), for instance, differ according to 
the similarities and dissinfilarities found in the ref- 
erence text. Translation templates in this system 
thus reflect holistic aspects of the example transla- 
tions. The way in which morphological analyses is 
processed is, however, independent front the transla- 
tion examples and is thus a molecular aspect in the 
system. 
Similarly, the ReVerb EBMT system (Collins, 
1998) makes use of holistic components. The ref- 
erence text is part-of-speech tagged. The length of 
translation segments as well as their most likely lift- 
tim and final words arc calculated based on proba- 
999 
Expected Coverage of the System 
high 
low 
% 
low high 
Expected 2?anslation Quality 
? 1: Sato and Nagao (1990) 
? 2: Carl (1999) 
? 3: Giivenir and Cicekli (1998) 
e4: Zer (1997) 
e~:  Heyn (1996) 
? 6: Collins (1998) 
? ?: Brown (1997) 
? s: Brown et al (1990) 
? 9: McLean (1992) 
Figure 2: A Model of Competence for CBMT 
bilities found in the reference text. 
4 Coarse vs. F ine Graining CBMT 
One task that all MT systems perform is to segment 
the text to be translated into translation units which 
- -  to a certain extent - -  can be translated indepen- 
dently. The ways in which segmentation takes place 
and how the translated segments are joined together 
in the target language are different in each MT sys- 
tem. 
In (Collins, 1998) segmentation takes place on a 
phrasal level. Due to the lack of a rich morphological 
representation, agreement cannot always be granted 
in the target language when translating single words 
from English to German. Reliable translation can- 
not be guaranteed when phrases in the target lan- 
guage - or parts of it - are moved from one position 
(e.g. the object position) into another one (e.g. a 
subject position). 
In (Giivenir and Cicekli, 1998), this situation is 
even more problematic because there are no restric- 
tions on possible fillers of translation template slots. 
Thus, a slot which has originally been filled with an 
object can, in the translation process, even accom- 
modate an adverb or the subject. 
SBMT approaches perform fine-grained segmen- 
tation. Brown et al (1990) segment he input 
sentences into words where for each source-target 
language word pair translation probabilities, fertil- 
ity probabilities, alignment probabilities etc. are 
computed. Coarse-grained segmentation are unre- 
alistic because sequences of 3 or more words (so- 
called n-grams) occur very rarely for n > 3 even ill 
huge learning corpora 1. Statistical (and probabilis- 
tic) systems rely on word frequencies found in texts 
and usually cannot extrapolate from a very small 
number of word occurrences. A statistical language 
1 Brown et al (1990) uses the Hansard French-English text 
containing several million words. 
model assigns to each n-gram a probability which 
enables the system to generate the most likely tar- 
get language strings. 
5 A Competence  Mode l  fo r  CBMT 
A competence model is presented as two indepen- 
dent parameters, i.e. Coverage and Quality (see Fig- 
ure 2). 
? Coverage  of the system refers to the extent to 
which a variety of source language texts can be 
translated. A system has a high coverage if a 
great variety of texts can be translated. A low- 
coverage system can translate only restricted 
texts of a certain domain with limited ternfi- 
nology and linguistic structures. 
? Qua l i ty  refers to the degree to which an MT 
system produces uccessful translations. A sys- 
tem has a low quality if the produced transla- 
tions are not even informative in the sense that 
a user cannot understand what the source text 
is about. A high quality MT-system produces 
user-oriented and correct translations with re- 
spect to text type, terminological preferences, 
personal style, etc. 
An MT systenr with low coverage and low quality 
is completely uninteresting. Such a system comes 
close to a randonr number generator as it translates 
few texts in an unpredictable way. 
An MT system with high coverage and "not-too- 
bad" quality can be useful in a Web-application 
where a great variety of texts are to be translated 
for occasional users which want to grasp the basic 
ideas of a foreign text. On the other hand a system 
with high quality and restricted coverage might be 
useful for in-house MT-applications or a controlled 
language. 
An MT sys tem with high coverage and high qual- 
ity would translate any type of text to everyone's 
1000 
satisfaction, lIowever, as one can expect, such a 
system seems to bc not feasible. 
Boitct (1999) proposes "the (tentative) formula: 
Coverage ? Quality -= K "where K depends on the 
MT technology and the amount of work encoded in 
the system. The question, then, is when is the max~ 
imum K possible and how nluch work do we want 
to invest for what purpose. Moreover a given K can 
mean high coverage and low quality, or it can mean 
the reverse. 
The expected quality of a CBMT system increases 
when segmenting more coarsely the input text. Con- 
sequently, a low coverage must bc expected ue to 
the combinatorial explosion of the number of longer 
(:hunks. in order for a fine-grailfing system to gencr- 
z~te at least informative translations, further knowl- 
edge resources need be considered. These knowledge 
resources may be either pre-defined and molecular or 
they can be derived fronl reference translations and 
holistic. 
TMs focus on the quality of translations. Only 
large clusters of nlcaning entities are translated into 
the target language in the hope that such clus- 
ters will not interfere with the context from which 
they are taken. Broader coverage can be achieved 
through finer grained segmentation f the input into 
phrases or single terms. Systems which finely seg- 
ment texts use rich representation languages in or- 
der to adapt the translation units to the target lan- 
guage context or, as in the case of SBMT systems, 
use holistic derived constraints. 
What can bc learned and what should be learned 
from the reference text, how to represent he in- 
ferred knowledge, how to combine it with pre-defincd 
knowledge and the impact of difl'erent settings on the 
constant K in the formula of Boitet (1999) are all 
still open question for CBMT-design. 
6 Conclusion 
Machine Tr'anslation (MT) is a lneaning preserving 
raapping from a source language text into a target 
language text. In order to enable a computer system 
to perform such a mapping, it is provided with a 
formalized theory of meaning. 
Theories of meaning are characterized by three di- 
chotomies: they call be holistic or molecular, aus- 
tere or rich and they can be fine-grained or coarsc- 
Muiucd. 
A number of CBMT systems - translation mem- 
ories, example-based and statistical-based nlachine 
translation systenls - arc examined with respect to 
these dichotomies. Ill a system that uses a rich the- 
ory of meaning, complex representations arc com- 
puted including morphological, syntactical and se- 
rnantical representations, while with an austere the- 
ory the system relics on the mere graphcnfic surface 
form of the text. In a holistie implementation mean- 
ing descriptions are derived from reference transla- 
tions while in a molecular approach the meaning dc- 
scriptions are obtained from a finite set of prede- 
fined features. In a fine-grained theory, the minimal 
length of a translation unit is equivalent o a mor- 
1)heme while in a coarse-grained theory this amounts 
to a morphenle cluster, a phrase or a sentence. 
According to the implemented theory of meaning, 
one can expect o obtain high quality translations or 
a good covera.qc of the CBMT system. 
The more the system makes use of coarse-grained 
translation units, the higher is tlle expected trans- 
lation quality. The more the theory uses rich repre- 
sentations thc more the system may achieve broad 
coverage. CBMT systems can be tuned to achieve 
cither of the two goals. 
References 
Christian Boitet. 1999. A research perspective 
on how to democratize machine translation and 
translation aides aiming at high quality final out- 
put. In MT-Summit  '99. 
Peter F. Brown, 3. Cockc, Stephen A. Della Pietra, 
Vincent 3. Della Pietra, F. Jelinek, Mercer Robert 
L., and Roossiu P.S. 1990. A statistical approach 
to machine translation. Computational Linguis- 
tics, 16:79-85. 
Ralf D. Brown. 1997. Automated Dictionary Ex- 
traction for "Knowledge-Free" Example-Based 
Translation. In TMI-97, pages 111-118. 
Michael Carl. 1999. Inducing Translation Templates 
for Example-Based Machine Translation. hi MT- 
Summit VII: 
Br6na Collins. 1998. Example-Based Machine 
Tra'nMation: An Adaptation-Guided Retrieval Ap- 
pTvach. Ph.D. thesis, Trinity College, Dublin. 
Michael Dummett. 1975. What is a Theory of 
Meaning? In Mind and Language. Oxford Uni- 
versity Press, Oxford. 
Halil Altay Giivenir and Ilyas Cicekli. 1998. Learn- 
ing Translation Templates from Examples. Infor- 
mation Systems, 23(6):353-363. 
Matthias Hcyn. 1996. Integrating machine trans- 
lation into translation memory systems. In Eu- 
ropean Asscociation for Machine Translation - 
Workshop Proceedings, pages 111-123, ISSCO~ 
Geneva. 
Ian J. McLean. 1992. Example-Based Machine 
Translation using Connectionist Matching. In 
TMI-92. 
Makoto Nagao. 1989. Machine Translation ftbw Far 
Can It Go. Oxford University Press, Oxford. 
S. Sato and M. Nagao. 1990. Towards memory- 
based translation. In COLING-90. 
Zeres GmbH, Bochunl, Germany, 1997. ZERE- 
STRANS Bcnutzcrhandbuch. 
1001 
Controlling Gender Equality with Shallow NLP Techniques
M. Carl, S. Garnier, J. Haller
Institut fu?r Angewandte Informationsforschung
66111 Saarbru?cken
Germany
{carl,sandrine,hans}@iai.uni-sb.de
A. Altmayer and B. Miemietz
Universita?t des Saarlandes
66123 Saarbru?cken, Germany
anne@altmayer.info
Miemietz.Baerbel@MH-Hannover.DE
Abstract
This paper introduces the ?Gendercheck
Editor?, a tool to check German texts for
gender discriminatory formulations. It re-
lays on shallow rule-based techniques as
used in the Controlled Language Author-
ing Technology (CLAT). The paper outlines
major sources of gender imbalances in Ger-
man texts. It gives a background on the
underlying CLAT technology and describes
the marking and annotation strategy to au-
tomatically detect and visualize the ques-
tionable pieces of text. The paper provides
a detailed evaluation of the editor.
1 Introduction
Times of feminist (language) revolution are
gone, but marks are left behind in the form
of a changed society with a changed conscious-
ness and changed gender roles. Nevertheless
language use seems to oppose changes much
stronger than society does. As the use of non-
discriminatory language is nowadays compul-
sory in administration and official documents, a
number of guidelines and recommendations ex-
ist, which help to avoid gender imbalance and
stereotypes in language use. Although in some
cases men may be concerned (as for example
most terms referring to criminals are mascu-
line nouns) the main concern is about adequate
representation of women in language, especially
in a professional context. Psychological tests
demonstrate that persons reading or hearing
masculine job titles (so-called generic terms al-
legedly meaning both women and men) do not
visualize women working in this field.
In order to avoid this kind of discrimination
two main principles are often suggested (e.g. as
in (Ges, 1999; Uni, 2000)):
1. use of gender-neutral language, which
rather ?disguises? the acting person by us-
ing impersonal phrases.
2. explicit naming of women and men as
equally represented acting persons.
Using and applying these guidelines in a faith-
ful manner is time-consuming and requires a
great amount of practice, which can not al-
ways be provided, particularly by unexperi-
enced writer. Moreover these guidelines are of-
ten completely unknown in non-feminist circles.
A tool which checks texts for discriminatory use
of language is thus mandatory to promote writ-
ten gender equality, educate and remind writer
of unacceptable forms to avoid.
In this paper we describe the project ?Gen-
dercheck? which uses a controlled-language au-
thoring tool (CLAT) as a platform and editor
to check German texts for discriminatory lan-
guage.
In section 2 we introduce three categories of
gender discrimination in German texts and pro-
vide possibilities for their reformulation.
Section 3 introduces the technology on which
the Gendercheck editor is based. The linguis-
tic engine proceeds in two steps, a marking and
filtering phase where gender discriminatory for-
mulations are automatically detected. A graph-
ical interface plots the detected formulations
and prompts the according messages for correc-
tion .
Section 4 then goes into the detail of the
marking and filtering technique. We use the
shallow pattern formalism kurd (Carl and
Schmidt-Wigger, 1998; Ins, 2004) first to mark
possible erroneous formulations and the to filter
out those which occur in ?gendered? context.
Section 5 evaluates the Gendercheck editor on
two texts.
2 Gender Inequality in German
Texts
Most prominent to achieve gender equality on a
linguistic level in German texts is to find solu-
tions and alternatives for the so-called generic
masculine: the masculine form is taken as
the generic form to designate all persons of
any sex. The major problem is to figure out
whether or not a given person denotation refers
to a particular person. For instance, in ex-
ample (1a) ?Beamter? (officer) is most likely
used in its generic reading and refers to fe-
male officers (Beamtinnen) and masculine offi-
cers (Beamten). To achieve gender equality an
appropriate reformulation is required as shown
in example (1b).
(1a) Der Beamte muss den Anforderungen
Genu?ge leisten.
(1b) Alle Beamten und Beamtinnen mu?ssen den
Anforderungen Genu?ge leisten.
Since we tackle texts from administrative and
legal domains we principally assume unspeci-
fied references. That is, a masculine (or fem-
inine!) noun will not denote a concrete person
but rather refers to all persons, irrespectively of
their sex.
A second class of errors are masculine rela-
tive, possessive and personal pronouns which
refer to a generic masculine or an indefinite mas-
culine pronoun.
(2a) Der Beamte muss seine Wohnung in der
Na?he des Arbeitsplatzes suchen.
(2b) Jeder muss seinen Beitrag dazu leisten.
(2c) Wer Rechte hat, der hat auch Pflichten.
The possessive pronoun ?seine? (his) in exam-
ple (2a) refers to the preceeding ?Beamte? (of-
ficer). The generic masculine use of ?Beamte?
and the referring pronoun will be marked. The
same holds for sentence (2b) where the posses-
sive pronoun refers to the indefinite pronoun
?jeder? (everymasc). The indefinite pronouns
?jemand? (someone) and ?wer? (who) count as
acceptable. However, masculine pronouns refer-
ring to it will be marked. In example (2c), the
masculin relative pronoun ?der? can be omitted.
A third class of gender inequality is lack of
agreement between the subject and the pred-
icative noun. Example (3a) gives an example
where the masculine subject ?Ansprechpartner?
(partnermasc) occurs with the a female object
?Frau Mu?ller? (Mrs. Mu?ller).
(3a) Ihr Ansprechpartner ist Frau Mu?ller.
(3b) Ihre Ansprechpartnerin ist Frau Mu?ller.
A solution for this class of errors is shown in
example (3b) where the subject (Ansprechpart-
nerin) is adapted to the female gender of the
predicate.
Suggestions to reformulate gender imbalances
as shown in examples (1) and (2) can be classi-
fied in two main categories:
1. Whenever possible, use gender neutral
formulations. These include collectiva
(e.g. Lehrko?rper (teaching staff) or
Arbeitnehmerschaft (collective of employ-
ees)) as well as nominalized participles
(Studierende (scholar)) or nominalized ad-
jectives (Berechtigte).
2. Use both forms if gender neutral formula-
tions cannot be found. That is, the femi-
nine and the masculine form are to be co-
ordinated with ?und?, ?oder? or ?bzw.?.
A coordination with slash ?/? will also be
suggested but should only be used in forms,
ordinance and regulations.
Amendments should accord to general Ger-
man writing rules. The so called ?Binnen-I?, an
upper case ?I? as in ?StudentInnen? will not be
suggested and also naming of the female suffix in
parenthesis should be avoided. The same holds
for the indefinite pronoun ?frau? (woman) which
was occasionally suggested to complement the
pronoun ?man?.
3 The Gendercheck Editor
Controlled-Language Authoring Technology
(CLAT) CLAT has been developed to suit the
need of some companies to automatically check
their technical texts for general language and
company specific language conventions. Within
CLAT, texts are checked with respect to:
? orthographic correctness
? company specific terminology and abbrevi-
ations
? general and company specific grammatical
correctness
? stylistic correctness according to general
and company specific requirements
The orthographic control examines texts for
orthographic errors and proposes alternative
writings. The terminology component matches
the text against a terminology and abbreviation
database where also term variants are detected
Figure 1: The Gendercheck Editor
(Carl et al, 2004). Grammar control checks
the text for grammatical correctness and dis-
ambiguates multiple readings. Stylistic control
detects stylistic inconsistencies.
The components build up on each other?s
output. Besides the described control mech-
anisms, CLAT also has a graphical front-end
which makes possible to mark segments in the
texts with different colors. Single error codes
can be switched off or on and segments of text
can be edited or ignored according to the au-
thors need. CLAT also allows batch processing
where XML-annotated text output is generated.
Figure 1 shows the graphical interface of the
editor. The lower part of the editor plots an in-
put sentence. The highlighted SGML codes are
manually annotated gender ?mistakes?. The
upper part plots the automatically annotated
sentence with underlined gender mistakes.
As we shall discuss in section 5, gender im-
balances are manually annotated to make eas-
ier automatic evaluation. In this example, the
highlighted words ?Deutscher? (German) and
?EG-Bu?rger? (EU-citizen) are identical in the
manually annotated text and in the automat-
ically annotated text. The user can click on
one of the highlighted words in the upper win-
dow to display the explanatory message in the
middle part of the screen. Further information
and correction or reformulation hints can also
be obtained by an additional window as shown
on the right side of the figure. The messages
are designed according to main classes of gender
discriminatory formulations as previously dis-
cussed.
4 Gender Checking Strategy
Gendercheck uses a marking and filtering strat-
egy: first all possible occurrences of words in
an error class are marked. In a second step
?gendered? formulations are filtered out. The
remaining marked words are assigned an error
code which is plotted in the Gendercheck editor.
According to the classification in section 2,
this section examines the marking and filtering
strategy for generic masculine agents in section
4.1, pronouns which refer to generic masculine
agents (section ??) and errors in agreement of
predicative nouns (section ??).
Marking and filtering is realized with kurd
a pattern matching formalism as described in
(Carl and Schmidt-Wigger, 1998; Ins, 2004). In-
put for kurd are morphologically analyzed and
semantically tagged texts.
4.1 Class 1: Agents
4.1.1 Marking Agents
Two mechanisms are used to mark denotations
of persons:
a) The morphological analysis of mpro
(Maas, 1996) generates not only derivational
and inflectional information for German words,
but also assigns a small set of semantic
values. Male and female human agents
such as ?Soldat? (soldier), ?Bu?rgermeister?
(mayormasc), ?Beamte? (officermasc), ?Kranken-
schwester? (nursefem) etc. are assigned a se-
mantic feature s=agent. Words that carry this
feature will be marked style=agent.
b) Problems occur for nouns if the base
word is a nominalized verb. For instance
?Gewichtheber? (weightlifter) und ?Busfahrer?
(bus driver) will not be assigned the feature
s=agent by mpro since a ?lifter? and a ?driver?
can be a thing or a human. Gender inequalities,
however, only apply to humans. Given that the
tool is used in a restricted domain, a special list
of lexemes can be used to assign these words
the style feature style=agent. The kurd
rule Include shows some of the lexemes from
this list. The list contains lexemes to cover
a maximum number of words. For instance
the lexeme absolvieren (graduate) will match
?Absolvent? (alumnusmasc), ?Absolventin?
(alumnusfem), ?Absolventen? (alumniplu,masc)
and ?Absolventinnen? (alumniplu,fem).
1 Include =
2 Ae{c=noun,
3 ls:absolvieren$;
4 dezernieren$;
5 richten$;
6 fahren$;
7 administrieren$;
8 vorstand$}
9 : Ag{style=agent}.
Lines 3 to 8 enumerate a list of lexemes sep-
arated by a semicolon. The column in line
3 following the attribute name ls tells kurd
to interpret the values as regular expressions.
Since the dollar sign $ matches the end of the
value in the input object, each lexeme in the
list can also be the head of a compound word.
Thus, the test ls:fahren$ matches all lexemes
that have fahren as their head words, such
as ?Fahrer? (driver), ?Busfahrer? (bus driver),
etc. The action Ag{style=agent} marks the
matched words as an agent.
4.1.2 Filtering ?gendered? Agents
The text then undergoes several filters to delete
marks in words if the appear within gendered
formulations.
a) Excluded are marked agents which preceed
a family name. The marking of ?Beamte? in
example (4) will be erased since it is followed
by the family name ?Meier?. ?Beamte Meier?
is likely to have a specific reference.
(4) Der Beamte Meier hat gegen die Vorschrift
versto?en.
In terms of kurd this can be achieved with
the rule AgentMitFname: if a family name
(s=fname) follows a sequence of marked agents
(style=agent) the marks in the agent nodes are
removed (r{style=nil}).
1 AgentMitFname =
2 +Ae{style=agent},
3 Ae{c=noun,s=fname}
4 : Ar{style=nil}.
b) Also excluded are nominalized plural ad-
jectives and participles since they are well
suited for gender neutral formulations. In ex-
ample (5), the nominalized plural adjective
?Sachversta?ndige? (experts) is ambiguous with
respect to gender. The mark will thus be re-
moved.
(5) Sind bereits Sachversta?ndige bestellt?
c) Marked words in already gendered formu-
lations are also erased. Pairing female and male
forms by conjunction is a recommended way to
produce gender equality. In example (6) the
subject ?Die Beamtin oder der Beamte? (the
officerfem or the officermasc) as well as the pro-
nouns which refer to it ?sie oder er? (she or he)
and ?ihrer oder seiner? (her or his) are gender
equal formulations.
(6) Die Beamtin oder der Beamte auf Lebens-
zeit oder auf Zeit ist in den Ruhestand
zu versetzen, wenn sie oder er infolge
eines ko?rperlichen Gebrechens oder wegen
Schwa?che ihrer oder seiner ko?rperlichen
oder geistigen Kra?fte zur Erfu?llung
ihrer oder seiner Dienstpflichten dauernd
unfa?hig (dienstunfa?hig) ist.
The kurd rule gegendert removes these
marks. The description in lines 2 to 5
matches a conjunction of two marked agents
(style=agent) which share the same lexeme
ls=_L but which are different in gender. This
latter constraint is expressed in two variables
ehead={g=_G} and ehead={g~=_G} which only
unify if the gender features ?g? have non-
identical sets of values.
1 gegendert =
2 Ae{style=agent,ls=_L,ehead={g=_G}},
3 e{lu=oder;und;bzw.;/},
4 *a{style~=agent}e{c=w},
5 Ae{style=agent,ls=_L,ehead={g~=_G}}
6 : Ar{style=nil}.
The rule allows the conjunctions ?und?,
?oder?, ?bzw.? and ?/?.
d) Some nouns are erroneously marked even if
no gender equal formulation is possible. For in-
stance words such as ?Mensch? (human being),a
?Gast? (guest), ?Flu?chtling? (refugee) are mas-
culine in gender, yet there is no corresponding
female form in German. These words are in-
cluded in an exclude list which works similar
to the include list previously discussed.
1 exclude =
2 Aa{style=agent,
3 lu:mensch$;
4 flu?chtling$;
5 sa?ugling$;
6 gast$;
7 rat$}
8 : Ar{style=nil}.
4.1.3 Non marked Expressions
a) Currently, we do not mark compound nouns
which have an agent as their modifier and a
non-agent as their head. However, also words
such as ?Rednerpult? (talker desk = lectern)
and ?Teilnehmerliste? (participants list = list
of participants) are suitable for gender main-
streaming and should be spelled as ?Redepult?
(talk desk) and ?Teilnehmendeliste? (participat-
ing list).
b) We do not mark articles and adjectives
which preceed the marked noun. This would
be troublesome in constructions like example
(7) where the article ?der? (the) and the cor-
responding noun ?Dezernent? (head of depart-
ment) are separated by an intervening adjectival
phrase.
(7) Den Vorsitz fu?hrt der jeweils fu?r die
Aufgaben zusta?ndige Dezernent.
c) It is currently impossible to look beyond
the sentence boundary. As a consequence, the
reference of a agent cannot be detected if it oc-
curs in the preceeding sentence. For instance
?Herr Mu?ller? is the reference of ?Beamte? in
the second sentence in example (8).
(8) Herr Mu?ller hat die Dienstvorschrift ver-
letzt. Der Beamte ist somit zu entlassen.
The word ?Beamte? will be erroneously
marked because information of the preceeding
sentence is not available to resolve the reference.
4.2 Class 2: Pronouns
Also personal pronouns, possessive pronouns,
relative pronouns and indefinite pronouns are
marked. The strategy is similar to the one for
agents above: first all pronouns are marked and
in a second step markings in correct formula-
tions are erased.
With the exception of indefinite pronouns
(?Mancher?, ?Jemand?, ?Niemand? etc.), a
marked referent agent must be available in the
same sentence. Three different rules are used to
mark relative pronouns, personal pronouns and
possessive pronouns.
1 MarkRelativPronomen =
2 e{style=agent,ehead={g=_G}},
3 *a{lu~=&cm},
4 e{lu=&cm},
5 Ae{lu=d_rel,ehead={g=_G}}
6 : Ag{style=agent}.
a) The rule MarkRelativPronomen detects a
marked agent in line 2. Lines 3 and 4 search
the next comma1 that follows the marked agent
and line 5 matches the relative pronoun2 that
immediately follows the comma. The relative
1commas are coded as ?&cm? in the formalism.
2relative pronouns are assigned the lexeme ?d rel?.
Size of Test Classes of errors
Text #sent. #words Errors/sent. Class 1 Class 2 Class 3 ?
ET1 95 1062 1,83 97 62 15 174
TT2 251 6473 0,46 95 21 ? 116
pronoun must agree in gender with the agent
(ehead={g=_G}). As we shall see in section 5,
this is an error prone approximation to reference
solution.
b) Personal and possessive pronouns are
only marked if they refer to a male agent.
The two rules MarkPersonalPronomen and
MarkPossesivPronomen work in a similar fash-
ion: in line 2 the marked masculine reference
is matched. Lines 3 and 4 match the follow-
ing personal pronoun (c=w,sc=pers) and pos-
sessive pronoun (c=w,sc=poss). In lines 5, the
pronouns are marked.
1 MarkPersonalPronomen =
2 e{style=agent,ehead={g=m}},
3 1Ae{lu=er;er_es,c=w,sc=pers}
4 |e{s~=agent,sc~=punct}
5 : Ar{style=agent}.
1 MarkPossesivPronomen =
2 e{style=agent,ehead={g=m}},
3 1Ae{lu=sein,c=w,sc=poss}
4 |e{s~=agent,sc~=punct}
5 : Ar{style=agent}.
After the marking step, pronoun marks are
filtered. Filtering of pronouns is similar to the
previously discussed rule gegendert.
4.3 Class 3: Predicative Noun
Missing agreement between subject and pred-
icative noun is detected with the following kurd
rule:
1 Praedikatsnomen =
2 +Ae{mark=np,style=agent,ehead={g=_G}},
3 *Ae{mark=np},
4 e{ls=sein,c=verb},
5 *Ae{style~=agent},
6 Ae{mark=np,style=agent,ehead={g~=_G}},
7 *Ae{mark=np}
8 : Ar{bstyle=Gen3,estyle=Gen3}.
Lines 2 and 3 detect the marked subject. No-
tice that noun groups are marked with the fea-
ture mark=np by a previous chunking module.
Lines 5 to 7 match the predicative noun. Both
parts of the sentence are connected by the cop-
ula ?sein? (be). Similar to the rule gegendert,
the rule only applies if both parts are different
in gender.
5 Evaluation of Gendercheck
We evaluated the Gendercheck editor based on
two texts:
ET1 A collection of unconnected negative exam-
ples taken from the (Ges, 1999) and (Sch,
1996).
TT2 The deputy law of the German Bundestag
Gender imbalances were manually annotated
with a SGML code, where each different code
refers to a different rewrite proposal to be plot-
ted in the editor as in the lower part in fig-
ure 1. Table 4.1.3 shows the distribution of er-
ror classes in the two texts. Each error class
had several subtypes which are omitted here for
sake of simplicity.
In ET1 every sentence has at least one er-
ror; on average one word out of six is marked
as ?ungendered?. Since ET1 is a set of negative
examples, errors are uniformly distributed. Dis-
tribution of errors in text TT2 is different from
ET1. TT2 does not contain a single occurrence
of a class 3 error. On average, only one word
out of 60 is manually marked and ? due to the
long size of sentences ? there are 0.46 errors
per sentence on average.
Text ET1 was used to develop and adjust the
kurd rule system for marking, filtering and er-
ror code assignment. We iteratively compared
the automatically annotated text with the man-
ually annotated text and computed precision
and recall. Based on the misses and the noise,
we adapted the style module as well as the error
annotation schema. Thus, in a first annotation
schema we assigned more than 30 different er-
ror codes literally taken from (Ges, 1999) and
(Uni, 2000). However, it turned out that this
was too fine a granularity to be automatically
reproduced and values for precision and recall
were very low. We than assigned only one er-
ror class and achieved very good values for pre-
cision of over 95% and recall over more than
89%. Based on these results we carefully re-
fined a number of subtypes of the three error
classes.
Final results are shown in table 5. Results for
the test text TT2 are slightly inferior to those of
Text ET1
Error hit misses noise precision recall
Class 1 85 12 1 0.988 0.876
Class 2 55 7 5 0.917 0.887
Class 3 15 0 1 0.937 1.000
?
155 19 7 0.957 0.891
Text TT2
Class 1 86 9 5 0.945 0.905
Class 2 14 7 4 0.778 0.667
? 100 16 9 0.917 0.862
the development text ET1. We briefly discuss
typical instances of misses and noise.
a) Noise in class 1 (generic use of masculine)
are mainly due to ?-ling? - derivations such as
?Abko?mmling? (descendant) which are mascu-
line in German and for which no female equiva-
lent forms exist. These words could be included
in the exclude lexicon (see section 4).
b) In some cases nominalized participles
such as ?Angestellte? (employee) and ?Hin-
terbliebene? (surviving dependant), which are
usually very well suited for gendered formula-
tions due to their ambiguity in gender, were er-
roneously disambiguated. These instances pro-
duced noise because filters did not apply.
c) Misses in class 1 can be traced back to
some words which have not been detected as
human agents such as ?Schriftfu?hrer? (recording
clerk) and ?Ehegatte? (spouse). These words
could be entered into the include lexicon. Both
lexicon should be made user-adaptable and user
extendible in future versions of the system.
d) Many of the misses in class 2 are due to a
reference in the preceeding sentence. Since the
system is currently sentence based, there is no
easy solution in enhancing this type of errors.
The possessive pronoun ?seiner? in the second
sentence of example (9) refers to ?Bewerber?
(applicant) in the first sentence. This connection
cannot be reproduced if the system works on a
sentence basis.
(9) Einem Bewerber um einen Sitz im Bun-
destag ist zur Vorbereitung seiner Wahl in-
nerhalb der letzten zwei Monate vor dem
Wahltag auf Antrag Urlaub von bis zu zwei
Monaten zu gewa?hren. Ein Anspruch auf
Fortzahlung seiner Bezu?ge besteht fu?r die
Dauer der Beurlaubung nicht.
e) An example for noise in class 2 is shown
in example (10). The relative pronoun ?der?
(who,which) was detected by Gendercheck but
erroneously been linked to ?Beamte? instead of
?Antrag? (application) which are both masculin
in German.
(10) Der Beamte ist auf seinen Antrag, der bin-
nen drei Monaten seit der Beendigung der
Mitgliedschaft zu stellen ist, . . .
Much more powerful mechanisms are required
to achieve a breakthrough for this kind of errors.
6 Conclusion
This paper describes and evaluates the ?Gen-
dercheck Editor? a tool to check German ad-
ministrative and legal texts for gender equal for-
mulations. The tool is based on the Controlled
Language Authoring Technology (CLAT), a
software package to control and check technical
documents for orthographical, grammatical and
styptic correctness. A part of the Style compo-
nent has been modified and adapted to the re-
quirements of linguistic gender main-streaming.
The paper outlines a shallow technique to dis-
cover gender-imbalance and evaluates the tech-
nique with two texts. Values for precision and
recall of more than 90% and 85% respectively
are reported.
References
Michael Carl and Antje Schmidt-Wigger.
1998. Shallow Postmorphological Process-
ing with KURD. In Proceedings of NeM-
LaP3/CoNLL98, pages 257?265, Sydney.
Michael Carl, Maryline Hernandez, Susanne
Preu?, and Chantal Enguehard. 2004. En-
glish Terminology in CLAT. In LREC-
Workshop on Computational & Computer-
assisted Terminology, Lisbonne.
Gesellschaft fu?r Informatik (Hg.), Bonn, 1999.
Gleichbehandlung im Sprachgebrauch: Reden
und Schreiben fu?r Frauen und Ma?nner.
Institut fu?r Angewandte Informationsforschung,
Saarbru?cken, 2004. Working paper 38. to ap-
pear.
Heinz-Dieter Maas. 1996. MPRO - Ein Sys-
tem zur Analyse und Synthese deutscher
Wo?rter. In Roland Hausser, editor, Linguis-
tische Verifikation, Sprache und Information.
Max Niemeyer Verlag, Tu?bingen.
Schweizerische Bundeskanzlei (Hg.), Bern,
1996. Leitfaden zur sprachlichen Gleichbe-
handlung im Deutschen.
Universita?t Zu?rich (Hg.), Zu?rich, 2000. Leit-
faden zur sprachlichen Gleichbehandlung von
Frau und Mann.
 	
ffPhrase-based Evaluation of Word-to-Word Alignments  
Michael Carl and Sisay Fissaha 
Institut f?r Angewandte Informationsforschung 
66111 Saarbr?cken, Germany 
{carl;sisay}@iai.uni-sb.de 
Abstract 
We evaluate the English?French word align-
ment data of the shared tasks from a phrase 
alignment perspective. We discuss pe-
culiarities of the submitted data and the test 
data. We show that phrase-based evaluation is 
closely related to word-based evaluation. We 
show examples of phrases which are easy to 
align and also phrases which are difficult to 
align. 
1 
2 
Introduction 
We describe a phrase-based evaluation of the 16 Eng-
lish-French alignment submissions for the shared task 
on Parallel Texts. The task was to indicate which word 
token in an English alignment sample corresponds to 
which word token in the French alignment sample. Two 
types of submission were permitted: for restricted sub-
missions were allowed a ?sentence? aligned segment of 
the Canadian Hansards to train the systems while unre-
stricted submission would be allowed to use additional 
resources. The performance of the systems was com-
pared for a set of 447 English?French hand-aligned 
test samples which were also taken from the Canadian 
Hansards.  
Five institutes participated in the English?French 
alignment task, submitting a total of 16 sets of align-
ment data. To evaluate the submitted data, we extracted 
bilingual phrase dictionaries from the word-alignment 
data. The extracted dictionaries of the submitted data 
were compared with the extracted dictionary of the test 
data. 
We first discuss word-to-word and phrase-to-phrase 
alignment format. We present two different methods for 
extracting bilingual dictionaries from the word align-
ment data: a minimal dictionary contains the least num-
ber of unambiguous phrase-to-phrase translations while 
an exhaustive dictionary contains all possible unambi-
guous translations. We examine the test data (i.e. the 
?golden standard?) and the submitted alignment data. 
We discuss their peculiarities and give examples of 
phrases easy and difficult to align. 
 
Types of Alignment  
The test set consists of 447 alignment samples from the 
Canadian Hansards which were pre-tokenized. A three-
tuple containing the alignment number, an English word 
offset and a French word offset would indicate an exact 
word-to-word translation1. The submitted data was sup-
posed to comply with this word-to-word alignment for-
mat. In example 1 the English sentence has 15 tokens 
while the French sentence has 16 tokens. Example 1 
shows the word-to-word alignment data of sample 91 
for submission 12 and a plot of the data. 
 
Example 1:   Alignment sample 91: 
 
English (vertical): 
i was not asking for a detailed explana-
tion as to what he was doing . 
 
French (horizontal):  
je ne lui ai pas demand? de me fournir de 
telles explications sur ces activit?s . 
 
Plot and word alignment data for submission 12: 
    Sample En Fr 
15                 x 91 15 16 
14               x   91 14 14 
13         x         91 13  8 
12         x         91 12  8 
11             x     91 11 12 
10          x        91 10  9 
09            x      91  9 11 
08             x     91  8 12 
07          x        91  7  9 
06          x        91  6  9 
05             x     91  5 12 
04    x              91  4  3 
03   x               91  3  2 
02         x         91  2  8 
01  x                91  1  1 
00     xxxx  x  x x  
   01234567890123456 
 
                                                           
1 There was also an optional slot to indicate whether this 
alignment would be [S]ure or [P]robable. We ignore this in-
formation in our evaluation.  
Figure1:   Number of word alignment points and size of extracted dictionaries
0
5000
10000
15000
20000
25000
30000
35000
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17
null-alignment
submitted
text-dic1
text-dic2
align-dic1
 
2.1 Word-to-word alignment 
There are two underlying assumptions in word-to-word 
alignment: 
(i) each word token on the English side can have 
any number of word correspondences -- includ-
ing zero -- on the French side and vice versa. 
Word alignments may have crossing and am-
biguous branches. For instance in example 1, 
the French word ?me? on position 8 has the 
translations ?was?, and ?he?, while ?ai? has no 
connection to the English side. 
(ii) words (English or French) for which no align-
ments are given in the submitted data are as-
signed a null-alignment.  
Example 1 has 22 word alignment points, where the 
evaluators inserted 7 null-alignments. In some cases (i.e. 
submission 11) this insertion accounts for almost 50% 
of the alignment data. In figure 1, ?null-alignment? plots 
the union of the submitted alignment data and the in-
serted null-alignments. Null-alignments were not added 
to submission 16 as it provides alignment information 
for every word. The last data point on the x-axis (i.e. 17) 
epresents the test data. 
s outlined in Melamed (1998), a sequence of words 
(ii) an English phrase may only be unambiguously 
linked to exactly one French phrase and vice 
versa. 
Phrase-to-phrase alignments can be nested. For instance, 
the shorter English?French phrase translation 9-9 <-
> 11-11 is included in the longer phrase translation 
5-11 <-> 9-12:  
5-11 9-12: for a detailed explanation 
 as to what  
 <-> fournir de telles explications 
9-9 11-11: as <-> telles 
In this way structural information can be stored. On the 
other hand, we do not allow ambiguous phrase align-
ments as e.g.: 
 8-8 12-12 explanation <-> explications 
11-11 12-12  what <-> explications 
When extracting phrase-to-phrase translations from the 
word-to-word alignment data we include a sufficient 
context which disambiguates the phrases. Given the 
word alignment data in example 1, the minimum con-
text required to disambiguate the French word ?explica-
tions? is the phrase 5-11 <-> 9-12. 
From the word alignment data we generate bilingual 
dictionaries in two different ways: a minimal dictionary 
contains only the shortest unambiguous phrase-to-r
A2.2 
                                                          
which translates in a non-compositional fashion into a 
target sequence is exhaustively linked (see example 2). 
Phrase-to-phrase alignment 
Phrase-to-phrase alignment is represented by intervals 
indicating the starting and ending words of the phrases. 
In phrase-to-phrase alignment:  
(i) a sequence of English word tokens (i.e. a 
phrase) are mutually linked with sequences of 
French word tokens (i.e. a French phrase)2.  
                                                           
2 We do not use the term ?phrase? here in its linguistic sense: a 
phrase in this paper can be any sequence of words, even if 
they are not a linguistic constituent. 
phrase translations. For instance, from the alignment 
data in example 1, the following 8 entries are generated 
as a minimal dictionary:3 
 En  Fr 
 1-1   1-1  
 2-13  2-12  
 3-3   2-2  
 4-4   3-3   
 5-11   9-12  
 9-9 11-11  
14-14 14-14  
15-15 16-16  
 
3 As shorthand notation we use here the offset numbers. In the 
generated dictionary, we have extracted the sequences of 
words instead of the offset numbers. 
In an exhaustive dictionary all possible unambiguous 
phrase translations are extracted. An exhaustive diction-
ary is a superset of the minimal dictionary. For example 
1, seven additional entries are generated: 
 En  Fr 
 1-13  1-12 
 1-14  1-14 
 1-15  1-16 
 2-14  2-14 
 2-15  2-16 
 3-4   2-3  
14-15 14-16 
Note that these additional phrase translations can be 
compositionally generated with the minimal dictionary. 
To evaluate the word alignment data through phrasal 
alignments, we generated three types of dictionaries for 
all 16 submissions and the test data:  
(i) an alignment-based minimal dictionary, 
align-dic1; actually 447 small dictionar-
ies for each sample alignment. 
(ii) a text-based minimal dictionary (text-
dic1)which is the union of the align-dic1. 
(iii) an exhaustive text-based dictionary (text-
dic2) which is the union of exhaustive 
alignment dictionaries. 
As can be seen from figure 1, the size of the ex-
haustive dictionary (text-dic2) is in most cases 
much bigger than those of the minimal dictionar-
ies align-dic1 and text-dic1. The reason is due to 
the way the data has been aligned.  
3 The word alignment data 
In this section we show that the test alignment 
data is structurally different from the submitted 
data. The hand aligned test data reflects the 
phrasal nature of the alignments, while the sub-
missions are to a greater extent compositional.  
The test data (see set 17 in figure 1) has about twice
three times as many word-alignment points than 
submissions. While this often leads to high precis
and lower recall for word alignment, the reverse is t
for the extracted phrasal dictionaries (also figure 3). T
test alignment data of sample 91 contains 68 word
word alignment points shown in example 2; about f
times the average number of word alignment points 
this sample. Comparing example 2 with the submit
data of submission 16 (example 3) brings to light 
phrasal nature of the test set. 
Extracting a minimal phrase dictionary from test 
word alignment data in example 2 produces the follo
ing three entries 
 1-14  1-15 
 5-8   7-12 
15-15 16-16 
The following additional entry is generated in the 
haustive dictionary:1-15 <-> 1-16. Note that m
word alignments could have been possible here, for in-
stance: 
i <-> je 
asking <-> demand? 
explanation <-> explications  
not <-> ne , pas 
Despite the existence of some fine-grained word-to-
word correspondences in the test data, human aligners 
tend to mark phrasal translations. In contrast to the 
phrasal nature of the test alignments, most submissions 
show a more compositional alignment structure. For 
instance, alignment data of sample 91 for submission 16 
has 18 word-to-word alignment points (example 3). 
When the alignments are more compositional, more 
coherent phrasal translations can be extracted. Thus, the 
minimal phrase dictionary extracted from example 3 
hile t -
efore e 
 the lt 
 prec -
Example 2: sample 91  
of test set: 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Example 3: sample 91  
of submission 16 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
  to 
the 
contains 13 entries, w
tains 54 entries. Ther
phrase dictionaries on
in high recall and low
15                 x
14              xxx 
13              xxx 
12    x         xxx 
11              xxx 
10              xxx 
09              xxx 
08        xxxxxx    
07        xxxxxx    
06        xxxxxx    
05        xxxxxx    
04  xxxxxx          
03  xxxxxx          
02  xxxxxx          
01  xxxxxx          
00                  
   012345678901234564 
ion 
rue 
he 
-to-
our 
for 
ted 
the 
set 
w-
ex-
ore 
mitted word alignment dat
in high precision and low r
Phrase-based Eva
Figure 2 shows the correla
extracted dictionaries and t
sure and probable). For eac
calculated as 2*precision
curves for the alignment-b
show the mean f-score com
Roughly all submissions 
word-to-word alignment an
We wanted to see what fa
recall. A correlation betw
alignment and its average he exhaustive dictionary con
, we expect that mapping th
test dictionaries would resu
ision while mapping the sub
15                 x
14                x 
13              xx  
12         x        
11           x      
10           x      
09            x     
08             x    
07          x       
06        x         
05       x          
04       x          
03      x           
02   xxx            
01  x               
00                  
   01234567890123456a on the test data would result 
ecall.  
luation 
tion of the f-score of the three 
he word alignment data (both 
h submission the f-score was 
*recall/precision+recall. The 
ased dictionaries (align-dic1) 
puted over all 447 samples. 
show a similar pattern for 
d phrase dictionaries. 
ctors influence precision and 
een the length of the sample 
recall and precision is shown 
Figure 2:  f-score of word alignments and dictionaries
0
10
20
30
40
50
60
70
80
90
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16
sure
prob
align-dic1
text-dic1
text dic2
in figure 3. As one would have expected, the graph 
shows a tendency that shorter samples are easier to align 
(higher precision and recall) than longer samples. How-
ever, there is higher variation among shorter alignments 
than among longer sample alignments which indicates 
unpredictability of shorter samples. As an example con-
sider alignment sample 7 (length 3):  
hear, hear ! <-> bravo ! 
The extracted minimal test dictionary contains the two 
entries:  
hear,hear <-> bravo 
! <->! 
While most of the minimal dictionaries extracted from 
the submitted data contain the entries: 
hear <-> bravo 
! <-> ! 
This leads to a value of 50 for recall and precision for 
both word and phrase alignments. The average recall 
and precision of sample 7 (length 3) is 53,1 and 57,8. 
Figure 3 also shows that PD-recall (phrasal dictionary) 
is higher than PD-precision as the samples become 
longer. For example, sample 91 (length 15,5) has PD-
recall and PD-precision values of 65, and 50,2 respec-
tively. For word-to-word evaluation, however, WD-
precision is higher than WD-recall.  For sample 91 
(length 15,5)  the WD-recall and WD-precision values 
are 18,03 and 53,86 respectively.  
Next we wanted to see which parts in the sample align-
ments would be easy and which parts would be difficult 
to align. We assume that correct translations which ap-
pear in all submissions would be easy to find while 
translations which occur only in the test set but in none 
of the submissions would be difficult to find. Finally, 
the same noisy translations produced by all submissions 
would indicate mistakes in the test data. The cardinality 
of these sets is shown in the table below. 
Intersection of text-dic1 text-dic2
correct 150 434
missing 837 1949
noise 11 22
There were 150 one-word entries in 
the intersection of the correct transla-
tions contained in all 16 dictionaries 
text-dic1. These translations include 
transfer rules which are easy to dis-
cover such as numbers, function 
words, pronouns, frequent content 
words and also domain specific trans-
lations: 
Figure3:  length of alignments vs. Recall and Precision
0
10
20
30
40
50
60
70
80
90
100
Le
ng
th
Length
PD_Recall
PD_Precision
WD_Recall
WD_Precision
1) pronouns 
he <-> il 
it <-> il 
there <-> il 
2) frequent content words 
women <-> femmes 
work <-> travaillent 
compulsory <--> obligatoire 
say <-> dire 
says <-> dit 
3) function words 4,
5 7
9,
5 12
14
,5 17
19
,5 22
24
,5 27
such <-> tel 
to <-> de 
to <-> pour 
5) Text typical translations: 
House <-> Chambre 
The set of translation equivalences missing in all sub-
missions was much larger. There were only the follow-
ing five one-word equivalences: 
1) on-word translations: 
and <-> puisque 
balance <-> niveau 
do <-> fait 
per <-> le 
very <-> fondamentalement 
Most of the missing entries were multi-word transla-
tions, such as idioms, compound words etc.  
1) idiomatic expressions 
A buck is a buck is a buck <-> une 
piastre est toujours une piastre 
thank you very much <-> je vous re-
mercie 
2) compound: 
Canadian Wheat Board <->  
  Commission canadienne de le bl? 
3) complex prepositions 
as for <-> en ce qui concerne 
4) complex verbs and negation 
does not like <-> ne aime pas 
will be <-> feront 
5) adverbs and adjective phrases 
previous <-> qui me a pr?c?d? 
a good thing <-> int?ressant 
6) unresolved pronouns 
the government <-> il 
There were also 11 noisy entries which occurred in all 
generated submissions dictionaries but not in the test 
data dictionary. The obvious explanation for this is, 
again, the phrasal nature of the test data: single word 
translations would be hidden in phrase translations and 
not extracted as separated word translations:  
before <-> avant 
believe <-> crois 
days <-> jours 
every <-> chacune 
facilities <-> installations 
jobs <-> emploi 
positive <-> positifs 
public <-> public 
representations <-> instances 
why <-> comment 
will <-> servira 
5 
6 
Submissions 
This section lists the origin of the submitted data. A 
more detailed description can be found in the system 
description contained in these proceedings. 
1 BiBr.EF.7 
Limited Resources  7. intersection of 1 & 3 
2 BiBr.EF.1  
Limited Resources 1.  Baseline of Bi-lingual Bracketing 
3 BiBr.EF.2  
Unlimited Resources 2. Baseline of Bi-lingual Bracket-
ing + POS (Brill's POS tagger for English only) 
4 BiBr.EF.8  
Unlimited Resources 8.  intersection of 3 & 6 
5 BiBr.EF.3  
Unlimited Resources 3.  Baseline of Bi-lingual Bracket-
ing + POS (Brill's POS tagger for English only) + Eng-
lish_Chunker. 
6 BiBr.EF.4  
Limited Resources 4.  reverse direction of (1) 
7 BiBr.EF.5  
Unlimited Resources 4.  reverse direction of (2) 
8 BiBr.EF.6  
Unlimited Resources 4.  reverse direction of (3) 
9 data withdrawn 
10 UMD.EF. 
Limited Resources Trained on House and Senate Data 
11 ProAlign.EF.1  
Unlimited Resources ProAlign uses the cohesion be-
tween the source and target languages to constrain the 
search for the most probable alignment (based on a 
novel probability model). The extra resources include: 
An English parser A distributional similarity database 
for English words. 
12 data withdrawn 
13 XRCE.Base.EF.1  
Limited Resources GIZA++ with English and French 
lemmatizer (no trinity lexicon) 
14 XRCE.Nolem.EF.2  
Limited Resources GIZA++ only (no lemmatizer, no 
trinity lexicon), Corpus used: Quarter 
15 XRCE.Nolem.EF.3  
Limited Resources GIZA++ only (no lemmatizer, no 
trinity lexicon), Corpus used: Half 
16 ralign.EF.1  
Limited Resources Recursive parallel segmentation of 
texts; scoring based on IBM-2 
17 test data (golden standard) 
References 
Ulrich Germann, editor (2001). Aligned Hansards of the 
36th Parliamentof Canada. http://www.isi.edu/natural-
language/download/hansard/index.html 
I. Dan Melamed (1998). Annotation Style Guide for the 
Blinker Project, IRCS Technical Report #98-06, 
http://www.cs.nyu.edu/~melamed/ftp/papers/styleguide.
ps.gz 
Franz Josef Och, Hermann Ney (2000). A Comparison 
of Alignment Models for Statistical Machine Transla-
tion.. COLING 2000..http://www-i6.informatik.rwth-
aachen.de/Colleagues/och/COLING00.ps 
 	

ffProceedings of the Demonstrations at the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 25?28,
Gothenburg, Sweden, April 26-30 2014.
c
?2014 Association for Computational Linguistics
CASMACAT: A Computer-assisted Translation Workbench
V. Alabau
?
, C. Buck
?
, M. Carl
?
, F. Casacuberta
?
, M. Garc??a-Mart??nez
?
U. Germann
?
, J. Gonz
?
alez-Rubio
?
, R. Hill
?
, P. Koehn
?
, L. A. Leiva
?
B. Mesa-Lao
?
, D. Ortiz
?
, H. Saint-Amand
?
, G. Sanchis
?
, C. Tsoukala
?
?
PRHLT Research Center, Universitat Polit`ecnica de Val`encia
{valabau,fcn,jegonzalez,luileito,dortiz,gsanchis}@dsic.upv.es
?
Copenhagen Business School, Department of International Business Communication
{ragnar.bonk,mc.isv,mgarcia,bm.ibc}@cbs.dk
?
School of Informatics, University of Edinburgh
{cbuck,ugermann,rhill2,pkoehn,hsamand,ctsoukal}@inf.ed.ac.uk
Abstract
CASMACAT is a modular, web-based
translation workbench that offers ad-
vanced functionalities for computer-aided
translation and the scientific study of hu-
man translation: automatic interaction
with machine translation (MT) engines
and translation memories (TM) to ob-
tain raw translations or close TM matches
for conventional post-editing; interactive
translation prediction based on an MT en-
gine?s search graph, detailed recording and
replay of edit actions and translator?s gaze
(the latter via eye-tracking), and the sup-
port of e-pen as an alternative input device.
The system is open source sofware and in-
terfaces with multiple MT systems.
1 Introduction
CASMACAT
1
(Cognitive Analysis and Statistical
Methods for Advanced Computer Aided Trans-
lation) is a three-year project to develop an
advanced, interactive workbench for computer-
assisted translation (CAT). Currently, at the end of
the second year, the tool includes an array of inno-
vative features that combine to offer a rich, user-
focused working environment not available in any
other CAT tool.
CASMACAT works in close collaboration with
the MATECAT project
2
, another open-source web-
based CAT tool. However, while MATECAT is
concerned with conventional CAT, CASMACAT is
focused on enhancing user interaction and facili-
tating the real-time involvement of human trans-
lators. In particular, CASMACAT provides highly
interactive editing and logging features.
1
http://www.casmacat.eu
2
http://www.matecat.com
Through this combined effort, we hope to foster
further research in the area of CAT tools that im-
prove the translation workflow while appealing to
both professional and amateur translators without
advanced technical skills.
GUI
web
server
CAT
server
MT
server
Javascript      PHP
    Python
  Python
web socket
HTTP
HTTP
Figure 1: Modular design of the workbench: Web-
based components (GUI and web server), CAT
server and MT server can be swapped out.
2 Design and components
The overall design of the CASMACAT workbench
is modular. The system consists of four com-
ponents. (1) a front-end GUI implemented in
HTML5 and JavaScript; (2) a back-end imple-
mented in PHP; (3) a CAT server that manages the
editing process and communicates with the GUI
through web sockets; (4) a machine translation
(MT) server that provides raw translation of source
text as well as additional information, such as a
search graph that efficiently encodes alternative
translation options. Figure 1 illustrates how these
components interact with each other. The CAT
and MT servers are written in Python and inter-
act with a number of software components imple-
mented in C++. All recorded information (source,
translations, edit logs) is permanently stored in a
MySQL database.
These components communicate through a
well-defined API, so that alternative implementa-
tions can be used. This modular architecture al-
25
Figure 2: Translation view for an interactive post-editing task.
lows the system to be used partially. For instance,
the CAT and MT servers can be used separately as
part of a larger translation workflow, or only as a
front-end when an existing MT solution is already
in place.
2.1 CAT server
Some of the interactive features of CASMACAT
require real-time interaction, such as interactive
text-prediction (ITP), so establishing an HTTP
connection every time would cause a significant
network overhead. Instead, the CAT server relies
on web sockets, by means of Python?s Tornadio.
When interactive translation prediction is en-
abled, the CAT server first requests a translation
together with the search graph of the current seg-
ment from the MT server. It keeps a copy of the
search graph and constantly updates and visualizes
the translation prediction based on the edit actions
of the human translator.
2.2 MT server
Many of the functions of the CAT server require
information from an MT server. This information
includes not only the translation of the input sen-
tence, but also n-best lists, search graphs, word
alignments, and so on. Currently, the CASMACAT
workbench supports two different MT servers:
Moses (Koehn et al., 2007) and Thot (Ortiz-
Mart??nez et al., 2005).
The main call to the MT server is a request for
a translation. The request includes the source sen-
tence, source and target language, and optionally
a user ID. The MT server returns an JSON object,
following an API based on Google Translate.
3 Graphical User Interface
Different views, based on the MATECAT GUI,
perform different tasks. The translation view is
the primary one, used when translating or post-
editing, including logging functions about the
translation/post-editing process. Other views im-
plement interfaces to upload new documents or to
manage the documents that are already in the sys-
tem. Additionally, a replay view can visualize all
edit actions for a particular user session, including
eye tracking information, if available.
3.1 Post-Editing
In the translation view (Figure 2), the document
is presented in segments and the assistance fea-
tures provided by CASMACAT work at the segment
level. If working in a post-editing task without
ITP, up to three MT or TM suggestions are pro-
vided for the user to choose. Keyboard shortcuts
are available for performing routine tasks, for in-
stance, loading the next segment or copying source
text into the edit box. The user can assign different
status to each segment, for instance, ?translated?
for finished ones or ?draft? for segments that still
need to be reviewed. Once finished, the translated
document can be downloaded in XLIFF format.
3
In the translation view, all user actions re-
lated to the translation task (e.g. typing activity,
mouse moves, selection of TM proposals, etc.) are
recorded by the logging module, collecting valu-
able information for off-line analyses.
3.2 Interactive Translation Prediction
Here we briefly describe the main advanced CAT
features implemented in the workbench so far.
Intelligent Autocompletion: ITP takes place
every time a keystroke is detected by the sys-
tem (Barrachina et al., 2009). In such event, the
system produces a prediction for the rest of the
sentence according to the text that the user has al-
ready entered. This prediction is placed at the right
of the text cursor.
Confidence Measures: Confidence mea-
sures (CMs) have two main applications in
3
XLIFF is a popular format in the translation industry.
26
MT (Gonz?alez-Rubio et al., 2010). Firstly, CMs
allow the user to clearly spot wrong translations
(e.g., by rendering in red those translations
with very low confidence according to the MT
module). Secondly, CMs can also inform the user
about the translated words that are dubious, but
still have a chance of being correct (e.g., rendered
in orange). Figure 3 illustrates this.
Figure 3: Visualisation of Confidence Measures
Prediction Length Control: Providing the user
with a new prediction whenever a key is pressed
has been proved to be cognitively demanding (Al-
abau et al., 2012). Therefore, the GUI just displays
the prediction up to the first wrong word according
to the CMs provided by the system (Figure 4).
Figure 4: Prediction Length Control
Search and Replace: Most of CAT tools pro-
vide the user with intelligent search and replace
functions for fast text revision. CASMACAT fea-
tures a straightforward function to run search and
replacement rules on the fly.
Word Alignment Information: Alignment of
source and target words is an important part of
the translation process (Brown et al., 1993). To
display their correspondence, they are hihglighted
every time the user places the mouse or the text
cursor on a word; see Figure 5.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Figure 5: Visualisation of Word Alignment
Prediction Rejection: With the purpose of eas-
ing user interaction, CASMACAT also supports a
one-click rejection feature (Sanchis-Trilles et al.,
2008). This feature invalidates the current predic-
tion made for the sentence that is being translated,
and provides the user with an alternate one.
3.3 Replay mode and logging functions
The CASMACAT workbench implements detailed
logging of user activity data, which enables both
automatic analysis of translator behaviour and
retrospective replay of a user session. Replay
takes place in the translation view of the GUI
and it displays the screen status of the recorded
translation/post-editing process. The workbench
also features a plugin to enrich the replay mode
with gaze data coming from an eye-tracker. This
eye-tracking integration is possible through a
project-developed web browser extension which,
at the moment, has only been fully tested with SR-
Research EyeLinks
4
.
4 E-pen Interaction
E-pen interaction is intended to be a complemen-
tary input rather than a substitution of the key-
board. The GUI features the minimum compo-
nents necessary for e-pen interaction; see Figure 6.
When the e-pen is enabled, the display of the cur-
rent segment is changed so that the source seg-
ment is shown above the target segment. Then the
drawing area is maximised horizontally, facilitat-
ing handwriting, particularly in tablet devices. An
HTML canvas is also added over the target seg-
ment, where the user?s drawings are handled. This
is achieved by means of MINGESTURES (Leiva
et al., 2013), a highly accurate, high-performance
gesture set for interactive text editing that can dis-
tinguish between gestures and handwriting. Ges-
tures are recognised on the client side so the re-
sponse is almost immediate. Conversely, when
handwritten text is detected, the pen strokes are
sent to the server. The hand-written text recog-
nition (HTR) server is based on iAtros, an open
source HMM decoder.
if any feature not
is available on your network
substitution
Figure 6: Word substitution with e-pen interaction
5 Evaluation
The CASMACAT workbench was recently evalu-
ated in a field trial at Celer Soluciones SL, a
language service provider based in Spain. The
trial involved nine professional translators work-
ing with the workbench to complete different post-
editing tasks from English into Spanish. The pur-
4
http://www.sr-research.com
27
pose of this evaluation was to establish which of
the workbench features are most useful to profes-
sional translators. Three different configurations
were tested:
? PE: The CASMACAT workbench was used
only for conventional post-editing, without
any additional features.
? IA: Only the Intelligent Autocompletion fea-
ture was enabled. This feature was tested sep-
arately because it was observed that human
translators substantially change the way they
interact with the system.
? ITP: All features described in Section 3.2
were included in this configuration, except-
ing CMs, which were deemed to be not accu-
rate enough for use in a human evaluation.
For each configuration, we measured the aver-
age time taken by the translator to produce the fi-
nal translation (on a segment basis), and the aver-
age number of edits required to produce the final
translation. The results are shown in Table 1.
Setup Avg. time (s) Avg. # edits
PE 92.2 ? 4.82 141.39 ? 7.66
IA 86.07 ? 4.92 124.29 ? 7.28
ITP 123.3 ? 29.72 137.22 ? 13.67
Table 1: Evaluation of the different configurations
of the CASMACAT workbench. Edits are measured
in keystrokes, i.e., insertions and deletions.
While differences between these numbers are
not statistically significant, the apparent slowdown
in translation with ITP is due to the fact that all
translators had experience in post-editing but none
of them had ever used a workbench featuring in-
telligent autocompletion before. Therefore, these
were somewhat unsurprising results.
In a post-trial survey, translators indicated that,
on average, they liked the ITP system the best.
They were not fully satisfied with the freedom of
interactivity provided by the IA system. The lack
of any visual aid to control the intelligent auto-
completions provided by the system made transla-
tors think that they had to double-check any of the
proposals made by the system when making only
a few edits.
6 Conclusions
We have introduced the current CASMACAT work-
bench, a next-generation tool for computer as-
sisted translation. Each of the features available
in the most recent prototype of the workbench has
been explained. Additionally, we have presented
an executive report of a field trial that evaluated
genuine users? performance while using the work-
bench. Although E-pen interaction has not yet
been evaluated outside of the laboratory, it will the
subject of future field trials, and a working demon-
stration is available.
Acknowledgements
Work supported by the European Union 7
th
Framework Program (FP7/2007-2013) under the
CASMACAT project (grant agreement n
o
287576).
References
Vicent Alabau, Luis A. Leiva, Daniel Ortiz-Mart??nez,
and Francisco Casacuberta. 2012. User evaluation
of interactive machine translation systems. In Proc.
EAMT, pages 20?23.
Sergio Barrachina et al. 2009. Statistical approaches to
computer-assisted translation. Computational Lin-
guistics, 35(1):3?28.
Peter Brown et al. 1993. The mathematics of statistical
machine translation: Parameter estimation. Compu-
tational linguistics, 19(2):263?311.
Jes?us Gonz?alez-Rubio, Daniel Ortiz-Mart??nez, and
Francisco Casacuberta. 2010. On the use of confi-
dence measures within an interactive-predictive ma-
chine translation system. In Proc. of EAMT.
Philipp Koehn et al. 2007. Moses: Open source toolkit
for statistical machine translation. In Proc. of ACL,
pages 177?180.
Luis A. Leiva, Vicent Alabau, and Enrique Vidal.
2013. Error-proof, high-performance, and context-
aware gestures for interactive text edition. In Proc.
of CHI, pages 1227?1232.
Daniel Ortiz-Mart??nez, Ismael Garc??a-Varea, and Fran-
cisco Casacuberta. 2005. Thot: a toolkit to train
phrase-based statistical translation models. In Proc.
of MT Summit X, pages 141?148.
G. Sanchis-Trilles et al. 2008. Improving interactive
machine translation via mouse actions. In Proc. of
EMNLP, pages 485?494.
28
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 346?351,
Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics
Automatically Predicting Sentence Translation Difficulty
Abhijit Mishra?, Pushpak Bhattacharyya?, Michael Carl?
? Department of Computer Science and Engineering, IIT Bombay, India
{abhijitmishra,pb}@cse.iitb.ac.in
? CRITT, IBC, Copenhagen Business School, Denmark,
mc.ibc@cbs.dk
Abstract
In this paper we introduce Translation Dif-
ficulty Index (TDI), a measure of diffi-
culty in text translation. We first de-
fine and quantify translation difficulty in
terms of TDI. We realize that any mea-
sure of TDI based on direct input by trans-
lators is fraught with subjectivity and ad-
hocism. We, rather, rely on cognitive ev-
idences from eye tracking. TDI is mea-
sured as the sum of fixation (gaze) and
saccade (rapid eye movement) times of
the eye. We then establish that TDI is
correlated with three properties of the in-
put sentence, viz. length (L), degree of
polysemy (DP) and structural complexity
(SC). We train a Support Vector Regres-
sion (SVR) system to predict TDIs for
new sentences using these features as in-
put. The prediction done by our frame-
work is well correlated with the empiri-
cal gold standard data, which is a repos-
itory of < L,DP, SC > and TDI pairs
for a set of sentences. The primary use of
our work is a way of ?binning? sentences
(to be translated) in ?easy?, ?medium? and
?hard? categories as per their predicted
TDI. This can decide pricing of any trans-
lation task, especially useful in a scenario
where parallel corpora for Machine Trans-
lation are built through translation crowd-
sourcing/outsourcing. This can also pro-
vide a way of monitoring progress of sec-
ond language learners.
1 Introduction
Difficulty in translation stems from the fact that
most words are polysemous and sentences can be
long and have complex structure. While length of
sentence is commonly used as a translation diffi-
culty indicator, lexical and structural properties of
a sentence also contribute to translation difficulty.
Consider the following example sentences.
1. The camera-man shot the policeman
with a gun. (length-8)
2. I was returning from my old office
yesterday. (length-8)
Clearly, sentence 1 is more difficult to process
and translate than sentence 2, since it has lexical
ambiguity (?Shoot? as an act of firing a shot or
taking a photograph?) and structural ambiguity
(Shot with a gun or policeman with a gun?). To
produce fluent and adequate translations, efforts
have to be put to analyze both the lexical and syn-
tactic properties of the sentences.
The most recent work on studying translation
difficulty is by Campbell and Hale (1999) who
identified several areas of difficulty in lexis and
grammar. ?Reading? researchers have focused on
developing readability formulae, since 1970. The
Flesch-Kincaid Readability test (Kincaid et al,
1975), the Fry Readability Formula (Fry, 1977)
and the Dale-Chall readability formula (Chall and
Dale, 1999) are popular and influential. These for-
mulae use factors such as vocabulary difficulty (or
semantic factors) and sentence length (or syntac-
tic factors). In a different setting, Malsburg et
al. (2012) correlate eye fixations and scanpaths
of readers with sentence processing. While these
approaches are successful in quantifying readabil-
ity, they may not be applicable to translation sce-
narios. The reason is that, translation is not
merely a reading activity. Translation requires
co-ordination between source text comprehension
and target text production (Dragsted, 2010). To
the best of our knowledge, our work on predicting
TDI is the first of its kind.
The motivation of the work is as follows. Cur-
rently, for domain specific Machine Translation
systems, parallel corpora are gathered through
translation crowdsourcing/outsourcing. In such
346
Figure 1: Inherent sentence complexity and per-
ceived difficulty during translation
a scenario, translators are paid on the basis of
sentence length, which ignores other factors con-
tributing to translation difficulty, as stated above.
Our proposed Translation Difficulty Index (TDI)
quantifies the translation difficulty of a sentence
considering both lexical and structural proper-
ties. This measure can, in turn, be used to clus-
ter sentences according to their difficulty levels
(viz. easy, medium, hard). Different payment and
schemes can be adopted for different such clusters.
TDI can also be useful for training and evalu-
ating second language learners. For example, ap-
propriate examples at particular levels of difficulty
can be chosen for giving assignments and monitor-
ing progress.
The rest of the paper is organized in the fol-
lowing way. Section 2 describes TDI as func-
tion of translation processing time. Section 3 is
on measuring translation processing time through
eye tracking. Section 4 gives the correlation of
linguistic complexity with observed TDI. In sec-
tion 5, we describe a technique for predicting TDIs
and ranking unseen sentences using Support Vec-
tor Machines. Section 6 concludes the paper with
pointers to future work.
2 Quantifying Translation Difficulty
As a first approximation, TDI of a sentence can
be the time taken to translate the sentence, which
can be measured through simple translation exper-
iments. This is based on the assumption that more
difficult sentences will require more time to trans-
late. However, ?time taken to translate? may not
be strongly related to the translation difficulty for
two reasons. First, it is difficult to know what
fraction of the total translation time is actually
spent on the translation-related-thinking. For ex-
ample, translators may spend considerable amount
of time typing/writing translations, which is ir-
relevant to the translation difficulty. Second, the
translation time is sensitive to distractions from
the environment. So, instead of the ?time taken
to translate?, we are more interested in the ?time
for which translation related processing is carried
out by the brain?. This can be termed as the Trans-
lation Processing Time (Tp). Mathematically,
Tp = Tp comp + Tp gen (1)
Where Tp comp and Tp gen are the processing times
for source text comprehension and target text gen-
eration respectively. The empirical TDI, is com-
puted by normalizing Tp with sentence length.
TDI = Tpsentencelength (2)
Measuring Tp is a difficult task as translators of-
ten switch between thinking and writing activities.
Here comes the role of eye tracking.
3 Measuring Tp by eye-tracking
We measure Tp by analyzing the gaze behavior
of translators through eye-tracking. The rationale
behind using eye-tracking is that, humans spend
time on what they see, and this ?time? is corre-
lated with the complexity of the information being
processed, as shown in Figure 1. Two fundamental
components of eye behavior are (a) Gaze-fixation
or simply, Fixation and (b) Saccade. The former
is a long stay of the visual gaze on a single loca-
tion. The latter is a very rapid movement of the
eyes between positions of rest. An intuitive feel
for these two concepts can be had by consider-
ing the example of translating the sentence The
camera-man shot the policeman with a gun men-
tioned in the introduction. It is conceivable that
the eye will linger long on the word ?shot? which
is ambiguous and will rapidly move across ?shot?,
?camera-man? and ?gun? to ascertain the clue for
disambiguation.
The terms Tp comp and Tp gen in (1) can now be
looked upon as the sum of fixation and saccadic
durations for both source and target sentences re-
spectively.
Modifying 1
Tp =
?
f?Fs
dur(f) +
?
s?Ss
dur(s)
+
?
f?Ft
dur(f) +
?
s?St
dur(s)
(3)
347
Figure 2: Screenshot of Translog. The circles rep-
resent fixations and arrow represent saccades.
Here, Fs and Ss correspond to sets of fixations and
saccades for source sentence and Ft and St corre-
spond to those for the target sentence respectively.
dur is a function returning the duration of fixations
and saccades.
3.1 Computing TDI using eye-tracking
database
We obtained TDIs for a set of sentences from
the Translation Process Research Database (TPR
1.0)(Carl, 2012). The database contains trans-
lation studies for which gaze data is recorded
through the Translog software1(Carl, 2012). Fig-
ure 2 presents a screendump of Translog. Out of
the 57 available sessions, we selected 40 transla-
tion sessions comprising 80 sentence translations2.
Each of these 80 sentences was translated from
English to three different languages, viz. Span-
ish, Danish and Hindi by at least 2 translators.
The translators were young professional linguists
or students pursuing PhD in linguistics.
The eye-tracking data is noisy and often ex-
hibits systematic errors (Hornof and Halverson,
2002). To correct this, we applied automatic er-
ror correction technique (Mishra et al, 2012) fol-
lowed by manually correcting incorrect gaze-to-
word mapping using Translog. Note that, gaze and
saccadic durations may also depend on the transla-
tor?s reading speed. We tried to rule out this effect
by sampling out translations for which the vari-
ance in participant?s reading speed is minimum.
Variance in reading speed was calculated after tak-
ing a samples of source text for each participant
and measuring the time taken to read the text.
After preprocessing the data, TDI was com-
puted for each sentence by using (2) and (3).The
observed unnormalized TDI score3 ranges from
0.12 to 0.86. We normalize this to a [0,1] scale
1http://www.translog.dk
220% of the translation sessions were discarded as it was
difficult to rectify the gaze logs for these sessions.
3Anything beyond the upper bound is hard to translate and
can be assigned with the maximum score.
Figure 3: Dependency graph used for computing
SC
using MinMax normalization.
If the ?time taken to translate? and Tp were
strongly correlated, we would have rather opted
?time taken to translate? for the measurement of
TDI. The reason is that ?time taken to translate?
is relatively easy to compute and does not require
expensive setup for conducting ?eye-tracking? ex-
periments. But our experiments show that there
is a weak correlation (coefficient = 0.12) between
?time taken to translate? and Tp. This makes us
believe that Tp is still the best option for TDI mea-
surement.
4 Relating TDI to sentence features
Our claim is that translation difficulty is mainly
caused by three features: Length, Degree of Poly-
semy and Structural Complexity.
4.1 Length
It is the total number of words occurring in a sen-
tence.
4.2 Degree of Polysemy (DP)
The degree of polysemy of a sentence is the sum of
senses possessed by each word in the Wordnet nor-
malized by the sentence length. Mathematically,
DPsentence =
?
w?W Senses(w)
length(sentence) (4)
Here, Senses(w) retrieves the total number senses
of a word P from the Wordnet. W is the set of
words appearing in the sentence.
4.3 Structural Complexity (SC)
Syntactically, words, phrases and clauses are at-
tached to each other in a sentence. If the attach-
ment units lie far from each other, the sentence
has higher structural complexity. Lin (1996) de-
fines it as the total length of dependency links in
the dependency structure of the sentence.
348
Figure 4: Prediction of TDI using linguistic prop-
erties such as Length(L), Degree of Polysemy
(DP) and Structural Complexity (SC)
Example: The man who the boy attacked
escaped.
Figure 3 shows the dependency graph for the
example sentence. The weights of the edges cor-
respond how far the two connected words lie from
each other in the sentence. Using Lin?s formula,
the SC score for the example sentence turns out to
be 15.
Lin?s way of computing SC is affected by sen-
tence length since the number of dependency links
for a sentence depends on its length. So we nor-
malize SC by the length of the sentence. After
normalization, the SC score for the example given
becomes 15/7 = 2.14
4.4 How are TDI and linguistic features
related
To validate that translation difficulty depends on
the above mentioned linguistic features, we tried
to find out the correlation coefficients between
each feature and empirical TDI. We extracted
three sets of sample sentences. For each sample,
sentence selection was done with a view to vary-
ing one feature, keeping the other two constant.
The Correlation Coefficients between L, DP and
SC and the empirical TDI turned out to be 0.72,
0.41 and 0.63 respectively. These positive correla-
tion coefficients indicate that all the features con-
tribute to the translation difficulty.
5 Predicting TDI
Our system predicts TDI from the linguistic prop-
erties of a sentence as shown in Figure 4.
The prediction happens in a supervised setting
through regression. Training such a system re-
quires a set sentences annotated with TDIs. In
our case, direct annotation of TDI is a difficult and
unintuitive task. So, we annotate TDI by observ-
Kernel(C=3.0) MSE (%) Correlation
Linear 20.64 0.69
Poly (Deg 2) 12.88 0.81
Poly (Deg 3) 13.35 0.78
Rbf (default) 13.32 0.73
Table 1: Relative MSE and Correlation with ob-
served data for different kernels used for SVR.
ing translator?s behavior (using equations (1) and
(2))instead of asking people to rate sentences with
TDI.
We are now prepared to give the regression sce-
nario for predicting TDI.
5.1 Preparing the dataset
Our dataset contains 80 sentences for which TDI
have been measured (Section 3.1). We divided this
data into 10 sets of training and testing datasets in
order to carry out a 10-fold evaluation. DP and SC
features were computed using Princeton Wordnet4
and Stanford Dependence Parser5.
5.2 Applying Support Vector Regression
To predict TDI, Support Vector Regression (SVR)
technique (Joachims et al, 1999) was preferred
since it facilitates multiple kernel-based methods
for regression. We tried using different kernels us-
ing default parameters. Error analysis was done
by means of Mean Squared Error estimate (MSE).
We also measured the Pearson correlation coeffi-
cient between the empirical and predicted TDI for
our test-sets.
Table 1 indicates Mean Square Error percent-
ages for different kernel methods used for SVR.
MSE (%) indicates by what percentage the pre-
dicted TDIs differ from the observed TDIs. In our
setting, quadratic polynomial kernel with c=3.0
outperforms other kernels. The predicted TDIs are
well correlated with the empirical TDIs. This tells
us that even if the predicted scores are not as ac-
curate as desired, the system is capable of ranking
sentences in correct order. Table 2 presents exam-
ples from the test dataset for which the observed
TDI (TDIO) and the TDI predicted by polynomial
kernel based SVR (TDIP ) are shown.
Our larger goal is to group unknown sentences
into different categories by the level of transla-
4http://www.wordnet.princeton.edu
5http://www.nlp.stanford.edu/software/
lex-parser.html
349
Example L DP SC TDIO TDIP Error
1. American Express recently
announced a second round
of job cuts. 10 10 1.8 0.24 0.23 4%
2. Sociology is a relatively
new academic discipline. 7 6 3.7 0.49 0.53 8%
Table 2: Example sentences from the test dataset.
tion difficulty. For that, we tried to manually as-
sign three different class labels to sentences viz.
easy, medium and hard based on the empirical
TDI scores. The ranges of scores chosen for easy,
medium and hard categories were [0-0.3], [0.3-
0.75] and [0.75-1.0] respectively (by trial and er-
ror). Then we trained a Support Vector Rank
(Joachims, 2006) with default parameters using
different kernel methods. The ranking framework
achieves a maximum 67.5% accuracy on the test
data. The accuracy should increase by adding
more data to the training dataset.
6 Conclusion
This paper introduces an approach to quantify-
ing translation difficulty and automatically assign-
ing difficulty levels to unseen sentences. It estab-
lishes a relationship between the intrinsic senten-
tial properties, viz., length (L), degree of polysemy
(DP) and structural complexity (SC), on one hand
and the Translation Difficulty Index (TDI), on the
other. Future work includes deeper investigation
into other linguistic factors such as presence of do-
main specific terms, target language properties etc.
and applying more sophisticated cognitive analy-
sis techniques for more reliable TDI score. We
would like to make use of inter-annotator agree-
ment to decide the boundaries for the translation
difficulty categories. Extending the study to differ-
ent language pairs and studying the applicability
of this technique for Machine Translation Quality
Estimation are also on the agenda.
Acknowledgments
We would like to thank the CRITT, CBS group for
their help in manual correction of TPR data. In
particular, thanks to Barto Mesa and Khristina for
helping with Spanish and Danish dataset correc-
tions.
References
Campbell, S., and Hale, S. 1999. What makes a text
difficult to translate? Refereed Proceedings of the
23rd Annual ALAA Congress.
Carl, M. 2012. Translog-II: A Program for Record-
ing User Activity Data for Empirical Reading and
Writing Research In Proceedings of the Eight In-
ternational Conference on Language Resources and
Evaluation, European Language Resources Associ-
ation (ELRA)
Carl, M. 2012 The CRITT TPR-DB 1.0: A Database
for Empirical Human Translation Process Research.
AMTA 2012 Workshop on Post-Editing Technology
and Practice (WPTP-2012).
Chall, J. S., and Dale, E. 1995. Readability revisited:
the new Dale-Chall readability formula Cambridge,
Mass.: Brookline Books.
Dragsted, B. 2010. Co-ordination of reading andwrit-
ing processes in translation. Contribution to Trans-
lation and Cognition, Shreve, G. and Angelone,
E.(eds.)Cognitive Science Society.
Fry, E. 1977 Fry?s readability graph: Clarification,
validity, and extension to level 17 Journal of Read-
ing, 21(3), 242-252.
Hornof, A. J. and Halverson, T. 2002 Cleaning up sys-
tematic error in eye-tracking data by using required
fixation locations. Behavior Research Methods, In-
struments, and Computers, 34, 592604.
Joachims, T., Schlkopf, B. ,Burges, C and A. Smola
(ed.). 1999. Making large-Scale SVM Learning
Practical. Advances in Kernel Methods - Support
Vector Learning. MIT-Press, 1999,
Joachims, T. 2006 Training Linear SVMs in Lin-
ear Time Proceedings of the ACM Conference on
Knowledge Discovery and Data Mining (KDD).
Kincaid, J. P., Fishburne, R. P., Jr., Rogers, R. L., and
Chissom, B. S. 1975. Derivation of New Read-
ability Formulas (Automated Readability Index, Fog
Count and Flesch Reading Ease Formula) for Navy
Enlisted Personnel Millington, Tennessee: Naval
Air Station Memphis,pp. 8-75.
350
Lin, D. 1996 On the structural complexity of natural
language sentences. Proceeding of the 16th Inter-
national Conference on Computational Linguistics
(COLING), pp. 729733.
Mishra, A., Carl, M, Bhattacharyya, P. 2012 A
heuristic-based approach for systematic error cor-
rection of gaze datafor reading. In MichaelCarl, P.B.
and Choudhary, K.K., editors, Proceedings of the
First Workshop on Eye-tracking and Natural Lan-
guage Processing, Mumbai, India. The COLING
2012 Organizing Committee
von der Malsburg, T., Vasishth, S., and Kliegl, R. 2012
Scanpaths in reading are informative about sen-
tence processing. In MichaelCarl, P.B. and Choud-
hary, K.K., editors, Proceedings of the First Work-
shop on Eye-tracking and Natural Language Pro-
cessing, Mumbai, India. The COLING 2012 Orga-
nizing Committee
351
Workshop on Humans and Computer-assisted Translation, pages 29?37,
Gothenburg, Sweden, 26 April 2014. c?2014 Association for Computational Linguistics
Measuring the Cognitive Effort of Literal Translation Processes 
 
Moritz Schaeffer  
Dalgas Have 15 
Copenhagen Business School 
Denmark 
ms.ibc@cbs.dk 
Michael Carl 
Dalgas Have 15 
 Copenhagen Business School 
Denmark 
mc.isv@cbs.dk 
  
 
Abstract 
It has been claimed that human translators rely 
on some sort of literal translation equivalences 
to produce translations and to check their 
validity. More effort would be required if 
translations are less literal. However, to our 
knowledge, there is no established metric to 
measure and quantify this claim. This paper 
attempts to bridge this gap by introducing a 
metric for measuring literality of translations 
and assesses the effort that is observed when 
translators produce translations which deviate 
from the introduced literality definition. 
1 Introduction 
In his seminal paper, Ivir (1981: 58) hypothises 
that: 
 ?The translator begins his search for translation 
equivalence from formal correspondence, and it is 
only when the identical-meaning formal 
correspondent is either not available or not able to 
ensure equivalence that he resorts to formal 
correspondents with not-quite-identical meanings or 
to structural and semantic shifts which destroy formal 
correspondence altogether. But even in the latter case 
he makes use of formal correspondence as a check on 
meaning - to know what he is doing, so to speak.? 
Related to this notion of ?formal 
correspondence? is the law of interference which 
accounts for the observation that ?in translation, 
phenomena pertaining to the make-up of the 
source text tend to be transferred to the target 
text? (Toury, 1995: 275).  
However, context or cross-linguistic differences 
may make it necessary to abandon formal 
correspondence: it is often necessary to depart 
from a one-to-one correspondence between 
source and target text items, levels or ranks, 
which is confirmed by the statement ?without it 
[formal correspondence], there would be nothing 
to shift from? (Malmkj?r 2011a: 61). 
Tirkkonen-Condit (2005) reformulates Ivir?s 
formal correspondence translation hypothesis 
into a monitor model: ?It looks as if literal 
translation is a default rendering procedure, 
which goes on until it is interrupted by a monitor 
that alerts about a problem in the outcome.? 
Tirkkonen-Condit (2005:408) 
Thus, the formal correspondence hypothesis, the 
literal translation default rendering procedure, 
the law of interference and the monitor model are 
all related concepts which seem to assume that 
one-to-one literal translation correspondences are 
easier to produce than translations that formally 
deviate from the source text, as the latter would 
require more effort, and hence will take longer 
for a translator to produce.  
While it has been difficult to describe in what 
exactly consist literal translation (Malmkj?r 
2011b), we define (ideal) literal translation in 
this paper by the following criteria:  
a) Word order is identical in the source and 
target languages 
b) Source and target text items correspond one-
to-one 
29
Killer nurse receives four live sentences 
11 asesino 7 el_enfermero 15 recibe 28 cuatro 12 perpetuas 13 cadenas 
6 el_asesino 5 enfermero_asesino 3 es_condenado 
  
12 cadenas 11 perpetuas 
3 el_enfermero 4 enfermero 3 condenado_a 
    
2 asesino 
2 enfermero_asesino 4 asesino 2 recibe_a 
      
  
3 un_enfermero 
        
  
2 enfermera 
        
 
c) Each source word has only one possible 
translated form in the given context  
Although this definition of literality ignores a 
wide range of phenomena and kinds of 
equivalence, it allows for quantification and 
comparison across multiple languages. Any 
(voluntary or structural) deviation from these 
criteria would imply a relaxation from a literal 
translation and thus lead to greater effort, as 
measured by e.g. longer production times and 
more gaze activities.  
In this paper we assess this hypothesis by 
analyzing the gazing behavior of translators. As a 
basis for our investigation we use the TPR-DB 
(Carl, 2012), which currently contains more than 
940 text production sessions (translation, post-
editing, editing and copying) in more than 10 
different languages1. For each translation and 
post-editing session keystroke and gaze data was 
collected and stored, and translations were 
manually aligned. The TPR-DB is therefore 
ideally suited for answering aspects of the 
cognitive processes during translation which are 
shared across individuals and language 
combinations. 
In section 2 we operationalize literal translation 
from a process point of view. We describe a 
transducer to measure the similarity of word 
order in the source and target language strings, to 
account for criteria (a) and (b). We introduce the 
                                                          
1 The figures relate to TPR-DBv1.4 which can be 
downloaded from: 
http://bridge.cbs.dk/platform/?q=CRITT_TPR-db  
notion of translation choices, derived from a 
corpus of alternative translations to account for 
criterion (c) above. In section 3, we correlate the 
predictions of the literal translation default 
rendering procedure with observed translators? 
behavior. Section 4 discusses the results. 
2 Operationalizing literal translation 
In this section, we first present a quantification 
of translation choices (literality criterion c) and 
then describe the computation of alignment cross 
values which account for literality criterion (b) 
and (c). 
2.1 Translation Choices 
A source word can often be translated in many 
different ways. In order to quantify such 
translation choices, Choice Network Analysis 
has been suggested (Campbell, 2000) as a 
method to infer cognitive processes from the 
different choices made by different translators: 
the more choices and the more complex choices 
a translator has to consider, the more effortful the 
translation of this particular item is. Campbell 
(2000) argues that translations by different 
translators of the same source text can be used to 
draw inferences about the cognitive processes 
during translation.  
In line with these considerations, to estimate the 
translation effort for lexical selection, we count 
the number of different translation realizations 
for each word. We use the TPR-DB (Carl, 2012, 
Carl et al. 2014) which contains (among others) a 
large number of different translations for the 
same source text. For instance, Figure 1 shows 
the number of Spanish translation choices 
Figure 1: Translation choices and numbers of occurrences as retrieved from 31 En -> ES translations in the TPR-DB 
30
produced by 31 different translators for the same 
English source sentence. Figure 1 only shows 
translations which occur at least twice. Figure 2 
shows one of the realized translations.  
There is a considerable variance in the number of 
translation variants for different words. In 11 out 
of 31 translations ?Killer? was aligned with 
?asesino?, in 6 cases with ?el asesino? etc. while 
for 28 out of 31 cases ?four? was translated as 
?cuatro?. Thus, according to the above 
hypothesis, the translation production of ?Killer? 
would be more effortful than it would be to 
translate ?live? than the translation of ?four?. 
 
Figure 2: Oracle translation with word 
alignments 
2.2 Alignment crossings 
In order to quantify translation locality criterion 
(a) and (b), we adopt a local metric to quantify 
the similarity of the source and target language 
word order, relative to the previous alignment 
position. The metric is implemented as a 
transducer which produces translations word by 
word, writing the correct target language word 
order into an output buffer, while a reading 
device successively scans the source text to find 
the reference word(s) for the next word in the 
translation.   
Given a reference source text (ST), an output 
oracle translation (TT), and the ST-TT 
alignments (as in Figure 2), the CrossT values 
indicate the distance between ST reference 
expressions of successive TT words, in terms of 
progressions and regressions. 
For instance, assume the English source sentence 
?Killer nurse receives four live sentences? was 
translated into Spanish with the alignment 
relations as shown in Figure 2. In order to 
produce the first Spanish TT word ?El?, two 
English words (?Killer? and ?nurse?) have to be 
consumed in the reference text, which results in a 
Cross value of 2. Since the second source word 
(?nurse?) emits two adjacent TT words, no 
further ST word has to be consumed to produce 
?enfermero?, which results in the value Cross=0. 
To produce the third Spanish word, ?asesino?, 
one ST word to the left of ?nurse? has to be 
processed, leading to the Cross value -1. The 
next Spanish word ?recibe? is the translation of 
two words to the right of the current ST cursor 
position; ?cuatro? one ST word ahead etc. with 
their respective Cross values of 2 and 1. Figure 3 
illustrates this process. The inclined reader may 
continue this example and reconstruct how the 
CrossT values {2,0,-1,2,1,2,-1} are incrementally 
generated. Thus, Cross values indicate the 
minimum length of the progressions and 
regressions on the reference text required to 
generate the output string. 
Figure 3: Computation of alignment crossings (CrossT) as 
length of progressions and regressions in the reference ST.  
Cross values can also be computed from the 
source text. For the CrossS values we would then 
31
assume the ST text to be the output text and the 
TT text to be the reference.  
 
While CrossT values reflect the alignment effort 
for mapping ST tokens on the TT structure, as is 
required for translation production, CrossS 
values have a reverse interpretation, as they 
represent the mapping effort of TT tokens on the 
ST structure, as is more likely the case during 
revision. Figure 4 shows the CrossS values for 
the sentence in Figure 2. Note that the sequence 
of CrossT and CrossS are not symmetrical: in the 
given example CrossS: {3,-2,3,1,2,-1}. In section 
3 we will show that both types of effort occur in 
translation and in post-editing.  
The Cross value is small if source and target 
languages are (structurally) similar, and consists 
only of one-to-one token correspondences. The 
more both languages structurally differ or the 
less compositional the translations are, the bigger 
will become the Cross values.  
Similarly, we expect to observe a larger number 
of translation choices as semantic shifts are 
introduced by the translator or if only ?not-quite-
identical meanings? are available. 
3 Translators behaviour  
Different parts of the TPR-DB have been used 
for the different analysis reported in this section. 
A set of 313 translations have been investigated 
to map translation crossings in section 3.1;  86 
sessions were used for the post-editing 
experiment in section 3.2, and 24 translations for 
translation choices reported in section 3.3. 
A simple linear regression was carried, to 
ascertain the extent to which total reading time 
(GazeS and GazeT) can be predicted by Cross 
values in sections 3.1 and 3.2, and by translation 
choices in section 3.3. The correlation for Cross 
values in sections 31 and 3.2 was calculated from 
value 1 to the peak in each distribution in the 
negative and positive directions. Only Cross 
values from -8 to 8 are reported because items 
with higher Cross values are very rare, resulting 
in vastly unequal numbers of items.  
3.1 Alignment Crossing 
This section reports an analysis of 313 
translation sessions with 17 different source texts 
into six different languages as contained in the 
TPR-DB. The target languages were Danish, 
Spanish, English, Chinese, Hindi and German; 
the source languages were English and Danish. 
Figure 5 depicts gazing time on an ST token with 
a given CrossT value, while Figure 6 depicts 
gazing time on the TT tokens with a given 
CrossS value. These figures show that higher 
CrossT and CrossS values are strongly correlated 
with GazeS and GazeT and thus more effortful to 
process than lower CrossT and CrossS values. 
 
Figure 5: Average gazing time (vertical) on ST token with 
different CrossT values (horizontal) 
Correlation between CrossT values and Total 
Reading Time on Source Text 
As shown in Figure 5, a strong positive 
correlation was found between CrossT values 
and total reading time on the source text (r=.97 
for negative CrossT values and r=.91 for positive 
CrossT values). The regression model predicted 
Figure 4: ST alignment crossings (CrossS), as generated 
when checking the ST against the TT 
32
97% and 82% of the variance for negative and 
positive values. The model was a good fit for the 
data (F=205.7, p<.0005 and F=22.89, p<.005, 
respectively). For every single increase in the 
negative CrossT value, the total reading time on 
the source text increased by 516ms, for positive 
CrossT value, the total reading time on the 
source text increased by 347ms. 
 
Figure 6: Average gazing time (vertical) on TT tokens for 
different CrossS values (horizontal) 
Correlation between CrossS values and Total 
Reading Time on Target Text 
Also for negative and positive CrossS values and 
total reading time on the TT a strong positive 
correlation was found (r=.92 and r=.93, 
respectively). The regression model predicted 
84% and 85% of the variance, and was a good fit 
for the data (F=36.97, p<.001, F=30.69, p<.003). 
For every single increase in the negative CrossS 
value, the total reading time on the target text 
increased by 389ms, for positive CrossS values 
the total reading time on the target text increased 
by 301ms. 
3.2 Alignment crossing in post-editing 
This section reports an analysis over 86 different 
post-editing sessions from the TPR-DB of 9 
different English source texts which were 
translated into three different target languages, 
German, Hindi and Spanish. As in section 3.1 the 
analysis shows that CrossT and CrossS values 
correlate with the total reading time per word 
(GazeS and GazeT). Figures 7 and 8 plot gazing 
times on ST and TT token with different CrossT 
and CrossS values during post-editing. 
 
Figure 7: Average gazing time on ST tokens during post-
editing for different CrossT values (horizontal) 
Correlation between CrossT values and total 
reading time on source text 
 
Figure 8: Average gazing time (vertical) on TT tokens 
during post-editing for different CrossS values (horizontal) 
Similarly, a strong positive correlation was found 
between negative CrossT values and total 
reading time on the source text (r=.95 and r=.98), 
and the regression model predicted 88% and 
94% of the variance for negative and positive 
CrossT values. The model was a good fit for the 
data (F=38.50, p<.003 and F=67.56, p<.004). For 
every single increase of the negative CrossT 
value, the total reading time on the target text 
increased by 723ms, while for positive CrossT 
values reading time increased by 566ms.  
33
Correlation between CrossS values and total 
reading time on target text 
A strong positive correlation was found between 
CrossS values and total reading time on the 
target text (r=.95), and the regression model 
predicted 87% and 89% of the variance for 
negative and positive CrossS values respective. 
The model was a good fit for the data (F=35.26, 
p<.004 and F=38.50, p<.003). For every single 
increase in the negative CrossS value, the total 
reading time on the target text increased by 
1179ms, while for positive CrossS values 
reading time increased by 1016ms. 
3.3 Translation choices 
The data used for translation from scratch used 
for this purpose are 24 translations of 3 different 
texts from English into Danish and the data for 
post-editing used for this purpose are 65 post-
edited translations of 9 different source texts 
involving one source language (English) and two 
target languages (German and Spanish). The 
number of alternative translations for every 
source item of the different source texts were 
counted. Only words which had up to 9 
alternative choices were included in the analysis, 
partly so that a comparison between translation 
from scratch and post-editing was possible and 
partly because there are few items with more 
than 9 alternative translations.  
 
Figure 9: Correlation of alternative translation (horizontal) 
and average production time (vertical) for translation 
(TRA) and post-editing (PE). 
Correlation between duration and alternatives 
As shown in Figure 9, for translation from 
scratch and for post-editing there was a strong 
correlation between the time it took participants 
to produce a target word and the number of 
alternatives for every source word (r=.89 and 
r=.99, respectively). With few choices post-
editors are quicker than translators, but this 
distance decreases as the number of translation 
choices increase. The regression model predicted 
76% and 97% of the variance and was a good fit 
for the data (F=26.14, p<.001) for Translation 
and (F=269.50, p<.0001) for post-editing. For 
every increase in the number of alternatives, the 
production time increased by 117ms, 
respectively 278ms for translation and post-
editing. 
 
Figure 10: Correlation of alternative translation 
(horizontal) and average gazing time on TT words (vertical) 
during translation (TRA) and post-editing (PE). 
Correlation between total reading time on the 
target text and alternatives 
Similarly, Figure 10 depicts a strong correlation 
for translation from scratch and for post-editing 
between the total reading time on the target text 
per word and the number of translation choices 
for every source word (r=.90 and r=.87 
respectively). The regression model predicted 
77% and 72% of the variance and the model was 
a good fit for the data; F=28.45, p<.001 and 
F=21.80, p<.002 for translation and post-editing 
respectively. For every increase in the number of 
alternatives, the total reading time on the target 
text increased by 153ms, and 120ms. 
Correlation between total reading time on the 
target text and alternatives 
For translation from scratch, there was a strong 
correlation between total reading time on the 
source text per word and he number of 
alternatives for every source word (r=.76), but 
the regression model only predicted 52% of the 
variance. The model was a good fit for the data 
34
(F = 9.74 , p < .017). For every increase in the 
number of alternatives, the total reading time on 
the source text increased by a modest 47ms. 
 
Figure 11 Correlation of alternative translation (horizontal) 
and average gazing time on ST words (vertical) during 
translation (TRA) and post-editing (PE). 
However, as depicted in Figure 11, for post-
editing there was no correlation between total 
reading time on the source text per word and the 
number of alternatives for every source word. 
4 Discussion  
The investigation reported here is not the first of 
its kind. Dragsted (2012) compared eye 
movement measures (total reading time and 
number of fixations) and pauses for words which 
were translated by 8 participants using the same 
target word with words for which the eight 
participants used different words.  
She found that the total reading time and the 
number of fixations on words with many (5-8) 
alternatives target text items was significantly 
higher than the number of fixations on words 
with only one or two different target items. She 
also found that the pauses prior to critical words 
were longer for words with many alternatives as 
compared to words with one or two alternatives.  
This seems to confirm the assumption that the 
more lexical choices a translator has to consider, 
the more effortful the processing of this item 
becomes. Campbell (2000: 38) suggests that ?the 
complexity of choices available to the translator 
to select from? can be taken as a measure of the 
effort of the related cognitive processes.  
Our analysis investigates this suggestion on a 
larger scale, involving more language pairs and 
two conditions: translation from scratch and 
post-editing. It shows similar results to those of 
Dragsted (2012), but in addition shows that 
effect of alternatives on production time per 
word was much stronger for post-editing as 
compared to translation (171ms for translation 
vs. 278ms for post-editing). This suggests that 
highly (translation) ambiguous texts should 
perhaps not be considered for post-editing. In 
(Carl and Schaeffer, 2014) we look at this effect 
in more detail by investigating the word 
translation entropy in human and machine 
produced translations and propose a translation 
ambiguity threshold that might be suitable for 
post-editing. 
The effect of translation choices on total TT 
reading time was comparable for translation and 
post-editing (153ms for translation vs. 120 for 
post-editing). For total ST reading time there was 
no effect for post-editing, while every additional 
translation choice increased the total ST reading 
time by 47ms, however modest compared to the 
effect on TT reading time. This finding suggests 
that in from scratch translation choices are 
already processed during ST reading, while 
during post-editing choices are considered 
mainly when the gaze is on the TT.  
As a second variable we investigate ST-TT 
crossing values. Higher Cross values indicate 
non-monotonous translation relations such as 
local distortions of ST-TT alignment, 
discontinuous, idiomatic or multi-word units, all 
of which require larger sequences of the source 
and/or target text to be integrated and related, 
and thus increased effort when maintaining and 
processing larger numbers of items in working 
memory. The large increases of total ST reading 
time for tokens with higher CrossT values in 
translation and post-editing suggests that 
integrating larger ST chunks is also more 
effortful during translation and post-editing. 
Similar findings are also reported by Jensen et al. 
(2010) who investigate gazing time for English-
Danish verbal translations when they switch their 
sentence position (SVO ? SOV) vs. they remain 
in both languages in the same sentence position 
(SVO ?  SVO). Our investigation generalizes 
35
these finding to different language pairs and all 
kinds of relative ST-TT distortion.  
Another observation is related to the large 
increases in total TT reading time for higher 
CrossS values, during translation and post-
editing. This observation suggests that translators 
not only read the ST to generate a TT equivalent, 
but they also check the produced TT whether it 
corresponds to the ST. As one could expect, this 
tendency is very pronounced during post-editing, 
but appears interestingly also during translation 
from scratch. The observation is in line with a 
previous assumption of Carl and Dragsted (2012: 
141) who find that source text related processes 
are ?triggered by problems associated with text 
production rather than? during source text 
reading. 
Note that for all analysis, both translation and 
post-editing, reading time increased much more 
with negative Cross values than this is the case 
for positive Cross values. This coincides with the 
finding that regressions - which negative Cross 
values reflect - are more effortful to process than 
progressions, since regressions often mirror 
misunderstanding and imply the integration of 
already parsed input text (e.g. Reichle et al 
2009). 
5 Conclusion and outlook 
There has been some discussion in the translation 
process research (TPR) literature on the 
?tendency of the translating process to proceed 
literally to a certain extent? Tirkkonen-Condit 
(2004: 183), where a deviation from the ideal 
default translation would result in higher effort. 
However, to our knowledge the literal default 
translation hypothesis has never been quantified 
and empirically assessed in a larger context. In 
this paper we bridge this gap. We provide a 
quantifiable definition of literal translation as a 
continuous concept involving alternative 
translation choices and source-target distortions, 
apply it to a collection of translation and post-
editing sessions from the TPR-DB and assess 
translation effort by measuring gazing and 
translation time. We find that gaze activity and 
production time is inversely proportional to the 
literality of the produced translations. Using 
linear regression we find in particular: 
? More translation choices lead to longer 
reading and processing time  
? Longer relative source-target language 
distortions increase gaze activity.  
? Regressions are more effortful than 
progressions 
? Translators and post-editors map not 
only the source text against the target, but 
also the target against the source text  
These findings suggest a model in which, 
paradoxically, translators already know the 
translations which they produce; they merely 
refer to the ST - and to the TT for cross-checking 
- to verify the translation hypothesis which they 
already have in mind.  
A number of issues remain open for further 
research. For instance, the impact of the target 
language and the (syntactic) similarity of the 
source and target languages. According to the 
hypothesis supported here, closely related 
languages with similar word order and similar 
conceptual repository will more likely have more 
literal translations. They will more often consist 
of monotonous one-to-one translations, 
approaching an ideal literal translation 
(Schaeffer, 2013). The more syntactic reordering 
between source and target text take place the 
more it will become non-literal. 
Another set of questions relates to whether and 
how the methods discussed here can be used to 
assess the cognitive effort for translating and/or 
post-editing entire sentences and texts and the 
impact on post-editing practice. 
Acknowledgments 
This project has received funding from the 
European Union's Seventh Framework 
Programme for research, technological 
development and demonstration under grant 
36
agreement no 287576. We are grateful to all 
contributors to the TPR database for allowing us 
the use of their data. 
References  
Campbell, Stuart. 2000. ?Choice Network Analaysis 
in Translation Research.? In Intercultural 
Faultlines. Research Models in Translation Studies 
I. Textual and Cognitive Aspects, edited by Maeve 
Olohan, 29?42. Manchester: St Jerome. 
Carl, Michael. 2012. ?The CRITT TPR-DB 1.0: A 
Database for Empirical Human Translation Process 
Research.? In Proceedings of the AMTA 2012 
Workshop on Post-Editing Technology and 
Practice (WPTP 2012), edited by Sharon O?Brien, 
Michel Simard, and Lucia Specia, 9?18. 
Stroudsburg, PA: Association for Machine 
Translation in the Americas (AMTA). 
Carl, Michael, Mercedes Garc?a Mart?nez, Bartolom? 
Mesa-Lao, Nancy Underwood. 2014. ?CFT13: A 
new resource for research into the post-editing 
process.? Proceedings of LREC 
Carl, Michael, and Barbara Dragsted. 2012. ?Inside 
the Monitor Model?: Processes of Default and 
Challenged Translation Production.? Translation: 
Computation, Corpora, Cognition 2 (1): 127?145. 
Carl, Michael, and Moritz Schaeffer (2014) ?Word 
Transition Entropy as an Indicator for Expected 
Machine Translation Quality?, Proceedings of 
LREC 
Dragsted, Barbara. 2012. ?Indicators of Difficulty in 
Translation ? Correlating Product and Process 
Data.? Across Languages and Cultures 13 (1) 
(June 1): 81?98. doi:10.1556/Acr.13.2012.1.5. 
http://www.akademiai.com/openurl.asp?genre=arti
cle&id=doi:10.1556/Acr.13.2012.1.5. 
Ivir, Vladimir. 1981. ?Formal Correspondence Vs. 
Translation Equivalence Revisited.? Poetics Today 
2 (4): 51?59. 
Jensen, Kristian T.H., Annette C. Sj?rup, and Laura 
W. Balling. 2010. ?Effects of L1 Syntax on L2 
Translation.? In Methodology, Technology and 
Innovation in Translation Process Research: A 
Tribute to Arnt Lykke Jakobsen, edited by F. Alves, 
S.   pferich, and Mees Inger M., 319?336. 
Copenhagen: Samfundslitteratur. 
Malmkj?r, Kirsten. 2011a. ?Linguistic Approaches to 
Translation.? In Oxford Handbook of Translation 
Studies, edited by Kirsten Malmkj?r and Kevin 
Windle, 57?70. Oxford: OUP. 
Malmkj?r, Kirsten. 2011b. ?Translation Universals.? 
In The Oxford Handbook of Translation Studies, 
edited by Kirsten Malmkj?r and Kevin Windle, 
83?94. Oxford: Oxford University Press. 
Reichle, Erik D., Tessa Warren, Kerry McConnell. 
(2009). ?Using E-Z Reader to model the effects of 
higher level language processing on eye 
movements during reading.? Psychonomic Bulletin 
& Review, 16(1), 1?21. 
Schaeffer, Moritz. 2013. The Ideal Literal Translation 
Hypothesis: The Role of Shared Representations 
During Translation. PhD Thesis. University of 
Leicester.  
Tirkkonen-Condit, Sonja. 2004. ?Unique Items: Over- 
or Under-Represented in Translated Language?? In 
Translation Universals: Do They Exist? 
Amsterdam and Philadelphia: John Benjamins. 
Tirkkonen-Condit, Sonja. 2005. ?The Monitor Model 
Revisited: Evidence from Process Research.? 
Meta: Translators? Journal 50 (2): 405?414. 
Toury, Gideon. 1995. Descriptive Translation Studies 
and Beyond. Benjamins Translation Library V4. 
Vol. 75. Amsterdam and Philadelphia: John 
Benjamins. 
 
37
