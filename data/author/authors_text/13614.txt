Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pp. 915?932,
Prague, June 2007. c?2007 Association for Computational Linguistics
The CoNLL 2007 Shared Task on Dependency Parsing
Joakim Nivre?? Johan Hall? Sandra Ku?bler? Ryan McDonald??
Jens Nilsson? Sebastian Riedel?? Deniz Yuret??
?Va?xjo? University, School of Mathematics and Systems Engineering, first.last@vxu.se
?Uppsala University, Dept. of Linguistics and Philology, joakim.nivre@lingfil.uu.se
?Indiana University, Department of Linguistics, skuebler@indiana.edu
??Google Inc., ryanmcd@google.com
??University of Edinburgh, School of Informatics, S.R.Riedel@sms.ed.ac.uk
??Koc? University, Dept. of Computer Engineering, dyuret@ku.edu.tr
Abstract
The Conference on Computational Natural
Language Learning features a shared task, in
which participants train and test their learn-
ing systems on the same data sets. In 2007,
as in 2006, the shared task has been devoted
to dependency parsing, this year with both a
multilingual track and a domain adaptation
track. In this paper, we define the tasks of the
different tracks and describe how the data
sets were created from existing treebanks for
ten languages. In addition, we characterize
the different approaches of the participating
systems, report the test results, and provide
a first analysis of these results.
1 Introduction
Previous shared tasks of the Conference on Compu-
tational Natural Language Learning (CoNLL) have
been devoted to chunking (1999, 2000), clause iden-
tification (2001), named entity recognition (2002,
2003), and semantic role labeling (2004, 2005). In
2006 the shared task was multilingual dependency
parsing, where participants had to train a single
parser on data from thirteen different languages,
which enabled a comparison not only of parsing and
learning methods, but also of the performance that
can be achieved for different languages (Buchholz
and Marsi, 2006).
In dependency-based syntactic parsing, the task is
to derive a syntactic structure for an input sentence
by identifying the syntactic head of each word in the
sentence. This defines a dependency graph, where
the nodes are the words of the input sentence and the
arcs are the binary relations from head to dependent.
Often, but not always, it is assumed that all words
except one have a syntactic head, which means that
the graph will be a tree with the single independent
word as the root. In labeled dependency parsing, we
additionally require the parser to assign a specific
type (or label) to each dependency relation holding
between a head word and a dependent word.
In this year?s shared task, we continue to explore
data-driven methods for multilingual dependency
parsing, but we add a new dimension by also intro-
ducing the problem of domain adaptation. The way
this was done was by having two separate tracks: a
multilingual track using essentially the same setup
as last year, but with partly different languages, and
a domain adaptation track, where the task was to use
machine learning to adapt a parser for a single lan-
guage to a new domain. In total, test results were
submitted for twenty-three systems in the multilin-
gual track, and ten systems in the domain adaptation
track (six of which also participated in the multilin-
gual track). Not everyone submitted papers describ-
ing their system, and some papers describe more
than one system (or the same system in both tracks),
which explains why there are only (!) twenty-one
papers in the proceedings.
In this paper, we provide task definitions for the
two tracks (section 2), describe data sets extracted
from available treebanks (section 3), report results
for all systems in both tracks (section 4), give an
overview of approaches used (section 5), provide a
first analysis of the results (section 6), and conclude
with some future directions (section 7).
915
2 Task Definition
In this section, we provide the task definitions that
were used in the two tracks of the CoNLL 2007
Shard Task, the multilingual track and the domain
adaptation track, together with some background
and motivation for the design choices made. First
of all, we give a brief description of the data format
and evaluation metrics, which were common to the
two tracks.
2.1 Data Format and Evaluation Metrics
The data sets derived from the original treebanks
(section 3) were in the same column-based format
as for the 2006 shared task (Buchholz and Marsi,
2006). In this format, sentences are separated by a
blank line; a sentence consists of one or more to-
kens, each one starting on a new line; and a token
consists of the following ten fields, separated by a
single tab character:
1. ID: Token counter, starting at 1 for each new
sentence.
2. FORM: Word form or punctuation symbol.
3. LEMMA: Lemma or stem of word form, or an
underscore if not available.
4. CPOSTAG: Coarse-grained part-of-speech tag,
where the tagset depends on the language.
5. POSTAG: Fine-grained part-of-speech tag,
where the tagset depends on the language, or
identical to the coarse-grained part-of-speech
tag if not available.
6. FEATS: Unordered set of syntactic and/or mor-
phological features (depending on the particu-
lar language), separated by a vertical bar (|), or
an underscore if not available.
7. HEAD: Head of the current token, which is
either a value of ID or zero (0). Note that,
depending on the original treebank annotation,
there may be multiple tokens with HEAD=0.
8. DEPREL: Dependency relation to the HEAD.
The set of dependency relations depends on
the particular language. Note that, depending
on the original treebank annotation, the depen-
dency relation when HEAD=0 may be mean-
ingful or simply ROOT.
9. PHEAD: Projective head of current token,
which is either a value of ID or zero (0), or an
underscore if not available.
10. PDEPREL: Dependency relation to the
PHEAD, or an underscore if not available.
The PHEAD and PDEPREL were not used at all
in this year?s data sets (i.e., they always contained
underscores) but were maintained for compatibility
with last year?s data sets. This means that, in prac-
tice, the first six columns can be considered as input
to the parser, while the HEAD and DEPREL fields
are the output to be produced by the parser. Labeled
training sets contained all ten columns; blind test
sets only contained the first six columns; and gold
standard test sets (released only after the end of the
test period) again contained all ten columns. All data
files were encoded in UTF-8.
The official evaluation metric in both tracks was
the labeled attachment score (LAS), i.e., the per-
centage of tokens for which a system has predicted
the correct HEAD and DEPREL, but results were
also reported for unlabeled attachment score (UAS),
i.e., the percentage of tokens with correct HEAD,
and the label accuracy (LA), i.e., the percentage of
tokens with correct DEPREL. One important differ-
ence compared to the 2006 shared task is that all to-
kens were counted as ?scoring tokens?, including in
particular all punctuation tokens. The official eval-
uation script, eval07.pl, is available from the shared
task website.1
2.2 Multilingual Track
The multilingual track of the shared task was orga-
nized in the same way as the 2006 task, with an-
notated training and test data from a wide range of
languages to be processed with one and the same
parsing system. This system must therefore be able
to learn from training data, to generalize to unseen
test data, and to handle multiple languages, possi-
bly by adjusting a number of hyper-parameters. Par-
ticipants in the multilingual track were expected to
submit parsing results for all languages involved.
1http://depparse.uvt.nl/depparse-wiki/SoftwarePage
916
One of the claimed advantages of dependency
parsing, as opposed to parsing based on constituent
analysis, is that it extends naturally to languages
with free or flexible word order. This explains the
interest in recent years for multilingual evaluation
of dependency parsers. Even before the 2006 shared
task, the parsers of Collins (1997) and Charniak
(2000), originally developed for English, had been
adapted for dependency parsing of Czech, and the
parsing methodology proposed by Kudo and Mat-
sumoto (2002) and Yamada and Matsumoto (2003)
had been evaluated on both Japanese and English.
The parser of McDonald and Pereira (2006) had
been applied to English, Czech and Danish, and the
parser of Nivre et al (2007) to ten different lan-
guages. But by far the largest evaluation of mul-
tilingual dependency parsing systems so far was the
2006 shared task, where nineteen systems were eval-
uated on data from thirteen languages (Buchholz and
Marsi, 2006).
One of the conclusions from the 2006 shared task
was that parsing accuracy differed greatly between
languages and that a deeper analysis of the factors
involved in this variation was an important problem
for future research. In order to provide an extended
empirical foundation for such research, we tried to
select the languages and data sets for this year?s task
based on the following desiderata:
? The selection of languages should be typolog-
ically varied and include both new languages
and old languages (compared to 2006).
? The creation of the data sets should involve as
little conversion as possible from the original
treebank annotation, meaning that preference
should be given to treebanks with dependency
annotation.
? The training data sets should include at least
50,000 tokens and at most 500,000 tokens.2
The final selection included data from Arabic,
Basque, Catalan, Chinese, Czech, English, Greek,
Hungarian, Italian, and Turkish. The treebanks from
2The reason for having an upper bound on the training set
size was the fact that, in 2006, some participants could not train
on all the data for some languages because of time limitations.
Similar considerations also led to the decision to have a smaller
number of languages this year (ten, as opposed to thirteen).
which the data sets were extracted are described in
section 3.
2.3 Domain Adaptation Track
One well known characteristic of data-driven pars-
ing systems is that they typically perform much
worse on data that does not come from the train-
ing domain (Gildea, 2001). Due to the large over-
head in annotating text with deep syntactic parse
trees, the need to adapt parsers from domains with
plentiful resources (e.g., news) to domains with lit-
tle resources is an important problem. This prob-
lem is commonly referred to as domain adaptation,
where the goal is to adapt annotated resources from
a source domain to a target domain of interest.
Almost all prior work on domain adaptation as-
sumes one of two scenarios. In the first scenario,
there are limited annotated resources available in the
target domain, and many studies have shown that
this may lead to substantial improvements. This in-
cludes the work of Roark and Bacchiani (2003), Flo-
rian et al (2004), Chelba and Acero (2004), Daume?
and Marcu (2006), and Titov and Henderson (2006).
Of these, Roark and Bacchiani (2003) and Titov and
Henderson (2006) deal specifically with syntactic
parsing. The second scenario assumes that there are
no annotated resources in the target domain. This is
a more realistic situation and is considerably more
difficult. Recent work by McClosky et al (2006)
and Blitzer et al (2006) have shown that the exis-
tence of a large unlabeled corpus in the new domain
can be leveraged in adaptation. For this shared-task,
we are assuming the latter setting ? no annotated re-
sources in the target domain.
Obtaining adequate annotated syntactic resources
for multiple languages is already a challenging prob-
lem, which is only exacerbated when these resources
must be drawn from multiple and diverse domains.
As a result, the only language that could be feasibly
tested in the domain adaptation track was English.
The setup for the domain adaptation track was as
follows. Participants were provided with a large an-
notated corpus from the source domain, in this case
sentences from the Wall Street Journal. Participants
were also provided with data from three different
target domains: biomedical abstracts (development
data), chemical abstracts (test data 1), and parent-
child dialogues (test data 2). Additionally, a large
917
unlabeled corpus for each data set (training, devel-
opment, test) was provided. The goal of the task was
to use the annotated source data, plus any unlabeled
data, to produce a parser that is accurate for each of
the test sets from the target domains.3
Participants could submit systems in either the
?open? or ?closed? class (or both). The closed class
requires a system to use only those resources pro-
vided as part of the shared task. The open class al-
lows a system to use additional resources provided
those resources are not drawn from the same domain
as the development or test sets. An example might
be a part-of-speech tagger trained on the entire Penn
Treebank and not just the subset provided as train-
ing data, or a parser that has been hand-crafted or
trained on a different training set.
3 Treebanks
In this section, we describe the treebanks used in the
shared task and give relevant information about the
data sets created from them.
3.1 Multilingual Track
Arabic The analytical syntactic annotation
of the Prague Arabic Dependency Treebank
(PADT) (Hajic? et al, 2004) can be considered a
pure dependency annotation. The conversion, done
by Otakar Smrz, from the original format to the
column-based format described in section 2.1 was
therefore relatively straightforward, although not all
the information in the original annotation could be
transfered to the new format. PADT was one of the
treebanks used in the 2006 shared task but then only
contained about 54,000 tokens. Since then, the size
of the treebank has more than doubled, with around
112,000 tokens. In addition, the morphological
annotation has been made more informative. It
is also worth noting that the parsing units in this
treebank are in many cases larger than conventional
sentences, which partly explains the high average
number of tokens per ?sentence? (Buchholz and
Marsi, 2006).
3Note that annotated development data for the target domain
was only provided for the development domain, biomedical ab-
stracts. For the two test domains, chemical abstracts and parent-
child dialogues, the only annotated data sets were the gold stan-
dard test sets, released only after test runs had been submitted.
Basque For Basque, we used the 3LB Basque
treebank (Aduriz et al, 2003). At present, the tree-
bank consists of approximately 3,700 sentences, 334
of which were used as test data. The treebank com-
prises literary and newspaper texts. It is annotated
in a dependency format and was converted to the
CoNLL format by a team led by Koldo Gojenola.
Catalan The Catalan section of the CESS-ECE
Syntactically and Semantically Annotated Cor-
pora (Mart?? et al, 2007) is annotated with, among
other things, constituent structure and grammatical
functions. A head percolation table was used for
automatically converting the constituent trees into
dependency trees. The original data only contains
functions related to the verb, and a function table
was used for deriving the remaining syntactic func-
tions. The conversion was performed by a team led
by Llu??s Ma`rquez and Anto`nia Mart??.
Chinese The Chinese data are taken from the
Sinica treebank (Chen et al, 2003), which con-
tains both syntactic functions and semantic func-
tions. The syntactic head was used in the conversion
to the CoNLL format, carried out by Yu-Ming Hsieh
and the organizers of the 2006 shared task, and the
syntactic functions were used wherever it was pos-
sible. The training data used is basically the same
as for the 2006 shared task, except for a few correc-
tions, but the test data is new for this year?s shared
task. It is worth noting that the parsing units in this
treebank are sometimes smaller than conventional
sentence units, which partly explains the low aver-
age number of tokens per ?sentence? (Buchholz and
Marsi, 2006).
Czech The analytical syntactic annotation of the
Prague Dependency Treebank (PDT) (Bo?hmova? et
al., 2003) is a pure dependency annotation, just as
for PADT. It was also used in the shared task 2006,
but there are two important changes compared to
last year. First, version 2.0 of PDT was used in-
stead of version 1.0, and a conversion script was
created by Zdenek Zabokrtsky, using the new XML-
based format of PDT 2.0. Secondly, due to the upper
bound on training set size, only sections 1?3 of PDT
constitute the training data, which amounts to some
450,000 tokens. The test data is a small subset of the
development test set of PDT.
918
English For English we used the Wall Street Jour-
nal section of the Penn Treebank (Marcus et al,
1993). In particular, we used sections 2-11 for train-
ing and a subset of section 23 for testing. As a pre-
processing stage we removed many functions tags
from the non-terminals in the phrase structure repre-
sentation to make the representations more uniform
with out-of-domain test sets for the domain adapta-
tion track (see section 3.2). The resulting data set
was then converted to dependency structures using
the procedure described in Johansson and Nugues
(2007a). This work was done by Ryan McDonald.
Greek The Greek Dependency Treebank
(GDT) (Prokopidis et al, 2005) adopts a de-
pendency structure annotation very similar to those
of PDT and PADT, which means that the conversion
by Prokopis Prokopidis was relatively straightfor-
ward. GDT is one of the smallest treebanks in
this year?s shared task (about 65,000 tokens) and
contains sentences of Modern Greek. Just like PDT
and PADT, the treebank contains more than one
level of annotation, but we only used the analytical
level of GDT.
Hungarian For the Hungarian data, the Szeged
treebank (Csendes et al, 2005) was used. The tree-
bank is based on texts from six different genres,
ranging from legal newspaper texts to fiction. The
original annotation scheme is constituent-based, fol-
lowing generative principles. It was converted into
dependencies by Zo?ltan Alexin based on heuristics.
Italian The data set used for Italian is a subset
of the balanced section of the Italian Syntactic-
Semantic Treebank (ISST) (Montemagni et al,
2003) and consists of texts from the newspaper Cor-
riere della Sera and from periodicals. A team led
by Giuseppe Attardi, Simonetta Montemagni, and
Maria Simi converted the annotation to the CoNLL
format, using information from two different anno-
tation levels, the constituent structure level and the
dependency structure level.
Turkish For Turkish we used the METU-Sabanc?
Turkish Treebank (Oflazer et al, 2003), which was
also used in the 2006 shared task. A new test set of
about 9,000 tokens was provided by Gu?ls?en Eryig?it
(Eryig?it, 2007), who also handled the conversion to
the CoNLL format, which means that we could use
all the approximately 65,000 tokens of the original
treebank for training. The rich morphology of Turk-
ish requires the basic tokens in parsing to be inflec-
tional groups (IGs) rather than words. IGs of a single
word are connected to each other deterministically
using dependency links labeled DERIV, referred to
as word-internal dependencies in the following, and
the FORM and the LEMMA fields may be empty
(they contain underscore characters in the data files).
Sentences do not necessarily have a unique root;
most internal punctuation and a few foreign words
also have HEAD=0.
3.2 Domain Adaptation Track
As mentioned previously, the source data is drawn
from a corpus of news, specifically the Wall Street
Journal section of the Penn Treebank (Marcus et al,
1993). This data set is identical to the English train-
ing set from the multilingual track (see section 3.1).
For the target domains we used three different
labeled data sets. The first two were annotated
as part of the PennBioIE project (Kulick et al,
2004) and consist of sentences drawn from either
biomedical or chemical research abstracts. Like the
source WSJ corpus, this data is annotated using the
Penn Treebank phrase structure scheme. To con-
vert these sets to dependency structures we used the
same procedure as before (Johansson and Nugues,
2007a). Additional care was taken to remove sen-
tences that contained non-WSJ part-of-speech tags
or non-terminals (e.g., HYPH part-of-speech tag in-
dicating a hyphen). Furthermore, the annotation
scheme for gaps and traces was made consistent with
the Penn Treebank wherever possible. As already
mentioned, the biomedical data set was distributed
as a development set for the training phase, while
the chemical data set was only used for final testing.
The third target data set was taken from the
CHILDES database (MacWhinney, 2000), in partic-
ular the EVE corpus (Brown, 1973), which has been
annotated with dependency structures. Unfortu-
nately the dependency labels of the CHILDES data
were inconsistent with those of the WSJ, biomedi-
cal and chemical data sets, and we therefore opted
to only evaluate unlabeled accuracy for this data
set. Furthermore, there was an inconsistency in how
main and auxiliary verbs were annotated for this data
set relative to others. As a result of this, submitting
919
Multilingual Domain adaptation
Ar Ba Ca Ch Cz En Gr Hu It Tu PCHEM CHILDES
Language family Sem. Isol. Rom. Sin. Sla. Ger. Hel. F.-U. Rom. Tur. Ger.
Annotation d d c+f c+f d c+f d c+f c+f d c+f d
Training data Development data
Tokens (k) 112 51 431 337 432 447 65 132 71 65 5
Sentences (k) 2.9 3.2 15.0 57.0 25.4 18.6 2.7 6.0 3.1 5.6 0.2
Tokens/sentence 38.3 15.8 28.8 5.9 17.0 24.0 24.2 21.8 22.9 11.6 25.1
LEMMA Yes Yes Yes No Yes No Yes Yes Yes Yes No
No. CPOSTAG 15 25 17 13 12 31 18 16 14 14 25
No. POSTAG 21 64 54 294 59 45 38 43 28 31 37
No. FEATS 21 359 33 0 71 0 31 50 21 78 0
No. DEPREL 29 35 42 69 46 20 46 49 22 25 18
No. DEPREL H=0 18 17 1 1 8 1 22 1 1 1 1
% HEAD=0 8.7 9.7 3.5 16.9 11.6 4.2 8.3 4.6 5.4 12.8 4.0
% HEAD left 79.2 44.5 60.0 24.7 46.9 49.0 44.8 27.4 65.0 3.8 50.0
% HEAD right 12.1 45.8 36.5 58.4 41.5 46.9 46.9 68.0 29.6 83.4 46.0
HEAD=0/sentence 3.3 1.5 1.0 1.0 2.0 1.0 2.0 1.0 1.2 1.5 1.0
% Non-proj. arcs 0.4 2.9 0.1 0.0 1.9 0.3 1.1 2.9 0.5 5.5 0.4
% Non-proj. sent. 10.1 26.2 2.9 0.0 23.2 6.7 20.3 26.4 7.4 33.3 8.0
Punc. attached S S A S S A S A A S A
DEPRELS for punc. 10 13 6 29 16 13 15 1 10 12 8
Test data PCHEM CHILDES
Tokens 5124 5390 5016 5161 4724 5003 4804 7344 5096 4513 5001 4999
Sentences 131 334 167 690 286 214 197 390 249 300 195 666
Tokens/sentence 39.1 16.1 30.0 7.5 16.5 23.4 24.4 18.8 20.5 15.0 25.6 12.9
% New words 12.44 24.98 4.35 9.70 12.58 3.13 12.43 26.10 15.07 36.29 31.33 6.10
% New lemmas 2.82 11.13 3.36 n/a 5.28 n/a 5.82 14.80 8.24 9.95 n/a n/a
Table 1: Characteristics of the data sets for the 10 languages of the multilingual track and the development
set and the two test sets of the domain adaptation track.
920
results for the CHILDES data was considered op-
tional. Like the chemical data set, this data set was
only used for final testing.
Finally, a large corpus of unlabeled in-domain
data was provided for each data set and made avail-
able for training. This data was drawn from theWSJ,
PubMed.com (specific to biomedical and chemical
research literature), and the CHILDES data base.
The data was tokenized to be as consistent as pos-
sible with the WSJ training set.
3.3 Overview
Table 1 describes the characteristics of the data sets.
For the multilingual track, we provide statistics over
the training and test sets; for the domain adaptation
track, the statistics were extracted from the develop-
ment set. Following last year?s shared task practice
(Buchholz and Marsi, 2006), we use the following
definition of projectivity: An arc (i, j) is projective
iff all nodes occurring between i and j are dominated
by i (where dominates is the transitive closure of the
arc relation).
In the table, the languages are abbreviated to their
first two letters. Language families are: Semitic,
Isolate, Romance, Sino-Tibetan, Slavic, Germanic,
Hellenic, Finno-Ugric, and Turkic. The type of the
original annotation is either constituents plus (some)
functions (c+f) or dependencies (d). For the train-
ing data, the number of words and sentences are
given in multiples of thousands, and the average
length of a sentence in words (including punctua-
tion tokens). The following rows contain informa-
tion about whether lemmas are available, the num-
ber of coarse- and fine-grained part-of-speech tags,
the number of feature components, and the number
of dependency labels. Then information is given on
how many different dependency labels can co-occur
with HEAD=0, the percentage of HEAD=0 depen-
dencies, and the percentage of heads preceding (left)
or succeeding (right) a token (giving an indication of
whether a language is predominantly head-initial or
head-final). This is followed by the average number
of HEAD=0 dependencies per sentence and the per-
centage of non-projective arcs and sentences. The
last two rows show whether punctuation tokens are
attached as dependents of other tokens (A=Always,
S=Sometimes) and specify the number of depen-
dency labels that exist for punctuation tokens. Note
that punctuation is defined as any token belonging to
the UTF-8 category of punctuation. This means, for
example, that any token having an underscore in the
FORM field (which happens for word-internal IGs
in Turkish) is also counted as punctuation here.
For the test sets, the number of words and sen-
tences as well as the ratio of words per sentence are
listed, followed by the percentage of new words and
lemmas (if applicable). For the domain adaptation
sets, the percentage of new words is computed with
regard to the training set (Penn Treebank).
4 Submissions and Results
As already stated in the introduction, test runs were
submitted for twenty-three systems in the multilin-
gual track, and ten systems in the domain adaptation
track (six of which also participated in the multilin-
gual track). In the result tables below, systems are
identified by the last name of the teammember listed
first when test runs were uploaded for evaluation. In
general, this name is also the first author of a paper
describing the system in the proceedings, but there
are a few exceptions and complications. First of all,
for four out of twenty-seven systems, no paper was
submitted to the proceedings. This is the case for the
systems of Jia, Maes et al, Nash, and Zeman, which
is indicated by the fact that these names appear in
italics in all result tables. Secondly, two teams sub-
mitted two systems each, which are described in a
single paper by each team. Thus, the systems called
?Nilsson? and ?Hall, J.? are both described in Hall et
al. (2007a), while the systems called ?Duan (1)? and
?Duan (2)? are both described in Duan et al (2007).
Finally, please pay attention to the fact that there
are two teams, where the first author?s last name is
Hall. Therefore, we use ?Hall, J.? and ?Hall, K.?,
to disambiguate between the teams involving Johan
Hall (Hall et al, 2007a) and Keith Hall (Hall et al,
2007b), respectively.
Tables 2 and 3 give the scores for the multilingual
track in the CoNLL 2007 shared task. The Average
column contains the average score for all ten lan-
guages, which determines the ranking in this track.
Table 4 presents the results for the domain adapta-
tion track, where the ranking is determined based on
the PCHEM results only, since the CHILDES data
set was optional. Note also that there are no labeled
921
Team Average Arabic Basque Catalan Chinese Czech English Greek Hungarian Italian Turkish
Nilsson 80.32(1) 76.52(1) 76.94(1) 88.70(1) 75.82(15) 77.98(3) 88.11(5) 74.65(2) 80.27(1) 84.40(1) 79.79(2)
Nakagawa 80.29(2) 75.08(2) 72.56(7) 87.90(3) 83.84(2) 80.19(1) 88.41(3) 76.31(1) 76.74(8) 83.61(3) 78.22(5)
Titov 79.90(3) 74.12(6) 75.49(3) 87.40(6) 82.14(7) 77.94(4) 88.39(4) 73.52(10) 77.94(4) 82.26(6) 79.81(1)
Sagae 79.90(4) 74.71(4) 74.64(6) 88.16(2) 84.69(1) 74.83(8) 89.01(2) 73.58(8) 79.53(2) 83.91(2) 75.91(10)
Hall, J. 79.80(5)* 74.75(3) 74.99(5) 87.74(4) 83.51(3) 77.22(6) 85.81(12) 74.21(6) 78.09(3) 82.48(5) 79.24(3)
Carreras 79.09(6)* 70.20(11) 75.75(2) 87.60(5) 80.86(10) 78.60(2) 89.61(1) 73.56(9) 75.42(9) 83.46(4) 75.85(11)
Attardi 78.27(7) 72.66(8) 69.48(12) 86.86(7) 81.50(8) 77.37(5) 85.85(10) 73.92(7) 76.81(7) 81.34(8) 76.87(7)
Chen 78.06(8) 74.65(5) 72.39(8) 86.66(8) 81.24(9) 73.69(10) 83.81(13) 74.42(3) 75.34(10) 82.04(7) 76.31(9)
Duan (1) 77.70(9)* 69.91(13) 71.26(9) 84.95(10) 82.58(6) 75.34(7) 85.83(11) 74.29(4) 77.06(5) 80.75(9) 75.03(12)
Hall, K. 76.91(10)* 73.40(7) 69.81(11) 82.38(14) 82.77(4) 72.27(12) 81.93(15) 74.21(5) 74.20(11) 80.69(10) 77.42(6)
Schiehlen 76.18(11) 70.08(12) 66.77(14) 85.75(9) 80.04(11) 73.86(9) 86.21(9) 72.29(12) 73.90(12) 80.46(11) 72.48(15)
Johansson 75.78(12)* 71.76(9) 75.08(4) 83.33(12) 76.30(14) 70.98(13) 80.29(17) 72.77(11) 71.31(13) 77.55(14) 78.46(4)
Mannem 74.54(13)* 71.55(10) 65.64(15) 84.47(11) 73.76(17) 70.68(14) 81.55(16) 71.69(13) 70.94(14) 78.67(13) 76.42(8)
Wu 73.02(14)* 66.16(14) 70.71(10) 81.44(15) 74.69(16) 66.72(16) 79.49(18) 70.63(14) 69.08(15) 78.79(12) 72.52(14)
Nguyen 72.53(15)* 63.58(16) 58.18(17) 83.23(13) 79.77(12) 72.54(11) 86.73(6) 70.42(15) 68.12(17) 75.06(16) 67.63(17)
Maes 70.66(16)* 65.12(15) 69.05(13) 79.21(16) 70.97(18) 67.38(15) 69.68(21) 68.59(16) 68.93(16) 73.63(18) 74.03(13)
Canisius 66.99(17)* 59.13(18) 63.17(16) 75.44(17) 70.45(19) 56.14(17) 77.27(19) 60.35(18) 64.31(19) 75.57(15) 68.09(16)
Jia 63.00(18)* 63.37(17) 57.61(18) 23.35(20) 76.36(13) 54.95(18) 82.93(14) 65.45(17) 66.61(18) 74.65(17) 64.68(18)
Zeman 54.87(19) 46.06(20) 50.61(20) 62.94(19) 54.49(20) 50.21(20) 53.59(22) 55.29(19) 55.24(20) 62.13(19) 58.10(19)
Marinov 54.55(20)* 54.00(19) 51.24(19) 69.42(18) 49.87(21) 53.47(19) 52.11(23) 54.33(20) 44.47(21) 59.75(20) 56.88(20)
Duan (2) 24.62(21)* 82.64(5) 86.69(7) 76.89(6)
Nash 8.65(22)* 86.49(8)
Shimizu 7.20(23) 72.02(20)
Table 2: Labeled attachment score (LAS) for the multilingual track in the CoNLL 2007 shared task. Teams
are denoted by the last name of their first member, with italics indicating that there is no corresponding
paper in the proceedings. The number in parentheses next to each score gives the rank. A star next to a score
in the Average column indicates a statistically significant difference with the next lower rank.
Team Average Arabic Basque Catalan Chinese Czech English Greek Hungarian Italian Turkish
Nakagawa 86.55(1)* 86.09(1) 81.04(5) 92.86(4) 88.88(2) 86.28(1) 90.13(2) 84.08(1) 82.49(3) 87.91(1) 85.77(3)
Nilsson 85.71(2) 85.81(2) 82.84(1) 93.12(3) 84.52(12) 83.59(4) 88.93(5) 81.22(4) 83.55(1) 87.77(2) 85.77(2)
Titov 85.62(3) 83.18(7) 81.93(2) 93.40(1) 87.91(4) 84.19(3) 89.73(4) 81.20(5) 82.18(4) 86.26(6) 86.22(1)
Sagae 85.29(4)* 84.04(4) 81.19(3) 93.34(2) 88.94(1) 81.27(8) 89.87(3) 80.37(11) 83.51(2) 87.68(3) 82.72(9)
Carreras 84.79(5) 81.48(10) 81.11(4) 92.46(5) 86.20(9) 85.16(2) 90.63(1) 81.37(3) 79.92(9) 87.19(4) 82.41(10)
Hall, J. 84.74(6)* 84.21(3) 80.61(6) 92.20(6) 87.60(5) 82.35(6) 86.77(12) 80.66(9) 81.71(6) 86.26(5) 85.04(5)
Attardi 83.96(7)* 82.53(8) 76.88(11) 91.41(7) 86.73(8) 83.40(5) 86.99(10) 80.75(8) 81.81(5) 85.54(8) 83.56(7)
Chen 83.22(8) 83.49(5) 78.65(8) 90.87(8) 85.91(10) 80.14(11) 84.91(13) 81.16(6) 79.25(11) 85.91(7) 81.92(12)
Hall, K. 83.08(9) 83.45(6) 78.55(9) 87.80(15) 87.91(3) 78.47(12) 83.21(15) 82.04(2) 79.34(10) 84.81(9) 85.18(4)
Duan (1) 82.77(10) 79.04(13) 77.59(10) 89.71(12) 86.88(7) 80.82(10) 86.97(11) 80.77(7) 80.66(7) 84.20(11) 81.03(13)
Schiehlen 82.42(11)* 81.07(11) 73.30(14) 90.79(10) 85.45(11) 81.73(7) 88.91(6) 80.47(10) 78.61(12) 84.54(10) 79.33(15)
Johansson 81.13(12)* 80.91(12) 80.43(7) 88.34(13) 81.30(15) 77.39(13) 81.43(18) 79.58(12) 75.53(15) 81.55(15) 84.80(6)
Mannem 80.30(13) 81.56(9) 72.88(15) 89.81(11) 78.84(17) 77.20(14) 82.81(16) 78.89(13) 75.39(16) 82.91(12) 82.74(8)
Nguyen 80.00(14)* 73.46(18) 69.15(18) 88.12(14) 84.05(13) 80.91(9) 88.01(7) 77.56(15) 78.13(13) 80.40(16) 80.19(14)
Jia 78.46(15) 74.20(17) 70.24(16) 90.83(9) 83.39(14) 70.41(18) 84.37(14) 75.65(16) 77.19(14) 82.36(14) 75.96(17)
Wu 78.44(16)* 77.05(14) 75.77(12) 85.85(16) 79.71(16) 73.07(16) 81.69(17) 78.12(14) 72.39(18) 82.57(13) 78.15(16)
Maes 76.60(17)* 75.47(16) 75.27(13) 84.35(17) 76.57(18) 74.03(15) 71.62(21) 75.19(17) 72.93(17) 78.32(18) 82.21(11)
Canisius 74.83(18)* 76.89(15) 70.17(17) 81.64(18) 74.81(19) 72.12(17) 78.23(19) 72.46(18) 67.80(19) 79.08(17) 75.14(18)
Zeman 62.02(19)* 58.55(20) 57.42(20) 68.50(20) 62.93(20) 59.19(20) 58.33(22) 62.89(19) 59.78(20) 68.27(19) 64.30(19)
Marinov 60.83(20)* 64.27(19) 58.55(19) 74.22(19) 56.09(21) 59.57(19) 54.33(23) 61.18(20) 50.39(21) 65.52(20) 64.13(20)
Duan (2) 25.53(21)* 86.94(6) 87.87(8) 80.53(8)
Nash 8.77(22)* 87.71(9)
Shimizu 7.79(23) 77.91(20)
Table 3: Unlabeled attachment scores (UAS) for the multilingual track in the CoNLL 2007 shared task.
Teams are denoted by the last name of their first member, with italics indicating that there is no correspond-
ing paper in the proceedings. The number in parentheses next to each score gives the rank. A star next to a
score in the Average column indicates a statistically significant difference with the next lower rank.
922
LAS UAS
Team PCHEM-c PCHEM-o PCHEM-c PCHEM-o CHILDES-c CHILDES-o
Sagae 81.06(1) 83.42(1)
Attardi 80.40(2) 83.08(3) 58.67(3)
Dredze 80.22(3) 83.38(2) 61.37(1)
Nguyen 79.50(4)* 82.04(4)*
Jia 76.48(5)* 78.92(5)* 57.43(5)
Bick 71.81(6)* 78.48(1)* 74.71(6)* 81.62(1)* 58.07(4) 62.49(1)
Shimizu 64.15(7)* 63.49(2) 71.25(7)* 70.01(2)*
Zeman 50.61(8) 54.57(8) 58.89(2)
Schneider 63.01(3)* 66.53(3)* 60.27(2)
Watson 55.47(4) 62.79(4) 45.61(3)
Wu 52.89(6)
Table 4: Labeled (LAS) and unlabeled (UAS) attachment scores for the closed (-c) and open (-o) classes of
the domain adaptation track in the CoNLL 2007 shared task. Teams are denoted by the last name of their
first member, with italics indicating that there is no corresponding paper in the proceedings. The number
in parentheses next to each score gives the rank. A star next to a score in the PCHEM columns indicates a
statistically significant difference with the next lower rank.
attachment scores for the CHILDES data set, for rea-
sons explained in section 3.2. The number in paren-
theses next to each score gives the rank. A star next
to a score indicates that the difference with the next
lower rank is significant at the 5% level using a z-
test for proportions. A more complete presentation
of the results, including the significance results for
all the tasks and their p-values, can be found on the
shared task website.4
Looking first at the results in the multilingual
track, we note that there are a number of systems
performing at almost the same level at the top of the
ranking. For the average labeled attachment score,
the difference between the top score (Nilsson) and
the fifth score (Hall, J.) is no more than half a per-
centage point, and there are generally very few sig-
nificant differences among the five or six best sys-
tems, regardless of whether we consider labeled or
unlabeled attachment score. For the closed class of
the domain adaptation track, we see a very similar
pattern, with the top system (Sagae) being followed
very closely by two other systems. For the open
class, the results are more spread out, but then there
are very few results in this class. It is also worth not-
ing that the top scores in the closed class, somewhat
unexpectedly, are higher than the top scores in the
4http://nextens.uvt.nl/depparse-wiki/AllScores
open class. But before we proceed to a more detailed
analysis of the results (section 6), we will make an
attempt to characterize the approaches represented
by the different systems.
5 Approaches
In this section we give an overview of the models,
inference methods, and learning methods used in the
participating systems. For obvious reasons the dis-
cussion is limited to systems that are described by
a paper in the proceedings. But instead of describ-
ing the systems one by one, we focus on the basic
methodological building blocks that are often found
in several systems although in different combina-
tions. For descriptions of the individual systems, we
refer to the respective papers in the proceedings.
Section 5.1 is devoted to system architectures. We
then describe the two main paradigms for learning
and inference, in this year?s shared task as well as in
last year?s, which we call transition-based parsers
(section 5.2) and graph-based parsers (section 5.3),
adopting the terminology of McDonald and Nivre
(2007).5 Finally, we give an overview of the domain
adaptation methods that were used (section 5.4).
5This distinction roughly corresponds to the distinction
made by Buchholz and Marsi (2006) between ?stepwise? and
?all-pairs? approaches.
923
5.1 Architectures
Most systems perform some amount of pre- and
post-processing, making the actual parsing compo-
nent part of a sequential workflow of varying length
and complexity. For example, most transition-
based parsers can only build projective dependency
graphs. For languages with non-projective depen-
dencies, graphs therefore need to be projectivized
for training and deprojectivized for testing (Hall et
al., 2007a; Johansson and Nugues, 2007b; Titov and
Henderson, 2007).
Instead of assigning HEAD and DEPREL in a
single step, some systems use a two-stage approach
for attaching and labeling dependencies (Chen et al,
2007; Dredze et al, 2007). In the first step unlabeled
dependencies are generated, in the second step these
are labeled. This is particularly helpful for factored
parsing models, in which label decisions cannot be
easily conditioned on larger parts of the structure
due to the increased complexity of inference. One
system (Hall et al, 2007b) extends this two-stage ap-
proach to a three-stage architecture where the parser
and labeler generate an n-best list of parses which in
turn is reranked.6
In ensemble-based systems several base parsers
provide parsing decisions, which are added together
for a combined score for each potential dependency
arc. The tree that maximizes the sum of these com-
bined scores is taken as the final output parse. This
technique is used by Sagae and Tsujii (2007) and in
the Nilsson system (Hall et al, 2007a). It is worth
noting that both these systems combine transition-
based base parsers with a graph-based method for
parser combination, as first described by Sagae and
Lavie (2006).
Data-driven grammar-based parsers, such as Bick
(2007), Schneider et al (2007), and Watson and
Briscoe (2007), need pre- and post-processing in or-
der to map the dependency graphs provided as train-
ing data to a format compatible with the grammar
used, and vice versa.
5.2 Transition-Based Parsers
Transition-based parsers build dependency graphs
by performing sequences of actions, or transitions.
Both learning and inference is conceptualized in
6They also flip the order of the labeler and the reranker.
terms of predicting the correct transition based on
the current parser state and/or history. We can fur-
ther subclassify parsers with respect to the model (or
transition system) they adopt, the inference method
they use, and the learning method they employ.
5.2.1 Models
The most common model for transition-based
parsers is one inspired by shift-reduce parsing,
where a parser state contains a stack of partially
processed tokens and a queue of remaining input
tokens, and where transitions add dependency arcs
and perform stack and queue operations. This type
of model is used by the majority of transition-based
parsers (Attardi et al, 2007; Duan et al, 2007; Hall
et al, 2007a; Johansson and Nugues, 2007b; Man-
nem, 2007; Titov and Henderson, 2007; Wu et al,
2007). Sometimes it is combined with an explicit
probability model for transition sequences, which
may be conditional (Duan et al, 2007) or generative
(Titov and Henderson, 2007).
An alternative model is based on the list-based
parsing algorithm described by Covington (2001),
which iterates over the input tokens in a sequen-
tial manner and evaluates for each preceding token
whether it can be linked to the current token or not.
This model is used by Marinov (2007) and in com-
ponent parsers of the Nilsson ensemble system (Hall
et al, 2007a). Finally, two systems use models based
on LR parsing (Sagae and Tsujii, 2007; Watson and
Briscoe, 2007).
5.2.2 Inference
The most common inference technique in transition-
based dependency parsing is greedy deterministic
search, guided by a classifier for predicting the next
transition given the current parser state and history,
processing the tokens of the sentence in sequen-
tial left-to-right order7 (Hall et al, 2007a; Mannem,
2007; Marinov, 2007; Wu et al, 2007). Optionally
multiple passes over the input are conducted until no
tokens are left unattached (Attardi et al, 2007).
As an alternative to deterministic parsing, several
parsers use probabilistic models and maintain a heap
or beam of partial transition sequences in order to
pick the most probable one at the end of the sentence
7For diversity in parser ensembles, right-to-left parsers are
also used.
924
(Duan et al, 2007; Johansson and Nugues, 2007b;
Sagae and Tsujii, 2007; Titov and Henderson, 2007).
One system uses as part of their parsing pipeline a
?neighbor-parser? that attaches adjacent words and
a ?root-parser? that identifies the root word(s) of a
sentence (Wu et al, 2007). In the case of grammar-
based parsers, a classifier is used to disambiguate
in cases where the grammar leaves some ambiguity
(Schneider et al, 2007; Watson and Briscoe, 2007)
5.2.3 Learning
Transition-based parsers either maintain a classifier
that predicts the next transition or a global proba-
bilistic model that scores a complete parse. To train
these classifiers and probabilitistic models several
approaches were used: SVMs (Duan et al, 2007;
Hall et al, 2007a; Sagae and Tsujii, 2007), modified
finite Newton SVMs (Wu et al, 2007), maximum
entropy models (Sagae and Tsujii, 2007), multiclass
averaged perceptron (Attardi et al, 2007) and max-
imum likelihood estimation (Watson and Briscoe,
2007).
In order to calculate a global score or probabil-
ity for a transition sequence, two systems used a
Markov chain approach (Duan et al, 2007; Sagae
and Tsujii, 2007). Here probabilities from the output
of a classifier are multiplied over the whole sequence
of actions. This results in a locally normalized
model. Two other entries used MIRA (Mannem,
2007) or online passive-aggressive learning (Johans-
son and Nugues, 2007b) to train a globally normal-
ized model. Titov and Henderson (2007) used an in-
cremental sigmoid Bayesian network to model the
probability of a transition sequence and estimated
model parameters using neural network learning.
5.3 Graph-Based Parsers
While transition-based parsers use training data to
learn a process for deriving dependency graphs,
graph-based parsers learn a model of what it means
to be a good dependency graph given an input sen-
tence. They define a scoring or probability function
over the set of possible parses. At learning time
they estimate parameters of this function; at pars-
ing time they search for the graph that maximizes
this function. These parsers mainly differ in the
type and structure of the scoring function (model),
the search algorithm that finds the best parse (infer-
ence), and the method to estimate the function?s pa-
rameters (learning).
5.3.1 Models
The simplest type of model is based on a sum of
local attachment scores, which themselves are cal-
culated based on the dot product of a weight vector
and a feature representation of the attachment. This
type of scoring function is often referred to as a first-
order model.8 Several systems participating in this
year?s shared task used first-order models (Schiehlen
and Spranger, 2007; Nguyen et al, 2007; Shimizu
and Nakagawa, 2007; Hall et al, 2007b). Canisius
and Tjong Kim Sang (2007) cast the same type of
arc-based factorization as a weighted constraint sat-
isfaction problem.
Carreras (2007) extends the first-order model to
incorporate a sum over scores for pairs of adjacent
arcs in the tree, yielding a second-order model. In
contrast to previous work where this was constrained
to sibling relations of the dependent (McDonald and
Pereira, 2006), here head-grandchild relations can
be taken into account.
In all of the above cases the scoring function is
decomposed into functions that score local proper-
ties (arcs, pairs of adjacent arcs) of the graph. By
contrast, the model of Nakagawa (2007) considers
global properties of the graph that can take multi-
ple arcs into account, such as multiple siblings and
children of a node.
5.3.2 Inference
Searching for the highest scoring graph (usually a
tree) in a model depends on the factorization cho-
sen and whether we are looking for projective or
non-projective trees. Maximum spanning tree al-
gorithms can be used for finding the highest scor-
ing non-projective tree in a first-order model (Hall
et al, 2007b; Nguyen et al, 2007; Canisius and
Tjong Kim Sang, 2007; Shimizu and Nakagawa,
2007), while Eisner?s dynamic programming algo-
rithm solves the problem for a first-order factoriza-
tion in the projective case (Schiehlen and Spranger,
2007). Carreras (2007) employs his own exten-
sion of Eisner?s algorithm for the case of projective
trees and second-order models that include head-
grandparent relations.
8It is also known as an edge-factored model.
925
The methods presented above are mostly efficient
and always exact. However, for models that take
global properties of the tree into account, they can-
not be applied. Instead Nakagawa (2007) uses Gibbs
sampling to obtain marginal probabilities of arcs be-
ing included in the tree using his global model and
then applies a maximum spanning tree algorithm to
maximize the sum of the logs of these marginals and
return a valid cycle-free parse.
5.3.3 Learning
Most of the graph-based parsers were trained using
an online inference-based method such as passive-
aggressive learning (Nguyen et al, 2007; Schiehlen
and Spranger, 2007), averaged perceptron (Carreras,
2007), or MIRA (Shimizu and Nakagawa, 2007),
while some systems instead used methods based on
maximum conditional likelihood (Nakagawa, 2007;
Hall et al, 2007b).
5.4 Domain Adaptation
5.4.1 Feature-Based Approaches
One way of adapting a learner to a new domain with-
out using any unlabeled data is to only include fea-
tures that are expected to transfer well (Dredze et
al., 2007). In structural correspondence learning a
transformation from features in the source domain
to features of the target domain is learnt (Shimizu
and Nakagawa, 2007). The original source features
along with their transformed versions are then used
to train a discriminative parser.
5.4.2 Ensemble-Based Approaches
Dredze et al (2007) trained a diverse set of parsers
in order to improve cross-domain performance by
incorporating their predictions as features for an-
other classifier. Similarly, two parsers trained with
different learners and search directions were used
in the co-learning approach of Sagae and Tsujii
(2007). Unlabeled target data was processed with
both parsers. Sentences that both parsers agreed on
were then added to the original training data. This
combined data set served as training data for one of
the original parsers to produce the final system. In
a similar fashion, Watson and Briscoe (2007) used a
variant of self-training to make use of the unlabeled
target data.
5.4.3 Other Approaches
Attardi et al (2007) learnt tree revision rules for the
target domain by first parsing unlabeled target data
using a strong parser; this data was then combined
with labeled source data; a weak parser was applied
to this new dataset; finally tree correction rules are
collected based on the mistakes of the weak parser
with respect to the gold data and the output of the
strong parser.
Another technique used was to filter sentences of
the out-of-domain corpus based on their similarity
to the target domain, as predicted by a classifier
(Dredze et al, 2007). Only if a sentence was judged
similar to target domain sentences was it included in
the training set.
Bick (2007) used a hybrid approach, where a data-
driven parser trained on the labeled training data was
given access to the output of a Constraint Grammar
parser for English run on the same data. Finally,
Schneider et al (2007) learnt collocations and rela-
tional nouns from the unlabeled target data and used
these in their parsing algorithm.
6 Analysis
Having discussed the major approaches taken in the
two tracks of the shared task, we will now return to
the test results. For the multilingual track, we com-
pare results across data sets and across systems, and
report results from a parser combination experiment
involving all the participating systems (section 6.1).
For the domain adaptation track, we sum up the most
important findings from the test results (section 6.2).
6.1 Multilingual Track
6.1.1 Across Data Sets
The average LAS over all systems varies from 68.07
for Basque to 80.95 for English. Top scores vary
from 76.31 for Greek to 89.61 for English. In gen-
eral, there is a good correlation between the top
scores and the average scores. For Greek, Italian,
and Turkish, the top score is closer to the average
score than the average distance, while for Czech, the
distance is higher. The languages that produced the
most stable results in terms of system ranks with re-
spect to LAS are Hungarian and Italian. For UAS,
Catalan also falls into this group. The language that
926
Setup Arabic Chinese Czech Turkish
2006 without punctuation 66.9 90.0 80.2 65.7
2007 without punctuation 75.5 84.9 80.0 71.6
2006 with punctuation 67.0 90.0 80.2 73.8
2007 with punctuation 76.5 84.7 80.2 79.8
Table 5: A comparison of the LAS top scores from 2006 and 2007. Official scoring conditions in boldface.
For Turkish, scores with punctuation also include word-internal dependencies.
produced the most unstable results with respect to
LAS is Turkish.
In comparison to last year?s languages, the lan-
guages involved in the multilingual track this year
can be more easily separated into three classes with
respect to top scores:
? Low (76.31?76.94):
Arabic, Basque, Greek
? Medium (79.19?80.21):
Czech, Hungarian, Turkish
? High (84.40?89.61):
Catalan, Chinese, English, Italian
It is interesting to see that the classes are more easily
definable via language characteristics than via char-
acteristics of the data sets. The split goes across
training set size, original data format (constituent
vs. dependency), sentence length, percentage of un-
known words, number of dependency labels, and ra-
tio of (C)POSTAGS and dependency labels. The
class with the highest top scores contains languages
with a rather impoverished morphology. Medium
scores are reached by the two agglutinative lan-
guages, Hungarian and Turkish, as well as by Czech.
The most difficult languages are those that combine
a relatively free word order with a high degree of in-
flection. Based on these characteristics, one would
expect to find Czech in the last class. However, the
Czech training set is four times the size of the train-
ing set for Arabic, which is the language with the
largest training set of the difficult languages.
However, it would be wrong to assume that train-
ing set size alone is the deciding factor. A closer
look at table 1 shows that while Basque and Greek
in fact have small training data sets, so do Turk-
ish and Italian. Another factor that may be asso-
ciated with the above classification is the percent-
age of new words (PNW) in the test set. Thus, the
expectation would be that the highly inflecting lan-
guages have a high PNW while the languages with
little morphology have a low PNW. But again, there
is no direct correspondence. Arabic, Basque, Cata-
lan, English, and Greek agree with this assumption:
Catalan and English have the smallest PNW, and
Arabic, Basque, and Greek have a high PNW. But
the PNW for Italian is higher than for Arabic and
Greek, and this is also true for the percentage of
new lemmas. Additionally, the highest PNW can be
found in Hungarian and Turkish, which reach higher
scores than Arabic, Basque, and Greek. These con-
siderations suggest that highly inflected languages
with (relatively) free word order need more training
data, a hypothesis that will have to be investigated
further.
There are four languages which were included in
the shared tasks on multilingual dependency pars-
ing both at CoNLL 2006 and at CoNLL 2007: Ara-
bic, Chinese, Czech, and Turkish. For all four lan-
guages, the same treebanks were used, which allows
a comparison of the results. However, in some cases
the size of the training set changed, and at least one
treebank, Turkish, underwent a thorough correction
phase. Table 5 shows the top scores for LAS. Since
the official scores excluded punctuation in 2006 but
includes it in 2007, we give results both with and
without punctuation for both years.
For Arabic and Turkish, we see a great improve-
ment of approximately 9 and 6 percentage points.
For Arabic, the number of tokens in the training
set doubled, and the morphological annotation was
made more informative. The combined effect of
these changes can probably account for the substan-
tial improvement in parsing accuracy. For Turkish,
the training set grew in size as well, although only by
600 sentences, but part of the improvement for Turk-
ish may also be due to continuing efforts in error cor-
927
rection and consistency checking. We see that the
choice to include punctuation or not makes a large
difference for the Turkish scores, since non-final IGs
of a word are counted as punctuation (because they
have the underscore character as their FORM value),
which means that word-internal dependency links
are included if punctuation is included.9 However,
regardless of whether we compare scores with or
without punctuation, we see a genuine improvement
of approximately 6 percentage points.
For Chinese, the same training set was used.
Therefore, the drop from last year?s top score to this
year?s is surprising. However, last year?s top scor-
ing system for Chinese (Riedel et al, 2006), which
did not participate this year, had a score that was
more than 3 percentage points higher than the sec-
ond best system for Chinese. Thus, if we compare
this year?s results to the second best system, the dif-
ference is approximately 2 percentage points. This
final difference may be attributed to the properties of
the test sets. While last year?s test set was taken from
the treebank, this year?s test set contains texts from
other sources. The selection of the textual basis also
significantly changed average sentence length: The
Chinese training set has an average sentence length
of 5.9. Last year?s test set alo had an average sen-
tence length of 5.9. However, this year, the average
sentence length is 7.5 tokens, which is a significant
increase. Longer sentences are typically harder to
parse due to the increased likelihood of ambiguous
constructions.
Finally, we note that the performance for Czech
is almost exactly the same as last year, despite the
fact that the size of the training set has been reduced
to approximately one third of last year?s training set.
It is likely that this in fact represents a relative im-
provement compared to last year?s results.
6.1.2 Across Systems
The LAS over all languages ranges from 80.32 to
54.55. The comparison of the system ranks aver-
aged over all languages with the ranks for single lan-
9The decision to include word-internal dependencies in this
way can be debated on the grounds that they can be parsed de-
terministically. On the other hand, they typically correspond to
regular dependencies captured by function words in other lan-
guages, which are often easy to parse as well. It is therefore
unclear whether scores are more inflated by including word-
internal dependencies or deflated by excluding them.
guages show considerably more variation than last
year?s systems. Buchholz and Marsi (2006) report
that ?[f]or most parsers, their ranking differs at most
a few places from their overall ranking?. This year,
for all of the ten best performing systems with re-
spect to LAS, there is at least one language for which
their rank is at least 5 places different from their
overall rank. The most extreme case is the top per-
forming Nilsson system (Hall et al, 2007a), which
reached rank 1 for five languages and rank 2 for
two more languages. Their only outlier is for Chi-
nese, where the system occupies rank 14, with a
LAS approximately 9 percentage points below the
top scoring system for Chinese (Sagae and Tsujii,
2007). However, Hall et al (2007a) point out that
the official results for Chinese contained a bug, and
the true performance of their system was actually
much higher. The greatest improvement of a sys-
tem with respect to its average rank occurs for En-
glish, for which the system by Nguyen et al (2007)
improved from the average rank 15 to rank 6. Two
more outliers can be observed in the system of Jo-
hansson and Nugues (2007b), which improves from
its average rank 12 to rank 4 for Basque and Turkish.
The authors attribute this high performance to their
parser?s good performance on small training sets.
However, this hypothesis is contradicted by their re-
sults for Greek and Italian, the other two languages
with small training sets. For these two languages,
the system?s rank is very close to its average rank.
6.1.3 An Experiment in System Combination
Having the outputs of many diverse dependency
parsers for standard data sets opens up the interest-
ing possibility of parser combination. To combine
the outputs of each parser we used the method of
Sagae and Lavie (2006). This technique assigns to
each possible labeled dependency a weight that is
equal to the number of systems that included the de-
pendency in their output. This can be viewed as
an arc-based voting scheme. Using these weights
it is possible to search the space of possible depen-
dency trees using directed maximum spanning tree
algorithms (McDonald et al, 2005). The maximum
spanning tree in this case is equal to the tree that on
average contains the labeled dependencies that most
systems voted for. It is worth noting that variants
of this scheme were used in two of the participating
928
5 10 15 20Number of Systems
80
82
84
86
88
Accu
racy
Unlabeled AccuracyLabeled Accuracy
Figure 1: System Combination
systems, the Nilsson system (Hall et al, 2007a) and
the system of Sagae and Tsujii (2007).
Figure 1 plots the labeled and unlabeled accura-
cies when combining an increasing number of sys-
tems. The data used in the plot was the output of all
competing systems for every language in the mul-
tilingual track. The plot was constructed by sort-
ing the systems based on their average labeled accu-
racy scores over all languages, and then incremen-
tally adding each system in descending order.10 We
can see that both labeled and unlabeled accuracy are
significantly increased, even when just the top three
systems are included. Accuracy begins to degrade
gracefully after about ten different parsers have been
added. Furthermore, the accuracy never falls below
the performance of the top three systems.
6.2 Domain Adaptation Track
For this task, the results are rather surprising. A look
at the LAS and UAS for the chemical research ab-
stracts shows that there are four closed systems that
outperform the best scoring open system. The best
system (Sagae and Tsujii, 2007) reaches an LAS of
81.06 (in comparison to their LAS of 89.01 for the
English data set in the multilingual track). Consider-
ing that approximately one third of the words of the
chemical test set are new, the results are noteworthy.
The next surprise is to be found in the relatively
low UAS for the CHILDES data. At a first glance,
this data set has all the characteristics of an easy
10The reason that there is no data point for two parsers is
that the simple voting scheme adopted only makes sense with at
least three parsers voting.
set; the average sentence is short (12.9 words), and
the percentage of new words is also small (6.10%).
Despite these characteristics, the top UAS reaches
62.49 and is thus more than 10 percentage points
below the top UAS for the chemical data set. One
major reason for this is that auxiliary and main
verb dependencies are annotated differently in the
CHILDES data than in the WSJ training set. As a
result of this discrepancy, participants were not re-
quired to submit results for the CHILDES data. The
best performing system on the CHILDES corpus is
an open system (Bick, 2007), but the distance to
the top closed system is approximately 1 percent-
age point. In this domain, it seems more feasible to
use general language resources than for the chemi-
cal domain. However, the results prove that the extra
effort may be unnecessary.
7 Conclusion
Two years of dependency parsing in the CoNLL
shared task has brought an enormous boost to the
development of dependency parsers for multiple lan-
guages (and to some extent for multiple domains).
But even though nineteen languages have been cov-
ered by almost as many different parsing and learn-
ing approaches, we still have only vague ideas about
the strengths and weaknesses of different methods
for languages with different typological characteris-
tics. Increasing our knowledge of the multi-causal
relationship between language structure, annotation
scheme, and parsing and learning methods probably
remains the most important direction for future re-
search in this area. The outputs of all systems for all
data sets from the two shared tasks are freely avail-
able for research and constitute a potential gold mine
for comparative error analysis across languages and
systems.
For domain adaptation we have barely scratched
the surface so far. But overcoming the bottleneck
of limited annotated resources for specialized do-
mains will be as important for the deployment of
human language technology as being able to handle
multiple languages in the future. One result from
the domain adaptation track that may seem surpris-
ing at first is the fact that closed class systems out-
performed open class systems on the chemical ab-
stracts. However, it seems that the major problem in
929
adapting pre-existing parsers to the new domain was
not the domain as such but the mapping from the
native output of the parser to the kind of annotation
provided in the shared task data sets. Thus, find-
ing ways of reusing already invested development
efforts by adapting the outputs of existing systems
to new requirements, without substantial loss in ac-
curacy, seems to be another line of research that may
be worth pursuing.
Acknowledgments
First and foremost, we want to thank all the peo-
ple and organizations that generously provided us
with treebank data and helped us prepare the data
sets and without whom the shared task would have
been literally impossible: Otakar Smrz, Charles
University, and the LDC (Arabic); Maxux Aranz-
abe, Kepa Bengoetxea, Larraitz Uria, Koldo Go-
jenola, and the University of the Basque Coun-
try (Basque); Ma. Anto`nia Mart?? Anton??n, Llu??s
Ma`rquez, Manuel Bertran, Mariona Taule?, Difda
Monterde, Eli Comelles, and CLiC-UB (Cata-
lan); Shih-Min Li, Keh-Jiann Chen, Yu-Ming
Hsieh, and Academia Sinica (Chinese); Jan Hajic?,
Zdenek Zabokrtsky, Charles University, and the
LDC (Czech); Brian MacWhinney, Eric Davis, the
CHILDES project, the Penn BioIE project, and
the LDC (English); Prokopis Prokopidis and ILSP
(Greek); Csirik Ja?nos and Zolta?n Alexin (Hun-
garian); Giuseppe Attardi, Simonetta Montemagni,
Maria Simi, Isidoro Barraco, Patrizia Topi, Kiril
Ribarov, Alessandro Lenci, Nicoletta Calzolari,
ILC, and ELRA (Italian); Gu?ls?en Eryig?it, Kemal
Oflazer, and Ruket C?ak?c? (Turkish).
Secondly, we want to thank the organizers of last
year?s shared task, Sabine Buchholz, Amit Dubey,
Erwin Marsi, and Yuval Krymolowski, who solved
all the really hard problems for us and answered all
our questions, as well as our colleagues who helped
review papers: Jason Baldridge, Sabine Buchholz,
James Clarke, Gu?ls?en Eryig?it, Kilian Evang, Ju-
lia Hockenmaier, Yuval Krymolowski, Erwin Marsi,
Bea?ta Megyesi, Yannick Versley, and Alexander
Yeh. Special thanks to Bertjan Busser and Erwin
Marsi for help with the CoNLL shared task website
and many other things, and to Richard Johansson for
letting us use his conversion tool for English.
Thirdly, we want to thank the program chairs
for EMNLP-CoNLL 2007, Jason Eisner and Taku
Kudo, the publications chair, Eric Ringger, the
SIGNLL officers, Antal van den Bosch, Hwee Tou
Ng, and Erik Tjong Kim Sang, and members of the
LDC staff, Tony Castelletto and Ilya Ahtaridis, for
great cooperation and support.
Finally, we want to thank the following people,
who in different ways assisted us in the organi-
zation of the CoNLL 2007 shared task: Giuseppe
Attardi, Eckhard Bick, Matthias Buch-Kromann,
Xavier Carreras, Tomaz Erjavec, Svetoslav Mari-
nov, Wolfgang Menzel, Xue Nianwen, Gertjan van
Noord, Petya Osenova, Florian Schiel, Kiril Simov,
Zdenka Uresova, and Heike Zinsmeister.
References
A. Abeille?, editor. 2003. Treebanks: Building and Using
Parsed Corpora. Kluwer.
I. Aduriz, M. J. Aranzabe, J. M. Arriola, A. Atutxa,
A. Diaz de Ilarraza, A. Garmendia, and M. Oronoz.
2003. Construction of a Basque dependency treebank.
In Proc. of the 2nd Workshop on Treebanks and Lin-
guistic Theories (TLT).
G. Attardi, F. Dell?Orletta, M. Simi, A. Chanev, and
M. Ciaramita. 2007. Multilingual dependency pars-
ing and domain adaptation using desr. In Proc. of the
CoNLL 2007 Shared Task. EMNLP-CoNLL.
E. Bick. 2007. Hybrid ways to improve domain inde-
pendence in an ML dependency parser. In Proc. of the
CoNLL 2007 Shared Task. EMNLP-CoNLL.
J. Blitzer, R. McDonald, and F. Pereira. 2006. Domain
adaptation with structural correspondence learning. In
Proc. of the Conf. on Empirical Methods in Natural
Language Processing (EMNLP).
A. Bo?hmova?, J. Hajic?, E. Hajic?ova?, and B. Hladka?. 2003.
The PDT: a 3-level annotation scenario. In Abeille?
(2003), chapter 7, pages 103?127.
R. Brown. 1973. A First Language: The Early Stages.
Harvard University Press.
S. Buchholz and E. Marsi. 2006. CoNLL-X shared
task on multilingual dependency parsing. In Proc. of
the Tenth Conf. on Computational Natural Language
Learning (CoNLL).
S. Canisius and E. Tjong Kim Sang. 2007. A constraint
satisfaction approach to dependency parsing. In Proc.
of the CoNLL 2007 Shared Task. EMNLP-CoNLL.
930
X. Carreras. 2007. Experiments with a high-order pro-
jective dependency parser. In Proc. of the CoNLL 2007
Shared Task. EMNLP-CoNLL.
E. Charniak. 2000. A maximum-entropy-inspired parser.
In Proc. of the First Meeting of the North American
Chapter of the Association for Computational Linguis-
tics (NAACL).
C. Chelba and A. Acero. 2004. Adaptation of maxi-
mum entropy capitalizer: Little data can help a lot. In
Proc. of the Conf. on Empirical Methods in Natural
Language Processing (EMNLP).
K. Chen, C. Luo, M. Chang, F. Chen, C. Chen, C. Huang,
and Z. Gao. 2003. Sinica treebank: Design criteria,
representational issues and implementation. In Abeille?
(2003), chapter 13, pages 231?248.
W. Chen, Y. Zhang, and H. Isahara. 2007. A two-stage
parser for multilingual dependency parsing. In Proc.
of the CoNLL 2007 Shared Task. EMNLP-CoNLL.
M. Collins. 1997. Three generative, lexicalised mod-
els for statistical parsing. In Proc. of the 35th Annual
Meeting of the Association for Computational Linguis-
tics (ACL).
M. A. Covington. 2001. A fundamental algorithm for
dependency parsing. In Proc. of the 39th Annual ACM
Southeast Conf.
D. Csendes, J. Csirik, T. Gyimo?thy, and A. Kocsor. 2005.
The Szeged Treebank. Springer.
H. Daume? and D. Marcu. 2006. Domain adaptation for
statistical classifiers. Journal of Artificial Intelligence
Research, 26:101?126.
M. Dredze, J. Blitzer, P. P. Talukdar, K. Ganchev,
J. Graca, and F. Pereira. 2007. Frustratingly hard do-
main adaptation for dependency parsing. In Proc. of
the CoNLL 2007 Shared Task. EMNLP-CoNLL.
X. Duan, J. Zhao, and B. Xu. 2007. Probabilistic parsing
action models for multi-lingual dependency parsing.
In Proc. of the CoNLL 2007 Shared Task. EMNLP-
CoNLL.
G. Eryig?it. 2007. ITU validation set for Metu-Sabanc?
Turkish Treebank. URL: http://www3.itu.edu.tr/
?gulsenc/papers/validationset.pdf.
R. Florian, H. Hassan, A. Ittycheriah, H. Jing, N. Kamb-
hatla, A. Luo, N. Nicolov, and S. Roukos. 2004. A
statisical model for multilingual entity detection and
tracking. In Proc. of the Human Language Technology
Conf. and the Annual Meeting of the North American
Chapter of the Association for Computational Linguis-
tics (HLT/NAACL).
D. Gildea. 2001. Corpus variation and parser perfor-
mance. In Proc. of the Conf. on Empirical Methods in
Natural Language Processing (EMNLP).
J. Hajic?, O. Smrz?, P. Zema?nek, J. S?naidauf, and E. Bes?ka.
2004. Prague Arabic dependency treebank: Develop-
ment in data and tools. In Proc. of the NEMLAR In-
tern. Conf. on Arabic Language Resources and Tools.
J. Hall, J. Nilsson, J. Nivre, G. Eryig?it, B. Megyesi,
M. Nilsson, and M. Saers. 2007a. Single malt or
blended? A study in multilingual parser optimization.
In Proc. of the CoNLL 2007 Shared Task. EMNLP-
CoNLL.
K. Hall, J. Havelka, and D. Smith. 2007b. Log-linear
models of non-projective trees, k-best MST parsing
and tree-ranking. In Proc. of the CoNLL 2007 Shared
Task. EMNLP-CoNLL.
R. Johansson and P. Nugues. 2007a. Extended
constituent-to-dependency conversion for English. In
Proc. of the 16th Nordic Conf. on Computational Lin-
guistics (NODALIDA).
R. Johansson and P. Nugues. 2007b. Incremental depen-
dency parsing using online learning. In Proc. of the
CoNLL 2007 Shared Task. EMNLP-CoNLL.
T. Kudo and Y. Matsumoto. 2002. Japanese dependency
analysis using cascaded chunking. In Proc. of the Sixth
Conf. on Computational Language Learning (CoNLL).
S. Kulick, A. Bies, M. Liberman, M. Mandel, R. Mc-
Donald, M. Palmer, A. Schein, and L. Ungar. 2004.
Integrated annotation for biomedical information ex-
traction. In Proc. of the Human Language Technology
Conf. and the Annual Meeting of the North American
Chapter of the Association for Computational Linguis-
tics (HLT/NAACL).
B. MacWhinney. 2000. The CHILDES Project: Tools
for Analyzing Talk. Lawrence Erlbaum.
P. R. Mannem. 2007. Online learning for determinis-
tic dependency parsing. In Proc. of the CoNLL 2007
Shared Task. EMNLP-CoNLL.
M. Marcus, B. Santorini, and M. Marcinkiewicz. 1993.
Building a large annotated corpus of English: the Penn
Treebank. Computational Linguistics, 19(2):313?330.
S. Marinov. 2007. Covington variations. In Proc. of the
CoNLL 2007 Shared Task. EMNLP-CoNLL.
M. A. Mart??, M. Taule?, L. Ma`rquez, and M. Bertran.
2007. CESS-ECE: A multilingual and multilevel
annotated corpus. Available for download from:
http://www.lsi.upc.edu/?mbertran/cess-ece/.
931
D. McClosky, E. Charniak, and M. Johnson. 2006.
Reranking and self-training for parser adaptation. In
Proceedings of the 21st International Conference on
Computational Linguistics and 44th Annual Meeting
of the Association for Computational Linguistics.
R. McDonald and J. Nivre. 2007. Characterizing the
errors of data-driven dependency parsing models. In
Proc. of the Joint Conf. on Empirical Methods in Nat-
ural Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL).
R. McDonald and F. Pereira. 2006. Online learning of
approximate dependency parsing algorithms. In Proc.
of the 11th Conf. of the European Chapter of the Asso-
ciation for Computational Linguistics (EACL).
R. McDonald, F. Pereira, K. Ribarov, and J. Hajic?. 2005.
Non-projective dependency parsing using spanning
tree algorithms. In Proc. of the Human Language
Technology Conf. and the Conf. on Empirical Methods
in Natural Language Processing (HLT/EMNLP).
S. Montemagni, F. Barsotti, M. Battista, N. Calzolari,
O. Corazzari, A. Lenci, A. Zampolli, F. Fanciulli,
M. Massetani, R. Raffaelli, R. Basili, M. T. Pazienza,
D. Saracino, F. Zanzotto, N. Nana, F. Pianesi, and
R. Delmonte. 2003. Building the Italian Syntactic-
Semantic Treebank. In Abeille? (2003), chapter 11,
pages 189?210.
T. Nakagawa. 2007. Multilingual dependency parsing
using Gibbs sampling. In Proc. of the CoNLL 2007
Shared Task. EMNLP-CoNLL.
L.-M. Nguyen, T.-P. Nguyen, and A. Shimazu. 2007. A
multilingual dependency analysis system using online
passive-aggressive learning. In Proc. of the CoNLL
2007 Shared Task. EMNLP-CoNLL.
J. Nivre, J. Hall, J. Nilsson, A. Chanev, G. Eryig?it,
S. Ku?bler, S. Marinov, and E. Marsi. 2007. Malt-
Parser: A language-independent system for data-
driven dependency parsing. Natural Language Engi-
neering, 13:95?135.
K. Oflazer, B. Say, D. Zeynep Hakkani-Tu?r, and G. Tu?r.
2003. Building a Turkish treebank. In Abeille? (2003),
chapter 15, pages 261?277.
P. Prokopidis, E. Desypri, M. Koutsombogera, H. Papa-
georgiou, and S. Piperidis. 2005. Theoretical and
practical issues in the construction of a Greek depen-
dency treebank. In Proc. of the 4th Workshop on Tree-
banks and Linguistic Theories (TLT).
S. Riedel, R. C?ak?c?, and I. Meza-Ruiz. 2006. Multi-
lingual dependency parsing with incremental integer
linear programming. In Proc. of the Tenth Conf. on
Computational Natural Language Learning (CoNLL).
B. Roark and M. Bacchiani. 2003. Supervised and un-
supervised PCFG adaptation to novel domains. In
Proc. of the Human Language Technology Conf. and
the Annual Meeting of the North American Chap-
ter of the Association for Computational Linguistics
(HLT/NAACL).
K. Sagae and A. Lavie. 2006. Parser combination by
reparsing. In Proc. of the Human Language Technol-
ogy Conf. of the North American Chapter of the Asso-
ciation of Computational Linguistics (HLT/NAACL).
K. Sagae and J. Tsujii. 2007. Dependency parsing
and domain adaptation with LR models and parser en-
sembles. In Proc. of the CoNLL 2007 Shared Task.
EMNLP-CoNLL.
M. Schiehlen and Kristina Spranger. 2007. Global learn-
ing of labelled dependency trees. In Proc. of the
CoNLL 2007 Shared Task. EMNLP-CoNLL.
G. Schneider, K. Kaljurand, F. Rinaldi, and T. Kuhn.
2007. Pro3Gres parser in the CoNLL domain adap-
tation shared task. In Proc. of the CoNLL 2007 Shared
Task. EMNLP-CoNLL.
N. Shimizu and H. Nakagawa. 2007. Structural corre-
spondence learning for dependency parsing. In Proc.
of the CoNLL 2007 Shared Task. EMNLP-CoNLL.
I. Titov and J. Henderson. 2006. Porting statistical
parsers with data-defined kernels. In Proc. of the Tenth
Conf. on Computational Natural Language Learning
(CoNLL).
I. Titov and J. Henderson. 2007. Fast and robust mul-
tilingual dependency parsing with a generative latent
variable model. In Proc. of the CoNLL 2007 Shared
Task. EMNLP-CoNLL.
R. Watson and T. Briscoe. 2007. Adapting the RASP
system for the CoNLL07 domain-adaptation task. In
Proc. of the CoNLL 2007 Shared Task. EMNLP-
CoNLL.
Y.-C. Wu, J.-C. Yang, and Y.-S. Lee. 2007. Multi-
lingual deterministic dependency parsing framework
using modified finite Newton method support vector
machines. In Proc. of the CoNLL 2007 Shared Task.
EMNLP-CoNLL.
H. Yamada and Y. Matsumoto. 2003. Statistical depen-
dency analysis with support vector machines. In Proc.
8th International Workshop on Parsing Technologies
(IWPT).
932
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 406?414,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Parsing Coordinations
Sandra Ku?bler
Indiana University
skuebler@indiana.edu
Erhard Hinrichs
Universita?t Tu?bingen
eh@sfs.uni-tuebingen.de
Wolfgang Maier
Unversita?t Tu?bingen
wo.maier@uni-tuebingen.de
Eva Klett
Universita?t Tu?bingen
eklett@sfs.uni-tuebingen.de
Abstract
The present paper is concerned with sta-
tistical parsing of constituent structures
in German. The paper presents four ex-
periments that aim at improving parsing
performance of coordinate structure: 1)
reranking the n-best parses of a PCFG
parser, 2) enriching the input to a PCFG
parser by gold scopes for any conjunct, 3)
reranking the parser output for all possi-
ble scopes for conjuncts that are permissi-
ble with regard to clause structure. Exper-
iment 4 reranks a combination of parses
from experiments 1 and 3.
The experiments presented show that n-
best parsing combined with reranking im-
proves results by a large margin. Provid-
ing the parser with different scope possi-
bilities and reranking the resulting parses
results in an increase in F-score from
69.76 for the baseline to 74.69. While the
F-score is similar to the one of the first ex-
periment (n-best parsing and reranking),
the first experiment results in higher re-
call (75.48% vs. 73.69%) and the third one
in higher precision (75.43% vs. 73.26%).
Combining the two methods results in the
best result with an F-score of 76.69.
1 Introduction
The present paper is concerned with statistical
parsing of constituent structures in German. Ger-
man is a language with relatively flexible phrasal
ordering, especially of verbal complements and
adjuncts. This makes processing complex cases
of coordination particularly challenging and error-
prone. The paper presents four experiments that
aim at improving parsing performance of coor-
dinate structures: the first experiment involves
reranking of n-best parses produced by a PCFG
parser, the second experiment enriches the input
to a PCFG parser by offering gold pre-bracketings
for any coordinate structures that occur in the sen-
tence. In the third experiment, the reranker is
given all possible pre-bracketed candidate struc-
tures for coordinated constituents that are permis-
sible with regard to clause macro- and microstruc-
ture. The parsed candidates are then reranked.
The final experiment combines the parses from the
first and the third experiment and reranks them.
Improvements in this final experiment corroborate
our hypothesis that forcing the parser to work with
pre-bracketed conjuncts provides parsing alterna-
tives that are not present in the n-best parses.
Coordinate structures have been a central is-
sue in both computational and theoretical linguis-
tics for quite some time. Coordination is one of
those phenomena where the simple cases can be
accounted for by straightforward empirical gen-
eralizations and computational techniques. More
specifically, it is the observation that coordination
involves two or more constituents of the same cat-
egories. However, there are a significant number
of more complex cases of coordination that defy
this generalization and that make the parsing task
of detecting the right scope of individual conjuncts
and correctly delineating the correct scope of the
coordinate structure as a whole difficult. (1) shows
some classical examples of this kind from English.
(1) a. Sandy is a Republican and proud of it.
b. Bob voted, but Sandy did not.
c. Bob supports him and Sandy me.
In (1a), unlike categories (NP and adjective) are
conjoined. (1b) and (1c) are instances of ellipsis
(VP ellipsis and gapping). Yet another difficult set
of examples present cases of non-constituent con-
junction, as in (2), where the direct and indirect
object of a ditransitive verb are conjoined.
(2) Bob gave a book to Sam and a record to Jo.
406
2 Coordination in German
The above phenomena have direct analogues in
German.1 Due to the flexible ordering of phrases,
their variability is even higher. For example, due
to constituent fronting to clause-initial position in
German verb-second main clauses, cases of non-
constituent conjunction can involve any two NPs
(including the subject) of a ditransitive verb to the
exclusion of the third NP complement that appears
in clause-initial position. In addition, German ex-
hibits cases of asymmetric coordination first dis-
cussed by Ho?hle (1983; 1990; 1991) and illus-
trated in (3).2
(3) In
Into
den
the
Wald
woods
ging
went
ein
a
Ja?ger
hunter
und
and
schoss
shot
einen
a
Hasen.
hare.
Such cases of subject gap coordination are fre-
quently found in text corpora (cf. (4) below) and
involve conjunction of a full verb-second clause
with a VP whose subject is identical to the subject
in the first conjunct.
3 Experimental Setup and Baseline
3.1 The Treebank
The data source used for the experiments is the
Tu?bingen Treebank of Written German (Tu?Ba-
D/Z) (Telljohann et al, 2005). Tu?Ba-D/Z uses
the newspaper ?die tageszeitung? (taz) as its data
source, version 3 comprises approximately 27 000
sentences. The treebank annotation scheme dis-
tinguishes four levels of syntactic constituency:
the lexical level, the phrasal level, the level of
topological fields, and the clausal level. The pri-
mary ordering principle of a clause is the inventory
of topological fields (VF, LK, MF, VC, and NF),
which characterize the word order regularities
among different clause types of German. Tu?Ba-
D/Z annotation relies on a context-free backbone
(i.e. proper trees without crossing branches) of
phrase structure combined with edge labels that
specify the grammatical function of the phrase in
question. Conjuncts are generally marked with the
1To avoid having to gloss German examples, they were
illustrated for English.
2Yet, another case of such asymmetric coordination dis-
cussed by Ho?hle involves cases of conjunction of different
clause types: [V?final Wenn du nach Hause kommst ] und
[V?2nd da warten Polizeibeamte vor der Tu?r. ?If you come
home and there are policemen waiting in front of the door ] .?
function label KONJ. Figure 1 shows the anno-
tation that sentence (4) received in the treebank.
Syntactic categories are displayed as nodes, gram-
matical functions as edge labels in gray (e.g. OA:
direct object, PRED: predicate). This is an exam-
ple of a subject-gap coordination, in which both
conjuncts (FKONJ) share the subject (ON) that is
realized in the first conjunct.
(4) Damit
So
hat
has
sich
itself
der
the
Bevo?lkerungs-
decline in
ru?ckgang
population
zwar
though
abgeschwa?cht,
lessened,
ist
is
aber
however
noch
still
doppelt
double
so
so
gro?
big
wie
as
1996.
1996.
?For this reason, although the decline in
population has lessened, it is still twice as
big as in 1996.?
The syntactic annotation scheme of the Tu?Ba-
D/Z is described in more detail in Telljohann et al
(2004; 2005).
All experiments reported here are based on a
data split of 90% training data and 10% test data.
3.2 The Parsers and the Reranker
Two parsers were used to investigate the influ-
ence of scope information on parser performance
on coordinate structures: BitPar (Schmid, 2004)
and LoPar (Schmid, 2000). BitPar is an effi-
cient implementation of an Earley style parser that
uses bit vectors. However, BitPar cannot han-
dle pre-bracketed input. For this reason, we used
LoPar for the experiments where such input was
required. LoPar, as it is used here, is a pure
PCFG parser, which allows the input to be par-
tially bracketed. We are aware that the results
that can be obtained by pure PCFG parsers are
not state of the art as reported in the shared task
of the ACL 2008 Workshop on Parsing German
(Ku?bler, 2008). While BitPar reaches an F-score
of 69.76 (see next section), the best performing
parser (Petrov and Klein, 2008) reaches an F-
score of 83.97 on Tu?Ba-D/Z (but with a different
split of training and test data). However, our ex-
periments require certain features in the parsers,
namely the capability to provide n-best analyses
and to parse pre-bracketed input. To our knowl-
edge, the parsers that took part in the shared task
do not provide these features. Should they become
available, the methods presented here could be ap-
plied to such parsers. We see no reason why our
407
Figure 1: A tree with coordination.
methods should not be able to improve the results
of these parsers further.
Since we are interested in parsing coordina-
tions, all experiments are conducted with gold
POS tags, so as to abstract away from POS tag-
ging errors. Although the treebank contains mor-
phological information, this type of information is
not used in the experiments presented here.
The reranking experiments were conducted us-
ing the reranker by Collins and Koo (2005). This
reranker uses a set of candidate parses for a sen-
tence and reranks them based on a set of features
that are extracted from the trees. The reranker uses
a boosting method based on the approach by Fre-
und et al (1998). We used a similar feature set
to the one Collins and Koo used; the following
types of features were included: rules, bigrams,
grandparent rules, grandparent bigrams, lexical
bigrams, two-level rules, two-level bigrams, tri-
grams, head-modifiers, PPs, and distance for head-
modifier relations, as well as all feature types in-
volving rules extended by closed class lexicaliza-
tion. For a more detailed description of the rules,
the interested reader is referred to Collins and
Koo (2005). For coordination, these features give
a wider context than the original parser has and
should thus result in improvements for this phe-
nomenon.
3.3 The Baseline
When trained on 90% of the approximately 27,000
sentences of the Tu?Ba-D/Z treebank, BitPar
reaches an F-Score of 69.73 (precision: 68.63%,
recall: 70.93%) on the full test set of 2611 sen-
tences. These results as well as all further re-
sults presented here are labeled results, including
grammatical functions. Since German has a rela-
tively free word order, it is impossible to deduce
the grammatical function of a noun phrase from
the configuration of the sentence. Consequently,
an evaluation based solely on syntactic constituent
labels would be meaningless (cf. (Ku?bler, 2008)
for a discussion of this point). The inclusion of
grammatical labels in the trees, makes the parsing
process significantly more complex.
Looking at sentences with coordination (i.e.
sentences that contain a conjunction which is not
in sentence-initial position), we find that 34.9%
of the 2611 test sentences contain coordinations.
An evaluation of only sentences with coordina-
tion shows that there is a noticeable difference: the
F-score reaches 67.28 (precision: 66.36%, recall:
68.23%) as compared to 69.73 for the full test set.
The example of a wrong parse shown below il-
lustrates why parsing of complex coordinations is
so hard. Complex coordinations can take up a con-
siderable part of the input string and accordingly
of the overall sentence structure. Such global phe-
nomena are particularly hard for pure PCFG pars-
ing, due to the independence assumption inherent
in the statistical models for PCFGs.
Sentence (4) has the following Viterbi parse:
(VROOT
(SIMPX
(VF
(SIMPX-OS
(VF (PX-MOD (PROP-HD Damit)))
(LK
(VXFIN-HD (VAFIN-HD hat)))
(MF
408
(NX-OA (PRF-HD sich))
(NX-ON (ART der)
(NN-HD Bevo?lkerungsru?ckgang))
(ADVX-MOD (ADV-HD zwar)))
(VC (VXINF-OV
(VVPP-HD abgeschwa?cht)))))
($, ,)
(LK
(VXFIN-HD (VAFIN-HD ist)))
(MF
(ADVX-MOD (ADV-HD aber))
(ADVX-MOD (ADV-HD noch))
(ADJX-PRED
(ADJX-HD (ADVX (ADV-HD mehr))
(ADJX (KOKOM als)
(ADJD-HD doppelt))
(ADVX (ADV-HD so))
(ADJD-HD gro?))
(NX (KOKOM wie)
(CARD-HD 1996)))))
($. .))
The parse shows that the parser did not
recognize the coordination. Instead, the first con-
junct including the fronted constituent, Damit
hat sich der Bevo?lkerungsru?ckgang
zwar abgeschwa?cht, is treated as a fronted
subordinate clause.
4 Experiment 1: n-Best Parsing and
Reranking
The first hypothesis for improving coordination
parsing is based on the assumption that the correct
parse may not be the most probable one in Viterbi
parsing but may be recovered by n-best parsing
and reranking, a technique that has become stan-
dard in the last few years. If this hypothesis holds,
we should find the correct parse among the n-best
parses. In order to test this hypothesis, we con-
ducted an experiment with BitPar (Schmid, 2004).
We parsed the test sentences in a 50-best setting.
A closer look at the 50-best parses shows that of
the 2611 sentences, 195 (7.5%) were assigned the
correct parse as the best parse. For 325 more sen-
tences (12.4%), the correct parse could be found
under the 50 best analyses. What is more, in
90.2% of these 520 sentences, for which the cor-
rect parse was among the 50 best parses, the best
parse was among the first 10 parses. Additionally,
only in 4 cases were the correct analyses among
the 40-best to 50-best parses, an indication that in-
creasing n may not result in improving the results
significantly. These findings resulted in the deci-
sion not to conduct experiments with higher n.
That the 50 best analyses contain valuable infor-
mation can be seen from an evaluation in which an
oracle chooses from the 50 parses. In this case, we
reach an F-score of 80.28. However, this F-score
is also the upper limit for improvement that can be
achieved by reranking the 50-best parses.
For reranking, the features of Collins and
Koo (2005) were extended in the following way:
Since the German treebank used for our exper-
iments includes grammatical function informa-
tion on almost all levels in the tree, all feature
types were also included with grammatical func-
tions attached: All nodes except the root node
of the subtree in question were annotated with
their grammatical information. Thus, for the noun
phrase (NX) rule with grandparent prepositional
phrase (PX) PXGP NX? ART ADJX NN, we add
an additional rule PXGP NX-HD ? ART ADJX
NN-HD.
After pruning all features that occurred in the
training data with a frequency lower than 5, the ex-
tractions produced more than 5 mio. different fea-
tures. The reranker was optimized on the training
data, the 50-best parses were produced in a 5-fold
cross-validation setting. A non-exhaustive search
for the best value for the ? parameter showed that
Collins and Koo?s value of 0.0025 produced the
best results. The row for exp. 1 in Table 1 shows
the results of this experiment. The evaluation of
the full data set shows an improvement of 4.77
points in the F-score, which reached 74.53. This is
a relative reduction in error rate of 18.73%, which
is slightly higher that the error rate reduction re-
ported by Collins and Koo for the Penn Treebank
(13%). However, the results for Collins and Koo?s
original parses were higher, and they did not eval-
uate on grammatical functions.
The evaluation of coordination sentences shows
that such sentences profit from reranking to the
same degree. These results prove that while coor-
dination structures profit from reranking, they do
not profit more than other phenomena. We thus
conclude that reranking is no cure-all for solving
the problem of accurate coordination parsing.
5 Experiment 2: Gold Scope
The results of experiment 1 lead to the conclusion
that reranking the n-best parses can only result
in restricted improvements on coordinations. The
fact that the correct parse often cannot be found
in the 50-best analyses suggests that the different
possible scopes of a coordination are so different
in their probability distribution that not all of the
possible scopes are present in the 50-best analyses.
409
all sentences coord. sentences
precision recall F-score precision recall F-score
baseline: 68.63 70.93 69.76 66.36 68.23 67.28
exp. 1: 50-best reranking: 73.26 75.84 74.53 70.67 72.72 71.68
exp. 2: with gold scope: 76.12 72.87 74.46 75.78 72.22 73.96
exp. 3: automatic scope: 75.43 73.96 74.69 72.88 71.42 72.14
exp. 4: comb. 1 and 3: 76.15 77.23 76.69 73.79 74.73 74.26
Table 1: The results of parsing all sentences and coordinated sentences only
If this hypothesis holds, forcing the parser to con-
sider the different scope readings should increase
the accuracy of coordination parsing. In order to
force the parser to use the different scope readings,
we first extract these scope readings, and then for
each of these scope readings generate a new sen-
tence with partial bracketing that represents the
corresponding scope (see below for an example).
LoPar is equipped to parse partially-bracketed in-
put. Given input sentences with partial brackets,
the parser restricts analyses to such cases that do
not contradict the brackets in the input.
(5) Was
Which
stimmt,
is correct,
weil
because
sie
they
unterhaltsam
entertaining
sind,
are,
aber
but
auch
also
falsche
wrong
Assoziationen
associations
weckt.
wakes.
?Which is correct because they are enter-
taining, but also triggers wrong associa-
tions.?
In order to test the validity of this hypothe-
sis, we conducted an experiment with coordination
scopes extracted from the treebank trees. These
scopes were translated into partial brackets that
were included in the input sentences. For the sen-
tence in (5) from the treebank (sic), the input for
LoPar would be the following:
Was/PWS stimmt/VVFIN ,/$, weil/
KOUS ( sie/PPER unterhalt-
sam/ADJD sind/VAFIN ) ,/$,
aber/KON ( auch/ADV falsche/ADJA
Assoziationen/NN weckt/VVFIN )
The round parentheses delineate the conjuncts.
LoPar was then forced to parse sentences contain-
ing coordination with the correct scope for the co-
ordination. The results for this experiment are
shown in Table 1 as exp. 2.
The introduction of partial brackets that delimit
the scope of the coordination improve overall re-
sults on the full test set by 4.7 percent points, a
rather significant improvement when we consider
that only approximately one third of the test sen-
tences were modified. The evaluation of the set
of sentences that contain coordination shows that
here, the difference is even higher: 6.7 percent
points. It is also worth noticing that provided with
scope information, the parser parses such sen-
tences with the same accuracy as other sentences.
The difference in F-scores between all sentences
and only sentences with coordination in this ex-
periment is much lower (0.5 percent points) than
for all other experiments (2.5?3.0 percent points).
When comparing the results of experiment 1 (n-
best parsing) with the present one, it is evident that
the F-scores are very similar: 74.53 for the 50-best
reranking setting, and 74.46 for the one where we
provided the gold scope. However, a comparison
of precision and recall shows that there are differ-
ences: 50-best reranking results in higher recall,
providing gold scope for coordinations in higher
precision. The lower recall in the latter experiment
indicates that the provided brackets in some cases
are not covered by the grammar. This is corrob-
orated by the fact that in n-best parsing, only 1
sentence could not be parsed; but in parsing with
gold scope, 8 sentences could not be parsed.
6 Experiment 3: Extracting Scope
The previous experiment has shown that providing
the scope of a coordination drastically improves
results for sentences with coordination as well as
for the complete test set (although to a lower de-
gree). The question that remains to be answered is
whether automatically generated possible scopes
can provide enough information for the reranker
to improve results.
The first question that needs to be answered is
how to find the possible scopes for a coordina-
tion. One possibility is to access the parse forest
of a chart parser such as LoPar and extract infor-
410
mation about all the possible scope analyses that
the parser found. If the same parser is used for
this step and for the final parse, we can be cer-
tain that only scopes are extracted that are com-
patible with the grammar of the final parser. How-
ever, parse forests are generally stored in a highly
packed format so that an exhaustive search of the
structures is very inefficient and proved impossi-
ble with present day computing power.
(6) ?Es
?There
gibt
are
zwar
indeed
ein
a
paar
few
Niederflurbusse,
low-floor buses,
aber
but
das
that
reicht
suffices
ja
part.
nicht?,
not?,
sagt
says
er.
he.
??There are indeed a few low-floor buses,
but that isn?t enough?, he says.
Another solution consists of generating all pos-
sible scopes around the coordination. Thus, for
the sentence in (6), the conjunction is aber. The
shortest possible left conjunct is Niederflurbusse,
the next one paar Niederflurbusse, etc. Clearly,
many of these possibilities, such as the last exam-
ple, are nonsensical, especially when the proposed
conjunct crosses into or out of base phrase bound-
aries. Another type of boundary that should not
be crossed is a clause boundary. Since the con-
junction is part of the subordinated clause in the
present example, the right conjunct cannot extend
beyond the end of the clause, i.e. beyond nicht.
For this reason, we used KaRoPars (Mu?ller and
Ule, 2002), a partial parser for German, to parse
the sentences. From the partial parses, we ex-
tracted base phrases and clauses. For (6), the rel-
evant bracketing provided by KaRoPars is the fol-
lowing:
( " Es gibt zwar { ein paar
Niederflurbusse } , ) aber ( das
reicht ja nicht ) " , sagt er .
The round parentheses mark clause boundaries,
the curly braces the one base phrase that is longer
than one word. In the creation of possible con-
juncts, only such conjuncts are listed that do not
cross base phrase or clause boundaries. In order to
avoid unreasonably high numbers of pre-bracketed
versions, we also use higher level phrases, such as
coordinated noun phrases. KaRoPars groups such
higher level phrases only in contexts that allow
a reliable decision. While a small percentage of
such decisions is wrong, the heuristic used turns
out to be reliable and efficient.
For each scope, a partially bracketed version
of the input sentence is created, in which only
the brackets for the suggested conjuncts are in-
serted. Each pre-bracketed version of the sentence
is parsed with LoPar. Then all versions for one
sentence are reranked. The reranker was trained
on the data from experiment 1 (n-best parsing).
The results of the reranker show that our restric-
tions based on the partial parser may have been
too restrictive. Only 375 sentences had more than
one pre-bracketed version, and only 328 sentence
resulted in more than one parse. Only the latter set
could then profit from reranking.
The results of this experiment are shown in Ta-
ble 1 as exp. 3. They show that extracting pos-
sible scopes for conjuncts from a partial parse
is possible. The difference in F-score between
this experiment and the baseline reaches 5.93 per-
cent points. The F-score is also minimally higher
than the F-score for experiment 2 (gold scope),
and recall is increased by approximately 1 per-
cent point (even though only 12.5% of the sen-
tences were reranked). This can be attributed to
two factors: First, we provide different scope pos-
sibilities. This means that if the correct scope is
not covered by the grammar, the parser may still
be able to parse the next closest possibility in-
stead of failing completely. Second, reranking is
not specifically geared towards improving coordi-
nated structures. Thus, it is possible that a parse is
reranked higher because of some other feature. It
is, however, not the case that the improvement re-
sults completely from reranking. This can be de-
duced from two points: First, while the F-score
for experiment 1 (50-best analyses plus reranking)
and the present experiment are very close (74.53
vs. 74.69), there are again differences in precision
and recall: In experiment 1, recall is higher, and in
the present experiment precision. Second, a look
at the evaluation on only sentences with coordi-
nation shows that the F-score for the present ex-
periment is higher than the one for experiment 1
(72.14 vs. 71.68). Additionally, precision for the
present experiment is more than 2 percent points
higher.
7 Experiment 4: Combining n-Best
Parses and Extracted Scope Parses
As described above, the results for reranking the
50-best analyses and for reranking the versions
411
with automatically extracted scope readings are
very close. This raises the question whether the
two methods produce similar improvements in the
parse trees. One indicator that this is not the case
can be found in the differences in precision and re-
call. Another possibility of verifying our assump-
tion that the improvements do not overlap lies in
the combination of the 50-best parses with the
parses resulting from the automatically extracted
scopes. This increases the number of parses be-
tween which the reranker can choose. In effect,
this means a combination of the methods of exper-
iments 1 (n-best) and 3 (automatic scope). Con-
sequently, if the results from this experiment are
very close to the results from experiment 1 (n-
best), we can conclude that adding the parses with
automatic scope readings does not add new infor-
mation. If, however, adding these parses improves
results, we can conclude that new information was
present in the parses with automatic scope that
was not covered in the 50-best parses. Note that
the combination of the two types of input for the
reranker should not be regarded as a parser ensem-
ble but rather as a resampling of the n-best search
space since both parsers use the same grammar,
parsing model, and probability model. The only
difference is that LoPar can accept partially brack-
eted input, and BitPar can list the n-best analyses.
The results of this experiment are shown in Ta-
ble 1 as exp. 4. For all sentences, both precision
and recall are higher than for experiment 1 and 3,
resulting in an F-score of 76.69. This is more than
2 percent points higher than for the 50-best parses.
This is a very clear indication that the parses con-
tributed by the automatically extracted scopes pro-
vide parses that were not present in the 50 best
parses from experiment 1 (n-best). The same trend
can be seen in the evaluation of the sentences con-
taining coordination: Here, the improvement in F-
score is higher than for the whole set, a clear in-
dication that this method is suitable for improving
coordination parsing. A comparison of the results
of the present experiment and experiment 3 (with
automatic scope only) shows that the gain in pre-
cision is rather small, but the combination clearly
improves recall, from 73.96% to 77.23%. We can
conclude that adding the 50 best parses remedies
the lacking coverage that was the problem of ex-
periment 3. More generally, experiment 4 suggests
that for the notoriously difficult problem of pars-
ing coordination structures, a hybrid approach that
combines parse selection of n best analyses with
pre-bracketed scope in the input results in a con-
siderable reduction in error rate compared to each
of these methods used in isolation.
8 Related Work
Parsing of coordinate structures for English has
received considerable attention in computational
linguistics. Collins (1999), among many other au-
thors, reports in the error analysis of his WSJ pars-
ing results that coordination is one of the most fre-
quent cases of incorrect parses, particularly if the
conjuncts involved are complex. He manages to
reduce errors for simple cases of NP coordination
by introducing a special phrasal category of base
NPs. In the experiments presented above, no ex-
plicit distinction is made between simple and com-
plex cases of coordination, and no transformations
are performed on the treebank annotations used for
training.
Our experiment 1, reranking 50-best parses, is
similar to the approaches of Charniak and John-
son (2005) and of Hogan (2007). However, it dif-
fers from their experiments in two crucial ways: 1)
Compared to Charniak and Johnson, who use 1.1
mio. features, our feature set is appr. five times
larger (more than 5 mio. features), with the same
threshold of at least five occurrences in the training
set. 2) Both Hogan and Charniak and Johnson use
special features for coordinate structures, such as a
Boolean feature for marking parallelism (Charniak
and Johnson) or for distinguishing between coor-
dination of base NPs and coordination of complex
conjuncts (Hogan), while our approach refrains
from such special-purpose features.
Our experiments using scope information are
similar to the approaches of Kurohashi and Na-
gao (1994) and Agarwal and Bogges (1992) in that
they try to identify coordinate structure bracket-
ings. However, the techniques used by Agarwal
and Bogges and in the present paper are quite dif-
ferent. Agarwal and Bogges and Kurohashi and
Nagao rely on shallow parsing techniques to de-
tect parallelism of conjuncts while we use a par-
tial parser only for suggesting possible scopes of
conjuncts. Both of these approaches are limited
to coordinate structures with two conjuncts only,
while our approach has no such limitation. More-
over, the goal of Agarwal and Bogges is quite dif-
ferent from ours. Their goal is robust detection of
coordinate structures only (with the intended ap-
412
plication of term extraction), while our goal is to
improve the performance of a parser that assigns a
complete sentence structure to an input sentence.
Finally, our approach at present is restricted to
purely syntactic structural properties. This is in
contrast to approaches that incorporate semantic
information. Hogan (2007) uses bi-lexical head-
head co-occurrences in order to identify nominal
heads of conjuncts more reliably than by syntactic
information alone. Chantree et al (2005) resolve
attachment ambiguities in coordinate structures, as
in (7a) and (7b), by using word frequency informa-
tion obtained from generic corpora as an effective
estimate of the semantic compatibility of a modi-
fier vis-a`-vis the candidate heads.
(7) a. Project managers and designers
b. Old shoes and boots
We view the work by Hogan and by Chantree
et al as largely complementary to, but at the same
time as quite compatible with our approach. We
must leave the integration of structural syntac-
tic and lexical semantic information to future re-
search.
9 Conclusion and Future Work
We have presented a study on improving the treat-
ment of coordinated structures in PCFG parsing.
While we presented experiments for German, the
methods are applicable for any language. We have
chosen German because it is a language with rel-
atively flexible phrasal ordering (cf. Section 2)
which makes parsing coordinations particularly
challenging. The experiments presented show that
n-best parsing combined with reranking improves
results by a large margin. However, the number
of cases in which the correct parse is present in
the n-best parses is rather low. This led us to the
assumption that the n-best analyses often do not
cover the whole range of different scope possibil-
ities but rather present minor variations of parses
with few differences in coordination scope. The
experiments in which the parser was forced to as-
sume predefined scopes show that the scope infor-
mation is important for parsing quality. Provid-
ing the parser with different scope possibilities and
reranking the resulting parses results in an increase
in F-score from 69.76 for the baseline to 74.69.
One of the major challenges for this approach lies
in extracting a list of possible conjuncts. Forc-
ing the parser to parse all possible sequences re-
sults in a prohibitively large number of possibili-
ties, especially for sentences with 3 or more con-
junctions. For this reason, we used chunks above
base phases, such as coordinated noun chunks, to
restrict the space. However, an inspection of the
lists of bracketed versions of the sentences shows
that the definition of base phrases is one of the ar-
eas that must be refined. As mentioned above, the
partial parser groups sequences of ?NP KON NP?
into a single base phrase. This may be correct in
many cases, but there are exceptions such as (8).
(8) Die
The
31ja?hrige
31-year-old
Gewerkschaftsmitarbei-
union staff member
terin und
and
ausgebildete
trained
Industriekauffrau
industrial clerk
aus
from
Oldenburg
Oldenburg
bereitet
is preparing
nun
now
ihre
her
erste
first
eigene
own
CD
CD
vor.
part..
For (8), the partial parser groups Die 31ja?hrige
Gewerkschaftsmitarbeiterin und ausgebildete In-
dustriekauffrau as one noun chunk. Since our
proposed conjuncts cannot cross these boundaries,
the correct second conjunct, ausgebildete Indus-
triekauffrau aus Oldenburg, cannot be suggested.
However, if we remove these chunk boundaries,
the number of possible conjuncts increases dra-
matically, and parsing times become prohibitive.
As a consequence, we will need to find a good bal-
ance between these two needs. Our plan is to in-
crease flexibility very selectively, for example by
enabling the use of wider scopes in cases where
the conjunction is preceded and followed by base
noun phrases. For the future, we are planning to
repeat experiment 3 (automatic scope) with differ-
ent phrasal boundaries extracted from the partial
parser. It will be interesting to see if improvements
in this experiment will still improve results in ex-
periment 4 (combining 50-best parses with exp. 3).
Another area of improvement is the list of fea-
tures used for reranking. At present, we use a fea-
ture set that is similar to the one used by Collins
and Koo (2005). However, this feature set does
not contain any coordination specific features. We
are planning to extend the feature set by features
on structural parallelism as well as on lexical sim-
ilarity of the conjunct heads.
413
References
Rajeev Agarwal and Lois Boggess. 1992. A simple but
useful approach to conjunct identification. In Pro-
ceedings of the 30th Annual Meeting of the Associa-
tion for Computational Linguistics (ACL-92), pages
15?21, Newark, DE.
Francis Chantree, Adam Kilgarriff, Anne de Roeck,
and Alistair Willis. 2005. Disambiguating coordi-
nations using word distribution information. In Pro-
ceedings of Recent Advances in NLP (RANLP 2005),
pages 144?151, Borovets, Bulgaria.
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-fine n-best parsing and MaxEnt discriminative
reranking. In Proceedings of the 43rd Annual Meet-
ing of the Association for Computational Linguistics
(ACL?05), pages 173?180, Ann Arbor, MI.
Michael Collins and Terry Koo. 2005. Discriminative
reranking for natural language parsing. Computa-
tional Linguistics, 31(1):25?69.
Michael Collins. 1999. Head-Driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania.
Anette Frank. 2002. A (discourse) functional analysis
of asymmetric coordination. In Proceedings of the
LFG-02 Conference, Athens, Greece.
Yoav Freund, Ray Iyer, Robert Shapire, and Yoram
Singer. 1998. An efficient boosting algorithm
for combining preferences. In Proceedings of the
15th International Conference on Machine Learn-
ing, Madison, WI.
Deirdre Hogan. 2007. Coordinate noun phrase disam-
biguation in a generative parsing model. In Proceed-
ings of the 45th Annual Meeting of the Association of
Computational Linguistics, pages 680?687, Prague,
Czech Republic.
Tilman Ho?hle. 1983. Subjektlu?cken in Koordinatio-
nen. Universita?t Tu?bingen.
Tilman Ho?hle. 1990. Assumptions about asymmetric
coordination in German. In Joan Mascaro? and Ma-
rina Nespor, editors, Grammar in Progress. Glow
Essays for Henk van Riemsdijk, pages 221?235.
Foris, Dordrecht.
Tilman Ho?hle. 1991. On reconstruction and coor-
dination. In Hubert Haider and Klaus Netter, ed-
itors, Representation and Derivation in the The-
ory of Grammar, volume 22 of Studies in Natural
Language and Linguistic Theory, pages 139?197.
Kluwer, Dordrecht.
Andreas Kathol. 1990. Linearization vs. phrase struc-
ture in German coordination constructions. Cogni-
tive Linguistics, 10(4):303?342.
Sandra Ku?bler. 2008. The PaGe 2008 shared task on
parsing German. In Proceedings of the ACL Work-
shop on Parsing German, pages 55?63, Columbus,
Ohio.
Sadao Kurohashi and Makoto Nagao. 1994. A syn-
tactic analysis method of long Japanese sentences
based on the detection of conjunctive structures.
Computational Linguistics, 20(4):507?534.
Frank Henrik Mu?ller and Tylman Ule. 2002. Annotat-
ing topological fields and chunks?and revising POS
tags at the same time. In Proceedings of the 19th In-
ternational Conference on Computational Linguis-
tics, COLING?02, pages 695?701, Taipei, Taiwan.
Slav Petrov and Dan Klein. 2008. Parsing German
with latent variable grammars. In Proceedings of
the ACL Workshop on Parsing German, pages 33?
39, Columbus, Ohio.
Helmut Schmid. 2000. LoPar: Design and implemen-
tation. Technical report, Universita?t Stuttgart.
Helmut Schmid. 2004. Efficient parsing of highly
ambiguous context-free grammars with bit vectors.
In Proceedings of the 20th International Confer-
ence on Computational Linguistics (COLING 2004),
Geneva, Switzerland.
Heike Telljohann, Erhard Hinrichs, and Sandra Ku?bler.
2004. The Tu?Ba-D/Z treebank: Annotating German
with a context-free backbone. In Proceedings of the
Fourth International Conference on Language Re-
sources and Evaluation (LREC 2004), pages 2229?
2235, Lisbon, Portugal.
Heike Telljohann, Erhard W. Hinrichs, Sandra Ku?bler,
and Heike Zinsmeister, 2005. Stylebook for the
Tu?bingen Treebank of Written German (Tu?Ba-
D/Z). Seminar fu?r Sprachwissenschaft, Universita?t
Tu?bingen, Tu?bingen, Germany.
Dieter Wunderlich. 1988. Some problems of coor-
dination in German. In Uwe Reyle and Christian
Rohrer, editors, Natural Language Parsing and Lin-
guistic Theories, Studies in Linguistics and Philoso-
phy, pages 289?316. Reidel, Dordrecht.
414
Proceedings of the ACL-08: HLT Workshop on Parsing German (PaGe-08), pages 55?63,
Columbus, Ohio, USA, June 2008. c?2008 Association for Computational Linguistics
The PaGe 2008 Shared Task on Parsing German?
Sandra Ku?bler
Department of Linguistics
Indiana University
Bloomington, IN, USA
skuebler@indiana.edu
Abstract
The ACL 2008 Workshop on Parsing German
features a shared task on parsing German. The
goal of the shared task was to find reasons
for the radically different behavior of parsers
on the different treebanks and between con-
stituent and dependency representations. In
this paper, we describe the task and the data
sets. In addition, we provide an overview of
the test results and a first analysis.
1 Introduction
German is one of the very few languages for which
more than one syntactically annotated resource ex-
ists. Other languages for which this is the case in-
clude English (with the Penn treebank (Marcus et
al., 1993), the Susanne Corpus (Sampson, 1993),
and the British section of the ICE Corpus (Wallis
and Nelson, 2006)) and Italian (with ISST (Mon-
tegmagni et al, 2000) and TUT (Bosco et al,
2000)). The three German treebanks are Negra
(Skut et al, 1998), TIGER (Brants et al, 2002), and
Tu?Ba-D/Z (Hinrichs et al, 2004). We will concen-
trate on TIGER and Tu?Ba-D/Z here; Negra is an-
notated with an annotation scheme very similar to
TIGER but is smaller. In contrast to other languages,
these two treebanks are similar on many levels:
Both treebanks are based on newspaper text, both
use the STTS part of speech (POS) tagset (Thie-
len and Schiller, 1994), and both use an annotation
?I am very grateful to Gerald Penn, who suggested this
workshop and the shared task, took over the biggest part of the
workshop organization and helped with the shared task.
scheme based on constituent structure augmented
with grammatical functions. However, they differ in
the choices made in the annotation schemes, which
makes them ideally suited for an investigation of
how these decisions influence parsing accuracy in
different parsers.
On a different level, German is an interesting
language for parsing because of the syntactic phe-
nomena in which the language differs from English,
the undoubtedly most studied language in parsing:
German is often listed as a non-configurational lan-
guage. However, while the word order is freer
than in English, the language exhibits a less flexible
word order than more typical non-configurational
languages. A short overview of German word order
phenomena is given in section 2.
The structure of this paper is as follows: Section
2 discusses three characteristics of German word or-
der, section 3 provides a definition of the shared task,
and section 4 gives a short overview of the treebanks
and their annotation schemes that were used in the
shared task. In section 5, we give an overview of the
participating systems and their results.
2 German Word Order
In German, the order of non-verbal phrases is rela-
tively free, but the placement of the verbal elements
is determined by the clause type. Thus, we will
first describe the placement of the finite verb, then
we will explain phrasal ordering, and finally we will
look at discontinuous constituents.
55
2.1 Verb Placement
In German, the clause type determines the place-
ment of finite verbs: In non-embedded declarative
clauses, as in (1a), the finite verb is in second posi-
tion (V2). In yes/no questions, as in (1b), the finite
verb is the clause-initial constituent (V1), and in em-
bedded clauses, as in (1c), it appears clause finally
(Vn).
(1) a. Der
The
Mann
man
hat
has
das
the
Auto
car
gekauft.
bought
?The man has bought the car.?
b. Hat
Has
der
the
Mann
man
das
the
Auto
car
gekauft?
bought
?Has the man bought the car??
a. dass
that
der
the
Mann
man
das
the
Auto
car
gekauft
bought
hat.
has
?. . . that the man has bought the car.?
All non-finite verbs appear at the right periphery
of the clause (cf. 2), independently of the clause
type.
(2) Der
The
Mann
man
sollte
should
das
the
Auto
car
gekauft
bought
haben.
have
?The man should have bought the car.?
2.2 Flexible Phrase Ordering
Apart from the fixed placement of the verbs, the or-
der of the non-verbal elements is flexible. In (3), any
of the four complements and adjuncts of the main
verb (ge)geben can be in sentence-initial position,
depending on the information structure of the sen-
tence.
(3) a. Das
The
Kind
child
hat
has
dem
the
Mann
man
gestern
yesterday
den
the
Ball
ball
gegeben.
given
?The child has given the ball to the man yes-
terday.?
b. Dem Mann hat das Kind gestern den Ball
gegeben.
c. Gestern hat das Kind dem Mann den Ball
gegeben.
d. Den Ball hat das Kind gestern dem Mann
gegeben.
In addition, the ordering of the elements that oc-
cur between the finite and the non-finite verb forms
is also free so that there are six possible lineariza-
tions for each of the examples in (3a-d).
One exception to the free ordering of non-verbal
elements is the ordering of pronouns. If the pro-
nouns appear to the right of the finite verb in V1
and V2 clauses, they are adjacent to the finite verb
in fixed order.
(4) Gestern
Yesterday
hat
has
sie
she
sie
her/them
ihm
him
gegeben.
given.
?Yesterday, she gave her/them to him.?
In (4), three pronouns are present. Although
the pronoun sie is ambiguous between nomina-
tive/accusative singular and nominative/accusative
plural, the given example is unambiguous with re-
spect to case since the nominative precedes the ac-
cusative, which in turn precedes the dative.
Due to the flexible phrase ordering, the grammat-
ical functions of constituents in German, unlike in
English, cannot be deduced from the constituents?
location in the constituent tree. As a consequence,
parsing approaches to German need to be based on
treebank data which contain a combination of con-
stituent structure and grammatical functions ? for
parsing and evaluation. For English, in contrast,
grammatical functions are often used internally in
parsers but suppressed in the final parser output.
2.3 Discontinuous Constituents
Another characteristic of German word order is the
frequency of discontinuous constituents. The sen-
tence in (5) shows an extraposed relative clause that
is separated from its head noun das Buch by the non-
finite verb gelesen.
(5) Der
The
Mann
man
hat
has
das
the
Buch
book
gelesen,
read,
das
which
ich
I
ihm
him
empfohlen
recommended
habe.
have
?The man read the book that I recommended to
him.?
56
In German, it is also possible to partially front
VPs, such as in sentence (6). This sentence is taken
from the Tu?Ba-D/Z treebank.
(6) Fu?r
For
den
the
Berliner
Berlin
Job
job
qualifiziert
qualified
hat
has
sich
himself
Zimmermann
Zimmermann
auch
also
durch
by
seinen
his
Blick
view
fu?rs
for the
finanziell
financially
Machbare.
doable
?Zimmermann qualified for the job in Berlin
partially because of his view for what is finan-
cially feasible.?
Here, the canonical word order would be Zimmer-
mann hat sich auch durch seinen Blick f?urs finanziell
Machbare fu?r den Berliner Job qualifiziert.
Such discontinuous structures occur frequently in
the TIGER and Tu?Ba-D/Z treebanks and are handled
differently in the two annotation schemes, as will be
discussed in more detail in section 4.
3 Task Definition
In this section, we give the definition of the shared
task. We provided two subtasks: parsing constituent
structure and parsing the dependency representa-
tions. Both subtasks involved training and testing on
data from the two treebanks, TIGER and Tu?Ba-D/Z.
The dependency format was derived from the con-
stituent format so that the sentences were identical
in the two versions. The participants were given
training sets, development sets, and test sets of the
two treebanks. The training sets contained 20894
sentences per treebank, the development and test
set consisted of 2611 sentences each. The test sets
contained gold standard POS labels. In these sets,
sentence length was restricted to a maximum of 40
words. Since for some sentences in both treebanks,
the annotation consists of more than one tree, all
trees were joined under a virtual root node, VROOT.
Since some parsers cannot assign grammatical
functions to part of speech tags, these grammati-
cal functions were provided for the test data as at-
tached to the POS tags. Participants were asked to
perform a test without these functions if their parser
was equipped to provide them. Two participants did
submit these results, and in both cases, these results
were considerably lower.
Evaluation for the constituent version consisted
of the PARSEVAL measures precision, recall, and
F1 measure. All these measures were calculated on
combinations of constituent labels and grammatical
functions. Part of speech labels were not considered
in the evaluation. Evaluation for the dependency
version consisted of labeled and unlabeled attach-
ment scores. For this evaluation, we used the scripts
provided by the CoNLL shared task 2007 on depen-
dency parsing (Nivre et al, 2007).
4 The Treebanks
The two treebanks used for the shared task were
the TIGER Corpus, (Brants et al, 2002) version
2, and the Tu?Ba-D/Z treebank (Hinrichs et al,
2004; Telljohann et al, 2006), version 3. Both
treebanks use German newspapers as their data
source: the Frankfurter Rundschau newspaper for
TIGER and the ?die tageszeitung? (taz) newspaper
for Tu?Ba-D/Z. The average sentence length is
very similar: In TIGER, sentences have an average
length of 17.0, and in Tu?Ba-D/Z, 17.3. This can
be regarded as an indication that the complexity of
the two texts is comparable. Both treebanks use
the same POS tagset, STTS (Thielen and Schiller,
1994), and annotations based on phrase structure
grammar, enhanced by a level of predicate-argument
structure.
4.1 The Constituent Data
Despite all the similarities presented above, the
constituent annotations differ in four important as-
pects: 1) TIGER does not allow for unary branch-
ing whereas Tu?Ba-D/Z does; 2) in TIGER, phrase
internal annotation is flat whereas Tu?Ba-D/Z uses
phrase internal structure; 3) TIGER uses crossing
branches to represent long-distance relationships
whereas Tu?Ba-D/Z uses a pure tree structure com-
bined with functional labels to encode this informa-
tion. The two treebanks also use different notions of
grammatical functions: Tu?Ba-D/Z defines 36 gram-
matical functions covering head and non-head in-
formation, as well as subcategorization for comple-
ments and modifiers. TIGER utilizes 51 grammati-
cal functions. Apart from commonly accepted gram-
matical functions, such as SB (subject) or OA (ac-
cusative object), TIGER grammatical functions in-
57
Figure 1: TIGER annotation with crossing branches.
Figure 2: TIGER annotation with resolved crossing branches.
clude others, e.g. RE (repeated element) or RC (rel-
ative clause).
(7) Beim
At the
Mu?nchner
Munich
Gipfel
Summit
ist
is
die
the
sprichwo?rtliche
proverbial
bayerische
Bavarian
Gemu?tlichkeit
?Gemu?tlichkeit?
von
by
einem
a
Bild
picture
verdra?ngt
supplanted
worden,
been,
das
which
im
in the
Wortsinne
literal sense
an
of
einen
a
Polizeistaat
police state
erinnert.
reminds
?At the Munich Summit, the proverbial Bavar-
ian ?Gemu?tlichkeit? was supplanted by an im-
age that is evocative of a police state.?
Figure 1 shows a typical tree from the TIGER
treebank for sentence (7). The syntactic categories
are shown in circular nodes, the grammatical func-
tions as edge labels in square boxes. A major
phrasal category that serves to structure the sen-
tence as a whole is the verb phrase (VP). It con-
tains non-finite verbs (here: verdra?ngt worden) as
well as their complements and adjuncts. The subject
NP (die sprichwo?rtliche bayerische Gemu?tlichkeit)
is outside the VP and, depending on its linear po-
sition, leads to crossing branches with the VP. This
happens in all cases where the subject follows the
finite verb as in Figure 1. Notice also that the PPs
are completely flat. An additional crossing branch
results from the direct attachment of the extraposed
relative clause (the lower S node with function RC)
to the noun that it modifies.
As mentioned in the previous section, TIGER
trees must be transformed into trees without crossing
branches prior to training PCFG parsers. The stan-
dard approach for this transformation is to re-attach
crossing non-head constituents as sisters of the low-
est mother node that dominates all the crossing con-
stituent and its sister nodes in the original TIGER
tree. Figure 2 shows the result of this transformation
58
Figure 3: Tu?Ba-D/Z annotation without crossing branches.
of the tree in Figure 1. Crossing branches not only
arise with respect to the subject at the sentence level
but also in cases of extraposition and fronting of par-
tial constituents. As a result, approximately 30% of
all TIGER trees contain at least one crossing branch.
Thus, tree transformations have a major impact on
the type of constituent structures that are used for
training probabilistic parsing models.
Figure 3 shows the Tu?Ba-D/Z annotation for sen-
tence (8), a sentence with a very similar structure to
the TIGER sentence shown in Figure 1. Crossing
branches are avoided by the introduction of topo-
logical structures (here: VF, LK, MF, VC, NF, and
C) into the tree. Notice also that compared to the
TIGER annotation, Tu?Ba-D/Z introduces more inter-
nal structure into NPs and PPs. In Tu?Ba-D/Z, long-
distance relationships are represented by a pure tree
structure and specific functional labels. Thus, the
extraposed relative clause is attached to the matrix
clause directly, but its functional label ON-MOD ex-
plicates that it modifies the subject ON.
(8) In
In
Bremen
Bremen
sind
are
bisher
so far
nur
only
Fakten
facts
geschaffen
produced
worden,
been,
die
which
jeder
any
modernen
modern
Stadtplanung
city planning
entgegenstehen.
contradict
?In Bremen, so far only such attempts have
been made that are opposed to any modern city
planning.?
4.2 The Dependency Data
The constituent representations from both treebanks
were converted into dependencies. The conver-
sion aimed at finding dependency representations
for both treebanks that are as similar to each other
as possible. Complete identity is impossible be-
cause the treebanks contain different levels of dis-
tinction for different phenomena. The conversion is
based on the original formats of the treebanks in-
cluding crossing branches. The target dependency
format was defined based on the dependency gram-
mar by Foth (2003). For the conversion, we used
pre-existing dependency converters for TIGER trees
(Daum et al, 2004) and for Tu?Ba-D/Z trees (Vers-
ley, 2005). The dependency representations of the
trees in Figures 1 and 3 are shown in Figures 4 and
5. Note that the long-distance relationships are con-
verted into non-projective dependencies.
5 Submissions and Results
The shared task drew submissions from 3 groups:
the Berkeley group, the Stanford group, and the
Va?xjo? group. Four more groups or individuals had
registered but did not submit any data. The submit-
ted systems and results are described in detail in pa-
pers in this volume (Petrov and Klein, 2008; Raf-
ferty and Manning, 2008; Hall and Nivre, 2008). All
three systems submitted results for the constituent
task. For the dependency task, the Va?xjo? group had
the only submission. For this reason, we will con-
centrate on the analysis of the constituent results and
will mention the dependency results only shortly.
59
Beim M. Gipfel ist die sprichw. bayer. Gem. von einem Bild verdra?ngt worden, das im Worts. an einen P.staat erinnert.
PP
ATTR
PN
DET
ATTR
ATTR
SUBJ
PP
DET
PN
AUX
AUX SUBJ
PP
PN
OBJP
DET
PN
REL
Figure 4: TIGER dependency annotation.
In Bremen sind bisher nur Fakten geschaffen worden, die jeder modernen Stadtplanung entgegenstehen.
PN
PP ADV
ADV
SUBJ
AUX
AUX
SUBJ
DET
ATTR OBJD
REL
Figure 5: Tu?Ba-D/Z dependency annotation.
5.1 Constituent Evaluation
The results of the constituent analysis are shown
in Table 1. The evaluation was performed with re-
gard to labels consisting of a combination of syn-
tactic labels and grammatical functions. A subject
noun phrase, for example, is only counted as correct
if it has the correct yield, the correct label (i.e. NP
for TIGER and NX for Tu?Ba-D/Z), and the correct
grammatical function (i.e. SB for TIGER and ON
for Tu?Ba-D/Z). The results show that the Berke-
ley parser reaches the best results for both treebanks.
The other two parsers compete for second place. For
TIGER, the Va?xjo? parser outperforms the Stanford
parser, but for Tu?Ba-D/Z, the situation is reversed.
This gives an indication that the Va?xjo? parser seems
better suited for the flat annotations in TIGER while
the Stanford parser is better suited for the more hier-
archical structure in Tu?Ba-D/Z. Note that all parsers
reach much higher F-scores for Tu?Ba-D/Z.
A comparison of howwell suited two different an-
notation schemes are for parsing is a surprisingly
difficult task. A first approach would be to com-
pare the parser performance for specific categories,
such as for noun phrases, etc. However, this is
not possible for TIGER and Tu?Ba-D/Z. On the one
hand, the range of phenomena described as noun
phrases, for example, is different in the two tree-
banks. The most obvious difference in annotation
schemes is that Tu?Ba-D/Z annotates unary branch-
ing structures while TIGER does not. As a conse-
quence, in Tu?Ba-D/Z, all pronouns and substitut-
ing demonstratives are annotated as noun phrases; in
TIGER, they are attached directly to the next higher
node (cf. the relative pronouns, POS tag PRELS, in
Figures 1 and 3). Ku?bler (2005) and Maier (2006)
suggest a method for comparing such different an-
notation schemes by approximating them stepwise
so that the decisions which result in major changes
can be isolated. They come to the conclusion that
the differences between the two annotation schemes
is a least partially due to inconsistencies introduced
into TIGER style annotations during the resolution
of crossing branches. However, even this method
cannot give any indication which annotation scheme
provides more useful information for systems that
use such parses as input. To answer this question, an
in vivo evaluation would be necessary. It is, how-
ever, rather difficult to find systems into which a
parser can be plugged in without too many modi-
fications of the system.
On the other hand, it is a well-known fact that
60
TIGER Tu?Ba-D/Z
system precision recall F-score precision recall F-score
Berkeley 69.23 70.41 69.81 83.91 84.04 83.97
Stanford 58.52 57.63 58.07 79.26 79.22 79.24
Va?xjo? 67.06 63.40 65.18 76.44 74.79 75.60
Table 1: The results of the constituent parsing task.
TIGER Tu?Ba-D/Z
system GF precision recall F-score precision recall F-score
Berkeley SB/ON 74.46 78.31 76.34 78.33 77.08 77.70
OA 60.08 66.61 63.18 58.11 65.81 61.72
DA/OD 49.28 41.72 43.19 59.46 44.72 51.05
Stanford SB/ON 64.40 63.11 63.75 71.16 77.76 74.31
OA 45.52 45.91 45.71 47.23 51.28 49.17
DA/OD 12.40 9.82 10.96 24.42 8.54 12.65
Va?xjo? SB/ON 75.33 73.00 74.15 72.37 69.53 70.92
OA 57.01 57.65 57.33 58.07 57.55 57.81
DA/OD 55.45 37.42 44.68 63.75 20.73 31.29
Table 2: The results for subjects, accusative objects, and dative objects.
the PARSEVALmeasures favor annotation schemes
with hierarchical structures, such as in Tu?Ba-D/Z,
in comparison to annotation schemes with flat struc-
tures (Rehbein and van Genabith, 2007). Here,
TIGER and Tu?Ba-D/Z differ significantly: in TIGER,
phrases receive a flat annotation. Prepositional
phrases, for example, do not contain an explicitly
annotated noun phrase. Tu?Ba-D/Z phrases, in con-
trast, are more hierarchical; preposition phrases do
contain a noun phrase, and non phrases distinguish
between pre- and post-modification. For this reason,
the evaluation presented in Table 1 must be taken
with more than a grain of salt as a comparison of an-
notation schemes. However, it seems safe to follow
Ku?bler et al (Ku?bler et al, 2006) in the assump-
tion that the major grammatical functions, subject
(SB/ON), accusative object (OA), and dative object
(DA/OD) are comparable. Again, this is not com-
pletely true because in the case of one-word NPs,
these functions are attached to the POS tags and
thus are given in the input. Another solution, which
was pursued by Rehbein and van Genabith (2007),
is the introduction of new unary branching nodes in
the tree in cases where such grammatical functions
are originally attached to the POS tag. We refrained
from using this solution because it introduces fur-
ther inconsistencies (only a subset of unary branch-
ing nodes are explicitly annotated), which make it
difficult for a parser to decide whether to group such
phrases or not. The evaluation shown in Table 2 is
based on all nodes which were annotated with the
grammatical function in question.
The results presented in Table 2 show that the
differences between the two treebanks are incon-
clusive. While the Stanford parser performs con-
sistently better on Tu?Ba-D/Z, the Berkeley parser
handles accusative objects better in TIGER, and the
Va?xjo? parser subjects and dative objects. The results
indicate that the Berkeley parser profits from the
TIGER annotation of accusative objects, which are
grouped in the verb phrase while Tu?Ba-D/Z groups
all objects in their fields directly without resorting to
a verb phrase. However, this does not explain why
the Berkeley parser cannot profit from the subject
attachment on the clause level in TIGER to the same
degree.
5.2 Dependency Evaluation
The results of the dependency evaluation for the
Va?xjo? system are shown in Table 3. The results are
61
TIGER Tu?Ba-D/Z
UAS 92.63 91.45
LAS 90.80 88.64
precision recall precision recall
SUBJ 90.20 89.82 88.99 88.55
OBJA 77.93 82.19 77.18 82.71
OBJD 57.00 44.02 67.88 45.90
Table 3: The results of the dependency evaluation.
important for the comparison of constituent and de-
pendency parsing since in the conversion to depen-
dencies, most of the differences between the anno-
tation schemes, and as a consequence, the prefer-
ence of the PARSEVAL measures have been neu-
tralized. Therefore, it is interesting to see that the
results for TIGER are slightly better than the results
for Tu?Ba-D/Z, both for unlabeled (UAS) and la-
beled attachment scores. The reasons for these dif-
ferences are unclear: either the TIGER texts are eas-
ier to parse, or the (original annotation and) conver-
sion from TIGER is more consistent. Another sur-
prising fact is that the dependency results are clearly
better than the constituent ones. This is partly due
to the fact that the dependency representation is of-
ten less informative than then constituent representa-
tion. One example for this can be found in coordina-
tions: In dependency representations, the scope am-
biguity in phrases like young men and women is not
resolved. This gives parsers fewer opportunities to
go wrong. However, this cannot explain all the dif-
ferences. Especially the better performance on the
major grammatical functions cannot be explained in
this way.
A closer look at the grammatical functions shows
that here, precision and recall are higher than for
constituent parses. This is a first indication that de-
pendency representation may be more appropriate
for languages with freer word order. A compari-
son between the two treebanks is inconclusive: for
the accusative object, the results are similar between
the treebanks. For subjects, the results for TIGER
are better while for dative objects, the results for
Tu?Ba-D/Z are better. This issue requires closer in-
vestigation.
6 Conclusion
This is the first shared task on parsing German,
which provides training and test sets from both ma-
jor treebanks for German, TIGER and Tu?Ba-D/Z.
For both treebanks, we provided a constituent and a
dependency representation. It is our hope that these
data sets will spark more interest in the comparison
of different annotation schemes and their influence
on parsing results. The evaluation of the three par-
ticipating systems has shown that for both treebanks,
the use of a latent variable grammar in the Berkeley
system is beneficial. However, many questions re-
main unanswered and require further investigation:
To what extent do the evaluation metrics distort the
results? Does a measure exist that is neutral towards
the differences in annotation? Is the dependency for-
mat better suited for parsing German? Are the dif-
ferences between the dependency results of the two
treebanks indicators that TIGER provides more im-
portant information for dependency parsing? Or can
the differences be traced back to the conversion al-
gorithms?
Acknowledgments
First and foremost, we want to thank all the people
and organizations that generously provided us with
treebank data and without whom the shared task
would have been literally impossible: Erhard Hin-
richs, University of Tu?bingen (Tu?Ba-D/Z), and Hans
Uszkoreit, Saarland University and DFKI (TIGER).
Secondly, we would like to thank Wolfgang Maier
and Yannick Versley who performed the data con-
versions necessary for the shared task. Additionally,
Wolfgang provided the scripts for the constituent
evaluation.
References
Cristina Bosco, Vincenzo Lombardo, D. Vassallo, and
Leonardo Lesmo. 2000. Building a treebank for Ital-
ian: a data-driven annotation scheme. In Proceedings
of the 2nd International Conference on Language Re-
sources and Evaluation, LREC-2000, Athens, Greece.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolf-
gang Lezius, and George Smith. 2002. The TIGER
treebank. In Erhard Hinrichs and Kiril Simov, edi-
tors, Proceedings of the First Workshop on Treebanks
62
and Linguistic Theories (TLT 2002), pages 24?41, So-
zopol, Bulgaria.
Michael Daum, Kilian Foth, and Wolfgang Menzel.
2004. Automatic transformation of phrase treebanks
to dependency trees. In Proceedings of the 4th In-
ternational Conference on Language Resources and
Evaluation, LREC-2004, Lisbon, Portugal.
Kilian Foth. 2003. Eine umfassende Dependenzgram-
matik des Deutschen. Technical report, Fachbereich
Informatik, Universita?t Hamburg.
Johan Hall and Joakim Nivre. 2008. A dependency-
driven parser for German dependency and con-
stituency representations. In Proceedings of the ACL
Workshop on Parsing German, Columbus, OH.
Erhard Hinrichs, Sandra Ku?bler, Karin Naumann, Heike
Telljohann, and Julia Trushkina. 2004. Recent de-
velopments in linguistic annotations of the Tu?Ba-D/Z
treebank. In Proceedings of the Third Workshop
on Treebanks and Linguistic Theories, pages 51?62,
Tu?bingen, Germany.
Sandra Ku?bler, ErhardW. Hinrichs, andWolfgangMaier.
2006. Is it really that difficult to parse German?
In Proceedings of the 2006 Conference on Empiri-
cal Methods in Natural Language Processing, EMNLP
2006, pages 111?119, Sydney, Australia.
Sandra Ku?bler. 2005. How do treebank annotation
schemes influence parsing results? Or how not to com-
pare apples and oranges. In Proceedings of the Inter-
national Conference on Recent Advances in Natural
Language Processing, RANLP 2005, pages 293?300,
Borovets, Bulgaria.
WolfgangMaier. 2006. Annotation schemes and their in-
fluence on parsing results. In Proceedings of the ACL-
2006 Student Research Workshop, Sydney, Australia.
Mitchell Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19(2):313?330.
S. Montegmagni, F. Barsotti, M. Battista, N. Calzolari,
O. Corazzari, A. Zampolli, F. Fanciulli, M. Massetani,
R. Raffaelli, R. Basili, M. T. Pazienza, D. Saracino,
F. Zanzotto, N. Mana, F. Pianesi, and R. Delmonte.
2000. The Italian syntactic-semantic treebank: Ar-
chitecture, annotation, tools and evaluation. In Pro-
ceedings of the Workshop on Linguistically Interpreted
Corpora LINC-2000, pages 18?27, Luxembourg.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan McDon-
ald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret.
2007. The CoNLL 2007 shared task on dependency
parsing. In Proceedings of the CoNLL 2007 Shared
Task. Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natural
Language Learning, EMNLP-CoNLL 2007, Prague,
Czech Republic.
Slav Petrov and Dan Klein. 2008. Parsing German with
language agnostic latent variable grammars. In Pro-
ceedings of the ACL Workshop on Parsing German,
Columbus, OH.
Anna Rafferty and Christopher Manning. 2008. Parsing
three German treebanks: Lexicalized and unlexical-
ized baselines. In Proceedings of the ACL Workshop
on Parsing German, Columbus, OH.
Ines Rehbein and Josef van Genabith. 2007. Treebank
annotation schemes and parser evaluation for German.
In Proceedings of the 2007 Joint Conference on Em-
pirical Methods in Natural Language Processing and
Computational Natural Language Learning, EMNLP-
CoNLL, pages 630?639, Prague, Czech Republic.
Geoffrey Sampson. 1993. The SUSANNE corpus.
ICAME Journal, 17:125 ? 127.
Wojciech Skut, Thorsten Brants, Brigitte Krenn, and
Hans Uszkoreit. 1998. A linguistically interpreted
corpus of German newspaper texts. In ESSLLI
Workshop on Recent Advances in Corpus Annotation,
Saarbru?cken, Germany.
Heike Telljohann, Erhard W. Hinrichs, Sandra Ku?bler,
and Heike Zinsmeister, 2006. Stylebook for
the Tu?bingen Treebank of Written German (Tu?Ba-
D/Z). Seminar fu?r Sprachwissenschaft, Universita?t
Tu?bingen, Germany.
Christine Thielen and Anne Schiller. 1994. Ein kleines
und erweitertes Tagset fu?rs Deutsche. In Helmut Feld-
weg and Erhard Hinrichs, editors, Lexikon & Text,
pages 215?226. Niemeyer, Tu?bingen.
Yannick Versley. 2005. Parser evaluation across text
types. In Proceedings of the Fourth Workshop on Tree-
banks and Linguistic Theories, TLT 2005, pages 209?
220, Barcelona, Spain.
Sean Wallis and Gerald Nelson. 2006. The British com-
ponent of the International Corpus of English. Release
2. CD-ROM. London: Survey of English Usage, UCL.
63
Tu?SBL: A Similarity-Based Chunk Parser
for Robust Syntactic Processing
Sandra Ku?bler
Seminar fu?r Sprachwissenschaft
University of Tu?bingen
Wilhelmstr. 113
D-72074 Tu?bingen, Germany
kuebler@sfs.nphil.uni-tuebingen.de
Erhard W. Hinrichs
Seminar fu?r Sprachwissenschaft
University of Tu?bingen
Wilhelmstr. 113
D-72074 Tu?bingen, Germany
eh@sfs.nphil.uni-tuebingen.de
ABSTRACT
Chunk parsing has focused on the recognition of partial constituent
structures at the level of individual chunks. Little attention has been
paid to the question of how such partial analyses can be combined
into larger structures for complete utterances.
The Tu?SBL parser extends current chunk parsing techniques by
a tree-construction component that extends partial chunk parses to
complete tree structures including recursive phrase structure as well
as function-argument structure. Tu?SBL?s tree construction algo-
rithm relies on techniques from memory-based learning that allow
similarity-based classification of a given input structure relative to
a pre-stored set of tree instances from a fully annotated treebank.
A quantitative evaluation of Tu?SBL has been conducted using
a semi-automatically constructed treebank of German that consists
of appr. 67,000 fully annotated sentences. The basic PARSEVAL
measures were used although they were developed for parsers that
have as their main goal a complete analysis that spans the entire in-
put. This runs counter to the basic philosophy underlying Tu?SBL,
which has as its main goal robustness of partially analyzed struc-
tures.
Keywords
robust parsing, chunk parsing, similarity-based learning
1. INTRODUCTION
Current research on natural language parsing tends to gravitate
toward one of two extremes: robust, partial parsing with the goal
of broad data coverage versus more traditional parsers that aim at
complete analysis for a narrowly defined set of data. Chunk pars-
ing [1, 2] offers a particularly promising and by now widely used
example of the former kind. The main insight that underlies the
chunk parsing strategy is to isolate the (finite-state) analysis of non-
recursive, syntactic structure, i.e. chunks, from larger, recursive
structures. This results in a highly-efficient parsing architecture
that is realized as a cascade of finite-state transducers and that pur-
.
sues a longest-match, right-most pattern-matching strategy at each
level of analysis.
Despite the popularity of the chunk parsing approach, there seem
to be two apparent gaps in current research:
1. Chunk parsing research has focused on the recognition of
partial constituent structures at the level of individual chunks.
By comparison, little or no attention has been paid to the
question of how such partial analyses can be combined into
larger structures for complete utterances.
2. Relatively little has been reported on quantitative evaluations
of chunk parsers that measure the correctness of the output
structures obtained by a chunk parser.
The main goal of the present paper is help close those two re-
search gaps.
2. THE T ?USBL ARCHITECTURE
In order to ensure a robust and efficient architecture, Tu?SBL, a
similarity-based chunk parser, is organized in a three-level archi-
tecture, with the output of each level serving as input for the next
higher level. The first level is part-of-speech (POS) tagging of the
input string with the help of the bigram tagger LIKELY [10].1 The
parts of speech serve as pre-terminal elements for the next step,
i.e. the chunk analysis. Chunk parsing is carried out by an adapted
version of Abney?s [2] scol parser, which is realized as a cascade
of finite-state transducers. The chunks, which extend if possible to
the simplex clause level, are then remodeled into complete trees in
the tree construction level.
The tree construction is similar to the DOP approach [3, 4] in that
it uses complete tree structures instead of rules. Contrary to Bod,
we do not make use of probabilities and do not allow tree cuts,
instead we only use the complete trees and minimal tree modifica-
tions. Thus the number of possible combinations of partial trees
is strictly controlled. The resulting parser is highly efficient (3770
English sentences took 106.5 seconds to parse on an Ultra Sparc
10).
3. CHUNK PARSING AND TREE CONSTRUC-
TION
The division of labor between the chunking and tree construction
modules can best be illustrated by an example.
1The inventory of POS tags is based on the Stuttgart-Tu?bingen
Tagset (STTS) [11].
0 1 2 3 4 5 6 7 8 9 10 11 12 13
500501502
503
504505
506
507
508509
510
511
512
513
514
515
516
517
dann
ADV
w"urde
VAFIN
ich
PPER
vielleicht
ADV
noch
ADV
vorschlagen
VVINF
Donnerstag
NN
den
ART
elften
NN
und
KON
Freitag
NN
den
ART
zw"olften
ADJA
August
NN
HDHDHD
VXINF
OV
HDHD
VXFIN
HD
? HD
NX
HD APP
ADVX
MOD
HD
NX ADVX ADVX
ON MOD MOD
HD
ADJX
? ? HD
NX
HD APP
NX
NX
? ? ?
NX
OA
VF LK MF VC
NF
SIMPX
? ? ? ? ?
Figure 2: Sample tree construction output
Input:
dann w?urde ich vielleicht noch vorschlagen Donnerstag den elften
und Freitag den zw?olften August
(then I would suggest maybe Thursday eleventh and Friday twelfth
of August)
Chunk parser output:
[simpx [advx [adv dann]]
[vxfin [vafin w"urde]]
[nx2 [pper ich]]
[advx [adv vielleicht]]
[advx [advmd noch]]
[vvinf vorschlagen]]
[nx3 [day Donnerstag]
[art den]
[adja elften]]
[kon und]
[nx3 [day Freitag]
[art den]
[adja zw"olften]
[month August]]
Figure 1: Chunk parser output
For complex sentences such as the German input dann w?urde
ich vielleicht noch vorschlagen Donnerstag den elften und Fre-
itag den zw?olften August (then I would suggest maybe Thursday
eleventh and Friday twelfth of August), the chunker produces a
structure in which some constituents remain unattached or partially
annotated in keeping with the chunk-parsing strategy to factor out
recursion and to resolve only unambigous attachments, as shown in
Fig. 1.
In the case at hand, the subconstituents of the extraposed co-
ordinated noun phrase are not attached to the simplex clause that
ends with the non-finite verb that is typically in clause-final posi-
tion in declarative main clauses of German. Moreover, each con-
junct of the coordinated noun phrase forms a completely flat struc-
ture. Tu?SBL?s tree construction module enriches the chunk output
as shown in Fig. 22. Here the internally recursive NP conjuncts
have been coordinated and integrated correctly into the clause as a
whole. In addition, function labels such as mod (for: modifier), hd
(for: head), on (for: subject), oa (for: direct object), and ov (for:
verbal object) have been added that encode the function-argument
structure of the sentence.
4. SIMILARITY-BASED TREE CONSTRUC-
TION
The tree construction algorithm is based on the machine learning
paradigm of memory-based learning [12].3 Memory-based learn-
ing assumes that the classification of a given input should be based
on the similarity to previously seen instances of the same type that
have been stored in memory. This paradigm is an instance of lazy
learning in the sense that these previously encountered instances
are stored ?as is? and are crucially not abstracted over, as is typi-
cally the case in rule-based systems or other learning approaches.
Past applications of memory-based learning to NLP tasks consist
of classification problems in which the set of classes to be learnt
is simple in the sense that the class items do not have any internal
structure and the number of distinct items is small.
The use of a memory-based approach for parsing implies that
parsing needs to be redefined as a classification task. There are two
fundamentally different, possible approaches: the one is to split
parsing up into different subtasks, that is, one needs separate clas-
sifiers for each functional category and for each level in a recur-
sive structure. Since the classifiers for the functional categories as
well as the individual decisions of the classifiers are independent,
multiple or no candidates for a specific grammatical function or
constituents with several possible functions may be found so that
an additional classifier is needed for selecting the most appropriate
assignment (cf. [6]).
The second approach, which we have chosen, is to regard the
complete parse trees as classes so that the task is defined as the
selection of the most similar tree from the instance base. Since in
2All trees in this contribution follow the data format for trees de-
fined by the NEGRA project of the Sonderforschungsbereich 378
at the University of the Saarland, Saarbru?cken. They were printed
by the NEGRA annotation tool [5].
3Memory-based learning has recently been applied to a variety of
NLP classification tasks, including part-of-speech tagging, noun
phrase chunking, grapheme-phoneme conversion, word sense dis-
ambiguation, and pp attachment (see [9], [14], [15] for details).
construct tree(chunk list, treebank):
while (chunk list is not empty) do
remove first chunk from chunk list
process chunk(chunk, treebank)
Figure 3: Pseudo-code for tree construction, main routine.
process chunk(chunk, treebank):
words := string yield(chunk)
tree := complete match(words, treebank)
if (tree is not empty) direct hit,
then output(tree) i.e. complete chunk found in treebank
else
tree := partial match(words, treebank)
if (tree is not empty)
then
if (tree = postfix of chunk)
then
tree1 := attach next chunk(tree, treebank)
if (tree is not empty)
then tree := tree1
if ((chunk - tree) is not empty) if attach next chunk succeeded
then tree := extend tree(chunk - tree, tree, treebank) chunk might consist of both chunks
output(tree)
if ((chunk - tree) is not empty) chunk might consist of both chunks (s.a.)
then process chunk(chunk - tree, treebank) i.e. process remaining chunk
else back off to POS sequence
pos := pos yield(chunk)
tree := complete match(pos, treebank)
if (tree is not empty)
then output(tree)
else back off to subchunks
while (chunk is not empty) do
remove first subchunk c1 from chunk
process chunk(c1, treebank)
Figure 4: Pseudo-code for tree construction, subroutine process chunk.
this case, the internal structure of the item to be classified (i.e. the
input sentence) and of the class item (i.e. the most similar tree in the
instance base) need to be considered, the classification task is much
more complex, and the standard memory-based approach needs to
be adapted to the requirements of the parsing task.
The features Tu?SBL uses for classification are the sequence of
words in the input sentence, their respective POS tags and (to a
lesser degree) the labels in the chunk parse. Rather than choosing a
bag-of-words approach, since word order is important for choosing
the most similar tree, the algorithm needed to be modified in order
to rely more on sequential information.
Another modification was necessitated by the need to generalize
from the limited number of trees in the instance base. The classifi-
cation is simple only in those cases where a direct hit is found, i.e.
where a complete match of the input with a stored instance exists.
In all other cases, the most similar tree from the instance base needs
to be modified to match the chunked input.
If these strategies for matching complete trees fail, Tu?SBL at-
tempts to match smaller subchunks in order to preserve the qual-
ity of the annotations rather than attempt to pursue only complete
parses.
The algorithm used for tree construction is presented in a slightly
simplified form in Figs. 3-6. For readability?s sake, we assume
here that chunks and complete trees share the same data structure
so that subroutines like string yield can operate on both of them
indiscriminately.
The main routine construct tree in Fig. 3 separates the list of in-
put chunks and passes each one to the subroutine process chunk in
Fig. 4 where the chunk is then turned into one or more (partial)
trees. process chunk first checks if a complete match with an in-
stance from the instance base is possible.4 If this is not the case,
a partial match on the lexical level is attempted. If a partial tree
is found, attach next chunk in Fig. 5 and extend tree in Fig. 6 are
used to extend the tree by either attaching one more chunk or by re-
sorting to a comparison of the missing parts of the chunk with tree
extensions on the POS level. attach next chunk is necessary to en-
sure that the best possible tree is found even in the rare case that the
original segmentation into chunks contains mistakes. If no partial
tree is found, the tree construction backs off to finding a complete
match in the POS level or to starting the subroutine for processing
a chunk recursively with all the subchunks of the present chunk.
The application of memory-based techniques is implemented in
the two subroutines complete match and partial match. The pre-
sentation of the two cases as two separate subroutines is for ex-
pository purposes only. In the actual implementation, the search
is carried out only once. The two subroutines exist because of
4string yield returns the sequence of words included in the input
structure, pos yield the sequence of POS tags.
attach next chunk(tree, treebank): attempts to attach the next chunk to the tree
take first chunk chunk2 from chunk list
words2 := string yield(tree, chunk2)
tree2 := complete match(words2, treebank)
if (tree2 is not empty)
then
remove chunk2 from chunk list
return tree2
else return empty
Figure 5: Pseudo-code for tree construction, subroutine attach next chunk.
extend tree(rest chunk, tree, treebank): extends the tree on basis of POS comparison
words := string yield(tree)
rest pos := pos yield(rest chunk)
tree2 := partial match(words + rest pos, treebank)
if ((tree2 is not empty) and (subtree(tree, tree2)))
then return tree2
else return empty
Figure 6: Pseudo-code for tree construction, subroutine extend tree.
the postprocessing of the chosen tree which is necessary for par-
tial matches and which also deviates from standard memory-based
applications. Postprocessing mainly consists of shortening the tree
from the instance base so that it covers only those parts of the chunk
that could be matched. However, if the match is done on the lexical
level, a correction of tagging errors is possible if there is enough ev-
idence in the instance base. Tu?SBL currently uses an overlap met-
ric, the most basic metric for instances with symbolic features, as
its similarity metric. This overlap metric is based on either lexical
or POS features. Instead of applying a more sophisticated metric
like the weighted overlap metric, Tu?SBL uses a backing-off ap-
proach that heavily favors similarity of the input with pre-stored
instances on the basis of substring identity. Splitting up the classi-
fication and adaptation process into different stages allows Tu?SBL
to prefer analyses with a higher likelihood of being correct. This
strategy enables corrections of tagging and segmentation errors that
may occur in the chunked input.
4.1 Example
Input:
dann w?urde ich sagen ist das vereinbart
(then I would say this is arranged)
Chunk parser output:
[simpx [advx [adv dann]]
[vxfin [vafin w"urde]]
[nx2 [pper ich]]
[vvinf sagen]]
[simpx [vafin ist]
[nx2 [pds das]]
[vvpp vereinbart]]
Figure 7: Chunk parser output
For the input sentence dann w?urde ich sagen ist das vereinbart
(then I would say this is arranged), the chunked output is shown in
Fig. 7. The chunk parser correctly splits the input into two clauses
Table 1: Quantitative evaluation
minimum maximum average
precision 76.82% 77.87% 77.23%
recall 66.90% 67.65% 67.28%
crossing accuracy 93.44% 93.95% 93.70%
dann w?urde ich sagen and ist das vereinbart. A look-up in the
instance base finds a direct hit for the first clause. Therefore, the
correct tree can be output directly. For the second clause, only a
partial match on the level of words can be found. The system finds
the tree for the subsequence of words ist das, as shown in Fig. 8.
By backing off to a comparison on the POS level, it finds a tree for
the sentence hatten die gesagt (they had said) with the same POS
sequence and the same structure for the first two words. Thus the
original tree that covers only two words is extended via the newly
found tree. Tu?SBL?s output for the complete sentence is shown in
Fig. 9.
5. QUANTITATIVE EVALUATION
A quantitative evaluation of Tu?SBL has been conducted using
a semi-automatically constructed treebank of German that consists
of appr. 67,000 fully annotated sentences or sentence fragments.5
The evaluation consisted of a ten-fold cross-validation test, where
the training data provide an instance base of already seen cases for
Tu?SBL?s tree construction module.
The evaluation focused on three PARSEVAL measures: labeled
precision, labeled recall and crossing accuracy, with the results
shown in Table 1.
While these results do not reach the performance reported for
other parsers (cf. [7], [8]), it is important to note that the task carried
out here is more difficult in a number of respects:
1. The set of labels does not only include phrasal categories, but
also functional labels marking grammatical relations such as
subject, direct object, indirect object and modifier. Thus, the
evaluation carried out here is not subject to the justified crit-
icism levelled against the gold standards that are typically
5See [13] for further details.
0 1
500 501
502 503
504
ist
VAFIN
das
PDS
HD HD
VXFIN
HD
NX
ON
LK MF
SIMPX
? ?
Figure 8: A partial tree found be the system
0 1 2 3 4 5 6
500 501 502 503 504 505
506 507 508 509510 511
513
514
515
516
dann
ADV
w"urde
VAFIN
ich
PPER
sagen
VVINF
ist
VAFIN
das
PDS
vereinbart
VVPP
HD HD HD HD HD HD
ADVX
MOD
VXFIN
HD
NX
ON
VXFIN
HD
VXINF
OV
NX
ON
VF LK MF VC
? ? ? ?
SIMPX
HD
VXINF
OV
LK MF VC
SIMPX
? ? ?
Figure 9: Tu?SBL?s output for the complete sentence
in conjunction with the PARSEVAL measures, namely that
the gold standards used typically do not include annotations
of syntactic-semantic dependencies between bracketed con-
stituents.
2. The German treebank consists of transliterated spontaneous
speech data. The fragmentary and partially ill-formed na-
ture of such spoken data makes them harder to analyze than
written data such as the Penn treebank typically used as gold
standard.
It should also be kept in mind that the basic PARSEVAL mea-
sures were developed for parsers that have as their main goal a
complete analysis that spans the entire input. This runs counter to
the basic philosophy underlying an amended chunk parser such as
Tu?SBL, which has as its main goal robustness of partially analyzed
structures: Precision and recall measure the percentage of brackets,
i.e. constituents with the same yield or bracketing scope, which are
identical in the parse tree and the gold standard. If Tu?SBL finds
only a partial grouping on one level, both measures consider this
grouping wrong, as a consequence of the different bracket scopes.
In most cases, the error ?percolates? up to the highest level. Fig.
10 gives an example of a partially matched tree structure for the
sentence ?bei mir ginge es im Februar ab Mittwoch den vierten?
(for me it would work in February after Wednesday the fourth).
The only missing branch is the branch connecting the second noun
phrase (NX) above ?Mittwoch? to the NX ?den vierten?. This re-
sults in precision and recall values of 10 out of 15 because of the
altered bracketing scopes of the noun phrase, the two prepositional
phrases (PX), the field level (MF) and the sentence level (SIMPX).
In order to capture this specific aspect of the parser, a second
evaluation was performed that focused on the quality of the struc-
tures produced by the parser. This evaluation consisted of manually
judging the Tu?SBL output and scoring the accuracy of the recog-
nized constituents. The scoring was performed by the human an-
notator who constructed the treebank and was thus in a privileged
position to judge constituent accuracy with respect to the treebank
annotation standards. This manual evaluation resulted in a score
of 92.4% constituent accuracy; that is: of all constituents that were
recognized by the parser, 92.4% were judged correct by the hu-
man annotator. This seems to indicate that approximately 20% of
the precision errors are due to partial constituents whose yield is
shorter than in the corresponding gold standard. Such discrepan-
cies typically arise when Tu?SBL outputs only partial trees. This
occurs when no complete tree structures can be constructed that
span the entire input.
6. CONCLUSION AND FUTURE RESEARCH
In this paper we have described how the Tu?SBL parser extends
current chunk parsing techniques by a tree-construction compo-
nent that completes partial chunk parses to tree structures including
function-argument structure.
As noted in section 4, Tu?SBL currently uses an overlap metric, i.
e. the most basic metric for instances with symbolic features, as its
0 1 2 3 4 5 6 7 8 9 10 11
500 501 502 503 504 505
506 507 508 509
510 511
512
513
514
bei
APPR
mir
PPER
ginge
VVFIN
es
PPER
im
APPRART
Februar
NN
ab
APPR
Mittwoch
NN
,
$,
den
ART
vierten
NN
.
$.
HD HD HD HD HD ? HD
NX
?
NX
HD
VXFIN
HD ?
NX
HD
NX
HD
PX
FOPP ?
NX
HD
PX
HD
PX
?
NX
ON
PX
V?MOD
VF
?
LK
?
MF
?
SIMPX
Figure 10: A partially grouped tree output of the T ?USBL system
similarity metric. We anticipate that the results reported in Fig. 1
can be further improved by experimenting with more sophisticated
similarity metrics. However, we will have to leave this matter to
future research.6
7. ACKNOWLEDGMENTS
The research reported here was funded both by the German Fed-
eral Ministry of Education, Science, Research, and Technology
(BMBF) in the framework of the VERBMOBIL Project under Grant
01 IV 101 N 0 and by the Deutsche Forschungsgemeinschaft (DFG)
in the framework of the Sonderforschungsbereich 441.
8. REFERENCES
[1] S. Abney. Parsing by chunks. In R. Berwick, S. Abney, and
C. Tenney, editors, Principle-Based Parsing. Kluwer
Academic Publishers, 1991.
[2] S. Abney. Partial parsing via finite-state cascades. In
J. Carroll, editor, Workshop on Robust Parsing (ESSLLI ?96),
1996.
[3] R. Bod. Beyond Grammar: An Experience-Based Theory of
Language. CSLI Publications, Stanford, California, 1998.
[4] R. Bod. Parsing with the shortest derivation. In Proceedings
of COLING 2000, 2000.
[5] T. Brants and W. Skut. Automation of treebank annotation.
In Proceedings of NeMLaP-3/CoNLL98, Sydney, Australia,
1998.
[6] S. Buchholz, J. Veenstra, and W. Daelemans. Cascaded
grammatical relation assignment. In Proceedings of
EMNLP/VLC-99, University of Maryland, USA, June 21-22,
1999, pages 239 ? 246, 1999.
[7] E. Charniak. Statistical parsing with a context-free grammar
and word statistics. In Proceedings of the Fourteenth
National Conference on Artifical Intelligence, Menlo Park,
1997.
6[9] reports that the gain ratio similarity metric has yielded excel-
lent results for the NLP applications considered by these investiga-
tors.
[8] M. Collins. Head-Driven Statistical Models for Natural
Language Parsing. PhD thesis, University of Pennsylvania,
1999.
[9] W. Daelemans, J. Zavrel, and A. van den Bosch. Forgetting
exceptions is harmful in language learning. Machine
Learning: Special Issue on Natural Language Learning, 34,
1999.
[10] H. Feldweg. Stochastische Wortartendisambiguierung fu?r das
Deutsche: Untersuchungen mit dem robusten System
LIKELY. Technical report, Universita?t Tu?bingen, 1993.
SfS-Report-08-93.
[11] A. Schiller, S. Teufel, and C. Thielen. Guidelines fu?r das
Tagging deutscher Textkorpora mit STTS. Technical report,
Universita?t Stuttgart and Universita?t Tu?bingen, 1995. (URL:
http://www.sfs.nphil.uni-tuebingen.de/Elwis/stts/stts.html).
[12] C. Stanfill and D. Waltz. Towards memory-based reasoning.
Communications of the ACM, 29(12), 1986.
[13] R. Stegmann, H. Schulz, and E. W. Hinrichs. Stylebook for
the German Treebank in VERBMOBIL. Technical Report
239, Verbmobil, 2000.
[14] J. Veenstra, A. van den Bosch, S. Buchholz, W. Daelemans,
and J. Zavrel. Memory-based word sense disambiguation.
Computers and the Humanities, Special Issue on Senseval,
Word Sense Disambiguations, 34, 2000.
[15] J. Zavrel, W. Daelemans, and J. Veenstra. Resolving PP
attachment ambiguities with memory-based learning. In
M. Ellison, editor, Proceedings of the Workshop on
Computational Natural Language Learning (CoNLL?97),
Madrid, 1997.
From Chunks to Function-Argument Structure: A Similarity-Based
Approach
Sandra Ku?bler   and Erhard W. Hinrichs  
 
Seminar fu?r Sprachwissenschaft
University of Tu?bingen
D-72074 Tu?bingen, Germany

kuebler,eh  @sfs.nphil.uni-tuebingen.de
Abstract
Chunk parsing has focused on the
recognition of partial constituent struc-
tures at the level of individual chunks.
Little attention has been paid to the
question of how such partial analyses
can be combined into larger structures
for complete utterances. Such larger
structures are not only desirable for a
deeper syntactic analysis. They also
constitute a necessary prerequisite for
assigning function-argument structure.
The present paper offers a similarity-
based algorithm for assigning func-
tional labels such as subject, object,
head, complement, etc. to complete
syntactic structures on the basis of pre-
chunked input.
The evaluation of the algorithm has
concentrated on measuring the quality
of functional labels. It was performed
on a German and an English treebank
using two different annotation schemes
at the level of function-argument struc-
ture. The results of 89.73 % cor-
rect functional labels for German and
90.40 % for English validate the general
approach.
1 Introduction
Current research on natural language parsing
tends to gravitate toward one of two extremes:
robust, partial parsing with the goal of broad
data coverage versus more traditional parsers that
aim at complete analysis for a narrowly defined
set of data. Chunk parsing (Abney, 1991; Ab-
ney, 1996) offers a particularly promising and by
now widely used example of the former kind.
The main insight that underlies the chunk pars-
ing strategy is to isolate the (finite-state) analysis
of non-recursive syntactic structure, i.e. chunks,
from larger, recursive structures. This results
in a highly-efficient parsing architecture that is
realized as a cascade of finite-state transducers
and that pursues a leftmost longest-match pattern-
matching strategy at each level of analysis.
Despite the popularity of the chunk parsing ap-
proach, there seems to be a gap in current re-
search:
Chunk parsing research has focused on the
recognition of partial constituent structures at the
level of individual chunks. By comparison, lit-
tle or no attention has been paid to the ques-
tion of how such partial analyses can be com-
bined into larger structures for complete utter-
ances. Such larger structures are not only de-
sirable for a deeper syntactic analysis; they also
constitute a necessary prerequisite for assigning
function-argument structure.
Automatic assignment of function-argument
structure has long been recognized as a desider-
atum beyond pure syntactic labeling (Marcus et
al., 1994)1. The present paper offers a similarity-
1With the exception of dependency-grammar-based
parsers (Tapanainen and Ja?rvinen, 1997; Bro?ker et al, 1994;
Lesmo and Lombardo, 2000), where functional labels are
treated as first-class citizens as relations between words, and
recent work on a semi-automatic method for treebank con-
struction (Brants et al, 1997), little has been reported on
based algorithm for assigning functional labels
such as subject, object, head, complement, etc.
to complete syntactic structures on the basis of
pre-chunked input. The evaluation of the algo-
rithm has concentrated on measuring the quality
of these functional labels.
2 The Tu?SBL Architecture
In order to ensure a robust and efficient archi-
tecture, Tu?SBL, a similarity-based chunk parser,
is organized in a three-level architecture, with
the output of each level serving as input for the
next higher level. The first level is part-of-speech
(POS) tagging of the input string with the help
of the bigram tagger LIKELY (Feldweg, 1993).2
The parts of speech serve as pre-terminal ele-
ments for the next step, i.e. the chunk analysis.
Chunk parsing is carried out by an adapted ver-
sion of Abney?s (1996) CASS parser, which is
realized as a cascade of finite-state transducers.
The chunks, which extend if possible to the sim-
plex clause level, are then remodeled into com-
plete trees in the tree construction level.
The tree construction level is similar to the
DOP approach (Bod, 1998; Bod, 2000) in that
it uses complete tree structures instead of rules.
Contrary to Bod, we only use the complete trees
and do not allow tree cuts. Thus the number of
possible combinations of partial trees is strictly
controlled. The resulting parser is highly efficient
(3770 English sentences took 106.5 seconds to
parse on an Ultra Sparc 10).
3 Chunking and Tree Construction
The division of labor between the chunking and
tree construction modules can best be illustrated
by an example.
For sentences such as the input shown in Fig.
1, the chunker produces a structure in which some
constituents remain unattached or partially anno-
tated in keeping with the chunk-parsing strategy
to factor out recursion and to resolve only unam-
biguous attachments.
Since chunks are by definition non-recursive
structures, a chunk of a given category cannot
fully automatic recognition of functional labels.
2The inventory of POS tags is based on the STTS
(Schiller et al, 1995) for German and on the Penn Treebank
tagset (Santorini, 1990) for English.
Input: alright and that should get us there about
nine in the evening
Chunk parser output:
[uh alright]
[simpx_ind
[cc and]
[that that]
[vp [md should]
[vb get]]
[pp us]
[adv [rb there]]
[prep_p [about about]
[np [cd nine]]]
[prep_p [in in]
[np [dt the]
[daytime evening]]]]
Figure 1: Chunk parser output.
contain another chunk of the same type. In
the case at hand, the two prepositional phrases
(?prep p?) about nine and in the evening in the
chunk output cannot be combined into a sin-
gle chunk, even though semantically these words
constitute a single constituent. At the level of tree
construction, as shown in Fig. 2, the prohibition
against recursive phrases is suspended. There-
fore, the proper PP attachment becomes possible.
Additionally, the phrase about nine was wrongly
categorized as a ?prep p?. Such miscategoriza-
tions can arise if a given word can be assigned
more than one POS tag. In the case of about
the tags ?in? (for: preposition) or ?rb? (for: ad-
verb) would be appropriate. However, since the
POS tagger cannot resolve this ambiguity from
local context, the underspecified tag ?about? is as-
signed, instead. However, this can in turn lead to
misclassification in the chunker.
The most obvious deficiency of the chunk out-
put shown in Fig. 1 is that the structure does
not contain any information about the function-
argument structure of the chunked phrases. How-
ever, once a (more) complete parse structure is
created, the grammatical function of each ma-
jor constituent needs to be identified. The la-
bels SUBJ (for: subject), HD (for: head), ADJ
(for: adjunct) COMP (for: complement), SPR
(for: specifier), which appear as edge-labels be-
tween tree nodes in Fig. 2, signify the grammati-
cal functions of the constituents in question. E.g.
the label SUBJ encodes that the NP that is the
alright
UH
and
CC
that
DT
should
MD
get
VB
us
PP
there
RB
about
RB
nine
CD
in
IN
the
DT
evening
NN
? ? HD HD HD ? ?
PR?DM
HD
DT?ART
HD
DTP
SPR HD
HD
NP
COMP
ADVP
ADJ
CNUM
HD
PP
ADJ
HD
NP
COMP
ADVP
ADJ
NP
ADJ
NP
SBJ HD
VP
COMP
CNJ
?
S
?
0 1 2 3 4 5 6 7 8 9 10 11
500 501 502 503 504 505 506
507 508
509
510
511
512
513
514
S
Figure 2: Sample tree construction output for the sentence in Fig. 1.
subject of the whole sentence. The label ADJ
above the phrase about nine in the evening signi-
fies that this phrase is an adjunct of the verb get.
Tu?SBL currently uses as its instance base two
semi-automatically constructed treebanks of Ger-
man and English that consist of appr. 67,000 and
35,000 fully annotated sentences, respectively3 .
Each treebank uses a different annotation scheme
at the level of function-argument structure4 . As
shown in Table 1, the English treebank uses a to-
tal of 13 functional labels, while the German tree-
bank has a richer set of 36 function labels.
For German, therefore, the task of tree con-
struction is slightly more complex because of the
larger set of functional labels. Fig. 3 gives an ex-
ample for a German input sentence and its corre-
sponding chunk parser output.
In this case, the subconstituents of the extra-
posed coordinated noun phrase are not attached
to the simplex clause that ends with the non-finite
verb that is typically in clause-final position in
declarative main clauses of German. Moreover,
each conjunct of the coordinated noun phrase
forms a completely flat structure. Tu?SBL?s tree
construction module enriches the chunk output
as shown in Fig. 4. Here the internally recur-
sive NP conjuncts have been coordinated and in-
3See (Stegmann et al, 2000; Kordoni, 2000) for further
details.
4The annotation for German follows the topological-
field-model standardly used in empirical studies of German
syntax. The annotation for English is modeled after the theo-
retical assumptions of Head-Driven Phrase Structure Gram-
mar.
Input:
dann w?urde ich vielleicht noch vorschlagen
Donnerstag den elften und Freitag den zw?olften
August (then I would suggest maybe Thursday eleventh
and Friday twelfth of August)
Chunk parser output:
[simpx [advx [adv dann]]
[vxfin [vafin w"urde]]
[nx2 [pper ich]]
[advx [adv vielleicht]]
[advx [advmd noch]]
[vvinf vorschlagen]]
[nx3 [day Donnerstag]
[art den]
[adja elften]]
[kon und]
[nx3 [day Freitag]
[art den]
[adja zw"olften]
[month August]]
Figure 3: Chunk parser output for a German sen-
tence.
tegrated correctly into the clause as a whole. In
addition, function labels such as MOD (for: mod-
ifier), HD (for head), ON (for: subject), OA (for:
direct object), OV (for: verbal object), and APP
(for: apposition) have been added that encode the
function-argument structure of the sentence.
4 Similarity-based Tree Construction
The tree construction algorithm is based on the
machine learning paradigm of memory-based
German label description English label description
HD head HD head
- non-head - intentionally empty
ON nominative object COMP complement
OD dative object SPR specifier
OA accusative object SBJ subject
OS sentential object SBQ subject, wh-
OPP prepositional object SBR subject, rel.
OADVP adverbial object ADJ adjunct
OADJP adjectival object ADJ? adjunct ambiguities
PRED predicate FIL filler
OV verbal object FLQ filler, wh-
FOPP optional prepositional object FLR filler, rel.
VPT separable verb prefix MRK marker
APP apposition
MOD ambiguous modifier
x-MOD 8 distinct labels for specific
modifiers, e.g. V-MOD
yK 13 labels for second conjuncts in
split-up coordinations, e.g. ONK
Table 1: The functional label set for the German and the English treebanks.
0 1 2 3 4 5 6 7 8 9 10 11 12 13
500501502
503
504505
506
507
508509
510
511
512
513
514
515
516
517
dann
ADV
w"urde
VAFIN
ich
PPER
vielleicht
ADV
noch
ADV
vorschlagen
VVINF
Donnerstag
NN
den
ART
elften
NN
und	
KON
Freitag
NN
den
ART
zw"olften

ADJA
August
NN
HDHDHD
VXINF
OV
HDHD
VXFIN
HD
? HD
NX
HD APP
ADVX
MOD
HD
NX ADVX ADVX
ON MOD MOD
HD
ADJX
? ? HD
NX
HD APP
NX
NX
? ? ?
NX
OA
VF LK MF VC
NF
SIMPX
? ? ? ? ?
Figure 4: Tree construction output for the German sentence in Fig. 3.
learning (Stanfill and Waltz, 1986).5 Memory-
based learning assumes that the classification of
a given input should be based on the similarity
to previously seen instances of the same type that
have been stored in memory. This paradigm is an
instance of lazy learning in the sense that these
previously encountered instances are stored ?as
is? and are crucially not abstracted over, as is
typically the case in rule-based systems or other
learning approaches. Previous applications of
5Memory-based learning has recently been applied to a
variety of NLP classification tasks, including part-of-speech
tagging, noun phrase chunking, grapheme-phoneme conver-
sion, word sense disambiguation, and PP attachment (see
(Daelemans et al, 1999; Veenstra et al, 2000; Zavrel et al,
1997) for details).
memory-based learning to NLP tasks consisted of
classification problems in which the set of classes
to be learnt was simple in the sense that the class
items did not have any internal structure and the
number of distinct items was small. Since in the
current application, the set of classes are parse
trees, the classification task is much more com-
plex. The classification is simple only in those
cases where a direct hit is found, i.e. where a com-
plete match of the input with a stored instance ex-
ists. In all other cases, the most similar tree from
the instance base needs to be modified to match
the chunked input. This means that the output
tree will group together only those elements from
the chunked input for which there is evidence in
the instance base. If these strategies fail for com-
plete chunks, Tu?SBL attempts to match smaller
subchunks.
The algorithm used for tree construction is pre-
sented in a slightly simplified form in Figs. 5-8.
For readability, we assume here that chunks and
complete trees share the same data structure so
that subroutines like string yield can operate on
both of them indiscriminately.
The main routine construct tree in Fig. 5 sepa-
rates the list of input chunks and passes each one
to the subroutine process chunk in Fig. 6 where
the chunk is then turned into one or more (partial)
trees. process chunk first checks if a complete
match with an instance from the instance base is
possible.6 If this is not the case, a partial match
on the lexical level is attempted. If a partial tree is
found, attach next chunk in Fig. 7 and extend tree
in Fig. 8 are used to extend the tree by either at-
taching one more chunk or by resorting to a com-
parison of the missing parts of the chunk with tree
extensions on the POS level. attach next chunk is
necessary to ensure that the best possible tree is
found even in the rare case that the original seg-
mentation into chunks contains mistakes. If no
partial tree is found, the tree construction backs
off to finding a complete match at the POS level or
to starting the subroutine for processing a chunk
recursively with all the subchunks of the present
chunk.
The application of memory-based techniques
is implemented in the two subroutines com-
plete match and partial match. The presentation
of the two cases as two separate subroutines is for
expository purposes only. In the actual implemen-
tation, the search is carried out only once. The
two subroutines exist because of the postprocess-
ing of the chosen tree, which is necessary for par-
tial matches and which also deviates from stan-
dard memory-based applications. Postprocessing
mainly consists of shortening the tree from the in-
stance base so that it covers only those parts of
the chunk that could be matched. However, if the
match is done on the lexical level, a correction of
tagging errors is possible if there is enough evi-
dence in the instance base. Tu?SBL currently uses
an overlap metric, the most basic metric for in-
6string yield returns the sequence of words included in
the input structure, pos yield the sequence of POS tags.
stances with symbolic features, as its similarity
metric. This overlap metric is based on either
lexical or POS features. Instead of applying a
more sophisticated metric like the weighted over-
lap metric, Tu?SBL uses a backing-off approach
that heavily favors similarity of the input with pre-
stored instances on the basis of substring identity.
Splitting up the classification and adaptation pro-
cess into different stages allows Tu?SBL to prefer
analyses with a higher likelihood of being correct.
This strategy enables corrections of tagging and
segmentation errors that may occur in the chun-
ked input.
5 Quantitative Evaluation
Quantitive evaluations of robust parsers typically
focus on the three PARSEVAL measures: labeled
precision, labeled recall and crossing accuracy. It
has frequently been pointed out that these evalu-
ation parameters provide little or no information
as to whether a parser assigns the correct seman-
tic structure to a given input, if the set of category
labels comprises only syntactic categories in the
narrow sense, i.e. includes only names of lexi-
cal and phrasal categories. This justified criticism
observes that a measure of semantic accuracy can
only be obtained if the gold standard includes an-
notations of syntactic-semantic dependencies be-
tween bracketed constituents. It is to answer this
criticism that the evaluation of the Tu?SBL system
presented here focuses on the correct assignment
of functional labels. For an in-depth evaluation
that focuses on syntactic categories, we refer the
interested reader to (Ku?bler and Hinrichs, 2001).
The quantitative evaluation of Tu?SBL has been
conducted on the treebanks of German and En-
glish described in section 3. Each treebank uses
a different annotation scheme at the level of
function-argument structure. As shown in Table
1, the English treebank uses a total of 13 func-
tional labels, while the German treebank has a
richer set of 36 function labels.
The evaluation consisted of a ten-fold cross-
validation test, where the training data provide an
instance base of already seen cases for Tu?SBL?s
tree construction module. The evaluation was per-
formed for both the German and English data.
For each language, the following parameters were
measured: 1. labeled precision for syntactic cat-
construct tree(chunk list, treebank):
while (chunk list is not empty) do
remove first chunk from chunk list
process chunk(chunk, treebank)
Figure 5: Pseudo-code for tree construction, main routine.
process chunk(chunk, treebank):
words := string yield(chunk)
tree := complete match(words, treebank)
if (tree is not empty) direct hit,
then output(tree) i.e. complete chunk found in treebank
else
tree := partial match(words, treebank)
if (tree is not empty)
then
if (tree = postfix of chunk)
then
tree1 := attach next chunk(tree, treebank)
if (tree is not empty)
then tree := tree1
if ((chunk - tree) is not empty) if attach next chunk succeeded
then tree := extend tree(chunk - tree, tree, treebank) chunk might consist of both chunks
output(tree)
if ((chunk - tree) is not empty) chunk might consist of both chunks (s.a.)
then process chunk(chunk - tree, treebank) i.e. process remaining chunk
else back off to POS sequence
pos := pos yield(chunk)
tree := complete match(pos, treebank)
if (tree is not empty)
then output(tree)
else back off to subchunks
while (chunk is not empty) do
remove first subchunk c1 from chunk
process chunk(c1, treebank)
Figure 6: Pseudo-code for tree construction, subroutine process chunk.
attach next chunk(tree, treebank): attempts to attach the next chunk to the tree
take first chunk chunk2 from chunk list
words2 := string yield(tree, chunk2)
tree2 := complete match(words2, treebank)
if (tree2 is not empty)
then
remove chunk2 from chunk list
return tree2
else return empty
Figure 7: Pseudo-code for tree construction, subroutine attach next chunk.
extend tree(rest chunk, tree, treebank): extends the tree on basis of POS comparison
words := string yield(tree)
rest pos := pos yield(rest chunk)
tree2 := partial match(words + rest pos, treebank)
if ((tree2 is not empty) and (subtree(tree, tree2)))
then return tree2
else return empty
Figure 8: Pseudo-code for tree construction, subroutine extend tree.
egories alone, and 2. labeled precision for func-
tional labels.
The results of the quantitative evaluation are
shown in Tables 2 and 3. The results for labeled
recall underscore the difficulty of applying the
classical PARSEVAL measures to a partial pars-
language parameter minimum maximum average
German true positives 60.38 % 64.23 % 61.45 %
false positives 2.93 % 3.14 % 3.03 %
unattached constituents 15.15 % 19.23 % 18.18 %
unmatched constituents 17.05 % 17.59 % 17.35 %
English true positives 59.11 % 60.18 % 59.78 %
false positives 3.11 % 3.39 % 3.25 %
unattached constituents 9.57 % 10.30 % 9.88 %
unmatched constituents 26.80 % 27.54 % 27.10 %
Table 2: Quantitative evaluation: recall.
language parameter minimum maximum average
German labeled precision for synt. cat. 81.28 % 82.08 % 81.56 %
labeled precision for funct. cat. 89.26 % 90.13 % 89.73 %
English labeled precision for synt. cat. 66.15 % 67.34 % 66.84 %
labeled precision for funct. cat. 90.07 % 90.93 % 90.40 %
Table 3: Quantitative evaluation: precision.
ing approach like ours. We have, therefore di-
vided the incorrectly matched nodes into three
categories: the genuine false positives where a
tree structure is found that matches the gold stan-
dard, but is assigned the wrong label; nodes
which, relative to the gold standard, remain
unattached in the output tree; and nodes contained
in the gold standard for which no match could be
found in the parser output. Our approach follows
a strategy of positing and attaching nodes only if
sufficient evidence can be found in the instance
base. Therefore the latter two categories can-
not really be considered errors in the strict sense.
Nevertheless, in future research we will attempt to
significantly reduce the proportion of unattached
and unmatched nodes by exploring matching al-
gorithms that permit a higher level of generaliza-
tion when matching the input against the instance
base. What is encouraging about the recall results
reported in Table 2 is that the parser produces gen-
uine false positives for an average of only 3.03 %
for German and 3.25 % for English.
For German, labeled precision for syntactic
categories yielded 81.56 % correctness. While
these results do not reach the performance re-
ported for other parsers (cf. (Collins, 1999; Char-
niak, 1997)), it is important to note that the two
treebanks consist of transliterated spontaneous
speech data. The fragmentary and partially ill-
formed nature of such spoken data makes them
harder to analyze than written data such as the
Penn treebank typically used as gold standard.
It should also be kept in mind that the basic
PARSEVAL measures were developed for parsers
that have as their main goal a complete analy-
sis that spans the entire input. This runs counter
to the basic philosophy underlying an amended
chunk parser such as Tu?SBL, which has as its
main goal robustness of partially analyzed struc-
tures.
Labeled precision of functional labels for the
German data resulted in a score of 89.73 % cor-
rectness. For English, precision of functional la-
bels was 90.40 %. The slightly lower correctness
rate for German is a reflection of the larger set of
function labels used by the grammar. This raises
interesting more general issues about trade-offs
in accuracy and granularity of functional annota-
tions.
6 Conclusion and Future Research
The results of 89.73 % (German) and 90.40 %
(English) correctly assigned functional labels val-
idate the general approach. We anticipate fur-
ther improvements by experimenting with more
sophisticated similarity metrics7 and by enrich-
ing the linguistic information in the instance base.
The latter can, for example, be achieved by pre-
serving more structural information contained in
the chunk parse. Yet another dimension for ex-
perimentation concerns the way in which the al-
gorithm generalizes over the instance base. In
the current version of the algorithm, generaliza-
tion heavily relies on lexical and part-of-speech
information. However, a richer set of backing-off
strategies that rely on larger domains of structure
are easy to envisage and are likely to significantly
improve recall performance.
While we intend to pursue all three dimensions
of refining the basic algorithm reported here, we
have to leave an experimentation of which modi-
fications yield improved results to future research.
References
Steven Abney. 1991. Parsing by chunks. In Robert
Berwick, Steven Abney, and Caroll Tenney, editors,
Principle-Based Parsing. Kluwer Academic Pub-
lishers.
Steven Abney. 1996. Partial parsing via finite-state
cascades. In John Carroll, editor, Workshop on Ro-
bust Parsing (ESSLLI ?96).
Rens Bod. 1998. Beyond Grammar: An Experience-
Based Theory of Language. CSLI Publications,
Stanford, California.
Rens Bod. 2000. Parsing with the shortest derivation.
In Proceedings of COLING 2000, Saarbru?cken,
Germany.
Thorsten Brants, Wojiech Skut, and Brigitte Krenn.
1997. Tagging grammatical functions. In Proceed-
ings of EMNLP-2 1997, Providence, RI.
Norbert Bro?ker, Udo Hahn, and Susanne Schacht.
1994. Concurrent lexicalized dependency parsing:
the ParseTalk model. In Proceedings of COLING
94, Kyoto, Japan.
Eugene Charniak. 1997. Statistical parsing with a
context-free grammar and word statistics. In Pro-
ceedings of the Fourteenth National Conference on
Artifical Intelligence, Menlo Park.
Michael Collins. 1999. Head-Driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania.
7(Daelemans et al, 1999) reports that the gain ratio sim-
ilarity metric has yielded excellent results for the NLP appli-
cations considered by these investigators.
Walter Daelemans, Jakub Zavrel, and Antal van den
Bosch. 1999. Forgetting exceptions is harmful in
language learning. Machine Learning: Special Is-
sue on Natural Language Learning, 34.
Helmut Feldweg. 1993. Stochastische Wortartendis-
ambiguierung fu?r das Deutsche: Untersuchungen
mit dem robusten System LIKELY. Technical re-
port, Universita?t Tu?bingen. SfS-Report-08-93.
Valia Kordoni. 2000. Stylebook for the English
Treebank in VERBMOBIL. Technical Report 241,
Verbmobil.
Sandra Ku?bler and Erhard W. Hinrichs. 2001.
Tu?SBL: A similarity-based chunk parser for robust
syntactic processing. In Proceedings of HLT 2001,
San Diego, Cal.
Leonardo Lesmo and Vincenzo Lombardo. 2000. Au-
tomatic assignment of grammatical relations. In
Proceedings of LREC 2000, Athens, Greece.
Mitchell Marcus, Grace Kim, Mary Ann
Marcinkiewicz, Robert MacIntyre, Anne Bies,
Mark Ferguson, Karen Katz, and Britta Schas-
berger. 1994. The Penn Treebank: Annotating
predicate argument structure. In Proceedings of
HLT 94, Plainsboro, New Jersey.
Beatrice Santorini. 1990. Part-Of-Speech Tagging
Guidelines for the Penn Treebank Project. Univer-
sity of Pennsylvania, 3rd Revision, 2nd Printing.
Anne Schiller, Simone Teufel, and Christine Thielen.
1995. Guidelines fu?r das Tagging deutscher Text-
korpora mit STTS. Technical report, Universita?ten
Stuttgart and Tu?bingen. http://www.sfs.nphil.uni-
tuebingen.de/Elwis/stts/stts.html.
Craig Stanfill and David L. Waltz. 1986. Towards
memory-based reasoning. Communications of the
ACM, 29(12).
Rosmary Stegmann, Heike Schulz, and Erhard W.
Hinrichs. 2000. Stylebook for the German Tree-
bank in VERBMOBIL. Technical Report 239,
Verbmobil.
Pasi Tapanainen and Timo Ja?rvinen. 1997. A non-
projective dependency parser. In Proceedings of
ANLP?97, Washington, D.C.
Jorn Veenstra, Antal van den Bosch, Sabine Buch-
holz, Walter Daelemans, and Jakub Zavrel. 2000.
Memory-based word sense disambiguation. Com-
puters and the Humanities, Special Issue on Sense-
val, Word Sense Disambiguations, 34.
Jakub Zavrel, Walter Daelemans, and Jorn Veen-
stra. 1997. Resolving PP attachment ambiguities
with memory-based learning. In Proceedings of
CoNLL?97, Madrid, Spain.
Proceedings of the Workshop on Frontiers in Corpus Annotation II: Pie in the Sky, pages 13?20,
Ann Arbor, June 2005. c?2005 Association for Computational Linguistics
A Unified Representation for Morphological, Syntactic, Semantic, and
Referential Annotations
Erhard W. Hinrichs, Sandra K?bler, Karin Naumann
SfS-CL, University of T?bingen
Wilhelmstr. 19
72074 T?bingen, Germany
{eh,kuebler,knaumann}@sfs.uni-tuebingen.de
Abstract
This paper reports on the SYN-RA
(SYNtax-based Reference Annotation)
project, an on-going project of annotating
German newspaper texts with referential
relations. The project has developed an in-
ventory of anaphoric and coreference rela-
tions for German in the context of a uni-
fied, XML-based annotation scheme for
combining morphological, syntactic, se-
mantic, and anaphoric information. The
paper discusses how this unified annota-
tion scheme relates to other formats cur-
rently discussed in the literature, in par-
ticular the annotation graph model of Bird
and Liberman (2001) and the pie-in-the-
sky scheme for semantic annotation.
1 Introduction
The purpose of this paper is threefold: (i) it dis-
cusses an annotation scheme for referential relations
for German that is significantly broader in scope
than existing schemes for the same task and lan-
guage and that also goes beyond the inventory of
anaphoric relations included in the pie-in-the-sky
sample feature structures1 , (ii) it presents a unified,
XML-based annotation scheme for combining mor-
phological, syntactic, semantic, and anaphoric infor-
mation, and (iii) it discusses how this unified anno-
tation scheme relates to other formats currently dis-
cussed in the literature, in particular the annotation
1See e.g. nlp.cs.nyu.edu/meyers/pie-in-the-sky/
analysis5.
graph model of Bird and Liberman (2001) and the
pie-in-the-sky scheme for semantic annotation2 .
2 Referential Relations
This section introduces the inventory of referential
relations adopted in the SYN-RA project. We define
referential relations as a cover-term for all contex-
tually dependent reference relations. The inventory
of such relations adopted for SYN-RA is inspired by
the annotation scheme first developed in the MATE
project (Davies et al, 1998). However, it takes a
cautious approach in that it only adopts those refer-
ential relations from MATE for which the develop-
ers of MATE report a sufficiently high level of inter-
annotator agreement (Poesio et al, 1999).
SYN-RA currently uses the following subset
of relations: coreferential, anaphoric, cataphoric,
bound, split antecedent, instance, and expletive. The
potential markables are definite NPs, personal pro-
nouns, relative, reflexive, and reciprocal pronouns,
demonstrative, indefinite and possessive pronouns.
There is a second research effort under way at the
European Media Laboratory Heidelberg, which also
annotates German text corpora and dialog data with
referential relations. Since their corpora are not pub-
licly available, it is difficult to verify their inventory
of referential relations. Kouchnir (2003) has used
their data and describes the relations anaphoric,
coreferential, bridging, and none.
Following van Deemter and Kibble (2000), we
define a coreference relation to hold between two
2See nlp.cs.nyu.edu/meyers/pie-in-the-sky/
pie-in-the-sky-descript.html.
13
NPs just in case they refer to the same extra-
linguistic referent in the real world. In the following
example, a coreference relation exists between the
noun phrases [1] and [2], and an anaphoric relation
between the noun phrase [2] and the personal pro-
noun [3]. Since noun phrases [1] and [2] are corefer-
ential, all three NPs belong to the same coreference
chain. In keeping with the MUC-6 annotation stan-
dard3, we establish the anaphoric relations of a pro-
noun only to its most recently mentioned antecedent.
(1) [1 Der
The
neue
new
Vorsitzende
chairman
der
of the
Gewerkschaft
union
Erziehung
Education
und
and
Wissenschaft]
Science
hei?t
is called
[2 Ulli
Ulli
Th?ne].
Th?ne.
[3 Er]
He
wurde
was
gestern
yesterday
mit
with
217
217
von
out of
355
355
Stimmen
votes
gew?hlt.
elected.
?The new chairman of the union of educators
and scholars is called Ulli Th?ne. He was
elected yesterday with 217 of 355 votes.?
Cataphoric relations hold between a preceding
pronoun and a following antecedent within the same
sentence, even if this antecedent has already been
mentioned within the preceding text. An example
for a cataphoric relation is shown in (2).
(2) Vier
Four
Wochen
weeks
sind
are
[sie]
they
nun
now
schon
already
in
in
Berlin,
Berlin,
[die
the
220
220
Albaner
Albanians
aus
from
dem
the
Kosovo].
Kosovo.
?They have already been in Berlin for four
weeks, the 200 Albanians from Kosovo.?
The relation bound holds between anaphoric ex-
pressions and quantified noun phrases as their an-
tecedents (see example (3)).
(3) [Niemandem]
To nobody
f?llt
is
es
it
schwer,
difficult,
das
the
Bild
picture
vor
in front of
[sich]
himself
zu
to
sehen.
see.
?Nobody has trouble imagining the picture.?
3See www.cs.nyu.edu/cs/faculty/grishman/
COtask21.book_1.html.
The split antecedent relation holds between co-
ordinate NPs/plural pronouns and pronouns/definite
NPs referring to one member of the plural expres-
sion. In example (4), the indefinite pronoun beide
enters into two split antecedent relations, with noun
phrases 1 and 2.
(4) Aber
But
pl?tzlich
suddenly
gibt
gives
es
it
da
there
einen
a
v?llig
completely
unglaubw?rdig
implausible
und
and
grotesk
grotesque
wirkenden
seeming
Anruf
phone call
[1 des
of the
Detektiven]
detective
bei
to
[2 der
the
Mutter
mother
des
of the
Opfers]
victim
,
,
[beide]
both
weinen
cry
sich
themselves
minutenlang
for some minutes
etwas
something
vor
verb part
,
,
...
...
?But suddenly, there is a completely implausi-
ble and grotesque phone call from the detective
to the mother of the victim, they both cry at
each other for several minutes, ...?
An instance relation exists between a preced-
ing/following pronoun and its NP antecedent when
the pronoun refers to a particular instantiation of the
class identified by the NP.
(5) Die
The
konservativen
conservative
Kr?fte
powers
warten
wait
ja
just
nur
only
darauf,
for that,
ihm
him
[S?tze]
sentences
um
around
die
the
Ohren
ears
zu
to
hauen
hit
wie
like
[jenen
the one
von
about
den
the
16
16
Mittelstrecklern],
middle-distance runners,
denen
to whom
er
he
in
in
vier
four
Wochen
weeks
die
the
Viererkette
double full-back formation
beibringe.
teaches.
?The conservative powers are just waiting to
bombard him with sentences like the one about
the 16 middle-distance runners who he is teach-
ing the double full-back formation in four
weeks.?
14
In sentence (5), the relation between the two
bracketed NPs is an example of such an instance re-
lation since the second NP is a particular instantia-
tion of the referent denoted by the first NP.
A third person singular neuter pronoun es is
marked as expletive if it has no proper antecedent.
This is the case for presentational es in example (6),
impersonal passive as in example (7), or es as sub-
ject for verbs without an agent as in example (8).
(6) [1 Es]
It
zeichnet sich
emerges
die
the
konkrete
concrete
M?glichkeit
possibility
ab.
verb part.
?The concrete possibility emerges.?
(7) [Es]
There
wird
is
bis zum
until the
Morgen
morning
getanzt.
danced.
?People are dancing until morning.?
(8) [Es]
It
steht
stands
schlecht
bad
um
for
ihn.
him.
?He is in a bad way.?
Apart from expletive uses of es and anaphoric
uses with an NP antecedent, the pronoun es can also
be used in cases of event anaphora as in sentence
(9). Here es refers to the event of Jochen?s win-
ning the lottery. Currently, the annotation in SYN-
RA is restricted to NP anaphora and therefore event
anaphors such as in sentence (9) remain unannotated
for anaphora.
(9) Jochen
Jochen
hat
has
im
in the
Lotto
lottery
gewonnen.
won.
Aber
But
er
he
weiss
knows
es
it
noch
yet
nicht.
not.
?Jochen has won the lottery. But he does not
know it yet.?
The annotation of such relations is performed
manually with the annotation tool MMAX (M?ller
and Strube, 2003). Its graphical user interface al-
lows for easy selection of the relevant markables and
the accompanying relation between the contextually
dependent expression and its antecedent.
3 Automatic Extraction of Markables and
of Semantic Information
Annotation of referential relations involves two
main tasks: the identification of markables, i.e.,
identifying the class of expressions that can enter
into referential relations, and the identification of the
particular referential relations that two or more ex-
pressions enter into. Identification of markables re-
quires at least partial syntactic annotation of the text.
If referential relations need to be annotated from
plain text, then markables must be identified semi-
automatically from the output of a chunker or full
parser, if available, or otherwise completely man-
ually. However, in each of these two scenarios,
identification of markables is a time-consuming pro-
cess. In case of semi-automatic annotation, the ef-
fort required depends on the quality of the parser, but
will require at least some amount of manual post-
correction of the parser output.
Identification of markables is considerably easier
for treebank data since treebanks already provide the
necessary syntactic information. For German, there
are currently two large-scale treebanks available: the
NEGRA/TIGER (Brants et al, 2002) treebank and
the T?bingen treebanks for spoken and written Ger-
man (Stegmann et al, 2000; Telljohann et al, 2003).
All the treebanks were annotated with the help of the
annotation tool Annotate (Plaehn, 1998). The tree-
bank annotations are available in the Annotate ex-
port format (Brants, 1997) and in an XML format.
The SYN-RA project is based on the T?bingen
treebank of written German (T?Ba-D/Z). This tree-
bank uses as its data source a collection of articles of
the German daily newspaper taz (die tageszeitung).
The treebank currently comprises appr. 15 000 sen-
tences, with a new release of 7 000 additional sen-
tences scheduled for June of this year.
Due to its fine grained syntactic annotation, the
T?Ba-D/Z treebank data are ideally suited as a basis
for the identification of markables and for extract-
ing relevant syntactic and semantic properties for
each markable. The T?Ba-D/Z annotation scheme
distinguishes four levels of syntactic constituency:
the lexical level, the phrasal level, the level of topo-
logical fields, and the clausal level. The primary
ordering principle of a clause is the inventory of
topological fields, which characterize the word or-
15
Ihre
PPOSAT
asf
Schulkameradin
NN
asf
Cassie
NE
asf
Bernall
NE
asf
fragten
VVFIN
3pit
sie
PPER
np*3
,
$,
??
ob
KOUS
??
sie
PPER
nsf3
an
APPR
a
Gott
NE
asm
glaube
VVFIN
3sks
.
$.
??
? HD ? ? HD HD ? HD HD HD
NX
?
VXFIN
HD
NX
ON ?
NX
HD
VXFIN
HD
NX
APP
EN?ADD
APP
NX
ON
PX
OPP
NX
OA
C
?
MF
?
VC
?
SIMPX
OS
VF
?
LK
?
MF
?
NF
?
0 1 2 3 4 5 6 7 8 9 10 11 12
500 501 502 503 504 505 506 507
508 509 510 511 512
513 514
515 516
517
518
SIMPX
Figure 1: A sample tree from the T?Ba/D-Z treebank.
der regularities among different clause types of Ger-
man and which are widely accepted among descrip-
tive linguists of German (cf. e.g. (Drach, 1937;
H?hle, 1986)). The T?Ba-D/Z annotation relies
on a context-free backbone (i.e. proper trees with-
out crossing branches) of phrase structure combined
with edge labels that specify the grammatical func-
tion of the phrase in question.
Figure 1 shows an example tree from the T?Ba-
D/Z treebank for sentence (10). The sentence is di-
vided into two clauses (SIMPX), and each clause is
subdivided into topological fields. The main clause
is made up of the following fields: VF (mnemonic
for: Vorfeld ? ?initial field?) contains the sentence-
initial, topicalized constituent. LK (for: linke Satz-
klammer ? ?left sentence bracket?) is occupied by the
finite verb. MF (for: Mittelfeld ? ?middle field?) con-
tains adjuncts and complements of the main verb.
NF (for: Nachfeld ? ?final field?) contains extra-
posed material ? in this case an indirect yes/no ques-
tion. The subordinate clause is again divided into
three topological fields: C (for: Komplementierer ?
?complementizer?), MF, and VC (for: Verbalkomp-
lex ? verbal complex). Edge labels are rendered
in boxes and indicate grammatical functions. The
sentence-initial NX (for: noun phrase) is marked as
OA (for: accusative complement), the pronouns sie
in the main and subordinate clause as ON (for: nom-
inative complement).
(10) Ihre
Their
Schulkameradin
fellow student
Cassie
Cassie
Bernall
Bernall
fragten
asked
sie
they[subj]
,
,
ob
whether
sie
she[subj]
an
in
Gott
God
glaube.
believes.
?They asked their fellow student Cassie Bernall
whether she believes in God.?
Topological field information and grammatical
function information is crucial for anaphora resolu-
tion since binding-theory constraints crucially rely
on sentence-structure (if the binding theory princi-
ples are stated configurationally (Chomsky, 1981))
or on argument-obliqueness (if the binding theory
principles are stated in terms of argument structure,
as in (Pollard and Sag, 1994)). In the case at hand,
the subject pronoun of the main clause, sie, can-
not be anaphorically related to the object NP Ihre
Schulkameradin Cassie Bernall since they are co-
arguments of the same verb. However, the posses-
sive pronoun ihre and the subject pronoun sie of the
subordinate clause, can be and, in fact, are anaphor-
ically related, since they are not co-arguments of the
same verb. This can be directly inferred from the
treebank annotation, specifically from the sentence
structure and the grammatical function information
16
encoded on the edge labels. Most published compu-
tational algorithms of anaphora resolution, including
(Hobbs, 1978; Lappin and Leass, 1994; Ingria and
Stallard, 1989), rely on such binding-constraint fil-
ters to minimize the set of potential antecedents for
pronouns and reflexives.
As already pointed out, the sample sentence con-
tains four markables: one possessive pronoun Ihre,
two occurrences of the pronoun sie and one complex
NP Ihre Schulkameradin Cassie Bernall. The latter
NP is a good example of SYN-RA?s longest-match
principle for identifying markables. In case of com-
plex NPs, the entire NP counts as a markable, but
so do its subconstituents ? in the case at hand, par-
ticularly the possessive pronoun ihre. All of this in-
formation can be directly derived from the treebank
account. Compared to other annotation efforts for
German where markables have to be chosen manu-
ally (M?ller and Strube, 2003), manual annotation
in the SYN-RA project can, thus, be restricted to the
selection of the appropriate referential relations be-
tween referentially dependent expressions and their
nominal antecedents.
4 The Unified, XML-based Annotation
Scheme
The annotation of referential expressions is em-
bedded in a unified format which also contains
morphological, syntactic, and semantic information.
The annotation scheme is represented in XML, the
widely acknowledged standard for exchanging data,
which guarantees portability and re-usability of the
data. Each sentence, as well as all words and
all nodes in the syntactic structure, are assigned a
unique ID. These IDs are used in the annotation of
referential relations. The annotation of the treebank
sentence 11976 (cf. example (10)) is shown in Fig-
ure 2.
The sentence number is encoded as the ID of the
sentence. The first word, Ihre, has an anaphoric rela-
tion to a noun phrase in the previous sentence. This
relation is marked in the element anaphora, which
gives the antecedent as node 517 of sentence 11975,
i.e. the previous sentence. The other two anaphoric
relations are sentence-internal, the first personal pro-
noun sie having Ihre (id: s11976w0) as antecedent,
the second one the noun phrase Ihre Schulfreundin
Cassie Bernall (id: s11976n513). The annotation of
the first personal pronoun is an example for the an-
notation of an anaphoric chain. Ihre and sie belong
to the same chain. However, in order to facilitate the
extraction of direct relations, such chains are repre-
sented in a way that each anaphoric expression refers
to the last occurrence of an antecedent.
The SYN-RA scheme is very similar to the
MUC-6 coreference annotation scheme4 but it is
more powerful in two respects: As described above,
the inventory is not restricted to coreference and
anaphoric relations, it also covers e.g. instance rela-
tions or split antecedent relations. The latter relation
is also the reason for encoding the relational infor-
mation as XML elements, and not as attributes of
a word or a node. If an anaphor enters into a split
antecedent relation, it has more than one distinct an-
tecedent. In this case, the element anaphora has two
(or more) relations. Such an example is graphically
displayed for sentence (4) in Figure 3. The rele-
vant XML representation of the complex entry for
the word beide is shown in Figure 4.
5 Related Work
This section discusses how the unified SYN-RA an-
notation scheme relates to other formats currently
discussed in the literature, in particular the pie-in-
the-sky scheme for semantic annotation5 and the
annotation graph model of (Bird and Liberman,
2001). While these two annotation schemes are by
no means the only contenders for corpus annotation
standards in the literature, they are certainly among
the most ambitious and promising.
While the pie-in-the-sky scheme is clearly still
under development, the following characteristics
and goals can already be gleaned from its web-
page and the annotation examples presented there:
The annotation is feature-structure-based and incor-
porates various levels of linguistic annotation, in
particular a PROPBANK style predicate-argument
structure, dependency style syntactic information,
as well as morpho-syntactic and word class infor-
mation. All this information is rooted in the at-
tributes needed for predicate-argument assignment,
4See www.cs.nyu.edu/cs/faculty/grishman/
COtask21.book_1.html.
5See nlp.cs.nyu.edu/meyers/pie-in-the-sky/
pie-in-the-sky-descript.html.
17
<sentence id="s11976">
<node id="s11976n518" cat="SIMPX" func="--" parent="0">
<node id="s11976n515" cat="VF" func="-">
<node id="s11976n513" cat="NX" func="OA">
<node id="s11976n500" cat="NX" func="APP">
<word id="s11976w0" form="Ihre" pos="PPOSAT" morph="asf" func="-">
< anaphora>
< relation type="ana" antecedent="s11975n517"/>
< /anaphora> </word>
<word id="s11976w1" form="Schulkameradin" pos="NN" morph="asf" func="HD"/>
</node>
<node id="s11976n508" cat="EN-ADD" func="APP">
<node id="s11976n501" cat="NX" func="-">
<word id="s11976w2" form="Cassie" pos="NE" morph="asf" func="-"/>
<word id="s11976w3" form="Bernall" pos="NE" morph="asf" func="-"/>
</node> </node> </node> </node>
<node id="s11976n509" cat="LK" func="-">
<node id="s11976n502" cat="VXFIN" func="HD">
<word id="s11976w4" form="fragten" pos="VVFIN" morph="3pit" func="HD"/>
</node> </node>
<node id="s11976n510" cat="MF" func="-">
<node id="s11976n503" cat="NX" func="ON">
<word id="s11976w5" form="sie" pos="PPER" morph="np*3" func="HD">
< anaphora>
< relation type="ana" antecedent="s11976w1"/>
< /anaphora> </word> </node> </node>
<word id="s11976w6" form="," pos="$," morph="--" func="--" parent="0"/>
<node id="s11976n517" cat="NF" func="-">
<node id="s11976n516" cat="SIMPX" func="OS">
<node id="s11976n504" cat="C" func="-">
<word id="s11976w7" form="ob" pos="KOUS" morph="--" func="-"/>
</node>
<node id="s11976n514" cat="MF" func="-">
<node id="s11976n505" cat="NX" func="ON">
<word id="s11976w8" form="sie" pos="PPER" morph="nsf3" func="HD">
< anaphora>
< relation type="ana" antecedent="s11976n513"/>
< /anophora> </word> </node>
<node id="s11976n511" cat="PX" func="OPP" comment="">
<word id="s11976w9" form="an" pos="APPR" morph="a" func="-"/>
<node id="s11976n506" cat="NX" func="HD">
<word id="s11976w10" form="Gott" pos="NE" morph="asm" func="HD"/>
</node> </node> </node>
<node id="s11976n512" cat="VC" func="-">
<node id="s11976n507" cat="VXFIN" func="HD">
<word id="s11976w11" form="glaube" pos="VVFIN" morph="3sks" func="HD"/>
</node> </node> </node> </node> </node>
<word form="." pos="$." morph="--" func="--" parent="0"/>
</sentence>
Figure 2: The XML format represents information on all levels of annotation. The words of the sentence
and the anaphoric annotation are shown in bold.
18
NP NP
Aber pl?tzlich gibt es da einen ... Anruf des Detektiven bei der Mutter ..., beide weinen sich
minutenlang etwas vor ...
split
split
Figure 3: The annotation of the split antecedent relation in sentence (4). For representational reasons, the
sentence is shortened and only relevant information is displayed. Syntactic boundaries are shown as dotted
lines, anaphoric relations as black lines.
<word id="s3426w20" form="beide" pos="PIS" morph="np*" func="HD">
< anaphora>
<relation type="split" antecedent="s3426n507"/>
<relation type="split" antecedent="s3426n526"/>
< /anaphora>
</word>
Figure 4: The XML representation of the encoding of split antecedents for the word beide in sentence (4).
A graphical representation of the relation is shown in Figure 3. The antecedent "s3426n507" refers to the
first NP, "s3426n526" to the second one in Figure 3.
with syntactic and morpho-syntactic information
distributed among the corresponding elements in
the predicate-argument structure representation. Ac-
cordingly, semantic representations provide the or-
ganizing principle while morpho-syntactic and syn-
tactic information play a subordinated role.
The SYN-RA annotation scheme resembles the
pie-in-the-sky scheme in that it also uses one level
of representation, in this case hierarchical syntac-
tic structure, as the organizing principle and treats
referential relations, grammatical function informa-
tion, and morpho-syntactic annotation as subordi-
nated types of information. More generally, the pie-
in-the-sky and the SYN-RA representations offer a
particular view of the annotation, each with its own
?perspective?: semantics-based (pie-in-the-sky) and
syntax-based (SYN-RA).
By contrast, Bird and Liberman?s (2001) anno-
tation graphs are intended as a graph-based, multi-
layered annotation scheme where each level of lin-
guistic annotation is treated equally, as an indepen-
dent layer. The graph-based annotation model is
powerful enough to also allow groupings of discon-
tinuous constituents and other non-adjacent linguis-
tic phenomena, without having to rearrange the lin-
ear order of the input. In both respects, their annota-
tion model is maximally general.
6 Future Directions
In the previous section we have compared two
perspective-dependent annotation schemes that use
a particular level of linguistic annotation as their pri-
mary organizing principle and have contrasted them
with the perspective-independent annotation-graph
model. We believe that both types of represen-
tation models have their independent justification.
Perspective-based representations, such as SYN-
RA and pie-in-the-sky, are well-justified for partic-
ular application scenarios. For example, for text
summarization and other semantic tasks, the pie-
in-the-sky model seems particularly well-motivated
since the pertinent semantic information can be eas-
ily extracted from its predicate-argument-structure-
rooted feature structures. For other tasks, such as
anaphora resolution, for which syntactic informa-
tion is more relevant, the syntax-based representa-
tion of SYN-RA allows for an easier extraction of
the relevant information for rule-based, statistical,
19
and machine-learning approaches to computational
anaphora resolution. More generally, perspective-
based representations are highly task-dependent. It
would be misguided to consider them as ideal, task-
independent annotation standards. If one wants
to establish a task-independent annotation standard,
then a perspective-independent annotation scheme
such as the annotation graph model looks like a
promising direction for future research. In particu-
lar, such research should focus on techniques that al-
low for easy conversion of perspective-independent
representations to task-dependent views of the rele-
vant linguistic information.
References
Steven Bird and Mark Liberman. 2001. A formal frame-
work for linguistic annotation. Speech Communica-
tion, 33(1,2):23?60.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolf-
gang Lezius, and George Smith. 2002. The TIGER
treebank. In Erhard Hinrichs and Kiril Simov, edi-
tors, Proceedings of the First Workshop on Treebanks
and Linguistic Theories (TLT 2002), pages 24?41, So-
zopol, Bulgaria.
Thorsten Brants, 1997. The NeGra Export Format for
Annotated Corpora. Universit?t des Saarlandes, Com-
putational Linguistics, Saarbr?cken, Germany.
Noam Chomsky. 1981. Lectures on Government and
Binding. Foris, Dordrecht.
Sarah Davies, Massimo Poesio, Florence Bruneseaux,
and Laurent Romary, 1998. Annotating Coreference in
Dialogues: Proposal for a Scheme for MATE. MATE.
Kees van Deemter and Rodger Kibble. 2000. On core-
ferring: Coreference in MUC and related annotation
schemes. Computational Linguistics, 26(2):629?637.
Erich Drach. 1937. Grundgedanken der Deutschen Satz-
lehre. Diesterweg, Frankfurt/M.
Jerry R. Hobbs. 1978. Resolving pronoun references.
Lingua, 44:311?338.
Tilman H?hle. 1986. Der Begriff "Mittelfeld", An-
merkungen ?ber die Theorie der topologischen Felder.
In Akten des Siebten Internationalen Germanistenkon-
gresses 1985, pages 329?340, G?ttingen, Germany.
Robert J. P. Ingria and David Stallard. 1989. A compu-
tational mechanism for pronominal reference. In Pro-
ceedings of the 27th Conference of the Association for
Computational Linguistics, pages 262?271, Vancou-
ver, Canada.
Beata Kouchnir. 2003. A machine learning approach to
German pronoun resolution. Master?s thesis, School
of Informatics, University of Edinburgh.
Shalom Lappin and Herbert Leass. 1994. An algorithm
for pronominal anaphora resolution. Computational
Linguistics, 20(4):535?561.
Christoph M?ller and Michael Strube. 2003. Multi-level
annotation in MMAX. In Proceedings of the 4th SIG-
dial Workshop on Discourse and Dialogue, Sapporo,
Japan.
Oliver Plaehn, 1998. Annotate Bedienungsanleitung.
Universit?t des Saarlandes, Sonderforschungsbereich
378, Projekt C3, Saarbr?cken, Germany, April.
Massimo Poesio, Florence Bruneseaux, and Laurent Ro-
mary. 1999. The MATE meta-scheme for coreference
in dialogues in multiple languages. In Proceedings of
the ACL Workshop on Standards for Discourse Tag-
ging, pages 65?74.
Carl Pollard and Ivan Sag. 1994. Head-Driven Phrase
Structure Grammar. Studies in Contemporary Lin-
guistics. University of Chicago Press, Chicago, IL.
Rosmary Stegmann, Heike Telljohann, and Erhard W.
Hinrichs. 2000. Stylebook for the German Treebank
in VERBMOBIL. Technical Report 239, Verbmobil.
Heike Telljohann, Erhard W. Hinrichs, and Sandra
K?bler, 2003. Stylebook for the T?bingen Treebank of
Written German (T?Ba-D/Z). Seminar f?r Sprachwis-
senschaft, Universit?t T?bingen, T?bingen, Germany.
20
Proceedings of the Workshop on Linguistic Distances, pages 73?81,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Towards Case-Based Parsing:
Are Chunks Reliable Indicators for Syntax Trees?
Sandra Ku?bler
SfS-CL, University of Tu?bingen
Wilhelmstr. 19
72074 Tu?bingen, Germany
kuebler@sfs.uni-tuebingen.de
Abstract
This paper presents an approach to the
question whether it is possible to construct
a parser based on ideas from case-based
reasoning. Such a parser would employ
a partial analysis of the input sentence
to select a (nearly) complete syntax tree
and then adapt this tree to the input sen-
tence. The experiments performed on Ger-
man data from the Tu?ba-D/Z treebank and
the KaRoPars partial parser show that a
wide range of levels of generality can be
reached, depending on which types of in-
formation are used to determine the simi-
larity between input sentence and training
sentences. The results are such that it is
possible to construct a case-based parser.
The optimal setting out of those presented
here need to be determined empirically.
1 Introduction
Linguistic similarity has often been used as a bias
in machine learning approaches to Computational
Linguistics problems. The success of applying
memory-based learning to problems such as POS
tagging, named-entity recognition, partial parsing,
or word sense disambiguation (cf. (Daelemans et
al., 1996; Daelemans et al, 1999; Mooney, 1996;
Tjong Kim Sang, 2002; Veenstra et al, 2000))
shows that the bias of this similarity-based ap-
proach is suitable for processing natural language
problems.
In (Ku?bler, 2004a; Ku?bler, 2004b), we extended
the application of memory-based learning to full
scale parsing, a problem which cannot easily be
described as a classification problem. In this ap-
proach, the most similar sentence is found in the
training data, and the respective syntax tree is then
adapted to the input sentence. The parser was de-
veloped for parsing German dialog data, and it is
based on the observation that dialogs tend to be
repetitive in their structure. Thus, there is a higher
than normal probability of finding the same or a
very similar sentence in the training data.
The present paper examines the possibilities of
extending the concepts in (Ku?bler, 2004a; Ku?bler,
2004b) to unrestricted newspaper text. Since in
newspaper text, the probability of finding the same
sentence or a very similar one is rather low, the
parser needs to be extended to a more flexible ap-
proach which does not rely as much on identity
between sentences as the original parser.
The paper is structured as follows: Section 2 ex-
plains the original parser in more detail, and sec-
tion 3 describes the treebank used in the investi-
gation. Section 4 investigates whether the chunk
sequences used for selecting the most similar sen-
tence in the training data give a reliable estimate
of the syntax tree, section 5 investigates properties
of tree sets associated with chunk sequences, and
section 6 draws conclusions on the architecture of
an extended case-based parser.
2 A Memory-Based Parser
The parser in (Ku?bler, 2004a; Ku?bler, 2004b)
approaches parsing as the task of finding a com-
plete syntax tree rather than incrementally build-
ing the tree by rule applications, as in standard
PCFGs. Despite this holistic approach to selecting
the most similar tree, the parser has a reasonable
performance: the first column of Table 1 shows
the parser?s evaluation on German spontaneous
speech dialog data. This approach profits from the
fact that it has a more global view on parsing than
a PCFG parser. In this respect, the memory-based
73
memory-based parser KaRoPars
labeled recall (syntactic categories) 82.45% 90.86%
labeled precision (syntactic categories) 87.25% 90.17%
F  84.78 90.51
labeled recall (incl. gramm. functions) 71.72%
labeled precision (incl. gramm. functions) 75.79%
F  73.70
Table 1: Results for the memory-based parser (Ku?bler, 2004a; Ku?bler, 2004b) and KaRoPars (Mu?ller
and Ule, 2002; Mu?ller, 2005). The evaluation of KaRoPars is based on chunk annotations only.
parser employs a similar strategy to the one in
Data-Oriented Parsing (DOP) (Bod, 1998; Scha et
al., 1999). Both parsers use larger tree fragments
than the standard trees. The two approaches differ
mainly in two respects: 1) DOP allows different
tree fragments to be extracted from one tree, thus
making different combinations of fragments avail-
able for the assembly of a specific tree. Our parser,
in contrast, allows only one clearly defined tree
fragment for each tree, in which only the phrase-
internal structure is variable. 2) Our parser does
not use a probabilistic model, but a simple cost
function instead. Both factors in combination re-
sult in a nearly deterministic, and thus highly effi-
cient parsing strategy.
Since the complete tree structure in the
memory-based parser is produced in two steps (re-
trieval of the syntax tree belonging to the most
similar sentence and adaptation of this tree to the
input sentence), the parser must rely on more in-
formation than the local information on which a
PCFG parser suggests the next constituent. For
this reason, we suggested a backing-off architec-
ture, in which each modules used different types of
easily obtainable linguistic information such as the
sequence of words, the sequence of POS tags, and
the sequence of chunks. Chunk parsing is a partial
parsing approach (Abney, 1991), which is gener-
ally implemented as cascade of finite-state trans-
ducers. A chunk parser generally gives an anal-
ysis on the clause level and on the phrase level.
However, it does not make any decisions concern-
ing the attachment of locally ambiguous phrases.
Thus, the German sentence in (1a) receives the
chunk annotation in (1b).
(1) a. In
In
der
the
bewu?ten
conscious
Wahrnehmung
perception
des
of the
Lebens
life
sieht
discerns
der
the
international
internationally
angesehene
distinguished
Ku?nstler
artist
den
the
Ursprung
origin
aller
of all
Kreativita?t.
creativity.
?The internationally recognized artist discerns
the origin of all creativity in the conscious
perception of life.?
b. [PC In der bewu?ten Wahrnehmung des
Lebens] [VCL sieht] [NC der international
angesehene Ku?nstler] [NC den Ursprung]
[NC aller Kreativita?t].
NCs are noun chunks, PC is a prepositional
chunk, and VCL is the finite verb chunk. While
for the chunks to the right of the verb chunk, no
attachment decision could be made, the genitive
noun phrase des Lebens could be grouped with
the PC because of German word order regularities,
which allow exactly one constituent in front of the
finite verb.
It can be hypothesized that the selection of
the most similar sentence based on sequences of
words or POS tags works best for dialog data be-
cause of the repetitive nature of such dialogs. The
strategy with the greatest potential for generaliza-
tion to newspaper texts is thus the usage of chunk
sequences. In the remainder of this paper, we will
therefore concentrate on this approach.
The proposed parser is based on the follow-
ing architecture: The parser needs a syntactically
annotated treebank for training. In the learning
phase, the training data are chunk parsed, the
chunk sequences are extracted from the chunk
parse and fitted to the syntax trees; then the trees
are stored in memory. In the annotation phase, the
new sentence is chunk parsed. Based on the se-
quence of chunks, the group of most similar sen-
tences, which all share the same chunk analysis, is
retrieved from memory. In a second step, the best
sentence from this group needs to be selected, and
the corresponding tree needs to be adapted to the
input sentence.
The complexity of such a parser crucially de-
pends on the question whether these chunk se-
74
quences are reliable indicators for the correct syn-
tax trees. Basically, there exist two extreme pos-
sibilities: 1) most chunk sequences are associated
with exactly one sentence, and 2) there is only a
small number of different chunk sequences, which
are each associated with many sentences. In the
first case, the selection of the correct tree based
on a chunk sequence is trivial but the coverage
of the parser would be rather low. The parser
would encounter many sentences with chunk se-
quences which are not present in the training data.
In the second case, in contrast, the coverage of
chunk sequences would be good, but then such
a chunk sequence would correspond to many dif-
ferent trees. As a consequence, the tree selection
process would have to be more elaborate. Both
extremes would be extremely difficult for a parser
to handle, so in the optimal case, we should have
a good coverage of chunk sequences combined
with a reasonable number of trees associated with
a chunk sequence.
The investigation on the usefulness of chunk se-
quences was performed on the data of the German
treebank Tu?Ba-D/Z (Telljohann et al, 2004) and
on output from KaRoPars, a partial parser for Ger-
man (Mu?ller and Ule, 2002). But in principle, the
parsing approach is valid for languages ranging
from a fixed to a more flexible word order. The
German data will be described in more detail in
the following section.
3 The German Data
3.1 The Treebank Tu?Ba-D/Z
The Tu?Ba-D/Z treebank is based on text from the
German newspaper ?die tageszeitung?, the present
release comprises approx. 22 000 sentences. The
treebank uses an annotation framework that is
based on phrase structure grammar enhanced by
a level of predicate-argument structure. The an-
notation scheme uses pure projective tree struc-
tures. In order to treat long-distance relationships,
Tu?Ba-D/Z utilizes a combination of topological
fields (Ho?hle, 1986) and specific functional labels
(cf. the tree in Figure 5, there the extraposed rel-
ative clause modifies the subject, which is anno-
tated via the label ON-MOD ). Topological fields
described the main ordering principles in a Ger-
man sentence: In a declarative sentence, the posi-
tion of the finite verb as the second constituent and
of the remaining verbal elements at the end of the
clause is fixed. The finite verb constitutes the left
sentence bracket (LK), and the remaining verbal
elements the right sentence bracket (VC). The left
bracket is preceded by the initial field (VF), be-
tween the two verbal fields, we have the unstruc-
tured middle field (MF). Extraposed constituents
are in the final field (NF).
The tree for sentence (1a) is shown in Figure
1. The syntactic categories are shown in circular
nodes, the function-argument structure as edge la-
bels in square boxes. Inside a phrase, the function-
argument annotation describes head/non-head re-
lations; on the clause level, directly below the
topological fields, grammatical functions are an-
notated. The prepositional phrase (PX) is marked
as a verbal modifier (V-MOD), the noun phrase
der international angesehene Ku?nstler as subject
(ON), and the complex noun phrase den Ursprung
aller Kreativita?t as accusative object (OA). The
topological fields are annotated directly below the
clause node (SIMPX): the finite verb is placed in
the left bracket, the prepositional phrase consti-
tutes the initial field, and the two noun phrases the
middle field.
3.2 Partially Parsed Data
KaRoPars (Mu?ller and Ule, 2002) is a partial
parser for German, based on the finite-state tech-
nology of the TTT suite of tools (Grover et al,
1999). It employs a mixed bottom-up top-down
routine to parse German. Its actual performance is
difficult to determine exactly because it employed
manually written rules. The figures presented in
Table 1 result from an evaluation (Mu?ller, 2005) in
which the parser output was compared with tree-
bank structures. The figures in the Table are based
on an evaluation of chunks only, i.e. the annotation
of topological fields and clause boundaries was not
taken into account.
The output of KaRoPars is a complex XML rep-
resentation with more detailed information than is
needed for the present investigation. For this rea-
son, we show a condensed version of the parser
output for sentence (1a) in Figure 2. The figure
shows only the relevant chunks and POS tags, the
complete output contains more embedded chunks,
the n-best POS tags from different taggers, mor-
phological information, and lemmas. As can be
seen from this example, chunk boundaries often
do not coincide with phrase boundaries. In the
present case, it is clear from the word ordering
constraints in German that the noun phrase des
75
0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15
500 501 502 503 504 505
506 507 508 509
510 511
512 513
514
515
In
APPR
der
ART
bewu?ten
ADJA
Wahrnehmung
NN
des
ART
Lebens
NN
sieht
VVFIN
der
ART
international
ADJD
angesehene
ADJA
K?nstler
NN
den
ART
Ursprung
NN
aller
PIDAT
Kreativit?t
NN
.
$.
HD ? HD HD HD ? HD ? HD
?
ADJX
? HD
VXFIN
HD
ADJX
? HD
NX
HD
NX
?
NX
HD
NX
? ?
ADJX
? HD
?
NX
HD
NX
ON
NX
OA
PX
V?MOD
VF
?
LK
?
MF
?
SIMPX
Figure 1: The Tu?Ba-D/Z tree for sentence (1a).
<s broken="no">
<cl c="V2">
<ch fd="VF" c="PC" prep="in">
<ch c="PC" prep="in">
<t f="In"><P t="APPR"></P></t>
<ch nccat="noun" hdnoun="Wahrnehmung" c="NC">
<t f="der"><P t="ART"></P></t>
<t f="bewu?ten"><P t="ADJA"></P></t>
<t f="Wahrnehmung"><P t="NN"></P></t></ch></ch>
<ch nccat="noun" hdnoun="Leben" c="NC">
<t f="des"><P t="ART"></P></t>
<t f="Lebens"><P t="NN"></P></t></ch></ch>
<ch finit="fin" c="VCLVF" mode="akt">
<t f="sieht"><P t="VVFIN"></P></t></ch>
<ch nccat="noun" hdnoun="Ku?nstler" c="NC">
<t f="der"><P t="ART"></P></t>
<t f="international"><P t="ADJD"></P></t>
<t f="angesehene"><P t="ADJA"></P></t>
<t f="Ku?nstler"><P t="NN"></P></t></ch>
<ch nccat="noun" hdnoun="Ur=Sprung" c="NC">
<t f="den"><P t="ART"></P></t>
<t f="Ursprung"><P t="NN"></P></t></ch>
<ch nccat="noun" hdnoun="Kreativita?t" c="NC">
<t f="aller"><P t="PIDAT"></P></t>
<t f="Kreativita?t"><P t="NN"></P></t></ch></cl></s>
Figure 2: The KaRoPars analysis for sentence (1a). For better readability, the words and the chunk types
are displayed in bold.
Lebens needs to be attached to the previous phrase.
In the treebank, it is grouped into a complex noun
phrase while in the KaRoPars output, this noun
phrase is the sister of the prepositional chunk In
der bewu?ten Wahrnehmung. Such boundary mis-
matches also occur on the clause level.
4 Chunk Sequences as Indicators for
Syntax Trees
The complexity of the proposed parser depends on
the proportion of chunk sequences versus syntax
trees, as explained in section 2. A first indication
of this proportion is given by the ratio of chunk
sequence types and tree types. Out of the 22 091
sentences in the treebank, there are 20 340 differ-
ent trees (types) and 14 894 different chunk se-
quences. This gives an average of 1.37 trees per
chunk sequence. At a first glance, the result indi-
cates that the chunk sequences are very good in-
dicators for selecting the correct syntax tree. The
negative aspect of this ratio is that many of these
chunk sequences will not be part of the training
data. This is corroborated by an experiment in
which one tenth of the complete data set of chunk
sequences (test set) was tested against the remain-
der of the data set (training set) to see how many
of the test sequences could be found in the train-
ing data. In order to reach a slightly more accurate
picture, a ten-fold setting was used, i.e. the exper-
iment was repeated ten times, each time using a
different segment as test set. The results show that
on average only 43.61% of the chunk sequences
76
could be found in the training data.
(2) Schon
Already
trifft
meets
sich
REFL
die
the
Mannschaft
team
erst
only
am
on the
Spieltag.
game day.
?So the team only meets on the day of the game.?
In a second experiment, we added more infor-
mation about chunk types, namely the information
from the fields nccat and finit in the XML rep-
resentation to the chunk categories. Field nccat
contains information about the head of the noun
chunk, whether it is a noun, a reflexive pronoun,
a relative pronoun, etc. Field finit contains in-
formation about the finiteness of a verb chunk.
For this experiment, sentence (2) is represented by
the chunk sequence ?NC:noun VCL NC:refl PC
NC:noun PC AVC NC:noun VCR:fin?. When us-
ing such chunk sequences, the ratio of sequences
found in the training set decreases to 36.59%.
In a third experiment, the chunk sequences were
constructed without adverbial phrases, i.e. with-
out the one category that functions as adjunct in
a majority of the cases. Thus sentence (3) is repre-
sented by the chunk sequence ?NC VCL NC NC?
instead of by the complete sequence: ?NC VCL
NC AVC AVC AVC NC?. In this case, 54.72%
of the chunk sequences can be found. Reducing
the information in the chunk sequence even fur-
ther seems counterproductive because every type
of information that is left out will make the final
decision on the correct syntax tree even more dif-
ficult.
(3) Wer
Who
gibt
gives
uns
us
denn
anyhow
jetzt
now
noch
still
einen
an
Auftrag?
order?
?Who will give us an order anyhow??
All the experiments reported above are based on
data in which complete sentences were used. One
possibility of gaining more generality in the chunk
sequences without losing more information con-
sists of splitting the sentences on the clause level.
(4) Ganz
Totally
abgesehen
irrespective
davon,
of it,
da?
that
man
one
dann
then
schon
already
mal
once
alle
all
die
the
Geschlechtsgenossinnen
fellow females
kennt,
knows,
mit
with
denen
whom
man
one
nach
after
der
the
Trennung
break-up
u?ber
about
den
the
Kerl
twerp
abla?stern
slander
kann,
can,
weil
because
sie
they
ja
already
genau
exactly
wissen,
know,
wie
how
mies
bad
er
he
eigentlich
really
ist.
is.
?Completely irrespective of the fact that one al-
ready knows all the other females with whom one
can slander the twerp after the break-up because
they already know what a loser he is.?
Thus, the complex sentence in (4) translates into
5 different clauses, i.e. into 5 different chunk se-
quences:
1. SubC NC:noun AVC AVC AVC NC:noun
NC:noun VCR:fin
2. PC NC:noun PC PC VCR:fin
3. SubC NC:noun AVC AJVC VCR:fin
4. SubC AJVC NC:noun AVC VCR:fin
5. AVC VCR:fin PC
The last sequence covers the elliptical ma-
trix clause ganz abgesehen davon, the first
four sequences describe the subordinated clauses;
i.e. the first sequence describes the subordi-
nate clause da? man dann schon mal alle die
Geschlechtsgenossinnen kennt, the second se-
quence covers the relative clause mit denen man
nach der Trennung u?ber den Kerl abla?stern kann.
The third sequence describes the subordinate
clause introduced by the conjunction weil, and the
fourth sequence covers the subordinate clause in-
troduced by the interrogative pronoun wie.
On the one hand, splitting the chunk sequences
into clause sequences makes the parsing task more
difficult because the clause boundaries annotated
during the partial parsing step do not always coin-
cide with the clause boundaries in the syntax trees.
In those cases where the clause boundaries do not
coincide, a deterministic solution must be found,
which allows a split that does not violate the paral-
lelism constraints between both structures. On the
other hand, the split into clauses allows a higher
coverage of new sentences without extending the
size of the training set. In an experiment, in which
the chunk sequences were represented by the main
chunk types plus subtypes (cf. experiment two)
and were split into clauses, the percentage of un-
seen sequences in a tenfold split was reduced from
66.41% to 44.16%. If only the main chunk type is
taken into account, the percentage of unseen se-
quences decreases from 56.39% to 36.34%.
The experiments presented in this section show
that with varying degrees of information and with
different ways of extracting chunk sequences, a
range of levels of generality can be represented.
If the maximum of information regarded here is
used, only 36.59% of the sequences can be found.
If, in contrast, the sentences are split into chunks
and only the main chunk type is used, the ratio
of found sequences reaches 63.66%. A final deci-
sion on which representation of chunks is optimal,
however, is also dependent on the sets of trees that
77
are represented by the chunk sequences and thus
needs to be postponed.
5 Tree Sets
In the previous section, we showed that if we
extract chunk sequences based on complete sen-
tences and on main chunk types, there are on av-
erage 1.37 sentences assigned to one chunk se-
quences. At a first glance, this results means that
for the majority of chunk sequences, there is ex-
actly one sentence which corresponds to the se-
quence, which makes the final selection of the cor-
rect tree trivial. However, 1261 chunk sequences
have more than one corresponding sentence, and
there is one chunk sequence which has 802 sen-
tences assigned. We will call these collections tree
sets. In these cases, the selection of the correct
tree from a tree set may be far from trivial, de-
pending on the differences in the trees. A minimal
difference constitutes a difference in the words
only. If all corresponding words belong to the
same POS class, there is no difference in the syn-
tax trees. Another type of differences in the trees
which does not overly harm the selection process
are differences in the internal structure of phrases.
In (Ku?bler, 2004a), we showed that the tree can
be cut at the phrase level, and new phrase-internal
structures can be inserted into the tree. Thus, the
most difficult case occurs when the differences
in the trees are located in the higher regions of
the trees where attachment information between
phrases and grammatical functions are encoded. If
such cases are frequent, the parser needs to employ
a detailed search procedure.
The question how to determine the similarity of
trees in a tree set is an open research question. It
is clear that the similarity measure should abstract
away from unimportant differences in words and
phrase-internal structure. It should rather concen-
trate on differences in the attachment of phrases
and in grammatical functions. As a first approx-
imation for such a similarity measure, we chose
a measure based on precision and recall of these
parts of the tree. In order to ignore the lower levels
of the tree, the comparison is restricted to nodes in
the tree which have grammatical functions.
(5) Der
The
Autokonvoi
car convoy
mit
with
den
the
Probenbesuchern
rehearsal visitors
fa?hrt
travels
eine
a
Stra?e
street
entlang,
down,
die
which
noch
still
heute
today
Lagerstra?e
Lagerstra?e
hei?t.
is called.
?The convoy of the rehearsal visitors? cars travels
down a street that is still called Lagerstra?e.?
For example, Figure 5 shows the tree for sen-
tence (5). The matrix clause consists of a com-
plex subject noun phrase (GF: ON), a finite verb
phrase, which is the head of the sentence, an
accusative noun phrase (GF: OA), a verb parti-
cle (GF: VPT), and an extraposed relative clause
(GF: ON-MOD). Here the grammatical function
indicates a long-distance relationship, the relative
clause modifies the subject. The relative clause,
in turn, consists of a subject (the relative pro-
noun), an adverbial phrase modifying the verb
(GF: V-MOD), a named entity predicate (EN-
ADD, GF: PRED), and the finite verb phrase. The
comparison of this tree to other trees in its tree
set will then be based on the following nodes:
NX:ON VXFIN:HD NX:OA PTKVC:VPT R-
SIMPX:ON-MOD NX:ON ADVX:V-MOD EN-
ADD:PRED VXFIN:HD. Precision and recall are
generally calculated based on the number of iden-
tical constituents between two trees. Two con-
stituents are considered identical if they have the
same node label and grammatical function and if
they cover the same range of words (i.e. have the
same yield). For our comparison, the concrete
length of constituents is irrelevant, as long as the
sequential order of the constituents is identical.
Thus, in order to abstract from the length of con-
stituents, their yield is normalized: All phrases are
set to length 1, the yield of a clause is determined
by the yields of its daughters. After this step, pre-
cision and recall are calculated on all pairs of trees
in a tree set. Thus, if a set contains 3 trees, tree 1 is
compared to tree 2 and 3, and tree 2 is compared to
tree 3. Since all pairs of trees are compared, there
is no clear separation of precision and recall, pre-
cision being the result of comparing tree A to B in
the pair and recall being the result of comparing B
to A. As a consequence only the F   -measure, a
combination of precision and recall, is used.
As mentioned above, the experiment is con-
ducted with chunk sequences based on complete
sentences and the main chunk types. The average
F-measure for the 1261 tree sets is 46.49%, a clear
indication that randomly selecting a tree from a
tree set is not sufficient. Only a very small number
of sets, 62, consists of completely identical trees,
and most of these sets contain only two trees.
The low F-measure can in part be explained
78
0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15
500 501 502 503 504 505 506 507 508
509 510 511 512 513 514 515
516 517
518 519
520
521
Der
ART
Autokonvoi
NN
mit
APPR
den
ART
Probenbesuchern
NN
f?hrt
VVFIN
eine
ART
Stra?e
NN
entlang
PTKVZ
,
$,
die
PRELS
noch
ADV
heute
ADV
Lagerstra?e
NN
hei?t
VVFIN
.
$.
? HD ? HD HD ? HD VPT HD HD HD HD
?
NX
HD
VXFIN
HD
NX
OA
NX
ON
ADVX
? HD
NX
?
VXFIN
HD
NX
HD
PX
?
ADVX
V?MOD
EN?ADD
PRED
NX
ON
C
?
MF
?
VC
?
R?SIMPX
OA?MOD
VF
?
LK
?
MF
?
VC
?
NF
?
SIMPX
Figure 3: The Tu?Ba-D/Z tree for sentence (5).
by the relatively free word order of German: In
contrast to English, the grammatical function of
a noun phrase in German cannot be determined by
its position in a sentence. Thus, if the partial parser
returns the chunk sequence ?NC VCL NC NC?, it
is impossible to tell which of the noun phrases is
the subject, the accusative object, or the dative ob-
ject. As a consequence, all trees with these three
arguments will appear in the same tree set. Since
German additionally displays case syncretism be-
tween nominative and accusative, a morphological
analysis can also only provide partial disambigua-
tion. As a consequence, it is clear that the selec-
tion of the correct syntax tree for an input sentence
needs to be based on a selection module that uti-
lizes lexical information.
Another source of differences in the trees are er-
rors in the partial analysis. In the tree set for the
chunk sequence ?NC VCL AVC PC PC VCR?,
there are sentences with rather similar structure,
one of them being shown in (6). Most of them
only differ in the grammatical functions assigned
to the prepositional phrases, which can serve ei-
ther as complements or adjuncts. However, the
tree set alo contains sentence (7).
(6) Die
The
Bru?der
brothers
im
in the
wehrfa?higen
fit for military service
Alter
age
seien
had
schon
already
vor
before
der
the
Polizeiaktion
police operation
in
into
die
the
Wa?lder
woods
geflohen.
fled.
?Those brothers who are considered fit for military
service had already fled into the woods before the
police operation.?
(7) Das
This
gilt
holds
auch
also
fu?r
for
den
the
Umfang,
extent,
in
to
dem
which
Montenegro
Montenegro
attakkiert
attacked
wird.
is.
?This is also true for the extent to which Montene-
gro is being attacked.?
In sentence (7), the relative pronoun was erro-
neously POS tagged as a definite determiner, thus
allowing an analysis in which the two phrases in
dem and Montenegro are grouped as a preposi-
tional chunk. As a consequence, no relative clause
was found. The corresponding trees, however,
are annotated correctly, and the similarity between
those two sentences is consequently low.
The low F-measure should not be taken as a
completely negative result. Admittedly, it necessi-
tates a rather complex tree selection module. The
positive aspect of this one-to-many relation be-
tween chunk sequences and trees is its generality.
If only very similar trees shared a tree set, then we
would need many chunk sequences. In this case,
the problem would be moved towards the question
how to extract a maximal number of different par-
tial parses from a limited number of training sen-
tences.
6 Consequences for a Case-Based Parser
The experiments in the previous two sections show
that the chunk sequences extracted from a par-
tial parse can serve as indicators for syntax trees.
While the best definition of chunk sequences can
only be determined empirically, the results pre-
sented in the previous section allow some conclu-
sions on how the parser must be designed.
6.1 Consequences for Matching Chunk
Sequences and Trees
From the experiments in section 4, it is clear that
a good measure of information needs to be found
for an optimal selection process. There needs to
be a good equilibrium between a high coverage
of different chunk sequences and a low number
of trees per chunk sequence. One possibility to
79
reach the first goal would be to ignore certain types
of phrases in the extraction of chunk sequences
from the partial parse. However, the experiments
show that it is impossible to reduce the informa-
tiveness of the chunk sequence to a level where all
possible chunk sequences are present in the train-
ing data. This means that the procedure which
matches the chunk sequence of the input sentence
to the chunk sequences in the training data must be
more flexible than a strict left-to-right comparison.
In (Ku?bler, 2004a; Ku?bler, 2004b), we allowed the
deletion of chunks in either the input sentence or
the training sentence. The latter operation is un-
critical because it results in a deletion of some part
of the syntax tree. The former operation, however,
is more critical, it either leads to a partial syntac-
tic analysis in which the deleted chunk is not at-
tached to the tree or to the necessity of guessing
the node to which the additional constituent needs
to be attached and possibly guessing the grammat-
ical function of the new constituent. Instead of
this deletion, which can be applied anywhere in
the sentence, we suggest the use of Levenshtein
distance (Levenshtein, 1966). This distance mea-
sure is, for example, used for spelling correction:
Here the most similar word in the lexicon is found
which can be reached via the smallest number of
deletion, substitution, and insertion operations on
characters. Instead of operating on characters, we
suggest to apply Levenshtein distance to chunk se-
quences. In this case, deletions from the input se-
quence could be given a much higher weight (i.e.
cost) than insertions. We also suggest a modi-
fication of the distance to allow an exchange of
chunks. This modification would allow a princi-
pled treatment of the relative free word order of
German. However, if such an operation is not re-
stricted to adjacent chunks, the algorithm will gain
in complexity but since the resulting parser is still
deterministic, it is rather unlikely that this modifi-
cation will lead to complexity problems.
6.2 Consequences for the Tree Selection
As explained in section 5, there are chunk se-
quences that correspond to more than one syntax
tree. Since differences in the trees also pertain to
grammatical functions, the module that selects the
best tree out of the tree set needs to use more in-
formation than the chunk sequences used for se-
lecting the tree set. Since the holistic approach
to parsing proposed in this paper does not lend it-
self easily to selecting grammatical functions sep-
arately for single constituents, we suggest to use
lexical co-occurrence information instead to se-
lect the best tree out of the tree set for a given
sentence. Such an approach generalizes Streiter?s
(2001) approach of selecting from a set of possi-
ble trees based on word similarity. However, an
approach based on lexical information will suffer
extremely from data sparseness. For this reason,
we suggest a soft clustering approach based on a
partial parse, similar to the approach by Wagner
(2005) for clustering verb arguments for learning
selectional preferences for verbs.
7 Conclusion and Future Work
In this paper, we have approached the question
whether it is possible to construct a parser based
on ideas from case-based reasoning. Such a parser
would employ a partial analysis (chunk analysis)
of the sentence to select a (nearly) complete syntax
tree and then adapt this tree to the input sentence.
In the experiments reported here, we have
shown that it is possible to obtain a wide range
of levels of generality in the chunk sequences,
depending on the types of information extracted
from the partial anaylses and on the decision
whether to use sentences or clauses as basic seg-
ments for the extraction of chunk sequences. Once
a robust method is implemented to split trees into
subtrees based on clauses, chunk sequences can
be extracted on the clause level rather than from
complete sentences. Consequently, the tree sets
will also reach a higher cardinality. However, a
tree selection method based on lexical information
will be indispensable even then. For this tree se-
lection, a method for determining the similarity of
tree structures needs to be developed. The mea-
sure used in the experiments reported here, F ,
is only a very crude approximation, which serves
well for an initial investigation, but which is not
good enough for a parser depending on such a
similarity measure. The optimal combination of
chunk sequences and tree selection methods will
have to be determined empirically.
References
Steven Abney. 1991. Parsing by chunks. In Robert
Berwick, Steven Abney, and Caroll Tenney, editors,
Principle-Based Parsing, pages 257?278. Kluwer
Academic Publishers, Dordrecht.
80
Rens Bod. 1998. Beyond Grammar: An Experience-
Based Theory of Language. CSLI Publications,
Stanford, CA.
Walter Daelemans, Jakub Zavrel, Peter Berck, and
Steven Gillis. 1996. MBT: A memory-based part
of speech tagger-generator. In Eva Ejerhed and Ido
Dagan, editors, Proceedings of the 4th Workshop
on Very Large Corpora, pages 14?27, Copenhagen,
Denmark.
Walter Daelemans, Antal van den Bosch, and Jakub Za-
vrel. 1999. Forgetting exceptions is harmful in lan-
guage learning. Machine Learning, 34:11?43. Spe-
cial Issue on Natural Language Learning.
Claire Grover, Colin Matheson, and Andrei Mikheev.
1999. TTT: Text Tokenization Tool. Language
Technology Group, University of Edinburgh.
Tilman Ho?hle. 1986. Der Begriff ?Mit-
telfeld?, Anmerkungen u?ber die Theorie der topo-
logischen Felder. In Akten des Siebten Interna-
tionalen Germanistenkongresses 1985, pages 329?
340, Go?ttingen, Germany.
Sandra Ku?bler. 2004a. Memory-Based Parsing. John
Benjamins, Amsterdam.
Sandra Ku?bler. 2004b. Parsing without grammar?
using complete trees instead. In Nicolas Ni-
colov, Ruslan Mitkov, Galia Angelova, and Kalina
Boncheva, editors, Recent Advances in Natural Lan-
guage Processing III: Selected Papers from RANLP
2003, Current Issues in Linguistic Theory. John
Benjamins, Amsterdam.
Vladimir I. Levenshtein. 1966. Binary codes capa-
ble of correcting deletions, insertions, and reversals.
Cybernetics and Control Theory, 10(8):707?710.
Raymond J. Mooney. 1996. Comparative experiments
on disambiguating word senses: An illustration of
the role of bias in machine learning. In Proceed-
ings of the 1st Conference on Empirical Methods in
Natural Language Processing EMNLP, pages 82?
91, Philadelphia, PA.
Frank Henrik Mu?ller and Tylman Ule. 2002. Annotat-
ing topological fields and chunks?and revising POS
tags at the same time. In Proceedings of the 19th In-
ternational Conference on Computational Linguis-
tics (COLING 2002), pages 695?701, Taipei, Tai-
wan.
Frank Henrik Mu?ller. 2005. A Finite-State Approach
to Shallow Parsing and Grammatical Functions An-
notation of German. Ph.D. thesis, Seminar fu?r
Sprachwissenschaft, Universita?t Tu?bingen. Version
of 16th Nov. 2005.
Remko Scha, Rens Bod, and Khalil Sima?an. 1999.
Memory-based syntactic analysis. Journal of Ex-
perimental and Theoretical Artificial Intelligence,
11:409?440. Special Issue on Memory-Based Lan-
guage Processing.
Oliver Streiter. 2001. Recursive top-down fuzzy
match, new perspectives on memory-based pars-
ing. In Proceedings of the 15th Pacific Asia Confer-
ence on Language, Information and Computation,
PACLIC 2001, Hong Kong.
Heike Telljohann, Erhard Hinrichs, and Sandra Ku?bler.
2004. The Tu?Ba-D/Z treebank: Annotating German
with a context-free backbone. In Proceedings of the
Fourth International Conference on Language Re-
sources and Evaluation (LREC 2004), pages 2229?
2235, Lisbon, Portugal.
Erik F. Tjong Kim Sang. 2002. Memory-based named
entity recognition. In Proceedings of CoNLL-2002,
pages 203?206. Taipei, Taiwan.
Jorn Veenstra, Antal van den Bosch, Sabine Buch-
holz, Walter Daelemans, and Jakub Zavrel. 2000.
Memory-based word sense disambiguation. Com-
puters and the Humanities, Special Issue on Sense-
val, Word Sense Disambiguation, 34(1/2):171?177.
Andreas Wagner. 2005. Learning Thematic Role Rela-
tions for Lexical Semantic Nets. Ph.D. thesis, Uni-
versita?t Tu?bingen.
81
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 111?119,
Sydney, July 2006. c?2006 Association for Computational Linguistics
Is it Really that Difficult to Parse German?
Sandra Ku?bler, Erhard W. Hinrichs, Wolfgang Maier
SfS-CL, SFB 441, University of Tu?bingen
Wilhelmstr. 19
72074 Tu?bingen, Germany
 
kuebler,eh,wmaier@sfs.uni-tuebingen.de
Abstract
This paper presents a comparative study
of probabilistic treebank parsing of Ger-
man, using the Negra and Tu?Ba-D/Z tree-
banks. Experiments with the Stanford
parser, which uses a factored PCFG and
dependency model, show that, contrary to
previous claims for other parsers, lexical-
ization of PCFG models boosts parsing
performance for both treebanks. The ex-
periments also show that there is a big
difference in parsing performance, when
trained on the Negra and on the Tu?Ba-
D/Z treebanks. Parser performance for the
models trained on Tu?Ba-D/Z are compara-
ble to parsing results for English with the
Stanford parser, when trained on the Penn
treebank. This comparison at least sug-
gests that German is not harder to parse
than its West-Germanic neighbor language
English.
1 Introduction
There have been a number of recent studies on
probabilistic treebank parsing of German (Dubey,
2005; Dubey and Keller, 2003; Schiehlen, 2004;
Schulte im Walde, 2003), using the Negra tree-
bank (Skut et al, 1997) as their underlying data
source. A common theme that has emerged from
this research is the claim that lexicalization of
PCFGs, which has been proven highly beneficial
for other languages1 , is detrimental for parsing
accuracy of German. In fact, this assumption
is by now so widely held that Schiehlen (2004)
does not even consider lexicalization as a possible
1For English, see Collins (1999).
parameter and concentrates instead only on tree-
bank transformations of various sorts in his exper-
iments.
Another striking feature of all studies men-
tioned above are the relatively low parsing F-
scores achieved for German by comparison to the
scores reported for English, its West-Germanic
neighbor, using similar parsers. This naturally
raises the question whether German is just harder
to parse or whether it is just hard to parse the Ne-
gra treebank.2
The purpose of this paper is to address pre-
cisely this question by training the Stanford parser
(Klein and Manning, 2003b) and the LoPar parser
(Schmid, 2000) on the two major treebanks
available for German, Negra and Tu?Ba-D/Z, the
Tu?bingen treebank of written German (Telljohann
et al, 2005). A series of comparative parsing
experiments that utilize different parameter set-
tings of the parsers is conducted, including lexi-
calization and markovization. These experiments
show striking differences in performance between
the two treebanks. What makes this comparison
interesting is that the treebanks are of compara-
ble size and are both based on a newspaper cor-
pus. However, both treebanks differ significantly
in their syntactic annotation scheme. Note, how-
ever, that our experiments concentrate on the orig-
inal (context-free) annotations of the treebank.
The structure of this paper is as follows: sec-
tion 2 discusses three characteristic grammatical
features of German that need to be taken into ac-
count in syntactic annotation and in choosing an
appropriate parsing model for German. Section 3
introduces the Negra and Tu?Ba-D/Z treebanks and
2German is not the first language for which this question
has been raised. See Levy and Manning (2003) for a similar
discussion of Chinese and the Penn Chinese Treebank.
111
discusses the main differences between their anno-
tation schemes. Section 4 explains the experimen-
tal setup, sections 5-7 the experiments, and section
8 discusses the results.
2 Grammatical Features of German
There are three distinctive grammatical features
that make syntactic annotation and parsing of Ger-
man particularly challenging: its placement of the
finite verb, its flexible phrasal ordering, and the
presence of discontinuous constituents. These fea-
tures will be discussed in the following subsec-
tions.
2.1 Finite Verb Placement
In German, the placement of finite verbs depends
on the clause type. In non-embedded assertion
clauses, the finite verb occupies the second posi-
tion in the clause, as in (1a). In yes/no questions,
as in (1b), the finite verb appears clause-initially,
whereas in embedded clauses it appears clause fi-
nally, as in (1c).
(1) a. Peter
Peter
wird
will
das
the
Buch
book
gelesen
read
haben.
have
?Peter will have read the book.?
b. Wird
Will
Peter
Peter
das
the
Buch
book
gelesen
have
haben?
read
?Will Peter have read the book??
c. dass
that
Peter
Peter
das
the
Buch
book
gelesen
read
haben
have
wird.
will
?... that Peter will have read the book.?
Regardless of the particular clause type, any
cluster of non-finite verbs, such as gelesen haben
in (1a) and (1b) or gelesen haben wird in (1c), ap-
pears at the right periphery of the clause.
The discontinuous positioning of the verbal el-
ements in verb-first and verb-second clauses is the
traditional reason for structuring German clauses
into so-called topological fields (Drach, 1937;
Erdmann, 1886; Ho?hle, 1986). The positions of
the verbal elements form the Satzklammer (sen-
tence bracket) which divides the sentence into a
Vorfeld (initial field), a Mittelfeld (middle field),
and a Nachfeld (final field). The Vorfeld and the
Mittelfeld are divided by the linke Satzklammer
(left sentence bracket), which is realized by the
finite verb or (in verb-final clauses) by a comple-
mentizer field. The rechte Satzklammer (right sen-
tence bracket) is realized by the verb complex and
consists of verbal particles or sequences of verbs.
This right sentence bracket is positioned between
the Mittelfeld and the Nachfeld. Thus, the theory
of topological fields states the fundamental regu-
larities of German word order.
The topological field structures in (2) for the ex-
amples in (1) illustrate the assignment of topolog-
ical fields for different clause types.
(2) a.       Peter     wird        das
Buch    	   
 gelesen haben.  
b.   Wird        Peter     das Buch  
 	   
 gelesen haben?  
c.    
  dass         Peter     das
Buch    	   
 gelesen haben wird.  
(2a) and (2b) are made up of the following
fields: LK (for: linke Satzklammer) is occupied
by the finite verb. MF (for: Mittelfeld) contains
adjuncts and complements of the main verb. RK
(for: rechte Satzklammer) is realized by the ver-
bal complex (VC). Additionally, (2a) realizes the
topological field VF (for: Vorfeld), which contains
the sentence-initial constituent. The left sentence
bracket (LK) in (2c) is realized by a complemen-
tizer field (CF) and the right sentence bracket (RK)
by a verbal complex (VC) that contains the finite
verb wird.
2.2 Flexible Phrase Ordering
The second noteworthy grammatical feature of
German concerns its flexible phrase ordering. In
(3), any of the three complements and adjuncts
of the main verb (ge)lesen can appear sentence-
initially.
(3) a. Der
The
Mann
man
hat
has
gestern
yesterday
den
the
Roman
novel
gelesen.
read
?The man read the novel yesterday.?
b. Gestern hat der Mann den Roman gelesen
c. Den Roman hat der Mann gestern gelesen
In addition, the ordering of the elements that oc-
cur in the Mittelfeld is also free so that there are
two possible linearizations for each of the exam-
ples in (3a) - (3b), yielding a total of six distinct
orderings for the three complements and adjuncts.
Due to this flexible phrase ordering, the gram-
matical functions of constituents in German, un-
like for English, cannot be deduced from the con-
stituents? location in the tree. As a consequence,
parsing approaches to German need to be based on
treebank data which contain a combination of con-
stituent structure and grammatical functions ? for
parsing and evaluation.
112
0 1 2 3 4 5 6 7 8 9 10 11
500 501 502
503
504
Diese
PDAT
Metapher
NN
kann
VMFIN
die
ART
Freizeitmalerin
NN
durchaus
ADV
auch
ADV
auf
APPR
ihr
PPOSAT
Leben
NN
anwenden
VVINF
.
$.
NK NK NK NK MO AC NK NK
NP
OA
PP
MO HD
HD
NP
SB MO
VP
OC
S
Figure 1: A sample tree from Negra.
2.3 Discontinuous Constituents
A third characteristic feature of German syntax
that is a challenge for syntactic annotation and
for parsing is the treatment of discontinuous con-
stituents.
(4) Der
The
Mann
man
hat
has
gestern
yesterday
den
the
Roman
novel
gelesen,
read
den
which
ihm
him
Peter
Peter
empfahl.
recommended
?Yesterday the man read the novel which Peter rec-
ommended to him.?
(5) Peter
Peter
soll
is to
dem
the
Mann
man
empfohlen
recommended
haben,
have
den
the
Roman
novel
zu
to
lesen.
read
?Peter is said to have recommended to the man to
read the novel.?
(4) shows an extraposed relative clause which
is separated from its head noun den Roman by the
non-finite verb gelesen. (5) is an example of an
extraposed non-finite VP complement that forms a
discontinuous constituent with its governing verb
empfohlen because of the intervening non-finite
auxiliary haben. Such discontinuous structures
occur frequently in both treebanks and are handled
differently in the two annotation schemes, as will
be discussed in more detail in the next section.
3 The Negra and the Tu?Ba-D/Z
Treebanks
Both treebanks use German newspapers as their
data source: the Frankfurter Rundschau news-
paper for Negra and the ?die tageszeitung? (taz)
newspaper for Tu?Ba-D/Z. Negra comprises 20 000
sentences, Tu?Ba-D/Z 15 000 sentences. There is
evidence that the complexity of sentences in both
treebanks is comparable: sentence length as well
as the percentage of clause nodes per sentence is
comparable. In Negra, a sentence is 17.2 words
long, in Tu?ba-D/Z, 17.5 words. Negra has an av-
erage of 1.4 clause nodes per sentence, Tu?Ba-D/Z
1.5 clause nodes.
Both treebanks use an annotation framework
that is based on phrase structure grammar and that
is enhanced by a level of predicate-argument struc-
ture. Annotation for both was performed semi-
automatically. Despite all these similarities, the
treebank annotations differ in four important as-
pects: 1) Negra does not allow unary branching
whereas Tu?Ba-D/Z does; 2) in Negra, phrases re-
ceive a flat annotation whereas Tu?Ba-D/Z uses
phrase internal structure; 3) Negra uses crossing
branches to represent long-distance relationships
whereas Tu?Ba-D/Z uses a pure tree structure com-
bined with functional labels to encode this infor-
mation; 4) Negra encodes grammatical functions
in a combination of structural and functional la-
beling whereas Tu?Ba-D/Z uses a combination of
topological fields functional labels, which results
in a flatter structure on the clausal level. The two
treebanks also use different notions of grammat-
ical functions: Tu?Ba-D/Z defines 36 grammati-
cal functions covering head and non-head infor-
mation, as well as subcategorization for comple-
ments and modifiers. Negra utilizes 48 grammat-
ical functions. Apart from commonly accepted
grammatical functions, such as SB (subject) or
OA (accusative object), Negra grammatical func-
tions comprise a more extended notion, e.g. RE
(repeated element) or RC (relative clause).
(6) Diese
This
Metapher
metaphor
kann
can
die
the
Freizeitmalerin
amateur painter
durchaus
by all means
auch
also
auf
to
ihr
her
Leben
life
anwenden.
apply.
?The amateur painter can by all means apply this
metaphor also to her life.?
Figure 1 shows a typical tree from the Negra
treebank for sentence (6). The syntactic categories
are shown in circular nodes, the grammatical func-
tions as edge labels in square boxes. A major
113
0 1 2 3 4 5 6 7 8 9 10 11
500 501 502
503
504
Diese
PDAT
Metapher
NN
kann
VMFIN
die
ART
Freizeitmalerin
NN
durchaus
ADV
auch
ADV
auf
APPR
ihr
PPOSAT
Leben
NN
anwenden
VVINF
.
$.
NK NK NK NK MO AC NK NK
PP
MO HD
NP
OA HD
NP
SB MO
VP
OC
S
Figure 2: A Negra tree with resolved crossing branches.
0 1 2 3 4 5 6 7 8 9 10 11 12 13
500 501 502 503 504 505
506 507 508 509 510
511 512
513
Den
ART
vorigen
ADJA
Sonntag
NN
h?tte
VAFIN
Frank
NE
Michael
NE
Nehr
NE
am
PTKA
liebsten
ADJD
aus
APPR
dem
ART
Kalender
NN
gestrichen
VVPP
.
$.
HD HD ? ? ? ? HD ? HD HD
?
ADJX
? HD
VXFIN
HD
NX
? ?
NX
HD
VXINF
OV
NX
OA
EN?ADD
ON
ADJX
MOD
PX
FOPP
VF
?
LK
?
MF
?
VC
?
SIMPX
Figure 3: A sample tree from Tu?ba-D/Z.
phrasal category that serves to structure the sen-
tence as a whole is the verb phrase (VP). It con-
tains non-finite verbs (here: anwenden) together
with their complements (here: the accusative ob-
ject Diese Metapher) and adjuncts (here: the ad-
verb durchaus and the PP modifier auch auf ihr
Leben). The subject NP (here: die Freizeitma-
lerin) stands outside the VP and, depending on its
linear position, leads to crossing branches with the
VP. This happens in all cases where the subject
follows the finite verb as in Figure 1. Notice also
that the PP is completely flat and does not contain
an internal NP.
Another phenomenon that leads to the introduc-
tion of crossing branches in the Negra treebank are
discontinuous constituents of the kind illustrated
in section 2.3. Extraposed relative clauses, as in
(4), are analyzed in such a way that the relative
clause constituent is a sister of its head noun in the
Negra tree and crosses the branch that dominates
the intervening non-finite verb gelesen.
The crossing branches in the Negra treebank
cannot be processed by most probabilistic parsing
models since such parsers all presuppose a strictly
context-free tree structure. Therefore the Negra
trees must be transformed into proper trees prior
to training such parsers. The standard approach
for this transformation is to re-attach crossing non-
head constituents as sisters of the lowest mother
node that dominates all constituents in question in
the original Negra tree.
Figure 2 shows the result of this transformation
of the tree in Figure 1. Here, the fronted accusative
object Diese Metapher is reattached on the clause
level. Crossing branches do not only arise with re-
spect to the subject at the sentence level but also in
cases of extraposition and fronting of partial con-
stituents. As a result, approximately 30% of all
Negra trees contain at least one crossing branch.
Thus, tree transformations have a major impact
on the type of constituent structures that are used
for training probabilistic parsing models. Previous
work, such as Dubey (2005), Dubey and Keller
(2003), and Schiehlen (2004), uses the version of
Negra in which the standard approach to resolving
crossing branches has been applied.
(7) Den
The
vorigen
previous
Sonntag
Sunday
ha?tte
would have
Frank
Frank
Michael
Michael
Nehr
Nehr
am liebsten
preferably
aus
from
dem
the
Kalender
calendar
gestrichen.
deleted.
?Frank Michael Nehr would rather have deleted the
previous Sunday from the calendar.?
Figure 3 shows the Tu?Ba-D/Z annotation for
sentence (7), a sentence with almost identi-
cal phrasal ordering to sentence (6). Crossing
branches are avoided by the introduction of topo-
114
0 1 2 3 4 5 6 7 8 9
500 501 502 503 504 505
506 507 508 509
510
511
F?r
APPR
diese
PDAT
Behauptung
NN
hat
VAFIN
Beckmeyer
NE
bisher
ADV
keinen
PIAT
Nachweis
NN
geliefert
VVPP
.
$.
? HD HD HD HD ? HD HD
?
NX
HD
VXFIN
HD
NX
ON
ADVX
MOD
NX
OA
VXINF
OV
PX
OA?MOD
VF
?
LK
?
MF
?
VC
?
SIMPX
Figure 4: Tu?Ba-D/Z annotation without crossing branches.
logical structures (here: VF, MF and VC) into the
tree. Notice also that compared to the Negra anno-
tation, Tu?Ba-D/Z introduces more internal struc-
ture into NPs and PPs.
(8) Fu?r
For
diese
this
Behauptung
claim
hat
has
Beckmeyer
Beckmeyer
bisher
yet
keinen
no
Nachweis
evidence
geliefert.
provided.
?For this claim, Beckmeyer has not provided evi-
dence yet.?
In Tu?Ba-D/Z, long-distance relationships are
represented by a pure tree structure and specific
functional labels. Figure 4 shows the Tu?Ba-D/Z
annotation for sentence (8). In this sentence,
the prepositional phrase Fu?r diese Behauptung is
fronted. Its functional label (OA-MOD ) provides
the information that it modifies the accusative ob-
ject (OA ) keinen Nachweis.
4 Experimental Setup
The main goals behind our experiments were
twofold: (1) to re-investigate the claim that lex-
icalization is detrimental for treebank parsing of
German, and (2) to compare the parsing results for
the two German treebanks.
To investigate the first issue, the Stanford Parser
(Klein and Manning, 2003b), a state-of-the-art
probabilistic parser, was trained with both lexical-
ized and unlexicalized versions of the two tree-
banks (Experiment I). For lexicalized parsing, the
Stanford Parser provides a factored probabilistic
model that combines a PCFG model with a depen-
dency model.
For the comparison between the two treebanks,
two types of experiments were performed: a
purely constituent-based comparison using both
the Stanford parser and the pure PCFG parser
LoPar (Schmid, 2000) (Experiment II), and an in-
depth evaluation of the three major grammatical
functions subject, accusative object, and dative
object, using the Stanford parser (Experiment III).
All three experiments use gold POS tags ex-
tracted from the treebanks as parser input. All
parsing results shown below are averaged over a
ten-fold cross-validation of the test data. Experi-
ments I and II used versions of the treebanks that
excluded grammatical information, thus only con-
tained constituent labeling. For Experiment III,
all syntactic labels were extended by their gram-
matical function (e.g NX-ON for a subject NP in
Tu?Ba-D/Z or NP-SB for a Negra subject). Experi-
ments I and II included all sentences of a maximal
length of 40 words. Due to memory limitations
(7 GB), Experiment III had to be restricted to sen-
tences of a maximal length of 35 words.
5 Experiment I: Lexicalization
Experiment I investigates the effect of lexicaliza-
tion on parser performance for the Stanford Parser.
The results, summarized in Table 1, show that lex-
icalization improves parser performance for both
the Negra and the Tu?Ba-D/Z treebank in compar-
ison to unlexicalized counterpart models: for la-
beled bracketing, an F-score improvement from
86.48 to 88.88 for Tu?Ba-D/Z and an improve-
ment from 66.92 to 67.13 for Negra. This di-
rectly contradicts the findings reported by Dubey
and Keller (2003) that lexicalization has a nega-
tive effect on probabilistic parsing models for Ger-
man. We therefore conclude that these previous
claims, while valid for particular configurations of
115
Negra Tu?Ba-D/Z
precision recall F-score precision recall F-score
Stanford PCFG unlabeled 71.24 72.68 71.95 93.07 89.41 91.20
labeled 66.26 67.59 66.92 88.25 84.78 86.48
Stanford lexicalized unlabeled 71.31 73.12 72.20 91.60 91.21 91.36
labeled 66.30 67.99 67.13 89.12 88.65 88.88
Table 1: The results of lexicalizing German.
Negra Tu?Ba-D/Z
precision recall F-score precision recall F-score
LoPar unlabeled 70.84 72.51 71.67 92.62 88.58 90.56
labeled 65.86 67.41 66.62 87.39 83.57 85.44
Stanford unlabeled 71.24 72.68 71.95 93.07 89.41 91.20
labeled 66.26 67.59 66.92 88.25 84.78 86.48
Stanford + markov unlabeled 74.13 74.12 74.12 92.28 90.90 91.58
labeled 69.96 69.95 69.95 89.86 88.51 89.18
Table 2: A comparison of unlexicalized parsing of Negra and Tu?Ba-D/Z.
parsers and parameters, should not be generalized
to claims about probabilistic parsing of German in
general.
Experiment I also shows considerable differ-
ences in the overall scores between the two tree-
banks, with the F-scores for Tu?Ba-D/Z parsing ap-
proximating scores reported for English, but with
Negra scores lagging behind by an average mar-
gin of appr. 20 points. Of course, it is impor-
tant to note that such direct comparisons with En-
glish are hardly possible due to different annota-
tion schemes, different underlying text corpora,
etc. Nevertheless, the striking difference in parser
performance between the two German treebanks
warrants further attention. Experiments II and III
will investigate this matter in more depth.
6 Experiment II: Different Parsers
The purpose of Experiment II is to rule out the pos-
sibility that the differences in parser performance
for the two German treebanks produced by Ex-
periment I may just be due to using a particular
parser ? in this particular case the hybrid PCFG
and dependency model of the Stanford parser. Af-
ter all, Experiment I also yielded different results
concerning the received wisdom about the utility
of lexicalization from previously reported results.
In order to obtain a broader experimental base, un-
lexicalized models of the Stanford parser and the
pure PCFG parser LoPar were trained on both tree-
banks. In addition we experimented with two dif-
ferent parameter settings of the Stanford parser,
one with and one without markovization. The ex-
periment with markovization used parent informa-
tion (v=1) and a second order Markov model for
horizontal markovization (h=2). The results, sum-
marized in Table 2, show that parsing results for all
unlexicalized experiments show roughly the same
20 point difference in F-score that were obtained
for the lexicalized models in Experiment I. We
can therefore conclude that the difference in pars-
ing performance is robust across two parsers with
different parameter settings, such as lexicalization
and markovization.
Experiment II also confirms the finding of Klein
and Manning (2003a) and of Schiehlen (2004) that
horizontal and vertical markovization has a pos-
itive effect on parser performance. Notice also
that markovization with unlexicalized grammars
yields almost the same improvement as lexicaliza-
tion does in Experiment I.
7 Experiment III: Grammatical
Functions
In Experiments I and II, only constituent structure
was evaluated, which is highly annotation depen-
dent. It could simply be the case that the Tu?Ba-
D/Z annotation scheme contains many local struc-
tures that can be easily parsed by a PCFG model
or the hybrid Stanford model. Moreover, such
easy to parse structures may not be of great im-
portance when it comes to determining the cor-
rect macrostructure of a sentence. To empirically
verify such a conjecture, a separate evaluation of
116
0 1 2 3 4
500
Moran
NE
ist
VAFIN
l?ngst
ADV
weiter
ADJD
.
$.
SB HD MO PD
S
Figure 5: Negra annotation without unary nodes.
Negra Tu?Ba-D/Z
lab. prec. lab. rec. lab. F-score lab. prec. lab. rec. lab. F-score
without gramm. functions 69.96 69.95 69.95 89.86 88.51 89.18
all gramm. functions 47.20 56.43 51.41 75.73 74.93 75.33
subjects 52.50 58.02 55.12 66.82 75.93 71.08
accusative objects 35.14 36.30 35.71 43.84 47.31 45.50
dative objects 8.38 3.58 5.00 24.46 9.96 14.07
Table 3: A comparison of unlexicalized, markovized parsing of constituent structure and grammatical
functions in Negra and Tu?Ba-D/Z.
parser performance for different constituent types
would be necessary. However, even such an eval-
uation would only be meaningful if the annotation
schemes agree on the defining characteristics of
such constituent types. Unfortunately, this is not
the case for the two treebanks under considera-
tion. Even for arguably theory-neutral constituents
such as NPs, the two treebanks differ considerably.
In the Negra annotation scheme, single word NPs
directly project from the POS level to the clausal
level, while in Tu?Ba-D/Z, they project by a unary
rule first to an NP. An extreme case of this Negra
annotation is shown in Figure 5 for sentence (9).
Here, all the phrases are one word phrases and are
thus projected directly to the clause level.
(9) Moran
Moran
ist
is
la?ngst
already
weiter.
further
?Moran is already one step ahead.?
There is an even more important motivation
for not focusing on the standard constituent-based
parseval measures ? at least when parsing Ger-
man. As discussed earlier in section 2.2, obtain-
ing the correct constituent structure for a German
sentence will often not be sufficient for determin-
ing its intended meaning. Due to the word order
freeness of phrases, a given NP in any one po-
sition may in principle fulfill different grammat-
ical functions in the sentence as a whole. There-
fore grammatical functions need to be explicitly
marked in the treebank and correctly assigned dur-
ing parsing. Since both treebanks encode gram-
matical functions, this information is available for
parsing and can ultimately lead to a more mean-
ingful comparison of the two treebanks when used
for parsing.
The purpose of Experiment III is to investigate
parser performance on the treebanks when gram-
matical functions are included in the trees. For
these experiments, the unlexicalized, markovized
PCFG version of the Stanford parser was used,
with markovization parameters v=1 and h=2, as
in Experiment II. The results of this experiment
are shown in Table 3. The comparison of the ex-
periments with (line 2) and without grammatical
functions (line 1) confirms the findings of Dubey
and Keller (2003) that the task of assigning cor-
rect grammatical functions is harder than mere
constituent-based parsing. When evaluating on all
grammatical functions, the results for Negra de-
crease from 69.95 to 51.41, and for Tu?Ba-D/Z
from 89.18 to 75.33. Notice however, that the rela-
tive differences between Negra and Tu?Ba-D/Z that
were true for Experiments I and II remain more or
less constant for this experiment as well.
In order to get a clearer picture of the quality
of the parser output for each treebank, it is im-
portant to consider individual grammatical func-
tions. As discussed in section 3, the overall in-
ventory of grammatical functions is different for
the two treebanks. We therefore evaluated those
grammatical functions separately that are crucial
for determining function-argument structure and
117
that are at the same time the most comparable for
the two treebanks. These are the functions of sub-
ject (encoded as SB in Negra and as ON in Tu?Ba-
D/Z), accusative object (OA ), and dative object
(DA in Negra and OD in Tu?Ba-D/Z). Once again,
the results are consistently better for Tu?Ba-D/Z
(cf. lines 3-5 in Table 3), with subjects yielding
the highest results (71.08 vs. 55.12 F-score) and
dative objects the lowest results (14.07 vs. 5.00).
The latter results must be attributed to data sparse-
ness, dative object occur only appr. 1 000 times
in each treebank while subjects occur more than
15 000 times.
8 Discussion
The experiments presented in sections 5-7 show
that there is a difference in results of appr. 20%
between Negra and Tu?Ba-D/Z. This difference is
consistent throughout, i.e. with different parsers,
under lexicalization and markovization. These re-
sults lead to the conjecture that the reasons for
these differences must be sought in the differences
in the annotation schemes of the two treebanks.
In section 3, we showed that one of the ma-
jor differences in annotation is the treatment of
discontinuous constituents. In Negra, such con-
stituents are annotated via crossing branches,
which have to be resolved before parsing. In such
cases, constituents are extracted from their mother
constituents and reattached at higher constituents.
In the case of the discontinuous VP in Figure 1,
it leads to a VP rule with the following daugh-
ters: head (HD ) and modifier (MO ), while the
accusative object is directly attached at the sen-
tence level as a sister of the VP. This conversion
leads to inconsistencies in the training data since
the annotation scheme requires that object NPs are
daughters of the VP rather than of S. The incon-
sistency introduced by tree conversion are con-
siderable since they cover appr. 30% of all Ne-
gra trees (cf. section 3). One possible explana-
tion for the better performance of Tu?ba-D/Z might
be that it has more information about the correct
attachment site of extraposed constituents, which
is completely lacking in the context-free version
of Negra. For this reason, Ku?bler (2005) and
Maier (2006) tested a version of Negra which con-
tained information of the original attachment site
of these discontinuous constituents. In this ver-
sion of Negra, the grammatical function OA in
Figure 2 would be changed to OA VP to show
that it was originally attached to the VP. Experi-
ments with this version showed a decrease in F-
score from 52.30 to 49.75. Consequently, adding
this information in a similar way to the encoding
of discontinuous constituents in Tu?ba-D/Z harms
performance.
By contrast, Tu?Ba-D/Z uses topological fields
as the primary structuring principle, which leads to
a purely context-free annotation of discontinuous
structures. There is evidence that the use of topo-
logical fields is advantageous also for other pars-
ing approaches (Frank et al, 2003; Ku?bler, 2005;
Maier, 2006).
Another difference in the annotation schemes
concerns the treatment of phrases. Negra phrases
are flat, and unary projections are not annotated.
Tu?Ba-D/Z always projects to the phrasal category
and annotates more phrase-internal structure. The
deeper structures in Tu?Ba-D/Z lead to fewer rules
for phrasal categories, which allows the parser a
more consistent treatment of such phrases. For ex-
ample, the direct attachment of one word subjects
on the clausal level in Negra leads to a high num-
ber of different S rules with different POS tags for
the subject phrase. An empirical proof for the as-
sumption that flat phrase structures and the omis-
sion of unary nodes decrease parsing results is pre-
sented by Ku?bler (2005) and Maier (2006).
We want to emphasize that our experiments
concentrate on the original context-free annota-
tions of the treebanks. We did not investigate
the influence of treebank refinement in this study.
However, we would like to note that by a com-
bination of suffix analysis and smoothing, Dubey
(2005) was able to obtain an F-score of 85.2 for
Negra. For other work in the area of treebank re-
finement using the German treebanks see Ku?bler
(2005), Maier (2006), and Ule (2003).
9 Conclusion and Future Work
We have presented a comparative study of proba-
bilistic treebank parsing of German, using the Ne-
gra and Tu?Ba-D/Z treebanks. Experiments with
the Stanford parser, which uses a factored PCFG
and dependency model, show that, contrary to
previous claims for other parsers, lexicalization
of PCFG models boosts parsing performance for
both treebanks. The experiments also show that
there is a big difference in parsing performance,
when trained on the Negra and on the Tu?Ba-D/Z
treebanks. This difference remains constant across
118
lexicalized, unlexicalized (also using the LoPar
parser), and markovized models and also extends
to parsing of major grammatical functions. Parser
performance for the models trained on Tu?Ba-D/Z
are comparable to parsing results for English with
the Stanford parser, when trained on the Penn tree-
bank. This comparison at least suggests that Ger-
man is not harder to parse than its West-Germanic
neighbor language English.
Additional experiments with the Tu?Ba-D/Z
treebank are planned in future work. A new re-
lease of the Tu?Ba-D/Z treebank has become avail-
able that includes appr. 22 000 trees, instead of
the release with 15 000 sentences used for the ex-
periments reported in this paper. This new re-
lease also contains morphological information at
the POS level, including case and number. With
this additional information, we expect consider-
able improvement in grammatical function assign-
ment for the functions subject, accusative object,
and dative object, which are marked by nomina-
tive, accusative, and dative case, respectively.
Acknowledgments
We are grateful to Helmut Schmid and to Chris
Manning and his group for making their parsers
publicly available as well as to Tylman Ule for
providing the evaluation scripts. We are also grate-
ful to the anonymous reviewers for many help-
ful comments. And we are especially grateful to
Roger Levy for all the help he gave us in creating
the language pack for Tu?Ba-D/Z in the Stanford
parser.
References
Michael Collins. 1999. Head-Driven Statistical Mod-
els for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania.
Erich Drach. 1937. Grundgedanken der Deutschen
Satzlehre. Diesterweg, Frankfurt/M.
Amit Dubey and Frank Keller. 2003. Probabilistic
parsing for German using sister-head dependencies.
In Proceedings of ACL 2003, pages 96?103, Sap-
poro, Japan.
Amit Dubey. 2005. What to do when lexicaliza-
tion fails: Parsing German with suffix analysis and
smoothing. In Proceedings of ACL 2005, Ann Ar-
bor, MI.
Oskar Erdmann. 1886. Grundzu?ge der deutschen
Syntax nach ihrer geschichtlichen Entwicklung
dargestellt. Verlag der Cotta?schen Buchhandlung,
Stuttgart, Germany.
Anette Frank, Markus Becker, Berthold Crysmann,
Bernd Kiefer, and Ulrich Scha?fer. 2003. Integrated
shallow and deep parsing: TopP meets HPSG. In
Proceedings of ACL 2003, Sapporo, Japan.
Tilman Ho?hle. 1986. Der Begriff ?Mittel-
feld?, Anmerkungen u?ber die Theorie der topo-
logischen Felder. In Akten des Siebten Interna-
tionalen Germanistenkongresses 1985, pages 329?
340, Go?ttingen, Germany.
Dan Klein and Christopher Manning. 2003a. Accurate
unlexicalized parsing. In Proceedings of ACL 2003,
pages 423?430, Sapporo, Japan.
Dan Klein and Christopher Manning. 2003b. Fast ex-
act inference with a factored model for natural lan-
guage parsing. In Advances in Neural Information
Processing Systems 15 (NIPS 2002), pages 3?10,
Vancouver, Canada.
Sandra Ku?bler. 2005. How do treebank annotation
schemes influence parsing results? Or how not to
compare apples and oranges. In Proceedings of
RANLP 2005, Borovets, Bulgaria.
Roger Levy and Christopher Manning. 2003. Is it
harder to parse Chinese, or the Chinese treebank? In
Proceedings of ACL 2003, pages 439?446, Sapporo,
Japan.
Wolfgang Maier. 2006. Annotation schemes and
their influence on parsing results. In Proceedings of
the ACL-2006 Student Research Workshop, Sydney,
Australia.
Michael Schiehlen. 2004. Annotation strategies for
probabilistic parsing in German. In Proceedings of
COLING 2004, Geneva, Switzerland.
Helmut Schmid. 2000. LoPar: Design and implemen-
tation. Technical report, Universita?t Stuttgart, Ger-
many.
Sabine Schulte im Walde. 2003. Experiments on
the Automatic Induction of German Semantic Verb
Classes. Ph.D. thesis, Institut fu?r Maschinelle
Sprachverarbeitung, Universita?t Stuttgart.
Wojciech Skut, Brigitte Krenn, Thorsten Brants, and
Hans Uszkoreit. 1997. An annotation scheme for
free word order languages. In Proceedings of ANLP
1997, Washington, D.C.
Heike Telljohann, Erhard W. Hinrichs, Sandra Ku?bler,
and Heike Zinsmeister, 2005. Stylebook for the
Tu?bingen Treebank of Written German (Tu?Ba-
D/Z). Seminar fu?r Sprachwissenschaft, Universita?t
Tu?bingen, Germany.
Tylman Ule. 2003. Directed treebank refinement for
PCFG parsing. In Proceedings of TLT 2003, Va?xjo?,
Sweden.
119
Parsing Morphologically Rich Languages:
Introduction to the Special Issue
Reut Tsarfaty?
Uppsala University
Djame? Seddah??
Universite? Paris-Sorbonne/INRIA
Sandra Ku?bler?
Indiana University
Joakim Nivre?
Uppsala University
Parsing is a key task in natural language processing. It involves predicting, for each natural
language sentence, an abstract representation of the grammatical entities in the sentence and
the relations between these entities. This representation provides an interface to compositional
semantics and to the notions of ?who did what to whom.? The last two decades have seen great
advances in parsing English, leading to major leaps also in the performance of applications that
use parsers as part of their backbone, such as systems for information extraction, sentiment
analysis, text summarization, and machine translation. Attempts to replicate the success of
parsing English for other languages have often yielded unsatisfactory results. In particular,
parsing languages with complex word structure and flexible word order has been shown to
require non-trivial adaptation. This special issue reports on methods that successfully address
the challenges involved in parsing a range of morphologically rich languages (MRLs). This
introduction characterizes MRLs, describes the challenges in parsing MRLs, and outlines the
contributions of the articles in the special issue. These contributions present up-to-date research
efforts that address parsing in varied, cross-lingual settings. They show that parsing MRLs
addresses challenges that transcend particular representational and algorithmic choices.
1. Parsing MRLs
Parsing is a central task in natural language processing, where a system accepts a
sentence in a natural language as input and provides a syntactic representation of the
? Uppsala University, Department of Linguistics and Philology, Box 635, 75126 Uppsala, Sweden.
E-mail: tsarfaty@stp.lingfil.uu.se.
?? Inria?s Alpage project & Universite? Paris Sorbonne, Maison de la Recherche, 28 rue Serpentes, 75006
Paris, France. E-mail: djame.seddah@paris-sorbonne.fr.
? Indiana University, Department of Linguistics, Memorial Hall 322, Bloomington IN-47405, USA.
E-mail: skuebler@indiana.edu.
? Uppsala University, Department of Linguistics and Philology, Box 635, 75126 Uppsala, Sweden.
E-mail: joakim.nivre@lingfil.uu.se.
? 2013 Association for Computational Linguistics
Computational Linguistics Volume 39, Number 1
entities and grammatical relations in the sentence as output. The input sentences to a
parser reflect language-specific properties (in terms of the order of words, the word
forms, the lexical items, and so on), whereas the output abstracts away from these
properties in order to yield a structured, formal representation that reflects the functions
of the different elements in the sentence.
The best broad-coverage parsing systems to date use statistical models, possibly
in combination with hand-crafted grammars. They use machine learning techniques
that allow the system to generalize the syntactic patterns characterizing the data. These
machine learning methods are trained on a treebank, that is, a collection of natural
language sentences which are annotated with their correct syntactic analyses. Based on
the patterns and frequencies observed in the treebank, parsing algorithms are designed
to suggest and score novel analyses for unseen sentences, and search for the most likely
analysis.
The release of a large-scale annotated corpus for English, the Wall Street Journal
Penn Treebank (PTB) (Marcus, Santorini, and Marcinkiewicz 1993), led to a significant
leap in the performance of statistical parsing for English (Magerman 1995; Collins 1997;
Charniak 2000; Charniak and Johnson 2005; Petrov et al 2006; Huang 2008; Finkel,
Kleeman, and Manning 2008; Carreras, Collins, and Koo 2008). At the time of their
publication, each of these models improved the state-of-the-art of English parsing,
bringing constituency-based parsing performance on the standard test set of the PTB
to the level of 92% F1-score using the PARSEVAL evaluation metrics (Black et al 1991).
The last decade has seen the development of large-scale annotated treebanks
for languages such as Arabic (Maamouri et al 2004), French (Abeille?, Cle?ment, and
Toussenel 2003), German (Uszkoreit 1987; Skut et al 1997), Hebrew (Sima?an et al
2001), Swedish (Nivre and Megyesi 2007), and others. The availability of syntactically
annotated corpora for these languages had initially raised the hope of attaining the
same level of parsing performance on these languages, by simply porting the existing
models to the newly available corpora.
Early attempts to apply the aforementioned constituency-based parsing models to
other languages have demonstrated that the success of these approaches was rather lim-
ited. This observation was confirmed for individual languages such as Czech (Collins
et al 1999), German (Dubey and Keller 2003), Italian (Corazza et al 2004), French (Arun
and Keller 2005), Modern Standard Arabic (Kulick, Gabbard, andMarcus 2006), Modern
Hebrew (Tsarfaty and Sima?an 2007), and many more (Tsarfaty et al 2010).
The same observation was independently confirmed by parallel research efforts on
data-driven dependency-based parsing (Ku?bler, McDonald, and Nivre 2009). Results
coming from multilingual parsing evaluation campaigns, such as the CoNLL shared
tasks on multilingual dependency parsing, showed significant variation in the results
of the same models applied to a range of typologically different languages. In partic-
ular, these results demonstrated that the morphologically rich nature of some of those
languages makes them inherently harder to parse, regardless of the parsing technique
used (Buchholz and Marsi 2006; Nivre et al 2007a).
Morphologically rich languages (MRLs) express multiple levels of information al-
ready at the word level. The lexical information for each word form in an MRL may
be augmented with information concerning the grammatical function of the word in
the sentence, its grammatical relations to other words, pronominal clitics, inflectional
affixes, and so on. In English, many of these notions are expressed implicitly by word
order and adjacency: The direct object, for example, is generally the first NP after the
verb and thus does not necessarily need an explicit marking. Expressing such functional
information morphologically allows for a high degree of word-order variation, since
16
Tsarfaty et al Parsing Morphologically Rich Languages:
grammatical functions need no longer be strongly associated with syntactic positions.
Furthermore, lexical items appearing in different syntactic contexts may be realized in
different forms. This leads to a high level of word-form variation and complicates lexical
acquisition from small sized corpora.
2. The Overarching Challenges
The complexity of the linguistic patterns found in MRLs was shown to challenge
parsing in many ways. For instance, standard models assume that a word always
corresponds to a unique terminal in the parse tree. In Arabic, Hebrew, Turkish, and other
languages, an input word-token may correspond to multiple terminals. Furthermore,
models developed primarily to parse English draw substantial inference based onword-
order patterns. Parsing non-configurational languages such as Hungarian may require
relying on morphological information to infer equivalent functions. Parsing Czech or
German is further complicated by case syncretism, which precludes a deterministic
correlation between morphological case and grammatical functions. In languages such
as Hungarian or Finnish, the diversity of word forms leads to a high rate of out-of-
vocabulary words unseen in the annotated data. MRL parsing is thus often associated
with increased lexical data sparseness. An MRL parser requires robust statistical meth-
ods for analyzing such phenomena.
Following Tsarfaty et al (2010) we distinguish three overarching challenges that are
associated with parsing MRLs.
(i) The Architectural Challenge. Contrary to English, where the input signal uniquely
determines the sequence of tree terminals, word forms in an MRLmay contain multiple
units of information (morphemes). These morphemes have to be segmented in order to
reveal the basic units of analysis. Furthermore, morphological analysis of MRL words
may be highly ambiguous, and morphological segmentation may be a non-trivial task
for certain languages. Therefore, a parsing architecture for an MRL must contain, at the
very least, a morphological component for segmentation and a syntactic component for
parsing. The challenge is thus to determine how these two models should be combined
in the overall parsing architecture: Should we assume a pipeline architecture, where the
morphological segmentation is disambiguated prior to parsing? Or should we construct
a joint architecture where the model picks out a parse tree and a segmentation at once?
(ii) The Modeling Challenge. The design of a statistical parsing model requires specifying
three formal elements: the formal output representation, the events that can be observed
in the data, and the independence assumptions between these events. For anMRL, com-
plex morphosyntactic interactions may impose constraints on the form of events and on
their possible combination. In such cases, we may need to incorporate morphological
information in the syntactic model explicitly. How should morphological information
be treated in the syntactic model: as explicit tree decoration, as hidden variables, or as
complex objects in their own right? Which morphological features should be explicitly
encoded? Where should we mark morphological features: at the part-of-speech level, at
phrase level, on dependency arcs? How domorphological and syntactic events interact,
and how can we exploit these interactions for inferring correct overall structures?
(iii) The Lexical Challenge. A parsing model for MRLs requires recognizing the morpho-
logical information in each word form. Due to the high level of morphological variation,
however, data-driven systems are not guaranteed to observe all morphological variants
17
Computational Linguistics Volume 39, Number 1
of a word form in a given annotated corpus. How can we assign correct morphological
signatures to the lexical items in the face of such extreme data spareseness? When
devising a model for parsing MRLs, one may want to make use of whatever additional
resources one has access to?morphological analyzers, unlabeled data, and lexica?in
order to extend the coverage of the parser and obtain robust and accurate predictions.
3. Contributions of this Special Issue
This special issue draws attention to the different ways in which researchers work-
ing on parsing MRLs address the challenges described herein. It contains six stud-
ies discussing parsing results for six languages, using both constituency-based and
dependency-based frameworks (cf. Table 1). The first three studies (Seeker and Kuhn;
Fraser et al; Kallmeyer and Maier) focus on parsing European languages and deal
with phenomena that lie within their flexible phrase ordering and rich morphology,
including problems posed by case syncretism. The next two papers (Goldberg and
Elhadad; Marton et al) focus on Semitic languages and study the application of general-
purpose parsing algorithms (constituency-based and dependency-based, respectively)
to parsing such data. They empirically show gaps in performance between different
architectures (pipeline vs. joint , gold vs. machine-predicted input), feature choices, and
techniques for increasing lexical coverage of the parser. The last paper (Green et al) is
a comparative study on multi-word expression (MWE) recognition via two specialized
parsing models applied to both French and Modern Standard Arabic. Let us briefly
outline the individual contributions made by each of the articles in this special issue.
Seeker and Kuhn present a comparative study of dependency parsing for three
European MRLs from different typological language families: German (Germanic),
Czech (Slavonic), and Hungarian (Finno-Ugric). Although all these languages possess
richer morphological marking than English, there is variation among these languages
in terms of the richness of the morphological information encoded in the word forms,
and the ambiguity of these morphological markers. Hungarian is agglutinating, that
is, morphological markers in Hungarian are non-ambiguous and easy to recognize.
German and Czech are fusional languages with different types of case syncretism.
Seeker and Kuhn use the Bohnet Parser (Bohnet 2010) to parse all these languages, and
show that not using morphological information in the statistical feature model is detri-
mental. Using gold morphology significantly improves results for all these languages,
whereas automatically predicted morphology leads to smaller improvements for the
fusional languages, relative to the agglutinating one. To combat this loss in performance,
they add linguistic constraints to the decoder, restricting the possible structures. They
show that a decoding algorithm which filters out dependency parses that do not obey
Table 1
Contributions to the CL special issue on parsing morphologically rich languages (CL-PMRL).
Constituency-Based Dependency-Based
Arabic Green, de Marneffe, and Manning 2013 Marton, Habash, and Rambow 2013
Czech Seeker and Kuhn 2013
French Green, de Marneffe, and Manning 2013
German Kallmeyer and Maier 2013 Seeker and Kuhn 2013
Fraser et al 2013
Hebrew Goldberg and Elhadad 2013
Hungarian Seeker and Kuhn 2013
18
Tsarfaty et al Parsing Morphologically Rich Languages:
predicate-argument constraints allows the authors to obtain more substantial gains
from morphology.
Fraser et al also focus on parsing German, though in a constituency-based setting.
They use a PCFG-based unlexicalized chart parser (Schmid 2004) along with a set of
manual treebank annotations that bring the treebank grammar performance to the level
of automatically predicted states learned by Petrov et al (2006). As in the previous
study, syncretism is shown to cause ambiguity that hurts parsing performance. To
combat this added ambiguity, they use external information sources. In particular, they
show different ways of using information from monolingual and bilingual data sets
in a re-ranking framework for improving parsing accuracy. The bilingual approach
is inspired by machine translation studies and exploits the variation in marking the
same grammatical functions differently across languages for increasing the confidence
of a disambiguation decision in one language by observing a parallel non-ambiguous
structure in the other one.
These two studies use German corpora stripped of discontinuous constituents in
order to benchmark their parsers. In each of these cases, the discontinuities are con-
verted into pure tree structures, thus ignoring the implied long distance dependencies.
Kallmeyer and Maier propose an alternative approach for parsing such languages
by presenting an overall solution for parsing discontinuous structures directly. They
present a parsing model based on Probabilistic Linear Context-Free Rewriting Systems
(PLCFRS), which implements many of the technological advances that were developed
in the context of parsing with PCFGs. In particular, they present a decoding algorithm
based on weighted deductive CKY parsing, and use it in conjunction with PLCFRS
parameters directly estimated from treebank data. Because PLCFRS is a powerful for-
malism, the parser needs to be tuned for speed. The authors present several admissible
heuristics that facilitate faster A* parsing. The authors present parsing results that are
competitive with constituency-based parsing of German while providing invaluable
information concerning discontinuous constituents and long distance dependencies.
Goldberg and Elhadad investigate constituency parsing for Modern Hebrew
(Semitic), a language which is known to have a very rich and ambiguous morpho-
logical structure. They empirically show that an application of the split-and-merge
general-purpose model of Petrov et al (2006) for parsing Hebrew does not guarantee
accurate parsing in and of itself. In order to obtain competitive parsing performance,
they address all three challenges we have noted. In order to deal with the problem of
word segmentation (the architectural challenge), they extend the chart-based decoder
of Petrov et al with a lattice-based decoder. In order to handle morphological marking
patterns (the modeling challenge), they refine the initial treebank with particularly
targeted state-splits, and add a set of linguistic constraints that act as a filter ruling
out trees that violate agreement. Finally, they add information from an external wide-
coverage lexicon to combat lexical sparseness (the lexical challenge). They show that the
contribution of these different methods is cumulative, yielding state-of-the-art results
on constituency parsing of Hebrew.
Marton et al study dependency parsing of Modern Standard Arabic (Semitic)
and attend to the same challenges. They show that for two transition-based parsers,
MaltParser (Nivre et al 2007b) and EasyFirst (Goldberg and Elhadad 2010), controlling
the architectural and modeling choices leads to similar effects. For instance, when
comparing parsing performance on gold and machine-predicted input conditions, they
show that rich informative tag sets are preferred in gold conditions, but smaller tag
sets are preferred in machine-predicted conditions. They further isolate a set of mor-
phological features which leads to significant improvements in the machine-predicted
19
Computational Linguistics Volume 39, Number 1
condition, for both frameworks. They also show that function-based morphological
features are more informative than surface-based features, and that performance loss
that is due to errors in part-of-speech tagging may be restored by training the model
on a joint set of trees encoding gold tags and machine-predicted tags. At the same time,
undirected parsing of EasyFirst shows better accuracy, possibly due to the flexiblity in
phrase ordering. The emerging insight is that tuning morphological information inside
general-purpose parsing systems is of crucial importance for obtaining competitive
performance.
Focusing on Modern Standard Arabic (Semitic) and French (Romance), the last
article of this special issue, by Green et al, may be seen as an applications paper,
treating the task of MWE recognition as a side effect of a joint model for parsing and
MWE identification. The key problem here is knowing what to consider a minimal
unit for parsing, and how to handle parsing in realistic scenarios where MWEs have
not yet been identified. The authors present two parsing models for such a task: a
factored model including a factored lexicon that integrates morphological knowledge
into the Stanford Parser word model (Klein and Manning 2003), and a Dirichlet Process
Tree Substitution Grammar based model (Cohn, Blunsom, and Goldwater 2010). The
latter can be roughly described as Data Oriented Parsing (Bod 1992; Bod, Scha, and
Sima?an 2003) in a Bayesian framework, extended to include specific features that ease
the extraction of tree fragments matching MWEs. Interestingly, those very different
models do provide the same range of performance when confronted with predicted
morphology input. Additional important challenges that are exposed in the context of
this study concern the design of experiments for cross-linguistic comparison in the face
of delicate asymmetries between the French and Arabic data sets.
4. Conclusion
This special issue highlights actively studied areas of research that address parsing
MRLs. Most approaches described in this issue rely on extending existing parsing
models to address three overarching challenges. The joint parsing and segmentation
architecture scenario can be addressed by extending a general-purpose CKY decoder
into a lattice-based decoder. The modeling challenge may be addressed by explicitly
marking morphological features as syntactic state-splits, by modeling discontinuities
in the formal syntactic representation directly, by incorporating hard-coded linguistic
constraints as filters, and so on. The lexical challenge can be addressed by using external
resources such as a wide-coverage lexicon for analyzing unknown words, and the use
of additional monolingual and bilingual data in order to obtain robust statistics in the
face of extreme sparseness.
An empirical observation reflected in the results presented here is that languages
which we refer to as MRLs exhibit their own cross-lingual variation and thus should not
be treated as a single, homogeneous class of languages. Some languages show richer
morphology than others; some languages possess more flexible word ordering than
others; some fusional languages show syncretism (coarse-grained underspecifiedmark-
ers) whereas others use a large set of fine-grained and unambiguous morphological
markers. The next challenge would then be to embrace these variations, and investigate
whether the typological properties of languages can inform us more directly concerning
the adequate methods that can be used to effectively parse them.
As the next research goal, we then set out to obtain a deeper understanding of
how annotation choices paired up with modeling choices systematically correlate with
parsing performance for different languages. Further work in the line of the studies
20
Tsarfaty et al Parsing Morphologically Rich Languages:
presented here is required in order to draw relevant generalizations. Furthermore,
the time is ripe for another multilingual parser evaluation campaign, which would
encourage the community to develop parsing systems that can easily be transferred
from one language type to another. By compiling these recent contributions, we hope
to encourage not only the development of novel systems for parsing individual MRLs,
but also to facilitate the search for more robust, generic cross-linguistic solutions.
Acknowledgments
As guest editors of this special issue, we
wish to thank the regular members and
the guest members of the Computational
Linguistics editorial board for their thorough
work, which allowed us to assemble this
special issue of high quality contributions in
the emerging field of parsing MRLs. We also
want to thank Marie Candito, Jennifer Foster,
Yoav Goldberg, Ines Rehbein, Lamia Tounsi,
and Yannick Versley for their contribution
to the initial proposal for this special issue.
Finally, we want to express our gratitude
to Robert Dale and Suzy Howlett for their
invaluable support throughout the editorial
process.
References
Abeille?, Anne, Lionel Cle?ment, and Franc?ois
Toussenel. 2003. Building a treebank for
French. In Anne Abeille?, editor, Treebanks.
Kluwer, Dordrecht, pages 165?188.
Arun, Abhishek and Frank Keller. 2005.
Lexicalization in crosslinguistic
probabilistic parsing: The case of French.
In Proceedings of the 43rd Annual Meeting of
the Association for Computational Linguistics,
pages 306?313, Ann Arbor, MI.
Black, Ezra, Steven Abney, Dan Flickinger,
Claudia Gdaniec, Ralph Grishman, Philip
Harrison, Donald Hindle, Robert Ingria,
Frederick Jelinek, Judith Klavans, Mark
Liberman, Mitchell Marcus, Salim Roukos,
Beatrice Santorini, and Tomek
Strzalkowski. 1991. A procedure for
quantitatively comparing the syntactic
coverage of English grammars. Speech
Communication, 33(1,2):306?311.
Bod, Rens. 1992. A computational model
of language performance: Data oriented
parsing. In Proceedings of the 14th Conference
on Computational linguistics-Volume 3,
pages 855?859, Nantes.
Bod, Rens, Remko Scha, and Khalil Sima?an,
editors. 2003. Data-Oriented Parsing. CSLI,
Stanford, CA.
Bohnet, Bernd. 2010. Top accuracy and fast
dependency parsing is not a contradiction.
In Proceedings of CoLing, pages 89?97, Sydney.
Buchholz, Sabine and Erwin Marsi. 2006.
CoNLL-X shared task on multilingual
dependency parsing. In Proceedings of
the Tenth Conference on Computational
Language Learning (CoNLL),
pages 149?164, New York, NY.
Carreras, Xavier, Michael Collins, and
Terry Koo. 2008. TAG, dynamic
programming, and the perceptron
for efficient, feature-rich parsing.
In Proceedings of the Twelfth Conference on
Computational Natural Language Learning
(CoNLL), pages 9?16, Manchester.
Charniak, Eugene. 2000. A maximum-
entropy-inspired parser. In Proceedings
of the 1st Annual Meeting of the North
American Chapter of the ACL (NAACL),
pages 132?139, Seattle, WA.
Charniak, Eugene and Mark Johnson.
2005. Coarse-to-fine n-best parsing
and maxent discriminative reranking.
In Proceedings of the 43rd Annual Meeting
of the Association for Computational
Linguistics (ACL 2005), pages 173?180,
Ann Arbor, MI.
Cohn, Trevor, Phil Blunsom, and Sharon
Goldwater. 2010. Inducing tree-
substitution grammars. The Journal of
Machine Learning Research, 11:3053?3096.
Collins, Michael. 1997. Three generative,
lexicalized models for statistical parsing.
In Proceedings of the 35th Annual Meeting of
the Association for Computational Linguistics,
pages 16?23, Madrid.
Collins, Michael, Jan Hajic?, Lance Ramshaw,
and Christoph Tillmann. 1999. A statistical
parser for Czech. In Proceedings of the 37th
Annual Meeting of the ACL, pages 505?512,
College Park, MD.
Corazza, Anna, Alberto Lavelli, Giogio Satta,
and Roberto Zanoli. 2004. Analyzing an
Italian treebank with state-of-the-art
statistical parsers. In Proceedings of the
Third Workshop on Treebanks and Linguistic
Theories (TLT 2004), pages 39?50, Tu?bingen.
Dubey, Amit and Frank Keller. 2003.
Probabilistic parsing for German using
sister-head dependencies. In Proceedings of
the 41st Annual Meeting of the Association for
Computational Linguistics, pages 96?103,
Ann Arbor, MI.
21
Computational Linguistics Volume 39, Number 1
Finkel, Jenny Rose, Alex Kleeman, and
Christopher D. Manning. 2008. Efficient,
feature-based, conditional random field
parsing. In Proceedings of ACL-08: HLT,
pages 959?967, Columbus, OH.
Goldberg, Yoav and Michael Elhadad. 2010.
An efficient algorithm for easy-first
non-directional dependency parsing.
In Human Language Technologies: The
2010 Annual Conference of the North
American Chapter of the Association
for Computational Linguistics,
pages 742?750, Los Angeles, CA.
Huang, Liang. 2008. Forest reranking:
Discriminative parsing with non-local
features. In Proceedings of ACL-08: HLT,
pages 586?594, Columbus, OH.
Klein, Dan and Christopher D. Manning.
2003. Accurate unlexicalized parsing.
In Proceedings of the 41st Annual Meeting on
Association for Computational Linguistics,
pages 423?430, Sapporo.
Ku?bler, Sandra, Ryan McDonald, and Joakim
Nivre. 2009. Dependency Parsing. Number 2
in Synthesis Lectures on Human Language
Technologies. Morgan & Claypool
Publishers.
Kulick, Seth, Ryan Gabbard, and Mitchell
Marcus. 2006. Parsing the Arabic treebank:
Analysis and improvements. In Proceedings
of the 5th International Workshop on
Treebanks and Linguistic Theories (TLT),
pages 31?42, Prague.
Maamouri, Mohamed, Anne Bies, Tim
Buckwalter, and Wigdan Mekki. 2004.
The Penn Arabic Treebank: Building a
large-scale annotated Arabic corpus. In
Proceedings of the NEMLAR Conference
on Arabic Language Resources and Tools,
pages 102?109, Cairo.
Magerman, David M. 1995. Statistical
decision-tree models for parsing.
In Proceedings of the 33rd Annual Meeting on
Association for Computational Linguistics,
pages 276?283, Cambridge, MA.
Marcus, Mitchell, Beatrice Santorini, and
Mary Ann Marcinkiewicz. 1993. Building
a large annotated corpus of English:
The Penn Treebank. Computational
Linguistics, 19(2):313?330.
Nivre, Joakim, Johan Hall, Sandra Ku?bler,
Ryan McDonald, Jens Nilsson, Sebastian
Riedel, and Deniz Yuret. 2007a. The
CoNLL 2007 shared task on dependency
parsing. In Proceedings of the CoNLL 2007
Shared Task. Joint Conference on Empirical
Methods in Natural Language Processing and
Computational Natural Language Learning
(EMNLP-CoNLL), pages 915?932, Prague.
Nivre, Joakim, Johan Hall, Jens Nilsson,
Atanas Chanev, Gu?ls?en Eryig?it,
Sandra Ku?bler, Svetoslav Marinov,
and Erwin Marsi. 2007b. MaltParser:
A language-independent system for
data-driven dependency parsing. Natural
Language Engineering, 13(2):95?135.
Nivre, Joakim and Beata Megyesi. 2007.
Bootstrapping a Swedish Treebank
using cross-corpus harmonization and
annotation projection. In Proceedings of the
Sixth International Workshop on Treebanks
and Linguistic Theories (TLT), pages 97?102,
Bergen.
Petrov, Slav, Leon Barrett, Romain Thibaux,
and Dan Klein. 2006. Learning accurate,
compact, and interpretable tree
annotation. In Proceedings of the 21st
International Conference on Computational
Linguistics and 44th Annual Meeting of the
Association for Computational Linguistics,
pages 433?440, Sydney.
Schmid, Helmut. 2004. Efficient parsing of
highly ambiguous context-free grammars
with bit vectors. In Proceedings of the 20th
International Conference on Computational
Linguistics (COLING 2004), pages 162?168,
Geneva.
Sima?an, Khalil, Alon Itai, Yoad Winter,
Alon Altmann, and Noa Nativ. 2001.
Building a tree-bank of Modern Hebrew
text. Traitement Automatique des Langues,
42:347?380.
Skut, Wojciech, Brigitte Krenn, Thorsten
Brants, and Hans Uszkoreit. 1997.
An annotation scheme for free word
order languages. In Proceedings
of the Fifth Conference on Applied
Natural Language Processing (ANLP),
pages 88?95, Washington, D.C.
Tsarfaty, Reut, Djame? Seddah, Yoav
Goldberg, Sandra Ku?bler, Marie
Candito, Jennifer Foster, Yannick Versley,
Ines Rehbein, and Lamia Tounsi. 2010.
Statistical parsing of morphologically
rich languages (SPMRL): What, how and
whither. In Proceedings of the NAACL
Workshop on Statistical Parsing of
Morphologically Rich Languages, pages 1?12,
Los Angeles, CA.
Tsarfaty, Reut and Khalil Sima?an. 2007.
Three-dimensional parametrization for
parsing morphologically rich languages.
In Proceedings of the Tenth International
Conference on Parsing Technologies,
pages 156?167, Prague.
Uszkoreit, Hans. 1987.Word Order and
Constituent Structure in German. CSLI,
Stanford, CA.
22
Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 705?708,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Is Arabic Part of Speech Tagging Feasible Without Word Segmentation?
Emad Mohamed, Sandra Ku?bler
Indiana University
Department of Linguistics
Memorial Hall 322
Bloomington, IN 47405
USA
{emohamed,skuebler}@indiana.edu
Abstract
In this paper, we compare two novel methods
for part of speech tagging of Arabic without
the use of gold standard word segmentation
but with the full POS tagset of the Penn Ara-
bic Treebank. The first approach uses com-
plex tags without any word segmentation, the
second approach is segmention-based, using
a machine learning segmenter. Surprisingly,
word-based POS tagging yields the best re-
sults, with a word accuracy of 94.74%.
1 Introduction
Arabic is a morphologically rich language, in
which a word carries not only inflections but
also clitics, such as pronouns, conjunctions, and
prepositions. This morphological complexity also
has consequences for the part-of-speech (POS)
annotation of Arabic: Since words can be com-
plex, POS tags refer to segments rather than to
whole words. Thus, the word wsyrfEwnhA
(in Buckwalter transliteration; engl.: and they
will raise it) is assigned the following POS tag:
[CONJ+FUTURE PARTICLE+IMPERFECT VERB PREFIX
+IMPERFECT VERB+IMPERFECT VERB SUFFIX MAS-
CULINE PLURAL 3RD PERSON+OBJECT PRONOUN
FEMININE SINGULAR] in the Penn Arabic Treebank
(ATB) (Bies and Maamouri, 2003); the boundaries
between segments are depicted by + signs. Auto-
matic approaches to POS tagging either must assign
such complex tags from a large tagset to complete
words, or they must segment the word first and
then assign POS tags to the segments. Previous
approaches (Diab et al, 2004; Habash and Rambow,
2005; van den Bosch et al, 2007; AlGahtani et al,
2009) chose the segmentation approach but concen-
trated on POS tagging by using the segmentation
provided by the ATB. Additionally, Diab et al and
Habash and Rambow used a reduced tagset. Diab et
al. and Habash and Rambow used Support Vector
Machines, the former with a standard windowing
approach, the latter performing a full morphological
analysis before POS tagging. Van den Bosch et
al., whose approach is the most similar to ours,
used memory-based learning with the full ATB
tagset. They report a POS tagging accuracy of
91.5% (93.3% on known words, 66.4% on unknown
words). However, they also evaluated on words
as defined in the ATB, which differs from written
Arabic in the treatment of affixes with syntactic
functions (see section 2 for details). AlGahtani et
al. used transformation-based learning combined
with a morphological analysis for unknown words
and words containing clitics. They reached a POS
tagging accuracy of 96.9% on ATB1. Surprisingly,
their results are lower for the experiment using the
whole ATB (96.1%).
In this paper, we present two methods for Ara-
bic POS tagging that do not require gold stan-
dard segmentation but can rather be used for natu-
rally occurring Arabic. We investigate two differ-
ent approaches: (1) Assigning complete POS tags
to whole words, without any segmentation, and (2)
a segmentation-based approach, for which we de-
veloped a machine learning based segmenter. In
this approach, the words are first passed to the
segmenter, then to the POS tagger. The first ap-
proach is surprisingly successful given the complex-
705
ity of the task, reaching an accuracy on the word
level of 94.74%, as compared to 93.47% for the
segmentation-based approach. Thus, the result for
the whole word approach is very close to the re-
sult obtained by using gold standard segmentation
(94.91%). However, a more detailed analysis shows
that this good performance of the word-based ap-
proach is due to its performance on known words
while the few unknown words are more often mis-
classified: we reach an accuracy of 96.61% on
known words but only 74.64% on unknown words.
2 Data, Methods, and Evaluation
Like the previous approaches, we base our experi-
ments on the ATB, specifically on the after-treebank
POS files, for extracting our training and test sets.
More specifically, we use two sections of the ATB
(P1V3 and P3V1) since those two sets do not contain
duplicate sentences. This data set contains approxi-
mately 500,000 words. In order to be as representa-
tive of real-world Arabic, we use the non-vocalized
version of the treebank. Since previous approaches,
to our knowledge, used different data sets, our re-
sults are not directly comparable.
For both segmentation and POS tagging, we mod-
ified the ATB representation of words in order to ob-
tain the text, as it would occur in newscasts. The
ATB treats inflectional affixes, including the defi-
nite article Al, as part of a word but splits off those
affixes that serve a syntactic function into separate
words. In order to obtain text as it occurs in news-
casts, we re-attached all conjunctions, prepositions,
pronouns, and any elements that constitute parts of
the word as an orthographic unit (with the excep-
tion of punctuation) to the word. The word ltxbrh
(engl.: in order to tell him), for example, is repre-
sented as three words in the ATB, l, txbr, and
h, but is treated as one single unit in our experi-
ment. Our second modification concerns the null
element in Arabic verbs. Since Arabic is pro-drop,
the ATB annotation includes a null element in place
of the omitted subject plus the POS tag it would
receive. Since this information is not available in
naturally occurring text, we delete the null element
and its tag. For example, {i$otaraY+(null)
and its tag PV+PVSUFF SUBJ: 3MS would occur
as {i$otaraY with the tag PV in our representa-
tion (we additionally remove the short vowels).
We perform 5-fold cross validation and use the
same data split for all three types of experiments: (1)
POS tagging using gold standard segmentation taken
from the ATB, (2) POS tagging using a segmenter,
and (3) POS tagging whole words with complex
POS tags. The first experiment serves as the upper
bound and as a comparison to previous approaches.
The second experiment uses an automatic segmenter
as a pre-processing component to the POS tagger.
This means that the accuracy of the segmenter is
also the upper limit of the POS tagger since errors
in segmentation inevitably lead to errors in POS tag-
ging. The last experiment uses full words and com-
plex POS tags. The purpose of this experiment is
to determine whether it is possible to tag complete
words without segmentation.
The segmenter and the two POS taggers use
memory-based learning. For segmentation, we use
TiMBL (Daelemans and van den Bosch, 2005); for
POS tagging MBT, a memory-based tagger (Daele-
mans et al, 1996). Memory-based learning is a lazy
learning paradigm that does not abstract over the
training data. During classification, the k nearest
neighbors to a new example are retrieved from the
training data, and the class that was assigned to the
majority of the neighbors is assigned to the new ex-
ample. MBT uses TiMBL as classifier; it offers the
possibility to use words from both sides of the focus
word as well as previous tagging decisions and am-
bitags as features. An ambitag is a combination of
all POS tags of the ambiguity class of the word.
Word segmentation is defined as a per-letter clas-
sification task: If a character in the word constitutes
the end of a segment, its class is ?+?, otherwise ?-?.
We use a sliding window approach with 5 characters
before and 5 characters after the focus character, the
previous decisions of the classifier, and the POS tag
of the focus word assigned by the whole word tag-
ger (cf. below) as features. The best results were
obtained for all experiments with the IB1 algorithm
with similarity computed as weighted overlap, rel-
evance weights computed with gain ratio, and the
number of k nearest neighbors equal to 1.
For POS tagging, we use the full tagset, with in-
formation about every segment in the word, rather
than the reduced tagset (RTS) used by Diab et al
and Habash and Rambow, since the RTS assumes
706
Gold Standard Segmentation Segmentation-Based Tagging Whole Words
SAR WAR SAR WAR WAR
96.72% 94.91% 94.70% 93.47% 94.74%
Table 1: POS tagging results.
a segmentation of words in which syntactically rel-
evant affixes are split from the stem. The word
w+y+bHv+wn+hA, for example, in RTS is split into
3 separate tokens, w, ybHvwn, hA. Then, each of
these tokens is assigned one POS tag, Conjunction
for w, Imperfective Verb for ybHvwn, and Pronoun
for hA. The split into tokens makes a preprocessing
step necessary, and it also affects evaluation since
a word-based evaluation is based on one word, the
RTS evaluation on 3 tokens for the above example.
For all the POS tagging experiments, we use
MBT. The best results were obtained with the Modi-
fied Value Difference Metric as a distance metric and
with k = 25. For known words, we use the IGTree
algorithm and 2 words to the left, their POS tags, the
focus word and its ambitag, 1 right context word and
its ambitag as features. For unknown words, we use
IB1 as algorithm and the unknown word itself, its
first 5 and last 3 characters, 1 left context word and
its POS tag, and 1 right context word and its ambitag
tag as features.
3 Experimental Results and Discussion
3.1 Word Segmentation
The memory-based word segmentation performs
very reliably with a word accuracy of 98.23%. This
also means that when the segmentation module is
used as a pre-processing step for POS tagging, the
accuracy of the tagger will have this accuracy as its
upper bound. While there are cases where wrong
segmentation results in the same number of seg-
ments, all of these words were assigned the wrong
POS tags in our data. In an error analysis, we found
that words of specific POS are more difficult to seg-
ment than others. Proper nouns constitute 33.87%
of all segmentation errors, possibly due to the fact
that many of these are either foreign names that re-
semble Arabic words (e.g. Knt, which is ambigu-
ous between the English name Kent, and the Ara-
bic verb I was), or they are ordinary nouns used as
proper nouns but with a different segmentation (e.g.
AlHyAp, engl.: the life). The POS tag with the
second highest error rate was the noun class with
30.67%.
3.2 Part of Speech Tagging
Table 1 shows the results of the three POS tagging
experiments described above. For the segmentation-
based experiments, we report per-segment (SAR)
and per-word (WAR) accuracy. As expected, POS
tagging using gold standard segments gives the best
results: 94.91% WAR. These results are approxi-
mately 3 percent points higher than those reported
by van den Bosch et al (2007). Although the results
are not absolutely comparable because of the dif-
ferent data sets, this experiment shows that our ap-
proach is competitive. The next experiments investi-
gate the two possibilities to perform POS tagging on
naturally occurring Arabic, i.e. when gold segmen-
tation is not available. The results of these experi-
ments show that POS tagging based on whole words
gives higher results (WAR: 94.74%) than tagging
based on automatic segmentation (WAR: 93.47%).
This result is surprising given that tagging whole
words is more difficult than assigning tags to seg-
ments, as there are 993 complex tags (22.70% of
which occur only once in the training set), versus
139 segment tags. A detailed error analysis of a pre-
vious but similar experiment can be found in Mo-
hamed and Ku?bler (2010).
We assume that these results are an artifact of the
ATB since it is based exclusively on newswire texts.
This means that there is only a limited vocabulary,
as shown by the very low rate of unknown words:
across the five folds, we calculated an average of
8.55% unknown words. In order to test our hypoth-
esis that unknown words are tagged more reliably
with a segment-based approach, we performed an
analysis on known and unknown words separately.
The results of this analysis are shown in Table 2.
This analysis shows that for all experiments, the
unknown words are tagged with a considerably
707
Gold Standard Segmentation Segmentation-Based Tagging Whole Words
Known words 95.90% 95.57% 96.61%
Unknown words 84.25% 71.06% 74.64%
Table 2: POS results for known and unknown words.
lower accuracy. However, the loss of performance
is more pronounced in the approaches without gold
segmentation. It is also evident that tagging whole
words reaches a higher accuracy than segment-based
tagging for both known words and unknown words.
From these results, we can conclude that while seg-
mentation makes properties of the words available,
it is not required for POS tagging. We also inves-
tigated the poor performance of the segmentation-
based tagger. A closer look at the results for un-
known words in segmentation-based tagging shows
that 59.68% of the tagging errors are direct results
from incorrect segmentation decisions. In compari-
son, for known words, only 6.24% of the incorrectly
tagged words are also ill-segmented. This means
that even though the quality of the segmenter is very
high, the errors still harm the POS tagging step.
To make our results more comparable to those by
Habash and Rambow (2005), we converted the test
set with the POS tags from the whole word tagger
to their tokenization and to a reduced tagset of 15
tags. In this setting, we reach a tokenization ac-
curacy of 99.36% and a POS tagging accuracy of
96.41%. This is very close to the results by Habash
and Rambow so that we conclude that high accu-
racy POS tagging for Arabic is possible without a
full morphological analysis.
4 Conclusions and Future Work
We have presented a method for POS tagging for
Arabic that does not assume gold segmentation,
which would be unrealistic for naturally occurring
Arabic. The approach we developed is competi-
tive although it uses the full POS tagset, without
any previous morphological analysis. The results
of our experiments suggest that segmentation is not
required for POS tagging. On the contrary, using
whole words as basis for POS tagging yields higher
accuracy, thus rendering a full morphological anal-
ysis or segmentation unnecessary. We reached the
best results in tagging whole words both for known
words and unknown words. These results were only
marginally worse that the results obtained by the ex-
periment based on gold segmentation.
The weakness of the segmentation-based ap-
proach is its low accuracy on unknown words. In the
future, we will investigate knowledge-richer meth-
ods for segmentation. In particular, we will inves-
tigate whether an automatic vocalization step previ-
ous to segmentation will improve POS tagging ac-
curacy for unknown words.
References
Shahib AlGahtani, William Black, and John Mc-
Naught. 2009. Arabic part-of-speech-tagging using
transformation-based learning. In Proceeedings of the
2nd International Conference on Arabic Language Re-
sources and Tools, Cairo, Egypt.
Ann Bies and Mohamed Maamouri. 2003. Penn Arabic
Treebank guidelines. Technical report, LDC, Univer-
sity of Pennsylvania.
Walter Daelemans and Antal van den Bosch. 2005.
Memory Based Language Processing. Cambridge
University Press.
Walter Daelemans, Jakub Zavrel, Peter Berck, and Steven
Gillis. 1996. MBT: A memory-based part of speech
tagger-generator. In Eva Ejerhed and Ido Dagan, ed-
itors, Proceedings of the 4th Workshop on Very Large
Corpora, pages 14?27, Copenhagen, Denmark.
Mona Diab, Kadri Hacioglu, and Daniel Jurafsky. 2004.
Automatic tagging of Arabic text: From raw text to
base phrase chunks. In Proceedings of HLT-NAACL,
Boston, MA.
Nizar Habash and Owen Rambow. 2005. Arabic tok-
enization, part-of-speech tagging and morphological
disambiguation in one fell swoop. In Proceedings of
ACL-2005, pages 573?580, Ann Arbor, MI.
Emad Mohamed and Sandra Ku?bler. 2010. Arabic part
of speech tagging. In Proceedings of LREC, Valetta,
Malta.
Antal van den Bosch, Erwin Marsi, and Abdelhadi Soudi.
2007. Memory-based morphological analysis and
part-of-speech tagging of Arabic. In Abdelhadi Soudi,
Antal van den Bosch, and Gu?nter Neumann, editors,
Arabic Computational Morphology. Springer.
708
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 96?99,
Uppsala, Sweden, 15-16 July 2010.
c
?2010 Association for Computational Linguistics
UBIU: A Language-Independent System for Coreference Resolution
Desislava Zhekova
University of Bremen
zhekova@uni-bremen.de
Sandra K?ubler
Indiana University
skuebler@indiana.edu
Abstract
We present UBIU, a language indepen-
dent system for detecting full coreference
chains, composed of named entities, pro-
nouns, and full noun phrases which makes
use of memory based learning and a fea-
ture model following Rahman and Ng
(2009). UBIU is evaluated on the task
?Coreference Resolution in Multiple Lan-
guages? (SemEval Task 1 (Recasens et al,
2010)) in the context of the 5th Interna-
tional Workshop on Semantic Evaluation.
1 Introduction
Coreference resolution is a field in which major
progress has been made in the last decade. Af-
ter a concentration on rule-based systems (cf. e.g.
(Mitkov, 1998; Poesio et al, 2002; Markert and
Nissim, 2005)), machine learning methods were
embraced (cf. e.g. (Soon et al, 2001; Ng and
Cardie, 2002)). However, machine learning based
coreference resolution is only possible for a very
small number of languages. In order to make such
resources available for a wider range of languages,
language independent systems are often regarded
as a partial solution. To this day, there have been
only a few systems reported that work on multiple
languages (Mitkov, 1999; Harabagiu and Maio-
rano, 2000; Luo and Zitouni, 2005). However, all
of those systems were geared towards predefined
language sets.
In this paper, we present a language indepen-
dent system that does require syntactic resources
for each language but does not require any effort
for adapting the system to a new language, except
for minimal effort required to adapt the feature ex-
tractor to the new language. The system was com-
pletely developed within 4 months, and will be ex-
tended to new languages in the future.
2 UBIU: System Structure
The UBIU system aims at being a language-
independent system in that it uses a combination
of machine learning, in the form of memory-based
learning (MBL) in the implementation of TiMBL
(Daelemans et al, 2007), and language indepen-
dent features. MBL uses a similarity metric to find
the k nearest neighbors in the training data in order
to classify a new example, and it has been shown
to work well for NLP problems (Daelemans and
van den Bosch, 2005). Similar to the approach
by Rahman and Ng (2009), classification in UBUI
is based on mention pairs (having been shown to
work well for German (Wunsch, 2009)) and uses
as features standard types of linguistic annotation
that are available for a wide range of languages
and are provided by the task.
Figure 1 shows an overview of the system. In
preprocessing, we slightly change the formatting
of the data in order to make it suitable for the next
step in which language dependent feature extrac-
tion modules are used, fromwhich the training and
test sets for the classification are extracted. Our
approach is untypical in that it first extracts the
heads of possible antecedents during feature ex-
traction. The full yield of an antecedent in the test
set is determined after classification in a separate
module. During postprocessing, final decisions
are made concerning which of the mention pairs
are considered for the final coreference chains.
In the following sections, we will describe fea-
ture extraction, classification, markable extraction,
and postprocessing in more detail.
2.1 Feature Extraction
The language dependent modules contain finite
state expressions that detect the heads based on the
linguistic annotations. Such a language module re-
quires a development time of approximately 1 per-
son hour in order to adapt the regular expressions
96
Figure 1: Overview of the system.
to the given language data (different POS tagsets,
differences in the provided annotations). This is
the only language dependent part of the system.
We decided to separate the task of finding heads
of markables, which then serve as the basis for the
generation of the feature vectors, from the identi-
fication of the scope of a markable. For the En-
glish sentence ?Any details or speculation on who
specifically, we don?t know that at this point.?, we
first detect the heads of possible antecedents, for
example ?details?. However, the decision on the
scope of the markable, i.e. the decision between
?details? or ?Any details or speculation on who
specifically? is made in the postprocessing phase.
One major task of the language modules is the
check for cyclic dependencies. Our system re-
lies on the assumption that cyclic dependencies do
not occur, which is a standard assumption in de-
pendency parsing (K?ubler et al, 2009). However,
since some of the data sets in the multilingual task
contained cycles, we integrated a module in the
preprocessing step that takes care of such cycles.
After the identification of the heads of mark-
ables, the actual feature extraction is performed.
The features that were used for training a classifier
(see Table 1) were selected from the feature pool
# Feature Description
1 m
j
- the antecedent
2 m
k
- the mention to be resolved
3 Y ifm
j
is pron.; else N
4 Y ifm
j
is subject; else N
5 Y ifm
j
is a nested NP; else N
6 number - Sg. or Pl.
7 gender - F(emale), M(ale), N(euter), U(nknown)
8 Y ifm
k
is a pronoun; else N
9 Y ifm
k
is a nested NP; else N
10 semantic class ? extracted from the NEs in the data
11 the nominative case ofm
k
if pron.; else NA
12 C if the mentions are the same string; else I
13 C if one mention is a substring of the other; else I
14 C if both mentions are pron. and same string; else I
15 C if both mentions are both non-pron. and same
string; else I
16 C if both m. are pron. and either same pron. or diff.
w.r.t. case; NA if at least one is not pron.; else I
17 C if the mentions agree in number; I if not; NA if the
number for one or both is unknown
18 C if both m. are pron. I if neither
19 C if both m. are proper nouns; I if neither; else NA
20 C if the m. have same sem. class; I if not; NA if the
sem. class for one or both m. is unknown
21 sentence distance between the mentions
22 concat. values for f. 6 form
j
andm
k
23 concat. values for f. 7 form
j
andm
k
24 concat. values for f. 3 form
j
andm
k
25 concat. values for f. 5 form
j
andm
k
26 concat. values for f. 10 form
j
andm
k
27 concat. values for f. 11 form
j
andm
k
Table 1: The pool of features for all languages.
presented by Rahman and Ng (2009). Note that
not all features could be used for all languages.
We extracted all the features in Table 1 if the cor-
responding type of annotation was available; oth-
erwise, a null value was assigned.
A good example for the latter concerns the gen-
der information represented by feature 7 (for pos-
sible feature values cf. Table 1). Let us consider
the following two entries - the first from the Ger-
man data set and the second from English:
1. Regierung Regierung Regierung NN NN
cas=d|num=sg|gend=fem cas=d|num=sg|gend=fem 31
31 PN PN . . .
2. law law NN NN NN NN 2 2 PMOD PMOD . . .
Extracting the value from entry 1, where
gend=fem, is straightforward; the value being F.
However, there is no gender information provided
in the English data (entry 2). As a result, the value
for feature 7 is U for the closed task.
2.2 Classifier Training
Based on the features extracted with the feature
extractors described above, we trained TiMBL.
Then we performed a non-exhaustive parameter
97
optimization across all languages. Since a full op-
timization strategy would lead to an unmanageable
number of system runs, we concentrated on vary-
ing k, the number of nearest neighbors considered
in classification, and on the distance metric.
Furthermore, the optimization is focused on
language independence. Hence, we did not op-
timize each classifier separately but selected pa-
rameters that lead to best average results across
all languages of the shared task. In our opinion,
this ensures an acceptable performance for new
languages without further adaptation. The optimal
settings for all the given languages were k=3 with
the Overlap distance and gain ratio weighting.
2.3 Markable Extraction
The markable extractor makes use of the depen-
dency relation labels. Each syntactic head together
with all its dependents is identified as a separate
markable. This approach is very sensitive to incor-
rect annotations and to dependency cycles in the
data set. It is also sensitive to differences between
the syntactic annotation and markables. In the
Dutch data, for example, markables for named en-
tities (NE) often exclude the determiner, a nominal
dependent in the dependency annotation. Thus,
the markable extractor suggests the whole phrase
as a markable, rather than just the NE.
During the development phase, we determined
experimentally that the recognition of markables
is one of the most important steps in order to
achieve high accuracy in coreference resolution:
We conducted an ablation study on the training
data set. We used the train data as training set and
the devel data as testing set and investigated three
different settings:
1. Gold standard setting: Uses gold markable
annotations as well as gold linguistic anno-
tations (upper bound).
2. Gold linguistic setting: Uses automatically
determined markables and gold linguistic an-
notations.
3. Regular setting: Uses automatically deter-
mined markables and automatic linguistic in-
formation.
Note that we did not include all six languages:
we excluded Italian and Dutch because there is
no gold-standard linguistic annotation provided.
The results of the experiment are shown in Table
2. From those results, we can conclude that the
S Lang. IM CEAF MUC B
3
BLANC
1
Spanish 85.8 52.3 12.8 60.0 56.9
Catalan 85.5 56.0 11.6 59.4 51.9
English 96.1 68.7 17.9 74.9 52.7
German 93.6 70.0 19.7 73.4 64.5
2
Spanish 61.0 41.5 11.3 42.4 48.7
Catalan 60.8 40.5 9.6 41.4 48.3
English 72.1 54.1 11.6 57.3 50.3
German 57.7 45.5 12.2 45.7 44.3
3
Spanish 61.2 41.8 10.3 42.3 48.5
Catalan 61.3 40.9 11.3 41.9 48.5
English 71.9 54.7 13.3 57.4 50.3
German 57.5 45.4 12.0 45.6 44.2
Table 2: Experiment results (as F1 scores) where
IM is identification of mentions and S - Setting.
figures in Setting 2 and 3 are very similar. This
means that the deterioration from gold to automat-
ically annotated linguistic information is barely
visible in the coreference results. This is a great
advantage, since gold-standard data has always
proved to be very expensive and difficult or im-
possible to obtain. The information that proved to
be extremely important for the performance of the
system is the one providing the boundaries of the
markables. As shown in Table 2, the latter leads to
an improvement of about 20%, which is observ-
able in the difference in the figures of Setting 1
and 2. The results for the different languages show
that it is more important to improve markable de-
tection than the linguistic information.
2.4 Postprocessing
In Section 2.1, we described that we decided to
separate the task of finding heads of markables
from the identification of the scope of a markable.
Thus, in the postprocessing step, we perform the
latter (by the Markables Extractor module) as well
as reformat the data for evaluation.
Another very important step during postpro-
cessing is the selection of possible antecedents. In
cases where more than one mention pair is classi-
fied as coreferent, only the pair with highest con-
fidence by TiMBL is selected. Since nouns can
be discourse-new, they do not necessarily have a
coreferent antecedent; pronouns however, require
an antecedent. Thus, in cases where all possible
antecedents for a given pronoun are classified as
not coreferent, we select the closest subject as an-
tecedent; or if this heuristic is not successful, the
antecedent that has been classified as not corefer-
ent with the lowest confidence score (i.e. the high-
est distance) by TiMBL.
98
Lang. S IM CEAF MUC B
3
BLANC
Catalan G 84.4 52.3 11.7 58.8 52.2
R 59.6 38.4 8.6 40.9 47.8
English G 95.9 65.7 20.5 74.8 54.0
R 74.2 53.6 14.2 58.7 51.0
German G 94.0 68.2 21.9 75.7 64.5
R 57.6 44.8 10.4 46.6 48.0
Spanish G 83.6 51.7 12.7 58.3 54.3
R 60.0 39.4 10.0 41.6 48.4
Italian R 40.6 32.9 3.6 34.8 37.2
Dutch R 34.7 17.0 8.3 17.0 32.3
Table 3: Final system results (as F1 scores) where
IM is identification of mentions and S - Setting.
For more details cf. (Recasens et al, 2010).
3 Results
UBIU participated in the closed task (i.e. only in-
formation provided in the data sets could be used),
in the gold and regular setting. It was one of two
systems that submitted results for all languages,
which we count as preliminary confirmation that
our system is language independent. The final re-
sults of UBIU are shown in Table 3. The figures
for the identification of mentions show that this is
an area in which the system needs to be improved.
The errors in the gold setting result from an in-
compatibility of our two-stage markable annota-
tion with the gold setting. We are planning to use
a classifier for mention identification in the future.
The results for coreference detection show that
English has a higher accuracy than all the other
languages. We assume that this is a consequence
of using a feature set that was developed for En-
glish (Rahman and Ng, 2009). This also means
that an optimization of the feature set for individ-
ual languages should result in improved system
performance.
4 Conclusion and Future Work
We have presented UBIU, a coreference resolution
system that is language independent (given differ-
ent linguistic annotations for languages). UBIU
is easy to maintain, and it allows the inclusion of
new languages with minimal effort.
For the future, we are planning to improve the
system while strictly adhering to the language in-
dependence. We are planning to separate pronoun
and definite noun classification, with the possibil-
ity of using different feature sets. We will also
investigate language independent features and im-
plement a markable classifier and a negative in-
stance sampling module.
References
Walter Daelemans and Antal van den Bosch. 2005.
Memory Based Language Processing. Cambridge
University Press.
Walter Daelemans, Jakub Zavrel, Ko van der Sloot, and
Antal van den Bosch. 2007. TiMBL: Tilburg mem-
ory based learner ? version 6.1 ? reference guide.
Technical Report ILK 07-07, Induction of Linguis-
tic Knowledge, Computational Linguistics, Tilburg
University.
Sanda M. Harabagiu and Steven J. Maiorano. 2000.
Multilingual coreference resolution. In Proceedings
of ANLP 2000, Seattle, WA.
Sandra K?ubler, Ryan McDonald, and Joakim Nivre.
2009. Dependency Parsing. Morgan Claypool.
Xiaoqiang Luo and Imed Zitouni. 2005. Multi-
lingual coreference resolution with syntactic fea-
tures. In Proceedings of HLT/EMNLP 2005, Van-
couver, Canada.
Katja Markert and Malvina Nissim. 2005. Comparing
knowledge sources for nominal anaphora resolution.
Computational Linguistics, 31(3).
Ruslan Mitkov. 1998. Robust pronoun resolu-
tion with limited knowledge. In Proceedings of
ACL/COLING 1998, Montreal, Canada.
Ruslan Mitkov. 1999. Multilingual anaphora resolu-
tion. Machine Translation, 14(3-4):281?299.
Vincent Ng and Claire Cardie. 2002. Improving
machine learning approaches to coreference resolu-
tion. In Proceedings of ACL 2002, pages 104?111,
Philadelphia, PA.
Massimo Poesio, Tomonori Ishikawa, Sabine
Schulte im Walde, and Renata Vieira. 2002.
Acquiring lexical knowledge for anaphora resolu-
tion. In Proceedings of LREC 2002, Las Palmas,
Gran Canaria.
Altaf Rahman and Vincent Ng. 2009. Supervised mod-
els for coreference resolution. In Proceedings of
EMNLP 2009, Singapore.
Marta Recasens, Llu??s M`arquez, Emili Sapena,
M.Ant`onia Mart??, Mariona Taul?e, V?eronique Hoste,
Massimo Poesio, and Yannick Versley. 2010.
Semeval-2010 task 1: Coreference resolution in
multiple languages. In Proceedings of the 5th
International Workshop on Semantic Evaluations
(SemEval-2010), Uppsala, Sweden.
Wee Meng Soon, Hwee Tou Ng, and Daniel
Chung Yong Lim. 2001. A machine learning ap-
proach to coreference resolution of noun phrases.
Computational Linguistics, 27(4):521?544.
Holger Wunsch. 2009. Rule-Based and Memory-
Based Pronoun Resolution for German: A Compar-
ison and Assessment of Data Sources. Ph.D. thesis,
Universit?at T?ubingen.
99
Proceedings of the NAACL HLT 2010 First Workshop on Statistical Parsing of Morphologically-Rich Languages, pages 1?12,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Statistical Parsing of Morphologically Rich Languages (SPMRL)
What, How and Whither
Reut Tsarfaty
Uppsala Universitet
Djame? Seddah
Alpage (Inria/Univ. Paris-Sorbonne)
Yoav Goldberg
Ben Gurion University
Sandra Ku?bler
Indiana University
Marie Candito
Alpage (Inria/Univ. Paris 7)
Jennifer Foster
NCLT, Dublin City University
Yannick Versley
Universita?t Tu?bingen
Ines Rehbein
Universita?t Saarbru?cken
Lamia Tounsi
NCLT, Dublin City University
Abstract
The term Morphologically Rich Languages
(MRLs) refers to languages in which signif-
icant information concerning syntactic units
and relations is expressed at word-level. There
is ample evidence that the application of read-
ily available statistical parsing models to such
languages is susceptible to serious perfor-
mance degradation. The first workshop on sta-
tistical parsing of MRLs hosts a variety of con-
tributions which show that despite language-
specific idiosyncrasies, the problems associ-
ated with parsing MRLs cut across languages
and parsing frameworks. In this paper we re-
view the current state-of-affairs with respect
to parsing MRLs and point out central chal-
lenges. We synthesize the contributions of re-
searchers working on parsing Arabic, Basque,
French, German, Hebrew, Hindi and Korean
to point out shared solutions across languages.
The overarching analysis suggests itself as a
source of directions for future investigations.
1 Introduction
The availability of large syntactically annotated cor-
pora led to an explosion of interest in automati-
cally inducing models for syntactic analysis and dis-
ambiguation called statistical parsers. The devel-
opment of successful statistical parsing models for
English focused on the Wall Street Journal Penn
Treebank (PTB, (Marcus et al, 1993)) as the pri-
mary, and sometimes only, resource. Since the ini-
tial release of the Penn Treebank (PTB Marcus et
al. (1993)), many different constituent-based parsing
models have been developed in the context of pars-
ing English (e.g. (Magerman, 1995; Collins, 1997;
Charniak, 2000; Chiang, 2000; Bod, 2003; Char-
niak and Johnson, 2005; Petrov et al, 2006; Huang,
2008; Finkel et al, 2008; Carreras et al, 2008)).
At their time, each of these models improved the
state-of-the-art, bringing parsing performance on the
standard test set of the Wall-Street-Journal to a per-
formance ceiling of 92% F1-score using the PARS-
EVAL evaluation metrics (Black et al, 1991). Some
of these parsers have been adapted to other lan-
guage/treebank pairs, but many of these adaptations
have been shown to be considerably less successful.
Among the arguments that have been proposed
to explain this performance gap are the impact of
small data sets, differences in treebanks? annotation
schemes, and inadequacy of the widely used PARS-
EVAL evaluation metrics. None of these aspects in
isolation can account for the systematic performance
deterioration, but observed from a wider, cross-
linguistic perspective, a picture begins to emerge ?
that the morphologically rich nature of some of the
languages makes them inherently more susceptible
to such performance degradation. Linguistic factors
associated with MRLs, such as a large inventory of
word-forms, higher degrees of word order freedom,
and the use of morphological information in indi-
cating syntactic relations, makes them substantially
harder to parse with models and techniques that have
been developed with English data in mind.
1
In addition to these technical and linguistic fac-
tors, the prominence of English parsing in the litera-
ture reduces the visibility of research aiming to solve
problems particular to MRLs. The lack of stream-
lined communication among researchers working
on different MRLs often leads to a reinventing the
wheel syndrome. To circumvent this, the first work-
shop on Statistical Parsing of Morphologically Rich
Languages (SPMRL 2010) offers a platform for
this growing community to share their views of the
different problems and oftentimes similar solutions.
We identify three main types of challenges, each
of which raises many questions. Many of the ques-
tions are yet to be conclusively answered. The first
type of challenges has to do with the architectural
setup of parsing MRLs: What is the nature of the in-
put? Can words be represented abstractly to reflect
shared morphological aspects? How can we cope
with morphological segmentation errors propagated
through the pipeline? The second type concerns the
representation of morphological information inside
the articulated syntactic model: Should morpholog-
ical information be encoded at the level of PoS tags?
On dependency relations? On top of non-terminals
symbols? How should the integrated representations
be learned and used? A final genuine challenge
has to do with sound estimation for lexical probabil-
ities: Given the finite, and often rather small, set of
data, and the large number of morphological analy-
ses licensed by rich inflectional systems, how can we
analyze words unseen in the training data?
Many of the challenges reported here are mostly
irrelevant when parsing Section 23 of the PTB but
they are of primordial importance in other tasks, in-
cluding out-of-domain parsing, statistical machine
translation, and parsing resource-poor languages.
By synthesizing the contributions to the workshop
and bringing it to the forefront, we hope to advance
the state of the art of statistical parsing in general.
In this paper we therefore take the opportunity
to analyze the knowledge that has been acquired in
the different investigations for the purpose of iden-
tifying main bottlenecks and pointing out promising
research directions. In section 2, we define MRLs
and identify syntactic characteristics associated with
them. We then discuss work on parsing MRLs in
both the dependency-based and constituency-based
setup. In section 3, we review the types of chal-
lenges associated with parsing MRLs across frame-
works. In section 4, we focus on the contributions to
the SPMRL workshop and identify recurring trends
in the empirical results and conceptual solutions. In
section 5, we analyze the emerging picture from a
bird?s eye view, and conclude that many challenges
could be more faithfully addressed in the context of
parsing morphologically ambiguous input.
2 Background
2.1 What are MRLs?
The term Morphologically Rich Languages (MRLs)
is used in the CL/NLP literature to refer to languages
in which substantial grammatical information, i.e.,
information concerning the arrangement of words
into syntactic units or cues to syntactic relations, is
expressed at word level.
The common linguistic and typological wisdom is
that ?morphology competes with syntax? (Bresnan,
2001). In effect, this means that rich morphology
goes hand in hand with a host of nonconfigurational
syntactic phenomena of the kind discussed by Hale
(1983). Because information about the relations be-
tween syntactic elements is indicated in the form of
words, these words can freely change their positions
in the sentence. This is referred to as free word or-
der (Mithun, 1992). Information about the group-
ing of elements together can further be expressed by
reference to their morphological form. Such logical
groupings of disparate elements are often called dis-
continuous constituents. In dependency structures,
such discontinuities impose nonprojectivity. Finally,
rich morphological information is found in abun-
dance in conjunction with so-called pro-drop or zero
anaphora. In such cases, rich morphological infor-
mation in the head (or co-head) of the clause of-
ten makes it possible to omit an overt subject which
would be semantically impoverished.
English, the most heavily studied language within
the CL/NLP community, is not an MRL. Even
though a handful of syntactic features (such as per-
son and number) are reflected in the form of words,
morphological information is often secondary to
other syntactic factors, such as the position of words
and their arrangement into phrases. German, an
Indo-European language closely related to English,
already exhibits some of the properties that make
2
parsing MRLs problematic. The Semitic languages
Arabic and Hebrew show an even more extreme case
in terms of the richness of their morphological forms
and the flexibility in their syntactic ordering.
2.2 Parsing MRLs
Pushing the envelope of constituency parsing:
The Head-Driven models of the type proposed
by Collins (1997) have been ported to parsing
many MRLs, often via the implementation of Bikel
(2002). For Czech, the adaptation by Collins et al
(1999) culminated in an 80 F1-score.
German has become almost an archetype of the
problems caused by MRLs; even though German
has a moderately rich morphology and a moder-
ately free word order, parsing results are far from
those for English (see (Ku?bler, 2008) and references
therein). Dubey (2005) showed that, for German
parsing, adding case and morphology information
together with smoothed markovization and an ade-
quate unknown-word model is more important than
lexicalization (Dubey and Keller, 2003).
For Modern Hebrew, Tsarfaty and Sima?an (2007)
show that a simple treebank PCFG augmented with
parent annotation and morphological information as
state-splits significantly outperforms Head-Driven
markovized models of the kind made popular by
Klein and Manning (2003). Results for parsing
Modern Standard Arabic using Bikel?s implemen-
tation on gold-standard tagging and segmentation
have not improved substantially since the initial re-
lease of the treebank (Maamouri et al, 2004; Kulick
et al, 2006; Maamouri et al, 2008).
For Italian, Corazza et al (2004) used the Stan-
ford parser and Bikel?s parser emulation of Collins?
model 2 (Collins, 1997) on the ISST treebank, and
obtained significantly lower results compared to En-
glish. It is notable that these models were ap-
plied without adding morphological signatures, us-
ing gold lemmas instead. Corazza et al (2004) fur-
ther tried different refinements including parent an-
notation and horizontal markovization, but none of
them obtained the desired improvement.
For French, Crabbe? and Candito (2008) and Sed-
dah et al (2010) show that, given a corpus compara-
ble in size and properties (i.e. the number of tokens
and grammar size), the performance level, both for
Charniak?s parser (Charniak, 2000) and the Berke-
ley parser (Petrov et al, 2006) was higher for pars-
ing the PTB than it was for French. The split-merge-
smooth implementation of (Petrov et al, 2006) con-
sistently outperform various lexicalized and unlexi-
calized models for French (Seddah et al, 2009) and
for many other languages (Petrov and Klein, 2007).
In this respect, (Petrov et al, 2006) is considered
MRL-friendly, due to its language agnostic design.
The rise of dependency parsing: It is commonly
assumed that dependency structures are better suited
for representing the syntactic structures of free word
order, morphologically rich, languages, because this
representation format does not rely crucially on the
position of words and the internal grouping of sur-
face chunks (Mel?c?uk, 1988). It is an entirely differ-
ent question, however, whether dependency parsers
are in fact better suited for parsing such languages.
The CoNLL shared tasks on multilingual depen-
dency parsing in 2006 and 2007 (Buchholz and
Marsi, 2006; Nivre et al, 2007a) demonstrated that
dependency parsing for MRLs is quite challenging.
While dependency parsers are adaptable to many
languages, as reflected in the multiplicity of the lan-
guages covered,1 the analysis by Nivre et al (2007b)
shows that the best result was obtained for English,
followed by Catalan, and that the most difficult lan-
guages to parse were Arabic, Basque, and Greek.
Nivre et al (2007a) drew a somewhat typological
conclusion, that languages with rich morphology
and free word order are the hardest to parse. This
was shown to be the case for both MaltParser (Nivre
et al, 2007c) and MST (McDonald et al, 2005), two
of the best performing parsers on the whole.
Annotation and evaluation matter: An emerg-
ing question is therefore whether models that have
been so successful in parsing English are necessar-
ily appropriate for parsing MRLs ? but associated
with this question are important questions concern-
ing the annotation scheme of the related treebanks.
Obviously, when annotating structures for languages
with characteristics different than English one has to
face different annotation decisions, and it comes as
no surprise that the annotated structures for MRLs
often differ from those employed in the PTB.
1The shared tasks involved 18 languages, including many
MRLs such as Arabic, Basque, Czech, Hungarian, and Turkish.
3
For Spanish and French, it was shown by Cowan
and Collins (2005) and in (Arun and Keller, 2005;
Schluter and van Genabith, 2007), that restructuring
the treebanks? native annotation scheme to match
the PTB annotation style led to a significant gain in
parsing performance of Head-Driven models of the
kind proposed in (Collins, 1997). For German, a
language with four different treebanks and two sub-
stantially different annotation schemes, it has been
shown that a PCFG parser is sensitive to the kind of
representation employed in the treebank.
Dubey and Keller (2003), for example, showed
that a simple PCFG parser outperformed an emula-
tion of Collins? model 1 on NEGRA. They showed
that using sister-head dependencies instead of head-
head dependencies improved parsing performance,
and hypothesized that it is due to the flatness of
phrasal annotation. Ku?bler et al (2006) showed con-
siderably lower PARSEVAL scores on NEGRA (Skut
et al, 1998) relative to the more hierarchically struc-
tured Tu?Ba-D/Z (Hinrichs et al, 2005), again, hy-
pothesizing that this is due to annotation differences.
Related to such comparisons is the question of the
relevance of the PARSEVAL metrics for evaluating
parsing results across languages and treebanks. Re-
hbein and van Genabith (2007) showed that PARS-
EVAL measures are sensitive to annotation scheme
particularities (e.g. the internal node ratio). It was
further shown that different metrics (i.e. the Leaf-
ancestor path (Sampson and Babarczy, 2003) and
dependency based ones in (Lin, 1995)) can lead to
different performance ranking. This was confirmed
also for French by Seddah et al (2009).
The questions of how to annotate treebanks for
MRLs and how to evaluate the performance of the
different parsers on these different treebanks is cru-
cial. For the MRL parsing community to be able to
assess the difficulty of improving parsing results for
French, German, Arabic, Korean, Basque, Hindi or
Hebrew, we ought to first address fundamental ques-
tions including: Is the treebank sufficiently large
to allow for proper grammar induction? Does the
annotation scheme fit the language characteristics?
Does the use of PTB annotation variants for other
languages influence parsing results? Does the space-
delimited tokenization allow for phrase boundary
detection? Do the results for a specific approach
generalize to more than one language?
3 Primary Research Questions
It is firmly established in theoretical linguistics that
morphology and syntax closely interact through pat-
terns of case marking, agreement, clitics and various
types of compounds. Because of such close interac-
tions, we expect morphological cues to help parsing
performance. But in practice, when trying to incor-
porate morphological information into parsing mod-
els, three types of challenges present themselves:
Architecture and Setup: When attempting to
parse complex word-forms that encapsulate both
lexical and functional information, important archi-
tectural questions emerge, namely, what is the na-
ture of the input that is given to the parsing system?
Does the system attempt to parse sequences of words
or does it aim to assign structures to sequences of
morphological segments? If the former is the case,
how can we represent words abstractly so as to re-
flect shared morphological aspects between them?
If the latter is the case, how can we arrive at a good
enough morphological segmentation for the purpose
of statistical parsing, given raw input texts?
When working with morphologically rich lan-
guages such as Hebrew or Arabic, affixes may have
syntactically independent functions. Many parsing
models assume segmentation of the syntactically in-
dependent parts, such as prepositions or pronominal
clitics, prior to parsing. But morphological segmen-
tation requires disambiguation which is non-trivial,
due to case syncretism and high morphological am-
biguity exhibited by rich inflectional systems. The
question is then when should we disambiguate the
morphological analyses of input forms? Should we
do that prior to parsing or perhaps jointly with it?2
Representation and Modeling: Assuming that
the input to our system reflects morphological infor-
mation, one way or another, which types of morpho-
2Most studies on parsing MRLs nowadays assume the gold
standard segmentation and disambiguated morphological infor-
mation as input. This is the case, for instance, for the Arabic
parsing at CoNLL 2007 (Nivre et al, 2007a). This practice de-
ludes the community as to the validity of the parsing results
reported for MRLs in shared tasks. Goldberg et al (2009), for
instance, show a gap of up to 6pt F1-score between performance
on gold standard segmentation vs. raw text. One way to over-
come this is to devise joint morphological and syntactic disam-
biguation frameworks (cf. (Goldberg and Tsarfaty, 2008)).
4
logical information should we include in the parsing
model? Inflectional and/or derivational? Case infor-
mation and/or agreement features? How can valency
requirements reflected in derivational morphology
affect the overall syntactic structure? In tandem with
the decision concerning the morphological informa-
tion to include, we face genuine challenges concern-
ing how to represent such information in the syntac-
tic model, be it constituency-based or dependency-
based. Should we encode morphological informa-
tion at the level of PoS tags and/or on top of syn-
tactic elements? Should we decorate non-terminals
nodes and/or dependency arcs or both?
Incorporating morphology in the statistical model
is often even more challenging than the sum of
these bare decisions, because of the nonconfigu-
rational structures (free word order, discontinuous
constituents) for rich markings are crucial (Hale,
1983). The parsing models designed for English of-
ten focus on learning rigid word order, and they do
not take morphological information into account (cf.
developing parsers for German (Dubey and Keller,
2003; Ku?bler et al, 2006)). The more complex ques-
tion is therefore: what type of parsing model should
we use for parsing MRLs? shall we use a general
purpose implementation and attempt to amend it?
how? or perhaps we should devise a new model from
first principles, to address nonconfigurational phe-
nomena effectively? using what form of representa-
tion? is it possible to find a single model that can
effectively cope with different kinds of languages?
Estimation and Smoothing: Compared to En-
glish, MRLs tend to have a greater number of word
forms and higher out-of-vocabulary (OOV) rates,
due to the many feature combinations licensed by
the inflectional system. A typical problem associ-
ated with parsing MRLs is substantial lexical data
sparseness due to high morphological variation in
surface forms. The question is therefore, given our
finite, and often fairly small, annotated sets of data,
how can we guess the morphological analyses, in-
cluding the PoS tag assignment and various features,
of an OOV word? How can we learn the probabil-
ities of such assignments? In a more general setup,
this problem is akin to handling out-of-vocabulary
or rare words for robust statistical parsing, and tech-
niques for domain adaptation via lexicon enhance-
Constituency-Based Dependency-Based
Arabic (Attia et al, 2010) (Marton et al, 2010)?
Basque - (Bengoetxea and Gojenola, 2010)
English (Attia et al, 2010) -
French (Attia et al, 2010)
(Seddah et al, 2010)
(Candito and Seddah, 2010)? -
German (Maier, 2010) -
Hebrew (Tsarfaty and Sima?an, 2010) (Goldberg and Elhadad, 2010)?
Hindi - (Ambati et al, 2010a)?
(Ambati et al, 2010b)
Korean (Chung et al, 2010) -
Table 1: An overview of SPMRL contributions. (? report
results also for non-gold standard input)
ment (also explored for English and other morpho-
logically impoverished languages).
So, in fact, incorporating morphological informa-
tion inside the syntactic model for the purpose of
statistical parsing is anything but trivial. In the next
section we review the various approaches taken in
the individual contributions of the SPMRL work-
shop for addressing such challenges.
4 Parsing MRLs: Recurring Trends
The first workshop on parsing MRLs features 11
contributions for a variety of languages with a
range of different parsing frameworks. Table 1 lists
the individual contributions within a cross-language
cross-framework grid. In this section, we focus on
trends that occur among the different contributions.
This may be a biased view since some of the prob-
lems that exist for parsing MRLs may have not been
at all present, but it is a synopsis of where we stand
with respect to problems that are being addressed.
4.1 Architecture and Setup: Gold vs. Predicted
Morphological Information
While morphological information can be very infor-
mative for syntactic analysis, morphological anal-
ysis of surface forms is ambiguous in many ways.
In German, for instance, case syncretism (i.e. a sin-
gle surface form corresponding to different cases) is
pervasive, and in Hebrew and Arabic, the lack of vo-
calization patterns in written texts leads to multiple
morphological analyses for each space-delimited to-
ken. In real world situations, gold morphological in-
formation is not available prior to parsing. Can pars-
ing systems make effective use of morphology even
when gold morphological information is absent?
5
Several papers address this challenge by present-
ing results for both the gold and the automatically
predicted PoS and morphological information (Am-
bati et al, 2010a; Marton et al, 2010; Goldberg and
Elhadad, 2010; Seddah et al, 2010). Not very sur-
prisingly, all evaluated systems show a drop in pars-
ing accuracy in the non-gold settings.
An interesting trend is that in many cases, us-
ing noisy morphological information is worse than
not using any at all. For Arabic Dependency pars-
ing, using predicted CASE causes a substantial drop
in accuracy while it greatly improves performance
in the gold setting (Marton et al, 2010). For
Hindi Dependency Parsing, using chunk-internal
cues (i.e. marking non-recursive phrases) is benefi-
cial when gold chunk-boundaries are available, but
suboptimal when they are automatically predicted
(Ambati et al, 2010a). For Hebrew Dependency
Parsing with the MST parser, using gold morpholog-
ical features shows no benefit over not using them,
while using automatically predicted morphological
features causes a big drop in accuracy compared to
not using them (Goldberg and Elhadad, 2010). For
French Constituency Parsing, Seddah et al (2010)
and Candito and Seddah (2010) show that while
gold information for the part-of-speech and lemma
of each word form results in a significant improve-
ment, the gain is low when switching to predicted
information. Reassuringly, Ambati et al (2010a),
Marton et al (2010), and Goldberg and Elhadad
(2010) demonstrate that some morphological infor-
mation can indeed be beneficial for parsing even in
the automatic setting. Ensuring that this is indeed
so, appears to be in turn linked to the question of
how morphology is represented and incorporated in
the parsing model.
The same effect in a different guise appears in
the contribution of Chung et al (2010) concerning
parsing Korean. Chung et al (2010) show a sig-
nificant improvement in parsing accuracy when in-
cluding traces of null anaphors (a.k.a. pro-drop) in
the input to the parser. Just like overt morphology,
traces and null elements encapsulate functional in-
formation about relational entities in the sentence
(the subject, the object, etc.), and including them at
the input level provides helpful disambiguating cues
for the overall structure that represents such rela-
tions. However, assuming that such traces are given
prior to parsing is, for all practical purposes, infeasi-
ble. This leads to an interesting question: will iden-
tifying such functional elements (marked as traces,
overt morphology, etc) during parsing, while com-
plicating that task itself, be on the whole justified?
Closely linked to the inclusion of morphological
information in the input is the choice of PoS tag set
to use. The generally accepted view is that fine-
grained PoS tags are morphologically more informa-
tive but may be harder to statistically learn and parse
with, in particular in the non-gold scenario. Mar-
ton et al (2010) demonstrate that a fine-grained tag
set provides the best results for Arabic dependency
parsing when gold tags are known, while a much
smaller tag set is preferred in the automatic setting.
4.2 Representation and Modeling:
Incorporating Morphological Information
Many of the studies presented here explore the use
of feature representation of morphological informa-
tion for the purpose of syntactic parsing (Ambati et
al., 2010a; Ambati et al, 2010b; Bengoetxea and
Gojenola, 2010; Goldberg and Elhadad, 2010; Mar-
ton et al, 2010; Tsarfaty and Sima?an, 2010). Clear
trends among the contributions emerge concerning
the kind of morphological information that helps sta-
tistical parsing. Morphological CASE is shown to be
beneficial across the board. It is shown to help for
parsing Basque, Hebrew, Hindi and to some extent
Arabic.3 Morphological DEFINITENESS and STATE
are beneficial for Hebrew and Arabic when explic-
itly represented in the model. STATE, ASPECT and
MOOD are beneficial for Hindi, but only marginally
beneficial for Arabic. CASE and SUBORDINATION-
TYPE are the most beneficial features for Basque
transition-based dependency parsing.
A closer view into the results mentioned in the
previous paragraph suggests that, beyond the kind
of information that is being used, the way in which
morphological information is represented and used
by the model has substantial ramification as to
whether or not it leads to performance improve-
ments. The so-called ?agreement features? GEN-
DER, NUMBER, PERSON, provide for an interesting
case study in this respect. When included directly as
3For Arabic, CASE is useful when gold morphology infor-
mation is available, but substantially hurt results when it is not.
6
machine learning features, agreement features ben-
efit dependency parsing for Arabic (Marton et al,
2010), but not Hindi (dependency) (Ambati et al,
2010a; Ambati et al, 2010b) or Hebrew (Goldberg
and Elhadad, 2010). When represented as simple
splits of non-terminal symbols, agreement informa-
tion does not help constituency-based parsing per-
formance for Hebrew (Tsarfaty and Sima?an, 2010).
However, when agreement patterns are directly rep-
resented on dependency arcs, they contribute an im-
provement for Hebrew dependency parsing (Gold-
berg and Elhadad, 2010). When agreement is en-
coded at the realization level inside a Relational-
Realizational model (Tsarfaty and Sima?an, 2008),
agreement features improve the state-of-the-art for
Hebrew parsing (Tsarfaty and Sima?an, 2010).
One of the advantages of the latter study is that
morphological information which is expressed at the
level of words gets interpreted elsewhere, on func-
tional elements higher up the constituency tree. In
dependency parsing, similar cases may arise, that
is, morphological information might not be as use-
ful on the form on which it is expressed, but would
be more useful at a different position where it could
influence the correct attachment of the main verb
to other elements. Interesting patterns of that sort
occur in Basque, where the SUBORDINATIONTYPE
morpheme attaches to the auxiliary verb, though it
mainly influences attachments to the main verb.
Bengoetxea and Gojenola (2010) attempted two
different ways to address this, one using a trans-
formation segmenting the relevant morpheme and
attaching it to the main verb instead, and another
by propagating the morpheme along arcs, through
a ?stacking? process, to where it is relevant. Both
ways led to performance improvements. The idea of
a segmentation transformation imposes non-trivial
pre-processing, but it may be that automatically
learning the propagation of morphological features
is a promising direction for future investigation.
Another, albeit indirect, way to include morpho-
logical information in the parsing model is using
so-called latent information or some mechanism
of clustering. The general idea is the following:
when morphological information is added to stan-
dard terminal or non-terminal symbols, it imposes
restrictions on the distribution of these no-longer-
equivalent elements. Learning latent informa-
tion does not represent morphological information
directly, but presumably, the distributional restric-
tions can be automatically learned along with the
splits of labels symbols in models such as (Petrov
et al, 2006). For Korean (Chung et al, 2010),
latent information contributes significant improve-
ments. One can further do the opposite, namely,
merging terminals symbols for the purpose of ob-
taining an abstraction over morphological features.
When such clustering uses a morphological signa-
ture of some sort, it is shown to significantly im-
prove constituency-based parsing for French (Can-
dito and Seddah, 2010).
4.3 Representation and Modeling: Free Word
Order and Flexible Constituency Structure
Off-the-shelf parsing tools are found in abundance
for English. One problematic aspect of using them
to parse MRLs lies in the fact that these tools fo-
cus on the statistical modeling of configurational
information. These models often condition on the
position of words relative to one another (e.g. in
transition-based dependency parsing) or on the dis-
tance between words inside constituents (e.g. in
Head-Driven parsing). Many of the contributions to
the workshop show that working around existing im-
plementations may be insufficient, and we may have
to come up with more radical solutions.
Several studies present results that support the
conjecture that when free word-order is explicitly
taken into account, morphological information is
more likely to contribute to parsing accuracy. The
Relational-Realizational model used in (Tsarfaty
and Sima?an, 2010) allows for reordering of con-
stituents at a configuration layer, which is indepen-
dent of the realization patterns learned from the data
(vis-a`-vis case marking and agreement). The easy-
first algorithm of (Goldberg and Elhadad, 2010)
which allows for significant flexibility in the order of
attachment, allows the model to benefit from agree-
ment patterns over dependency arcs that are easier
to detect and attach first. The use of larger subtrees
in (Chung et al, 2010) for parsing Korean, within a
Bayesian framework, allows the model to learn dis-
tributions that take more elements into account, and
thus learn the different distributions associated with
morphologically marked elements in constituency
structures, to improve performance.
7
In addition to free word order, MRLs show higher
degree of freedom in extraposition. Both of these
phenomena can result in discontinuous structures.
In constituency-based treebanks, this is either an-
notated as additional information which has to be
recovered somehow (traces in the case of the PTB,
complex edge labels in the German Tu?Ba-D/Z), or
as discontinuous phrase structures, which cannot be
handled with current PCFG models. Maier (2010)
suggests the use of Linear Context-Free Rewriting
Systems (LCFRSs) in order to make discontinuous
structure transparent to the parsing process and yet
preserve familiar notions from constituency.
Dependency representation uses non-projective
dependencies to reflect discontinuities, which is
problematic to parse with models that assume pro-
jectivity. Different ways have been proposed to deal
with non-projectivity (Nivre and Nilsson, 2005; Mc-
Donald et al, 2005; McDonald and Pereira, 2006;
Nivre, 2009). Bengoetxea and Gojenola (2010)
discuss non-projective dependencies in Basque and
show that the pseudo-projective transformation of
(Nivre and Nilsson, 2005) improves accuracy for de-
pendency parsing of Basque. Moreover, they show
that in combination with other transformations, it
improves the utility of these other ones, too.
4.4 Estimation and Smoothing: Coping with
Lexical Sparsity
Morphological word form variation augments the
vocabulary size and thus worsens the problem of lex-
ical data sparseness. Words occurring with medium-
frequency receive less reliable estimates, and the
number of rare/unknown words is increased. One
way to cope with the one of both aspects of this
problem is through clustering, that is, providing an
abstract representation over word forms that reflects
their shared morphological and morphosyntactic as-
pects. This was done, for instance, in previous work
on parsing German. Versley and Rehbein (2009)
cluster words according to linear context features.
These clusters include valency information added to
verbs and morphological features such as case and
number added to pre-terminal nodes. The clusters
are then integrated as features in a discriminative
parsing model to cope with unknown words. Their
discriminative model thus obtains state-of-the-art re-
sults on parsing German.
Several contribution address similar challenges.
For constituency-based generative parsers, the sim-
ple technique of replacing word forms with more
abstract symbols is investigated by (Seddah et al,
2010; Candito and Seddah, 2010). For French, re-
placing each word form by its predicted part-of-
speech and lemma pair results in a slight perfor-
mance improvement (Seddah et al, 2010). When
words are clustered, even according to a very local
linear-context similarity measure, measured over a
large raw corpus, and when word clusters are used in
place of word forms, the gain in performance is even
higher (Candito and Seddah, 2010). In both cases,
the technique provides more reliable estimates for
in-vocabulary words, since a given lemma or cluster
appear more frequently. It also increases the known
vocabulary. For instance, if a plural form is un-
seen in the training set but the corresponding singu-
lar form is known, then in a setting of using lemmas
in terminal symbols, both forms are known.
For dependency parsing, Marton et al (2010) in-
vestigates the use of morphological features that in-
volve some semantic abstraction over Arabic forms.
The use of undiacritized lemmas is shown to im-
prove performance. Attia et al (2010) specifically
address the handling of unknown words in the latent-
variable parsing model. Here again, the technique
that is investigated is to project unknown words to
more general symbols using morphological clues. A
study on three languages, English, French and Ara-
bic, shows that this method helps in all cases, but
that the greatest improvement is obtained for Arabic,
which has the richest morphology among three.
5 Where we?re at
It is clear from the present overview that we are
yet to obtain a complete understanding concerning
which models effectively parse MRLs, how to an-
notate treebanks for MRLs and, importantly, how
to evaluate parsing performance across types of lan-
guages and treebanks. These foundational issues are
crucial for deriving more conclusive recommenda-
tions as to the kind of models and morphological
features that can lead to advancing the state-of-the-
art for parsing MRLs. One way to target such an
understanding would be to encourage the investiga-
tion of particular tasks, individually or in the context
8
of shared tasks, that are tailored to treat those prob-
lematic aspects of MRLs that we surveyed here.
So far, constituency-based parsers have been as-
sessed based on their performance on the PTB (and
to some extent, across German treebanks (Ku?bler,
2008)) whereas comparison across languages was
rendered opaque due to data set differences and
representation idiosyncrasies. It would be interest-
ing to investigate such a cross-linguistic compari-
son of parsers in the context of a shared task on
constituency-based statistical parsing, in additional
to dependency-based ones as reported in (Nivre et
al., 2007a). Standardizing data sets for a large
number of languages with different characteristics,
would require us, as a community, to aim for
constituency-representation guidelines that can rep-
resent the shared aspects of structures in different
languages, while at the same time allowing differ-
ences between them to be reflected in the model.
Furthermore, it would be a good idea to intro-
duce parsing tasks, for either constituent-based or
dependency-based setups, which consider raw text
as input, rather than morphologically segmented
and analyzed text. Addressing the parsing prob-
lem while facing the morphological disambiguation
challenge in its full-blown complexity would be il-
luminating and educating for at least two reasons:
firstly, it would give us a better idea of what is the
state-of-the-art for parsing MRLs in realistic scenar-
ios. Secondly, it might lead to profound insights
about the potentially successful ways to use mor-
phology inside a parser, which may differ from the
insights concerning the use of morphology in the
less realistic parsing scenarios, where gold morpho-
logical information is given.
Finally, to be able to perceive where we stand
with respect to parsing MRLs and how models fare
against one another across languages, it would be
crucial to arrive at evaluation metrics that capture
information that is shared among the different repre-
sentations, for instance, functional information con-
cerning predicate-argument relations. Using the dif-
ferent kinds of measures in the context of cross-
framework tasks will help us understand the util-
ity of the different evaluation metrics that have been
proposed and to arrive at a clearer picture of what it
is that we wish to compare, and how we can faith-
fully do so across models, languages and treebanks.
6 Conclusion
This paper presents the synthesis of 11 contributions
to the first workshop on statistical parsing for mor-
phologically rich languages. We have shown that
architectural, representational, and estimation issues
associated with parsing MRLs are found to be chal-
lenging across languages and parsing frameworks.
The use of morphological information in the non
gold-tagged input scenario is found to cause sub-
stantial differences in parsing performance, and in
the kind of morphological features that lead to per-
formance improvements.
Whether or not morphological features help pars-
ing also depends on the kind of model in which
they are embedded, and the different ways they are
treated within. Furthermore, sound statistical esti-
mation methods for morphologically rich, complex
lexica, turn out to be crucial for obtaining good pars-
ing accuracy when using general-purpose models
and algorithms. In the future we hope to gain better
understanding of the common pitfalls in, and novel
solutions for, parsing morphologically ambiguous
input, and to arrive at principled guidelines for se-
lecting the model and features to include when pars-
ing different kinds of languages. Such insights may
be gained, among other things, in the context of
more morphologically-aware shared parsing tasks.
Acknowledgements
The program committee would like to thank
NAACL for hosting the workshop and SIGPARSE
for their sponsorship. We further thank INRIA Al-
page team for their generous sponsorship. We are
finally grateful to our reviewers and authors for their
dedicated work and individual contributions.
References
Bharat Ram Ambati, Samar Husain, Sambhav Jain,
Dipti Misra Sharma, and Rajeev Sangal. 2010a. Two
methods to incorporate local morphosyntactic features
in Hindi dependency parsing. In Proceedings of the
NAACL/HLT Workshop on Statistical Parsing of Mor-
phologically Rich Languages (SPMRL 2010), Los An-
geles, CA.
Bharat Ram Ambati, Samar Husain, Joakim Nivre, and
Rajeev Sangal. 2010b. On the role of morphosyntactic
features in Hindi dependency parsing. In Proceedings
9
of the NAACL/HLT Workshop on Statistical Parsing of
Morphologically Rich Languages (SPMRL 2010), Los
Angeles, CA.
Abhishek Arun and Frank Keller. 2005. Lexicalization
in crosslinguistic probabilistic parsing: The case of
French. In Proceedings of the 43rd Annual Meeting of
the Association for Computational Linguistics, pages
306?313, Ann Arbor, MI.
Mohammed Attia, Jennifer Foster, Deirdre Hogan,
Joseph Le Roux, Lamia Tounsi, and Josef van Gen-
abith. 2010. Handling unknown words in statistical
latent-variable parsing models for Arabic, English and
French. In Proceedings of the NAACL/HLT Workshop
on Statistical Parsing of Morphologically Rich Lan-
guages (SPMRL 2010), Los Angeles, CA.
Kepa Bengoetxea and Koldo Gojenola. 2010. Applica-
tion of different techniques to dependency parsing of
Basque. In Proceedings of the NAACL/HLT Workshop
on Statistical Parsing of Morphologically Rich Lan-
guages (SPMRL 2010), Los Angeles, CA.
Daniel M. Bikel. 2002. Design of a multi-lingual,
parallel-processing statistical parsing engine. In Pro-
ceedings of the Second International Conference on
Human Language Technology Research, pages 178?
182. Morgan Kaufmann Publishers Inc. San Francisco,
CA, USA.
E. Black, S. Abney, D. Flickinger, C. Gdaniec, R. Gr-
ishman, P. Harrison, D. Hindle, R. Ingria, F. Jelinek,
J. Klavans, M. Liberman, M. Marcus, S. Roukos,
B. Santorini, and T. Strzalkowski. 1991. A procedure
for quantitatively comparing the syntactic coverage
of English grammars. In Proceedings of the DARPA
Speech and Natural Language Workshop, pages 306?
311, San Mateo (CA). Morgan Kaufman.
Rens Bod. 2003. An efficient implementation of a new
DOP model. In Proceedings of the tenth conference
on European chapter of the Association for Computa-
tional Linguistics, pages 19?26, Budapest, Hungary.
Joan Bresnan. 2001. Lexical-Functional Syntax. Black-
well, Oxford.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of the Tenth Conference on Computa-
tional Language Learning (CoNLL), pages 149?164,
New York, NY.
Marie Candito and Djame? Seddah. 2010. Parsing word
clusters. In Proceedings of the NAACL/HLT Workshop
on Statistical Parsing of Morphologically Rich Lan-
guages (SPMRL 2010), Los Angeles, CA.
Xavier Carreras, Michael Collins, and Terry Koo. 2008.
TAG, dynamic programming, and the perceptron for
efficient, feature-rich parsing. In Proceedings of the
Twelfth Conference on Computational Natural Lan-
guage Learning (CoNLL), pages 9?16, Manchester,
UK.
Eugene Charniak and Mark Johnson. 2005. Course-to-
fine n-best-parsing and maxent discriminative rerank-
ing. In Proceedings of the 43rd Annual Meeting of the
ACL, pages 173?180, Barcelona, Spain, June.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proceedings of the 1st Annual Meeting of the
North American Chapter of the ACL (NAACL), Seattle.
David Chiang. 2000. Statistical parsing with an
automatically-extracted Tree Adjoining Grammar. In
Proceedings of the 38th Annual Meeting on Associ-
ation for Computational Linguistics, pages 456?463,
Hong Kong. Association for Computational Linguis-
tics Morristown, NJ, USA.
Tagyoung Chung, Matt Post, and Daniel Gildea. 2010.
Factors affecting the accuracy of Korean parsing. In
Proceedings of the NAACL/HLT Workshop on Sta-
tistical Parsing of Morphologically Rich Languages
(SPMRL 2010), Los Angeles, CA.
Michael Collins, Jan Hajic?, Lance Ramshaw, and
Christoph Tillmann. 1999. A statistical parser for
Czech. In Proceedings of the 37th Annual Meeting
of the ACL, volume 37, pages 505?512, College Park,
MD.
Michael Collins. 1997. Three Generative, Lexicalized
Models for Statistical Parsing. In Proceedings of the
35th Annual Meeting of the Association for Computa-
tional Linguistics, pages 16?23, Madrid, Spain.
Anna Corazza, Alberto Lavelli, Giogio Satta, and
Roberto Zanoli. 2004. Analyzing an Italian treebank
with state-of-the-art statistical parsers. In Proceedings
of the Third Third Workshop on Treebanks and Lin-
guistic Theories (TLT 2004), Tu?bingen, Germany.
Brooke Cowan and Michael Collins. 2005. Morphology
and reranking for the statistical parsing of Spanish. In
in Proceedins of EMNLP.
Benoit Crabbe? and Marie Candito. 2008. Expe?riences
d?analyse syntaxique statistique du franc?ais. In Actes
de la 15e`me Confe?rence sur le Traitement Automatique
des Langues Naturelles (TALN?08), pages 45?54, Avi-
gnon, France.
Amit Dubey and Frank Keller. 2003. Probabilistic pars-
ing for German using sister-head dependencies. In In
Proceedings of the 41st Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 96?103,
Ann Arbor, MI.
Amit Dubey. 2005. What to do when lexicalization fails:
parsing German with suffix analysis and smoothing.
In 43rd Annual Meeting of the Association for Compu-
tational Linguistics.
10
Jenny Rose Finkel, Alex Kleeman, and Christopher D.
Manning. 2008. Efficient, feature-based, conditional
random field parsing. In Proceedings of ACL.
Yoav Goldberg and Michael Elhadad. 2010. Easy-
first dependency parsing of Modern Hebrew. In Pro-
ceedings of the NAACL/HLT Workshop on Statistical
Parsing of Morphologically Rich Languages (SPMRL
2010), Los Angeles, CA.
Yoav Goldberg and Reut Tsarfaty. 2008. A single frame-
work for joint morphological segmentation and syntac-
tic parsing. In Proceedings of the 46nd Annual Meet-
ing of the Association for Computational Linguistics.
Yoav Goldberg, Reut Tsarfaty, Meni Adler, and Michael
Elhadad. 2009. Enhancing unlexicalized parsing per-
formance using a wide coverage lexicon, fuzzy tag-set
mapping, and em-hmm-based lexical probabilities. In
Proceedings of the 12th Conference of the European
Chapter of the Association for Computational Linguis-
tics, pages 327?335.
Kenneth L. Hale. 1983. Warlpiri and the grammar of
non-configurational languages. Natural Language and
Linguistic Theory, 1(1).
Erhard W. Hinrichs, Sandra Ku?bler, and Karin Naumann.
2005. A unified representation for morphological,
syntactic, semantic, and referential annotations. In
Proceedings of the ACL Workshop on Frontiers in Cor-
pus Annotation II: Pie in the Sky, pages 13?20, Ann
Arbor, MI.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of
ACL.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of ACL, pages
423?430.
Sandra Ku?bler, Erhard W. Hinrichs, and Wolfgang Maier.
2006. Is it really that difficult to parse German?
In Proceedings of the 2006 Conference on Empirical
Methods in Natural Language Processing, pages 111?
119, Sydney, Australia, July. Association for Compu-
tational Linguistics.
Sandra Ku?bler. 2008. The PaGe 2008 shared task on
parsing German. In Proceedings of the Workshop on
Parsing German, pages 55?63. Association for Com-
putational Linguistics.
Seth Kulick, Ryan Gabbard, and Mitchell Marcus. 2006.
Parsing the Arabic treebank: Analysis and improve-
ments. In Proceedings of TLT.
Dekang Lin. 1995. A dependency-based method for
evaluating broad-coverage parsers. In International
Joint Conference on Artificial Intelligence, pages
1420?1425, Montreal.
Mohamed Maamouri, Ann Bies, Tim Buckwalter, and
Wigdan Mekki. 2004. The Penn Arabic treebank:
Building a large-scale annotated Arabic corpus. In
Proceedings of NEMLAR International Conference on
Arabic Language Resources and Tools.
Mohamed Maamouri, Ann Bies, and Seth Kulick. 2008.
Enhanced annotation and parsing of the Arabic tree-
bank. In Proceedings of INFOS.
David M. Magerman. 1995. Statistical decision-tree
models for parsing. In Proceedings of the 33rd An-
nual Meeting on Association for Computational Lin-
guistics, pages 276?283, Cambridge, MA.
Wolfgang Maier. 2010. Direct parsing of discontin-
uous constituents in german. In Proceedings of the
NAACL/HLT Workshop on Statistical Parsing of Mor-
phologically Rich Languages (SPMRL 2010), Los An-
geles, CA.
Mitchell Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19(2):313?330.
Yuval Marton, Nizar Habash, and Owen Rambow. 2010.
Improving Arabic dependency parsing with lexical and
inflectional morphological features. In Proceedings of
the NAACL/HLT Workshop on Statistical Parsing of
Morphologically Rich Languages (SPMRL 2010), Los
Angeles, CA.
Ryan T. McDonald and Fernando C. N. Pereira. 2006.
Online learning of approximate dependency parsing
algorithms. In Proc. of EACL?06.
Ryan T. McDonald, Koby Crammer, and Fernando C. N.
Pereira. 2005. Online large-margin training of depen-
dency parsers. In Proc. of ACL?05, Ann Arbor, USA.
Igor Mel?c?uk. 1988. Dependency Syntax: Theory and
Practice. State University of New York Press.
Marianne Mithun. 1992. Is basic word order universal?
In Doris L. Payne, editor, Pragmatics of Word Order
Flexibility. John Benjamins, Amsterdam.
Joakim Nivre and Jens Nilsson. 2005. Pseudo-projective
dependency parsing. In Proceedings of the 43rd An-
nual Meeting of the Association for Computational
Linguistics (ACL), Ann Arbor, MI.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan Mc-
Donald, Jens Nilsson, Sebastian Riedel, and Deniz
Yuret. 2007a. The CoNLL 2007 shared task on depen-
dency parsing. In Proceedings of the CoNLL Shared
Task Session of EMNLP-CoNLL 2007, pages 915?932,
Prague, Czech Republic, June. Association for Com-
putational Linguistics.
Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan McDon-
ald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret.
2007b. The CoNLL 2007 shared task on dependency
parsing. In Proceedings of the CoNLL Shared Task
Session of EMNLP-CoNLL 2007, pages 915?932.
11
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev,
Gu?ls?en Eryig?it, Sandra Ku?bler, Svetoslav Marinov,
and Erwin Marsi. 2007c. MaltParser: A language-
independent system for data-driven dependency pars-
ing. Natural Language Engineering, 13(2):95?135.
Joakim Nivre. 2009. Non-projective dependency parsing
in expected linear time. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL and
the 4th International Joint Conference on Natural Lan-
guage Processing of the AFNLP.
Slav Petrov and Dan Klein. 2007. Improved inference
for unlexicalized parsing. In Human Language Tech-
nologies 2007: The Conference of the North American
Chapter of the Association for Computational Linguis-
tics; Proceedings of the Main Conference.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of the 21st In-
ternational Conference on Computational Linguistics
and 44th Annual Meeting of the Association for Com-
putational Linguistics, Sydney, Australia, July. Asso-
ciation for Computational Linguistics.
Ines Rehbein and Josef van Genabith. 2007. Treebank
annotation schemes and parser evaluation for German.
In Proceedings of the 2007 Joint Conference on Em-
pirical Methods in Natural Language Processing and
Computational Natural Language Learning (EMNLP-
CoNLL), Prague, Czech Republic.
Geoffrey Sampson and Anna Babarczy. 2003. A test of
the leaf-ancestor metric for parse accuracy. Natural
Language Engineering, 9(04):365?380.
Natalie Schluter and Josef van Genabith. 2007. Prepar-
ing, restructuring, and augmenting a French Treebank:
Lexicalised parsers or coherent treebanks? In Proc. of
PACLING 07, Melbourne, Australia.
Djame? Seddah, Marie Candito, and Benoit Crabbe?. 2009.
Cross parser evaluation and tagset variation: A French
Treebank study. In Proceedings of the 11th Interna-
tion Conference on Parsing Technologies (IWPT?09),
pages 150?161, Paris, France, October. Association
for Computational Linguistics.
Djame? Seddah, Grzegorz Chrupa?a, Ozlem Cetinoglu,
Josef van Genabith, and Marie Candito. 2010.
Lemmatization and statistical lexicalized parsing of
morphologically-rich languages. In Proceedings of the
NAACL/HLT Workshop on Statistical Parsing of Mor-
phologically Rich Languages (SPMRL 2010), Los An-
geles, CA.
Wojciech Skut, Thorsten Brants, Brigitte Krenn, and
Hans Uszkoreit. 1998. A linguistically interpreted
corpus of German newspaper texts. In ESSLLI
Workshop on Recent Advances in Corpus Annotation,
Saarbru?cken, Germany.
Reut Tsarfaty and Khalil Sima?an. 2007. Three-
dimensional parametrization for parsing morphologi-
cally rich languages. In Proceedings of the 10th Inter-
national Conference on Parsing Technologies (IWPT),
pages 156?167.
Reut Tsarfaty and Khalil Sima?an. 2008. Relational-
Realizational parsing. In Proceedings of the 22nd In-
ternational Conference on Computational Linguistics,
pages 889?896.
Reut Tsarfaty and Khalil Sima?an. 2010. Model-
ing morphosyntactic agreement in constituency-based
parsing of Modern Hebrew. In Proceedings of the
NAACL/HLT Workshop on Statistical Parsing of Mor-
phologically Rich Languages (SPMRL 2010), Los An-
geles, CA.
Yannick Versley and Ines Rehbein. 2009. Scalable dis-
criminative parsing for german. In Proceedings of the
11th International Conference on Parsing Technolo-
gies (IWPT?09), pages 134?137, Paris, France, Octo-
ber. Association for Computational Linguistics.
12
Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010, pages 147?151,
Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational Linguistics
Chunking German: An Unsolved Problem
Sandra Ku?bler
Indiana University
Bloomington, IN, USA
skuebler@indiana.edu
Kathrin Beck, Erhard Hinrichs, Heike Telljohann
Universita?t Tu?bingen
Tu?bingen, Germany
{kbeck,eh,telljohann}@sfs.
uni-tuebingen.de
Abstract
This paper describes a CoNLL-style
chunk representation for the Tu?bingen
Treebank of Written German, which as-
sumes a flat chunk structure so that each
word belongs to at most one chunk. For
German, such a chunk definition causes
problems in cases of complex prenominal
modification. We introduce a flat annota-
tion that can handle these structures via a
stranded noun chunk.
1 Introduction
The purpose of this paper is to investigate how the
annotation of noun phrases in the Tu?bingen Tree-
bank of Written German (Tu?Ba-D/Z) can be trans-
formed into chunks with no internal structure, as
proposed in the CoNLL 2000 shared task (Tjong
Kim Sang and Buchholz, 2000). Chunk parsing is
a form of partial parsing, in which non-recursive
phrases are annotated while difficult decisions,
such as prepositional phrase attachment, are left
unsolved. Flat chunk representations are particu-
larly suitable for machine learning approaches to
partial parsing and are inspired by the IOB ap-
proach to NP chunking first proposed by Ramshaw
and Marcus (1995). They are particularly relevant
for approaches that require an efficient analysis but
not necessarily a complete syntactic analysis.
German allows a higher degree of syntactic
complexity in prenominal modification of the syn-
tactic head of an NP compared to English. This
is particularly evident in written texts annotated
in the Tu?Ba-D/Z. The complexity of German
NPs that causes problems in the conversion to
CoNLL-style chunks also affects PCFG parsing
approaches to German.The complexity of NPs is
one of the phenomena that have been addressed in
tree transformation approaches for German pars-
ing (Trushkina, 2004; Ule, 2007; Versley and Reh-
bein, 2009).
2 Defining Chunks
The notion of a chunk is orginally due to Abney
(1991), who considers chunks as non-recursive
phrases which span from the left periphery of a
phrase to the phrasal head. Accordingly, the sen-
tence ?The woman in the lab coat thought you
had bought an expensive book.? is assigned the
chunk structure: ?[S [NP The woman] [PP in [NP
the lab coat] ] [VP thought] ] [S [NP you] [VP
had bought] [NP an [ADJP expensive] book]] .?.
Abney-style chunk parsing is implemented as cas-
caded, finite-state transduction (cf. (Abney, 1996;
Karlsson et al, 1995)).
Notice that cascaded, finite-state transduction
allows for the possibility of chunks containing
other chunks as in the above sentence, where the
prepositional chunk contains a noun chunk within.
The only constraint on such nested chunks is the
prohibition on recursive structures. This rules out
chunks in which, for example, a noun chunk con-
tains another noun chunk. A much stricter con-
straint on the internal structure of chunks was sub-
sequently adopted by the shared task on chunk
parsing as part of the Conference for Natural Lan-
guage Learning (CoNLL) in the year 2000 (Tjong
Kim Sang and Buchholz, 2000). In this shared
task, chunks were defined as non-overlapping,
non-recursive phrases so that each word is part of
at most one chunk. Based on this definition, the
prepositional phrase in the sentence above would
be chunked as ?[Prep in] [NP the lab coat]?. Since
the prepositional chunk cannot have an embedded
noun chunk, the definition of the CoNLL shared
task assumed that the prepositional chunk only
contains the preposition, thus taking the definition
seriously that the chunk ends with the head. The
noun chunk remains separate. Additionally, the
noun phrase ?an expensive book? is annotated as a
noun chunk without internal structure.
The CoNLL shared task definition of chunks is
147
Figure 1: Treebank annotation for the sentence in (2).
useful for machine learning based approaches to
chunking since it only requires one level of anal-
ysis, which can be represented as IOB-chunking
(Tjong Kim Sang and Buchholz, 2000). For En-
glish, this definition of chunks has become stan-
dard in the literature on machine learning.
For German, chunk parsing has been investi-
gated by Kermes and Evert (2002) and by Mu?ller
(2004). Both approaches used an Abney-style
chunk definition. However, there is no corre-
sponding flat chunk representation for German be-
cause of the complexity of pre-head modification
in German noun phrases. Sentence (1) provides a
typical example of this kind.
(1) [NC der
the
[NC seinen
his
Sohn]
son
liebende
loving
Vater]
father
?the father who loves his son?
The structure in (1) violates both the Abney-
style and the CoNLL-style definitions of chunks ?
Abney?s because it is recursive and the CoNLL-
style definition because of the embedding. A
single-level, CoNLL-style chunk analysis will
have to cope with the separation of the determiner
?der? and the head of the outer phrase. We will
discuss an analysis in section 5.
3 The Treebank: Tu?Ba-D/Z
The Tu?bingen Treebank of Written German
(Tu?Ba-D/Z) is a linguistically annotated corpus
based on data of the German newspaper ?die
tageszeitung? (taz). Currently, it comprises ap-
proximately 45 000 sentences. For the syntactic
annotation, a theory-neutral and surface-oriented
annotation scheme has been adopted that is in-
spired by the notion of topological fields and
enriched by a level of predicate-argument struc-
ture. The annotation scheme comprises four lev-
els of syntactic annotation: the lexical level, the
phrasal level, the level of topological fields, and
the clausal level. The primary ordering princi-
ple of a clause is the inventory of topological
fields, which characterize the word order regu-
larities among different clause types of German,
and which are widely accepted among descrip-
tive linguists of German (cf. (Drach, 1937; Ho?hle,
1986)). Below this level of annotation, i.e. strictly
within the bounds of topological fields, a phrase
level of predicate-argument structure is applied
with its own descriptive inventory based on a min-
imal set of assumptions that has to be captured by
any syntactic theory. The context-free backbone of
phrase structure (Telljohann et al, 2004) is com-
bined with edge labels specifying the grammatical
functions and long-distance relations. For more
details on the annotation scheme see Telljohann et
al. (2009).
(2) Der Spitzenreiter in der europa?ischen
Gastgeberliga war bei den bosnischen
Bu?rgerkriegsflu?chtlingen noch weitaus
gro?zu?giger.
?The front-runner in the European league of host
countries was far more generous with the Bosnian
civil war refugees.?
Figure 1 shows the tree for the sentence in (2).
The main clause (SIMPX) is divided into three
topological fields: initial field (VF), left sentence
bracket (LK), and middle field (MF). The finite
148
verb in LK is the head (HD) of the sentence.
The edge labels between the level of topological
fields and the phrasal level constitute the gram-
matical function of the respective phrase: sub-
ject (ON), ambiguous modifier (MOD), and predi-
cate (PRED). The label V-MOD specifies the long-
distance dependency of the prepositional phrase
on the main verb. Below the lexical level, the parts
of speech are annotated. The hierarchical annota-
tion of constituent structure and head (HD) / non-
head (-) labels capture phrase internal dependen-
cies. While premodifiers are attached directly on
the same level, postmodifiers are attached higher
in order to keep their modification scope ambigu-
ous. The PP ?in der europa?ischen Gastgeberliga?
is the postmodifier of the head-NX and therefore
attached on a higher phrase level.
4 General Conversion Strategy
The conversion to CoNLL-style chunks starts
from the syntactic annotation of the Tu?Ba-D/Z.
In general, we directly convert the lowest phrasal
projections with lexical content to chunks. For
the sentence in (2) above, the chunk annotation is
shown in (3). Here, the first noun phrase1, ?Der
Spitzenreiter?, as well as the finite verb phrase and
the adverbial phrase are used as chunks.
(3) [NX Der Spitzenreiter] [PX in
der europa?ischen Gastgeberliga]
[VXFIN war] [PX bei den bosnischen
Bu?rgerkriegsflu?chtlingen] [ADVX noch]
[ADJX weitaus gro?zu?giger].
This sentence also shows exceptions to the
general conversion rule: We follow Tjong
Kim Sang and Buchholz (2000) in including
ADJPs into the NCs, such as in ?den bos-
nischen Bu?rgerkriegsflu?chtlingen?. We also in-
clude premodifying adverbs into ADJCs, such as
in ?weitaus gro?zu?giger?. But we deviate from
Tjong Kim Sang and Buchholz in our definition of
the PCs and include the head NP into this chunk,
such as in ?in der europa?ischen Gastgeberliga?.
(4) a. Allerdings werden wohl Rational-
isierungen mit der Modernisierung
1For the sake of convenience, we will use acronyms in the
remainder of the paper. Since we use the same labels in the
treebank annotation and in the chunk representation (mostly
ending in X), we will use labels ending in P (e.g. NP, PP) to
talk about phrases in the treebank and labels ending in C (e.g.
NC, PC) to talk about chunks.
der Beho?rdenarbeit einhergehen.
?However, rationalizations will accompany
modernization in the workflow of civil service
agencies.?
b. [ADVX Allerdings] [VXFIN wer-
den] [ADVX wohl] [NX Rationalis-
ierungen] [PX mit der Moder-
nisierung] [NX der Beho?rdenarbeit]
[VXINF einhergehen].
In cases of complex, post-modified noun
phrases grouped under the prepositional phrase,
we include the head noun phrase into the preposi-
tional chunk but group the postmodifying phrase
into a separate phrase. The sentence in (4a)
gives an example for such a complex noun phrase.
This sentence is assigned the chunk annotation in
(4b). Here, the head NP ?der Modernisierung? is
grouped in the PC while the post-modifying NP
?der Beho?rdenarbeit? constitutes its own NC.
The only lexical constituent in the treebank that
is exempt from becoming a chunk is the named
entity constituent (EN-ADD). Since these con-
stituents do not play a syntactic role in the tree,
they are elided in the conversion to chunks.
5 Complications in German
While the conversion based on the phrasal anno-
tation of Tu?Ba-D/Z results in the expected chunk
structures, it is incapable of handling a small num-
ber of cases correctly. Most of these cases involve
complex NPs. We will concentrate here on one
case: complex premodified NPs that include the
complement of a participle or an adjective, as dis-
cussed in section 2. This is a non-trivial problem
since the treebank contains 1 497 cases in which
an ADJP within an NP contains a PP and 415
cases, in which an ADJP within an NP contains
another NP. Sentence (5a) with the syntactic an-
notation in Figure 2 gives an example for such an
embedded PP.
(5) a. Die teilweise in die Erde gebaute
Sporthalle wird wegen ihrer futuris-
tischen Architektur auch als ?Sport-
Ei? bezeichnet.
?The partially underground sports complex is
also called the ?sports egg? because of its fu-
turistic architecture.?
b. [sNX Die] [ADVX teilweise] [PX in
die Erde] [NX gebaute Sporthalle]
[VXFIN wird] [PX wegen ihrer futu-
ristischen Architektur] [ADVX auch]
149
Figure 2: Treebank annotation for the sentence in (5a).
[NX als ? Sport-Ei] ? [VXINF be-
zeichnet].
Since we are interested in a flat chunk annota-
tion in which each word belongs to at most one
chunk, the Abney-style embedded chunk defini-
tion shown in sentence (1) is impossible. If we de-
cide to annotate the PP ?in die Erde? as a chunk,
we are left with two parts of the embedding NP:
the determiner ?Die? and the ADVP ?teilweise? to
the left of the PP and the ADJP ?gebaute? and the
noun on the right. The right part of the NP can
be easily grouped into an NC, and the ADVP can
stand on its own. The only remaining problem is
the treatment of the determiner, which in German,
cannot constitute a phrase on its own. We decided
to create a new type of chunk, stranded NC (sNX),
which denotes that this chunk is part of an NC, to
which it is not adjacent. Thus the sentence in (5a)
has the chunk structure shown in (5b).
The type of complex NPs shown in the previ-
ous section can become arbitrarily complex. The
example in (6a) with its syntactic analysis in Fig-
ure 3 shows that the attributively used adjective
?sammelnden? can have all its complements and
adjuncts. Here, we have a reflexive pronoun ?sich?
and a complex PP ?direkt vor ihrem Sezessions-
Standort am Karlsplatz?. The chunk analysis
based on the principles from section 4 gives us the
analysis in (6b). The complex PP is represented as
three different chunks: an ADVC, and two PCs.
(6) a. Sie ?thematisierten? auf Anraten des
jetzigen Staatskurators Wolfgang
Zinggl die sich direkt vor ihrem
Sezessions-Standort am Karlsplatz
sammelnden Fixer.
?On the advice of the current state curator
Wolfgang Zinggl, they ?broach the issue? of
the junkies who gather right in front of their
location of secession at the Karlsplatz .?
b. [NX Sie] ? [VXFIN thematisierten]
? [PX auf Anraten] [NX des jet-
zigen Staatskurators] [NX Wolfgang
Zinggl] [sNX die] [NX sich] [ADVX
direkt] [PX vor ihrem Sezessions-
Standort] [PX am Karlsplatz] [NX
sammelnden Fixer].
6 Conclusion
In this paper, we have shown how a CoNLL-
style chunk representation can be derived from
Tu?Ba-D/Z. For the complications stemming from
complex prenominal modification, we proposed
an analysis in which the stranded determiner is
marked as such. For the future, we are planning
to make this chunk representation available to li-
cense holders of the treebank.
References
Steven Abney. 1991. Parsing by chunks. In Robert
Berwick, Steven Abney, and Caroll Tenney, editors,
Principle-Based Parsing, pages 257?278. Kluwer
Academic Publishers, Dordrecht.
Steven Abney. 1996. Partial parsing via finite-state
cascades. In John Carroll, editor, ESSLLI Workshop
on Robust Parsing, pages 8?15, Prague, Czech Re-
public.
Erich Drach. 1937. Grundgedanken der Deutschen
Satzlehre. Diesterweg, Frankfurt/M.
150
Figure 3: Treebank annotation for the sentence in (6a).
Tilman Ho?hle. 1986. Der Begriff ?Mit-
telfeld?, Anmerkungen u?ber die Theorie der topo-
logischen Felder. In Akten des Siebten Interna-
tionalen Germanistenkongresses 1985, pages 329?
340, Go?ttingen, Germany.
Fred Karlsson, Atro Voutilainen, J. Heikkila?, and Atro
Anttila, editors. 1995. Constraint Grammar: A
Language-Independent System for Parsing Unre-
stricted Text. Mouton de Gruyter.
Hannah Kermes and Stefan Evert. 2002. YAC ? a
recursive chunker for unrestricted German text. In
Proceedings of the Third International Conference
on Language Resources and Evaluation (LREC),
Las Palmas, Gran Canaria.
Frank H. Mu?ller. 2004. Annotating grammatical func-
tions in German using finite-state cascades. In Pro-
ceedings of COLING 2004, Geneva, Switzerland.
Lance A. Ramshaw and Mitchell P. Marcus. 1995.
Text chunking using transformation-based learning.
In Proceedings of the ACL 3rd Workshop on Very
Large Corpora, pages 82?94, Cambridge, MA.
Heike Telljohann, Erhard Hinrichs, and Sandra Ku?bler.
2004. The Tu?Ba-D/Z treebank: Annotating German
with a context-free backbone. In Proceedings of the
Fourth International Conference on Language Re-
sources and Evaluation (LREC), pages 2229?2235,
Lisbon, Portugal.
Heike Telljohann, Erhard W. Hinrichs, Sandra Ku?bler,
Heike Zinsmeister, and Kathrin Beck, 2009. Style-
book for the Tu?bingen Treebank of Written German
(Tu?Ba-D/Z). Seminar fu?r Sprachwissenschaft, Uni-
versita?t Tu?bingen, Germany.
Erik Tjong Kim Sang and Sabine Buchholz. 2000. In-
troduction to the CoNLL shared task: Chunking. In
Proceedings of The Fourth Conference on Computa-
tional Language Learning, CoNLL?00, and the Sec-
ond Learning Language in Logic Workshop, LLL?00,
pages 127?132, Lisbon, Portugal.
Julia S. Trushkina. 2004. Morpho-Syntactic Annota-
tion andDependency Parsing of German. Ph.D. the-
sis, Eberhard-Karls Universita?t Tu?bingen.
Tylman Ule. 2007. Treebank Refinement: Opti-
mising Representations of Syntactic Analyses for
Probabilistic Context-Free Parsing. Ph.D. thesis,
Eberhard-Karls Universita?t Tu?bingen.
Yannick Versley and Ines Rehbein. 2009. Scalable dis-
criminative parsing for German. In Proceedings of
the International Conference on Parsing Technology
(IWPT?09), Paris, France.
151
Proceedings of the Fifteenth Conference on Computational Natural Language Learning, pages 200?209,
Portland, Oregon, USA, 23?24 June 2011. c?2011 Association for Computational Linguistics
Filling the Gap:
Semi-Supervised Learning for Opinion Detection Across Domains
Ning Yu
Indiana University
nyu@indiana.edu
Sandra Ku?bler
Indiana University
skuebler@indiana.edu
Abstract
We investigate the use of Semi-Supervised
Learning (SSL) in opinion detection both in
sparse data situations and for domain adapta-
tion. We show that co-training reaches the best
results in an in-domain setting with small la-
beled data sets, with a maximum absolute gain
of 33.5%. For domain transfer, we show that
self-training gains an absolute improvement in
labeling accuracy for blog data of 16% over
the supervised approach with target domain
training data.
1 Introduction
Rich and free opinions published electronically and,
more recently, on the WWW offer ample opportuni-
ties to discover individual?s attitudes towards certain
topics, products, or services. To capitalize on this
enormous body of opinions, researchers have been
working in the area of opinion mining since the late
1990s. Opinion detection seeks to automatically de-
termine the presence or absence of opinions in a text,
and it is therefore a fundamental task for opinion
mining.
In order to capture subtle and creative opinions,
opinion detection systems generally assume that a
large body of opinion-labeled data are available.
However, collections of opinion-labeled data are of-
ten limited, especially at the granularity level of sen-
tences; and manual annotation is tedious, expensive
and error-prone. The shortage of opinion-labeled
data is less challenging in some data domains (e.g.,
reviews) than in others (e.g., blog posts). A sim-
ple method for improving accuracies in challenging
domains would be to borrow opinion-labeled data
from a non-target data domain; but this approach
often fails because opinion detection strategies de-
signed for one data domain generally do not perform
well in another domain. One reason for failure of
the simple transfer approach is that the information
used for opinion detection is typically lexical, and
lexical means of expressing opinions may vary not
only from domain to domain, but also from register
to register. For example, while the word ?awesome?
is a good indicator of an opinion in blogs, it is less
likely to occur in the same role in newspaper texts.
While it is difficult to obtain opinion-labeled data,
one can easily collect almost infinite unlabeled user-
generated data that contain opinions. The use of
Semi-Supervised Learning (SSL), motivated by lim-
ited labeled data and plentiful unlabeled data in the
real world, has achieved promising results in vari-
ous NLP studies (e.g., (Fu?rstenau and Lapata, 2009;
Talukdar and Pereira, 2010)), yet it has not been
fully investigated for use in opinion detection. Al-
though studies have shown that simple SSL meth-
ods are promising for extracting opinion features
or patterns using limited opinion-labeled data (e.g.,
(Wiebe and Riloff, 2005)), few efforts have been
made either to apply SSL directly to opinion detec-
tion or to examine more sophisticated SSL methods.
This research is intended to fill the gap regarding ap-
plication of SSL in opinion detection. We investi-
gate a range of SSL algorithms with a focus on self-
training and co-training in three types of electronic
documents: edited news articles, semi-structured
movie reviews, and the informal and unstructured
content of the blogosphere. We conclude that SSL
is a successful method for handling the shortage of
opinion labeled data and the domain transfer prob-
lem.
200
2 Background and Related Work
There is a wide range of literature on opinion detec-
tion. We concentrate here on supervised and semi-
supervised approaches.
2.1 Supervised Learning for Opinion Detection
Supervised learning algorithms that can automati-
cally learn important opinion-bearing features from
an annotated corpus have been adopted and inves-
tigated for opinion detection and yielded satisfying
results (Wiebe et al, 2004; Yu and Hatzivassiloglou,
2003; Zhang and Yu, 2007). With no classifica-
tion techniques developed specifically for opinion
detection, state-of-the-art topical supervised classifi-
cation algorithms can achieve performance compa-
rable to complex linguistic approaches when using
binary values (i.e., presence or absence) and incor-
porating different types of features. Commonly used
opinion-bearing features include bag-of-words, POS
tags, ngrams, low frequency words or unique words
(Wiebe et al, 2004; Yang et al, 2007), semantically
oriented adjectives (e.g., ?great?, ?poor?) and more
complex linguistic patterns. Both the scale and qual-
ity of the annotated corpus play an important role in
the supervised learning approach.
2.2 SSL for Opinion Detection
In contrast to supervised learning, SSL learns from
both labeled and unlabeled data. SSL assumes that,
although unlabeled data hold no information about
classes (e.g., ?opinion? or ?non-opinion?), they do
contain information about joint distribution over
classification features. Therefore, when a limited set
of labeled data is available in the target domain, us-
ing SSL with unlabeled data is expected to achieve
an improvement over supervised learning.
Self-training Self-training is the simplest and
most commonly adopted form of SSL for opinion
detection. Self-training was originally used to fa-
cilitate automatic identification of opinion-bearing
features. For example, Riloff and Wiebe (2003) pro-
posed a bootstrapping process to automatically iden-
tify subjective patterns. Self-training has also been
applied directly for identifying subjective sentences
by following a standard self-training procedure: (1)
train an initial supervised classifier on the labeled
data; (2) apply this classifier to unlabeled data and
select the most confidently labeled data, as deter-
mined by the classifier, to augment the labeled data
set; and (3) re-train the classifier by restarting the
whole process. Wiebe and Riloff (2005) used a self-
trained Na??ve Bayes classifier for classifying subjec-
tive sentences and achieved better recall with modest
precision over several rule-based classifiers.
One shortcoming of self-training is that the result-
ing data may be biased: That is, the final labeled data
may consist of examples that are easiest for this par-
ticular opinion detector to identify.
Co-training The core idea of co-training is to use
two classifiers and trade additional examples be-
tween them, assuming that the resulting union of
classified examples is more balanced than examples
resulting from using either classifier alone. When
labeling new examples, a final decision is made by
combining the predictions of the two updated learn-
ers. The original co-training algorithm assumes re-
dundancy in the training data and thus more than
one view can be used to represent and classify each
example independently and successfully (Blum and
Mitchell, 1998). For example, an image can be nat-
urally represented by its text description or by its
visual attributes. Even when a natural split in the
feature set is not available, studies have shown that
the key to co-training is the existence of two largely
different initial learners, regardless of whether they
are built by using two feature sets or two learning
algorithms (Wang and Zhou, 2007).
When there are different views for the target ex-
amples, co-training is conceptually clearer than self-
training, which simply mixes features. Since co-
training uses each labeled example twice, it requires
less labeled data and converges faster than self-
training. However, the lack of natural feature splits
has kept researchers from exploring co-training for
opinion detection. To the best of our knowledge,
the only co-training application for opinion detec-
tion was reported by Jin et al (2009), who created
disjoint training sets for building two initial classi-
fiers and successfully identified opinion sentences in
camera reviews by selecting auto-labeled sentences
agreed upon by both classifiers.
EM-Based SSL Expectation-Maximization (EM)
refers to a class of iterative algorithms for
maximum-likelihood estimation when dealing with
201
incomplete data. Nigam et al (1999) combined
EM with a Na??ve Bayes classifier to resolve the
problem of topical classification, where unlabeled
data were treated as incomplete data. The EM-NB
SSL algorithm yielded better performance than ei-
ther an unsupervised lexicon-based approach or a
supervised approach for sentiment classification in
different data domains, including blog data (Aue and
Gamon, 2005; Takamura et al, 2006). No opinion
detection applications of EM-based SSL have been
reported in the literature.
S3VMs Semi-Supervised Support Vector Ma-
chines (S3VMs) are a natural extension of SVMs in
the semi-supervised spectrum. They are designed to
find the maximal margin decision boundary in a vec-
tor space containing both labeled and unlabeled ex-
amples. Although SVMs are the most favored super-
vised learning method for opinion detection, S3VMs
have not been used in opinion detection. Graph-
based SSL learning has been successfully applied to
opinion detection (Pang and Lee, 2004) but is not
appropriate for dealing with large scale data sets.
2.3 Domain Adaptation for Opinion Detection
When there are few opinion-labeled data in the
target domain and/or when the characteristics of
the target domain make it challenging to detect
opinions, opinion detection systems usually borrow
opinion-labeled data from other data domains. This
is especially common in opinion detection in the bl-
ogosphere (Chesley et al, 2006). To evaluate this
shallow approach, Aue and Gamon (2005) com-
pared four strategies for utilizing opinion-labeled
data from one or more non-target domains and con-
cluded that using non-targeted labeled data without
an adaptation strategy is less efficient than using la-
beled data from the target domain, even when the
majority of labels are assigned automatically by a
self-training algorithm.
Blitzer et al (2007) and Tan et al (2009) imple-
mented domain adaptation strategies for sentiment
analysis. Although promising, their domain adapta-
tion strategies involved sophisticated and computa-
tionally expensive methods for selecting general fea-
tures to link target and non-target domains.
3 Motivation and Objective
While SSL is especially attractive for opinion de-
tection because it only requires a small number of
labeled examples, the studies described in the previ-
ous section have concentrated on simple SSL meth-
ods. We intend to fill this research gap by comparing
the feasibility and effectiveness of a range of SSL
approaches for opinion detection. Specifically, we
aim to achieve the following goals:
First, to gain a more comprehensive understand-
ing of the utility of SSL in opinion detection. We
examine four major SSL methods: self-training, co-
training, EM-NB, and S3VM. We focus on self-
training and co-training because they are both wrap-
per approaches that can be easily adopted by any ex-
isting opinion detection system.
Second, to design and evaluate co-training strate-
gies for opinion detection. Since recent work has
shown that co-training is not restricted by the orig-
inal multi-view assumption for target data and that
it is more robust than self-training, we evaluate new
co-training strategies for opinion detection.
Third, to approach domain transfer using SSL,
assuming that SSL can overcome the problem of
domain-specific features by gradually introducing
targeted data and thus diminishing bias from the
non-target data set.
4 SSL Experiments
Our research treats opinion detection as a binary
classification problem with two categories: subjec-
tive sentences and objective sentences. It is evalu-
ated in terms of classification accuracy.
Since a document is normally a mixture of facts
and opinions (Wiebe et al, 2001), sub-document
level opinion detection is more useful and meaning-
ful than document-level opinion detection. Thus, we
conduct all experiments on the sentence level.
The remainder of this section explains the data
sets and tools used in this study and presents the ex-
perimental design and parameter settings.
4.1 Data Sets
Three types of data sets have been explored in opin-
ion detection studies: news articles, online reviews,
and online discourse in blogs or discussion forums.
These three types of text differ from one another in
202
terms of structure, text genre (e.g., level of formal-
ity), and proportion of opinions found therein. We
selected a data set from each type in order to inves-
tigate the robustness and adaptability of SSL algo-
rithms for opinion detection and to test the feasibil-
ity of SSL for domain adaptation.
Movie Review One of the standard data sets in
opinion detection is the movie review data set cre-
ated by Pang and Lee (2004). It contains 5,000 sub-
jective sentences or snippets from the Rotten Toma-
toes pages and 5,000 objective sentences or snip-
pets from IMDB plot summaries, all in lowercase.
Sentences containing less than 10 tokens were ex-
cluded and the data set was labeled automatically
by assuming opinion inheritance: every sentence in
an opinion-bearing document expresses an opinion,
and every sentence in a factual document is factual.
Although this assumption appears to be acceptable
for movie review data, it is generally unreliable for
other domains.
News Article The Wall Street Journal part of the
Penn Treebank III has been manually augmented
with opinion related annotations. This set is widely
used as a gold-standard corpus in opinion detection
research. According to the coding manual (Wiebe
et al, 1999), subjective sentences are those express-
ing evaluations, opinions, emotions, and specula-
tions. For our research, 5,174 objective sentences
and 5,297 subjective sentences were selected based
on the absence or presence of manually labeled sub-
jective expressions.
JDPA Blog Post The JDPA corpus (Kessler et al,
2010) is a new opinion corpus released in 2010. It
consists of blog posts that express opinions about
automobile and digital cameras with named entities
and sentiments expressed about them manually an-
notated. For our purpose, we extracted all sentences
containing sentiment-bearing expressions as subjec-
tive sentences and manually chose objective sen-
tences from the rest by eliminating subjective sen-
tences that were not targeted to any labeled entities.
After this process, we had approximately 10,000
subjective sentences and 4,348 objective sentences.
To balance the number of subjective and objective
sentences, we used 4,348 sentences from each cate-
gory.
4.2 Data Preparation
We removed a small number of stop words. No
stemming was conducted since the literature shows
no clear gain from stemming in opinion detection.
One reason for this may be that stemming actually
erases subtle opinion cues such as past tense verbs.
All words were converted to lowercase and numbers
were replaced by a placeholder #. Both unigrams
and bigrams were generated for each sentence.
Each data set was randomly split into three por-
tions: 5% of the sentences were selected as the eval-
uation set and were not available during SSL and
supervised learning (SL) runs; 90% were treated as
unlabeled data (U) for SSL runs and i% (1 ? i ? 5)
as labeled data (L) for both SL and SSL runs. For
each SSL run, a baseline SL run was designed with
the same number of labeled sentences (i%) and a
full SL run was designed with all available sentences
(90% + i%). If effective, an SSL run would signifi-
cantly outperform its corresponding baseline SL run
and approach the performance of a full SL run.
4.3 Experimental Design
We conducted three groups of experiments: 1) to in-
vestigate the effectiveness of the SSL approach for
opinion detection; 2) to explore different co-training
strategies; and 3) to evaluate the applicability of SSL
for domain adaptation.
4.3.1 General Settings for SSL
The Na??ve Bayes classifier was selected as the
initial classifier for self-training because of its abil-
ity to produce prediction scores and to work well
with small labeled data sets. We used binary values
for unigram and bigram features, motivated by the
brevity of the text unit at the sentence level as well
as by the characteristics of opinion detection, where
occurrence frequency has proven to be less influen-
tial. We implemented two feature selection options:
Chi square and Information Gain.
Parameters for SSL included: (1) Threshold k for
number of iterations. If k is set to 0, the stopping
criterion is convergence; (2) Number of unlabeled
sentences available in each iteration u (u << U );
(3) Number of opinion and non-opinion sentences,
p and n, to augment L during each iteration; and (4)
Weighting parameter ? for auto-labeled data. When
? is set to 0, auto-labeled and labeled data are treated
203
equally; when ? is set to 1, feature values in an
auto-labeled sentence are multiplied by the predic-
tion score assigned to the sentence.
We used the WEKA data mining software (Hall
et al, 2009) for data processing and classifica-
tion of the self-training and co-training experi-
ments. EM implemented in LingPipe (Alias-i, 2008)
was used for the EM-NB runs. S3VMs imple-
mented in SVMlight (Joachims, 1999) and based
on local search were adopted for the S3VM runs.
Since hyper-parameter optimization for EM-NB and
S3VM is not the focus of this research and prelim-
inary explorations on parameter settings suggested
no significant benefit, default settings were applied
for EM-NB and S3VM.
4.3.2 Co-Training Strategies
For co-training, we investigated five strategies for
creating two initial classifiers following the criteria
that these two classifiers either capture different fea-
tures or based on different learning assumptions.
Two initial classifiers were generated: (1) Us-
ing unigrams and bigrams respectively to create two
classifiers based on the assumption that low-order
n-grams and high-order n-grams contain redundant
information and represent different views of an ex-
ample: content and context; (2) Randomly splitting
feature set into two; (3) Randomly splitting train-
ing set into two; (4) Applying two different learn-
ing algorithms (i.e., Na??ve Bayes and SVM) with
different biases; and (5) Applying a character-based
language model (CLM) and a bag-of-words (BOW)
model where the former takes into consideration the
sequence of words while the latter does not. In prac-
tice, for strategy (1), bigrams were used in combina-
tion with unigrams because bigrams alone are weak
features when extracted from limited labeled data at
sentence level.
Auto-labeled sentences were selected if they were
assigned a label that both classifiers agreed on with
highest confidence. Because our initial classifiers vi-
olated the original co-training assumptions, forcing
agreement between confident predictions improved
the maintenance of high precision.
4.3.3 Self-Training for Domain Adaptation
Based on the literature and our preliminary re-
sults (Yu and Ku?bler, 2010), movie reviews achieve
# Labeled Examples
Type 100 200 300 400 500 all
Self-tr 85.2 86.6 87.0 87.2 86.6
SL 63.8 73.6 77.2 79.4 80.2 89.4
Co-tr. 92.2 93.8 92.6 93.2 91.4
SL 75.8 80.8 82.6 85.2 84.8 95.2
EM-NB 88.1 88.7 88.6 88.4 89.0
SL 73.5 78.7 81.3 82.8 83.9 91.6
S3VM 59.0 68.4 67.8 67.0 75.2
SL 70.0 72.8 75.6 76.2 80.0 90.0
Table 1: Classification accuracy(%) of SSL and SL on
movie reviews
the highest accuracy while news articles and blog
reviews are considerably more challenging. Thus,
we decided to use movie reviews as source data
and news articles and blog posts as target data do-
mains. While the data split for the target domain re-
mains the same as in section 4.2, all sentences in the
source domain, except for the 5% evaluation data,
were treated as labeled data. For example, in order
to identify opinion-bearing sentences from the blog
data set, all 9,500 movie review sentences and i%
of blog sentences were used as labeled data, 90% of
blog sentences were used as unlabeled data, and 5%
as evaluation data. We also applied a parameter to
gradually decrease the weight of the source domain
data, similar to the work done by Tan et al (2009).
5 Results and Evaluation
Overall, our results suggest that SSL improves ac-
curacy for opinion detection although the contribu-
tion of SSL varies across data domains and different
strategies need to be applied to achieve optimized
performance. For the movie review data set, almost
all SSL runs outperformed their corresponding base-
line SL runs and approached full SL runs; for the
news article data set, SSL performance followed a
similar trend but with only a small rate of increase;
for the blog post data set, SSL runs using only blog
data showed no benefits over the SL baseline, but
with labeled movie review data, SSL runs produced
results comparable with full SL result.
5.1 SSL vs. SL
Table 1 reports the performance of SSL and SL runs
on movie review data based on different numbers
204
of initial labeled sentences. Both the self- and co-
training runs reported here used the same parame-
ter settings: k=0, u=20, p=2, n=2, ? =0, with no
feature selection. The co-training results in Table
1 used a CLM and a BOW model (see section 5.2).
SL runs for co-training classified sentences based on
the highest score generated by two classifiers; SL
runs for S3VM applied the default SVM setting in
SVMlight; and SL runs for EM-NB used the Na??ve
Bayes classifier in the EM-NB implementation in
LingPipe.
Table 1 shows that, except for S3VM, SSL al-
ways outperforms the corresponding SL baseline on
movie reviews: When SSL converges, it achieves
improvement in the range of 8% to 34% over the SL
baseline. The fewer initial labeled data, the more
benefits an SSL run gained from using unlabeled
data. For example, using 100 labeled sentences, self-
training achieved a classification accuracy of 85.2%
and outperformed the baseline SL by 33.5%. Al-
though this SSL run was surpassed by 4.9% by the
full SL run using all labeled data, a great amount
of effort was saved by labeling only 100 sentences
rather than 9,500. Co-training produced the best
SSL results. For example, with only 200 labeled
sentences, co-training yielded accuracy as high as
93.8%. Overall, SSL for opinion detection on movie
reviews shows similar trends to SSL for traditional
topical classification (Nigam and Ghani, 2000).
However, the advantages of SSL were not as sig-
nificant in other data domains. Figure 1 demon-
strates the performance of four types of SSL runs
relative to corresponding baseline and full SL runs
for all three data sets. All SSL runs reported here
used 5% data as labeled data. Lines with different
patterns indicate different data sets, green triangles
mark baseline SL runs, green dots mark full SL runs,
and red crosses mark SSL runs. Numbers next to
symbols indicate classification accuracy. For each
line, if the red cross is located above the triangle,
it indicates that the SSL run improved over the SL
baseline; and, the closer the red cross to the upper
dot, the more effective was the SSL run. Figure 1
shows that S3VM degrades in performance for all
three data sets and we exclude it from the follow-
ing discussion. From movie reviews to news articles
to blog posts, the classification accuracy of baseline
SL runs as well as the improvement gained by SSL
Figure 1: Classification accuracy(%) of SSL and SL on
three data sets (i=5)
runs decreased: With greater than 80% baseline ac-
curacy on movie reviews, SSL runs were most effec-
tive; with slightly above 70% baseline accuracy on
news articles, self-training actually decreased per-
formance of the corresponding SL baseline while
co-training and EM-NB outperformed the SL base-
line only slightly; and with 60% or so baseline accu-
racy on blog posts, none of the SSL methods showed
improvement. We assume that the lower the baseline
accuracy, the worse the quality of auto-labeled data,
and, therefore, the less advantages is application of
SSL. We also found that the average sentence length
in blog posts (17 words) is shorter than the average
sentence length in either movie reviews (23.5 words)
or news articles (22.5 words), which posed an addi-
tional challenge because there is less information for
the classifier in terms of numbers of features.
Overall, for movie reviews and news articles, co-
training proved to be most robust and effective and
EM-NB showed consistent improvement over the
SL baseline. For news articles, EM-NB increased
accuracy from 63.5% to 68.8% with only 100 la-
beled sentences. For movie reviews, a close look at
EM-NB iterations shows that, with only 32 labeled
sentences, EM-NB was able to achieve 88% clas-
sification accuracy, which is close to the best per-
formance of simple Na??ve Bayes self-training using
300 labeled sentences. This implies that the prob-
205
Figure 2: Performance of four co-training strategies on movie review data
lem space of opinion detection may be successfully
described by the mixture model assumption of EM.
As for blog posts, since the performance of the base-
line classifiers was only slightly better than chance
(50%), we needed to improve the baseline accuracy
in order for SSL to work. One solution was to intro-
duce high quality features. We augmented feature
set with domain independent opinion lexicons that
have been suggested as effective in creating high
precision opinion classifier, but improvement was
only minimal. An alternative solution was to bor-
row more labeled data from non-blog domains(s).
Section 5.3 discusses dealing with a ?difficult? data
domain using data from an ?easy? domain.
The preliminary exploration of different parame-
ter settings for both self- and co-training showed no
significant benefit gained by setting the weight pa-
rameter ? or applying feature selection; and using
a larger number of unlabeled sentences u available
for each iteration did not improve results. Further
investigation is needed for an in-depth explanation.
5.2 Co-training
The best co-training runs reported in Table 1 and
Figure 1 used an 8-grams CLM to train one clas-
sifier and a BOW model to train the other classifier.
These two classifiers differ both in feature represen-
tation (i.e., character vs. word) and in learning al-
gorithm (language model vs. pure statistical model).
To investigate whether the two different classifiers
improve each other?s performance during iterations,
we analyzed the CLM and BOW classifiers individ-
ually. When comparing the BOW classifier during
co-training iterations to the performance of corre-
sponding SL runs based on BOW, the former us-
ing both CLM and BOW classifiers always outper-
formed the latter, indicating that the BOW classi-
fier learned from CLM. Similarly, the CLM classi-
fier also gained from the BOW classifier during co-
training.
Figure 2 shows that for the movie review do-
main, other simple co-training configurations also
produced promising results by using different fea-
ture sets (e.g., unigrams and the union of unigrams
and bigrams, or randomly split feature sets) or differ-
ent training sets. In the news domain, we observed
similar trends. This shows the robustness and great
potential of co-training. Because even with the lo-
gistic model to output probabilistic scores for the
SVM classifier, the difference in probabilities was
too small to select a small number of top predic-
tions, adding an SVM classifier for co-training did
206
not improve accuracy and is not discussed here.
An observation of the performance of self-
training and co-training over iterations confirmed
that co-training used labeled data more effectively
for opinion detection than self-training, as sug-
gested for traditional topical classification. We
found that, overall, co-training produces better per-
formance than self-training and reaches optimal per-
formance faster. For instance, with 500 labeled sen-
tences, a self-training run reached an optimal classi-
fication accuracy of 88.2% after adding 4,828 auto-
matically annotated sentences for training, while the
co-training run reached an optimal performance of
89.4% after adding only 2,588 sentences.
5.3 Domain Transfer
Even without any explicit domain adaptation meth-
ods, results indicate that simple self-training alone
is promising for tackling domain transfer between
the source domain movie reviews and the target do-
mains news articles and blog posts.
Target domain news articles We used 9,500 la-
beled movie review sentences to train a Na??ve Bayes
classifier for news articles. Although this classi-
fier produced a fairly good classification accuracy
of 89.2% on movie review data, its accuracy was
poor (64.1%) on news data (i.e., domain-transfer
SL), demonstrating the severity of the domain trans-
fer problem. Self-training with Na??ve Bayes using
unlabeled data from the news domain (i.e., domain-
transfer SSL run) improved the situation somewhat:
it achieved a classification accuracy of 75.1% sur-
passing the domain-transfer SL run by more than
17%. To finvestigate how well SSL handles the do-
main transfer problem, a full in-domain SL run that
used all labeled news sentences was also performed.
This full SL run achieved 76.9% classification accu-
racy, only 1.8% higher than the domain-transfer SSL
run, which did not use any labeled news data.
Target domain blog posts Because blog data are
more challenging than news data, we kept 5% blog
data as labeled data. Both SSL runs with and without
out-of-domain data are depicted in Figure 3. Self-
training using only blog data decreases SL baseline
performance (dashed black line). Keeping the same
settings, we added additional labeled data from the
movie reviews, and self-training (gray line) came
Figure 3: Self-training for domain transfer between
movie reviews (source domain) and blogs (target domain)
closer to the performance of the full SL run (red
line), which used 90% of the labeled blog data. We
then added a control factor that reduced the impact
of movie review data gradually (i.e., a decrease of
0.001 in each iteration). Using this control, the self-
training run (solid black line) reached and occasion-
ally exceeded the performance of the full SL run.
6 Conclusion and Future Work
We investigated major SSL methods for identify-
ing opinionated sentences in three domains. For
movie review data, SSL methods attained state-of-
the-art results with a small number of labeled sen-
tences. Even without a natural feature split, dif-
ferent co-training strategies increased the baseline
SL performance and outperformed other SSL meth-
ods. Due to the nature of the movie review data, we
suspect that opinion detection on movie reviews is
an ?easy? problem because it relies, strictly speak-
ing, on distinguishing movie reviews from plot sum-
maries, which also involves genre classification. For
other manually created data sets that are expected
to reflect real opinion characteristics, the SSL ap-
proach was impeded by low baseline precision and
showed limited improvement. With the addition of
out-of-domain labeled data, however, self-training
exceeded full SL. This constitutes a successful new
approach to domain adaptation.
Future work will include integrating opinion lex-
icons to bootstrap baseline precision and exploring
co-training for domain adaptation.
207
References
Alias-i. 2008. LingPipe (version 4.0.1). Available from
http://alias-i.com/lingpipe.
Anthony Aue and Michel Gamon. 2005. Customizing
sentiment classifiers to new domains: A case study. In
Proceedings of the International Conference on Recent
Advances in Natural Language Processing (RANLP),
Borovets, Bulgaria.
John Blitzer, Mark Dredze, and Fernando Pereira. 2007.
Biographies, Bollywood, boom-boxes and blenders:
Domain adaptation for sentiment classification. In
Proceedings of the 45th Annual Meeting of the Associ-
ation of Computational Linguistics (ACL), pages 440?
447, Prague, Czech Republic.
Avrim Blum and Tom Mitchell. 1998. Combining la-
beled and unlabeled data with co-training. In Proceed-
ings of the 11th Annual Conference on Computational
Learning Theory, pages 92?100, Madison, WI.
Paula Chesley, Bruce Vincent, Li Xu, and Rohini K. Sri-
hari. 2006. Using verbs and adjectives to automati-
cally classify blog sentiment. In Proceedings of AAAI-
CAAW-06, the Spring Symposia on Computational Ap-
proaches to Analyzing Weblogs, Menlo Park, CA.
Hagen Fu?rstenau and Mirella Lapata. 2009. Semi-
supervised semantic role labeling. In Proceedings of
the 12th Conference of the European Chapter of the
ACL (EACL), pages 220?228, Athens, Greece.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA data mining software: An update.
SIGKDD Explorations, 11(1).
Wei Jin, Hung Hay Ho, and Rohini K. Srihari. 2009.
OpinionMiner: A novel machine learning system for
web opinion mining. In Proceedings of the 15th
ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining, Paris, France.
Thorsten Joachims. 1999. Making large-scale SVM
learning practical. In B. Scho?lkopf, C. Burges, and
A. Smola, editors, Advances in Kernel Methods - Sup-
port Vector Learning. MIT-Press.
Jason S. Kessler, Miriam Eckert, Lyndsie Clark, and
Nicolas Nicolov. 2010. The ICWSM 2010 JDPA sen-
timent corpus for the automotive domain. In 4th Inter-
national AAAI Conference on Weblogs and Social Me-
dia Data Workshop Challenge (ICWSM-DWC), Wash-
ington, D.C.
Kamal Nigam and Rayid Ghani. 2000. Analyzing the
effectiveness and applicability of co-training. In Pro-
ceedings of the Ninth International Conference on In-
formation and Knowledge Management, McLean, VA.
Kamal Nigam, Andrew Kachites Mccallum, Sebastian
Thrun, and Tom Mitchell. 1999. Text classification
from labeled and unlabeled documents using EM. Ma-
chine Learning, 39:103?134.
Bo Pang and Lillian Lee. 2004. A sentimental edu-
cation: Sentiment analysis using subjectivity summa-
rization based on minimum cuts. In Proceedings of
the 42nd Annual Meeting on Association for Compu-
tational Linguistics, Barcelona, Spain.
Ellen Riloff and Janyce Wiebe. 2003. Learning extrac-
tion patterns for subjective expressions. In Proceed-
ings of the Conference on Empirical Methods in Natu-
ral Language Processing (EMNLP), Sapporo, Japan.
Hiroya Takamura, Takashi Inui, and Manabu Okumura.
2006. Latent variable models for semantic orientations
of phrases. In Proceedings of the 11th Conference of
the European Chapter of the Association for Compu-
tational Linguistics (EACL), Trento, Italy.
Partha Pratim Talukdar and Fernando Pereira. 2010.
Experiments in graph-based semi-supervised learning
methods for class-instance acquisition. In Proceed-
ings of the 48th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 1473?1481,
Uppsala, Sweden.
Songbo Tan, Xueqi Cheng, Yufen Wang, and Hongbo Xu.
2009. Adapting naive Bayes to domain adaptation for
sentiment analysis. In Proceedings of the 31st Eu-
ropean Conference on Information Retrieval (ECIR),
Toulouse, France.
Wei Wang and Zhi-Hua Zhou. 2007. Analyzing co-
training style algorithms. In Proceedings of the 18th
European Conference on Machine Learning, Warsaw,
Poland.
Janyce Wiebe and Ellen Riloff. 2005. Creating sub-
jective and objective sentence classifiers from unan-
notated texts. In Proceedings of the 6th International
Conference on Intelligent Text Processing and Compu-
tational Linguistics (CICLing), Mexico City, Mexico.
Janyce Wiebe, Rebecca Bruce, and Thomas O?Hara.
1999. Development and use of a gold standard data
set for subjectivity classifications. In Proceedings of
the 37th Annual Meeting of the Association for Com-
putational Linguistics (ACL), College Park, MD.
Janyce Wiebe, Rebecca Bruce, Matthew Bell, Melanie
Martin, and Theresa Wilson. 2001. A corpus study of
evaluative and speculative language. In Proceedings
of the 2nd ACL SIGdial Workshop on Discourse and
Dialogue, Aalborg, Denmark.
Janyce Wiebe, Theresa Wilson, Rebecca Bruce, Matthew
Bell, and Melanie Martin. 2004. Learning subjective
language. Computational Linguistics, 30(3):277?308.
Kiduk Yang, Ning Yu, and Hui Zhang. 2007. WIDIT
in TREC-2007 blog track: Combining lexicon-based
methods to detect opinionated blogs. In Proceed-
ings of the 16th Text Retrieval Conference (TREC),
Gaithersburg, MD.
208
Hong Yu and Vasileios Hatzivassiloglou. 2003. Towards
answering opinion questions: Separating facts from
opinions and identifying the polarity of opinion sen-
tences. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing (EMNLP),
Sapporo, Japan.
Ning Yu and Sandra Ku?bler. 2010. Semi-supervised
learning for opinion detection. In Proceedings of the
IEEE/WIC/ACM International Conference on Web In-
telligence and Intelligent Agent Technology, volume 3,
pages 249?252, Toronto, Canada.
Wei Zhang and Clement Yu. 2007. UIC at TREC 2007
blog track. In Proceedings of the 16th Text Retrieval
Conference (TREC), Gaithersburg, MD.
209
Proceedings of the 15th Conference on Computational Natural Language Learning: Shared Task, pages 112?116,
Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational Linguistics
UBIU: A Robust System for Resolving Unrestricted Coreference
Desislava Zhekova
University of Bremen
zhekova@uni-bremen.de
Sandra Ku?bler
Indiana University
skuebler@indiana.edu
Abstract
In this paper, we discuss the application
of UBIU to the CONLL-2011 shared task
on ?Modeling Unrestricted Coreference? in
OntoNotes. The shared task concentrates on
the detection of coreference not only in noun
phrases but also involving verbs. The infor-
mation provided for the closed track included
WordNet as well as corpus generated number
and gender information. Our system shows
no improvement when using WordNet infor-
mation, and the number information proved
less reliable than the information in the part
of speech tags.
1 Introduction
Coreference Resolution is the process of identify-
ing the linguistic expressions in a discourse that re-
fer to the same real world entity and to divide those
expressions into equivalence classes that represent
each discourse entity. For this task, a deeper knowl-
edge of the discourse is often required. However,
such knowledge is difficult to acquire. For this rea-
son, many systems use superficial information such
as string match. The CoNLL shared task on ?Model-
ing Unrestricted Coreference in OntoNotes? (Prad-
han et al, 2011) presents challenges that go be-
yond previous definitions of the task. On the one
hand, mention extraction is part of the task while
many previous approaches assumed gold standard
mentions. On the other hand, coreference is not
restricted to noun phrases, verbs are also included.
Thus, in Sales of passenger cars grew 22%. The
strong growth followed year-to-year increases., the
verb grew has an identity relation with the noun
phrase The strong growth.
The system that we used for the shared task is
the memory-based machine learning system UBIU
(Zhekova and Ku?bler, 2010). We describe the most
important components of the system in section 2.
The system was originally developed for robust,
multilingual coreference resolution, and thus had to
be adapted to this shared task. We investigate the
quality of our mention extraction in section 2.1 and
the quality of the features used in the classifier in
section 2.2. In section 3, we present UBIU?s results
on the development set, and in section 4, UBIU?s
final results in the shared task.
2 UBIU
UBIU (Zhekova and Ku?bler, 2010) was developed
as a multilingual coreference resolution system. A
robust approach is necessary to make the system ap-
plicable for a variety of languages. For this rea-
son, we use a machine learning approach to clas-
sify mention pairs. We use TiMBL (Daelemans et
al., 2007), a memory-based learner (MBL) that la-
bels the feature vectors from the test set based on
the k nearest neighbors in the training instances.
We chose TiMBL since MBL has been shown to
work well with small training sets. A non-exhaustive
parameter optimization on the development set led
us to use the IB1 algorithm, similarity is computed
based on weighted overlap, the relevance weights
are computed using gain ratio and the number of
nearest neighbors is set to k = 3 (for a description
of the algorithm and parameters cf. (Daelemans et
al., 2007)). The classifier is preceded by a mention
extractor, which identifies possible mentions, and a
feature extractor. The latter creates a feature vec-
tor for each possible pair of a potentially coreferring
112
mention and all possible antecedents in a context of
3 sentences. Another important step is to separate
singleton mentions from coreferent ones since only
the latter are annotated in OntoNotes. Our markable
extractor overgenerates in that it extracts all possi-
ble mentions, and only after classification, the sys-
tem can decide which mentions are singletons. We
investigate the performance of the mention and fea-
ture extraction modules in more detail below.
2.1 Mention Extraction
UBIU?s mention extractor uses part-of-speech
(POS), syntactic, and lemma information provided
in the OntoNotes data set to detect mentions. The
module defines a mention for each noun phrase,
based on syntactic information, as well as for all
possessive pronouns and all proper nouns, based on
their POS tags. Since for the shared task, verbs are
also potentially coreferent, we included a mention
for each of the verbs with a predicate lemma. An ex-
ample of the output of the mention extraction mod-
ule is shown in table 1. Each mention is numbered
with an individual number and thus still represents a
distinct entity. Since singleton mentions are not an-
notated in the OntoNotes data set, mentions without
coreference relations after classification need to be
removed from the answer set, which can only be per-
formed after coreference resolution when all coref-
erent pairs are identified. For this reason, the mark-
able extractor is bound to overgenerate. The latter
can clearly be seen when the mention extraction out-
put is compared to the provided gold mentions (cf.
the last column in table 1).
We conducted a simple experiment on the devel-
opment data in order to gain insight into the per-
formance of the mention extraction module. Using
the scorer provided by the shared task, we evaluated
the output of the module, without performing coref-
erence resolution and without removing singleton
mentions. This led to a recall of 96.55 % and a preci-
sion of 18.55%, resulting in an F-score of 31.12. The
high recall shows that the system is very reliable in
finding mentions with the correct boundaries. How-
ever, since we do not remove any singletons, UBIU
overgenerates and thus the system identified a con-
siderable number of singletons, too. Nevertheless,
the fact that UBIU identified 96.55% of all mentions
shows that the performance of the mention extrac-
# Word POS Parse bit ME output Gold
0 Devastating VBG (TOP(NP(NP* (1)|(2|(3 -
1 Critique NN *) 3) -
2 of IN (PP* - -
3 the DT (NP* (4 (32
4 Arab JJ * - -
5 World NN *)) 4) 32)
6 by IN (PP* - -
7 One CD (NP(NP*) (5|(6) -
8 of IN (PP* - -
9 Its PRP$ (NP* (7)|(8 (32)
10 Own JJ *)))))) 8)|5)|2) -
Table 1: The output of the mention extractor for a sample
sentence.
tion module is close to optimal.
2.2 Feature Extraction
Feature extraction is the second important subtask
for the UBIU pipeline. Since mentions are repre-
sented by their syntactic head, the feature extractor
uses a heuristic that selects the rightmost noun in a
noun phrase. However, since postmodifying prepo-
sitional phrases may be present in the mention, the
noun may not be followed by a preposition. For each
mention, a feature vector is created for all of its pre-
ceding mentions in a window of 3 sentences. Af-
ter classification, a filter can optionally be applied
to filter out mention pairs that disagree in number,
and another filter deletes all mentions that were not
assigned an antecedent in classification. Note that
the number information was derived from the POS
tags and not from the number/gender data provided
by the shared task since the POS information proved
more reliable in our system.
Initially, UBIU was developed to use a wide set of
features (Zhekova and Ku?bler, 2010), which consti-
tutes a subset of the features described by Rahman
and Ng (2009). For the CONLL-2011 shared task,
we investigated the importance of various additional
features that can be included in the feature set used
by the memory-based classifier. Thus, we conducted
experiments with a base set and an extended feature
set, which makes use of lexical semantic features.
Base Feature Set Since the original feature set in
Zhekova and Ku?bler (2010) contained information
that is not easily accessible in the OntoNotes data
set (such as grammatical functions), we had to re-
strict the feature set to information that can be de-
rived solely from POS annotations. Further infor-
113
# Feature Description
1 mj - the antecedent
2 mk - the mention to be resolved
3 Y ifmj is a pronoun; else N
4 number - S(ingular) or P(lural)
5 Y ifmk is a pronoun; else N
6 C if the mentions are the same string; else I
7 C if one mention is a substring of the other; else I
8 C if both mentions are pronominal and are the same
string; else I
9 C if the two mentions are both non-pronominal and
are the same string; else I
10 C if both mentions are pronominal and are either the
same pronoun or different only w.r.t. case;
NA if at least one of them is not pronominal; else I
11 C if the mentions agree in number; I if they disagree;
NA if the number for one or
both mentions cannot be determined
12 C if both mentions are pronouns; I if neither are
pronouns; else NA
13 C if both mentions are proper nouns; I if neither are
proper nouns; else NA
14 sentence distance between the mentions
Table 2: The pool of features used in the base feature set.
mation as sentence distance, word overlap etc. was
included as well. The list of used features is shown
in table 2.
Extended Feature Set Since WordNet informa-
tion was provided for the closed setting of the
CONLL-2011 shared task, we also used an ex-
tended feature set, including all features from the
base set alng with additional features derived from
WordNet. The latter features are shown in table 3.
2.3 Singletons
In section 2.1, we explained that singletons need to
be removed after classification. However, this leads
to a drastic decrease in system performance for two
reasons. First, if a system does not identify a coref-
erence link, the singleton mentions will be removed
from the coreference chains, and consequently, the
system is penalized for the missing link as well as
for the missing mentions. If singletons are included,
the system will still receive partial credit for them
from all metrics but MUC. For this reason, we in-
vestigated filtered and non-filtered results in combi-
nation with the base and the extended feature sets.
3 Results on the Development Set
The results of our experiment on the development
set are shown in table 4. Since the official scores
of the shared task are based on an average of MUC,
# Feature Description
15 C if both are nouns andmk is hyponym ofmj ; I if both
are nouns butmk is not a hyponym ofmj ; NA otherwise
16 C if both are nouns andmj is hyponym ofmk; I if both
are nouns butmj is not a hyponym ofmk; NA otherwise
17 C if both are nouns andmk is a partial holonym ofmj ;
I if both are nouns butmk is not a partial holonym ofmj ;
NA otherwise
18 C if both are nouns andmj is a partial holonym ofmk;
I if both are nouns butmj is not a partial holonym ofmk;
NA otherwise
19 C if both are nouns andmk is a partial meronym ofmj ;
I if both are nouns butmk is not a partial meronym ofmj ;
NA otherwise
20 C if both are nouns andmj is a partial meronym ofmk;
I if both are nouns butmj is not a partial meronym ofmk;
NA otherwise
21 C if both are verbs andmk entailsmj ; I if both are
verbs butmk does not entailmj ; NA otherwise
22 C if both are verbs andmj entailsmk; I if both are
verbs butmj does not entailmk; NA otherwise
23 C if both are verbs andmk is a hypernym ofmj ;
I if both are verbs butmk is not a hypernym ofmj ;
NA otherwise
24 C if both are verbs andmj is a hypernym ofmk;
I if both are verbs butmj is not a hypernym ofmk;
NA otherwise
25 C if both are verbs andmk is a troponym ofmj ;
I if both are verbs butmk is not a troponym ofmj ;
NA otherwise
26 C if both are verbs andmj is a troponym ofmk;
I if both are verbs butmj is not a troponym ofmk;
NA otherwise
Table 3: The features extracted from WordNet.
B3, and CEAFE, we report these measures and their
average. All the results in this section are based on
automatically annotated linguistic information. The
first part of the table shows the results for the base
feature set (UBIUB), the second part for the ex-
tended feature set (UBIUE). We also report results
if we keep all singletons (& Sing.) and if we filter
out coreferent pairs that do not agree in number (&
Filt.). The results show that keeping the singletons
results in lower accuracies on the mention and the
coreference level. Only recall on the mention level
profits from the presence of singletons. Filtering for
number agreement with the base set has a detrimen-
tal effect on mention recall but increases mention
precision so that there is an increase in F-score of
1%. However, on the coreference level, the effect is
negligible. For the extended feature set, filtering re-
sults in a decrease of approximately 2.0% in mention
precision, which also translates into lower corefer-
ence scores. We also conducted an experiment in
which we filter before classification (& Filt. BC),
following a more standard approach. The reasoning
114
IM MUC B3 CEAFE Average
R P F1 R P F1 R P F1 R P F1 F1
UBIUB 62.71 38.66 47.83 30.59 24.65 27.30 67.06 62.65 64.78 34.19 40.16 36.94 43.01
UBIUB & Sing. 95.11 18.27 30.66 30.59 24.58 27.26 67.10 62.56 64.75 34.14 40.18 36.92 42.97
UBIUB & Filt. 61.30 40.58 48.83 29.10 25.77 27.33 64.88 64.63 64.76 35.38 38.74 36.98 43.02
UBIUB & Filt. BC 61.33 40.49 48.77 28.96 25.54 27.14 64.95 64.48 64.71 35.23 38.71 36.89 42.91
UBIUE 62.72 39.09 48.16 30.63 24.94 27.49 66.72 62.76 64.68 34.19 39.90 36.82 43.00
UBIUE & Sing. 95.11 18.27 30.66 29.87 20.96 24.64 69.13 57.71 62.91 32.28 42.24 36.59 41.38
UBIUE & Filt. 63.01 36.62 46.32 28.65 21.05 24.27 68.10 58.72 63.06 32.91 41.53 36.72 41.35
Gold ME 100 100 100 38.83 82.97 52.90 39.99 92.33 55.81 66.73 26.75 38.19 48.97
Table 4: UBIU system results on the development set.
is that the training set for the classifier is biased to-
wards not assuming coreference since the majority
of mention pairs does not have a coreference rela-
tion. Thus filtering out non-agreeing mention pairs
before classification reduces not only the number of
test mention pairs to be classified but also the num-
ber of training pairs. However, in our system, this
approach leads to minimally lower results, which is
why we decided not to pursue this route. We also
experimented with instance sampling in order to re-
duce the bias towards non-coreference in the training
set. This also did not improve results.
Contrary to our expectation, using ontological in-
formation does not improve results. Only on the
mention level, we see a minimal gain in precision.
But this does not translate into any improvement on
the coreference level. Using filtering in combination
with the extended feature set results in a more pro-
nounced deterioration than with the base set.
The last row of table 4 (Gold ME) shows re-
sults when the system has access to the gold stan-
dard mentions. The MUC and B3 results show that
the classifier reaches an extremely high precision
(82.97% and 92.33%), from which we conclude that
the coreference links that our system finds are re-
liable, but it is also too conservative in assuming
coreference relations. For the future, we need to
investigate undersampling the negative examples in
the training set and more efficient methods for filter-
ing out singletons.
4 Final Results
In the following, we present the UBIU system re-
sults in two separate settings: using the test set with
automatically extracted mentions (section 4.1) and
using a test set with gold standard mentions, includ-
ing singletons (section 4.2). An overview of all sys-
tems participating in the CONLL-2011 shared task
and their results is provided by Pradhan et al (2011).
4.1 Automatic Mention Identification
The final results of UBIU for the test set without
gold standard mentions are shown in the first part
of table 5. They are separated into results for the
coreference resolution module based on automati-
cally annotated linguistic information and the gold
annotations. Again, we report results for both the
base feature set (UBIUB) and the extended feature
set usingWordNet features (UBIUE). A comparison
of the system results on the test and the development
set in the UBIUB setting shows that the average F-
score is considerably lower for the test set, 40.46 vs.
43.01 although the quality of the mentions remains
constant with an F-score of 48.14 on the test set and
47.83 on the development set.
The results based on the two data sets show that
UBIU?s performance improves when the system has
access to gold standard linguistic annotations. How-
ever, the difference between the results is in the area
of 2%. The improvement is due to gains of 3-5%
in precision for MUC and B3, which are counter-
acted by smaller losses in recall. In contrast, CEAFE
shows a loss in precision and a similar gain in recall,
resulting in a minimal increase in F-score.
A comparison of the results for the experiments
with the base set as opposed to the extended set in
5 shows that the extended feature set using Word-
Net information is detrimental to the final results av-
eraged over all metrics while it led to a slight im-
provement on the mention level. Our assumption
is that while in general, the ontological information
is useful, the additional information may be a mix-
ture of relevant and irrelevant information. Mihalcea
(2002) showed for word sense disambiguation that
115
IM MUC B3 CEAFE Average
R P F1 R P F1 R P F1 R P F1 F1
Automatic Mention Identification
auto
UBIUB 67.27 37.48 48.14 28.75 20.61 24.01 67.17 56.81 61.55 31.67 41.22 35.82 40.46
UBIUE 67.49 37.60 48.29 28.87 20.66 24.08 67.14 56.67 61.46 31.57 41.21 35.75 40.43
gold
UBIUB 65.92 40.56 50.22 31.05 25.57 28.04 64.94 62.23 63.56 33.53 39.08 36.09 42.56
UBIUE 66.11 40.37 50.13 30.84 25.14 27.70 65.07 61.83 63.41 33.23 39.05 35.91 42.34
Gold Mention Boundaries
auto
UBIUB 67.57 58.66 62.80 34.14 40.43 37.02 54.24 71.09 61.53 39.65 33.73 36.45 45.00
UBIUE 69.19 57.27 62.67 33.48 37.15 35.22 55.47 68.23 61.20 38.29 34.65 36.38 44.27
gold
UBIUB 67.64 58.75 62.88 34.37 40.68 37.26 54.28 71.18 61.59 39.69 33.76 36.49 45.11
UBIUE 67.72 58.66 62.87 34.18 40.40 37.03 54.30 71.04 61.55 39.64 33.78 36.47 45.02
Table 5: Final system results for the coreference resolution module on automatically extracted mentions on the gold
standard mentions for the base and extended feature sets.
memory-based learning is extremely sensitive to ir-
relevant features. For the future, we are planning
to investigate this problem by applying forward-
backward feature selection, as proposed by Mihal-
cea (2002) and Dinu and Ku?bler (2007).
4.2 Gold Mention Boundaries
UBIU was also evaluated in the experimental set-
ting in which gold mention boundaries were pro-
vided in the test set, including for singletons. The
results of the setting using both feature sets are re-
ported in the second part of table 5. The results show
that overall the use of gold standard mentions re-
sults in an increase of the average F-score of approx.
4.5%. Where mention quality and MUC are con-
cerned, gold standard mentions have a significant
positive influence on the average F-score. For B3
and CEAFE, however, there is no significant change
in scores. The increase in performance is most no-
ticeable in mention identification, for which the F-
score increases from 48.14 to 62.80. But this im-
provement has a smaller effect on the overall coref-
erence system performance leading to a 5% increase
of results. In contrast to the gold mention results in
the development set, we see lower precision values
in the test set. This is due to the fact that the test set
contains singletons. Detecting singletons reliably is
a difficult problem that needs further investigation.
5 Conclusion and Future Work
In the current paper, we presented the results of
UBIU in the CONLL-2011 shared task. We showed
that for a robust system for coreference resolution
such as UBIU, automatically annotated linguistic
data is sufficient for mention-pair based coreference
resolution. We also showed that ontological infor-
mation as well as filtering non-agreeing mention
pairs leads to an insignificant improvement of the
overall coreference system performance. The treat-
ment of singletons in the data remains a topic that
requires further investigation.
References
Walter Daelemans, Jakub Zavrel, Ko van der Sloot, and
Antal van den Bosch. 2007. TiMBL: Tilburg memory
based learner ? version 6.1 ? reference guide. Techni-
cal Report ILK 07-07, Induction of Linguistic Knowl-
edge, Computational Linguistics, Tilburg University.
Georgiana Dinu and Sandra Ku?bler. 2007. Sometimes
less is more: Romanian word sense disambiguation
revisited. In Proceedings of the International Confer-
ence on Recent Advances in Natural Language Pro-
cessing, RANLP 2007, Borovets, Bulgaria.
Rada Mihalcea. 2002. Instance based learning with
automatic feature selection applied to word sense
disambiguation. In Proceedings of the 19th Inter-
national Conference on Computational Linguistics,
COLING?02, Taipeh, Taiwan.
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,
Martha Palmer, Ralph Weischedel, and Nianwen Xue.
2011. CoNLL-2011 Shared Task: Modeling unre-
stricted coreference in OntoNotes. In Proceedings of
the Fifteenth Conference on Computational Natural
Language Learning (CoNLL), Portland, Oregon.
Altaf Rahman and Vincent Ng. 2009. Supervised models
for coreference resolution. In Proceedings of EMNLP
2009, Singapore.
Desislava Zhekova and Sandra Ku?bler. 2010. UBIU: A
language-independent system for coreference resolu-
tion. In Proceedings of the 5th International Workshop
on Semantic Evaluation (SemEval), pages 96?99, Up-
psala, Sweden.
116
The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 95?104,
Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational Linguistics
Predicting Learner Levels for Online Exercises of Hebrew
Markus Dickinson, Sandra Ku?bler, Anthony Meyer
Indiana University
Bloomington, IN, USA
{md7,skuebler,antmeyer}@indiana.edu
Abstract
We develop a system for predicting the level of
language learners, using only a small amount
of targeted language data. In particular, we
focus on learners of Hebrew and predict level
based on restricted placement exam exercises.
As with many language teaching situations, a
major problem is data sparsity, which we ac-
count for in our feature selection, learning al-
gorithm, and in the setup. Specifically, we de-
fine a two-phase classification process, isolat-
ing individual errors and linguistic construc-
tions which are then aggregated into a second
phase; such a two-step process allows for easy
integration of other exercises and features in
the future. The aggregation of information
also allows us to smooth over sparse features.
1 Introduction and Motivation
Several strands of research in intelligent computer-
assisted language learning (ICALL) focus on deter-
mining learner ability (Attali and Burstein, 2006;
Yannakoudakis et al, 2011). One of the tasks, de-
tecting errors in a range of languages and for a range
of types of errors, is becoming an increasingly popu-
lar topic (Rozovskaya and Roth, 2011; Tetreault and
Chodorow, 2008); see, for example, the recent HOO
(Helping Our Own) Challenge for Automated Writ-
ing Assistance (Dale and Kilgarriff, 2011). Only
rarely has there been work on detecting errors in
more morphologically-complex languages (Dickin-
son et al, 2011).
In our work, we extend the task to predicting the
learner?s level based on the errors, focusing on He-
brew. Our system is targeted to be used in a uni-
versity setting where incoming students need to be
placed into the appropriate language level?i.e., the
appropriate course?based on their proficiency in
the language. Such a level prediction system for He-
brew faces a number of challenges: 1) unclear cor-
respondence between errors and levels, 2) missing
NLP resources, and, most critically, 3) data sparsity.
Placing learners into levels is generally done by
a human, based on a written exam (e.g. (Fulcher,
1997)). To model the decision process automati-
cally, we need to understand how the types of er-
rors, as well as their frequencies, correspond to
learner levels. There is only little work investigat-
ing this correspondence formally (see (Hawkins and
Filipovic?, 2010; Alexopoulou et al, 2010) for dis-
cussion) and only on error-annotated English learner
corpora. For this reason, we follow a data-driven
approach to learn the correspondence between er-
rors and levels, based on exercises from written
placement exams. Although the exact exercises will
vary across languages and language programs, the
methodology is widely applicable, as developing
a small set of exercises requires minimal effort?
effort already largely expended for paper exams.
Currently, we focus on an exercise in which the
learner has to order a set of words into a grammat-
ical sentence. Our goal is to move towards freer
language production and to analyze language pro-
ficiency through more variables, but, in the interest
of practicality, we start in a more restricted way.
For lesser-resourced languages, there is generally
little data and few NLP resources available. For He-
brew, for example, we must create our own pool of
95
learner data, and while NLP tools and resources ex-
ist (Goldberg and Elhadad, 2011; Yona and Wintner,
2008; Itai and Wintner, 2008), they are not adapted
for dealing with potentially ill-formed learner pro-
ductions. For this reason, we are performing linguis-
tic analysis on the gold standard answers to obtain
optimal linguistic analyses. Then, the system aligns
the learner answer to the gold standard answer and
determines the types of deviations.
Since Hebrew is a less commonly taught language
(LCTL), we have few placement exams from which
to learn correspondences. Compounding the data
sparsity problem is that each piece of data is com-
plex: if a learner produces an erroneous answer,
there are potentially a number of ways to analyze it
(cf. e.g. (Dickinson, 2011)). An error could feature,
for instance, a letter inserted in an irregular verb
stem, or between two nouns; any of these proper-
ties may be relevant to describing the error (cf. how
errors are described in different taxonomies, e.g.
(D??az-Negrillo and Ferna?ndez-Dom??nguez, 2006;
Boyd, 2010)). Specific error types are unlikely to
recur, making sparsity even more of a concern. We
thus need to develop methods which generalize well,
finding the most useful aspects of the data.
Our system is an online system to be used at the
Hebrew Language Program at our university. The
system is intended to semi-automatically place in-
coming students into the appropriate Hebrew course,
i.e., level. As with many exams, the main purpose is
to ?reduce the number of students who attend an oral
interview? (Fulcher, 1997).
2 The Data
Exercise type We focus on a scrambled sentence
exercise, in which learners are given a set of well-
formed words and must put them into the correct or-
der. For example, given (1), they must produce one
of the correct choices in (2). This gives them the
opportunity to practice skills in syntactic ordering.1
(1) barC beph dibrw hybrit ieral la tmid
(2) a. la
not
tmid
always
dibrw
spoke
beph
in-the-language
hybrit
the-Hebrew
barC
in-land-of
ieral
Israel
.
.
1We follow the transliteration scheme of the Hebrew Tree-
bank (Sima?an et al, 2001).
?They did not always speak in the He-
brew language in the land of Israel.?
b. barC ieral la dibrw tmid beph hybrit .
c. la tmid dibrw barC ieral beph hybrit .
(3) barC ieral la tmid dibrw beph hybriM .
Although the lexical choice is restricted?in that
learners are to select from a set of words?learners
must write the words. Thus, in addition to syntactic
errors, there is possible variation in word form, as in
(3), where hybrit is misspelled as hybriM.
This exercise was chosen because: a) it has been
used on Hebrew placement exams for many years;
and b) the amount of expected answers is con-
strained. Starting here also allows us to focus less
on the NLP preprocessing and more on designing
a machine learning set-up to analyze proficiency.
It is important to note that the proficiency level is
determined by experts looking at the whole exam,
whereas we are currently predicting the proficiency
level on the basis of a single exercise.
Placement exams The data for training and test-
ing is pooled from previous placement exams at our
university. Students who intend to take Hebrew have
in past years been given written placement exams,
covering a range of question types, including scram-
bled sentences. The learners are grouped into the
first to the sixth semester, or they test out. We are
using the following levels: the first four semesters
(100, 150, 200, 250), and anything above (300+).
We use a small set of data?38 learners covering
128 sentences across 11 exercises?all the data that
is available. While this is very small, it is indicative
of the type of situation we expect for resource-poor
languages, and it underscores the need to develop
methods appropriate for data-scarce situations.
(Manual) annotation For each of the 11 unique
exercises, we annotate an ordered list of correct an-
swers, ranked from best to worst. Since Hebrew pos-
sesses free word order, there are between 1 and 10
correct answers per exercise, with an average of 3.4
gold standard answers. The sentences have between
8 and 15 words, with an average of 9.7 words per ex-
ercise. This annotation concerns only the gold stan-
dard answers. It requires minimal effort and needs
to be performed only once per exercise.
96
T09: SURFACE qnw
SEGMENTATION (VB-BR3V qnw)
PRE_PARTICLES -
MAIN_WORD:
INDICES 0,1,2,
TAG VB-BR3V
BINYAN PAAL
INFL_PREFIX -
STEM 0,1,
ROOT 0,1,h,
INFL_SUFFIX 2,
PRO_SUFFIX -
Figure 1: An example annotated word for qnw (?bought?),
token T09 in one particular exercise
To annotate, we note that all the correct answers
share the same set of words, varying in word or-
der and not in morphological properties. Thus,
we store word orders separately from morphologi-
cal annotation, annotating morphology once for all
possible word orders. An example of morpholog-
ical annotation is given in fig. 1 for the verb qnw
(?bought?). Segmentation information is provided
by referring to indices (e.g., STEM 0,1), while TAG
and BINYAN provide morphosyntactic properties.
Since the annotation is on controlled, correct data,
i.e., not potentially malformed learner data, we can
explore automatically annotating exercises in the fu-
ture, as we expect relatively high accuracy.
3 System overview
The overall system architecture is given in fig. 2; the
individual modules are described below. In brief,
we align a learner sentence with the gold standard;
use three specialized classifiers to classify individ-
ual phenomena; and then combine the information
from these classifiers into an overall classifier for the
learner level. This means the classification is per-
formed in two phases: the first phase looks at indi-
vidual phenomena (i.e., errors and other properties);
the second phase aggregates all phenomena of one
learner over all exercises and makes a final decision.
4 Feature extraction
To categorize learners into levels, we first need to ex-
tract relevant information from each sentence. That
is, we need to perform a linguistic and/or error anal-
ysis on each sentence, which can be used for classi-
Learner
sentence
(L)
Alignment
Gold
standard
answers
(G1 . . . G2)
Feature
extraction
Intertoken
errors
Intratoken
errors
Global
features
Intra-
Classifier
Inter-
Classifier
Global-
Classifier
Classified
intra
vectors
Classified
inter
vectors
Classified
global
vectors
Learner
classifier
L? Gi
Figure 2: Overall system architecture (boxes = system
components, circles = data)
fication (sec. 5). Although we extract features for
classification, this analysis could also be used for
other purposes, such as providing feedback.
4.1 Phenomena of interest
We extract features capturing individual phenom-
ena. These can be at the level of individual words,
bigrams of words, or anything up to a whole sen-
tence; and they may represent errors or correctly-
produced language. Importantly, at this stage, each
phenomenon is treated uniquely and is not combined
or aggregated until the second phase (see sec. 5).
While features can be based on individual phe-
nomena of any type, we base our extracted features
largely upon learner errors. Errors have been shown
to have a significant impact on predicting learner
level (Yannakoudakis et al, 2011). To detect errors,
we align the learner sentence with a gold standard
and extract the features. Although we focus on er-
rors, we model some correct language (sec. 4.3.3).
4.2 Token alignment
With a listing of correct answers, we align the
learner sentence to the answer which matches best:
97
We iterate over the correct answers and align learner
tokens with correct tokens, based on the cost of map-
ping one to the other. The aligned sentence is the
one with the lowest overall cost. The cost between a
source token ws and target token wt accounts for:
1. Levenshtein distance between ws & wt (Lev)
2. similiarity between ws & wt (longest common
subsequence (LCSq) & substring (LCSt))
3. displacement between ws & wt (Displ)
This method is reminiscent of alignment ap-
proaches in paraphrasing (e.g. (Grigonyte` et al,
2010)), but note that our problem is more restricted
in that we have the same number of words, and in
most cases identical words. We use different dis-
tance and similarity metrics, to ensure robustness
across different kinds of errors. The third metric is
the least important, as learners can shift tokens far
from their original slot, and thus it is given a low
weight. The only reason to use it at all is to distin-
guish cases where more than one target word is a
strong possibility, favoring the closer one.
The formula for the cost between source and tar-
get words ws and wt is given in (4), where the dis-
tance metrics are averaged and normalized by the
length of the target word wt. This length is also used
to convert the similarity measures into distances, as
in (5). We non-exhaustively tested different weight
distributions on about half the data, and our final set
is given in (6), where slightly less weight is given
for the longest common substring and only a minor
amount for the displacement score.
(4) cost(ws, wt) = ?1Displ(ws, wt) +
?2Lev(ws,wt)+?3dLCSq(ws,wt)+?4dLCSt(ws,wt)
3?len(wt)
(5) dLCS(ws, wt) = len(wt)? LCS(ws, wt)
(6) ?1 = 0.05; ?2 = 1.0; ?3 = 1.0; ?4 = 0.7
In calculating Levenshtein distance, we hand-
created a small table of weights for insertions, dele-
tions, and substitutions, to reflect likely modifica-
tions in Hebrew. All weights can be tweaked in the
future, but we have observed good results thus far.
The total alignment is the one which minimizes
the total cost (7). A is an alignment between the
learner sentence s and a given correct sentence t.
Alignments to NULL have a cost of 0.6, so that
words with high costs can instead align to nothing.
(7) align = argminA
?
(ws,wt)?A cost(ws, wt)
4.3 Extracted features
We extract three different types of features; as these
have different feature sets, we correspondingly have
three different classifiers, as detailed in sec. 5.1.
They are followed by a fourth classifier that tallies
up the results of these three classifiers.
4.3.1 Intra-token features
Based on the token alignments, it is straightfor-
ward to calculate differences within the tokens and
thus to determine values for many features (e.g., a
deleted letter in a prefix). We calculate such intra-
token feature vectors for each word-internal error.
For instance, consider the learner attempt (8b) for
the target in (8a). We find in the learner answer two
intra-token errors: one in hmtnwt (cf. hmtnh), where
the fem.pl. suffix -wt has been substituted for the
fem.sg. ending -h, and another in hnw (cf. qnw),
where h has been substituted for q. These two errors
yield the feature vectors presented as example cases
in table 1.
(8) a. haM
Q
hN
they.FEM
eilmw
paid
hrbh
much
ksP
money
bebil
for
hmtnh1
the-gift
ehN
which-they.FEM
qnw2
bought
?
?
?Did they pay much money for the gift
that they bought??
b. haM hN eilmw hrbh ksP bebil hmtnwt1
ehN hnw2 ?
Features 1 and 11 in table 1 are the POS tags of the
morphemes preceding and following the erroneous
morpheme, respectively. The POS tag of the mor-
pheme containing the error is given by feature 2, and
its person, gender, and number by feature 3. The re-
maining features describe the error itself (f. 6?8), as
well as its word-internal context, i.e., both its left (f.
4?5) and right (f. 9?10) contexts.
The context features refer to individual character
slots, which may or may not be occupied by actual
characters. For example, since the error in hmtnwt
is word-final, its two right-context slots are empty,
hence the ?#? symbol for both features 9 and 10.
The feature values for these character slots are
generally not literal characters, but rather abstract la-
bels representing various categories, most of which
98
Features hnw hmtnwt
1. Preceding POS PRP H
2. Current POS VB NN
3. Per.Gen.Num. 3cp -fs
4. Left Context (2) # R2
5. Left Context (1) # R3
6. Source String h wt
7. Target String q h
8. Anomaly h?q wt?h
9. Right Context (1) R2 #
10. Right Context (2) INFL-SFX #
11. Following POS yyQM REL
Table 1: Intra-token feature categories
are morphological in nature. In hmtnwt, for exam-
ple, the two left-context characters t and n are the
second and third radicals of the root, hence the fea-
ture values R2 and R3, respectively.
4.3.2 Inter-token features
The inter-token features encode anomalies whose
scope is not confined to a particular token. Such
anomalies include token displacements and missing
tokens. We again use the Levenshtein algorithm to
detect inter-token anomalies, but we disable the sub-
stitution operation here so that we can link up corre-
sponding deletions and insertions to yield ?shifts.?
For example, suppose the target is A B C D, and
the learner has D A B C. Without substitutions, the
minimal cost edit sequence is to delete D from the
beginning of the learner?s input and insert D at the
end. Merging the two operations results in a D shift.
The learner sentence in (9b) shows two inter-
token anomalies with respect to the target in (9a).
First, the learner has transposed the two tokens in
sequence 1, namely the verb dibrw (?speak-PAST?)
and the adverb tmid (?always?). Second, sequence 2
(the PP beph hybrit, ?in the Hebrew language?) has
been shifted from its position in the target sentence.
(9) a. barC
in-land-of
ieral
Israel
la
not
dibrw1
speak-PAST
tmid1
always
beph2
in-the-language
hybrit2
the-Hebrew
.
.
b. barC ieral beph2 hybrit2 la tmid1
dibrw1 .
Table 2 presents the inter-token feature vectors
for the two anomalies in (9b). After Anomaly,
Features Seq. 1 Seq. 2
1. Anomaly TRNS SHFT
2. Sequence Label RB?VP PP
3. Head Per.Gen.Num. 3cp ---
4. Head POS.(Binyan) VB.PIEL IN
5. Sequence-Initial POS VB IN
6. Sequence-Final POS RB JJ
7. Left POS (Learner) RB NNP
8. Right POS (Learner) IN RB
9. Left POS (Target) RB RB
10. Right POS (Target) IN yyDOT
11. Sequence Length 2 2
12. Normalized Error Cost 0.625 0.250
13. Sent-Level@Rank 200@2 200@2
Table 2: Inter-token feature categories
the next three features provide approximations of
phrasal properties, e.g., the phrasal category and
head, based on a few syntactically-motivated heuris-
tics. Sequence Label identifies the lexical or phrasal
category of the shifted token/token-sequence (e.g.,
PP). Note that sequence labels for transpositions are
special cases consisting of two category labels sep-
arated by an arrow. Head Per.Gen.Num and Head
POS.(Binyan) represent the morphosyntactic prop-
erties of the sequence?s (approximate) head word,
namely its person, gender, and number, and its POS
tag. If the head is a verb, the POS tag is followed by
the verb?s binyan (i.e., verb class), as in VB.PIEL.
The cost feature, Normalized Error Cost, is com-
puted as follows: for missing, extra, and transposed
sequences, the cost is simply the sequence length
divided by the sentence length. For shifts, the se-
quence length and the shift distance are summed
and then divided by the sentence length. Sent-
Level@Rank indicates both the difficulty level of the
exercise and the word-order rank of target sentence
to which the learner sentence has been matched.
4.3.3 Global features
In addition to errors, we also look at global fea-
tures capturing global trends in a sentence, in order
to integrate information about the learner?s overall
performance on a sentence. For example, we note
the percentage of target POS bigrams present in the
learner sentence (POS recall). Table 3 presents the
global features. The two example feature vectors are
those for the sentences (8b) and (9b) above.
99
Features Ex. (8b) Ex. (9b)
1. POS Bigram Recall 2.000 1.273
2. LCSeq Ratio 2.000 1.250
3. LCStr Ratio 1.200 0.500
4. Relaxed LCStr Ratio 2.000 0.500
5. Intra-token Error Count 1.500 0.000
6. Inter-token Error Count 0.000 1.500
7. Intra-token Net Cost 1.875 0.000
8. Norm. Aggregate Displ. 0.000 0.422
9. Sentence Level 200 200
Table 3: Global feature categories
Except for feature 9 (Sentence Level), every fea-
ture in table 3 is multiplied by a weight derived from
the sentence level. These weights serve either to pe-
nalize or compensate for a sentence?s difficulty, de-
pending on the feature type. Because features 1?
4 are ?positive? measures, they are multiplied by
a factor proportional to the sentence level, namely
l = 1. . . 4, whose values correspond directly to the
levels 150?300+, respectively. Features 5?8, in con-
trast, are ?negative? measures, so they are multiplied
by a factor inversely proportional to l, namely 5?l4 .
Among the positive features, LCSeq looks for the
longest common subsequence between the learner
sentence and the target, while LCStr Ratio and Re-
laxed LCStr Ratio both look for longest common
substrings. However, Relaxed LCStr Ratio allows
for token-internal anomalies (as long as the token it-
self is present) while LCStr Ratio does not.
As for the negative features, the two Error Count
features simply tally up the errors of each type
present in the sentence. The Intra-token Net Cost
sums over the token-internal Levenshtein distances
between corresponding learner and target tokens.
Normalized Aggregate Displacement is the sum of
insertions and deletions carried out during inter-
token alignment, normalized by sentence length.
5 Two-phase classification
To combine the features for individual phenomena,
we run a two-phase classifier. In the first phase, we
classify each feature vector for each phenomenon
into a level. In the second phase, we aggregate over
this output to classify the overall learner level.
We use two-phase classification in order to: 1)
modularize each individual phenomenon, mean-
ing that new phenomena are more easily incorpo-
rated into future models; 2) better capture sparsely-
represented phenomena, by aggregating over them;
and 3) easily integrate other exercise types simply
by having more specialized phase 1 classifiers and
by then integrating the results into phase 2.
One potential drawback of two-phase classifica-
tion is that of not having gold standard annotation of
phase 1 levels or even knowing for sure whether in-
dividual phenomena can be classified into consistent
and useful categories. That is, even if a 200-level
learner makes an error, that error is not necessarily a
200-level error. We discuss this next.
5.1 Classifying individual phenomena
With our three sets of features (sec. 4), we set up
three classifiers. Depending upon the type, the ap-
propriate classifier is used to categorize each phase
1 vector. For classification, every phase 1 vector is
assigned a single learner level. However, this as-
sumes that each error indicates a unique level, which
is not always true. A substitution of i for w, for ex-
ample, may largely be made by 250-level (interme-
diate) learners, but also by learners of other levels.
One approach is to thus view each phenomenon as
mapping to a set of levels, and for a new vector, clas-
sification predicts the set of appropriate levels, and
their likelihood. Another approach to overcome the
fact that each uniquely-classified phenomenon can
be indicative of many levels is to rely on phase 2
to aggregate over different phenomena. The advan-
tage of the first approach is that it makes no assump-
tions about individual phenomena being indicative
of a single level, but the disadvantage is that one
may start to add confusion for phase 2 by includ-
ing less relevant levels, especially when using little
training data. The second approach counteracts this
confusion by selecting the most prototypical level
for an individual phenomenon (cf. criterial features
in (Hawkins and Buttery, 2010)), giving less noise
to phase 2. We may lose important non-best level
information, but as we show in sec. 6, with a range
of classifications from phase 1, the second phase can
learn the proper learner level.
In either case, from the perspective of training,
an individual phenomenon can be seen, in terms of
level, as the set of learners who produced such a phe-
nomenon. We thus approximate the level of each
100
Feature type Feature type
1. 100-level classes 7. Intra-token error sum
2. 150-level classes 8. Inter-token error sum
3. 200-level classes 9. Sentences attempted
4. 250-level classes 10. 250-level attempts
5. 300-level classes 11. 300-level attempts
6. Composite error
Table 4: Feature categories for learner level prediction
phenomenon by using the level of the learner from
the gold standard training data. This allows us not to
make a theoretical classification of phenomena (as
opposed to taxonomically labeling phenomena).
5.2 Predicting learner level
We aggregate the information from phase 1 classifi-
cation to classify overall learner levels. We assume
that the set of all individual phenomena and their
quantities (e.g., proportion of phenomena classified
as 200-level in phase 1) characterize a learner?s level
(Hawkins and Buttery, 2010). The feature types
are given in table 4. Features 1?6 are discussed in
sec. 6.1; features 7?8 are (normalized) sums; and the
rest record the number of sentences attempted, bro-
ken down by intended level of the sentence. Lower-
level attempts are not included, as they are the same
values for nearly all students. When we incorporate
other exercise types in the future, additional features
can be added?and the current features modified?
to fold in information from those exercise types.
An example To take an example, one of our
(300+) learners attempts four sentences, giving four
sets of global features, and makes four errors, for
a total of eight phase 1 individual phenomena. One
phenomenon is automatically classified as 100-level,
one as 150, four as 200, one as 250, and one as 300+.
Taking the 1-best phase 1 output (see section 6.3),
the phase 2 vector in this case is as in (10a), corre-
sponding directly to the features in table 4.
(10) a. 0.25, 0.25, 1.00, 0.25, 0.25, 2.00, 0.50,
0.50, 4, 1, 0
b. 0.25, 0.00, 1.00, 0.25, 0.00, 1.625, 0.00,
0.50, 4, 1, 0
In training, we find a 300+-level learner with a
very similar vector, namely that of (10b). Depending
upon the exact experimental set-up (e.g., k2 = 1,
see section 6.3), then, this vector helps the system to
correctly classify our learner as 300+.
6 Evaluation
6.1 Details of the experiments
We use TiMBL (Daelemans et al, 2010; Daelemans
et al, 1999), a memory-based learner (MBL), for
both phases. We use TiMBL because MBL has been
shown to work well with small data sets (Banko and
Brill, 2001); allows for the use of both text-based
and numeric features; and does not suffer from a
fragmented class space. We mostly use the default
settings of TiMBL?the IB1 learning algorithm and
overlap comparison metric between instances?and
experiment with different values of k.
For prediction of phenomenon level (phase 1) and
learner level (phase 2), the system is trained on data
from placement exams previously collected in a He-
brew language program, as described in sec. 2. With
only 38 learners, we use leave-one-out testing, train-
ing on the data from the 37 other learners in order
to run a model on each learner?s sentences. All of
phase 1 is completed (i.e., automatically analyzed)
before training the phase 2 models. As a baseline,
we use the majority class (level 150); choosing this
for all learners gives an accuracy of 34.2% (13/38).2
Phase 1 probability distributions Because
TiMBL retrieves all neighbor with the k nearest
distances rather than the k nearest neighbors, we
can use the number of neighbors in phase 1 to adjust
the values of, e.g., 150-level classes. For example,
the output from phase 1 for two different vectors
might be as in (11). Both have a distribution of 23
150-level and 13 200-level; however, in one case,
this is based on 6 neighbors, whereas for the other,
there are 12 neighbors within the nearest distance.
(11) a) 150:4, 200:2 b) 150:8, 200:4
With more data, we may have more confidence
in the prediction of the second case. The classes
features (fx) of table 4 are thus calculated as in
(12), multiplying counts of each class (c(x)) by their
probabilities (p(x)).
2We are aware that the baseline is not very strong, but the
only alternative would be to use a classifier since we observed
no direct correlation between level and number of errors.
101
k Intra Inter Global Overall
1 28.1% 38.6% 34.4% 34.7%
3 34.2% 44.6% 44.6% 41.9%
5 34.2% 37.1% 36.7% 36.3%
Table 5: Phase 1 accuracies
(12) fx =
?
phase1
c(x)p(x)
The Composite error feature combines all classes
features into one score, inversely weighing them by
level, so that more low-level errors give a high value.
6.2 Predicting phenomena levels
We first evaluate phase 1 accuracy, as in table 5. Us-
ing k = 3 gives the best phase 1 result, 41.9%. We
evaluate with respect to the single-best class, i.e.,
the level of the learner of interest. Accuracy is the
percentage of correctly-classified instances out of all
instances. We assume an instance is classified cor-
rectly if its class corresponds to the learner level.
Accuracy is rather low, at 41.9%. However, we
must bear in mind that we cannot expect 100% accu-
racy, given that individual phenomena do not clearly
belong to a single level. Intra-token classification is
lowest, likely due to greater issues of sparsity: ran-
dom typos are unlikely to occur more than once.
6.3 Predicting learner level
For the second phase, we use different settings for
phase 1 instances. The results are shown in table 6.
The overall best results are reached using single-best
classification for phase 1 and k = 1 for phase 2, giv-
ing an accuracy of 60.5%. Note that the best result
does not use the best performing setting for phase 1
but rather the one with the lowest performance for
phase 1. This shows clearly that optimizing the two
phases individually is not feasible. We obtain the
same accuracy using k = 5 for both phases.
Since we are interested in how these two settings
differ, we extract confusion matrices for them; they
are shown in table 7. The matrices show that the in-
herent smoothing via the k nearest neighbors leads
to a good performance for lower levels, to the ex-
clusion of levels higher than 200. The higher lev-
els are also the least frequent: the k1 = 5/k2 = 5
case shows a bias towards the overall distribution of
levels, whereas the 1-best/k2 = 1 setting is more
Phase 1
1-best k1 = 1 k1 = 3 k1 = 5
P
ha
se
2 Max 42.1 47.4 57.9 42.1
k2 = 1 60.5 57.9 36.8 39.5
k2 = 3 42.1 44.7 44.7 42.1
k2 = 5 39.5 42.1 44.7 60.5
Table 6: Phase 2 accuracies for different phase 1 settings
System
1-best 100 150 200 250 300+ Acc.
G
ol
d
100 6 1 6/7
150 2 7 3 1 7/13
200 2 7 1 1 7/11
250 1 1 0/2
300+ 1 1 3 3/5
k=5 100 150 200 250 300+ Acc.
G
ol
d
100 5 2 5/7
150 2 9 2 9/13
200 2 9 9/11
250 2 0/2
300+ 1 4 0/5
Table 7: Classification confusion matrices
likely to guess neighboring classes. In order to better
account for incorrect classifications which are close
to the correct answer (e.g., 250 for 200), we also
calculated weighted kappa for all the results in ta-
ble 6. Based on kappa, the best result is based on
the setting k1 = 1/k2 = 1 (0.647), followed by
1-best/k2 = 1 (0.639). The weighted kappa for
k1 = 5/k2 = 5 is significantly lower (0.503).
We are also interested in whether we need such
a complex system: phase 1 can outputs a distribu-
tion of senses (k1 = n), or we can use the single
best class as input to phase 2 (1-best). In a different
vein, phase 2 is a machine learner (k2 = n) trained
on phase 1 classified data, but could be simplified
to take the maximum phase 1 class (Max). The re-
sults in table 6 show that using the single-best result
from phase 1 in combination with k2 = 1 provides
the best results, indicating that phase 2 can properly
aggregate over individual phenomena (see sec. 5.1).
However, for all other phase 2 settings, adding the
distribution over phase 1 results increases accuracy.
Using the maximum class rather than the machine
learner in phase 2 generally works best in combina-
102
tion with more nearest neighbors in phase 1, provid-
ing a type of smoothing. However, using the maxi-
mum has an overall detrimental effect.
While the results may not be robust enough to de-
ploy, they are high, given that this is only one type of
exercise, and we have used a very small set of train-
ing data. When performing the error analysis, we
found one student who had attempted only half of
the sentences?generally a sign of a low level?who
was put into level 300. We assume this student per-
formed better on other exercises in the exam. Given
this picture, it is not surprising that our system con-
sistently groups this student into a lower level.
6.4 Ablation studies
We are particularly interested in how the different
phases interact, 1) because one major way to expand
the system is to add different exercises and incor-
porate them into the second phase, and 2) because
the results in table 6 show a strong interdependence
between phases. We thus performed a set of exper-
iments to gauge the effect of different types of fea-
tures. By running ablation studies?i.e., removing
one or more sets of features (cf. e.g. (Yannakoudakis
et al, 2011))?we can determine their relative impor-
tance and usefulness. We run phase 2 (k = 1) using
different combinations of phase 1 classifiers (1-best)
as input. The results are presented in table 8.
Intra Inter Global Acc.
Y Y Y 60.5%
Y Y N 47.4%
Y N N 42.1%
N Y Y 42.1%
N Y N 42.1%
Y N Y 36.8%
N N Y 34.2%
Table 8: Ablation studies, evaluating on phase 2 accuracy
Perhaps unsurprisingly, the combination of all
feature types results in the highest results of 60.5%.
Also, using only one type of features results in the
lowest performance, with the global features being
the least informative set, on par with the baseline of
34.2%. If we use only two feature sets, removing
the global features results in the least deterioration.
Since these features do not directly model errors but
rather global sentence trends, this is to be expected.
However, leaving out inter-token features results in
the second-lowest results (36.8%), thus showing that
this set is extremely important?again not surprising
given that we are working with an exercise designed
to test word order skills.
7 Summary and Outlook
We have developed a system for predicting the level
of Hebrew language learners, using only a small
amount of targeted language data. We have pre-
dicted level based on a single placement exam exer-
cise, finding a surprising degree of accuracy despite
missing much of the information normally used on
such exams. We accounted for the problem of data
sparsity by breaking the problem into a two-phase
classification and through our choice of learning al-
gorithm. The classification process isolates individ-
ual errors and linguistic constructions which are then
aggregated into a second phase; such a two-step pro-
cess allows for easy integration of other exercises
and features in the future. The aggregation of infor-
mation allows us to smooth over sparse features.
In the immediate future, we are integrating other
exercises, to improve the overall accuracy of level
prediction (i.e., the second phase) and make auto-
matic testing more valid (cf. e.g. (Fulcher, 1997)),
while at the same time incorporating more linguistic
processing for more complex input. For example,
with question formation exercises, no closed set of
correct answers exists, and one must use parse tree
distance to delineate features. With multiple exer-
cises, we have plans to test the system with incoming
students to the Hebrew program at our university.
Acknowledgments
We would like to thank Ayelet Weiss for help
throughout, as well as Chris Riley and Amber Smith.
Part of this work was funded by the IU Jewish Stud-
ies Center, as well as by the Institute for Digital Arts
and Humanities and the Data to Insight Center at IU.
We also thank Stephanie Dickinson from the Indiana
Statistical Consulting Center (ISCC) for analysis as-
sistance and the three anonymous reviewers for their
comments.
103
References
Dora Alexopoulou, Helen Yannakoudakis, and Ted
Briscoe. 2010. From discriminative features to learner
grammars: a data driven approach to learner corpora.
Talk given at Second Language Research Forum, Uni-
versity of Maryland, October 2010.
Yigal Attali and Jill Burstein. 2006. Automated essay
scoring with e-rater v.2. Journal of Technology, Learn-
ing, and Assessment, 4(3), February.
Michele Banko and Eric Brill. 2001. Mitigating the
paucity-of-data problem: Exploring the effect of train-
ing corpus size on classifier performance for natural
language processing. In Proceedings of HLT 2001,
First International Conference on Human Language
Technology Research, pages 253?257, San Diego, CA.
Adriane Boyd. 2010. EAGLE: an error-annotated cor-
pus of beginning learner German. In Proceedings of
LREC-10, Valetta, Malta.
Walter Daelemans, Antal van den Bosch, and Jakub Za-
vrel. 1999. Forgetting exceptions is harmful in lan-
guage learning. Machine Learning, 34:11?41.
Walter Daelemans, Jakub Zavrel, Ko van der Sloot, and
Antal van den Bosch. 2010. Timbl: Tilburg memory
based learner, version 6.3, reference guide. Technical
report, ILK Research Group. Technical Report Series
no. 10-01.
Robert Dale and Adam Kilgarriff. 2011. Helping our
own: The HOO 2011 pilot shared task. In Proceedings
of the Generation Challenges Session at the 13th Eu-
ropean Workshop on Natural Language Generation,
pages 242?249, Nancy, France, September.
Ana D??az-Negrillo and Jesu?s Ferna?ndez-Dom??nguez.
2006. Error tagging systems for learner corpora.
Spanish Journal of Applied Linguistics (RESLA),
19:83?102.
Markus Dickinson, Ross Israel, and Sun-Hee Lee. 2011.
Developing methodology for Korean particle error de-
tection. In Proceedings of the Sixth Workshop on In-
novative Use of NLP for Building Educational Appli-
cations, pages 81?86, Portland, OR, June.
Markus Dickinson. 2011. On morphological analysis for
learner language, focusing on Russian. Research on
Language and Computation, 8(4):273?298.
Glenn Fulcher. 1997. An English language placement
test: issues in reliability and validity. Language Test-
ing, 14(2):113?138.
Yoav Goldberg and Michael Elhadad. 2011. Joint He-
brew segmentation and parsing using a PCFGLA lat-
tice parser. In Proceedings of ACL-HLT, pages 704?
709, Portland, OR, June.
Gintare` Grigonyte`, Joa?o Paulo Cordeiro, Gae?l Dias, Ru-
men Moraliyski, and Pavel Brazdil. 2010. Para-
phrase alignment for synonym evidence discovery.
In Proceedings of the 23rd International Conference
on Computational Linguistics (COLING), pages 403?
411, Beijing, China.
John A. Hawkins and Paula Buttery. 2010. Criterial fea-
tures in learner corpora: Theory and illustrations. En-
glish Profile Journal, 1(1):1?23.
John A. Hawkins and Luna Filipovic?. 2010. Criterial
Features in L2 English: Specifying the Reference Lev-
els of the Common European Framework. Cambridge
University Press.
Alon Itai and Shuly Wintner. 2008. Language resources
for Hebrew. Language Resources and Evaluation,
42(1):75?98.
Alla Rozovskaya and Dan Roth. 2011. Algorithm selec-
tion and model adaptation for ESL correction tasks. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 924?933, Portland, OR,
June.
Khalil Sima?an, Alon Itai, Yoad Winter, Alon Altman,
and N. Nativ. 2001. Building a tree-bank of Mod-
ern Hebrew text. Traitment Automatique des Langues,
42(2).
Joel Tetreault and Martin Chodorow. 2008. The ups and
downs of preposition error detection in ESL writing.
In Proceedings of COLING-08, Manchester.
Helen Yannakoudakis, Ted Briscoe, and Ben Medlock.
2011. A new dataset and method for automatically
grading ESOL texts. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies, pages
180?189, Portland, OR, June.
Shlomo Yona and Shuly Wintner. 2008. A finite-state
morphological grammar of Hebrew. Natural Lan-
guage Engineering, 14(2):173?190.
104
Proceedings of the Joint Conference on EMNLP and CoNLL: Shared Task, pages 88?94,
Jeju Island, Korea, July 13, 2012. c?2012 Association for Computational Linguistics
UBIU for Multilingual Coreference Resolution in OntoNotes
Desislava Zhekova Sandra Ku?bler Joshua Bonner Marwa Ragheb Yu-Yin Hsu
Indiana University
Bloomington, IN, USA
{dzhekova, skuebler, jebonner, mragheb, hsuy}@indiana.edu
Abstract
The current work presents the participa-
tion of UBIU (Zhekova and Ku?bler, 2010)
in the CoNLL-2012 Shared Task: Model-
ing Multilingual Unrestricted Coreference in
OntoNotes (Pradhan et al, 2012). Our system
deals with all three languages: Arabic, Chi-
nese and English. The system results show
that UBIU works reliably across all three lan-
guages, reaching an average score of 40.57 for
Arabic, 46.12 for Chinese, and 48.70 for En-
glish. For Arabic and Chinese, the system pro-
duces high precision, while for English, preci-
sion and recall are balanced, which leads to
the highest results across languages.
1 Introduction
Multilingual coreference resolution has been gain-
ing considerable interest among researchers in re-
cent years. Yet, only a very small number of sys-
tems target coreference resolution (CR) for more
than one language (Mitkov, 1999; Harabagiu and
Maiorano, 2000; Luo and Zitouni, 2005). A first
attempt at gaining insight into the comparability of
systems on different languages was accomplished in
the SemEval-2010 Task 1: Coreference Resolution
in Multiple Languages (Recasens et al, 2010). Six
systems participated in that task, UBIU (Zhekova
and Ku?bler, 2010) among them. However, since sys-
tems participated across the various languages rather
irregularly, Recasens et al (2010) reported that the
data points were too few to allow for a proper com-
parison between different approaches. Further sig-
nificant issues concerned system portability across
the various languages and the respective language
tuning, the influence of the quantity and quality of
diverse linguistic annotations as well as the perfor-
mance and behavior of various evaluation metrics.
The CoNLL-2011 Shared Task: Modeling Unre-
stricted Coreference in OntoNotes (Pradhan et al,
2011) targeted unrestricted CR, which aims at iden-
tifying nominal coreference but also event corefer-
ence, within an English data set from the OntoNotes
corpus. Not surprisingly, attempting to include such
event mentions had a detrimental effect on over-
all accuracy, and the best performing systems (e.g.,
(Lee et al, 2011)) did not attempt event anaphora.
The current shared task extends the task definition to
three different languages (Arabic, Chinese and En-
glish), which can prove challenging for rule-based
approaches such as the best performing system from
2011 (Lee et al, 2011).
In the current paper, we present UBIU, a memory-
based coreference resolution system, and its re-
sults in the CoNLL-2012 Shared Task. We give an
overview of UBIU in Section 2. In Section 3, we
present the system results, after which Section 4 lays
out some conclusive remarks.
2 UBIU
UBIU (Zhekova and Ku?bler, 2010) is a corefer-
ence resolution system designed specifically for a
multilingual setting. As shown by Recasens et al
(2010), multilingual coreference resolution can be
approached by various machine learning methods
since machine learning provides a possibility for ro-
bust abstraction over the variation of language phe-
nomena and specificity. Therefore, UBIU employs88
a machine learning approach, memory-based learn-
ing (MBL) since it has proven to be a good so-
lution to various natural language processing tasks
(Daelemans and van den Bosch, 2005). We em-
ploy TiMBL (Daelemans et al, 2010), which uses
k nearest neighbour classification to assign class la-
bels to the targeted instances. The classifier set-
tings we used were determined by a non-exhaustive
search over the development data and are as follows:
the IB1 algorithm, similarity is computed based on
weighted overlap, gain ratio is used for the relevance
weights and the number of nearest neighbors is set to
k=3 (cf. (Daelemans et al, 2010) for an explanation
of the system parameters).
In UBIU, we use a pairwise mention model (Soon
et al, 2001; Broscheit et al, 2010) since this model
has proven more robust towards multiple languages
(Wunsch, 2009) than more elaborate ones. We con-
centrate on nominal coreference resolution, i.e. we
ignore the more unrestricted cases of event corefer-
ence. Below, we describe the modules used in UBIU
in more detail.
2.1 Preprocessing
The preprocessing module oversees the proper for-
matting of the data for all modules applied in later
stages during coreference resolution. During pre-
processing, we use the speaker information, if pro-
vided, and replace all 1st person singular pronouns
from the token position with the information pro-
vided in the speaker column and adjust the POS tag
correspondingly.
2.2 Mention Detection
Mention detection is the process of detecting the
phrases that are potentially coreferent and are thus
considered candidates for the coreference process.
Mention detection in UBIU is based on the parse and
named entity information provided by the shared
task. This step is crucial for the overall system per-
formance, and we aim for high recall at this stage.
Singleton mentions that are added in this step can
be filtered out in later stages. However, if we fail
to detect a mention in this stage, it cannot be added
later. We predict a mention for each noun phrase and
named entity provided in the data. Additionally, we
extract mentions for possessive pronouns in English
as only those did not correspond to a noun phrase in
MD
R P F1
Arabic 97.13 19.06 31.87
Chinese 98.33 31.64 47.88
English 96.73 30.75 46.67
Table 1: Mention detection (development set).
the syntactic structure provided by the task. In Ara-
bic and Chinese, possessives are already marked as
noun phrases.
The system results on mention detection on the
development set are listed in Table 1. The results
show that we reach very high recall but low preci-
sion, as intended. The majority of the errors are due
to discrepancies between noun phrases and named
entities on the one hand and mentions on the other.
Furthermore, since we do not target event corefer-
ence, we do not add mentions for the verbs in the
data, which leads to a reduction of recall.
In all further system modules, we represent a
mention by its head, which is extracted via heuris-
tic methods. For Arabic, we select the first noun or
pronoun while for Chinese and English, we extract
the the pronoun or the last noun of a mention unless
it is a common title. Additionally, we filter out men-
tions that correspond to types of named entities that
in a majority of the cases in the training data are not
coreferent (i.e. cardinals, ordinals, etc.).
One problem with representing mentions mostly
by their head is that it is difficult to decide between
the different mention spans of a head. Since auto-
matic mentions are considered correct only if they
match the exact span of a gold mention, we include
all identified mention spans for every extracted head
for classification, which can lead to losses in evalu-
ation. For example, consider the instance from the
development set in (1): the noun phrase the Avenue
of Stars is coreferent and thus marked as a gold men-
tion (key 7). UBIU extracts two different spans for
the same head Avenue: the Avenue (MD 3) and the
Avenue of Stars (MD 5).
(1)
token POS parse key MD output
the DT (NP(NP* (7 (3|(5 (9
Avenue NNP *) - 3) 9)
of IN (PP* - - -
Stars NNPS (NP*))) 7) (4)|5) -
Both mention spans are passed to the coreference
resolver, together with additional features (i.e. men-89
MD MUC B3 CEAFE Average
F1 F1 F1 F1 F1
long 100.0 100.0 100.0 100.0 100.0
short 50.00 0 66.66 66.66 44.44
Table 2: The scores for the short example in (1).
tion length, head modification, etc.) that will allow
the resolver to distinguish between the spans. The
classifier decides that the shorter mention is coref-
erent and that the longer mention is a singleton. In
order to show the effect of this decision, we assume
that there is one coreferent mention to key 7. We
consider the two possible spans and show the re-
spective scores in Table 2. The evaluation in Table 2
shows that providing the correct coreference link but
the wrong, short mention span, the Avenue, has con-
siderable effects to the overall performance. First,
as defined by the task, the mention is ignored by all
evaluation metrics leading to a decrease in mention
detection and coreference performance. Moreover,
the fact that this mention is ignored means that the
second mention becomes a singleton and is not con-
sidered by MUC either, leading to an F1 score of 0.
This example shows the importance of selecting the
correct mention span.
2.3 Singleton Classification
A singleton is a mention which corefers with no
other mention, either because it does not refer to any
entity or because it refers to an entity with no other
mentions in the discourse. Because singletons com-
prise the majority of mentions in a discourse, their
presence can have a substantial effect on the perfor-
mance of machine learning approaches to CR, both
because they complicate the learning task and be-
cause they heavily skew the proportion in the train-
ing data towards negative instances, which can bias
the learner towards assuming no coreference relation
between pairs of mentions. For this reason, informa-
tion concerning singletons needs to be incorporated
into the CR process so that such mentions can be
eliminated from consideration.
Boyd et al (2005), Ng and Cardie (2002), and
Evans (2001) experimented with machine learning
approaches to detect and/or eliminate singletons,
finding that such a module provides an improve-
ment in CR performance provided that the classifier
# Feature Description
1 the depth of the mention in the syntax tree
2 the length of the mention
3 the head token of the mention
4 the POS tag of the head
5 the NE of the head
6 the NE of the mention
7 PR if the head is premodified, PO if it is not; UN otherwise
8 D if the head is in a definite mention; I otherwise
9 the predicate argument corresponding to the mention
10 left context token on position token -3
11 left context token on position token -2
12 left context token on position token -1
13 left context POS tag of token on position token -3
14 left context POS tag of token on position token -2
15 left context POS tag of token on position token -1
10 right context token on position token +1
11 right context token on position token +2
12 right context token on position token +3
13 right context POS tag of token on position token +1
14 right context POS tag of token on position token +2
15 right context POS tag of token on position token +3
16 the syntactic label of the mother node
17 the syntactic label of the grandmother node
18 a concatenation of the labels of the preceding nodes
19 C if the mention is in a PP; else I
Table 3: The features used by the singleton classifier.
does not eliminate non-singletons too frequently. Ng
(2004) additionally compared various feature- and
constraint-based approaches to incorporating single-
ton information into the CR pipeline. Feature-based
approaches integrate information from the single-
ton classifier as features while constraint-based ap-
proaches filter singletons from the mention set. Fol-
lowing these works, we include a k nearest neigh-
bor classifier for singleton mentions in UBIU with
19 commonly-used features described below. How-
ever, unlike Ng (2004), we use a combination of the
feature- and constraint-based approaches to incorpo-
rate the classifier?s results.
Each training/testing instance represents a noun
phrase or a named entity from the data together with
features describing this phrase in its discourse. The
list of features is shown in Table 3. The instances
that are classified by the learner as singletons with
a distance to their nearest neighbor below a thresh-
old (i.e., half the average distance observed in the
training data) are filtered from the mention set, and
are thus not considered in the pairwise coreference
classification. For the remainder of the mentions, the
class that the singletons classifier has assigned to the
instance is used as a feature in the coreference clas-
sifier. Experiments on the development set showed90
MD MUC B3 CEAFE Average
F1 F1 F1 F1 F1
Arabic
+SC 58.36 34.75 58.26 37.39 43.47
-SC 56.12 34.96 58.52 36.05 43.18
Chinese
+SC 52.30 42.70 61.11 32.86 45.56
-SC 50.40 41.19 60.96 32.47 44.87
English
+SC 67.38 53.20 59.23 34.90 49.11
-SC 65.55 51.57 59.18 34.38 48.38
Table 4: Evaluation of using (+SC) or not (-SC) the sin-
gleton classifier in UBIU on the development set.
that the most important features across all languages
are the POS tag of the head word, definiteness, and
the mother node in the syntactic representation. In-
formation about head modification is helpful for En-
glish and Arabic, but not for Chinese.
The results of using the singleton classifier in
UBIU on the development set are shown in Table 4.
They show a moderate improvement for all evalu-
ation metrics and all languages, with the exception
of MUC and B3 for Arabic. The most noticeable
improvement can be observed in mention detection,
which gains approx. 2% in all languages. A man-
ual inspection of the development data shows that
the version using the singleton classifier extracts a
slightly higher number of coreferent mentions than
the version without. However, the reduction of men-
tions that are never coreferent, which was the main
goal of the singleton classifier, is also present in the
version without the classifier, so that the results of
the classifier only have a minimal influence on the
final results.
2.4 Coreference Classification
Coreference classification is the process in which
all identified mentions are paired up and features
are extracted to build feature vectors that represent
the mention pairs in their context. Each mention
is represented in the feature vector by its syntactic
head. The vectors for the pairs are then used by the
memory-based learner TiMBL.
As anaphoric mentions, we consider all definite
phrases; we then create a pair for each anaphor with
each mention preceding it within a window of 10
(English, Chinese) or 7 (Arabic) sentences. We con-
sider a shorter window of sentences for Arabic be-
cause of its NP-rich syntactic structure and its longer
sentences, which leads to an increased number of
possible mention pairs. The set of features that we
use, listed in Table 5, is an extension of the set by
Rahman and Ng (2009). Before classification, we
apply a morphological filter, which excludes vectors
that disagree in number or gender (applied only if
the respective information is provided or can be de-
duced from the data).
Both the anaphor and the antecedent carry a la-
bel assigned to them by the singletons classifier.
Yet, we consider as anaphoric only the heads of
definite mentions. Including a feature representing
the class assigned by the singletons classifier for
each anaphor triggers a conservative learner behav-
ior, i.e., fewer positive classes are assigned. Thus, to
account for this behavior, we ignore those labels for
the anaphor and include only one feature (no. 25 in
Table 5) in the vector for the antecedent.
2.5 Postprocessing
In postprocessing, we create the equivalence classes
of mentions that were classified as coreferent and
# Feature Description
1 mj - the antecedent
2 mk - the mention (further m.) to be resolved
3 C if mj is a pronoun; else I
4 C if mk is a pronoun; else I
5 the concatenated values of feature 3 and feature 4
6 C if the m. are the same string; else I
7 C if one m. is a substring of the other; else I
8 C if both m. are pronominal and are the same string; else I
9 C if both are non-pronominal and are the same string; else I
10 C if both are pronouns; I if neither is a pronoun; else U
11 C if both are proper nouns; I if neither is; else U
12 C if both m. have the same speaker; I if they do not
13 C if both m. are the same named entity; I if they are not and
U if they are not assigned a NE
14 token distance between mj and mk
15 sentence distance between mj and mk
16 normalised levenstein distance for both m.
17 PR if mj is premodified, PO if it is not; UN otherwise
18 PR if mk is premodified, PO if it is not; UN otherwise
19 the concatenated values for feature 17 and 18
20 D if mj is in a definite m.; I otherwise
21 C if mj is within the subject; I-within an object; U otherwise
22 C if mk is within the subject; I-within an object; U otherwise
23 C if neither is embedded in a PP; I otherwise
24 C if neither is embedded in a NP; I otherwise
25 C if mj has been classified as singleton; I otherwise
26 C if both are within ARG0-ARG4; I-within ARGM; else U
27 C if mj is within ARG0-ARG4; I-within ARGM; else U
28 C if mk is within ARG0-ARG4; I-within ARGM; else U
29 concatenated values for features 27 and 28
30 the predicate argument label for mj
31 the predicate argument label for mk
32 C if both m. agree in number; else I
33 C if both m. agree in gender; else I
Table 5: The features used by the coreference classifier.91
MD MUC B3 CEAFE Average
R P F1 R P F1 R P F1 R P F1 F1
Automatic Mention Detection
auto
Arabic 27.54 80.34 41.02 19.64 62.13 29.85 41.91 90.72 57.33 56.79 24.81 34.53 40.57
Chinese 35.12 72.52 47.32 31.19 57.97 40.56 49.49 77.65 60.45 45.92 25.24 32.58 44.53
English 65.78 68.49 67.11 54.28 52.79 53.52 62.26 54.90 58.35 33.52 34.96 34.22 48.70
gold
Arabic 28.00 82.21 41.78 15.47 45.92 23.15 39.22 84.86 53.65 55.10 24.22 33.65 36.82
Chinese 37.84 74.84 50.27 33.95 60.29 43.44 50.95 77.28 61.41 46.68 26.13 33.50 46.12
English 66.05 69.62 67.79 54.45 53.59 54.02 61.66 55.62 58.48 33.82 34.65 34.23 48.91
Gold Mention Boundaries
auto
Arabic 27.48 75.53 40.29 18.75 56.47 28.16 42.67 89.25 57.74 55.53 25.36 34.82 40.24
Chinese 36.97 73.98 49.30 32.09 58.30 41.39 49.43 77.38 60.32 46.35 25.71 33.07 44.93
English 66.45 70.91 68.61 54.96 54.67 54.82 61.85 55.60 58.56 34.38 34.67 34.53 49.30
gold
Arabic 28.06 82.39 41.87 15.56 46.18 23.28 39.23 84.95 53.67 55.10 24.20 33.63 36.86
Chinese 37.89 74.79 50.30 33.93 60.19 43.39 50.87 77.27 61.35 46.62 26.13 33.49 46.08
English 65.82 71.72 68.65 54.68 55.51 55.09 61.22 56.59 58.82 34.85 34.04 34.44 49.45
Gold Mentions
auto
Arabic 100 100 100 42.48 80.36 55.58 50.87 89.69 64.92 71.96 34.52 46.66 55.72
Chinese 100 100 100 42.02 79.57 55.00 50.22 80.81 61.94 60.27 27.08 37.37 51.44
English 100 100 100 68.38 78.11 72.92 63.04 58.60 60.74 52.64 37.10 43.53 59.06
gold
Arabic 100 100 100 45.58 73.27 56.20 52.27 82.35 63.95 70.17 37.54 48.91 56.35
Chinese 100 100 100 44.12 80.89 57.10 51.79 80.53 63.04 60.37 27.69 37.96 52.70
English 100 100 100 68.54 78.10 73.01 63.14 58.63 60.80 52.84 37.44 43.83 59.21
Table 6: UBIU system performance in the shared task.
insert the appropriate class/entity IDs in the data,
removing mentions that constitute a class on their
own ? singletons. We bind all pronouns (except the
ones that were labeled as singletons by the singleton
classifier) that were not assigned an antecedent to
the last seen subject and if such is not present to the
last seen mention. We consider all positively classi-
fied instances in the clustering process.
3 Evaluation
The results of the final system evaluation are pre-
sented in Table 6. Comparing the results for mention
detection (MD) on the development set (see Table 1,
which shows MD before the resolution step) and the
final test set (Table 6, showing MD after resolution
and the deletion of singletons), we encounter a rever-
sal of precision and recall tendencies (even though
the results are not fully comparable since they are
based on different data sets). This is due to the fact
that during mention detection, we aim for high re-
call, and after coreference resolution, all mentions
identified as singletons by the system are excluded
from the answer set. Thus mentions that are coref-
erent in the key set but wrongly classified in the an-
swer set are removed, leading to a decrease in re-
call. With regard to MD precision, a considerable
increase is recorded, showing that the majority of
the mentions that the system indicates as coreferent
have the correct mention spans. Additionally, the
problem of selecting the correct span (as described
in Section 2) is another factor that has a considerable
effect on precision at that stage ? mentions that were
accurately attached to the correct coreference chain
are not considered if their span is not identical to the
span of their counterparts in the key set.
Automatic Mention Detection In the first part in
Table 6, we show the system scores for UBIU?s per-
formance when no mention information is provided
in the data. We report both gold (using gold linguis-
tic annotations) and auto (using automatically an-
notated data) settings. A comparison of the results
shows that there are only minor differences between
them with gold outperforming auto apart from Ara-
bic for which there is a drop of 3.75 points in the
gold setting. However, the small difference between
all results shows that the quality of the automatic an-
notation is good enough for a CR system and that
further improvements in the quality of the linguistic
information will not necessarily improve CR.
If we compare results across languages, we see
that Arabic has the lowest results. One of the rea-
sons for this decreased performance can be found in
the NP-rich syntactic structure of Arabic. This leads
to a high number of identified mentions and in com-
bination with the longer sentence length to a higher92
number of training/test instances. Another reason
for the drop in performance for Arabic can be found
in the lack of annotations expected by our system
(named entities and predicted arguments) that were
not provided by the task due to time constraints and
the accuracy of the annotations. Further, Arabic is
a morphologically rich language for which only the
simplified standard POS tags were provided and not
the gold standard ones that contain much richer and
thus more helpful morphology information.
The results for Chinese and English are relatively
close. We can also see that the CEAFE results are
extremely close, with a difference of less than 1%.
MUC, in contrast, shows the largest differences with
more than 30% between Arabic and English in the
gold setting. It is also noteworthy that the results for
English show a balance between precision and recall
while both Arabic and Chinese favor precision over
recall in terms of mention detection, MUC, and B3.
The reasons for this difference between languages
need to be investigated further.
Gold Mention Boundaries The results for this set
of experiments is based on a version of the test set
that contains the gold boundaries of all mentions, in-
cluding singletons. Thus, we use these gold men-
tion boundaries instead of the ones generated by our
system. These experiments give us an insight on
how well UBIU performs on selecting the correct
boundaries. Since we do not expect the system?s
selection to be perfect, we would expect to see im-
proved system performance given the correct bound-
aries. The results are shown in the second part of
Table 6. As for using automatically generated men-
tions the tendencies in scores between gold and auto
linguistic annotations are kept. A further compari-
son of the overall results between the two settings
also shows only minor changes. The only exception
is the auto setting for Arabic, for which we see drop
in MD precision of approximately 5%. This also re-
sults in lower MUC and B3 precision and CEAFE
recall. The reasons for this drop in performance
need to be investigated further. The fact that most
results for both auto and gold settings change only
sightly shows that having information about the cor-
rect mention boundaries is not very helpful. Thus,
the system seems to have reached its optimal per-
formance on selecting mention boundaries given the
information that it has.
Gold Mentions The last set of experiments is
based on a version of the test set that contains the
gold mentions, i.e., all mentions that are coreferent,
but without any information about the identity of the
coreference chains. The results of this set of exper-
iments gives us information about the quality of the
coreference classifier. The results are shown in the
third part of Table 6. Using gold parses leads to
only minor improvement of the overall system per-
formance, yet, in that case all languages, including
Arabic, show consistent increase of results. Alto-
gether, there is a major improvement of the scores in
MD, MUC, and CEAFE . The B
3 scores only show
minor improvements, resulting from a slight drop in
precision across languages. The results also show
considerably higher precision than recall for MUC
and B3, and higher recall for CEAFE . This means
that the coreference decisions that the system makes
are highly reliable but that it still has a preference
for treating coreferent mentions as singletons.
A comparison across languages shows that pro-
viding gold mentions has a considerable positive ef-
fect on the system performance for Arabic since for
that setting Chinese leads to lower overall scores.
We assume that this is again due to the NP-rich syn-
tactic structure of Arabic and the fact that provid-
ing the mentions decreases drastically the number of
mentions the system works with and has to choose
from during the resolution process.
4 Conclusion and Future Work
We presented the UBIU system for coreference res-
olution in a multilingual setting. The system per-
formed reliably across all three languages of the
CoNLL 2012 shared task. For the future, we are
planning an in-depth investigation of the perfor-
mance of the mention detection module and the sin-
gleton classifier, as well as in investigation into more
complex models for coreference classification than
the mention pair model.
Acknowledgments
This work is based on research supported by the US
Office of Naval Research (ONR) Grant #N00014-
10-1-0140. We would also like to thank Kiran Ku-
mar for his help with tuning the system.93
References
Adriane Boyd, Whitney Gegg-Harrison, and Donna By-
ron. 2005. Identifying non-referential it: A machine
learning approach incorporating linguistically moti-
vated patterns. In Proceedings of the ACL Workshop
on Feature Engineering for Machine Learning in Nat-
ural Language Processing, FeatureEng ?05, pages 40?
47, Ann Arbor, MI.
Samuel Broscheit, Massimo Poesio, Simone Paolo
Ponzetto, Kepa Joseba Rodriguez, Lorenza Romano,
Olga Uryupina, Yannick Versley, and Roberto Zanoli.
2010. BART: A Multilingual Anaphora Resolution
System. In Proceedings of the 5th International Work-
shop on Semantic Evaluation (SemEval), pages 104?
107, Uppsala, Sweden.
Walter Daelemans and Antal van den Bosch. 2005.
Memory-Based Language Processing. Studies in
Natural Language Processing. Cambridge University
Press, Cambridge, UK.
Walter Daelemans, Jakub Zavrel, Ko van der Sloot, and
Antal van den Bosch. 2010. TiMBL: Tilburg Memory
Based Learner, version 6.3,reference guide. Techni-
cal Report ILK 10-01, Induction of Linguistic Knowl-
edge, Computational Linguistics, Tilburg University.
Richard Evans. 2001. Applying machine learning to-
ward an automatic classification of it. Literary and
Linguistic Computing, 16(1):45 ? 57.
Sanda M. Harabagiu and Steven J. Maiorano. 2000.
Multilingual coreference resolution. In Proceedings
of ANLP 2000, Seattle, WA.
Heeyoung Lee, Yves Peirsman, Angel Chang, Nathanael
Chambers, Mihai Surdeanu, and Dan Jurafsky. 2011.
Stanford?s multi-pass sieve coreference resolution sys-
tem at the CoNLL-2011 shared task. In Proceedings
of the Fifteenth Conference on Computational Natural
Language Learning: Shared Task, pages 28?34, Port-
land, OR.
Xiaoqiang Luo and Imed Zitouni. 2005. Multi-
Lingual Coreference Resolution with Syntactic Fea-
tures. In Proceedings of HLT/EMNLP 2005, Vancou-
ver, Canada.
Ruslan Mitkov. 1999. Multilingual anaphora resolution.
Machine Translation, 14(3-4):281?299.
Vincent Ng and Claire Cardie. 2002. Identifying
anaphoric and non-anaphoric noun phrases to improve
coreference resolution. In Proceedings COLING ?02,
pages 1?7, Taipei, Taiwan.
Vincent Ng. 2004. Learning noun phrase anaphoricity to
improve coreference resolution: Issues in representa-
tion and optimization. In Proceedings of the 42nd An-
nual Meeting on Association for Computational Lin-
guistics, ACL ?04, Barcelona, Spain.
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,
Martha Palmer, Ralph Weischedel, and Nianwen Xue.
2011. CoNLL-2011 shared task: Modeling unre-
stricted coreference in OntoNotes. In Proceedings of
CoNLL 2011, Portland, OR.
Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,
Olga Uryupina, and Yuchen Zhang. 2012. CoNLL-
2012 shared task: Modeling multilingual unrestricted
coreference in OntoNotes. In Proceedings of the
Sixteenth Conference on Computational Natural Lan-
guage Learning (CoNLL 2012), Jeju, Korea.
Altaf Rahman and Vincent Ng. 2009. Supervised models
for coreference resolution. In Proceedings of EMNLP,
pages 968?977, Singapore.
Marta Recasens, Llu??s Ma`rquez, Emili Sapena,
M. Anto`nia Mart??, Mariona Taule?, Ve?ronique
Hoste, Massimo Poesio, and Yannick Versley. 2010.
SemEval-2010 task 1: Coreference resolution in
multiple languages. In Proceedings of the 5th Interna-
tional Workshop on Semantic Evaluation, pages 1?8,
Uppsala, Sweden.
Wee Meng Soon, Hwee Tou Ng, and Daniel Chung Yong
Lim. 2001. A machine learning approach to corefer-
ence resolution of noun phrases. Computational Lin-
guistics, 27(4):521?544.
Holger Wunsch. 2009. Rule-Based and Memory-Based
Pronoun Resolution for German: A Comparison and
Assessment of Data Sources. Ph.D. thesis, Universita?t
Tu?bingen.
Desislava Zhekova and Sandra Ku?bler. 2010. UBIU: A
language-independent system for coreference resolu-
tion. In Proceedings of the 5th International Work-
shop on Semantic Evaluation, pages 96?99, Uppsala,
Sweden.
94
Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically Rich Languages, pages 146?182,
Seattle, Washington, USA, 18 October 2013. c?2013 Association for Computational Linguistics
Overview of the SPMRL 2013 Shared Task:
Cross-Framework Evaluation of Parsing Morphologically Rich Languages?
Djam? Seddaha, Reut Tsarfatyb, Sandra K?blerc,
Marie Canditod, Jinho D. Choie, Rich?rd Farkasf , Jennifer Fosterg, Iakes Goenagah,
Koldo Gojenolai, Yoav Goldbergj , Spence Greenk, Nizar Habashl, Marco Kuhlmannm,
Wolfgang Maiern, Joakim Nivreo, Adam Przepi?rkowskip, Ryan Rothq, Wolfgang Seekerr,
Yannick Versleys, Veronika Vinczet, Marcin Wolin?skiu,
Alina Wr?blewskav, Eric Villemonte de la Cl?rgeriew
aU. Paris-Sorbonne/INRIA, bWeizman Institute, cIndiana U., dU. Paris-Diderot/INRIA, eIPsoft Inc., f,tU. of Szeged,
gDublin City U., h,iU. of the Basque Country, jBar Ilan U., kStanford U., l,qColumbia U., m,oUppsala U., nD?sseldorf U.,
p,u,vPolish Academy of Sciences, rStuttgart U., sHeidelberg U., wINRIA
Abstract
This paper reports on the first shared task on
statistical parsing of morphologically rich lan-
guages (MRLs). The task features data sets
from nine languages, each available both in
constituency and dependency annotation. We
report on the preparation of the data sets, on
the proposed parsing scenarios, and on the eval-
uation metrics for parsing MRLs given dif-
ferent representation types. We present and
analyze parsing results obtained by the task
participants, and then provide an analysis and
comparison of the parsers across languages and
frameworks, reported for gold input as well as
more realistic parsing scenarios.
1 Introduction
Syntactic parsing consists of automatically assigning
to a natural language sentence a representation of
its grammatical structure. Data-driven approaches
to this problem, both for constituency-based and
dependency-based parsing, have seen a surge of inter-
est in the last two decades. These data-driven parsing
approaches obtain state-of-the-art results on the de
facto standard Wall Street Journal data set (Marcus et
al., 1993) of English (Charniak, 2000; Collins, 2003;
Charniak and Johnson, 2005; McDonald et al, 2005;
McClosky et al, 2006; Petrov et al, 2006; Nivre et
al., 2007b; Carreras et al, 2008; Finkel et al, 2008;
?Contact authors: djame.seddah@paris-sorbonne.fr,
reut.tsarfaty@weizmann.ac.il, skuebler@indiana.edu
Huang, 2008; Huang et al, 2010; Zhang and Nivre,
2011; Bohnet and Nivre, 2012; Shindo et al, 2012),
and provide a foundation on which many tasks oper-
ating on semantic structure (e.g., recognizing textual
entailments) or even discourse structure (coreference,
summarization) crucially depend.
While progress on parsing English ? the main
language of focus for the ACL community ? has in-
spired some advances on other languages, it has not,
by itself, yielded high-quality parsing for other lan-
guages and domains. This holds in particular for mor-
phologically rich languages (MRLs), where impor-
tant information concerning the predicate-argument
structure of sentences is expressed through word for-
mation, rather than constituent-order patterns as is the
case in English and other configurational languages.
MRLs express information concerning the grammati-
cal function of a word and its grammatical relation to
other words at the word level, via phenomena such
as inflectional affixes, pronominal clitics, and so on
(Tsarfaty et al, 2012c).
The non-rigid tree structures and morphological
ambiguity of input words contribute to the challenges
of parsing MRLs. In addition, insufficient language
resources were shown to also contribute to parsing
difficulty (Tsarfaty et al, 2010; Tsarfaty et al, 2012c,
and references therein). These challenges have ini-
tially been addressed by native-speaking experts us-
ing strong in-domain knowledge of the linguistic
phenomena and annotation idiosyncrasies to improve
the accuracy and efficiency of parsing models. More
146
recently, advances in PCFG-LA parsing (Petrov et al,
2006) and language-agnostic data-driven dependency
parsing (McDonald et al, 2005; Nivre et al, 2007b)
have made it possible to reach high accuracy with
classical feature engineering techniques in addition
to, or instead of, language-specific knowledge. With
these recent advances, the time has come for estab-
lishing the state of the art, and assessing strengths
and weaknesses of parsers across different MRLs.
This paper reports on the first shared task on sta-
tistical parsing of morphologically rich languages
(the SPMRL Shared Task), organized in collabora-
tion with the 4th SPMRL meeting and co-located
with the conference on Empirical Methods in Natural
Language Processing (EMNLP). In defining and exe-
cuting this shared task, we pursue several goals. First,
we wish to provide standard training and test sets for
MRLs in different representation types and parsing
scenarios, so that researchers can exploit them for
testing existing parsers across different MRLs. Sec-
ond, we wish to standardize the evaluation protocol
and metrics on morphologically ambiguous input,
an under-studied challenge, which is also present in
English when parsing speech data or web-based non-
standard texts. Finally, we aim to raise the awareness
of the community to the challenges of parsing MRLs
and to provide a set of strong baseline results for
further improvement.
The task features data from nine, typologically di-
verse, languages. Unlike previous shared tasks on
parsing, we include data in both dependency-based
and constituency-based formats, and in addition to
the full data setup (complete training data), we pro-
vide a small setup (a training subset of 5,000 sen-
tences). We provide three parsing scenarios: one in
which gold segmentation, POS tags, and morphologi-
cal features are provided, one in which segmentation,
POS tags, and features are automatically predicted
by an external resource, and one in which we provide
a lattice of multiple possible morphological analyses
and allow for joint disambiguation of the morpholog-
ical analysis and syntactic structure. These scenarios
allow us to obtain the performance upper bound of
the systems in lab settings using gold input, as well
as the expected level of performance in realistic pars-
ing scenarios ? where the parser follows a morpho-
logical analyzer and is a part of a full-fledged NLP
pipeline.
The remainder of this paper is organized as follows.
We first survey previous work on parsing MRLs (?2)
and provide a detailed description of the present task,
parsing scenarios, and evaluation metrics (?3). We
then describe the data sets for the nine languages
(?4), present the different systems (?5), and empiri-
cal results (?6). Then, we compare the systems along
different axes (?7) in order to analyze their strengths
and weaknesses. Finally, we summarize and con-
clude with challenges to address in future shared
tasks (?8).
2 Background
2.1 A Brief History of the SPMRL Field
Statistical parsing saw initial success upon the avail-
ability of the Penn Treebank (PTB, Marcus et al,
1994). With that large set of syntactically annotated
sentences at their disposal, researchers could apply
advanced statistical modeling and machine learning
techniques in order to obtain high quality structure
prediction. The first statistical parsing models were
generative and based on treebank grammars (Char-
niak, 1997; Johnson, 1998; Klein and Manning, 2003;
Collins, 2003; Petrov et al, 2006; McClosky et al,
2006), leading to high phrase-structure accuracy.
Encouraged by the success of phrase-structure
parsers for English, treebank grammars for additional
languages have been developed, starting with Czech
(Hajic? et al, 2000) then with treebanks of Chinese
(Levy and Manning, 2003), Arabic (Maamouri et
al., 2004b), German (K?bler et al, 2006), French
(Abeill? et al, 2003), Hebrew (Sima?an et al, 2001),
Italian (Corazza et al, 2004), Spanish (Moreno et al,
2000), and more. It quickly became apparent that
applying the phrase-based treebank grammar tech-
niques is sensitive to language and annotation prop-
erties, and that these models are not easily portable
across languages and schemes. An exception to that
is the approach by Petrov (2009), who trained latent-
annotation treebank grammars and reported good
accuracy on a range of languages.
The CoNLL shared tasks on dependency parsing
(Buchholz and Marsi, 2006; Nivre et al, 2007a) high-
lighted the usefulness of an alternative linguistic for-
malism for the development of competitive parsing
models. Dependency relations are marked between
input tokens directly, and allow the annotation of
147
non-projective dependencies that are parseable effi-
ciently. Dependency syntax was applied to the de-
scription of different types of languages (Tesni?re,
1959; Mel?c?uk, 2001), which raised the hope that in
these settings, parsing MRLs will further improve.
However, the 2007 shared task organizers (Nivre
et al, 2007a) concluded that: "[Performance] classes
are more easily definable via language characteris-
tics than via characteristics of the data sets. The
split goes across training set size, original data for-
mat [...], sentence length, percentage of unknown
words, number of dependency labels, and ratio of
(C)POSTAGS and dependency labels. The class
with the highest top scores contains languages with
a rather impoverished morphology." The problems
with parsing MRLs have thus not been solved by de-
pendency parsing, but rather, the challenge has been
magnified.
The first event to focus on the particular challenges
of parsing MRLs was a dedicated panel discussion
co-located with IWPT 2009.1 Work presented on
Hebrew, Arabic, French, and German made it clear
that researchers working on non-English parsing face
the same overarching challenges: poor lexical cover-
age (due to high level of inflection), poor syntactic
coverage (due to more flexible word ordering), and,
more generally, issues of data sparseness (due to
the lack of large-scale resources). Additionally, new
questions emerged as to the evaluation of parsers in
such languages ? are the word-based metrics used
for English well-equipped to capture performance
across frameworks, or performance in the face of
morphological complexity? This event provoked ac-
tive discussions and led to the establishment of a
series of SPMRL events for the discussion of shared
challenges and cross-fertilization among researchers
working on parsing MRLs.
The body of work on MRLs that was accumulated
through the SPMRL workshops2 and hosting ACL
venues contains new results for Arabic (Attia et al,
2010; Marton et al, 2013a), Basque (Bengoetxea
and Gojenola, 2010), Croatian (Agic et al, 2013),
French (Seddah et al, 2010; Candito and Seddah,
2010; Sigogne et al, 2011), German (Rehbein, 2011),
Hebrew (Tsarfaty and Sima?an, 2010; Goldberg and
1http://alpage.inria.fr/iwpt09/panel.en.
html
2See http://www.spmrl.org/ and related workshops.
Elhadad, 2010a), Hindi (Ambati et al, 2010), Ko-
rean (Chung et al, 2010; Choi and Palmer, 2011) and
Spanish (Le Roux et al, 2012), Tamil (Green et al,
2012), amongst others. The awareness of the model-
ing challenges gave rise to new lines of work on top-
ics such as joint morpho-syntactic processing (Gold-
berg and Tsarfaty, 2008), Relational-Realizational
Parsing (Tsarfaty, 2010), EasyFirst Parsing (Gold-
berg, 2011), PLCFRS parsing (Kallmeyer and Maier,
2013), the use of factored lexica (Green et al, 2013),
the use of bilingual data (Fraser et al, 2013), and
more developments that are currently under way.
With new models and data, and with lingering in-
terest in parsing non-standard English data, questions
begin to emerge, such as: What is the realistic per-
formance of parsing MRLs using today?s methods?
How do the different models compare with one an-
other? How do different representation types deal
with parsing one particular language? Does the suc-
cess of a parsing model on a language correlate with
its representation type and learning method? How to
parse effectively in the face of resource scarcity? The
first step to answering all of these questions is pro-
viding standard sets of comparable size, streamlined
parsing scenarios, and evaluation metrics, which are
our main goals in this SPMRL shared task.
2.2 Where We Are At: The Need for
Cross-Framework, Realistic, Evaluation
Procedures
The present task serves as the first attempt to stan-
dardize the data sets, parsing scenarios, and evalu-
ation metrics for MRL parsing, for the purpose of
gaining insights into parsers? performance across lan-
guages. Ours is not the first cross-linguistic task on
statistical parsing. As mentioned earlier, two previ-
ous CoNLL shared tasks focused on cross-linguistic
dependency parsing and covered thirteen different
languages (Buchholz and Marsi, 2006; Nivre et al,
2007a). However, the settings of these tasks, e.g.,
in terms of data set sizes or parsing scenarios, made
it difficult to draw conclusions about strengths and
weaknesses of different systems on parsing MRLs.
A key aspect to consider is the relation between
input tokens and tree terminals. In the standard sta-
tistical parsing setup, every input token is assumed
to be a terminal node in the syntactic parse tree (after
deterministic tokenization of punctuation). In MRLs,
148
morphological processes may have conjoined several
words into a single token. Such tokens need to be seg-
mented and their analyses need to be disambiguated
in order to identify the nodes in the parse tree. In
previous shared tasks on statistical parsing, morpho-
logical information was assumed to be known in ad-
vance in order to make the setup comparable to that
of parsing English. In realistic scenarios, however,
morphological analyses are initially unknown and are
potentially highly ambiguous, so external resources
are used to predict them. Incorrect morphological
disambiguation sets a strict ceiling on the expected
performance of parsers in real-world scenarios. Re-
sults reported for MRLs using gold morphological
information are then, at best, optimistic.
One reason for adopting this less-than-realistic
evaluation scenario in previous tasks has been the
lack of sound metrics for the more realistic scenario.
Standard evaluation metrics assume that the number
of terminals in the parse hypothesis equals the num-
ber of terminals in the gold tree. When the predicted
morphological segmentation leads to a different num-
ber of terminals in the gold and parse trees, standard
metrics such as ParsEval (Black et al, 1991) or At-
tachment Scores (Buchholz and Marsi, 2006) fail
to produce a score. In this task, we use TedEval
(Tsarfaty et al, 2012b), a metric recently suggested
for joint morpho-syntactic evaluation, in which nor-
malized tree-edit distance (Bille, 2005) on morpho-
syntactic trees allows us to quantify the success on
the joint task in realistic parsing scenarios.
Finally, the previous tasks focused on dependency
parsing. When providing both constituency-based
and dependency-based tracks, it is interesting to com-
pare results across these frameworks so as to better
understand the differences in performance between
parsers of different types. We are now faced with
an additional question: how can we compare pars-
ing results across different frameworks? Adopting
standard metrics will not suffice as we would be com-
paring apples and oranges. In contrast, TedEval is
defined for both phrase structures and dependency
structures through the use of an intermediate repre-
sentation called function trees (Tsarfaty et al, 2011;
Tsarfaty et al, 2012a). Using TedEval thus allows us
to explore both dependency and constituency parsing
frameworks and meaningfully compare the perfor-
mance of parsers of different types.
3 Defining the Shared-Task
3.1 Input and Output
We define a parser as a structure prediction function
that maps sequences of space-delimited input tokens
(henceforth, tokens) in a language to a set of parse
trees that capture valid morpho-syntactic structures
in that language. In the case of constituency parsing,
the output structures are phrase-structure trees. In de-
pendency parsing, the output consists of dependency
trees. We use the term tree terminals to refer to the
leaves of a phrase-structure tree in the former case
and to the nodes of a dependency tree in the latter.
We assume that input sentences are represented
as sequences of tokens. In general, there may be a
many-to-many relation between input tokens and tree
terminals. Tokens may be identical to the terminals,
as is often the case in English. A token may be
mapped to multiple terminals assigned their own POS
tags (consider, e.g., the token ?isn?t?), as is the case
in some MRLs. Several tokens may be grouped into
a single (virtual) node, as is the case with multiword
expressions (MWEs) (consider ?pomme de terre? for
?potatoe?). This task covers all these cases.
In the standard setup, all tokens are tree terminals.
Here, the task of a parser is to predict a syntactic
analysis in which the tree terminals coincide with the
tokens. Disambiguating the morphological analyses
that are required for parsing corresponds to selecting
the correct POS tag and possibly a set of morpho-
logical features for each terminal. For the languages
Basque, French, German, Hungarian, Korean, Polish,
and Swedish, we assume this standard setup.
In the morphologically complex setup, every token
may be composed of multiple terminals. In this case,
the task of the parser is to predict the sequence of tree
terminals, their POS tags, and a correct tree associ-
ated with this sequence of terminals. Disambiguating
the morphological analysis therefore requires split-
ting the tokens into segments that define the terminals.
For the Semitic languages Arabic and Hebrew, we
assume this morphologically complex setup.
In the multiword expression (MWEs) setup, pro-
vided here for French only, groupings of terminals
are identified as MWEs (non-terminal nodes in con-
stituency trees, marked heads in dependency trees).
Here, the parser is required to predict how terminals
are grouped into MWEs on top of predicting the tree.
149
3.2 Data Sets
The task features nine languages from six language
families, from Germanic languages (Swedish and
German) and Romance (French) to Slavic (Polish),
Koreanic (Korean), Semitic (Arabic, Hebrew), Uralic
(Hungarian), and the language isolate Basque.
These languages cover a wide range of morpho-
logical richness, with Arabic, Basque, and Hebrew
exhibiting a high degree of inflectional and deriva-
tional morphology. The Germanic languages, Ger-
man and Swedish, have greater degrees of phrasal
ordering freedom than English. While French is not
standardly classified as an MRL, it shares MRLs char-
acteristics which pose challenges for parsing, such as
a richer inflectional system than English.
For each contributing language, we provide two
sets of annotated sentences: one annotated with la-
beled phrase-structure trees, and one annotated with
labeled dependency trees. The sentences in the two
representations are aligned at token and POS levels.
Both representations reflect the predicate-argument
structure of the same sentence, but this information
is expressed using different formal terms and thus
results in different tree structures.
Since some of our native data sets are larger than
others, we provide the training set in two sizes: Full
containing all sentences in the standard training set
of the language, and 5k containing the number of
sentences that is equivalent in size to our smallest
training set (5k sentences). For all languages, the data
has been split into sentences, and the sentences are
parsed and evaluated independently of one another.
3.3 Parsing Scenarios
In the shared task, we consider three parsing scenar-
ios, depending on how much of the morphological
information is provided. The scenarios are listed
below, in increasing order of difficulty.
? Gold: In this scenario, the parser is provided
with unambiguous gold morphological segmen-
tation, POS tags, and morphological features for
each input token.
? Predicted: In this scenario, the parser is pro-
vided with disambiguated morphological seg-
mentation. However, the POS tags and mor-
phological features for each input segment are
unknown.
Scenario Segmentation PoS+Feat. Tree
Gold X X ?
Predicted X 1-best ?
Raw (1-best) 1-best 1-best ?
Raw (all) ? ? ?
Table 1: A summary of the parsing and evaluation sce-
narios. X depicts gold information, ? depicts unknown
information, to be predicted by the system.
? Raw: In this scenario, the parser is provided
with morphologically ambiguous input. The
morphological segmentation, POS tags, and
morphological features for each input token are
unknown.
The Predicted and Raw scenarios require predict-
ing morphological analyses. This may be done using
a language-specific morphological analyzer, or it may
be done jointly with parsing. We provide inputs that
support these different scenarios:
? Predicted: Gold treebank segmentation is given
to the parser. The POS tags assignment and mor-
phological features are automatically predicted
by the parser or by an external resource.
? Raw (1-best): The 1st-best segmentation and
POS tags assignment is predicted by an external
resource and given to the parser.
? Raw (all): All possible segmentations and POS
tags are specified by an external resource. The
parser selects jointly a segmentation and a tree.
An overview of all shown in table 1. For languages
in which terminals equal tokens, only Gold and Pre-
dicted scenarios are considered. For Semitic lan-
guages we further provide input for both Raw (1-
best) and Raw (all) scenarios. 3
3.4 Evaluation Metrics
This task features nine languages, two different repre-
sentation types and three different evaluation scenar-
ios. In order to evaluate the quality of the predicted
structures in the different tracks, we use a combina-
tion of evaluation metrics that allow us to compare
the systems along different axes.
3The raw Arabic lattices were made available later than the
other data. They are now included in the shared task release.
150
In this section, we formally define the different
evaluation metrics and discuss how they support sys-
tem comparison. Throughout this paper, we will be
referring to different evaluation dimensions:
? Cross-Parser Evaluation in Gold/Predicted
Scenarios. Here, we evaluate the results of dif-
ferent parsers on a single data set in the Gold
or Predicted setting. We use standard evalu-
ation metrics for the different types of anal-
yses, that is, ParsEval (Black et al, 1991)
on phrase-structure trees, and Labeled At-
tachment Scores (LAS) (Buchholz and Marsi,
2006) for dependency trees. Since ParsEval is
known to be sensitive to the size and depth of
trees (Rehbein and van Genabith, 2007b), we
also provide the Leaf-Ancestor metric (Samp-
son and Babarczy, 2003), which is less sensitive
to the depth of the phrase-structure hierarchy. In
both scenarios we also provide metrics to evalu-
ate the prediction of MultiWord Expressions.
? Cross-Parser Evaluation in Raw Scenarios.
Here, we evaluate the results of different parsers
on a single data set in scenarios where morpho-
logical segmentation is not known in advance.
When a hypothesized segmentation is not iden-
tical to the gold segmentation, standard evalua-
tion metrics such as ParsEval and Attachment
Scores break down. Therefore, we use TedEval
(Tsarfaty et al, 2012b), which jointly assesses
the quality of the morphological and syntactic
analysis in morphologically-complex scenarios.
? Cross-Framework Evaluation. Here, we com-
pare the results obtained by a dependency parser
and a constituency parser on the same set of sen-
tences. In order to avoid comparing apples and
oranges, we use the unlabeled TedEval metric,
which converts all representation types inter-
nally into the same kind of structures, called
function trees. Here we use TedEval?s cross-
framework protocol (Tsarfaty et al, 2012a),
which accomodates annotation idiosyncrasies.
? Cross-Language Evaluation. Here, we com-
pare parsers for the same representation type
across different languages. Conducting a com-
plete and faithful evaluation across languages
would require a harmonized universal annota-
tion scheme (possibly along the lines of (de
Marneffe and Manning, 2008; McDonald et al,
2013; Tsarfaty, 2013)) or task based evaluation.
As an approximation we use unlabeled TedEval.
Since it is unlabeled, it is not sensitive to label
set size. Since it internally uses function-trees,
it is less sensitive to annotation idiosyncrasies
(e.g., head choice) (Tsarfaty et al, 2011).
The former two dimensions are evaluated on the full
sets. The latter two are evaluated on smaller, compa-
rable, test sets. For completeness, we provide below
the formal definitions and essential modifications of
the evaluation software that we used.
3.4.1 Evaluation Metrics for Phrase Structures
ParsEval The ParsEval metrics (Black et al, 1991)
are evaluation metrics for phrase-structure trees. De-
spite various shortcomings, they are the de-facto stan-
dard for system comparison on phrase-structure pars-
ing, used in many campaigns and shared tasks (e.g.,
(K?bler, 2008; Petrov and McDonald, 2012)). As-
sume that G and H are phrase-structure gold and
hypothesized trees respectively, each of which is rep-
resented by a set of tuples (i, A, j) where A is a
labeled constituent spanning from i to j. Assume
that g is the same as G except that it discards the
root, preterminal, and terminal nodes, likewise for h
and H . The ParsEval scores define the accuracy of
the hypothesis in terms of the normalized size of the
intersection of the constituent sets.
Precision(g, h) = |g?h||h|
Recall(g, h) = |g?h||g|
F1(g, h) = 2?P?RP+R
We evaluate accuracy on phrase-labels ignoring any
further decoration, as it is in standard practices.
Evalb, the standard software that implements Par-
sEval,4 takes a parameter file and ignores the labels
specified therein. As usual, we ignore root and POS
labels. Contrary to the standard practice, we do take
punctuation into account. Note that, as opposed to the
official version, we used the SANCL?2012 version5
modified to actually penalize non-parsed trees.
4http://www.spmrl.org/
spmrl2013-sharedtask-metrics.html/#Evalb
5Modified by Petrov and McDonald (2012) to be less sensi-
tive to punctuation errors.
151
Leaf-Ancestor The Leaf-Ancestor metric (Samp-
son and Babarczy, 2003) measures the similarity be-
tween the path from each terminal node to the root
node in the output tree and the corresponding path
in the gold tree. The path consists of a sequence of
node labels between the terminal node and the root
node, and the similarity of two paths is calculated
by using the Levenshtein distance. This distance is
normalized by path length, and the score of the tree
is an aggregated score of the values for all terminals
in the tree (xt is the leaf-ancestor path of t in tree x).
LA(h, g) =
?
t?yield(g) Lv(ht,gt)/(len(ht)+len(gt))
|yield(g)|
This metric was shown to be less sensitive to dif-
ferences between annotation schemes in (K?bler et
al., 2008), and was shown by Rehbein and van Gen-
abith (2007a) to evaluate trees more faithfully than
ParsEval in the face of certain annotation decisions.
We used the implementation of Wagner (2012).6
3.4.2 Evaluation Metrics for Dependency
Structures
Attachment Scores Labeled and Unlabeled At-
tachment scores have been proposed as evaluation
metrics for dependency parsing in the CoNLL shared
tasks (Buchholz and Marsi, 2006; Nivre et al, 2007a)
and have since assumed the role of standard metrics
in multiple shared tasks and independent studies. As-
sume that g, h are gold and hypothesized dependency
trees respectively, each of which is represented by
a set of arcs (i, A, j) where A is a labeled arc from
terminal i to terminal j. Recall that in the gold and
predicted settings, |g| = |h| (because the number of
terminals determines the number of arcs and hence it
is fixed). So Labeled Attachment Score equals preci-
sion and recall, and it is calculated as a normalized
size of the intersection between the sets of gold and
parsed arcs.7
Precision(g, h) = |g?h||g|
Recall(g, h) = |g?h||h|
LAS(g, h) = |g?h||g| =
|g?h|
|h|
6The original version is available at
http://www.grsampson.net/Resources.
html, ours at http://www.spmrl.org/
spmrl2013-sharedtask-metrics.html/#Leaf.
7http://ilk.uvt.nl/conll/software.html.
3.4.3 Evaluation Metrics for Morpho-Syntactic
Structures
TedEval The TedEval metrics and protocols have
been developed by Tsarfaty et al (2011), Tsarfaty
et al (2012a) and Tsarfaty et al (2012b) for coping
with non-trivial evaluation scenarios, e.g., comparing
parsing results across different frameworks, across
representation theories, and across different morpho-
logical segmentation hypotheses.8 Contrary to the
previous metrics, which view accuracy as a normal-
ized intersection over sets, TedEval computes the ac-
curacy of a parse tree based on the tree-edit distance
between complete trees. Assume a finite set of (pos-
sibly parameterized) edit operations A = {a1....an},
and a cost function c : A ? 1. An edit script is the
cost of a sequence of edit operations, and the edit dis-
tance of g, h is the minimal cost edit script that turns
g into h (and vice versa). The normalized distance
subtracted from 1 provides the level of accuracy on
the task. Formally, the TedEval score on g, h is de-
fined as follows, where ted is the tree-edit distance,
and the |x| (size in nodes) discards terminals and root
nodes.
TedEval(g, h) = 1?
ted(g, h)
|g|+ |h|
In the gold scenario, we are not allowed to manipu-
late terminal nodes, only non-terminals. In the raw
scenarios, we can add and delete both terminals and
non-terminals so as to match both the morphological
and syntactic hypotheses.
3.4.4 Evaluation Metrics for
Multiword-Expression Identification
As pointed out in section 3.1, the French data set is
provided with tree structures encoding both syntactic
information and groupings of terminals into MWEs.
A given MWE is defined as a continuous sequence of
terminals, plus a POS tag. In the constituency trees,
the POS tag of the MWE is an internal node of the
tree, dominating the sequence of pre-terminals, each
dominating a terminal. In the dependency trees, there
is no specific node for the MWE as such (the nodes
are the terminals). So, the first token of a MWE is
taken as the head of the other tokens of the same
MWE, with the same label (see section 4.4).
8http://www.tsarfaty.com/unipar/
download.html.
152
To evaluate performance on MWEs, we use the
following metrics.
? R_MWE, P_MWE, and F_MWE are recall, pre-
cision, and F-score over full MWEs, in which
a predicted MWE counts as correct if it has the
correct span (same group as in the gold data).
? R_MWE +POS, R_MWE +POS, and F_MWE
+POS are defined in the same fashion, except
that a predicted MWE counts as correct if it has
both correct span and correct POS tag.
? R_COMP, R_COMP, and F_COMP are recall,
precision and F-score over non-head compo-
nents of MWEs: a non-head component of MWE
counts as correct if it is attached to the head of
the MWE, with the specific label that indicates
that it is part of an MWE.
4 The SPMRL 2013 Data Sets
4.1 The Treebanks
We provide data from nine different languages anno-
tated with two representation types: phrase-structure
trees and dependency trees.9 Statistics about size,
average length, label set size, and other character-
istics of the treebanks and schemes are provided in
Table 2. Phrase structures are provided in an ex-
tended bracketed style, that is, Penn Treebank brack-
eted style where every labeled node may be extended
with morphological features expressed. Dependency
structures are provided in the CoNLL-X format.10
For any given language, the dependency and con-
stituency treebanks are aligned at the token and ter-
minal levels and share the same POS tagset and mor-
phological features. That is, any form in the CoNLL
format is a terminal of the respective bracketed tree.
Any CPOS label in the CoNLL format is the pre-
terminal dominating the terminal in the bracketed
tree. The FEATS in the CoNLL format are repre-
sented as dash-features decorated on the respective
pre-terminal node in the bracketed tree. See Fig-
ure 1(a)?1(b) for an illustration of this alignment.
9Additionally, we provided the data in TigerXML format
(Brants et al, 2002) for phrase structure trees containing cross-
ing branches. This allows the use of more powerful parsing
formalisms. Unfortunately, we received no submissions for this
data, hence we discard them in the rest of this overview.
10See http://ilk.uvt.nl/conll/.
For ambiguous morphological analyses, we pro-
vide the mapping of tokens to different segmentation
possibilities through lattice files. See Figure 1(c) for
an illustration, where lattice indices mark the start
and end positions of terminals.
For each of the treebanks, we provide a three-way
dev/train/set split and another train set containing the
first 5k sentences of train (5k). This section provides
the details of the original treebanks and their anno-
tations, our data-set preparation, including prepro-
cessing and data splits, cross-framework alignment,
and the prediction of morphological information in
non-gold scenarios.
4.2 The Arabic Treebanks
Arabic is a morphologically complex language which
has rich inflectional and derivational morphology. It
exhibits a high degree of morphological ambiguity
due to the absence of the diacritics and inconsistent
spelling of letters, such as Alif and Ya. As a conse-
quence, the Buckwalter Standard Arabic Morpholog-
ical Analyzer (Buckwalter, 2004; Graff et al, 2009)
produces an average of 12 analyses per word.
Data Sets The Arabic data set contains two tree-
banks derived from the LDC Penn Arabic Treebanks
(PATB) (Maamouri et al, 2004b):11 the Columbia
Arabic Treebank (CATiB) (Habash and Roth, 2009),
a dependency treebank, and the Stanford version
of the PATB (Green and Manning, 2010), a phrase-
structure treebank. We preprocessed the treebanks
to obtain strict token matching between the treebanks
and the morphological analyses. This required non-
trivial synchronization at the tree token level between
the PATB treebank, the CATiB treebank and the mor-
phologically predicted data, using the PATB source
tokens and CATiB feature word form as a dual syn-
chronized pivot.
The Columbia Arabic Treebank The Columbia
Arabic Treebank (CATiB) uses a dependency repre-
sentation that is based on traditional Arabic grammar
and that emphasizes syntactic case relations (Habash
and Roth, 2009; Habash et al, 2007). The CATiB
treebank uses the word tokenization of the PATB
11The LDC kindly provided their latest version of the Arabic
Treebanks. In particular, we used PATB 1 v4.1 (Maamouri et al,
2005), PATB 2 v3.1 (Maamouri et al, 2004a) and PATB 3 v3.3.
(Maamouri et al, 2009)
153
Arabic Basque French German Hebrew Hungarian Korean Polish Swedish
train:
#Sents 15,762 7,577 14,759 40,472 8,146 23,010 6,578
#Tokens 589,220 96,368 443,113 719,532 170,141 351,184 68,424
Lex. Size 36,906 25,136 27,470 77,222 40,782 11,1540 22,911
Avg. Length 37.38 12.71 30.02 17.77 20.88 15.26 10.40
Ratio #NT/#Tokens 0.19 0.82 0.34 0.60 0.59 0.60 0.94
Ratio #NT/#Sents 7.40 10.50 10.33 10.70 12.38 9.27 9.84
#Non Terminals 22 12 32 25 16 8 34
#POS tags 35 25 29 54 16 1,975 29
#total NTs 116,769 79,588 152,463 433,215 100,885 213,370 64,792
Dep. Label Set Size 9 31 25 43 417 22 27
train5k:
#Sents 5,000 5,000 5,000 5,000 5,000 5,000 5,000 5,000 5,000
#Tokens 224,907 61,905 150,984 87,841 128,046 109,987 68,336 52,123 76,357
Lex. Size 19,433 18,405 15,480 17,421 15,975 29,009 29,715 18,632 14,110
Avg. Length 44.98 12.38 30.19 17.56 25.60 21.99 13.66 10.42 15.27
Ratio #NT/#Tokens 0.15 0.83 0.34 0.60 0.42 0.57 0.68 0.94 0.58
Ratio #NT/#Sents 7.18 10.33 10.32 10.58 10.97 12.57 9.29 9.87 8.96
#Non Terminals 22 12 29 23 60 16 8 34 8
#POS Tags 35 25 29 51 50 16 972 29 25
#total NTs 35,909 5,1691 51,627 52,945 54,856 62,889 46,484 49,381 44,845
Dep. Label Set Size 9 31 25 42 43 349 20 27 61
dev:
#Sents 1,985 948 1,235 5,000 500 1,051 2,066 821 494
#Tokens 73,932 13,851 38,820 76,704 11,301 29,989 30,480 8,600 9,341
Lex. Size 12,342 5,551 6,695 15,852 3,175 10,673 15,826 4,467 2,690
Avg. Length 37.24 14.61 31.43 15.34 22.60 28.53 14.75 10.47 18.90
Ratio #NT/#Tokens 0.19 0.74 0.33 0.63 0.47 047 0.63 0.94 0.48
Ratio #NT/#Sents 7.28 10.92 10.48 9.71 10.67 13.66 9.33 9.90 9.10
#Non Terminals 21 11 27 24 55 16 8 31 8
#POS Tags 32 23 29 50 47 16 760 29 24
#total NTs 14,452 10,356 12,951 48,560 5,338 14,366 19,283 8,132 4,496
Dep. Label Set Size 9 31 25 41 42 210 22 26 59
test:
#Sents 1959 946 2541 5000 716 1009 2287 822 666
#Tokens 73878 11457 75216 92004 16998 19908 33766 8545 10690
Lex. Size 12254 4685 10048 20149 4305 7856 16475 4336 3112
Avg. Length 37.71 12.11 29.60 18.40 23.74 19.73 14.76 10.39 16.05
Ratio #NT/#Tokens 0.19 0.83 0.34 0.60 0.47 0.62 0.61 0.95 0.57
Ratio #NT/#Sents 7.45 10.08 10.09 11.07 11.17 12.26 9.02 9.94 9.18
#Non Terminals 22 12 30 23 54 15 8 31 8
#POS Tags 33 22 30 52 46 16 809 27 25
#total NTs 14,610 9,537 25,657 55,398 8,001 12,377 20,640 8,175 6,118
Dep. Label Set Size 9 31 26 42 41 183 22 27 56
Table 2: Overview of participating languages and treebank properties. ?Sents? = number of sentences, ?Tokens? =
number of raw surface forms. ?Lex. size? and ?Avg. Length? are computed in terms of tagged terminals. ?NT? = non-
terminals in constituency treebanks, ?Dep Labels? = dependency labels on the arcs of dependency treebanks. ? A more
comprehensive table is available at http://www.spmrl.org/spmrl2013-sharedtask.html/#Prop.
154
(a) Constituency Tree
% % every line is a single tree in a bracketed Penn Treebank format
(ROOT (S (NP ( NNP-#pers=3|num=sing# John))(VP ( VB-#pers=3|num=sing# likes)(NP ( NNP-#pers=3|num=sing# Mary)))))
(b) Dependency Tree
%% every line describes a terminal: terminal-id form lemma CPOS FPOS FEATS Head Rel PHead PRel
1 John John NNP NNP pers=3|num=sing 2 sbj _ _
2 likes like VB VB pers=3|num=sing 0 root _ _
3 Mary Mary NNP NNP pers=3|num=sing 2 obj _ _
Input Lattice
0 1 2 3 4 5 6
1:AIF/NN
1:AIF/VB
1:AIF/NNT
2:LA/RB
3:NISH/VB
3:NISH/NN
4:L/PREP
4:LHSTIR/VB
4:HSTIR/VB
5:ZAT/PRP
%% every line describes a terminal: start-id end-id form lemma CPOS FPOS FEATS token-id
0 1 AIF AIF NN NN _ 1
0 1 AIF AIF NNT NNT _ 1
0 1 AIF AIF VB VB _ 1
1 2 LA LA RB RB _ 2
2 3 NISH NISH VB VB _ 3
2 3 NISH NISH NN NN _ 3
3 5 LHSTIR HSTIR VB VB _ 4
3 4 L L PREP PREP _ 4
4 5 HSTIR HSTIR VB VB _ 4
5 6 ZAT ZAT PRP PRP _ 5
Figure 1: File formats. Trees (a) and (b) are aligned constituency and dependency trees for a mockup English example.
Boxed labels are shared across the treebanks. Figure (c) shows an ambiguous lattice. The red part represents the yield
of the gold tree. For brevity, we use empty feature columns, but of course lattice arcs may carry any morphological
features, in the FEATS CoNLL format.
and employs a reduced POS tagset consisting of six
tags only: NOM (non-proper nominals including
nouns, pronouns, adjectives and adverbs), PROP
(proper nouns), VRB (active-voice verbs), VRB-
PASS (passive-voice verbs), PRT (particles such as
prepositions or conjunctions) and PNX (punctuation).
(This stands in extreme contrast with the Buckwalter
Arabic tagset (PATB official tagset) which is almost
500 tags.) To obtain these dependency trees, we used
the constituent-to-dependency tool (Habash and Roth,
2009). Additional CATiB trees were annotated di-
rectly, but we only use the portions that are converted
from phrase-structure representation, to ensure that
the constituent and dependency yields can be aligned.
The Stanford Arabic Phrase Structure Treebank
In order to stay compatible with the state of the art,
we provide the constituency data set with most of the
pre-processing steps of Green and Manning (2010),
as they were shown to improve baseline performance
on the PATB parsing considerably.12
To convert the original PATB to preprocessed
phrase-structure trees ? la Stanford, we first discard
all trees dominated by X, which indicates errors and
non-linguistic text. At the phrasal level, we collapse
unary chains with identical categories like NP? NP.
We finally remove all traces, but, unlike Green and
Manning (2010), we keep all function tags.
In the original Stanford instance, the pre-terminal
morphological analyses were mapped to the short-
ened Bies tag set provided with the treebank (where
Determiner markers, ?DT?, were added to definite
noun and adjectives, resulting in 32 POS tags). Here
we use the Kulick tagset (Kulick et al, 2006) for
12Both the corpus split and pre-processing code are available
with the Stanford parser at http://nlp.stanford.edu/
projects/arabic.shtml.
155
pre-terminal categories in the phrase-structure trees,
where the Bies tag set is included as a morphological
feature (stanpos) in our PATB instance.
Adapting the Data to the Shared Task We con-
verted the CATiB representation to the CoNLL rep-
resentation and added a ?split-from-previous? and
?split-from-next? markers as in LDC?s tree-terminal
fields.
A major difference between the CATiB treebank
and the Stanford treebank lies in the way they han-
dle paragraph annotations. The original PATB con-
tains sequences of annotated trees that belong to a
same discourse unit (e.g., paragraph). While the
CATiB conversion tool considers each sequence a
single parsing unit, the Stanford pre-processor treats
each such tree structure rooted at S, NP or Frag as
a tree spanning a single sentence. To be compati-
ble with the predicted morphology data which was
bootstrapped and trained on the CATiB interpretation,
we deterministically modified the original PATB by
adding pseudo XP root nodes, so that the Stanford
pre-proprecessor will generate the same tree yields
as the CATiB treebank.
Another important aspect of preprocessing (often-
delegated as a technicality in the Arabic parsing lit-
erature) is the normalization of token forms. Most
Arabic parsing work used transliterated text based on
the schemes proposed by Buckwalter (2002). The
transliteration schemes exhibit some small differ-
ences, but enough to increase the out-of-vocabulary
rate by a significant margin (on top of strictly un-
known morphemes). This phenomenon is evident in
the morphological analysis lattices (in the predicted
dev set there is a 6% OOV rate without normalization,
and half a point reduction after normalization is ap-
plied, see (Habash et al, 2009b; Green and Manning,
2010)). This rate is much lower for gold tokenized
predicted data (with an OOV rate of only 3.66%,
similar to French for example). In our data set, all
tokens are minimally normalized: no diacritics, no
normalization.13
Data Splits For the Arabic treebanks, we use the
data split recommended by the Columbia Arabic and
Dialect Modeling (CADiM) group (Diab et al, 2013).
13Except for the minimal normalization present in MADA?s
back-end tools. This script was provided to the participants.
The data of the LDC first three annotated Arabic Tree-
banks (ATB1, ATB2 and ATB3) were divided into
roughly a 10/80/10% dev/train/test split by word vol-
ume. When dividing the corpora, document bound-
aries were maintained. The train5k files are simply
the first 5,000 sentences of the training files.
POS Tagsets Given the richness of Arabic mor-
phology, there are multiple POS tag sets and tokeniza-
tion schemes that have been used by researchers, (see,
e.g., Marton et al (2013a)). In the shared task, we fol-
low the standard PATB tokenization which splits off
several categories of orthographic clitics, but not the
definite article Al+. On top of that, we consider three
different POS tag sets with different degrees of gran-
ularity: the Buckwalter tag set (Buckwalter, 2004),
the Kulick Reduced Tag set (Kulick et al, 2006), and
the CATiB tag set (Habash et al, 2009a), considering
that granularity of the morphological analyses may
affect syntactic processing. For more information see
Habash (2010).
Predicted Morphology To prepare input for the
Raw scenarios (?3.3), we used the MADA+TOKAN
system (Habash et al, 2009b). MADA is a system
for morphological analysis and disambiguation of
Arabic. It can predict the 1-best tokenization, POS
tags, lemmas and diacritization in one fell swoop.
The MADA output was also used to generate the
lattice files for the Raw-all scenario.
To generate input for the gold token / predicted
tag input scenario, we used Morfette (Chrupa?a et al,
2008), a joint lemmatization and POS tagging model
based on an averaged perceptron. We generated two
tagging models, one trained with the Buckwalter tag
set, and the other with the Kulick tag set. Both were
mapped back to the CATiB POS tag set such that all
predicted tags are contained in the feature field.14
4.3 The Basque Treebank
Basque is an agglutinative language with a high ca-
pacity to generate inflected wordforms, with free
constituent order of sentence elements with respect
to the main verb. Contrary to many other treebanks,
the Basque treebank was originally annotated with
dependency trees, which were later on converted to
constituency trees.
14A conversion script from the rich Buckwalter tagset to
CoNLL-like features was provided to the participants.
156
The Basque Dependency Treebank (BDT) is a
dependency treebank in its original design, due to
syntactic characteristics of Basque such as its free
word order. Before the syntactic annotation, mor-
phological analysis was performed, using the Basque
morphological analyzer of Aduriz et al (2000). In
Basque each lemma can generate thousands of word-
forms ? differing in morphological properties such
as case, number, tense, or different types of subordi-
nation for verbs. If only POS category ambiguity is
resolved, the analyses remain highly ambiguous.
For the main POS category, there is an average of
1.55 interpretations per wordform, which rises to 2.65
for the full morpho-syntactic information, resulting
in an overall 64% of ambiguous wordforms. The
correct analysis was then manually chosen.
The syntactic trees were manually assigned. Each
word contains its lemma, main POS category, POS
subcategory, morphological features, and the la-
beled dependency relation. Each form indicates mor-
phosyntactic features such as case, number and type
of subordination, which are relevant for parsing.
The first version of the Basque Dependency Tree-
bank, consisting of 3,700 sentences (Aduriz et al,
2003), was used in the CoNLL 2007 Shared Task on
Dependency Parsing (Nivre et al, 2007a). The cur-
rent shared task uses the second version of the BDT,
which is the result of an extension and redesign of the
original requirements, containing 11,225 sentences
(150,000 tokens).
The Basque Constituency Treebank (BCT) was
created as part of the CESS-ECE project, where the
main aim was to obtain syntactically annotated con-
stituency treebanks for Catalan, Spanish and Basque
using a common set of syntactic categories. BCT
was semi-automatically derived from the dependency
version (Aldezabal et al, 2008). The conversion pro-
duced complete constituency trees for 80% of the
sentences. The main bottlenecks have been sentence
connectors and non-projective dependencies which
could not be straightforwardly converted into projec-
tive tree structures, requiring a mechanism similar to
traces in the Penn English Treebank.
Adapting the Data to the Shared Task As the
BCT did not contain all of the original non-projective
dependency trees, we selected the set of 8,000 match-
ing sentences in both treebanks for the shared task.15
This implies that around 2k trees could not be gen-
erated and therefore were discarded. Furthermore,
the BCT annotation scheme does not contain attach-
ment for most of the punctuation marks, so those
were inserted into the BCT using a simple lower-left
attachment heuristic. The same goes for some con-
nectors that could not be aligned in the first phase.
Predicted Morphology In order to obtain pre-
dicted tags for the non-gold scenarios, we used the
following pipeline. First, morphological analysis as
described above was performed, followed by a dis-
ambiguation step. At that point, it is hard to obtain a
single interpretation for each wordform, as determin-
ing the correct interpretation for each wordform may
require knowledge of long-distance elements on top
of the free constituency order of the main phrasal el-
ements in Basque. The disambiguation is performed
by the module by Ezeiza et al (1998), which uses
a combination of knowledge-based disambiguation,
by means of Constraint Grammar (Karlsson et al,
1995; Aduriz et al, 1997), and a posterior statistical
disambiguation module, using an HMM.16
For the shared task data, we chose a setting that
disambiguates most word forms, and retains ? 97%
of the correct interpretations, leaving an ambiguity
level of 1.3 interpretations. For the remaining cases
of ambiguity, we chose the first interpretation, which
corresponds to the most frequent option. This leaves
open the investigation of more complex approaches
for selecting the most appropriate reading.17
4.4 The French Treebank
French is not a morphologically rich language per se,
though its inflectional system is richer than that of
English, and it also exhibits a limited amount of word
order variation occurring at different syntactic levels
including the word level (e.g. pre- or post-nominal
15We generated a 80/10/10 split, ? train/dev/test ? The first 5k
sentences of the train set were used as a basis for the train5k.
16Note that the statistical module can be parametrized accord-
ing to the level of disambiguation to trade off precision and
recall. For example, disambiguation based on the main cate-
gories (abstracting over morpho-syntactic features) maintains
most of the correct interpretations but still gives an output with
several interpretations per wordform.
17This is not an easy task. The ambiguity left is the hardest to
solve given that the knowledge-based and statistical disambigua-
tion processes have not been able to pick out a single reading.
157
adjective, pre- or post-verbal adverbs) and the phrase
level (e.g. possible alternations between post verbal
NPs and PPs). It also has a high degree of multi-
word expressions, that are often ambiguous with a
literal reading as a sequence of simple words. The
syntactic and MWE analysis shows the same kind of
interaction (though to a lesser extent) as morphologi-
cal and syntactic interaction in Semitic languages ?
MWEs help parsing, and syntactic information may
be required to disambiguate MWE identification.
The Data Set The French data sets were gener-
ated from the French Treebank (Abeill? et al, 2003),
which consists of sentences from the newspaper Le
Monde, manually annotated with phrase structures
and morphological information. Part of the treebank
trees are also annotated with grammatical function
tags for dependents of verbs. In the SPMRL shared
task release, we used only this part, consisting of
18,535 sentences,18 split into 14,759 sentences for
training, 1,235 sentences for development, and 2,541
sentences for the final evaluation.19
Adapting the Data to the Shared Task The con-
stituency trees are provided in an extended PTB
bracketed format, with morphological features at the
pre-terminal level only. They contain slight, auto-
matically performed, modifications with respect to
the original trees of the French treebank. The syntag-
matic projection of prepositions and complementiz-
ers was normalized, in order to have prepositions and
complementizers as heads in the dependency trees
(Candito et al, 2010).
The dependency representations are projective de-
pendency trees, obtained through automatic conver-
sion from the constituency trees. The conversion pro-
cedure is an enhanced version of the one described
by Candito et al (2010).
Both the constituency and the dependency repre-
sentations make use of coarse- and fine-grained POS
tags (CPOS and FPOS respectively). The CPOS are
the categories from the original treebank. The FPOS
18The process of functional annotation is still ongoing, the
objective of the FTB providers being to have all the 20000 sen-
tences annotated with functional tags.
19The first 9,981 training sentences correspond to the canoni-
cal 2007 training set. The development set is the same and the
last 1235 sentences of the test set are those of the canonical test
set.
are merged using the CPOS and specific morphologi-
cal information such as verbal mood, proper/common
noun distinction (Crabb? and Candito, 2008).
Multi-Word Expressions The main difference
with respect to previous releases of the bracketed
or dependency versions of the French treebank
lies in the representation of multi-word expressions
(MWEs). The MWEs appear in an extended format:
each MWE bears an FPOS20 and consists of a se-
quence of terminals (hereafter the ?components? of
the MWE), each having their proper CPOS, FPOS,
lemma and morphological features. Note though that
in the original treebank the only gold information
provided for a MWE component is its CPOS. Since
leaving this information blank for MWE components
would have provided a strong cue for MWE recog-
nition, we made sure to provide the same kind of
information for every terminal, whether MWE com-
ponent or not, by providing predicted morphological
features, lemma, and FPOS for MWE components
(even in the ?gold? section of the data set). This infor-
mation was predicted by the Morfette tool (Chrupa?a
et al, 2008), adapted to French (Seddah et al, 2010).
In the constituency trees, each MWE corresponds
to an internal node whose label is the MWE?s FPOS
suffixed by a +, and which dominates the component
pre-terminal nodes.
In the dependency trees, there is no ?node? for a
MWE as a whole, but one node (a terminal in the
CoNLL format) per MWE component. The first com-
ponent of a MWE is taken as the head of the MWE.
All subsequent components of the MWE depend on
the first one, with the special label dep_cpd. Further-
more, the first MWE component bears a feature mwe-
head equal to the FPOS of the MWE. For instance,
the MWE la veille (the day before) is an adverb, con-
taining a determiner component and a common noun
component. Its bracketed representation is (ADV+
(DET la) (NC veille)), and in the dependency repre-
sentation, the noun veille depends on the determiner
la, which bears the feature mwehead=ADV+.
Predicted Morphology For the predicted mor-
phology scenario, we provide data in which the
mwehead has been removed and with predicted
20In the current data, we did not carry along the lemma and
morphological features pertaining to the MWE itself, though this
information is present in the original trees.
158
FPOS, CPOS, lemma, and morphological features,
obtained by training Morfette on the whole train set.
4.5 The German Treebank
German is a fusional language with moderately free
word order, in which verbal elements are fixed in
place and non-verbal elements can be ordered freely
as long as they fulfill the ordering requirements of
the clause (H?hle, 1986).
The Data Set The German constituency data set
is based on the TiGer treebank release 2.2.21 The
original annotation scheme represents discontinuous
constituents such that all arguments of a predicate
are always grouped under a single node regardless of
whether there is intervening material between them
or not (Brants et al, 2002). Furthermore, punctua-
tion and several other elements, such as parentheses,
are not attached to the tree. In order to make the
constituency treebank usable for PCFG parsing, we
adapted this treebank as described shortly.
The conversion of TiGer into dependencies is a
variant of the one by Seeker and Kuhn (2012), which
does not contain empty nodes. It is based on the same
TiGer release as the one used for the constituency
data. Punctuation was attached as high as possible,
without creating any new non-projective edges.
Adapting the Data to the Shared Task For
the constituency version, punctuation and other
unattached elements were first attached to the tree.
As attachment target, we used roughly the respec-
tive least common ancestor node of the right and
left terminal neighbor of the unattached element (see
Maier et al (2012) for details), and subsequently, the
crossing branches were resolved.
This was done in three steps. In the first step, the
head daughters of all nodes were marked using a
simple heuristic. In case there was a daughter with
the edge label HD, this daughter was marked, i.e.,
existing head markings were honored. Otherwise, if
existing, the rightmost daughter with edge label NK
(noun kernel) was marked. Otherwise, as default, the
leftmost daughter was marked. In a second step, for
each continuous part of a discontinuous constituent,
a separate node was introduced. This corresponds
21This version is available from http://www.ims.
uni-stuttgart.de/forschung/ressourcen/
korpora/tiger.html
to the "raising" algorithm described by Boyd (2007).
In a third steps, all those newly introduced nodes
that did not cover the head daughter of the original
discontinuous node were deleted. For the second
and the third step, we used the same script as for the
Swedish constituency data.
Predicted Morphology For the predicted scenario,
a single sequence of POS tags and morphologi-
cal features has been assigned using the MATE
toolchain via a model trained on the train set via cross-
validation on the training set. The MATE toolchain
was used to provide predicted annotation for lem-
mas, POS tags, morphology, and syntax. In order to
achieve the best results for each annotation level, a
10-fold jackknifing was performed to provide realis-
tic features for the higher annotation levels. The pre-
dicted annotation of the 5k training set were copied
from the full data set.22
4.6 The Hebrew Treebank
Modern Hebrew is a Semitic language, characterized
by inflectional and derivational (templatic) morphol-
ogy and relatively free word order. The function
words for from/to/like/and/when/that/the are prefixed
to the next token, causing severe segmentation ambi-
guity for many tokens. In addition, Hebrew orthogra-
phy does not indicate vowels in modern texts, leading
to a very high level of word-form ambiguity.
The Data Set Both the constituency and the de-
pendency data sets are derived from the Hebrew
Treebank V2 (Sima?an et al, 2001; Guthmann et
al., 2009). The treebank is based on just over 6000
sentences from the daily newspaper ?Ha?aretz?, man-
ually annotated with morphological information and
phrase-structure trees and extended with head infor-
mation as described in Tsarfaty (2010, ch. 5). The
unlabeled dependency version was produced by con-
version from the constituency treebank as described
in Goldberg (2011). Both the constituency and depen-
dency trees were annotated with a set grammatical
function labels conforming to Unified Stanford De-
pendencies by Tsarfaty (2013).
22We also provided a predicted-all scenario, in which we
provided morphological analysis lattices with POS and mor-
phological information derived from the analyses of the SMOR
derivational morphology (Schmid et al, 2004). These lattices
were not used by any of the participants.
159
Adapting the Data to the Shared Task While
based on the same trees, the dependency and con-
stituency treebanks differ in their POS tag sets, as
well as in some of the morphological segmentation
decisions. The main effort towards the shared task
was unifying the two resources such that the two tree-
banks share the same lexical yields, and the same
pre-terminal labels. To this end, we took the layering
approach of Goldberg et al (2009), and included two
levels of POS tags in the constituency trees. The
lower level is lexical, conforming to the lexical re-
source used to build the lattices, and is shared by
the two treebanks. The higher level is syntactic, and
follows the tag set and annotation decisions of the
original constituency treebank.23 In addition, we uni-
fied the representation of morphological features, and
fixed inconsistencies and mistakes in the treebanks.
Data Split The Hebrew treebank is one of the
smallest in our language set, and hence it is provided
in only the small (5k) setting. For the sake of com-
parability with the 5k set of the other treebanks, we
created a comparable size of dev/test sets containing
the first and last 500 sentences respectively, where
the rest serve as the 5k training.24
Predicted Morphology The lattices encoding the
morphological ambiguity for the Raw (all) scenario
were produced by looking up the possible analyses
of each input token in the wide-coverage morpholog-
ical analyzer (lexicon) of the Knowledge Center for
Processing Hebrew (Itai and Wintner, 2008; MILA,
2008), with a simple heuristic for dealing with un-
known tokens. A small lattice encoding the possible
analyses of each token was produced separately, and
these token-lattices were concatenated to produce the
sentence lattice. The lattice for a given sentence may
not include the gold analysis in cases of incomplete
lexicon coverage.
The morphologically disambiguated input files for
the Raw (1-best) scenario were produced by run-
ning the raw text through the morphological disam-
23Note that this additional layer in the constituency treebank
adds a relatively easy set of nodes to the trees, thus ?inflating?
the evaluation scores compared to previously reported results.
To compensate, a stricter protocol than is used in this task would
strip one of the two POS layers prior to evaluation.
24This split is slightly different than the split in previous stud-
ies.
biguator (tagger) described in Adler and Elhadad
(2006; Goldberg et al (2008),Adler (2007). The
disambiguator is based on the same lexicon that is
used to produce the lattice files, but utilizes an extra
module for dealing with unknown tokens Adler et al
(2008). The core of the disambiguator is an HMM
tagger trained on about 70M unannotated tokens us-
ing EM, and being supervised by the lexicon.
As in the case of Arabic, we also provided data
for the Predicted (gold token / predicted morphol-
ogy) scenario. We used the same sequence labeler,
Morfette (Chrupa?a et al, 2008), trained on the con-
catenation of POS and morphological gold features,
leading to a model with respectable accuracy.25
4.7 The Hungarian Treebank
Hungarian is an agglutinative language, thus a lemma
can have hundreds of word forms due to derivational
or inflectional affixation (nominal declination and
verbal conjugation). Grammatical information is typ-
ically indicated by suffixes: case suffixes mark the
syntactic relationship between the head and its argu-
ments (subject, object, dative, etc.) whereas verbs
are inflected for tense, mood, person, number, and
the definiteness of the object. Hungarian is also char-
acterized by vowel harmony.26 In addition, there are
several other linguistic phenomena such as causa-
tion and modality that are syntactically expressed in
English but encoded morphologically in Hungarian.
The Data Set The Hungarian data set used in
the shared task is based on the Szeged Treebank,
the largest morpho-syntactic and syntactic corpus
manually annotated for Hungarian. This treebank
is based on newspaper texts and is available in
both constituent-based (Csendes et al, 2005) and
dependency-based (Vincze et al, 2010) versions.
Around 10k sentences of news domain texts were
made available to the shared task.27 Each word is
manually assigned all its possible morpho-syntactic
25POS+morphology prediction accuracy is 91.95% overall
(59.54% for unseen tokens). POS only prediction accuracy is
93.20% overall (71.38% for unseen tokens).
26When vowel harmony applies, most suffixes exist in two
versions ? one with a front vowel and another one with a back
vowel ? and it is the vowels within the stem that determine which
form of the suffix is selected.
27The original treebank contains 82,000 sentences, 1.2 million
words and 250,000 punctuation marks from six domains.
160
tags and lemmas and the appropriate one is selected
according to the context. Sentences were manu-
ally assigned a constituency-based syntactic struc-
ture, which includes information on phrase structure,
grammatical functions (such as subject, object, etc.),
and subcategorization information (i.e., a given NP
is subcategorized by a verb or an infinitive). The
constituency trees were later automatically converted
into dependency structures, and all sentences were
then manually corrected. Note that there exist some
differences in the grammatical functions applied to
the constituency and dependency versions of the tree-
bank, since some morpho-syntactic information was
coded both as a morphological feature and as dec-
oration on top of the grammatical function in the
constituency trees.
Adapting the Data to the Shared Task Origi-
nally, the Szeged Dependency Treebank contained
virtual nodes for elided material (ELL) and phonolog-
ically covert copulas (VAN). In the current version,
they have been deleted, their daughters have been
attached to the parent of the virtual node, and have
been given complex labels, e.g. COORD-VAN-SUBJ,
where VAN is the type of the virtual node deleted,
COORD is the label of the virtual node and SUBJ is
the label of the daughter itself. When the virtual node
was originally the root of the sentence, its daughter
with a predicative (PRED) label has been selected as
the new root of the sentence (with the label ROOT-
VAN-PRED) and all the other daughters of the deleted
virtual node have been attached to it.
Predicted Morphology In order to provide the
same POS tag set for the constituent and dependency
treebanks, we used the dependency POS tagset for
both treebank instances. Both versions of the tree-
bank are available with gold standard and automatic
morphological annotation. The automatic POS tag-
ging was carried out by a 10-fold cross-validation
on the shared task data set by magyarlanc, a natu-
ral language toolkit for processing Hungarian texts
(segmentation, morphological analysis, POS tagging,
and dependency parsing). The annotation provides
POS tags and deep morphological features for each
input token (Zsibrita et al, 2013).28
28The full data sets of both the constituency and de-
pendency versions of the Szeged Treebank are available at
4.8 The Korean Treebank
The Treebank The Korean corpus is generated by
collecting constituent trees from the KAIST Tree-
bank (Choi et al, 1994), then converting the con-
stituent trees to dependency trees using head-finding
rules and heuristics. The KAIST Treebank consists
of about 31K manually annotated constituent trees
from 97 different sources (e.g., newspapers, novels,
textbooks). After filtering out trees containing an-
notation errors, a total of 27,363 trees with 350,090
tokens are collected.
The constituent trees in the KAIST Treebank29 also
come with manually inspected morphological analy-
sis based on ?eojeol?. An eojeol contains root-forms
of word tokens agglutinated with grammatical affixes
(e.g., case particles, ending markers). An eojeol can
consist of more than one word token; for instance, a
compound noun ?bus stop? is often represented as
one eojeol in Korean, ????????????, which can be
broken into two word tokens,???? (bus) and????????
(stop). Each eojeol in the KAIST Treebank is sepa-
rated by white spaces regardless of punctuation. Fol-
lowing the Penn Korean Treebank guidelines (Han
et al, 2002), punctuation is separated as individual
tokens, and parenthetical notations surrounded by
round brackets are grouped into individual phrases
with a function tag (PRN in our corpus).
All dependency trees are automatically converted
from the constituent trees. Unlike English, which
requires complicated head-finding rules to find the
head of each phrase (Choi and Palmer, 2012), Ko-
rean is a head final language such that the rightmost
constituent in each phrase becomes the head of that
phrase. Moreover, the rightmost conjunct becomes
the head of all other conjuncts and conjunctions in
a coordination phrase, which aligns well with our
head-final strategy.
The constituent trees in the KAIST Treebank do
not consist of function tags indicating syntactic or
semantic roles, which makes it difficult to generate
dependency labels. However, it is possible to gener-
ate meaningful labels by using the rich morphology
in Korean. For instance, case particles give good
the following website: www.inf.u-szeged.hu/rgai/
SzegedTreebank, and magyarlanc is downloadable from:
www.inf.u-szeged.hu/rgai/magyarlanc.
29See Lee et al (1997) for more details about the bracketing
guidelines of the KAIST Treebank.
161
indications of what syntactic roles eojeols with such
particles should take. Given this information, 21
dependency labels were generated according to the
annotation scheme proposed by Choi (2013).
Adapting the Data to the Shared Task All details
concerning the adaptation of the KAIST treebank
to the shared task specifications are found in Choi
(2013). Importantly, the rich KAIST treebank tag set
of 1975 POS tag types has been converted to a list of
CoNLL-like feature-attribute values refining coarse
grained POS categories.
Predicted Morphology Two sets of automatic
morphological analyses are provided for this task.
One is generated by the HanNanum morphological
analyzer.30 The HanNanum morphological ana-
lyzer gives the same morphemes and POS tags as the
KAIST Treebank. The other is generated by the Se-
jong morphological analyzer.31 The Sejong morpho-
logical analyzer gives a different set of morphemes
and POS tags as described in Choi and Palmer (2011).
4.9 The Polish Treebank
The Data Set Sk?adnica is a constituency treebank
of Polish (Wolin?ski et al, 2011; S?widzin?ski and
Wolin?ski, 2010). The trees were generated with
a non-probabilistic DCG parser S?wigra and then
disambiguated and validated manually. The ana-
lyzed texts come from the one-million-token sub-
corpus of the National Corpus of Polish (NKJP,
(Przepi?rkowski et al, 2012)) manually annotated
with morpho-syntactic tags.
The dependency version of Sk?adnica is a re-
sult of an automatic conversion of manually disam-
biguated constituent trees into dependency structures
(Wr?blewska, 2012). The conversion was an entirely
automatic process. Conversion rules were based
on morpho-syntactic information, phrasal categories,
and types of phrase-structure rules encoded within
constituent trees. It was possible to extract dependen-
cies because the constituent trees contain information
about the head of the majority of constituents. For
other constituents, heuristics were defined in order to
select their heads.
30http://kldp.net/projects/hannanum
31http://www.sejong.or.kr
The version of Sk?adnica used in the shared task
comprises parse trees for 8,227 sentences.32
Predicted Morphology For the shared task Pre-
dicted scenario, an automatic morphological an-
notation was generated by the PANTERA tagger
(Acedan?ski, 2010).
4.10 The Swedish Treebank
Swedish is moderately rich in inflections, including
a case system. Word order obeys the verb second
constraint in main clauses but is SVO in subordinate
clauses. Main clause order is freer than in English
but not as free as in some other Germanic languages,
such as German. Also, subject agreement with re-
spect to person and number has been dropped in
modern Swedish.
The Data Set The Swedish data sets are taken
from the Talbanken section of the Swedish Treebank
(Nivre and Megyesi, 2007). Talbanken is a syntacti-
cally annotated corpus developed in the 1970s, orig-
inally annotated according to the MAMBA scheme
(Teleman, 1974) with a syntactic layer consisting
of flat phrase structure and grammatical functions.
The syntactic annotation was later automatically con-
verted to full phrase structure with grammatical func-
tions and from that to dependency structure, as de-
scribed by Nivre et al (2006).
Both the phrase structure and the dependency
version use the functional labels from the original
MAMBA scheme, which provides a fine-grained clas-
sification of syntactic functions with 65 different la-
bels, while the phrase structure annotation (which
had to be inferred automatically) uses a coarse set
of only 8 labels. For the release of the Swedish tree-
bank, the POS level was re-annotated to conform to
the current de facto standard for Swedish, which is
the Stockholm-Ume? tagset (Ejerhed et al, 1992)
with 25 base tags and 25 morpho-syntactic features,
which together produce over 150 complex tags.
For the shared task, we used version 1.2 of the
treebank, where a number of conversion errors in
the dependency version have been corrected. The
phrase structure version was enriched by propagating
morpho-syntactic features from preterminals (POS
32Sk?adnica is available from http://zil.ipipan.waw.
pl/Sklicense.
162
tags) to higher non-terminal nodes using a standard
head percolation table, and a version without crossing
branches was derived using the lifting strategy (Boyd,
2007).
Adapting the Data to the Shared Task Explicit
attribute names were added to the feature field and the
split was changed to match the shared task minimal
training set size.
Predicted Morphology POS tags and morpho-
syntactic features were produced using the Hun-
PoS tagger (Hal?csy et al, 2007) trained on the
Stockholm-Ume? Corpus (Ejerhed and K?llgren,
1997).
5 Overview of the Participating Systems
With 7 teams participating, more than 14 systems for
French and 10 for Arabic and German, this shared
task is on par with the latest large-scale parsing evalu-
ation campaign SANCL 2012 (Petrov and McDonald,
2012). The present shared task was extremely de-
manding on our participants. From 30 individuals or
teams who registered and obtained the data sets, we
present results for the seven teams that accomplished
successful executions on these data in the relevant
scenarios in the given the time frame.
5.1 Dependency Track
Seven teams participated in the dependency track.
Two participating systems are based on MaltParser:
MALTOPTIMIZER (Ballesteros, 2013) and AI:KU
(Cirik and S?ensoy, 2013). MALTOPTIMIZER uses
a variant of MaltOptimizer (Ballesteros and Nivre,
2012) to explore features relevant for the processing
of morphological information. AI:KU uses a combi-
nation of MaltParser and the original MaltOptimizer.
Their system development has focused on the inte-
gration of an unsupervised word clustering method
using contextual and morphological properties of the
words, to help combat sparseness.
Similarly to MaltParser ALPAGE:DYALOG
(De La Clergerie, 2013) also uses a shift-reduce
transition-based parser but its training and decoding
algorithms are based on beam search. This parser is
implemented on top of the tabular logic programming
system DyALog. To the best of our knowledge, this
is the first dependency parser capable of handling
word lattice input.
Three participating teams use the MATE parser
(Bohnet, 2010) in their systems: the BASQUETEAM
(Goenaga et al, 2013), IGM:ALPAGE (Constant et
al., 2013) and IMS:SZEGED:CIS (Bj?rkelund et al,
2013). The BASQUETEAM uses the MATE parser in
combination with MaltParser (Nivre et al, 2007b).
The system combines the parser outputs via Malt-
Blender (Hall et al, 2007). IGM:ALPAGE also uses
MATE and MaltParser, once in a pipeline architec-
ture and once in a joint model. The models are com-
bined via a re-parsing strategy based on (Sagae and
Lavie, 2006). This system mainly focuses on MWEs
in French and uses a CRF tagger in combination
with several large-scale dictionaries to handle MWEs,
which then serve as input for the two parsers.
The IMS:SZEGED:CIS team participated in both
tracks, with an ensemble system. For the depen-
dency track, the ensemble includes the MATE parser
(Bohnet, 2010), a best-first variant of the easy-first
parser by Goldberg and Elhadad (2010b), and turbo
parser (Martins et al, 2010), in combination with
a ranker that has the particularity of using features
from the constituent parsed trees. CADIM (Marton et
al., 2013b) uses their variant of the easy-first parser
combined with a feature-rich ensemble of lexical and
syntactic resources.
Four of the participating teams use exter-
nal resources in addition to the parser. The
IMS:SZEGED:CIS team uses external morpholog-
ical analyzers. CADIM uses SAMA (Graff et al,
2009) for Arabic morphology. ALPAGE:DYALOG
and IGM:ALPAGE use external lexicons for French.
IGM:ALPAGE additionally uses Morfette (Chrupa?a
et al, 2008) for morphological analysis and POS
tagging. Finally, as already mentioned, AI:KU clus-
ters words and POS tags in an unsupervised fashion
exploiting additional, un-annotated data.
5.2 Constituency Track
A single team participated in the constituency parsing
task, the IMS:SZEGED:CIS team (Bj?rkelund et al,
2013). Their phrase-structure parsing system uses a
combination of 8 PCFG-LA parsers, trained using a
product-of-grammars procedure (Petrov, 2010). The
50-best parses of this combination are then reranked
by a model based on the reranker by Charniak and
163
Johnson (2005).33
5.3 Baselines
We additionally provide the results of two baseline
systems for the nine languages, one for constituency
parsing and one for dependency parsing.
For the dependency track, our baseline system is
MaltParser in its default configuration (the arc-eager
algorithm and liblinear for training). Results marked
as BASE:MALT in the next two sections report the
results of this baseline system in different scenarios.
The constituency parsing baseline is based on the
most recent version of the PCFG-LA model of Petrov
et al (2006), used with its default settings and five
split/merge cycles, for all languages.34 We use this
parser in two configurations: a ?1-best? configura-
tion where all POS tags are provided to the parser
(predicted or gold, depending on the scenario), and
another configuration in which the parser performs
its own POS tagging. These baselines are referred to
as BASE:BKY+POS and BASE:BKY+RAW respec-
tively in the following results sections. Note that
even when BASE:BKY+POS is given gold POS tags,
the Berkeley parser sometimes fails to reach a perfect
POS accuracy. In cases when the parser cannot find a
parse with the provided POS, it falls back on its own
POS tagging for all tokens.
6 Results
The high number of submitted system variants and
evaluation scenarios in the task resulted in a large
number of evaluation scores. In the following evalu-
ation, we focus on the best run for each participant,
and we aim to provide key points on the different
dimensions of analysis resulting from our evaluation
protocol. We invite our interested readers to browse
the comprehensive representation of our results on
the official shared-task results webpages.35
33Note that a slight but necessary change in the configuration
of one of our metrics, which occurred after the system submis-
sion deadline, resulted in the IMS:SZEGED:CIS team to submit
suboptimal systems for 4 languages. Their final scores are ac-
tually slightly higher and can be found in (Bj?rkelund et al,
2013).
34For Semitic languages, we used the lattice based PCFG-LA
extension by Goldberg (2011).
35http://www.spmrl.org/
spmrl2013-sharedtask-results.html.
6.1 Gold Scenarios
This section presents the parsing results in gold sce-
narios, where the systems are evaluated on gold seg-
mented and tagged input. This means that the se-
quence of terminals, POS tags, and morphological
features are provided based on the treebank anno-
tations. This scenario was used in most previous
shared tasks on data-driven parsing (Buchholz and
Marsi, 2006; Nivre et al, 2007a; K?bler, 2008). Note
that this scenario was not mandatory. We thank our
participants for providing their results nonetheless.
We start by reviewing dependency-based parsing
results, both on the trees and on multi-word expres-
sion, and continue with the different metrics for
constituency-based parsing.
6.1.1 Dependency Parsing
Full Training Set The results for the gold parsing
scenario of dependency parsing are shown in the top
block of table 3.
Among the six systems, IMS:SZEGED:CIS
reaches the highest LAS scores, not only on aver-
age, but for every single language. This shows that
their approach of combining parsers with (re)ranking
provides robust parsing results across languages with
different morphological characteristics. The second
best system is ALPAGE:DYALOG, the third best sys-
tem is MALTOPTIMIZER. The fact that AI:KU is
ranked below the Malt baseline is due to their sub-
mission of results for 6 out of the 9 languages. Simi-
larly, CADIM only submitted results for Arabic and
ranked in the third place for this language, after the
two IMS:SZEGED:CIS runs. IGM:ALPAGE and
BASQUETEAM did not submit results for this setting.
Comparing LAS results across languages is prob-
lematic due to the differences between languages,
treebank size and annotation schemes (see section 3),
so the following discussion is necessarily tentative. If
we consider results across languages, we see that the
lowest results (around 83% for the best performing
system) are reached for Hebrew and Swedish, the
languages with the smallest data sets. The next low-
est result, around 86%, is reached for Basque. Other
languages reach similar LAS scores, around 88-92%.
German, with the largest training set, reaches the
highest LAS, 91.83%.
Interstingly, all systems have high LAS scores
on the Korean Treebank given a training set size
164
team Arabic Basque French German Hebrew Hungarian Korean Polish Swedish avg.
1) gold setting / full training set
IMS:SZEGED:CIS 89.83 86.68 90.29 91.83 83.87 88.06 89.59 89.58 83.97 88.19
ALPAGE:DYALOG 85.87 80.39 87.69 88.25 80.70 79.60 88.23 86.00 79.80 84.06
MALTOPTIMIZER 87.03 82.07 85.71 86.96 80.03 83.14 89.39 80.49 77.67 83.61
BASE:MALT 82.28 69.19 79.86 79.98 76.61 72.34 88.43 77.70 75.73 78.01
AI:KU 86.39 86.98 79.42 83.67 85.16 78.87 55.61
CADIM 85.56 9.51
2) gold setting / 5k training set
IMS:SZEGED:CIS 87.35 85.69 88.73 87.70 83.87 87.21 83.38 89.16 83.97 86.34
ALPAGE:DYALOG 83.25 79.11 85.66 83.88 80.70 78.42 81.91 85.67 79.80 82.04
MALTOPTIMIZER 85.30 81.40 84.93 83.59 80.03 82.37 83.74 79.79 77.67 82.09
BASE:MALT 80.36 67.13 78.16 76.64 76.61 71.27 81.93 76.64 75.73 76.05
AI:KU 84.98 83.47 79.42 82.84 84.37 78.87 54.88
CADIM 82.67 9.19
3) predicted setting / full training set
IMS:SZEGED:CIS 86.21 85.14 85.24 89.65 80.89 86.13 86.62 87.07 82.13 85.45
ALPAGE:DYALOG 81.20 77.55 82.06 84.80 73.63 75.58 81.02 82.56 77.54 79.55
MALTOPTIMIZER 81.90 78.58 79.00 82.75 73.01 79.63 82.65 79.89 75.82 79.25
BASE:MALT 80.36 70.11 77.98 77.81 69.97 70.15 82.06 75.63 73.21 75.25
AI:KU 72.57 82.32 69.01 78.92 81.86 76.35 51.23
BASQUETEAM 84.25 84.51 88.66 84.97 80.88 47.03
IGM:ALPAGE 85.86 9.54
CADIM 83.20 9.24
4) predicted setting / 5k training set
IMS:SZEGED:CIS 83.66 83.84 83.45 85.08 80.89 85.24 80.80 86.69 82.13 83.53
MALTOPTIMIZER 79.64 77.59 77.56 79.22 73.01 79.00 75.90 79.50 75.82 77.47
ALPAGE:DYALOG 78.65 76.06 80.11 73.07 73.63 74.48 73.79 82.04 77.54 76.60
BASE:MALT 78.48 68.12 76.54 74.81 69.97 69.08 74.87 75.29 73.21 73.37
AI:KU 71.23 79.16 69.01 78.04 81.30 76.35 50.57
BASQUETEAM 83.19 82.65 84.70 84.01 80.88 46.16
IGM:ALPAGE 83.60 9.29
CADIM 80.51 8.95
Table 3: Dependency parsing: LAS scores for full and 5k training sets and for gold and predicted input. Results in bold
show the best results per language and setting.
of approximately 23,000 sentences, which is a little
over half of the German treebank. For German, on
the other hand, only the IMS:SZEGED:CIS system
reaches higher LAS scores than for Korean. This
final observation indicates that more than treebank
size is important for comparing system performance
across treebanks. This is the reason for introducing
the reduced set scenario, in which we can see how the
participating system perform on a common ground,
albeit small.
5k Training Set The results for the gold setting
on the 5k train set are shown in the second block
of Table 3. Compared with the full training, we
see that there is a drop of around 2 points in this
setting. Some parser/language pairs are more sensi-
tive to data sparseness than others. CADIM, for in-
stance, exhibit a larger drop than MALTOPTIMIZER
on Arabic, and MALTOPTIMIZER shows a smaller
drop than IMS:SZEGED:CIS on French. On average,
among all systems that covered all languages, MALT-
OPTIMIZER has the smallest drop when moving to
5k training, possibly since the automatic feature opti-
mization may differ for different data set sizes.
Since all languages have the same number of sen-
tences in the train set, these results can give us limited
insight into the parsing complexity of the different
treebanks. Here, French, Arabic, Polish, and Korean
reach the highest LAS scores while Swedish reaches
165
Team F_MWE F_COMP F_MWE+POS
1) gold setting / full training set
AI:KU 99.39 99.53 99.34
IMS:SZEGED:CIS 99.26 99.39 99.21
MALTOPTIMIZER 98.95 98.99 0
ALPAGE:DYALOG 98.32 98.81 0
BASE:MALT 68.7 72.55 68.7
2) predicted setting / full training set
IGM:ALPAGE 80.81 81.18 77.37
IMS:SZEGED:CIS 79.45 80.79 70.48
ALPAGE:DYALOG 77.91 79.25 0
BASQUE-TEAM 77.19 79.81 0
MALTOPTIMIZER 70.29 74.25 0
BASE:MALT 67.49 71.01 0
AI:KU 0 0 0
3) predicted setting / 5k training set
IGM:ALPAGE 77.66 78.68 74.04
IMS:SZEGED:CIS 77.28 78.92 70.42
ALPAGE:DYALOG 75.17 76.82 0
BASQUETEAM 73.07 76.58 0
MALTOPTIMIZER 65.76 70.42 0
BASE:MALT 62.05 66.8 0
AI:KU 0 0 0
Table 4: Dependency Parsing: MWE results
the lowest one. Treebank variance depends not only
on the language but also on annotation decisions,
such as label set (Swedish, interestingly, has a rela-
tively rich one). A more careful comparison would
then take into account the correlation of data size,
label set size and parsing accuracy. We investigate
these correlations further in section 7.1.
6.1.2 Multiword Expressions
MWE results on the gold setting are found at
the top of Table 4. All systems, with the excep-
tion of BASE:MALT, perform exceedingly well in
identifying the spans and non-head components of
MWEs given gold morphology.36 These almost per-
fect scores are the consequence of the presence of
two gold MWE features, namely MWEHEAD and
PRED=Y, which respectively indicate the node span
of the whole MWE and its dependents, which do not
have a gold feature field. The interesting scenario is,
of course, the predicted one, where these features are
not provided to the parser, as in any realistic applica-
tion.
36Note that for the labeled measure F_MWE+POS, both
MALTOPTIMIZER and ALPAGE:DYALOG have an F-score of
zero, since they do not attempt to predict the MWE label at all.
6.1.3 Constituency Parsing
In this part, we provide accuracy results for phrase-
structure trees in terms of ParsEval F-scores. Since
ParsEval is sensitive to the non-terminals-per-word
ratio in the data set (Rehbein and van Genabith,
2007a; Rehbein and van Genabith, 2007b), and given
the fact that this ratio varies greatly within our data
set (as shown in Table 2), it must be kept in mind that
ParsEval should only be used for comparing parsing
performance over treebank instances sharing the ex-
act same properties in term of annotation schemes,
sentence length and so on. When comparing F-Scores
across different treebanks and languages, it can only
provide a rough estimate of the relative difficulty or
ease of parsing these kinds of data.
Full Training Set The F-score results for the gold
scenario are provided in the first block of Table 5.
Among the two baselines, BASE:BKY+POS fares
better than BASE:BKY+RAW since the latter selects
its own POS tags and thus cannot benefit from the
gold information. The IMS:SZEGED:CIS system
clearly outperforms both baselines, with Hebrew as
an outlier.37
As in the dependency case, the results are not
strictly comparable across languages, yet we can
draw some insights from them. We see consider-
able differences between the languages, with Basque,
Hebrew, and Hungarian reaching F-scores in the low
90s for the IMS:SZEGED:CIS system, Korean and
Polish reaching above-average F-scores, and Ara-
bic, French, German, and Swedish reaching F-scores
below the average, but still in the low 80s. The per-
formance is, again, not correlated with data set sizes.
Parsing Hebrew, with one of the smallest training
sets, obtains higher accuracy many other languages,
including Swedish, which has the same training set
size as Hebrew. It may well be that gold morphologi-
cal information is more useful for combatting sparse-
ness in languages with richer morphology (though
Arabic here would be an outlier for this conjecture),
or it may be that certain treebanks and schemes are
inherently harder to parser than others, as we investi-
gate in section 7.
For German, the language with the largest training
37It might be that the easy layer of syntactic tags benefits from
the gold POS tags provided. See section 4 for further discussion
of this layer.
166
team Arabic Basque French German Hebrew Hungarian Korean Polish Swedish avg.
1) gold setting / full training set
IMS:SZEGED:CIS 82.20 90.04 83.98 82.07 91.64 92.60 86.50 88.57 85.09 86.97
BASE:BKY+POS 80.76 76.24 81.76 80.34 92.20 87.64 82.95 88.13 82.89 83.66
BASE:BKY+RAW 79.14 69.78 80.38 78.99 87.32 81.44 73.28 79.51 78.94 78.75
2) gold setting / 5k training set
IMS:SZEGED:CIS 79.47 88.45 82.25 74.78 91.64 91.87 80.10 88.18 85.09 84.65
BASE:BKY+POS 77.54 74.06 78.07 71.37 92.20 86.74 72.85 87.91 82.89 80.40
BASE:BKY+RAW 75.22 67.16 75.91 68.94 87.32 79.34 60.40 78.30 78.94 74.61
3) predicted setting / full training set
IMS:SZEGED:CIS 81.32 87.86 81.83 81.27 89.46 91.85 84.27 87.55 83.99 85.49
BASE:BKY+POS 78.66 74.74 79.76 78.28 85.42 85.22 78.56 86.75 80.64 80.89
BASE:BKY+RAW 79.19 70.50 80.38 78.30 86.96 81.62 71.42 79.23 79.18 78.53
4) predicted setting / 5k training set
IMS:SZEGED:CIS 78.85 86.65 79.83 73.61 89.46 90.53 78.47 87.46 83.99 83.21
BASE:BKY+POS 74.84 72.35 76.19 69.40 85.42 83.82 67.97 87.17 80.64 77.53
BASE:BKY+RAW 74.57 66.75 75.76 68.68 86.96 79.35 58.49 78.38 79.18 74.24
Table 5: Constituent Parsing: ParsEval F-scores for full and 5k training sets and for gold and predicted input. Results in
bold show the best results per language and setting.
set and the highest scores in dependency parsing,
the F-scores are at the lower end. These low scores,
which are obtained despite the larger treebank and
only moderately free word-order, are surprising. This
may be due to case syncretism; gold morphological
information exhibits its own ambiguity and thus may
not be fully utilized.
5k Training Set Parsing results on smaller com-
parable test sets are presented in the second block
of Table 5. On average, IMS:SZEGED:CIS is less
sensitive than BASE:BKY+POS to the reduced size.
Systems are not equally sensitive to reduced training
sets, and the gaps range from 0.4% to 3%, with Ger-
man and Korean as outliers (Korean suffering a 6.4%
drop in F-score and German 7.3%). These languages
have the largest treebanks in the full setting, so it is
not surprising that they suffer the most. But this in
itself does not fully explain the cross-treebank trends.
Since ParsEval scores are known to be sensitive to
the label set sizes and the depth of trees, we provide
LeafAncestor scores in the following section.
6.1.4 Leaf-Ancestor Results
The variation across results in the previous subsec-
tion may have been due to differences across annota-
tion schemes. One way to neutralize this difference
(to some extent) is to use a different metric. We
evaluated the constituency parsing results using the
Leaf-Ancestor (LA) metric, which is less sensitive
to the number of nodes in a tree (Rehbein and van
Genabith, 2007b; K?bler et al, 2008). As shown in
Table 6, these results are on a different (higher) scale
than ParsEval, and the average gap between the full
and 5k setting is lower.
Full Training Set The LA results in gold setting
for full training sets are shown in the first block of Ta-
ble 6. The trends are similar to the ParsEval F-scores.
German and Arabic present the lowest LA scores
(in contrast to the corresponding F-scores, Arabic is
a full point below German for IMS:SZEGED:CIS).
Basque and Hungarian have the highest LA scores.
Hebrew, which had a higher F-score than Basque,
has a lower LA than Basque and is closer to French.
Korean also ranks worse in the LA analysis. The
choice of evaluation metrics thus clearly impacts sys-
tem rankings ? F-scores rank some languages suspi-
ciously high (e.g., Hebrew) due to deeper trees, and
another metric may alleviate that.
5k Training Set The results for the leaf-ancestor
(LA) scores in the gold setting for the 5k training set
are shown in the second block of Table 6. Across
167
team Arabic Basque French German Hebrew Hungarian Korean Polish Swedish avg.
1) gold setting / full training set
IMS:SZEGED:CIS 88.61 94.90 92.51 89.63 92.84 95.01 91.30 94.52 91.46 92.31
BASE:BKY+POS 87.85 91.55 91.74 88.47 92.69 92.52 90.82 92.81 90.76 91.02
BASE:BKY+RAW 87.05 89.71 91.22 87.77 91.29 90.62 87.11 90.58 88.97 89.37
2) gold setting / 5k training set
IMS:SZEGED:CIS 86.68 94.21 91.56 85.74 92.84 94.79 88.87 94.17 91.46 91.15
BASE:BKY+POS 86.26 90.72 89.71 84.11 92.69 92.11 86.75 92.91 90.76 89.56
BASE:BKY+RAW 84.97 88.68 88.74 83.08 91.29 89.94 81.82 90.31 88.97 87.53
3) predicted setting / full training set
IMS:SZEGED:CIS 88.45 94.50 91.79 89.32 91.95 94.90 90.13 94.11 91.05 91.80
BASE:BKY+POS 86.60 90.90 90.96 87.46 89.66 91.72 89.10 92.56 89.51 89.83
BASE:BKY+RAW 86.97 89.91 91.11 87.46 90.77 90.50 86.68 90.48 89.16 89.23
4) predicted setting / 5k training set
IMS:SZEGED:CIS 86.69 93.85 90.76 85.20 91.95 94.05 87.99 93.99 91.05 90.61
BASE:BKY+POS 84.76 89.83 89.18 83.05 89.66 91.24 84.87 92.74 89.51 88.32
BASE:BKY+RAW 84.63 88.50 89.00 82.69 90.77 89.93 81.50 90.08 89.16 87.36
Table 6: Constituent Parsing: Leaf-Ancestor scores for full and 5k training sets and for gold and predicted input.
parsers, IMS:SZEGED:CIS again has a smaller drop
than BASE:BKY+POS on the reduced size. German
suffers the most from the reduction of the training
set, with a loss of approximately 4 points. Korean,
however, which was also severely affected in terms
of F-scores, only loses 1.17 points in the LA score.
On average, the LA seem to reflect a smaller drop
when reducing the training set ? this underscores
again the impact of the choice of metrics on system
evaluation.
6.2 Predicted Scenarios
Gold scenarios are relatively easy since syntactically
relevant morphological information is disambiguated
in advance and is provided as input. Predicted scenar-
ios are more difficult: POS tags and morphological
features have to be automatically predicted, by the
parser or by external resources.
6.2.1 Dependency Parsing
Eight participating teams submitted dependency
results for this scenario. Two teams submitted for a
single language. Four teams covered all languages.
Full Training Set The results for the predicted
scenario in full settings are shown in the third
block of Table 3. Across the board, the re-
sults are considerably lower than the gold sce-
nario. Again, IMS:SZEGED:CIS is the best per-
forming system, followed by ALPAGE:DYALOG and
MALTOPTIMIZER. The only language for which
IMS:SZEGED:CIS is outperformed is French, for
which IGM:ALPAGE reaches higher results (85.86%
vs. 85.24%). This is due to the specialized treatment
of French MWEs in the IGM:ALPAGE system, which
is thereby shown to be beneficial for parsing in the
predicted setting.
If we compare the results for the predicted set-
ting and the gold one, given the full training set,
the IMS:SZEGED:CIS system shows small differ-
ences between 1.5 and 2 percent. The only ex-
ception is French, for which the LAS drops from
90.29% to 85.24% in the predicted setting. The
other systems show somewhat larger differences than
IMS:SZEGED:CIS, with the highest drops for Ara-
bic and Korean. The AI:KU system shows a similar
problem as IMS:SZEGED:CIS for French.
5k Training Set When we consider the predicted
setting for the 5k training set, in the last block of
Table 3, we see the same trends as comparing with
the full training set or when comparing to the gold
setting. Systems suffer from not having gold stan-
dard data, and they suffer from the small training set.
Interestingly, the loss between the different training
set sizes in the predicted setting is larger than in the
168
gold setting, but only marginally so, with a differ-
ence < 0.5. In other words, the predicted setting
adds a challenge to parsing, but it only minimally
compounds data sparsity.
6.2.2 Multiword Expressions Evaluation
In the predicted setting, shown in the second
block of table 4 for the full training set and in the
third block of the same table for the 5k training set,
we see that only two systems, IGM:ALPAGE and
IMS:SZEGED:CIS can predict the MWE label when
it is not present in the training set. IGM:ALPAGE?s
approach of using a separate classifier in combination
with external dictionaries is very successful, reach-
ing an F_MWE+POS score of 77.37. This is com-
pared to the score of 70.48 by IMS:SZEGED:CIS,
which predicts this node label as a side effect of
their constituent feature enriched dependency model
(Bj?rkelund et al, 2013). AI:KU has a zero score
for all predicted settings, which results from an erro-
neous training on the gold data rather than the pre-
dicted data.38
6.2.3 Constituency Parsing
Full Training Set The results for the predicted set-
ting with the full training set are shown in the third
block of table 5. A comparison with the gold setting
shows that all systems have a lower performance in
the predicted scenario, and the differences are in the
range of 0.88 for Arabic and 2.54 for Basque. It is
interesting to see that the losses are generally smaller
than in the dependency framework: on average, the
loss across languages is 2.74 for dependencies and
1.48 for constituents. A possible explanation can be
found in the two-dimensional structure of the con-
stituent trees, where only a subset of all nodes is
affected by the quality of morphology and POS tags.
The exception to this trend is Basque, for which the
loss in constituents is a full point higher than for de-
pendencies. Another possible explanation is that all
of our constituent parsers select their own POS tags
in one way or another. Most dependency parsers ac-
cept predicted tags from an external resource, which
puts an upper-bound on their potential performance.
5k Training Set The results for the predicted set-
ting given the 5k training set are shown in the bottom
38Unofficial updated results are to to be found in (Cirik and
S?ensoy, 2013)
block of table 5. They show the same trends as the
dependency ones: The results are slightly lower than
the results obtained in gold setting and the ones uti-
lizing the full training set.
6.2.4 Leaf Ancestor Metrics
Full Training Set The results for the predicted sce-
nario with a full training set are shown in the third
block of table 6. In the LA evaluation, the loss
in moving from gold morphology are considerably
smaller than in F-scores. For most languages, the
loss is less than 0.5 points. Exceptions are French
with a loss of 0.72, Hebrew with 0.89, and Korean
with 1.17. Basque, which had the highest loss in
F-scores, only shows a minor loss of 0.4 points. Also,
the average loss of 0.41 points is much smaller than
the one in the ParsEval score, 1.48.
5k Training Set The results for the predicted set-
ting given the 5k training set are shown in the last
block of table 6. These results, though considerably
lower (around 3 points), exhibit the exact same trends
as observed in the gold setting.
6.3 Realistic Raw Scenarios
The previous scenarios assume that input surface to-
kens are identical to tree terminals. For languages
such as Arabic and Hebrew, this is not always the
case. In this scenario, we evaluate the capacity of a
system to predict both morphological segmentation
and syntactic parse trees given raw, unsegmented
input tokens. This may be done via a pipeline as-
suming a 1-st best morphological analysis, or jointly
with parsing, assuming an ambiguous morpholog-
ical analysis lattice as input. In this task, both of
these scenarios are possible (see section 3). Thus,
this section presents a realistic evaluation of the par-
ticipating systems, using TedEval, which takes into
account complete morpho-syntactic parses.
Tables 7 and 8 present labeled and unlabeled
TedEval results for both constituency and depen-
dency parsers, calculated only for sentence of length
<= 70.39 We firstly observe that labeled TedEval
scores are considerably lower than unlabeled Ted-
Eval scores, as expected, since unlabeled scores eval-
uate only structural differences. In the labeled setup,
39TedEval builds on algorithms for calculating edit distance
on complete trees (Bille, 2005). In these algorithms, longer
sentences take considerably longer to evaluate.
169
Arabic Arabic Hebrew All
full training set 5k training set
Acc (x100) Ex (%) Acc (x100) Ex (%) Acc (x100) Ex (%) Avg. Soft Avg.
IMS:SZEGED:CIS (Bky) 83.34 1.63 82.54 0.67 56.47 0.67 69.51 69.51
IMS:SZEGED:CIS 89.12 8.37 87.82 5.56 86.08 8.27 86.95 86.95
CADIM 87.81 6.63 86.43 4.21 - - 43.22 86.43
MALTOPTIMIZER 86.74 5.39 85.63 3.03 83.05 5.33 84.34 84.34
ALPAGE:DYALOG 86.60 5.34 85.71 3.54 82.96 6.17 41.48 82.96
ALPAGE:DYALOG (RAW) - - - - 82.82 4.35 41.41 82.82
AI:KU - - - - 78.57 3.37 39.29 78.57
Table 7: Realistic Scenario: Tedeval Labeled Accuracy and Exact Match for the Raw scenario.
The upper part refers to constituency results, the lower part refers to dependency results
Arabic Arabic Hebrew All
full training set 5k training set
Acc (x100) Ex (%) Acc (x100) Ex (%) Acc (x100) Ex (%) Avg. Soft Avg.
IMS:SZEGED:CIS (Bky) 92.06 9.49 91.29 7.13 89.30 13.60 90.30 90.30
IMS:SZEGED:CIS 91.74 9.83 90.85 7.30 89.47 16.97 90.16 90.16
ALPAGE:DYALOG 89.99 7.98 89.46 5.67 88.33 12.20 88.90 88.90
MALTOPTIMIZER 90.09 7.08 89.47 5.56 87.99 11.64 88.73 88.73
CADIM 90.75 8.48 89.89 5.67 - - 44.95 89.89
ALPAGE:DYALOG (RAW) - - - - 87.61 10.24 43.81 87.61
AI:KU - - - - 86.70 8.98 43.35 86.70
Table 8: Realistic Scenario: Tedeval Unlabeled Accuracy and Exact Match for the Raw scenario.
Top upper part refers to constituency results, the lower part refers to dependency results.
the IMS:SZEGED:CIS dependency parser are the
best for both languages and data set sizes. Table 8
shows that their unlabeled constituency results reach
a higher accuracy than the next best system, their
own dependency results. However, a quick look at
the exact match metric reveals lower scores than for
its dependency counterparts.
For the dependency-based joint scenarios, there
is obviously an upper bound on parser performance
given inaccurate segmentation. The transition-based
systems, ALPAGE:DYALOG & MALTOPTIMIZER,
perform comparably on Arabic and Hebrew, with
ALPAGE:DYALOG being slightly better on both lan-
guages. Note that ALPAGE:DYALOG reaches close
results on the 1-best and the lattice-based input set-
tings, with a slight advantage for the former. This is
partly due to the insufficient coverage of the lexical
resource we use: many lattices do not contain the
gold path, so the joint prediction can only as be high
as the lattice predicted path allows.
7 Towards In-Depth Cross-Treebank
Evaluation
Section 6 reported evaluation scores across systems
for different scenarios. However, as noted, these re-
sults are not comparable across languages, represen-
tation types and parsing scenarios due to differences
in the data size, label set size, length of sentences and
also differences in evaluation metrics.
Our following discussion in the first part of this
section highlights the kind of impact that data set
properties have on the standard metrics (label set size
on LAS, non-terminal nodes per sentence on F-score).
Then, in the second part of this section we use the
TedEval cross-experiment protocols for comparative
evaluation that is less sensitive to representation types
and annotation idiosyncrasies.
7.1 Parsing Across Languages and Treebanks
To quantify the impact of treebank characteristics on
parsing parsing accuracy we looked at correlations
of treebank properties with parsing results. The most
highly correlated combinations we have found are
shown in Figures 2, 3, and 4 for the dependency track
and the constituency track (F-score and LeafAnces-
170
21/09/13 03:00SPMRL charts
Page 3 sur 3http://pauillac.inria.fr/~seddah/updated_official.spmrl_results.html
Correlation between label set size, treebank size, and mean LAS
FrP
FrP
GeP
GeP
HuP
HuP
SwP
ArP
ArP
ArG
ArG
BaP
BaP
FrG
FrG
GeG
GeG
HeP
HeG
HuG
HuG
PoP
PoP
PoG
PoG
SwG
BaG
BaG
KoP
KoP
KoG
KoG
10 50 100 500 1 000
72
74
76
78
80
82
84
86
88
90
treebank size / #labels
L
A
S
 
(
%
)
Figure 2: The correlation between treebank size, label set size, and LAS scores. x: treebank size / #labels ; y: LAS (%)
01/10/13 00:43SPMRL charts: all sent.
Page 1 sur 5file:///Users/djame2/=Boulot/=PARSING-FTB/statgram/corpus/SPMRL-S?/SPMRL_FINAL/RESULTS/OFFICIAL/official_ptb-all.spmrl_results.html
SPMRL Results charts (Parseval): Const. Parsing Track (gold tokens, all sent.)
(13/10/01 00:34:34
Arabic Basque French German Hebrew Hungarian Korean Polish Swedish Synthesis
pred/full pred/5k gold/full gold/5k Correlation charts
Correlation between treebank size (#Non terminal), number of sentences (#sent) and mean F1
Arabic
Basque
French
German
Hebrew
Hungarian
Korean
Polish
Swedish
ArP
ArG
BaP
BaG
FrP
FrG
GeP
GeG
HeP
HeG
HuP
HuG
KoP
KoG
PoP
PoG
SwP
SwG
8 9 10
72
74
76
78
80
82
84
86
88
90
92
treebank size (#Non terminal) / #sent
F
1
 
(
%
)
Figure 3: The correlation between the non terminals per sentence ratio and F-scores. x: #non terminal/ #sentence ; y:
F1 (%)
171
tor) respectively.
Figure 2 presents the LAS against the average num-
ber of tokens relative to the number of labels. The
numbers are averaged per language over all partici-
pating systems, and the size of the ?bubbles? is pro-
portional to the number of participants for a given
language setting. We provide ?bubbles? for all lan-
guages in the predicted (-P) and gold (-G) setting,
for both training set sizes. The lower dot in terms
of parsing scores always corresponds to the reduced
training set size.
Figure 2 shows a clear correlation between data-
set complexity and parsing accuracy. The simpler
the data set is (where ?simple" here translates into
large data size with a small set of labels), the higher
the results of the participating systems. The bubbles
reflects a diagonal that indicates correlation between
these dimensions. Beyond that, we see two interest-
ing points off of the diagonal. The Korean treebank
(pink) in the gold setting and full training set can be
parsed with a high LAS relative to its size and label
set. It is also clear that the Hebrew treebank (purple)
in the predicted version is the most difficult one to
parse, relative to our expectation about its complexity.
Since the Hebrew gold scenario is a lot closer to the
diagonal again, it may be that this outlier is due to the
coverage and quality of the predicted morphology.
Figure 340 shows the correlation of data complex-
ity in terms of the average number of non-terminals
per sentence, and parsing accuracy (ParsEval F-
score). Parsing accuracy is again averaged over all
participating systems for a given language. In this
figure, we see a diagonal similar to the one in figure 2,
where Arabic (dark blue) has high complexity of the
data (here interpreted as flat trees, low number of
non terminals per sentence) and low F-scores accord-
ingly. Korean (pink), Swedish (burgundy), Polish
(light green), and Hungarian (light blue) follow, and
then Hebrew (purple) is a positive outlier, possibly
due to an additional layer of ?easy" syntactic POS
nodes which increases tree size and inflates F-scores.
French (orange), Basque (red), and German (dark
green) are negative outliers, falling off the diago-
nal. German has the lowest F-score with respect to
40This figure was created from the IMS:SZEGED:CIS
(Const.) and our own PCFG-LA baseline in POS Tagged mode
(BASE:BKY+POS) so as to avoid the noise introduced by the
parser?s own tagging step (BASE:BKY+RAW).
what would be expected for the non-terminals per
sentence ratio, which is in contrast to the LAS fig-
ure where German occurs among the less complex
data set to parse. A possible explanation may be
the crossing branches in the original treebank which
were re-attached. This creates flat and variable edges
which might be hard predict accurately.
Figure 441 presents the correlation between parsing
accuracy in terms the LeafAncestor metrics (macro
averaged) and treebank complexity in terms of the
average number of non-terminals per sentence. As
in the correlation figures, the parsing accuracy is
averaged over the participanting systems for any lan-
guage. The LeafAncestor accuracy is calculated over
phrase structure trees, and we see a similar diago-
nal to the one in Figure 3 showing that flatter tree-
banks are harder (that is, are correlated with lower
averaged scores) But, its slope is less steep than for
the F-score, which confirms the observation that the
LeafAncestor metric is less sensitive than F-score to
the non-terminals-per-sentence ratio.
Similarly to Figure 3, German is a negative outlier,
which means that this treebank is harder to parse ? it
obtains lower scores on average than we would ex-
pect. As for Hebrew, it is much closer to the diagonal.
As it turns out, the "easy" POS layer that inflates the
scores does not affect the LA ratings as much.
7.2 Evaluation Across Scenarios, Languages
and Treebanks
In this section we analyze the results in cross-
scenario, cross-annotation, and cross-framework set-
tings using the evaluation protocols discussed in
(Tsarfaty et al, 2012b; Tsarfaty et al, 2011; Tsarfaty
et al, 2012a).
As a starting point, we select comparable sections
of the parsed data, based on system runs trained on
the small train set (train5k). For those, we selected
subsets containing the first 5,000 tree terminals (re-
specting sentence boundaries) of the test set. We only
used TedEval on sentences up to 70 terminals long,
and projectivized non-projective sentences in all sets.
We use the TedEval metrics to calculate scores on
both constituency and dependency structures in all
languages and all scenarios. Since the metric de-
fines one scale for all of these different cases, we can
41This figure was created under the same condition as the
F-score correlation in figure (Figure 3).
172
04/10/13 23:05SPMRL charts:
Page 1 sur 6file:///Users/djame2/=Boulot/=PARSING-FTB/statgram/corpus/SPMRL-SHAREDTASK/SPMRL_FINAL/RESULTS/TESTLEAF.spmrl_results.html
SPMRL Results charts (Parseval): Const. Parsing Track (gold tokens, )
(13/10/04 23:05:31
Arabic Basque French German Hebrew Hungarian Korean Polish Swedish
Synthesis
pred/full pred/5k gold/full gold/5k Correlation charts
Correlation between treebank size (#Non terminal), number of sentences (#sent) and mean Leaf Accuracy
ArP
ArG
BaP
BaG
FrP
FrG
GeP
HeP
HeG
HuP
HuG
KoP
KoG
PoP
PoG
SwP
SwG
GeG
8 9 10
74
76
78
80
82
84
86
88
90
92
94
treebank size (#Non terminal) / #sent
F
1
 
(
%
)
Figure 4: The correlation between the non terminals per sentence ratio and Leaf Accuracy (macro) scores. x: #non
terminal/ #sentence ; y: Acc.(%)
compare the performance across annotation schemes,
assuming that those subsets are representative of their
original source.42
Ideally, we would be using labeled TedEval scores,
as the labeled parsing task is more difficult, and la-
beled parses are far more informative than unlabeled
ones. However, most constituency-based parsers do
not provide function labels as part of the output, to
be compared with the dependency arcs. Furthermore,
as mentioned earlier, we observed a huge difference
between label set sizes for the dependency runs. Con-
sequently, labeled scores will not be as informative
across treebanks and representation types. We will
therefore only use labels across scenarios for the
same language and representation type.
42We choose this sample scheme for replicability. We first
tried sampling sentences, aiming at the same average sentence
length (20), but that seemed to create artificially difficult test sets
for languages as Polish and overly simplistic ones for French or
Arabic.
7.2.1 Cross-Scenario Evaluation: raw vs. gold
One novel aspect of this shared task is the evalu-
ation on non-gold segmentation in addition to gold
morphology. One drawback is that the scenarios are
currently not using the same metrics ? the metrics
generally applied for gold and predicted scenrios can-
not apply for raw. To assess how well state of the art
parsers perform in raw scenarios compared to gold
scenarios, we present here TedEval results comparing
raw and gold systems using the evaluation protocol
of Tsarfaty et al (2012b).
Table 9 presents the labeled and unlabeled results
for Arabic and Hebrew (in Full and 5k training set-
tings), and Table 10 presents unlabeled TedEval re-
sults (for all languages) in the gold settings. The
unlabeled TedEval results for the raw settings are
substantially lower then TedEval results on the gold
settings for both languages.
When comparing the unlabeled TedEval results for
Arabic and Hebrew on the participating systems, we
see a loss of 3-4 points between Table 9 (raw) and Ta-
ble 10 (gold). In particular we see that for the best per-
173
forming systems on Arabic (IMS:SZEGED:CIS for
both constituency and dependency), the gap between
gold and realistic scenarios is 3.4 and 4.3 points,
for the constituency and the dependency parser re-
spectively. These results are on a par with results
by Tsarfaty et al (2012b), who showed for different
settings, constituency and dependency based, that
raw scenarios are considerably more difficult to parse
than gold ones on the standard split of the Modern
Hebrew treebank.
For Hebrew, the performance gap between unla-
beled TedEval in raw (Table 9) and gold (Table 10)
is even more salient, with around 7 and 8 points of
difference between the scenarios. We can only specu-
late that such a difference may be due to the difficulty
of resolving Hebrew morpho-syntactic ambiguities
without sufficient syntactic information. Since He-
brew and Arabic now have standardized morpholog-
ically and syntactically analyzed data sets available
through this task, it will be possible to investigate
further how cross-linguistic differences in morpho-
logical ambiguity affect full-parsing accuracy in raw
scenarios.
This section compared the raw and gold parsing
results only on unlabeled TedEval metrics. Accord-
ing to what we have seen so far is expected that
for labeled TedEval metrics using the same protocol,
the gap between gold and raw scenario will be even
greater.
7.2.2 Cross-Framework Evaluation:
Dependency vs. Constituency
In this section, our focus is on comparing parsing
results across constituency and dependency parsers
based on the protocol of Tsarfaty et al (2012a) We
have only one submission from IMS:SZEGED:CIS
in the constituency track, and. from the same group,
a submission on the dependency track. We only com-
pare the IMS:SZEGED:CIS results on constituency
and dependency parsing with the two baselines we
provided. The results of the cross-framework evalua-
tion protocol are shown in Table 11.
The results comparing the two variants of the
IMS:SZEGED:CIS systems show that they are very
close for all languages, with differences ranging from
0.03 for German to 0.8 for Polish in the gold setting.
It has often been argued that dependency parsers
perform better than a constituency parser, but we
notice that when using a cross framework protocol,
such as TedEval, and assuming that our test set sam-
ple is representative, the difference between the in-
terpretation of both representation?s performance is
alleviated. Of course, here the metric is unlabeled, so
it simply tells us that both kind of parsing models are
equally able to provide similar tree structures. Said
differently, the gaps in the quality of predicting the
same underlying structure across representations for
MRLs is not as large as is sometimes assumed.
For most languages, the baseline constituency
parser performs better than the dependency base-
line one, with Basque and Korean as an exception,
and at the same time, the dependency version of
IMS:SZEGED:CIS performs slightly better than their
constituent parser for most languages, with the excep-
tion of Hebrew and Hungarian. It goes to show that,
as far as these present MRL results go, there is no
clear preference for a dependency over a constituency
parsing representation, just preferences among par-
ticular models.
More generally, we can say that even if the linguis-
tic coverage of one theory is shown to be better than
another one, it does not necessarily mean that the
statistical version of the formal theory will perform
better for structure prediction. System performance
is more tightly related to the efficacy of the learning
and search algorithms, and feature engineering on
top of the selected formalism.
7.2.3 Cross-Language Evaluation: All
Languages
We conclude with an overall outlook of the Ted-
Eval scores across all languages. The results on the
gold scenario, for the small training set and the 5k
test set are presented in Table 10. We concentrate
on gold scenarios (to avoid the variation in cover-
age of external morphological analyzers) and choose
unlabeled metrics as they are not sensitive to label
set sizes. We emphasize in bold, for each parsing
system (row in the table), the top two languages that
most accurately parsed by it (boldface) and the two
languages it performed the worse on (italics).
We see that the European languages German
and Hungarian are parsed most accurately in the
constituency-based setup, with Polish and Swedish
having an advantage in dependency parsing. Across
all systems, Korean is the hardest to parse, with Ara-
174
Arabic Hebrew AVG1 SOFT AVG Arabic Hebrew AVG2 SOFT AVG2
1) Constituency Evaluation
Labeled TedEval Unabeled TedEval
IMS:SZEGED:CIS (Bky) 83.59 56.43 70.01 70.01 92.18 88.02 90.1 90.1
2) Dependency Evaluation
Labeled TedEval Unabeled TedEval
IMS:SZEGED:CIS 88.61 84.74 86.68 86.68 91.41 88.58 90 90
ALPAGE:DYALOG 87.20 81.65 40.83 81.65 90.74 87.44 89.09 89.09
CADIM 87.99 - 44 87.99 91.22 - 45.61 91.22
MALTOPTIMIZER 86.62 81.74 43.31 86.62 90.26 87.00 45.13 90.26
ALPAGE:DYALOG (RAW) - 82.82 41.41 82.82 - 87.43 43.72 87.43
AI:KU - 77.8 38.9 77.8 - 85.87 42.94 85.87
Table 9: Labeled and Unlabeled TedEval Results for raw Scenarios, Trained on 5k sentences and tested on 5k terminals.
The upper part refers to constituency parsing and the lower part refers to dependency parsing.
Arabic Basque French German Hebrew Hungarian Korean Polish Swedish
1) Constituency Evaluation
IMS:SZEGED:CIS (Bky) 95.35 96.91 95.98 97.12 96.22 97.92 92.91 97.19 96.65
BASE:BKY+POS 95.11 94.69 95.08 97.01 95.85 97.08 90.55 96.99 96.38
BASE:BKY+RAW 94.58 94.32 94.72 96.74 95.64 96.15 87.08 95.93 95.90
2) Dependency Evaluation
IMS:SZEGED:CIS 95.76 97.63 96.59 96.88 96.29 97.56 94.62 98.01 97.22
ALPAGE:DYALOG 93.76 95.72 95.75 96.4 95.34 95.63 94.56 96.80 96.55
BASE:MALT 94.16 95.08 94.21 94.55 94.98 95.25 94.27 95.83 95.33
AI:KU - - 95.46 96.34 95.07 96.53 - 96.88 95.87
MALTOPTIMIZER 94.91 96.82 95.23 96.32 95.46 96.30 94.69 96.06 95.90
CADIM 94.66 - - - - - - - -
Table 10: Cross-Language Evaluation: Unlabeled TedEval Results in gold input scenario, On a 5k-sentences set set and
a 5k-terminals test set. The upper part refers to constituency parsing and the lower part refers to dependency parsing.
For each system we mark the two top scoring languages in bold and the two lowest scoring languages in italics.
team Arabic Basque French German Hebrew Hungarian Korean Polish Swedish
1) gold setting
IMS:SZEGED:CIS (Bky) 95.82 97.30 96.15 97.43 96.37 98.25 94.07 97.22 96.89
IMS:SZEGED:CIS 95.87 98.06 96.61 97.46 96.31 97.93 94.62 98.04 97.24
BASE:BKY+POS 95.61 95.25 95.48 97.31 96.03 97.53 92.15 96.97 96.66
BASE:MALT 94.26 95.76 94.23 95.53 95.00 96.09 94.27 95.90 95.35
2) predicted setting
IMS:SZEGED:CIS (Bky) 95.74 97.07 96.21 97.31 96.10 98.03 94.05 96.92 96.90
IMS:SZEGED:CIS 95.18 97.67 96.15 97.09 96.22 97.63 94.43 97.50 97.02
BASE:BKY+POS 95.03 95.35 97.12 95.36 97.20 91.34 96.92 96.25
BASE:MALT 95.49 93.84 95.39 94.41 95.72 93.74 96.04 95.09
Table 11: Cross Framework Evaluation: Unlabeled TedEval on generalized gold trees in gold scenario, trained on 5k
sentences and tested on 5k terminals.
bic, Hebrew and to some extent French following. It
appears that on a typological scale, Semitic and Asian
languages are still harder to parse than a range of Eu-
ropean languages in terms of structural difficulty and
complex morpho-syntactic interaction. That said,
note that we cannot tell why certain treebanks appear
more challenging to parse then others, and it is still
unclear whether the difficulty is inherent on the lan-
guage, in the currently available models, or because
of the annotation scheme and treebank consistency.43
43The latter was shown to be an important factor orthogonal
to the morphologically-rich nature of the treebank?s language
175
8 Conclusion
This paper presents an overview of the first shared
task on parsing morphologically rich languages. The
task features nine languages, exhibiting different lin-
guistic phenomena and varied morphological com-
plexity. The shared task saw submissions from seven
teams, and results produced by more than 14 different
systems. The parsing results were obtained in dif-
ferent input scenarios (gold, predicted, and raw) and
evaluated using different protocols (cross-framework,
cross-scenario, and cross-language). In particular,
this is the first time an evaluation campaign reports
on the execution of parsers in realistic, morphologi-
cally ambiguous, setting.
The best performing systems were mostly ensem-
ble systems combining multiple parser outputs from
different frameworks or training runs, or integrat-
ing a state-of-the-art morphological analyzer on top
of a carefully designed feature set. This is con-
sistent with previous shared tasks such as ConLL
2007 or SANCL?2012. However, dealing with am-
biguous morphology is still difficult for all systems,
and a promising approach, as demonstrated by AL-
PAGE:DYALOG, is to deal with parsing and morphol-
ogy jointly by allowing lattice input to the parser. A
promising generalization of this approach would be
the full integration of all levels of analysis that are
mutually informative into a joint model.
The information to be gathered from the results of
this shared task is vast, and we only scratched the
surface with our preliminary analyses. We uncov-
ered and documented insights of strategies that make
parsing systems successful: parser combination is
empirically proven to reach a robust performance
across languages, though language-specific strategies
are still a sound avenue for obtaining high quality
parsers for that individual language. The integration
of morphological analysis into the parsing needs to
be investigated thoroughly, and new approaches that
are morphologically aware need to be developed.
Our cross-parser, cross-scenario, and cross-
framework evaluation protocols have shown that, as
expected, more data is better, and that performance
on gold morphological input is significantly higher
than that in more realistic scenarios. We have shown
that gold morphological information is more help-
(Schluter and van Genabith, 2007)
ful to some languages and parsers than others, and
that it may also interact with successful identification
of multiword expressions. We have shown that dif-
ferences between dependency and constituency are
smaller than previously assumed and that properties
of the learning model and granularity of the output
labels are more influential. Finally, we observed
that languages which are typologically farthest from
English, such as Semitic and Asian languages, are
still amongst the hardest to parse, regardless of the
parsing method used.
Our cross-treebank, in-depth analysis is still pre-
liminary, owing to the limited time between the end
of the shared task and the deadline for publication
of this overview. but we nonetheless feel that our
findings may benefit researchers who aim to develop
parsers for diverse treebanks.44
A shared task is an inspection of the state of the
art, but it may also accelerate research in an area
by providing a stable data basis as well as a set of
strong baselines. The results produced in this task
give a rich picture of the issues associated with pars-
ing MRLs and initial cues towards their resolution.
This set of results needs to be further analyzed to be
fully understood, which will in turn contribute to new
insights. We hope that this shared task will provide
inspiration for the design and evaluation of future
parsing systems for these languages.
Acknowledgments
We heartily thank Miguel Ballesteros and Corentin
Ribeire for running the dependency and constituency
baselines. We warmly thank the Linguistic Data Con-
sortium: Ilya Ahtaridis, Ann Bies, Denise DiPersio,
Seth Kulick and Mohamed Maamouri for releasing
the Arabic Penn Treebank for this shared task and
for their support all along the process. We thank
Alon Itai and MILA, the knowledge center for pro-
cessing Hebrew, for kindly making the Hebrew tree-
bank and morphological analyzer available for us,
Anne Abeill? for allowing us to use the French tree-
bank, and Key-Sun Choi for the Kaist Korean Tree-
bank. We thank Grzegorz Chrupa?a for providing
the morphological analyzer Morfette, and Joachim
44The data set will be made available as soon as possible under
the license distribution of the shared-task, with the exception
of the Arabic data, which will continue to be distributed by the
LDC.
176
Wagner for his LeafAncestor implementation. We
finally thank ?zlem ?etinog?lu, Yuval Marton, Benoit
Crabb? and Benoit Sagot who have been nothing but
supportive during all that time.
At the end of this shared task (though watch out
for further updates and analyses), what remains to be
mentioned is our deep gratitude to all people involved,
either data providers or participants. Without all of
you, this shared task would not have been possible.
References
Anne Abeill?, Lionel Cl?ment, and Fran?ois Toussenel.
2003. Building a treebank for French. In Anne Abeill?,
editor, Treebanks. Kluwer, Dordrecht.
Szymon Acedan?ski. 2010. A Morphosyntactic Brill Tag-
ger for Inflectional Languages. In Advances in Natural
Language Processing, volume 6233 of Lecture Notes
in Computer Science, pages 3?14. Springer-Verlag.
Meni Adler and Michael Elhadad. 2006. An unsupervised
morpheme-based HMM for Hebrew morphological dis-
ambiguation. In Proceedings COLING-ACL, pages
665?672, Sydney, Australia.
Meni Adler, Yoav Goldberg, David Gabay, and Michael
Elhadad. 2008. Unsupervised lexicon-based resolution
of unknown words for full morphological analysis. In
Proceedings of ACL-08: HLT, pages 728?736, Colum-
bus, OH.
Meni Adler. 2007. Hebrew Morphological Disambigua-
tion: An Unsupervised Stochastic Word-based Ap-
proach. Ph.D. thesis, Ben-Gurion University of the
Negev.
Itziar Aduriz, Jos? Mar?a Arriola, Xabier Artola, A D?az
de Ilarraza, et al 1997. Morphosyntactic disambigua-
tion for Basque based on the constraint grammar for-
malism. In Proceedings of RANLP, Tzigov Chark, Bul-
garia.
Itziar Aduriz, Eneko Agirre, Izaskun Aldezabal, I?aki
Alegria, Xabier Arregi, Jose Maria Arriola, Xabier Ar-
tola, Koldo Gojenola, Aitor Maritxalar, Kepa Sarasola,
et al 2000. A word-grammar based morphological
analyzer for agglutinative languages. In Proceedings
of COLING, pages 1?7, Saarbr?cken, Germany.
Itziar Aduriz, Maria Jesus Aranzabe, Jose Maria Arriola,
Aitziber Atutxa, A Diaz de Ilarraza, Aitzpea Garmen-
dia, and Maite Oronoz. 2003. Construction of a
Basque dependency treebank. In Proceedings of the
2nd Workshop on Treebanks and Linguistic Theories
(TLT), pages 201?204, V?xj?, Sweden.
Zeljko Agic, Danijela Merkler, and Dasa Berovic. 2013.
Parsing Croatian and Serbian by using Croatian depen-
dency treebanks. In Proceedings of the Fourth Work-
shop on Statistical Parsing of Morphologically Rich
Languages (SPMRL), Seattle, WA.
I. Aldezabal, M.J. Aranzabe, A. Diaz de Ilarraza, and
K. Fern?ndez. 2008. From dependencies to con-
stituents in the reference corpus for the processing of
Basque. In Procesamiento del Lenguaje Natural, no
41 (2008), pages 147?154. XXIV edici?n del Congreso
Anual de la Sociedad Espa?ola para el Procesamiento
del Lenguaje Natural (SEPLN).
Bharat Ram Ambati, Samar Husain, Joakim Nivre, and
Rajeev Sangal. 2010. On the role of morphosyntactic
features in Hindi dependency parsing. In Proceedings
of the NAACL/HLT Workshop on Statistical Parsing of
Morphologically Rich Languages (SPMRL 2010), Los
Angeles, CA.
Mohammed Attia, Jennifer Foster, Deirdre Hogan,
Joseph Le Roux, Lamia Tounsi, and Josef van Gen-
abith. 2010. Handling unknown words in statistical
latent-variable parsing models for Arabic, English and
French. In Proceedings of the NAACL/HLT Workshop
on Statistical Parsing of Morphologically Rich Lan-
guages (SPMRL), Los Angeles, CA.
Miguel Ballesteros and Joakim Nivre. 2012. MaltOpti-
mizer: An optimization tool for MaltParser. In Pro-
ceedings of EACL, pages 58?62, Avignon, France.
Miguel Ballesteros. 2013. Effective morphological fea-
ture selection with MaltOptimizer at the SPMRL 2013
shared task. In Proceedings of the Fourth Workshop on
Statistical Parsing of Morphologically-Rich Languages,
pages 53?60, Seattle, WA.
Kepa Bengoetxea and Koldo Gojenola. 2010. Appli-
cation of different techniques to dependency parsing
of Basque. In Proceedings of the NAACL/HLT Work-
shop on Statistical Parsing of Morphologically Rich
Languages (SPMRL 2010), Los Angeles, CA.
Philip Bille. 2005. A survey on tree edit distance and re-
lated problems. Theoretical Computer Science, 337(1?
3):217?239, 6.
Anders Bj?rkelund, Ozlem Cetinoglu, Rich?rd Farkas,
Thomas Mueller, and Wolfgang Seeker. 2013.
(Re)ranking meets morphosyntax: State-of-the-art re-
sults from the SPMRL 2013 shared task. In Proceed-
ings of the Fourth Workshop on Statistical Parsing
of Morphologically-Rich Languages, pages 134?144,
Seattle, WA.
Ezra Black, Steven Abney, Dan Flickinger, Claudia
Gdaniec, Ralph Grishman, Philip Harrison, Donald
Hindle, Robert Ingria, Frederick Jelinek, Judith Kla-
vans, Mark Liberman, Mitchell Marcus, Salim Roukos,
Beatrice Santorini, and Tomek Strzalkowski. 1991. A
procedure for quantitatively comparing the syntactic
coverage of English grammars. In Proceedings of the
DARPA Speech and Natural Language Workshop 1991,
pages 306?311, Pacific Grove, CA.
177
Bernd Bohnet and Joakim Nivre. 2012. A transition-
based system for joint part-of-speech tagging and la-
beled non-projective dependency parsing. In Proceed-
ings of the EMNLP-CoNLL, pages 1455?1465, Jeju,
Korea.
Bernd Bohnet. 2010. Top accuracy and fast dependency
parsing is not a contradiction. In Proceedings of COL-
ING, pages 89?97, Beijing, China.
Adriane Boyd. 2007. Discontinuity revisited: An im-
proved conversion to context-free representations. In
Proceedings of the Linguistic Annotation Workshop,
Prague, Czech Republic.
Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolfgang
Lezius, and George Smith. 2002. The TIGER treebank.
In Proceedings of the First Workshop on Treebanks
and Linguistic Theories (TLT), pages 24?41, Sozopol,
Bulgaria.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of CoNLL, pages 149?164, New York, NY.
Tim Buckwalter. 2002. Arabic morphological analyzer
version 1.0. Linguistic Data Consortium.
Tim Buckwalter. 2004. Arabic morphological analyzer
version 2.0. Linguistic Data Consortium.
Marie Candito and Djam? Seddah. 2010. Parsing word
clusters. In Proceedings of the NAACL/HLT Workshop
on Statistical Parsing of Morphologically Rich Lan-
guages (SPMRL 2010), Los Angeles, CA.
Marie Candito, Benoit Crabb?, and Pascal Denis. 2010.
Statistical French dependency parsing: Treebank con-
version and first results. In Proceedings of LREC, Val-
letta, Malta.
Xavier Carreras, Michael Collins, and Terry Koo. 2008.
TAG, dynamic programming, and the perceptron for ef-
ficient, feature-rich parsing. In Proceedings of CoNLL,
pages 9?16, Manchester, UK.
Eugene Charniak and Mark Johnson. 2005. Course-to-
fine n-best-parsing and maxent discriminative rerank-
ing. In Proceedings of ACL, pages 173?180, Barcelona,
Spain.
Eugene Charniak. 1997. Statistical parsing with a context-
free grammar and word statistics. In AAAI/IAAI, pages
598?603.
Eugene Charniak. 2000. A maximum entropy inspired
parser. In Proceedings of NAACL, pages 132?139, Seat-
tle, WA.
Jinho D. Choi and Martha Palmer. 2011. Statistical de-
pendency parsing in Korean: From corpus generation
to automatic parsing. In Proceedings of Second Work-
shop on Statistical Parsing of Morphologically Rich
Languages, pages 1?11, Dublin, Ireland.
Jinho D. Choi and Martha Palmer. 2012. Guidelines
for the Clear Style Constituent to Dependency Conver-
sion. Technical Report 01-12, University of Colorado
at Boulder.
Key-sun Choi, Young S. Han, Young G. Han, and Oh W.
Kwon. 1994. KAIST Tree Bank Project for Korean:
Present and Future Development. In In Proceedings
of the International Workshop on Sharable Natural
Language Resources, pages 7?14, Nara, Japan.
Jinho D. Choi. 2013. Preparing Korean data for the
shared task on parsing morphologically rich languages.
arXiv:1309.1649.
Grzegorz Chrupa?a, Georgiana Dinu, and Josef van Gen-
abith. 2008. Learning morphology with Morfette. In
Proceedings of LREC, Marrakech, Morocco.
Tagyoung Chung, Matt Post, and Daniel Gildea. 2010.
Factors affecting the accuracy of Korean parsing. In
Proceedings of the NAACL/HLT Workshop on Sta-
tistical Parsing of Morphologically Rich Languages
(SPMRL 2010), Los Angeles, CA.
Volkan Cirik and H?sn? S?ensoy. 2013. The AI-KU
system at the SPMRL 2013 shared task: Unsuper-
vised features for dependency parsing. In Proceed-
ings of the Fourth Workshop on Statistical Parsing of
Morphologically-Rich Languages, pages 68?75, Seat-
tle, WA.
Michael Collins. 2003. Head-driven statistical models for
natural language parsing. Computational Linguistics,
29(4):589?637.
Matthieu Constant, Marie Candito, and Djam? Seddah.
2013. The LIGM-Alpage architecture for the SPMRL
2013 shared task: Multiword expression analysis and
dependency parsing. In Proceedings of the Fourth
Workshop on Statistical Parsing of Morphologically-
Rich Languages, pages 46?52, Seattle, WA.
Anna Corazza, Alberto Lavelli, Giogio Satta, and Roberto
Zanoli. 2004. Analyzing an Italian treebank with
state-of-the-art statistical parsers. In Proceedings of
the Third Workshop on Treebanks and Linguistic Theo-
ries (TLT), T?bingen, Germany.
Benoit Crabb? and Marie Candito. 2008. Exp?riences
d?analyse syntaxique statistique du fran?ais. In Actes
de la 15?me Conf?rence sur le Traitement Automatique
des Langues Naturelles (TALN?08), pages 45?54, Avi-
gnon, France.
D?ra Csendes, J?nos Csirik, Tibor Gyim?thy, and Andr?s
Kocsor. 2005. The Szeged treebank. In Proceedings of
the 8th International Conference on Text, Speech and
Dialogue (TSD), Lecture Notes in Computer Science,
pages 123?132, Berlin / Heidelberg. Springer.
Eric De La Clergerie. 2013. Exploring beam-based
shift-reduce dependency parsing with DyALog: Re-
sults from the SPMRL 2013 shared task. In Proceed-
ings of the Fourth Workshop on Statistical Parsing of
178
Morphologically-Rich Languages, pages 81?89, Seat-
tle, WA.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. The stanford typed dependencies repre-
sentation. In Proceedings of the workshop on Cross-
Framework and Cross-Domain Parser Evaluation.
Mona Diab, Nizar Habash, Owen Rambow, and Ryan
Roth. 2013. LDC Arabic treebanks and associated cor-
pora: Data divisions manual. Technical Report CCLS-
13-02, Center for Computational Learning Systems,
Columbia University.
Eva Ejerhed and Gunnel K?llgren. 1997. Stockholm
Ume? Corpus. Version 1.0. Department of Linguis-
tics, Ume? University and Department of Linguistics,
Stockholm University.
Eva Ejerhed, Gunnel K?llgren, Ola Wennstedt, and Mag-
nus ?str?m. 1992. The linguistic annotation system
of the Stockholm?Ume? Corpus project. Technical
Report 33, University of Ume?: Department of Linguis-
tics.
Nerea Ezeiza, I?aki Alegria, Jos? Mar?a Arriola, Rub?n
Urizar, and Itziar Aduriz. 1998. Combining stochastic
and rule-based methods for disambiguation in aggluti-
native languages. In Proceedings of COLING, pages
380?384, Montr?al, Canada.
Jenny Rose Finkel, Alex Kleeman, and Christopher D.
Manning. 2008. Efficient, feature-based, conditional
random field parsing. In Proceedings of ACL, pages
959?967, Columbus, OH.
Alexander Fraser, Helmut Schmid, Rich?rd Farkas, Ren-
jing Wang, and Hinrich Sch?tze. 2013. Knowledge
sources for constituent parsing of German, a morpho-
logically rich and less-configurational language. Com-
putational Linguistics, 39(1):57?85.
Iakes Goenaga, Koldo Gojenola, and Nerea Ezeiza. 2013.
Exploiting the contribution of morphological informa-
tion to parsing: the BASQUE TEAM system in the
SPRML?2013 shared task. In Proceedings of the Fourth
Workshop on Statistical Parsing of Morphologically-
Rich Languages, pages 61?67, Seattle, WA.
Yoav Goldberg and Michael Elhadad. 2010a. Easy-first
dependency parsing of Modern Hebrew. In Proceed-
ings of the NAACL/HLT Workshop on Statistical Pars-
ing of Morphologically Rich Languages (SPMRL 2010),
Los Angeles, CA.
Yoav Goldberg and Michael Elhadad. 2010b. An ef-
ficient algorithm for easy-first non-directional depen-
dency parsing. In Proceedings of HLT: NAACL, pages
742?750, Los Angeles, CA.
Yoav Goldberg and Reut Tsarfaty. 2008. A single frame-
work for joint morphological segmentation and syntac-
tic parsing. In Proceedings of ACL, Columbus, OH.
Yoav Goldberg, Meni Adler, and Michael Elhadad. 2008.
EM can find pretty good HMM POS-taggers (when
given a good start). In Proc. of ACL, Columbus, OH.
Yoav Goldberg, Reut Tsarfaty, Meni Adler, and Michael
Elhadad. 2009. Enhancing unlexicalized parsing per-
formance using a wide coverage lexicon, fuzzy tag-set
mapping, and EM-HMM-based lexical probabilities. In
Proceedings of EALC, pages 327?335, Athens, Greece.
Yoav Goldberg. 2011. Automatic syntactic processing of
Modern Hebrew. Ph.D. thesis, Ben Gurion University
of the Negev.
David Graff, Mohamed Maamouri, Basma Bouziri, Son-
dos Krouna, Seth Kulick, and Tim Buckwalter. 2009.
Standard Arabic Morphological Analyzer (SAMA) ver-
sion 3.1. Linguistic Data Consortium LDC2009E73.
Spence Green and Christopher D. Manning. 2010. Better
Arabic parsing: Baselines, evaluations, and analysis.
In Proceedings of COLING, pages 394?402, Beijing,
China.
Nathan Green, Loganathan Ramasamy, and Zden?k
?abokrtsk?. 2012. Using an SVM ensemble system for
improved Tamil dependency parsing. In Proceedings
of the ACL 2012 Joint Workshop on Statistical Pars-
ing and Semantic Processing of Morphologically Rich
Languages, pages 72?77, Jeju, Korea.
Spence Green, Marie-Catherine de Marneffe, and Christo-
pher D. Manning. 2013. Parsing models for identify-
ing multiword expressions. Computational Linguistics,
39(1):195?227.
Noemie Guthmann, Yuval Krymolowski, Adi Milea, and
Yoad Winter. 2009. Automatic annotation of morpho-
syntactic dependencies in a Modern Hebrew Treebank.
In Proceedings of the Eighth International Workshop on
Treebanks and Linguistic Theories (TLT), Groningen,
The Netherlands.
Nizar Habash and Ryan Roth. 2009. CATiB: The
Columbia Arabic Treebank. In Proceedings of ACL-
IJCNLP, pages 221?224, Suntec, Singapore.
Nizar Habash, Ryan Gabbard, Owen Rambow, Seth
Kulick, and Mitch Marcus. 2007. Determining case in
Arabic: Learning complex linguistic behavior requires
complex linguistic features. In Proceedings of EMNLP-
CoNLL, pages 1084?1092, Prague, Czech Republic.
Nizar Habash, Reem Faraj, and Ryan Roth. 2009a. Syn-
tactic Annotation in the Columbia Arabic Treebank. In
Proceedings of MEDAR International Conference on
Arabic Language Resources and Tools, Cairo, Egypt.
Nizar Habash, Owen Rambow, and Ryan Roth. 2009b.
MADA+TOKAN: A toolkit for Arabic tokenization,
diacritization, morphological disambiguation, POS tag-
ging, stemming and lemmatization. In Proceedings of
the Second International Conference on Arabic Lan-
guage Resources and Tools. Cairo, Egypt.
179
Nizar Habash. 2010. Introduction to Arabic Natural
Language Processing. Morgan & Claypool Publishers.
Jan Hajic?, Alena B?hmov?, Eva Hajic?ov?, and Barbora
Vidov?-Hladk?. 2000. The Prague Dependency Tree-
bank: A three-level annotation scenario. In Anne
Abeill?, editor, Treebanks: Building and Using Parsed
Corpora. Kluwer Academic Publishers.
P?ter Hal?csy, Andr?s Kornai, and Csaba Oravecz. 2007.
HunPos ? an open source trigram tagger. In Proceed-
ings of ACL, pages 209?212, Prague, Czech Republic.
Johan Hall, Jens Nilsson, Joakim Nivre, G?ls?en Eryig?it,
Be?ta Megyesi, Mattias Nilsson, and Markus Saers.
2007. Single malt or blended? A study in multilingual
parser optimization. In Proceedings of the CoNLL
Shared Task Session of EMNLP-CoNLL 2007, pages
933?939, Prague, Czech Republic.
Chung-hye Han, Na-Rae Han, Eon-Suk Ko, Martha
Palmer, and Heejong Yi. 2002. Penn Korean Treebank:
Development and evaluation. In Proceedings of the
16th Pacific Asia Conference on Language, Information
and Computation, Jeju, Korea.
Tilman H?hle. 1986. Der Begriff "Mittelfeld", Anmerkun-
gen ?ber die Theorie der topologischen Felder. In Ak-
ten des Siebten Internationalen Germanistenkongresses
1985, pages 329?340, G?ttingen, Germany.
Zhongqiang Huang, Mary Harper, and Slav Petrov. 2010.
Self-training with products of latent variable grammars.
In Proceedings of EMNLP, pages 12?22, Cambridge,
MA.
Liang Huang. 2008. Forest reranking: Discriminative
parsing with non-local features. In Proceedings of ACL,
pages 586?594, Columbus, OH.
Alon Itai and Shuly Wintner. 2008. Language resources
for Hebrew. Language Resources and Evaluation,
42(1):75?98, March.
Mark Johnson. 1998. PCFG models of linguistic tree
representations. Computational Linguistics, 24(4):613?
632.
Laura Kallmeyer and Wolfgang Maier. 2013. Data-driven
parsing using probabilistic linear context-free rewriting
systems. Computational Linguistics, 39(1).
Fred Karlsson, Atro Voutilainen, Juha Heikkilae, and Arto
Anttila. 1995. Constraint Grammar: a language-
independent system for parsing unrestricted text. Wal-
ter de Gruyter.
Dan Klein and Christopher D. Manning. 2003. Accurate
unlexicalized parsing. In Proceedings of ACL, pages
423?430, Sapporo, Japan.
Sandra K?bler, Erhard W. Hinrichs, and Wolfgang Maier.
2006. Is it really that difficult to parse German? In Pro-
ceedings of EMNLP, pages 111?119, Sydney, Australia,
July.
Sandra K?bler, Wolfgang Maier, Ines Rehbein, and Yan-
nick Versley. 2008. How to compare treebanks. In
Proceedings of LREC, pages 2322?2329, Marrakech,
Morocco.
Sandra K?bler. 2008. The PaGe 2008 shared task on
parsing German. In Proceedings of the Workshop on
Parsing German, pages 55?63, Columbus, OH.
Seth Kulick, Ryan Gabbard, and Mitch Marcus. 2006.
Parsing the Arabic Treebank: Analysis and Improve-
ments. In Proceedings of the Treebanks and Linguistic
Theories Conference, pages 31?42, Prague, Czech Re-
public.
Joseph Le Roux, Benoit Sagot, and Djam? Seddah. 2012.
Statistical parsing of Spanish and data driven lemmati-
zation. In Proceedings of the Joint Workshop on Statis-
tical Parsing and Semantic Processing of Morphologi-
cally Rich Languages, pages 55?61, Jeju, Korea.
Kong Joo Lee, Byung-Gyu Chang, and Gil Chang Kim.
1997. Bracketing Guidelines for Korean Syntactic Tree
Tagged Corpus. Technical Report CS/TR-97-112, De-
partment of Computer Science, KAIST.
Roger Levy and Christopher D. Manning. 2003. Is it
harder to parse Chinese, or the Chinese treebank? In
Proceedings of ACL, Sapporo, Japan.
Mohamed Maamouri, Ann Bies, Tim Buckwalter, and
Hubert Jin. 2004a. Arabic Treebank: Part 2 v 2.0.
LDC catalog number LDC2004T02.
Mohamed Maamouri, Ann Bies, Tim Buckwalter, and
Wigdan Mekki. 2004b. The Penn Arabic Treebank:
Building a large-scale annotated Arabic corpus. In
NEMLAR Conference on Arabic Language Resources
and Tools, pages 102?109, Cairo, Egypt.
Mohamed Maamouri, Ann Bies, Tim Buckwalter, and
Hubert Jin. 2005. Arabic Treebank: Part 1 v 3.0. LDC
catalog number LDC2005T02.
Mohamed Maamouri, Ann Bies, Seth Kulick, Fatma Gad-
deche, Wigdan Mekki, Sondos Krouna, and Basma
Bouziri. 2009. The Penn Arabic Treebank part 3 ver-
sion 3.1. Linguistic Data Consortium LDC2008E22.
Wolfgang Maier, Miriam Kaeshammer, and Laura
Kallmeyer. 2012. Data-driven PLCFRS parsing re-
visited: Restricting the fan-out to two. In Proceedings
of the Eleventh International Conference on Tree Ad-
joining Grammars and Related Formalisms (TAG+11),
Paris, France.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn TreeBank. Computational
Linguistics, 19(2):313?330.
Andre Martins, Noah Smith, Eric Xing, Pedro Aguiar,
and Mario Figueiredo. 2010. Turbo parsers: Depen-
dency parsing by approximate variational inference. In
Proceedings of EMNLP, pages 34?44, Cambridge, MA.
180
Yuval Marton, Nizar Habash, and Owen Rambow. 2013a.
Dependency parsing of Modern Standard Arabic with
lexical and inflectional features. Computational Lin-
guistics, 39(1):161?194.
Yuval Marton, Nizar Habash, Owen Rambow, and Sarah
Alkhulani. 2013b. SPMRL?13 shared task system:
The CADIM Arabic dependency parser. In Proceed-
ings of the Fourth Workshop on Statistical Parsing of
Morphologically-Rich Languages, pages 76?80, Seat-
tle, WA.
David McClosky, Eugene Charniak, and Mark Johnson.
2006. Effective self-training for parsing. In Proceed-
ings of HLT:NAACL, pages 152?159, New York, NY.
Ryan T. McDonald, Koby Crammer, and Fernando C. N.
Pereira. 2005. Online large-margin training of depen-
dency parsers. In Proceedings of ACL, pages 91?98,
Ann Arbor, MI.
Ryan McDonald, Joakim Nivre, Yvonne Quirmbach-
Brundage, Yoav Goldberg, Dipanjan Das, Kuzman
Ganchev, Keith Hall, Slav Petrov, Hao Zhang, Oscar
Tackstrom, Claudia Bedini, Nuria Bertomeu Castello,
and Jungmee Lee. 2013. Universal dependency anno-
tation for multilingual parsing. In Proceedings of ACL,
Sofia, Bulgaria.
Igor Mel?c?uk. 2001. Communicative Organization in Nat-
ural Language: The Semantic-Communicative Struc-
ture of Sentences. J. Benjamins.
Knowledge Center for Processing Hebrew
MILA. 2008. Hebrew morphological analyzer.
http://mila.cs.technion.ac.il.
Antonio Moreno, Ralph Grishman, Susana Lopez, Fer-
nando Sanchez, and Satoshi Sekine. 2000. A treebank
of Spanish and its application to parsing. In Proceed-
ings of LREC, Athens, Greece.
Joakim Nivre and Be?ta Megyesi. 2007. Bootstrapping a
Swedish treeebank using cross-corpus harmonization
and annotation projection. In Proceedings of the 6th
International Workshop on Treebanks and Linguistic
Theories, pages 97?102, Bergen, Norway.
Joakim Nivre, Jens Nilsson, and Johan Hall. 2006. Tal-
banken05: A Swedish treebank with phrase structure
and dependency annotation. In Proceedings of LREC,
pages 1392?1395, Genoa, Italy.
Joakim Nivre, Johan Hall, Sandra K?bler, Ryan McDon-
ald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret.
2007a. The CoNLL 2007 shared task on dependency
parsing. In Proceedings of the CoNLL Shared Task Ses-
sion of EMNLP-CoNLL 2007, pages 915?932, Prague,
Czech Republic.
Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev,
G?ls?en Eryig?it, Sandra K?bler, Svetoslav Marinov,
and Erwin Marsi. 2007b. MaltParser: A language-
independent system for data-driven dependency pars-
ing. Natural Language Engineering, 13(2):95?135.
Slav Petrov and Ryan McDonald. 2012. Overview of the
2012 Shared Task on Parsing the Web. In Proceedings
of the First Workshop on Syntactic Analysis of Non-
Canonical Language (SANCL), a NAACL-HLT 2012
workshop, Montreal, Canada.
Slav Petrov, Leon Barrett, Romain Thibaux, and Dan
Klein. 2006. Learning accurate, compact, and inter-
pretable tree annotation. In Proceedings of COLING-
ACL, Sydney, Australia.
Slav Petrov. 2009. Coarse-to-Fine Natural Language
Processing. Ph.D. thesis, University of California at
Bekeley, Berkeley, CA.
Slav Petrov. 2010. Products of random latent variable
grammars. In Proceedings of HLT: NAACL, pages 19?
27, Los Angeles, CA.
Adam Przepi?rkowski, Miros?aw Ban?ko, Rafa? L. G?rski,
and Barbara Lewandowska-Tomaszczyk, editors. 2012.
Narodowy Korpus Jkezyka Polskiego. Wydawnictwo
Naukowe PWN, Warsaw.
Ines Rehbein and Josef van Genabith. 2007a. Eval-
uating Evaluation Measures. In Proceedings of the
16th Nordic Conference of Computational Linguistics
NODALIDA-2007, Tartu, Estonia.
Ines Rehbein and Josef van Genabith. 2007b. Treebank
annotation schemes and parser evaluation for German.
In Proceedings of EMNLP-CoNLL, Prague, Czech Re-
public.
Ines Rehbein. 2011. Data point selection for self-training.
In Proceedings of the Second Workshop on Statistical
Parsing of Morphologically Rich Languages, pages 62?
67, Dublin, Ireland.
Kenji Sagae and Alon Lavie. 2006. Parser combination
by reparsing. In Proceedings of HLT-NAACL, pages
129?132, New York, NY.
Geoffrey Sampson and Anna Babarczy. 2003. A test of
the leaf-ancestor metric for parse accuracy. Natural
Language Engineering, 9(04):365?380.
Natalie Schluter and Josef van Genabith. 2007. Prepar-
ing, restructuring, and augmenting a French Treebank:
Lexicalised parsers or coherent treebanks? In Proc. of
PACLING 07, Melbourne, Australia.
Helmut Schmid, Arne Fitschen, and Ulrich Heid. 2004.
SMOR: A German computational morphology covering
derivation, composition and inflection. In Proceedings
of LREC, Lisbon, Portugal.
Djam? Seddah, Grzegorz Chrupa?a, Ozlem Cetinoglu,
Josef van Genabith, and Marie Candito. 2010.
Lemmatization and statistical lexicalized parsing of
morphologically-rich languages. In Proceedings of the
First Workshop on Statistical Parsing of Morphologi-
cally Rich Languages (SPMRL), Los Angeles, CA.
Wolfgang Seeker and Jonas Kuhn. 2012. Making el-
lipses explicit in dependency conversion for a German
181
treebank. In Proceedings of LREC, pages 3132?3139,
Istanbul, Turkey.
Hiroyuki Shindo, Yusuke Miyao, Akinori Fujino, and
Masaaki Nagata. 2012. Bayesian symbol-refined tree
substitution grammars for syntactic parsing. In Pro-
ceedings of ACL, pages 440?448, Jeju, Korea.
Anthony Sigogne, Matthieu Constant, and Eric Laporte.
2011. French parsing enhanced with a word clustering
method based on a syntactic lexicon. In Proceedings
of the Second Workshop on Statistical Parsing of Mor-
phologically Rich Languages, pages 22?27, Dublin,
Ireland.
Khalil Sima?an, Alon Itai, Yoad Winter, Alon Altmann,
and Noa Nativ. 2001. Building a tree-bank of Modern
Hebrew text. Traitement Automatique des Langues,
42:347?380.
Marek S?widzin?ski and Marcin Wolin?ski. 2010. Towards
a bank of constituent parse trees for Polish. In Pro-
ceedings of Text, Speech and Dialogue, pages 197?204,
Brno, Czech Republic.
Ulf Teleman. 1974. Manual f?r grammatisk beskrivning
av talad och skriven svenska. Studentlitteratur.
Lucien Tesni?re. 1959. ?l?ments De Syntaxe Structurale.
Klincksieck, Paris.
Reut Tsarfaty and Khalil Sima?an. 2010. Modeling mor-
phosyntactic agreement in constituency-based parsing
of Modern Hebrew. In Proceedings of the First Work-
shop on Statistical Parsing of Morphologically Rich
Languages (SPMRL), Los Angeles, CA.
Reut Tsarfaty, Djame Seddah, Yoav Goldberg, Sandra
K?bler, Marie Candito, Jennifer Foster, Yannick Vers-
ley, Ines Rehbein, and Lamia Tounsi. 2010. Statistical
parsing for morphologically rich language (SPMRL):
What, how and whither. In Proceedings of the First
workshop on Statistical Parsing of Morphologically
Rich Languages (SPMRL), Los Angeles, CA.
Reut Tsarfaty, Joakim Nivre, and Evelina Andersson.
2011. Evaluating dependency parsing: Robust and
heuristics-free cross-framework evaluation. In Pro-
ceedings of EMNLP, Edinburgh, UK.
Reut Tsarfaty, Joakim Nivre, and Evelina Andersson.
2012a. Cross-framework evaluation for statistical pars-
ing. In Proceeding of EACL, Avignon, France.
Reut Tsarfaty, Joakim Nivre, and Evelina Andersson.
2012b. Joint evaluation for segmentation and parsing.
In Proceedings of ACL, Jeju, Korea.
Reut Tsarfaty, Djam? Seddah, Sandra K?bler, and Joakim
Nivre. 2012c. Parsing morphologically rich languages:
Introduction to the special issue. Computational Lin-
guistics, 39(1):15?22.
Reut Tsarfaty. 2010. Relational-Realizational Parsing.
Ph.D. thesis, University of Amsterdam.
Reut Tsarfaty. 2013. A unified morpho-syntactic scheme
of Stanford dependencies. In Proceedings of ACL,
Sofia, Bulgaria.
Veronika Vincze, D?ra Szauter, Attila Alm?si, Gy?rgy
M?ra, Zolt?n Alexin, and J?nos Csirik. 2010. Hungar-
ian Dependency Treebank. In Proceedings of LREC,
Valletta, Malta.
Joachim Wagner. 2012. Detecting Grammatical Errors
with Treebank-Induced Probabilistic Parsers. Ph.D.
thesis, Dublin City University.
Marcin Wolin?ski, Katarzyna G?owin?ska, and Marek
S?widzin?ski. 2011. A preliminary version of
Sk?adnica?a treebank of Polish. In Proceedings of
the 5th Language & Technology Conference, pages
299?303, Poznan?, Poland.
Alina Wr?blewska. 2012. Polish Dependency Bank. Lin-
guistic Issues in Language Technology, 7(1):1?15.
Yue Zhang and Joakim Nivre. 2011. Transition-based
dependency parsing with rich non-local features. In
Proceedings of ACL:HLT, pages 188?193, Portland,
OR.
J?nos Zsibrita, Veronika Vincze, and Rich?rd Farkas.
2013. magyarlanc: A toolkit for morphological and
dependency parsing of Hungarian. In Proceedings of
RANLP, pages 763?771, Hissar, Bulgaria.
182
