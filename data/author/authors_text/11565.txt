Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 257?260,
Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLP
Transfer Learning, Feature Selection and Word Sense Disambguation
Paramveer S. Dhillon and Lyle H. Ungar
Computer and Information Science
University of Pennsylvania, Philadelphia, PA, U.S.A
{pasingh,ungar}@seas.upenn.edu
Abstract
We propose a novel approach for improv-
ing Feature Selection for Word Sense Dis-
ambiguation by incorporating a feature
relevance prior for each word indicating
which features are more likely to be se-
lected. We use transfer of knowledge from
similar words to learn this prior over the
features, which permits us to learn higher
accuracy models, particularly for the rarer
word senses. Results on the ONTONOTES
verb data show significant improvement
over the baseline feature selection algo-
rithm and results that are comparable to or
better than other state-of-the-art methods.
1 Introduction
The task of WSD has been mostly studied in
a supervised learning setting e.g. (Florian and
Yarowsky, 2002) and feature selection has always
been an important component of high accuracy
word sense disambiguation, as one often has thou-
sands of features but only hundreds of observa-
tions of the words (Florian and Yarowsky, 2002).
The main problem that arises with supervised
WSD techniques, including ones that do feature
selection, is the paucity of labeled data. For ex-
ample, the training set of SENSEVAL-2 English
lexical sample task has only 10 labeled examples
per sense (Florian and Yarowsky, 2002), which
makes it difficult to build high accuracy models
using only supervised learning techniques. It is
thus an attractive alternative to use transfer learn-
ing (Ando and Zhang, 2005), which improves per-
formance by generalizing from solutions to ?sim-
ilar? learning problems. (Ando, 2006) (abbrevi-
ated as Ando[CoNLL?06]) have successfully ap-
plied the ASO (Alternating Structure Optimiza-
tion) technique proposed by (Ando and Zhang,
2005), in its transfer learning configuration, to the
problem of WSD by doing joint empirical risk
minimization of a set of related problems (words
in this case). In this paper, we show how a novel
form of transfer learning that learns a feature rel-
evance prior from similar word senses, aids in the
process of feature selection and hence benefits the
task of WSD.
Feature selection algorithms usually put a uni-
form prior over the features. I.e., they consider
each feature to have the same probability of being
selected. In this paper we relax this overly sim-
plistic assumption by transferring a prior for fea-
ture relevance of a given word sense from ?simi-
lar? word senses. Learning this prior for feature
relevance of a test word sense makes those fea-
tures that have been selected in the models of other
?similar? word senses become more likely to be
selected.
We learn the feature relevance prior only from
distributionally similar word senses, rather than
?all? senses of each word, as it is difficult to find
words which are similar in ?all? the senses. We
can, however, often find words which have one or
a few similar senses. For example, one sense of
?fire? (as in ?fire someone?) should share features
with one sense of ?dismiss? (as in ?dismiss some-
one?), but other senses of ?fire? (as in ?fire the
gun?) do not. Similarly, other meanings of ?dis-
miss? (as in ?dismiss an idea?) should not share
features with ?fire?.
As just mentioned, knowledge can only be
fruitfully transfered between the shared senses of
different words, even though the models being
learned are for disambiguating different senses of
a single word. To address this problem, we cluster
similar word senses of different words, and then
use the models learned for all but one of the word
senses in the cluster (called the ?training word
senses?) to put a feature relevance prior on which
features will be more predictive for the held out
test word sense. We hold out each word sense in
the cluster once and learn a prior from the remain-
ing word senses in that cluster. For example, we
can use the models for discriminating the senses
of the words ?kill? and the senses of ?capture?, to
257
put a prior on what features should be included in
a model to disambiguate corresponding senses of
the distributionally similar word ?arrest?.
The remainder of the paper is organized as fol-
lows. In Section 2 we describe our ?baseline? in-
formation theoretic feature selection method, and
extend it to our ?TRANSFEAT? method. Section 3
contains experimental results comparing TRANS-
FEAT with the baseline and Ando[CoNLL?06] on
ONTONOTES data. We conclude with a brief sum-
mary in Section 4.
2 Feature Selection for WSD
We use an information theoretic approach to fea-
ture selection based on the Minimum Description
Length (MDL) (Rissanen, 1999) principle, which
makes it easy to incorporate information about
feature relevance priors. These information theo-
retic models have a ?dual? Bayesian interpretation,
which provides a clean setting for feature selec-
tion.
2.1 Information Theoretic Feature Selection
The state-of-the-art feature selection methods in
WSD use either an ?
0
or an ?
1
penalty on the coef-
ficients. ?
1
penalty methods such as Lasso, being
convex, can be solved by optimization and give
guaranteed optimal solutions. On the other hand,
?
0
penalty methods, like stepwise feature selec-
tion, give approximate solutions but produce mod-
els that are much sparser than the models given by
?
1
methods, which is quite crucial in WSD (Flo-
rian and Yarowsky, 2002). ?
0
models are also more
amenable to theoretical analysis for setting thresh-
olds, and hence for incorporating priors.
Penalized likelihood methods which are widely
used for feature selection minimize a score:
Score = ?2log(likelihood) + Fq (1)
where F is a function designed to penalize model
complexity, and q represents the number of fea-
tures currently included in the model at a given
point. The first term in the above equation repre-
sents a measure of the in-sample error given the
model, while the second term is a model complex-
ity penalty.
As is obvious from Eq. 1, the description length
of the MDL (Minimum Description Length) mes-
sage is composed of two parts: S
E
, the num-
ber of bits for encoding the residual errors given
the models and S
M
, the number of bits for en-
coding the model. Hence the description length
can be written as: S = S
E
+ S
M
. Now, when
we evaluate a feature for possible addition to our
model, we want to maximize the reduction of ?de-
scription length? incurred by adding this feature
to the model. This change in description length
is: ?S = ?S
E
? ?S
M
; where ?S
E
? 0 is the
number of bits saved in describing residual error
due to increase in the likelihood of the data given
the new feature and ?S
M
> 0 is the extra bits
used for coding this new feature.
In our baseline feature selection model, we use
the following coding schemes:
Coding Scheme for S
E
:
The term S
E
represents the cost of coding the
residual errors given the models and can be written
as:
S
E
= ? log(P (y|w, x))
?S
E
represents the increase in likelihood (in
bits) of the data by adding this new feature to the
model. We assume a Gaussian model, giving:
P (y|w, x) ? exp
(
?
(
?
n
i=1
(y
i
? w ? x
i
)
2
2?
2
))
where y is the response (word senses in our case),
x?s are the features, w?s are the regression weights
and ?2 is the variance of the Gaussian noise.
Coding Scheme for ?S
M
: For describing S
M
,
the number of bits for encoding the model, we
need the bits to code the index of the feature (i.e.,
which feature from amongst the total m candidate
features) and the bits to code the coefficient of this
feature.
The total cost can be represented as:
S
M
= l
f
+ l
?
where l
f
is the cost to code the index of the feature
and l
?
is the number of bits required to code the
coefficient of the selected feature.
In our baseline feature selection algorithm, we
code l
f
by using log(m) bits (where m is the
total number of candidate features), which is
equivalent to the standard RIC (or the Bonferroni
penalty) (Foster and George, 1994) commonly
used in information theory. The above coding
scheme1 corresponds to putting a uniform prior
over all the features; I.e., each feature is equally
likely to get selected.
For coding the coefficients of the selected fea-
ture we use 2 bits, which is quite similar to the AIC
1There is a duality between Information Theory and
Bayesian terminology: If there is 1
k
probability of a fact being
true, then we need ?log( 1
k
) = log(k) bits to code it.
258
(Akaike Information Criterion) (Rissanen, 1999).
Our final equation for S
M
is therefore:
S
M
= log(m) + 2 (2)
2.2 Extension to TRANSFEAT
We now extend the baseline feature selection al-
gorithm to include the feature relevance prior. We
define a binary random variable f
i
? {0,1} that
denotes the event of the ith feature being in or not
being in the model for the test word sense. We can
parameterize the distribution as p(f
i
= 1|?
i
) = ?
i
.
I.e., we have a Bernoulli Distribution over the fea-
tures.
Given the data for the ith feature for all the
training word senses, we can write: D
i
=
{f
i1
, ..., f
iv
, ..., f
it
}. We then construct the like-
lihood functions from the data (under the i.i.d as-
sumption) as:
p(D
f
i
|?
i
) =
t
?
v=1
p(f
iv
|?
i
) =
t
?
v=1
?
f
iv
(1? ?
i
)
1?f
iv
The posteriors can be calculated by putting a prior
over the parameters ?
i
and using Bayes rule as fol-
lows:
p(?
i
|D
f
i
) = p(D
f
i
|?
i
) ? p(?
i
|a, b)
where a and b are the hyperparameters of the Beta
Prior (conjugate of Bernoulli). The predictive dis-
tribution of ?
i
is:
p(f
i
= 1|D
f
i
) =
?
1
0
?
i
p(?
i
|D
f
i
)d?
i
= E[?
i
|D
f
i
]
=
k + a
k + l + a + b
(3)
where k is the number of times that the ith feature
is selected and l is the complement of k, i.e. the
number of times the ith feature is not selected in
the training data.
In light of above, the coding scheme, which in-
corporates the prior information about the predic-
tive quality of the various features obtained from
similar word senses, can be formulated as follows:
S
M
= ? log (p(f
i
= 1|D
f
i
)) + 2
In the above equation, the first term repre-
sents the cost of coding the features, and the sec-
ond term codes the coefficients. The negative
signs appear due to the duality between Bayesian
and Information-Theoretic representation, as ex-
plained earlier.
3 Experimental Results
In this section we present the experimental results
of TRANSFEAT on ONTONOTES data.
3.1 Similarity Determination
To determine which verbs to transfer from, we
cluster verb senses into groups based on the
TF/IDF similarity of the vector of features se-
lected for that verb sense in the baseline (non-
transfer learning) model. We use only those
features that are positively correlated with the
given sense; they are the features most closely
associated with the given sense. We cluster
senses using a ?foreground-background? cluster-
ing algorithm (Kandylas et al, 2007) rather than
the more common k-means clustering because
many word senses are not sufficiently similar to
any other word sense to warrant putting into a
cluster. Foreground-background clustering gives
highly cohesive clusters of word senses (the ?fore-
ground?) and puts all the remaining word senses
in the ?background?. The parameters that it takes
as input are the % of data points to put in ?back-
ground? (i.e., what would be the singleton clus-
ters) and a similarity threshold which impacts
the number of ?foreground? clusters. We exper-
imented with putting 20% and 33% data points in
background and adjusted the similarity threshold
to give us 50 ? 100 ?foreground? clusters. The
results reported below have 20% background and
50 ? 100 ?foreground? clusters.
3.2 Description of Data and Results
We performed our experiments on ONTONOTES
data of 172 verbs (Hovy et al, 2006). The data
consists of a rich set of linguistic features which
have proven to be beneficial for WSD.
A sample feature vector for the word ?add?,
given below, shows typical features.
word_added pos_vbd morph_normal
subj_use subjsyn_16993 dobj_money
dobjsyn_16993 pos+1+2+3_rp+to+cd
tp_account tp_accumulate tp_actual
The 172 verbs each had between 1,000 and 10,000
nonzero features. The number of senses varied
from 2 (For example, ?add?) to 15 (For example,
?turn?).
We tested our transfer learning algorithm in
three slightly varied settings to tease apart the con-
tributions of different features to the overall per-
formance. In our main setting, we cluster the word
259
senses based on the ?semantic + syntactic? fea-
tures. In Setting 2, we do clustering based only on
?semantic? features (topic features) and in Setting
3 we cluster based on only ?syntactic? (pos, dobj
etc.) features.
Table 1: 10-fold CV (microaveraged) accuracies
of various methods for various Transfer Learning
settings. Note: These are true cross-validation ac-
curacies; No parameters have been tuned on them.
Method Setting 1 Setting 2 Setting 3
TRANSFEAT 85.75 85.11 85.37
Baseline Feat. Sel. 83.50 83.09 83.34
SVM (Poly. Kernel) 83.77 83.44 83.57
Ando[CoNLL?06] 85.94 85.00 85.51
Most Freq. Sense 76.59 77.14 77.24
We compare TRANSFEAT against Baseline Fea-
ture Selection, Ando[CoNLL?06], SVM (libSVM
package) with a cross-validated polynomial kernel
and a simple most frequent sense baseline. We
tuned the ?d? parameter of the polynomial kernel
using a separate cross validation.
The results for the different settings are shown
in Table 1 and are significantly better at the 5%
significance level (Paired t-test) than the base-
line feature selection algorithm and the SVM. It
is comparable in accuracy to Ando[CoNLL?06].
Settings 2 and 3, in which we cluster based on
only ?semantic? or ?syntactic? features, respec-
tively, also gave significant (5% level in a Paired
t-Test) improvement in accuracy over the baseline
and SVM model. But these settings performed
slightly worse than Setting 1, which suggests that
it is a good idea to have clusters in which the word
senses have ?semantic? as well as ?syntactic? dis-
tributional similarity.
Some examples will help to emphasize the point
that we made earlier that transfer helps the most in
cases in which the target word sense has much less
data than the word senses from which knowledge
is being transferred. ?kill? had roughly 6 times
more data than all other word senses in its cluster
(i.e., ?arrest?, ?capture?, ?strengthen?, etc.) In this
case, TRANSFEAT gave 3.19 ? 8.67% higher ac-
curacies than competing methods2 on these three
words. Also, for the case of word ?do,? which
had roughly 10 times more data than the other
word senses in its cluster (E.g., ?die? and ?save?),
TRANSFEAT gave 4.09?6.21% higher accuracies
2TRANSFEAT does better than Ando[CoNLL?06] on these
words even though on average over all 172 verbs, the differ-
ence is slender.
than other methods. Transfer makes the biggest
difference when the target words have much less
data than the word senses they are generalizing
from, but even in cases where the words have sim-
ilar amounts of data we still get a 1.5 ? 2.5% in-
crease in accuracy.
4 Summary
This paper presented a Transfer Learning formula-
tion which learns a prior suggesting which features
are most useful for disambiguating ambiguous
words. Successful transfer requires finding similar
word senses. We used ?foreground/background?
clustering to find cohesive clusters for various
word senses in the ONTONOTES data, consider-
ing both ?semantic? and ?syntactic? similarity be-
tween the word senses. Learning priors on features
was found to give significant accuracy boosts,
with both syntactic and semantic features con-
tributing to successful transfer. Both feature sets
gave substantial benefits over the baseline meth-
ods that did not use any transfer and gave compa-
rable accuracy to recent Transfer Learning meth-
ods like Ando[CoNLL?06]. The performance im-
provement of our Transfer Learning becomes even
more pronounced when the word senses that we
are generalizing from have more observations than
the ones that are being learned.
References
R. Ando and T. Zhang. 2005. A framework for learn-
ing predictive structures from multiple tasks and un-
labeled data. JMLR, 6:1817?1853.
R. Ando. 2006. Applying alternating structure
optimization to word sense disambiguation. In
(CoNLL).
R. Florian and D. Yarowsky. 2002. Modeling consen-
sus: classifier combination for word sense disam-
biguation. In EMNLP ?02, pages 25?32.
D. P. Foster and E. I. George. 1994. The risk infla-
tion criterion for multiple regression. The Annals of
Statistics, 22(4):1947?1975.
E. H. Hovy, M. P. Marcus, M. Palmer, L. A. Ramshaw,
and R. M. Weischedel. 2006. Ontonotes: The 90%
solution. In HLT-NAACL.
V. Kandylas, S. P. Upham, and L. H. Ungar. 2007.
Finding cohesive clusters for analyzing knowledge
communities. In ICDM, pages 203?212.
J. Rissanen. 1999. Hypothesis selection and testing by
the mdl principle. The Computer Journal, 42:260?
269.
260
Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 725?735,
MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational Linguistics
A New Approach to Lexical Disambiguation of Arabic Text
Rushin Shah
Carnegie Mellon University
5000 Forbes Avenue
Pittsburgh, PA 15213, USA
rnshah@cs.cmu.edu
Paramveer S. Dhillon, Mark Liberman,
Dean Foster, Mohamed Maamouri
and Lyle Ungar
University of Pennsylvania
3451 Walnut Street
Philadelphia, PA 19104, USA
{dhillon|myl|ungar}@cis.upenn.edu,
foster@wharton.upenn.edu,
maamouri@ldc.upenn.edu
Abstract
We describe a model for the lexical analy-
sis of Arabic text, using the lists of alterna-
tives supplied by a broad-coverage morpho-
logical analyzer, SAMA, which include sta-
ble lemma IDs that correspond to combina-
tions of broad word sense categories and POS
tags. We break down each of the hundreds
of thousands of possible lexical labels into
its constituent elements, including lemma ID
and part-of-speech. Features are computed
for each lexical token based on its local and
document-level context and used in a novel,
simple, and highly efficient two-stage super-
vised machine learning algorithm that over-
comes the extreme sparsity of label distribu-
tion in the training data. The resulting system
achieves accuracy of 90.6% for its first choice,
and 96.2% for its top two choices, in selecting
among the alternatives provided by the SAMA
lexical analyzer. We have successfully used
this system in applications such as an online
reading helper for intermediate learners of the
Arabic language, and a tool for improving the
productivity of Arabic Treebank annotators.
1 Background and Motivation
This paper presents a methodology for generating
high quality lexical analysis of highly inflected lan-
guages, and demonstrates excellent performance ap-
plying our approach to Arabic. Lexical analysis of
the written form of a language involves resolving,
explicitly or implicitly, several different kinds of am-
biguities. Unfortunately, the usual ways of talking
about this process are also ambiguous, and our gen-
eral approach to the problem, though not unprece-
dented, has uncommon aspects. Therefore, in order
to avoid confusion, we begin by describing how we
define the problem.
In an inflected language with an alphabetic writ-
ing system, a central issue is how to interpret strings
of characters as forms of words. For example, the
English letter-string ?winds? will normally be in-
terpreted in one of four different ways, all four
of which involve the sequence of two formatives
wind+s. The stem ?wind? might be analyzed as (1) a
noun meaning something like ?air in motion?, pro-
nounced [wInd] , which we can associate with an ar-
bitrary but stable identifier like wind n1; (2) a verb
wind v1 derived from that noun, and pronounced the
same way; (3) a verb wind v2 meaning something
like ?(cause to) twist?, pronounced [waInd]; or (4)
a noun wind n2 derived from that verb, and pro-
nounced the same way. Each of these ?lemmas?, or
dictionary entries, will have several distinguishable
senses, which we may also wish to associate with
stable identifiers. The affix ?-s? might be analyzed
as the plural inflection, if the stem is a noun; or as
the third-person singular inflection, if the stem is a
verb.
We see this analysis as conceptually divided into
four parts: 1) Morphological analysis, which rec-
ognizes that the letter-string ?winds? might be (per-
haps among other things) wind/N + s/PLURAL or
wind/V + s/3SING; 2) Morphological disambigua-
tion, which involves deciding, for example, that in
the phrase ?the four winds?, ?winds? is probably a
plural noun, i.e. wind/N + s/PLURAL; 3) Lemma
analysis, which involves recognizing that the stem
wind in ?winds? might be any of the four lem-
mas listed above ? perhaps with a further listing of
senses or other sub-entries for each of them; and 4)
Lemma disambiguation, deciding, for example, that
725
the phrase ?the four winds? probably involves the
lemma wind n1.
Confusingly, the standard word-analysis tasks in
computational linguistics involve various combina-
tions of pieces of these logically-distinguished op-
erations. Thus, ?part of speech (POS) tagging? is
mainly what we?ve called ?morphological disam-
biguation?, except that it doesn?t necessarily require
identifying the specific stems and affixes involved.
In some cases, it also may require a small amount of
?lemma disambiguation?, for example to distinguish
a proper noun from a common noun. ?Sense disam-
biguation? is basically a form of what we?ve called
?lemma disambiguation?, except that the sense dis-
ambiguation task may assume that the part of speech
is known, and may break down lexical identity more
finely than our system happens to do. ?Lemmatiza-
tion? generally refers to a radically simplified form
of ?lemma analysis? and ?lemma disambiguation?,
where the goal is simply to collapse different in-
flected forms of any similarly-spelled stems, so that
the strings ?wind?, ?winds?, ?winded?, ?winding? will
all be treated as instances of the same thing, without
in fact making any attempt to determine the identity
of ?lemmas? in the traditional sense of dictionary
entries.
Linguists use the term morphology to include all
aspects of lexical analysis under discussion here.
But in most computational applications, ?morpho-
logical analysis? does not include the disambigua-
tion of lemmas, because most morphological ana-
lyzers do not reference a set of stable lemma IDs.
So for the purposes of this paper, we will continue to
discuss lemma analysis and disambiguation as con-
ceptually distinct from morphological analysis and
disambiguation, although, in fact, our system dis-
ambiguates both of these aspects of lexical analysis
at the same time.
The lexical analysis of textual character-strings
is a more complex and consequential problem in
Arabic than it is in English, for several reasons.
First, Arabic inflectional morphology is more com-
plex than English inflectional morphology is. Where
an English verb has five basic forms, for example,
an Arabic verb in principle may have dozens. Sec-
ond, the Arabic orthographic system writes elements
such as prepositions, articles, and possessive pro-
nouns without setting them off by spaces, roughly
as if the English phrase ?in a way? were written ?in-
away?. This leads to an enormous increase in the
number of distinct ?orthographic words?, and a sub-
stantial increase in ambiguity. Third, short vowels
are normally omitted in Arabic text, roughly as if
English ?in a way? were written ?nway?.
As a result, a whitespace/punctuation-delimited
letter-string in Arabic text typically has many more
alternative analyses than a comparable English
letter-string does, and these analyses have many
more parts, drawn from a much larger vocabulary of
form-classes. While an English ?tagger? can spec-
ify the morphosyntactic status of a word by choos-
ing from a few dozen tags, an equivalent level of
detail in Arabic would require thousands of alterna-
tives. Similarly, the number of lemmas that might
play a role in a given letter-sequence is generally
much larger in Arabic than in English.
We start our labeling of Arabic text with the alter-
native analyses provided by SAMA v. 3.1, the Stan-
dard Arabic Morphological Analyzer (Maamouri et
al., 2009). SAMA is an updated version of the ear-
lier Buckwalter analyzers (Buckwalter, 2004), with
a number of significant differences in analysis to
make it compatible with the LDC Arabic Treebank
3-v3.2 (Maamouri et al, 2004). The input to SAMA
is an Arabic orthographic word (a string of letters
delimited by whitespace or punctuation), and the
output of SAMA is a set of alternative analyses, as
shown in Table 1. For a typical word, SAMA pro-
duces approximately a dozen alternative analyses,
but for certain highly ambiguous words it can pro-
duce hundreds of alternatives.
The SAMA analyzer has good coverage; for typ-
ical texts, the correct analysis of an orthographic
word can be found somewhere in SAMA?s list of
alternatives about 95% of the time. However, this
broad coverage comes at a cost; the list of analytic
alternatives must include a long Zipfian tail of rare
or contextually-implausible analyses, which collec-
tively are correct often enough to make a large con-
tribution to the coverage statistics. Furthermore,
SAMA?s long lists of alternative analyses are not
evaluated or ordered in terms of overall or contex-
tual plausibility. This makes the results less useful
in most practical applications.
Our goal is to rank these alternative analyses so
that the correct answer is as near to the top of the list
726
Token Lemma Vocalization Segmentation Morphology Gloss
yHlm Halam-u 1 yaHolumu ya + Holum +
u
IV3MS + IV + IV-
SUFF MOOD:I
he / it + dream + [ind.]
yHlm Halam-u 1 yaHoluma ya + Holum +
a
IV3MS + IV + IV-
SUFF MOOD:S
he / it + dream + [sub.]
yHlm Halum-u 1 yaHolumo ya + Holum +
o
IV3MS + IV + IV-
SUFF MOOD:J
he / it + be gentle + [jus.]
qbl qabil-a 1 qabila qabil + a PV + PV-
SUFF SUBJ:3MS
accept/receive/approve +
he/it [verb]
qbl qabol 1 qabol qabol NOUN Before
Table 1: Partial output of SAMA for yHlm and qbl. On average, every token produces more than 10 such analyses
as possible. Despite some risk of confusion, we?ll
refer to SAMA?s list of alternative analyses for an
orthographic word as potential labels for that word.
And despite a greater risk of confusion, we?ll refer to
the assignment of probabilities to the set of SAMA
labels for a particular Arabic word in a particular
textual context as tagging, by analogy to the oper-
ation of a stochastic part-of-speech tagger, which
similarly assigns probabilities to the set of labels
available for a word in textual context.
Although our algorithms have been developed for
the particular case of Arabic and the particular set
of lexical-analysis labels produced by SAMA, they
should be applicable without modification to the sets
of labels produced by any broad-coverage lexical
analyzer for the orthographic words of any highly-
inflected language.
In choosing our approach, we have been moti-
vated by two specific applications. One applica-
tion aims to help learners of Arabic in reading text,
by offering a choice of English glosses with asso-
ciated Arabic morphological analyses and vocaliza-
tions. SAMA?s excellent coverage is an important
basis for this help; but SAMA?s long, unranked list
of alternative analyses for a particular letter-string,
where many analyses may involve rare words or al-
ternatives that are completely implausible in the con-
text, will be confusing at best for a learner. It is
much more helpful for the list to be ranked so that
the correct answer is almost always near the top, and
is usually one of the top two or three alternatives.
In our second application, this same sort of rank-
ing is also helpful for the linguistically expert native
speakers who do Arabic Treebank analysis. These
annotators understand the text without difficulty, but
find it time-consuming and fatiguing to scan a long
list of rare or contextually-implausible alternatives
for the correct SAMA output. Their work is faster
and more accurate if they start with a list that is
ranked accurately in order of contextual plausibility.
Other applications are also possible, such as vo-
calization of Arabic text for text-to-speech synthe-
sis, or lexical analysis for Arabic parsing. However,
our initial goals have been to rank the list of SAMA
outputs for human users.
We note in passing that the existence of set of sta-
ble ?lemma IDs? is an unusual feature of SAMA,
which in our opinion ought to be emulated by ap-
proaches to lexical analysis in other languages. The
lack of such stable lemma IDs has helped to disguise
the fact that without lemma analysis and disam-
biguation, morphological analyses and disambigua-
tion is only a partial solution to the problem of lexi-
cal analysis.
In principle, it is obvious that lemma disambigua-
tion and morphological disambiguation are mutually
beneficial. If we know the answer to one of the ques-
tions, the other one is easier to answer. However,
these two tasks require rather different sets of con-
textual features. Lemma disambiguation is similar
to the problem of word-sense disambiguation ? on
some definitions, they are identical ? and as a re-
sult, it benefits from paragraph-level and document-
level bag-of-words attributes that help to character-
ize what the text is ?about? and therefore which lem-
mas are more likely to play a role in it. In contrast,
morphological disambiguation mainly depends on
features of nearby words, which help to character-
727
ize how inflected forms of these lemmas might fit
into local phrasal structures.
2 Problem and Methodology
Consider a collection of tokens (observations), ti, re-
ferred to by index i ? {1, . . . , n}, where each token
is associated with a set of p features, xij , for the jth
feature, and a label, li, which is a combination of
a lemma and a morphological analysis. We use in-
dicator functions yik to indicate whether or not the
kth label for the ith token is present. We represent
the complete set of features and labels for the en-
tire training data using matrix notation as X and Y ,
respectively. Our goal is to predict the label l (or
equivalently, the vector y for a given feature vector
x.
A standard linear regression model of this prob-
lem would be
y = x? +  (1)
The standard linear regression estimate of ? (ig-
noring, for simplicity the fact that the ys are 0/1) is:
?? = (XTtrainXtrain)
?1XTtrainYtrain (2)
where Ytrain is an n?h matrix containing 0s and
1s indicating whether or not each of the h possible
labels is the correct label (li) for each of the n tokens
ti, Xtrain is an n ? p matrix of context features for
each of the n tokens, the coefficients ?? are p? h.
However, this is a large, sparse, multiple label
problem, and the above formulation is neither statis-
tically nor computationally efficient. Each observa-
tion (x,y) consists of thousands of features associ-
ated with thousands of potential labels, almost all of
which are zero. Worse, the matrix of coefficients ?,
to be estimated is large (p? h) and one should thus
use some sort of transfer learning to share strength
across the different labels.
We present a novel principled and highly compu-
tationally efficient method of estimating this multi-
label model. We use a two stage procedure, first
using a subset (Xtrain1, Ytrain1) of training data
to give a fast approximate estimate of ?; we then
use a second smaller subset of the training data
(Xtrain2, Ytrain2,) to ?correct? these estimates in a
way that we will show can be viewed as a spe-
cialized shrinkage. Our first stage estimation ap-
proximates ?, but avoids the expensive computa-
tion of (XTtrainXtrain)
?1. Our second stage corrects
(shrinks) these initial estimates in a manner special-
ized to this problem. The second stage takes ad-
vantage of the fact that we only need to consider
those candidate labels produced by SAMA. Thus,
only dozens of the thousands of possible labels are
considered for each token.
We now present our algorithm. We start with a
corpus D of documents d of labeled Arabic text. As
described above, each token, ti is associated with a
set of features characterizing its context, computed
from the other words in the same document, and a la-
bel, li = (lemmai,morphologyi), which is a combi-
nation of a lemma and a morphological analysis. As
described below, we introduce a novel factorization
of the morphology into 15 different components.
Our estimation algorithm, shown in Algorithm 1,
has two stages. We partition the training corpus into
two subsets, one of which (Xtrain1) is used to es-
timate the coefficients ?s and the other of which
(Xtrain2) is used to optimally ?shrink? these coeffi-
cient estimates to reduce variance and prevent over-
fitting due to data sparsity.
For the first stage of our estimation procedure, we
simplify the estimate of the (?) matrix (Equation 2)
to avoid the inversion of the very high dimensional
(p?p) matrix (XTX) by approximating (XTX) by
its diagonal, Var(X), the inverse of which is trivial
to compute; i.e. we estimate ? using
?? = Var(Xtrain1)
?1XTtrain1Ytrain1 (3)
For the second stage, we assume that the coeffi-
cients for each feature can be shrunk differently, but
that coefficients for each feature should be shrunk
the same regardless of what label they are predict-
ing. Thus, for a given observation we predict:
g?ik =
p?
j=1
wj ??jkxij (4)
where the weightswj indicate how much to shrink
each of the p features.
In practice, we fold the variance of each of the j
features into the weight, giving a slightly modified
equation:
g?ik =
p?
j=1
?j?
?
jkxij (5)
728
where ?? = XTtrain1Ytrain1 is just a matrix of the
counts of how often each context feature shows up
with each label in the first training set. The vec-
tor ?, which we will estimate by regression, is just
the shrinkage weightsw rescaled by the feature vari-
ance.
Note that the formation here is different from the
first stage. Instead of having each observation be
a token, we now let each observation be a (token,
label) pair, but only include those labels that were
output by SAMA. For a given token ti and poten-
tial label lk, our goal is to approximate the indica-
tor function g(i, k), which is 1 if the kth label of
token ti is present, and 0 otherwise. We find candi-
date labels using a morphological analyzer (namely
SAMA), which returns a set of possible candidate
labels, say C(t), for each Arabic token t. Our pre-
dicted label for ti is then argmaxk?C(ti)g(i, k).
The regression model for learning the weights ?j
in the second stage thus has a row for each label
g(i, k) associated with a SAMA candidate for each
token i = ntrain1+1 . . . ntrain2 in the second train-
ing set. The value of g(i, k) is predicted as a func-
tion of the feature vector zijk = ??jkxij .
The shrinkage coefficients, ?j , could be estimated
from theory, using a version of James-Stein shrink-
age (James and Stein, 1961), but in practice, superior
results are obtained by estimating them empirically.
Since there are only p of them (unlike the p ? h ?s),
a relatively small training set is sufficient. We found
that regression-SVMs work slightly better than lin-
ear regression and significantly better than standard
classification SVMs for this problem.
Prediction is then done in the obvious way by tak-
ing the tokens in a test corpusDtest, generating con-
text features and candidate SAMA labels for each
token ti, and selected the candidate label with the
highest score g?(i, k) that we set out to learn. More
formally, The model parameters ?? and ? produced
by the algorithm allow one to estimate the most
likely label for a new token ti out of a set of can-
didate labels C(ti) using
kpred = argmaxk?C(ti)
p?
j=1
?j?
?
jkxij (6)
The most expensive part of the procedure is es-
timating ??, which requires for each token in cor-
Algorithm 1 Training algorithm.
Input: A training corpusDtrain of n observations
(Xtrain, Ytrain)
PartitionDtrain into two sets,D1 andD2, of sizes
ntrain1 and ntrain2 = n? ntrain1 observations
// Using D1, estimate ??
??jk =
?ntrain1
i=1 xijyik for the j
th feature and kth
label
// Using D2, estimate ?j
// Generate new ?features? Z and the true labels
g(i, k) for each of the SAMA candidate labels for
each of the tokens in D2
zijk = ??jkxij for i in i = ntrain1 + 1 . . . ntrain2
Estimate ?j for the above (feature,label) pairs
(zijk, g(i, k)) using Regression SVMs
Output: ? and ??
pus D1, (a subset of D), finding the co-occurrence
frequencies of each label element (a lemma, or a
part of the morphological segmentation) with the
target token and jointly with the token and with
other tokens or characters in the context of the to-
ken of interest. For example, given an Arabic to-
ken, ?yHlm?, we count what fraction of the time
it is associated with each lemma (e.g. Halam-
u 1), count(lemma=Halam-u 1, token=yHlm) and
each segment (e.g. ?ya?), count(segment=ya, to-
ken=yHlm). (Of course, most tokens never show up
with most lemmas or segments; this is not a prob-
lem.) We also find the base rates of the components
of the labels (e.g., count(lemma=Halam-u 1), and
what fraction of the time the label shows up in vari-
ous contexts, e.g. count(lemma=Halam-u 1, previ-
ous token = yHlm). We describe these features in
more detail below.
3 Features and Labels used for Training
Our approach to tagging Arabic differs from conven-
tional approaches in the two-part shrinkage-based
method used, and in the choice of both features and
labels used in our model. For features, we study
both local context variables, as described above, and
document-level word frequencies. For the labels, the
key question is what labels are included and how
they are factored. Standard ?taggers? work by doing
an n-way classification of all the alternatives, which
is not feasible here due to the thousands of possi-
729
ble labels. Standard approaches such as Conditional
Random Fields (CRFs) are intractable with so many
labels. Moreover, few if any taggers do any lemma
disambiguation; that is partly because one must start
with some standard inventory of lemmas, which are
not available for most languages, perhaps because
the importance of lemma disambiguation has been
underestimated.
We make a couple of innovations to deal with
these issues. First, we perform lemma disambigua-
tion in addition to ?tagging?. As mentioned above,
lemmas and morphological information are not in-
dependent; the choice of lemma often influences
morphology and vice versa. For example, Table 1
contains two analyses for the word qbl. For the first
analysis, where the lemma is qabil-a 1 and the gloss
is accept/receive/approve + he/it [verb], the word is
a verb. However, for the second analysis, where the
lemma is qabol 1 and the gloss is before, the word
is a noun.
Simultaneous lemma disambiguation and tagging
introduces additional complexity: An analysis of
ATB and SAMA shows that there are approximately
2,200 possible morphological analyses (?tags?) and
40,000 possible lemmas; even accounting for the
fact that most combinations of lemmas and morpho-
logical analyses don?t occur, the size of the label
space is still in the order of tens of thousands. To
deal with data sparsity, our second innovation is to
factor the labels. We factor each label l into a set of
16 label elements (LEs). These include lemmas, as
well as morphological elements such as basic part-
of-speech, suffix, gender, number, mood, etc. These
are explained in detail below. Thus, since each la-
bel l is a set of 15 categorical variables, each y in
the first learning stage is actually a vector with 16
nonzero components and thousands of zeros. Since
we do simultaneous estimation of the entire set of
label elements, the value g(i, k) being predicted in
the second learning phase is 1 if the entire label set
is correct, and zero otherwise. We do not learn sep-
arate models for each label.
3.1 Label Elements (LEs)
The fact that there are tens of thousands of possible
labels presents the problem of extreme sparsity of
label distribution in the training data. We find that a
model that estimates coefficients ?? to predict a sin-
LE Description
lemma Lemma
pre1 Closer prefix
pre2 Farther prefix
det Determiner
pos Basic POS
dpos Additional data on basic pos
suf Suffix
perpos Person (basic pos)
numpos Number (basic pos)
genpos Gender (basic pos)
persuf Person (suffix)
numsuf Number (suffix)
gensuf Gender (suffix)
mood Mood of verb
pron Pronoun suffix
Table 2: Label Elements (LEs). Examples of additional
data on basic POS include whether a noun is proper or
common, whether a verb is transitive or not, etc. Both
the basic POS and its suffix may have person, gender and
number data.
gle label (a label being in the Cartesian product of
the set of label elements) yields poor performance.
Therefore, as just mentioned, we factor each label
l into a set of label elements (LEs), and learn the
correlations ?? between features and label elements,
rather than features and entire label sets. This re-
duces, but does not come close to eliminating, the
problem sparsity. A complete list of these LEs and
their possible values is detailed in Table 2.
3.2 Features
3.2.1 Local Context Features
We take (t, l) pairs from D2, and for each such
pair generate features Z based on co-occurrence
statistics ?? in D1, as mentioned in Algorithm 2.
These statistics include unigram co-occurrence fre-
quencies of each label with the target token and bi-
gram co-occurrence of the label with the token and
with other tokens or characters in the context of the
target token. We define them formally in Table 3.
Let Zbaseline denote the set of all such basic features
based on the local context statistics of the target to-
ken, namely the words and letters preceding and fol-
lowing it. We will use this set to create a baseline
model.
730
Statistic Description
Freq countD1(t, l)
PrevWord countD1(t, l, t?1)
NextWord countD1(t, l, t+1)
PreviLetter countD1(t, l, first letter(t?1))
NextiLetter countD1(t, l, first letter(t+1)
PrevfLetter countD1(t, l, last letter(t?1)
NextfLetter countD1(t, l, last letter(t+1)
Table 3: Co-occurrence statistics ??. We use these to
generate feature sets for our regression SVMs.
For each label element (LE) e, we define a set of
features Ze similar to Zbaseline; these features are
based on co-occurrence frequencies of the particular
LE e, not the entire label l.
Finally, we define an aggregate feature set Zaggr
as follows:
Zaggr = Zbaseline
?
{Ze} (7)
where e ? {lemma, pre1, pre2, det, pos, dpos,
suf, perpos, numpos, genpos, persuf, numsuf, gensuf,
mood, pron}.
3.2.2 Document Level Features
When trying to predict the lemma, it is useful to
include not just the words and characters immedi-
ately adjacent to the target token, but also the all the
words in the document. These words capture the
?topic? of the document, and help to disambiguate
different lemmas, which tend to be used or not used
based on the topic being discussed, similarly to the
way that word sense disambiguation systems in En-
glish sometimes use the ?bag of words? the docu-
ment to disambiguate, for example a ?bank? for de-
positing money from a ?bank? of a river. More pre-
cisely, we augment the features for each target token
with the counts of each word in the document (the
?term frequency? tf) in which the token occurs with
a given label.
Zfull = Zaggr
?
Ztf (8)
This setZfull is our final feature set. We useZfull
to train an SVM model Mfull; this is our final pre-
dictive model.
3.3 Corpora used for Training and Testing
We use three modules of the Penn Arabic Tree-
bank (ATB) (Maamouri et al, 2004), namely ATB1,
ATB2 and ATB3 as our corpus of labeled Ara-
bic text, D. Each ATB module is a collection
of newswire data from a particular agency. ATB1
uses the Associated Press as a source, ATB2 uses
Ummah, and ATB3 uses Annahar. D contains a total
of 1,835 documents, accounting for approximately
350,000 words. We construct the training and test-
ing setsDtrain andDtest fromD using 10-fold cross
validation, and we constructD1 andD2 fromDtrain
by randomly performing a 9:1 split.
As mentioned earlier, we use the SAMA mor-
phological analyzer to obtain candidate labels C(t)
for each token t while training and testing an SVM
model on D2 and Dtest respectively. A sample out-
put of SAMA is shown in Table 1. To improve cov-
erage, we also add to C(t) all the labels l seen for t
in D1. We find that doing so improves coverage to
98%. This is an upper bound on the accuracy of our
model.
C(t) = SAMA(t)
?
{l|(t, l) ? D1} (9)
4 Results
We use two metrics of accuracy: A1, which mea-
sures the percentage of tokens for which the model
assigns the highest score to the correct label or LE
value (or E1= 100?A1, the corresponding percent-
age error), and A2, which measures the percentage
of tokens for which the correct label or LE value
is one of the two highest ranked choices returned
by the model (or E2 = 100 ? A2). We test our
modelMfull onDtest and achieve A1 and A2 scores
of 90.6% and 96.2% respectively. The accuracy
achieved by our Mfull model is, to the best of our
knowledge, higher than prior approaches have been
able to achieve so far for the problem of combined
morphological and lemma disambiguation. This is
all the more impressive considering that the upper
bound on accuracy for our model is 98% because,
as described above, our set of candidate labels is in-
complete.
In order to analyze how well different LEs can be
predicted, we train an SVM model Me for each LE
e using the feature set Ze, and test all such models
731
on Dtest. The results for all the LEs are reported in
the form of error percentages E1 and E2 in Table 4.
Model E1 E2 Model E1 E2
Mlemma 11.1 4.9 Mpre1 1.9 1.4
Mpre2 0.2 0 Mdet 0.7 0.1
Mpos 23.4 4.0 Mdpos 10.3 1.9
Msuf 7.6 2.5 Mperpos 3.0 0.1
Mnumpos 3.2 0.2 Mgenpos 1.8 0.1
Mpersuf 3.2 0.1 Mnumsuf 8.2 0.5
Mgensuf 11.6 0.4 Mmood 1.6 1.4
Mpron 1.8 0.6 Mcase 14.7 5.9
Mfull 9.4 3.8 - - -
Table 4: Results of Me for each LE e. Note: The results
reported are 10 fold cross validation test accuracies and
no parameters have been tuned on them.
A comparison of the results for Mfull with the
results for Mlemma and Mpos is particularly infor-
mative. We see that Mfull is able to achieve a sub-
stantially lower E1 error score (9.4%) than Mlemma
(11.1%) and Mpos (23.4%); in other words, we find
that our full model is able to predict lemmas and ba-
sic parts-of-speech more accurately than the individ-
ual models for each of these elements.
We examine the effect of varying the size of D2,
i.e. the number of SVM training instances, on the
performance of Mfull on Dtest, and find that with
increasing sizes of D2, E1 reduces only slightly
from 9.5% to 9.4%, and shows no improvement
thereafter. We also find that the use of document-
level features in Mlemma reduces E1 and E2 per-
centages for Mlemma by 5.7% and 3.2% respec-
tively.
4.1 Comparison to Alternate Approaches
4.1.1 Structured Prediction Models
Preliminary experiments showed that knowing the
predicted labels (lemma + morphology) of the sur-
rounding words can slightly improve the predic-
tive accuracy of our model. To further investi-
gate this effect, we tried running experiments us-
ing different structured models, namely CRF (Con-
ditional Random Fields) (Lafferty et al, 2001),
(Structured) MIRA (Margin Infused Relaxation Al-
gorithm) (Crammer et al, 2006) and Structured
Perceptron (Collins, 2002). We used linear chain
CRFs as implemented in MALLET Toolbox (Mc-
Callum, 2001) and for Structured MIRA and Per-
ceptron we used their implementations from EDLIN
Toolbox (Ganchev and Georgiev, 2009). However,
given the vast label space of our problem, running
these methods proved infeasible. The time complex-
ity of these methods scales badly with the number of
labels; It took a week to train a linear chain CRF
for only ? 50 labels and though MIRA and Per-
ceptron are online algorithms, they also become in-
tractable beyond a few hundred labels. Since our
label space contains combinations of lemmas and
morphologies, so even after factoring, the dimension
of the label space is in the order of thousands.
We also tried a na??ve version (two-pass approxi-
mation) of these structured models. In addition to
the features in Zfull, we include the predicted la-
bels for the tokens preceding and following the tar-
get token as features. This new model is not only
slow to train, but also achieves only slightly lower
error rates (1.2% lower E1 and 1.0% lower E2) than
Mfull. This provides an upper bound on the bene-
fit of using the more complex structured models, and
suggests that given their computational demands our
(unstructured) model Mfull is a better choice.
4.1.2 MADA
(Habash and Rambow, 2005) perform morpho-
logical disambiguation using a morphological ana-
lyzer. (Roth et al, 2008) augment this with lemma
disambiguation; they call their system MADA. Our
work differs from theirs in a number of respects.
Firstly, they don?t use the two step regression proce-
dure that we use. Secondly, they use only ?unigram?
features. Also, they do not learn a single model from
a feature set based on labels and LEs; instead, they
combine models for individual elements by using
weighted agreement. We trained and tested MADA
v2.32 using its full feature set on the same Dtrain
andDtest. We should point out that this is not an ex-
act comparison, since MADA uses the older Buck-
walter morphological analyzer.1
4.1.3 Other Alternatives
Unfactored Labels: To illustrate the benefit ob-
tained by breaking down each label l into
1A new version of MADA was released very close to the
submission deadline for this conference.
732
LEs, we contrast the performance of our Mfull
model to an SVM model Mbaseline trained us-
ing only the feature set Zbaseline, which only
contains features based on entire labels, those
based on individual LEs.
Independent lemma and morphology prediction:
Another alternative approach is to pre-
dict lemmas and morphological analyses
separately. We construct a feature set
Zlemma? = Zfull ? Zlemma and train an SVM
model Mlemma? using this feature set. Labels
are then predicted by simply combining the
results predicted independently by Mlemma
and Mlemma? . Let Mind denote this approach.
Unigram Features: Finally, we also consider a
context-less approach, i.e. using only ?uni-
gram? features for labels as well as LEs. We
call this feature set Zuni, and the correspond-
ing SVM model Muni.
The results of these various models, along with
those of Mfull are summarized in Table 5. We see
thatMfull has roughly half the error rate of the state-
of-the-art MADA system.
Model E1 E2
Mbaseline 13.6 9.1
Mind 18.7 6.0
Muni 11.6 6.4
Mcheat 8.2 2.8
MADA 16.9 12.6
Mfull 9.4 3.8
Table 5: Percent error rates of alternative approaches.
Note: The results reported are 10 fold cross validation
test accuracies and no parameters have been tuned on
them. We used same train-test splits for all the datasets.
5 Related Work
(Hajic, 2000) show that for highly inflectional
languages, the use of a morphological analyzer
improves accuracy of disambiguation. (Diab et
al., 2004) perform tokenization, POS tagging
and base phrase chunking using an SVM based
learner. (Ahmed and Nu?rnberger, 2008) perform
word-sense disambiguation using a Naive Bayesian
model and rely on parallel corpora and match-
ing schemes instead of a morphological ana-
lyzer. (Kulick, 2010) perform simultaneous tok-
enization and part-of-speech tagging for Arabic by
separating closed and open-class items and focus-
ing on the likelihood of possible stems of open-
class words. (Mohamed and Ku?bler, 2010) present
a hybrid method between word-based and segment-
based POS tagging for Arabic and report good re-
sults. (Toutanova and Cherry, 2009) perform joint
lemmatization and part-of-speech tagging for En-
glish, Bulgarian, Czech and Slovene, but they do
not use the two step estimation-shrinkage model de-
scribed in this paper; nor do they factor labels. The
idea of joint lemmatization and part-of-speech tag-
ging has also been discussed in the context of Hun-
garian in (Kornai, 1994).
A substantial amount of relevant work has been
done previously for Hebrew. (Adler and Elhadad,
2006) perform Hebrew morphological disambigua-
tion using an unsupervised morpheme-based HMM,
but they report lower scores than those achieved by
our model. Moreover, their analysis doesn?t include
lemma IDs, which is a novelty of our model. (Gold-
berg et al, 2008) extend the work of (Adler and El-
hadad, 2006) by using an EM algorithm, and achieve
an accuracy of 88% for full morphological analy-
sis, but again, this does not include lemma IDs. To
the best of our knowledge, there is no existing re-
search for Hebrew that does what we did for Arabic,
namely to use simultaneous lemma and morpholog-
ical disambiguation to improve both. (Dinur et al,
2009) show that prepositions and function words can
be accurately segmented using unsupervised meth-
ods. However, by using this method as a preprocess-
ing step, we would lose the power of a simultaneous
solution for these problems. Our method is closer in
style to a CRF, giving much of the accuracy gains of
simultaneous solution, while being about 4 orders of
magnitude easier to train.
We believe that our use of factored labels is novel
for the problem of simultaneous lemma and mor-
phological disambiguation; however, (Smith et al,
2005) and (Hatori et al, 2008) have previously
made use of features based on parts of labels in
CRF models for morphological disambiguation and
word-sense disambiguation respectively. Also, we
note that there is a similarity between our two-stage
733
machine learning approach and log-linear models in
machine translation that break the data in two parts,
estimating log-probabilities of generative models
from one part, and discriminatively re-weighting the
models using the second part.
6 Conclusions
We introduced a new approach to accurately predict
labels consisting of both lemmas and morphologi-
cal analyses for Arabic text. We obtained an accu-
racy of over 90% ? substantially higher than current
state-of-the-art systems. Key to our success is the
factoring of labels into lemma and a large set of mor-
phosyntactic elements, and the use of an algorithm
that computes a simple initial estimate of the coef-
ficient relating each contextual feature to each la-
bel element (simply by counting co-occurrence) and
then regularizes these features by shrinking each of
the coefficients for each feature by an amount deter-
mined by supervised learning using only the candi-
date label sets produced by SAMA.
We also showed that using features of word n-
grams is preferable to using features of only individ-
ual tokens of data. Finally, we showed that a model
using a full feature set based on labels as well as
factored components of labels, which we call label
elements (LEs) works better than a model created
by combining individual models for each LE. We
believe that the approach we have used to create our
model can be successfully applied not just to Arabic
but also to other languages such as Turkish, Hungar-
ian and Finnish that have highly inflectional mor-
phology. The current accuracy of of our model, get-
ting the correct answer among the top two choices
96.2% of the time is high enough to be highly use-
ful for tasks such as aiding the manual annotation
of Arabic text; a more complete automation would
require that accuracy for the single top choice.
Acknowledgments
We woud like to thank everyone at the Linguis-
tic Data Consortium, especially Christopher Cieri,
David Graff, Seth Kulick, Ann Bies, Wajdi Za-
ghouani and Basma Bouziri for their help. We also
wish to thank the anonymous reviewers for their
comments and suggestions.
References
Meni Adler and Michael Elhadad. 2006. An Unsuper-
vised Morpheme-Based HMM for Hebrew Morpho-
logical Disambiguation. In Proceedings of the 21st
International Conference on Computational Linguis-
tics and the 44th annual meeting of the Association for
Computational Linguistics.
Farag Ahmed and Andreas Nu?rnberger. 2008. Ara-
bic/English Word Translation Disambiguation using
Parallel Corpora and Matching Schemes. In Proceed-
ings of EAMT?08, Hamburg, Germany.
Tim Buckwalter. 2004. Buckwalter Arabic Morphologi-
cal Analyzer version 2.0.
Michael Collins. 2002. Discriminative Training Meth-
ods for Hidden Markov Models: Theory and Experi-
ments with Perceptron Algorithms. In Proceedings of
EMNLP?02.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online Passive-
Aggressive Algorithms. Journal of Machine Learning
Research, 7:551?585.
Mona Diab, Kadri Hacioglu, and Daniel Jurafsky. 2004.
Automatic Tagging of Arabic text: From Raw Text to
Base Phrase Chunks. In Proceedings of the 5th Meet-
ing of the North American Chapter of the Associa-
tion for Computational Linguistics/Human Language
Technologies Conference (HLT-NAACL?04).
Elad Dinur, Dmitry Davidov, and Ari Rappoport. 2009.
Unsupervised Concept Discovery in Hebrew Using
Simple Unsupervised Word Prefix Segmentation for
Hebrew and Arabic. In Proceedings of the EACL 2009
Workshop on Computational Approaches to Semitic
Languages.
Kuzman Ganchev and Georgi Georgiev. 2009. Edlin:
An Easy to Read Linear Learning Framework. In Pro-
ceedings of RANLP?09.
Yoav Goldberg, Meni Adler, and Michael Elhadad. 2008.
EM Can Find Pretty Good HMM POS-Taggers (When
Given a Good Start)*. In Proceedings of ACL?08.
Nizar Habash and Owen Rambow. 2005. Arabic Tok-
enization, Part-of-Speech Tagging and Morphological
Disambiguation in One Fell Swoop. In Proceedings of
ACL?05, Ann Arbor, MI, USA.
Jan Hajic. 2000. Morphological Tagging: Data vs. Dic-
tionaries. In Proceedings of the 1st Meeting of the
North American Chapter of the Association for Com-
putational Linguistics (NAACL?00).
Jun Hatori, Yusuke Miyao, and Jun?ichi Tsujii. 2008.
Word Sense Disambiguation for All Words using Tree-
Structured Conditional Random Fields. In Proceed-
ings of COLing?08.
W. James and Charles Stein. 1961. Estimation with
Quadratic Loss. In Proceedings of the Fourth Berkeley
734
Symposium on Mathematical Statistics and Probabil-
ity, Volume 1.
Andra?s Kornai. 1994. On Hungarian morphology (Lin-
guistica, Series A: Studia et Dissertationes 14). Lin-
guistics Institute of Hungarian Academy of Sciences,
Budapest.
Seth Kulick. 2010. Simultaneous Tokenization and Part-
of-Speech Tagging for Arabic without a Morphologi-
cal Analyzer. In Proceedings of ACL?10.
John D. Lafferty, Andrew McCallum, and Fernando C. N.
Pereira. 2001. Conditional Random Fields: Proba-
bilistic Models for Segmenting and Labeling Sequence
Data. In Proceedings of ICML?01, pages 282?289.
Mohamed Maamouri, Ann Bies, and Tim Buckwalter.
2004. The Penn Arabic Treebank: Building a Large
Scale Annotated Arabic Corpus. In Proceedings of
NEMLAR Conference on Arabic Language Resources
and Tools.
Mohamed Maamouri, David Graff, Basma Bouziri, Son-
dos Krouna, and Seth Kulick. 2009. LDC Standard
Arabic Morphological Analyzer (SAMA) v. 3.0.
Andrew McCallum, 2001. MALLET: A Machine Learn-
ing for Language Toolkit. Software available at
http://mallet.cs.umass.edu.
Emad Mohamed and Sandra Ku?bler. 2010. Arabic Part
of Speech Tagging. In Proceedings of LREC?10.
Ryan Roth, Owen Rambow, Nizar Habash, Mona Diab,
and Cynthia Rudin. 2008. Arabic Morphological Tag-
ging, Diacritization, and Lemmatization Using Lex-
eme Models and Feature Ranking. In Proceedings of
ACL?08, Columbus, Ohio, USA.
Noah A. Smith, David A. Smith, and Roy W. Tromble.
2005. Context-Based Morphological Disambiguation
with Random Fields*. In Proceedings of Human
Language Technology Conference and Conference on
Empirical Methods in Natural Language Processing
(HLT/EMNLP).
Kristina Toutanova and Colin Cherry. 2009. A Global
Model for Joint Lemmatization and Part-of-Speech
Prediction. In Proceedings of the Joint Conference of
the 47th Annual Meeting of the ACL and the 4th Inter-
national Joint Conference on Natural Language Pro-
cessing, pages 486?494.
735
Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning, pages 205?213, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational Linguistics
Spectral Dependency Parsing with Latent Variables
Paramveer S. Dhillon1, Jordan Rodu2, Michael Collins3, Dean P. Foster2
and Lyle H. Ungar1
1Computer & Information Science/ 2Statistics, University of Pennsylvania, Philadelphia, PA, U.S.A
3 Computer Science, Columbia University, New York, NY, U.S.A
{dhillon|ungar@cis.upenn.edu}, {jrodu|foster@wharton.upenn.edu}
mcollins@cs.columbia.edu
Abstract
Recently there has been substantial interest in
using spectral methods to learn generative se-
quence models like HMMs. Spectral meth-
ods are attractive as they provide globally con-
sistent estimates of the model parameters and
are very fast and scalable, unlike EM meth-
ods, which can get stuck in local minima. In
this paper, we present a novel extension of
this class of spectral methods to learn depen-
dency tree structures. We propose a simple
yet powerful latent variable generative model
for dependency parsing, and a spectral learn-
ing method to efficiently estimate it. As a pi-
lot experimental evaluation, we use the spec-
tral tree probabilities estimated by our model
to re-rank the outputs of a near state-of-the-
art parser. Our approach gives us a moderate
reduction in error of up to 4.6% over the base-
line re-ranker.
1 Introduction
Markov models have been for two decades a
workhorse of statistical pattern recognition with ap-
plications ranging from speech to vision to lan-
guage. Adding latent variables to these models gives
us additional modeling power and have shown suc-
cess in applications like POS tagging (Merialdo,
1994), speech recognition (Rabiner, 1989) and ob-
ject recognition (Quattoni et al2004). However,
this comes at the cost that the resulting parameter
estimation problem becomes non-convex and tech-
niques like EM (Dempster et al1977) which are
used to estimate the parameters can only lead to lo-
cally optimal solutions.
Recent work by Hsu et al2008) has shown that
globally consistent estimates of the parameters of
HMMs can be found by using spectral methods, par-
ticularly by singular value decomposition (SVD) of
appropriately defined linear systems. They avoid the
NP Hard problem of the global optimization prob-
lem of the HMM parameters (Terwijn, 2002), by
putting restrictions on the smallest singular value
of the HMM parameters. The main intuition be-
hind the model is that, although the observed data
(i.e. words) seem to live in a very high dimensional
space, but in reality they live in a very low dimen-
sional space (size k ? 30 ? 50) and an appropriate
eigen decomposition of the observed data will re-
veal the underlying low dimensional dynamics and
thereby revealing the parameters of the model. Be-
sides ducking the NP hard problem, the spectral
methods are very fast and scalable to train compared
to EM methods.
In this paper we generalize the approach of Hsu
et al2008) to learn dependency tree structures with
latent variables.1 Petrov et al2006) and Musillo
and Merlo (2008) have shown that learning PCFGs
and dependency grammars respectively with latent
variables can produce parsers with very good gen-
eralization performance. However, both these ap-
proaches rely on EM for parameter estimation and
can benefit from using spectral methods.
We propose a simple yet powerful latent vari-
able generative model for use with dependency pars-
1Actually, instead of using the model by Hsu et al2008)
we work with a related model proposed by Foster et al2012)
which addresses some of the shortcomings of the earlier model
which we detail below.
205
ing which has one hidden node for each word in
the sentence, like the one shown in Figure 1 and
work out the details for the parameter estimation
of the corresponding spectral learning model. At
a very high level, the parameter estimation of our
model involves collecting unigram, bigram and tri-
gram counts sensitive to the underlying dependency
structure of the given sentence.
Recently, Luque et al2012) have also proposed
a spectral method for dependency parsing, however
they deal with horizontal markovization and use hid-
den states to model sequential dependencies within a
word?s sequence of children. In contrast with that, in
this paper, we propose a spectral learning algorithm
where latent states are not restricted to HMM-like
distributions of modifier sequences for a particular
head, but instead allow information to be propagated
through the entire tree.
More recently, Cohen et al2012) have proposed
a spectral method for learning PCFGs.
Its worth noting that recent work by Parikh et al
(2011) also extends Hsu et al2008) to latent vari-
able dependency trees like us but under the restric-
tive conditions that model parameters are trained for
a specified, albeit arbitrary, tree topology.2 In other
words, all training sentences and test sentences must
have identical tree topologies. By doing this they al-
low for node-specific model parameters, but must re-
train the model entirely when a different tree topol-
ogy is encountered. Our model on the other hand al-
lows the flexibility and efficiency of processing sen-
tences with a variety of tree topologies from a single
training run.
Most of the current state-of-the-art dependency
parsers are discriminative parsers (Koo et al2008;
McDonald, 2006) due to the flexibility of represen-
tations which can be used as features leading to bet-
ter accuracies and the ease of reproducibility of re-
sults. However, unlike discriminative models, gen-
erative models can exploit unlabeled data. Also, as
is common in statistical parsing, re-ranking the out-
puts of a parser leads to significant reductions in er-
ror (Collins and Koo, 2005).
Since our spectral learning algorithm uses a gen-
2This can be useful in modeling phylogeny trees for in-
stance, but precludes most NLP applications, since there is a
need to model the full set of different tree topologies possible
in parsing.
h0
h1 h2
was
Kilroy here
Figure 1: Sample dependency parsing tree for ?Kilroy
was here?
erative model of words given a tree structure, it can
score a tree structure i.e. its probability of genera-
tion. Thus, it can be used to re-rank the n-best out-
puts of a given parser.
The remainder of the paper is organized as fol-
lows. In the next section we introduce the notation
and give a brief overview of the spectral algorithm
for learning HMMs (Hsu et al2008; Foster et al
2012). In Section 3 we describe our proposed model
for dependency parsing in detail and work out the
theory behind it. Section 4 provides experimental
evaluation of our model on Penn Treebank data. We
conclude with a brief summary and future avenues
for research.
2 Spectral Algorithm For Learning HMMs
In this section we describe the spectral algorithm for
learning HMMs.3
2.1 Notation
The HMM that we consider in this section is a se-
quence of hidden states h ? {1, . . . , k} that follow
the Markov property:
p(ht|h1, . . . , ht?1) = p(ht|ht?1)
and a sequence of observations x ? {1, . . . , n} such
that
p(xt|x1, . . . , xt?1, h1, . . . , ht) = p(xt|ht)
3As mentioned earlier, we use the model by Foster et al
(2012) which is conceptually similar to the one by Hsu et al
(2008), but does further dimensionality reduction and thus has
lower sample complexity. Also, critically, the fully reduced di-
mension model that we use generalizes much more cleanly to
trees.
206
The parameters of this HMM are:
? A vector pi of length k where pii = p(h1 = i):
The probability of the start state in the sequence
being i.
? A matrix T of size k ? k where
Ti,j = p(ht+1 = i|ht = j): The probability of
transitioning to state i, given that the previous
state was j.
? A matrix O of size n? k where
Oi,j = p(x = i|h = j): The probability of
state h emitting observation x.
Define ?j to be the vector of length n with a 1 in
the jth entry and 0 everywhere else, and diag(v) to
be the matrix with the entries of v on the diagonal
and 0 everywhere else.
The joint distribution of a sequence of observa-
tions x1, . . . , xm and a sequence of hidden states
h1, . . . , hm is:
p(x1, . . . ,xm, h1, . . . , hm)
= pih1
m?1?
j=2
Thj ,hj?1
m?
j=1
Oxj ,hj
Now, we can write the marginal probability of a
sequence of observations as
p(x1, . . . xm)
=
?
h1,...,hm
p(x1, . . . , xm, h1, . . . , hm)
which can be expressed in matrix form4 as:
p(x1, . . . , xm) = 1>AxmAxm?1 ? ? ?Am1pi
where Axm ? Tdiag(O
>?xm), and 1 is a k-
dimensional vector with every entry equal to 1.
A is called an ?observation operator?, and is ef-
fectively a third order tensor, and Axm which is a
matrix, gives the distribution vector over states at
timem+1 as a function of the state distribution vec-
tor at the current time m and the current observation
?xm . SinceAxm depends on the hidden state, it is not
observable, and hence cannot be directly estimated.
4This is essentially the matrix form of the standard dynamic
program (forward algorithm) used to estimate HMMs.
However, Hsu et al2008) and Foster et al2012)
showed that under certain conditions there exists a
fully observable representation of the observable op-
erator model.
2.2 Fully observable representation
Before presenting the model, we need to address a
few more points. First, let U be a ?representation
matrix? (eigenfeature dictionary) which maps each
observation to a reduced dimension space (n ? k)
that satisfies the conditions:
? U>O is invertible
? |Uij | < 1.
Hsu et al2008); Foster et al2012) discuss U
in more detail, but U can, for example, be obtained
by the SVD of the bigram probability matrix (where
Pij = p(xt+1 = i|xt = j)) or by doing CCA on
neighboring n-grams (Dhillon et al2011).
Letting yi = U>?xi , we have
p(x1, . . . , xm)
= c>?C(ym)C(ym?1) . . . C(y1)c1 (1)
where
c1 = ?
c? = ?
>??1
C(y) = K(y)??1
and ?, ? and K, described in more detail below, are
quantities estimated by frequencies of unigrams, bi-
grams, and trigrams in the observed (training) data.
Under the assumption that data is generated by
an HMM, the distribution p? obtained by substituting
the estimated values c?1, c??, and C?(y) into equation
(1) converges to p sufficiently fast as the amount of
training data increases, giving us consistent param-
eter estimates. For details of the convergence proof,
please see Hsu et al2008) and Foster et al2012).
3 Spectral Algorithm For Learning
Dependency Trees
In this section, we first describe a simple latent vari-
able generative model for dependency parsing. We
then define some extra notation and finally present
207
the details of the corresponding spectral learning al-
gorithm for dependency parsing, and prove that our
learning algorithm provides a consistent estimation
of the marginal probabilities.
It is worth mentioning that an alternate way of ap-
proaching the spectral estimation of latent states for
dependency parsing is by converting the dependency
trees into linear sequences from root-to-leaf and do-
ing a spectral estimation of latent states using Hsu
et al2008). However, this approach would not
give us the correct probability distribution over trees
as the probability calculations for different paths
through the trees are not independent. Thus, al-
though one could calculate the probability of a path
from the root to a leaf, one cannot generalize from
this probability to say anything about the neighbor-
ing nodes or words. Put another way, when a par-
ent has more than the one descendant, one has to be
careful to take into account that the hidden variables
at each child node are all conditioned on the hidden
variable of the parent.
3.1 A latent variable generative model for
dependency parsing
In the standard setting, we are given training exam-
ples where each training example consists of a se-
quence of words x1, . . . , xm together with a depen-
dency structure over those words, and we want to
estimate the probability of the observed structure.
This marginal probability estimates can then be used
to build an actual generative dependency parser or,
since the marginal probability is conditioned on the
tree structure, it can be used re-rank the outputs of a
parser.
As in the conventional HMM described in the pre-
vious section, we can define a simple latent variable
first order dependency parsing model by introduc-
ing a hidden variable hi for each word xi. The
joint probability of a sequence of observed nodes
x1, . . . , xm together with hidden nodes h1, . . . , hm
can be written as
p(x1, . . . ,xm, h1, . . . , hm)
= pih1
m?
j=2
td(j)(hj |hpa(j))
m?
j=1
o(xj |hj)
(2)
h1
h2 h3
y1
y2 y3
Figure 2: Dependency parsing tree with observed vari-
ables y1, y2, and y3.
where pa(j) is the parent of node j and d(j) ?
{L,R} indicates whether hj is a left or a right node
of hpa(j). For simplicity, the number of hidden and
observed nodes in our tree are the same, however
they are not required to be so.
As is the case with the conventional HMM, the
parameters used to calculate this joint probability
are unobservable, but it turns out that under suitable
conditions a fully observable model is also possible
for the dependency tree case with the parameteriza-
tion as described below.
3.2 Model parameters
We will define both the theoretical representations
of our observable parameters, and the sampling ver-
sions of these parameters. Note that in all the cases,
the estimated versions are unbiased estimates of the
theoretical quantities.
Define Td and T ud where d ? {L,R} to be the
hidden state transition matrices from parent to left
or right child, and from left or right child to parent
(hence the u for ?up?), respectively. In other words
(referring to Figure 2)
TR = t(h3|h1)
TL = t(h2|h1)
T uR = t(h1|h3)
T uL = t(h1|h2)
Let Ux(i) be the i
th entry of vector U>?x andG =
U>O. Further, recall the notation diag(v), which is
a matrix with elements of v on its diagonal, then:
? Define the k-dimensional vector ? (unigram
208
counts):
? = Gpi
[??]i =
n?
u=1
c?(u)Uu(i)
where c?(u) = c(u)N1 , c(u) is the count of ob-
servation u in the training sample, and N1 =?
u?n c(u).
? Define the k?k matrices ?L and ?R (left child-
parent and right child-parent bigram counts):
[??L]i,j =
n?
u=1
n?
v=1
c?L(u, v)Uu(j)Uv(i)
?L = GT
u
Ldiag(pi)G
>
[??R]i,j =
n?
u=1
n?
v=1
c?R(u, v)Uu(j)Uv(i)
?R = GT
u
Rdiag(pi)G
>
where c?L(u, v) =
cL(u,v)
N2L
, cL(u, v) is the count
of bigram (u, v) where u is the left child and
v is the parent in the training sample, and
N2L =
?
(u,v)?n?n cL(u, v). Define c?R(u, v)
similarly.
? Define k ? k ? k tensor K (left child-parent-
right child trigram counts):
K?i,j,l =
n?
u=1
n?
v=1
n?
w=1
c?(u, v, w)Uw(i)Uu(j)Uv(l)
K(y) = GTLdiag(G
>y)T uRdiag(pi)G
>
where c?(w, u, v) = c(w,u,v)N3 , c(w, u, v) is
the count of bigram (w, u, v) where w is
the left child, u is the parent and v is the
right child in the training sample, and N3 =?
(w,u,v)?n?n?n c(w, u, v).
? Define k?k matrices ?L and ?R (skip-bigram
counts (left child-right child) and (right child-
left child)) 5:
[??L]i,j =
n?
u=1
n?
v=1
n?
w=1
c?(u, v, w)Uw(i)Uu(j)
?L = GTLT
u
Rdiag(pi)G
>
[??R]i,j =
n?
u=1
n?
v=1
n?
w=1
c?(u, v, w)Uw(j)Uu(i)
?R = GTRT
u
Ldiag(pi)G
>
3.3 Parameter estimation
Using the above definitions, we can estimate the pa-
rameters of the model, namely ?,?L,?R,?L,?R
andK, from the training data and define observables
useful for the dependency model as6
c1 = ?
cT? = ?
T??1R
EL = ?L?
?1
R
ER = ?R?
?1
L
D(y) = E?1L K(y)?
?1
R
As we will see, these quantities allow us to recur-
sively compute the marginal probability of the de-
pendency tree, p?(x1, . . . , xm), in a bottom up man-
ner by using belief propagation.
To see this, let hch(i) be the set of hidden chil-
dren of hidden node i (in Figure 2 for instance,
hch(1) = {2, 3}) and let och(i) be the set of ob-
served children of hidden node i (in the same figure
och(i) = {1}). Then compute the marginal proba-
bility p(x1, . . . , xm) from Equation 2 as
ri(h) =
?
j?hch(i)
?j(h)
?
j?och(i)
o(xj |h) (3)
where ?i(h) is defined by summing over all
the hidden random variables i.e., ?i(h) =?
h? p(h
?|h)ri(h?).
This can be written in a compact matrix form as
??ri
> = 1>
?
j?hch(i)
diag(T>dj
??rj )
?
?
j?och(i)
diag(O>?xj ) (4)
5Note than ?R = ?TL , which is not immediately obvious
from the matrix representations.
6The details of the derivation follow directly from the matrix
versions of the variables.
209
where ??ri is a vector of size k (the dimensionality of
the hidden space) of values ri(h). Note that since in
Equation 2 we condition on whether xj is the left or
right child of its parent, we have separate transition
matrices for left and right transitions from a given
hidden node dj ? {L,R}.
The recursive computation can be written in terms
of observables as:
??ri
> = c>?
?
j?hch(i)
D(E>dj
??rj )
?
?
j?och(i)
D((U>U)?1U>?xj )
The final calculation for the marginal probability
of a given sequence is
p?(x1, . . . , xm) =
??r1
>c1 (5)
The spectral estimation procedure is described be-
low in Algorithm 1.
Algorithm 1 Spectral dependency parsing (Comput-
ing marginal probability of a tree.)
1: Input: Training examples- x(i) for i ? {1, . . . ,M}
along with dependency structures where each se-
quence x(i) = x(i)1 , . . . , x
(i)
mi .
2: Compute the spectral parameters ??, ??R, ??L, ??R,
??L, and K?
#Now, for a given sentence, we can recursively com-
pute the following:
3: for x(i)j for j ? {mi, . . . , 1} do
4: Compute:
??ri
> = c>?
?
j?hch(i)
D(E>dj
??rj )
?
?
j?och(i)
D((U>U)?1U>?xj )
5: end for
6: Finally compute
p?(x1, . . . , xmi) =
??r1
>c1
#The marginal probability of an entire tree.
3.4 Sample complexity
Our main theoretical result states that the above
scheme for spectral estimation of marginal proba-
bilities provides a guaranteed consistent estimation
scheme for the marginal probabilities:
Theorem 3.1. Let the sequence {x1, . . . , xm} be
generated by an k ? 2 state HMM. Suppose we are
given a U which has the property that U>O is in-
vertible, and |Uij | ? 1. Suppose we use equation
(5) to estimate the probability based on N indepen-
dent triples. Then
N ? Cm
k2
2
log
(
k
?
)
(6)
where Cm is specified in the appendix, implies that
1?  ?
?
?
?
?
p?(x1, . . . , xm)
p(x1, . . . , xm)
?
?
?
? ? 1 + 
holds with probability at least 1? ?.
Proof. A sketch of the proof, in the case without di-
rectional transition parameters, can be found in the
appendix. The proof with directional transition pa-
rameters is almost identical.
4 Experimental Evaluation
Since our algorithm can score any given tree struc-
ture by computing its marginal probability, a natu-
ral way to benchmark our parser is to generate n-
best dependency trees using some standard parser
and then use our algorithm to re-rank the candidate
dependency trees, e.g. using the log spectral prob-
ability as described in Algorithm 1 as a feature in a
discriminative re-ranker.
4.1 Experimental Setup
Our base parser was the discriminatively trained
MSTParser (McDonald, 2006), which implements
both first and second order parsers and is trained
using MIRA (Crammer et al2006) and used the
standard baseline features as described in McDon-
ald (2006).
We tested our methods on the English Penn Tree-
bank (Marcus et al1993). We use the standard
splits of Penn Treebank; i.e., we used sections 2-21
for training, section 22 for development and section
23 for testing. We used the PennConverter7 tool to
convert Penn Treebank from constituent to depen-
dency format. Following (McDonald, 2006; Koo
7http://nlp.cs.lth.se/software/treebank_
converter/
210
et al2008), we used the POS tagger by Ratnaparkhi
(1996) trained on the full training data to provide
POS tags for development and test sets and used 10-
way jackknifing to generate tags for the training set.
As is common practice we stripped our sentences of
all the punctuation. We evaluated our approach on
sentences of all lengths.
4.2 Details of spectral learning
For the spectral learning phase, we need to just col-
lect word counts from the training data as described
above, so there are no tunable parameters as such.
However, we need to have access to an attribute dic-
tionary U which contains a k dimensional represen-
tation for each word in the corpus. A possible way
of generating U as suggested by Hsu et al2008) is
by performing SVD on bigrams P21 and using the
left eigenvectors as U . We instead used the eigen-
feature dictionary proposed by Dhillon et al2011)
(LR-MVL) which is obtained by performing CCA
on neighboring words and has provably better sam-
ple complexity for rare words compared to the SVD
alternative.
We induced the LR-MVL embeddings for words
using the Reuters RCV1 corpus which contains
about 63 million tokens in 3.3 million sentences and
used their context oblivious embeddings as our esti-
mate of U . We experimented with different choices
of k (the size of the low dimensional projection)
on the development set and found k = 10 to work
reasonably well and fast. Using k = 10 we were
able to estimate our spectral learning parameters
?,?L,R,?L,R,K from the entire training data in un-
der 2 minutes on a 64 bit Intel 2.4 Ghz processor.
4.3 Re-ranking the outputs of MST parser
We could not find any previous work which de-
scribes features for discriminative re-ranking for de-
pendency parsing, which is due to the fact that un-
like constituency parsing, the base parsers for depen-
dency parsing are discriminative (e.g. MST parser)
which obviates the need for re-ranking as one could
add a variety of features to the baseline parser itself.
However, parse re-ranking is a good testbed for our
spectral dependency parser which can score a given
tree. So, we came up with a baseline set of features
to use in an averaged perceptron re-ranker (Collins,
2002). Our baseline features comprised of two main
Method Accuracy Complete
I Order
MST Parser (No RR) 90.8 37.2
RR w. Base. Features 91.3 37.5
RR w. Base. Features +log p? 91.7 37.8
II Order
MST Parser (No RR) 91.8 40.6
RR w. Base. Features 92.4 41.0
RR w. Base. Features +log p? 92.7 41.3
Table 1: (Unlabeled) Dependency Parse re-ranking re-
sults for English test set (Section 23). Note: 1). RR =
Re-ranking 2). Accuracy is the number of words which
correctly identified their parent and Complete is the num-
ber of sentences for which the entire dependency tree was
correct. 3). Base. Features are the two re-ranking fea-
tures described in Section 4.3. 4). log p? is the spectral log
probability feature.
features which capture information that varies across
the different n-best parses and moreover were not
used as features by the baseline MST parser, ?POS-
left-modifier ? POS-head ? POS-right-modifier?
and ?POS-left/right-modifier ? POS-head ? POS-
grandparent?8. In addition to that we used the log of
spectral probability (p?(x1, . . . , xm) - as calculated
using Algorithm 1) as a feature.
We used the MST parser trained on entire training
data to generate a list of n-best parses for the devel-
opment and test sets. The n-best parses for the train-
ing set were generated by 3-fold cross validation,
where we train on 2 folds to get the parses for the
third fold. In all our experiments we used n = 50.
The results are shown in Table 1. As can be seen,
the best results give up to 4.6% reduction in error
over the re-ranker which uses just the baseline set of
features.
5 Discussion and Future Work
Spectral learning of structured latent variable mod-
els in general is a promising direction as has been
shown by the recent interest in this area. It al-
lows us to circumvent the ubiquitous problem of get-
ting stuck in local minima when estimating the la-
tent variable models via EM. In this paper we ex-
8One might be able to come up with better features for de-
pendency parse re-ranking. Our goal in this paper was just to
get a reasonable baseline.
211
tended the spectral learning ideas to learn a simple
yet powerful dependency parser. As future work, we
are working on building an end-to-end parser which
would involve coming up with a spectral version of
the inside-outside algorithm for our setting. We are
also working on extending it to learn more power-
ful grammars e.g. split head-automata grammars
(SHAG) (Eisner and Satta, 1999).
6 Conclusion
In this paper we proposed a novel spectral method
for dependency parsing. Unlike EM trained gen-
erative latent variable models, our method does not
get stuck in local optima, it gives consistent param-
eter estimates, and it is extremely fast to train. We
worked out the theory of a simple yet powerful gen-
erative model and showed how it can be learned us-
ing a spectral method. As a pilot experimental evalu-
ation we showed the efficacy of our approach by us-
ing the spectral probabilities output by our model for
re-ranking the outputs of MST parser. Our method
reduced the error of the baseline re-ranker by up to
a moderate 4.6%.
7 Appendix
This appendix offers a sketch of the proof of The-
orem 1. The proof uses the following definitions,
which are slightly modified from those of Foster
et al2012).
Definition 1. Define ? as the smallest element of ?,
??1, ??1, and K(). In other words,
? ?min{min
i
|?i|,min
i,j
|??1ij |,mini,j
|??1ij |,
min
i,j,k
|Kijk|,min
i,j
|?ij |,min
i,j
|?ij |, }
where Kijk = K(?j)ik are the elements of the ten-
sor K().
Definition 2. Define ?k as the smallest singular
value of ? and ?.
The proof relies on the fact that a row vector mul-
tiplied by a series of matrices, and finally multiplied
by a column vector amounts to a sum over all possi-
ble products of individual entries in the vectors and
matrices. With this in mind, if we bound the largest
relative error of any particular entry in the matrix by,
say, ?, and there are, say, s parameters (vectors and
matrices) being multiplied together, then by simple
algebra the total relative error of the sum over the
products is bounded by ?s.
The proof then follows from two basic steps.
First, one must bound the maximal relative error, ?
for any particular entry in the parameters, which can
be done using central limit-type theorems and the
quantity ? described above. Then, to calculate the
exponent s one simply counts the number of param-
eters multiplied together when calculating the prob-
ability of a particular sequence of observations.
Since each hidden node is associated with exactly
one observed node, it follows that s = 12m + 2L,
where L is the number of levels (for instance in our
example ?Kilroy was here? there are two levels). s
can be easily computed for arbitrary tree topologies.
It follows from Foster et al2012) that we achieve
a sample complexity
N ?
128k2s2
2 ?2?4k
log
(
2k
?
)
?
?1
? ?? ?
2/s2
( s
?
1 + ? 1)2
(7)
leading to the theorem stated above.
Lastly, note that in reality one does not see ? and
?k but instead estimates of these quantities; Foster
et al2012) shows how to incorporate the accuracy
of the estimates into the sample complexity.
Acknowledgement: We would like to thank
Emily Pitler for valuable feedback on the paper.
References
Shay Cohen, Karl Stratos, Michael Collins, Dean
Foster, and Lyle Ungar. Spectral learning of
latent-variable pcfgs. In Association of Compu-
tational Linguistics (ACL), volume 50, 2012.
Michael Collins. Ranking algorithms for named-
entity extraction: boosting and the voted percep-
tron. In Proceedings of the 40th Annual Meet-
ing on Association for Computational Linguis-
tics, ACL ?02, pages 489?496, Stroudsburg, PA,
USA, 2002. Association for Computational Lin-
guistics. URL http://dx.doi.org/10.
3115/1073083.1073165.
Michael Collins and Terry Koo. Discriminative
reranking for natural language parsing. Comput.
212
Linguist., 31(1):25?70, March 2005. ISSN 0891-
2017.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai
Shalev-Shwartz, and Yoram Singer. Online
passive-aggressive algorithms. Journal of Ma-
chine Learning Research, 7:551?585, 2006.
A. P. Dempster, N. M. Laird, and D. B. Rubin. Max-
imum likelihood from incomplete data via the em
algorithm. JRSS, SERIES B, 39(1):1?38, 1977.
Paramveer S. Dhillon, Dean Foster, and Lyle Un-
gar. Multi-view learning of word embeddings via
cca. In Advances in Neural Information Process-
ing Systems (NIPS), volume 24, 2011.
Jason Eisner and Giorgio Satta. Efficient pars-
ing for bilexical context-free grammars and head-
automaton grammars. In Proceedings of the 37th
Annual Meeting of the Association for Computa-
tional Linguistics (ACL), pages 457?464, Univer-
sity of Maryland, June 1999. URL http://cs.
jhu.edu/?jason/papers/#acl99.
Dean Foster, Jordan Rodu, and Lyle Ungar. Spec-
tral dimensionality reduction for HMMs. ArXiV
http://arxiv.org/abs/1203.6130, 2012.
D Hsu, S M. Kakade, and Tong Zhang. A spec-
tral algorithm for learning hidden markov models.
arXiv:0811.4413v2, 2008.
Terry Koo, Xavier Carreras, and Michael Collins.
Simple semi-supervised dependency parsing. In
In Proc. ACL/HLT, 2008.
F. Luque, A. Quattoni, B. Balle, and X. Carreras.
Spectral learning for non-deterministic depen-
dency parsing. In EACL, 2012.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. Building a large annotated
corpus of english: the penn treebank. Comput.
Linguist., 19:313?330, June 1993. ISSN 0891-
2017.
Ryan McDonald. Discriminative learning and span-
ning tree algorithms for dependency parsing. PhD
thesis, University of Pennsylvania, Philadelphia,
PA, USA, 2006. AAI3225503.
Bernard Merialdo. Tagging english text with a prob-
abilistic model. Comput. Linguist., 20:155?171,
June 1994. ISSN 0891-2017.
Gabriele Antonio Musillo and Paola Merlo. Un-
lexicalised hidden variable models of split de-
pendency grammars. In Proceedings of the 46th
Annual Meeting of the Association for Computa-
tional Linguistics on Human Language Technolo-
gies: Short Papers, HLT-Short ?08, pages 213?
216, Stroudsburg, PA, USA, 2008. Association
for Computational Linguistics.
Ankur P. Parikh, Le Song, and Eric P. Xing. A spec-
tral algorithm for latent tree graphical models. In
ICML, pages 1065?1072, 2011.
Slav Petrov, Leon Barrett, Romain Thibaux, and
Dan Klein. Learning accurate, compact, and in-
terpretable tree annotation. In Proceedings of the
21st International Conference on Computational
Linguistics and the 44th annual meeting of the As-
sociation for Computational Linguistics, ACL-44,
pages 433?440, Stroudsburg, PA, USA, 2006. As-
sociation for Computational Linguistics.
Ariadna Quattoni, Michael Collins, and Trevor Dar-
rell. Conditional random fields for object recog-
nition. In In NIPS, pages 1097?1104. MIT Press,
2004.
Lawrence R. Rabiner. A tutorial on hidden markov
models and selected applications in speech recog-
nition. In Proceedings of the IEEE, pages 257?
286, 1989.
Adwait Ratnaparkhi. A Maximum Entropy Model
for Part-Of-Speech Tagging. In Eric Brill and
Kenneth Church, editors, Proceedings of the Em-
pirical Methods in Natural Language Processing,
pages 133?142, 1996.
Sebastiaan Terwijn. On the learnability of hidden
markov models. In Proceedings of the 6th Inter-
national Colloquium on Grammatical Inference:
Algorithms and Applications, ICGI ?02, pages
261?268, London, UK, UK, 2002. Springer-
Verlag. ISBN 3-540-44239-1.
213
Proceedings of the ACL 2010 Conference Short Papers, pages 377?381,
Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational Linguistics
Learning Better Data Representation using Inference-Driven Metric
Learning
Paramveer S. Dhillon
CIS Deptt., Univ. of Penn.
Philadelphia, PA, U.S.A
dhillon@cis.upenn.edu
Partha Pratim Talukdar?
Search Labs, Microsoft Research
Mountain View, CA, USA
partha@talukdar.net
Koby Crammer
Deptt. of Electrical Engg.
The Technion, Haifa, Israel
koby@ee.technion.ac.il
Abstract
We initiate a study comparing effective-
ness of the transformed spaces learned by
recently proposed supervised, and semi-
supervised metric learning algorithms
to those generated by previously pro-
posed unsupervised dimensionality reduc-
tion methods (e.g., PCA). Through a va-
riety of experiments on different real-
world datasets, we find IDML-IT, a semi-
supervised metric learning algorithm to be
the most effective.
1 Introduction
Because of the high-dimensional nature of NLP
datasets, estimating a large number of parameters
(a parameter for each dimension), often from a
limited amount of labeled data, is a challenging
task for statistical learners. Faced with this chal-
lenge, various unsupervised dimensionality reduc-
tion methods have been developed over the years,
e.g., Principal Components Analysis (PCA).
Recently, several supervised metric learning al-
gorithms have been proposed (Davis et al, 2007;
Weinberger and Saul, 2009). IDML-IT (Dhillon et
al., 2010) is another such method which exploits
labeled as well as unlabeled data during metric
learning. These methods learn a Mahalanobis dis-
tance metric to compute distance between a pair
of data instances, which can also be interpreted as
learning a transformation of the input data, as we
shall see in Section 2.1.
In this paper, we make the following contribu-
tions:
Even though different supervised and semi-
supervised metric learning algorithms have
recently been proposed, effectiveness of the
transformed spaces learned by them in NLP
? Research carried out while at the University of Penn-
sylvania, Philadelphia, PA, USA.
datasets has not been studied before. In
this paper, we address that gap: we com-
pare effectiveness of classifiers trained on the
transformed spaces learned by metric learn-
ing methods to those generated by previ-
ously proposed unsupervised dimensionality
reduction methods. We find IDML-IT, a
semi-supervised metric learning algorithm to
be the most effective.
2 Metric Learning
2.1 Relationship between Metric Learning
and Linear Projection
We first establish the well-known equivalence be-
tween learning a Mahalanobis distance measure
and Euclidean distance in a linearly transformed
space of the data (Weinberger and Saul, 2009). Let
A be a d?d positive definite matrix which param-
eterizes the Mahalanobis distance, dA(xi, xj), be-
tween instances xi and xj , as shown in Equation
1. Since A is positive definite, we can decompose
it as A = P>P , where P is another matrix of size
d? d.
dA(xi, xj) = (xi ? xj)
>A(xi ? xj) (1)
= (Pxi ? Pxj)
>(Pxi ? Pxj)
= dEuclidean(Pxi, Pxj)
Hence, computing Mahalanobis distance pa-
rameterized by A is equivalent to first projecting
the instances into a new space using an appropriate
transformation matrix P and then computing Eu-
clidean distance in the linearly transformed space.
In this paper, we are interested in learning a better
representation of the data (i.e., projection matrix
P ), and we shall achieve that goal by learning the
corresponding Mahalanobis distance parameterA.
We shall now review two recently proposed
metric learning algorithms.
377
2.2 Information-Theoretic Metric Learning
(ITML): Supervised
Information-Theoretic Metric Learning (ITML)
(Davis et al, 2007) assumes the availability of
prior knowledge about inter-instance distances. In
this scheme, two instances are considered simi-
lar if the Mahalanobis distance between them is
upper bounded, i.e., dA(xi, xj) ? u, where u
is a non-trivial upper bound. Similarly, two in-
stances are considered dissimilar if the distance
between them is larger than certain threshold l,
i.e., dA(xi, xj) ? l. Similar instances are rep-
resented by set S, while dissimilar instances are
represented by set D.
In addition to prior knowledge about inter-
instance distances, sometimes prior information
about the matrix A, denoted by A0, itself may
also be available. For example, Euclidean dis-
tance (i.e., A0 = I) may work well in some do-
mains. In such cases, we would like the learned
matrixA to be as close as possible to the prior ma-
trix A0. ITML combines these two types of prior
information, i.e., knowledge about inter-instance
distances, and prior matrix A0, in order to learn
the matrix A by solving the optimization problem
shown in (2).
min
A0
Dld(A,A0) (2)
s.t. tr{A(xi ? xj)(xi ? xj)
>} ? u,
?(i, j) ? S
tr{A(xi ? xj)(xi ? xj)
>} ? l,
?(i, j) ? D
whereDld(A,A0) = tr(AA
?1
0 )? log det(AA
?1
0 )
?n, is the LogDet divergence.
To handle situations where exactly solving the
problem in (2) is not possible, slack variables may
be introduced to the ITML objective. To solve this
optimization problem, an algorithm involving re-
peated Bregman projections is presented in (Davis
et al, 2007), which we use for the experiments re-
ported in this paper.
2.3 Inference-Driven Metric Learning
(IDML): Semi-Supervised
Notations: We first define the necessary notations.
Let X be the d ? n matrix of n instances in a
d-dimensional space. Out of the n instances, nl
instances are labeled, while the remaining nu in-
stances are unlabeled, with n = nl+nu. Let S be
a n ? n diagonal matrix with Sii = 1 iff instance
xi is labeled. m is the total number of labels. Y
is the n?m matrix storing training label informa-
tion, if any. Y? is the n?m matrix of estimated la-
bel information, i.e., output of any classifier, with
Y?il denoting score of label l at node i. .
The ITML metric learning algorithm, which we
reviewed in Section 2.2, is supervised in nature,
and hence it does not exploit widely available un-
labeled data. In this section, we review Infer-
ence Driven Metric Learning (IDML) (Algorithm
1) (Dhillon et al, 2010), a recently proposed met-
ric learning framework which combines an exist-
ing supervised metric learning algorithm (such as
ITML) along with transductive graph-based la-
bel inference to learn a new distance metric from
labeled as well as unlabeled data combined. In
self-training styled iterations, IDML alternates be-
tween metric learning and label inference; with
output of label inference used during next round
of metric learning, and so on.
IDML starts out with the assumption that ex-
isting supervised metric learning algorithms, such
as ITML, can learn a better metric if the number
of available labeled instances is increased. Since
we are focusing on the semi-supervised learning
(SSL) setting with nl labeled and nu unlabeled
instances, the idea is to automatically label the
unlabeled instances using a graph based SSL al-
gorithm, and then include instances with low as-
signed label entropy (i.e., high confidence label
assignments) in the next round of metric learning.
The number of instances added in each iteration
depends on the threshold ?1. This process is con-
tinued until no new instances can be added to the
set of labeled instances, which can happen when
either all the instances are already exhausted, or
when none of the remaining unlabeled instances
can be assigned labels with high confidence.
The IDML framework is presented in Algo-
rithm 1. In Line 3, any supervised metric
learner, such as ITML, may be used as the
METRICLEARNER. Using the distance metric
learned in Line 3, a new k-NN graph is constructed
in Line 4 , whose edge weight matrix is stored in
W . In Line 5 , GRAPHLABELINF optimizes over
the newly constructed graph, the GRF objective
(Zhu et al, 2003) shown in (3).
min
Y? ?
tr{Y?
?>LY?
?
}, s.t. S?Y? = S?Y?
?
(3)
where L = D ?W is the (unnormalized) Lapla-
1During the experiments in Section 3, we set ? = 0.05
378
Algorithm 1: Inference Driven Metric Learn-
ing (IDML)
Input: instancesX , training labels Y , training
instance indicator S, label entropy threshold ?,
neighborhood size k
Output: Mahalanobis distance parameter A
1: Y? ? Y , S? ? S
2: repeat
3: A? METRICLEARNER(X, S?, Y? )
4: W ? CONSTRUCTKNNGRAPH(X,A, k)
5: Y?
?
? GRAPHLABELINF(W, S?, Y? )
6: U ? SELECTLOWENTINST(Y?
?
, S?, ?)
7: Y? ? Y? + UY?
?
8: S? ? S? + U
9: until convergence (i.e., Uii = 0, ?i)
10: return A
cian, and D is a diagonal matrix with Dii =?
jWij . The constraint, S?Y? = S?Y?
?
, in (3)
makes sure that labels on training instances are not
changed during inference. In Line 6, a currently
unlabeled instance xi (i.e., S?ii = 0) is consid-
ered a new labeled training instance, i.e., Uii = 1,
for next round of metric learning if the instance
has been assigned labels with high confidence in
the current iteration, i.e., if its label distribution
has low entropy (i.e., ENTROPY(Y?
?
i:) ? ?). Fi-
nally in Line 7, training instance label information
is updated. This iterative process is continued till
no new labeled instance can be added, i.e., when
Uii = 0 ?i. IDML returns the learned matrix A
which can be used to compute Mahalanobis dis-
tance using Equation 1.
3 Experiments
3.1 Setup
Dataset Dimension Balanced
Electronics 84816 Yes
Books 139535 Yes
Kitchen 73539 Yes
DVDs 155465 Yes
WebKB 44261 Yes
Table 1: Description of the datasets used in Sec-
tion 3. All datasets are binary with 1500 total in-
stances in each.
Description of the datasets used during experi-
ments in Section 3 are presented in Table 1. The
first four datasets ? Electronics, Books, Kitchen,
and DVDs ? are from the sentiment domain and
previously used in (Blitzer et al, 2007). WebKB
is a text classification dataset derived from (Sub-
ramanya and Bilmes, 2008). For details regard-
ing features and data pre-processing, we refer the
reader to the origin of these datasets cited above.
One extra preprocessing that we did was that we
only considered features which occurred more 20
times in the entire dataset to make the problem
more computationally tractable and also since the
infrequently occurring features usually contribute
noise. We use classification error (lower is better)
as the evaluation metric. We experiment with the
following ways of estimating transformation ma-
trix P :
Original2: We set P = I , where I is the
d ? d identity matrix. Hence, the data is not
transformed in this case.
RP: The data is first projected into a lower
dimensional space using the Random Pro-
jection (RP) method (Bingham and Mannila,
2001). Dimensionality of the target space
was set at d
?
= logn
2log 1
, as prescribed in
(Bingham and Mannila, 2001). We use the
projection matrix constructed by RP as P . 
was set to 0.25 for the experiments in Sec-
tion 3, which has the effect of projecting the
data into a much lower dimensional space
(84 for the experiments in this section). This
presents an interesting evaluation setting as
we already run evaluations in much higher di-
mensional space (e.g., Original).
PCA: Data instances are first projected into
a lower dimensional space using Principal
Components Analysis (PCA) (Jolliffe, 2002)
. Following (Weinberger and Saul, 2009), di-
mensionality of the projected space was set
at 250 for all experiments. In this case, we
used the projection matrix generated by PCA
as P .
ITML: A is learned by applying ITML (see
Section 2.2) on the Original space (above),
and then we decompose A as A = P>P to
obtain P .
2Note that ?Original? in the results tables refers to orig-
inal space with features occurring more than 20 times. We
also ran experiments with original set of features (without
any thresholding) and the results were worse or comparable
to the ones reported in the tables.
379
Datasets Original RP PCA ITML IDML-IT
?? ? ?? ? ?? ? ?? ? ?? ?
Electronics 31.3? 0.9 42.5? 1.0 46.4? 2.0 33.0? 1.0 30.7?0.7
Books 37.5? 1.1 45.0? 1.1 34.8? 1.4 35.0? 1.1 32.0?0.9
Kitchen 33.7? 1.0 43.0? 1.1 34.0? 1.6 30.9? 0.7 29.0?1.0
DVDs 39.0? 1.2 47.7? 1.2 36.2? 1.6 37.0? 0.8 33.9?1.0
WebKB 31.4? 0.9 33.0? 1.0 27.9? 1.3 28.9? 1.0 25.5?1.0
Table 2: Comparison of SVM % classification errors (lower is better), with 50 labeled instances (Sec.
3.2). nl=50. and nu = 1450. All results are averaged over ten trials. All hyperparameters are tuned on a
separate random split.
Datasets Original RP PCA ITML IDML-IT
?? ? ?? ? ?? ? ?? ? ?? ?
Electronics 27.0? 0.9 40.0? 1.0 41.2? 1.0 27.5? 0.8 25.3?0.8
Books 31.0? 0.7 42.9? 0.6 31.3? 0.7 29.9? 0.5 27.7?0.7
Kitchen 26.3? 0.5 41.9? 0.7 27.0? 0.9 26.1? 0.8 24.8?0.9
DVDs 34.7? 0.4 46.8? 0.6 32.9? 0.8 34.0? 0.8 31.8?0.9
WebKB 25.7? 0.5 31.1? 0.5 24.9? 0.6 25.6? 0.4 23.9?0.4
Table 3: Comparison of SVM % classification errors (lower is better), with 100 labeled instances (Sec.
3.2). nl=100. and nu = 1400. All results are averaged over ten trials. All hyperparameters are tuned on
a separate random split.
IDML-IT: A is learned by applying IDML
(Algorithm 1) (see Section 2.3) on the Orig-
inal space (above); with ITML used as
METRICLEARNER in IDML (Line 3 in Al-
gorithm 1). In this case, we treat the set of
test instances (without their gold labels) as
the unlabeled data. In other words, we essen-
tially work in the transductive setting (Vap-
nik, 2000). Once again, we decompose A as
A = P>P to obtain P .
We also experimented with the supervised
large-margin metric learning algorithm (LMNN)
presented in (Weinberger and Saul, 2009). We
found ITML to be more effective in practice than
LMNN, and hence we report results based on
ITML only. Each input instance, x, is now pro-
jected into the transformed space as Px. We
now train different classifiers on this transformed
space. All results are averaged over ten random
trials.
3.2 Supervised Classification
We train a SVM classifier, with an RBF kernel, on
the transformed space generated by the projection
matrix P . SVM hyperparameter, C and RBF ker-
nel bandwidth, were tuned on a separate develop-
ment split. Experimental results with 50 and 100
labeled instances are shown in Table 2, and Ta-
ble 3, respectively. From these results, we observe
that IDML-IT consistently achieves the best per-
formance across all experimental settings. We also
note that in Table 3, performance difference be-
tween ITML and IDML-IT in the Electronics and
Kitchen domains are statistically significant.
3.3 Semi-Supervised Classification
In this section, we trained the GRF classifier (see
Equation 3), a graph-based semi-supervised learn-
ing (SSL) algorithm (Zhu et al, 2003), using
Gaussian kernel parameterized by A = P>P to
set edge weights. During graph construction, each
node was connected to its k nearest neighbors,
with k treated as a hyperparameter and tuned on
a separate development set. Experimental results
with 50 and 100 labeled instances are shown in
Table 4, and Table 5, respectively. As before, we
experimented with nl = 50 and nl = 100. Once
again, we observe that IDML-IT is the most effec-
tive method, with the GRF classifier trained on the
data representation learned by IDML-IT achieving
best performance in all settings. Here also, we ob-
serve that IDML-IT achieves the best performance
across all experimental settings.
380
Datasets Original RP PCA ITML IDML-IT
?? ? ?? ? ?? ? ?? ? ?? ?
Electronics 47.9? 1.1 49.0? 1.2 43.2? 0.9 34.9? 0.5 34.0?0.5
Books 50.0? 1.0 49.4? 1.0 47.9? 0.7 42.1? 0.7 40.6?0.7
Kitchen 49.8? 1.1 49.6? 0.9 48.6? 0.8 31.1? 0.5 30.0?0.5
DVDs 50.1? 0.5 49.9? 0.7 49.4? 0.6 42.1? 0.4 41.2?0.5
WebKB 33.1? 0.4 33.1? 0.3 33.1? 0.3 30.0? 0.4 28.7?0.5
Table 4: Comparison of transductive % classification errors (lower is better) over graphs constructed
using different methods (see Section 3.3), with nl = 50 and nu = 1450. All results are averaged over
ten trials. All hyperparameters are tuned on a separate random split.
Datasets Original RP PCA ITML IDML-IT
?? ? ?? ? ?? ? ?? ? ?? ?
Electronics 43.5? 0.7 47.2? 0.8 39.1? 0.7 31.3? 0.2 30.8?0.3
Books 48.3? 0.5 48.9? 0.3 43.3? 0.4 35.2? 0.5 33.3?0.6
Kitchen 45.3? 0.6 48.2? 0.5 41.0? 0.7 30.7? 0.6 29.9?0.3
DVDs 48.6? 0.3 49.3? 0.5 45.9? 0.5 42.6? 0.4 41.7?0.3
WebKB 33.4? 0.4 33.4? 0.4 33.4? 0.3 30.4? 0.5 28.6?0.7
Table 5: Comparison of transductive % classification errors (lower is better) over graphs constructed
using different methods (see Section 3.3), with nl = 100 and nu = 1400. All results are averaged over
ten trials. All hyperparameters are tuned on a separate random split.
4 Conclusion
In this paper, we compared the effectiveness
of the transformed spaces learned by recently
proposed supervised, and semi-supervised metric
learning algorithms to those generated by previ-
ously proposed unsupervised dimensionality re-
duction methods (e.g., PCA). To the best of our
knowledge, this is the first study of its kind in-
volving NLP datasets. Through a variety of ex-
periments on different real-world NLP datasets,
we demonstrated that supervised as well as semi-
supervised classifiers trained on the space learned
by IDML-IT consistently result in the lowest clas-
sification errors. Encouraged by these early re-
sults, we plan to explore further the applicability
of IDML-IT in other NLP tasks (e.g., entity classi-
fication, word sense disambiguation, polarity lexi-
con induction, etc.) where better representation of
the data is a pre-requisite for effective learning.
Acknowledgments
Thanks to Kuzman Ganchev for providing detailed
feedback on a draft of this paper. This work
was supported in part by NSF IIS-0447972 and
DARPA HRO1107-1-0029.
References
E. Bingham and H. Mannila. 2001. Random projec-
tion in dimensionality reduction: applications to im-
age and text data. In ACM SIGKDD.
J. Blitzer, M. Dredze, and F. Pereira. 2007. Biogra-
phies, bollywood, boom-boxes and blenders: Do-
main adaptation for sentiment classification. In
ACL.
J.V. Davis, B. Kulis, P. Jain, S. Sra, and I.S. Dhillon.
2007. Information-theoretic metric learning. In
ICML.
P. S. Dhillon, P. P. Talukdar, and K. Crammer. 2010.
Inference-driven metric learning for graph construc-
tion. Technical Report MS-CIS-10-18, CIS Depart-
ment, University of Pennsylvania, May.
IT Jolliffe. 2002. Principal component analysis.
Springer verlag.
A. Subramanya and J. Bilmes. 2008. Soft-Supervised
Learning for Text Classification. In EMNLP.
V.N. Vapnik. 2000. The nature of statistical learning
theory. Springer Verlag.
K.Q. Weinberger and L.K. Saul. 2009. Distance metric
learning for large margin nearest neighbor classifica-
tion. The Journal of Machine Learning Research.
X. Zhu, Z. Ghahramani, and J. Lafferty. 2003. Semi-
supervised learning using Gaussian fields and har-
monic functions. In ICML.
381
