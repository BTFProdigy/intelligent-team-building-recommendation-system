Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1190?1199,
Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational Linguistics
An Affect-Enriched Dialogue Act Classification Model  for Task-Oriented Dialogue 
Kristy  Elizabeth  Boyer Joseph F. Grafsgaard Eun Young  Ha Robert  Phillips* James C.  Lester  Department of Computer Science North Carolina State University Raleigh, NC, USA  * Dual Affiliation with Applied Research Associates, Inc. Raleigh, NC, USA  {keboyer, jfgrafsg, eha, rphilli, lester}@ncsu.edu 
 
 
Abstract 
Dialogue act classification is a central chal-lenge for dialogue systems. Although the im-portance of emotion in human dialogue is widely recognized, most dialogue act classifi-cation models make limited or no use of affec-tive channels in dialogue act classification. This paper presents a novel affect-enriched dialogue act classifier for task-oriented dia-logue that models facial expressions of users, in particular, facial expressions related to con-fusion. The findings indicate that the affect-enriched classifiers perform significantly bet-ter for distinguishing user requests for feed-back and grounding dialogue acts within textual dialogue. The results point to ways in which dialogue systems can effectively lever-age affective channels to improve dialogue act classification.  1 Introduction Dialogue systems aim to engage users in rich, adaptive natural language conversation. For these systems, understanding the role of a user?s utter-ance in the broader context of the dialogue is a key challenge (Sridhar, Bangalore, & Narayanan, 2009). Central to this endeavor is dialogue act classification, which categorizes the intention be-hind the user?s move (e.g., asking a question, providing declarative information). Automatic dia-logue act classification has been the focus of a 
large body of research, and a variety of approach-es, including sequential models (Stolcke et al, 2000), vector-based models (Sridhar, Bangalore, & Narayanan, 2009), and most recently, feature-enhanced latent semantic analysis (Di Eugenio, Xie, & Serafin, 2010), have shown promise. These models may be further improved by leveraging regularities of the dialogue from both linguistic and extra-linguistic sources. Users? expressions of emotion are one such source. Human interaction has long been understood to include rich phenomena consisting of verbal and nonverbal cues, with facial expressions playing a vital role (Knapp & Hall, 2006; McNeill, 1992; Mehrabian, 2007; Russell, Bachorowski, & Fernandez-Dols, 2003; Schmidt & Cohn, 2001). While the importance of emotional expressions in dialogue is widely recognized, the majority of dia-logue act classification projects have focused either peripherally (or not at all) on emotion, such as by leveraging acoustic and prosodic features of spo-ken utterances to aid in online dialogue act classi-fication (Sridhar, Bangalore, & Narayanan, 2009). Other research on emotion in dialogue has in-volved detecting affect and adapting to it within a dialogue system (Forbes-Riley, Rotaru, Litman, & Tetreault, 2009; L?pez-C?zar, Silovsky, & Griol, 2010), but this work has not explored leveraging affect information for automatic user dialogue act classification. Outside of dialogue, sentiment anal-ysis within discourse is an active area of research (L?pez-C?zar et al, 2010), but it is generally lim-
1190
ited to modeling textual features and not multi-modal expressions of emotion such as facial ac-tions. Such multimodal expressions have only just begun to be explored within corpus-based dialogue research (Calvo & D'Mello, 2010; Cavicchio, 2009).   This paper presents a novel affect-enriched dia-logue act classification approach that leverages knowledge of users? facial expressions during computer-mediated textual human-human dia-logue. Intuitively, the user?s affective state is a promising source of information that may help to distinguish between particular dialogue acts (e.g., a confused user may be more likely to ask a ques-tion). We focus specifically on occurrences of stu-dents? confusion-related facial actions during task-oriented tutorial dialogue.  Confusion was selected as the focus of this work for several reasons. First, confusion is known to be prevalent within tutoring, and its implications for student learning are thought to run deep (Graesser, Lu, Olde, Cooper-Pye, & Whitten, 2005). Second, while identifying the ?ground truth? of emotion based on any external display by a user presents challenges, prior research has demonstrated a correlation between particular faci-al action units and confusion during learning (Craig, D'Mello, Witherspoon, Sullins, & Graesser, 2004; D'Mello, Craig, Sullins, & Graesser, 2006; McDaniel et al, 2007). Finally, automatic facial action recognition technologies are developing rap-idly, and confusion-related facial action events are among those that can be reliably recognized auto-matically (Bartlett et al, 2006; Cohn, Reed, Ambadar, Xiao, & Moriyama, 2004; Pantic & Bartlett, 2007; Zeng, Pantic, Roisman, & Huang, 2009). This promising development bodes well for the feasibility of automatic real-time confusion detection within dialogue systems.  2 Background and Related Work 2.1 Dialogue Act Classification Because of the importance of dialogue act classifi-cation within dialogue systems, it has been an ac-tive area of research for some time. Early work on automatic dialogue act classification modeled dis-course structure with hidden Markov models, ex-perimenting with lexical and prosodic features, and applying the dialogue act model as a constraint to 
aid in automatic speech recognition (Stolcke et al, 2000). In contrast to this sequential modeling ap-proach, which is best suited to offline processing, recent work has explored how lexical, syntactic, and prosodic features perform for online dialogue act tagging (when only partial dialogue sequences are available) within a maximum entropy frame-work (Sridhar, Bangalore, & Narayanan, 2009). A recently proposed alternative approach involves treating dialogue utterances as documents within a latent semantic analysis framework, and applying feature enhancements that incorporate such infor-mation as speaker and utterance duration (Di Eugenio et al, 2010). Of the approaches noted above, the modeling framework presented in this paper is most similar to the vector-based maximum entropy approach of Sridhar et al (2009). Howev-er, it takes a step beyond the previous work by in-cluding multimodal affective displays, specifically facial expressions, as features available to an af-fect-enriched dialogue act classification model. 2.2 Detecting Emotions in Dialogue Detecting emotional states during spoken dialogue is an active area of research, much of which focus-es on detecting frustration so that a user can be automatically transferred to a human dialogue agent (L?pez-C?zar et al, 2010). Research on spo-ken dialogue has leveraged lexical features along with discourse cues and acoustic information to classify user emotion, sometimes at a coarse grain along a positive/negative axis (Lee & Narayanan, 2005). Recent work on an affective companion agent has examined user emotion classification within conversational speech (Cavazza et al, 2010). In contrast to that spoken dialogue research, the work in this paper is situated within textual dialogue, a widely used modality of communica-tion for which a deeper understanding of user af-fect may substantially improve system performance. While many projects have focused on linguistic cues, recent work has begun to explore numerous channels for affect detection including facial ac-tions, electrocardiograms, skin conductance, and posture sensors (Calvo & D'Mello, 2010). A recent project in a map task domain investigates some of these sources of affect data within task-oriented dialogue (Cavicchio, 2009). Like that work, the current project utilizes facial action tagging, for 
1191
which promising automatic technologies exist (Bartlett et al, 2006; Pantic & Bartlett, 2007; Zeng, Pantic, Roisman, & Huang, 2009). However, we leverage the recognized expressions of emotion for the task of dialogue act classification.  2.3 Categorizing Emotions within Dialogue and Discourse Sets of emotion taxonomies for discourse and dia-logue are often application-specific, for example, focusing on the frustration of users who are inter-acting with a spoken dialogue system (L?pez-C?zar et al, 2010), or on uncertainty expressed by students while interacting with a tutor (Forbes-Riley, Rotaru, Litman, & Tetreault, 2007). In con-trast, the most widely utilized emotion frameworks are not application-specific; for example, Ekman?s Facial Action Coding System (FACS) has been widely used as a rigorous technique for coding fa-cial movements based on human facial anatomy (Ekman & Friesen, 1978).  Within this framework, facial movements are categorized into facial action units, which represent discrete movements of mus-cle groups. Additionally, facial action descriptors (for movements not derived from facial muscles) and movement and visibility codes are included. Ekman?s basic emotions (Ekman, 1999) have been used in recent work on classifying emotion ex-pressed within blog text (Das & Bandyopadhyay, 2009), while other recent work (Nguyen, 2010) utilizes Russell?s core affect model (Russell, 2003) for a similar task. During tutorial dialogue, students may not fre-quently experience Ekman?s basic emotions of happiness, sadness, anger, fear, surprise, and dis-gust. Instead, students appear to more frequently experience cognitive-affective states such as flow and confusion (Calvo & D'Mello, 2010). Our work leverages Ekman?s facial tagging scheme to identi-fy a particular facial action unit, Action Unit 4 (AU4), that has been observed to correlate with confusion (Craig, D'Mello, Witherspoon, Sullins, & Graesser, 2004; D'Mello, Craig, Sullins, & Graesser, 2006; McDaniel et al, 2007).   2.4 Importance of Confusion in Tutorial Dia-logue Among the affective states that students experience during tutorial dialogue, confusion is prevalent, and its implications for student learning are signif-
icant. Confusion is associated with cognitive dise-quilibrium, a state in which students? existing knowledge is inconsistent with a novel learning experience (Graesser, Lu, Olde, Cooper-Pye, & Whitten, 2005). Students may express such confu-sion within dialogue as uncertainty, to which hu-man tutors often adapt in a context-dependent fashion (Forbes-Riley et al, 2007). Moreover, im-plementing adaptations to student uncertainty with-in a dialogue system can improve the effectiveness of the system (Forbes-Riley et al, 2009).  For tutorial dialogue, the importance of under-standing student utterances is paramount for a sys-tem to positively impact student learning (Dzikovska, Moore, Steinhauser, & Campbell, 2010). The importance of frustration as a cogni-tive-affective state during learning suggests that the presence of student confusion may serve as a useful constraining feature for dialogue act classi-fication of student utterances. This paper explores the use of facial expression features in this way.  3 Task-Oriented Dialogue Corpus The corpus was collected during a textual human-human tutorial dialogue study in the domain of introductory computer science (Boyer, Phillips, et al, 2010). Students solved an introductory com-puter programming problem and carried on textual dialogue with tutors, who viewed a synchronized version of the students? problem-solving work-space. The original corpus consists of 48 dia-logues, one per student. Each student interacted with one of two tutors. Facial videos of students were collected using built-in webcams, but were not shown to the tutors. Video quality was ranked based on factors such as obscured foreheads due to hats or hair, and improper camera position result-ing in students? faces not being fully captured on the video. The highest-quality set contained 14 videos, and these videos were used in this analysis. They have a total running time of 11 hours and 55 minutes, and include dialogues with three female subjects and eleven male subjects.  3.1 Dialogue act annotation The dialogue act annotation scheme (Table 1) was applied manually. The kappa statistic for inter-annotator agreement on a 10% subset of the corpus was ?=0.80, indicating good reliability.   
1192
Table 1. Dialogue act tags and relative frequencies across fourteen dialogues in video corpus Student Dialogue Act Example Rel. Freq. EXTRA-DOMAIN (EX) Little sleep deprived today .08 GROUNDING (G) Ok or Thanks .21 NEGATIVE FEEDBACK WITH ELABORATION (NE) I?m still confused on what this next for loop is doing. .02 NEGATIVE FEEDBACK (N) I don?t see the diff. .04 POSITIVE FEEDBACK WITH ELABORATION (PE) 
It makes sense now that you explained it, but I never used an else if in any of my other programs .04 POSITIVE FEEDBACK (P) Second part complete. .11 QUESTION (Q) Why couldn?t I have said if (i<5) .11 STATEMENT (S) i is my only index .07 
REQUEST FOR FEEDBACK (RF) So I need to create a new method that sees how many elements are in my array? .16 RESPONSE (RSP) You mean not the length but the contents .14 UNCERTAIN FEEDBACK WITH ELABORATION (UE) I?m trying to remember how to copy arrays .008 UNCERTAIN FEEDBACK (U) Not quite yet .008  3.2 Task action annotation The tutoring sessions were task-oriented, focusing on a computer programming exercise. The task had several subtasks consisting of programming mod-ules to be implemented by the student. Each of those subtasks also had numerous fine-grained goals, and student task actions either contributed or did not contribute to the goals. Therefore, to obtain a rich representation of the task, a manual annota-tion along two dimensions was conducted (Boyer, Phillips, et al, 2010). First, the subtask structure was annotated hierarchically, and then each task action was labeled for correctness according to the requirements of the assignment. Inter-annotator agreement was computed on 20% of the corpus at the leaves of the subtask tagging scheme, and re-
sulted in a simple kappa of ?=.56. However, the leaves of the annotation scheme feature an implicit ordering (subtasks were completed in order, and adjacent subtasks are semantically more similar than subtasks at a greater distance); therefore, a weighted kappa is also meaningful to consider for this annotation. The weighted kappa is ?weighted=.80. An annotated excerpt of the corpus is displayed in Table 2.   Table 2. Excerpt from corpus illustrating annota-tions and interplay between dialogue and task 13:38:09 Student: How do I know where to end? [RF] 13:38:26 Tutor: Well you told me how to get how many elements in an array by using .length right? 13:38:26 Student: [Task action:  Subtask 1-a-iv, Buggy] 13:38:56 Tutor: Great 13:38:56 Student: [Task action: Subtask 1-a-v, Correct] 13:39:35 Student: Well is it "array.length"? [RF]  **Facial Expression: AU4 13:39:46 Tutor: You just need to use the correct array name 13:39:46 Student: [Task action:  Subtask 1-a-iv, Buggy] 3.3 Lexical and Syntactic Features In addition to the manually annotated dialogue and task features described above, syntactic features of each utterance were automatically extracted using the Stanford Parser (De Marneffe et al, 2006). From the phrase structure trees, we extracted the top-most syntactic node and its first two children. In the case where an utterance consisted of more than one sentence, only the phrase structure tree of the first sentence was considered. Individual word tokens in the utterances were further processed with the Porter Stemmer (Porter, 1980) in the NLTK package (Loper & Bird, 2004). Our prior work has shown that these lexical and syntactic features are highly predictive of dialogue acts dur-ing task-oriented tutorial dialogue (Boyer, Ha et al 2010).  
1193
4 Facial Action Tagging An annotator who was certified in the Facial Ac-tion Coding System (FACS) (Ekman, Friesen, & Hager, 2002) tagged the video corpus consisting of fourteen dialogues. The FACS certification process requires annotators to pass a test designed to ana-lyze their agreement with reference coders on a set of spontaneous facial expressions (Ekman & Rosenberg, 2005). This annotator viewed the vide-os continuously and paused the playback whenever notable facial displays of Action Unit 4 (AU4: Brow Lowerer) were seen. This action unit was chosen for this study based on its correlations with confusion in prior research (Craig, D'Mello, Witherspoon, Sullins, & Graesser, 2004; D'Mello, Craig, Sullins, & Graesser, 2006; McDaniel et al, 2007). To establish reliability of the annotation, a se-cond FACS-certified annotator independently an-notated 36% of the video corpus (5 of 14 dialogues), chosen randomly after stratification by gender and tutor. This annotator followed the same method as the first annotator, pausing the video at any point to tag facial action events. At any given time in the video, the coder was first identifying whether an action unit event existed, and then de-scribing the facial movements that were present. The annotators also specified the beginning and ending time of each event. In this way, the action unit event tags spanned discrete durations of vary-ing length, as specified by the coders. Because the two coders were not required to tag at the same point in time, but rather were permitted the free-dom to stop the video at any point where they felt a notable facial action event occurred, calculating agreement between annotators required discretiz-ing the continuous facial action time windows across the tutoring sessions. This discretization was performed at granularities of 1/4, 1/2, 3/4, and 1 second, and inter-rater reliability was calculated at each level of granularity (Table 3). Windows in which both annotators agreed that no facial action event was present were tagged by default as neu-tral. Figure 1 illustrates facial expressions that dis-play facial Action Unit 4. 
  Table 3. Kappa values for inter-annotator agree-ment on facial action events  Granularity  ? sec ? sec ? sec 1 sec Presence of AU4 (Brow Lowerer)  .84 .87 .86 .86   
  
  Figure 1. Facial expressions displaying AU4 (Brow Lowerer)  Despite the fact that promising automatic ap-proaches exist to identifying many facial action units (Bartlett et al, 2006; Cohn, Reed, Ambadar, Xiao, & Moriyama, 2004; Pantic & Bartlett, 2007; Zeng, Pantic, Roisman, & Huang, 2009), manual annotation was selected for this project for two reasons. First, manual annotation is more robust than automatic recognition of facial action units, and manual annotation facilitated an exploratory, comprehensive view of student facial expressions during learning through task-oriented dialogue. Although a detailed discussion of the other emo-tions present in the corpus is beyond the scope of this paper, Figure 2 illustrates some other sponta-neous student facial expressions that differ from those associated with confusion.    
1194
   
  Figure 2. Other facial expressions from the corpus 5 Models The goal of the modeling experiment was to de-termine whether the addition of confusion-related facial expression features significantly boosts dia-logue act classification accuracy for student utter-ances.  5.1 Features We take a vector-based approach, in which the fea-tures consist of the following:  Utterance Features ? Dialogue act features: Manually annotated dialogue act for the past three utterances. These features include tutor dialogue acts, annotated with a scheme analogous to that used to annotate student utterances (Boyer et al, 2009). ? Speaker: Speaker for past three utterances ? Lexical features: Word unigrams ? Syntactic features: Top-most syntactic node and its first two children  Task-based Features ? Subtask: Hierarchical subtask structure for past three task actions (semantic pro-gramming actions taken by student) ? Correctness: Correctness of past three task actions taken by student ? Preceded by task: Indicator for whether the most recent task action immediately pre-ceded the target utterance, or whether it 
was immediately preceded by the last dia-logue move  Facial Expression Features ? AU4_1sec: Indicator for the display of the brow lowerer within 1 second prior to this utterance being sent, for the most recent three utterances ?  AU4_5sec: Indicator for the display of the brow lowerer within 5 seconds prior to this utterance being sent, for the most recent three utterances ? AU4_10sec: Indicator for the display of the brow lowerer within 10 seconds prior to this utterance being sent, for the most recent three utterances  5.2 Modeling Approach A logistic regression approach was used to classify the dialogue acts based on the above feature vec-tors. The Weka machine learning toolkit (Hall et al, 2009) was used to learn the models and to first perform feature selection in a best-first search. Lo-gistic regression is a generalized maximum likeli-hood model that discriminates between pairs of output values by calculating a feature weight vec-tor over the predictors.  The goal of this work is to explore the utility of confusion-related facial features in the context of particular dialogue act types. For this reason, a specialized classifier was learned by dialogue act. 5.3 Classification Results The classification accuracy and kappa for each specialized classifier is displayed in Table 4. Note that kappa statistics adjust for the accuracy that would be expected by majority-baseline chance; a kappa statistic of zero indicates that the classifier performed equal to chance, and a positive kappa statistic indicates that the classifier performed bet-ter than chance. A kappa of 1 constitutes perfect agreement. As the table illustrates, the feature se-lection chose to utilize the AU4 feature for every dialogue act except STATEMENT (S). When consid-ering the accuracy of the model across the ten folds, two of the affect-enriched classifiers exhibit-ed statistically significantly better performance. For GROUNDING (G) and REQUEST FOR FEEDBACK (RF), the facial expression features significantly 
1195
improved the classification accuracy compared to a model that was learned without affective features.  6 Discussion Dialogue act classification is an essential task for dialogue systems, and it has been addressed with a variety of modeling approaches and feature sets. We have presented a novel approach that treats facial expressions of students as constraining fea-tures for an affect-enriched dialogue act classifica-tion model in task-oriented tutorial dialogue. The results suggest that knowledge of the student?s confusion-related facial expressions can signifi-cantly enhance dialogue act classification for two types of dialogue acts, GROUNDING and REQUEST FOR FEEDBACK.   Table 4. Classification accuracy and kappa for spe-cialized DA classifiers. Statistically significant differences (across ten folds, one-tailed t-test) are shown in bold.    Classifier with AU4 Classifier without AU4  Dialogue Act % acc ? % acc ? p-value EX 90.7 .62 89.0 .28 >.05 G 92.6 .76 91 .71 .018 P 93 .49 92.2 .40 >.05 Q 94.6 .72 94.2 .72 >.05 S Not chosen in feat. sel. 93 .22 n/a RF 90.7 .62 88.3 .53 .003 
RSP 93 .68 95 .75 >.05 NE * *  N * * PE * * U * * UE * * *Too few instances for ten-fold cross-validation. 
6.1 Features Selected for Classification Out of more than 1500 features available during feature selection, each of the specialized dialogue act classifiers selected between 30 and 50 features in each condition (with and without affect fea-tures). To gain insight into the specific features that were useful for classifying these dialogue acts, it is useful to examine which of the AU4 history features were chosen during feature selection.  For GROUNDING, features that indicated the presence of absence of AU4 in the immediately preceding utterance, either at the 1 second or 5 se-cond granularity, were selected. Absence of this confusion-related facial action unit was associated with a higher probability of a grounding act, such as an acknowledgement. This finding is consistent with our understanding of how students and tutors interacted in this corpus; when a student experi-enced confusion, she would be unlikely to then make a simple grounding dialogue move, but in-stead would tend to inspect her computer program, ask a question, or wait for the tutor to explain more. For REQUEST FOR FEEDBACK, the predictive features were presence or absence of AU4 within ten seconds of the longest available history (three turns in the past), as well as the presence of AU4 within five seconds of the current utterance (the utterance whose dialogue act is being classified). This finding suggests that there may be some lag between the student experiencing confusion and then choosing to make a request for feedback, and that the confusion-related facial expressions may re-emerge as the student is making a request for feedback, since the five-second window prior to the student sending the textual dialogue message would overlap with the student?s construction of the message itself.    Although the improvements seen with AU4 fea-tures for QUESTION, POSITIVE FEEDBACK, and EXTRA-DOMAIN acts were not statistically reliable, examining the AU4 features that were selected for classifying these moves points toward ways in which facial expressions may influence classifica-tion of these acts (Table 5).      
1196
Table 5. Number of features, and AU4 features selected, for specialized DA classifiers  Dialogue Act # fea-tures selected AU4 features selected G 43 One utterance ago: AU4_1sec, AU4_5sec 
RF 37 Three utterances ago: AU4_10sec Target utterance: AU4_5sec EX 50 Three utterances ago: AU4_1sec P 36 Current utterance: AU4_10sec Q 30 One utterance ago: AU4_5sec  6.2 Implications The results presented here demonstrate that lever-aging knowledge of user affect, in particular of spontaneous facial expressions, may improve the performance of dialogue act classification models. Perhaps most interestingly, displays of confusion-related facial actions prior to a student dialogue move enabled an affect-enriched classifier to rec-ognize requests for feedback with significantly greater accuracy than a classifier that did not have access to the facial action features. Feedback is known to be a key component of effective tutorial dialogue, through which tutors provide adaptive help (Shute, 2008). Requesting feedback also seems to be an important behavior of students, characteristically engaged in more frequently by women than men, and more frequently by students with lower incoming knowledge than by students with higher incoming knowledge (Boyer, Vouk, & Lester, 2007). 6.3 Limitations The experiments reported here have several nota-ble limitations. First, the time-consuming nature of manual facial action tagging restricted the number of dialogues that could be tagged. Although the highest quality videos were selected for annotation, other medium quality videos would have been suf-ficiently clear to permit tagging, which would have increased the sample size and likely revealed sta-tistically significant trends. For example, the per-
formance of the affect-enriched classifier was bet-ter for dialogue acts of interest such as positive feedback and questions, but this difference was not statistically reliable.  An additional limitation stems from the more fundamental question of which affective states are indicated by particular external displays. The field is only just beginning to understand facial expres-sions during learning and to correlate these facial actions with emotions. Additional research into the ?ground truth? of emotion expression will shed additional light on this area. Finally, the results of manual facial action annotation may constitute up-per-bound findings for applying automatic facial expression analysis to dialogue act classification. 7 Conclusions and Future Work Emotion plays a vital role in human interactions. In particular, the role of facial expressions in human-human dialogue is widely recognized. Facial ex-pressions offer a promising channel for under-standing the emotions experienced by users of dialogue systems, particularly given the ubiquity of webcam technologies and the increasing number of dialogue systems that are deployed on webcam-enabled devices. This paper has reported on a first step toward using knowledge of user facial expres-sions to improve a dialogue act classification mod-el for tutorial dialogue, and the results demonstrate that facial expressions hold great promise for dis-tinguishing the pedagogically relevant dialogue act REQUEST FOR FEEDBACK, and the conversational moves of GROUNDING. These early findings highlight the importance of future work in this area. Dialogue act classifica-tion models have not fully leveraged some of the techniques emerging from work on sentiment anal-ysis. These approaches may prove particularly use-ful for identifying emotions in dialogue utterances. Another important direction for future work in-volves more fully exploring the ways in which af-fect expression differs between textual and spoken dialogue. Finally, as automatic facial tagging tech-nologies mature, they may prove powerful enough to enable broadly deployed dialogue systems to feasibly leverage facial expression data in the near future.    
1197
Acknowledgments This work is supported in part by the North Caroli-na State University Department of Computer Sci-ence and by the National Science Foundation through Grants REC-0632450, IIS-0812291, DRL-1007962 and the STARS Alliance Grant CNS-0739216. Any opinions, findings, conclusions, or recommendations expressed in this report are those of the participants, and do not necessarily represent the official views, opinions, or policy of the Na-tional Science Foundation.  References  A. Andreevskaia and S. Bergler. 2008. When specialists and generalists work together: Overcoming do-main dependence in sentiment tagging. Proceed-ings of the Annual Meeting of the Association for Computational Linguistics and Human Language Technologies (ACL HLT), 290-298.  M.S. Bartlett, G. Littlewort, M. Frank, C. Lainscsek, I. Fasel, and J. Movellan. 2006. Fully Automatic Facial Action Recognition in Spontaneous Behav-ior. 7th International Conference on Automatic Face and Gesture Recognition (FGR06), 223-230.  K.E. Boyer, M. Vouk, and J.C. Lester. 2007. The influ-ence of learner characteristics on task-oriented tu-torial dialogue. Proceedings of the International Conference on Artificial Intelligence in Educa-tion, 365?372.  K.E. Boyer, E.Y. Ha, R. Phillips, M.D. Wallis, M. Vouk, and J.C. Lester. 2010. Dialogue act model-ing in a complex task-oriented domain. Proceed-ings of the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), 297-305.  K.E. Boyer, R. Phillips, E.Y. Ha, M.D. Wallis, M.A. Vouk, and J.C. Lester. 2009. Modeling dialogue structure with adjacency pair analysis and hidden Markov models. Proceedings of the Annual Con-ference of the North American Chapter of the As-sociation for Computational Linguistics: Short Papers, 49-52.  K.E. Boyer, R. Phillips, E.Y. Ha, M.D. Wallis, M.A. Vouk, and J.C. Lester. 2010. Leveraging hidden dialogue state to select tutorial moves. Proceed-ings of the NAACL HLT 2010 Fifth Workshop on Innovative Use of NLP for Building Educational Applications, 66-73. R.A. Calvo and S. D?Mello. 2010. Affect Detection: An Interdisciplinary Review of Models, Methods, and Their Applications. IEEE Transactions on Affec-tive Computing, 1(1): 18-37. 
M. Cavazza, R.S.D.L. C?mara, M. Turunen, J. Gil, J. Hakulinen, N. Crook, et al 2010. How was your day? An affective companion ECA prototype. Proceedings of the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), 277-280.  F. Cavicchio. 2009. The modulation of cooperation and emotion in dialogue: the REC Corpus. Proceed-ings of the ACL-IJCNLP 2009 Student Research Workshop, 43-48.  J.F. Cohn, L.I. Reed, Z. Ambadar, J. Xiao, and T. Mori-yama. 2004. Automatic Analysis and Recognition of Brow Actions and Head Motion in Spontaneous Facial Behavior. IEEE International Conference on Systems, Man and Cybernetics, 610-616. S.D. Craig, S. D?Mello, A. Witherspoon, J. Sullins, and A.C. Graesser. 2004. Emotions during learning: The first steps toward an affect sensitive intelli-gent tutoring system. In J. Nall and R. Robson (Eds.), E-learn 2004: World conference on E-learning in Corporate, Government, Healthcare, & Higher Education, 241-250.  D. Das and S. Bandyopadhyay. 2009. Word to sentence level emotion tagging for Bengali blogs. Proceed-ings of the ACL-IJCNLP Conference, Short Pa-pers, 149-152.  S. Dasgupta and V. Ng. 2009. Mine the easy, classify the hard: a semi-supervised approach to automatic sentiment classification. Proceedings of the 46th Annual Meeting of the ACL and the 4th IJCNLP, 701-709.  B. Di Eugenio, Z. Xie, and R. Serafin. 2010. Dialogue Act Classification, Higher Order Dialogue Struc-ture, and Instance-Based Learning. Dialogue & Discourse, 1(2): 1-24.  M. Dzikovska, J.D. Moore, N. Steinhauser, and G. Campbell. 2010. The impact of interpretation problems on tutorial dialogue. Proceedings of the 48th Annual Meeting of the Association for Com-putational Linguistics, Short Papers, 43-48.  S. D?Mello, S.D. Craig, J. Sullins, and A.C. Graesser. 2006. Predicting Affective States expressed through an Emote-Aloud Procedure from AutoTu-tor?s Mixed- Initiative Dialogue. International Journal of Artificial Intelligence in Education, 16(1): 3-28. P. Ekman. 1999. Basic Emotions. In T. Dalgleish and M. J. Power (Eds.), Handbook of Cognition and Emotion. New York: Wiley. P. Ekman, W.V. Friesen. 1978. Facial Action Coding System. Palo Alto, CA: Consulting Psychologists Press. P. Ekman, W.V. Friesen, and J.C. Hager. 2002. Facial Action Coding System: Investigator?s Guide. Salt Lake City, USA: A Human Face. 
1198
P. Ekman and E.L. Rosenberg (Eds.). 2005. What the Face Reveals: Basic and Applied Studies of Spon-taneous Expression Using the Facial Action Cod-ing System (FACS) (2nd ed.). New York: Oxford University Press. K. Forbes-Riley, M. Rotaru, D.J. Litman, and J. Tetreault. 2007. Exploring affect-context depend-encies for adaptive system development. The Con-ference of the North American Chapter of the Association for Computational Linguistics and Human Language Technologies (NAACL HLT), Short Papers, 41-44.  K. Forbes-Riley, M. Rotaru, D.J. Litman, and J. Tetreault. 2009. Adapting to student uncertainty improves tutoring dialogues. Proceedings of the 14th International Conference on Artificial Intelli-gence in Education (AIED), 33-40.  A.C. Graesser, S. Lu, B. Olde, E. Cooper-Pye, and S. Whitten. 2005. Question asking and eye tracking during cognitive disequilibrium: comprehending illustrated texts on devices when the devices break down. Memory & Cognition, 33(7): 1235-1247.  S. Greene and P. Resnik. 2009. More than words: Syn-tactic packaging and implicit sentiment. Proceed-ings of the 2009 Annual Conference of the North American Chapter of the ACL and Human Lan-guage Technologies (NAACL HLT), 503-511.  M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reute-mann, and I.H. Witten. 2009. The WEKA data mining software: An update. SIGKDD Explora-tions, 11(1): 10?18.  R. Iida, S. Kobayashi, and T. Tokunaga. 2010. Incorpo-rating extra-linguistic information into reference resolution in collaborative task dialogue. Proceed-ings of the 48th Annual Meeting of the Associa-tion for Computational Linguistics, 1259-1267.  M.L. Knapp and J.A. Hall. 2006. Nonverbal Communi-cation in Human Interaction (6th ed.). Belmont, CA: Wadsworth/Thomson Learning. C.M. Lee, S.S. Narayanan. 2005. Toward detecting emotions in spoken dialogs. IEEE Transactions on Speech and Audio Processing, 13(2): 293-303.  R. L?pez-C?zar, J. Silovsky, and D. Griol. 2010. F2?New Technique for Recognition of User Emotion-al States in Spoken Dialogue Systems. Proceed-ings of the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), 281-288.  B.T. McDaniel, S. D?Mello, B.G. King, P. Chipman, K. Tapp, and A.C. Graesser. 2007. Facial Features for Affective State Detection in Learning Envi-ronments. Proceedings of the 29th Annual Cogni-tive Science Society, 467-472. D. McNeill. 1992. Hand and mind: What gestures reveal about thought. Chicago: University of Chicago Press. 
A. Mehrabian. 2007. Nonverbal Communication. New Brunswick, NJ: Aldine Transaction. T. Nguyen. 2010. Mood patterns and affective lexicon access in weblogs. Proceedings of the ACL 2010 Student Research Workshop, 43-48.  M. Pantic and M.S. Bartlett. 2007. Machine Analysis of Facial Expressions. In K. Delac and M. Grgic (Eds.), Face Recognition, 377-416. Vienna, Aus-tria: I-Tech Education and Publishing. J.A. Russell. 2003. Core affect and the psychological construction of emotion. Psychological Review, 110(1): 145-172. J.A. Russell, J.A. Bachorowski, and J.M. Fernandez-Dols. 2003. Facial and vocal expressions of emo-tion. Annual Review of Psychology, 54, 329-49. K.L. Schmidt and J.F. Cohn. 2001. Human Facial Ex-pressions as Adaptations: Evolutionary Questions in Facial Expression Research. Am J Phys An-thropol, 33: 3-24. V.J. Shute. 2008. Focus on Formative Feedback. Re-view of Educational Research, 78(1): 153-189.  V.K.R Sridar, S. Bangalore, and S.S. Narayanan. 2009. Combining lexical, syntactic and prosodic cues for improved online dialog act tagging. Computer Speech & Language, 23(4): 407-422. Elsevier Ltd.  A. Stolcke, K. Ries, N. Coccaro, E. Shriberg, R. Bates, D. Jurafsky, et al 2000. Dialogue Act Modeling for Automatic Tagging and Recognition of Con-versational Speech. Computational Linguistics, 26(3): 339-373.  C. Toprak, N. Jakob, and I. Gurevych. 2010. Sentence and expression level annotation of opinions in us-er-generated discourse. Proceedings of the 48th Annual Meeting of the Association for Computa-tional Linguistics, 575-584.  T. Wilson, J. Wiebe, and P. Hoffmann. 2009. Recogniz-ing Contextual Polarity: An Exploration of Fea-tures for Phrase-Level Sentiment Analysis. Computational Linguistics, 35(3): 399-433.  Z. Zeng, M. Pantic, G.I. Roisman, and T.S. Huang. 2009. A Survey of Affect Recognition Methods: Audio, Visual, and Spontaneous Expressions. IEEE Transactions on Pattern Analysis and Ma-chine Intelligence, 31(1): 39-58. 
1199
Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 247?256,
Seoul, South Korea, 5-6 July 2012. c?2012 Association for Computational Linguistics
Combining Verbal and Nonverbal Features to Overcome the ?Information Gap? in Task-Oriented Dialogue 
 Eun Young Ha, Joseph F. Grafsgaard, Christopher M. Mitchell,  Kristy Elizabeth Boyer, and James C. Lester Department of Computer Science North Carolina State University Raleigh, NC, USA {eha, jfgrafsg, cmmitch2, keboyer, lester}@ncsu.edu    Abstract Dialogue act modeling in task-oriented dialogue poses significant challenges. It is particularly challenging for corpora consisting of two interleaved communication streams: a dialogue stream and a task stream. In such corpora, information can be conveyed implicitly by the task stream, yielding a dialogue stream with seemingly missing information. A promising approach leverages rich resources from both the dialog and the task streams, combining verbal and non-verbal features. This paper presents work on dialogue act modeling that leverages body posture, which may be indicative of particular dialogue acts. Combining three information sources (dialogue exchanges, task context, and users? posture), three types of machine learning frameworks were compared. The results indicate that some models better preserve the structure of task-oriented dialogue than others, and that automatically recognized postural features may help to disambiguate user dialogue moves.  1 Introduction Dialogue act classification is concerned with understanding users? communicative intentions as reflected in their utterances. It is an important first step toward building automated dialogue systems. To date, the majority of work on dialogue act 
modeling has addressed spoken dialogue (Samuel et al, 1998; Stolcke et al, 2000; Surendran and Levow, 2006; Bangalore et al, 2008; Sridhar et al, 2009; Di Eugenio et al, 2010). However, with the increasing popularity of computer-mediated means of conversation, such as instant messaging and social networking services, automated analysis of textual dialogue holds much appeal. Dialogue act modeling for textual conversations has many practical application areas, which include web-based intelligent tutoring systems (Boyer et al, 2010a), chat-based online customer service (Kim et al, 2010), and social media analysis (Joty et al, 2011). Human interaction involves not only verbal communication but also nonverbal communication. Research on nonverbal communication (Knapp and Hall, 2006; Mehrabian, 2007; Russell et al, 2003) has identified a range of nonverbal cues, such as posture, gestures, eye gaze, and facial and vocal expressions. However, the utility of these nonverbal cues has not been fully explored within the context of dialogue act classification research. Previous research has leveraged prosodic cues (Sridhar et al, 2009; Stolcke et al, 2000) and facial expressions (Boyer et al, 2011) for automatic dialogue act classification, but other types of nonverbal cues remain unexplored. As a first step toward a dialogue system that learns its behavior from a human corpus, this paper proposes a novel approach to dialogue act classification that leverages information about users? posture. Posture has been found to be a significant indicator of a broad range of emotions (D?Mello and Graesser, 2010; Kapoor et al, 2007; Woolf et al, 2009). Based on the premise that emotion plays an 
247
important role in dialogue, this work hypothesizes that adding posture features will improve the performance of automatic dialogue act models.   The domain considered in this paper is task-oriented textual dialogue collected in a human tutoring study. In contrast to conventional task-oriented dialogue corpora (e.g., Carletta et al, 1997; Jurafsky et al, 1998; Ivanovic, 2008) in which conversational exchanges are carried out within a single channel of dialogue between the dialogue participants, the corpus used in this work utilizes two separate and interleaved streams of communication. One stream is the textual conversation between a student and a tutor (dialogue stream). The other is the student?s problem-solving activity (task stream). As will be described in Section 3, the interface used in the corpus collection was designed to allow the tutor to monitor the student?s problem-solving activities. Thus, the student?s problem-solving activities and the tutor?s monitoring of those activities functioned as an implicit communication channel. This characteristic of the corpus poses significant challenges for dialogue act modeling. First, because the dialogue stream and the task stream are interleaved, the dialogue stream alone may not be coherent. Second, since information can be exchanged implicitly via the task stream, the dialogue likely contains substantial information gaps1. Addressing these challenges, the dialogue act models described in this paper combine three sources of information: the verbal information from the dialogue stream, the task-related context from the task stream, and information about users? posture. This paper makes several contributions to the dialogue research community. First, it is the first effort to explore posture as a nonverbal cue for dialogue act classification. Second, the proposed approach is fully automatic and ready for real-world application. Third, this paper explicitly defines the notion of information gap in task-oriented dialogue consisting of multiple communication channels, which has only begun to be explored in the context of dialogue act classification (Boyer et al, 2010a). Finally, this                                                 1 In this paper, information gap is defined as the information that is missing from the explicit verbal exchanges between the dialogue participants but conveyed by the implicit task stream. 
paper examines adaptability of previous dialogue act classification approaches in conventional task-oriented domains by comparing three classifiers previously applied to dialogue act modeling for task-oriented dialogue. 2 Related Work A rich body of research has addressed data-driven approaches for dialogue act modeling. Russell et al (2003) applied a transformation-based learning approach for dialogue act tagging for spoken dialogue, using speaker direction, punctuation, marks, and cue phrases. Stolcke et al (2000) modeled the structure of dialogue as an HMM, treating the dialogue acts as the observations emitted from the hidden states of the learned HMM. More recently, Bangalore et al (2008) proposed a unified approach to task-oriented dialogue, in which both the user dialogue act classification and the system dialogue act selection were informed by a shared maximum entropy dialogue act classifier. Sridhar et al (2009) also used a maximum entropy model, exploring the utility of different representations of prosodic features. Di Eugenio et al (2010) used a memory-based classifier, in combination with a modified latent semantic analysis (LSA) technique by augmenting the original word-document matrix in LSA with rich linguistic features. While most work on dialogue act modeling has focused on spoken dialogue, a recent line of investigation has explored the analysis of textual conversation, such as asynchronous online chat conversation (Wu et al, 2005; Forsyth, 2007; Reitter et al, 2010; Joty et al, 2011) and synchronous online chat conversation   (Ivanovic, 2008; Kim et al, 2010; Boyer et al, 2010a). Wu et al (2005) proposed a transformation-based learning approach for an asynchronous chat posting domain, utilizing regular expression-based selection rules. For a similar domain, Forsyth (2007) applied neural networks and Na?ve Bayes classification technique using lexical cues. Ritter et al (2010) and Joty et al (2011) applied unsupervised learning approaches to dialogue act modeling for Twitter conversations, in which dialogue acts were automatically discovered by clustering raw utterances. Work by Ivanovic (2008) and Kim et al (2010) analyzed one-to-one synchronous online chat dialogue in a task-oriented 
248
customer service domain. Ivanovic (2008) applied maximum entropy, na?ve Bayes, and support vector machines using word n-gram features. Kim et al (2010) compared the CRF, HMM-SVM, and Na?ve Bayes classifiers using word n-grams and features extracted from the dialogue structure, in which CRF achieved the highest performance. Boyer et al (2010a) investigated dialogue act modeling for task-oriented tutorial dialogue, applying a logistic regression approach using lexical, syntactic, dialogue structure, and task structure features. Some previous dialogue act modeling work (Boyer et al, 2011; Sridhar et al, 2009; Stolcke et al, 2000) leveraged nonverbal information such as prosodic cues (Sridhar et al, 2009; Stolcke et al, 2000) and facial expressions (Boyer et al, 2011). Stolcke et al (2000) combined various prosodic features such as pitch, duration, and energy. Sridhar et al (2009) represented the sequence of prosodic features as n-grams. Boyer et al (2011) leveraged confusion-related facial expressions for tutorial dialogue. Like Boyer et al (2010a), this work addresses dialogue act classification for task-oriented textual conversation in a web-based tutoring domain. In contrast to Boyer et al (2010a), whose approach directly leveraged manually annotated features, making it challenging to apply the proposed model to a real-world system, the present work is fully automatic and ready for real-world application.  A novel feature of this work is its utilization of nonverbal cues carried by users? posture. This is the first dialogue act classification work that leverages posture information. 3 Data The corpus used in this paper consists of textual exchanges between a student and a tutor in a web-based remote-tutoring interface for introductory programming in Java. The corpus was collected from a series of six tutoring lessons, covering progressive topics in computer science over the course of four weeks. The tutoring interface consisted of four windows: a task window displaying the current programming task; a code window in which the student writes Java code; an output window for displaying the result of compiling and running the code; and a chat window for instant exchange of textual dialogue 
between the student and tutor. With this tutoring interface, the student and the tutor were able to exchange textual dialogue and share a synchronized view of the task. Apart from sending dialogue messages, the only action the tutor could perform to affect the student?s interface was advancing to the next programming task.  3.1 Data Collection The data collection conducted in Fall 2011 paired 42 students with one of four tutors for six forty-minute tutoring sessions on introductory computer science topics.  The students were chosen from a first-year engineering course and were pre-screened to filter out those with significant programming experience. The tutors were graduate students with previous tutoring or teaching experience in Java programming. Students were compensated for their participation with partial course credit. The students worked with the same tutor for the entire study. Each lesson consisted of between four and thirteen distinct subtasks. During each tutoring session, the dialogue text exchanged between the student and the tutor was logged to a database. Additional runtime data including content of the student?s Java code, the result (e.g., success or failure) of compiling and running the student?s code, and the IDs of the subtask were logged. All logged data were time-stamped at a millisecond precision. Students? body posture was recorded at a rate of 8 frames per second with a Kinect depth camera, which emits infrared rays to measure distance for each pixel in a depth image frame. The camera was positioned above the student?s computer monitor, ensuring the student?s upper body is centered in the recorded image. Tutors were not recorded. 3.2 Dialogue Act Annotation For the work described in this paper, a subset of the collected data was manually annotated, which include the first of the six tutoring lessons from 21 students. This corpus contains 2564 utterances (1777 tutor, 787 student). The average number of utterances per tutoring session was 122 (min = 74; max = 201). The average number of tutor utterances per session was 84.6 (min = 51; max = 137) and the average number of student utterances per session was 37.4 (min = 22; max = 64). 
249
Extending a previous annotation scheme used for similar task-oriented tutorial dialogue (Boyer et al, 2010b), the scheme used in this work consists of 13 dialogue act tags (Appendix). The dialogue turns that contained more than one dialogue function were segmented into multiple utterances before being assigned a dialogue act tag. The annotation scheme did not constrain any of the dialogue act tags as applying either to students? or tutors? utterances only; however, the resulting distribution of the tags in the annotated corpus show certain dialogue act tags were more relevant to either students? or tutors? utterances. Figure 1 depicts an excerpt from the corpus with the manually applied dialogue act annotations.  
 Three human annotators were trained to apply the scheme. The training consisted of an iterative process involving collaborative and independent tagging, followed by refinements of the tagging protocol. At the initial phase of training, the annotators tagged the corpus collaboratively. In later phases annotators tagged independently. To compute agreement between different annotators, 24% (5 of the 21 sessions) of the corpus were doubly annotated by two annotators. All possible 
pairs of the annotators participated in double annotation. The aggregate agreement was .80 in Cohen?s Kappa (Cohen, 1960). 3.3 Posture Estimation Posture has been found to be a significant indicator of a broad range of emotions such as anxiety, boredom, confusion, engaged concentration (or flow), frustration, and joy (D?Mello and Graesser, 2010; Kapoor et al, 2007; Woolf et al, 2009). Early investigations into posture utilized pressure-sensitive chairs which provided indirect measures of upper-body posture (D?Mello and Graesser, 2010; Kapoor et al, 2007; Woolf et al, 2009). Newer, computer vision-based techniques provide more detailed postural data (Sanghvi et al, 2011). The present work uses a posture estimation algorithm developed to automatically detect the head, mid torso, and lower torso through depth image recordings of seated individuals (Grafsgaard et al, 2012). With this estimation algorithm, posture is represented as a triple of head depth (distance between camera and head), mid torso depth, and lower torso depth. A dataset of depth camera recordings from the first of the six tutoring lessons consists of 512,977 depth image frames collected across 18.5 hours of computer-mediated human-human tutoring among 33 participants.2 For each depth image frame, the posture algorithm scanned through the three middle regions that corresponded to head, mid-torso, and lower-torso of the recorded person, and selected a single representative depth pixel from each region. The boundaries for each region were heuristically determined relying on the placement of the students? chairs in the middle of the depth recording view at a common distance. Given these constraints, the model was manually verified by two independent human judges to have 95.1% accuracy across 1,109 depth image snapshots corresponding to one-minute intervals across the dataset. The algorithm output for each depth image was labeled as erroneous if either judge found that any of the posture tracking points did not coincide with its target region. Example output of the algorithm is shown in Figure 2.  
                                                2 The other 9 sessions were not successfully recorded because of technical errors. 
Tutor: hang on :) [S] Tutor: When we show you example code, it is not the code you need to write. [S] Tutor: Look at the task again. [H] Student writes programming code Tutor: YUP [PF] Tutor: Perfect [PF] Tutor: OK. Go ahead and test. [DIR] Student: And I don't need anything in the parentheses? [Q] Tutor: Line 9 is correct. You do NOT need anything inside the parentheses. [A] Student: Ok [ACK] Student compiles and runs code successfully Tutor: Good. [PF]  Tutor: Moving on. [S] Tutor advances to the next task. Student writes programming code Tutor: Syntactically correct. But there is a logic error [LF] Tutor: When will the output statement display your request to the player? [Q] Student: AFTER they put in their name [A] Tutor: Exactly [PF] Figure 1. Corpus Excerpt with Dialogue Act Annotation 
250
4 Features For web-based one-to-one dialogue systems, it is important to achieve efficient runtime performance. To maximize real-world feasibility of the learned dialogue act classifiers, this work only considers the features that can be automatically extracted at runtime. In addition, the use of linguistic analysis software, such as a part-of-speech tagger and a syntactic parser, is intentionally restrained. One might argue that rich linguistic analysis may provide additional information to dialogue act classifiers, potentially improving the performance of learned models. However, there is a trade-off between additional information obtained by rich linguistic analysis and processing time. In addition, previous work (Boyer et al, 2010a) found part-of-speech and syntax features did not provide obvious benefit for dialogue act classification in a domain similar to the one considered in this work. The dialogue act classifiers described in this paper integrate four classes of features automatically extracted from three sources of information: the textual dialogue utterances, task-related runtime information logged into the database, and the images of the students recorded by depth cameras. Each feature class is explained in the following subsections. 4.1 Lexical Features Based on previous dialogue act classification research (Bangalore et al, 2008; Boyer et al, 2010a; Kim et al, 2010), this work utilizes word n-grams as features for dialogue act classification. In the experiment reported in Section 5, unigrams and 
bigrams were used. Adding higher order n-grams did not improve model accuracies. In our corpus (Section 3), the nature of the student dialogues is informal and utterances contain many typos. To remove undesirable noise in the data such as typos and rare words, n-grams were filtered out according to their frequency in the training data (i.e., n-grams that appear less than a predefined cutoff threshold in the training data are not included as features). The value of the cutoff threshold was empirically determined by testing the values between 0 and 10 on a development data set that consisted of 20% of randomly selected dialogue sessions. The value of 3 was selected as it yielded the highest classification accuracy. 4.2 Dialogue Context Features While lexical features characterize the intrinsic nature of individual utterances, the context of the utterance within a larger dialogue structure provides additional information about a given utterance in relation with other utterances. This work considers the following dialogue context features: ? Utterance Position: Specifies the relative position of an utterance at a given turn. The value of this feature indicates whether the utterance is the first one in a given turn, the second or later one in a given turn, or the given turn consists of a single utterance. ? Length: Specifies the number of a given utterance in terms of individual word tokens. ? Previous Author: Indicates whether the author of the previous utterance was student or tutor. ? Previous Tutor Dialogue Act: Specifies dialogue act of the most recent tutor utterance. The value of this feature is directly extracted from the manual annotation in the corpus, because in the broader context of our work, tutor dialogue moves will be determined by an external dialogue management module.   4.3 Task Context Features In our data, students? problem-solving activities (e.g., reading the problem description, writing computer programming code, and compiling and running the code) functioned as an implicit communication channel between students and tutors (Section 1). Because of the existence of this 
Figure 2. Automatically detected posture points (H = headDepth, M = midTorsoDepth, L = lowerTorsoDepth)  
 H 
 M  L 
251
implicit communication channel, the dialogue exchanges between students and tutors likely contain substantial information gaps. To overcome such information gaps, it is important to identify effective task context features. The present work leverages the following task context features, which can be automatically extracted during runtime: ? Previous Task Action: Specifies the type of the most recent problem-solving action performed by the student. The value could be message (writing a textual message to the tutor) code (writing code in the code window), or compile_run (compiling or running the code). ? Task Begin Flag: A binary feature that indicates whether a given utterance is the first one since the current problem task was posted.  ? Task Activity Flag: Another binary feature indicating that a given utterance was preceded by a student?s task activity. ? Last Compile/Run Status: Specifies the status (e.g., begin, stop, success, error, input sent) of the most recent compile/run action performed by the students.  In addition to the listed task context features, the utility of time information was also explored, such as the amount of time taken for previous coding activity and the elapsed time since the beginning of the current task. However, these features did not positively impact the performance of the learned models and were thus excluded. 4.4 Posture Features After preprocessing recorded image frames with the estimation algorithm (Section 3.3), students? postures were represented as tuples of three different integer values, each respectively representing head depth, mid torso depth, and lower torso depth. To extract posture features, the time window of n seconds directly preceding a given utterance was compared with the previous time window of the same size in terms of min, max, median, average, and variance of each depth value. The indicators of whether each of these values has increased, decreased or remained the same were considered as potential posture features. To avoid introducing errors to the model by insignificant changes in posture, an error tolerance ?  was allowed (i.e., the two compared postures 
were considered the same unless the amount of the change in the posture was greater than ?). Optimal values for n and ?  were empirically determined, selecting the values that maximized classification accuracy on the development data set. For n, the values between 0 and 60 were compared at an interval of 10. The value of 50 was selected for head depth and 60 for both mid torso depth and lower torso depth.  Similarly, the value of ?  was determined by comparing the values between 0 and 200 with an increment of 10. The selected value was 100.  All the potential posture features were examined in an informal experiment, in which each of the potential posture features were added to the combination of the lexical, the dialogue context, and the task context features. The posture features that improved the classification accuracy after adding them were included in the present dialogue act models. The selected posture features are min of head depth and max, median, and average of lower torso depth. None of the mid torso depth features were selected. 5 Experiment The goal of this experiment is twofold: (1) to evaluate the effectiveness of the feature classes and (2) to compare the performance of three classifiers: maximum entropy (ME), na?ve Bayes (NB), and conditional random field (CRF). These classifiers are chosen because they have been shown effective for dialogue act modeling in traditional task-oriented textual dialogue, in which conversational exchanges were carried out by a single channel of dialogue (Ivanovic, 2008; Kim et al, 2010). Previous result by Kim et al (2010) suggests a structured model such as CRF yields more accurate dialogue act model compared to unstructured models (e.g., Na?ve Bayes), because of its ability to model the sequential patterns in target classification labels. This experiment examines whether a similar finding is observed for our domain, which exhibits substantial information gaps due to the existence of an implicit communication channel, the task stream. 5.1 Dialogue Act Modeling All classifiers were built using the MALLET package (McCallum, 2002). This experiment used the manually annotated portion of the data 
252
described in Section 3. The original dialogue scheme (Section 3.2) was slightly modified by introducing an additional dialogue act, GR, in order to distinguish conventional expressions, such as greetings and thanks, from other information-delivering utterances. For this modified scheme, annotator agreement was 0.81 in Cohen?s Kappa on the doubly annotated portion of the corpus. 6 among the 21 dialogue sessions in the annotated data do not have accompanying images due to technical problems with the depth camera, thus these sessions were excluded from this experiment. Table 1 shows the distribution of the student dialogue act tags in the resulting corpus of 15 dialogues used in this experiment. The most frequent tag was A (answer), followed by ACK (acknowledgement) and Q (question). The features were extracted by aligning three sources of information (the textual dialogue corpus, the task-related runtime log data, and the recorded images) by timestamp. Word boundaries in the dialogue corpus were recognized by the surrounding white spaces and punctuations. The dialogue context features (D) leveraged in this paper includes previous tutor dialogue act. This feature takes the manually annotated value in the corpus, because this work assumes the existence of an external dialogue manager. However, since the external dialogue manager is not likely to achieve 100% accuracy in predicting human tutor dialogue acts, it would be informative to estimate a reasonable range of the accuracies of the student dialogue act model, taking into account the errors introduced by the dialogue manager. For this reason, two versions of the dialogue context features were considered in this experiment: one that leverages the full set of dialogue context features (D) and the other that excludes previous 
tutor dialogue act (D-). These respectively provide the maximum and the minimum expected accuracy of the student dialogue act model, when used with a dialogue manager. The models were trained and tested using five-fold cross validation, in which the 15 dialogue sessions were partitioned into 5 non-overlapping sets of the same size (i.e., 3 sessions per partition). Each set was used for testing exactly once. 5.2 Results Table 2 reports the average classification accuracies from the five-fold cross validation. The majority baseline accuracy for our data is .347, when the classifier always chooses the most frequent dialog act (A). The first group of rows in Table 3 report the accuracies of individual feature classes. All of the individual features performed better than the baseline. The improvement from the baseline was significant except for D- with CRF. The most powerful feature class was dialogue context class when the full set was used. The second group in Table 3 shows the effects of incrementally combining the feature classes. Adding dialogue act features to the lexical features (L + D) brought significant improvement in the classification accuracy for ME and CRF. Adding posture features (L + D + T + P) also improved the accuracy of ME by a statistically significant margin. The last group shows similar results for ME when the previous tutor dialogue act was excluded from the dialogue context, except that the improvement achieved by adding the posture features (L + D- + T + P) was not significant.  
Student Dialogue Act Distribution A (answer) 192 (34.7%) ACK (acknowledgement) 124 (22.4%) Q (question)  92 (16.6%)  S (statement) 76 (13.7%) GR (greeting and thanks) 52 (9.4%) C (clarification) 6 (1.0%) RF (request for feedback) 5 (.9%) RC (request confirmation) 2 (.4%) O (other) 5 (.9%) Total 554 Table 1. Student dialogue acts in the experiment data 
Features ME NB CRF 
 Indiv
idual  Lexical (L)     .696
**     .703**     .599**  Dialogue (D)     .711**     .715**     .696**  Dialogue- (D-)     .477**     .473**     .405  Task (T)     .405**     .396*      .386*  Posture (P)     .382*     .385*     .399* 
 Max  L + D     .772
??     .724     .692??  L + D + T     .777     .729     .694  L + D + T + P     .789?     .714     .682 
 Min  L + D-     .724
??     .681     .606  L + D- + T     .733     .671     .627  L + D- + T + P     .750     .676     .644 Table 2. Classification accuracies (*p < .05, **p < .01 compared to baseline; ??p < .01 compared to L; and ?p < .05 compared to L + D + T, with paired-samples t-test)  
253
The highest accuracy was achieved by ME when using all four classes of the features, with maximum (L + D + T + P) .789 and minimum (L + D- + T + P) .750. For both the maximum and the minimum conditions, the differences among the classifiers were significant (p < .01, one-way repeated measure ANOVA), with post-hoc Tukey HSD tests revealing ME was significantly better than both NB (p < .05) and CRF (p < .01). There was no significant difference between NB and CRF. 6 Discussion The experiment described in Section 5 compared the utility of lexical, dialogue context, task context, and posture features for dialogue act classification. The results indicate the effectiveness of these features. Particularly, adding the dialogue context and the posture features improved the accuracy of the maximum entropy model. Although the margin of improvement achieved by adding posture features was relatively small, the improvement was statistically significant (p < .05) for the maximum condition (L + D + T + P), which suggests that the users? posture during computer-mediated textual dialogue conveys important communicative messages. The experiment also compared three classifiers: maximum entropy, na?ve Bayes, and CRF. Interestingly, CRF was the worst-performing model for our data, contradicting the previous finding by Kim et al (2010), in which CRF (a structured classifier) performed significantly better than Na?ve Bayes (a non-structured classifier). This contradictive result suggests that, in our domain, the presence of an implicit communication channel resulted in substantial information gaps in the dialogue and it poses new challenges that were not encountered by conventional task-oriented domains consisting of a single communication channel.  The maximum entropy classifier achieved the best overall performance, reaching accuracy of .789. This is an encouraging result compared to previous work in a similar domain. Boyer et al (2010a) reported an accuracy of .628 for dialogue act classification in a similar domain. However, a direct comparison is not applicable since different data were used in their work. 
7 Conclusions and Future Work Dialogue act modeling for a task-oriented domain in which the dialogue stream is interleaved with the task stream poses significant challenges. With the goal of effective dialogue act modeling, this work leverages information about users? posture as non-verbal features. An experiment found that posture is a significant indicator of dialogue acts, in addition to lexical features, dialogue context, and task context. The experiment also compared three statistical classifiers: maximum entropy, naive Bayes, and CRF. The best performing model was maximum entropy. Using all features, the maximum entropy achieved .789 in accuracy. Several directions for future work are promising. First, given the encouraging finding that nonverbal information plays a significant role as a communicative means for task-oriented dialogue, various types of non-verbal information can be investigated, such as gesture and facial expressions. Second, incorporating richer task features, such as in our case, deep analysis of student code, may contribute to more accurate dialogue act modeling. Third, it is important to generalize the findings to a larger data set, including across other task-oriented domains.  Finally, the community is embracing a move toward annotation-lean approaches such as unsupervised or semi-supervised learning, which hold great promise for dialogue modeling. Acknowledgments This research was supported by the National Science Foundation under Grant DRL-1007962. Any opinions, findings, and conclusions expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation. References Bangalore, S., Di Fabbrizio, G., & Stent, A. (2008). Learning the structure of task-driven human-human dialogs. IEEE Transactions on Audio, Speech, and Language Processing, 16(7), 1249-1259. Boyer, K. E., Grafsgaard, J. F., Ha, E. Y., Phillips, R., & Lester, J. C. (2011). An affect-enriched dialogue act classification model for task-oriented dialogue. Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human 
254
Language Technologies (pp. 1190-1199). Portland, OR. Boyer, K. E., Ha, E. Y., Phillips, R., Wallis, M. D., Vouk, M. A., & Lester, J. C. (2010a). Dialogue Act Modeling in a Complex Task-Oriented Domain. Proceedings of the 11th Annual SIGDIAL Meeting on Discourse and Dialogue (pp. 297-305). Tokyo, Japan. Boyer, K. E., Phillips, R., Ingram, A., Ha, E. Y., Wallis, M., Vouk, M., & Lester, J. (2010b). Characterizing the effectiveness of tutorial dialogue with hidden markov models. Proceedings of the 10th international conference on Intelligent Tutoring Systems (pp. 55-64). Pittsburgh, PA. Carletta, J., Isard, A., Isard, S., Kowtko, J., Doherty-Sneddon, G., & Anderson, A. (1997). The reliability of a dialogue structure coding scheme. Computational Linguistics, 23, 13?31. Cavicchio, F. (2009). The modulation of cooperation and emotion in dialogue: The REC corpus. Proceedings of the ACL-IJCNLP 2009 Student Research Workshop (pp. 81 - 87). Suntec, Singapore. Cohen, J. (1960). A coefficient of agreement for nominal scales. Educational and Psychological Measurement, 20(1), 37 - 46. Di Eugenio, B., Xie, Z., & Serafin, R. (2010). Dialogue act classification, instance-based learning, and higher order dialogue structure. Dialogue and Discourse, 1(2), 81 - 104. D?Mello, S., & Graesser, A. (2010). Mining Bodily Patterns of Affective Experience during Learning. Proceedings of the 3rd International Conference on Educational Data Mining (pp. 31-40). Pittsburgh, PA. Forsyth, E. N. (2007). Improving Automated Lexical and Discourse Analysis of Online Chat Dialog. Master's thesis. Naval Postgraduate School. Grafsgaard, J. F., Boyer, K. E., Wiebe, E. N., & Lester, J. C. (2012). Analyzing Posture and Affect in Task-Oriented Tutoring. Proceedings of the 25th Florida Artificial Intelligence Research Society Conference (pp. 438-443). Marco Island, FL. Ivanovic, E. (2008). Automatic instant messaging dialogue using statistical models and dialogue acts. Master's thesis. The University of Melbourne. Joty, S. R., Carenini, G., & Lin, C.-Y. (2011). Unsupervised Modeling of Dialog Acts in Asynchronous Conversations. Proceedings of the 22nd International Joint Conference on Artificial 
Intelligence (pp. 1807-1813). Barcelona, Catalonia, Spain. Jurafsky, D., Bates, R., Coccaro, N., Martin, R., Meteer, M., Ries, K., Shriberg, E., et al (1998). Switchboard discourse language modeling project report. Baltimore, MD. Kapoor, A., Burleson, W., & Picard, R. W. (2007). Automatic prediction of frustration. International Journal of Human-Computer Studies, 65(8), 724-736. Kim, S. N., Cavedon, L., & Baldwin, T. (2010). Classifying dialogue acts in one-on-one live chats. Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing (pp. 862-871). Cambridge, MA. Knapp, M. L., & Hall, J. A. (2006). Nonverbal Communication in Human Interaction (6th ed.). Belmont, CA: Wadsworth/Thomson Learning. McCallum, A. K. (2002). MALLET: A Machine Learning for Language Toolkit. Available from  http://mallet.cs.umass.edu Mehrabian, A. (2007). Nonverbal Communication. New Brunswick, NJ: Aldine Transaction. Ritter, A., Cherry, C., & Dolan, B. (2010). Unsupervised modeling of twitter conversations. Proceedings of Human Language Technologies: The 2010 Annual Conference of the North American Chapter (pp. 172 - 180). Los Angeles, CA. Russell, J. A., Bachorowski, J. A., & Fernandez-dols, J. M. (2003). Facial and vocal expressions of emotion. Annual Review of Psychology, 54, 329-349. Sanghvi, J., Castellano, G., Leite, I., Pereira, A., McOwan, P. W., & Paiva, A. (2011). Automatic analysis of affective postures and body motion to detect engagement with a game companion. Proceedings of the 6th international conference on Human-robot interaction (pp. 305-312). Lausanne, Switzerland. Sridhar, R., Bangalore, S., & Narayanan, S. (2009). Combining lexical, syntactic and prosodic cues for improved online dialog act tagging. Computer Speech and Language, 23(4), 407 - 422. Stolcke, A., Ries, K., Coccaro, N., Shriberg, E., Bates, R., Jurafsky, D., Taylor, P., et al (2000). Dialogue act modeling for automatic tagging and recognition of conversational speech. Computational Linguistics, 26(3), 339-373. Surendran, D., & Levow, G.-A. (2006). Dialog act tagging with support vector machines and hidden 
255
Markov models. Proceedings of Interspeech (pp. 1950 - 1953). Pittsburgh, PA. Woolf, B., Burleson, W., Arroyo, I., Dragon, T., Cooper, D., & Picard, R. W. (2009). Affect-aware tutors recognising and responding to student affect. International Journal of Learning Technology, 4(3/4), 129-164. Wu, T., Khan, F. M., Fisher, T. A., Shuler, L. A., & Pottenger, W. M. (2005). Posting Act Tagging Using Transformation-Based Learning. In T. Y. Lin, S. Ohsuga, C.-J. Liau, X. Hu, & S. Tsumoto (Eds.), Foundations of Data Mining and knowledge Discovery (pp. 319 - 331). Springer. 
  
Appendix. Dialogue Act Annotation Scheme and Inter-rater Agreement Tag Description Frequency Agreement (k) H  Hint:  The tutor gives advice to help the student proceed with the task Tutor:     Student:     133 0 .50 DIR   Directive:  The tutor explicitly tells the student the next step to take Tutor:     Student:     121 0 .63 ACK   Acknowledgement:  Either the tutor or the student acknowledges previous utterance; conversational grounding Tutor:       Student:  41 175 .73 RC   Request for Confirmation:  Either the tutor or the student requests confirmation from the other participant (e.g., ?Make sense??) Tutor:       Student:  11 2 Insufficient data RF   Request for Feedback:  The student requests an assessment of performance or work from the tutor Tutor:     Student:    0 5 1.0 PF  Positive Feedback:  The tutor provides a positive assessment of the student?s performance Tutor:     Student:     327 0 .90 LF Lukewarm Feedback:  The tutor provides an assessment that has both positive and negative elements Tutor:      Student:    13 0 .80 NF Negative Feedback:  The tutor provides a negative assessment of the student?s performance Tutor:        Student:     1 0 .40 Q Question:  A question regarding the task that is not a direct request for confirmation or feedback Tutor:     Student:  327 120   .95 A Answer:  An answer to an utterance marked Q Tutor:       Student:  96 295 .94 C Correction:  Correction of a typo in a previous utterance Tutor:       Student:  10 6 .54 S  Statement:  A statement regarding the task that does not fit into any of the above categories Tutor:     Student:  681 174 .71 O Other: Other utterances, usually containing only affective content Tutor:     Student:  6 10 .69 
256
