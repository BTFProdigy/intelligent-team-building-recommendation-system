Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 238?247,
Baltimore, Maryland, USA, June 23-25 2014.
c?2014 Association for Computational Linguistics
Don?t count, predict! A systematic comparison of
context-counting vs. context-predicting semantic vectors
Marco Baroni and Georgiana Dinu and Germ
?
an Kruszewski
Center for Mind/Brain Sciences (University of Trento, Italy)
(marco.baroni|georgiana.dinu|german.kruszewski)@unitn.it
Abstract
Context-predicting models (more com-
monly known as embeddings or neural
language models) are the new kids on the
distributional semantics block. Despite the
buzz surrounding these models, the litera-
ture is still lacking a systematic compari-
son of the predictive models with classic,
count-vector-based distributional semantic
approaches. In this paper, we perform
such an extensive evaluation, on a wide
range of lexical semantics tasks and across
many parameter settings. The results, to
our own surprise, show that the buzz is
fully justified, as the context-predicting
models obtain a thorough and resounding
victory against their count-based counter-
parts.
1 Introduction
A long tradition in computational linguistics has
shown that contextual information provides a good
approximation to word meaning, since semanti-
cally similar words tend to have similar contex-
tual distributions (Miller and Charles, 1991). In
concrete, distributional semantic models (DSMs)
use vectors that keep track of the contexts (e.g.,
co-occurring words) in which target terms appear
in a large corpus as proxies for meaning represen-
tations, and apply geometric techniques to these
vectors to measure the similarity in meaning of
the corresponding words (Clark, 2013; Erk, 2012;
Turney and Pantel, 2010).
It has been clear for decades now that raw co-
occurrence counts don?t work that well, and DSMs
achieve much higher performance when various
transformations are applied to the raw vectors,
for example by reweighting the counts for con-
text informativeness and smoothing them with di-
mensionality reduction techniques. This vector
optimization process is generally unsupervised,
and based on independent considerations (for ex-
ample, context reweighting is often justified by
information-theoretic considerations, dimension-
ality reduction optimizes the amount of preserved
variance, etc.). Occasionally, some kind of indi-
rect supervision is used: Several parameter set-
tings are tried, and the best setting is chosen based
on performance on a semantic task that has been
selected for tuning.
The last few years have seen the development
of a new generation of DSMs that frame the vec-
tor estimation problem directly as a supervised
task, where the weights in a word vector are set to
maximize the probability of the contexts in which
the word is observed in the corpus (Bengio et al,
2003; Collobert and Weston, 2008; Collobert et
al., 2011; Huang et al, 2012; Mikolov et al,
2013a; Turian et al, 2010). The traditional con-
struction of context vectors is turned on its head:
Instead of first collecting context vectors and then
reweighting these vectors based on various crite-
ria, the vector weights are directly set to optimally
predict the contexts in which the corresponding
words tend to appear. Since similar words occur
in similar contexts, the system naturally learns to
assign similar vectors to similar words.
This new way to train DSMs is attractive be-
cause it replaces the essentially heuristic stacking
of vector transforms in earlier models with a sin-
gle, well-defined supervised learning step. At the
same time, supervision comes at no manual anno-
tation cost, given that the context windows used
for training can be automatically extracted from
an unannotated corpus (indeed, they are the very
same data used to build traditional DSMs). More-
over, at least some of the relevant methods can ef-
ficiently scale up to process very large amounts of
input data.
1
1
The idea to directly learn a parameter vector based on
an objective optimum function is shared by Latent Dirichlet
238
We will refer to DSMs built in the traditional
way as count models (since they initialize vectors
with co-occurrence counts), and to their training-
based alternative as predict(ive) models.
2
Now,
the most natural question to ask, of course, is
which of the two approaches is best in empirical
terms. Surprisingly, despite the long tradition of
extensive evaluations of alternative count DSMs
on standard benchmarks (Agirre et al, 2009; Ba-
roni and Lenci, 2010; Bullinaria and Levy, 2007;
Bullinaria and Levy, 2012; Sahlgren, 2006; Pad?o
and Lapata, 2007), the existing literature contains
very little in terms of direct comparison of count
vs. predictive DSMs. This is in part due to the fact
that context-predicting vectors were first devel-
oped as an approach to language modeling and/or
as a way to initialize feature vectors in neural-
network-based ?deep learning? NLP architectures,
so their effectiveness as semantic representations
was initially seen as little more than an interest-
ing side effect. Sociological reasons might also be
partly responsible for the lack of systematic com-
parisons: Context-predictive models were devel-
oped within the neural-network community, with
little or no awareness of recent DSM work in com-
putational linguistics.
Whatever the reasons, we know of just three
works reporting direct comparisons, all limited in
their scope. Huang et al (2012) compare, in pass-
ing, one count model and several predict DSMs
on the standard WordSim353 benchmark (Table
3 of their paper). In this experiment, the count
model actually outperforms the best predictive ap-
proach. Instead, in a word-similarity-in-context
task (Table 5), the best predict model outperforms
the count model, albeit not by a large margin.
Blacoe and Lapata (2012) compare count and
predict representations as input to composition
functions. Count vectors make for better inputs
in a phrase similarity task, whereas the two repre-
sentations are comparable in a paraphrase classifi-
cation experiment.
3
Allocation (LDA) models (Blei et al, 2003; Griffiths et al,
2007), where parameters are set to optimize the joint prob-
ability distribution of words and documents. However, the
fully probabilistic LDA models have problems scaling up to
large data sets.
2
We owe the first term to Hinrich Sch?utze (p.c.). Predic-
tive DSMs are also called neural language models, because
their supervised context prediction training is performed with
neural networks, or, more cryptically, ?embeddings?.
3
We refer here to the updated results reported in
the erratum at http://homepages.inf.ed.ac.uk/
s1066731/pdf/emnlp2012erratum.pdf
Finally, Mikolov et al (2013d) compare their
predict models to ?Latent Semantic Analysis?
(LSA) count vectors on syntactic and semantic
analogy tasks, finding that the predict models are
highly superior. However, they provide very little
details about the LSA count vectors they use.
4
In this paper, we overcome the comparison
scarcity problem by providing a direct evaluation
of count and predict DSMs across many parameter
settings and on a large variety of mostly standard
lexical semantics benchmarks. Our title already
gave away what we discovered.
2 Distributional semantic models
Both count and predict models are extracted from
a corpus of about 2.8 billion tokens constructed
by concatenating ukWaC,
5
the English Wikipedia
6
and the British National Corpus.
7
For both model
types, we consider the top 300K most frequent
words in the corpus both as target and context ele-
ments.
2.1 Count models
We prepared the count models using the DISSECT
toolkit.
8
We extracted count vectors from sym-
metric context windows of two and five words to
either side of target. We considered two weight-
ing schemes: positive Pointwise Mutual Informa-
tion and Local Mutual Information (akin to the
widely used Log-Likelihood Ratio scheme) (Ev-
ert, 2005). We used both full and compressed vec-
tors. The latter were obtained by applying the Sin-
gular Value Decomposition (Golub and Van Loan,
1996) or Non-negative Matrix Factorization (Lee
and Seung, 2000), Lin (2007) algorithm, with re-
duced sizes ranging from 200 to 500 in steps of
100. In total, 36 count models were evaluated.
Count models have such a long and rich his-
tory that we can only explore a small subset of
the counting, weighting and compressing meth-
ods proposed in the literature. However, it is
worth pointing out that the evaluated parameter
subset encompasses settings (narrow context win-
dow, positive PMI, SVD reduction) that have been
4
Chen et al (2013) present an extended empirical evalua-
tion, that is however limited to alternative context-predictive
models, and does not include the word2vec variant we use
here.
5
http://wacky.sslmit.unibo.it
6
http://en.wikipedia.org
7
http://www.natcorp.ox.ac.uk
8
http://clic.cimec.unitn.it/composes/
toolkit/
239
found to be most effective in the systematic explo-
rations of the parameter space conducted by Bul-
linaria and Levy (2007; 2012).
2.2 Predict models
We trained our predict models with the word2vec
toolkit.
9
The toolkit implements both the skip-
gram and CBOW approaches of Mikolov et
al. (2013a; 2013c). We experimented only with
the latter, which is also the more computationally-
efficient model of the two, following Mikolov et
al. (2013b) which recommends CBOW as more
suitable for larger datasets.
The CBOW model learns to predict the word in
the middle of a symmetric window based on the
sum of the vector representations of the words in
the window. We considered context windows of
2 and 5 words to either side of the central ele-
ment. We vary vector dimensionality within the
200 to 500 range in steps of 100. The word2vec
toolkit implements two efficient alternatives to the
standard computation of the output word proba-
bility distributions by a softmax classifier. Hi-
erarchical softmax is a computationally efficient
way to estimate the overall probability distribu-
tion using an output layer that is proportional to
log(unigram.perplexity(W )) instead of W (for
W the vocabulary size). As an alternative, nega-
tive sampling estimates the probability of an out-
put word by learning to distinguish it from draws
from a noise distribution. The number of these
draws (number of negative samples) is given by
a parameter k. We test both hierarchical softmax
and negative sampling with k values of 5 and 10.
Very frequent words such as the or a are not very
informative as context features. The word2vec
toolkit implements a method to downsize their ef-
fect (and simultaneously improve speed perfor-
mance). More precisely, words in the training
data are discarded with a probability that is pro-
portional to their frequency (capturing the same
intuition that motivates traditional count vector
weighting measures such as PMI). This is con-
trolled by a parameter t and words that occur with
higher frequency than t are aggressively subsam-
pled. We train models without subsampling and
with subsampling at t = 1e
?5
(the toolkit page
suggests 1e
?3
? 1e
?5
as a useful range based on
empirical observations).
In total, we evaluate 48 predict models, a num-
9
https://code.google.com/p/word2vec/
ber comparable to that of the count models we
consider.
2.3 Out-of-the-box models
Baroni and Lenci (2010) make the vectors of
their best-performing Distributional Memory (dm)
model available.
10
This model, based on the same
input corpus we use, exemplifies a ?linguistically
rich? count-based DSM, that relies on lemmas
instead or raw word forms, and has dimensions
that encode the syntactic relations and/or lexico-
syntactic patterns linking targets and contexts. Ba-
roni and Lenci showed, in a large scale evaluation,
that dm reaches near-state-of-the-art performance
in a variety of semantic tasks.
We also experiment with the popular predict
vectors made available by Ronan Collobert.
11
Fol-
lowing the earlier literature, with refer to them
as Collobert and Weston (cw) vectors. These are
100-dimensional vectors trained for two months
(!) on the Wikipedia. In particular, the vectors
were trained to optimize the task of choosing the
right word over a random alternative in the middle
of an 11-word context window (Collobert et al,
2011).
3 Evaluation materials
We test our models on a variety of benchmarks,
most of them already widely used to test and com-
pare DSMs. The following benchmark descrip-
tions also explain the figures of merit and state-
of-the-art results reported in Table 2.
Semantic relatedness A first set of semantic
benchmarks was constructed by asking human
subjects to rate the degree of semantic similarity
or relatedness between two words on a numeri-
cal scale. The performance of a computational
model is assessed in terms of correlation between
the average scores that subjects assigned to the
pairs and the cosines between the corresponding
vectors in the model space (following the previ-
ous art, we use Pearson correlation for rg, Spear-
man in all other cases). The classic data set of
Rubenstein and Goodenough (1965) (rg) consists
of 65 noun pairs. State of the art performance
on this set has been reported by Hassan and Mi-
halcea (2011) using a technique that exploits the
Wikipedia linking structure and word sense dis-
ambiguation techniques. Finkelstein et al (2002)
10
http://clic.cimec.unitn.it/dm/
11
http://ronan.collobert.com/senna/
240
introduced the widely used WordSim353 set (ws)
that, as the name suggests, consists of 353 pairs.
The current state of the art is reached by Halawi
et al (2012) with a method that is in the spirit
of the predict models, but lets synonymy infor-
mation from WordNet constrain the learning pro-
cess (by favoring solutions in which WordNet syn-
onyms are near in semantic space). Agirre et al
(2009) split the ws set into similarity (wss) and re-
latedness (wsr) subsets. The first contains tighter
taxonomic relations, such as synonymy and co-
hyponymy (king/queen) whereas the second en-
compasses broader, possibly topical or syntag-
matic relations (family/planning). We report state-
of-the-art performance on the two subsets from the
work of Agirre and colleagues, who used different
kinds of count vectors extracted from a very large
corpus (orders of magnitude larger than ours). Fi-
nally, we use (the test section of) MEN (men), that
comprises 1,000 word pairs. Bruni et al (2013),
the developers of this benchmark, achieve state-of-
the-art performance by extensive tuning on ad-hoc
training data, and by using both textual and image-
extracted features to represent word meaning.
Synonym detection The classic TOEFL (toefl)
set was introduced by Landauer and Dumais
(1997). It contains 80 multiple-choice questions
that pair a target term with 4 synonym candidates.
For example, for the target levied one must choose
between imposed (correct), believed, requested
and correlated. The DSMs compute cosines of
each candidate vector with the target, and pick the
candidate with largest cosine as their answer. Per-
formance is evaluated in terms of correct-answer
accuracy. Bullinaria and Levy (2012) achieved
100% accuracy by a very thorough exploration of
the count model parameter space.
Concept categorization Given a set of nominal
concepts, the task is to group them into natural cat-
egories (e.g., helicopters and motorcycles should
go to the vehicle class, dogs and elephants into the
mammal class). Following previous art, we tackle
categorization as an unsupervised clustering task.
The vectors produced by a model are clustered
into n groups (with n determined by the gold stan-
dard partition) using the CLUTO toolkit (Karypis,
2003), with the repeated bisections with global op-
timization method and CLUTO?s default settings
otherwise (these are standard choices in the liter-
ature). Performance is evaluated in terms of pu-
rity, a measure of the extent to which each cluster
contains concepts from a single gold category. If
the gold partition is reproduced perfectly, purity
reaches 100%; it approaches 0 as cluster quality
deteriorates. The Almuhareb-Poesio (ap) bench-
mark contains 402 concepts organized into 21 cat-
egories (Almuhareb, 2006). State-of-the-art purity
was reached by Rothenh?ausler and Sch?utze (2009)
with a count model based on carefully crafted syn-
tactic links. The ESSLLI 2008 Distributional Se-
mantic Workshop shared-task set (esslli) contains
44 concepts to be clustered into 6 categories (Ba-
roni et al, 2008) (we ignore here the 3- and 2-
way higher-level partitions coming with this set).
Katrenko and Adriaans (2008) reached top per-
formance on this set using the full Web as a cor-
pus and manually crafted, linguistically motivated
patterns. Finally, the Battig (battig) test set intro-
duced by Baroni et al (2010) includes 83 concepts
from 10 categories. Current state of the art was
reached by the window-based count model of Ba-
roni and Lenci (2010).
Selectional preferences We experiment with
two data sets that contain verb-noun pairs that
were rated by subjects for the typicality of the
noun as a subject or object of the verb (e.g., peo-
ple received a high average score as subject of
to eat, and a low score as object of the same
verb). We follow the procedure proposed by Ba-
roni and Lenci (2010) to tackle this challenge: For
each verb, we use the corpus-based tuples they
make available to select the 20 nouns that are most
strongly associated to the verb as subjects or ob-
jects, and we average the vectors of these nouns
to obtain a ?prototype? vector for the relevant ar-
gument slot. We then measure the cosine of the
vector for a target noun with the relevant proto-
type vector (e.g., the cosine of people with the eat-
ing subject prototype vector). Systems are eval-
uated by Spearman correlation of these cosines
with the averaged human typicality ratings. Our
first data set was introduced by Ulrike Pad?o (2007)
and includes 211 pairs (up). Top-performance was
reached by the supervised count vector system of
Herda?gdelen and Baroni (2009) (supervised in the
sense that they directly trained a classifier on gold
data, as opposed to the 0-cost supervision of the
context-learning methods). The mcrae set (McRae
et al, 1998) consists of 100 noun?verb pairs, with
top performance reached by the DepDM system of
Baroni and Lenci (2010), a count DSM relying on
241
syntactic information.
Analogy While all the previous data sets are rel-
atively standard in the DSM field to test traditional
count models, our last benchmark was introduced
in Mikolov et al (2013a) specifically to test pre-
dict models. The data-set contains about 9K se-
mantic and 10.5K syntactic analogy questions. A
semantic question gives an example pair (brother-
sister), a test word (grandson) and asks to find
another word that instantiates the relation illus-
trated by the example with respect to the test word
(granddaughter). A syntactic question is similar,
but in this case the relationship is of a grammatical
nature (work?works, speak. . . speaks). Mikolov
and colleagues tackle the challenge by subtract-
ing the second example term vector from the first,
adding the test term, and looking for the nearest
neighbour of the resulting vector (what is the near-
est neighbour of
~
brother?
~
sister+
~
grandson?).
Systems are evaluated in terms of proportion of
questions where the nearest neighbour from the
whole semantic space is the correct answer (the
given example and test vector triples are excluded
from the nearest neighbour search). Mikolov et al
(2013a) reach top accuracy on the syntactic subset
(ansyn) with a CBOW predict model akin to ours
(but trained on a corpus twice as large). Top ac-
curacy on the entire data set (an) and on the se-
mantic subset (ansem) was reached by Mikolov
et al (2013c) using a skip-gram predict model.
Note however that, because of the way the task
is framed, performance also depends on the size
of the vocabulary to be searched: Mikolov et al
(2013a) pick the nearest neighbour among vectors
for 1M words, Mikolov et al (2013c) among 700K
words, and we among 300K words.
Some characteristics of the benchmarks we use
are summarized in Table 1.
4 Results
Table 2 summarizes the evaluation results. The
first block of the table reports the maximum per-
task performance (across all considered parameter
settings) for count and predict vectors. The latter
emerge as clear winners, with a large margin over
count vectors in most tasks. Indeed, the predic-
tive models achieve an impressive overall perfor-
mance, beating the current state of the art in sev-
eral cases, and approaching it in many more. It is
worth stressing that, as reviewed in Section 3, the
state-of-the-art results were obtained in almost all
cases using specialized approaches that rely on ex-
ternal knowledge, manually-crafted rules, parsing,
larger corpora and/or task-specific tuning. Our
predict results were instead achieved by simply
downloading the word2vec toolkit and running it
with a range of parameter choices recommended
by the toolkit developers.
The success of the predict models cannot be
blamed on poor performance of the count mod-
els. Besides the fact that this would not explain
the near-state-of-the-art performance of the pre-
dict vectors, the count model results are actually
quite good in absolute terms. Indeed, in several
cases they are close, or even better than those at-
tained by dm, a linguistically-sophisticated count-
based approach that was shown to reach top per-
formance across a variety of tasks by Baroni and
Lenci (2010).
Interestingly, count vectors achieve perfor-
mance comparable to that of predict vectors only
on the selectional preference tasks. The up task
in particular is also the only benchmark on which
predict models are seriously lagging behind state-
of-the-art and dm performance. Recall from Sec-
tion 3 that we tackle selectional preference by cre-
ating average vectors representing typical verb ar-
guments. We conjecture that this averaging ap-
proach, that worked well for dm vectors, might
be problematic for prediction-trained vectors, and
we plan to explore alternative methods to build the
prototypes in future research.
Are our results robust to parameter choices, or
are they due to very specific and brittle settings?
The next few blocks of Table 2 address this ques-
tion. The second block reports results obtained
with single count and predict models that are best
in terms of average performance rank across tasks
(these are the models on the top rows of tables
3 and 4, respectively). We see that, for both ap-
proaches, performance is not seriously affected by
using the single best setup rather than task-specific
settings, except for a considerable drop in perfor-
mance for the best predict model on esslli (due to
the small size of this data set?), and an even more
dramatic drop of the count model on ansem. A
more cogent and interesting evaluation is reported
in the third block of Table 2, where we see what
happens if we use the single models with worst
performance across tasks (recall from Section 2
above that, in any case, we are exploring a space
of reasonable parameter settings, of the sort that an
242
name task measure source soa
rg relatedness Pearson Rubenstein and Goodenough Hassan and Mihalcea (2011)
(1965)
ws relatedness Spearman Finkelstein et al (2002) Halawi et al (2012)
wss relatedness Spearman Agirre et al (2009) Agirre et al (2009)
wsr relatedness Spearman Agirre et al (2009) Agirre et al (2009)
men relatedness Spearman Bruni et al (2013) Bruni et al (2013)
toefl synonyms accuracy Landauer and Dumais Bullinaria and Levy (2012)
(1997)
ap categorization purity Almuhareb (2006) Rothenh?ausler and Sch?utze
(2009)
esslli categorization purity Baroni et al (2008) Katrenko and Adriaans
(2008)
battig categorization purity Baroni et al (2010) Baroni and Lenci (2010)
up sel pref Spearman Pad?o (2007) Herda?gdelen and Baroni
(2009)
mcrae sel pref Spearman McRae et al (1998) Baroni and Lenci (2010)
an analogy accuracy Mikolov et al (2013a) Mikolov et al (2013c)
ansyn analogy accuracy Mikolov et al (2013a) Mikolov et al (2013a)
ansem analogy accuracy Mikolov et al (2013a) Mikolov et al (2013c)
Table 1: Benchmarks used in experiments, with type of task, figure of merit (measure), original reference
(source) and reference to current state-of-the-art system (soa).
rg ws wss wsr men toefl ap esslli battig up mcrae an ansyn ansem
best setup on each task
cnt 74 62 70 59 72 76 66 84 98 41 27 49 43 60
pre 84 75 80 70 80 91 75 86 99 41 28 68 71 66
best setup across tasks
cnt 70 62 70 57 72 76 64 84 98 37 27 43 41 44
pre 83 73 78 68 80 86 71 77 98 41 26 67 69 64
worst setup across tasks
cnt 11 16 23 4 21 49 24 43 38 -6 -10 1 0 1
pre 74 60 73 48 68 71 65 82 88 33 20 27 40 10
best setup on rg
cnt (74) 59 66 52 71 64 64 84 98 37 20 35 42 26
pre (84) 71 76 64 79 85 72 84 98 39 25 66 70 61
other models
soa 86 81 77 62 76 100 79 91 96 60 32 61 64 61
dm 82 35 60 13 42 77 76 84 94 51 29 NA NA NA
cw 48 48 61 38 57 56 58 61 70 28 15 11 12 9
Table 2: Performance of count (cnt), predict (pre), dm and cw models on all tasks. See Section 3 and
Table 1 for figures of merit and state-of-the-art results (soa). Since dm has very low coverage of the an*
data sets, we do not report its performance there.
243
experimenter might be tempted to choose without
tuning). The count model performance is severely
affected by this unlucky choice (2-word window,
Local Mutual Information, NMF, 400 dimensions,
mean performance rank: 83), whereas the predict
approach is much more robust: To put its worst in-
stantiation (2-word window, hierarchical softmax,
no subsampling, 200 dimensions, mean rank: 51)
into perspective, its performance is more than 10%
below the best count model only for the an and
ansem tasks, and actually higher than it in 3 cases
(note how on esslli the worst predict models per-
forms much better than the best one, confirming
our suspicion about the brittleness of this small
data set). The fourth block reports performance in
what might be the most realistic scenario, namely
by tuning the parameters on a development task.
Specifically, we pick the models that work best
on the small rg set, and report their performance
on all tasks (we obtained similar results by pick-
ing other tuning sets). The selected count model
is the third best overall model of its class as re-
ported in Table 3. The selected predict model is
the fourth best model in Table 4. The overall count
performance is not greatly affected by this choice.
Again, predict models confirm their robustness,
in that their rg-tuned performance is always close
(and in 3 cases better) than the one achieved by the
best overall setup.
Tables 3 and 4 let us take a closer look at
the most important count and predict parame-
ters, by reporting the characteristics of the best
models (in terms of average performance-based
ranking across tasks) from both classes. For the
count models, PMI is clearly the better weight-
ing scheme, and SVD outperforms NMF as a di-
mensionality reduction technique. However, no
compression at all (using all 300K original dimen-
sions) works best. Compare this to the best over-
all predict vectors, that have 400 dimensions only,
making them much more practical to use. For the
predict models, we observe in Table 4 that nega-
tive sampling, where the task is to distinguish the
target output word from samples drawn from the
noise distribution, outperforms the more costly hi-
erarchical softmax method. Subsampling frequent
words, which downsizes the importance of these
words similarly to PMI weighting in count mod-
els, is also bringing significant improvements.
Finally, we go back to Table 2 to point out the
poor performance of the out-of-the-box cw model.
window weight compress dim. mean
rank
2 PMI no 300K 35
5 PMI no 300K 38
2 PMI SVD 500 42
2 PMI SVD 400 46
5 PMI SVD 500 47
2 PMI SVD 300 50
5 PMI SVD 400 51
2 PMI NMF 300 52
2 PMI NMF 400 53
5 PMI SVD 300 53
Table 3: Top count models in terms of mean
performance-based model ranking across all tasks.
The first row states that the window-2, PMI, 300K
count model was the best count model, and, across
all tasks, its average rank, when ALL models are
decreasingly ordered by performance, was 35. See
Section 2.1 for explanation of the parameters.
We must leave the investigation of the parameters
that make our predict vectors so much better than
cw (more varied training corpus? window size?
objective function being used? subsampling? . . . )
to further work. Still, our results show that it?s
not just training by context prediction that ensures
good performance. The cw approach is very popu-
lar (for example both Huang et al (2012) and Bla-
coe and Lapata (2012) used it in the studies we dis-
cussed in Section 1). Had we also based our sys-
tematic comparison of count and predict vectors
on the cw model, we would have reached opposite
conclusions from the ones we can draw from our
word2vec-trained vectors!
5 Conclusion
This paper has presented the first systematic com-
parative evaluation of count and predict vectors.
As seasoned distributional semanticists with thor-
ough experience in developing and using count
vectors, we set out to conduct this study because
we were annoyed by the triumphalist overtones of-
ten surrounding predict models, despite the almost
complete lack of a proper comparison to count
vectors.
12
Our secret wish was to discover that it is
all hype, and count vectors are far superior to their
predictive counterparts. A more realistic expec-
12
Here is an example, where word2vec is called the crown
jewel of natural language processing: http://bit.ly/
1ipv72M
244
win. hier. neg. subsamp. dim mean
softm. samp. rank
5 no 10 yes 400 10
2 no 10 yes 300 13
5 no 5 yes 400 13
5 no 5 yes 300 13
5 no 10 yes 300 13
2 no 10 yes 400 13
2 no 5 yes 400 15
5 no 10 yes 200 15
2 no 10 yes 500 15
2 no 5 yes 300 16
Table 4: Top predict models in terms of mean
performance-based model ranking across all tasks.
See Section 2.2 for explanation of the parameters.
tation was that a complex picture would emerge,
with predict and count vectors beating each other
on different tasks. Instead, we found that the pre-
dict models are so good that, while the triumphal-
ist overtones still sound excessive, there are very
good reasons to switch to the new architecture.
However, due to space limitations we have only
focused here on quantitative measures: It remains
to be seen whether the two types of models are
complementary in the errors they make, in which
case combined models could be an interesting av-
enue for further work.
The space of possible parameters of count
DSMs is very large, and it?s entirely possible that
some options we did not consider would have im-
proved count vector performance somewhat. Still,
given that the predict vectors also outperformed
the syntax-based dm model, and often approxi-
mated state-of-the-art performance, a more profic-
uous way forward might be to focus on parameters
and extensions of the predict models instead: Af-
ter all, we obtained our already excellent results
by just trying a few variations of the word2vec de-
faults. Add to this that, beyond the standard lex-
ical semantics challenges we tested here, predict
models are currently been successfully applied in
cutting-edge domains such as representing phrases
(Mikolov et al, 2013c; Socher et al, 2012) or fus-
ing language and vision in a common semantic
space (Frome et al, 2013; Socher et al, 2013).
Based on the results reported here and the con-
siderations we just made, we would certainly rec-
ommend anybody interested in using DSMs for
theoretical or practical applications to go for the
predict models, with the important caveat that they
are not all created equal (cf. the big difference be-
tween word2vec and cw models). At the same
time, given the large amount of work that has been
carried out on count DSMs, we would like to ex-
plore, in the near future, how certain questions
and methods that have been considered with re-
spect to traditional DSMs will transfer to predict
models. For example, the developers of Latent
Semantic Analysis (Landauer and Dumais, 1997),
Topic Models (Griffiths et al, 2007) and related
DSMs have shown that the dimensions of these
models can be interpreted as general ?latent? se-
mantic domains, which gives the corresponding
models some a priori cognitive plausibility while
paving the way for interesting applications. An-
other important line of DSM research concerns
?context engineering?: There has been for exam-
ple much work on how to encode syntactic in-
formation into context features (Pad?o and Lapata,
2007), and more recent studies construct and com-
bine feature spaces expressing topical vs. func-
tional information (Turney, 2012). To give just
one last example, distributional semanticists have
looked at whether certain properties of vectors re-
flect semantic relations in the expected way: e.g.,
whether the vectors of hypernyms ?distribution-
ally include? the vectors of hyponyms in some
mathematical precise sense.
Do the dimensions of predict models also en-
code latent semantic domains? Do these models
afford the same flexibility of count vectors in cap-
turing linguistically rich contexts? Does the struc-
ture of predict vectors mimic meaningful seman-
tic relations? Does all of this even matter, or are
we on the cusp of discovering radically new ways
to tackle the same problems that have been ap-
proached as we just sketched in traditional distri-
butional semantics?
Either way, the results of the present investiga-
tion indicate that these are important directions for
future research in computational semantics.
Acknowledgments
We acknowledge ERC 2011 Starting Independent
Research Grant n. 283554 (COMPOSES).
References
Eneko Agirre, Enrique Alfonseca, Keith Hall, Jana
Kravalova, Marius Pasc?a, and Aitor Soroa. 2009.
245
A study on similarity and relatedness using distribu-
tional and WordNet-based approaches. In Proceed-
ings of HLT-NAACL, pages 19?27, Boulder, CO.
Abdulrahman Almuhareb. 2006. Attributes in Lexical
Acquisition. Phd thesis, University of Essex.
Marco Baroni and Alessandro Lenci. 2010. Dis-
tributional Memory: A general framework for
corpus-based semantics. Computational Linguis-
tics, 36(4):673?721.
Marco Baroni, Stefan Evert, and Alessandro Lenci, ed-
itors. 2008. Bridging the Gap between Semantic
Theory and Computational Simulations: Proceed-
ings of the ESSLLI Workshop on Distributional Lex-
ical Semantic. FOLLI, Hamburg.
Marco Baroni, Eduard Barbu, Brian Murphy, and Mas-
simo Poesio. 2010. Strudel: A distributional seman-
tic model based on properties and types. Cognitive
Science, 34(2):222?254.
Yoshua Bengio, R?ejean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Re-
search, 3:1137?1155.
William Blacoe and Mirella Lapata. 2012. A com-
parison of vector-based representations for seman-
tic composition. In Proceedings of EMNLP, pages
546?556, Jeju Island, Korea.
David Blei, Andrew Ng, and Michael Jordan. 2003.
Latent Dirichlet alocation. Journal of Machine
Learning Research, 3:993?1022.
Elia Bruni, Nam Khanh Tran, and Marco Ba-
roni. 2013. Multimodal distributional seman-
tics. Journal of Artificial Intelligence Research.
In press; http://clic.cimec.unitn.it/
marco/publications/mmds-jair.pdf.
John Bullinaria and Joseph Levy. 2007. Extracting
semantic representations from word co-occurrence
statistics: A computational study. Behavior Re-
search Methods, 39:510?526.
John Bullinaria and Joseph Levy. 2012. Extracting
semantic representations from word co-occurrence
statistics: Stop-lists, stemming and SVD. Behavior
Research Methods, 44:890?907.
Yanqing Chen, Bryan Perozzi, Rami Al-Rfou?, and
Steven Skiena. 2013. The expressive power of
word embeddings. In Proceedings of the ICML
Workshop on Deep Learning for Audio, Speech and
Language Processing, Atlanta, GA. Published on-
line: https://sites.google.com/site/
deeplearningicml2013/accepted_
papers.
Stephen Clark. 2013. Vector space mod-
els of lexical meaning. In Shalom Lappin
and Chris Fox, editors, Handbook of Contem-
porary Semantics, 2nd ed. Blackwell, Malden,
MA. In press; http://www.cl.cam.ac.uk/
?
sc609/pubs/sem_handbook.pdf.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In Pro-
ceedings of ICML, pages 160?167, Helsinki, Fin-
land.
Ronan Collobert, Jason Weston, L?eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. Journal of Machine Learning Research,
12:2493?2537.
Katrin Erk. 2012. Vector space models of word mean-
ing and phrase meaning: A survey. Language and
Linguistics Compass, 6(10):635?653.
Stefan Evert. 2005. The Statistics of Word Cooccur-
rences. Ph.D dissertation, Stuttgart University.
Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,
Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-
tan Ruppin. 2002. Placing search in context: The
concept revisited. ACM Transactions on Informa-
tion Systems, 20(1):116?131.
Andrea Frome, Greg Corrado, Jon Shlens, Samy Ben-
gio, Jeff Dean, Marc?Aurelio Ranzato, and Tomas
Mikolov. 2013. DeViSE: A deep visual-semantic
embedding model. In Proceedings of NIPS, pages
2121?2129, Lake Tahoe, Nevada.
Gene Golub and Charles Van Loan. 1996. Matrix
Computations (3rd ed.). JHU Press, Baltimore, MD.
Tom Griffiths, Mark Steyvers, and Josh Tenenbaum.
2007. Topics in semantic representation. Psycho-
logical Review, 114:211?244.
Guy Halawi, Gideon Dror, Evgeniy Gabrilovich, and
Yehuda Koren. 2012. Large-scale learning of
word relatedness with constraints. In Proceedings
of KDD, pages 1406?1414.
Samer Hassan and Rada Mihalcea. 2011. Semantic
relatedness using salient semantic analysis. In Pro-
ceedings of AAAI, pages 884?889, San Francisco,
CA.
Amac? Herda?gdelen and Marco Baroni. 2009. Bag-
Pack: A general framework to represent semantic
relations. In Proceedings of GEMS, pages 33?40,
Athens, Greece.
Eric Huang, Richard Socher, Christopher Manning,
and Andrew Ng. 2012. Improving word represen-
tations via global context and multiple word proto-
types. In Proceedings of ACL, pages 873?882, Jeju
Island, Korea.
George Karypis. 2003. CLUTO: A clustering toolkit.
Technical Report 02-017, University of Minnesota
Department of Computer Science.
246
Sophia Katrenko and Pieter Adriaans. 2008. Qualia
structures and their impact on the concrete noun
categorization task. In Proceedings of the ESS-
LLI Workshop on Distributional Lexical Semantics,
pages 17?24, Hamburg, Germany.
Thomas Landauer and Susan Dumais. 1997. A solu-
tion to Plato?s problem: The latent semantic analysis
theory of acquisition, induction, and representation
of knowledge. Psychological Review, 104(2):211?
240.
Daniel Lee and Sebastian Seung. 2000. Algorithms for
Non-negative Matrix Factorization. In Proceedings
of NIPS, pages 556?562.
Chih-Jen Lin. 2007. Projected gradient methods for
Nonnegative Matrix Factorization. Neural Compu-
tation, 19(10):2756?2779.
Ken McRae, Michael Spivey-Knowlton, and Michael
Tanenhaus. 1998. Modeling the influence of the-
matic fit (and other constraints) in on-line sentence
comprehension. Journal of Memory and Language,
38:283?312.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word represen-
tations in vector space. http://arxiv.org/
abs/1301.3781/.
Tomas Mikolov, Quoc Le, and Ilya Sutskever. 2013b.
Exploiting similarities among languages for Ma-
chine Translation. http://arxiv.org/abs/
1309.4168.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-
rado, and Jeff Dean. 2013c. Distributed representa-
tions of words and phrases and their compositional-
ity. In Proceedings of NIPS, pages 3111?3119, Lake
Tahoe, Nevada.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013d. Linguistic regularities in continuous space
word representations. In Proceedings of NAACL,
pages 746?751, Atlanta, Georgia.
George Miller and Walter Charles. 1991. Contex-
tual correlates of semantic similarity. Language and
Cognitive Processes, 6(1):1?28.
Sebastian Pad?o and Mirella Lapata. 2007.
Dependency-based construction of semantic space
models. Computational Linguistics, 33(2):161?199.
Ulrike Pad?o. 2007. The Integration of Syntax and
Semantic Plausibility in a Wide-Coverage Model of
Sentence Processing. Dissertation, Saarland Univer-
sity, Saarbr?ucken.
Klaus Rothenh?ausler and Hinrich Sch?utze. 2009.
Unsupervised classification with dependency based
word spaces. In Proceedings of GEMS, pages 17?
24, Athens, Greece.
Herbert Rubenstein and John Goodenough. 1965.
Contextual correlates of synonymy. Communica-
tions of the ACM, 8(10):627?633.
Magnus Sahlgren. 2006. The Word-Space Model.
Ph.D dissertation, Stockholm University.
Richard Socher, Brody Huval, Christopher Manning,
and Andrew Ng. 2012. Semantic compositionality
through recursive matrix-vector spaces. In Proceed-
ings of EMNLP, pages 1201?1211, Jeju Island, Ko-
rea.
Richard Socher, Milind Ganjoo, Christopher Manning,
and Andrew Ng. 2013. Zero-shot learning through
cross-modal transfer. In Proceedings of NIPS, pages
935?943, Lake Tahoe, Nevada.
Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio.
2010. Word representations: A simple and general
method for semi-supervised learning. In Proceed-
ings of ACL, pages 384?394, Uppsala, Sweden.
Peter Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of se-
mantics. Journal of Artificial Intelligence Research,
37:141?188.
Peter Turney. 2012. Domain and function: A dual-
space model of semantic relations and compositions.
Journal of Artificial Intelligence Research, 44:533?
585.
247
Proceedings of the Third Joint Conference on Lexical and Computational Semantics (*SEM 2014), pages 171?181,
Dublin, Ireland, August 23-24 2014.
Dead parrots make bad pets:
Exploring modifier effects in noun phrases
Germ
?
an Kruszewski and Marco Baroni
Center for Mind/Brain Sciences (University of Trento, Italy)
(german.kruszewski|marco.baroni)@unitn.it
Abstract
Sometimes modifiers have a strong effect
on core aspects of the meaning of the
nouns they are attached to: A parrot is
a desirable pet, but a dead parrot is, at
the very least, a rather unusual household
companion. In order to stimulate compu-
tational research into the impact of mod-
ification on phrase meaning, we collected
and made available a large dataset contain-
ing subject ratings for a variety of noun
phrases and the categories they might be-
long to. We propose to use compositional
distributional semantics to model these
data, experimenting with numerous distri-
butional semantic spaces, phrase compo-
sition methods and asymmetric similarity
measures. Our models capture a statis-
tically significant portion of the data, al-
though much work is still needed before
we achieve a full computational account of
modification effects.
1 Introduction
Not all modifiers are created equal. Green parrots
have all essential qualities of parrots, but dead par-
rots don?t. For example, as vocally argued by the
disgruntled costumer in Monty Python?s famous
Dead Parrot Sketch,
1
dead parrots make rather
poor pet birds. In modifier-head constructions
(that, for the purpose of this article, we restrict to
right-headed adjective-noun and noun-noun con-
structions), modifiers are not simply picking a sub-
set of the denotation of the head they modify, but
they are often distorting the properties of the head
in a radical manner.
These modifier effects on phrase meaning have
been studied extensively by theoretical linguists,
1
http://en.wikipedia.org/wiki/Dead_
Parrot_sketch
who have focused primarily on the extreme case
of intensional modifiers such as fake, alleged and
toy, where the phrase denotes something that is
no longer (or is not necessarily) a head (a toy
gun is not a gun). See McNally (2013) for a re-
cent review of the linguistic literature. Cognitive
scientists have looked at modification phenomena
within the general study of conceptual combina-
tion (see Chapter 12 of Murphy (2002) for an ex-
tensive review). The cognitive tradition has fo-
cused on how modification affects prototypicality:
a guppy is the prototypical pet fish, but it is neither
a typical pet nor a typical fish (Smith and Osher-
son, 1984). This line of research has highlighted
how strong modification effects might be the rule,
rather than the exception: Wisniewski (1997) re-
ports that, when subjects were asked to provide
the meaning for more than 200 novel modifier-
head constructions, ?70% [of the answers] in-
volved the construal of a noun?s referent as some-
thing other than the typical category named by the
noun [head].? Indeed, recent research suggests
that even the most stereotypical modifiers affect
prototypicality, so that subjects are less willing
to attribute to quacking ducks such obvious duck
properties as having webbed feet (Connolly et al.,
2007).
The impact of modification on phrase mean-
ing is not only very interesting from a linguistic
and cognitive perspective, but also important from
a practical point of view, as it might affect ex-
pected entailment patterns: If parrot entails pet,
then lively parrot also entails pet. However, as we
saw above, dead parrot doesn?t necessarily entail
pet (at least not from the point of view of a dis-
gruntled costumer who was just sold the corpse).
Being able to track the impact that modifiers have
on heads should thus have a positive effect on im-
portant tasks such as recognizing textual entail-
ment, paraphrasing and anaphora resolution (An-
droutsopoulos and Malakasiotis, 2010; Dagan et
171
al., 2009; Poesio et al., 2010).
Despite their theoretical and practical import,
modification effects have been largely overlooked
in computational linguistics, with the notable ex-
ception of Boleda et al. (2012; 2013), who only
focused on the extreme case of intensional adjec-
tives, studied a limited number of modifiers, and
did not attempt to capture the graded nature of
modification (a dead parrot is not a prototypical
animal, but a toy parrot is not an animal at all).
This paper aims to stimulate computational re-
search into modifier effects on phrase meaning in
two ways. First, we introduce a new, large, pub-
licly available data set of modifier-head phrases
annotated with four kinds of modification-related
subject ratings: whether the concept denoted by
the phrase is an instance of the concept denoted by
its head (is a dead parrot still a parrot?), to what
extent it is a member of one of the larger categories
the head belongs to (is it still a pet?), and typical-
ity ratings for the same questions (how typical is a
dead parrot as a parrot? and as a pet?).
Second, we present a first attempt to model the
collected judgments computationally. We choose
distributional semantics (Erk, 2012) as our frame
of reference, as it produces continuous similarity
scores, in line with the graded nature of the mod-
ification effects we are investigating. In partic-
ular, we look at the compositional extension of
distributional semantics (Baroni, 2013), because
we need representations not only for words, but
also phrases, and we adopt the asymmetric simi-
larity measures developed in the literature on lex-
ical entailment (Kotlerman et al., 2010; Lenci and
Benotto, 2012), because we are interested in an
asymmetric relation (to what extent the concept
denoted by the phrase is a good instance of the tar-
get class, and not vice versa). As far as we know,
this is the first time these asymmetric measures
are applied to composed representations (Baroni
et al. (2012) experimented with entailment mea-
sures applied to phrase representations directly
harvested from corpora, and not derived composi-
tionally). We are thus also providing a novel eval-
uation of compositional models and asymmetric
measures on a challenging task where they could
potentially be very useful.
2
2
Connell and Ramscar (2001) showed good correlation of
similarity scores produced by the LSA distributional seman-
tic model with human category typicality judgments, how-
ever they did not consider phrases nor adopted an asymmetric
measure to take directionality into account.
2 The Norwegian Blue Parrot data set
We introduce Norwegian Blue Parrot (NBP),
3
a
new, large data set to explore modification effects.
Given a head noun h and a modifier adjective or
noun m, NBP contains average membership and
typicality ratings for the phrase mh both as an
instance of h and as an instance of c (a broader
category h belongs to). As a control, we also
present ratings for unmodified h as an instance
of c (we will use them below to test similarity
measures on their ability to capture the direction
of the membership relation, and to zero in on the
effect of modification vs. more general member-
ship/typicality effects). We include, and indeed fo-
cus on, relations with broader categories because
they are more prone to modification effects: In-
tuitively, a dead parrot is still a parrot, but it is,
at the very least, an atypical pet. The statistics
in Table 1, discussed below, confirm our intuition
that subjects are more likely to assign lower scores
with respect to a broader category than to the head
category itself (although this is, no doubt, in part
by construction, since we started constructing the
dataset by mining examples where mh is atypi-
cal of c, not h). We collect both membership and
typicality ratings because we expect them to have
different implications for sound entailment. If x
is not a member of class y, then x obviously does
not entail y. However, if x is an atypical y, en-
tailment still holds, but some typical properties of
y might not carry over (e.g., in an anaphora reso-
lution setting, we might still consider co-indexing
dead parrot with animal, but not with breathing
creature, despite the fact that breathing is a highly
characteristic property of animals).
In order to make sure that NBP would contain a
fair number of examples affected by strong mod-
ification effects, we first came up with a set of
?m,h, c? tuples where, according to our own in-
tuition, m makes h fairly atypical as an instance
of c. For example, a bottle is a piece of drinkware.
If we add the modifier perfume, we expect that,
while subjects might still agree that a perfume bot-
tle is a bottle, they should generally disagree on
the statement that a perfume bottle belongs to the
drinkware category. We refer to tuples of this
sort (e.g., ?perfume, bottle, drinkware?) as dis-
torted tuples in what follows.
4
3
Available from http://clic.cimec.unitn.it/
composes/
4
When creating the tuples, we also used some adjectives
172
We then constructed a number of tuples that
should not display a strong modification effect. In
particular, in order to insure that any atypical rat-
ing we obtained on the distorted tuples could not
be explained away by characteristics of m or h
alone (rather than by their combination), for each
distorted tuple we constructed a few more tuples
with the same h and c but a different m, that
we did not expect to be strongly distorting (e.g.,
?plastic, bottle, drinkware?). Similarly, for each
distorted tuple we generated a few more with the
same m, but combined with (the same or differ-
ent) h and c on which the m should not exert a
strong effect (?perfume, bottle, container?). In
total, NBP is based on 489 distorted tuples and
1938 more matching tuples.
We constructed NBP to insure that it would
contain many tuples displaying strong modifica-
tion effects, and highly comparable tuples that do
not feature such effects. An alternative approach
would have been to rate phrases that were ran-
domly selected from a corpus. This would have
led to a dataset reflecting a more realistic distribu-
tion of modification effects, but it would not have
guaranteed, for the same number of pairs, a fair
amount of distorted tuples and comparable con-
trols. We leave the study of the natural distribution
of modification strength in text to further work.
To find inspiration for the tuples, we looked into
various databases containing concepts organized
by category, namely BLESS (Baroni and Lenci,
2011), ConceptNet (Speer and Havasi, 2013) and
WordNet (Fellbaum, 1998). We insured that all
words in our tuples occurred at least 200 times in
the large corpus we describe below (phrases were
not filtered by frequency, due to data sparseness).
Finally, when looking for tuples matching the dis-
torted ones, we made sure that the mh phrases in
the new tuples have similar Pointwise Mutual In-
formation to the corresponding phrases in the dis-
torted tuple (or, where the latter were not attested
in the corpus, similar m and h frequencies). Find-
ing meaningful combinations among unattested or
infrequent phrases was not an easy task and there
was not always a perfect candidate. However, the
phrases selected in this way yielded challenging
items for which there is little or no direct cor-
pus evidence, so that compositional models are re-
quired to account for them.
that have been traditionally labeled as intensional by seman-
ticists: artificial, toy, former.
From each source tuple (e.g.,
?plastic, bottle, drinkware?), we generated 3
instance-class combinations to be rated: mh ? c
(plastic bottle ? drinkware), mh ? h (plastic
bottle? bottle), h? c (bottle? drinkware), for
a total of 5,849 pairs, that constitute the final NBP
data set (2,417 mh ? c pairs, 2,115 mh ? h
pairs and 1,317 h? c pairs).
5
For each of these pairs, we collected both mem-
bership and typicality ratings through two surveys
on the CrowdFlower platform.
6
Subjects came
exclusively from English speaking countries and
no special qualifications were required from them.
Membership ratings were collected by asking sub-
jects whether the instance is a member of the class
(formulated as a yes/no question). In a separate
study, we asked subjects to rate how typical the in-
stance is as member of the class on a 7-point scale.
For both questions, we collected 10 judgments per
pair and report their averages in NBP. For both sur-
veys, we added 48 control pairs with an expected
answer (yes/no for membership, high/low range
for typicality), that the subjects had to provide in
order for their ratings to be included in the final
set (?gold standard? items in crowd-sourcing par-
lance). These controls included highly prototypi-
cal pairs (dog? animal), possibly with stereotyp-
ical modifiers (beautiful rose? flower), and unre-
lated pairs (biology? dance), also possibly under
modification (popular magazine? animal).
We asked for binary rather than graded member-
ship judgments because these are more in line with
commonsense intuitions about category member-
ship (we might naturally speak of sparrows being
more typical birds than penguins, but it is strange
to say that they are ?more birds?). The standard
view in the psychology of concepts (Hampton,
1991) is that membership judgments are the prod-
uct of a hard threshold we impose on the typicality
scale (x is not y if the typicality of x as y is below
a certain, subject-dependent threshold), although
under certain experimental conditions subjects can
also conceptualize membership as a graded prop-
erty (Kalish, 1995).
Membership and typicality ratings, especially
in borderline cases such as those we constructed,
are the output of complex cognitive processes
where large inter-subject differences are expected,
5
There is a larger number of mh ? c pairs because dif-
ferent tuples can lead to the same mh? h or h? c combi-
nations.
6
http://crowdflower.com/
173
measure mh? c mh? h h? c tot.
memb. 0.84 (0.2) 0.97 (0.1) 0.88 (0.2) 0.89 (0.2)
typ. 5.45 (1.1) 6.29 (0.6) 5.81 (1.0) 5.84 (1.0)
Table 1: NBP summary statistics: Mean average
ratings and their standard deviations across pairs,
itemized by instance-class type and in total. Mem-
bership values range from 0 to 1, typicality values
from 1 to 7.
so it doesn?t make sense to worry about ?inter-
annotator agreement? in this context. Still, several
sanity checks indicate that, overall, our subjects
understood our questions as we meant them, and
behaved in a reasonably coherent manner. First,
both average membership and typicality, ratings
are significantly lower (p < 0.001) for the mh ?
c pairs deriving from those tuples that we manu-
ally labeled as distorted than for the non-distorted
ones. Moreover, for membership, in 86% of the
cases at least 8 over 10 subjects gave the same re-
sponse. For typicality, the observed average rat-
ing standard deviation across pairs (1.2) is signifi-
cantly below what expected by chance (p < 0.05),
based on a simulated random rating distribution.
Membership and typicality ratings are highly cor-
related, but not identical (r = 0.76)
Table 1 reports mean membership and typicality
scores in NBP. Both ratings are negatively skewed,
that is, subjects had the tendency to respond as-
sertively to the membership question and to give
high typicality scores. This is not surprising: Be-
cause of the way NBP was constructed, there are
about 4 tuples with no expected strong modifica-
tion effect for each distorted tuple. Furthermore,
except for the negative control items (not entered
in NBP), our questions did not feature cases where
a negative/low response would be entirely straight-
forward (of the ?is a cat a building?? kind). We
observe moreover that, in accordance with the in-
tuition we discussed at the beginning of this sec-
tion, the ratings are extremely high when the class
is identical to the phrase head. On the other hand,
the mh ? c condition displays, as expected, the
lowest averages, suggesting that this will be the
most interesting type to model experimentally.
Table 2 presents a few example entries from
NBP. The first block of the table illustrates cases
with the highest possible membership and typical-
ity scores. At the other extreme, the second block
contains examples with very low membership and
typicality. Interestingly, there are also cases, such
instance class memb. typ.
top membership, top typicality
gourmet soup food 1.00 7.00
huge tiger predator 1.00 7.00
sugared soda drink 1.00 7.00
live fish animal 1.00 7.00
Thai rice rice 1.00 7.00
silver spoon spoon 1.00 7.00
low membership, low typicality
fatal shooting sport 0.20 1.40
human egg food 0.40 1.50
perfume bottle drinkware 0.10 1.30
explosive vest commodity 0.30 1.90
lemon water chemical 0.20 1.60
creamy rice bean 0.20 1.30
top membership, (relatively) low typicality
sick tuna tuna 1.00 3.20
explosive vest vest 1.00 3.50
perforated sieve tool 1.00 4.20
bottled oxygen substance 1.00 4.30
grilled trout creature 1.00 4.40
educational toy amusement 1.00 4.50
Table 2: Instance-class pairs illustrating various
combinations of membership and typicality rat-
ings in NBP.
as the ones in the third block of the table, where all
subjects agreed on class membership, but the typ-
icality scores are relatively low (we did not find
clear cases of the opposite pattern, and indeed we
would have been surprised to find highly typical
instances of a class not being treated as members
of the class).
Some examples in Table 2 illustrate an impor-
tant design choice we made in constructing NBP,
namely, to ignore the issue of whether potential
modification effects are actually due to the modi-
fier and the category pertaining to different word
senses of the head term. One might argue, for
example, that egg has a food sense and a repro-
ductive vessel sense. The human modifier picks
the second sense, and so, obviously, human eggs
are judged as bad instances of food. While we
see the point of this objection, we think it?s im-
possible to draw a clear-cut distinction between
discrete word senses (even in the rather extreme
egg case, the eggs we eat are reproductive ves-
sels from a chicken point of view!). This has
been long recognized in the linguistic and cog-
nitive literature (Kilgarriff, 1997; Murphy, 2002),
174
and even by the computational word sense disam-
biguation community, that is currently addressing
the continuous nature of polysemy by shifting to
the lexical-substitution-in-context task (McCarthy
and Navigli, 2009). Context provides fundamen-
tal cues to disambiguating polysemous words, and
noun modifiers typically act as important disam-
biguating contexts for the nouns. Thus, we think
that it is more productive for computational sys-
tems to handle modifier-triggered disambiguation
as a special case of the more general class of mod-
ification effects, than to engage in the quixotic
pursuit to determine, a priori, what?s the bound-
ary between a word-sense and a ?pure? modifi-
cation effect. Note in Table 2 that grilled trout
was unanimously rated by subjects as an instance
of the creature category, despite the fact that the
cooking-related grilled modifier cues a classic
shift from an animal (and thus creature) sense to
food (Copestake and Briscoe, 1995). Examples
like this suggest that our agnosticism is warranted.
3 Methods
3.1 Composition models
We experiment with many ways to derive a phrase
vector by combining the vectors of its constituents.
Mitchell and Lapata (2010) proposed a set of sim-
ple models in which each component of the phrase
vector is a function of the corresponding compo-
nents of the constituent vectors. Given vectors ~a
and
~
b, the weighted additive model (wadd) returns
their weighted sum: ~p = w
1
~a + w
2
~
b. In the dila-
tion model (dil), the output vector is obtained by
decomposing one of the input vectors, say
~
b, into
a vector parallel to ~a and its orthogonal counter-
part, and then dilating only the parallel vector by a
factor ? before re-combining. The corresponding
formula is: (~a ?~a)
~
b + (? ? 1)(~a ?
~
b)~a. In our ex-
periments, we stretch the head vector in the direc-
tion of the modifier (i.e., ~a is the modifier,
~
b is the
head). In the multiplicative model (mult), vectors
are combined by component-wise multiplication,
such that each phrase component p
i
is given by:
p
i
= a
i
b
i
.
Guevara (2010) and Zanzotto et al. (2010) pro-
pose a full form of the additive model (fulladd),
where the two constituent vectors are multiplied
by weight matrices before being added, so that
each phrase component is a weighted sum of all
constituent components: ~p = W
1
~a+W
2
~
b.
Finally, the lexical function (lexfunc) model of
Baroni and Zamparelli (2010) and Coecke et al.
(2010) takes inspiration from formal semantics
to characterize composition as function applica-
tion. In particular, in modifier-head phrases, the
modifier is treated as a linear function operating
on the head vector. Given that linear functions
can be expressed by matrices and their application
by matrix-by-vector multiplication, the modifier is
represented by a matrix A to be multiplied with
the modifier vector
~
b, so that: ~p = A
~
b.
We use the DISSECT toolkit
7
to estimate the
parameters of the composition methods and de-
rive phrase vectors. In particular, DISSECT finds
optimal parameter settings by learning to approx-
imate corpus-extracted phrase vector examples
with least-squares methods (Dinu et al., 2013).
We use as training examples all the modifier-head
phrases that contain a modifier of interest and oc-
cur at least 50 times in our source corpus (see Sec-
tion 3.3 below).
3.2 Asymmetric similarity measures
Several measures to identify word pairs that stand
in an instance-class relationship by comparing
their vectors have been proposed in the recent dis-
tributional semantics literature (Kotlerman et al.,
2010; Lenci and Benotto, 2012; Weeds et al.,
2004).
8
While the task of deciding if u is in class v
is typically framed (also by distributional semanti-
cists) in binary, yes-or-no terms, all proposed mea-
sures return a continuous numerical score.
9
Con-
sequently, we conjecture that they might be well-
suited to capture the graded notions of class mem-
bership and typicality we recorded in NBP.
10
In what follows, we use w
x
(f) to denote the
weight (value) of feature (dimension) f in the dis-
tributional vector of term x. F
x
denotes the set of
features (dimensions) in the vector of x such that
w
x
(f) > t, where t is a predefined threshold to
decide whether a feature is active.
11
Importantly,
7
http://clic.cimec.unitn.it/composes/
toolkit/
8
We speak of ?instance-class relations? in a very broad
and loose sense, to encompass classic relations such as hy-
ponymy but also the fuzzier notion of lexical entailment.
9
SVM classifiers have also been shown by Baroni et al.
(2012) to be well-suited for entailment detection, but they do
not naturally return continuous scores.
10
Subjects had to answer a yes/no question concerning
class membership, but by averaging their response we derive
continuous membership scores.
11
The obvious choice for t is 0. However, when work-
ing with the low-rank spaces described in Section 3.3 below,
we set t to 0.1, since after SVD/NMF smoothing we observe
175
all measures assume non-negative values.
Most asymmetric measures proposed in the lit-
erature build upon the distributional inclusion hy-
pothesis, stating that ?if u is a semantically nar-
rower term than v, then a significant number
of salient distributional features of u is included
in the feature vector of v as well? (Lenci and
Benotto, 2012). In our terminology, u is the poten-
tial instance, and v is the class. We re-implement
all the measures adopted by Lenci and Benotto,
namely weedsprec, cosweeds, clarkede and invcl
(see their paper for the original references):
weedsprec(u, v) =
?
f?F
u
?F
v
w
u
(f)
?
f?F
u
w
u
(f)
cosweeds(u, v) =
?
weedsprec(u, v)? cosine(u, v)
clarkede(u, v) =
?
f?F
u
?F
v
min(w
u
(f), w
v
(f))
?
f?F
u
w
u
(f)
invcl(u, v) =
?
clarkede(u, v)? (1? clarkede(u, v))
The cosweeds formula combines weedsprec
with the widely used symmetric cosine measure:
cosine(u, v) =
?
f?F
u
?F
v
w
u
(f)? w
v
(f)
?
?
f?F
u
w
u
(f)
2
?
?
?
f?F
v
w
v
(f)
2
Finally, we experiment with the carefully
crafted balapinc measure of Kotlerman et al.
(2010):
balapinc(u, v) =
?
lin(u, v) ? apinc(u, v)
where the lin term is computed as follows:
lin(u, v) =
?
f?F
u
?F
v
w
u
(f) + w
v
(f)
?
f?F
u
w
u
(f) +
?
f?F
v
w
v
(f)
The balapinc score is the geometric average
of a symmetric similarity measure (lin) and the
strongly asymmetric apinc measure, that takes
large values when dimensions with high values in
the vector of the more specific term are also high
in the vector of the more general term (refer to
Kotlerman et al. (2010) for the apinc formula).
widespread low-frequency noise.
3.3 Distributional semantic spaces
We extract co-occurrence information from a cor-
pus of about 2.8 billion words obtained by con-
catenating ukWaC,
12
Wikipedia
13
and the British
National Corpus.
14
With DISSECT, we build co-
occurrence vectors for the top 20K most frequent
lemmas in the source corpus (plus any NBP term
missing from this list). We treat the top 10K
most frequent lemmas as context elements. We
consider context windows of 2 and 20 words on
the two sides of the targets. We weight the vec-
tors by non-negative Pointwise Mutual Informa-
tion and Local Mutual Information (Evert, 2005).
We experiment with vectors in the resulting full-
rank (10K-dimensional) semantic spaces as well
as with vectors in spaces of ranks 100 and 300.
Rank reduction is performed by applying the Sin-
gular Value Decomposition (Golub and Van Loan,
1996) or Non-negative Matrix Factorization (Lee
and Seung, 2000). It is customary to represent the
output of these operations directly in a dense low-
dimensional space. However, the asymmetric sim-
ilarity measures we use assume sparse vectors (or
the ?inclusion? criterion would be meaningless),
so we project back the outcome of SVD and NMF
to sparse 10K-dimensional but low-rank spaces. In
total, we explore 20 distinct semantic spaces.
We also collect co-occurrence vectors for
the phrases needed to estimate the composi-
tion method parameters (see Section 3.1 above).
We use DISSECT?s ?peripheral space? option to
project the phrase raw count vectors into the vari-
ous spaces without affecting their structure.
Due to memory constraints, we restrict evalua-
tion in the full-rank spaces to the wadd and mult
models.
4 Experiments
Given the methods described above, the main
question we want to answer is: Which combina-
tion of compositional model and asymmetric sim-
ilarity measure yields a better fit for the data in the
NBP dataset?
We start however with a sanity check on the
ability of the measures to capture the direction of
the instance-class membership relation. Even a
measure that is good at capturing degrees of mem-
bership/typicality won?t be of much practical use
12
http://wacky.sslmit.unibo.it
13
http://en.wikipedia.org
14
http://www.natcorp.ox.ac.uk
176
clarkede weedsprec balapinc cosweeds invcl
Low-rank spaces
10 8 11 8 7
Full-rank spaces
2 4 4 4 2
Table 3: Number of spaces (over totals of 16 low-
rank and 4 full-rank spaces) in which each mea-
sure was able to predict class membership direc-
tion significantly above chance.
if it is not able to tell us which item in a pair is the
instance and which is the class.
Detecting membership direction As described
in Section 2 above, NBP also contains single-
word h? c pairs (parrot? pet). We extracted
the subset of those that all judges considered to
be in the category membership relation, and we
checked them manually to make sure that the di-
rection was one-way only. This resulted in a set
of 639 pairs where the membership relation holds
unidirectionally. We tested all combination of se-
mantic spaces (Section 3.3) and asymmetric sim-
ilarity measures (Section 3.2) on the task of as-
signing a higher score to the pairs in the h ? c
(vs. c ? h) direction (e.g., (score(parrot ?
pet) > score(pet ? parrot)). Table 3 reports,
for each measure, the number of spaces in which
the measure was able to predict membership di-
rection significantly better than chance (binomial
test, p < 0.05). We report results on full- and
low-rank (SVD, NMF) spaces separately since, as
discussed above, for most composition models we
can only use the latter. We observe that all mea-
sures are able to significantly detect directionality
in at least some spaces. For all the analyses below,
we exclude from further testing the space-measure
combinations that failed to pass this sanity check,
since they are clearly failing to capture properties
pertaining to the instance-class relation (if a com-
bination is not able to tell that it is a parrot that is
a pet, and not vice versa, there is no point in ask-
ing if the same combination is able to model how
typical a dead parrot is as a pet).
Modeling typicality ratings of mh ? c pairs
Next, for each of the remaining spaces, we first
performed composition as described in Section 3.1
above to build the representations for the nominal
phrases in the NBP dataset, and then computed
asymmetric similarity scores for pairs made of a
phrase and the corresponding potential class.
We computed the correlations between mean
human membership or typicality ratings and the
scores produced with each combination of com-
position model, similarity measure and space.
The resulting performance profiles for member-
ship and typicality are very highly correlated (r =
.99), and we thus report only the latter. We leave
it to further work to devise measures that are more
specifically tuned to capture membership or typi-
cality.
Table 4 reports the top correlation coefficients
between typicality judgments and scores of each
mh ? c pair (dead parrot? pet) across spaces,
organized by measures and composition meth-
ods. The best correlation is achieved with the
weedsprec measure using the mult composition
model in a full-rank space (precisely that of con-
text window size 2 and ppmi weighting). Recall
that mult returns the component-wise product of
the vectors it combines. Thus, modification un-
der mult is carried out by picking only those fea-
tures of the head that are also present in the mod-
ifier, and enhancing them by a factor given by the
modifier?s feature value. The weedsprec measure
is then given by the weighted proportion of active
features in mh that are also active in c. Therefore,
the more the modifier shares features with the par-
ent category, the higher weedsprec will be. This
might explain why weedsprec is a good fit for the
mult model in measuring degrees of category typ-
icality.
Looking at composition methods, there is no ev-
idence that the more complex, matrix-based ful-
ladd and lexfunc approaches are performing any
better than the simple multiplicative and additive
methods. Indeed, mult shows the most consistent
overall performance, confirming the conclusion of
Blacoe and Lapata (2012) that, at the present time,
when it comes to composition, ?simpler is better?.
A related point emerges from the comparison of
the low- and full-rank results for mult and wadd.
The smoothing process due to dimensionality re-
duction is quite disruptive for the current asym-
metric measures, that are based on feature inclu-
sion. This is a further reason to stick to simpler
composition methods, that can be applied directly
in the full-rank spaces.
Regarding the measures themselves, we see that
cosweeds, that balances weedsprec with the clas-
sic cosine score, is the most robust, returning good
177
clarkede weedsprec balapinc cosweeds invcl
Low-rank spaces
dil 9* 15* 16* 19* 8*
fulladd 17* 16* 12* 24* ?3
lexfunc 17* 12* 12* 27* ?2
mult 13* 19* 19* 29* 12*
wadd 14* 14* 16* 27* ?2
Full-rank spaces
mult 9* 39* 33* 36* 15*
wadd 30* 34* 31* 35* 14*
Table 4: Percentage Pearson r between asymmet-
ric similarity measures andmh? c typicality rat-
ings. *p < 0.001
results across all composition methods. On the
other hand, the related clarkede and invcl mea-
sures turn out to be quite brittle.
The highly significant correlations show that the
measures do capture to some extent the patterns of
variance in the data. However, when considering
potential practical applications, even the highest
reported correlation (.39) is certainly not impres-
sive, indicating that there is plenty of room for fur-
ther research into developing better composition
methods and/or membership/typicality measures.
Focusing on the modifier effect for mh? c
pairs The typicality judgment for dead parrot as
a pet is influenced by two factors: how typical par-
rots are as pets, and how much more or less typical
dead parrots are as pets, as opposed to parrots in
general. A good model must be able to capture
both factors (and this is what we tested above).
However, we are also interested in assessing to
what extent the models are capturing the modifi-
cation effectproper, as opposed to the overall de-
gree of typicality of the h concept as member of
the c category. To focus on the modification fac-
tor, we partialed out the h?c (parrot?pet) rat-
ings from the mh?c (dead parrot?pet) ratings
and from the corresponding model scores (that is,
we correlated the residuals of mh?c ratings and
model-produced scores after regressing the h?c
ratings on both). The results are shown in Table
5. Correlations are lower overall, but the general
picture from the previous analysis still holds, con-
firming that the computational models are (also)
capturing modifier effects. Interestingly, wadd, dil
and fulladd generally undergo larger performance
drops than mult and lexfunc. Evidently, models
like the latter, in which the modifier selects the
relevant features from the head, are better suited
to explain modification than the former, in which
clarkede weedsprec balapinc cosweeds invcl
Low-rank spaces
dil 5 ?1 ?1 ?2 7*
fulladd 10* 7* 5+ 7+ ?2
lexfunc 15* 9* 10* 18* ?2
mult 4+ 14* 13* 15* 9*
wadd 7+ 7* 9* 12+ ?2
Full-rank spaces
mult 1 25* 21* 24* 5+
wadd 11* 18* 13* 20* 2
Table 5: Percentage Pearson r between asymmet-
ric similarity measures andmh? c typicality rat-
ings where h ? c scores have been partialed out.
*p < 0.001, +p < 0.05
the modifier features are just added to those of the
head by means of a linear combination.
Modeling typicality ratings of mh ? h pairs
We repeated the first analysis for pairs of the type
mh ? h (dead parrot? parrot). The results,
shown in Table 6, are lower than in the previous
analysis. This is probably due to the fact that, as
discussed in Section 2, when the very same con-
cept is used as phrase head and category, judg-
ments are subject to a strong ceiling effect, and
none of our measures is designed to flatten out
above a certain threshold. Indeed, if we measure
the skewness of the typicality ratings,
15
we obtain
that, while for h? c and mh? c the skewness is
of?1.9 and?1.5, respectively, formh?h it gets
to ?3.9.
In any case, the results confirm the brittleness of
the clarkede and invcl measures. The linguistically
motivated lexfunc model emerges here as a com-
petitive alternative to the simpler models. Still, the
best results are obtained with mult and cosweeds
(on the full-rank, context window size 20, ppmi
weighted space). Notably, weedsprec applied to a
pair of the type mh? h, where the phrase is con-
structed using the mult model, results in a constant
value of 1, whatever the modifier and the head
noun is. This is due to the fact that the features of
a phrase composed using mult are a subset of the
features of the head,
16
and in this case the head is
the same as the category. Therefore, by definition,
weedsprec yields a score of 1 for every pair, the
variance is null and hence the correlation is unde-
15
A skewness factor of 0 means that the distribution is bal-
anced around the mean, while the more negative the coeffi-
cient is, the more the left tail is longer and the distribution is
concentrated to the right (toward high typicality values in our
case).
16
In set notation: F
u
? F
v
= F
u
since F
u
? F
v
178
clarkede weedsprec balapinc cosweeds invcl
Low-rank spaces
dil 2 ?1 ?2 ?3 4
fulladd 5+ 5+ 2 1 ?1
lexfunc 14* 8* 14* 17* ?1
mult 3 - 13* 15* 5+
wadd 6+ 8* 7+ 6 ?3
Full-rank spaces
mult ?2 - 18* 19* ?2
wadd 7* 13* 7* 12* ?2
Table 6: Percentage Pearson r between asymmet-
ric similarity measures andmh? h typicality rat-
ings. *p < 0.001, +p < 0.05
fined. As a consequence, in this case cosweeds,
which is the geometric mean between weedsprec
and cosine, reduces to cosine similarity! The latter
might be effective in capturing the degree of simi-
larity between the phrase and its potential category
but, as a symmetric measure, it cannot, alone, pro-
vide a full account of category typicality effects.
5 Conclusion
We introduced the challenge of quantifying the
impact of modification on the meaning of noun
phrases to the computational linguistics commu-
nity. We presented a new dataset that collects
membership and typicality ratings for modifier-
head phrases with respect to the category repre-
sented by the head as well as a broader category.
Since accounting for modifier distortion requires
semantic representations of phrases and model-
ing graded judgments, we consider this an ideal
testbed for compositional distributional semantics.
In the interaction between compositional mod-
els and directional similarity measures, we have
observed that simpler models yield better results.
Specifically, mult and wadd are economical com-
position models than can be applied on full-rank
spaces, which in turn work best with our similar-
ity measures.
Psychologists studying modification effects in
concept combination have proposed models that
are usually quite complex, relying on hand-crafted
feature definitions and making very strong as-
sumptions about the combination process (see for
example Cohen and Murphy (1984), Smith et al.
(1988)). Some of these assumptions have led other
researchers to argue that prototypes do not com-
pose at all (Connolly et al., 2007). In contrast,
the approach we borrow from distributional se-
mantics, while only mildly successful for now, has
the advantage of being very simple both in its con-
struction and application, and in the assumptions
that it makes.
Also notable is that we are putting under the
same umbrella tasks that have been traditionally
tackled separately. For example, among the ef-
fects present in the dataset, we can find both word
sense disambiguation (see discussion at the end of
Section 2) and what Murphy (2002) calls ?knowl-
edge effects? (e.g., a plane makes a very good ma-
chine, but a paper plane doesn?t). Moreover, these
effects can also interact (people know that a hu-
man egg is actually a single, small cell, and hence
not even cannibals would consider it satisfactory
food). We can thus explore the empirical ques-
tion of whether all these related phenomena can
be tackled together, with a single model account-
ing for all of them.
In conclusion, the challenge that we intro-
duced brings together concept combination and
non-subsective modification phenomena studied
in psychology and theoretical linguistics, and tries
to handle them with the standard machinery of
computational linguistics. This challenge has
proved quite difficult for current tools, but this is
exactly what we expected in the first place. Our
goal, from the outset, was to create a task that
could help us delimiting the boundaries of com-
putational methods for characterizing human con-
cepts, while delimiting, at the same time, the no-
tion of human concepts itself.
Acknowledgments
We acknowledge ERC 2011 Starting Independent
Research Grant n. 283554 (COMPOSES).
References
Ion Androutsopoulos and Prodromos Malakasiotis.
2010. A survey of paraphrasing and textual entail-
ment methods. Journal of Artificial Intelligence Re-
search, 38:135?187.
Marco Baroni and Alessandro Lenci. 2011. How
we BLESSed distributional semantic evaluation. In
Proceedings of the EMNLP GEMS Workshop, pages
1?10, Edinburgh, UK.
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of EMNLP, pages 1183?1193, Boston,
MA.
Marco Baroni, Raffaella Bernardi, Ngoc-Quynh Do,
and Chung-Chieh Shan. 2012. Entailment above
179
the word level in distributional semantics. In Pro-
ceedings of EACL, pages 23?32, Avignon, France.
Marco Baroni. 2013. Composition in distributional
semantics. Language and Linguistics Compass,
7(10):511?522.
William Blacoe and Mirella Lapata. 2012. A com-
parison of vector-based representations for seman-
tic composition. In Proceedings of EMNLP, pages
546?556, Jeju Island, Korea.
Gemma Boleda, Eva Maria Vecchi, Miquel Cornudella,
and Louise McNally. 2012. First order vs. higher
order modification in distributional semantics. In
Proceedings of EMNLP, pages 1223?1233, Jeju Is-
land, Korea.
Gemma Boleda, Marco Baroni, Louise McNally, and
Nghia The Pham. 2013. Intensionality was only
alleged: On adjective-noun composition in distribu-
tional semantics. In Proceedings of IWCS, pages
35?46, Potsdam, Germany.
Graham Chapman. 1989. The complete Monty
Python?s flying circus : all the words. Pantheon
Books, New York.
Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen
Clark. 2010. Mathematical foundations for a com-
positional distributional model of meaning. Linguis-
tic Analysis, 36:345?384.
Benjamin Cohen and Gregory L Murphy. 1984. Mod-
els of concepts. Cognitive Science, 8(1):27?58.
Louise Connell and Michael Ramscar. 2001. Using
distributional measures to model typicality in cate-
gorization. In Proceedings of CogSci, pages 226?
231, Edinburgh, UK.
Andrew Connolly, Jerry Fodor, Lila Gleitman, and
Henry Gleitman. 2007. Why stereotypes don?t even
make good defaults. Cognition, 103(1):1?22.
Ann Copestake and Ted Briscoe. 1995. Semi-
productive polysemy and sense extension. Journal
of Semantics, 12:15?67.
Ido Dagan, Bill Dolan, Bernardo Magnini, and Dan
Roth. 2009. Recognizing textual entailment: ratio-
nale, evaluation and approaches. Natural Language
Engineering, 15:459?476.
Georgiana Dinu, Nghia The Pham, and Marco Baroni.
2013. General estimation and evaluation of com-
positional distributional semantic models. In Pro-
ceedings of ACL Workshop on Continuous Vector
Space Models and their Compositionality, pages 50?
58, Sofia, Bulgaria.
Katrin Erk. 2012. Vector space models of word mean-
ing and phrase meaning: A survey. Language and
Linguistics Compass, 6(10):635?653.
Stefan Evert. 2005. The Statistics of Word Cooccur-
rences. Ph.D dissertation, Stuttgart University.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press, Cambridge,
MA.
Gene Golub and Charles Van Loan. 1996. Matrix
Computations (3rd ed.). JHU Press, Baltimore, MD.
Emiliano Guevara. 2010. A regression model of
adjective-noun compositionality in distributional se-
mantics. In Proceedings of GEMS, pages 33?37,
Uppsala, Sweden.
James Hampton. 1991. The combination of prototype
concepts. In Paula Schwanenflugel, editor, The psy-
chology of word meanings, pages 91?116. Erlbaum,
Hillsdale, NJ.
Charles Kalish. 1995. Essentialism and graded mem-
bership in animal and artifact categories. Memory
and Cognition, 23(3):335?353.
Adam Kilgarriff. 1997. I don?t believe in word senses.
Computers and the Humanities, 31:91?113.
Lili Kotlerman, Ido Dagan, Idan Szpektor, and Maayan
Zhitomirsky-Geffet. 2010. Directional distribu-
tional similarity for lexical inference. Natural Lan-
guage Engineering, 16(4):359?389.
Daniel Lee and Sebastian Seung. 2000. Algorithms for
Non-negative Matrix Factorization. In Proceedings
of NIPS, pages 556?562.
Alessandro Lenci and Giulia Benotto. 2012. Identi-
fying hypernyms in distributional semantic spaces.
In Proceedings of *SEM, pages 75?79, Montreal,
Canada.
Diana McCarthy and Roberto Navigli. 2009. The En-
glish lexical substitution task. Language Resources
and Evaluation, 43(2):139?159.
Louise McNally. 2013. Modification. In Maria Aloni
and Paul Dekker, editors, Cambridge Handbook of
Semantics. Cambridge University Press, Cambridge,
UK. In press.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive Sci-
ence, 34(8):1388?1429.
Gregory Murphy. 2002. The Big Book of Concepts.
MIT Press, Cambridge, MA.
Massimo Poesio, Simone Ponzetto, and Yan-
nick Versley. 2010. Computational models
of anaphora resolution: A survey. http:
//clic.cimec.unitn.it/massimo/
Publications/lilt.pdf.
Edward Smith and Daniel Osherson. 1984. Concep-
tual combination with prototype concepts. Cogni-
tive Science, 8(4):337?361.
Edward E Smith, Daniel N Osherson, Lance J Rips,
and Margaret Keane. 1988. Combining prototypes:
A selective modification model. Cognitive Science,
12(4):485?527.
180
Robert Speer and Catherine Havasi. 2013. Con-
ceptNet 5: A large semantic network for relational
knowledge. In Iryna Gurevych and Jungi Kim, edi-
tors, The People?s Web Meets NLP, pages 161?176.
Springer, Berlin.
Julie Weeds, David Weir, and Diana McCarthy. 2004.
Characterising measures of lexical distributional
similarity. In Proceedings of COLING, pages 1015?
1021, Geneva, Switzerland.
Edward Wisniewski. 1997. When concepts combine.
Psychonomic Bulletin & Review, 4(2):167?183.
Fabio Zanzotto, Ioannis Korkontzelos, Francesca
Falucchi, and Suresh Manandhar. 2010. Estimat-
ing linear models for compositional distributional
semantics. In Proceedings of COLING, pages 1263?
1271, Beijing, China.
181
