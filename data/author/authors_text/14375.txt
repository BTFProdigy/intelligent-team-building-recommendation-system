Coling 2010: Demonstration Volume, pages 1?4,
Beijing, August 2010
A Paraphrasing System for Transforming 
Regular Expressions into Honorifics 
Dongli Han, Shuntaro Kamochi, Xin Song, Naoki Akegawa, Tomomasa Hori 
Department of Computer Science and System Analysis,  
College of Humanities and Sciences, Nihon University 
han@cssa.chs.nihon-u.ac.jp 
 
Abstract 
Honorifics in Japanese plays an incred-
ibly important role in all walks of 
social life. The demand to transform 
regular expressions in Japanese into 
honorifics automatically has increased 
rapidly especially in business situations. 
This paper reviews existing studies and 
proposes a system to fill this demand 
with more practicable functions. The 
experiment shows the effectiveness of 
our strategy. 
1 Introduction 
The Japanese language is a kind of highly 
specific language in establishing hierarchical 
relations among people, or paying respects to 
people comparing with other languages. The 
honorifics in Japanese include different levels 
of respectful, humble, and polite speeches 
which are frequently used in various social or 
business situations. The mechanism of 
honorifics in Japanese is so complicated that 
recent young generations in Japan could hardly 
master it or use it properly.  
This situation has encouraged the study 
dealing with honorifics in Japanese including 
automatic paraphrasing. For instance, Noguchi 
et al generate all kinds of honorific forms for 
single verbs automatically (Noguchi et al 
2007). In their study, verbs are considered 
exclusively, and hence no contextual 
information has been employed. 
In another study, Tazoe et al have proposed 
a computer model to translate regular 
expressions into respectful speeches (Tazoe et 
al. 2005). They determine the type and level of 
honorifics for a verb in a sentence based on the 
subject of the sentence and the listener level, 
the situation level, and the topic level retrieved 
from the entire article. Comparing with the 
study of Noguchi et al , this one is more 
practical. However, there exist some problems 
in this work. No strategy seems to have been 
prepared in case multiple verbs with different 
agents appear in a same sentence. Another 
problem is the omission of subjects in Japanese 
sentences. This will obstruct the determination 
of the honorific form of the verb. Worst of all, 
the method proposed in this work seems to be 
remaining as a computer model without being 
implemented at all.  
This paper describes a practical system 
developed to transform regular expressions in 
Japanese into honorific forms automatically. 
Specifically, we manage to retrieve the 
hierarchical relationships among characters in 
a sentence so as to determine different 
honorific forms for multiple verbs with 
different agents in the same sentence. Another 
major difference from previous studies is that 
we employ a series of strategies to cope with 
the problem of subject-omission. Here in our 
study, we mainly concentrate our attention on 
the situation of composing business e-mails. 
We first describe the framework of our sys-
tem in Section 2, and then some main modules 
in the following sections. Finally we discuss 
the experiment and conclude this paper in Sec-
tion 7. 
2 Framework of Our System 
Our system contains four main parts: Informa-
tion-retrieval Unit, Subject-complement Unit, 
Honorific-Form-Determination Unit, and Pa-
raphrasing Unit. We illustrate the whole 
framework in Figure 1. 
1
 
Figure 1: Framework of our system 
 
  Before running the system, the user (the per-
son who has composed an e-mail and wants to 
check the honorifics with the system) is rec-
ommended to input the names and the posi-
tions or statuses of both himse lf and the person 
he is going to contact by e-mail, represented as 
the Writer and the Reader in Figure 1. This 
optional function is to help the system make 
more precise judgment on the hierarchical rela-
tions among characters in the e-mail article , 
and hence make more reasonable decision on 
respect type and status level which will be 
used in Honorific-form-determination Unit.  
  The procedure will be repeated until all sen-
tences in the input article are processed. We 
describe the main parts next in section 3, 4, 5, 
and 6 in detail.  
3 Information Retrieval 
Information-retrieval Unit is the first and most 
essential part in our system. We first retrieve 
basic information including parts of speech 
and dependency relations among constituent 
words from a sentence through a free morpho-
logical and dependency parsing software, Ca-
bocha1.  
Then based on the basic information ob-
tained above, the system attempts to extract 
nouns or pronouns representing characters 
                                                 
1 http://chasen.org/~taku/software/cabocha/  
from the sentence, using a Japanese concept 
thesaurus: EDR Concept Dictionary2. The ex-
tracted nouns will be divided into three catego-
ries: first-person group, second-person group, 
and third-person group, by checking them 
against a first-person noun list and a second-
person noun list we have made beforehand. 
The identification results will be used later in 
the Honorific-form-determination Unit.  
Finally, the system assigns a respect type 
and a status level to each character that is ap-
pearing together with nouns showing duty po-
sitions or social statuses. Respect type reflects 
the degree of respect. A larger number indi-
cates a character with higher position, suggest-
ing that a higher honorific form with more re-
gard to the character should be appropriate. 
Status level has a similar nuance with respect 
type. It breaks each respect type down into 
several positions and ranks them according to 
tiny difference among them.  
4 Subject Complement 
The system could not determine which kind of 
honorific form should be applied to a verb with 
the information on characters only. We need to 
know the subject of each verb as well. Gener-
ally, the subject of a predicate is identified 
through the dependency analyzing process. 
However, in case the subject of a verb is omit-
ted, we have to find the subject to help deter-
mine the honorific form as described later in 
Section 5.  
  In our system, we employ five factors to help 
recognize the subject for a verb. In this section, 
we first explain the factors and then describe 
the method for complementing subjects based 
on the five factors. 
4.1 Nonhuman-behavior Verbs 
Our final purpose with this system is to 
transform a verb into an appropriate kind of 
honorific form to show the writer?s regard or 
respect to the reader. Situation will be different 
when the subject of a verb is not a person or 
character. No respect is needed to be paid to a 
thing.  
    In our system, before supplementing the 
subject, we check the verb against the EDR 
                                                 
2 http://www2.nict.go.jp/r/r312/EDR/J_index.html  
2
Dictionary to see whether the verb represents a 
nonhuman-behavior. For example, the 
Japanese verb ?????? meaning boil, will 
never appear with a person as its subject.  In 
this case, the system will not supplement the 
subject for the verb, but leave a check-mark 
here to change the verb into a polite speech 
later in Section 6.  
4.2 Expressions for Estimation 
There are a number of expressions in Japanese 
following verbs and implying estimation or 
hearsay. For example, ????? or ?????? 
indicates possibility but uncertainty. In case 
one of these expressions appears following a 
verb in a subject-omitted sentence, the subject 
of the verb tends to be the second person or the 
third person. We prepare a list containing these 
expressions and supplement the subject as non-
first-person if we find such an expression after 
the verb.  
4.3 Auxiliary Predicates  
Expressions following the predicate and ap-
pearing in the end of a sentence are called as 
auxiliary predicates (Kudo et al 1993). They 
help predicates describe the modality of a sen-
tence, and at the same time contain the infor-
mation on subjects. For instance, ?~(?)??? 
represents the desire of the writer, while ?~??
????? meaning that it is all right (for some-
body) to do (something), indicates that some-
body here should be the second person or the 
third person. 
4.4 Expressions of Internal Feeling 
Internal feeling means the emotion or feeling 
in the back of one's mind, implying that no one 
could understand or represent your feeling ex-
cept yourself. In Japanese, we use adjectives or 
adjective verbs to express internal feelings, and 
the second person or the third person will nev-
er act as the subject of such an adjective or 
adjective verb. Here is an example. ????? 
meaning happy, is a frequently used adjective. 
But different from happy which can be used 
for anybody, ????? in Japanese is used only 
for the first person.  
This fact helps us supplement the subject in 
a sentence with the first-person noun that we 
have extracted in the Information-retrieval 
Unit. We use a Japanese lexicon, Goi-Taikei 
(Ikehara et al 1999) as the data source, and 
employ all the adjectives or adjective verbs in 
the category of state of mind of a person in our 
system. 
4.5 Property of Case 
In most situations, if a character or person 
noun has been used as a surface case with 
some certain particles in a sentence, the cha-
racter will seldom act as other surface cases in 
the same sentence (Isozaki et al 2006). Along 
this idea, we avoid supplementing subjects 
with non-first-person characters or person 
nouns if they have appeared as other surface 
cases. Here the reason we exclude the first-
person characters from applying the rule lies in 
the fact that some first person characters do act 
as multiple surface cases although not that fre-
quently.  
4.6 Subject-complement Procedure 
Our system tries to supplement the subject for 
a verb in a sentence utilizing all the previously 
described factors in a comprehensive manner. 
At first, every rule is checked to see whether it 
is applicable or not. Then we generate a slot 
containing four bits representing nonhuman, 
the first person, the second person, and the 
third person respectively for each rule.  
 
 
Figure 2: An example of subject complement 
 
According to the applying result of each rule, 
each slot is updated with 1 or 0 representing 
possibility and impossibility at the appropriate 
bit. At last, we carry out an And Operation 
3
with all slots and get the final answer. Figure 2 
is an example of subject complement.  
    If we get multiple candidates for the omitted 
subject, we have to determine the final one 
based on the priority order: nonhuman > the 
first person > the second person > the third 
person as shown in Figure 2. We have estab-
lished the above priority order from the result 
of a preliminary experiment.  
Here in this example, the system will sup-
plement the subject of the corresponding verb 
in the sentence with the first-person noun. 
5 Honorific Form Determination 
In this section, we describe the method of de-
termining the honorific forms for verbs. We 
have obtained the respect types, the status le-
vels, and have supplemented the subjects for 
verbs in Information-retrieval Unit and Sub-
ject-complement Unit respectively. Now, the 
system will determine the honorific form for 
each verb according to the following rules 
(R1~R4). Here, the signals sub, nth P, and 
nSL  
indicates the subject, the nth person, and the 
status level of the nth person. 
 
R1. If ((sub = 2nd P) and (
1SL < 2SL  )) or 
((sub = 3rd P) and (
1SL < 3SL  ) and 
(
2SL < 3SL )) 
Then Respectful Speech 
R2. If ((sub = 1st P) and (
1SL < 2SL  )) 
Then Humble Speech 
R3. If ((sub = 1st P) or (sub = 3rd P)) and 
(
1SL < 2SL  ) and ( 3SL < 2SL ) 
Then Teichogo Speech 
R4. Otherwise Polite Speech 
 
The formula 
mSL < nSL  means that the n
th per-
son has a higher position than the mth person.  
6 Paraphrasing 
In accordance with the results of honorific-
form determination, we transform verbs in 
each sentence into their corresponding speech-
es. There are two types of transformation. One 
is with most normal verbs based on general 
paraphrasing rules and the respect levels that 
we have got in Section 3, such as the verb ??
?? meaning work , and ???? meaning write.  
Another transformation is more complicated. 
We have to convert the original verb into some 
particular form first, and then inflect the new 
form according to the same general paraphras-
ing rules as those being used for normal verbs. 
Here is an example. The verb ???? meaning 
go, holds a particular form: ???????? for 
expressing respect, and ???? for expressing 
modesty. 
    Besides, we have added some exception 
processing into our system to cope with indi-
vidual or isolated cases. 
7 Conclusions 
We have conducted a questionnaire to examine 
the practicality of our system. Participants in 
the questionnaires include 5 Japanese college 
students. They are told to evaluate the natural-
ness and correctness of a set of transformed 
articles from our system in 3 levels: 2 for good, 
0 for bad, and 1 for the intermediate level be-
tween good and bad: not good but acceptable. 
The average evaluation result is 1.32 showing 
the effectiveness of our system. We believe 
that the system could be utilized in situations 
of creating business documents or learning 
honorifics in Japanese. 
References 
Ikehara, S., Miyazaki, M., Shirai, S., Yokoo, A., 
Nakaiwa, H., Ogura, K., Ooyama, Y., and Haya-
shi, Y. 1999. Goi-Taikei - A Japanese Lexicon, 
Iwanami Shoten, Tokyo. (in Japanese) 
Isozaki, H., Kazawa, H., and Hirao, T. 2006. Japa-
nese Zero Pronoun Resolution Based on Lexico-
graphical Ordering of Penalties . IPSJ Trans. 
47(7):2279-2294. (in Japanese) 
Kudo, I., and Tomokiyo, M. 1993. An Ellipsis-
Resolution Mechanism by Using Japanese Pre-
dicate Particu larity. IEICE Trans. J76-D-
II(3):624-635. (in Japanese) 
Noguchi, S., Nanjo. H., and Yoshimi, T. 2007. 
Doushi No Tsujohyogen Kara Keigohyogen Eno 
Kangen. Proc. of the 13th Annual Meeting of the 
Association for Natural Language Processing, 
pages 978-981. (in Japanese) 
Tazoe, T., Watanabe, C., Shiino, T. 2005. Devel-
opment of a Computer Model for Translating in 
Respect Language. IPSJ SIG Notes 2005(94):1-6. 
(in Japanese). 
4
Proceedings of the 6th Workshop on Statistical Machine Translation, pages 123?129,
Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational Linguistics
Regression and Ranking based Optimisation for Sentence Level Machine
Translation Evaluation
Xingyi Song and Trevor Cohn
The Department of Computer Science
University of Sheffield
Sheffield, S1 4DP. UK
{xsong2,t.cohn}@shef.ac.uk
Abstract
Automatic evaluation metrics are fundamen-
tally important for Machine Translation, al-
lowing comparison of systems performance
and efficient training. Current evaluation met-
rics fall into two classes: heuristic approaches,
like BLEU, and those using supervised learn-
ing trained on human judgement data. While
many trained metrics provide a better match
against human judgements, this comes at the
cost of including lots of features, leading to
unwieldy, non-portable and slow metrics. In
this paper, we introduce a new trained met-
ric, ROSE, which only uses simple features
that are easy portable and quick to compute.
In addition, ROSE is sentence-based, as op-
posed to document-based, allowing it to be
used in a wider range of settings. Results show
that ROSE performs well on many tasks, such
as ranking system and syntactic constituents,
with results competitive to BLEU. Moreover,
this still holds when ROSE is trained on hu-
man judgements of translations into a different
language compared with that use in testing.
1 Introduction
Human judgements of translation quality are very
expensive. For this reason automatic MT evalu-
ation metrics are used to as an approximation by
comparing predicted translations to human authored
references. An early MT evaluation metric, BLEU
(Papineni et al, 2002), is still the most commonly
used metric in automatic machine translation evalu-
ation. However, several drawbacks have been stated
by many researchers (Chiang et al, 2008a; Callison-
Burch et al, 2006; Banerjee and Lavie, 2005), most
notably that it omits recall (substituting this with a
penalty for overly short output) and not being easily
applied at the sentence level. Later heuristic metrics
such as METEOR (Banerjee and Lavie, 2005) and
TER (Snover et al, 2006) account for both precision
and recall, but their relative weights are difficult to
determine manually.
In contrast to heuristic metrics, trained met-
rics use supervised learning to model directly hu-
man judgements. This allows the combination
of different features and can better fit specific
tasks, such as evaluation focusing more on flu-
ency/adequacy/relative ranks or post editing effort.
Previous work includes approaches using classifica-
tion (Corston-Oliver et al, 2001), regression (Alber-
cht and Hwa, 2008; Specia and Gimenez, 2010), and
ranking (Duh, 2008). Most of which achieved good
results and better correlations with human judg-
ments than heuristic baseline methods.
Overall automatic metrics must find a balance be-
tween several key issues: a) applicability to differ-
ent sized texts (documents vs sentences), b) easy
of portability to different languages, c) runtime re-
quirements and d) correlation with human judge-
ment data. Previous work has typically ignored at
least one of these issues, e.g., BLEU which applies
only to documents (A), trained metrics (Albercht
and Hwa, 2008; Specia and Gimenez, 2010) which
tend to ignore B and C.
This paper presents ROSE, a trained metric which
is loosely based on BLEU, but seeks to further sim-
plify its components such that it can be used for sen-
tence level evaluation. This contrasts with BLEU
which is defined over large documents, and must
123
be coarsely approximated to allow sentence level
application. The increased flexibility of ROSE al-
lows the metric to be used in a wider range of situ-
ations, including during decoding. ROSE is a linear
model with a small number of simple features, and
is trained using regression or ranking against human
judgement data. A benefit of using only simple fea-
tures is that ROSE can be trivially ported between
target languages, and that it can be run very quickly.
Features include precision and recall over different
sized n-grams, and the difference in word counts
between the candidate and the reference sentences,
which is further divided into content word, func-
tion word and punctuation. An extended versions
also includes features over Part of Speech (POS) se-
quences.
The paper is structured as follows: Related work
on metrics for statistical machine translation is de-
scribed in Section 2. Four variations of ROSE and
their features will be introduced in Section 3. In sec-
tion 4 we presents the result, showing how ROSE
correlates well with human judgments on both sys-
tem and sentence levels. Conclusions are given at
the end of the paper.
2 Related Work
The defacto standard metric in machine translation
is BLEU (Papineni et al, 2002). This measures
n-gram precision (n normally equal to 1,2,3,4) be-
tween a document of candidate sentences and a
set of human authored reference documents. The
idea is that high quality translations share many n-
grams with the references. In order to reduce re-
peatedly generating the same word, BLEU clips the
counts of each candidate N-gram to the maximum
counts of that n-gram that in references, and with
a brevity penalty to down-scale the score for out-
put shorter than the reference. In BLEU, each n-
gram precision is given equal weight in geometric
mean, while NIST (Doddington and George, 2002)
extended BLEU by assigning more informative n-
grams higher weight.
However, BLEU and NIST have several draw-
backs, the first being that BLEU uses a geometric
mean over all n-grams which makes BLEU almost
unusable for sentence level evaluations 1. Secondly,
1Note that various approximations exits (Lin and Och, 2004;
BLEU and NIST both use the brevity penalty to re-
place recall, but Banerjee and Lavie (2005) in exper-
iments show that the brevity penalty is a poor sub-
stitute for recall.
Banerjee and Lavie (2005) proposed a METEOR
metric, which that uses recall instead of the BP.
Callison-Burch et al (2007; Callison-Burch et al
(2008) show that METEOR does not perform well in
out of English task. This may because that Stemmer
or WordNet may not available in some languages,
which unable to model synonyms in these cases. In
addition, the performance also varies when adjusting
weights in precision and recall.
Supervised learning approaches have been pro-
posed by many researchers (Corston-Oliver et al,
2001; Duh, 2008; Albercht and Hwa, 2008; Spe-
cia and Gimenez, 2010). Corston-Oliver et al
(2001) use a classification method to measure ma-
chine translation system quality at the sentence level
as being human-like translation (good) or machine
translated (bad). Features extracted from references
and machine translation include heavy linguistic fea-
tures (requires parser).
Quirk (2004) proposed a linear regression model
which is trained to match translation quality. Alber-
cht and Hwa (2008) introduced pseudo-references
when data driven regression does not have enough
training data. Most recently, Specia and Gimenez
(2010) combined confidence estimation (without
reference, just using the source) and reference-based
metrics together in a regression framework to mea-
sure sentence-level machine translation quality.
Duh (2008) compared the ranking with the re-
gression, with the results that with same feature set,
ranking and regression have similar performance,
while ranking can tolerate more training data noise.
3 Model
ROSE is a trained automatic MT evaluation metric
that works on sentence level. It is defined as a linear
model, and its weights will be trained by Support
Vector Machine. It is formulated as
S = ??w ?f(??c ,??r ) (1)
where ??w is the feature weights vector, f(??c ,??r ) is
the feature function which takes candidate transla-
Chiang et al, 2008b)
124
tion (??c ) and reference (??c ), and returns the feature
vector. S is the response variable, measuring the
?goodness? of the candidate translation. A higher
score means a better translation, although the mag-
nitude is not always meaningful.
We present two different method for training:
a linear regression approach ROSE-reg, trained to
match human evaluation score, and a ranking ap-
proach ROSE-rank to match the relative ordering of
pairs of translations assigned by human judge. Un-
like ROSE-reg, ROSE-rank only gives relative score
between sentences, such as A is better than B. The
features that used in ROSE will be listed in section
3.1, and the regression and ranking training are de-
scribed in section 3.2
3.1 ROSE Features
Features used in ROSE listed in Table 1 include
string n-gram matching, Word count and Part of
Speech (POS). String N-gram matching features, are
used for measure how closely of the candidate sen-
tence resembles the reference. Both precision and
recall are considered. Word count features measure
length differences between the candidate and refer-
ence, which is further divided into function words,
punctuation and content words. POS features are
defined over POS n-gram matches between the can-
didate and reference.
3.1.1 String Matching Features
The string matching features include n-gram pre-
cision, n-gram recall and F1-measure. N-gram
precision measures matches between sequence of
words in the candidate sentence compared to the ref-
erences,
Pn =
?
n-gram???c Count(n-gram)Jn-gram ?
??r K
?
n-gram???c Count(ngram)
(2)
where Count are the occurrence counts of n-grams
in the candidate sentence, the numerator measures
the number of predicted n-grams that also occur in
the reference.
Recall is also used in ROSE, so clipping was
deemed unnecessary in precision calculation, where
the repeating words will increasing precision but at
the expense of recall. F-measure is also included,
which is the harmonic mean of precision and recall.
ID Description
1-4 n-gram precision, n=1...4
5-8 n-gram recall, n=1...4
9-12 n-gram f-measure, n=1...4
13 Average n-gram precision
14 Words count
15 Function words count
16 Punctuation count
17 Content words count
18-21 n-gram POS precision, n=1...4
22-25 n-gram POS recall, n=1...4
26-29 n-gram POS f-measure, n=1...4
30-33 n-gram POS string mixed precision,
n=1...4
Table 1: ROSE Features. The first column is the feature
number. The dashed line separates the core features from
the POS extended features.
With there are multiple references, the n-gram preci-
sion error uses the same strategy as BLEU: n-grams
in candidate can match any of the references. For
recall, ROSE will match the n-grams in each refer-
ence separately, and then choose the recall for the
reference with minimum error.
3.1.2 Word Count Features
The word count features measure the length dif-
ference between a candidate sentence and reference
sentence. In a sentence, content words are more in-
formative than function words (grammatical words)
and punctuation. Therefore, the number of content
word candidate is a important indicator in evalua-
tion. In this case, besides measuring the length at
whole sentences, we also measure difference in the
number of function words, punctuation and content
words. We normalise by the length of the refer-
ence which allows comparability between short ver-
sus long sentences. In multiple reference cases we
choose the ratio that is closest to 1.
3.1.3 Part of Speech Features
The string matching features and word count fea-
tures only measure similarities on the lexical level,
but not over sentence structure or synonyms. To add
this capability we also include Part of Speech (POS)
features which work similar to the String Matching
features, but using POS instead of words. The fea-
125
tures measure precision, recall and F-measure over
POS n-grams (n=1...4). In addition, we also include
features that mixed string and POS.
The string/POS mixed feature is used for handling
synonyms. One problem in string n-gram match-
ing is not being able to deal with the synonyms be-
tween the candidate translation and the reference.
One approach for doing so is to use an external re-
source such as WordNet (Banerjee and Lavie, 2005),
however this would limit the portability of the met-
ric. Instead we use POS as a proxy. In most of
the cases, synonyms share the same POS, so this
can be rewarded by forming n-grams over a mix-
ture of tokens and POS. During the matching pro-
cess, both words and its POS shall be considered, if
either matches between reference and candidate, the
n-gram matches will be counted.
Considering the example in table 2, candidate 1
has better translation than candidate 2 and 3. If only
the string N-gram matching is used, that will give the
same score to candidate 1, 2 and 3. The n-gram pre-
cision scores obtained by all candidate sentences in
this example are: 2-gram = 1, 3-gram = 0. However,
we can at least distinguish candidate 1 is better than
candidate 3 if string POS mixed precision is used ,
n-gram precision for candidate 1 will be: 2-gram =
2, 3-gram = 1, which ranks candidate 1 better than
candidate 3.
Example
reference: A/DT red/ADJ vehicle/NN
candidate 1: A/DT red/ADJ car/NN
candidate 2: A/DT red/ADJ rose/NN
candidate 3: A/DT red/ADJ red/ADJ
Table 2: Evaluation Example
3.2 Training
The model was trained on human evaluation data
in two different ways, regression and ranking.These
both used SVM-light (Joachims, 1999). In the rank-
ing model, the training data are candidate translation
and their relative rankings were ranked by human
judge for a given input sentence. The SVM finds
the minimum magnitude weights that are able to cor-
rectly rank training data which is framed as a series
of constraints reflecting all pairwise comparisons. A
soft-margin formulation is used to allow training er-
rors with a penalty (Joachims, 2002). For regres-
sion, the training data is human annotation of post-
edit effort (this will be further described in section
4.1). The Support vector Regression learns weights
with minimum magnitude that limit prediction er-
ror to within an accepted range, again with a soft-
margin formulation (Smola and Schlkopf, 2004).
A linear kernel function will be used, because
non-linear kernels are much slower to use and are
not decomposable. Our experiments showed that the
linear kernel performed at similar accuracy to other
kernel functions (see section 4.2).
4 Experimental Setup
Our experiments test ROSE performance on docu-
ment level with three different Kernel functions: lin-
ear, polynomial and radial basis function. Then we
compare four variants of ROSE with BLEU on both
sentence and system (document) level.
The BLEU version we used here is NIST Open
MT Evaluation tool mteval version 13a, smooth-
ing was disabled and except for the sentence level
evaluation experiment. The system level evalua-
tion procedure follows WMT08 (Callison-Burch et
al., 2008), which ranked each system submitted on
WMT08 in three types of tasks:
? Rank: Human judges candidate sentence rank
in order of quality. On the document level, doc-
uments are ranked according to the proportion
of candidate sentences in a document that are
better than all of the candidates.
? Constituent: The constituent task is the same
as for ranking but operates over chosen syntac-
tic constituents.
? Yes/No: WMT08 Yes/No task is to let human
judge decide whether the particular part of a
sentence is acceptable or not. Document level
Yes/No ranks a document according to their
number of YES sentences
Spearman?s rho correlation was used to measure
the quality of the metrics on system level. Four tar-
get languages (English, German, French and Span-
ish) were used in system level experiments. ROSE-
126
reg and ROSE-rank were tested in all target lan-
guage sets, but ROSE-regpos was only tested in the
into-English set as it requires a POS tagger. On the
sentence level, we compare sentences ranking that
ranked by metrics against human ranking. The eval-
uation quality was examined by Kendall?s tau cor-
relation, and tied results from human judges were
excluded.
Rank es-en fr-en de-en avg
Linear 0.57 0.97 0.69 0.74
Polynomial 0.62 0.97 0.71 0.76
RBF 0.60 0.98 0.62 0.73
Constituent
Linear 0.79 0.90 0.39 0.69
Polynomial 0.80 0.89 0.41 0.70
RBF 0.83 0.93 0.34 0.70
Yes/No
Linear 0.92 0.93 0.67 0.84
Polynomial 0.86 0.90 0.66 0.81
RBF 0.87 0.93 0.65 0.82
Table 3: ROSE-reg in with SVM kernel functions
Metric Kendall?s tau
BLEU-smoothed 0.219
ROSE-reg 0.120
ROSE-regpos 0.164
ROSE-rank 0.206
ROSE-rankpos 0.172
Table 4: Sentence Level Evaluation
4.1 Data
Training data used for ROSE is from WMT10
(Callison-Burch et al, 2010) human judged sen-
tences. A regression model was trained by sentences
with human annotation for post editing effort. The
three levels used in WMT10 are ?OK?, ?EDIT? and
?BAD?, which we treat as response values of 3, 2
and 1. In total 2885 sentences were used in the re-
gression training. The ranking model was trained by
sentences with human annotating sentence ranking,
and tied results are allowed in training. In this exper-
iment, 1675 groups of sentences were used for train-
ing, and each group contains five sentences, which
are manually ranked from 5 (best) to 1 (worst). In or-
der to test the ROSE?s ability to adapt the language
without training data, ROSE was only trained with
English data.
The testing data on sentence level used in this
paper is human ranked sentences from WMT09
(Callison-Burch et al, 2009). Tied rankings were re-
moved, leaving 1702 pairs. We only consider trans-
lations into English sentences. On system level, the
testing data are the submissions for ?test2008? test
set in WMT08 (Callison-Burch et al, 2008). ROSE,
and BLEU were compared with human ranked sub-
mitted system in ?RANK?, ?CONSTITUENT? and
?YES/NO? tasks.
English punctuation and 100 common function
words list of four languages in this experiment were
generated. English POS was tagged by NLTK (Bird
and Loper, 2004).
4.2 Results and Discussion
Table 3 shows the results of ROSE-reg with three
different SVM Kernel functions. Performance are
similar among three different Kernel functions.
However, the linear kernel is the fastest and simplest
and there is no overall winner. Therefore, linear Ker-
nel function was used in ROSE.
The results of Kendall?s tau on sentence level
evaluation are shown in Table 4. According to Ta-
ble 4 ROSE-rank has the highest score in all ver-
sions of ROSE. The score is close to the smoothed
version of BLEU. Results also showed adding POS
feature helped in improving accuracy in the regres-
sion model, but not in ranking, The reason for this is
not clear, but it may be due to over fitting.
Table 5 and Table 6 are the Spearman?s rho in sys-
tem ranking. Table 5 is the task evaluation for trans-
lation into English. ROSE-rank performed the best
in the system ranking task. Also, ROSE-regpos is
the best in the syntactic constituents task. This may
because of ROSE-rank is a ranking based metric and
ROSE-regpos incorporates POS that contains more
linguistic information. Table 6 shows the results of
evaluating translations from English. According to
the table, ROSE performs less accurately than for
the into-English tasks, but overall the ROSE scores
are similar to those of BLEU.
127
Rank es-en fr-en de-en avg
BLEU 0.66 0.97 0.69 0.77
ROSE-reg 0.57 0.97 0.69 0.74
ROSE-rank 0.85 0.96 0.76 0.86
ROSE-regpos 0.59 0.98 0.71 0.76
ROSE-rankpos 0.83 0.96 0.69 0.82
Constituent
BLEU 0.78 0.92 0.30 0.67
ROSE-reg 0.79 0.90 0.39 0.69
ROSE-rank 0.66 0.92 0.33 0.64
ROSE-regpos 0.79 0.90 0.41 0.70
ROSE-rankpos 0.64 0.93 0.31 0.63
Yes/No
BLEU 0.99 0.96 0.66 0.87
ROSE-reg 0.92 0.93 0.67 0.84
ROSE-rank 0.78 0.96 0.61 0.78
ROSE-regpos 0.97 0.93 0.66 0.85
ROSE-rankpos 0.81 0.96 0.57 0.78
Table 5: System Level evaluation that translation into En-
glish
5 Conclusion
We presented the ROSE metric to make up for sev-
eral drawbacks of BLEU and other trained metrics.
Features including string matching, words ratio and
POS were combined by the supervised learning ap-
proach. ROSE?s overall performance was close to
BLEU on system level and sentence level. However,
it is better on tasks ROSE was specifically trained,
such as ROSE-rank in the system level ranking task
and ROSE-regpos in the syntactic constituents task.
Results also showed that when training data is not
available in the right language ROSE produces rea-
sonable results.
Smoothed BLEU slightly outperformed ROSE in
sentence evaluation. This might be due to the train-
ing data not being expert judgments, and conse-
quently very noisy. In further work, we shall mod-
ify the training method to better tolerate noise. In
addition, we will modify ROSE by substitute less
informative features with more informative features
in order to improve its performance and reduce over
fitting.
Rank es-en fr-en de-en avg
BLEU 0.85 0.98 0.88 0.90
ROSE-reg 0.75 0.98 0.93 0.89
ROSE-rank 0.69 0.93 0.94 0.85
Constituent
BLEU 0.83 0.87 0.35 0.68
ROSE-reg 0.73 0.87 0.36 0.65
ROSE-rank 0.72 0.78 0.32 0.61
Yes/No
BLEU 0.75 0.97 0.89 0.87
ROSE-reg 0.72 0.97 0.93 0.87
ROSE-rank 0.82 0.96 0.87 0.88
Table 6: System Level evaluation that translation from
English
References
Josha S. Albercht and Rebecca Hwa. 2008. Regres-
sion for machine translation evaluation at the sentence
level. Machine Translation, 22:1?27.
Satanjeev Banerjee and Alon Lavie. 2005. Meteor: An
automatic metric for mt evaluation with improved cor-
relation with human judgments. Proceedings of the
ACL-05 Workshop.
Steven Bird and Edward Loper. 2004. Nltk: The natural
language toolkit. In Proceedings of the ACL demon-
stration session, pages 214?217, Barcelona, July.
Chris Callison-Burch, Miles Osborne, and Philipp
Koehn. 2006. Re-evaluating the role of bleu in ma-
chine translation research. In In EACL, pages 249?
256.
Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,
Christof Monz, and Josh Schroeder. 2007. (meta-)
evaluation of machine translation. In Proceedings of
the Second Workshop on Statistical Machine Transla-
tion, pages 136?158, Prague, Czech Republic, June.
Association for Computational Linguistics.
Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,
Christof Monz, and Josh Schroeder. 2008. Further
meta-evaluation of machine translation. In Proceed-
ings of the Third Workshop on Statistical Machine
Translation, pages 70?106, Columbus, Ohio, June.
Association for Computational Linguistics.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the 2009
Workshop on Statistical Machine Translation. In
Proceedings of the Fourth Workshop on Statistical
Machine Translation, pages 1?28, Athens, Greece,
March. Association for Computational Linguistics.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Kay Peterson, Mark Przybocki, and Omar Zaidan.
128
2010. Findings of the 2010 joint workshop on sta-
tistical machine translation and metrics for machine
translation. In Proceedings of the Joint Fifth Workshop
on Statistical Machine Translation and MetricsMATR,
pages 17?53, Uppsala, Sweden, July. Association for
Computational Linguistics. Revised August 2010.
David Chiang, Steve DeNeefe, Yee Seng Chan, and
Hwee Tou Ng. 2008a. Decomposability of trans-
lation metrics for improved evaluation and efficient
algorithms. In Proceedings of the Conference on
Empirical Methods in Natural Language Processing,
EMNLP ?08, pages 610?619, Stroudsburg, PA, USA.
Association for Computational Linguistics.
David Chiang, Yuval Marton, and Philip Resnik. 2008b.
Online large-margin training of syntactic and struc-
tural translation features. In Proceedings of the Con-
ference on Empirical Methods in Natural Language
Processing, EMNLP ?08, pages 224?233, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Simon Corston-Oliver, Michael Gamon, and Chris
Brockett. 2001. A machine learning approach to the
automatic evaluation of machine translation. In pro-
ceedings of the Association for Computational Lin-
guistics.
Doddington and George. 2002. Automatic evalua-
tion of machine translation quality using n-gram co-
occurrence statistics. In Proceedings of the second
international conference on Human Language Tech-
nology Research, HLT ?02, pages 138?145, San Fran-
cisco, CA, USA. Morgan Kaufmann Publishers Inc.
Kevin Duh. 2008. Ranking vs. regression in machine
translation evaluation. In In Proceedings of the Third
Workshop on Statistical Machine Translation, pages
191?194, Columbus,Ohio,, June.
T. Joachims. 1999. Making large-scale svm learning
practical. Advances in Kernel Methods - Support Vec-
tor Learning,.
T. Joachims. 2002. Optimizing search engines using
clickthrough data. In Proceedings of the ACM Con-
ference on Knowledge Discovery and Data Mining
(KDD).
Chin-Yew Lin and Franz Josef Och. 2004. Orange: a
method for evaluating automatic evaluation metrics for
machine translation. In Proceedings of the 20th in-
ternational conference on Computational Linguistics,
COLING ?04, Stroudsburg, PA, USA. Association for
Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of the
40th Annual Meeting on Association for Computa-
tional Linguistics, ACL ?02, pages 311?318, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
C Quirk. 2004. Training a sentence-level machine trans-
lation confidence measure. In In: Proceedings of the
international conference on language resources and
evaluation, pages 825?828, Lisbon, Portugal.
Alex J. Smola and Bernhard Schlkopf. 2004. A tuto-
rial on support vector regression. STATISTICS AND
COMPUTING, 14:199?222.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea
Micciulla, and Ralph Weischedel. 2006. A study of
translation error rate with targeted human annotation.
L. Specia and J. Gimenez. 2010. Combining confidence
estimation and reference-based metrics for segment-
level mt evaluation. In The Ninth Conference of the
Association for Machine Translation in the Americas,
Denver,Colorado.
129
