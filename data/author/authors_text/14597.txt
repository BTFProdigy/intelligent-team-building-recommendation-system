Proceedings of the NAACL HLT 2010 Student Research Workshop, pages 29?33,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Distinguishing Use and Mention in Natural Language 
 
 
Shomir Wilson 
Computer Science 
University of Maryland 
College Park, MD 20742, USA 
shomir@umd.edu 
 
 
 
 
 
 
Abstract 
When humans communicate via natural lan-
guage, they frequently make use of metalan-
guage to clarify what they mean and promote 
a felicitous exchange of ideas. One key aspect 
of metalanguage is the mention of words and 
phrases, as distinguished from their use. This 
paper presents ongoing work on identifying 
and categorizing instances of language-
mention, with the goal of building a system 
capable of automatic recognition of the phe-
nomenon. A definition of language-mention 
and a corpus of instances gathered from Wiki-
pedia are discussed, and the future direction of 
the project is described. 
1 Introduction 
Costello: Well then who's on first? 
Abbott: Yes. 
Costello: I mean the fellow's name. 
Abbott: Who. 
Costello: The guy on first. 
Abbott: Who. 
 
 In Abbott and Costello?s famous routine 
?Who?s on First??, Costello asks Abbott for the 
names of the players on a baseball team. In the 
above excerpt1, Abbott tries unsuccessfully to ex-
plain that the name of the first baseman is Who, but 
Costello interprets this as another question instead 
                                                          
1
 Quoted from http://www.phoenix5.org/ 
humor/WhoOnFirst.html. 
of a response to his own. If Abbott had been more 
explicit and less terse (by responding with ?The 
fellow?s name is the word ?Who??, for instance), 
he would have avoided the ambiguity in his an-
swers and might have succeeded in conveying to 
Costello the name of the first baseman. Instead, 
this misunderstanding is repeated throughout the 
routine with comic results, as the two become in-
creasingly agitated by their breakdown in commu-
nication. 
 As Abbott and Costello unwittingly demon-
strated, we sometimes must refer to the language 
that we speak and write in order to understand lan-
guage itself. Metalanguage is our facility for doing 
this, and its interpretation often implicitly relies on 
the use-mention distinction?that is, simply, the 
distinction between using elements of language 
and mentioning them. In both written and spoken 
communication, the mention of letters, sounds, 
words, phrases, or entire sentences (termed lan-
guage-mention in this paper for brevity) is essen-
tial for indicating titles, explaining meaning, 
introducing new words, attributing exact utterances 
to others, and other common functions of metalan-
guage (Saka 2005). There is evidence that human 
conversation makes frequent use of the use-
mention distinction, and that we would be severely 
handicapped without it (Perlis et al, 1998). More-
over, this distinction has close ties to the appear-
ance-reality distinction in cognitive science (Miller 
1993). 
 It is surprising, then, that the use-mention dis-
tinction has thus far received little attention in 
computational linguistics. The need for greater 
study is unmistakable, as human audiences gener-
29
ally navigate through this linguistic phenomenon 
with a natural ease that computers do not possess. 
The complexity behind this natural ease is apparent 
in our ability to understand simple sentences such 
as ?Sky is spelled S K Y? and ?The letters S, K, 
and Y make the word sky?, which mean essentially 
the same thing but are structured and worded very 
differently. To gain the benefits of understanding 
the use-mention distinction, natural language 
processing systems must detect the subtle cues that 
signal this phenomenon. 
This paper presents some preliminary results 
from a project on characterizing and identifying 
instances of language-mention in the English lan-
guage. The use-mention distinction is first ex-
plained in greater detail, and a working definition 
is proposed for the phenomenon of language-
mention. A corpus of instances of language-
mention from Wikipedia is then introduced, with 
analysis of the categories in which the phenome-
non appears to occur. The hypothesis of this con-
tinuing project is that lexical and syntactic cues 
will be sufficient to automatically identify at least a 
large subset of instances of mentioned language. 
2 The Use-Mention Distinction  
The use-mention distinction, as previously stated, 
is the distinction between using linguistic entities 
(such as letters, sounds, words, phrases, or entire 
sentences) and mentioning them. Since this expla-
nation is slightly opaque at best and possibly circu-
lar, some examples and a proposal for a definition 
are appropriate. Consider the following sentences: 
(1) The cat is on the mat. 
(2) The word ?cat? is spelled with three letters. 
In (1), the reader?s attention to meaning does not 
focus on the words themselves, but instead upon 
the presumed cat on the mat. In (2), the reader un-
derstands that the word cat?a string of three let-
ters, as opposed to any particular cat or an abstract 
idea of a cat?is in the focus of the sentence. Quo-
tation marks around cat in (2) are a convention to 
further reinforce that the word is being mentioned, 
and in some contexts (such as this sentence) italics 
may serve the same purpose. 
 The other linguistic entities listed above can also 
be mentioned, and the reader may easily conjure 
appropriate examples. Of particular note is quota-
tion, a form of language-mention in which lan-
guage from another source is reproduced as part of 
a statement, as in (3) below: 
(3) Eric said, ?We should meet for lunch.? 
In (3), the phrase between quote marks is men-
tioned as what Eric has said. However, the reader 
is likely to treat the quoted text in (3) as a string 
with semantic depth, indicating that the use half of 
the use-mention distinction is present as well. Ex-
amples such as this illustrate that use and mention 
are not mutually exclusive (Maier 2007). 
 If writers always and consistently used cues 
such as quotation marks and italics, and if speakers 
followed a convention for delimiting mentioned 
utterances2, recognizing language-mention would 
be an easier task. However, it frequently falls upon 
the intuition of the audience to determine when, 
where, and how it occurs (Anderson et al 2002). 
Sentences (2) and (3) above, if typed less formally 
(sans quotation marks) or transcribed from speech, 
would still be easily understood by a human read-
er. Moreover, cues such as italics and quotation 
marks are also used for other purposes, such as 
distancing (?scare quotes?) and emphasis, meaning 
that they are uncertain indicators of language-
mention. It seems that subtler cues are responsible 
for our ability to distinguish use and mention. 
 In spite of the ubiquity of the phrase use-
mention distinction, it is difficult to find an explicit 
definition for either the distinction itself or its two 
halves. The effort here will be to define language-
mention, since this will aid in identifying where 
and how it occurs. What follows is a working defi-
nition, in the sense that it is a ?rough draft?; sug-
gestions for improvement are invited. For the 
moment, it restricts the scope of this work to sen-
tential language-mention, where the mentioned 
linguistic entity is referred to inside of the same 
sentence that it occurs. (An example of a sentence 
that fails this additional requirement is: ?Disregard 
the last thing I said.?) This restriction is necessary 
to reduce the complexity of the identification and 
labeling problems, and it will be assumed for the 
rest of the paper. 
Definition: For T a token or a set of tokens in a 
sentence, if T refers to a property of the token T or 
the type of T, then T is an instance of language-
mention. 
                                                          
2
 One might observe that spoken language sometimes contains 
nonverbal cues for language-mention. While worthy of study, 
these cues fall beyond the scope of this paper, which will fo-
cus on written or transcribed language. 
30
Here, a token can be any one of the linguistic enti-
ties listed at the beginning of this section?letters, 
sounds, words, phrases, or entire sentences.  A 
property might be its spelling, pronunciation, orig-
inal source (in the case of quotation), meaning (for 
a variety of interpretations of that term), or another 
aspect for which language is shown or demonstrat-
ed3. The type of T is relevant in some instances of 
language-mention (such as in (2)) and the token 
itself is relevant in others (including unusual cases 
such as ?The is the first word in this sentence?). 
3 A Language-Mention Corpus 
The second task of this project has been to create a 
corpus of sentences that contain instances of lan-
guage-mention. The corpus will be valuable to 
move beyond laboratory examples and to begin 
mining for patterns in syntax and vocabulary that 
predict the occurrence of the phenomenon. 
 Wikipedia was chosen as a source of text for 
several reasons. Its text is freely available and cov-
ers a wide variety of subjects. Articles are written 
to be informative, which suggests that new names 
and terms are introduced frequently?a common 
function of language-mention. Contributors tend to 
highlight language-mention with italicization, bold 
text, or quotation marks. (This convention is men-
tioned in the Wikipedia Manual of Style, though it 
is unclear whether most contributors read it there 
or simply follow it out of habit.) While language-
mention can certainly occur outside of those stylis-
tic cues, the decision was made to concentrate on 
sentences that contained them, since this greatly 
accelerated the annotation process. 
 The annotation effort focused on the markup 
text of 1000 randomly chosen articles from English 
Wikipedia. Except for delimiters for bold and italic 
text, most of the markup was removed, and the 
remaining text was segmented into sentences using 
NLTK?s implementation of the Punkt sentence 
tokenizer (Kiss and Strunk, 2006). The sentences 
then were filtered for only those that contained 
bold text, italic text, or text between single or 
double quotation marks, yielding a set of 1339 sen-
tences that contained one or more of them. 
 Hand annotation required approximately three 
person-hours, with that time heavily skewed to-
ward approximately the first third of the sentences, 
                                                          
3
 These properties are based upon the ostentions of language 
in Paul Saka?s treatment of the use-mention distinction (1998). 
as the set of categories for language-mention was 
also developed during this labeling process. Cate-
gories were formed with an informal "diagnostic 
test" of substitution of the category's theme (e.g., 
"this proper name", "this translation", "this sym-
bol", "this quotation") in the place of the candidate 
token or tokens. Only text highlighted by one of 
the cues mentioned above was considered for labe-
ling. Although only one researcher participated in 
the annotation, at the time of writing this paper an 
effort was in progress to build a much larger cor-
pus using multiple annotators via Amazon?s Me-
chanical Turk service. This service has shown 
promise in other natural language annotation tasks 
(Snow et al, 2008). 
 Out of the 1339 sentences inspected by hand, 
171 contained at least one instance of language-
mention. Many of those sentences contained sever-
al instances. Table 1 below lists the categories ob-
served and the frequencies of each one, and Table 
2 provides examples from each category. 
 
Language-Mention Category Frequency 
Proper name (PN) 
Translation or Transliteration (TR) 
Attributed Language (AT) 
Words/Phrases as Themselves (WD) 
Symbols/Nonliteral Marks (SY) 
Phonetic/Sound (PH) 
Spelling (SP) 
Abbreviation (AB) 
119 
61 
47 
46 
8 
2 
2 
1 
 
Table 1: Frequencies of the different categories of lan-
guage-mention found in the corpus. 
 
Cat. Example 
PN In 2005, Ashley Page created another short 
piece on Scottish Ballet, a strikingly mod-
ern piece called ''The Pump Room'', set to 
pulsating music by Aphex Twin. 
TR The Latin title translates as ''a method for 
finding curved lines enjoying properties of 
maximum or minimum, or solution of iso-
perimetric problems in the broadest ac-
cepted sense''. 
AT "It is still fresh in my memory that I read a 
chess book of Karpov by chance in 1985 
which I liked very much," the 21-year-old 
said. 
WD '''Submerged forest''' is a term used to de-
scribe the remains of trees (especially tree 
31
stumps) which have been submerged by 
marine transgression, i.e. sea level rise. 
SY He also introduced the modern notation 
for the trigonometric functions, the letter 
''e'' for the base of the natural logarithm 
(now also known as Euler's number) ? 
PH The call of this species is a high pitched 
''ke-ke-ke'' like American Kestrel. 
SP '''James Breckenridge Speed''' (middle 
name sometimes spelled '''Breckinridge''') 
(1844-1912) was a successful businessman 
in Louisville, Kentucky and an important 
philanthropist. 
AB ? ''Moskovskiy gosudarstvennyy univer-
sitet putej soobshcheniya'', often abbre-
viated '''MIIT''' for '''Moscow Institute of 
Transport Engineers''' ? 
 
Table 2: Examples from the corpus of each category of 
language-mention. Triple quote marks indicate bold text 
in the original markup. The longer sentences for SY and 
AB have been truncated. The relevant instance of lan-
guage-mention in each example appears underlined. 
 
As shown, proper names were by far the most 
common category, with almost twice as many in-
stances as the next most frequent category.  This 
follows intuition, since Wikipedia articles often 
describe entities identified by proper names. In 
contrast, there were just a few instances of pronun-
ciation (phonetic/sound) or spelling. Either the pre-
filtering of sentences eliminated many instances of 
these before human annotation could find them, or 
Wikipedia is not a fertile source for them. 
Of particular note are the 46 instances of words 
or phrases as themselves, since these are examples 
of language being either introduced or clarified for 
the reader. While there exists a body of work on 
named entity recognition (Nadeau and Sekine, 
2007), very little exists on identifying when words 
serve a very similar function, essentially as rigid 
designators for their types.  One of the future goals 
of this project will be to fill that gap. 
4 Related Work 
A similar corpus-building project was undertaken 
by Anderson, et. al (2004) to study the occurrence 
of metalanguage in human dialogue. In addition to 
the difference in focus (metalanguage broadly ver-
sus language-mention in particular), their project 
concentrated on the classification of utterances 
from casual speech, as opposed to the structure of 
well-formed sentences. The automatic recognition 
of language-mention will require a specific focus 
on the phenomenon, since it differs from other 
forms of metalanguage in its unusual syntactic 
structure (as shown in the next section). 
 In applications, the use-mention distinction has 
also received some treatment within dialog man-
agement and commonsense reasoning, as imple-
mented in the ALFRED system (Josyula et al, 
2003). However, its ability to recognize language-
mention is limited to the task of learning new 
words from a limited set of sentence structures. 
The ongoing project described in this paper instead 
has the goal of recognizing and eventually inter-
preting language-mention in a wide variety of nat-
ural cases. 
5 Future Work 
The next step in this project will be to enlarge the 
language-mention corpus, using more data from 
Wikipedia and other promising sources. Language 
learning materials have also been considered for 
this purpose, as they necessarily contain a high 
frequency of metalanguage. The presence of stylis-
tic cues in the text will be useful but perhaps not 
essential, as it is anticipated that bootstrapping the 
annotation process will become possible once 
enough indicators in sentence structure and voca-
bulary have been identified. This identification will 
be accomplished through a combination of eyebal-
ling of patterns in parse trees and automated 
searching through the corpus using a tool such as 
Tregex (Levy and Andrew, 2006). 
 One eventual goal of this project is to detect 
language-mention without the presence of stylistic 
cues, with the intent of correcting egregious errors 
common in syntactic parsing of the phenomenon. 
Statistically-trained parsers have achieved great 
levels of accuracy at the macro level of examining 
large quantities of text, but this comes at a cost. 
Such accuracy tends not to include the phenome-
non of language-mention, which often has an un-
usual structure. Consider the following two 
sentences paired with the resulting output from the 
Stanford Parser (Klein and Manning 2003): 
(4a) Car is spelled c a r 
32
(4b) (ROOT (S (NP (NNP Car)) (VP (VBZ is) 
(VP (VBN spelled) (S (NP (SYM c)) (NP (DT 
a) (NN r))))))) 
(5a) The pronunciation of potato is pough tayh 
toe 
(5b) (ROOT (S (NP (NP (DT The) (NN pro-
nunciation)) (PP (IN of) (NP (NN potato)))) 
(VP (VBZ is) (NP (JJ pough) (NN tayh) (NN 
toe))))) 
Both of these sentences are easily interpretable 
by a human audience, but the parser garbles their 
structure where language-mention occurs. Such 
unusual structure and vocabulary are likely not to 
lend well to the methods used to train such a pars-
er. Because of this, the feasibility of a ?hybrid? 
system is being investigated, which would com-
bine an existing high-performance parser with a 
rule-based system to modify and correct its output 
where appropriate. 
Preliminary work on a language-mention parser 
has shown the feasibility of this hybrid approach. 
A trial system has been built that uses parse trees 
produced by the Stanford Parser as input to five 
rules that detect common syntactic patterns indicat-
ing the phenomenon occurs in a sentence. In (4a), 
for instance, the presence of the verb spell and the 
sequence of two or more single-letter words indi-
cates that the sequence is likely an instance of lan-
guage-mention and falls into the category of 
spelling. Although language-mention exhibits sub-
stantial variety in its forms (and certainly will not 
be conquered by the five rules in the trial system), 
this approach should be able to take advantage of 
additional patterns mined from the corpus of the 
phenomenon currently being created.  It is hy-
pothesized that such a parser, using lexical and 
syntactic cues, will be sufficient to identify and 
categorize a large percentage of instances of lan-
guage-mention in the absence of any stylistic cues. 
References  
Anderson, Michael L., Andrew Fister, Bryant Lee, and 
Danny Wang. 2004. On the frequency and types of 
meta-language in conversation: a preliminary report.  
Paper presented at the 14th Annual Conference of the 
Society for Text and Discourse. 
Anderson, Michael L., Yoshi Okamoto, Darsana Josyu-
la, and Don Perlis. 2002. The use-mention distinction 
and its importance to HCI. In Proceedings of the 
Sixth Workshop on the Semantics and Pragmatics of 
Dialog. 
Josyula, Darsana, Mike Anderson, and Don Perlis. 
2003. Towards domain-independent, task-oriented, 
conversational adequacy. In Proceedings of IJCAI-
2003 Intelligent Systems Demonstrations. 
Kiss, Tibor and Jan Strunk. 2006. Unsupervised multi-
lingual sentence boundary detection. Computational 
Linguistics, 32(4): 485-525. 
Klein, Dan and Christopher Manning. 2003. Accurate 
Unlexicalized Parsing. In Proceedings of the 41st 
Meeting of the Association for Computational Lin-
guistics.  
Levy, Roger and Galen Andrew. 2006. Tregex and 
Tsurgeon: tools for querying and manipulating tree 
data structures. In Proceedings of the 8th Interna-
tional Conference on Knowledge-Based Intelligent 
Information and Engineering Systems. 
Maier, Emar. 2007. Mixed quotation: between use and 
mention. In Proceedings of LENLS2007, Miyazaki, 
Japan. 
Miller, Michael. 1993. A view of one?s past and other 
aspects of reasoned change in belief. Ph.D. thesis, 
University of Maryland, College Park, Maryland. 
Nadeau, David and Satoshi Sekine. 2007. A survey of 
named entity recognition and classification. Linguis-
ticae Investigationes, 30(1):3?26. 
Perlis, Don, Khemdut Purang, and Carl Andersen. 1998. 
Conversational adequacy: mistakes are the essence. 
International Journal of Human-Computer Studies, 
48:553-575. 
Saka, Paul. 1998. Quotation and the use-mention dis-
tinction. Mind, 107(425):113?135. 
Saka, Paul. 2005. Quotational constructions. Belgian 
Journal of Linguistics, 17(1):187?212. 
Snow, Rion, Brendan O?Connor, Daniel Jurafsky, and 
Andrew Y. Ng. Cheap and fast?but is it good? Eva-
luating non-expert annotations for natural language 
tasks. 2008. In Proceedings of the Conference on 
Empirical Methods in Natural Language Processing. 
Honolulu, Hawaii. 
33
Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 638?646,
Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational Linguistics
The Creation of a Corpus of English Metalanguage 
 
Shomir Wilson* 
Carnegie Mellon University 
Pittsburgh, PA 15213, USA 
shomir@cs.cmu.edu 
 
 
 
 
 
 
Abstract 
Metalanguage is an essential linguistic 
mechanism which allows us to communicate 
explicit information about language itself. 
However, it has been underexamined in 
research in language technologies, to the 
detriment of the performance of systems that 
could exploit it. This paper describes the 
creation of the first tagged and delineated 
corpus of English metalanguage, accompanied 
by an explicit definition and a rubric for 
identifying the phenomenon in text. This 
resource will provide a basis for further studies 
of metalanguage and enable its utilization in 
language technologies. 
1 Introduction 
In order to understand the language that we speak, 
we sometimes must refer to the language itself. 
Language users do this through an understanding 
of the use-mention distinction, as exhibited by the 
mechanism of metalanguage: that is, language that 
describes language. The use-mention distinction is 
illustrated simply in Sentences (1) and (2) below: 
(1) I watch football on weekends. 
(2) Football may refer to one of several sports. 
A reader understands that football in Sentence (1) 
refers to a sporting activity, while the same word in 
Sentence (2) refers to the term football itself. 
Evidence suggests that human communication 
frequently employs metalanguage (Anderson et al 
2002), and the phenomenon is essential for many 
activities, including the introduction of new 
                                                          
* This research was performed during a prior affiliation with 
the University of Maryland at College Park. 
vocabulary, attribution of statements, explanation 
of meaning, and assignment of names (Saka 2003). 
Sentences (3) through (8) below further illustrate 
the phenomenon, highlighted in bold. 
(3) This is sometimes called tough love. 
(4) I wrote ?meet outside? on the chalkboard. 
(5) Has is a conjugation of the verb have. 
(6) The button labeled go was illuminated. 
(7) That bus, was its name 61C? 
(8) Mississippi is fun to spell. 
Recognizing a wide variety of metalinguistic 
constructions is a skill that humans take for granted 
in fellow interlocutors (Perlis, Purang & Andersen 
1998), and it is a core language skill that children 
demonstrate at an early age (Clark & Schaefer 
1989). Regardless of context, topic, or mode of 
communication (spoken or written), we are able to 
refer directly to language, and we expect others to 
recognize and understand when we do so. 
The study of the syntax and semantics of 
metalanguage is well developed for formal 
languages. However, the study of the phenomenon 
in natural language is relatively nascent, and its 
incorporation into language technologies is almost 
non-existent. Parsing the distinction is difficult, as 
shown in Figure 1 below: go does not function as a 
verb in Sentence (6), but it is tagged as such. 
Delineating an instance of metalanguage with 
quotation marks is a common convention, but this 
often fails to ameliorate the parsing problem. 
Quotation marks, italic text, and bold text?three 
common conventions used to highlight 
metalanguage?are inconsistently applied and are 
already ?overloaded? with several distinct uses. 
Moreover, applications of natural language 
processing generally lack the ability to recognize 
and interpret metalanguage (Anderson et al 2002). 
638
Systems using sentiment analysis are affected, as 
sentiment-suggestive terms appearing in 
metalanguage (especially in quotation, a form of 
the phenomenon (Maier 2007)) are not necessarily 
reflective of the writer or speaker. Applications of 
natural language understanding cannot process 
metalanguage without detecting it, especially when 
upstream components (such as parsers) mangle its 
structure. Interactive systems that could leverage 
users? expectations of metalanguage competency 
currently fail to do so. Figure 2 below shows a 
fragment of conversation with the Let?s Go! (Raux 
et al 2005) spoken dialog system, designed to help 
users plan trips on Pittsburgh?s bus system. 
 
(ROOT 
  (S 
    (NP 
      (NP (DT The) (NN button)) 
      (VP (VBN labeled) 
        (S 
          (VP (VB go))))) 
    (VP (VBD was) 
      (VP (VBN illuminated))) 
    (. .))) 
 
Figure 1. Output of the Stanford Parser (Klein & 
Manning 2003) for Sentence (6). Adding quotation 
marks around go alters the parser output slightly 
(not shown), but go remains labeled VB. 
 
Let?s Go!: Where do you wish to depart 
from? 
User: Arlington. 
Let?s Go!: Departing from Allegheny 
West. Is this right? 
User: No, I said ?Arlington?. 
Let?s Go!: Please say where you are 
leaving from. 
 
Figure 2: A conversation with Let?s Go! in which 
the user responds to a speech recognition error. 
 
The exchange shown in Figure 2 is 
representative of the reactions of nearly all dialog 
systems: in spite of the domain generality of 
metalanguage and the user?s expectation of its 
availability, the system does not recognize it and 
instead ?talks past? the user. In effect, language 
technologies that ignore metalanguage are 
discarding the most direct source of linguistic 
information that text or utterances can provide. 
This paper describes the first substantial study to 
characterize and gather instances of English 
metalanguage. Section 2 presents a definition and a 
rubric for metalanguage in the form of mentioned 
language. Section 3 describes the procedure used 
to create the corpus and some notable properties of 
its contents, and Section 4 discusses insights 
gained into the phenomenon. The remaining 
sections discuss the context of these results and 
future directions for this research. 
2 Metalanguage and the Use-Mention 
Distinction1 
Although the reader is likely to be familiar with the 
terms use-mention distinction and metalanguage, 
the topic merits further explanation to precisely 
establish the phenomenon being studied. 
Intuitively, the vast majority of utterances are 
produced for use rather than mention, as the roles 
of language-mention are auxiliary (albeit 
indispensible) to language use. This paper will 
adopt the term mentioned language to describe the 
literal, delineable phenomenon illustrated in 
examples thus far. Other forms of metalanguage 
occur through deictic references to linguistic 
entities that do not appear in the relevant statement. 
(For example, consider ?That word was 
misspelled? where the referred-to word resides 
outside of the sentence.) For technical tractability, 
this study focuses on mentioned language. 
2.1 Definition 
Although the use-mention distinction has enjoyed a 
long history of theoretical discussion, attempts to 
explicitly define one or both of the distinction?s 
disjuncts are difficult (or impossible) to find. 
Below is the definition of mentioned language 
adopted by this study, followed by clarifications. 
Definition: For T a token or a set of tokens in a 
sentence, if T is produced to draw attention to a 
property of the token T or the type of T, then T is 
an instance of mentioned language. 
Here, a token is the specific, situated (i.e., as 
appearing in the sentence) instantiation of a 
linguistic entity: a letter, symbol, sound, word, 
phrase, or another related entity. A property might 
                                                          
1  The definition and rubric in this section were originally 
introduced by Wilson (2011a). For brevity, their full 
justifications and the argument for equivalence between the 
two are not reproduced here. 
639
be a token?s spelling, pronunciation, meaning (for 
a variety of interpretations of meaning), structure, 
connotation, original source (in cases of quotation), 
or another aspect for which language is shown or 
demonstrated. The type of T is relevant in most 
instances of mentioned language, but the token 
itself is relevant in others, as in the sentence below: 
(9) ?The? appears between quote marks here. 
Constructions like (9) are unusual and are of 
limited practical value, but the definition 
accommodates them for completeness. 
The adoption of this definition was motivated by 
a desire to study mentioned language with precise, 
repeatable results. However, it was too abstract to 
consistently apply to large quantities of candidate 
phrases in sentences, a necessity for corpus 
creation. A brief attempt to train annotators using 
the definition was unsuccessful, and instead a 
rubric was created for this purpose. 
2.2 Annotation Rubric 
A human reader with some knowledge of the use-
mention distinction can often intuit the presence of 
mentioned language in a sentence. However, to 
operationalize the concept and move toward corpus 
construction, it was necessary to create a rubric for 
labeling it. The rubric is based on substitution, and 
it may be applied, with caveats described below, to 
determine whether a linguistic entity is mentioned 
by the sentence in which it occurs. 
Rubric: Suppose X is a linguistic entity in a 
sentence S. Construct sentence S' as follows: 
replace X in S with a phrase X' of the form "that 
[item]", where [item] is the appropriate term for X 
in the context of S (e.g., "letter", "symbol", "word", 
"name", "phrase", "sentence", etc.). X is an 
instance of mentioned language if, when assuming 
that X' refers to X, the meaning of S' is equivalent 
to the meaning of S. 
To further operationalize the rubric, Figure 3 
shows it rewritten in pseudocode form. To verify 
the rubric, the reader can follow a positive example 
and a negative example in Figure 4. 
To maintain coherency, minor adjustments in 
sentence wording will be necessary for some 
candidate phrases. For instance, Sentence (10) 
below must be rewritten as (11): 
(10) The word cat is spelled with three letters. 
(11) Cat is spelled with three letters. 
This is because S? for (10) and (11) are 
respectively (12) and (13): 
(12) The word that word is spelled with three  
        letters. 
(13) That word is spelled with three letters. 
 
Given S a sentence and X a copy of a 
linguistic entity in S: 
(1) Create X': the phrase ?that [item]?, 
where [item] is the appropriate term 
for linguistic entity X in the 
context of S. 
(2) Create S': copy S and replace the 
occurrence of X with X'. 
(3) Create W: the set of truth 
conditions of S. 
(4) Create W': the set of truth 
conditions of S', assuming that X' 
in S' is understood to refer 
deictically to X. 
(5) Compare W and W'. If they are equal, 
X is mentioned language in S. Else, 
X is not mentioned language in S. 
 
Figure 3: Pseudocode equivalent of the rubric. 
 
Positive Example 
S: Spain is the name of a European 
country. 
X: Spain. 
(1) X': that name 
(2) S': That name is the name of a 
European country. 
(3) W: Stated briefly, Spain is the name 
of a European country. 
(4) W': Stated briefly, Spain is the 
name of a European country. 
(5) W and W' are equal. Spain is 
mentioned language in S. 
 
Negative Example 
S: Spain is a European country. 
X: Spain. 
(1) X': that name 
(2) S': That name is a European country. 
(3) W: Stated briefly, Spain is a 
European country. 
(4) W': Stated briefly, the name Spain 
is a European country. 
(5) W and W' are not equal. Spain is not 
mentioned language in S. 
 
Figure 4: Examples of rubric application using the 
pseudocode in Figure 3. 
 
Also, quotation marks around or inside of a 
candidate phrase require special attention, since 
their inclusion or exclusion in X can alter the 
meaning of S?. For this discussion, quotation marks 
640
and other stylistic cues are considered informal 
cues which aid a reader in detecting mentioned 
language. Style conventions may call for them, and 
in some cases they might be strictly necessary, but 
a competent language user possesses sufficient 
skill to properly discard or retain them as each 
instance requires (Saka 1998). 
3 The Mentioned Language Corpus 
?Laboratory examples? of mentioned language 
(such as the examples thus far in this paper) only 
begin to illustrate the variation in the phenomenon. 
To conduct an empirical examination of mentioned 
language and to study the feasibility of automatic 
identification, it was necessary to gather a large, 
diverse set of samples. This section describes the 
process of building a series of three progressively 
more sophisticated corpora of mentioned language. 
The first two were previously constructed by 
Wilson (2010; 2011b) and will be described only 
briefly. The third was built with insights from the 
first two, and it will be described in greater detail. 
This third corpus is the first to delineate mentioned 
language: that is, it identifies precise subsequences 
of words in a sentence that are subject to the 
phenomenon. Doing so will enable analysis of the 
syntax and semantics of English metalanguage. 
3.1 Approach 
The article set of English Wikipedia2 was chosen as 
a source for text, from which instances were mined 
using a combination of automated and manual 
efforts. Four factors led to its selection: 
1) Wikipedia is collaboratively written. Since any 
registered user can contribute to articles, 
Wikipedia reflects the language habits of a large 
sample of English writers (Adler et al 2008). 
2) Stylistic cues that sometimes delimit mentioned 
language are present in article text. 
Contributors tend to use quote marks, italic text, 
or bold text to delimit mentioned language3, thus 
following conventions respected across many 
domains of writing (Strunk & White 1979; 
Chicago Editorial Staff 2010; American 
Psychological Association. 2001). Discussion 
                                                          
2 Described in detail at 
http://en.wikipedia.org/wiki/English_Wikipedia. 
3 These conventions are stated in Wikipedia?s style manual, 
though it is unclear whether most contributors read the manual 
or follow the conventions out of habit. 
boards and other sources of informal language 
were considered, but the lack of consistent (or 
any) stylistic cues would have made candidate 
phrase collection untenably time-consuming. 
3) Articles are written to introduce a wide variety 
of concepts to the reader. Articles are written 
informatively and they generally assume the 
reader is unfamiliar with their topics, leading to 
frequent instances of mentioned language. 
4) Wikipedia is freely available. Various language 
learning materials were also considered, but 
legal and technical obstacles made them 
unsuitable for creating a freely available corpus. 
To construct each of the three corpora, a general 
procedure was followed. First, a set of current 
article revisions was downloaded from Wikipedia. 
Then, the main bodies of article text (excluding 
discussion pages, image captions, and other 
peripheral text) were scanned for sentences that 
contained instances of highlighted text (i.e., text 
inside of the previously mentioned stylistic cues). 
Since stylistic cues are also used for other language 
tasks, candidate instances were heuristically 
filtered and then annotated by human readers. 
3.2 Previous Efforts 
In previous work, a pilot corpus was constructed to 
verify the fertility of Wikipedia as a source for 
mentioned language. From 1,000 articles, 1,339 
sentences that contained stylistic cues were 
examined by a human reader, and 171 were found 
to contain at least one instance of mentioned 
language. Although this effort verified Wikipedia?s 
viability for the project, it also revealed that the 
hand-labeling procedure was time-consuming, and 
prior heuristic filtering would be necessary. 
Next, the ?Combined Cues? corpus was 
constructed to test the combination of stylistic 
filtering and a new lexical filter for selecting 
candidate instances. A set of 23 ?mention-
significant? words was gathered informally from 
the pilot corpus, consisting of nouns and verbs: 
Nouns: letter, meaning, name, phrase, 
pronunciation, sentence, sound, symbol, term, title, 
word 
Verbs: ask, call, hear, mean, name, pronounce, 
refer, say, tell, title, translate, write 
Instances of highlighted text were only 
promoted to the hand annotation stage if they 
contained at least one of these words within the 
three-word phrase directly preceding the 
641
highlighted text. From 3,831 articles, a set of 898 
sentences were found to contain 1,164 candidate 
instances that passed the combination of stylistic 
and lexical filters. Hand annotation of those 
candidates yielded 1,082 instances of mentioned 
language. Although the goal of the filters was only 
to ease hand annotation, it could be stated that the 
filters had almost 93% precision in detecting the 
phenomenon. It did not seem plausible that the set 
of mention-significant words was complete enough 
to justify that high percentage, and concerns were 
raised that the lexical filter was rejecting many 
instances of mentioned language. 
3.3 The ?Enhanced Cues? Corpus 
The construction of the present corpus (referred to 
as the ?Enhanced Cues? Corpus) was similar to 
previous efforts but used a much-enlarged set of 
mention-significant nouns and verbs gathered from 
the WordNet (Fellbaum 1998) lexical ontology. 
For each of the 23 original mention-significant 
words, a human reader started with its containing 
synset and followed hypernym links until a synset 
was reached that did not refer to a linguistic entity. 
Then, backtracking one synset, all lemmas of all 
descendants of the most general linguistically-
relevant synset were gathered. Figure 5 illustrates 
this procedure with an example. 
 
  
Figure 5: Gathering mention-significant words 
from WordNet using the seed noun ?term?. Here, 
?Language unit?, ?word?, ?syllable?, ?anagram?, 
and all their descendants are gathered. 
Using the combination of stylistic and lexical 
cues, 2,393 candidate instances were collected, and 
the researcher used the rubric and definition from 
Section 2 to identify 629 instances of mentioned 
language 4 . The researcher also identified four 
categories of mentioned language based on the 
nature of the substitution phrase X? specified by 
the rubric. These categories will be discussed in 
the following subsection. Figure 6 summarizes this 
procedure and the numeric outcomes. 
 
  
Figure 6: The procedure used to create the 
Enhanced Cues Corpus. 
3.4 Corpus Composition 
As stated previously, categories for mentioned 
language were identified based on intuitive 
relationships among the substitution phrases 
created for the rubric (e.g., ?that word?, ?that title?, 
?that symbol?). The categories are: 
1) Words as Words (WW): Within the context of 
the sentence, the candidate phrase is used to 
refer to the word or phrase itself and not what it 
usually refers to. 
                                                          
4 This corpus is available at 
 http://www.cs.cmu.edu/~shomir/um_corpus.html. 
x 
term.n.01 
part.n.01 
word.n.01 
language unit.n.01 language unit.n.01
word.n.01 
Automated mass 
collection of hyponyms 
anagram.n.01 
syllable.n.01
629 instances of mentioned language 
1,764 negative instances 
5,000 Wikipedia articles (in HTML) 
Main body text of articles 
17,753 sentences containing 
25,716 instances of highlighted text 
Article section filtering 
and sentence tokenizer 
Stylistic cue filter and  
heuristics 
Human annotator 
1,914 sentences containing 
2,393 candidate instances 
Mention word proximity 
filter 
100 instances labeled by three 
additional human annotators 
Random selection 
procedure for  
100 instances 
23 hand selected 
mention words
8,735 mention 
words and 
co-locations 
WordNet 
crawl 
Manual search for 
relevant hypernyms 
642
2) Names as Names (NN): The sentence directly 
refers to the candidate phrase as a proper name, 
nickname, or title. 
3) Spelling or Pronunciation (SP): The candidate 
text appears only to illustrate spelling, 
pronunciation, or a character symbol. 
4) Other Mention/Interesting (OM): The candidate 
phrase is an instance of mentioned language that 
does not fit the above three categories. 
5) Not Mention (XX): The candidate phrase is not 
mentioned language. 
Table 1 presents the frequencies of each category 
in the Enhanced Cues corpus, and Table 2 provides 
examples for each from the corpus. WW was by 
far the most common label to appear, which is 
perhaps an artifact of the use of Wikipedia as the 
text source. Although Wikipedia articles contain 
many names, NN was not as common, and 
informal observations suggested that names and 
titles are not as frequently introduced via 
metalanguage. Instead, their referents are 
introduced directly by the first appearance of the 
referring text. Spelling and pronunciation were 
particularly sparse; again, a different source might 
have yielded more examples for this category. The 
OM category was occupied mostly by instances of 
speech or language production by an agent, as 
illustrated by the two OM examples in Table 2. 
 
Category Code Frequency 
Words as Words WW 438 
Names as Names NN 117 
Spelling or Pronunciation SP 48 
Other Mention/Interesting OM 26 
Not Mention XX 1,764 
 
Table 1: The by-category composition of candidate 
instances in the Enhanced Cues corpus. 
 
In the interest of revealing both lexical and 
syntactic cues for mentioned language, part-of-
speech tags were computed (using NLTK (Loper 
& Bird 2002)) for words in all of the sentences 
containing candidate instances. Tables 3 and 4 list 
the ten most common words (as POS-tagged) in 
the three-word phrases before and after 
(respectively) candidate instances. Although the 
heuristics for collecting candidate instances were 
not intended to function as a classifier, figures for 
precision are shown for each word: these represent 
the percentage of occurrences of the word which 
were associated with candidates identified as 
mentioned language. For example, 80% of 
appearances of the verb call preceded a candidate 
instance that was labeled as mentioned language. 
 
Code Example 
WW The IP Multimedia Subsystem architecture 
uses the term transport plane to describe a 
function roughly equivalent to the routing 
control plane. 
The material was a heavy canvas known as 
duck, and the brothers began making work 
pants and shirts out of the strong material. 
NN Digeri is the name of a Thracian tribe 
mentioned by Pliny the Elder, in The 
Natural History. 
Hazrat Syed Jalaluddin Bukhari's 
descendants are also called Naqvi al-
Bukhari. 
SP The French changed the spelling to 
bataillon, whereupon it directly entered 
into German. 
Welles insisted on pronouncing the word 
apostles with a hard t. 
OM He kneels over Fil, and seeing that his 
eyes are open whispers: brother. 
During Christmas 1941, she typed The end
on the last page of Laura. 
XX NCR was the first U.S. publication to 
write about the clergy sex abuse scandal. 
Many Croats reacted by expelling all 
words in the Croatian language that had, in 
their minds, even distant Serbian origin. 
 
Table 2: Two examples from the corpus for each 
category. Candidate phrases appear underlined, 
with the original stylistic cues removed. 
 
Many of these words appeared as mention words 
for the Combined Cues corpus, indicating that 
prior intuitions about framing metalanguage were 
correct. In particular, call (v), word(n), and term (n) 
were exceptionally frequent and effective at 
associating  with mentioned language. In contrast, 
the distribution of frequencies for the words 
following candidate instances exhibited a ?long 
tail?, indicating greater variation in vocabulary. 
 
 
643
Rank Word Freq. Precision (%)
1 call (v) 92 80 
2 word (n) 68 95.8 
3 term (n) 60 95.2 
4 name (n) 31 67.4 
5 use (v) 17 70.8 
6 know (v) 15 88.2 
7 also (rb) 13 59.1 
8 name (v) 11 100 
9 sometimes (rb) 9 81.9 
10 Latin (n) 9 69.2 
 
Table 3: The top ten words appearing in the three-
word sequences before candidate instances, with 
precisions of association with mentioned language. 
 
Rank Word Freq. Precision (%)
1 mean (v) 31 83.4 
2 name (n) 24 63.2 
3 use (v) 11 55 
4 meaning (n) 8 57.1 
5 derive (v) 8 80 
6 refers (n) 7 87.5 
7 describe (v) 6 60 
8 refer (v) 6 54.5 
9 word (n) 6 50 
10 may (md) 5 62.5 
 
Table 4: The top ten words appearing in the three-
word sequences after candidate instances, with 
precisions of association with mentioned language. 
3.5 Reliability and Consistency of Annotation 
To provide some indication of the reliability and 
consistency of the Enhanced Cues Corpus, three 
additional expert annotators were recruited to label 
a subset of the candidate instances. These 
additional annotators received guidelines for 
annotation that included the five categories, and 
they worked separately (from each other and from 
the primary annotator) to label 100 instances 
selected randomly with quotas for each category.  
Calculations first were performed to determine 
the level of agreement on the mere presence of 
mentioned language, by mapping labels WW, NN, 
SP, and OM to true and XX to false. All four 
annotators agreed upon a true label for 46 
instances and a false label for 30 instances, with an 
average pairwise Kappa (computed via NTLK) of 
0.74. Kappa between the primary annotator and a 
hypothetical ?majority voter? of the three 
additional annotators was 0.90. These results were 
taken as moderate indication of the reliability of 
?simple? use-mention labeling. 
However, the per-category results showed 
reduced levels of agreement. Kappa was calculated 
to be 0.61 for the original coding. Table 5 shows 
the Kappa statistic for binary re-mapping for each 
of the categories. This was done similarly to the 
?XX versus all others? procedure described above. 
 
Code Frequency K 
WW 17 0.38 
NN 17 0.72 
SP 16 0.66 
OM 4 0.09 
XX 46 0.74 
 
Table 5: Frequencies of each category in the subset 
labeled by additional annotators and the values of 
the Kappa statistic for binary relabelings. 
 
The low value for remapped OM was expected, 
since the category was small and intentionally not 
well-defined. The relatively low value for WW 
was not expected, though it seems possible that the 
redaction of specific stylistic cues made annotators 
less certain when to apply this category. Overall, 
these numbers suggest that, although annotators 
tend to agree whether a candidate instance is 
mentioned language or not, there is less of a 
consensus on how to qualify positive instances. 
4 Discussion 
The Enhanced Cues corpus confirms some of the 
hypothesized properties of metalanguage and 
yields some unexpected insights. Stylistic cues 
appear to be strongly associated with mentioned 
language; although the examination of candidate 
phrases was limited to ?highlighted? text, informal 
perusal of the remainder of article text confirmed 
this association. Further evidence can be seen in 
examples from other texts, shown below with their 
original stylistic cues intact: 
? Like so many words, the meaning of ?addiction? 
has varied wildly over time, but the trajectory 
might surprise you.5 
                                                          
5 News article from CNN.com: 
http://www.cnn.com/2011/LIVING/03/23/addicted.t
o.addiction/index.html 
644
? Sending a signal in this way is called a speech 
act.6 
? M1 and M2 are Slashdot shorthand for 
?moderation? and ?metamoderation,? 
respectively.7 
? He could explain foreordination thoroughly, and 
he used the terms ?baptize? and ?Athanasian.?8  
? They use Kabuki precisely because they and 
everyone else have only a hazy idea of the 
word?s true meaning, and they can use it purely 
on the level of insinuation.9 
However, the connection between mentioned 
language and stylistic cues is only valuable when 
stylistic cues are available. Still, even in their 
absence there appears to be an association between 
mentioned language and a core set of nouns and 
verbs. Recurring patterns were observed in how 
mention-significant words related to mentioned 
language. Two were particularly common: 
? Noun apposition between a mention-significant 
noun and mentioned language. An example of 
this appears in Sentence (5), consisting of the 
noun verb and the mentioned word have. 
? Mentioned language appearing in appropriate 
semantic roles for mention-significant verbs. 
Sentence (3) illustrates this, with the verb call 
assigning the label tough love as an attribute of 
the sentence subject. 
With further study, it should be possible to exploit 
these relationships to automatically detect 
mentioned language in text. 
5 Related Work 
The use-mention distinction has enjoyed a long 
history of chiefly theoretical discussion. Beyond 
those authors already cited, many others have 
addressed it as the formal topic of quotation 
(Davidson 1979; Cappelen & Lepore 1997; Garc?a-
Carpintero 2004; Partee 1973; Quine 1940; Tarski 
1933). Nearly all of these studies have eschewed 
empirical treatments, instead hand-picking 
illustrations of the phenomenon. 
                                                          
6 Page 684 of Russell and Norvig?s 1995 edition of Artificial 
Intelligence, a textbook. 
7 Frequently Asked Questions (FAQ) list on Slashdot.org: 
http://slashdot.org/faq/metamod.shtml 
8 Novel Elmer Gantry by Sinclair Lewis. 
9 Opinion column on Slate.com: 
http://www.slate.com/id/2250081/ 
One notable exception was a study by Anderson 
et al (2004), who created a corpus of 
metalanguage from a subset of the British National 
Corpus, finding that approximately 11% of spoken 
utterances contained some form (whether explicit 
or implicit) of metalanguage. However, limitations 
in the Anderson corpus? structure (particularly lack 
of word- or phrase-level annotations) and content 
(the authors admit it is noisy) served as compelling 
reasons to start afresh and create a richer resource. 
6 Future Work 
As explained in the introduction, the long-term 
goal of this research program is to apply an 
understanding of metalanguage to enhance 
language technologies. However, the more 
immediate goal for creating this corpus was to 
enable (and to begin) progress in research on 
metalanguage. Between these long-term and 
immediate goals lies an intermediate step: methods 
must be developed to detect and delineate 
metalanguage automatically. 
Using the Enhanced Cues Corpus, a two-stage 
approach to automatic identification of mentioned 
language is being developed. The first stage is 
detection, the determination of whether a sentence 
contains an instance of mentioned language. 
Preliminary results indicate that approximately 
70% of instances can be detected using simple 
machine learning methods (e.g., bag of words input 
to a decision tree). The remaining instances will 
require more advanced methods to detect, such as 
word sense disambiguation to validate occurrences 
of mention-significant words. The second stage is 
delineation, the determination of the subsequence 
of words in a sentence that functions as mentioned 
language. Early efforts have focused on the 
associations discussed in Section 5 between 
mentioned language and mention-significant words. 
The total number of such associations appears to 
be small, making their collection a tractable 
activity. 
Acknowledgements 
The author would like to thank Don Perlis and 
Scott Fults for valuable input. This research was 
supported in part by NSF (under grant 
#IIS0803739), AFOSR (#FA95500910144), and 
ONR (#N000140910328). 
645
References  
Adler, B. Thomas, Luca de Alfaro, Ian Pye & 
Vishwanath Raman. 2008. Measuring author 
contributions to the Wikipedia. In Proc. of WikiSym 
'08. New York, NY, USA: ACM. 
American Psychological Association. 2001. Publication 
Manual of the American Psychological Association. 
5th ed. Washington,  DC: American Psychological 
Association. 
Anderson, Michael L, Yoshi A Okamoto, Darsana 
Josyula & Donald Perlis. 2002. The use-mention 
distinction and its importance to HCI. In Proc. of 
EDILOG 2002. 21?28. 
Anderson, Michael L., Andrew Fister, Bryant Lee & 
Danny Wang. 2004. On the frequency and types of 
meta-language in conversation: A preliminary report. 
In Proc. of the 14th Annual Conference of the Society 
for Text & Discourse. 
Cappelen, H & E Lepore. 1997. Varieties of quotation. 
Mind 106(423). 429 ?450. 
Chicago Editorial Staff. 2010. The Chicago Manual of 
Style. 16th ed. University of Chicago Press. 
Clark, Herbert H. & Edward F. Schaefer. 1989. 
Contributing to discourse. Cognitive Science 13(2). 
259?294. 
Davidson, Donald. 1979. Quotation. Theory and 
Decision 11(1). 27?40. 
Fellbaum, Christiane. 1998. WordNet: An Electronic 
Lexical Database. Cambridge: MIT Press. 
Garc?a-Carpintero, Manuel. 2004. The deferred 
ostension theory of quotation. No?s 38(4). 674?692. 
Klein, Dan & Christopher D. Manning. 2003. Fast exact 
inference with a factored model for natural language 
parsing. Advances in Neural Information Processing 
Systems 15. 
Loper, Edward & Steven Bird. 2002. NLTK: The 
Natural Language Toolkit. In Proceedings of the 
ACL-02 Workshop on Effective Tools and 
Methodologies for Teaching Natural Language 
Processing and Computational Linguistics 1. 63?70. 
Association for Computational Linguistics. 
Maier, Emar. 2007. Mixed quotation: Between use and 
mention. In Proc. of LENLS 2007. 
Partee, Barbara. 1973. The syntax and semantics of 
quotation. In Stephen Anderson & Paul Kiparsky 
(eds.), A Festschrift for Morris Halle. New York: 
Holt, Rinehart, Winston. 
Perlis, Donald, Khemdut Purang & Carl Andersen. 
1998. Conversational adequacy: Mistakes are the 
essence. International Journal of Human-Computer 
Studies 48(5). 553?575. 
Quine, W. V. O. 1940. Mathematical Logic. Cambridge, 
MA: Harvard University Press. 
Raux, Antoine, Brian Langner, Dan Bohus, Alan W 
Black & Maxine Eskenazi. 2005. Let?s Go public! 
Taking a spoken dialog system to the real world. In 
Proc. of Interspeech 2005. 
Saka, Paul. 1998. Quotation and the use-mention 
distinction. Mind 107(425). 113 ?135. 
Saka, Paul. 2003. Quotational constructions. Belgian 
Journal of Linguistics 17(1). 
Strunk, Jr. & E. B. White. 1979. The Elements of Style, 
Third Edition. Macmillan. 
Tarski, Alfred. 1933. The concept of truth in formalized 
languages. In J. H. Woodger (ed.), Logic, Semantics, 
Mathematics. Oxford: Oxford University Press. 
Wilson, Shomir. 2010. Distinguishing use and mention 
in natural language. In Proc. of the NAACL HLT 
2010 Student Research Workshop, 29?33. 
Association for Computational Linguistics. 
Wilson, Shomir. 2011a. A Computational Theory of the 
Use-Mention Distinction in Natural Language. Ph.D. 
dissertation, University of Maryland at College Park. 
Wilson, Shomir. 2011b. In search of the use-mention 
distinction and its impact on language processing 
tasks. International Journal of Computational 
Linguistics and Applications 2(1-2). 139?154. 
 
646
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 409?414,
Baltimore, Maryland, USA, June 23-25 2014. c?2014 Association for Computational Linguistics
Determiner-Established Deixis to Communicative Artifacts in Pedagogical Text 
Shomir Wilson1,2 and Jon Oberlander1 1School of Informatics, University of Edinburgh, United Kingdom 2School of Computer Science, Carnegie Mellon University, USA shomir@cs.cmu.edu, jon@inf.ed.ac.uk    Abstract 
Pedagogical materials frequently contain deixis to communicative artifacts such as textual structures (e.g., sections and lists), discourse entities, and illustrations. By relating such artifacts to the prose, deixis plays an essential role in structuring the flow of information in informative writing. However, existing language technologies have largely overlooked this mechanism. We examine properties of deixis to communicative artifacts using a corpus rich in determiner-established instances of the phenomenon (e.g., ?this section?, ?these equations?, ?those reasons?) from Wikibooks, a collection of learning texts. We use this corpus in combination with WordNet to determine a set of word senses that are characteristic of the phenomenon, showing its diversity and validating intuitions about its qualities. The results motivate further research to extract the connections encoded by such deixis, with the goals of enhancing tools to present pedagogical e-texts to readers and, more broadly, improving language technologies that rely on deictic phenomena. 1 Introduction Deixis often appears in written language as an anaphoric mechanism to refer to communicative entities in a document. Such deixis can have a variety of referent types. For example, consider that idea in Sentence (1), those names in (2), this section in (3), and these figures in (4): (1) That idea has been challenged by many.  (2) Those names are Welsh in origin. (3) In this section, we cover some early work. (4) Quantities in these figures are approximate. The kinds of deixis represented in (1) and (2) are similar to discourse deixis (Webber, 1991) and textual deixis (Lyons, 1977), respectively. Sentence (3) contains deixis to a structural element of a document (Paraboni and Deemter, 
2006), and (4) contains an example of deixis to illustrative items such as figures or examples. We collectively term such deictic acts as communicative deixis (CD for brevity), recognizing their shared characteristics, and we name their referents communicative artifacts (CAs). Prior studies have focused on narrow varieties of CD (such as those identified above), leaving unknown their properties when viewed together as a whole. Moreover, efforts to automatically identify or resolve CD have been piecemeal at best. Given the complexity of the referents, conventional tools for coreference or anaphora resolution are poorly applicable. This paper describes analysis of the first collection of instances of deixis in English targeted to refer to a broad variety of CAs. Texts from the website Wikibooks are used, for the intuitive density of CD in pedagogical material and the potential value of augmenting them with interpretive metadata. The diversity of referents in this corpus enables new inferences on the composition and relative frequencies of CD varieties in text. We focus on determiner-established instances, i.e., anaphoric noun phrases that begin with determiners this, that, these, or those (e.g., (1)-(4)). This focus has the advantage of collecting instances that explicitly identify the relevant capacities of their referents (e.g., (1) reifies its referent as an ?idea?). The remainder of this paper is structured as follows. Section 2 surveys related work on deixis to specific types of CAs. Section 3 describes the text source for this study and the procedure used to collect and label instances. Section 4 describes our use of WordNet to characterize CAs, resulting in an ontology of such referents and inter-annotator agreement results for labeling of artifact types. Finally, Section 5 provides some conclusions and directions for future work. 
409
2 Related Work  The value of CD in pedagogical contexts has been established by studies such as those by Mayer (2009) and Buisine and Martin (2007). Those motivate our work to fill the present lack of corpus-based linguistic knowledge of the phenomenon. Also, although spatial deixis falls beyond the scope of this paper, we acknowledge the efforts of others such as Gergle et al (2013) to study its value in collaborative communication. Prior works have examined discourse deixis in text, though little attention has been given to CD as a phenomenon or deixis to other CAs. Seminal papers by Webber (1988, 1991) established the importance of discourse deixis, although they focused upon demonstrative pronouns such as ?this? or ?that?. Many efforts have addressed discourse deixis in the context of anaphora; these include Poesio and Artstein?s (2008), who created a corpus of anaphoric relations inclusive of (but not limited to) discourse. Their collection included 455 instances of discourse deixis, although they noted ambiguity in the set of markables. Dipper and Zinsmeister (2012) also addressed discourse deixis through anaphora resolution and produced a collection of 225 abstract anaphors out of 643 candidate instances. Prior studies of shell nouns revealed capacities of referents similar to a subset of those found in our work. Such nouns are used anaphorically to refer to complex, proposition-like pieces of information such as points, assumptions, or acts (Schmid, 2000). Kolhatkar et al (2013) noted the pervasiveness of shell nouns in text and their tendency to ?characterize and label? their antecedents. However, such antecedents only partly intersect with CAs. The set of shell nouns studied by Schmid did not include typical document entities such as section, figure, or list. Simultaneously, the set included many nouns with little or no relevance as CAs, such as fury, miracle, and pride.  The task of identifying CD in text and referent CAs bears some similarity to coreference resolution. However, coreference resolvers tried by the authors (namely CoreNLP (Recasens et al, 2013), ArkRef (O?Connor and Heilman, 2013) and the work of Roth and Bengston (2008)) were ineffective at this task. We posit that many CAs are not noun phrases, which makes them difficult or inappropriate to characterize as referring expressions. This limits the effectiveness of traditional approaches to coreference resolution toward the present problem. 
 Our results are further distinct from prior work by focusing on the communicative capacities of a variety of referents represented in documents. However, the present focus upon determiner-established phrases is more exclusive, and our results do not include demarcation of referents. We posit that the tradeoff is worthwhile, given limited prior work on identifying CD and the lack of prior efforts to study CAs other than discourse entities. 3 Corpus Creation  Textbooks from Wikibooks were chosen to supply pedagogical text. Among the alternatives, this source provided the largest volume of material with a license amenable to corpus redistribution. Moreover, the collection of English language textbooks on the site covers a diverse set of topics and contains samples from a variety of writers. Below we describe our text pre-processing and then explain how candidate instances of CD were identified.  3.1 Source Material To simplify collection and processing, 122 Wikibooks textbooks with printable versions were selected for use. Contained in this set are textbooks in eleven different subject areas, such as computing, humanities, and the sciences. In preparation for analysis, the documents were POS tagged and parsed by the Stanford CoreNLP suite (Socher et al, 2013; Toutanova et al, 2003). Table 1 presents some statistics on the texts in aggregate. They illustrate the substantial size of most texts, though a few were freshly started or incomplete. Overall, the corpus is comparable in size with corpora from efforts cited in Section 2, though text genera and sought markables vary. Next, potential instances of CD were identified. Such instances were noun phrases beginning with determiners this, that, these, or those. We include these and those to collect CD to sets of entities, a nuance absent from any previous work. 9252 sentences, or 8% of the corpus, contained at least one potential instance. 
Statistic Total Min. Median Mean Max. Words 2883178 1721 20337 23633 57465 Sentences 114474 71 832 938 2121 Candidates  10495 4 85 86 285 Table 1. Statistics for the 122 selected printable Wikibooks and the candidate instances of CD.  
410
This collection contained substantial boilerplate text, and sentences that appeared verbatim in at least ten different books were discarded. This filtering produced a set of 7613 candidate instances. Table 2 shows the most frequent head nouns in candidate instances. Some resemble the shell nouns of prior work, but the presence of others illustrates the diversity of CD. Diversity was expected from pedagogical texts and validates Wikibooks as a rich source of CD.   We conducted a preliminary survey of the corpus contents by reading a random selection of 10% of candidates and judging their statuses as instances of CA. Table 3 shows examples of candidate instances, categorized by the foci of prior studies (cited in the Introduction) of CD phenomena. The researchers estimated that 48% of candidates were instances of CD, although directly labeling large numbers of candidates was deemed impractical. Instead, we noted that the word sense of the noun in a candidate instance is an important (albeit not definitive) indication of its CD status. Accordingly, we shift our focus from individual candidate instances to words that appear in them (i.e., lemmas) and word senses.  
 3.2 Word Senses The noun in an instance of CD has a doubly salient role in CA, by providing a cue to the intended referent and also by reifying the referent. For example, an illustrating referent might be referred to as ?this example? or ?this ideal?, with divergent consequences. The noun choice semantically identifies the relevant capacity of the referent, affecting its message. To identify the varieties and characteristics of CD in pedagogical text, we examine in aggregate the senses of those words that appear in candidate phrases in the corpus. WordNet 3.0 (Fellbaum, 1998) was chosen to provide an ontological structure for relevant word senses and thus for CAs. First, synsets for the 27 most frequent nouns in candidate phrases were collected, irrespective of viability for CD. This covered 34% of candidate instances and resulted in a set of 200 synsets. Their glosses were labeled as viable or non-viable for CD by two expert annotators, who first worked separately and then collaborated to resolve differences in their annotations. 
Lemma Freq.  Lemma Freq. page 314  function 83 book 287  chapter 73 case 249  information 70 example 126  problem 69 point 121  value 62 section 116  type 59 way 112  process 56 option 102  feature 56 time 101  number 54 message 93  text 54 Table 2. The 20 most frequent head nouns in candidate instances. 
 
For each synset gloss, perform the following: Imagine instantiating the type represented by the gloss. Judge its suitability for the following statements. (1) [an instantiation of the type] is about a topic. (2) [an instantiation of the type] is intended to communicate an idea. (3) [an instantiation of the type] can be produced in a document or as a document to convey information. If at least two of the three statements above are coherent, mark 'y' for the gloss. Otherwise, mark 'n'. Figure 1. Instructions given to annotators.  
 Category Examples Structural Many of the resources listed elsewhere in this section have? In this chapter, we will show you how to draw? 
Illustrative Consider these sentences: [followed by example sentences] [following a source code fragment] ?the first time the computer sees this statement, ?a? is zero, so it is less than 10. Discourse Utilizing this idea, subunit analogies were invented? In this case, you?ve narrowed the topic down to ?Badges.? Non-CD Devices similar to resistors turn this energy into light, motion? What type of things does a person in that career field know?  Table 3. Examples of candidate instances. Bold text denotes the determiner and head noun in each instance. Sentences are truncated in the table for brevity.  
411
Figure 1 shows the annotation instructions, which were designed to address the combined range of CAs from prior work. To illustrate its application, consider the noun chapter. One gloss of chapter is ?a subdivision of a written work; usually numbered and titled?. This sense clearly satisfies the third numbered statement in Figure 1. Coherency arguments for the first and second statements are less definitional, but both annotators decided at least one was satisfactory, leading to a y mark. Another gloss of chapter is ?any distinct period in history or in a person?s life?. This sense fails to satisfy the second or third statement, leading to an n mark. 4 Results and Discussion Resolving differences between the annotators? labels produced a set of 62 synsets whose glosses characterized CAs. We refer to the sets of 200 synsets and 62 synsets as the CCS (candidates for communicative senses) and VCS (verified  communicative senses) sets, respectively. We offer the complete results of our annotations online 1  to encourage further research on this topic. In this section we present inter-annotator agreement statistics and describe the composition of the VCS set using the structure of WordNet. 4.1 Inter-Annotator Agreement The kappa statistic for category agreement between the two annotators was 0.70, with matching annotations on 174 of 200 senses. Although this metric is an imperfect indicator, this value is generally regarded as substantial (Viera and Garrett, 2005) albeit with some tentativeness (Carletta, 1996). The annotators respectively placed 33% and 30% of instances in the VCS set, suggesting general agreement on the distribution of labels irrespective of specific instances. The annotators agreed that some cases were difficult to label without context, and a combination of sense labeling and in-text instance labeling may be fruitful for future work. 4.2 Representation in WordNet We use the structure of WordNet to illustrate the properties of CAs that VCS senses represent. To do this, the hypernym closure (i.e., the sequence(s) of hypernyms from a given synset to the root synset) was computed for each VCS sense. These ?traces? were aggregated into a                                                             
1 http://www.cs.cmu.edu/~shomir/wb_cd_study/ 
reproduction of a subset of WordNet?s synsets and relations, resulting in a de facto ontology of CAs. The same procedure was performed for the CCS set to create an illustrative baseline. Table 4 shows the structure of the most general synsets in the ontologies constructed from VCS and CCS traces. Fractions illustrate the relative constituent weight of each synset, by virtue of the traces that include it. For example, 65 of the 72 traces for VCS synsets pass through abstraction.n.06, and 37 of those 65 traces pass through communication.n.02. The total quantities of traces for CCS and VCS are greater than their respective set sizes because of a small number of synsets in those sets with multiple hyponym paths to the root. The rightmost column of Table 4 shows the decimal result of subtracting the CCS constituent weight fraction from the VCS fraction. Positive numbers indicate that the manual labeling of senses magnified the weight of a synset over the CCS baseline. The constituent weights confirm some intuitions but also hold a few surprises. The vast majority of CAs are abstractions rather than physical entities, and most of the abstractions are ?something that is communicated by or to or between people or groups? (the gloss of communication.n.02). Psychological features are also a substantial constituency, with traces to VCS synsets that represent words such as method, plan, and question. Most of the few VCS physical entities are communicative artifacts in their complete form (e.g., a book or a periodical issue). Matter as a physical entity may seem out of place in Table 4. The VCS synset responsible for its inclusion is page.n.01, which 
Synset CCS VCS Chg. 0 entity.n.01   1 abstraction.n.06     2 psych._feature.n.01     2 communication.n.02     2 attribute.n.02     2 group.n.01     2 measure.n.02     2 relation.n.01   1 physical_entity.n.01     2 object.n.01     2 causal_agent.n.01     2 thing.n.12     2 process.n.06     2 matter.n.03 
217 / 217 166 / 217 51 / 166 47 / 166 24 / 166 18 / 166 15 / 166 11 / 166 51 / 217 38 / 51 7 / 51 4 / 51 1 / 51 1 / 51 
72 / 72 65 / 72 15 / 65 37 / 65 2 / 65 4 / 65 3 / 65 4 / 65 7 / 72 6 / 7 0 / 7 0 / 7 0 / 7 1 / 7 
0 .14 -.08 .29 -.11 -.05 -.04 .00 -.14 .11 -.14 -.08 -.02 .12 Table 4. Distributions of traces through the first two hyponym relations emanating from the root synset entity.n.01, for CCS and VCS. Fractions indicate the constituent weight of each synset.   
412
has the gloss ?one side of one leaf (of a book or magazine or newspaper or letter etc.) or the written or pictorial matter it contains.? Both annotators believed it merited inclusion in VCS. Finally, we observed that many VCS senses (58%) were not the first sense for their words, indicating different senses appear more often2. This likely hinders word sense disambiguation of nouns in CD instances: the common baseline of first sense tagging is futile in these cases, and their extra-topical nature means that appropriate CA senses are not implied by the surrounding words (Wilson, 2011). This suggests that identification of CD instances may require a dedicated approach to word sense tagging. 5 Conclusion The results of this study illustrate the significance of CD, both for the processing of pedagogical texts and for the broader project of understanding anaphora. Its pervasiveness and its diversity show its potential as a conduit for language technologies to enrich documents with pragmatic metadata. Our next effort will be to identify the referents of CD instances using knowledge from the present study of the character and distribution of those referents. CAs are represented by spans of content in a document (e.g., text or figures), and accordingly the identification of a CD referent will involve the selection of the correct span of content. We expect that the word sense of the noun in a CD phrase will limit the set of potentially relevant CAs, and that both localized features (such as paragraph position of a CD instance and the expected CA count) and document-level features (e.g., proximity of potential referents) will be valuable.  Acknowledgment This research was supported by grant #1159236 from the US National Science Foundation?s International Research Fellowship Program.  
                                                            
2 The WordNet manual advises that senses are ?generally? ordered by frequency. 
References  Bengtson, E. and Roth, D. (2008). Understanding the value of features for coreference resolution. In Proc. EMNLP. Buisine, S. and Martin, J.-C. (2007). The effects of speech?gesture cooperation in animated agents? behavior in multimedia presentations. Interacting with Computers, 19(4), 484?493. doi:10.1016/j.intcom.2007.04.002 Carletta, J. (1996). Assessing agreement on classification tasks: The kappa statistic. Computational Linguistics, 22(2), 249?254. Dipper, S. and Zinsmeister, H. (2012). Annotating abstract anaphora. In Proc. LREC, 46(1), 37?52. doi:10.1007/s10579-011-9160-1 Fellbaum, C. (1998). WordNet: An Electronic Lexical Database. Cambridge: MIT Press. Gergle, D., Kraut, R. E., and Fussell, S. R. (2013). Using visual information for grounding and awareness in collaborative tasks. Human-Computer Interaction, 28(1), 1?39. Kolhatkar, V., Zinsmeister, H., and Hirst, G. (2013). Interpreting anaphoric shell nouns using antecedents of cataphoric shell nouns as training data. In Proc. EMNLP (pp. 300?310). Lyons, J. (1977). Semantics. Cambridge University Press. Mayer, R. E. (2009). Multimedia Learning. Cambridge University Press. O'Connor, B. and Heilman, M. (2013). ARKref: A rule-based coreference resolution system. arXiv:1310.1975, Paraboni, I. and Deemter, K. (2006). Referring via document parts. In A. Gelbukh (Ed.), Computational Linguistics and Intelligent Text Processing (Vol. 3878, pp. 299?310). Springer Berlin Heidelberg. Retrieved from http://dx.doi.org/10.1007/11671299_31 Poesio, M. and Artstein, R. (2008). Anaphoric annotation in the ARRAU Corpus. In Proc. LREC. Marrakech, Morocco: European Language Resources Association (ELRA). Recasens, M., Catherine de Marneffe, M., and Potts, C. (2013). The life and death of discourse entities: Identifying singleton mentions. In Proc. NAACL. Schmid, H.-J. (2000). English Abstract Nouns as Conceptual Shells: From Corpus to Cognition. Walter de Gruyter. 
413
Socher, R., Bauer, J., Manning, C. D., and Ng, A. Y. (2013). Parsing with compositional vector grammars. In Proc. ACL (pp. 455?465). Toutanova, K., Klein, D., Manning, C. D., and Singer, Y. (2003). Feature-rich part-of-speech tagging with a cyclic dependency network. In Proc. NAACL. doi:10.3115/1073445.1073478 Viera, A. J., and Garrett, J. M. (2005). Understanding interobserver agreement: The kappa statistic. Family Medicine, 37(5), 360?363. 
Webber, B. L. (1988). Discourse deixis: Reference to discourse segments. In Proc. ACL (pp. 113?122). doi:10.3115/982023.982037 Webber, B. L. (1991). Structure and ostension in the interpretation of discourse deixis. In Natural Language and Cognitive Processes. Wilson, S. (2011). A Computational Theory of the Use-Mention Distinction in Natural Language. University of Maryland at College Park. PhD Thesis, College Park, MD, USA.   
414
