Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 921?928
Manchester, August 2008
Class-Driven Attribute Extraction
Benjamin Van Durme, Ting Qian and Lenhart Schubert
Department of Computer Science
University of Rochester
Rochester, NY 14627, USA
Abstract
We report on the large-scale acquisition
of class attributes with and without the
use of lists of representative instances, as
well as the discovery of unary attributes,
such as typically expressed in English
through prenominal adjectival modifica-
tion. Our method employs a system based
on compositional language processing, as
applied to the British National Corpus. Ex-
perimental results suggest that document-
based, open class attribute extraction can
produce results of comparable quality as
those obtained using web query logs, indi-
cating the utility of exploiting explicit oc-
currences of class labels in text.
1 Introduction
Recent work on the task of acquiring attributes
for concept classes has focused on the use of pre-
compiled lists of class representative instances,
where attributes recognized as applying to multi-
ple instances of the same class are inferred as be-
ing likely to apply to most, or all, members of
that class. For example, the class US President
might be represented as a list containing the en-
tries Bill Clinton, George Bush, Jimmy Carter, etc.
Phrases such as Bill Clinton?s chief of staff ..., or
search queries such as chief of staff bush, provide
evidence that the class US President has as an at-
tribute chief of staff.
Usually the focus of such systems has been on
on binary attributes, such as the example chief of
staff, while less attention has been paid to unary
c
? 2008. Licensed under the Creative Commons
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
class attributes such as illegal for the class Drug,
or warm-blooded for the class Animal.
1
These
attributes are most typically expressed in English
through prenominal adjectival modification, with
the nominal serving as a class designator. When
attribute extraction is based entirely on instances
and not the class labels themselves, this form of
modification goes undiscovered.
In what follows we explore both the impact of
gazetteers in attribute extraction as well as the
acquisition and filtering of unary class attributes,
through a process based on logical form genera-
tion from syntactic parses derived from the British
National Corpus.
2 Extraction Framework
Extraction was performed using a modified ver-
sion of the KNEXT system, a knowledge acquisi-
tion framework constructed for large scale genera-
tion of abstracted logical forms through composi-
tional linguistic analysis. The following provides
an overview of KNEXT and its target knowledge
representation, Episodic Logic.
2.1 Episodic Logic
Automatically acquiring general world knowledge
from text is not a task that provides an immedi-
ate solution to any real world problem.
2
Rather,
the motivation for acquiring large stores of back-
ground knowledge is to enable research within
other areas of artificial intelligence, e.g., the con-
struction of systems that can engage in dialogues
about everyday topics in unrestricted English, use
1
Almuhareb and Poesio (2004) treat unary attributes as
values of binary attributes; e.g., illegal might be the value of
a legality attribute. But for many unary attributes, this is a
stretch.
2
Unless one regularly needs reminding of facts such as, A
WOMAN MAY BOIL A GOAT.
921
(Some e0:
[e0 at-about Now0]
[(Many.det x :
[x ((attr athletic.a) (plur youngster.n))]
[x want.v
(Ka
(become.v
(plur
((attr professional.a) athlete.n))))])
** e0])
Figure 1: Example EL formula; square brackets indicate
a sentential infix syntax of form [subject pred object ...], Ka
reifies action predicates, and attr ?raises? adjectival predicates
to predicate modifiers; e0 is the situation characterized by the
sentence.
common sense in answering questions or solv-
ing problems, pursue intrinsic goals independently,
and show awareness of their own characteristics,
biography, and cognitive capacities and limita-
tions. An important challenge in the pursuit of
these long-range goals is the design and implemen-
tation of a knowledge representation that is as ex-
pressively rich as natural language and facilitates
language understanding and commonsense reason-
ing.
Episodic Logic (EL) (Schubert and Hwang,
2000), is a superset of FOL augmented with cer-
tain semantic features common to all human lan-
guages: generalized quantification, intensionality,
uncertainty, modification and reification of predi-
cates and propositions, and event characterization
by sentences. An implementation of EL exists as
the EPILOG system (Schaeffer et al, 1993), which
supports both forward and backward inference,
along with various specialized routines for dealing
with, e.g., color, time, class subsumption, etc. EPI-
LOG is under current development as a platform for
studying a notion of explicit self-awareness as de-
fined by Schubert (2005).
As an indication of EL?s NL-like syntax, figure
1 contains the output of EPILOG?s parser/logical-
form generator for the sentence, Many athletic
youngsters want to become professional athletes.
2.2 KNEXT
If ?deep? language understanding and common-
sense reasoning involve items as complex and
structured as seen in figure 1, then automated
knowledge acquisition cannot simply be a mat-
ter of accumulating rough associations between
word strings, along the lines ?(Youngster) (want
become) (professional athlete)?. Rather, acquired
knowledge needs to conform with a systematic,
highly expressive KR syntax such as EL.
The KNEXT project is aimed at extracting such
structured knowledge from text. One of the major
obstacles is that the bulk of commonsense knowl-
edge on which people rely is not explicitly written
down ? precisely because it is common. Even it
were written down, most of it could not be reliably
interpreted, because reliable interpretation of lan-
guage is itself dependent on commonsense knowl-
edge (among other things).
In view of these difficulties, KNEXT has initially
focused on attempting to abstract world knowl-
edge ?factoids? from texts, based on the logical
forms derived from parsed sentences. The idea is
that nominal pre- and post-modifiers, along with
subject-verb-object relations, captured in logical
forms similar to that in figure 1, give a glimpse
of the common properties and relationships in the
world ? even if the source sentences describe in-
vented situations. For example, the following were
extracted by KNEXT, then automatically verbal-
ized back into English for ease of readability:
? SOME NUMBER OF YOUNGSTERS MAY WANT
TO BECOME ATHLETES.
? YOUNGSTERS CAN BE ATHLETIC.
? ATHLETES CAN BE PROFESSIONAL.
2.3 Attribute Extraction via KNEXT
In order to study the contribution of lists of in-
stances (i.e., generalized gazetteers) to the task of
attribute extraction, the version of KNEXT as pre-
sented by Schubert (2002) was modified to provide
output of a form similar to that of the extraction
work of Pas?ca and Van Durme (2007).
KNEXT?s abstracted, propositional output was
automatically verbalized into English, with any re-
sultant statements of the form, A(N) X MAY HAVE
A(N) Y, taken to suggest that the class X has as an
attribute the property Y.
KNEXT was designed from the beginning to
make use of gazetteers if available, where a
phrase such as Bill Clinton vetoed the bill sup-
ports the (verbalized) proposition A PRESIDENT
MAY VETO A BILL. just as would The president
vetoed the bill. We instrumented the system to
record which propositions did or did not require
gazetteers in their construction, allowing for a nu-
merical breakdown of the respective contributions
of known instances of a class, versus the class label
itself.
922
Pas?ca and Van Durme (2007) described the re-
sults of an informal survey asking participants to
enumerate what they felt to be important attributes
for a small set of example classes. Some of these
resultant attributes were not of the form targeted
by the authors? system. For example, nonprofit
was given as an important potential attribute for
the class Company, as well as legal for the class
Drug. These attributes correspond to unary predi-
cates as compared to the targeted binary predicates
underlying such attributes as cost(X,Y) for the class
Drug.
We extracted such unary attributes by focusing
on verbalizations of the form, A(N) X CAN BE Y
as in AN ANIMAL CAN BE WARM-BLOODED.
3 Experimental Setting
3.1 Corpus Processing
Initial reports on the use of KNEXT were focused
on the processing of manually created parse trees,
on a corpus of limited size (the Brown corpus of
Kucera and Francis (1967)). Since that time the
system has been modified into a fully automatic
extraction system, making use of syntactic parse
trees generated by parsers trained on the Penn
Treebank.
For our studies here, the parser employed was
that of Collins (1997) applied to the sentences
of the British National Corpus (BNC Consortium,
2001). Our choice of the BNC was motivated by
its breadth of genre, its substantial size (100 mil-
lion words) and its familiarity (and accessibility)
to the community.
3.2 Gazetteers
KNEXT?s gazetteers were used as-is, and were
defined based on a variety of sources: miscella-
neous publicly available lists, as well as manual
enumeration. The classes covered can be seen in
the Results section in table 2, where the minimum,
maximum and mean size were 2, 249, and 41, re-
spectively.
3.3 Filtering out Non-predicative Adjectives
Beyond the pre-existing KNEXT framework, ad-
ditional processing was introduced for the extrac-
tion of unary attributes in order to filter out vacu-
ous or unsupported propositions derived from non-
compositional phrases.
This filtering was performed through the cre-
ation of three lists: a whitelist of accepted pred-
icative adjectives; a graylist containing such adjec-
tives that are meaningful as unary predicates only
when applied to plural nouns; and a blacklist de-
rived from Wikipedia topic titles, representing lex-
icalized, non-compositional phrases.
Whitelist The creation of the whitelist began with
calculating part-of-speech (POS) tagged bigram
counts using the Brown corpus. The advantage
of using a POS-tagged bigram model lies in the
saliency of phrase structures, which enabled fre-
quency calculations for both attributive and pred-
icative uses of a given adjective. Attributive counts
were based on instances when an adjective appears
in the pre-nominal position and modifies another
noun. Predicative counts were derived by sum-
ming over occurrences of a given adjective after
all possible copulas. These counts were used to
compute a p/a ratio - the quotient of predicative
count over attributive count - for each word classi-
fied by WordNet (Fellbaum, 1998) as a having an
adjectival use. After manual inspection, two cut-
off points were chosen at ratios of .06 and 0, as
seen in table 1.
Words not appearing in the Brown corpus (i.e.
having 0 count for both uses), were sampled and
inspected, with the decision made to place the
majority within the whitelist, excluding just those
with suffixes including -al, -c, -an, -st, -ion, -th, -o,
-ese, -er, -on, -i, -x, -v, and -ing.
This process resulted in a combined whitelist of
14,249 (usually) predicative adjectives.
p/a ratio (r) Cut-off decision
r ? .06 keep the adjective*
0 < r < .06 remove the adjective*
otherwise keep the adjective*
Table 1: Cut-off decision given the p/a ratio of an
adjective. *Note: except for hand-selected cases.
Graylist We manually constructed a short list (cur-
rently 33 words) containing adjectives that are gen-
erally inappropriate as whitelist entries, but could
be acceptable when applied to plurals. For exam-
ple, the verbalized proposition OBJECTS CAN BE
SIMILAR was deemed acceptable, while a state-
ment such as AN OBJECT CAN BE SIMILAR is
erroneous because of a missing argument.
Blacklist From an exhaustive set of Wikipedia
topic titles was derived a blacklist consisting of en-
tries that had to satisfy four criteria: 1) no more
than three words in length; 2) has no closed-class
923
words, such as prepositions or adverbs; 3) must
begin with an adjective and end with a noun (de-
termined by WordNet); and 4) does not contain
any numerical characters or miscellaneous sym-
bols that are usually not meaningful in English.
Therefore, each title in the resultant list is liter-
ally a short noun phrase with adjectives as pre-
modifiers. It was observed that in these encyclope-
dia titles, the role of adjectives is predominantly to
restrict the scope of the object that is being named
(e.g. CRIMINAL LAW), rather than to describe
its attributions or features (e.g. DARK EYES).
More often than not, only cases similar to the sec-
ond example can be safely verbalized as X CAN
BE Y from a noun phrase Y X, with Y being the
pre-nominal adjective.
We further refined this list by examining trigram
frequencies as reported in the web-derived n-gram
collection of Brants and Franz (2006). For each ti-
tle of the form (Adj N) ..., we gathered trigram fre-
quencies for adverbial modifications such as (very
Adj N) ..., and (truly Adj N) .... Intuitively, high rel-
ative frequency of such modification with respect
to the non-modified bigram supports removal of
the given title from the blacklist.
Trigram counts were collected using the modi-
fiers: absolutely, almost, entirely, highly, nearly,
perfectly, truly and very. These counts were
summed for a given title then divided by the afore-
mentioned bigram score. Upon sampled inspec-
tion, all three-word titles were kept on the black-
list, along with any two-word title with a resultant
ratio less than 0.028. For example, the titles Hardy
Fish, Young Galaxy, and Sad Book were removed,
while Common Cause, Bouncy Ball, and Heavy Oil
were retained.
4 Results
From the parsed BNC, 6,205,877 propositions
were extracted, giving an average of 1.396 propo-
sitions per input sentence.
3
These results were
then used to explore the necessity of gazetteers,
and the potential for extracting unary attributes.
Quality judgements were performed using a 5
point scale as seen in figure 2.
3
These approximately six million verbalized propo-
sitions, along with their underlying logical form and
respective source sentence(s), may be browsed in-
teractively through an online browser available at:
http://www.cs.rochester.edu/u/vandurme/epik
THE STATEMENT ABOVE IS A REASONABLY
CLEAR, ENTIRELY PLAUSIBLE GENERAL
CLAIM AND SEEMS NEITHER TOO SPECIFIC
NOR TOO GENERAL OR VAGUE TO BE USEFUL:
1. I agree.
2. I lean towards agreement.
3. I?m not sure.
4. I lean towards disagreement.
5. I disagree.
Figure 2: Instructions for scaled judging.
4.1 Necessity of Gazetteers
From the total set of extracted propositions,
638,809 could be verbalized as statements of the
form X MAY HAVE Y. There were 71,531 unique
classes (X) for which at least a single candidate at-
tribute (Y) was extracted, with 9,743 of those hav-
ing at least a single such attribute that was sup-
ported by a minimum of two distinct sentences.
Table 2 gives the number of attributes extracted
for the given classes when using only gazetteers,
when using only the given names as class labels,
and when using both together. While instance-
based extraction generated more unique attributes,
there were still a significant number of results de-
rived based exclusively on class labels. Further,
as can be seen for cases such as Artist, class-
driven extraction provided a large number of at-
tribute candidates not observed when relying only
on gazetteers (701 total candidate attributes were
gathered based on the union of 441 and 303 can-
didates respectively extracted with, and without a
gazetteer for Artist).
We note that this volume measure is potentially
biased against class-driven extraction, as no ef-
fort was made to pick an optimal label for a given
gazetteer, (the original hand-specified class labels
were retained). For example, one might expect the
label Drink to generate more, yet still appropriate,
propositions than Beverage, Actor and/or Actress
as compared to Show Biz Star, or the semantically
similar Book versus Literary Work. This is sug-
gested by the entries in the table based on using
supertypes of the given class, as well as in figure
3, which favorably compares top attributes discov-
ered for select classes against those reported else-
where in the literature.
Table 3 gives the assessed quality for the top ten
attributes extracted for five of the classes in table
2. As can be seen, class-driven extraction can pro-
duce attributes of quality assessed at par with at-
tributes extracted using only gazetteers.
924
BasicFood Religion
K (Food): quality, part, taste, value, portion.. K: basis, influence, name, truths, symbols, principles,
D: species, pounds, cup, kinds, lbs, bowl.. strength, practice, origin, adherent, god, defence..
Q: nutritional value, health benefits, glycemic index, D: teachings, practice, beliefs, religion spread,
varieties, nutrition facts, calories.. principles, emergence, doctrines..
Q: basic beliefs, teachings, holy book, practices, rise,
branches, spread, sects..
HeavenlyBody Painter
K
G
(Planet): surface, orbit, bars, history, atmosphere.. K
G
(Artist) : works, life, career, painting, impression,
K (Planet): surface, history, future, orbit, mass, field.. drawings, paintings, studio, exhibition..
K (Star): surface, mass, field, regions.. K (Artist): works, impression, career, life, studio..
D: observations, spectrum, planet, spectra, conjunction, K (Painter) : works, life, wife, eye..
transit, temple, surface.. Q?: paintings, works, portrait, death, style, artwork,
Q: atmosphere, surface, gravity, diameter, mass, bibliography, bio, autobiography, childhood..
rotation, revolution, moons, radius..
Figure 3: Qualitative comparison of top extracted attributes; K
G
is KNEXT using gazetteers, K (class) is KNEXT for a class
label similar to the heading, D and Q are document- and query-based results as reported in (Pas?ca et al, 2007), Q? is query-based
results reported in (Pas?ca and Van Durme, 2007).
The noticeable drop in quality for the class
Planet when only using gazetteers (3.2 mean
judged acceptability) highlights the recurring
problem of word sense ambiguity in extraction.
The names of Roman deities, such as Mars or Mer-
cury, are used to refer to a number of conceptu-
ally distinct items, such as planets within our so-
lar system. Two of the attributes judged as poor
quality for this class were bars and customers, re-
spectively derived from the noun phrases: (NP
(NNP Mars) (NNS bars)), and (NP (NNP Mer-
cury) (NNS customers)). Note that in both cases
the underlying extraction is correctly performed;
the error comes from abstracting to the wrong
class. These NPs may arguably support the ver-
balized propositions, e.g.: A CANDY-COMPANY
MAY HAVE BARS, and A CAR-COMPANY
MAY HAVE CUSTOMERS.
These examples point to additional areas for
improvement beyond sense disambiguation: non-
compositional phrase filtering for all NPs, rather
than just in the cases of adjectival modification
(Mars bar is a Wikipedia topic); and relative dis-
counting of patterns used in the extraction pro-
cess
4
. This later technique is commonly used in
specialized extraction systems, such as constructed
by Snow et al (2005) who fit a logistic regression
model for hypernym (X is-a Y) classification based
on WordNet, and Girju et al (2003) who trained a
classifier to look specifically for part-whole rela-
tions.
4
For example, (NP (NNP X) (NNS Y)) may be more se-
mantically ambiguous than, e.g., the possessive construction
(NP (NP (NNP X) (POS ?s)) (NP (NNS Y))).
4.2 Unary Attributes
Table 4 shows how filtering non-compositional
phrases from CAN BE propositions affects extrac-
tion volume. Table 5 shows the difference between
such post-filtered propositions and those that were
deleted. As our filter lists were not built fully au-
tomatically, evaluation was performed exclusively
by an author with negligible direct involvement in
the lists? creation (so-as to minimize judgement
bias).
As examples, the top ten unary attributes for
select classes are given in table 6, which the au-
thors believe to be high quality on average, with
some bad entries present. Attributes such as
pre-raphaelite for Painter are considered obscure,
while those such as favourite for Animal are con-
sidered unlikely to be useful as a unary predicate.
The importance of class-driven extraction can be
seen in results such as those given for the class Ap-
ple. Even if it were the case that gazetteer-based
extraction could deliver perfect results for those
classes whose instances occasionally appear ex-
plicitly in text, there are a number of classes for
which such instances are entirely lacking. For ex-
ample, there are many instances of the class Com-
pany which have been individually named and ap-
pear in text with some frequency, e.g., Microsoft,
Walmart, or Boeing. However, despite the many
real-world instantiations of the class Apple, this
does not translate into a list of individually named
members in text.
5
If our goal is to acquire at-
tributes for as many classes as possible, our results
5
Instances of Apple are referred to directly as such; ?an
apple.?
925
Class Both Gaz. Class Lbl.
Continent 777 698 96
Country 7,285 5,993 1,696
US State 1,289 1,286 609*
US City 2,216 2,120 813*
World City 4,780 4,747 813*
Beverage 53 53 0
Tycoon 19 10 10
TV Network 71 71 0
Artist 706 441 303
Medicine 29 2 27
Weekday 1,234 1,232 2
Month 2,282 1,875 474
Dictator 533 509 28
Conqueror 103 84 19
Philosopher 672 649 37
Conductor 118 74 45
Singer 220 179 49
Band 349 58 303
King 811 208 664
Queen 541 17 532
Religious Leader 127 127 0
Adventurer 32 27 5
Planet 289 163 141
Criminal/Outlaw 30 30 6/4*
Service Agency 85 83 2
Architect 72 67 63
Show Biz Star 82 82 0
Film Maker 42 33 9
Composer 722 651 98
Humanitarian 5 5 0
Pope 235 123 113
River 402 168 253
Company 3,968 1,553 2,941
Deity 1,037 1,027 19
Scientist 798 750 60
Religious Holiday 594 593 65*
Civic Holiday 3 3 65*
Military Commander 71 71 26*
Intl Political Entity 673 673 0
Sports Celebrity 45 45 0
Activist Organization 63 63 0
Martial Art 3 3 0
Government Agency 295 294 2
Criminal Organization 0 0 0
US President 596 596 1,421*
Political Leader 568 568 170*
Supreme Court Justice 0 0 18*
Emperor 436 211 259
Fictitious Character 227 227 180*
Literary Work 9 9 0
Engineer/Inventor 10 10 73/13*
Famous Lawyer 0 0 72*
Writer 1,116 957 236
TOTAL 35,723 29,518 8,506
Table 2: Extraction volume with and without using
gazetteers. *Note: When results are zero after gaz. omission,
values are reported for super-types, such as Holiday for the
sub-type Civic Holiday, or City for US City. A/B scores re-
ported for each class used separately, e.g., Engineer/Inventor.
Class Both Gazetteer Class Label
King 1.2 1.9 1.3
Composer 1.5 1.5 2.1
River 1.9 1.9 1.5
Continent 1.5 1.9 2.0
Planet 1.9 3.2 1.6
Table 3: Average judged acceptability for the top ten at-
tributes extracted for the given classes when using/not-using
gazetteer information.
Collection Size
% of
Original CAN BE
Original total 6,204,184 100 -
Filtered total 5,382,282 87 -
Original CAN BE 2,895,325 46 100
Filtered CAN BE 2,073,417 33 72
Whitelist 812,146 15 28
Blacklist 19,786 1< 1
Table 4: Impact of filtering on volume. For example, those
propositions removed because of the whitelist comprised 15%
of the total propositions extracted, or 28% of those specifi-
cally verbalized as X CAN BE Y.
indicate the benefits of exploiting the explicit ap-
pearance of class labels in text.
5 Related Work
Pas?ca and Van Durme (2007) presented an ap-
proach to attribute extraction based on the use
of search engine query logs, a previously unex-
plored source of data within information extrac-
tion. Results confirmed the intuition that a sig-
nificant number of high quality, characteristic at-
tributes for many classes may be derived based
on the relative frequency with which anonymous
users request particular pieces of information for
known instances of a concept class. Pas?ca et al
(2007) compared the quality of shallow attribute
extraction techniques as applied to documents ver-
sus search engine query logs, concluding that such
methods are more applicable to query logs than to
documents. We note that while search queries do
seem ideally suited for extracting class attributes,
existing large-scale collections of query logs are
proprietary and thus unavailable to the general re-
search community. At least until such a resource
becomes available, it is of interest to the commu-
nity that (qualitatively) similar extraction results
may be achieved exclusively using publicly avail-
able document collections.
Alternative approaches to harvesting large-scale
knowledge repositories based on logical forms in-
clude that reported by Suchanek et al (2007).
The authors used non-linguistic information avail-
926
1 10 100 1,000
Filtered 3.18 3.60 2.74 2.76
Blacklist 3.88 4.00 4.08 4.06
Whitelist 3.78 3.76 3.74 3.80
Table 5: Mean evaluated acceptability for 50 unary at-
tributes randomly sampled from each of the given levels of
support (attribute occurred once, less than 10 times, less than
100 times, ...). Filtered refers to the final ?clean? results,
Blacklist and Whitelist refer to propositions deleted due to
the given list.
Painter
famous, romantic, distinguished, celebrated,
well-known, pre-raphaelite, flemish, dutch,
abstract
Animal
dead, trapped, dangerous, unfortunate, intact,
hungry, wounded, tropical, sick, favourite
Drug
dangerous, powerful, addictive, safe, illegal,
experimental, effective, prescribed, harmful,
hallucinatory
Apple
red, juicy, fresh, bad, substantive, stuffed,
shiny, ripe, green, baked
Earthquake
disastrous, violent, underwater, prolonged,
powerful, popular, monstrous, fatal, famous,
epic
Table 6: Top ten unary attributes for select classes, gathered
exclusively without the use of gazetteers.
able via Wikipedia to populate a KB based on
a variant of the logic underlying the Web On-
tology Language (OWL). Results were limited to
14 predefined relation types, such as diedInYear
and politicianOf, with membership of instances
within particular concept classes inferred based on
Wikipedia?s category pages. Authors report 5 mil-
lion so-called ontological facts being extracted.
Almuhareb and Poesio (2004) performed at-
tribute extraction on webtext using simple extrac-
tion patterns (e.g., ?the * of the C [is|was]?, and
?[a|an|the] * C [is|was]?, which respectively match
The color of the rose was red and A red rose was
...), and showed that such attributes could improve
concept clustering. Subsequently they tested an
alternative approach to the same problem using a
dependency parser, extracting syntactic relations
such as (ncmod, rose, red) and (ncsubj, grow, rose)
(Almuhareb and Poesio, ). They concluded that
syntactic information is relatively expensive to de-
rive, and serves primarily to alleviate data spar-
sity problems (by capturing dependencies between
potentially widely separated words) that may no
longer be an issue given the scale of the Web. We
take a different view, first because attribute extrac-
tion is an offline task for which a 60% overhead
cost (reported by the authors) is not a major is-
sue, but more importantly because we regard ap-
proaches that process language compositionally as
ultimately necessary for deeper meaning represen-
tation and language understanding.
Following intuitions similar to those laid out by
Schubert (2002), Banko et al (2007) presented
TextRunner, the latest in a series of ever more so-
phisticated general information extraction systems
(Cafarella et al, 2005; Etzioni et al, 2004). The
authors constructed a non-parser based extractor
for open domain text designed to efficiently pro-
cess web-sized datasets. Results are in the form of
bracketed text sequences that hint at a sentence?s
underlying semantics. For example, (Bletchley
Park) was location of (Station X).
Cimiano et al (2005) performed a limited form
of class-driven extraction in order to induce class
hierarchies via the methods of Formal Concept
Analysis (FCA). For example, a car is both drive-
able and rentable based on its occurrence in object
position of the relevant verbs. A bike shares these
properties with car, as well as having the property
rideable, leading to these classes being near in the
resultant automatically constructed taxonomy. Ex-
periments were performed on limited domains for
which pre-existing ontologies existed for measur-
ing performance (tourism and finance).
Lin (1999) gave a corpus-based method for find-
ing various types of non-compositional phrases,
including the sort discussed in this paper. Identi-
fication was based on mutual information statistics
conditioned on a given syntactic context (such as
our targeted prenominal adjectival modification).
If the mutual information of, e.g., white house,
shows strong differences from that for construc-
tions with similar components, e.g., red house, and
white barn, then the given phrase was determined
to be non-compositional. The use of this method to
supplement that explored here is a matter of cur-
rent investigation. Early results confirm our in-
tuition regarding the correlation between such au-
tomatically discovered non-compositional phrases
and Wikipedia topic titles, where high scoring
phrases not already in our list tend to suggest miss-
Yes
cooking pot, magic flute, runny nose, skimmed milk,
acquired dyslexia, charged particles, earned income
No
causal connectives, golden oldies, ruling junta,
graduated pension, unsung heroes, viral rna
Table 7: Example high-scoring phrases as ranked by Lin?s
metric when applied to KNEXT logical forms, along with
whether there is, at the time of this writing, an associated
Wikipedia entry.
927
A CAR MAY HAVE A ...
back, boot, side, driver, front, roof, seat, end, interior,
owner, door, control, value, bonnet, wheel, window,
engine, headlights..
A CAR CAN BE ...
black, parked, red, white, armoured, nice, hired, bloody,
open, beautiful, wrecked, unmarked, secondhand,
powerful, brand-new, out-of use, damaged, heavy, dark,
competitive, broken-down..
A CAR MAY BE ... IN SOME WAY
parked, stolen, driven, damaged, serviced, stopped,
lost, clamped, overturned, locked, involved in an accident,
found, turned, transported..
Table 8: Top attributes extracted for the class Car, where
MAY BE relational properties (akin to those used by Cimi-
ano et al (2005)) are similarly acquired via verbalization of
abstracted logical forms.
ing entries in the encyclopedia (see table 7). The
ability to perform such ?missing topic discovery?
should be of interest to those within the emerging
community of Wikipedia-focused AI researchers.
6 Conclusion
We have shown that an open knowledge extraction
system can effectively yield class attributes, even
when named instances of the class are unavailable
or scarce (as a final example see table 8). We stud-
ied the quantitative contributions of instances (as
given in KNEXT gazetteers) and explicitly occur-
ring class nominals to the discovery of attributes,
and found both to be important. We paid partic-
ular attention to the acquisition of unary class at-
tributes, for which access to class labels is of par-
ticular importance because of their typical manner
of expression in text.
Acknowledgements The authors are grateful to
Daniel Gildea for contributing a parsed version of
the BNC. This work was supported by NSF grants
IIS-0328849 and IIS-0535105.
References
Almuhareb, Abdulrahman and Massimo Poesio. Finding con-
cept attributes in the web using a parser. In Proceedings
Corpus Linguistics Conference.
Almuhareb, Abdulrahman and Massimo Poesio. 2004.
Attribute-based and value-based clustering: an evaluation.
In Proceedings of EMNLP.
Banko, Michele, Michael J Cafarella, Stephen Soderland,
Matt Broadhead, and Oren Etzioni. 2007. Open Infor-
mation Extraction from the Web. In Proceedings of IJCAI.
BNC Consortium. 2001. The British National Corpus, ver-
sion 2 (BNC World). Distributed by Oxford University
Computing Services.
Brants, Thorsten and Alex Franz. 2006. Web 1T 5-gram Ver-
sion 1. Distributed by the Linguistic Data Consortium.
Cafarella, Michael J., Doug Downey, Stephen Soderland, and
Oren Etzioni. 2005. KnowItNow: Fast, Scalable Infor-
mation Extraction from the Web. In Proceedings of HLT-
EMNLP.
Cimiano, Philipp, Andreas Hotho, and Steffen Stabb. 2005.
Learning concept hierarchies from text corpora using for-
mal concept analysis. Journal of Artificial Inteligence Re-
search.
Collins, Michael. 1997. Three generative, lexicalised models
for statistical parsing. In Proceedings of ACL.
Etzioni, Oren, Michael Cafarella, Doug Downey, Stanley
Kok, AnaMaria Popescu, Tal Shaked, Stephen Soderland,
Daniel S. Weld, and Alexander Yates. 2004. Web-scale
Information Extraction in KnowItAll. In Proceedings of
WWW.
Fellbaum, Christiane. 1998. WordNet: An Electronic Lexical
Database. MIT Press.
Girju, R., A. Badulescu, and D. Moldovan. 2003. Learning
semantic constraints for the automatic discovery of part-
whole relations. In Proceedings of HLT-NAACL.
Kucera, H. and W. N. Francis. 1967. Computational Anal-
ysis of Present-Day American English. Brown University
Press, Providence, RI.
Lin, Dekang. 1999. Automatic identification of non-
compositional phrases. In Proceedings of ACL.
Pas?ca, Marius and Benjamin Van Durme. 2007. What you
seek is what you get: Extraction of class attributes from
query logs. In Proceedings of IJCAI.
Pas?ca, Marius, Benjamin Van Durme, and Nikesh Garera.
2007. The role of documents vs. queries in extracting class
attributes from text. In Proceedings of CIKM.
Schaeffer, S.A., C.H. Hwang, J. de Haan, and L.K. Schubert.
1993. EPILOG, the computational system for episodic
logic: User?s guide. Technical report, Dept. of Comput-
ing Science, Univ. of Alberta, August.
Schubert, Lenhart K. and Chung Hee Hwang. 2000. Episodic
logic meets little red riding hood: A comprehensive, natu-
ral representation for language understanding. In Iwanska,
L. and S.C. Shapiro, editors, Natural Language Processing
and Knowledge Representation: Language for Knowledge
and Knowledge for Language. MIT/AAAI Press.
Schubert, Lenhart K. 2002. Can we derive general world
knowledge from texts? In Proceedings of HLT.
Schubert, Lenhart K. 2005. Some Knowledge Representa-
tion and Reasoning Requirements for Self-awareness. In
Proc. AAAI Spring Symposium on Metacognition in Com-
putation.
Snow, Rion, Daniel Jurafsky, and Andrew Y. Ng. 2005.
Learning syntactic patterns for automatic hypernym dis-
covery. In Proceedings of NIPS 17.
Suchanek, Fabian M., Gjergji Kasneci, and Gerhard Weikum.
2007. YAGO: A core of semantic knowledge unifying
WordNet and Wikipedia. In Proceedings of WWW.
928
Proceedings of the 12th Conference of the European Chapter of the ACL, pages 808?816,
Athens, Greece, 30 March ? 3 April 2009. c?2009 Association for Computational Linguistics
Deriving Generalized Knowledge from Corpora using WordNet
Abstraction
Benjamin Van Durme, Phillip Michalak and Lenhart K. Schubert
Department of Computer Science
University of Rochester
Rochester, NY 14627, USA
Abstract
Existing work in the extraction of com-
monsense knowledge from text has been
primarily restricted to factoids that serve
as statements about what may possibly ob-
tain in the world. We present an ap-
proach to deriving stronger, more general
claims by abstracting over large sets of
factoids. Our goal is to coalesce the ob-
served nominals for a given predicate ar-
gument into a few predominant types, ob-
tained as WordNet synsets. The results can
be construed as generically quantified sen-
tences restricting the semantic type of an
argument position of a predicate.
1 Introduction
Our interest is ultimately in building systems
with commonsense reasoning and language un-
derstanding abilities. As is widely appreciated,
such systems will require large amounts of gen-
eral world knowledge. Large text corpora are
an attractive potential source of such knowledge.
However, current natural language understand-
ing (NLU) methods are not general and reliable
enough to enable broad assimilation, in a formal-
ized representation, of explicitly stated knowledge
in encyclopedias or similar sources. As well, such
sources typically do not cover the most obvious
facts of the world, such as that ice cream may be
delicious and may be coated with chocolate, or
that children may play in parks.
Methods currently exist for extracting simple
?factoids? like those about ice cream and children
just mentioned (see in particular (Schubert, 2002;
Schubert and Tong, 2003)), but these are quite
weak as general claims, and ? being unconditional
? are unsuitable for inference chaining. Consider
however the fact that when something is said, it
is generally said by a person, organization or text
source; this a conditional statement dealing with
the potential agents of saying, and could enable
useful inferences. For example, in the sentence,
?The tires were worn and they said I had to re-
place them?, they might be mistakenly identified
with the tires, without the knowledge that saying
is something done primarily by persons, organiza-
tions or text sources. Similarly, looking into the
future one can imagine telling a household robot,
?The cat needs to drink something?, with the ex-
pectation that the robot will take into account that
if a cat drinks something, it is usually water or
milk (whereas people would often have broader
options).
The work reported here is aimed at deriving
generalizations of the latter sort from large sets of
weaker propositions, by examining the hierarchi-
cal relations among sets of types that occur in the
argument positions of verbal or other predicates.
The generalizations we are aiming at are certainly
not the only kinds derivable from text corpora (as
the extensive literature on finding isa-relations,
partonomic relations, paraphrase relations, etc. at-
tests), but as just indicated they do seem poten-
tially useful. Also, thanks to their grounding in
factoids obtained by open knowledge extraction
from large corpora, the propositions obtained are
very broad in scope, unlike knowledge extracted
in a more targeted way.
In the following we first briefly review the
method developed by Schubert and collaborators
to abstract factoids from text; we then outline our
approach to obtaining strengthened propositions
from such sets of factoids. We report positive re-
sults, while making only limited use of standard
808
corpus statistics, concluding that future endeav-
ors exploring knowledge extraction and WordNet
should go beyond the heuristics employed in re-
cent work.
2 KNEXT
Schubert (2002) presented an approach to ac-
quiring general world knowledge from text
corpora based on parsing sentences and mapping
syntactic forms into logical forms (LFs), then
gleaning simple propositional factoids from these
LFs through abstraction. Logical forms were
based on Episodic Logic (Schubert and Hwang,
2000), a formalism designed to accommodate in
a straightforward way the semantic phenomena
observed in all languages, such as predication,
logical compounding, generalized quantification,
modification and reification of predicates and
propositions, and event reference. An example
from Schubert and Tong (2003) of factoids
obtained from a sentence in the Brown corpus by
their KNEXT system is the following:
Rilly or Glendora had entered her room while
she slept, bringing back her washed clothes.
A NAMED-ENTITY MAY ENTER A ROOM.
A FEMALE-INDIVIDUAL MAY HAVE A ROOM.
A FEMALE-INDIVIDUAL MAY SLEEP.
A FEMALE-INDIVIDUAL MAY HAVE CLOTHES.
CLOTHES CAN BE WASHED.
((:I (:Q DET NAMED-ENTITY) ENTER[V]
(:Q THE ROOM[N]))
(:I (:Q DET FEMALE-INDIVIDUAL) HAVE[V]
(:Q DET ROOM[N]))
(:I (:Q DET FEMALE-INDIVIDUAL) SLEEP[V])
(:I (:Q DET FEMALE-INDIVIDUAL) HAVE[V]
(:Q DET (:F PLUR CLOTHE[N])))
(:I (:Q DET (:F PLUR CLOTHE[N])) WASHED[A]))
Here the upper-case sentences are automatically
generated verbalizations of the abstracted LFs
shown beneath them.1
The initial development of KNEXT was based
on the hand-constructed parse trees in the Penn
Treebank version of the Brown corpus, but sub-
sequently Schubert and collaborators refined and
extended the system to work with parse trees ob-
tained with statistical parsers (e.g., that of Collins
(1997) or Charniak (2000)) applied to larger cor-
pora, such as the British National Corpus (BNC),
a 100 million-word, mixed genre collection, along
with Web corpora of comparable size (see work of
Van Durme et al (2008) and Van Durme and Schu-
bert (2008) for details). The BNC yielded over 2
1Keywords like :i, :q, and :f are used to indicate in-
fix predication, unscoped quantification, and function appli-
cation, but these details need not concern us here.
factoids per sentence on average, resulting in a to-
tal collection of several million. Human judging of
the factoids indicates that about 2 out of 3 factoids
are perceived as reasonable claims.
The goal in this work, with respect to the ex-
ample given, would be to derive with the use of a
large collection of KNEXT outputs, a general state-
ment such as If something may sleep, it is probably
either an animal or a person.
3 Resources
3.1 WordNet and Senses
While the community continues to make gains
in the automatic construction of reliable, general
ontologies, the WordNet sense hierarchy (Fell-
baum, 1998) continues to be the resource of
choice for many computational linguists requiring
an ontology-like structure. In the work discussed
here we explore the potential of WordNet as an un-
derlying concept hierarchy on which to base gen-
eralization decisions.
The use of WordNet raises the challenge of
dealing with multiple semantic concepts associ-
ated with the same word, i.e., employing Word-
Net requires word sense disambiguation in order
to associate terms observed in text with concepts
(synsets) within the hierarchy.
In their work on determining selectional prefer-
ences, both Resnik (1997) and Li and Abe (1998)
relied on uniformly distributing observed frequen-
cies for a given word across all its senses, an ap-
proach later followed by Pantel et al (2007).2 Oth-
ers within the knowledge acquisition community
have favored taking the first, most dominant sense
of each word (e.g., see Suchanek et al (2007) and
Pas?ca (2008)).
As will be seen, our algorithm does not select
word senses prior to generalizing them, but rather
as a byproduct of the abstraction process. More-
over, it potentially selects multiple senses of a
word deemed equally appropriate in a given con-
text, and in that sense provides coarse-grained dis-
ambiguation. This also prevents exaggeration of
the contribution of a term to the abstraction, as a
result of being lexicalized in a particularly fine-
grained way.
3.2 Propositional Templates
While the procedure given here is not tied to a
particular formalism in representing semantic con-
2Personal communication
809
text, in our experiments we make use of proposi-
tional templates, based on the verbalizations aris-
ing from KNEXT logical forms. Specifically, a
proposition F with m argument positions gener-
ates m templates, each with one of the arguments
replaced by an empty slot. Hence, the statement,
A MAN MAY GIVE A SPEECH, gives rise to two
templates, A MAN MAY GIVE A , and A MAY
GIVE A SPEECH. Such templates match statements
with identical structure except at the template?s
slots. Thus, the factoid A POLITICIAN MAY GIVE
A SPEECH would match the second template. The
slot-fillers from matching factoids (e.g., MAN and
POLITICIAN form the input lemmas to our abstrac-
tion algorithm described below.
Additional templates are generated by further
weakening predicate argument restrictions. Nouns
in a template that have not been replaced by a free
slot can be replaced with an wild-card, indicating
that anything may fill its position. While slots
accumulate their arguments, these do not, serv-
ing simply as relaxed interpretive constraints on
the original proposition. For the running exam-
ple we would have; A MAY GIVE A ?, and, A ?
MAY GIVE A , yielding observation sets pertain-
ing to things that may give, and things that may be
given.3
We have not restricted our focus to two-
argument verbal predicates; examples such as A
PERSON CAN BE HAPPY WITH A , and, A CAN
BE MAGICAL, can be seen in Section 5.
4 Deriving Types
Our method for type derivation assumes access to
a word sense taxonomy, providing:
W : set of words, potentially multi-token
N : set of nodes, e.g., word senses, or synsets
P : N ? {N ?} : parent function
S : W? (N+) : sense function
L : N ?N?Q?0 : path length function
L is a distance function based on P that gives
the length of the shortest path from a node to a
dominating node, with base case: L(n, n) = 1.
When appropriate, we write L(w, n) to stand for
the arithmetic mean over L(n?, n) for all senses n?
3It is these most general templates that best correlate with
existing work in verb argument preference selection; how-
ever, a given KNEXT logical form may arise from multiple
distinct syntactic constructs.
function SCORE (n ? N , ? ? R+, C ?W ? W) :
C? ? D(n) \ C
return
P
w?C? L(w,n)
|C?|?
function DERIVETYPES (W ? W , m ? N+, p ? (0, 1]) :
?? 1, C ? {}, R? {}
 while too few words covered
while |C| < p? |W | :
n?? argmin
n?N \R
SCORE(n, ?,C)
R?R ? {n?}
C?C ? D(n?)
if |R| > m :
 cardinality bound exceeded ? restart
?? ?+ ?, C ? {}, R? {}
return R
Figure 1: Algorithm for deriving slot type restrictions, with
? representing a fixed step size.
of w that are dominated by n.4 In the definition of
S, (N+) stands for an ordered list of nodes.
We refer to a given predicate argument position
for a specified propositional template simply as a
slot. W ? W will stand for the set of words found
to occupy a given slot (in the corpus employed),
and D : N?W ? is a function mapping a node to
the words it (partially) sense dominates. That is,
for all n ? N and w ? W , if w ? D(n) then
there is at least one sense n? ? S(w) such that n is
an ancestor of n? as determined through use of P .
For example, we would expect the word bank to be
dominated by a node standing for a class such as
company as well as a separate node standing for,
e.g., location.
Based on this model we give a greedy search al-
gorithm in Figure 1 for deriving slot type restric-
tions. The algorithm attempts to find a set of dom-
inating word senses that cover at least one of each
of a majority of the words in the given set of obser-
vations. The idea is to keep the number of nodes in
the dominating set small, while maintaining high
coverage and not abstracting too far upward.
For a given slot we start with a set of observed
words W , an upper bound m on the number of
types allowed in the result R, and a parameter p
setting a lower bound on the fraction of items inW
that a valid solution must dominate. For example,
when m = 3 and p = 0.9, this says we require the
solution to consist of no more than 3 nodes, which
together must dominate at least 90% of W .
The search begins with initializing the cover set
C, and the result set R as empty, with the variable
4E.g., both senses of female in WN are dominated by the
node for (organism, being), but have different path lengths.
810
? set to 1. Observe that at any point in the exe-
cution of DERIVETYPES, C represents the set of
all words from W with at least one sense having
as an ancestor a node in R. While C continues to
be smaller than the percentage required for a so-
lution, nodes are added to R based on whichever
element of N has the smallest score.
The SCORE function first computes the modi-
fied coverage of n, setting C ? to be all words in W
that are dominated by n that haven?t yet been ?spo-
ken for? by a previously selected (and thus lower
scoring) node. SCORE returns the sum of the path
lengths between the elements of the modified set
of dominated nodes and n, divided by that set?s
size, scaled by the exponent ?. Note when ? = 1,
SCORE simply returns the average path length of
the words dominated by n.
If the size of the result grows beyond the speci-
fied threshold,R andC are reset, ? is incremented
by some step size ?, and the search starts again.
As ? grows, the function increasingly favors the
coverage of a node over the summed path length.
Each iteration of DERIVETYPES thus represents a
further relaxation of the desire to have the returned
nodes be as specific as possible. Eventually, ?
will be such that the minimum scoring nodes will
be found high enough in the tree to cover enough
of the observations to satisfy the threshold p, at
which point R is returned.
4.1 Non-reliance on Frequency
As can be observed, our approach makes no use of
the relative or absolute frequencies of the words in
W , even though such frequencies could be added
as, e.g., relative weights on length in SCORE. This
is a purposeful decision motivated both by practi-
cal and theoretical concerns.
Practically, a large portion of the knowledge ob-
served in KNEXT output is infrequently expressed,
and yet many tend to be reasonable claims about
the world (despite their textual rarity). For ex-
ample, a template shown in Section 5, A MAY
WEAR A CRASH HELMET, was supported by just
two sentences in the BNC. However, based on
those two observations we were able to conclude
that usually If something wears a crash helmet, it
is probably a male person.
Initially our project began as an application of
the closely related MDL approach of Li and Abe
(1998), but was hindered by sparse data. We ob-
served that our absolute frequencies were often too
low to perform meaningful comparisons of rela-
tive frequency, and that different examples in de-
velopment tended to call for different trade-offs
between model cost and coverage. This was due
as much to the sometimes idiosyncratic structure
of WordNet as it was to lack of evidence.5
Theoretically, our goal is distinct from related
efforts in acquiring, e.g., verb argument selec-
tional preferences. That work is based on the de-
sire to reproduce distributional statistics underly-
ing the text, and thus relative differences in fre-
quency are the essential characteristic. In this
work we aim for general statements about the real
world, which in order to gather we rely on text as
a limited proxy view. E.g., given 40 hypothetical
sentences supporting A MAN MAY EAT A TACO,
and just 2 sentences supporting A WOMAN MAY
EAT A TACO, we would like to conclude simply
that A PERSON MAY EAT A TACO, remaining ag-
nostic as to relative frequency, as we?ve no reason
to view corpus-derived counts as (strongly) tied to
the likelihood of corresponding situations in the
world; they simply tell us what is generally possi-
ble and worth mentioning.
5 Experiments
5.1 Tuning to WordNet
Our method as described thus far is not tied to a
particular word sense taxonomy. Experiments re-
ported here relied on the following model adjust-
ments in order to make use of WordNet (version
3.0).
The function P was set to return the union of
a synset?s hypernym and instance hypernym rela-
tions.
Regarding the function L , WordNet is con-
structed such that always picking the first sense
of a given nominal tends to be correct more of-
ten than not (see discussion by McCarthy et al
(2004)). To exploit this structural bias, we em-
ployed a modified version of L that results in
a preference for nodes corresponding to the first
sense of words to be covered, especially when the
number of distinct observations were low (such as
earlier, with crash helmet):
L(n, n) =
{
1? 1|W | ?w ?W : S(w) = (n, ...)
1 otherwise
5For the given example, this method (along with the con-
straints of Table 1) led to the overly general type, living thing.
811
word # gloss
abstraction 6 a general concept formed by extracting common features from specific examples
attribute 2 an abstraction belonging to or characteristic of an entity
matter 3 that which has mass and occupies space
physical entity 1 an entity that has physical existence
whole 2 an assemblage of parts that is regarded as a single entity
Table 1: ?word, sense #? pairs in WordNet 3.0 considered overly general for our purposes.
Propositional Template Num.
A CAN BE WHISKERED 4
GOVERNORS MAY HAVE -S 4
A CAN BE PREGNANT 28
A PERSON MAY BUY A 105
A MAY BARK 6
A COMPANY MAY HAVE A 713
A MAY SMOKE 8
A CAN BE TASTY 33
A SONG MAY HAVE A 31
A CAN BE SUCCESSFUL 664
CAN BE AT A ROAD 20
A CAN BE MAGICAL 96
CAN BE FOR A DICTATOR 5
MAY FLOAT 5
GUIDELINES CAN BE FOR -S 4
A MAY WEAR A CRASH HELMET 2
A MAY CRASH 12
Table 2: Development templates, paired with the number of
distinct words observed to appear in the given slot.
Note that when |W | = 1, then L returns 0 for
the term?s first sense, resulting in a score of 0 for
that synset. This will be the unique minimum,
leading DERIVETYPES to act as the first-sense
heuristic when used with single observations.
Parameters were set for our data based on man-
ual experimentation using the templates seen in
Table 2. We found acceptable results when us-
ing a threshold of p = 70%, and a step size of
? = 0.1. The cardinality bound m was set to 4
when |W | > 4, and otherwise m = 2.
In addition, we found it desirable to add a few
hard restrictions on the maximum level of general-
ity. Nodes corresponding to the word sense pairs
given in Table 1 were not allowed as abstraction
candidates, nor their ancestors, implemented by
giving infinite length to any path that crossed one
of these synsets.
5.2 Observations during Development
Our method assumes that if multiple words occur-
ring in the same slot can be subsumed under the
same abstract class, then this information should
be used to bias sense interpretation of these ob-
served words, even when it means not picking the
first sense. In general this bias is crucial to our ap-
proach, and tends to select correct senses of the
words in an argument set W . But an example
where this strategy errs was observed for the tem-
plate A MAY BARK, which yielded the general-
ization that If something barks, then it is proba-
bly a person. This was because there were numer-
ous textual occurrences of various types of people
?barking? (speaking loudly and aggressively), and
so the occurrences of dogs barking, which showed
no type variability, were interpreted as involving
the unusual sense of dog as a slur applied to cer-
tain people.
The template, A CAN BE WHISKERED, had
observations including both face and head. This
prompted experiments in allowing part holonym
relations (e.g., a face is part of a head) as part
of the definition of P , with the final decision be-
ing that such relations lead to less intuitive gen-
eralizations rather than more, and thus these re-
lation types were not included. The remaining
relation types within WordNet were individually
examined via inspection of randomly selected ex-
amples from the hierarchy. As with holonyms we
decided that using any of these additional relation
types would degrade performance.
A shortcoming was noted in WordNet, regard-
ing its ability to represent binary valued attributes,
based on the template, A CAN BE PREGNANT.
While we were able to successfully generalize to
female person, there were a number of words ob-
served which unexpectedly fell outside that asso-
ciated synset. For example, a queen and a duchess
may each be a female aristocrat, a mum may be a
female parent,6 and a fiancee has the exclusive in-
terpretation as being synonymous with the gender
entailing bride-to-be.
6 Experiments
From the entire set of BNC-derived KNEXT
propositional templates, evaluations were per-
formed on a set of 21 manually selected examples,
6Serving as a good example of distributional preferencing,
the primary sense of mum is as a flower.
812
Propositional Template Num.
A MAY HAVE A BROTHER 28
A ? MAY ATTACK A 23
A FISH MAY HAVE A 38
A CAN BE FAMOUS 665
A ? MAY ENTERTAIN A 8
A MAY HAVE A CURRENCY 18
A MALE MAY BUILD A 42
A CAN BE FAST-GROWING 15
A PERSON MAY WRITE A 47
A ? MAY WRITE A 99
A PERSON MAY TRY TO GET A 11
A ? MAY TRY TO GET A 17
A MAY FALL DOWN 5
A PERSON CAN BE HAPPY WITH A 36
A ? MAY OBSERVE A 38
A MESSAGE MAY UNDERGO A 14
A ? MAY WASH A 5
A PERSON MAY PAINT A 8
A MAY FLY TO A ? 9
A ? MAY FLY TO A 4
A CAN BE NERVOUS 131
Table 3: Templates chosen for evaluation.
together representing the sorts of knowledge for
which we are most interested in deriving strength-
ened argument type restrictions. All modification
of the system ceased prior to the selection of these
templates, and the authors had no knowledge of
the underlying words observed for any particular
slot. Further, some of the templates were purpose-
fully chosen as potentially problematic, such as, A
? MAY OBSERVE A , or A PERSON MAY PAINT
A . Without additional context, templates such
as these were expected to allow for exceptionally
broad sorts of arguments.
For these 21 templates, 65 types were derived,
giving an average of 3.1 types per slot, and allow-
ing for statements such as seen in Table 4.
One way in which to measure the quality of an
argument abstraction is to go back to the under-
lying observed words, and evaluate the resultant
sense(s) implied by the chosen abstraction. We say
senses plural, as the majority of KNEXT propo-
sitions select senses that are more coarse-grained
than WordNet synsets. Thus, we wish to evaluate
these more coarse-grained sense disambiguation
results entailed by our type abstractions.7 We per-
formed this evaluation using as comparisons the
first-sense, and all-senses heuristics.
The first-sense heuristic can be thought of as
striving for maximal specificity at the risk of pre-
cluding some admissible senses (reduced recall),
7Allowing for multiple fine-grained senses to be judged
as appropriate in a given context goes back at least to Sussna
(1993); discussed more recently by, e.g., Navigli (2006).
while the all-senses heuristic insists on including
all admissible senses (perfect recall) at the risk of
including inadmissible ones.
Table 5 gives the results of two judges evaluat-
ing 314 ?word, sense? pairs across the 21 selected
templates. These sense pairs correspond to pick-
ing one word at random for each abstracted type
selected for each template slot. Judges were pre-
sented with a sampled word, the originating tem-
plate, and the glosses for each possible word sense
(see Figure 2). Judges did not know ahead of time
the subset of senses selected by the system (as en-
tailed by the derived type abstraction). Taking the
judges? annotations as the gold standard, we report
precision, recall and F-score with a ? of 0.5 (favor-
ing precision over recall, owing to our preference
for reliable knowledge over more).
In all cases our method gives precision results
comparable or superior to the first-sense heuristic,
while at all times giving higher recall. In partic-
ular, for the case of Primary type, corresponding
to the derived type that accounted for the largest
number of observations for the given argument
slot, our method shows strong performance across
the board, suggesting that our derived abstractions
are general enough to pick up multiple acceptable
senses for observed words, but not so general as to
allow unrelated senses.
We designed an additional test of our method?s
performance, aimed at determining whether the
distinction between admissible senses and inad-
missible ones entailed by our type abstractions
were in accord with human judgement. To this
end, we automatically chose for each template
the observed word that had the greatest num-
ber of senses not dominated by a derived type
A MAY HAVE A BROTHER
1 WOMAN : an adult female person (as opposed to a
man); ?the woman kept house while the man hunted?
2 WOMAN : a female person who plays a significant
role (wife or mistress or girlfriend) in the life of a partic-
ular man; ?he was faithful to his woman?
3 WOMAN : a human female employed to do house-
work; ?the char will clean the carpet?; ?I have a woman
who comes in four hours a day while I write?
*4WOMAN : women as a class; ?it?s an insult to Amer-
ican womanhood?; ?woman is the glory of creation?;
?the fair sex gathered on the veranda?
Figure 2: Example of a context and senses provided for
evaluation, with the fourth sense being judged as inappropri-
ate.
813
If something is famous, it is probably a person1, an artifact1, or a communication2
If ? writes something, it is probably a communication2
If a person is happy with something, it is probably a communication2, a work1, a final result1, or a state of affairs1
If a fish has something, it is probably a cognition1, a torso1, an interior2, or a state2
If something is fast growing, it is probably a group1 or a business3
If a message undergoes something, it is probably a message2, a transmission2, a happening1, or a creation1
If a male builds something, it is probably a structure1, a business3, or a group1
Table 4: Examples, both good and bad, of resultant statements able to be made post-derivation. Authors manually selected
one word from each derived synset, with subscripts referring to sense number. Types are given in order of support, and thus the
first are examples of ?Primary? in Table 5.
Method
?
j
?
j Type
Prec Recall F.5 Prec Recall F.5
derived 80.2 39.2 66.4 61.5 47.5 58.1
All
first 81.5 28.5 59.4 63.1 34.7 54.2
all 59.2 100.0 64.5 37.6 100.0 42.9
derived 90.0 50.0 77.6 73.3 71.0 72.8
Primaryfirst 85.7 33.3 65.2 66.7 45.2 60.9
all 69.2 100.0 73.8 39.7 100.0 45.2
Table 5: Precision, Recall and F-score (? = 0.5) for coarse grained WSD labels using the methods: derive from corpus data,
first-sense heuristic and all-sense heuristic. Results are calculated against both the union
S
j and intersection
T
j of manual
judgements, calculated for all derived argument types, as well as Primary derived types exclusively.
THE STATEMENT ABOVE IS A REASONABLY
CLEAR, ENTIRELY PLAUSIBLE GENERAL
CLAIM AND SEEMS NEITHER TOO SPECIFIC
NOR TOO GENERAL OR VAGUE TO BE USEFUL:
1. I agree.
2. I lean towards agreement.
3. I?m not sure.
4. I lean towards disagreement.
5. I disagree.
Figure 3: Instructions for evaluating KNEXT propositions.
restriction. For each of these alternative (non-
dominated) senses, we selected the ancestor ly-
ing at the same distance towards the root from the
given sense as the average distance from the dom-
inated senses to the derived type restriction. In
the case where going this far from an alternative
sense towards the root would reach a path passing
through the derived type and one of its subsumed
senses, the distance was cut back until this was no
longer the case.
These alternative senses, guaranteed to not be
dominated by derived type restrictions, were then
presented along with the derived type and the
original template to two judges, who were given
the same instructions as used by Van Durme and
Schubert (2008), which can be found in Figure 3.
Results for this evaluation are found in Table 6,
where we see that the automatically derived type
restrictions are strongly favored over alternative
judge 1 judge 2 corr
derived 1.76 2.10 0.60
alternative 3.63 3.54 0.58
Table 6: Average assessed quality for derived and alterna-
tive synsets, paired with Pearson correlation values.
abstracted types that were possible based on the
given word. Achieving even stronger rejection of
alternative types would be difficult, since KNEXT
templates often provide insufficient context for
full disambiguation of all their constituents, and
judges were allowed to base their assessments on
any interpretation of the verbalization that they
could reasonably come up with.
7 Related Work
There is a wealth of existing research focused on
learning probabilistic models for selectional re-
strictions on syntactic arguments. Resnik (1993)
used a measure he referred to as selectional pref-
erence strength, based on the KL-divergence be-
tween the probability of a class and that class
given a predicate, with variants explored by Ribas
(1995). Li and Abe (1998) used a tree cut model
over WordNet, based on the principle of Minimum
Description Length (MDL). McCarthy has per-
formed extensive work in the areas of selectional
814
preference and WSD, e.g., (McCarthy, 1997; Mc-
Carthy, 2001). Calling the generalization problem
a case of engineering in the face of sparse data,
Clark and Weir (2002) looked at a number of pre-
vious methods, one conclusion being that the ap-
proach of Li and Abe appears to over-generalize.
Cao et al (2008) gave a distributional method
for deriving semantic restrictions for FrameNet
frames, with the aim of building an Italian
FrameNet. While our goals are related, their work
can be summarized as taking a pre-existing gold
standard, and extending it via distributional simi-
larity measures based on shallow contexts (in this
case, n-gram contexts up to length 5). We have
presented results on strengthening type restrictions
on arbitrary predicate argument structures derived
directly from text.
In describing ALICE, a system for lifelong
learning, Banko and Etzioni (2007) gave a sum-
mary of a proposition abstraction algorithm devel-
oped independently that is in some ways similar
to DERIVETYPES. Beyond differences in node
scoring and their use of the first sense heuristic,
the approach taken here differs in that it makes no
use of relative term frequency, nor contextual in-
formation outside a particular propositional tem-
plate.8 Further, while we are concerned with gen-
eral knowledge acquired over diverse texts, AL-
ICE was built as an agent meant for construct-
ing domain-specific theories, evaluated on a 2.5-
million-page collection of Web documents per-
taining specifically to nutrition.
Minimizing word sense ambiguity by focus-
ing on a specific domain was later seen in the
work of Liakata and Pulman (2008), who per-
formed hierarchical clustering using output from
their KNEXT-like system first described in (Li-
akata and Pulman, 2002). Terminal nodes of the
resultant structure were used as the basis for in-
ferring semantic type restrictions, reminiscent of
the use of CBC clusters (Pantel and Lin, 2002) by
Pantel et al (2007), for typing the arguments of
paraphrase rules.
Assigning pre-compiled instances to their first-
sense reading in WordNet, Pas?ca (2008) then gen-
eralized class attributes extracted for these terms,
using as a resource Google search engine query
logs.
Katrenko and Adriaans (2008) explored a con-
8Banko and Etzioni abstracted over subsets of pre-
clustered terms, built using corpus-wide distributional fre-
quencies
strained version of the task considered here. Using
manually annotated semantic relation data from
SemEval-2007, pre-tagged with correct argument
senses, the authors chose the least common sub-
sumer for each argument of each relation consid-
ered. Our approach keeps with the intuition of
preferring specific over general concepts in Word-
Net, but allows for the handling of relations au-
tomatically discovered, whose arguments are not
pre-tagged for sense and tend to be more wide-
ranging. We note that the least common sub-
sumer for many of our predicate arguments would
in most cases be far too abstract.
8 Conclusion
As the volume of automatically acquired knowl-
edge grows, it becomes more feasible to abstract
from existential statements to stronger, more gen-
eral claims on what usually obtains in the real
world. Using a method motivated by that used
in deriving selectional preferences for verb argu-
ments, we?ve shown progress in deriving semantic
type restrictions for arbitrary predicate argument
positions, with no prior knowledge of sense in-
formation, and with no training data other than a
handful of examples used to tune a few simple pa-
rameters.
In this work we have made no use of rela-
tive term counts, nor corpus-wide, distributional
frequencies. Despite foregoing these often-used
statistics, our methods outperform abstraction
based on a strict first-sense heuristic, employed in
many related studies.
Future work may include a return to the MDL
approach of Li and Abe (1998), but using a fre-
quency model that ?corrects? for the biases in texts
relative to world knowledge ? for example, cor-
recting for the preponderance of people as sub-
jects of textual assertions, even for verbs like bark,
glow, or fall, which we know to be applicable to
numerous non-human entities.
Acknowledgements Our thanks to Matthew
Post and Mary Swift for their assistance in eval-
uation, and Daniel Gildea for regular advice. This
research was supported in part by NSF grants IIS-
0328849 and IIS-0535105, as well as a University
of Rochester Provost?s Multidisciplinary Award
(2008).
815
References
Michele Banko and Oren Etzioni. 2007. Strategies for Life-
long Knowledge Extraction from the Web. In Proceedings
of K-CAP.
BNC Consortium. 2001. The British National Corpus, ver-
sion 2 (BNC World). Distributed by Oxford University
Computing Services.
Diego De Cao, Danilo Croce, Marco Pennacchiotti, and
Roberto Basili. 2008. Combining Word Sense and Us-
age for Modeling Frame Semantics. In Proceedings of
Semantics in Text Processing (STEP).
Eugene Charniak. 2000. A Maximum-Entropy-Inspired
Parser. In Proceedings of NAACL.
Stephen Clark and David Weir. 2002. Class-based probabil-
ity estimation using a semantic hierarchy. Computational
Linguistics, 28(2).
Michael Collins. 1997. Three Generative, Lexicalised Mod-
els for Statistical Parsing. In Proceedings of ACL.
Christiane Fellbaum. 1998. WordNet: An Electronic Lexical
Database. MIT Press.
Sophia Katrenko and Pieter Adriaans. 2008. Semantic
Types of Some Generic Relation Arguments: Detection
and Evaluation. In Proceedings of ACL-HLT.
Hang Li and Naoki Abe. 1998. Generalizing case frames
using a thesaurus and the MDL principle. Computational
Linguistics, 24(2).
Maria Liakata and Stephen Pulman. 2002. From Trees to
Predicate Argument Structures. In Proceedings of COL-
ING.
Maria Liakata and Stephen Pulman. 2008. Automatic Fine-
Grained Semantic Classification for Domain Adaption. In
Proceedings of Semantics in Text Processing (STEP).
Diana McCarthy, Rob Koeling, Julie Weeds, and John Car-
roll. 2004. Using automatically acquired predominant
senses for Word Sense Disambiguation. In Proceedings
of Senseval-3: Third International Workshop on the Eval-
uation of Systems for the Semantic Analysis of Text.
Diana McCarthy. 1997. Estimation of a probability distribu-
tion over a hierarchical classification. In The Tenth White
House Papers COGS - CSRP 440.
Diana McCarthy. 2001. Lexical Acquisition at the Syntax-
Semantics Interface: Diathesis Alternations, Subcatego-
rization Frames and Selectional Preferences. Ph.D. the-
sis, University of Sussex.
Roberto Navigli. 2006. Meaningful Clustering of Senses
Helps Boost Word Sense Disambiguation Performance. In
Proceedings of COLING-ACL.
Marius Pas?ca. 2008. Turning Web Text and Search Queries
into Factual Knowledge: Hierarchical Class Attribute Ex-
traction. In Proceedings of AAAI.
Patrick Pantel and Dekang Lin. 2002. Discovering Word
Senses from Text. In Proceedings of KDD.
Patrick Pantel, Rahul Bhagat, Bonaventura Coppola, Timothy
Chklovski, and Eduard Hovy. 2007. ISP: Learning Infer-
ential Selectional Preferences. In Proceedings of NAACL-
HLT.
Philip Resnik. 1993. Selection and Information: A Class-
Based Approach to Lexical Relationships. Ph.D. thesis,
University of Pennsylvania.
Philip Resnik. 1997. Selectional preference and sense dis-
ambiguation. In Proceedings of the ACL SIGLEX Work-
shop on Tagging Text with Lexical Semantics: Why, What,
and How?
Francesc Ribas. 1995. On learning more appropriate Selec-
tional Restrictions. In Proceedings of EACL.
Lenhart K. Schubert and Chung Hee Hwang. 2000. Episodic
Logic meets Little Red Riding Hood: A comprehensive,
natural representation for language understanding. In
L. Iwanska and S.C. Shapiro, editors, Natural Language
Processing and Knowledge Representation: Language
for Knowledge and Knowledge for Language. MIT/AAAI
Press.
Lenhart K. Schubert and Matthew H. Tong. 2003. Extracting
and evaluating general world knowledge from the brown
corpus. In Proceedings of HLT/NAACL Workshop on Text
Meaning, May 31.
Lenhart K. Schubert. 2002. Can we derive general world
knowledge from texts? In Proceedings of HLT.
Fabian M. Suchanek, Gjergji Kasneci, and Gerhard Weikum.
2007. YAGO: A Core of Semantic Knowledge Unifying
WordNet and Wikipedia. In Proceedings of WWW.
Michael Sussna. 1993. Word sense disambiguation for free-
text indexing using a massive semantic network. In Pro-
ceedings of CIKM.
Benjamin Van Durme and Lenhart Schubert. 2008. Open
Knowledge Extraction through Compositional Language
Processing. In Proceedings of Semantics in Text Process-
ing (STEP).
Benjamin Van Durme, Ting Qian, and Lenhart Schubert.
2008. Class-driven Attribute Extraction. In Proceedings
of COLING.
816
Proceedings of the NAACL HLT Student Research Workshop and Doctoral Consortium, pages 37?42,
Boulder, Colorado, June 2009. c?2009 Association for Computational Linguistics
Building a Semantic Lexicon of English Nouns via Bootstrapping
Ting Qian1, Benjamin Van Durme2 and Lenhart Schubert2
1Department of Brain and Cognitive Sciences
2Department of Computer Science
University of Rochester
Rochester, NY 14627 USA
ting.qian@rochester.edu, {vandurme, schubert}@cs.rochester.edu
Abstract
We describe the use of a weakly supervised
bootstrapping algorithm in discovering con-
trasting semantic categories from a source lex-
icon with little training data. Our method pri-
marily exploits the patterns in sentential con-
texts where different categories of words may
appear. Experimental results are presented
showing that such automatically categorized
terms tend to agree with human judgements.
1 Introduction
There are important semantic distinctions between
different types of English nouns. For example, some
nouns typically refer to a concrete physical object,
such as book, tree, etc. Others are used to represent
the process or the result of an event (e.g. birth, cele-
bration). Such information is useful in disambiguat-
ing syntactically similar phrases and sentences, so as
to provide more accurate semantic interpretations.
For instance, A MAN WITH HOBBIES and A MAN
WITH APPLES share the same structure, but convey
very different aspects about the man being referred
to (i.e. activities vs possessions).
Compiling such a lexicon by hand, e.g., WordNet
(Fellbaum, 1998), requires tremendous time and ex-
pertise. In addition, when new words appear, these
will have to be analyzed and added manually. Fur-
thermore, a single, global lexicon may contain er-
roneous categorizations when used within a specific
domain/genre; we would like a ?flexible? lexicon,
adaptable to a given corpus. Also, in adapting se-
mantic classifications of words to a particular genre
or domain, we would like to be able to exploit con-
tinuing improvements in methods of extracting se-
mantic occurrence patterns from text.
We present our initial efforts in discovering se-
mantic classes incrementally under a weakly super-
vised bootstrapping process. The approach is able
to selectively learn from its own discoveries, thereby
minimizing the effort needed to provide seed exam-
ples as well as maintaining a reasonable accuracy
rate. In what follows, we first focus on its appli-
cation to an event-noun classification task, and then
use a physical-object vs non-physical-object experi-
ment as a showcase for the algorithm?s generality.
2 Bootstrapping Algorithm
The bootstrapping algorithm discovers words with
semantic properties similar to a small set of labelled
seed examples. These examples can be manually se-
lected from an existing lexicon. By simply changing
the semantic property of the seed set, this algorithm
can be applied to the task of discovering a variety of
semantic classes.
Features Classification is performed using a
perceptron-based model (Rosenblatt, 1958) that ex-
amines features of each word. We use two kinds
of features in our model: morphological (affix and
word length), and contextual. Suffixes, such as -ion,
often reveal the semantic type that a noun belongs
to (e.g., destruction, explosion). Other suffixes like
-er typically suggest non-event nouns (e.g. waiter,
hanger). The set of affixes can be modified to re-
flect meaningful distinctions in the task at hand. Re-
garding word length, longer words tend to have more
37
syllables, and thus are more likely to contain affixes.
For example, if a word ends with -ment, its num-
ber of letters must be ? 5. We defined a partition
of words based on word length: shortest (fewer than
5 letters), short (5-7), medium (8-12), long (13-19),
and longest (> 19).
Besides morphological features, we also make use
of verbalized propositions resulting from the experi-
ments of Van Durme et al (2008) as contextual fea-
tures. These outputs are in the form of world knowl-
edge ?factoids? abstracted from texts, based on log-
ical forms from parsed sentences, produced by the
KNEXT system (see Schubert (2002) for details).
The followings are some sample factoids about the
word destruction, extracted from the British Na-
tional Corpus.
? A PERSON-OR-ORGANIZATION MAY UNDERGO A DE-
STRUCTION
? INDIVIDUAL -S MAY HAVE A DESTRUCTION
? PROPERTY MAY UNDERGO A DESTRUCTION
We take each verbalization (with the target word
removed) as a contextual feature, such as PROPERTY
MAY UNDERGO A . Words from the same seman-
tic category (e.g., event nouns) should have seman-
tic and syntactic similarities on the sentential level.
Thus their contextual features, which reflect the use
of words both semantically and syntactically, should
be similar. For instance, PROPERTY MAY UNDERGO
A PROTECTION is another verbalization produced
by KNEXT, suggesting the word protection may be-
long to the same category as destruction.
A few rough-and-ready heuristics are already em-
ployed by KNEXT to do the same task as we wish
to automate here. A built-in classifier judges nomi-
nals to be event or non-event ones based on analysis
of endings, plus a list of event nouns whose endings
are unrevealing, and a list of non-event nouns whose
endings tend to suggest they are event nouns. As a
result, the factoids used as contextual features in our
work already reflect the built-in classifier?s attempt
to distinguish event nouns from the rest. Thus, the
use of these contextual features may bias the algo-
rithm to perform seemingly well on event-noun clas-
sification. However, we will show that our algorithm
works for classification of other semantic categories,
for which KNEXT does not yet have discriminative
procedures.
Iterative Training We use a bootstrapping pro-
cedure to iteratively train a perceptron-based lin-
ear classifier. A perceptron algorithm determines
whether the active features of a test case are similar
to those learned from given categories of examples.
In an iterative training process, the classifier first
learns from a small seed set, which contains exam-
ples of all categories (in binary classification, both
positive and negative examples) manually selected
to reflect human knowledge of semantic categories.
The classifier then discovers new instances (and cor-
responding features) of each category. Based on
activation values, these newly discovered instances
are selectively admitted into the original training set,
which increases the size of training examples for the
next iteration.
The iterative training algorithm described above
is adopted from Klementiev and Roth (2006). The
advantage of bootstrapping is the ability to auto-
matically learn from new discoveries, which saves
both time and effort required to manually examine
a source lexicon. However, if implemented exactly
as described above, this process has two apparent
disadvantages: New examples may be wrongly clas-
sified by the model; and it is difficult to evaluate the
discriminative models produced in successive itera-
tions, as there are no standard data against which to
judge them (the new examples are by definition pre-
viously unexamined). We propose two measures to
alleviate these problems. First, we admit into the
training set only those instances whose activation
values are higher than the mean activation of their
corresponding categories in an iteration. This sets
a variable threshold that is correlated with the per-
formance of the model at each iteration. Second, we
evaluate iterative results post hoc, using a Bootstrap-
ping Score. This measures the efficacy of bootstrap-
ping (i.e. the ratio of correct newly discovered in-
stances to training examples) and precision (i.e. the
proportion of correct discoveries among all those re-
turned by the algorithm). We compute this score to
decide which iteration has yielded the optimal dis-
criminative model.
3 Building an Event-noun Lexicon
We applied the bootstrapping algorithm to the task
of discovering event nouns from a source lexicon.
38
Event nouns are words that typically describe the
occurrence, the process, or the result of an event.
We first explore the effectiveness of this algorithm,
and then describe a method of extracting the optimal
model. Top-ranked features in the optimal model are
used to find subcategories of event nouns.
Experimental Setup The WordNet noun-list is
chosen as the source lexicon (Fellbaum, 1998),
which consists of 21,512 nouns. The purpose of
this task is to explore the separability of event nouns
from this collection.
typical suffixes: appeasement, arrival, renewal,
construction, robbery, departure, happening
irregular cases: birth, collapse, crash, death, de-
cline, demise, loss, murder
Table 1: Examples of event-nouns in initial training set.
We manually selected 15 event nouns and 215
non-event nouns for the seed set. Event-noun exam-
ples are representative of subcategories within the
semantic class, as well as their commonly seen mor-
phological structures (Table 1). Non-event examples
are primarily exceptions to morphological regulari-
ties (to prevent the algorithm from overly relying on
affix features), such as, anything, ambition, diago-
nal. The subset of all contextual and morphological
features represented by both event and non-event ex-
amples are used to bootstrap the training process.
Event Noun Discovery Reducing the number of
working features is often an effective strategy in
training a perceptron. We experimented with two
cut-off thresholds for features: in Trial 1, features
must appear at least 10 times (55,354 remaining);
in Trial 2, features must appear at least 15 times
(35,457 remaining).
We set the training process to run for 20 iterations
in both trials. Classification results of each iteration
were collected. We expect the algorithm to discover
few event nouns during early iterations. But with
new instances found in each subsequent iteration,
it ought to utilize newly seen features and discover
more. Figure 1 confirms our intuition.
The number of classified event-noun instances in-
creased sharply at the 15th iteration in Trial 1 and the
11th iteration in Trial 2, which may suggest overfit-
ting of the training examples used in those iterations.
If so, this should also correlate with an increase of
error rate in the classification results (error rate de-
fined as the percentage of non-event nouns identi-
fied as event nouns in all discovered event nouns).
We manually marked all misclassified event noun in-
stances for the first 10 iterations in both trials. The
error rate in Trial 2 is expected to significantly in-
crease at the 10th iteration, while Trial 1 should ex-
hibit little increase in error rate within this interval.
This expectation is confirmed in Figure 2.
Extracting the Optimal Model We further pur-
sued the task of finding the iteration that has yielded
the best model. Optimality is judged from two as-
pects: 1) the number of correctly identified event
nouns should be significantly larger than the size of
seed examples; and 2) the accuracy of classification
results should be relatively high so that it takes lit-
tle effort to clean up the result. Once the optimal
model is determined, we analyze its most heavily
weighted features and try to derive finer categories
from them. Furthermore, the optimal model could
be used to discover new instances from other source
lexicons in the future.
We define a measure called the Bootstrapping
Score (BS), serving a similar purpose as an F-score.
BS is computed as in Formula (1).
BS = 2 ?BR ? PrecisionBR + Precision . (1)
Here the Bootstrapping Rate (BR) is computed as:
BR = |NEW ||NEW |+ |SEED| , (2)
where |NEW | is the number of correctly identi-
fied new instances (seed examples excluded), and
|SEED| is the size of seed examples. The rate
of bootstrapping reveals how large the effect of the
bootstrapping process is. Note that BR is different
from the classic measure recall, for which the total
number of relevent documents (i.e. true event nouns
in English) must be known a priori ? again, this
knowledge is what we are discovering. The score
is a post hoc solution; both BR and precision are
computed for analysis after the algorithm has fin-
ished. Combining Formulas (1) and (2), a higher
Bootstrapping Score means better model quality.
Bootstrapping scores of models in the first ten it-
erations are plotted in Figure 3. Model quality in
39
5 10 15 20
02
000
4000
6000
8000
10000
Iteration
Numb
er of E
vent N
ouns D
iscove
red
Trial 1Trial 2
Figure 1: Classification rate
2 4 6 8 100
.05
0.10
0.15
0.20
0.25
0.30
Iteration
Error r
ate
Trial 1Trial 2
Figure 2: Error rate
2 4 6 8 100
.82
0.84
0.86
0.88
0.90
0.92
0.94
Iteration
Boots
trappin
g Scor
e
Trial 1Trial 2
Figure 3: Bootstrapping score
1 . . . 6 . . . 10
incorrect 5 . . . 32 . . . 176
correct 79 . . . 236 . . . 497
error rate 5.9% . . . 11.9% . . . 26.2%
score 87.0% . . . 90.8% . . . 83.8%
Table 2: From iterations 1 to 10, comparison between
instance counts, error rates, and bootstrapping scores as
the measure of model quality.
Trial 2 is better than in Trial 1 on average. In ad-
dition, within Trial 2, Iteration 6 yielded the best
discriminative model with a bootstrapping score of
90.8%. Compared to instance counts and error rate
measures as shown in Table 2, this bootstrapping
score provides a balanced measure of model qual-
ity. The model at the 6th iteration (hereafter, Model
6) can be considered the optimal model generated
during the bootstrapping training process.
Top-ranked Features in the Optimal Model In
order to understand why Model 6 is optimal, we
extracted its top 15 features that activate the event-
noun target in Model 6, as listed in Table 3. Inter-
estingly, top-ranked features are all contextual ones.
In fact, in later models where the ranks of mor-
phological features are boosted, the algorithm per-
formed worse as a result of relying too much on
those context-insensitive features.
Collectively, top-ranked features define the con-
textual patterns of event nouns. We are interested
in finding semantic subcategories within the set of
event nouns (497 nouns, Trial 2) by exploiting these
features individually. For instance, some events typ-
ically happen to people only (e.g. birth, betrayal),
while others usually happen to inanimate objects
(e.g. destruction, removal). Human actions can also
be distinguished by the number of participants, such
as group activities (e.g. election) or individual ac-
tivities (e.g. death). It is thus worth distinguishing
nouns that describe different sorts of events.
Manual Classification We extracted the top 100
contextual features from Model 6 and grouped
them into feature classes. A feature class con-
sists of contextual features sharing similar mean-
ings. For instance, A COUNTRY MAY UNDERGO
and A STATE MAY UNDERGO both belong to the
class social activity. For each feature class, we enu-
merate all words that correspond to its feature in-
stances. Examples are shown in Table 4.
Not all events can be unambiguously classified
into one of the subcategories. However, this is also
not necessary because these categories overlap with
one another. For example, death describes an event
that tends to occur both individually and briefly. In
addition to the six categories listed here, new cate-
gories can be added by creating more feature classes.
Automatic Clustering Representing each noun as
a frequency vector over the top 100 most discrim-
inating contextual features, we employed k-means
clustering and compared the results to our manually
crafted subcategories.
Through trial-and-error, we set k to 12, with the
smallest resulting cluster containing 2 nouns (inter-
pretation and perception), while the biggest result-
ing cluster contained 320 event nouns (that seemed
to share no apparent semantic properties). Other
clusters varied from 5 to 50 words in size, with ex-
amples shown in Table 5.
The advantage of automatic clustering is that the
results may reflect an English speaker?s impression
of word similarity gained through language use. Un-
40
a person-or-organization may undergo a a state may undergo a a can be attempted
a country may undergo a a child may have a a can be for a country
a company may undergo a a project may undergo a authority may undergo a
an explanation can be for a an empire may undergo a a war may undergo a
days may have a a can be abrupt a can be rapid
Table 3: Top 15 features that promote activation of the event-noun target, ranked from most weighted to least.
human events: adoption, arrival, birth, betrayal,
death, development, disappearance, emancipation,
funeral . . .
events of inanimate objects: collapse, construc-
tion, definition, destruction, identification, incep-
tion, movement, recreation, removal . . .
individual activities: birth, death, execution, fu-
neral, promotion . . .
social activities: abolition, evolution, federation,
fragmentation, invasion . . .
lasting events: campaign, development, growth,
trial . . .
brief events: awakening, collapse, death, mention,
onset, resignation, thunderstorm . . .
Table 4: Six subcategories of event nouns.
fortunately, the discovered clusters do not typically
come with the same obvious semantic properties as
were defined in manual classification. In the exam-
ple given above, neither of Cluster 1 and Cluster 3
seems to have a centralized semantic theme. But
Cluster 2 seems to be mostly about human activities.
Comparison with WordNet To compare our re-
sults with WordNet resources, we enumerated all
children of the gloss ?something that happens at a
given place and time?, giving 7655 terms (phrases
excluded). This gave a broader range of event nouns,
such as proper nouns and procedures (e.g. 9/11, CT,
MRI), onomatopoeias (e.g. mew, moo), and words
whose event reading is only secondary (e.g. pic-
ture, politics, teamwork). These types of words tend
to have very different contextual features from what
our algorithm had discovered.
While our method may be limited by the choice of
seed examples, we were able to discover event nouns
not classified under this set by WordNet, suggest-
ing that the discovery mechanism itself is a robust
one. Among them were low-frequency nouns (e.g.
crescendo, demise, names of processes (e.g. absorp-
Cluster 1 (17): cancellation, cessation, closure,
crackle, crash, demise, disappearance, dismissal, dis-
solution, division, introduction, onset, passing, resig-
nation, reversal, termination, transformation
Cluster 2 (32): alienation, backing, betrayal, contem-
plation, election, execution, funeral, hallucination,
imitation, juxtaposition, killing, mention, moulding,
perfection, prosecution, recognition, refusal, removal,
resurrection, semblance, inspection, occupation, pro-
motion, trial . . .
Cluster 3 (7): development, achievement, arrival,
birth, death, loss, survival
Table 5: Examples resulting from automatic clustering.
tion, evolution), and particular cases like thunder-
storm.
4 Extension to Other Semantic Categories
To verify that our bootstrapping algorithm was not
simply relying on KNEXT?s own event classifica-
tion heuristics, we set the algorithm to learn the
distinction between physical and non-physical ob-
jects/entities.
(Non-)Physical-object Nouns 15 physical-object/
entity nouns (e.g. knife, ring, anthropologist) and
34 non-physical ones (e.g. happiness, knowledge)
were given to the model as the initial training set.
At the 9th iteration, the number of discovered physi-
cal objects (which form the minority group between
the two) approaches 2,200 and levels off. We ran-
domly sampled five 20-word groups (a subset of
these words are listed in Table 6) from this entire
set of discovered physical objects, and computed an
average error rate of 4%. Prominent features of the
model at the 9th iteration are shown in Table 7.
5 Related Work
The method of using distributional patterns in a
large text corpus to find semantically related En-
41
heifer, sheriff, collector, hippie, accountant, cape, scab,
pebble, box, dick, calculator, sago, brow, ship, ?john,
superstar, border, rabbit, poker, garter, grinder, million-
aire, ash, herdsman, ?cwm, pug, bra, fulmar, *cam-
paign, stallion, deserter, boot, tear, elbow, cavalry,
novel, cardigan, nutcase, ?bulge, businessman, cop, fig,
musician, spire, butcher, dog, elk, . . .
Table 6: Physical-object nouns randomly sampled from
results; words with an asterisk are misclassified, ones
with a question mark are doubtful.
a male-individual can be a a can be small
a person can be a a can be large
a can be young a can be german
-S*morphological feature a can be british
a can be old a can be good
Table 7: Top-10 features that promote activation of the
physical-object target in the model.
glish nouns first appeared in Hindle (1990). Roark
and Charniak (1998) constructed a semantic lexicon
using co-occurrence statistics of nouns within noun
phrases. More recently, Liakata and Pulman (2008)
induced a hierarchy over nominals using as features
knowledge fragments similar to the sort given by
KNEXT. Our work might be viewed as aiming for
the same goal (a lexico-semantic based partition-
ing over nominals, tied to corpus-based knowledge),
but allowing for an a priori bias regarding preferred
structure.
The idea of bootstrapping lexical semantic prop-
erties goes back at least to Hearst (1998), where the
idea is suggested of using seed examples of a rela-
tion to discover lexico-syntactic extraction patterns
and then using these to discover further examples
of the desired relation. The Basilisk system devel-
oped by Thelen and Riloff (2002) almost paralleled
our effort. However, negative features ? features
that would prevent a word from being classified into
a semantic category ? were not considered in their
model. In addition, in scoring candidate words, their
algorithm only looked at the average relevance of
syntactic patterns. Our perceptron-based algorithm
examines the combinatorial effect of those patterns,
which has yielded results suggesting improved ac-
curacy and bootstrapping efficacy.
Similar to our experiments here using k-means,
Lin and Pantel (2001) gave a clustering algorithm
for iteratively building semantic classes, using as
features argument positions within fragments from
a syntactic dependency parser.
6 Conclusion
We have presented a bootstrapping approach for cre-
ating semantically tagged lexicons. The method can
effectively classify nouns with contrasting semantic
properties, even when the initial training set is a very
small. Further classification is possible with both
manual and automatic methods by utilizing individ-
ual contextual features in the optimal model.
Acknowledgments
This work was supported by NSF grants IIS-
0328849 and IIS-0535105.
References
BNC Consortium. 2001. The British National Corpus,
version 2 (BNC World). Distributed by Oxford Uni-
versity Computing Services.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Lexical Database. MIT Press.
Marti A. Hearst. 1998. Automated discovery of Word-
Net relations. In (Fellbaum, 1998), pages 131?153.
Donald Hindle. 1990. Noun classification from
predicate-argument structures. In ACL.
Alexandre Klementiev and Dan Roth. 2006. Weakly
supervised named entity transliteration and discovery
from multilingual comparable corpora. In ACL.
Maria Liakata and Stephen Pulman. 2008. Auto-
matic Fine-Grained Semantic Classification for Do-
main Adaption. In Proceedings of Semantics in Text
Processing (STEP).
Dekang Lin and Patrick Pantel. 2001. Induction of se-
mantic classes from natural language text. In KDD.
Brian Roark and Eugene Charniak. 1998. Noun-phrase
co-occurrence statistics for semi-automatic semantic
lexicon construction. In ACL, pages 1110?1116.
Frank Rosenblatt. 1958. The perceptron: A probabilistic
model for information storage and organization in the
brain. Psychological Review, 65(6):386?408.
Lenhart K. Schubert. 2002. Can we derive general world
knowledge from text? In HLT.
Michael Thelen and Ellen Riloff. 2002. A bootstrapping
method for learning semantic lexicons using extraction
pattern contexts. In EMNLP.
Benjamin Van Durme, Ting Qian, and Lenhart Schubert.
2008. Class-driven Attribute Extraction. In COLING.
42
Extracting and Evaluating General World Knowledge from the Brown
Corpus
Lenhart Schubert
University of Rochester
schubert@cs.rochester.edu
Matthew Tong
University of Rochester
mt004i@mail.rochester.edu
Abstract
We have been developing techniques for ex-
tracting general world knowledge from miscel-
laneous texts by a process of approximate inter-
pretation and abstraction, focusing initially on
the Brown corpus. We apply interpretive rules
to clausal patterns and patterns of modifica-
tion, and concurrently abstract general ?possi-
bilistic? propositions from the resulting formu-
las. Two examples are ?A person may believe
a proposition?, and ?Children may live with
relatives?. Our methods currently yield over
117,000 such propositions (of variable quality)
for the Brown corpus (more than 2 per sen-
tence). We report here on our efforts to eval-
uate these results with a judging scheme aimed
at determining how many of these propositions
pass muster as ?reasonable general claims?
about the world in the opinion of human judges.
We find that nearly 60% of the extracted propo-
sitions are favorably judged according to our
scheme by any given judge. The percentage
unanimously judged to be reasonable claims by
multiple judges is lower, but still sufficiently
high to suggest that our techniques may be of
some use in tackling the long-standing ?knowl-
edge acquisition bottleneck? in AI.
1 Introduction: deriving general
knowledge from texts
We have been exploring a new method of gaining gen-
eral world knowledge from texts, including fiction. The
method does not depend on full or exact interpretation,
but rather tries to glean general facts from particulars by
combined processes of compositional interpretation and
abstraction. For example, consider a sentence such as the
following from the Brown corpus (Kucera and Francis,
1967):
Rilly or Glendora had entered her room while she
slept, bringing back her washed clothes.
From the clauses and patterns of modification of this sen-
tence, we can glean that an individual may enter a room, a
female individual may sleep, and clothes may be washed.
In fact, given the following Treebank bracketing, our pro-
grams produce the output shown:
((S
(NP
(NP (NNP Rilly) )
(CC or)
(NP (NNP Glendora) ))
(AUX (VBD had) )
(VP (VBN entered)
(NP (PRP\$ her) (NN room) ))
(SBAR (IN while)
(S
(NP (PRP she) )
(VP (VBD slept) )))
(\, \,)
(S
(NP (\-NONE\- \*) )
(VP (VBG bringing)
(PRT (RB back) )
(NP (PRP\$ her) (JJ washed) (NNS clothes) ))))
(\. \.) )
A NAMED-ENTITY MAY ENTER A ROOM.
A FEMALE-INDIVIDUAL MAY HAVE A ROOM.
A FEMALE-INDIVIDUAL MAY SLEEP.
A FEMALE-INDIVIDUAL MAY HAVE CLOTHES.
CLOTHES CAN BE WASHED.
((:I (:Q DET NAMED-ENTITY) ENTER[V] (:Q THE ROOM[N]))
(:I (:Q DET FEMALE-INDIVIDUAL) HAVE[V] (:Q DET ROOM[N]))
(:I (:Q DET FEMALE-INDIVIDUAL) SLEEP[V])
(:I (:Q DET FEMALE-INDIVIDUAL) HAVE[V]
(:Q DET (:F PLUR CLOTHE[N])))
(:I (:Q DET (:F PLUR CLOTHE[N])) WASHED[A]))
The results are produced as logical forms (the last five
lines above ? see Schubert, 2002, for some details), from
which the English glosses are generated automatically.
Our work so far has focused on data in the Penn Tree-
bank (Marcus et al, 1993), particularly the Brown corpus
and some examples from the Wall Street Journal corpus.
The advantage is that Treebank annotations allow us to
postpone the challenges of reasonably accurate parsing,
though we will soon be experimenting with ?industrial
strength? parsers on unannotated texts.
We reported some specifics of our approach and some
preliminary results in (Schubert, 2002). Since then we
have refined our extraction methods to the point where we
can reliably apply them the Treebank corpora, on average
extracting more than 2 generalized propositions per sen-
tence. Applying these methods to the Brown corpus, we
have extracted 137,510 propositions, of which 117,326
are distinct. Some additional miscellaneous examples
are ?A PERSON MAY BELIEVE A PROPOSITION?, ?BILLS
MAY BE APPROVED BY COMMITTEES?, ?A US-STATE MAY
HAVE HIGH SCHOOLS?, ?CHILDREN MAY LIVE WITH RELA-
TIVES?, ?A COMEDY MAY BE DELIGHTFUL?, ?A BOOK MAY
BE WRITE-ED (i.e., written) BY AN AGENT?, ?A FEMALE-
INDIVIDUAL MAY HAVE A SPOUSE?, ?AN ARTERY CAN BE
THICKENED?, ?A HOUSE MAY HAVE WINDOWS?, etc.
The programs that produce these results consist of (1) a
Treebank preprocessor that makes various modifications
to Treebank trees so as to facilitate the extraction of se-
mantic information (for instance, differentiating different
kinds of ?SBAR?, such as S-THAT and S-ALTHOUGH,
and identifying certain noun phrases and prepositional
phrases, such as ?next Friday?, as temporal); (2) a pat-
tern matcher that uses a type of regular-expression lan-
guage to identify particular kinds of phrase structure pat-
terns (e.g., verb + complement patterns, with possible in-
serted adverbials or other material); (3) a semantic pat-
tern extraction routine that associates particular semantic
patterns with particular phrase structure patterns and re-
cursively instantiates and collects such patterns for the
preprocessed tree, in bottom-up fashion; (4) abstraction
routines that abstract away modifiers and other ?type-
preserving operators?, before semantic patterns are con-
structed at the next-higher level in the tree (for instance,
stripping the interpreted modifier ?washed? from the in-
terpreted noun phrase ?her washed clothes?); (5) routines
for deriving propositional patterns from the resulting mis-
cellaneous semantic patterns, and rendering them in a
simple, approximate English form; and (6) heuristic rou-
tines for filtering out many ill-formed or vacuous propo-
sitions. In addition, semantic interpretation of individual
words involves some simple morphological analysis, for
instance to allow the interpretation of (VBD SLEPT) in
terms of a predicate SLEEP[V].
In (Schubert, 2002) we made some comparisons be-
tween our project and earlier work in knowledge extrac-
tion (e.g., (muc, 1993; muc, 1995; muc, 1998; Berland
and Charniak, 1999; Clark and Weir, 1999; Hearst, 1998;
Riloff and Jones, 1999)) and in discovery of selectional
preferences (e.g., (Agirre and Martinez, 2001; Grishman
and Sterling, 1992; Resnik, 1992; Resnik, 1993; Zernik,
1992; Zernik and Jacobs, 1990)). Reiterating briefly, we
note that knowledge extraction work has generally em-
ployed carefully tuned extraction patterns to locate and
extract some predetermined, specific kinds of facts; our
goal, instead, is to process every phrase and sentence that
is encountered, abstracting from it miscellaneous general
knowledge whenever possible. Methods for discovering
selectional preferences do seek out conventional patterns
of verb-argument combination, but tend to ?lose the con-
nection? between argument types (e.g., that a road may
carry traffic, a newspaper may carry a story, but a road is
unlikely to carry a story); in any event, they have not led
so far to amassment of data interpretable as general world
knowledge.
Our concern in this paper is with the evaluation of the
results we currently obtain for the Brown corpus. The
overall goal of this evaluation is to gain some idea of
what proportion of the extracted propositions are likely
to be credible as world knowledge. The ultimate test of
this will of course be systems (e.g., QA systems) that use
such extracted propositions as part of their knowledge
base, but such a test is not immediately feasible. In the
meantime it certainly seems worthwhile to evaluate the
outputs subjectively with multiple judges, to determine
if this approach holds any promise at all as a knowledge
acquisition technique.
In the following sections we describe the judging
method we have developed, and two experiments based
on this method, one aimed at determining whether ?lit-
erary style makes a difference? to the quality of outputs
obtained, and one aimed at assessing the overall success
rate of the extraction method, in the estimation of several
judges.
2 Judging the output propositions
We have created judging software that can be used by the
researchers and other judges to assess the quality and cor-
rectness of the extracted information. The current scheme
evolved from a series of trial versions, starting initially
with a 3-tiered judging scheme, but this turned out to
be difficult to use, and yielded poor inter-judge agree-
ment. We ultimately converged on a simplified scheme,
for which ease of use and inter-judge agreement are sig-
nificantly better.
The following are the instructions to a judge using the
judger program in its current form:
Welcome to the sentence evaluator for the KNEXT knowledge ex-
traction program. Thank you for your participation. You will be
asked to evaluate a series of sentences based on such criteria as
comprehensibility and truth. Do your best to give accurate re-
sponses. The judgement categories are selected to try to ensure
that each sentence fits best in one and only one category. Help is
available for each menu item, along with example sentences, by
selecting ?h?; PLEASE consult this if this is your first time using
this program even if you feel confident of your choice. There is
also a tutorial available, which should also be done if this is your
first time. If you find it hard to make a choice for a particular sen-
tence even after carefully considering the alternatives, you should
probably choose 6 (HARD TO JUDGE)! But if you strongly feel
none of the choices fit a sentence, even after consulting the help
file, please notify Matthew Tong (mtong@cs.rochester.edu) to al-
low necessary modifications to the menus or available help infor-
mation to occur. You may quit at any time by typing ?q?; if you quit
partway through the judgement of a sentence, that partial judge-
ment will be discarded, so the best time to quit is right after being
presented with a new sentence.
  here the first sentence to be judged is presented 
1. SEEMS LIKE A REASONABLE GENERAL CLAIM (Of course. Yes.)
A grand-jury may say a proposition. A report can be favorable.
2. SEEMS REASONABLE BUT EXTREMELY SPECIFIC OR OBSCURE
(I suppose so)
A surgeon may carry a cage. Gladiator pecs can be Reeves-type.
3. SEEMS VACUOUS (That?s not saying anything)
A thing can be a hen. A skiff can be nearest.
4. SEEMS FALSE (No. I don?t think so. Hardly)
A square can be round. Individual -s may have a world.
5. SOMETHING IS OBVIOUSLY MISSING (Give me a complete sentence)
A person may ask. A male-individual may attach an importance.
6. HARD TO JUDGE (Huh?? How do you mean that? I don?t know.)
A female-individual can be psychic. Supervision can be with a company.
Based on this judging scheme, we performed two types
of experiments: an experiment to determine whether lit-
erary style significantly impacts the percentage of propo-
sitions judged favorably; and experiments to assess over-
all success rate, in the judgement of multiple judges. We
obtained clear evidence that literary style matters, and
achieved a moderately high success rate ? but certainly
sufficiently high to assure us that large numbers of po-
tentially useful propositions are extracted by our meth-
ods. The judging consistency remains rather low, but
this does not invalidate our approach. In the worst case,
hand-screening of output propositions by multiple judges
could be used to reject propositions of doubtful validity or
value. But of course we are very interested in developing
less labor-intensive alternatives. The following subsec-
tions provide some details.
2.1 Dependence of extracted propositions on
literary style
The question this experiment addressed was whether dif-
ferent literary styles correlated with different degrees of
success in extracting intuitively reasonable propositions.
The experiment was carried out twice, first by one of the
authors (who was unaware of the contents of the files be-
ing sampled) and the second time by an outside recruit.
While further experimentation is desirable, we believe
that the evidence from two judges that literary style corre-
lates with substantial differences in the perceived quality
of extracted propositions demonstrates that future work
on larger corpora should control the materials used for
literary style.
Judgements were based on 4 Brown files (ck01, ck13,
cd02, cd01). The 4 files were chosen by one of us on
purely subjective grounds. Each contains about 2,200
words of text. (Our extraction methods yield about 1
proposition for every 8 words of text. So each file yields
about 250-300 propositions.) The first two, ck01 and
ck13, are straightforward, realistic narratives in plain, un-
adorned English, while cd01 and cd02 are philosophical
and theological essays employing much abstract and fig-
urative language. The expectation was that the first two
texts would yield significantly more propositions judged
to be reasonable general claims about the world than the
latter two. To give some sense of the contents, the first
few sentences from each of the texts are extracted here:
Initial segments of each of the four texts
ck01: Scotty did not go back to school. His parents talked seri-
ously and lengthily to their own doctor and to a specialist
at the University Hospital? Mr. McKinley was entitled
to a discount for members of his family? and it was de-
cided it would be best for him to take the remainder of the
term off, spend a lot of time in bed and, for the rest, do
pretty much as he chose? provided, of course, he chose
to do nothing too exciting or too debilitating. His teacher
and his school principal were conferred with and everyone
agreed that, if he kept up with a certain amount of work at
home, there was little danger of his losing a term.
ck13: In the dim underwater light they dressed and straight-
ened up the room, and then they went across the hall to
the kitchen. She was intimidated by the stove. He found
the pilot light and turned on one of the burners for her. The
gas flamed up two inches high. They found the teakettle.
And put water on to boil and then searched through the
icebox.
cd01: As a result, although we still make use of this distinction,
there is much confusion as to the meaning of the basic
terms employed. Just what is meant by ?spirit? and by
?matter??? The terms are generally taken for granted as
though they referred to direct and axiomatic elements in
the common experience of all. Yet in the contemporary
context this is precisely what one must not do. For in the
modern world neither ?spirit? nor ?matter? refer to any
generally agreed-upon elements of experience...
cd02: If the content of faith is to be presented today in a form
that can be ?understanded of the people?? and this, it must
not be forgotten, is one of the goals of the perennial the-
ological task? there is no other choice but to abandon
completely a mythological manner of representation. This
does not mean that mythological language as such can no
longer be used in theology and preaching. The absurd
notion that demythologization entails the expurgation of
all mythological concepts completely misrepresents Bult-
mann?s intention.
Extracted propositions were uniformly sampled from
the 4 files, for a total count of 400, and the number of
judgements in each judgement category were then sep-
arated out for the four files. In a preliminary version of
this experiment, the judgement categories were still the 3-
level hierarchical ones we eventually dropped in favor of
a 6-alternatives scheme. Still, the results clearly indicated
that the ?plain? texts yielded significantly more propo-
sitions judged to be reasonable claims than the more
abstract texts. Two repetitions of the experiment (with
newly sampled propositions from the 4 files) using the 6-
category judging scheme, and the heuristic postprocess-
ing and filtering routines, yielded the following unequiv-
ocal results. (The exact sizes of the samples from files
ck01, ck13, cd01, and cd02 in both repetitions were 120,
98, 85, and 97 respectively, where the relatively high
count for ck01 reflects the relatively high count of ex-
tracted propositions for that text.)
  For ck01 and ck13 around 73% of the propositions
(159/218 for judge 1 and 162/218 for judge 2) were
judged to be in the ?reasonable general claim? cat-
egory; for cd01 and cd02, the figures were much
lower, at 41% (35/85 for judge 1 and 40/85 for judge
2) and less than 55% (53/97 for judge 1 and 47/97
for judge 2) respectively.
  For ck01 and ck13 the counts in the ?hard to judge?
category were 12.5-15% (15-18/120) and 7.1-8.2%
(6-7/85) respectively, while for cd01 and cd02 the
figures were substantially higher, viz., 25.9-28.2%
(22-24/85) and 19.6-23% (19-34/97) respectively.
Thus, as one would expect, simple narrative texts
yield more propositions recognized as reasonable claims
about the world (nearly 3 out of 4) than abstruse an-
alytical materials (around 1 out of 2). The question
then is then how to control for style when we turn
our methods to larger corpora. One obvious answer
is to hand-select texts in relevant categories, such as
literature for young readers, or from authors whose
writings are realistic and stylistically simple (e.g., Hem-
ingway). However, this could be quite laborious since
large literary collections available online (such as the
works in Project Gutenberg, http://promo.net/pg/,
http://www.thalasson.com/gtn/, with expired
copyrights) are not sorted by style. Thus we expect to use
automated style analysis methods, taking account of such
factors as vocabulary (checking for esoteric vocabulary
and vocabulary indicative of fairy tales and other fanciful
fiction), tense (analytical material is often in present
tense), etc. We may also turn our knowledge extraction
methods themselves to the task: if, for instance, we find
propositions about animals talking, it may be best to skip
the text source altogether.
2.2 Overall quality of extracted propositions
To assess the quality of extracted propositions over a wide
variety of Brown corpus texts, with judgements made by
multiple judges, the authors and three other individuals
made judgements on the same set of 250 extracted propo-
sitions. The propositions were extracted from the third
of the Brown corpus (186 files) that had been annotated
with WordNet senses in the SEMCOR project (Landes et
al., 1998) (chiefly because those were the files at hand
when we started the experiment ? but they do represent a
broad cross-section of the Brown Corpus materials). We
excluded the cj-files, which contain highly technical ma-
terial.
Table 1 shows the judgements of the 5 judges (as per-
centages of counts out of 250) in each of the six judge-
ment categories. The category descriptions have been
mnemonically abbreviated at the top of the table. Judge
1 appears twice, and this represents a repetition, as a test
of self-consistency, of judgements on the same data pre-
sented in different randomized orderings.
reasonable obscure vacuous false incomplete hard
9.6 0.4 7.6 12.8
9.6 0.4 7.2 11.6
54.8 14.8 5.6 8.8 5.2
6.4 3.2 7.6
8.4 4.8
60.0 9.6
61.6
49.0
10.4
9.2
2.8
8.4
12.4
10.0
22.5
judge 1
judge 1
judge 2
judge 4 64.0
judge 5
judge 3
58.4 4.4 0.8 2.8 10.0 23.2
Judgements (in %) for 250 randomly sampled propositionsTable 1.
As can be seen from the first column, the judges placed
about 49-64% of the propositions in the ?reasonable gen-
eral claim? category. This result is consistent with the re-
sults of the style-dependency study described above, i.e.,
the average lies between the ones for ?straightforward?
narratives (which was nearly 3 out of 4) and the ones for
abstruse texts (which was around 1 out of 2). This is an
encouraging result, suggesting that mining general world
knowledge from texts can indeed be productive.
One point to note is that the second and third judge-
ment categories need not be taken as an indictment of the
propositions falling under them ? while we wanted to dis-
tinguish overly specific, obscure, or vacuous propositions
from ones that seem potentially useful, such propositions
would not corrupt a knowledge base in the way the other
categories would (false, incomplete, or incoherent propo-
sitions). Therefore, we have also collapsed our data into
three more inclusive categories, namely ?true? (collaps-
ing the first 3 categories), ?false? (same as the original
?false? category), and ?undecidable? (collapsing the last
two categories). The corresponding variant of Table 1
would thus be obtained by summing the first 3 and last
2 columns. We won?t do so explicitly, but it is easy to
verify that the proportion of ?true? judgements comprise
about three out of four judgements, when averaged over
the 5 judges.
We now turn to the extent of agreement among the
judgements of the five judges (and judge 1 with himself
on the same data). The overall pairwise agreement results
for classification into six judgement catagories are shown
in Table 2.
judge 1
90.1
56.9 10.4
61.7 62.4
57.358.5
judge 1
judge 2
judge 3
judge 4
judge 5
54.5
56.0 49.3
judge 2 judge 3 judge 4
Table 2. Overall % agreement among judges
for 250 propositions
60.1
A commonly used metric for evaluating interrater relia-
bility in categorization of data is the kappa statistic (Car-
letta, 1996). As a concession to the popularity of that
statistic, we compute it in a few different ways here,
though ? as we will explain ? we do not consider it par-
ticularly appropriate. For 6 judgement categories, kappa
computed in the conventional way for pairs of judges
ranges from .195 to .367, averaging .306. For 3 (more in-
clusive) judgement categories, the pairwise kappa scores
range from .303 to .462, with an average of .375.
These scores, though certainly indicating a positive
correlation between the assessments of multiple judges,
are well below the lower threshold of .67 often employed
in deciding whether judgements are sufficiently consis-
tent across judges to be useful. However, to see that there
is a problem with applying the conventional statistic here,
imagine that we could improve our extraction methods to
the point where 99% of extracted propositions are judged
by miscellaneous judges to be reasonable general claims.
This would be success beyond our wildest dreams ? yet
the kappa statistic might well be 0 (the worst possible
score), if the judges generally reject a different one out of
every one hundred propositions!
One somewhat open-ended aspect of the kappa statistic
is the way ?expected? agreement is calculated. In the con-
ventional calculation (employed above), this is based on
the observed average frequency in each judgement cate-
gory. This leads to low scores when one category is over-
whelmingly favored by all judges, but the exceptions to
the favored judgement vary randomly among judges (as
in the hypothetical situation just described). A possible
way to remedy this problem is to use a uniform distri-
bution over judgement categories to compute expected
agreement. Under such an assumption, our kappa scores
are significantly better: for 6 categories, they range from
.366 to .549, averaging .482; for 3 categories, they range
from .556 to .730, averaging .645. This approaches, and
for several pairs of judges exceeds, the minimum thresh-
old for significance of the judgements.1
Since the ideal result, as implied above, would be
agreement by multiple judges on the ?reasonableness? or
truth of a large proportion of extracted propositions, it
seems worthwhile to measure the extent of such agree-
ment as well. Therefore we have also computed the
?survival rates? of extracted propositions, when we re-
ject those not judged to be reasonable general claims by
  judges (or, in the case of 3 categories, not judged to be
true by   judges). Figure 1 shows the results, where the
survival rate for   judges is averaged over all subsets of
size   of the 5 available judges.
1 2 3 4 5
.6
.8
.4
.2
0
1
Fraction of survivors
number of  concurring  judges
"true" (3 categories)
(6 categories)
"reasonable general claim"
by multiple judges
.75
.65
.31
.28
.35
.43
.57 .59
.52
.55
 Fraction of propositions placed in best categoryFigure 1.
Thus we find that the survival rate for ?reasonable gen-
eral claims? starts off at 57%, drops to 43% and then 35%
for 2 and 3 judges, and drops further to 31% and 28% for
4 and 5 judges. It appears as if an asymptotic level above
20% might be reached. But this may be an unrealistic
extrapolation, since virtually any proposition, no matter
how impeccable from a knowledge engineering perspec-
tive, might eventually be relegated to one of the other 5
categories by some uninvolved judge. The survival rates
based on 2 or 3 judges seem to us more indicative of the
likely proportion of (eventually) useful propositions than
an extrapolation to infinitely many judges. For the 3-way
judgements, we see that 75% of extracted propositions
are judged ?true? by individual judges (as noted earlier),
and this drops to 65% and then 59% for 2 and 3 judges.
Though again sufficiently many judges may eventually
bring this down to 40% or less, the survival rate is cer-
tainly high enough to support the claim that our method
of deriving propositions from texts can potentially deliver
very large amounts of world knowledge.
1The fact that for some pairs of judges the kappa-agreement
(with this version of kappa) exceeds 0.7 indicates that with more
careful training of judges significant levels of agreement could
be reached consistently.
3 Conclusions and further work
We now know that large numbers of intuitively reason-
able general propositions can be extracted from a cor-
pus that has been bracketed in the manner of the Penn
Treebank. The number of ?surviving? propositions for
the Brown corpus, based on the judgements of multiple
judges, is certainly in the tens of thousands, and the dupli-
cation rate is a rather small fraction of the overall number
(about 15%).
Of course, there is the problem of screening out, as
far as possible, the not-so-reasonable propositions. One
step strongly indicated by our experiment on the effect of
style is to restrict extraction to the kinds of texts that yield
higher success rates ? namely those written in straightfor-
ward, unadorned language. As we indicated, both style
analysis techniques and our own proposition extraction
methods could be used to select stylistically suitable ma-
terials from large online corpora.
Even so, a significant residual error rate will remain.
There are two remedies ? a short-term, brute-force rem-
edy, and a longer-term computational remedy. The brute-
force remedy would be to hand-select acceptable propo-
sitions. This would be tedious work, but it would still
be far less arduous than ?dreaming up? such proposi-
tions; besides, most of the propositions are of a sort one
would not readily come up with spontaneously (?A per-
son may paint a porch?, ?A person may plan an attack?,
?A house may have a slate roof?, ?Superstition may blend
with fact?, ?Evidence of shenanigans may be gathered by
a tape recorder?, etc.)
The longer-term computational remedy is to use a
well-founded parser and grammar, providing syntactic
analyses better suited to semantic interpretation than
Treebank trees. Our original motivation for using the
Penn Treebank, apart from the fact that it instantly pro-
vides a large number of parsed sentences from miscella-
neous genres, was to determine how readily such parses
might permit semantic interpretation. The Penn Tree-
bank pays little heed to many of the structural principles
and features that have preoccupied linguists for decades.
Would these turn out to be largely irrelevant to seman-
tics? We were actually rather pessimistic about this,
since the Treebank data tacitly posit tens of thousands of
phrase structure rules with inflated, heterogeneous right-
hand sides, and phrase classifications are very coarse (no-
tably, with no distinctions between adjuncts and com-
plements, and with many clause-like constructs, whether
infinitives, subordinate clauses, clausal adverbials, nom-
inalized questions, etc., lumped together as ?SBAR? ?
and these are surely semantically crucial distinctions). So
we are actually surprised at our degree of success in ex-
tracting sensible general propositions on the basis of such
rough-and-ready syntactic annotations.
Nonetheless, our extracted propositions in the ?some-
thing missing? and ?hard to judge? categories do quite
often reflect the limitations of the Treebank analyses. For
example, the incompleteness of the proposition ?A male-
individual may attach an importance? seen above as an il-
lustration of judgement category 5 can be attributed to the
lack of any indication that the PP[to] constituent of the
verb phrase in the source sentence is a verb complement
rather than an adjunct. Though our heuristics try to sort
out complements from adjuncts, they cannot fully make
up for the shortcomings of the Treebank annotations. It
therefore seems clear that we will ultimately need to base
knowledge extraction on more adequate syntactic analy-
ses than those provided by the Brown annotations.
Another general conclusion concerns the ease or dif-
ficulty of broad-coverage semantic interpretation. Even
though our interpretive goals up to this point have been
rather modest, our success in providing rough semantic
rules for much of the Brown corpus suggests to us that
full, broad-coverage semantic interpretation is not very
far out of reach. The reason for optimism lies in the ?sys-
tematicity? of interpretation. There is no need to hand-
construct semantic rules for each and every phrase struc-
ture rule. We were able provide reasonably comprehen-
sive semantic coverage of the many thousands of distinct
phrase types in Brown with just 80 regular-expression
patterns (each aimed at a class of related phrase types)
and corresponding semantic rules. Although our seman-
tic rules do omit some constituents (such as prenominal
participles, non-initial conjuncts in coordination, adver-
bials injected into the complement structure of a verb,
etc.) and gloss over subtleties involving gaps (traces),
comparatives, ellipsis, presupposition, etc., they are not
radical simplifications of what would be required for full
interpretation. The simplicity of our outputs is due not so
much to oversimplification of the semantic rules, as to the
deliberate abstraction and culling of information that we
perform in extracting general propositions from a specific
sentence. Of course, what we mean here by semantic in-
terpretation is just a mapping to logical form. Our project
sheds no light on the larger issues in text understanding
such as referent determination, temporal analysis, infer-
ence of causes, intentions and rhetorical relations, and
so on. It was the relative independence of the kind of
knowledge we are extracting of these issues that made
our project attractive and feasible in the first place.
Among the miscellaneous improvements under consid-
eration are the use of lexical distinctions and WordNet
abstraction to arrive at more reliable interpretations; the
use of modules to determine the types of neuter pronouns
and of traces (e.g., in ?She looked in the cookie jar, but it
was empty?, we should be able to abstract the proposition
that a cookie jar may be empty, using the referent of ?it?);
and extracting properties of events by making use of in-
formation in adverbials (e.g.,, from ?He slept soundly?
we should be able to abstract the proposition that sleep
may be sound; also many causal propositions can be in-
ferred from adverbial constructions). We also hope to
demonstrate extraction results through knowledge elici-
taton questions (e.g.,?What do you know about books??,
etc.)
4 Acknowledgements
The authors are grateful to David Ahn for contributing
ideas and for extensive help in preparing and processing
Brown corpus files, conducting some of the reported ex-
periments, and performing some differential analyses of
results. We also benefited from the discussions and ideas
contributed by Greg Carlson and Henry Kyburg in the
context of our ?Knowledge Mining? group, and we ap-
preciate the participation of group members and outside
recruits in the judging experiments. As well, we thank
Peter Clark and Phil Harrison (at Boeing Company) for
their interest and suggestions. This work was supported
by the National Science Foundation under Grant No. IIS-
0082928.
References
Eneko Agirre and David Martinez. 2001. Learning
class-to-class selectional preferences. In Proc. of the
5th Workshop on Computational Language Learning
(CoNLL-2001), Toulouse, France, July 6-7,.
Matthew Berland and Eugene Charniak. 1999. Find-
ing parts in very large corpora. In Proc. of the 37th
Ann. Meet. of the Assoc. for Computational Linguistics
(ACL-99), Univ. of Maryland, June 22 - 27,.
Jean Carletta. 1996. Assessing agreement on classifica-
tion tasks: the kappa statistic. Computational Linguis-
tics, 22(2):249?254.
Stephen Clark and David Weir. 1999. An iterative
approach to estimating frequencies over a semantic
hierarchy. In Proc. of the Joint SIGDAT Confer-
ence on Empirical Methods in Natural Language Pro-
cessing and Very Large Corpora. Also available at
http://www.cogs.susx.ac.uk/users/davidw/research/
papers.html.
Ralph Grishman and John Sterling. 1992. Acquisition
of selectional patterns. In Proc. of COLING-92, pages
658?664, Nantes, France.
Marti A. Hearst. 1998. Automated discovery of Word-
Net relations. In Christiane Fellbaum, editor, Word-
Net: An Electronic Lexical Database, pages 131?153?
MIT Press.
H. Kucera and W.N. Francis. 1967. Computational Anal-
ysis of Present-Day American English. Brown Univer-
sity Press, Providence, RI.
Shari Landes, Claudia Leacock, and Randee I. Tengi.
1998. Building semantic concordances. In Chris-
tiane Fellbaum, editor, WordNet: An Electronic Lexi-
cal Database, pages chapter 8, 199?216. MIT Press,
Cambridge, MA.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19(2):313?330, June.
1993. Proc. of the 5th Message Understanding Confer-
ence (MUC-5). Morgan Kaufmann, Los Altos, CA.
1995. Proc. of the 6th Message Understanding Confer-
ence (MUC-6). Morgan Kaufmann, Los Altos, CA.
1998. Proc. of the 7th Message Understanding Confer-
ence (MUC-7). Morgan Kaufmann, Los Altos, CA,
April 29 ? May 1, Virginia.
P. Resnik. 1992. A class-based approach to lexical dis-
covery. In Proc. of the 30th Ann. Meet. of the Assoc.
for Computational Linguistics (ACL-92), pages 327?
329, Newark, DE.
P. Resnik. 1993. Semantic classes and syntactic ambigu-
ity. In Proc. of ARPA Workshop on Human Language
Technology, Plainsboro, NJ.
Ellen Riloff and Rosie Jones. 1999. Learning dictio-
naries for information extraction by multi-level boot-
strapping. In Proc. of the 16th Nat. Conf. on Artificial
Intelligence (AAAI-99).
Lenhart K. Schubert. 2002. Can we derive general world
knowledge from texts? In Proc. of 2nd Int. Conf. on
Human Language Technology Research (HLT 2002),
pages 94?97, San Diego, CA, March 24-27.
Uri Zernik and Paul Jacobs. 1990. Tagging for learning:
Collecting thematic relations from corpus. In Proc.
of the 13th Int. Conf. on Computational Linguistics
(COLING-90), pages 34?39, Helsinki.
Uri Zernik. 1992. Closed yesterday and closed minds:
Asking the right questions of the corpus to distin-
guish thematic from sentential relations. In Proc. of
COLING-92, pages 1304?1311, Nantes, France, Aug.
23-28,.
Open Knowledge Extraction
through Compositional
Language Processing
Benjamin Van Durme
Lenhart Schubert
University of Rochester (USA)
email: vandurme@cs.rochester.edu
Abstract
We present results for a system designed to perform Open Knowledge
Extraction, based on a tradition of compositional language processing,
as applied to a large collection of text derived from the Web. Evaluation
through manual assessment shows that well-formed propositions of rea-
sonable quality, representing general world knowledge, given in a logical
form potentially usable for inference, may be extracted in high volume
from arbitrary input sentences. We compare these results with those ob-
tained in recent work on Open Information Extraction, indicating with
some examples the quite different kinds of output obtained by the two
approaches. Finally, we observe that portions of the extracted knowledge
are comparable to results of recent work on class attribute extraction.
239
240 Van Durme and Schubert
1 Introduction
Several early studies in large-scale text processing (Liakata and Pulman, 2002; Gildea
and Palmer, 2002; Schubert, 2002) showed that having access to a sentence?s syn-
tax enabled credible, automated semantic analysis. These studies suggest that the
use of increasingly sophisticated linguistic analysis tools could enable an explosion
in available symbolic knowledge. Nonetheless, much of the subsequent work in ex-
traction has remained averse to the use of the linguistic deep structure of text; this
decision is typically justified by a desire to keep the extraction system as computa-
tionally lightweight as possible.
The acquisition of background knowledge is not an activity that needs to occur
online; we argue that as long as the extractor will finish in a reasonable period of
time, the speed of such a system is an issue of secondary importance. Accuracy and
usefulness of knowledge should be of paramount concern, especially as the increase
in available computational power makes such ?heavy? processing less of an issue.
The system explored in this paper is designed for Open Knowledge Extraction: the
conversion of arbitrary input sentences into general world knowledge represented in a
logical form possibly usable for inference. Results show the feasibility of extraction
via the use of sophisticated natural language processing as applied to web texts.
2 Previous Work
Given that the concern here is with open knowledge extraction, the myriad projects
that target a few prespecified types of relations occurring in a large corpus are set
aside.
Among early efforts, one might count work on deriving selectional preferences
(e.g., Zernik (1992); Resnik (1993); Clark and Weir (1999)) or partial predicate-
argument structure (e.g., Abney (1996)) as steps in the direction of open knowledge
extraction, though typically few of the tuples obtained (often a type of subject plus a
verb, or a verb plus a type of object) can be interpreted as complete items of world
knowledge. Another somewhat relevant line of research was initiated by Zelle and
Mooney (1996), concerned with learning to map NL database queries into formal DB
queries (a kind of semantic interpretation). This was pursued further, for instance,
by Zettlemoyer and Collins (2005) and Wong and Mooney (2007), aimed at learning
log-linear models, or (in the latter case) synchronous CF grammars augmented with
lambda operators, for mapping English queries to DB queries. However, this approach
requires annotation of texts with logical forms, and extending this approach to gen-
eral texts would seemingly require a massive corpus of hand-annotated text ? and the
logical forms would have to cover far more phenomena than are found in DB queries
(e.g., attitudes, generalized quantifiers, etc.).
Another line of relevant work is that on semantic role labelling. One early example
was MindNet (Richardson et al, 1998), which was based on collecting 24 semantic
role relations from MRDs such as the American Heritage Dictionary. More recent
representative efforts includes that of Gildea and Jurafsky (2002), Gildea and Palmer
(2002), and Punyakanok et al (2008). The relevance of this work comes from the fact
that identifying the arguments of the verbs in a sentence is a first step towards forming
predications, and these may in many cases correspond to items of world knowledge.
Open Knowledge Extraction through Compositional Language Processing 241
Liakata and Pulman (2002) built a system for recovering Davidsonian predicate-
argument structures from the Penn Treebank through the application of a small set
of syntactic templates targeting head nodes of verb arguments. The authors illustrate
their results for the sentence ?Apple II owners, for example, had to use their television
sets as screens and stored data on audiocassettes? (along with the Treebank anno-
tations); they obtain the following QLF, where verb stems serve as predicates, and
arguments are represented by the head words of the source phrases:
have(e1,owner, (use(e3,owner,set), and as(e3,screen)),
and (store(e2,owner,datum), and on(e2,audiocassette)))
For a test set of 100 Treebank sentences, the authors report recall figures for vari-
ous aspects of such QLFs ranging from 87% to 96%. While a QLF like the one above
cannot in itself be regarded as world knowledge, one can readily imagine postprocess-
ing steps that could in many cases obtain credible propositions from such QLFs. How
accurate the results would be with machine-parsed sentences is at this point unknown.
In the same year, Schubert (2002) described a project aimed directly at the extrac-
tion of general world knowledge from Treebank text, and Schubert and Tong (2003)
provided the results of hand-assessment of the resulting propositions. The Brown cor-
pus yielded about 117,000 distinct simple propositions (somewhat more than 2 per
sentence, of variable quality). Like Liakata and Pulman?s approach the method relied
on the computation of unscoped logical forms from Treebank trees, but it abstracted
propositional information along the way, typically discarding modifiers at deeper lev-
els from LFs at higher levels, and also replacing NPs (including named entities) by
their types as far as possible. Judges found about 2/3 of the output propositions (when
automatically verbalized in English) acceptable as general claims about the world. The
next section provides more detail on the extraction system, called KNEXT, employed
in this work.
Clark et al (2003), citing the 2002 work of Schubert, report undertaking a sim-
ilar extraction effort for the 2003 Reuters corpus, based on parses produced by the
Boeing parser, (see Holmback et al (2000)), and obtained 1.1 million subject-verb-
object fragments. Their goal was eventually to employ such tuples as common-sense
expectations to guide the interpretation of text and the retrieval of possibly relevant
knowledge in question-answering. This goal, unlike the goal of inferential use of ex-
tracted knowledge, does not necessarily require the extracted information to be in the
form of logical propositions. Still, since many of their tuples were in a form that could
be quite directly converted into propositional forms similar to those of Schubert, their
work indicated the potential for scalability in parser-based approaches to information
extraction or knowledge extraction.
A recent project aimed at large-scale, open extraction of tuples of text fragments
representing verbal predicates and their arguments is TextRunner (Banko et al, 2007).
This systems does part-of-speech tagging of a corpus, identifies noun phrases with a
noun phrase chunker, and then uses tuples of nearby noun phrases within sentences to
form apparent relations, using intervening material to represent the relation. Apparent
modifiers such as prepositional phrases after a noun or adverbs are dropped. Every
candidate relational tuple is classified as trustworthy (or not) by a Bayesian classifier,
using such features as parts of speech, number of relevant words between the noun
242 Van Durme and Schubert
phrases, etc. The Bayesian classifier is obtained through training on a parsed corpus,
where a set of heuristic rules determine the trustworthiness of apparent relations be-
tween noun phrases in that corpus. As a preview of an example we will discuss later,
here are two relational tuples in the format extracted by TextRunner:1
(the people) use (force),
(the people) use (force) to impose (a government).
No attempt is made to convert text fragments such as ?the people? or ?use _ to impose?
into logically formal terms or predicates. Thus much like semantic role-labelling sys-
tems, TextRunner is an information extraction system, under the terminology used
here; however, it comes closer to knowledge extraction than the former, in that it often
strips away much of the modifying information of complex terms (e.g., leaving just a
head noun phrase).
2.1 KNEXT
KNEXT (Schubert, 2002) was originally designed for application to collections of
manually annotated parse trees, such as the Brown corpus. In order to extract knowl-
edge from larger text collections, the system has been extended for processing arbi-
trary text through the use of third-party parsers. In addition, numerous improvements
have been made to the semantic interpretation rules, the filtering techniques, and other
components of the system. The extraction procedure is as follows:
1. Parse each sentence using a Treebank-trained parser (Collins, 1997; Charniak,
1999).
2. Preprocess the parse tree, for better interpretability (e.g., distinguish different
types of SBAR phrases and different types of PPs, identify temporal phrases,
etc.).
3. Apply a set of 80 interpretive rules for computing unscoped logical forms (ULFs)
of the sentence and all lower-level constituents in a bottom-up sweep; at the
same time, abstract and collect phrasal logical forms that promise to yield
stand-alone propositions (e.g., ULFs of clauses and of pre- or post-modified
nominals are prime candidates). The ULFs are rendered in Episodic Logic
(e.g., (Schubert and Hwang, 2000)), a highly expressive representation allowing
for generalized quantifiers, predicate modifiers, predicate and sentence reifica-
tion operators, and other devices found in NL. The abstraction process drops
modifiers present in lower-level ULFs (e.g., adjectival premodifiers of nominal
predicates) in constructing higher-level ULFs (e.g., for clauses). In addition,
named entities are generalized as far as possible using several gazetteers (e.g.,
for male and female given names, US states, world cities, actors, etc.) and some
morphological processing.
4. Construct complete sentential ULFs from the phrasal ULFs collected in the pre-
vious step; here some filtering is performed to exclude vacuous or ill-formed
results.
1Boldface indicates items recognized as head nouns.
Open Knowledge Extraction through Compositional Language Processing 243
5. Render the propositions from the previous step in (approximate) English; again
significant heuristic filtering is done here.
As an example of KNEXT output, the sentence:
Cock fights, however, are still legal in six of the United States, perhaps
because we still eat chicken regularly, but no-longer dogs.
yields a pair of propositions expressed logically as:
[(K (NN cock.n (PLUR fight.n))) legal.a],
[(DET (PLUR person.n)) eat.v (K chicken.n)]
and these are automatically rendered in approximate English as:
COCK FIGHTS CAN BE LEGAL.
PERSONS MAY EAT CHICKEN.
As can be seen, KNEXT output does not conform to the ?relation, arg1, arg2, ...?, tuple
style of knowledge representation favored in information extraction (stemming from
that community?s roots in populating DB tables under a fixed schema). This is further
exemplified by the unscoped logical form:2
[(DET (PLUR person.n)) want.v (Ka (rid.a (of.p (DET dictator.n))))]
which is verbalized as PERSONS MAY WANT TO BE RID OF A DICTATOR and is sup-
ported by the text fragment:
... and that if the Spanish people wanted to be rid of Franco, they must
achieve this by ...
Later examples will be translated into a more conventional logical form.
One larger collection we have processed since the 2002-3 work on Treebank cor-
pora is the British National Corpus (BNC), consisting of 100 million words of mixed-
genre text passages. The quality of resulting propositions has been assessed by the
hand-judging methodology of Schubert and Tong (2003), yielding positive judge-
ments almost as frequently as for the Brown Treebank corpus. The next section,
concerned with the web corpus collected and used by Banko et al (2007), contains
a fuller description of the judging method. The BNC-based KB, containing 6,205,877
extracted propositions, is publicly searchable via a recently developed online knowl-
edge browser.3
2Where Ka is an action/attribute reification operator.
3http://www.cs.rochester.edu/u/vandurme/epik
244 Van Durme and Schubert
3 Experiments
The experiments reported here were aimed at a comparative assessment of linguisti-
cally based knowledge extraction (by KNEXT), and pattern-based information extrac-
tion (by TextRunner, and by another system, aimed at class attribute discovery). The
goal being to show that logically formal results (i.e. knowledge) based on syntactic
parsing may be obtained at a subjective level of accuracy similar to methods aimed
exclusively at acquiring correspondences between string pairs based on shallow tech-
niques.
Dataset Experiments were based on sampling 1% of the sentences from each doc-
ument contained within a corpus of 11,684,774 web pages harvested from 1,354,123
unique top level domains. The top five contributing domains made up 30% of the
documents in the collection.4 There were 310,463,012 sentences in all, the sample
containing 3,000,736. Of these, 1,373 were longer than a preset limit of 100 tokens,
and were discarded.5 Sentences containing individual tokens of length greater than
500 characters were similarly removed.6
As this corpus derives from the work of Banko et al (2007), each sentence in the
collection is paired with zero or more tuples as extracted by the TextRunner system.
Note that while websites such as Wikipedia.org contain large quantities of (semi-
)structured information stored in lists and tables, the focus here is entirely on natural
language sentences. In addition, as the extraction methods discussed in this paper do
not make use of intersentential features, the lack of sentence to sentence coherence
resulting from random sampling had no effect on the results.
ExtractionSentences were processed using the syntactic parser of Charniak (1999).
From the resultant trees, KNEXT extracted 7,406,371 propositions, giving a raw av-
erage of 2.47 per sentence. Of these, 4,151,779 were unique, so that the average
extraction frequency per sentence is 1.78 unique propositions. Post-processing left
3,975,197 items, giving a per sentence expectation of 1.32 unique, filtered proposi-
tions. Selected examples regarding knowledge about people appear in Table 1.
For the same sample, TextRunner extracted 6,053,983 tuples, leading to a raw av-
erage of 2.02 tuples per sentence. As described by its designers, TextRunner is an
information extraction system; one would be mistaken in using these results to say
that KNEXT ?wins? in raw extraction volume, as these numbers are not in fact directly
comparable (see section on Comparison).
Table 1: Verbalized propositions concerning the class PERSON
A PERSON MAY...
SING TO A GIRLFRIEND RECEIVE AN ORDER FROM A GENERAL KNOW STUFF PRESENT A PAPER
EXPERIENCE A FEELING CARRY IMAGES OF A WOMAN BUY FOOD PICK_UP A PHONE
WALK WITH A FRIEND CHAT WITH A MALE-INDIVIDUAL BURN A SAWMILL FEIGN A DISABILITY
DOWNLOAD AN ALBUM MUSH A TEAM OF (SEASONED SLED DOGS) RESPOND TO A QUESTION
SING TO A GIRLFRIEND OBTAIN SOME_NUMBER_OF (PERCULA CLOWNFISH) LIKE (POP CULTURE)
4en.wikipedia.org, www.answers.com, www.amazon.com, www.imdb.com, www.britannica.com
5Typically enumerations, e.g., There have been 29 MET deployments in the city of Florida since the
inception of the program : three in Ft. Pierce , Collier County , Opa Locka , ... .
6For example, Kellnull phenotypes can occur through splice site and splice-site / frameshift muta-
tions301,302 450039003[...]3000 premature stop codons and missense mutations.
Open Knowledge Extraction through Compositional Language Processing 245
1. A REASONABLE GENERAL CLAIM
e.g., A grand-jury may say a proposition
2. TRUE BUT TOO SPECIFIC TO BE USEFUL
e.g., Bunker walls may be decorated with seashells
3. TRUE BUT TOO GENERAL TO BE USEFUL
e.g., A person can be nearest an entity
4. SEEMS FALSE
e.g., A square can be round
5. SOMETHING IS OBVIOUSLY MISSING
e.g., A person may ask
6. HARD TO JUDGE
e.g., Supervision can be with a company
Figure 1: Instructions for categorical judging
Evaluation Extraction quality was determined through manual assessment of ver-
balized propositions drawn randomly from the results. Initial evaluation was done
using the method proposed in Schubert and Tong (2003), in which judges were asked
to label propositions according to their category of acceptability; abbreviated instruc-
tions may be seen in Figure 1.7 Under this framework, category one corresponds to
a strict assessment of acceptability, while an assignment to any of the categories be-
tween one and three may be interpreted as a weaker level of acceptance. As seen in
Table 2, average acceptability was judged to be roughly 50 to 60%, with associated
Kappa scores signalling fair (0.28) to moderate (0.48) agreement.
Table 2: Percent propositions labeled under the given category(s), paired with Fleiss?
Kappa scores. Results are reported both for the authors (judges one and two), along
with two volunteers
Category % Selected Kappa % Selected Kappa
1 49% 0.4017 50% 0.2822
1, 2, or 3 54% 0.4766 60% 0.3360
judges judges w/ volunteers
Judgement categories at this level of specificity are useful both for system analysis
at the development stage, as well as for training judges to recognize the disparate ways
in which a proposition may not be acceptable. However, due to the rates of agreement
observed, evaluation moved to the use of a five point sliding scale (Figure 2). This
scale allows for only a single axis of comparison, thus collapsing the various ways
in which a proposition may or may not be flawed into a single, general notion of
acceptability.
7Judges consisted of the authors and two volunteers, each with a background in linguistics and knowl-
edge representation.
246 Van Durme and Schubert
THE STATEMENT ABOVE IS A REASONABLY
CLEAR, ENTIRELY PLAUSIBLE GENERAL
CLAIM AND SEEMS NEITHER TOO SPECIFIC
NOR TOO GENERAL OR VAGUE TO BE USEFUL:
1. I agree.
2. I lean towards agreement.
3. I?m not sure.
4. I lean towards disagreement.
5. I disagree.
Figure 2: Instructions for scaled judging
The authors judged 480 propositions sampled randomly from amongst bins corre-
sponding to frequency of support (i.e., the number of times a given proposition was
extracted). 60 propositions were sampled from each of 8 such ranges.8 As seen in
Figure 3, propositions that were extracted at least twice were judged to be more ac-
ceptable than those extracted only once. While this is to be expected, it is striking that
as frequency of support increased further, the level of judged acceptability remained
roughly the same.
4 Comparison
To highlight differences between an extraction system targeting knowledge (repre-
sented as logical statements) as compared to information (represented as segmented
text fragments), the output of KNEXT is compared to that of TextRunner for two select
inputs.
4.1 Basic
Consider the following sentence:
A defining quote from the book, ?An armed society is a polite society?,
is very popular with those in the United States who support the personal
right to bear arms.
From this sentence TextRunner extracts the tuples:9
(A defining quote) is a (polite society ?),
(the personal right) to bear (arms).
We might manually translate this into a crude sort of logical form:
IS-A(A-DEFINING-QUOTE, POLITE-SOCIETY-?),
TO-BEAR(THE-PERSONAL-RIGHT, ARMS).
8(0,20,21,23,24,26,28,210,212), i.e., (0,1], (1,2], (2,8], ... .
9Tuple arguments are enclosed in parenthesis, with the items recognized as head given in bold. All
non-enclosed, conjoining text makes up the tuple predicate.
Open Knowledge Extraction through Compositional Language Processing 247
0 2 4 6 8 10 12
5
4
3
2
1
Natural
Number of Classes (lg scale)
Av
er
ag
e 
As
se
ss
m
en
t
judge 1
judge 2
Figure 3: As a function of frequency of support, average assessment for propositions
derived from natural sentences
Better would be to consider only those terms classified as head, and make the assump-
tion that each tuple argument implicitly introduces its own quantified variable:
?x,y. QUOTE(x) & SOCIETY(y) & IS-A(x,y),
?x,y. RIGHT(x) & ARMS(y) & TO-BEAR(x,y).
Compare this to the output of KNEXT:10
?x. SOCIETY(x) & POLITE(x),
?x,y,z. THING-REFERRED-TO(x)& COUNTRY(y) & EXEMPLAR-OF(z,y) & IN(x,z),
?x. RIGHT(x) & PERSONAL(x),
?x,y. QUOTE(x) & BOOK(y) & FROM(x,y),
?x. SOCIETY(x) & ARMED(x),
which is automatically verbalized as:
A SOCIETY CAN BE POLITE,
A THING-REFERRED-TO CAN BE IN AN EXEMPLAR-OF A COUNTRY,
A RIGHT CAN BE PERSONAL,
A QUOTE CAN BE FROM A BOOK,
A SOCIETY CAN BE ARMED.
10For expository reasons, scoped, simplified versions of KNEXT?s ULFs are shown. More accurately
propositions are viewed as weak generic conditionals, with a non-zero lower bound on conditional fre-
quency, e.g., [?x. QUOTE(x)] ?0.1 [?y. BOOK(y) & FROM(x,y)], where x is dynamically bound in the
consequent.
248 Van Durme and Schubert
0 2 4 6 8 10 12
5
4
3
2
1
Core
Number of Classes (lg scale)
Av
er
ag
e 
As
se
ss
m
en
t
judge 1
judge 2
Figure 4: As a function of frequency of support, average assessment for propositions
derived from core sentences
4.2 Extended Tuples
While KNEXT uniquely recognizes, e.g., adjectival modification and various types of
possessive constructions, TextRunner more aggressively captures constructions with
extended cardinality. For example, from the following:
James Harrington in The Commonwealth of Oceana uses the term anarchy to
describe a situation where the people use force to impose a government on an
economic base composed of either solitary land ownership, or land in the owner-
ship of a few.
TextRunner extracts 19 tuples, some with three or even four arguments, thus aiming
beyond the binary relations that most current systems are limited to. That so many
tuples were extracted for a single sentence is explained by the fact that for most tuples
containing N > 2 arguments, TextRunner will also output the same tuple with N? 1
arguments, such as:
(the people) use (force),
(the people) use (force) to impose (a government),
(the people) use (force) to impose (a government) on (an economic base).
In addition, tuples may overlap, without one being a proper subset of another:
Open Knowledge Extraction through Compositional Language Processing 249
(a situation) where (the people) use (force),
(force) to impose (a government),
(a government) on (an economic base) composed of
(either solitary land ownership).
This overlap raises the question of how to accurately quantify system performance.
Whenmeasuring average extraction quality, should samples be drawn randomly across
tuples, or from originating sentences? If from tuples, then sample sets will be biased
(for good or ill) towards fragments derived from complex syntactic constructions. If
sentence based, the system fails to be rewarded for extracting as much from an input
as possible, as it may conservatively target only those constructions most likely to be
correct. With regards to volume, it is not clear whether adjuncts should each give
rise to additional facts added to a final total; optimal would be the recognition of such
optionality. Failing this, perhaps a tally may be based on unique predicate head terms?
As a point of merit according to its designers, TextRunner does not utilize a parser
(though as mentioned it does part of speech tagging and noun phrase chunking). This
is said to be justified in view of the known difficulties in reliably parsing open domain
text as well as the additional computational costs. However, a serious consequence
of ignoring syntactic structure is that incorrect bracketing across clausal boundaries
becomes all too likely, as seen for instance in the following tuple:
(James Harrington) uses (the term anarchy) to describe (a situation)
where (the people),
or in the earlier example where from the book, ?An armed society appears to have been
erroneously treated as a post-nominal modifier, intervening between the first argument
and the is-a predicate.
KNEXT extracted the following six propositions, the first of which was automati-
cally filtered in post-processing for being overly vague:11
? A MALE-INDIVIDUAL CAN BE IN A NAMED-ENTITY OF A NAMED-ENTITY,
A MALE-INDIVIDUAL MAY USE A (TERM ANARCHY),
PERSONS MAY USE FORCE,
A BASE MAY BE COMPOSED IN SOME WAY,
A BASE CAN BE ECONOMIC,
A (LAND OWNERSHIP) CAN BE SOLITARY.
5 Extracting from Core Sentences
We have noted the common argument against the use of syntactic analysis when per-
forming large-scale extraction viz. that it is too time consuming to be worthwhile.
We are skeptical of such a view, but decided to investigate whether an argument-
bracketing system such as TextRunner might be used as an extraction preprocessor to
limit what needed to be parsed.
For each TextRunner tuple extracted from the sampled corpus, core sentences were
constructed from the predicate and noun phrase arguments,12 which were then used as
input to KNEXT for extraction.
11The authors judge the third, fifth and sixth propositions to be both well-formed and useful.
12Minor automated heuristics were used to recover, e.g., missing articles dropped during tuple
construction.
250 Van Durme and Schubert
From 6,053,981 tuples came an equivalent number of core sentences. Note that
since TextRunner tuples may overlap, use of these reconstructed sentences may lead to
skewed propositional frequencies relative to ?normal? text. This bias was very much
in evidence in the fact that of the 10,507,573 propositions extracted from the core
sentences, only 3,787,701 remained after automatic postprocessing and elimination
of duplicates. This gives a per-sentence average of 0.63, as compared to 1.32 for the
original text.
While the raw number of propositions extracted for each version of the underlying
data look similar, 3,975,197 (natural) vs. 3,787,701 (core), the actual overlap was less
than would be expected. Just 2,163,377 propositions were extracted jointly from both
natural and core sentences, representing a percent overlap of 54% and 57% respec-
tively.
Table 3: Mean judgements (lower is better) on propositions sampled from those sup-
ported either exclusively by natural or core sentences, or those supported by both
Natural Core Overlap
judge 1 3.35 3.85 2.96
judge 2 2.95 3.59 2.55
Quality was evaluated by each judge assessing 240 randomly sampled propositions
for each of: those extracted exclusively from natural sentences, those extracted ex-
clusively from core sentences, those extracted from both (Table 3). Results show that
propositions exclusively derived from core sentences were most likely to be judged
poorly. Propositions obtained both by KNEXT alone and by KNEXT- processing of
TextRunner-derived core sentences (the overlap set) were particularly likely to be
judged favorably.
On the one hand, many sentential fragments ignored by TextRunner yield KNEXT
propositions; on the other, TextRunner?s output may be assembled to produce sen-
tences yielding propositions that KNEXT otherwise would have missed. Ad-hoc anal-
ysis suggests these new propositions derived with the help of TextRunner are a mix
of noise stemming from bad tuples (usually a result of the aforementioned incorrect
clausal bracketing), along with genuinely useful propositions coming from sentences
with constructions such as appositives or conjunctive enumerations where TextRun-
ner outguessed the syntactic parser as to the correct argument layout. Future work
may consider whether (syntactic) language models can be used to help prune core
sentences before being given to KNEXT.
Figure 4 differs from Figure 3 at low frequency of support. This is the result of the
partially redundant tuples extracted by TextRunner for complex sentences; the core
verb-argument structures are those most likely to be correctly interpreted by KNEXT,
while also being those most likely to be repeated across tuples for the same sentence.
6 Class Properties
While TextRunner is perhaps the extraction system most closely related to KNEXT
in terms of generality, there is also significant overlap with work on class attribute
Open Knowledge Extraction through Compositional Language Processing 251
Table 4: By frequency, the top ten attributes a class MAY HAVE. Emphasis added to
entries overlapping with those reported by Pas?ca and Van Durme. Results for starred
classes were derived without the use of prespecified lists of instances
COUNTRY government, war, team, history, rest, coast,census, economy, population, independence
DRUG*
side effects, influence, uses, doses,
manufacturer, efficacy, release, graduates,
plasma levels, safety
CITY* makeup, heart, center, population, history,side, places, name, edge, area
PAINTER* works, art, brush, skill, lives, sons,friend, order quantity, muse, eye
COMPANY windows, products, word, page, review, film,team, award, studio, director
extraction. Pas?ca and Van Durme (2007) recently described this task, going on to
detail an approach for collecting such attributes from search engine query logs. As an
example, the search query ?president of Spain? suggests that a Country may have a
president.
If one were to consider attributes to correspond, at least in part, to things a class
MAY HAVE, CAN BE, or MAY BE, then a subset of KNEXT?s results may be dis-
cussed in terms of this specialized task. For example, for the five classes used in those
authors? experiments, Table 4 contains the top ten most frequently extracted things
each class MAY HAVE, as determined by KNEXT, without any targeted filtering or
adaptation to the task.
Table 5: Mean assessed acceptability for properties occurring for a single class (1),
and more than a single class (2+). Final column contains Pearson correlation scores
1 2+ 1 2+ corr.
MAY HAVE 2.80 2.35 2.50 2.28 0.68
MAY BE 3.20 2.85 2.35 2.13 0.59
CAN BE 3.78 3.58 3.28 2.75 0.76
judge 1 judge 2
For each of these three types of attributive categories the authors judged 80 ran-
domly drawn propositions, constrained such that half (40 for each) were supported by
a single sentence, while the other half were required only to have been extracted at
least twice, but potentially many hundreds or even thousands of times. As seen in Ta-
ble 5, the judges were strongly correlated in their assessments, where for MAY HAVE
and MAY BE they were lukewarm (3.0) or better on the majority of those seen.
In a separate evaluation judges considered whether the number of classes sharing a
given attribute was indicative of its acceptability. For each unique attributive propo-
252 Van Durme and Schubert
0 2 4 6 8 10 12
5
4
3
2
1
Number of Classes (lg scale)
Av
er
ag
e 
As
se
ss
m
en
t
judge 1
judge 2
Figure 5: Mean quality of class attributes as a function of the number of classes sharing
a given property
sition the class in ?subject? position was removed, leaving fragments such as that
bracketed: A ROBOT [CAN BE SUBHUMAN]. These attribute fragments were tallied
and binned by frequency,13 with 40 then sampled from each. For a given attribute
selected, a single attributive proposition matching that fragment was randomly drawn.
For example, having selected the attribute CAN BE FROM A US-CITY, the proposition
SOME_NUMBER_OF SHERIFFS CAN BE FROM A US-CITY was drawn from the
390 classes sharing this property. As seen in Figure 5, acceptability rose as a property
became more common.
7 Conclusions
Work such as TextRunner (Banko et al, 2007) is pushing extraction researchers to
consider larger and larger datasets. This represents significant progress towards the
greater community?s goal of having access to large, expansive stores of general world
knowledge.
The results presented here support the position that advances made over decades
of research in parsing and semantic interpretation do have a role to play in large-
scale knowledge acquisition from text. The price paid for linguistic processing is not
excessive, and an advantage is the logical formality of the results, and their versatility,
as indicated by the application to class attribute extraction.
13Ranges: (0,20,21,23,26,?)
Open Knowledge Extraction through Compositional Language Processing 253
Acknowledgements We are especially grateful to Michele Banko and her colleagues
for generously sharing results, and to Daniel Gildea for helpful feedback. This work
was supported by NSF grants IIS-0328849 and IIS-0535105.
References
Abney, S. (1996). Partial Parsing via Finite-State Cascades. Natural Language Engi-
neering 2(4), 337?344.
Banko, M., M. Cafarella, S. Soderland, M. Broadhead, and O. Etzioni (2007). Open
Information Extraction from the Web. In Proceedings of the 20th International
Joint Conference on Artificial Intelligence (IJCAI-07), pp. 2670?2676.
Charniak, E. (1999). A Maximum-Entropy-Inspired Parser. In Proceedings of the 1st
Conference of the North American Chapter of the Association for Computational
Linguistics (NAACL 2000), pp. 132?139.
Clark, P., P. Harrison, and J. Thompson (2003). A Knowledge-Driven Approach to
Text Meaning Processing. In Proceedings of the HLT-NAACL 2003 Workshop on
Text Meaning, pp. 1?6.
Clark, S. and D. Weir (1999). An iterative approach to estimating frequencies over
a semantic hierarchy. In Proceedings of the 1999 Joint SIGDAT Conference on
Empirical Methods in Natural Language Processing and Very Large Corpora
(EMNLP/VLC-99), pp. 258?265.
Collins, M. (1997). Three Generative, Lexicalised Models for Statistical Parsing. In
Proceedings of the 35th Annual Conference of the Association for Computational
Linguistics (ACL-97), pp. 16?23.
Gildea, D. and D. Jurafsky (2002). Automatic labeling of semantic roles. Computa-
tional Linguistics 28(3), 245?288.
Gildea, D. and M. Palmer (2002). The necessity of syntactic parsing for predicate
argument recognition. In Proceedings of the 40th Annual Conference of the Asso-
ciation for Computational Linguistics (ACL-02), Philadelphia, PA, pp. 239?246.
Holmback, H., L. Duncan, and P. Harrison (2000). A word sense checking applica-
tion for Simplified English. In Proceedings of the 3rd International Workshop on
Controlled Language Applications (CLAW00), pp. 120?133.
Liakata, M. and S. Pulman (2002). From Trees to Predicate Argument Structures.
In Proceedings of the 19th International Conference on Computational Linguistics
(COLING-02), pp. 563?569.
Pas?ca, M. and B. Van Durme (2007). What You Seek is What You Get: Extraction of
Class Attributes from Query Logs. In Proceedings of the 20th International Joint
Conference on Artificial Intelligence (IJCAI-07), pp. 2832?2837.
254 Van Durme and Schubert
Punyakanok, V., D. Roth, andW. tau Yih (2008). The Importance of Syntactic Parsing
and Inference in Semantic Role Labeling. Computational Linguistics 34(2), 257?
287.
Resnik, P. (1993). Semantic classes and syntactic ambiguity. In Proceedings of ARPA
Workshop on Human Language Technology, pp. 278?283.
Richardson, S. D., W. B. Dolan, and L. Vanderwende (1998). MindNet: Acquiring
and Structuring Semantic Information from Text. In Proceedings of the 17th Inter-
national Conference on Computational linguistics (COLING-98), pp. 1098?1102.
Schubert, L. K. (2002). Can we derive general world knowledge from texts? In
Proceedings of the 2nd International Conference on Human Language Technology
Research (HLT 2002), pp. 94?97.
Schubert, L. K. and C. H. Hwang (2000). Episodic Logic meets Little Red Riding
Hood: A comprehensive, natural representation for language understanding. In
L. Iwanska and S. Shapiro (Eds.), Natural Language Processing and Knowledge
Representation: Language for Knowledge and Knowledge for Language, pp. 111?
174.
Schubert, L. K. and M. H. Tong (2003). Extracting and evaluating general world
knowledge from the Brown corpus. In Proceedings of the HLT-NAACL 2003Work-
shop on Text Meaning, pp. 7?13.
Wong, Y. W. and R. J. Mooney (2007). Learning Synchronous Grammars for Seman-
tic Parsing with Lambda Calculus. In Proceedings of the 45th Annual Conference
of the Association for Computational Linguistics (ACL-07), pp. 960?967.
Zelle, J. M. and R. J. Mooney (1996). Learning to Parse Database Queries using
Inductive Logic Programming. In Proceedings of the 13th National Conference on
Artificial Intelligence (AAAI-96), pp. 1050?1055.
Zernik, U. (1992). Closed yesterday and closed minds: Asking the right questions
of the corpus to distinguish thematic from sentential relations. In Proceedings of
the 19th International Conference on Computational Linguistics (COLING-02), pp.
1305?1311.
Zettlemoyer, L. and M. Collins (2005). Learning to Map Sentences to Logical Form:
Structured Classification with Probabilistic Categorial Grammars. In Proceedings
of the 21st Conference on Uncertainty in Artificial Intelligence (UAI-05), pp. 658?
666.
Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s Mechanical Turk, pages 159?162,
Los Angeles, California, June 2010. c?2010 Association for Computational Linguistics
Evaluation of Commonsense Knowledge with Mechanical Turk
Jonathan Gordon
Dept. of Computer Science
University of Rochester
Rochester, NY, USA
jgordon@cs.rochester.edu
Benjamin Van Durme
HLTCOE
Johns Hopkins University
Baltimore, MD, USA
vandurme@cs.jhu.edu
Lenhart K. Schubert
Dept. of Computer Science
University of Rochester
Rochester, NY, USA
schubert@cs.rochester.edu
Abstract
Efforts to automatically acquire world knowl-
edge from text suffer from the lack of an easy
means of evaluating the resulting knowledge.
We describe initial experiments using Mechan-
ical Turk to crowdsource evaluation to non-
experts for little cost, resulting in a collection
of factoids with associated quality judgements.
We describe the method of acquiring usable
judgements from the public and the impact
of such large-scale evaluation on the task of
knowledge acquisition.
1 Introduction
The creation of intelligent artifacts that can achieve
human-level performance at problems like question-
answering ultimately depends on the availability of
considerable knowledge. Specifically, what is needed
is commonsense knowledge about the world in a form
suitable for reasoning. Open knowledge extraction
(Van Durme and Schubert, 2008) is the task of mining
text corpora to create useful, high-quality collections
of such knowledge.
Efforts to encode knowledge by hand, such as Cyc
(Lenat, 1995), require expensive man-hours of labor
by experts. Indeed, results from Project Halo (Fried-
land et al, 2004) suggest that properly encoding the
(domain-specific) knowledge from just one page of a
textbook can cost $10,000. OKE, on the other hand,
creates logical formulas automatically from existing
stores of human knowledge, such as books, newspa-
pers, and the Web. And while crowdsourced efforts to
gather knowledge, such as Open Mind (Singh, 2002),
learn factoids people come up with off the tops of
their heads to contribute, OKE learns from what peo-
ple normally write about and thus consider important.
Open knowledge extraction differs from open infor-
mation extraction (Banko et al, 2007) in the focus
on everyday, commonsense knowledge rather than
specific facts, and on the logical interpretability of
the outputs. While an OIE system might learn that
Tolstoy wrote using a dip pen, an OKE system would
prefer to learn that an author may write using a pen.
An example of an OKE effort is the KNEXT sys-
tem1 (Schubert, 2002), which uses compositional se-
mantic interpretation rules to produce logical formu-
las from the knowledge implicit in parsed text. These
formulas are then automatically expressed as English-
like ?factoids?, such as ?A PHILOSOPHER MAY HAVE
A CONVICTION? or ?NEGOTIATIONS CAN BE LIKELY
TO GO ON FOR SOME HOURS?.
While it is expected that eventually sufficiently
clean knowledge bases will be produced for infer-
ences to be made about everyday things and events,
currently the average quality of automatically ac-
quired knowledge is not good enough to be used in
traditional reasoning systems. An obstacle for knowl-
edge extraction is the lack of an easy method for
evaluating ? and thus improving ? the quality of re-
sults. Evaluation in acquisition systems is typically
done by human judging of random samples of output,
usually by the reporting authors themselves (e.g., Lin
and Pantel, 2002; Schubert and Tong, 2003; Banko et
al., 2007). This is time-consuming, and it has the po-
tential for bias: it would be preferable to have people
other than AI researchers label whether an output is
commonsense knowledge or not. We explore the use
of Amazon?s Mechanical Turk service, an online la-
bor market, as a means of acquiring many non-expert
judgements for little cost.
2 Related Work
While Open Mind Commons (Speer, 2007) asks users
to vote for or against commonsense statements con-
tributed by others users in order to come to a consen-
sus, we seek to evaluate an automatic system. Snow
et al (2008) compared the quality of labels produced
by non-expert Turkers against those made by experts
for a variety of NLP tasks and found that they re-
quired only four responses per item to emulate expert
annotations. Kittur et al (2008) describe the use and
1Public release of the basic KNEXT engine is forthcoming.
159
The statement above is a reasonably clear, entirely
plausible, generic claim and seems neither too spe-
cific nor too general or vague to be useful:
? I agree.
? I lean towards agreement.
? I?m not sure.
? I lean towards disagreement.
? I disagree.
Figure 1: Rating instructions and answers.
necessity of verifiable questions in acquiring accurate
ratings of Wikipedia articles from Mechanical Turk
users. These results contribute to our methods below.
3 Experiments
Previous evaluations of KNEXT output have tried to
judge the relative quality of knowledge learned from
different sources and by different techniques. Here
the goal is simply to see whether the means of evalu-
ation can be made to work reasonably, including at
what scale it can be done for limited cost. For these
experiments, we relied on $100 in credit provided by
Amazon as part of the workshop shared task. This
amount was used for several small experiments in or-
der to empirically estimate what $100 could achieve,
given a tuned method of presentation and evaluation.
We took a random selection of factoids generated
from the British National Corpus (BNC Consortium,
2001), split into sets of 20, and removed those most
easily filtered out as probably being of low quality
or malformed. We skipped the more stringent filters
(originally created for dealing with noisy Web text),
leaving more variety in the quality of the factoids
Turkers were asked to rate.
The first evaluation followed the format of previ-
ous, offline ratings. For each factoid, Turkers were
given the instructions and choices in Fig. 1, where
the options correspond in our analysis to the num-
bers 1?5, with 1 being agreement. To help Turkers
make such judgements, they were given a brief back-
ground statement: ?We?re gathering the sort of every-
day, commonsense knowledge an intelligent computer
system should know. You?re asked to rate several pos-
sible statements based on how well you think they
meet this goal.? Mason and Watts (2009) suggest that
while money may increase the number and speed of
responses, other motivations such as wanting to help
with something worthwhile or interesting are more
likely to lead to high-quality responses.
Participants were then shown the examples and
explanations in Fig. 2. Note that while they are told
some categories that bad factoids can fall into, the
Turkers are not asked to make such classifications
Examples of good statements:
? A SONG CAN BE POPULAR
? A PERSON MAY HAVE A HEAD
? MANEUVERS MAY BE HOLD -ED IN SECRET
It?s fine if verb conjugations are not attached or are a bit
unnatural, e.g. ?hold -ed? instead of ?held?.
Examples of bad statements:
? A THING MAY SEEK A WAY
This is too vague. What sort of thing? A way for/to
what?
? A COCKTAIL PARTY CAN BE AT
SCOTCH_PLAINS_COUNTRY_CLUB
This is too specific. We want to know that a cocktail
party can be at a country club, not at this particular one.
The underscores are not a problem.
? A PIG MAY FLY
This is not literally true even though it happens to be an
expression.
? A WORD MAY MEAN
This is missing information. What might a word mean?
Figure 2: The provided examples of good and bad factoids.
themselves, as this is a task where even experts have
low agreement (Van Durme and Schubert, 2008).
An initial experiment (Round 1) only required
Turkers to have a high (90%) approval rate. Under
these conditions, out of 100 HITs2, 60 were com-
pleted by participants whose IP addresses indicated
they were in India, 38 from the United States, and
2 from Australia. The average Pearson correlation
between the ratings of different Indian Turkers an-
swering the same questions was a very weak 0.065,
and between the Indian responders and those from
the US and Australia was 0.132. On the other hand,
the average correlation among non-Indian Turkers
was 0.508, which is close to the 0.6?0.8 range seen
between the authors in the past, and which can be
taken as an upper bound on agreement for the task.
Given the sometimes subtle judgements of mean-
ing required, being a native English speaker has pre-
viously been assumed to be a prerequisite. This differ-
ence in raters? agreements may thus be due to levels
of language understanding, or perhaps to different
levels of attentiveness to the task. However, it does
not seem to be the case that the Indian respondents
rushed: They took a median time of 201.5 seconds
(249.18 avg. with a high standard deviation of 256.3s
? some took more than a minute per factoid). The non-
Indian responders took a median time of just 115.5 s
(124.5 avg., 49.2 std dev.).
Regardless of the cause, given these results, we re-
stricted the availability of all following experiments
to Turkers in the US.Ideally we would include other
English-speaking countries, but there is no straight-
2Human Intelligence Tasks ? Mechanical Turk assignments.
In this case, each HIT was a set of twenty factoids to be rated.
160
All High Corr. (> 0.3)
Round Avg. Std. Dev. Avg. Std. Dev.
1 (BNC) 2.59 1.55 2.71 1.64
3 (BNC) 2.80 1.66 2.83 1.68
4 (BNC) 2.61 1.64 2.62 1.64
5 (BNC) 2.76 1.61 2.89 1.68
6 (Weblogs) 2.83 1.67 2.85 1.67
7 (Wikipedia) 2.75 1.64 2.75 1.64
Table 1: Average ratings for all responses and for highly
correlated responses. to other responses. Lower numbers
are more positive. Round 2 was withdrawn without being
completed.
forward way to set multiple allowable countries on
Mechanical Turk.When Round 2 was posted with
a larger set of factoids to be rated and the location
requirement, responses fell off sharply, leading us
to abort and repost with a higher payrate (7? for 20
factoids vs 5? originally) in Round 3.
To avoid inaccurate ratings, we rejected submis-
sions that were improbably quick or were strongly
uncorrelated with other Turkers? responses. We col-
lected five Turkers? ratings for each set of factoids,
and for each persons? response to a HIT computed
the average of their three highest correlations with
others? responses. We then rejected if the correla-
tions were so low as to indicate random responses.
The scores serve a second purpose of identifying a
more trustworthy subset of the responses. (A cut-off
score of 0.3 was chosen based on hand-examination.)
In Table 1, we can see that these more strongly corre-
lated responses rate factoids as slightly worse overall,
possibly because those who either casual or uncertain
are more likely to judge favorably on the assumption
that this is what the task authors would prefer, or they
are simply more likely to select the top-most option,
which was ?I agree?.
An example of a factoid that was labeled incor-
rectly by one of the filtered out users is ?A PER-
SON MAY LOOK AT SOME THING-REFERRED-TO OF
PRESS RELEASES?, for which a Turker from Madras
in Round 1 selected ?I agree?. Factoids containing
the vague ?THING-REFERRED-TO? are often filtered
out of our results automatically, but leaving them in
gave us some obviously bad inputs for checking Turk-
ers? responses. Another (US) Turker chose ?I agree?
when told ?TES MAY HAVE 1991ES? but ?I disagree?
when shown ?A TRIP CAN BE TO A SUPERMARKET?.
We are interested not only in whether there is a gen-
eral consensus to be found among the Turkers but also
how that consensus correlates with the judgements
of AI researchers. To this end, one of the authors
rated five sets (100 factoids) presented in Round 3,
0
500
1000
1500
2000
2500
0 1 2 3 4 5
Fr
eq
ue
nc
y
Rating
Figure 3: Frequency of ratings in the high-corr. results of
Round 3.
which yielded an average correlation between all the
Turkers and the author of 0.507, which rises slightly
to 0.532 if we only count those Turkers considered
?highly correlated? as described above.
As another test of agreement, for ten of the sets in
Round 3, two factoids were designated as fixpoints ?
the single best and worst factoid in the set, assigned
ratings 1 and 5 respectively. From the Turkers who
rated these factoids, 65 of the 100 ratings matched
the researchers? designations and 77 were within one
point of the chosen rating.3
A few of the Turkers who participated had fairly
strong negative correlations to the other Turkers, sug-
gesting that they may have misunderstood the task
and were rating backwards.4 Furthermore, one Turker
commented that she was unsure whether the state-
ment she was being asked to agree with (Fig. 1) ?was
a positive or negative?. To see how it would affect the
results, we ran (as Round 4) twenty sets of factoids,
asking simplified question ?Do you agree this is a
good statement of general knowledge?? The choices
were also reversed in order, running from ?I disagree?
to ?I agree? and color-coded, with agree being green
and disagree red. This corresponded to the coloring
of the good and bad examples at the top of the page,
which the Turkers were told to reread when they were
halfway through the HIT. The average correlation for
responses in Round 4 was 0.47, which is an improve-
ment over the 0.34 avg. correlation of Round 3.
Using the same format as Round 4, we ran factoids
from two other corpora. Round 6 consisted of 300 ran-
dom factoids taken from running KNEXT on weblog
data (Gordon et al, 2009) and Round 7 300 random
factoids taken from running KNEXT on Wikipedia.
3If we only look at the highly correlated responses, this in-
creases slightly to 68% exact match, 82% within one point.
4This was true for one Turker who completed many HITs, a
problem that might be prevented by accepting/rejecting HITs as
soon as all scores for that set of factoids were available rather
than waiting for the entire experiment to finish.
161
The average ratings for factoids from these sources
are lower than for the BNC, reflecting the noisy na-
ture of much writing on weblogs and the many overly
specific or esoteric factoids learned from Wikipedia.
The results achieved can be quite sensitive to the
display of the task. For instance, the frequency of
ratings in Fig. 3 shows that Turkers tended toward
the extremes: ?I agree? and ?I disagree? but rarely
?I?m not sure?. This option might have a negative
connotation (?Waffling is undesirable?) that another
phrasing would not. As an alternative presentation of
the task (Round 5), for 300 factoids, we asked Turk-
ers to first decide whether a factoid was ?incoher-
ent (not understandable)? and, otherwise, whether it
was ?bad?, ?not very good?, ?so-so?, ?not so bad?, or
?good? commonsense knowledge. Turkers indicated
factoids were incoherent 14% of the time, with a cor-
responding reduction in the number rated as ?bad?,
but no real increase in middle ratings. The average
ratings for the ?coherent? factoids are in Table 1.
4 Uses of Results
Beyond exploring the potential of Mechanical Turk
as a mechanism for evaluating the output of KNEXT
and other open knowledge extraction systems, these
experiments have two useful outcomes:
First, they give us a large collection of almost 3000
factoids that have associated average ratings and al-
low for the release of the subset of those factoids
that are believed to probably be good (rated 1?2).
This data set is being publicly released at http://
www.cs.rochester.edu/research/knext, and
it includes a wide range of factoids, such as ?A REP-
RESENTATION MAY SHOW REALITY? and ?DEMON-
STRATIONS MAY MARK AN ANNIVERSARY OF AN
UPRISING?.
Second, the factoids rated from Round 2 onward
were associated with the KNEXT extraction rules used
to generate them: The factoids generated by different
rules have average ratings from 1.6 to 4.8. We hope in
future to use this data to improve KNEXT?s extraction
methods, improving or eliminating rules that often
produce factoids judged to be bad. Inexpensive, fast
evaluation of output on Mechanical Turk could be a
way to measure incremental improvements in output
quality coming from the same source.
5 Conclusions
These initial experiments have shown that untrained
Turkers evaluating the natural-language verbaliza-
tions of an open knowledge extraction system will
generally give ratings that correlate strongly with
those of AI researchers. Some simple methods were
described to find those responses that are likely to
be accurate. This work shows promise for cheap and
quick means of measuring the quality of automati-
cally constructed knowledge bases and thus improv-
ing the tools that create them.
Acknowledgements
This work was supported by NSF grants IIS-0535105
and IIS-0916599.
References
Michele Banko, Michael J. Cafarella, Stephen Soderland,
Matt Broadhead, and Oren Etzioni. 2007. Open infor-
mation extraction from the Web. In Proc. of IJCAI-07.
BNC Consortium. 2001. The British National Corpus,
v.2. Dist. by Oxford University Computing Services.
Noah S. Friedland et al. 2004. Project Halo: Towards a
digital Aristotle. AI Magazine, 25(4).
Jonathan Gordon, Benjamin Van Durme, and Lenhart K.
Schubert. 2009. Weblogs as a source for extracting
general world knowledge. In Proc. of K-CAP-09.
Aniket Kittur, Ed H. Chi, and Bongwon Suh. 2008.
Crowdsourcing user studies with Mechanical Turk. In
Proc. of CHI ?08.
Douglas B. Lenat. 1995. Cyc: A Large-scale Investment
in Knowledge Infrastructure. Communications of the
ACM, 38(11):33?48.
Dekang Lin and Patrick Pantel. 2002. Concept discovery
from text. In Proc. of COLING-02.
Winter Mason and Duncan J. Watts. 2009. Financial
incentives and the ?performance of crowds?. In Proc.
of HCOMP ?09.
Lenhart K. Schubert and Matthew H. Tong. 2003. Extract-
ing and evaluating general world knowledge from the
Brown corpus. In Proc. of the HLT-NAACL Workshop
on Text Meaning.
Lenhart K. Schubert. 2002. Can we derive general world
knowledge from texts? In Proc. of HLT-02.
Push Singh. 2002. The public acquisition of common-
sense knowledge. In Proc. of AAAI Spring Sympo-
sium on Acquiring (and Using) Linguistic (and World)
Knowledge for Information Access.
Rion Snow, Brendan O?Connor, Daniel Jurafsky, and An-
drew Y. Ng. 2008. Cheap and fast ? but is it good? In
Proc. of EMNLP-08.
Robert Speer. 2007. Open mind commons: An inquisitive
approach to learning common sense. In Workshop on
Common Sense and Intelligent User Interfaces.
Benjamin Van Durme and Lenhart K. Schubert. 2008.
Open knowledge extraction through compositional lan-
guage processing. In Proc. of STEP 2008.
162
Proceedings of the TextInfer 2011 Workshop on Textual Entailment, EMNLP 2011, pages 59?63,
Edinburgh, Scotland, UK, July 30, 2011. c?2011 Association for Computational Linguistics
Discovering Commonsense Entailment Rules Implicit in Sentences
Jonathan Gordon
Department of Computer Science
University of Rochester
Rochester, NY, USA
jgordon@cs.rochester.edu
Lenhart K. Schubert
Department of Computer Science
University of Rochester
Rochester, NY, USA
schubert@cs.rochester.edu
Abstract
Reasoning about ordinary human situations
and activities requires the availability of di-
verse types of knowledge, including expecta-
tions about the probable results of actions and
the lexical entailments for many predicates.
We describe initial work to acquire such a col-
lection of conditional (if?then) knowledge by
exploiting presuppositional discourse patterns
(such as ones involving ?but?, ?yet?, and ?hop-
ing to?) and abstracting the matched material
into general rules.
1 Introduction
We are interested, ultimately, in enabling an infer-
ence system to reason forward from facts as well
as backward from goals, using lexical knowledge to-
gether with world knowledge. Creating appropriate
collections of general world knowledge to support
reasoning has long been a goal of researchers in Arti-
ficial Intelligence. Efforts in information extraction,
e.g., Banko et al (2007), have focused on learning
base facts about specific entities (such as that Barack
Obama is president), and work in knowledge extrac-
tion, e.g., Van Durme and Schubert (2008), has found
generalizations (such as that a president may make
a speech). While the latter provides a basis for pos-
sibilistic forward inference (Barack Obama proba-
bly makes a speech at least occasionally) when its
meaning is sharpened (Gordon and Schubert, 2010),
these resources don?t provide a basis for saying what
we might expect to happen if, for instance, someone
crashes their car.
That the driver in a car crash might be injured
and the car damaged is a matter of common sense,
and, as such, is rarely stated directly. However, it
can be found in sentences where this expectation
is disconfirmed: ?Sally crashed her car into a tree,
but she wasn?t hurt.? We have been exploring the
use of lexico-syntactic discourse patterns indicating
disconfirmed expectations, as well as people?s goals
(?Joe apologized repeatedly, hoping to be forgiven?).
The resulting rules, expressed at this point in natural
language, are a first step toward obtaining classes of
general conditional knowledge typically not obtained
by other methods.
2 Related Work
One well-known approach to conditional knowledge
acquisition is that of Lin and Pantel (2001), where
inference rules are learned using distributional simi-
larity between dependency tree paths. These results
include entailment rules like ?x is the author of y ? x
wrote y? (which is true provided x is a literary work)
and less dependable ones like ?x caused y ? y is
blamed on x?. This work was refined by Pantel et al
(2007) by assigning the x and y terms semantic types
(inferential selectional preferences ? ISP) based on
lexical abstraction from empirically observed argu-
ment types. A limitation of the approach is that the
conditional rules obtained are largely limited to ones
expressing some rough synonymy or similarity re-
lation. Pekar (2006) developed related methods for
learning the implications of an event based on the
regular co-occurrence of two verbs within ?locally
coherent text?, acquiring rules like ?x was appointed
as y? suggests that ?x became y?, but, as in DIRT, we
lack information about the types of x and y, and only
acquire binary relations.
Girju (2003) applied Hearst?s (1998) procedure for
finding lexico-syntactic patterns to discover causal
relations between nouns, as in ?Earthquakes gener-
ate tsunami?. Chklovski and Pantel (2004) used pat-
59
(S < (NP $. (VP < (/,/ $. (S < (VP < (VBG <hoping)< (S < (VP < TO))))))))
(S < (NP $. (VP < ((CC < but) $.. (VP < (AUX < did) < (RB < /n[?o]t/))))))
(S < (NP $. (VP < (AUX $. (ADJP < (JJ $. ((CC < /(but|yet)/) $. JJ)))))))
(S < (NP $. (VP < (/,/ $. (S < (VP < ((VBG < expecting) $.
(S < (VP < TO)))))))))
Figure 1: Examples of TGrep2 patterns for finding parse
tree fragments that might be abstracted to inference rules.
See Rohde (2001) for an explanation of the syntax.
terns like ?x-ed by y-ing? (?obtained by borrowing?)
to get co-occurrence data on candidate pairs from the
Web. They used these co-occurrence counts to obtain
a measure of mutual information between pairs of
verbs, and hence to assess the strengths of the rela-
tions. A shortcoming of rules obtained in this way is
their lack of detailed predicative structure. For infer-
ence purposes, it would be insufficient to know that
?crashes cause injuries? without having any idea of
what is crashing and who or what is being injured.
Schoenmackers et al (2010) derived first-order
Horn clauses from the tuple relations found by TEXT-
RUNNER (Banko et al, 2007). Their system produces
rules like ?IsHeadquarteredIn(Company, State) :- Is-
BasedIn(Company, City) ? IsLocatedIn(City, State)?,
which are intended to improve inference for question-
answering. A limitation of this approach is that, op-
erating on the facts discovered by an information
extraction system, it largely obtains relations among
simple attributes like locations or roles rather than
consequences or reasons.
3 Method
Our method first uses TGrep2 (Rohde, 2001) to find
parse trees matching hand-authored lexico-syntactic
patterns, centered around certain pragmatically sig-
nificant cue words such as ?hoping to? or ?but didn?t?.
Some of the search patterns are in Figure 1. While
we currently use eight query patterns, future work
may add rules to cover more constructions.
The matched parse trees are filtered to remove
those unlikely to produce reasonable results, such
as those containing parentheses or quoted utterances,
and the trees are preprocessed in a top-down traversal
to rewrite or remove constituents that are usually
extraneous. For instance, the parse tree for
The next day he and another Bengali boy who
lives near by [sic] chose another way home,
hoping to escape the attackers.
is preprocessed to
People chose another way home, hoping to
escape the attackers.
Examples of the preprocessing rules include re-
moving interjections (INTJ) and some prepositional
phrases, heuristically turning long expressions into
keywords like ?a proposition?, abstracting named en-
tities, and reordering some sentences to be easier to
process. E.g., ?Fourteen inches from the floor it?s sup-
posed to be? is turned to ?It?s supposed to be fourteen
inches from the floor?.
The trees are then rewritten as conditional expres-
sions based on which semantic pattern they match,
as outlined in the following subsections. The sample
sentences are from the Brown Corpus (Kuc?era and
Francis, 1967) and the British National Corpus (BNC
Consortium, 2001), and the rules are those derived
by our current system.
3.1 Disconfirmed Expectations
These are sentences where ?but? or ?yet? is used to
indicate that the expected inference people would
make does not hold. In such cases, we want to flip the
polarity of the conclusion (adding or removing ?not?
from the output) so that the expectation is confirmed.
For instance, from
The ship weighed anchor and ran out her big
guns, but did not fire a shot.
we get that the normal case is the opposite:
If a ship weighs anchor and runs out her big
guns, then it may fire a shot.
Or for two adjectives, ?She was poor but proud?:
If a female is poor, then she may not be proud.
3.2 Contrasting Good and Bad
A different use of ?but? and ?yet? is to contrast some-
thing considered good with something considered
bad, as in ?He is very clever but eccentric?:
If a male is very clever,
then he may be eccentric.
If we were to treat this as a case of disconfirmed ex-
pectation as above, we would have claimed that ?If a
male is very clever, then he may not be eccentric?. To
identify this special use of ?but?, we consult a lexicon
of sentiment annotations, SentiWordNet (Baccianella
et al, 2010). Finding that ?clever? is positive while
?eccentric? is negative, we retain the surface polarity
in this case.
60
For sentences with full sentential complements for
?but?, recognizing good and bad items is quite difficult,
more often depending on pragmatic information. For
instance, in
Central government knew this would happen
but did not want to admit to it in its plans.
knowing something is generally good while being
unwilling to admit something is bad. At present, we
don?t deal with these cases.
3.3 Expected Outcomes
Other sentences give us a participant?s intent, and we
just want to abstract sufficiently to form a general
rule:
He stood before her in the doorway, evidently
expecting to be invited in.
If a male stands before a female in the
doorway, then he may expect to be invited in.
When we abstract from named entities (using a va-
riety of hand-built gazetteers), we aim low in the
hierarchy:
Elisabeth smiled, hoping to lighten the
conversational tone and distract the Colonel
from his purpose.
If a female smiles, then she may hope to
lighten the conversational tone.
While most general rules about ?a male? or ?a female?
could instead be about ?a person?, there are ones that
can?t, such as those about giving birth. We leave the
raising of terms for later work, following Van Durme
et al (2009).
4 Evaluation
Development was based on examples from the (hand-
parsed) Brown Corpus and the (machine-parsed)
British National Corpus, as alluded to above. These
corpora were chosen for their broad coverage of ev-
eryday situations and edited writing.
As the examples in the preceding subsections in-
dicate, rules extracted by our method often describe
complex consequences or reasons, and subtle rela-
tions among adjectival attributes, that appear to be
quite different from the kinds of rules targeted in pre-
vious work (as discussed earlier, or at venues such
as that of (Sekine, 2008)). While we would like to
evaluate the discovered rules by looking at inferences
made with them, that must wait until logical forms
are automatically created; here we judge the rules
themselves.
The statement above is a reasonably clear, entirely
plausible, generic claim and seems neither too specific
nor too general or vague to be useful:
1. I agree.
2. I lean towards agreement.
3. I?m not sure.
4. I lean towards disagreement.
5. I disagree.
Figure 2: Instructions for judging of unsharpened factoids.
Judge 1 Judge 2 Correlation
1.84 2.45 0.55
Table 1: Average ratings and Pearson correlation for rules
from the personal stories corpus. Lower ratings are better;
see Fig. 2.
For evaluation, we used a corpus of personal stories
from weblogs (Gordon and Swanson, 2009), parsed
with a statistical parser (Charniak, 2000). We sampled
100 output rules and rated them on a scale of 1?5
(1 being best) based on the criteria in Fig. 2. To
decide if a rule meets the criteria, it is helpful to
imagine a dialogue with a computer agent. Told an
instantiated form of the antecedent, the agent asks for
confirmation of a potential conclusion. E.g., for
If attacks are brief,
then they may not be intense,
the dialogue would go:
?The attacks (on Baghdad) were brief.?
?So I suppose they weren?t intense, were they??
If this is a reasonable follow-up, then the rule is prob-
ably good, although we also disprefer very unlikely
antecedents ? rules that are vacuously true.
As the results in Table 1 and Fig. 3 indicate, the
overall quality of the rules learned is good but there
is room for improvement. We also see a rather low
correlation between the ratings of the two judges,
indicating the difficulty of evaluating the quality of
the rules, especially since their expression in natural
language (NL) makes it tempting to ?fill in the blanks?
of what we understand them to mean. We hypothesize
that the agreement between judges will be higher
for rules in logical form, where malformed output
is more readily identified ? for instance, there is no
guessing about coreference or attachment.
Rules that both judges rated favorably (1) include:
If a pain is great, it may not be manageable.
If a person texts a male, then he-or-she may
get a reply.
61
020
40
60
80
1 2 3 4 5
Fr
eq
ue
nc
y
Rating
Figure 3: Counts for how many rules were assigned each
rating by judges. Lower ratings are better; see Fig. 2.
If a male looks around, then he may hope to
see someone.
If a person doesn?t like some particular store,
then he-or-she may not keep going to it.
While some bad rules come from parsing or pro-
cessing mistakes, these are less of a problem than
the heavy tail of difficult constructions. For instance,
there are idioms that we want to filter out (e.g., ?I?m
embarrassed but. . . ?) and other bad outputs show
context-dependent rather than general relations:
If a girl sits down in a common room, then she
may hope to avoid some pointless
conversations.
The sitting-down may not have been because she
wanted to avoid conversation but because of some-
thing prior.
It?s difficult to compare our results to other systems
because of the differences of representation, types of
rules, and evaluation methods. ISP?s best performing
method (ISP.JIM) achieves 0.88 specificity (defined as
a filter?s probability of rejecting incorrect inferences)
and 0.53 accuracy. While describing their SHERLOCK
system, Schoenmackers et al (2010) argue that ?the
notion of ?rule quality? is vague except in the context
of an application? and thus they evaluate the Horn
clauses they learn in the context of the HOLMES
inference-based QA system, finding that at precision
0.8 their rules allow the system to find twice as many
correct facts. Indeed, our weak rater agreement shows
the difficulty of judging rules on their own, and future
work aims to evaluate rules extrinsically.
5 Conclusion and Future Work
Enabling an inference system to reason about com-
mon situations and activities requires more types of
general world knowledge and lexical knowledge than
are currently available or have been targeted by previ-
ous work. We?ve suggested an initial approach to
acquiring rules describing complex consequences
or reasons and subtle relations among adjectival at-
tributes: We find possible rules by looking at interest-
ing discourse patterns and rewriting them as condi-
tional expressions based on semantic patterns.
A natural question is why we don?t use the
machine-learning/bootstrapping techniques that are
common in other work on acquiring rules. These tech-
niques are particularly successful when (a) they are
aimed at finding fixed types of relationships, such
as hyponymy, near-synonymy, part-of, or causal rela-
tions between pairs of lexical items (often nominals
or verbs); and (b) the fixed type of relationship be-
tween the lexical items is hinted at sufficiently often
either by their co-occurrence in certain local lexico-
syntactic patterns, or by their occurrences in simi-
lar sentential environments (distributional similarity).
But in our case, (a) we are looking for a broad range
of (more or less strong) consequence relationships,
and (b) the relationships are between entire clauses,
not lexical items. We are simply not likely to find
multiple occurrences of the same pair of clauses in
a variety of syntactic configurations, all indicating a
consequence relation ? you?re unlikely to find multi-
ple redundant patterns relating clauses, as in ?Went
up to the door but didn?t knock on it?.
There is more work to be done to arrive at a reli-
able, inference-ready knowledge base of such rules.
The primary desideratum is to produce a logical rep-
resentation for the rules such that they can be used in
the EPILOG reasoner (Schubert and Hwang, 2000).
Computing logical forms (as, e.g., in Bos (2008)) and
then deriving logically formulated rules from these
rather than deriving sentential forms directly from
text should also allow us to be more precise about
dropping modifiers, reshaping into generic present
tense from other tenses, and other issues that affect
the quality of the statements. We have a preliminary
version of a logical form generator that derives LFs
from TreeBank parses that can support this direc-
tion. Further filtering techniques (based both on the
surface form and the logical form) should keep the
desired inference rules while improving quality.
Acknowledgements
This work was supported by NSF grants IIS-
1016735 and IIS-0916599, and ONR STTR subcon-
tract N00014-10-M-0297.
62
References
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebas-
tiani. 2010. SentiWordNet 3.0: An enhanced lexical
resource for sentiment analysis and opinion mining.
In Proc. of the Seventh Conference on International
Language Resources and Evaluation (LREC?10).
Michele Banko, Michael J. Cafarella, Stephen Soderland,
Matt Broadhead, and Oren Etzioni. 2007. Open in-
formation extraction from the Web. In Proc. of the
Twentieth International Joint Conference on Artificial
Intelligence (IJCAI-07).
BNC Consortium. 2001. The British National Corpus, v.2.
Distributed by Oxford University Computing Services.
Johan Bos. 2008. Wide-coverage semantic analysis with
Boxer. In Proc. of the Symposium on Semantics in Text
Processing (STEP 2008).
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proc. of the First Annual Meeting of the North
American Chapter of the Association for Computational
Linguistics (NAACL 2000), pages 132?139.
Timothy Chklovski and Patrick Pantel. 2004. VerbOcean:
Mining the Web for fine-grained semantic verb relations.
In Proc. of the Conference on Empirical Methods in
Natural Language Processing (EMNLP-04).
Roxana Girju. 2003. Automatic detection of causal rela-
tions for question answering. In Proc. of the ACL 2003
Workshop on Multilingual Summarization and Question
Answering ? Machine Learning and Beyond.
Jonathan Gordon and Lenhart K. Schubert. 2010. Quan-
tificational sharpening of commonsense knowledge. In
Proc. of the AAAI 2010 Fall Symposium on Common-
sense Knowledge.
Andrew Gordon and Reid Swanson. 2009. Identifying
personal stories in millions of weblog entries. In Proc.
of the Third International Conference on Weblogs and
Social Media (ICWSM), Data Challenge Workshop.
Marti Hearst. 1998. Automated discovery of WordNet
relations. In Christiane Fellbaum, editor, WordNet: An
Electronic Lexical Database and Some of Its Applica-
tions. MIT Press.
Henry Kuc?era and W. N. Francis. 1967. Computational
Analysis of Present-Day American English. Brown
University Press.
Dekang Lin and Patrick Pantel. 2001. DIRT: Discov-
ery of inference rules from text. In Proc. of the ACM
Conference on Knowledge Discovery and Data Mining
(KDD).
Patrick Pantel, Rahul Bhagat, Timothy Chklovski, and Ed-
uard Hovy. 2007. ISP: Learning inferential selectional
preferences. In Proc. of NAACL-HLT 2007.
Viktor Pekar. 2006. Acquisition of verb entailment from
text. In Proc. of HLT-NAACL 2006.
Doug Rohde. 2001. TGrep2 manual. Unpublished
manuscript, Brain & Cognitive Science Department,
MIT.
Stefan Schoenmackers, Oren Etzioni, Daniel S. Weld, and
Jesse Davis. 2010. Learning first-order horn clauses
from Web text. In Proc. of EMNLP 2010.
Lenhart K. Schubert and Chung Hee Hwang. 2000.
Episodic Logic Meets Little Red Riding Hood: A com-
prehensive, natural representation for language under-
standing. In L. Iwanska and S. C. Shapiro, editors,
Natural Language Processing and Knowledge Repre-
sentation: Language for Knowledge and Knowledge for
Language. MIT/AAAI Press.
Satoshi Sekine, editor. 2008. Notebook of the NSF Sympo-
sium on Semantic Knowledge Discovery, Organization,
and Use. New York University, 14?15 November.
Benjamin Van Durme and Lenhart K. Schubert. 2008.
Open knowledge extraction through compositional lan-
guage processing. In Proc. of the Symposium on Se-
mantics in Text Processing (STEP 2008).
Benjamin Van Durme, Phillip Michalak, and Lenhart K.
Schubert. 2009. Deriving Generalized Knowledge
from Corpora using WordNet Abstraction. In Proc. of
EACL 2009.
63
Proc. EACL 2012 Workshop on Applications of Tree Automata Techniques in Natural Language Processing, pages 21?30,
Avignon, France, April 24 2012. c?2012 Association for Computational Linguistics
TTT: A tree transduction language for syntactic and semantic processing
Adam Purtee
University of Rochester
Department of Computer Science
apurtee@cs.rochester.edu
Lenhart Schubert
University of Rochester
Department of Computer Science
schubert@cs.rochester.edu
Abstract
In this paper we present the tree to tree
transduction language, TTT. We moti-
vate the overall ?template-to-template? ap-
proach to the design of the language, and
outline its constructs, also providing some
examples. We then show that TTT al-
lows transparent formalization of rules for
parse tree refinement and correction, log-
ical form refinement and predicate disam-
biguation, inference, and verbalization of
logical forms.
1 Introduction
Pattern matching and pattern-driven transforma-
tions of list-structured symbolic expressions or
trees are fundamental tools in AI. They facilitate
many symbol manipulation tasks, including oper-
ations on parse trees and logical forms, and even
inference and aspects of dialogue and translation.
The TTT system allows concise and transpar-
ent specification of rules for such tasks, in par-
ticular (as we will show), parse tree refinement
and correction, predicate disambiguation, logical
form refinement, inference, and verbalization into
English.
In parse tree refinement, our particular focus
has been on repair of malformed parses of image
captions, as obtained by the Charniak-Johnson
parser (Charniak and Johnson, 2005). This has
encompassed such tasks as distinguishing pas-
sive participles from past participles and temporal
nominals from non-temporal ones, among other
tasks which will be discussed later. For exam-
ple, standard treebank parses tag both past par-
ticiples (as in ?has written?) and passive partici-
ples (as in ?was written?) as VBN. This is undesir-
able for subsequent compositional interpretation,
as the meanings of past and passive participles are
distinct. We can easily relabel the past partici-
ples as VBEN by looking for parse tree subex-
pressions where a VBN is preceded by a form of
?have?, either immediately or with an interven-
ing adverb or adverbial, and replacing VBN by
VBEN in such subexpressions. Of course this can
be accomplished in a standard symbol manipula-
tion language like Lisp, but the requisite multi-
ple lines of code obscure the simple nature of the
task. We have also been able to repair system-
atic PP (prepositional phrase) misattachments, at
least in the limited domain of image captions. For
example, a common error is attachment of a PP
to the last conjunct of a conjunction, where in-
stead the entire conjunction should be modified by
the PP. Thus when a statistically obtained parse of
the sentence ? Tanya and Grandma Lillian at her
highschool graduation party? brackets as ?Tanya
and (Grandma Lillian (at her highschool gradu-
ation party.))?, we want to lift the PP so that ?at
her highschool graduation party? modifies ?Tanya
and Grandma Lillian?.
Another systematic error is faulty classification
of relative pronouns/determiners as wh-question
pronouns/determiners, e.g., ?the student whose
mother contacted you? vs. ?I know whose mother
contacted you? ? an important distinction in com-
positional semantics. (Note that only the first oc-
currence, i.e., the relative determiner, can be para-
phrased as with the property that his, and only the
second occurrence, in which whose forms a wh-
nominal, can be paraphrased as the person with
the property that his.) An important point here is
that detecting the relative-determiner status of a
wh-word like whose may require taking account
21
of an arbitrarily deep context. For example, in
the phrase ?the student in front of whose par-
ents you are standing?, whose lies two levels of
phrasal structure below the nominal it is seman-
tically bound to. Such phenomena motivate the
devices in TTT for detecting ?vertical patterns?
of arbitrary depth. Furthermore, we need to be
able to make local changes ?on the fly? in match-
ing vertical patterns, because the full set of tree
fragments flanking a vertical match cannot in gen-
eral be saved using match variables. In the case
of a wh-word that is to be re-tagged as a relative
word, we need to rewrite it at the point where
the vertical pattern matches it, rather than in a
separate tree-(re)construction phase following the
tree-matching phase.
An example of a discourse phenomenon that
requires vertical matching is anaphoric referent
determination. In particular, consider the well-
known rule that a viable referent for an anaphoric
pronoun is an NP that C-commands it, i.e., that is
a (usually left) sibling of an ancestor of the pro-
noun. For example, in the sentence ?John shows
Lillian the snowman that he built?, the NP for
John C-commands the pronominal NP for he, and
thus is a viable referent for it (modulo gender and
number agreement). We will later show a sim-
ple TTT rule that tags such an anaphoric pronoun
with the indices of its C-commanding NP nodes,
thus setting the stage for semantic interpretation.
We have also been able to perform Skolemiza-
tion, conjunct separation, simple inference, and
logical form verbalization with TTT and suspect
its utility to logic tasks will increase as develop-
ment continues.
The rest of the paper is organized as follows:
we discuss related work in section 2, discuss the
TTT language (including pattern matching and
transduction syntax, and some theoretical proper-
ties) in section 3, and go though several detailed
example applications in section 4.
A beta version of the system can be found at
http://www.cs.rochester.edu/research/ttt/.
2 Related Work
There are several pattern matching and transduc-
tion facilities available; however, none proved
sufficiently general and perspicuous to serve our
various purposes. The Tiburon tool is a com-
prehensive system for manipulating regular tree
grammars, tree automata, and tree transducers,
including weighted variants (May and Knight,
2008). It supports many useful algorithms, such
as intersection, determinization, recognition, top-
k generation, and maximum likelihood training.
However, variables that appear in both a rule?s lhs
and rhs must occur at a depth less than two on the
left, and Tiburon cannot easily simulate our verti-
cal path or sequence operators.
Timbuk is a system for deciding reachability
with term rewriting systems and tree automata
(Genet, 2003), and it also performs intersec-
tion, union, and determinization of tree automata.
Though variables can appear at arbitrary locations
in terms, they always match exactly one term from
a fixed set, and therefore do not match sequences
or vertical paths.
The three related tools Tgrep, Tregex, and
Tsurgeon provide powerful tree matching and re-
structuring capabilities (Levy and Andrew, 2006).
However, Tgrep and Tregex provide no transduc-
tion mechanism, and Tsurgeon?s modifications
are limited to local transformations on trees. Also,
it presupposes list structures that begin with an
atom (as in Treebank trees, but not in parse trees
with explicit phrasal features), and its patterns are
fundamentally tree traversal patterns rather than
tree templates, and can be quite hard to read.
Xpath and XSLT are languages for manipula-
tion of XML trees (World Wide Web Consortium,
1999; World Wide Web Consortium, 1999). As its
name indicates, Xpath expressions describe paths
in trees to the relevant nodes, rather than patterns
representing the trees to be matched, as in the
TTT approach. It is useful for extracting struc-
tured but unordered information from trees, and
supports numerous functions and predicates over
matched nodes, but does not match ordered se-
quences. XSLT is also more procedurally oriented
than TTT, and is useful for constructing XML rep-
resentations of transformations of data extracted
by Xpath. The primary advantages of TTT over
Xpath and XSLT are a more concise syntax, or-
dered sequence matching, compositional patterns
and templates, and in-place modification of trees.
Peter Norvig?s pattern matching language,
?pat-match?, from (Norvig, 1991) provides a nice
pattern matching facility within the Lisp environ-
ment, allowing for explicit templates with vari-
ables (that can bind subexpressions or sequences
of them), and including ways to apply arbitrary
tests to expressions and to match boolean combi-
22
nations of patterns. However, there is no provi-
sion for ?vertical? pattern matching or subexpres-
sion replacement ?on the fly?, which are features
of TTT we have found useful. Also the notation
for alternatives, along with exclusions, is more
concise than in Norvig?s matcher, for instance not
requiring explicit ORs. Like TTT, pat-match sup-
ports matching multi-level structures, but unlike
TTT, the pattern operators are not composable.
Mathematica also allows for sophisticated pat-
tern matching, including matching of sequences
and trees. It also includes a term rewriting sys-
tem that is also capable of rewriting ordered se-
quences. It provides functions to apply patterns to
arbitrary subtrees of a tree until all matches have
been found or some threshold count is reached,
and it can return all possible ways of applying a
set of rules to an expression. However, as in the
case of Norvig?s matcher there is no provision for
vertical patterns or on-the-fly transduction (Wol-
fram Research, Inc, 2010).
3 TTT
Pattern Matching
Patterns in TTT are hierarchically composed of
sub-patterns. The simplest kind of pattern is an
arbitrary, explicit list structure (tree) containing
no match operators, and this will match only an
identical list structure. Slightly more flexible pat-
terns are enabled by the ?underscore operators?
!, +, ?, *. These match any single tree, any
non-empty sequence of trees, the empty sequence
or a sequence of one tree, and any (empty or non-
empty) sequence of trees respectively. These op-
erators (as well as all others) can also be thought
of as match variables, as they pick up the tree or
sequence of trees they match as their binding.
The bindings are ?non-sticky?, i.e., an operator
such as ! will match any tree, causing replace-
ment of any prior binding (within the same pat-
tern) by that tree. However, bindings can be pre-
served in two ways: by use of new variable names,
or by use of sticky variables. New variable names
are obtained by appending additional characters
? conventionally, digits ? to the basic ones, e.g.,
!1, !2, etc. Sticky variables are written with a
dot, i.e., !., +., ?., *., where again these
symbols may be followed by additional digits or
other characters. The important point concern-
ing sticky variables is that multiple occurrences of
such a variable in a pattern can only be bound by
the same unique value. Transductions are spec-
ified by a special pattern operator / and will be
described in the next section.
More flexible operators, allowing for alter-
natives, negation, and vertical patterns among
other constructs, are written as a list headed by
an operator without an underscore, followed by
one or more arguments. For example, (! A
(B C)) will match either the symbol A or the
list (B C), i.e., the two arguments provide al-
ternatives. As an example involving negation,
(+ A (B !) ? (B B)) will match any
nonempty sequence whose elements are As or
two-element lists headed by B, but disallowing el-
ements of type (B B). Successful matches cause
the matched expression or sequence of expres-
sions to become the value of the operator. Again,
sticky versions of match operators use a dot, and
the operators may be extended by appending dig-
its or other characters.
The ten basic argument-taking pattern opera-
tors are:
! Match exactly one sub-pattern argument.
+ Match a sequence of one or more arguments.
? Match the empty sequence or one argument.
* Match the empty sequence or one or more
arguments.
{} Match any permutation of the arguments.
<> Match the sequence of arguments directly
(without the parenthesis enclosing the <>
operator)
? Match a tree that has a child matching one of
the arguments.
?* Match a tree that has a descendant matching
one of the arguments.
?@ Match a vertical path.
/ Attempt a transduction. (Explained later.)
Various examples will be provided below. Any of
the arguments to a pattern operator may be com-
posed of arbitrary patterns.
Negation: The operators !, +, ?, *, and ? sup-
port negation (pattern exclusion); i.e., the argu-
ments of these operators may include not only al-
ternatives, but also a negation sign ? (after the
23
alternatives) that is immediately followed by one
or more precluded patterns. If no alternatives
are provided, only precluded patterns, this is in-
terpreted as ?anything goes?, except for the pre-
cluded patterns. For example, (+ ? (A A) B)
will match any nonempty sequence of expressions
that contains no elements of type (A A) or B.
Note that the negation operator does not appear
by itself; one must instead specify it in conjunc-
tion with one of the other operators. The pattern
(! ? P) matches any single tree which does not
match pattern P.
Conjunction: We have so far found no com-
pelling need for an explicit conjunction operator.
If necessary, a way to say that a tree must match
each of two or more patterns is to use double
negation. For example, suppose we want to say
that an expression must begin with an A or B but
must contain an A (at the top level); this could be
expressed as
(! ? (! ? ((! A B) *) ( * A *))).
However, this would be more perspicuously ex-
pressed in terms of alternatives, i.e.,
(! (A *) (B * A *)).
We also note that the allowance for computable
predicates (discussed below) enables introduction
of a simple construct like
(! (and? P1 P2)) , where P1 and P2 are ar-
bitrary TTT patterns, and and? is an executable
predicate that applies the TTT matcher to its argu-
ments and returns a non-nil value if both succeed
and nil otherwise. In the former case, the binding
of the outer ! will become the matched tree.
Bounded Iteration: The operators !, +, ?,
*, and ? also support bounded iteration, using
square brackets. This enables one to write pat-
terns that match exactly n, at least n, at most n,
or from n to m times, where n and m are integers.
Eg. (![3] A) would match the sequence A A
A. The vertical operator ?[n] matches trees with
a depth-n descendant that matches one of the op-
erator?s arguments.
Vertical Paths: The operators ?* and ?@ en-
able matching of vertical paths of arbitrary depth.
The first, as indicated, requires the existence of
a descendant of the specified type, while the sec-
ond, with arguments such as (?@ P1 P2 ...
Pn) matches a tree whose root matches P1, and
has a child matching P2, which in turn has a child
matching P3, and so on. Note that this basic form
is indifferent to the point of attachment of each
successive offspring to its parent; but we can also
specify a point of attachment in any of the P1, P2,
etc., by writing @ for one of its children. Because
this operator (@) does not appear outside the ver-
tical path context, it was not listed with the other
operators above. Note as well that the argument
sequence P1 P2 ... can itself be specified as a
pattern (e.g., via (+ ...)), and in this case there
is no advance commitment to the depth of the tree
being matched.
Computable Predicates: Arbitrary predicates
can be used during the pattern matching pro-
cess (and consequently the transduction process).
Symbols with names ending in a question mark,
and with associated function definitions, are in-
terpreted as predicates. When a predicate is en-
countered during pattern matching, it is called
with the current subtree as input. The result is
nil or non-nil, and when nil is returned the current
match fails, otherwise it succeeds (but the non-
nil value is not used further). Additionally, sup-
porting user-defined predicates enables the use of
named patterns.
Some Example Patterns: Here are examples
of particular patterns, with verbal explanations.
Also see Table 1, at the top of the next page, for
additional patterns with example bindings.
? (! (+ A) (+ B))
Matches a non-empty sequence of A?s or a
non-empty sequence of B?s, but not a se-
quence containing both.
? (* (<> A A))
Matches an even number of A?s.
? (B (* (<> B B)))
Matches an odd number of B?s.
? (({} A B C))
Matches (A B C), (A C B), (B A C),
(B C A), (C A B) and (C B A) and
nothing else.
? ((<> A B C))
Matches (A B C) and nothing else.
? (?* X)
Matches any tree that has descendant X.
? (?@ (+ (@ *)) X)
Matches any tree with leftmost leaf X.
24
Pattern Tree Bindings
! (A B C) ( ! (A B C)
( * F) (A B (C D E) F) ( * A B (C D E))
(A B ? F) (A B (C D E) F) ( ? (C D E))
(A B ? (C D E) F) (A B (C D E) F) ( ?)
(?@ ! (C *) E) (A B (C D E) F) (?@ (A B (C D E) F)) ( * D E)
(A B (<> (C D E)) F) (A B (C D E) F) (<> (C D E))
(A B (<> C D E) F) (A B (C D E) F) nil
Table 1: Binding Examples
Transductions
Transductions are specified with the transduction
operator, /, which takes two arguments. The left
argument may be any tree pattern and the right
argument may be constructed of literals, variables
from the lhs pattern, and function calls.
Transductions may be applied to the roots of
trees or arbitrary subtrees, and they may be re-
stricted to apply at most once, or until conver-
gence. When applying transductions to arbitrary
subtrees, trees are searched top-down, left to right.
When a match to the transduction lhs pattern oc-
curs, the resulting bindings and transduction rhs
are used to create a new tree, which then replaces
the tree (or subtree) that matched the lhs.
Here are a few examples of simple template to
template transductions:
? (/ X Y)
Replaces the symbol X with the symbol Y.
? (/ (! X Y Z) (A))
Replaces any X, Y, or Z with A.
? (/ (! X) (! !))
Duplicates an X.
? (/ (X * Y) (X Y))
Remove all subtrees between X and Y.
? (/ ( ! * !1) ( !1 * !))
Swaps the subtrees on the boundaries.
A transduction operator may appear nested within
a composite pattern. The enclosing pattern ef-
fectively restricts the context in which the trans-
duction will be applied, because only a match to
the entire pattern will trigger a transduction. In
this case, the transduction is applied at the lo-
cation in the tree where it matches. The rhs of
such a transduction is allowed to reference the
bindings of variables that appear in the enclos-
ing pattern. We call these local transductions, as
distinct from replacement of entire trees. Local
transductions are especially advantageous when
performing vertical path operations, allowing for
very concise specifications of local changes. For
example, the transduction
(?@ (* ((! S SBAR) +))
(/ (WH !)
(REL-WH (WH !))))
wraps (REL-WH ...) around a (WH ...)
constituent occurring as a descendant of a ver-
tical succession of clausal (S or SBAR) con-
stituents. Applied to the tree (S (SBAR (WH
X) B) A), this yields the new tree (S (SBAR
(REL-WH (WH X)) B) A). Additional ex-
amples appear later (especially in the parse tree
refinement section).
TTT also supports constructive functions, with
bound variables as arguments, in the rhs tem-
plates, such as join-with-dash!, which con-
catenates all the bound symbols with interven-
ing dashes, and subst-new!, which will be
discussed later. One can imagine additional
functions, such as reverse!, l-shift!,
r-shift!, or any other function of a list of
terms that may be useful to the application at
hand. Symbols with names ending in the excla-
mation mark are assumed to be associated with
function definitions, and when appearing as the
first element of a list are executed during out-
put template construction. To avoid writing many
near-redundant functions, we use the simple func-
tion apply! to apply arbitrary Lisp functions
during template construction.
Theoretical Properties
A thorough treatment of the formal properties
of tree transducers is (Comon, 2007). A good
25
overview of the dimensions of variability among
formal tree transducers is given in (Knight, 2007).
The main properties are restrictions on the height
of the tree fragments allowed in rules, linearity,
and whether the rules can delete arbitrary sub-
trees. Among the more popular and recent ones,
synchronous tree substitution grammars (STSG),
synchronous tree sequence substitution grammars
(STSSG), and multi bottom-up tree transduc-
ers (MBOT) constrain their rules to be linear
and non-deleting, which is important for efficient
rule learning and transduction execution (Chiang,
2004; Galley et. al, 2004; Yamada and Knight,
2001; Zhang et. al, 2008; Maletti, 2010).
The language TTT does not have any such
restrictions, as it is intended as a general pro-
gramming aid, with a concise syntax for po-
tentially radical transformations, rather than a
model of particular classes of linguistic opera-
tions. Thus, for example, the 5-element pat-
tern (! ((* A) B) ((* A) C) ((* A) D)
((* A) E) ((* A))) applied to the expres-
sion (A A A A A) rescans the latter 5 times, im-
plying quadratic complexity. (Our current imple-
mentation does not attempt regular expression re-
duction for efficient recognition.) With the addi-
tion of the permutation operator {}, we can force
all permutations of certain patterns to be tried in
an unsuccessful match (e.g., (({} (! A B C)
(! A B C) (! A B C))) applied to (C B
E)), leading to exponential complexity. (Again,
our current implementation does not attempt to
optimize.) Also, allowance for repeated applica-
tion of a set of rules to a tree, until no further
applications are possible, leads to Turing equiv-
alence. This of course is true even if only the 4
underscore-operators are allowed: We can simu-
late the successive transformations of the config-
urations of a Turing machine with string rewriting
rules, which are easily expressed in terms of those
operators and /. Additionally, pattern predicates
and function application in the right-hand sides of
rules are features present in TTT that are not in-
cluded in the above formal models. In themselves
(even without iterative rule application), these un-
restricted predicates and functions lead to Turing
equivalence.
The set of pattern matching operators was cho-
sen so that a number of disparate pattern match-
ing programs could all be replaced with concise
TTT rules. It does subsume regular tree expres-
sions and can therefore be used to match any reg-
ular tree language. Specifically, alternation can
be expressed with ! and (vertical) iteration with
?@ and *. The example expression from (Comon,
2007) can be specified as (?@ (* (cons 0
@)) nil), which matches Lisp expressions cor-
responding to lists of zero or more zeros. TTT
also differs from standard tree automata by lack
of an explicit state.
Nondeterminism and noncommutativity: In
general, given a set of transductions (or even a sin-
gle transduction) and an input tree there may be
several ways to apply the transductions, resulting
in different trees. This phenomenon comes from
three sources:
? Rule application order - transductions are not
in general commutative.
? Bindings - a pattern may have many sets of
consistent bindings to a tree (e.g., pattern
( * *1) can be bound to the tree (X Y
Z) in four distinct ways).
? Subtree search order - a single transduction
may be applicable to a tree in multiple lo-
cations (e.g., (/ ! X) could replace any
node of a tree, including the root, with a sin-
gle symbol).
Therefore some trees may have many reduced
forms with respect to a set of transductions (where
by reduced we mean a tree to which no trans-
ductions are applicable) and even more reachable
forms.
Our current implementation does not attempt to
enumerate possible transductions. Rather, for a
given tree and a list of transductions, each trans-
duction (in the order given) is applied in top-down
fashion at each feasible location (matching the
lhs), always using the first binding that results
from this depth-first, left-to-right (i.e., pre-order)
search. Our assumption is that the typical user has
a clear sense of the order in which transformations
are to be performed, and is working with rules that
do not interact in unexpected ways. For exam-
ple, consider the cases of PP misattachment men-
tioned earlier. In most cases, such misattachments
are disjoint (e.g., consider a caption reading ?John
and Mary in front and David and Sue in the back?,
where both PPs may well have been attached to
the proper noun immediately to the left, instead
26
of to the appropriate conjunction). It is also pos-
sible for one rule application to change the context
of another, but this is not necessarily problematic.
For instance, suppose that in the sentence ?John
drove the speaker to the airport in a hurry? the PP
?to the airport? has been misattached to the NP
for ?the speaker? and that the PP ?in a hurry? has
been misattached to the NP for ?the airport?. Sup-
pose further that we have a repair rule that carries
a PP attached to an NP upward in the parse tree
until it reaches a VP node, reattaching the PP as a
child of that VP. (The repair rule might incorpo-
rate a computable predicate that detects a poor fit
between an NP and a PP that modifies it.) Then
the result will be the same regardless of the order
in which the two repairs are carried out. The dif-
ference is just that with a preorder discipline, the
second PP (?in a hurry?) will move upward by one
step less than if the order is reversed, because the
first rule application will have shortened the path
to the dominating VP by one step.
In the future it may be worthwhile to implement
exhaustive exploration of all possible matches and
expression rewrites, as has been done in Mathe-
matica. In general this would call for lazy compu-
tation, since the set of rewrites may be an infinite
set.
4 Some linguistic examples
Parse Tree Refinement: First, here is a sim-
ple transduction to delete empty brackets, which
sometimes occur in the Brown corpus: (/ ( *
() *1) ( * *1)).
To distinguish between past and passive partici-
ples, we want to search for the verb have, and
change the participle token correspondingly, as
discussed earlier. The next two transductions are
equivalent ? the first is global and the second is an
example of a local or on-the-fly transduction. For
simplicity we consider only the has form of have.
Observe the more concise form, and simpler vari-
able specifications of the second transduction.
(/ (VP _* (VBZ HAS) _*1 (VBN _!) _*2)
(VP _* (VBZ HAS) _*1 (VBEN _!) _*2))
(VP _* (VBZ HAS) _* ((/ VBN VBEN) _!) _*)
To distinguish temporal and non-temporal
nominals, we use a computable predicate to de-
tect temporal nouns, and then annotate the NP tag
accordingly. (One more time, we show global and
local variants.)
(/ (NP * nn-temporal?)
(NP-TIME * nn-temporal?))
((/ NP NP-TIME) * nn-temporal?)
Assimilation of verb particles into single con-
stituents is useful to semantic interpretation, and
is accomplished with the transduction:
(/ (VP (VB \_!1)
(\{\} (PRT (RP _!2)) (NP _*1)))
(VP (VB _!1 _!2) (NP _*1)))
We often particularize PPs to show the
preposition involved, e.g., PP-OF, PP-FROM,
etc. Note that this transduction uses the
join-with-dash! function, which enables us
to avoid writing a separate transduction for each
preposition:
(/ (PP (IN !) *1)
((join-with-dash! PP !)
(IN !) *1))
Such a rule transforms subtrees such as (PP (IN
FROM)) by rewriting the PP tag as (PP-FROM
(IN FROM) .
As a final syntactic processing example (tran-
sitioning to discourse phenomena and semantics),
we illustrate the use of TTT in establishing poten-
tial coreferents licensed by C-command relations,
for the sentence mentioned earlier. We assume
that for reference purposes, NP nodes are deco-
rated with a SEM-INDEX feature (with an integer
value), and pronominal NPs are in addition deco-
rated with a CANDIDATE-COREF feature, whose
value is a list of indices (initially empty). Thus we
have the following parse structure for the sentence
at issue (where for understandabilty of the rela-
tively complex parse tree we depart from Tree-
bank conventions not only in the use of some ex-
plicit features but also in using linguistically more
conventional phrasal and part-of-speech category
names; R stands for relative clause):
(S ((NP SEM-INDEX 1) (NAME John))
(VP (V shows)
((NP SEM-INDEX 2) (NAME Lillian))
((NP SEM-INDEX 3) (DET the)
(N (N snowman)
(R (RELPRON that)
((S GAP NP)
((NP SEM-INDEX 4
CANDIDATE-COREF ())
(PRON he))
((VP GAP NP) (V built)
((NP SEM-INDEX 4)
(PRON *trace*)))))))))
Here is a TTT rule that adjoins the index of
a C-commanding NP node to the CANDIDATE-
COREF list of a C-commanded pronominal NP:
(_* ((NP _* SEM-INDEX _!. _*) _+) _*
27
(?* ((NP _* CANDIDATE-COREF
(/ _!(adjoin! _!. _!)) _*) (PRON _!))) _*)
The NP on the first line is the C-commanding
NP, and note that we are using a sticky vari-
able ? !.? for its index, since we need to use it
later. (None of the other match variables need
to be sticky, and we reuse ? *? and ? !? multi-
ple times.) The key to understanding the rule is
the constituent headed by ??*?, which triggers a
search for a (right) sibling or descendant of a sib-
ling of the NP node that reaches an NP consisting
of a pronoun, and thus bearing the CANDIDATE-
COREF feature. This feature is replaced ?on the
fly? by adjoining the index of the C-commanding
node (the value of ? !.?) to it. For the sample
tree, the result is the following (note the value
?(1)? of the CANDIDATE-COREF list):
(S ((NP SEM-INDEX 1) (NAME John))
(VP (V shows)
((NP SEM-INDEX 2) (NAME Lillian))
((NP SEM-INDEX 3) (DET the)
(N (N snowman)
(R (RELPRON that)
((S GAP NP)
((NP SEM-INDEX 4
CANDIDATE-COREF (1))
(PRON he))
((VP GAP NP) (V built)
((NP SEM-INDEX 4)
(PRON *trace*)))))))))
Of course, this does not yet incorporate number
and gender checks, but while these could be in-
cluded, it is preferable to gather candidates and
heuristically pare them down later. Thus repeated
application of the rule would also add the index 2
(for Lillian) to CANDIDATE-COREF.
Working with Logical Forms
Skolemization: Skolemization of an existential
formula of type (some x R S), where x is
a variable, R is a restrictor formula and S is the
nuclear scope, is performed via the transduction
(/ (some ! !1 !2)
(subst-new! ! ( !1 and.cc !2))).
The function subst-new! replaces all oc-
currences of a free variable symbol in an
expression with a new one. (We assume that
no variable occurs both bound and free in the
same expression.) It uses a TTT transduction
to accomplish this. For example, (some x
(x politician.n) (x honest.a)) be-
comes ((C1.skol politician.n) and.cc
(C1.skol honest.a)).
Inference: We can use the following rule to ac-
complish simple default inferences such as that if
most things with property P have property Q, and
most things with property Q have property R,
then (in the absence of knowledge to the contrary)
many things with property P also have property
R. (Our logical forms use infix syntax for predica-
tion, i.e., the predicate follows the ?subject? argu-
ment. Predicates can be lambda abstracts, and the
computable boolean function pred? checks for
arbitrary predicative constructs.)
(/
(_* (most _!.1
(_!.1 (!.p pred?))
(_!.1 (!.q pred?)))
_* (most _!.2
(_!.2 !.q)
(_!.2 (!.r pred?))) _*)
(many _!.1 (_!.1 !.p) (_!.1 !.r)))
For example, ((most x (x dog.n) (x pet.n))
(most y (y pet.n) (x friendly.a))) yields
the default inference (many (x dog.n) (x
friendly.a)).
The assumption here is that the two most-
formulas are embedded in a list of formulas (se-
lected by the inference algorithm), and the three
occurrences of * allow for miscellaneous sur-
rounding formulas. (To allow for arbitrary or-
dering of formulas in the working set, we also
provide a variant with the two most-formulas in
reverse order.) Inference with tree transduction
rules has also been performed by (Koller and Ste-
fan, 2010).
Predicate Disambiguation: The following
rules are applicable to patterns of predica-
tion such as ((det dog.n have.v (det
tail.n)), ((det bird.n have.v (det
nest.n)), and ((det man.n) have.v
(det accident.n)). (Think of det as an
unspecified, unscoped quantifier.) The rules
simultaneously introduce plausible patterns of
quantification and plausible disambiguations of
the various senses of have.v (e.g., have as part,
possess, eat, experience):
(/ ((det (! animal?)) have.v
(det (!1 animal-part?)))
(all-or-most x (x !)
(some e ((pair x e) enduring)
(some y (y !1)
((x have-as-part.v y) ** e)))))
(/ ((det (! agent?)) have.v
(det (!1 possession?)))
(many x (x !)
(some e
(some y (y !1)
(x possess.v y) ** e))))
28
(/ ((det (! animal?)) have.v
(det (!1 food?)))
(many x (x !)
(occasional e
(some y (y !1)
(x eat.v y) ** e))))
(/ ((det (! person?)) have.v
(det (!1 event?)))
(many x (x !)
(occasional e
(some y (y !1)
((x experience.v y) ** e)))))
Computable predicates such as animal? and
event? are evaluated with the help of WordNet
and other resources. Details of the logical form
need not concern us, but it should be noted that
the ?**? connects sentences to events they charac-
terize much as in various other theories of events
and situations.
Thus, for example, ((det dog.n have.v
(det tail.n)) is mapped to:
(all-or-most x (x dog.n
(some e ((pair x e) enduring)
(some y (y tail.n)
((x have-as-part.v y) ** e)))))
This expresses that for all or most dogs, the dog
has an enduring attribute (formalized as an agent-
event pair) of having a tail as a part.
Logical Interpretation: The following trans-
ductions directly map some simple parse trees to
logical forms. The rules, applied as often as possi-
ble to a parse tree, replace all syntactic constructs,
recognizable from (Treebank-style) phrase head-
ers like (JJ ...), (NP ...), (VP ...), (S
...), etc., by corresponding semantic constructs.
For example, ?The dog bit John Doe?, parsed as
(S (NP (DT the) (NN dog))
(VP (VBD bit)
(NP (NNP John) (NNP Doe))))
yields (the x (x dog.n) (x bit.v
John Doe.name)).
Type-extensions such as ?.a?, ?.n?, and ?.v?
indicate adjectival, nominal, and verbal predi-
cates, and the extension ?.name? indicates an in-
dividual constant (name); these are added by the
functions make-adj!, make-noun!, and so
on. The fourth rule below combines two succes-
sive proper nouns (NNPs) into one. We omit event
variables, tense and other refinements.
(/ (JJ !) (make-adj! !))
(/ (NN !) (make-noun! !))
(/ (VBD !) (make-verb! !))
(/ ( *.a (NNP !.1) (NNP !.2) *.b)
( *.a (NNP !.1 !.2) *.b))
(/ (NNP +) (make-name! ( +)))
(/ (NP !) !)
(/ (S (NP (DT the) !) (VP +))
(the x (x !) (x +))
These rules are illustrative only, and are not
fully compositional, as they interpret an NP with
a determiner only in the context of a senten-
tial subject, and a VP only in the context of a
sentential predicate. Also, by scoping the vari-
able of quantification, they do too much work at
once. A more general approach would use com-
positional rules such as (/ (S (!1 NP?) (!2
VP?)) ((sem! !1) (sem! !2))), where the
sem! function again makes use of TTT, re-
cursively unwinding the semantics, with rules
like the first five above providing lexical-level
sem!-values.
We have also experimented with rendering log-
ical forms back into English, which is rather eas-
ier, mainly requiring dropping of variables and
brackets and some reshuffling of constituents.
5 Conclusion
The TTT language is well-suited to the applica-
tions it was aimed at, and is already proving use-
ful in current syntactic/semantic applications. It
provides a very concise, transparent way of speci-
fying transformations that previously required ex-
tensive symbolic processing. Some remaining is-
sues are efficient access to, and deployment of,
rules that are locally relevant to a transduction;
and heuristics for executing matches and trans-
ductions more efficiently (e.g., recognizing vari-
ous cases where a complex rule cannot possibly
match a given tree, because the tree lacks some
constituents called for by the rule; or use of ef-
ficient methods for matching regular-expression
subpatterns).
The language also holds promise for rule-
learning, thanks to its simple template-to-
template basic syntax. The kinds of learning en-
visioned are learning parse-tree repair rules, and
perhaps also LF repair rules and LF-to-English
rules.
Acknowledgments
The work was supported by ONR-STTR award
N00014-11-10417, and NSF grants IIS-1016735,
NSF IIS-0916599, and NSF IIS-0910611.
29
References
Eugene Charniak and Mark Johnson. 2005. Coarse-
to-Fine n-Best Parsing and MaxEnt Discriminative
Reranking. ACL 2005, 173?180. Association for
Computational Linguistics, Ann Arbor, MI, USA.
David Chiang. 2004. Evaluation of Grammar For-
malisms for Applications to Natural Language Pro-
cessing and Biological Sequence Analysis. Phd.
Thesis. University of Pennsylvania.
H. Comon and M. Dauchet and R. Gilleron and
C. Lo?ding and F. Jacquemard and D. Lugiez
and S. Tison and M. Tommasi 2007. Tree Au-
tomata Techniques and Applications Available on:
http://www.grappa.univ-lille3.fr/tata
Michel Galley and Mark Hopkins and Kevin Knight
and Daniel Marcu 2004. What?s in a Transla-
tion Rule?. NAACL 2004, 273?280. Boston, MA,
USA.
Thomas Genet and Valerie View Triem Tong
2003. Timbuk: A Tree Automata Library
http://www.irisa.fr/celtique/genet/timbuk/
Ralph Griswold 1971. The SNOBOL4 Programming
Language. Prentice-Hall, Inc. Upper Saddle River,
NJ, USA.
Paul Hudak, John Peterson, and Joseph Fasel.
2000. A Gentle Introduction To Haskell: Ver-
sion 98. Los Alamos National Laboratory.
http://www.haskell.org/tutorial/patterns.html.
Alexander Koller and Stefan Thater. 2010. Comput-
ing weakest readings. ACL 2010. 30?39. Strouds-
burg, PA, USA.
Kevin Knight. 2007. Capturing practical natural lan-
guage transformations. Machine Translation, Vol
21, Issue 2, 121?133. Kluwer Academic Publish-
ers. Hingham, MA, USA.
Roger Levy and Galen Andrew 2006. Tregex and
Tsurgeon: tools for querying and manipulating tree
data structures. Language Resources Evaluation
Conference (LREC ?06).
Andreas Maletti 2010. Why synchronous tree sub-
stitution grammars?. HLT 2010. Association for
Computational Linguistics, Stroudsburg, PA, USA.
Jonathan May and Kevin Knight 2008 A Primer on
Tree Automata Software for Natural Language Pro-
cessing. http://www.isi.edu/licensed-sw/tiburon/
Peter Norvig 1991. Paradigms of Artificial Intelli-
gence Programming Morgan Kaufmann. Waltham,
MA, USA.
Don Rozenberg 2002. SnoPy - Snobol
Pattern Matching Extension for Python.
http://snopy.sourceforge.net/user-guide.html.
Wolfram Research, Inc. 2010. Wolfram Mathe-
matica 8 Documentation. Champagne, IL, USA.
http://reference.wolfram.com/mathematica/guide/
RulesAndPatterns.html.
World Wide Web Consortium. 1999. XML Path Lan-
guage (XPath) http://www.w3.org/TR/xpath/
1999. XSL Transformations (XSLT)
http://www.w3.org/TR/xslt
Kenji Yamada and Kevin Knight 2001. A Syntax-
Based Statistical Translation Model. ACL 2001,
523?530. Stroudsburg, PA, USA.
Min Zhang and Hongfei Jiang and Aiti Aw and
Haizhou Li and Chew Lim Tan and Sheng Li 2008.
A tree sequence alignment-based tree-to-tree trans-
lation model. ACL 2008.
30
Proceedings of the ACL 2014 Workshop on Semantic Parsing, pages 55?60,
Baltimore, Maryland USA, June 26 2014.
c?2014 Association for Computational Linguistics
From Treebank Parses to Episodic Logic and Commonsense Inference
Lenhart Schubert
Univ. of Rochester
schubert@cs.rochester.edu
1. Introduction and overview
We have developed an approach to broad-coverage
semantic parsing that starts with Treebank parses
and yields scoped, deindexed formulas in Episodic
Logic (EL) that are directly usable for knowledge-
based inference. Distinctive properties of our ap-
proach are
? the use of a tree transduction language, TTT,
to partially disambiguate, refine (and some-
times repair) raw Treebank parses, and also to
perform many deindexing and logical canon-
icalization tasks;
? the use of EL, a Montague-inspired logical
framework for semantic representation and
knowledge representation;
? allowance for nonclassical restricted quanti-
fiers, several forms of modification and reifi-
cation, quasi-quotes and syntactic closures;
? an event semantics that directly represents
events with complex characterizations;
? a scoping algorithm that heuristically scopes
quantifiers, logical connectives, and tense;
? a compositional approach to tense deindexing
making use of tense trees; and
? the use of an inference engine, EPILOG, that
supports input-driven and goal-driven infer-
ence in EL, in a style similar to (but more
general than) Natural Logic.
We have applied this framework to general
knowledge acquisition from text corpora and the
web (though with tense meaning and many other
semantic details stripped away) (e.g., Schubert &
Hwang 2000, Van Durme & Schubert 2008), and
more recently to caption interpretation for family
photos, enabling alignment of names and other de-
scriptors with human faces in the photos, and to in-
terpreting sentences in simple first-reader stories.
Ongoing projects are aimed at full interpretation
of lexical glosses and other sources of explicitly
expressed general knowledge.
We now elaborate some of the themes in the pre-
ceding overview, concluding with comments on
related work and important remaining challenges.
2. Refinement of Treebank parses using TTT
We generate initial logical forms by compositional
interpretation of Treebank parses produced by the
Charniak parser.
1
This mapping is encumbered by
a number of difficulties. One is that current Tree-
bank parsers produce many thousands of distinct
expansions of phrasal categories, especially VPs,
into sequences of constituents. We have overcome
this difficulty through use of enhanced regular-
expression patterns applied to sequences of con-
stituent types, where our interpretive rules are as-
sociated directly with these patterns. About 100
patterns and corresponding semantic rules cover
most of English.
Two other difficulties are that parsers still intro-
duce about one phrasal error for every 10 words,
and these can render interpretations nonsensical;
and even when parses are deemed correct accord-
ing to ?gold standard" annotated corpora, they
often conflate semantically disparate word and
phrase types. For example, prepositional phrases
(PPs) functioning as predicates are not distin-
guished from ones functioning as adverbial modi-
fiers; the roles of wh-words that form questions,
relative clauses, or wh-nominals are not distin-
guished; and constituents parsed as SBARs (sub-
ordinate clauses) can be relative clauses, adver-
bials, question clauses, or clausal nominals. Our
approach to these problems makes use of a new
tree transduction language, TTT (Purtee & Schu-
bert 2012) that allows concise, modular, declara-
tive representation of tree transductions. (As in-
dicated below, TTT also plays a key role in log-
ical form postprocessing.) While we cannot ex-
1
ftp://ftp.cs.brown.edu/pub/nlparser/
55
pect to correct the majority of parse errors in gen-
eral texts, we have found it easy to use TTT for
correction of certain systematic errors in particu-
lar domains. In addition, we use TTT to subclas-
sify many function words and phrase types, and to
partially disambiguate the role of PPs and SBARs,
among other phrase types, allowing more reliable
semantic interpretation.
3. EL as a semantic representation and
knowledge representation
From a compositional perspective, the semantics
of natural language is intensional and richly ex-
pressive, allowing for nonclassical quantifiers and
several types of modification and reification. Yet
many approaches to semantic interpretation rely
on first-order logic (FOL) or some subset thereof
as their target semantic representation. This is jus-
tifiable in certain restricted applications, grounded
in extensional domains such as databases. How-
ever, FOL or description logics are often chosen
as the semantic target even for broad-coverage se-
mantic parsing, because of their well-understood
semantics and proof theory and well-developed in-
ference technology and, in some cases, by a pu-
tative expressiveness-tractability tradeoff. We re-
ject such motivations ? tools should be made to fit
the phenomenon rather than the other way around.
The tractability argument, for example, is simply
mistaken: Efficient inference algorithms for sub-
sets of an expressive representation can also be
implemented within a more comprehensive infer-
ence framework, without forfeiting the advantages
of expressiveness. Moreover, recent work in Nat-
ural Logic, which uses phrase-structured NL di-
rectly for inference, indicates that the richness of
language is no obstacle to rapid inference of many
obvious lexical entailments (e.g., MacCartney &
Manning 2009).
Thus our target representation, EL, taking its
cue from Montague allows directly for the kinds
of quantification, intensionality, modification, and
reification found in all natural languages (e.g.,
Schubert & Hwang 2000, Schubert, to appear).
In addition, EL associates episodes (events, sit-
uations, processes) directly with arbitrarily com-
plex sentences, rather than just with atomic pred-
ications, as in Davidsonian event semantics. For
example, the initial sentence in each of the follow-
ing pairs is interpreted as directly characterizing
an episode, which then serves as antecedent for a
pronoun or definite:
For many months, no rain fell;
this totally dried out the topsoil.
Each superpower menaced the other with its nuclear
arsenal; this situation persisted for decades.
Also, since NL allows for discussion of linguis-
tic and other symbolic entities, so does EL, via
quasi-quotation and substitutional quantification
(closures). These can also express axiom schemas,
and autocognitive reasoning (see further com-
ments in Section 5).
4. Comprehensive scoping and tense
deindexing
Though EL is Montague-inspired, one difference
from a Montague-style intensional logic is that
we treat noun phrase (NP) interpretations as un-
scoped elements, rather than second-order predi-
cates. These elements are heuristically scoped to
the sentence level in LF postprocessing, as pro-
posed in (Schubert & Pelletier 1982). The latter
proposal also covered scoping of logical connec-
tives, which exhibit the same scope ambiguities
as quantifiers. Our current heuristic scoping al-
gorithm handles these phenomena as well as tense
scope, allowing for such factors as syntactic or-
dering, island constraints, and differences in wide-
scoping tendencies among different operators.
Episodes characterized by sentences remain im-
plicit until application of a ?deindexing" algo-
rithm. This algorithm makes use of a contextual
element called a tense tree which is built and tra-
versed in accordance with simple recursive rules
applied to indexical LFs. A tense tree contains
branches corresponding to tense and aspect op-
erators, and in the course of processing one or
more sentences, sequences of episode tokens cor-
responding to clauses are deposited at the nodes by
the deindexing rules, and adjacent tokens are used
by these same rules to posit temporal or causal re-
lations among ?evoked" episodes. A comprehen-
sive set of rules covering all tenses, aspects, and
temporal adverbials was specified in (Hwang &
Schubert 1994); the current semantic parsing ma-
chinery incorporates the tense and aspect rules but
not yet the temporal adverbial rules.
Further processing steps, many implemented
through TTT rules, further transform the LFs so
as to Skolemize top-level existentials and defi-
nite NPs (in effect accommodating their presup-
positions), separate top-level conjuncts, narrow
56
the scopes of certain negations, widen quantifier
scopes out of episodic operator scopes where pos-
sible, resolve intrasentential coreference, perform
lambda and equality reductions, and also gener-
ate some immediate inferences (e.g., inferring that
Mrs. Smith refers to a married woman).
The following example, for the first sentence
above, illustrates the kind of LF generated by our
semantic parser (first in unscoped, indexical form,
then the resulting set of scoped, deindexed, and
canonicalized formulas). Note that EL uses pred-
icate infixing at the sentence level, for readabil-
ity; so for example we have (E0 BEFORE NOW0)
rather than (BEFORE E0 NOW0). ?**? is the op-
erator linking a sentential formula to the episode
it characterizes (Schubert 2000). ADV-S is a type-
shifting operator, L stands for ?, and PLUR is a
predicate modifer that converts a predicate over in-
dividuals into a predicate over sets of individuals.
For many months, no rain fell;
Refined Treebank parse:
(S (PP-FOR (IN for) (NP (CD many) (NNS months))) (|,| |,|)
(NP (DT no) (NN rain)) (VP (VBD fell)) (|:| |;|))
Unscoped, indexical LF (keys :F, :P, etc., are dropped later):
(:F (:F ADV-S (:P FOR.P (:Q MANY.DET (:F PLUR MONTH.N))))
(:I (:Q NO.DET RAIN.N) (:O PAST FALL.V)))
Canonicalized LFs (without adverbial-modifier deindexing):
(MONTHS0.SK (PLUR MONTH.N)), (MONTHS0.SK MANY.A),
((ADV-S (L Y (Y FOR.P MONTHS0.SK)))
((NO Z (Z RAIN.N) (SOME E0 (E0 BEFORE NOW0)
(Z FALL.V))) ** E0)
With adverbial deindexing, the prefixed adver-
bial modifier would become a predication (E0
LASTS-FOR.V MONTHS0.SK); E0 is the episode of
no rain falling and MONTHS0.SK is the Skolem
name generated for the set of many months.
5. Inference using the EPILOG inference engine
Semantic parsers that employ FOL or a subset of
FOL (such as a description logic) as the target rep-
resentation often employ an initial ?abstract" rep-
resentation mirroring some of the expressive de-
vices of natural languages, which is then mapped
to the target representation enabling inference. An
important feature of our approach is that (scoped,
deindexed) LFs expressed in EL are directly us-
able for inference in conjunction with lexical and
world knowledge by our EPILOG inference en-
gine. This has the advantages of not sacrificing
any of the expressiveness of language, of linking
inference more directly to surface form (in prin-
ciple enabling incremental entailment inference),
and of being easier to understand and edit than rep-
resentations remote from language.
EPILOG?s two main inference rules, for
input-driven (forward-chaining) and goal-driven
(backward-chaining) inference, substitute conse-
quences or anti-consequences for subformulas as a
function of polarity, much as in Natural Logic. But
substitutions can be based on world knowledge as
well as lexical knowledge, and to assure first-order
completeness the chaining rules are supplemented
with natural deduction rules such as proof by con-
tradiction and proof of conditional formulas by as-
sumption of the antecedent.
Moreover, EPILOG can reason with the ex-
pressive devices of EL mentioned in Sections 1
and 3 that lie beyond FOL, including general-
ized quantifiers, and reified predicates and propo-
sitions. (Schubert, to appear) contains relevant ex-
amples, such as the inference from Most of the
heavy Monroe resources are located in Monroe-
east, and background knowledge, to the conclu-
sion Few heavy resources are located in Monroe-
west; and inference of an answer to the modally
complex question Can the small crane be used
to hoist rubble from the collapsed building on
Penfield Rd onto a truck? Also, the ability to
use axiom schemas that involve quasi-quotes and
syntactic closures allows lexical inferences based
on knowledge about syntactic classes of lexical
items (i.e., meaning postulates), as well as vari-
ous forms of metareasoning, including reasoning
about the system?s own knowledge and percep-
tions (Morbini & Schubert 2011). Significantly,
the expressiveness of EL/EPILOG does not pre-
vent competitive performance on first-order com-
monsense knowledge bases (derived from Doug
Lenat?s Cyc), especially as the number of KB for-
mulas grows into the thousands (Morbini & Schu-
bert 2009).
In the various inference tasks to which EPI-
LOG was applied in the past, the LFs used for
natural language sentences were based on pre-
sumed compositional rules, without the machin-
ery to derive them automatically (e.g., Schubert &
Hwang 2000, Morbini & Schubert 2011, Stratos
et al. 2011). Starting in 2001, in developing
our KNEXT system for knowledge extraction from
text, we used broad-coverage compositional inter-
pretion into EL for the first time, but since our
goal was to obtain simple general ?factoids"?such
57
as that a person may believe a proposition, peo-
ple may wish to get rid of a dictator, clothes can
be washed, etc. (expressed logically)?our interpre-
tive rules ignored tense, many modifiers, and other
subtleties (e.g., Van Durme & Schubert 2008).
Factoids like the ones mentioned are uncondi-
tional and as such not directly usable for inference,
but many millions of the factoids have been auto-
matically strengthened into quantified, inference-
enabling commonsense axioms (Gordon & Schu-
bert 2010), and allow EPILOG to draw conclusions
from short sentences (Gordon 2014, chapter 6).
An example is the inference from Tremblay is a
singer to the conclusion Quite possibly Tremblay
occasionally performs (or performed) a song (au-
tomatically verbalized from an EL formula). Here
the modal and frequency modification would not
easily be captured within an FOL framework.
Recently, we have begun to apply much more
complete compositional semantic rules to sen-
tences ?in the wild", choosing two settings where
sentences tend to be short (to minimize the impact
of parse errors on semantic interpretation): deriva-
tion and integration of caption-derived knowledge
and image-derived knowledge in a family photo
domain, and interpretation of sentences in first-
reader stories. In the family photo domain, we
have fully interpreted the captions in a small de-
velopment set, and used an EPILOG knowledge
base to derive implicit attributes of the individuals
mentioned in the captions (by name or other des-
ignations). These attributes then served to align
the caption-derived individuals with individuals
detected in the images, and were subsequently
merged with image-derived attributes (with al-
lowance for uncertainty). For example, for the
caption Tanya and Grandma Lillian at her high
school graduation party, after correct interpreta-
tion of her as referring to Tanya, Tanya was in-
ferred to be a teenager (from the knowledge that
a high school graduation party is generally held
for a recent high school graduate, and a recent
high school graduate is likely to be a teenager);
while Grandma Lillian was inferred to be a grand-
mother, hence probably a senior, hence quite pos-
sibly gray-haired, and this enabled correct align-
ment of the names with the persons detected in the
image, determined via image processing to be a
young dark-haired female and a senior gray-haired
female respectively.
In the first-reader domain (where we are using
McGuffey (2005)), we found that we could obtain
correct or nearly correct interpretations for most
simple declaratives (and some of the stories con-
sist entirely of such sentences). At the time of
writing, we are still working on discourse phe-
nomena, especially in stories involving dialogues.
For example, our semantic parser correctly derived
and canonicalized the logical content of the open-
ing line of one of the stories under consideration,
Oh Rosie! Do you see that nest in the apple tree?
The interpretation includes separate speech acts
for the initial interjection and the question. Our
goal in this work is integration of symbolic infer-
ence with inferences from imagistic modeling (for
which we are using the Blender open source soft-
ware), where the latter provides spatial inferences
such as that the contents of a nest in a tree are not
likely to be visible to children on the ground (set-
ting the stage for the continuation of the story).
Phenomena not handled well at this point in-
clude intersentential anaphora, questions with
gaps, imperatives, interjections, and direct ad-
dress (Look, Lucy, ...). We are making progress
on these, by using TTT repair rules for phenom-
ena where Treebank parsers tend to falter, and by
adding LF-level and discourse-level interpretive
rules for the resulting phrasal patterns. Ongoing
projects are aimed at full interpretation of lexical
glosses and other sources of explicitly expressed
general knowledge. However, as we explain in
the concluding section, we do not believe that full-
fledged, deep story understanding will be possible
until we have large amounts of general knowledge,
including not only the kinds of ?if-then" knowl-
edge (about word meanings and the world) we
and others have been deriving and are continuing
to derive, but also large amounts of pattern-like,
schematic knowledge encoding our expectations
about typical object configurations and event se-
quences (especially ones directed towards agents?
goals) in the world and in dialogue.
6. Related work
Most current projects in semantic parsing either
single out domains that assure highly restricted
natural language usage, or greatly limit the seman-
tic content that is extracted from text. For exam-
ple, projects may be aimed at question-answering
over relational databases, with themes such as
geography, air travel planning, or robocup (e.g.,
Ge & Mooney 2009, Artzi & Zettlemoyer 2011,
58
Kwiatkowski et al. 2011, Liang et al. 2011, Poon
2013). Impressive thematic scope is achieved in
(Berant et al. 2013, Kwiatkowski et al. 2013), but
the target semantic language (for Freebase access)
is still restricted to database operations such as
join, intersection, and set cardinality. Another
popular domain is command execution by robots
(e.g., Tellex 2011, Howard et al. 2013, Artzi &
Zettlemoyer 2013).
Examples of work aimed at broader lin-
guistic coverage are Johan Bos? Boxer project
(Bos 2008), Lewis & Steedman?s (2013) CCG-
Distributional system, James Allen et al.?s (2013)
work on extracting an OWL-DL verb ontology
from WordNet, and Draicchio et al.?s (2013)
FRED system for mapping from NL to OWL on-
tology. Boxer
2
is highly developed, but inter-
pretations are limited to FOL, so that the kinds
of general quantification, reification and modifi-
cation that pervade ordinary language cannot be
adequately captured. The CCG-Distributional ap-
proach combines logical and distributional seman-
tics in an interesting way, but apart from the FOL
limitation, the induced cluster-based predicates
lose distinctions such as that between town and
country or between elected to and ran for. As
such, the system is applicable to (soft) entailment
verification, but probably not to reasoning. A
major limitation of mapping natural language to
OWL-DL is that the assertion component of the
latter is essentially limited to atomic predications
and their negations, so that ordinary statements
such as Most students who passed the AI exam
also passed the theory exam, or If Kim and Sandy
get divorced, then Kim will probably get custody
of their children, cannot be represented, let alone
reasoned with.
7. Concluding thoughts
The history of research in natural language under-
standing shows two seemingly divergent trends:
One is the attempt to faithfully capture the log-
ical form of natural language sentences, and to
study entailment relations based on such forms.
The other is the effort to map language onto
preexisting, schematic knowledge structures of
some sort, intended as a basis for understand-
ing and inference ? these might be FrameNet-like
or Minsky-like frames, concepts in a description
logic, Schankian scripts, general plans as under-
2
www.meaningfactory.com/bos/pubs/Bos2008STEP2.pdf
stood in AI, Pustejovskyan telic event schemas,
or something similar. Both perspectives seem to
have compelling merits, and this leads us to sup-
pose that deep understanding may indeed require
both surface representations and schematic repre-
sentations, where surface representations can be
viewed as concise abstractions from, or summaries
of, schema instances or (for generic statements) of
the schemas themselves. But where we differ from
most approaches is that we would want both levels
of representation to support inference. The surface
level should support at least Natural-Logic-like
entailment inference, along with inference chain-
ing ? for which EL and EPILOG are well-suited.
The schematic level would support ?reasonable"
(or default) expectations based on familiar patterns
of events, actions, or relationships. Further, the
schematic level should itself allow for language-
like expressiveness in the specification of roles,
steps, goals, or other components, which might
again be abstractions from more basic schemas.
In other words, we envisage hierarchically orga-
nized schemas whose constituents are expressed
in a language like EL and allow for EPILOG-like
inference. We see the acquisition of such schemas
as the most pressing need in machine understand-
ing. Without them, we are limited to either narrow
or shallow understanding.
Acknowledgements
This work was supported by NSF grant IIS-
0916599 and a subcontract to ONR STTR
N00014-10-M-0297. The comments of the anony-
mous referees helped to improve the paper.
References
J. Allen, J. Orfan, W. de Beaumont, C. M. Teng, L.
Galescu, M. Swift, 2013. Automatically deriv-
ing event ontologies for a commonsense knowledge
base. 10th Int. Conf. on Computational Semantics
(IWCS 2013), Potsdam, Germany, March 19-22.
Y. Artzi and L. Zettlemoyer, 2011. Bootstrap-
ping Semantic Parsers from Conversations. Em-
pirical Methods in Natural Language Processing
(EMNLP), Edinburgh, UK.
Y. Artzi and L. Zettlemoyer, 2013. Weakly supervised
learning of semantic parsers for mapping instruc-
tions to actions. In Trans. of the Assoc. for Com-
putational Linguistics (TACL).
J. Berant, A. Chou, R. Frostig, P. Liang, 2013. Seman-
tic parsing on Freebase from question-answer pairs.
59
Empirical Methods in Natural Language Processing
(EMNLP), Seattle, WA.
J. Bos, 2008. Wide-coverage semantic analysis with
Boxer. In J. Bos and R. Delmonte (eds.), Seman-
tics in Text Processing. STEP 2008 Conference Pro-
ceedings. Research in Computational Semantics,
College Publications, 277?286.
F. Draicchio, A. Gangemi, V. Presutti, and A.G. Nuz-
zolese, 2013. FRED: From natural language text
to RDF and OWL in one click. In P. Cimiano et al.
(eds.). ESWC 2013, LNCS 7955, Springer, 263-267.
R. Ge and R. J. Mooney, 2009. Learning a Compo-
sitional Semantic Parser using an Existing Syntactic
Parser, In Proceedings of the ACL-IJCNLP 2009,
Suntec, Singapore.
J. Gordon, 2014. Inferential Commonsense
Knowledge from Text. Ph.D. Thesis, De-
partment of Computer Science, University
of Rochester, Rochester, NY. Available at
http://www.cs.rochester.edu/u/jgordon/.
J. Gordon and L. K. Schubert, 2010. Quantificational
sharpening of commonsense knowledge. Com-
mon Sense Knowledge Symposium (CSK-10), AAAI
2010 Fall Symposium Series, Arlington, VA.
T. M. Howard, S. Tellex, and N. Roy, 2013. A Natu-
ral Language Planner Interface for Mobile Manipu-
lators. 30th Int. Conf. on Machine Learning, Atlanta,
GA, JMLR: W&CP volume 28.
C. H. Hwang and L. K. Schubert, 1994. Interpreting
tense, aspect, and time adverbials: a compositional,
unified approach. D. M. Gabbay and H. J. Ohlbach
(eds.), Proc. of the 1st Int. Conf. on Temporal Logic,
Bonn, Germany, Springer, 238?264.
T. Kwiatkowski, E. Choi, Y. Artzi, and L. Zettlemoyer,
2013. Scaling semantic parsers with on-the-fly on-
tology matching. Empirical Methods in Natural
Language Processing (EMNLP), Seattle, WA.
T. Kwiatkowski, L. Zettlemoyer, S. Goldwater, and
M. Steedman, 2011. Lexical generalization in
CCG grammar induction for semantic parsing. Em-
pirical Methods in Natural Language Processing
(EMNLP), Edinburgh, UK.
M. Lewis and M. Steedman, 2013. Combined distri-
butional and logical semantics. Trans. of the Assoc.
for Computational Linguistics 1, 179?192.
P. Liang, M. Jordan, and D. Klein, 2011. Learning
dependency-based compositional semantics. Conf.
of the Assoc. for Computational Linguistics (ACL),
Portland, OR.
B. MacCartney and C. D. Manning, 2009. An ex-
tended model of natural logic. 8th Int. Conf. on
Computational Semantics (IWCS-8), Tilburg Uni-
versity, Netherlands. Assoc. for Computational Lin-
guistics (ACL), Stroudsberg, PA.
W. H. McGuffey, 2005 (original edition 1879).
McGuffey?s First Eclectic Reader. John Wiley and
Sons, New York.
F. Morbini and L. K. Schubert, 2011. Metareasoning
as an integral part of commonsense and autocog-
nitive reasoning. In M.T. Cox and A. Raja (eds.),
Metareasoning: Thinking about Thinking, Ch. 17.
MIT Press, Cambridge, MA, 267?282.
F. Morbini & Schubert, 2009. Evaluation of
Epilog: A reasoner for Episodic Logic. Common-
sense?09, June 1-3, Toronto, Canada. Available at
http://commonsensereasoning.org/2009/papers.html.
H. Poon, 2013. Grounded unsupervised semantic pars-
ing. 51st Ann. Meet. of the Assoc. for Computational
Linguistics (ACL), Sofia, Bulgaria.
A. Purtee and L. K. Schubert, 2012. TTT: A tree trans-
duction language for syntactic and semantic process-
ing. EACL 2012 Workshop on Applications of Tree
Automata Techniques in Natural Language Process-
ing (ATANLP 2012), Avignon, France.
L. K. Schubert, to appear. NLog-like inference and
commonsense reasoning. In A. Zaenen, V. de
Paiva and C. Condoravdi (eds.), Semantics for
Textual Inference, CSLI Publications. Available at
http://www.cs.rochester.edu/u/schubert/papers/nlog-
like-inference12.pdf.
L. K. Schubert, 2000. The situations we talk about. In
J. Minker (ed.), Logic-Based Artificial Intelligence,
Kluwer, Dortrecht, 407?439.
L. K. Schubert and C. H. Hwang, 2000. Episodic
Logic meets Little Red Riding Hood: A comprehen-
sive, natural representation for language understand-
ing. In L. Iwanska and S.C. Shapiro (eds.), Natural
Language Processing and Knowledge Representa-
tion: Language for Knowledge and Knowledge for
Language. MIT/AAAI Press, Menlo Park, CA, and
Cambridge, MA, 111?174.
L. K. Schubert and F. J. Pelletier, 1982. From En-
glish to logic: Context-free computation of ?conven-
tional? logical translations. Am. J. of Computational
Linguistics 8, 27?44. Reprinted in B.J. Grosz, K.
Sparck Jones, and B.L. Webber (eds.), Readings in
Natural Language Processing, Morgan Kaufmann,
Los Altos, CA, 1986, 293-311.
S. Tellex, T. Kollar, S. Dickerson, M. R. Walter, A. G.
Banerjee, S. Teller, and N. Roy, 2011. Understand-
ing natural language commands for robotic naviga-
tion and mobile manipulation. Nat. Conf. on Artifi-
cial Intelligence (AAAI 2011), San Francisco, CA.
B. Van Durme and L. K. Schubert, 2008. Open
knowledge extraction through compositional lan-
guage processing. Symposium on Semantics in Sys-
tems for Text Processing (STEP 2008), Venice, Italy.
60
