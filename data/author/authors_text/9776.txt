Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 1061?1069,
Honolulu, October 2008. c?2008 Association for Computational Linguistics
Information Retrieval Oriented Word Segmentation based on Character
Associative Strength Ranking
Yixuan Liu, Bin Wang, Fan Ding, Sheng Xu
Information Retrieval Group
Center for Advanced Computing Research
Institute of Computing Technology
Chinese Academy of Sciences
Beijing, 100190, P.R.China
{liuyixuan, wangbin, dingfan, xusheng}@ict.ac.cn
Abstract
This paper presents a novel, ranking-style
word segmentation approach, called RSVM-
Seg, which is well tailored to Chinese informa-
tion retrieval(CIR). This strategy makes seg-
mentation decision based on the ranking of the
internal associative strength between each pair
of adjacent characters of the sentence. On the
training corpus composed of query items, a
ranking model is learned by a widely-used tool
Ranking SVM, with some useful statistical
features, such as mutual information, differ-
ence of t-test, frequency and dictionary infor-
mation. Experimental results show that, this
method is able to eliminate overlapping am-
biguity much more effectively, compared to
the current word segmentation methods. Fur-
thermore, as this strategy naturally generates
segmentation results with different granular-
ity, the performance of CIR systems is im-
proved and achieves the state of the art.
1 Introduction
To improve information retrieval systems? perfor-
mance, it is important to comprehend both queries
and corpus precisely. Unlike English and other
western languages, Chinese does not delimit words
by white-space. Word segmentation is therefore a
key preprocessor for Chinese information retrieval
to comprehend sentences.
Due to the characteristics of Chinese, two main
problems remain unresolved in word segmentation:
segmentation ambiguity and unknown words, which
are also demonstrated to affect the performance of
Chinese information retrieval (Foo and Li, 2004).
Overlapping ambiguity and combinatory ambiguity
are two forms of segmentation ambiguity. The first
one refers to that ABC can be segmented into AB
C or A BC. The second one refers to that string
AB can be a word, or A can be a word and B can
be a word. In CIR, the combinatory ambiguity is
also called segmentation granularity problem (Fan
et al, 2007). There are many researches on the
relationship between word segmentation and Chi-
nese information retrieval (Foo and Li, 2004; Peng
et al, 2002a; Peng et al, 2002b; Jin and Wong,
2002). Their studies show that the segmentation
accuracy does not monotonically influence subse-
quent retrieval performance. Especially the overlap-
ping ambiguity, as shown in experiments of (Wang,
2006), will cause more performance decrement of
CIR. Thus a CIR system with a word segmenter bet-
ter solving the overlapping ambiguity, may achieve
better performance. Besides, it also showed that the
precision of new word identification was more im-
portant than the recall.
There are some researches show that when com-
pound words are split into smaller constituents, bet-
ter retrieval results can be achieved (Peng et al,
2002a). On the other hand, it is reasonable that the
longer the word which co-exists in query and cor-
pus, the more similarity they may have. A hypothe-
sis, therefore, comes to our mind, that different seg-
mentation granularity can be incorporated to obtain
better CIR performance.
In this paper we present a novel word segmenta-
tion approach for CIR, which can not only obviously
reduce the overlapping ambiguity, but also introduce
different segmentation granularity for the first time.
1061
In our method, we first predict the ranking result of
all internal association strength (IAS) between each
pair of adjacent characters in a sentence using Rank-
ing SVM model, and then, we segment the sentence
into sub-sentences with smaller and smaller granu-
larity by cutting adjacent character pairs according
to this rank. Other machine-learning based segmen-
tation algorithms (Zhang et al, 2003; Lafferty et al,
2001; Ng and Low, 2004) treat segmentation prob-
lem as a character sequence tagging problem based
on classification. However, these methods cannot di-
rectly obtain different segmentation granularity. Ex-
periments show that our method can actually im-
prove information retrieval performance.
This paper is structured as follows. It starts with
a brief introduction of the related work on the word
segmentation approaches. Then in Section 3, we in-
troduce our segmentation method. Section 4 evalu-
ates the method based on experimental results. Fi-
nally, Section 5 makes summary of this whole paper
and proposes the future research orientation.
2 Related Work
Various methods have been proposed to address
the word segmentation problem in previous studies.
They fall into two main categories, rule-based ap-
proaches that make use of linguistic knowledge and
statistical approaches that train on corpus with ma-
chine learning methods. In rule-based approaches,
algorithms of string matching based on dictionary
are the most commonly used, such as maximum
matching. They firstly segment sentences accord-
ing to a dictionary and then resort to some rules
to resolve ambiguities (Liu, 2002; Luo and Song,
2001). These rule-based methods are fast, how-
ever, their performances depend on the dictionary
which cannot include all words, and also on the rules
which cost a lot of time to make and must be up-
dated frequently. Recent years statistical approaches
became more popular. These methods take advan-
tage of various probability information gained from
large corpus to segment sentences. Among them,
Wang?s work (Wang, 2006) is the most similar to
our method, since both of us apply statistics infor-
mation of each gap in the sentence to eliminate over-
lapping ambiguity in methods. However, when com-
bining different statistics, Wang decided the weight
by a heuristic way which was too simply to be suit-
able for all sentences. In our method, we employ a
machine-learning method to train features? weights.
Many machine-learning methods, such as
HMM (Zhang et al, 2003), CRF (Lafferty et al,
2001), Maximum Entropy (Ng and Low, 2004),
have been exploited in segmentation task. To our
knowledge, machine-learning methods used in seg-
mentation treated word segmentation as a character
tagging problem. According to the model trained
from training corpus and features extracted from the
context in the sentence, these methods assign each
character a positional tag, indicating its relative po-
sition in the word. These methods are difficult to get
different granularity segmentation results directly.
Our method has two main differences with them.
Firstly, we tag the gap between characters rather
than characters themselves. Secondly, our method
is based on ranking rather than classification.
Then, we will present our ranking-based segmen-
tation method, RSVM-Seg.
3 Ranking based Segmentation
Traditional segmentation methods always take the
segmentation problem as classification problem and
give a definite segmentation result. In our approach,
we try to solve word segmentation problem from the
view of ranking. For easy understanding, let?s rep-
resent a Chinese sentence S as a character sequence:
C1:n = C1C2 . . . Cn
We also explicitly show the gap Gi(i = 1 . . . n? 1)
between every two adjacent characters Ci and Ci+1:
C1:n|G1:n?1 = C1G1C2G2 . . . Gn?1Cn
IASi(i = 1 . . . n) is corresponding to Gi(i =
1 . . . n), reflecting the internal association strength
between Ci and Ci+1. The higher the IAS value is,
the stronger the associative between the two charac-
ters is. If the association between two characters is
weak, then they can be segmented. Otherwise, they
should be unsegmented. That is to say we could
make segmentation based on the ranking of IAS
value. In our ranking-style segmentation method,
Ranking SVM is exploited to predict IAS ranking.
In next subsections, we will introduce how to
take advantage of Ranking SVM model to solve our
1062
problem. Then, we will describe features used for
training the Ranking SVM model. Finally, we will
give a scheme how to get segmentation result from
predicted ranking result of Ranking SVM.
3.1 Segmentation based on Ranking SVM
Ranking SVM is a classical algorithm for ranking,
which formalizes learning to rank as learning for
classification on pairs of instances and tackles the
classification issue by using SVM (Joachims, 2002).
Suppose that X?Rd is the feature space, where d is
the number of features, and Y = r1, r2, . . . , rK is
the set of labels representing ranks. And there exists
a total order between ranks r1 > r2 > . . . > rK ,
where > denotes the order relationship. The actual
task of learning is formalized as a Quadratic Pro-
gramming problem as shown below:
min?,???
1
2???
2 + C????
s.t.??, x? ? x?? > 1? ???,?x? ? x?, ??? ? 0
(1)
where ??? denotes l2 norm measuring the margin
of the hyperplane and ?ij denotes a slack variable.
xi ? xj means the rank class of xi has an order
prior to that of xj , i.e. Y (xi) > Y (xj). Suppose
that the solution to (1) is ??, then we can make the
ranking function as f(x) = ???, x?.
When applying Ranking SVM model to our prob-
lems, an instance (feature vector x) is created from
all bigrams (namely CiCi+1, i = 1 . . . n ? 1) of
a sentence in the training corpus. Each feature
is defined as a function of bigrams (we will de-
scribe features in detail in next subsection). The
instances from all sentences are then combined for
training. And Y refers to the class label of the
IAS degree. As we mentioned above, segmenta-
tion decision is based on IAS value. Therefore,
the number of IAS degree?s class label is also cor-
respondent to the number of segmentation class la-
bel. In traditional segmentation algorithms, they al-
ways label segmentation as two classes, segmented
and unsegmented. However, for some phrases, it is
a dilemma to make a segmentation decision based
on this two-class scheme. For example, Chinese
phrase ??????(Notepad)? can be segmented
as ????(Note)? and ???(computer)? or can
be viewed as one word. We cannot easily classify
the gap between ??? and ??? as segmented or un-
segmented. Therefore, beside these two class la-
bels, we define another class label, semisegmented,
which means that the gap between two characters
could be segmented or unsegmented, either will be
right. Correspondingly, IAS degree is also divided
into three classes, definitely inseparable (marked as
3), partially inseparable (marked as 2), and sepa-
rable (marked as 1). ?Separable? corresponds to
be segmented?; ?partially inseparable? corresponds
to semisegmented; ?definitely inseparable? corre-
sponds to be unsegmented. Obviously, there exists
orders between these labels? IAS values, namely
IAS(1) < IAS(2) < IAS(3), IAS(?) represents
the IAS value of different labels. Next, we will
describe the features used to train Ranking SVM
model.
3.2 Features for IAS computation
Mutual Information: Mutual information, mea-
suring the relationship between two variables, has
been extensively used in computational language re-
search. Given a Chinese character string ?xy? (as
mentioned above, in our method, ?xy? refers to bi-
gram in a sentence), mutual information between
characters x and y is defined as follows:
mi(x, y) = log2 p(x, y)p(x)p(y) (2)
where p(x, y) is the co-occurrence probability of x
and y, namely the probability that bigram ?xy? oc-
curs in the training corpus, and p(x), p(y) are the
independent probabilities of x and y respectively.
From (2), we conclude that mi(x, y) ? 0 means
that IAS is strong; mi(x, y) ? 0 means that it
is indefinite for IAS between characters x and y;
mi(x, y) ? 0 means that there is no association
been characters x and y. However, mutual infor-
mation has no consideration of context, so it can-
not solve the overlapping ambiguity effectively (Sili
Wang 2006). To remedy this defect, we introduce
another statistics measure, difference of t-test.
Difference of t-score (DTS): Difference of t-
score is proposed on the basis of t-score. Given
a Chinese character string ?xyz?, the t-score of the
character y relevant to character x and z is defined
1063
as:
tx,z(y) = p(z|y)? p(y|x)??2(p(z|y)) + ?2(p(y|x))
(3)
where p(y|x) is the conditional probability of y
given x, and p(z|y), of z given y, and ?2(p(y|x)),
?2(p(z|y)) are variances of p(y|x) and of p(z|y) re-
spectively. Sun et al gave the derivation formula of
?2(p(y|x)), ?2(p(z|y)) (Sun et al, 1997) as
?2(p(z|y)) ? r(y, z)r2(y) ?2(p(y|x)) ?
r(x, y)
r2(x) (4)
where r(x, y), r(y, z), r(y), r(z) are the frequency
of string xy, yz, y, and z respectively. Thus formula
(3) is deducted as
tx,z(y) =
r(y,z)
r(y) ?
r(x,y)
r(x)?
r(y,z)
r2(y) +
r(x,y)
r2(x)
(5)
tx,z(y) indicates the binding tendency of y in the
context of x and z: if tx,z(y) > 0 then y tends to
be bound with z rather than with x; if tx,z(y) < 0,
they y tends to be bound with x rather than with z.
To measure the binding tendency between two ad-
jacent characters ?xy? (also, it refers to bigram in a
sentence in our method), we use difference of t-score
(DTS) (Sun et al, 1998) which is defined as
dts(x, y) = tv,y(x)? tx,w(y) (6)
Higher dts(x, y) indicates stronger IAS between
adjacent characters x and y.
Dictionary Information: Both statistics mea-
sures mentioned above cannot avoid sparse data
problem. Then Dictionary Information is used to
compensate for the shortage of statistics informa-
tion. The dictionary we used includes 75784 terms.
We use binary value to denote the dictionary feature.
If a bigram is in the dictionary or a part of dictionary
term, we label it as ?1?, otherwise, we label is as ?0?.
Frequency: An important characteristic of new
word is its repeatability. Thus, we also use fre-
quency as another feature to train Ranking SVM
model. Here, the frequency is referred to the number
of times that a bigram occurs in the training corpus.
We give a training sentence for a better under-
standing of features mentioned above. The sentence
Algorithm 1 : Generate various granularity terms
1: Input: A Chinese sentence S = C1 : Cn
IAS = IAS1:n?1 LB = 1;RB = n
2: Iterative(S, IAS):
3: while length(S) ? 3 do
4: MB = FindMinIAS(IAS)
5: SL = CLB:MB
6: SR = CMB+1:RB
7: IASL = IASLB:MB
8: IASR = IASMB+1:RB
9: Iterative(SL, IASL)
10: Iterative(SR, IASR)
11: end while
is ????????(China Construction Bank net-
work)? We extract all bigrams in this sentence, com-
pute the four above features and give the IAS a la-
bel for each bigram. The feature vectors of all these
bigrams for training are shown in Table 1.
3.3 Segmentation scheme
In order to compare with other segmentation meth-
ods, which give a segmentation result based on two
class labels, segmented and unsegmented, it is nec-
essary to convert real numbers result given by Rank-
ing SVM to these two labels. Here, we make a
heuristic scheme to segment the sentence based on
IAS ranking result predicted by Ranking SVM. The
scheme is described in Algorithm 1. In each itera-
tion we cut the sentence at the gap with minimum
IAS value. Nie et.al. pointed out that the average
length of words in usage is 1.59 (Nie et al, 2000).
Therefore, we stop the segmentation iterative when
the length of sub sentence is 2 or less than 2. By
this method, we could represent the segmentation re-
sult as a binary tree. Figure 1 shows an example of
this tree. With this tree, we can obtain various gran-
ularity segmentations easily, which could be used
in CIR. This segmentation scheme may cause some
combinatory ambiguity. However, Nie et.al. (Nie
et al, 2000) also pointed out that there is no accu-
rate word definition, thus whether combinatory am-
biguity occurs is uncertain. What?s more, compared
to overlapping ambiguity, combinatory ambiguity is
not the fatal factor for information retrieval perfor-
mance as mentioned in introduction. Therefore, this
scheme is reasonable for Chinese information re-
1064
Bigram MI DTS Dictionary Frequency IAS
??(China) 6.67 1985.26 1 1064561 3
?? 2.59 -1447.6 0 14325 1
??(Construction) 8.67 822.64 1 200129 3
?? 5.94 -844.05 0 16098 2
??(Bank) 9.22 931.25 1 236976 3
?? 2.29 -471.24 0 15282 1
Table 1: Example of feature vector
???????
(Traffic map of JiangXi Province) 
???           ????
(JiangXi Province)     (Traffic map) 
???      ????
(JiangXi) (Province) (Traffic)    (Map)
Figure 1: Example 1
trieval.
4 Experiments and analysis
4.1 Data
Since the label scheme and evaluation measure (de-
scribed in next subsection) of our segmentation
method are both different from the traditional seg-
mentation methods, we did not carry out experi-
ments on SIGHAN. Instead, we used two query logs
(QueryLog1 and QueryLog2) as our experiment cor-
pus, which are from two Chinese search engine com-
panies. 900 queries randomly from QueryLog1 were
chosen as training corpus. 110 Chinese queries from
PKU Tianwang1 , randomly selected 150 queries
from QueryLog1 and 100 queries from QueryLog2
were used as test corpus. The train and test cor-
pus have been tagged by three people. They were
given written information need statements, and were
asked to judge the IAS of every two adjacent char-
acters in a sentence on a three level scale as men-
tioned above, separable, partially inseparable, and
definitely inseparable. The assessors agreed in 84%
of the sentences, the other sentences were checked
1Title field of SEWM2006 and SEWM2007 web retrieval
TD task topics. See http://www.cwirf.org/
by all assessors, and a more plausible alternative was
selected. We exploited SVM light2 as the toolkit to
implement Ranking SVM model.
4.2 Evaluation Measure
Since our approach is based on the ranking of IAS
values, it is inappropriate to evaluate our method by
the traditional method used in other segmentation
algorithms. Here, we proposed an evaluation mea-
sure RankPrecision based on Kendall?s ? (Joachims,
2002), which compared the similarity between the
predicted ranking of IAS values and the rankings
of these tags as descending order. RankPrecision
formula is as follows:
RankPrecision =
1? ?
n
i=1InverseCount(si)
?ni=1CompInverseCount(si)
(7)
where si represents the ith sentence (unsegmented
string), InverseCount(si) represents the number
of discordant pairs inversions in the ranking of the
predicted IAS value compared to the correct labeled
ranking. CompInverseCount(si) represents the
number of discordant pairs inversions when the la-
bels totally inverse.
4.3 Experiments Results
Contributions of the Features: We investi-
gated the contribution of each feature by gen-
erating many versions of Ranking SVM model.
RankPrecision as described above was used for
evaluations in these and following experiments.
We used Mutual Information(MI); Difference
of T-Score(DTS); Frequency(F); mutual informa-
tion and difference of t-score(MI+DTS); mu-
2http://svmlight.joachims.org/
1065
Feature Corpus
Train Query Query Tian
Log1 Log2 Wang
MI 0.882 0.8719 0.8891 0.9444
DTS 0.9054 0.8954 0.9086 0.9444
F 0.8499 0.8416 0.8563 0.9583
MI+DTS 0.9077 0.9117 0.923 0.9769
MI+DTS+F 0.8896 0.8857 0.9209 0.9815
MI+DTS+D 0.933 0.916 0.9384 0.9954
MI+DTS+F+D 0.932 0.93 0.9374 0.9954
Table 2: The segmentation performance with different
features
Features
MI DTS F MI+DTS MI+DTS+F MI+DTS+D MI+DTS+F+D
Ra
nk
Pr
ec
isi
on
.82
.84
.86
.88
.90
.92
.94
.96
.98
1.00
1.02
TrainCorpus
QueryLog1
QueryLog2
TianWang
Figure 2: Effects of features
tual information, difference of t-score and Fre-
quency(MI+DTS+F); mutual information, differ-
ence of t-score and dictionary(MI+DTS+D); mutual
information, difference of t-score, frequency and
Dictionary(MI+DTS+F+D) as features respectively.
The results are shown in Table 2 and Figure 2.
From the results, we can see that:
? Using all described features together, the Rank-
ing SVM achieved a good performance. And
when we added MI, DTS, frequency, dictio-
nary as features one by one, the RankPrecision
improved step by step. It demonstrates that the
features we selected are useful for segmenta-
tion.
Size of Corpus
Train Train Query Query Tian
Corpus Log1 Log2 Wang
100 0.9149 0.9070 0.9209 0.9630
200 0.9325 0.9304 0.9446 0.9907
400 0.9169 0.9057 0.9230 0.9630
500 0.9320 0.9300 0.9374 0.9954
600 0.9106 0.9050 0.9312 0.9907
700 0.9330 0.9284 0.9353 0.9954
900 0.9217 0.9104 0.9240 0.9907
Table 3: The segmentation performance with different
size training corpus
Number of Train Query
0 200 400 600 800 1000
Ra
nk
Pr
ec
isi
on
.80
.85
.90
.95
1.00
1.05
1.10
TrainCorpus
QueryLog1
QueryLog2
TianWang
Figure 3: Effects of Corpus Size
? The lowest RankPrecision is above 85%, which
suggests that the predicted rank result by our
approach is very close to the right rank. It is
shown that our method is effective.
? When we used each feature alone, difference
of t-score achieved highest RankPrecise, fre-
quency was worst on most of test corpus (ex-
cept TianWang). It is induced that difference
of t-test is the most effective feature for seg-
mentation. It is explained that because dts is
combined with the context information, which
eliminates overlapping ambiguity errors.
? It is surprising that when mutual information
and difference of t-score was combined with
1066
frequency, the RankPrecision was hurt on three
test corpus, even worse than dts feature. The
reason is supposed that some non-meaning but
common strings, such as ???? would be took
for a word with high IAS values. To correct
this error, we could build a stop word list, and
when we meet a character in this list, we treat
them as a white-space.
Effects of corpus size:We trained different Rank-
ing SVM models with different corpus size to in-
vestigate the effects of training corpus size to our
method performance. The results are shown in Ta-
ble 3 and Figure 3. From the results, we can see that
the effect of corpus size to the performance of our
approach is minors. Our segmentation approach can
achieve good performance even with small training
corpus, which indicates that Ranking SVM has gen-
eralization ability. Therefore we can use a relative
small corpus to train Ranking SVM, saving labeling
effort.
Effects on Finding Boundary: In algorithm
1, we could get different granularity segmentation
words when we chose different length as stop
condition. Figure 4 shows the ?boundary precision?
at each stop condition. Here, ?boundary precision?
is defined as
No.of right cut boundaries
No.of all cut boundaries (8)
From the result shown in figure 4, we can see
that as the segmentation granularity gets smaller, the
boundary precision gets lower. The reason is obvi-
ous, that we may segment a whole word into smaller
parts. However, as we analyzed in introduction, in
CIR, we should judge words boundaries correctly to
avoid overlapping ambiguity. As for combinatory
ambiguity, through setting different stop length con-
dition, we can obtain different granularity segmen-
tation result.
Effects on Overlapping Ambiguity: Due to the
inconsistency of train and test corpus, it is difficult to
keep fair for Chinese word segmentation evaluation.
Since ICTCLAS is considered as the best Chinese
word segmentation systems. We chose ICTCLAS
as the comparison object. Moreover, we chose
Maximum Match segmentation algorithm, which is
rule-based segmentation method, as the baseline.
Stop length 
2~3 4~5 6~7
Pr
ec
isi
on
 of
 B
ou
nd
ary
.88
.89
.90
.91
.92
.93
.94
.95
.96
Figure 4: Precision of boundary with different stop word
length conditions
Corpus NOA NOA NOA
(RSVM Seg) (ICTCLAS) (MM)
Query
Log1 7 10 21
Query
Log2 2 6 16
Tian
Wang 0 0 1
Table 4: Number of Overlapping Ambiguity
We compared the number of overlapping ambigu-
ity(NOA) among these three approaches on test cor-
pus QueryLog1, QueryLog2 and TianWang. The re-
sult is shown in Table 4. On these three test cor-
pus, the NOA of our approach is smallest, which
indicates our method resolve overlapping ambiguity
more effectively. For example, the sentence ???
??(basic notes)?, the segmentation result of ICT-
CLAS is ????(basic class)/?(article)?, the word
???(notes)? is segmented, overlapping ambiguity
occurring. However, with our method, the predicted
IAS value rank of positions between every two ad-
jacent characters in this sentence is ??3?1?2??,
which indicates that the character ??? has stronger
internal associative strength with the character ???
than with the character ???, eliminating overlap-
ping ambiguity according to this ISA rank results.
Effects on Recognition Boundaries of new
word: According to the rank result of all IAS values
1067
??????
(Hainan High School?s Entry Recruitme) 
??                ????
(Hainan) (High School?s Entry Recruitment) 
                   ??          ??
(High School?s Entry)(Recruitment)  
Figure 5: Example of New Word boundary
in a sentence, our method can recognize the bound-
aries of new words precisely, avoiding the overlap-
ping ambiguity caused by new words. For example,
the phrase ???????(Hainan High School?s
Entry Recruitment)?, the ICTCLAS segmentation
result is ???/?/??/??, because the new word
???? cannot be recognized accurately, thus the
character ??? is combined with its latter charac-
ter ???, causing overlapping ambiguity. By our
method, the segmentation result is shown as figure
5, in which no overlapping ambiguity occurs.
Performance of Chinese Information Re-
trieval: To evaluate the effectiveness of RSVM-Seg
method on CIR, we compared it with the FMM seg-
mentation. Our retrieval system combines differ-
ent query representations obtained by our segmen-
tation method, RSVM-Seg. In previous TREC Tere-
byte Track, Markov Random Field(MRF) (Metzler
and Croft, 2005) model has displayed better perfor-
mance than other information retrieval models, and
it can much more easily include dependence fea-
tures. There are three variants of MRF model, full
independence(FI), sequential dependence(SD), and
full dependence(FD). We chose SD as our retrieval
model, since Chinese words are composed by char-
acters and the adjacent characters have strong de-
pendence relationship. We evaluated the CIR per-
formance on the Chinese Web Corpora CWT200g
provided by Tianwang 3, which, as we know, is
the largest publicly available Chinese web corpus
till now. It consists of 37, 482, 913 web pages
with total size of 197GB. We used the topic set
3http://www.cwirf.org/
Segmentation
Method MAP R-P GMAP
FMM 0.0548 0.0656 0.0095
RSVM-Seg 0.0623 0.0681 0.0196
Table 5: Evaluation of CIR performance
for SEWM2007 and SEWM2006 Topic Distillation
(TD) task which contains 121 topics. MAP, R-
Precision and GMAP (Robertson, 2006) were as
main evaluation metrics. GMAP is the geometric
mean of AP(Average Precision) through different
queries, which was introduced to concentrate on dif-
ficult queries. The result is shown in 5. From the
table, we can see that our segmentation method im-
prove the CIR performance compared to FMM.
5 Conclusion and Future work
From what we have discussed above, we can safely
draw the conclusion that our work includes several
main contributions. Firstly, to our best known, this
is the first time to take the Chinese word segmenta-
tion problem as ranking problem, which provides a
new view for Chinese word segmentation. This ap-
proach has been proved to be able to eliminate over-
lapping ambiguity and also be able to obtain various
segmentation granularities. Furthermore, our seg-
mentation method can improve Chinese information
retrieval performance to some extent.
As future work, we would search another more
encouraging method to make a segmentation deci-
sion from the ranking result. Moreover, we will try
to relabel SIGHAN corpus on our three labels, and
do experiments on them, which will be more con-
venient to compare with other segmentation meth-
ods. Besides, we will carry out more experiments to
search the effectiveness of our segmentation method
to CIR.
Acknowledgments
This paper is supported by China Natural Science
Founding under No. 60603094 and China National
863 key project under No. 2006AA010105. We ap-
preciate Wenbin Jiang?s precious modification ad-
vices. Finally, we would like to thank the three
anonymous EMNLP reviewers for their helpful and
constructive comments.
1068
References
D. Fan, W. Bin, and W. Sili. 2007. A Heuristic Approach
for Segmentation Granularity Problem in Chinese In-
formation Retrieval. Advanced Language Processing
and Web Information Technology, 2007. ALPIT 2007.
Sixth International Conference on, pages 87?91.
S. Foo and H. Li. 2004. Chinese word segmentation and
its effect on information retrieval. volume 40, pages
161?190. Elsevier.
H. Jin and K.F. Wong. 2002. A Chinese dictionary
construction algorithm for information retrieval. ACM
Transactions on Asian Language Information Process-
ing (TALIP), 1(4):281?296.
T. Joachims. 2002. Optimizing search engines using
clickthrough data. Proceedings of the eighth ACM
SIGKDD international conference on Knowledge dis-
covery and data mining, pages 133?142.
J.D. Lafferty, A. McCallum, and F.C.N. Pereira. 2001.
Conditional Random Fields: Probabilistic Models for
Segmenting and Labeling Sequence Data. Proceed-
ings of the Eighteenth International Conference on
Machine Learning table of contents, pages 282?289.
Q. Liu. 2002. Review of Chinese lexical and syntactic
technology.
Z.Y. Luo and R. Song. 2001. Proper noun recognition in
Chinese word segmentation research. Conference of
international Chinese computer, 328:2001?323.
D. Metzler and W.B. Croft. 2005. A Markov random
field model for term dependencies. Proceedings of
the 28th annual international ACM SIGIR conference
on Research and development in information retrieval,
pages 472?479.
H.T. Ng and J.K. Low. 2004. Chinese part-of-speech
tagging: one-at-a-time or all-at-once? word-based or
character-based. Proc of EMNLP.
J.Y. Nie, J. Gao, J. Zhang, and M. Zhou. 2000. On
the use of words and n-grams for Chinese informa-
tion retrieval. Proceedings of the fifth international
workshop on on Information retrieval with Asian lan-
guages, pages 141?148.
F. Peng, X. Huang, D. Schuurmans, and N. Cercone.
2002a. Investigating the relationship between word
segmentation performance and retrieval performance
in Chinese IR. Proceedings of the 19th international
conference on Computational linguistics-Volume 1,
pages 1?7.
F. Peng, X. Huang, D. Schuurmans, N. Cercone, and S.E.
Robertson. 2002b. Using self-supervised word seg-
mentation in Chinese information retrieval. Proceed-
ings of the 25th annual international ACM SIGIR con-
ference on Research and development in information
retrieval, pages 349?350.
S. Robertson. 2006. On GMAP: and other transforma-
tions. Proceedings of the 15th ACM international con-
ference on Information and knowledge management,
pages 78?83.
Sili Wang. 2006. Research on chinese word segmenta-
tion for large scale information retrieval.
H.P. Zhang, H.K. Yu, D.Y. Xiong, and Q. Liu. 2003.
HHMM-based Chinese Lexical Analyzer ICTCLAS.
Proceedings of Second SIGHAN Workshop on Chinese
Language Processing, pages 184?187.
1069
A Study on Effectiveness of Syntactic Relationship in Dependence Re-
trieval Model 
Fan Ding1,2
1: Graduate University,  
Chinese Academy of Sciences  
Beijing, 100080, China  
dingfan@ict.ac.cn 
Bin Wang2
2: Institute of Computing Technology, 
Chinese Academy of Sciences  
Beijing, 100080, China 
wangbin@ict.ac.cn 
 
 
Abstract 
To relax the Term Independence Assump-
tion, Term Dependency is introduced and it 
has improved retrieval precision dramati-
cally. There are two kinds of term depend-
encies, one is defined by term proximity, 
and the other is defined by linguistic de-
pendencies. In this paper, we take a com-
parative study to re-examine these two 
kinds of term dependencies in dependence 
language model framework. Syntactic rela-
tionships, derived from a dependency 
parser, Minipar, are used as linguistic term 
dependencies. Our study shows: 1) Lin-
guistic dependencies get a better result than 
term proximity. 2) Dependence retrieval 
model achieves more improvement in sen-
tence-based verbose queries than keyword-
based short queries. 
1 Introduction 
For the sake of computational simplicity, Term 
Independence Assumption (TIA) is widely used in 
most retrieval models. It states that terms are statis-
tically independent from each other. Though un-
reasonable, TIA did not cause very bad perform-
ance. However, relaxing the assumption by adding 
term dependencies into the retrieval model is still a 
basic IR problem. Relaxing TIA is not easy be-
cause improperly relaxing may introduce much 
noisy information which will hurt the final per-
formance. Defining the term dependency is the 
first step in dependence retrieval model. Two re-
search directions are taken to define the term de-
pendency. The first is to treat term dependencies as 
term proximity, for example, the Bi-gram Model 
(F. Song and W. B. Croft, 1999) and Markov Ran-
dom Field Model (D. Metzler and W. B. Croft, 
2005) in language model. The second direction is 
to derive term dependencies by using some linguis-
tic structures, such as POS block (Lioma C. and 
Ounis I., 2007) or Noun/Verb Phrase (Mitra et al, 
1997), Maximum Spanning Tree (C. J. van 
Rijsbergen, 1979) and Linkage Model (Gao et al, 
2004) etc.  
Though linguistic information is intensively 
used in QA (Question Answering) and IE (Infor-
mation Extraction) task, it is seldom used in docu-
ment retrieval (T. Brants, 2004). In document re-
trieval, how effective linguistic dependencies 
would be compared with term proximity still needs 
to be explored thoroughly. 
In this paper, we use syntactic relationships de-
rived by a popular dependency parser, Minipar (D. 
Lin, 1998), as linguistic dependencies. Minipar is a 
broad-coverage parser for the English language. It 
represents the grammar as a network of nodes and 
links, where the nodes represent grammatical cate-
gories and the links represent types of dependency.  
We extract the dependencies between content 
words as term dependencies. 
To systematically compare term proximity with 
syntactic dependencies, we study the dependence 
retrieval models in language model framework and 
present a smooth-based dependence language 
model (SDLM). It can incorporate these two kinds 
of term dependencies. The experiments in TREC 
collections show that SDLM with syntactic rela-
tionships achieves better result than with the term 
proximity. 
The rest of this paper is organized as follows. 
Section 2 reviews some previous relevant work, 
197
Section 3 presents the definition of term depend-
ency using syntactic relationships derived by 
Minipar. Section 4 presents in detail the smooth-
based dependence language model. A series of ex-
periments on TREC collections are presented in 
Section 5. Some conclusions are summarized in 
Section 6. 
2 Related Work 
Generally speaking, when using term dependencies 
in language modeling framework, two problems 
should be considered: The first is to define and 
identify term dependencies; the second is to 
integrate term dependencies into a weighting 
schema. Accordingly, this section briefly reviews 
some recent relevant work, which is summarized 
into two parts: the definition of term dependencies 
and weight of term dependencies. 
2.1 Definition of Term Dependencies 
In definition of term dependencies, there are two 
main methods: shallow parsing by some linguistic 
tools and term proximity with co-occurrence in-
formation. Both queries and documents are repre-
sented as a set of terms and term dependencies 
among terms. Table 1 summarizes some recent 
related work according to the method they use to 
identify term dependencies in queries and docu-
ments. 
Methods Document 
Parsing 
Document Proximity
Query 
Parsing 
I: DM,LDM, 
etc. 
II: CULM, RP, etc. 
Query 
Proximity 
III: NIL IV: BG ,WPLM, 
MRF, etc. 
Table 1. Methods in identifying dependencies 
In the part I of table 1, DM is Dependence Lan-
guage Model (Gao et al, 2004). It introduces a de-
pendency structure, called linkage model. The 
linkage structure assumes that term dependencies 
in a sentence form an acyclic, planar graph, where 
two related terms are linked. LDM (Gao et al, 
2005) represents the related terms as linguistic 
concepts, which can be semantic chunks (e.g. 
named entities like person name, location name, 
etc.) and syntactic chunks (e.g. noun phrases, verb 
phrases, etc.).  
In the part II of table 1, CULM (M. Srikanth and 
R. Srihari, 2003) is a concept unigram language 
model. The parser tree of a user query is used to 
identify the concepts in the query. Term sequence 
in a concept is treated as bi-grams in the document 
model.  RP (Recognized Phrase, S. Liu et al, 2004) 
uses some linguistic tools and statistical tools to 
recognize four types of phrase in the query, includ-
ing proper names, dictionary phrase, simple phrase 
and complex phrase. A phrase is in a document if 
all its content words appear in the document within 
a certain window size. The four kinds of phrase 
correspond to variant window size. 
In the part IV of table 1, BG (bi-gram language 
model) is the simplest model which assumes term 
dependencies exist only between adjacent words 
both in queries and documents. WPLM (word pairs 
in language model, Alvarez et al, 2004) relax the 
co-occurrence window size in documents to 5 and 
relax the order constraint in bi-gram model. MRF 
(Markov Random Field) classify the term depend-
encies in queries into sequential dependence and 
full dependence, which respectively corresponds to 
ordered and unordered co-occurrence within a pre-
define-sized window in documents. 
From above discussion we can see that when the 
query is sentence-based, parsing method is pre-
ferred to proximity method. When the query is 
keyword-based, proximity method is preferred to 
parsing method. Thorsten (T. Brants, 2004) note: 
the longer the queries, the bigger the benefit of 
NLP. This conclusion also holds for the definition 
of query term dependencies. 
2.2 Weight of Term Dependencies 
In dependence retrieval model, the final relevance 
score of a query and a document consists of both 
the independence score and dependence score, 
such as Bahadur Lazarsfeld expansion (R. M. 
Losee, 1994) in classical probabilistic IR models. 
However, Spark Jones et al point out that without 
a theoretically motivated integration model, docu-
ments containing dependencies (e.g. phrases) may 
be over-scored if they are weighted in the same 
way as single words (Jones et al, 1998). Smooth-
ing strategy in language modeling framework pro-
vide such an elegant solution to incorporate term 
dependencies. 
In the simplest bi-gram model, the probability of 
bi-gram (qi-1,qi) in document D is smoothed by its 
unigram: 
198
)|(
)|(
),|(,
),|()1()|(),|(
1
1
1
11
DqP
DqqP
DqqPwhere
DqqPDqPDqqP
i
ii
ii
iiiiismoothed
?
?
?
??
?
??+?= ??
  (1) 
Further, the probability of bi-gram (qi-1,qi) in 
document P(qi|qi-1,D) can be smoothed by its prob-
ability in collection P(qi|qi-1,C). If P(qi|qi-1,D) is 
smoothed as Equation (1), the relevance score of 
query Q={q1q2?qm} and document D is: 
)|()|(
)|(
log)|,(,
)|,()|(log
)
)|()|(
)|(1
1log()|(log
)
)|(
),|(
)1(log()|(log
)),|()1()|(log()|(log
),|(log)|(log)|(log
1
1
1
...2
1
...1
...2 1
1
...1
...2
1
...1
...2
11
...2
11
DqPDqP
DqqP
DqqMIusually
DqqMIDqP
DqPDqP
DqqP
DqP
DqP
DqqP
DqP
DqqPDqPDqP
DqqPDqPDQP
ii
ii
ii
mi
iismoothed
mi
i
mi ii
ii
mi
i
mi i
ii
mi
i
mi
iii
mi
iismoothed
?
?
?
=
?
=
= ?
?
=
=
?
=
=
?
=
?
??
+=
??
?++?
??++=
??+?+=
+=
??
??
??
?
?
?
?
??
??
 (2) 
In Equation (2), the first score term is independ-
ence unigram score and the second score term is 
smoothed dependence score. Usually ? is set to 0.9, 
i.e., the dependence score is given a less weight 
compared with the independence score. 
DM (Gao et al, 2004), which can be regarded as 
the generalization of the bi-gram model, gives the 
relevance score of a document as:  
?
?
?
=
+
+=
Lji
ji
mi
i
DLqqMI
DLPDqPDQP
),(
...1
),|,(
)|(log)|(log)|(log
     (3) 
In Equation (3),L is the set of term dependencies 
in query Q. The score function consists of three 
parts: a unigram score, a smoothing factor 
logP(L|D), and a dependence score MI(qi,qj|L,D). 
MRF (D. Metzler and W. B. Croft, 2005) com-
bines the score of full independence, sequential 
dependence and full dependence in an interpolated 
way with the weight (0.8, 0.1, 0.1).  
Though these above models are derived from 
different theories, smoothing is an important part 
when incorporating term dependencies.  
3 Syntactic Parsing of Queries and 
Documents 
Term dependencies defined as term proximity may 
contain many ?noisy? dependencies. It?s our belief 
that parsing technique can filter out some of these 
noises and syntactic relationship is a clue to define 
parser, Minipar, to extract the syntactic depend-
ency between words.  In this section we will dis-
cuss the extraction of syntactic dependencies and 
the indexing schemes of term dependencies. 
3.1 Extraction of Syntactic Dependencie
term dependencies.  We use a popular dependency 
s 
ary 
an
des in the parsing result are single 
w
A dependency relationship is an asymmetric bin
relationship between a word called head (or 
governor, parent), and another word called 
modifier (or dependent, daughter). Dependency 
grammars represent sentence structures as a set of 
dependency relationships. For example, Figure 1 
takes the description field of TREC topic 651 as an 
example and shows part of the parsing result of 
Minipar. 
 
In Figure 1, Cat is the lexical category of word, 
d Rel is a label assigned to the syntactic depend-
encies, such as subject (sub), object (obj), adjunct 
(mod:A), prepositional attachment (Prep:pcomp-n), 
etc. Since function words have no meaning, the 
dependency relationships including function words, 
such as N:det:Det, are ignored. Only the depend-
ency relationships between content words are ex-
tracted. However, prepositional attachment is an 
exception. A prepositional noun phrase contains 
two parts: (N:mod:Prep) and (Prep:pcomp-n:N). 
We combine these two parts and get a relationship 
between nouns. 
Mostly, the no
ords. When the nodes are proper names, diction-
ary phrases, or compound words connected by hy-
phen, there are more than one word in the node. 
For example, the 5th and 6th relationship in Figure 1 
describes a compound word ?make up?.  We di-
vide these nodes into bi-grams, which assume de-
pendencies exist between adjacent words inside the 
Figure 1. Parsing Result of Minipar 
Node2 Node1 Cat1:Rel:Cat2 
TREC Topic 651: ?How is the ethnic make-
up of the U.S. population changing?? 
? 
3   makeup N:det:Det the 
4   makeup N:mod:A ethnic 
5   makeup N:lex-mod:U make 
6   makeup N:lex-mod:U - 
8   makeup N:mod:Prep of 
11 of  Prep:pcomp-n:N population 
9   population N:det:Det the 
10 population N:nn:N  U.S. 
? 
199
nodes.  If the compound-word node has a relation-
ship with other nodes, each word in the compound-
word node is assumed to have a relationship with 
the other nodes. Finally, the term dependencies are 
represented as word pairs. The direction of syntac-
tic dependencies is ignored. 
3.2 Indexing of Term Dependencies 
And the 
 
of
e that 
trieval status value (RSV) has the form: 
Parsing is a time-consuming process. 
documents parsing should be an off-line process. 
The parsing results, recognized as term dependen-
cies, should be organized efficiently to support the 
computation of relevance score at the retrieval step. 
As a supplement of regular documents?words 
inverted index, the indexing of term dependencies 
is organized as documents?dependencies lists. 
For example, Document A has n unique words; 
each of these n words has relationships with at 
least one other word. Then the term dependencies 
inside these n words can be represented as a half-
angle matrix as Figure 2 shows.  
 
The (i,j)-th element of the matrix is the number
 times that tidi and tidj have a dependency in 
document A. The matrix has the size of (n-1)*n/2 
and it is stored as list of size (n-1)*n/2. Each 
document corresponds to such a matrix. When ac-
cessing the term dependencies index, the global 
word id in the regular index is firstly converted to 
the internal id according to the word?s appearance 
order in the document. The internal id is the index 
of the half-angle matrix. Using the internal id pair, 
we can get its position in the matrix. 
4 Smooth-based Dependence Model 
From the discussion in section 2.2, we can se
smoothing is very important not only in unigram 
language model, but also in dependence language 
model. Taking the smoothed unigram model (C. 
Zhai and J. Lafferty, 2001) as the example, the re-
D
DQw D
DML
UG QCwp
Dwp
QwcDQRSV ?? log||)|(
)|(
log),(),( += ?
??
In Equation (4), c(w,Q) is the frequency of w
Q. The equation has three parts: PDML(w|D), ?D and 
P(
 (4) 
 in 
w|C). PDML(w|D) is the discounted maximum 
likelihood estimation of unigram P(w|D), ?D is the 
smoothing coefficient of document D, and P(w|C) 
is collection language model. If we use a smooth-
ing strategy as the smoothed MI in Equation (2), 
and replace term w with term pair (wi,wj), we can 
get the smoothed dependence model as: 
?
??
?+
=
DLww jismooth
j
ji
ji
Cwwp
D
Qwwc
),(
0 ))|,(
)|
1log(),,( ?  (5) 
In Equation (5), ?0 is the smoothing coefficient. 
Psm (w ,w |D) and Psm (w ,w |C) is the smoothed 
w
i j
e the Psmooth(wi,wj|D): 
 pair with relation-
sh
ismooth
DEP
wwp
DQRSV
,(
),(
ooth i j ooth i j
eight of term pair (wi,wj) in document D and col-
lection C.  
4.1 Smoothing P(w ,w |D) 
We use two parts to estimat
one is the weight of the term
ips in D, P(wi,wj|R,D), the other is the weight of 
the term co-occurrence in D, Pco(wi,wj|D). These 
two parts are defined as below: 
|D|)/(wCD)|P(w
D)|P(wD)|P(wD)|w,(wP
|D|R)/,w,(wCD)R,|w,P(w jiDji
?=
tid1 tid2 ? tidn-1 tidn
tid1
tid2
? 
 
tidn-1
tidn
 
??
?
?
??
?
?
?
??
?
?
??
?
?
?
0****
10...**
03...**
45..0*
20...10
 
iDi
jijiCO
=
=
   (6) 
|D| is the document length, CD(wi,wj,R) denotes 
the count of the dependency (wi,w ) in the docu-
m
jico1
j
ent D, and CD(wi) is the frequency of word wi in 
D. Psmooth(wi,wj|D) is defined as a combination of 
the two parts: 
P )?-(1
D)R,|w,P(w? D)|w,(wP ji1jismooth
?+ D)|w,(w
?=
 (7) 
Figure 2. Half-angle matrix of term dependencies
4.2 Smoothing P(wi,wj|C) 
bability of term pair 
. We use docu-
m
To directly estimate the pro
(w ,w ) in the collection is not easyi j
ent frequency of term pair (wi,wj) as its approxi-
mation. Same as Psmooth(wi,wj|D), Psmooth(wi,wj|C) 
consists of two parts: one is the document fre-
quency of term pair (wi,wj), DF(wi,wj), the other is 
the averaged document frequency of wi and wj. 
Then, Psmooth(wi,wj|C) is defined as: 
Dji
Djijismooth
CwDFwDF
CwwDFCwwP
||)()()1(
||),()|,( 2
???+ 2
?=
?
?
  (8) 
200
In Equation (8), |C|D is the count of Document in 
Collection C.  
Finally, if substituting Equation (7) and (8) into 
Eq
). The final retrieval status value of 
th
s 
To answer the question whether the syntactic de-
term proximity, 
,w ,R) in Equa-
tio
parameter 
is 
ns. Some statistics of the col-
lec
rameters (? ,? ,? ), 
SD
(MB) Doc.
uation (5), there are three parameters (?0,?1,?2) 
in RSVDEP(Q,D
e smooth-based dependence model, RSVSDLM, is 
the sum of RSVDEP and RSVUG: 
),(),(),( DQRSVDQRSVDQRSV UGDEPSDLM +=   (9) 
5 Experiments and Result
pendencies is more effective than 
we systematically compared their performance on 
two kinds of queries. One is verbose queries (the 
description field of TREC topics), the other is short 
queries (the title field of TREC topics). Since the 
verbose queries are sentence-level, they are parsed 
by Minipar to get the syntactic dependencies. In 
short queries, term proximity is used to define the 
dependencies, which assume every two words in 
the queries have a dependency.  
Our smooth-based dependence language model 
(SDLM) is used as dependence retrieval model in 
the experiments. If defining CD(wi j
n (6) to different meanings, we can get a de-
pendence model with syntactic dependencie, 
SDLM_Syn, or a dependence model with term 
proximity, SDLM_Prox. In SDLM_Syn, 
CD(wi,wj,R) is the count of syntactic dependencies 
between wi and wj in D. In SDLM_Prox, 
CD(wi,wj,R) is the number of times the terms wi 
and wj appear within a window N terms. 
We use Dirichlet-Prior smoothed KL-
Divergence model as the unigram model in Equa-
tion (9). The Dirichlet-Prior smoothing 
set to 2000. This unigram model, UG, is also the 
baseline in the experiments. The main evaluation 
metric in this study is the non-interpolated average 
precision (AvgPr.) 
We evaluated the smooth-based dependence 
language model in two document collections and 
four query collectio
tions are shown in Table 2. 
Three retrieval models are evaluated in the 
TREC collections: UG, SDLM_Syn and 
SDLM_Prox. Besides the pa 0 1 2
LM_Prox has one more parameter than 
SDLM_Syn. It is the window size N of 
CD(wi,wj,R). In the experiments, we tried the win-
dow size N of 5, 10, 20 and 40 to find the optimal 
setting. We find the optimal N is 10. This size is 
close to sentence length and it is used in the fol-
lowing experiments. 
Coll. Queries Documents Size # 
AP 51-200 Associated Press 
(1  
489 164,597
988,1989) in
Disk2 
TR -8EC7 351-450
Ro
Hard queries in
bust04 35 hard
351-450
Robust04
New 
651-700 
ex.672
Disk 4&5 
(no CR) 
3,120 528,155
Table 2.  TREC collections 
eter ,?2) were trained on three query 
se 700. Each query set 
was divided into two halves, and we applied two-
fo
Param s (?0,?1
ts: 51-200, 351-450 and 651-
ld cross validation to get the final result. We 
trained (?0,?1,?2) by directly maximizing MAP 
(mean average precision). Since the parameter 
range was limited, we used a linear search method 
at step 0.1 to find the optimal setting of (?0,?1,?2). 
 
Topic Index
0 20 40 60 80 100 120 140 160
A
vg
P
r
0.0
.2
.4
.6
.8
1.0
UG
SDLM_Syn
Topic Index
0 20 40 60 80 100
A
vg
P
r
0.0
.2
.4
.6
.8
1.0
UG
SDLM_Syn
Topic Index
0 10 20 30 40
A
vg
P
r
0.00
.05
.10
.15
.20
.25
.30
.35
UG
SDLM_Syn
Topic Index
0 10 20 30 40 50
A
vg
P
r
0.0
.2
.4
.6
.8
1.0
UG
SDLM_Syn
 
Figure 3 UG vs. SDLM_Syn in verbose queries: 
Top Left (51-200), Top Right (351-450), Bottom 
Left (hard topics in 351-450), and Bottom Right 
 Table 3 and Table 4 respectively. The 
settings of (?0,?1,?2) used in the experiments are 
al
(651-700) 
The results on verbose queries and short queries 
are listed in
so listed. A star mark after the change percent 
value indicates a statistical significant difference at 
the 0.05 level(one-sided Wilcoxon test). In verbose 
queries, we can see that SDLM has distinct 
201
 
UG SDLM_Prox SDLM_Syn collections 
AvgPr. AvgPr. %ch over UG (?0,?1,?2) AvgPr. %ch over UG (?0,?1,?2) 
AP 0.2159 0.2360 9.31* (1.8,0.6,0.9) 0.2393 10.84* (1.9,0.7,0.9)
TREC7-8 0.1893 0.2049 8.24* (1.2,0.1,0.2) 0.2061 8.87* (0.4,0.1,0.9)
Robust04_hard 0.0909 0.1049 15.40* (1.2,0.1,0.2) 0.1064 17.05* (0.4,0.1,0.9)
Robust04_new 0.2754 0.3022 9.73* (0.7,0.1,0.3) 0.3023 9.77* (0.7,0.1,0.3)
Table 3. Comparison results on verbose queries 
UG SDLM_Prox SDLM_Syn collections 
AvgPr. AvgPr. %ch over UG (?0,?1,?2) AvgPr. %ch over UG (?0,?1,?2) 
AP 0.2643 0.2644 0 (1.3,0.6,0.1) 0.2647 0.15 (1.1,0.5,0.2)
TREC7-8 0.2069 0.2076 0.34 (1.2,0.3,0.2) 0.2070 0 (1,0.1,0.2) 
Robust04_hard 0.1037 0.1044 0.68 (1.2,0.3,0.2) 0.1045 0.77 (1,0.1,0.2) 
Robust04_new 0.2771 0.2888 4.22* (1.3,0.3,0.4) 0.2869 3.54* (1.3,0.1,0.4)
Table 4. Comparison results on short queries 
Topic Index
0 20 40 60 80 100 120 140 160
A
vg
P
r
0.0
.2
.4
.6
.8
1.0
SDLM_Prox
SDLM_Syn
Topic Index
0 20 40 60 80 100
A
vg
P
r
0.0
.2
.4
.6
.8
1.0
SDLM_Prox
SDLM_Syn
 
Topic Index
0 10 20 30 40
A
vg
P
r
0.00
.05
.10
.15
.20
.25
.30
.35
SDLM_Prox
SDLM_Syn
Topic Index
0 10 20 30 40 50
A
vg
P
r
0.0
.2
.4
.6
.8
1.0
SDLM_Prox
SDLM_Syn
 
Figure 4. SDLM_Prox vs. SDLM_Syn in verbose 
queries: Top Left (51-200), Top Right (351-450), 
Bottom Left (hard topics in 351-450), Bottom 
Right (651-700) 
improvement over UG and SDLM_Syn has robust 
improvement over SDLM_Prox. In short queries, 
SDLM has slight improvement over UG and 
SDLM_Syn is comparative with SDLM_Prox. 
To study the effectiveness of syntactic depend-
encies in detail, Figure 3 and 4 compare 
SDLM_Syn and UG, SDLM_Syn and 
SDLM_Prox topic by topic in verbose queries. 
As shown in Figure 3 and Figure 4, SDLM_Syn 
achieves substantial improvements over UG in the 
majority of queries. While SDLM_Syn is com-
parative with SDLM_Prox in most of the queries, 
SDLM_Syn still get some noticeable improve-
ments over SDLM_Prox. 
From Table 3 and 4, we can see while the pa-
rameters (?0,?1,?2) change a lot in two different 
document collections, there is little change in the 
same document collection. This shows the robust-
ness of our smooth-based dependence language 
model. 
6 Conclusion 
In this paper we have systematically studied the 
effectiveness of syntactic dependencies compared 
with term proximity in dependence retrieval 
model. To compare the effectiveness of syntactic 
dependencies and term proximity, we develop a 
smooth-based dependence language model that 
can incorporate different term dependencies.  
Experiments on four TREC collections indicate 
the effectiveness of syntactic dependencies: In 
verbose queries, the improvement of syntactic 
dependencies over term proximity is noticeable; 
In short queries, the improvement is not notice-
able.  For keywords-based short queries with av-
erage length of 2-3 words, the term dependencies 
in the queries are very few. So the improvement 
of dependence retrieval model over independence 
unigram model is very limited. Meanwhile, the 
difference between syntactic dependencies and 
term proximity is not noticeable. For dependence 
retrieval model, we can get the same conclusion as 
Thorsten Brants: the longer the queries are, the 
bigger the benefit of NLP is. 
 
202
References 
ijsber R
worths, 1979. 
varez, s, Ji Nie
e g fo tion
s  200 686-
s for l e m  a
formation re ,
ges 334?342
b aluati MINI
 the E ion o
ndom 
field model for term dependencies, In Proceedings of 
SIGIR?05, Pages 472-479, 2005 
Fei Song and W. Bruce Croft. A general language 
model for information retrieval. In Proceedings of 
SIGIR?99, pages 279-280, 1999. 
Jianfeng Gao, Jian-Yun Nie, Guangyuan Wu and Gu -
hong Cao, Dependence Language Model for Info -
mation Retrieval, In Proceedings of SIGIR?04, 
Pages:170-177, 2004 
Jianfeng Gao, Haoliang Qi, Xinsong Xia and Jian-Yun 
Nie. Linear Discriminant Model for Information Re-
trieval. In Proceedings of SIGIR?05, Pages:290-297, 
2005 
y Computer Laboratory. 1998 
Shuang Liu, Fang Liu, Clement Yu and Weiyi Meng, 
An Effective Approach to Document Retrieval via 
Utilizing WordNet an ses, In Pro-
in I
 Ter ence he 
u feld e n. Inf ess-
d men 3?3
 atur uage  In-
formation Retrieval . In Proceedings of 20th Interna-
tional Conference o nal Linguistics, 
e
C. J. van R g foen. In rmation etrieval. Butter-
Carmen Al  Philippe Langlai an-Yun , Bahad
Word Pairs in 
Retrieval, In Pr
Languag
oceeding
 Modelin
 of RIAO
r Informa
4, Pages 
 ing an
705, 2004 
Chengxiang 
ing method
Zhai an n Lafferty. A stud Joh
angua
dy of smooth-
lied to ad hoc g
trieval. In
odels
 Proceedi
pp
ngs of SIGIR?01in
pa
 
 , 2001 
Dekang Lin, Dep
PAR, Proceedin
endency-
gs of Wo
ased Ev
rkshop on
on of 
valuat
-
f 
Parsing Systems, Granada, Spain, May,
Donald Metzler and W. Bruce Croft, A Markov ra
 1998. 
i
r
K. Sparck Jones, S. Walker, and S. E. Robertson, A 
probabilistic model of information retrieval: devel-
opment and status. Technical Report TR-446, Cam-
bridge Universit
Lioma C. and Ounis I., A Syntactically-Based Query 
Reformulation Technique for Information Retrieval, 
Information Processing and Management (IPM), El-
sevier Science, 2007 
M. Mitra, C. Buckley, A. Singhal, and C. Cardie. An 
Analysis of Statistical and Syntactic Phrases. In 
Proceedings of RIAO-97, 5th International Confer-
ence ?Recherche d?Information Assistee par Ordi-
nateur?, pages 200-214, Montreal, CA, 1997. 
Munirathnam Srikanth and Rohini Srihari, Exploiting 
Syntactic Structure of Queries in a Language Mod-
eling Approach to IR, In Proceedings of CIKM?03, 
Pages: 476-483, 2003 
 
d Recognizing Phra
-2ceed gs of SIG R?04, Pages: 266 72, 2004 
Robert M. Losee. m depend : Truncating t
r Lazars xpansio ormation Proc
 Manage t, 30(2):29 03, 1994. 
Thorsten Brants. N al Lang Processing in
n Computatio
Antw rp, Belgium, 2004:1-13. 
 
203
