Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1261?1272,
October 25-29, 2014, Doha, Qatar.
c
?2014 Association for Computational Linguistics
Confidence-based Rewriting of Machine Translation Output
Benjamin Marie
LIMSI-CNRS, Orsay, France
Lingua et Machina, Le Chesnay, France
benjamin.marie@limsi.fr
Aur
?
elien Max
LIMSI-CNRS, Orsay, France
Univ. Paris Sud, Orsay, France
aurelien.max@limsi.fr
Abstract
Numerous works in Statistical Machine
Translation (SMT) have attempted to iden-
tify better translation hypotheses obtained
by an initial decoding using an improved,
but more costly scoring function. In this
work, we introduce an approach that takes
the hypotheses produced by a state-of-
the-art, reranked phrase-based SMT sys-
tem, and explores new parts of the search
space by applying rewriting rules se-
lected on the basis of posterior phrase-
level confidence. In the medical do-
main, we obtain a 1.9 BLEU improve-
ment over a reranked baseline exploiting
the same scoring function, corresponding
to a 5.4 BLEU improvement over the orig-
inal Moses baseline. We show that if an
indication of which phrases require rewrit-
ing is provided, our automatic rewriting
procedure yields an additional improve-
ment of 1.5 BLEU. Various analyses, in-
cluding a manual error analysis, further il-
lustrate the good performance and poten-
tial for improvement of our approach in
spite of its simplicity.
1 Introduction
The standard configuration of modern phrase-
based Statistical Machine Translation (SMT)
(Koehn et al., 2003) systems can produce very ac-
ceptable results on some tasks. However, early
integration of better features to guide the search
for the best hypothesis can result in significant im-
provements, an expression of the complexity of
modeling translation quality. For instance, im-
provements have been obtained by integrating fea-
tures into decoding that better model semantic co-
herence at the sentence level (Hasan and Ney,
2009) or syntactic well-formedness (Schwartz et
al., 2011). However, early use of such complex
features typically comes at a high computational
cost. Moreover, some informative features require
or are better computed when complete translation
hypotheses are available. This is addressed in nu-
merous works on reranking of the highest scored
sub-space of hypotheses, on so-called n-best lists
(Och et al., 2004; Zhang et al., 2006; Carter and
Monz, 2011) or output lattices (Schwenk et al.,
2006; Blackwood et al., 2010), where many works
specifically target the inclusion of better language
modelling capabilities, a well-known weakness of
current automatic generation approaches (Knight,
2007).
Another way to improve translation a posteriori
can be done by rewriting initial hypotheses, for in-
stance in a greedy fashion by including new mod-
els (Langlais et al., 2007; Hardmeier et al., 2012),
or by specifically modeling a task of automatic
post-editing targeting a specific system (Simard et
al., 2007; Dugast et al., 2007). While such auto-
matic post-editing may seem to be too limited, no-
tably because of the limited initial diversity con-
sidered and the fact that it may be in some in-
stances agnostic to the internals of the initial sys-
tem, it has been shown to potentially improve ac-
curacy of the new translation hypotheses (Parton
et al., 2012) and to offer very high oracle perfor-
mance (Marie and Max, 2013).
However, an important issue for such ap-
proaches is their capacity to only rewrite incor-
rect parts of the translation hypotheses and to use
appropriate replacement candidates. Many works
have tackled the issue of word to n-gram confi-
dence estimation in SMT output (Zens and Ney,
2006; Ueffing and Ney, 2007; Bach et al., 2011;
de Gispert et al., 2013), and some attempts have
been made to exploit confidence estimates for lat-
tice rescoring (Blackwood et al., 2010) or n-best
reranking (Bach et al., 2011; Luong et al., 2014b).
In this work, we present an approach in which
1261
new complete hypotheses are produced by rewrit-
ing existing hypotheses, and are scored using com-
plex models that could not be used during the ini-
tial decoding. We will use as competitive baselines
systems that rerank the output of an initial decoder
using the complete set of available features, and
will show that we manage to improve their trans-
lation. The difference between our approach and
the reranking baseline lies in the manner in which
we expand our training data, as well as in our use
of high-confidence rewritings to obtain new trans-
lation hypotheses. Importantly, this work will only
exploit simple confidence estimates corresponding
to phrase-based posteriors, which do not require
that large sets of human-annotated data be avail-
able as in other works (Bach et al., 2011; Luong et
al., 2014b).
The remainder of this paper is organized as fol-
lows. Section 2 is devoted to the description of
our approach, with details on our rewriting ap-
proach (2.1), additional features (2.2), rewriting
phrase table (2.3), and training examples (2.4).
Section 3 presents experiments. We first describe
our experimental setup (3.1) and our baseline sys-
tems (3.2). We then report results when naive
rewriting is performed and then with confidence-
based rewriting (3.3). We next devote a significant
part of the paper in section 4 to report further re-
sults and analyses: an analysis of the performance
of our system depending on the quality of initial
hypotheses (4.1); a semi-oracle experiment where
correct phrases are known (4.2); an oracle exper-
iment where only correct rewriting decisions are
made (4.3); a manual error analysis of the main
configurations studied in this work (4.4); and, fi-
nally, a study of the performance of our approach
on a more difficult translation task (4.5). Related
work is discussed in section 5 and we conclude
and introduce our future work in section 6.
2 Description of the approach
2.1 Rewriting of translation hypotheses
Langlais et al (2007) proposed a greedy search
procedure to improve translations by reusing the
same translation table and scoring function that
were used during an initial phrase-based decoding.
In our approach, we rewrite hypotheses by using
the same greedy search algorithm, adding more
complex models and using the most-confident bi-
phrases according to the initial decoder?s search
space. To select the hypothesis to rewrite for
each sentence, we produce a n-best list of the ini-
tial decoder and rerank this list with a new, bet-
ter informed scoring function (see section 2.2).
The one-best hypothesis obtained after rerank-
ing is then rewritten by our system (denoted as
rewriter). In this way, we ensure that the hy-
pothesis that was rewritten had been so far the
best one according to the initial decoding best sub-
space and the new models used.
At each iteration, new hypotheses are obtained
from a current hypothesis by applying one rewrit-
ing operation on bi-phrases. The set of all new hy-
potheses is called the neighborhood of the current
hypothesis. Focusing in this work on local rewrit-
ing, we used the following set of operations (N de-
notes the number of bi-phrases, T the maximum
number of entries per source phrase in a rewriting
phrase table (see 2.3), and S the average number
of tokens per source phrase)
1
:
1. replace (O(N.T )): replaces the transla-
tion of a source phrase with another transla-
tion from the rewriting phrase table;
2. split (O(N.S.T
2
)): splits a source phrase
into all possible sets of two (contiguous)
phrases, and uses replace on each of the
resulting phrases;
3. merge (O(T.N)): merges two contiguous
source phrases and uses replace on the re-
sulting new phrase.
This rewriting algorithm is described in pseudo-
code in Algorithm 1.
Algorithm 1 rewriter Algorithm
Require: source a sentence to translate
nbestList? TRANSLATE(source)
oneBest? RERANK(nbestList)
sCurrent? GET SCORE(oneBest)
loop
hypothesesSet? NEIGHBORHOOD(oneBest)
newOneBest? RANK(hypothesesSet)
s? GET SCORE(newOneBest)
if s ? sCurrent then
return oneBest
else
oneBest? newOneBest
sCurrent? s
end if
end loop
1
Complexity is expressed in terms of the maximum num-
ber of hypotheses that will be considered given some hypoth-
esis to rewrite.
1262
The produced hypotheses are then ranked ac-
cording to a new, better informed scoring function
(see 2.2). At the next iteration, the hypothesis now
ranked at the top of the list is rewritten, and search
terminates when no better hypothesis is found.
Such a greedy search has several obvious lim-
itations, in particular it can only perform a lim-
ited exploration of the search space, a situation
that can be improved by using a beam (see Sec-
tion 3.3). However, associated with a small and
precise rewriting phrase table, this approach only
visits small numbers of more-confident hypothe-
ses, which is a critical property given the cost of
computing the new scoring function used.
2.2 Reranking and features
The rerankings of the hypotheses sets de-
scribe in this work are all performed with
kb-mira (Cherry and Foster, 2012) using the ini-
tial features set of the decoder in conjunction with
the following additional features:
2
? SOUL models: SOUL models are structured
output layer neural network language mod-
els (LMs) which have been shown to be use-
ful in reranking tasks, for instance for WMT
evaluations (Allauzen et al., 2013; P?echeux et
al., 2014). SOUL scoring being too costly to
be integrated during decoding, it fits perfectly
the reranker scenario, which furthermore
enables to use larger contexts for n-grams.
We used both monolingual (Le et al., 2011)
and bilingual (Le et al., 2012) SOUL 10-gram
models, which were trained on the WMT?12
data.
? POS language model: part-of-speech (POS)
LMs have been shown to yield improvements
in n-best list reranking (Carter and Monz,
2011). In this work, we trained a 6-gram POS
LM using Witten-Bell smoothing.
? IBM1 : the IBM1 scores (p(e|f) and p(f |e))
of the complete hypothesis (Och et al., 2004).
? phrase-based confidence score : bi-phrases
are associated to a posterior probability, in-
spired from n-gram posterior probability esti-
mation as defined in (de Gispert et al., 2013).
Let E be the set of all hypotheses in the
space of translation hypotheses defined by
2
Note that we did not try to explore the independant con-
tribution of each feature in this work.
the n-best list used for source sentence f , and
E
?
be the subset of E such that word align-
ments in sentence pairs (e
?
, f), ?e
?
? E
?
,
allow us to extract bi-phrase ?. Let also
H(e, f) be the score assigned by a base-
line decoder (denoted as 1-pass Moses
henceforth) to sentence pair (e, f). We use
the following posterior probability for ?:
P (?|F ) =
?
e
?
?E
?
exp(H(e
?
, f))
?
e
??
?E
exp(H(e
??
, f))
(1)
Then, the logarithms of each phrase?s con-
fidence score are summed to use as a confi-
dence score for the complete hypothesis.
2.3 Rewriting phrase table
Taking the whole translation table of the decoder
as a rewriting phrase table to perform the greedy
search produces very large neighborhoods that
rewriter cannot handle due to the cost of the
models that have to be computed. We tried two
different approaches to extract a rewriting phrase
table from the translation table of the system.
We first tried a naive approach where the rewrit-
ing phrase table of rewriter for the test set
uses the phrase table of 1-pass Moses, filtered
to keep the k best entries according to the direct
translation model. We denote such a configuration
rptkpef.
Our second approach consists in extracting the
rewriting phrase table containing bi-phrases that
were the most probable according to the set of all
models used in 1-pass Moses. Selection of bi-
phrases for each sentence is done in a binary fash-
ion, depending on their presence in k-best lists of
1-pass Moses for a given value of k. This con-
figuration will be denoted confk.
2.4 Training examples
We tried several sets of examples to train the
ranker of rewriter. We used the 1,000-best
list of the development set produced by 1-pass
Moses during its tuning. In other configurations
we mixed a) the neighborhood of the reranker
n-best hypotheses computed by our system on the
development set using a rewriting phrase table
containing the bi-phrases found in the k-best list
produced by 1-pass Moses; and b) the neigh-
borhood of the one-best hypotheses of reranker
using a rewriting phrase table containing the 10-
best translations from the 1-pass Moses trans-
lation table according to the direct translation
1263
model. Both neighborhoods are produced by a
single iteration of rewriter. We denote re-
spectively these sets of hypotheses n-bestNeigh
and 10PefNeigh. Our intuition behind the consti-
tution of these training sets is that the ranker of
rewriter needs, in order to perform well, train-
ing examples that will be similar to hypotheses
that it actually generates.
3 Experiments
3.1 Experimental setup
We used two datasets from two different domains:
the data provided for the WMT?14 medical trans-
lation task
3
(Medical) and a smaller task using
the TED talks
4
(TED Talks) data of the IWSLT
evaluation campaigns. For the Medical task we
used only the English to French translation di-
rection, and both translation directions, English
to French and French to English, for the TED
Talks task. In this work, the main part of our ex-
periments uses Medical, and TED Talks will
be used at a later stage to study a lower-quality
situation (cf. 4.5). For the Medical task, initial
decodings were produced using a LM trained on
all WMT?14 monolingual and bilingual medical
data, while for the TED Talks task we used a
much larger LM trained on all the data provided
for WMT?13
5
. Both are 4-gram LMs estimated
with Kneser-Ney smoothing (Chen and Goodman,
1998). For the 6-gram POS LMs used (see 2.2),
we used the same data as used for the token-based
LM for Medical, and the concatenation of the
News Commentaries and Europarl sub-parts of the
WMT?13 data for TED Talks. Table 1 provides
relevant statistics about the data used.
Tasks Corpus Sentences Tokens (en-fr)
Medical
train 4.9M 78M - 91M
dev 500 10k - 12k
test 1,000 21k - 26k
LM - 146M
TED Talks
train 107 758 2M - 2.2M
dev 934 20k - 20k
test 1,664 31k - 34k
LM 6B - 2.5B
Table 1: Corpora used in this work.
3
http://www.statmt.org/wmt14/
medical-task/
4
https://wit3.fbk.eu/mt.php?release=
2013-01
5
http://www.statmt.org/wmt13
We first built a state-of-the-art phrase-based
SMT system using Moses (Koehn et al., 2003)
with standard settings. We tuned its parameters to-
wards BLEU (Papineni et al., 2002) on the tuning
dataset using the kb-mira implementation avail-
able in Moses with default parameters.
Our results will be compared using BLEU
and TER (Snover et al., 2006) to a) the initial
best translation produced by the Moses decoder
(1-pass Moses) and b) the best translation ob-
tained by reranking the 1,000-best list of 1-pass
Moses (reranker). Since reranker imple-
ments a well-documented approach and uses types
of features commonly used in reranking tasks we
will consider it as our main baseline. It was trained
using kb-mira on the 1,000-best of the develop-
ment data decoded by 1-pass Moses.
In our experiments, rewriter rewrites the
one-best hypothesis
6
produced by reranker
using the operators Replace, Split and
Merge as described in section 2.1.
3.2 Baseline results
Table 2 gives the results of the 1-pass Moses
decoding for the Medical task and the rerank-
ing results of reranker applied to the 1-pass
Moses 1,000-best list.
1-pass Moses obtains a score of 38.2 BLEU
on the test set, which can be considered as
a good baseline system.
7
reranker outper-
forms 1-pass Moses by 3.5 BLEU, indicating
a strong performance of the features used on this
task. In particular, SOUL is known to be a use-
ful feature for reranking n-best lists on highly-
inflected languages such as French. Note also that
the SOUL models we used were trained on the
WMT?12 monolingual and bilingual data and so
were better informed than the models used dur-
ing the 1-pass Moses decoding.
8
Moreover,
as can be seen on Figure 1, the 1,000-best ora-
cle reveals a large potential for improvement over
the one-best (+12.4 BLEU). We further observe
that the reranked list of reranker shows a much
faster potential for translation improvement.
6
Note that we will also provide results where a beam of
k-best hypotheses are rewritten.
7
Distribution of error types on a sub-part of the test set
will be provided in section 4.4.
8
However, SOUL considers only a small sample of the
training data for training. For instance, the training of the
French monolingual model used roughly only 1% (895K sen-
tences) of all the WMT?12 data.
1264
Figure 1: n-best list oracle for 1-pass Moses
and reranker
3.3 rewriter results
Results for the different rewriting phrase tables
and training examples are given in Table 2. First,
concerning the rewriting phrase table, for the
k=5 (rpt5pef) and k=10 (rpt10pef) con-
figurations
9
a decrease of 0.7-0.8 BLEU over
reranker is obtained. This illustrates that naive
rewritings applied on the test set cannot be used
with our training regime to improve translation
quality.
In the next experiments, we used a confk
rewriting table. Table 2
10
shows the results of
rewriter when rewriting the one-best hypothe-
sis from reranker for various values of k to de-
fine the k-best list from which the rewriting table
is built. Various training sets are also considered
in the table.
The 1-pass Moses 1,000-best configuration
reused the same set of hypotheses used to train
reranker. For this configuration, rewriter
loses 2.6 BLEU over reranker on the test set
with conf10k. Of course, this training data set
is of a quite different nature compared to the hy-
potheses built by rewriter.
In the 10pefNeigh training, the ranker is trained
with the neighborhoods produced by the first itera-
tion of rewriter on the development set with a
rewriting phrase table containing only the k-best
translations for each source phrase according to
the direct translation model. This configuration
9
We did not experiment with higher values of k because of
the computationnal cost of the features used by reranker.
Indeed, adding more phrase translations increases the size of
the neighborhoods corresponding to many additional n-grams
to score by SOUL, the most expensive model.
10
In Table 2 the number of unique bi-phrases for the
rpt rewriting phrase tables is computed by considering only
source phrases appearing in the test set, for the n-best Neigh-
borhood configurations we merged the phrase tables of each
sentence into one and count just as one unique entry bi-
phrases appearing several times.
improves over the previous one by 1.7 BLEU, but
is still 0.9 BLEU below reranker. Adding the
neighborhoods of the reranker n-best hypothe-
ses produced with a conf10k rewriting phrase
table to the training data does not improve over
the previous situation for n = 10, but increasing n
to 30 and then 50 produces strong improvements
on the test set (resp. +1.4 and +1.6 BLEU). Con-
sidering a larger neighborhood obtained by rewrit-
ing the best n = 90 hypotheses does not yield
further gains. We denote from now on opti
our best configuration thus far, considering the
performance on the development set and having
the largest confidence-based rewriting phrase ta-
ble considered.
Letting rewriter perform a beam search on
the 10-best hypotheses of the test set, further gains
are obtained, corresponding now to an improve-
ment of +1.9 BLEU over our reranker base-
line, or +5.4 BLEU over 1-pass Moses.
11
Fur-
thermore, although taking the bi-phrases from the
10,000-best is our best configuration, it is inter-
esting to note that taking bi-phrases from the 10-
best only already yields a moderate improvement
of +0.6 BLEU over reranker. Figure 2a shows
that up to k = 10, 000 higher value of k to ex-
tract the rewriting phrase table increase the BLEU
score on the test set.
12
We did not experiment with
higher values of k, but plan to use the output lat-
tice produced by 1-pass Moses to compute ef-
ficiently posteriors for larger sets of bi-phrases (de
Gispert et al., 2013).
As illustrated on Figure 2b, rewriter mostly
improves the BLEU score during the three first
iterations and then converges at the ninth iteration.
However, it is important to note that not all
sentences are actually improved by our system.
As illustrated on Figure 3a, opti improves
40.8% of the sentences of the test set but degrades
29.2% of them according to sentence-BLEU (Lin
and Och, 2004). It is certainly the case that
more informative confidence features may help
idenfity more precisely which fragments of the
translations should really undergo rewriting. We
will investigate the exploitation of an oracle
phrase-based confidence measure in Section 4.2.
11
Using a beam becomes quickly prohibitive: using
12 threads, 25 mn vs. 3h were needed for the test set for
the configurations of size 1 and 10, respectively.
12
Note that even for k = 10, 000 the computed neighbor-
hoods are still quite small with an average of 116 hypotheses
for each hypothesis to rewrite per iteration, against an average
of 788 hypotheses for the rpt10pef configuration.
1265
(a) Results of rewriter with rpt5pef, rpt10pef and dif-
ferent values of k for confk
(b) Iterations of rewriter on test with opti and two beam
sizes : 1 and 10.
Figure 2: Performance of rewriter depending on the type of the rewriting phrase table and the number
of iterations and beam sizes.
baseline
dev test
BLEU BLEU TER GOS BLEU
1-pass Moses 40.9 38.3 44.6
reranker 44.1 41.8 41.6
training data
rewriting unique beam
phrase table bi-phrases size
1-pass Moses 1 000-best conf10k 38 455 1 44.1 39.2
(?2.6)
43.8
(+2.2)
58.7
10pefNeigh conf10k 38 455 1 43.9 40.9
(?0.9)
41.2
(?0.4)
58.7
10-bestNeigh + 10pefNeigh conf10k 38 455 1 43.8 40.9
(?0.9)
41.2
(?0.4)
58.7
30-bestNeigh + 10pefNeigh conf10k 38 455 1 44.2 43.2
(+1.4)
40.6
(?1.0)
58.7
50-bestNeigh + 10pefNeigh rpt5pef 85 530 1 44.5 41.0
(?0.8)
42.0
(+0.4)
50.6
= rpt10pef 149 887 1 44.5 41.1
(?0.7)
42.1
(+0.5)
54.5
= conf10 21 398 1 44.5 42.4
(+0.6)
41.0
(?0.6)
45.9
= conf100 28 730 1 44.5 42.9
(+1.1)
40.8
(?0.8)
50.2
= conf1k 33 929 1 44.5 43.0
(+1.2)
40.6
(?1.0)
53.3
= (opti) conf10k 38 455 1 44.5 43.4
(+1.6)
40.4
(?1.2)
58.7
= conf10k 38 455 10 44.5 43.7
(+1.9)
40.1
(?1.5)
59.6
90-bestNeigh + 10pefNeigh conf10k 38 455 1 44.4 43.4
(+1.6)
40.4
(?1.2)
58.7
Table 2: Results on Medical for different training configurations, rewriting phrase tables and beam
sizes. opti denotes our optimal configuration for rewriter.
4 Analysis of confidence-based rewriting
4.1 Performance of rewriter depending on
the quality of initial hypotheses
The first question we address in our analysis of
rewriter is whether its performance depends
on the difficulty of each individual sentence. As
a proxy of sentence difficulty we used sentence-
BLEU of 1-pass Moses, and used it to di-
vide the sentences of the test set into quartiles.
Figure 4 shows that reranker improves more
over 1-pass Moses and that at the same time
rewriter improves more over reranker as
the sentences are more difficult. In particular,
rewriter obtains a 8.6 BLEU improvement
over 1-pass Moses on the more difficult quar-
tile, but only a 1.3 BLEU improvement on the least
difficult quartile. We hypothesize that better per-
formance may be achieved if adapting the training
and rewriting of rewriter to sentences of vary-
ing quality, which may, for instance, be estimated
with off-the-shelf estimators (Specia et al., 2013).
4.2 Semi-oracle experiments: rewriting only
incorrect fragments
We observed in section 3.3 that our opti con-
figuration, which obtains strong improvements in
translation quality (as given by corpus-BLEU),
in fact degrades (as given by sentence-BLEU)
a significant proportion of sentences. To fur-
ther analyze these results, we simulate a situa-
tion where oracle confidence information is avail-
able at the phrase-level: in particular, rewriter
is prevented from rewriting bi-phrases whose tar-
get phrase appears exactly in the reference transla-
1266
(a) automatic
(b) semi-oracle
Figure 3: sBLEU delta, for each sentence, between the reranker one-best to rewrite and its auto-
matic (3a) or semi-oracle (3b) rewriting computed by rewriter with the opti configuration.
Figure 4: Source sentences were divided into
quartiles according to sBLEU of the 1-pass
Moses system. For each quartile we reported the
performance of 1-pass Moses, reranker,
rewriter, GOS.
tion.
13
Furthermore, this ?freezing? of bi-phrases
can be repeated after each iteration of rewriter.
Thus, we now have an oracle situation for
choosing which source phrases may be rewrit-
ten, but the rest of the rewriting procedure is
still fully automatic. Moreover, we purposefully
did not adapt the training procedure to this new
configuration, and reused opti as is. Results,
reported in Table 3, indicate that an additional
1.5 BLEU is obtained from opti, or 3.1 BLEU
from reranker and 6.6 BLEU from 1-pass
Moses. The use of a larger beam of size 10
did not improve those results any further. At
13
This is obviously not an optimal solution.
the first iteration, rewriter ?froze? approx-
imatively 65.6% of the bi-phrases, and 70.5%
at the last iteration, demonstrating the ability of
rewriter to find good rewritings that match the
reference translation. Looking at Figure 3b, we
now find that, as expected, only a limited num-
ber of sentences are now degraded by rewriter.
The large improvements obtained clearly under-
lines the important role that better confidence esti-
mates could play in our framework.
System test
BLEU TER
reranker 41.8 41.6
opti 43.4 40.4
semi-oracle, beam 1 44.9
(+1.5)
39.2
(?1.2)
semi-oracle, beam 10 44.9
(+1.5)
39.0
(?1.4)
Table 3: Results for the semi-oracle using opti.
4.3 Oracle experiments: making only the
correct decisions
We now turn to the situation where only rewrit-
ings that actually improve translation performance
would be made. In practice, we use a sim-
ple solution: we resort to greedy oracle search
(GOS) (Marie and Max, 2013), where sentence-
BLEU is maximized using rewritings from the
opti phrase table. At each iteration the rewrit-
ing in the neighborhood that maximizes sentence-
BLEU is selected until convergence.
Results for this greedy search oracle appear in
the last column of Table 2 and allow us to put
in perspective the individual potential of the var-
1267
ious tested configurations. We can first notice
that the rpt5pef phrase table allows the ora-
cle to reach 50.6 BLEU, 8.1 BLEU below the
oracle value obtained with conf10k, although
rpt5pef contains twice as many bi-phrases. The
same conclusion can be made about rpt10pef,
which is 3.9 BLEU higher than rpt5pef but con-
tains nearly twice as many bi-phrases. Finally, al-
though conf10k contains approximatively four
times fewer bi-phrases than rpt10pef, its ora-
cle value is 4.2 BLEU higher. This points out the
fact that conf10k is a lot more precise rewrit-
ing phrase table for the translations to rewrite, as
well as the fact that rpt5pef and rpt10pef
are much noisier and consequently difficult to use
efficiently by our automatic rewriting procedure.
4.4 Manual error analysis
In the previous sections, we have shown that our
automatic rewriting procedure can improve trans-
lation quality over both an initial Moses baseline,
and a reranked baseline using the same features
as our procedure. We have further shown in sec-
tion 4.3 that much larger improvements could be
obtained by using an oracle procedure.
We now focus on the four following con-
figurations: 1-pass Moses, reranker,
rewriter and GOS. Although this four configu-
rations are well separated both in terms of BLEU
and TER scores, it is informative to look more
precisely into what makes their results different.
We performed a small-scale manual error analysis
of these four configurations. A French native
speaker annotated 70 translation hypotheses using
an error typology adapted from (Vilar et al.,
2006).
Results of the manual error analysis are re-
ported in Table 4. The most significant results
are for the disamb(iguation) and form error types,
the former being more related to translation accu-
racy, and the later to fluency. In both cases, we
first observe a strong reduction of errors between
1-pass Moses and reranker, which demon-
strates the positive impact of the features used
on these levels. Then, another, similar reduction
is obtained between reranker and rewriter,
demonstrating that our reranking procedure man-
ages to identify more precise and fluent hypothe-
ses. Finally, a further reduction is found between
rewriter and GOS, indicating that our proposed
local, greedy rewriting can still be improved, no-
tably by using more informative features and bet-
ter confidence estimates.
The other types of error categories are less in-
formative. We find no clear differences in er-
ror types attributable to style issues, which seem
to be irrecoverable even for GOS. reranker
and rewriter both improve on order-related er-
rors over 1-pass Moses, but our local rewrit-
ing unsurprisingly did not fix any of these errors.
Finally, reranker and rewriter decreased
slightly the number of extra words from 1-pass
Moses, while GOS sometimes artificially intro-
duces extra words.
4.5 Lower-quality SMT experiments
We now turn to the question of how our rewrit-
ing system fares on a more difficult task, and used
TED Talks, 6 BLEU below Medical for the
English to French direction, for this purpose. In
the same way as we did for Medical, we first
tried to find the best training configuration for the
ranker of the rewriting system. For this task, mix-
ing the n-best neighborhood and 10pefNeigh with
n=10 seemed to be sufficient to have no more im-
provement on the development set by increasing n
for both language directions, so we used this train-
ing configuration. As for the rewriting phrase table
used on the test set, we simply selected conf10k
as in the Medical task. Results are reported
in Table 5 for French to English and English to
French.
We first observe that reranker performed
similarly for the two translation directions, by
improving 1-pass Moses by 0.5 BLEU. The
smaller improvements may be partly attributed to
the better LM used in 1-pass Moses, implying
a better early modeling of grammaticality, but also
by the fact that models such as SOUL and POS
LMs rely on accurate contexts and are therefore
more apt to help in choosing translations among
generally better candidates.
Finally, rewriter obtains smaller but consis-
tent improvements over reranker: +0.4 BLEU
for translation into English, and +0.9 BLEU for
translation into French. The smaller improvement
in the former situation may be attributed to the na-
ture of the target language which has a simpler
agreement system. Consequently, the form-related
errors discussed in Section 4.4 are possibly less
subject to improvement here.
1268
extra missing incorrect unknown
word word disamb form style order word all
1-pass Moses 11 1 57 91 13 31 10 214
reranker 5 3 47 73 11 19 10 168
rewriter 4 4 40 55 12 19 10 144
rewriter oracle 19 2 26 44 14 22 10 137
Table 4: Results for manual error analysis for the first 70 test sentences.
System fr-en en-fr
BLEU TER BLEU TER
1-pass Moses 32.5 47.7 32.3 49.9
reranker 33.0 47.3 32.8 49.4
rewriter 33.4
(+0.4)
47.4
(+0.1)
33.7
(+0.9)
49.3
(?0.1)
semi-oracle 34.1
(+1.1)
46.6
(?0.7)
34.2
(+1.4)
48.6
(?0.8)
Table 5: Results for the baselines, our best configuration and the semi-oracle for the TED Talks.
5 Related work
Reranking of translation hypotheses n-best
list reranking was extensively studied in (Och et
al., 2004), using features not used in the initial
decoder such as IBM1 scores (which also proved
useful for word-level confidence estimation (Blatz
et al., 2004)) and generative syntactic models.
While the experiments in (Och et al., 2004) did
not show any clear contribution of syntactic in-
formation used in this manner, the later work by
Carter and Monz (2011) managed to successfully
exploit syntactic features using discriminative lan-
guage modeling for n-best reranking. Gimpel et
al. (2013) outperformed n-best reranking by gen-
erating, with an expensive but simple method, di-
verse hypotheses used as training data. Recently,
Luong et al. (2014b) reranked n-best lists using
confidence scores at the hypothesis level com-
puted from word-level confidence measures learnt
from roughly 10,000 SMT system outputs anno-
tated by humans.
Rewriting of translation hypotheses Langlais
et al. (2007) described a greedy search decoder,
first introduced in (Germann et al., 2001), able to
improve translations produced by a dynamic pro-
gramming decoder using the same scoring func-
tion and translation table. However, the more re-
cent work by Arun et al. (2010) using a Gibbs
sampler for approximating maximum translation
decoding showed the adequacy of the approxima-
tions made by state-of-the-art decoders for finding
the best translation in their search space. Other
works were more directly targeted at automatic
post-editing of SMT output, and approached the
problem as one of second-pass translation be-
tween automatic predictions and correct transla-
tions (Simard et al., 2007; Dugast et al., 2007).
The recent work of Zhu et al. (2013) attempts to
repair translations by exploiting confidence esti-
mates for examples derived from the similarity
between source words in the input text and in
training examples. Luong et al. (2014a) obtained
improvements by computing word confidence es-
timation, trained on human annotated data, and
large sets of lexical, syntactic and semantic fea-
tures, for the words in the n-best list produced
during a first-pass decoding, and performing a
second-pass decoding exploiting these new scores.
Confidence estimation of Machine Translation
The Word Posterior Probability (WPP) proposed
by Ueffing and Ney (2007), derived from informa-
tion from the n-best list produced by a decoder,
proved to be useful for estimating word-level con-
fidence. Bach et al. (2011) worked on the issue
of predicting sentence-level and word-level MT
errors by using WPP and other features derived
from the source context, the source-target align-
ment, and dependency structures, but relied on a
significantly large manually annotated corpus of
MT errors. De Gispert et al. (2013) calculate k-
1269
gram posterior probabilities from n-best lists or
word lattices, and demonstrated that they were rea-
sonably accurate indications of whether specific k-
grams would be found or not in human reference
translations. Finally, the work of Blackwood et
al. (2010) proposed to segment translation lattices
according to confidence measures over the maxi-
mum likelihood translation hypothesis to focus on
regions with potential translation errors. Hypothe-
sis space constraints based on monolingual cover-
age are then applied to the low confidence regions
to improve translation fluency.
6 Conclusions and perspectives
In this paper, we have described an approach
that improves translations a posteriori by applying
simple local rewritings. We have shown that the
quality of phrase-level confidence estimates has
a direct impact of the amplitude of the improve-
ments that can be obtained, as well as the initial
quality of the rewritten hypotheses. We have used
a very simple definition for confidence estimates
under the form of phrase posteriors estimated from
n-best lists from an initial decoder, which obtained
good empirical performance, in spite of not requir-
ing large human-annotated datasets as in other ap-
proaches (Bach et al., 2011; Luong et al., 2014b).
Our work could be extended in several direc-
tions. First, we could use a larger set of rewrit-
ing operations (Langlais et al., 2007), including
the rewrite (sic) operation introduced in (Marie
and Max, 2013) that paraphrases source phrases
and then translates them.
We could also possibly consider any phrase seg-
mentation compatible with a specific word align-
ment rather than rely on specific phrase segmenta-
tions. This would allow us to attain faster some
rewritings that could otherwise require several
rewriting iterations and may never be attained by
the greedy procedure.
More features could also be used, for instance
to model more fine-grained syntax (Post, 2011)
or document-level lexical coherence (Hardmeier
et al., 2012). However, anticipating that some
features might be very expensive to compute, we
could adapt our procedure to work in several
passes: initial passes would tend to restrict the
search space more and more using an initial set
of features, before a more expensive pass would
concentrate on a limited number of hypotheses.
Figure 1 indeed already showed a much faster or-
acle improvement between 1-pass Moses and
reranker for n-best list of small sizes.
Another avenue for improvement lies in the pos-
sibility to perform the training of our rewriter
by providing it with more reference translations.
As these are typically not readily available, we
could resort to targeted paraphrasing (Madnani
and Dorr, 2013) to rewrite reference translations
into acceptable paraphrases that reuse n-grams
from the best hypotheses of the system so far.
Contrarily to (Madnani and Dorr, 2013), we could
bias the paraphrasing table so that it only con-
tains paraphrases that correspond to target phrases
of high confidence values, which would add new
n-grams likely of being produced by rewriter.
It is furthermore worth noticing that our work
proposes a potential answer to an original ques-
tion: contrarily to typical works on sub-sentencial
MT confidence estimation, which predict whether
a word or phrase is correct or not, our rewriter
system could be used to determine automatically
whether a rewriting system could (if asked to) at-
tempt to improve locally a translation, or whether
a human post-editor should already tackle work-
ing on improving it. As we showed in our manual
error analysis in section 4.4, there are in fact many
instances of errors that could not be recovered by
our approach, be it because of its local rewriting
strategy or of the bilingual resources or models
used, so that some knowledge would have to be
provided as hard constraints by a human transla-
tor, as hinted in (Crego et al., 2010). We could
then finally have our rewriter system work in a
turn-based fashion in collaboration with a human
translator, fixing errors or making improvements
that are being made possible by the last edits from
the translator.
Acknowledgments
The authors would like to thank the anonymous re-
viewers and Guillaume Wisniewski for their use-
ful remarks. Additional thanks go to Hai Son Le
for ?anticipating? the need for a large and effi-
cient cache in his SOUL implementation, Quoc
Khanh Do for his assistance on using SOUL, and
Li Gong and Nicolas P?echeux for providing the
authors with data used in the experiments. The
work of the first author is supported by a CIFRE
grant from French ANRT.
1270
References
Alexandre Allauzen, Nicolas P?echeux, Quoc Khanh
Do, Marco Dinarelli, Thomas Lavergne, Aur?elien
Max, Hai-son Le, and Franc?ois Yvon. 2013.
LIMSI @ WMT13. In Proceedings of WMT, Sofia,
Bulgaria.
Abhishek Arun, Phil Blunsom, Chris Dyer, Adam
Lopez, Barry Haddow, and Philipp Koehn. 2010.
Monte Carlo inference and maximization for phrase-
based translation. In Proceedings of CoNLL, Boul-
der, USA.
Nguyen Bach, Fei Huang, and Yaser Al-Onaizan.
2011. Goodness: A Method for Measuring Ma-
chine Translation Confidence. In Proceedings of
ACL, Portland, USA.
Graeme Blackwood, Adri`a de Gispert, and William
Byrne. 2010. Fluency Constraints for Minimum
Bayes-Risk Decoding of Statistical Machine Trans-
lation Lattices. In Proceedings of COLING, Beijing,
China.
John Blatz, Erin Fitzgerald, George Foster, Simona
Gandrabur, Cyril Goutte, Alex Kulesza, Alberto
Sanchis, and Nicola Ueffing. 2004. Confidence Es-
timation for Machine Translation. In Proceedings of
COLING, Geneva, Switzerland.
Simon Carter and Christof Monz. 2011. Syntactic
Discriminative Language Model Rerankers for Sta-
tistical Machine Translation. Machine Translation,
25(4):317?339.
Stanley F. Chen and Joshua T. Goodman. 1998. An
Empirical Study of Smoothing Techniques for Lan-
guage Modeling. Technical Report TR-10-98, Com-
puter Science Group, Harvard University.
Colin Cherry and George Foster. 2012. Batch Tun-
ing Strategies for Statistical Machine Translation. In
Proceedings of NAACL, Montr?eal, Canada.
Josep M. Crego, Aur?elien Max, and Franc?ois Yvon.
2010. Local lexical adaptation in Machine Trans-
lation through triangulation: SMT helping SMT. In
Proceedings of COLING, Beijing, China.
Adri`a de Gispert, Graeme Blackwood, Gonzalo Igle-
sias, and William Byrne. 2013. N-gram poste-
rior probability confidence measures for statistical
machine translation: an empirical study. Machine
Translation, 27(2):85?114.
Lo??c Dugast, Jean Senellart, and Philipp Koehn. 2007.
Statistical Post-Editing on SYSTRANs Rule-Based
Translation System. In Proceedings of WMT,
Prague, Czech Republic.
Ulrich Germann, Michael Jahr, Kevin Knight, Daniel
Marcu, and Kenji Yamada. 2001. Fast Decoding
and Optimal Decoding for Machine Translation. In
Proceedings of ACL, Toulouse, France.
Kevin Gimpel, Dhruv Batra, Chris Dyer, Gregory
Shakhnarovich, and Virginia Tech. 2013. A Sys-
tematic Exploration of Diversity in Machine Trans-
lation. In Proceedings of EMNLP, Seatlle, USA.
Christian Hardmeier, Joakim Nivre, and Jorg Tiede-
man. 2012. Document-Wide Decoding for Phrase-
Based Statistical Machine Translation. In Proceed-
ings of EMNLP, Jeju Island, Korea.
Sa?sa Hasan and Hermann Ney. 2009. Comparison of
Extended Lexicon Models in Search and Rescoring
for SMT. In Proceedings of NAACL, short papers,
Boulder, USA.
Kevin Knight. 2007. Automatic Language Translation
Generation Help Needs Badly. In MT Summit (in-
vited talk), Copenhagen, Denmark.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical Phrase-Based Translation. In Pro-
ceedings of NAACL, Edmonton, Canada.
Philippe Langlais, Alexandre Patry, and Fabrizio Gotti.
2007. A Greedy Decoder for Phrase-Based Statisti-
cal Machine Translation. In Proceedings of Confer-
ence on Theoretical and Methodological Issues in
Machine Translation (TMI), Skovde, Sweden.
Hai-Son Le, Ilya Oparin, Alexandre Allauzen, Jean-
Luc Gauvain, and Franc?ois Yvon. 2011. Structured
Output Layer Neural Network Language Model. In
Proceedings of ICASSP, Prague, Czech Republic.
Hai-Son Le, Alexandre Allauzen, and Franc?ois Yvon.
2012. Continuous Space Translation Models with
Neural Networks. In Proceedings of NAACL,
Montr?eal, Canada.
Chin Y. Lin and Franz J. Och. 2004. ORANGE: a
method for evaluating automatic evaluation metrics
for machine translation. In Proceedings of COL-
ING, Geneva, Switzerland.
Ngoc-Quang Luong, Laurent Besacier, and Benjamin
Lecouteux. 2014a. An Efficient Two-Pass Decoder
for SMT Using Word Confidence Estimation. In
Proceedings of EAMT, Dubrovnik, Croatia.
Ngoc-Quang Luong, Laurent Besacier, and Benjamin
Lecouteux. 2014b. Word Confidence Estimation
for SMT N -best List Re-ranking. In Proceedings
of the Workshop on Humans and Computer-assisted
Translation (HaCaT), Gothenburg, Sweden.
Nitin Madnani and Bonnie J. Dorr. 2013. Generat-
ing Targeted Paraphrases for Improved Translation.
ACM Transactions on Intelligent Systems and Tech-
nology, special issue on Paraphrasing, 4(3).
Benjamin Marie and Aur?elien Max. 2013. A Study
in Greedy Oracle Improvement of Translation Hy-
potheses. In Proceedings of IWSLT, Heidelberg,
Germany.
1271
Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur,
Anoop Sarkar, Kenji Yamada, Alex Fraser, Shankar
Kumar, Libin Shen, David Smith, Katherine Eng,
Viren Jain, Zhen Jin, and Dragomir Radev. 2004.
A Smorgasbord of Features for Statistical Machine
Translation. In Proceedings of NAACL, Boston,
USA.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
jing Zhu. 2002. BLEU: a Method for Automatic
Evaluation of Machine Translation. In Proceedings
of ACL, Philadelphia, USA.
Kristen Parton, Nizar Habash, Kathleen R. McKeown,
Gonzalo Iglesias, and Adri`a de Gispert. 2012. Can
Automatic Post-editing Make MT more Meaning-
ful? In Proceedings of EAMT, Trento, Italy.
Nicolas P?echeux, Li Gong, Quoc Khanh Do, Ben-
jamin Marie, Yulia Ivanishcheva, Alexander Al-
lauzen, Thomas Lavergne, Jan Niehues, Aur?elien
Max, and Franc?ois Yvon. 2014. LIMSI @ WMT?14
Medical Translation Task. In Proceedings of WMT,
Baltimore, USA.
Matt Post. 2011. Judging Grammaticality with Tree
Substitution Grammar Derivations. In Proceedings
of ACL, short papers, Portland, USA.
Lane Schwartz, Chris Callison-Burch, William
Schuler, and Stephen Wu. 2011. Incremental
Syntactic Language Models for Phrase-based
Translation. In Proceedings of ACL, Portland, USA.
Holger Schwenk, Daniel D?echelotte, and Jean-Luc
Gauvain. 2006. Continuous Space Language Mod-
els for Statistical Machine Translation. In Proceed-
ings of COLING-ACL, Sydney, Australia.
Michel Simard, Cyril Goutte, and Pierre Isabelle.
2007. Statistical Phrase-based Post-editing. In Pro-
ceedings of NAACL, Rochester, USA.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, , and John Makhoul. 2006. A
Study of Translation Edit Rate with Targeted Human
Annotation. In Proceedings of AMTA, Cambridge,
USA.
Lucia Specia, Kashif Shah, Jose G.C. de Souza, and
Trevor Cohn. 2013. QuEst - A Translation Qual-
ity Estimation Framework. In Proceedings of ACL,
System Demonstrations, Sofia, Bulgaria.
Nicola Ueffing and Hermann Ney. 2007. Word-
Level Confidence Estimation for Machine Transla-
tion. Computational Linguistics.
David Vilar, Jia Xu, Luis Fernando D?Haro, and Her-
mann Ney. 2006. Error Analysis of Statistical Ma-
chine Translation Output. In Proceedings of LREC,
Genoa, Italy.
Richard Zens and Hermann Ney. 2006. N -Gram Pos-
terior Probabilities for Statistical Machine Transla-
tion. In Proceedings of WMT, New York, USA.
Ying Zhang, Almut Silja Hildebrand, and Stephan Vo-
gel. 2006. Distributed Language Modeling for N-
best List Re-ranking. In Proceedings of EMNLP,
Sydney, Australia.
Junguo Zhu, Muyun Yang, Sheng Li, and Tiejun Zhao.
2013. Repairing Incorrect Translation with Exam-
ples. In Proceedings of IJCNLP, Nagoya, Japan.
1272
Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 246?253,
Baltimore, Maryland USA, June 26?27, 2014.
c?2014 Association for Computational Linguistics
LIMSI @ WMT?14 Medical Translation Task
Nicolas P
?
echeux
1,2
, Li Gong
1,2
, Quoc Khanh Do
1,2
, Benjamin Marie
2,3
,
Yulia Ivanishcheva
2,4
, Alexandre Allauzen
1,2
, Thomas Lavergne
1,2
,
Jan Niehues
2
, Aur
?
elien Max
1,2
, Franc?ois Yvon
2
Univ. Paris-Sud
1
, LIMSI-CNRS
2
B.P. 133, 91403 Orsay, France
Lingua et Machina
3
, Centre Cochrane franc?ais
4
{firstname.lastname}@limsi.fr
Abstract
This paper describes LIMSI?s submission
to the first medical translation task at
WMT?14. We report results for English-
French on the subtask of sentence trans-
lation from summaries of medical ar-
ticles. Our main submission uses a
combination of NCODE (n-gram-based)
and MOSES (phrase-based) output and
continuous-space language models used in
a post-processing step for each system.
Other characteristics of our submission in-
clude: the use of sampling for building
MOSES? phrase table; the implementation
of the vector space model proposed by
Chen et al. (2013); adaptation of the POS-
tagger used by NCODE to the medical do-
main; and a report of error analysis based
on the typology of Vilar et al. (2006).
1 Introduction
This paper describes LIMSI?s submission to the
first medical translation task at WMT?14. This
task is characterized by high-quality input text
and the availability of large amounts of training
data from the same domain, yielding unusually
high translation performance. This prompted us
to experiment with two systems exploring differ-
ent translation spaces, the n-gram-based NCODE
(?2.1) and an on-the-fly variant of the phrase-
based MOSES (?2.2), and to later combine their
output. Further attempts at improving translation
quality were made by resorting to continuous lan-
guage model rescoring (?2.4), vector space sub-
corpus adaptation (?2.3), and POS-tagging adap-
tation to the medical domain (?3.3). We also per-
formed a small-scale error analysis of the outputs
of some of our systems (?5).
2 System Overview
2.1 NCODE
NCODE implements the bilingual n-gram ap-
proach to SMT (Casacuberta and Vidal, 2004;
Mari?no et al., 2006; Crego and Mari?no, 2006) that
is closely related to the standard phrase-based ap-
proach (Zens et al., 2002). In this framework, the
translation is divided into two steps. To translate
a source sentence f into a target sentence e, the
source sentence is first reordered according to a
set of rewriting rules so as to reproduce the tar-
get word order. This generates a word lattice con-
taining the most promising source permutations,
which is then translated. Since the translation step
is monotonic, the peculiarity of this approach is to
rely on the n-gram assumption to decompose the
joint probability of a sentence pair in a sequence
of bilingual units called tuples.
The best translation is selected by maximizing
a linear combination of feature functions using the
following inference rule:
e
?
= argmax
e,a
K
?
k=1
?
k
f
k
(f , e,a) (1)
where K feature functions (f
k
) are weighted by
a set of coefficients (?
k
) and a denotes the set of
hidden variables corresponding to the reordering
and segmentation of the source sentence. Along
with the n-gram translation models and target n-
gram language models, 13 conventional features
are combined: 4 lexicon models similar to the ones
used in standard phrase-based systems; 6 lexical-
ized reordering models (Tillmann, 2004; Crego et
al., 2011) aimed at predicting the orientation of
the next translation unit; a ?weak? distance-based
distortion model; and finally a word-bonus model
and a tuple-bonus model which compensate for the
system preference for short translations. Features
are estimated during the training phase. Training
source sentences are first reordered so as to match
246
the target word order by unfolding the word align-
ments (Crego and Mari?no, 2006). Tuples are then
extracted in such a way that a unique segmenta-
tion of the bilingual corpus is achieved (Mari?no et
al., 2006) and n-gram translation models are then
estimated over the training corpus composed of tu-
ple sequences made of surface forms or POS tags.
Reordering rules are automatically learned during
the unfolding procedure and are built using part-
of-speech (POS), rather than surface word forms,
to increase their generalization power (Crego and
Mari?no, 2006).
2.2 On-the-fly System (OTF)
We develop an alternative approach implement-
ing an on-the-fly estimation of the parameter of
a standard phrase-based model as in (Le et al.,
2012b), also adding an inverse translation model.
Given an input source file, it is possible to compute
only those statistics which are required to trans-
late the phrases it contains. As in previous works
on on-the-fly model estimation for SMT (Callison-
Burch et al., 2005; Lopez, 2008), we first build
a suffix array for the source corpus. Only a lim-
ited number of translation examples, selected by
deterministic random sampling, are then used by
traversing the suffix array appropriately. A coher-
ent translation probability (Lopez, 2008) (which
also takes into account examples where translation
extraction failed) is then estimated. As we cannot
compute exactly an inverse translation probability
(because sampling is performed independently for
each source phrase), we resort to the following ap-
proximation:
p(
?
f |e?) = min
(
1.0,
p(e?|
?
f)? freq(
?
f)
freq(e?)
)
(2)
where the freq(?) is the number of occurrences of
the given phrase in the whole corpus, and the nu-
merator p(e?|
?
f)?freq(
?
f) represents the predicted
joint count of
?
f and e?. The other models in this
system are the same as in the default configuration
of MOSES.
2.3 Vector Space Model (VSM)
We used the vector space model (VSM) of Chen
et al. (2013) to perform domain adaptation. In
this approach, each phrase pair (
?
f, e?) present in
the phrase table is represented by a C-dimensional
vector of TF-IDF scores, one for each sub-corpus,
where C represents the number of sub-corpora
(see Table 1). Each component w
c
(
?
f, e?) is a stan-
dard TF-IDF weight of each phrase pair for the
c
th
sub-corpus. TF(
?
f, e?) is the raw joint count of
(
?
f, e?) in the sub-corpus; the IDF(
?
f, e?) is the in-
verse document frequency across all sub-corpora.
A similar C-dimensional representation of the
development set is computed as follows: we first
perform word alignment and phrase pairs extrac-
tion. For each extracted phrase pair, we compute
its TF-IDF vector and finally combine all vectors
to obtain the vector for the develompent set:
w
dev
c
=
J
?
j=0
K
?
k=0
count
dev
(
?
f
j
, e?
k
)w
c
(
?
f
j
, e?
k
) (3)
where J and K are the total numbers of source
and target phrases extracted from the development
data, respectively, and count
dev
(
?
f
j
, e?
k
) is the joint
count of phrase pairs (
?
f
j
, e?
k
) found in the devel-
opment set. The similarity score between each
phrase pair?s vector and the development set vec-
tor is added into the phrase table as a VSM fea-
ture. We also replace the joint count with the
marginal count of the source/target phrase to com-
pute an alternative average representation for the
development set, thus adding two VSM additional
features.
2.4 SOUL
Neural networks, working on top of conventional
n-gram back-off language models, have been in-
troduced in (Bengio et al., 2003; Schwenk et al.,
2006) as a potential means to improve discrete
language models. As for our submitted transla-
tion systems to WMT?12 and WMT?13 (Le et al.,
2012b; Allauzen et al., 2013), we take advantage
of the recent proposal of (Le et al., 2011). Using
a specific neural network architecture, the Struc-
tured OUtput Layer (SOUL), it becomes possible
to estimate n-gram models that use large vocab-
ulary, thereby making the training of large neural
network language models feasible both for target
language models and translation models (Le et al.,
2012a). Moreover, the peculiar parameterization
of continuous models allows us to consider longer
dependencies than the one used by conventional
n-gram models (e.g. n = 10 instead of n = 4).
Additionally, continuous models can also be
easily and efficiently adapted as in (Lavergne et
al., 2011). Starting from a previously trained
SOUL model, only a few more training epochs are
247
Corpus Sentences Tokens (en-fr) Description wrd-lm pos-lm
in-domain
COPPA 454 246 10-12M -3 -15
EMEA 324 189 6-7M 26 -1
PATTR-ABSTRACTS 634 616 20-24M 22 21
PATTR-CLAIMS 888 725 32-36M 6 2
PATTR-TITLES 385 829 3-4M 4 -17
UMLS 2 166 612 8-8M term dictionary -7 -22
WIKIPEDIA 8 421 17-18k short titles -5 -13
out-of-domain
NEWSCOMMENTARY 171 277 4-5M 6 16
EUROPARL 1 982 937 54-60M -7 -33
GIGA 9 625 480 260-319M 27 52
all parallel all 17M 397-475M concatenation 33 69
target-lm
medical-data -146M 69 -
wmt13-data -2 536M 49 -
devel/test
DEVEL 500 10-12k khresmoi-summary
LMTEST 3 000 61-69k see Section 3.4
NEWSTEST12 3 003 73-82k from WMT?12
TEST 1 000 21-26k khresmoi-summary
Table 1: Parallel corpora used in this work, along with the number of sentences and the number of English
and French tokens, respectively. Weights (?
k
) from our best NCODE configuration are indicated for each
sub-corpora?s bilingual word language model (wrd-lm) and POS factor language model (pos-lm).
needed on a new corpus in order to adapt the pa-
rameters to the new domain.
3 Data and Systems Preparation
3.1 Corpora
We use all the available (constrained) medical data
extracted using the scripts provided by the orga-
nizers. This resulted in 7 sub-corpora from the
medical domain with distinctive features. As out-
of-domain data, we reuse the data processed for
WMT?13 (Allauzen et al., 2013).
For pre-processing of medical data, we closely
followed (Allauzen et al., 2013) so as to be able to
directly integrate existing translation and language
models, using in-house text processing tools for
tokenization and detokenization steps (D?echelotte
et al., 2008). All systems are built using a
?true case? scheme, but sentences fully capital-
ized (plentiful especially in PATTR-TITLES) are
previously lowercased. Duplicate sentence pairs
are removed, yielding a sentence reduction up to
70% for EMEA. Table 1 summarizes the data used
along with some statistics after the cleaning and
pre-processing steps.
3.2 Language Models
A medical-domain 4-gram language model is built
by concatenating the target side of the paral-
lel data and all the available monolingual data
1
,
with modified Kneser-Ney smoothing (Kneser and
Ney, 1995; Chen and Goodman, 1996), using the
SRILM (Stolcke, 2002) and KENLM (Heafield,
2011) toolkits. Although more similar to term-to-
term dictionaries, UMLS and WIKIPEDIA proved
better to be included in the language model.
The large out-of-domain language model used for
WMT?13 (Allauzen et al., 2013) is additionaly
used (see Table 1).
3.3 Part-of-Speech Tagging
Medical data exhibit many peculiarities, includ-
ing different syntactic constructions and a specific
vocabulary. As standard POS-taggers are known
not to perform very well for this type of texts, we
use a specific model trained on the Penn Treebank
and on medical data from the MedPost project
(Smith et al., 2004). We use Wapiti (Lavergne
et al., 2010), a state-of-the-art CRF implementa-
tion, with a standard feature set. Adaptation is per-
formed as in (Chelba and Acero, 2004) using the
out-of-domain model as a prior when training the
in-domain model on medical data. On a medical
test set, this adaptation leads to a 8 point reduc-
tion of the error rate. A standard model is used for
WMT?13 data. For the French side, due to the lack
of annotaded data for the medical domain, corpora
are tagged using the TreeTagger (Schmid, 1994).
1
Attempting include one language model per sub-corpora
yielded a significant drop in performance.
248
3.4 Proxy Test Set
For this first edition of a Medical Translation Task,
only a very small development set was made avail-
able (DEVEL in Table 1). This made both system
design and tuning challenging. In fact, with such a
small development set, conventional tuning meth-
ods are known to be very unstable and prone to
overfitting, and it would be suboptimal to select
a configuration based on results on the develop-
ment set only.
2
To circumvent this, we artificially
created our own internal test set by randomly se-
lecting 3 000 sentences out from the 30 000 sen-
tences from PATTR-ABSTRACTS having the low-
est perplexity according to 3-gram language mod-
els trained on both sides of the DEVEL set. This
test set, denoted by LMTEST, is however highly
biaised, especially because of the high redundancy
in PATTR-ABSTRACTS, and should be used with
great care when tuning or comparing systems.
3.5 Systems
NCODE We use NCODE with default settings, 3-
gram bilingual translation models on words and 4-
gram bilingual translation factor models on POS,
for each included corpora (see Table 1) and for the
concatenation of them all.
OTF When using our OTF system, all in-
domain and out-of-domain data are concatenated,
respectively. For both corpora, we use a maxi-
mum random sampling size of 1 000 examples and
a maximum phrase length of 15. However, all
sub-corpora but GIGA
3
are used to compute the
vectors for VSM features. Decoding is done with
MOSES
4
(Koehn et al., 2007).
SOUL Given the computational cost of com-
puting n-gram probabilities with neural network
models, we resort to a reranking approach. In
the following experiments, we use 10-gram SOUL
models to rescore 1 000-best lists. SOUL models
provide five new features: a target language model
score and four translation scores (Le et al., 2012a).
We reused the SOUL models trained for our par-
ticipation to WMT?12 (Le et al., 2012b). More-
over, target language models are adapted by run-
ning 6 more epochs on the new medical data.
2
This issue is traditionally solved in Machine Learning by
folded cross-validation, an approach that would be too pro-
hibitive to use here.
3
The GIGA corpus is actually very varied in content.
4
http://www.statmt.org/moses/
System Combination As NCODE and OTF dif-
fer in many aspects and make different errors, we
use system combination techniques to take advan-
tage of their complementarity. This is done by
reranking the concatenation of the 1 000-best lists
of both systems. For each hypothesis within this
list, we use two global features, corresponding
either to the score computed by the correspond-
ing system or 0 otherwise. We then learn rerank-
ing weights using Minimum Error Rate Training
(MERT) (Och, 2003) on the development set for
this combined list, using only these two features
(SysComb-2). In an alternative configuration, we
use the two systems without the SOUL rescoring,
and add instead the five SOUL scores as features in
the system combination reranking (SysComb-7).
Evaluation Metrics All BLEU scores (Pap-
ineni et al., 2002) are computed using cased
multi-bleu with our internal tokenization. Re-
ported results correspond to the average and stan-
dard deviation across 3 optimization runs to bet-
ter account for the optimizer variance (Clark et al.,
2011).
4 Experiments
4.1 Tuning Optimization Method
MERT is usually used to optimize Equation 1.
However, with up to 42 features when using
SOUL, this method is known to become very sen-
sitive to local minima. Table 2 compares MERT,
a batch variant of the Margin Infused Relaxation
Algorithm (MIRA) (Cherry and Foster, 2012) and
PRO (Hopkins and May, 2011) when tuning an
NCODE system. MIRA slightly outperforms PRO
on DEVEL, but seems prone to overfitting. How-
ever this was not possible to detect before the re-
lease of the test set (TEST), and so we use MIRA
in all our experiments.
DEVEL TEST
MERT 47.0? 0.4 44.1? 0.8
MIRA 47.9? 0.0 44.8? 0.1
PRO 47.1? 0.1 45.1? 0.1
Table 2: Impact of the optimization method during
the tuning process on BLEU score, for a baseline
NCODE system.
249
4.2 Importance of the Data Sources
Table 3 shows that using the out-of-domain data
from WMT?13 yields better scores than only using
the provided medical data only. Moreover, com-
bining both data sources drastically boosts perfor-
mance. Table 1 displays the weights (?
k
) given by
NCODE to the different sub-corpora bilingual lan-
guage models. Three corpora seems particulary
useful: EMEA, PATTR-ABSTRACTS and GIGA.
Note that several models are given a negative
weight, but removing them from the model sur-
prisingly results in a drop of performance.
DEVEL TEST
medical 42.2? 0.1 39.6? 0.1
WMT?13 43.0? 0.1 41.0? 0.0
both 48.3? 0.1 45.4? 0.0
Table 3: BLEU scores obtained by NCODE trained
on medical data only, WMT?13 data only, or both.
4.3 Part-of-Speech Tagging
Using the specialized POS-tagging models for
medical data described in Section 3.3 instead of a
standart POS-tagger, a 0.5 BLEU points increase
is observed. Table 4 suggests that a better POS
tagging quality is mainly beneficial to the reorder-
ing mechanism in NCODE, in contrast with the
POS-POS factor models included as features.
Reordering Factor model DEVEL TEST
std std 47.9? 0.0 44.8? 0.1
std spec 47.9? 0.1 45.0? 0.1
spec std 48.4? 0.1 45.3? 0.1
spec spec 48.3? 0.1 45.4? 0.0
Table 4: BLEU results when using a standard POS
tagging (std) or our medical adapted specialized
method (spec), either for the reordering rule mech-
anism (Reordering) or for the POS-POS bilingual
language models features (Factor model).
4.4 Development and Proxy Test Sets
In Table 5, we assess the importance of domain
adaptation via tuning on the development set used
and investigate the benefits of our internal test set.
Best scores are obtained when using the pro-
vided development set in the tuning process. Us-
DEVEL LMTEST NEWSTEST12 TEST
48.3? 0.1 46.8? 0.1 26.2? 0.1 45.4? 0.0
41.8? 0.2 48.9? 0.1 18.5? 0.1 40.1? 0.1
39.8? 0.1 37.4? 0.2 29.0? 0.1 39.0? 0.3
Table 5: Influence of the choice of the develop-
ment set when using our baseline NCODE system.
Each row corresponds to the choice of a develop-
ment set used in the tuning process, indicated by a
surrounded BLEU score.
Table 6: Contrast of our two main systems and
their combination, when adding SOUL language
(LM) and translation (TM) models. Stars indicate
an adapted LM. BLEU results for the best run on
the development set are reported.
DEVEL TEST
NCODE 48.5 45.2
+ SOUL LM 49.4 45.7
+ SOUL LM
?
49.8 45.9
+ SOUL LM + TM 50.1 47.0
+ SOUL LM
?
+ TM 50.1 47.0
OTF 46.6 42.5
+ VSM 46.9 42.8
+ SOUL LM 48.6 44.0
+ SOUL LM
?
48.4 44.2
+ SOUL LM + TM 49.6 44.8
+ SOUL LM
?
+ TM 49.7 44.9
SysComb-2 50.5 46.6
SysComb-7 50.7 46.5
ing NEWSTEST12 as development set unsurpris-
ingly leads to poor results, as no domain adapta-
tion is carried out. However, using LMTEST does
not result in much better TEST score. We also note
a positive correlation between DEVEL and TEST.
From the first three columns, we decided to use the
DEVEL data set as development set for our sub-
mission, which is a posteriori the right choice.
4.5 NCODE vs. OTF
Table 6 contrasts our different approaches. Prelim-
inary experiments suggest that OTF is a compara-
ble but cheaper alternative to a full MOSES sys-
tem.
5
We find a large difference in performance,
5
A control experiment for a full MOSES system (using a
single phrase table) yielded a BLEU score of 45.9 on DEVEL
and 43.2 on TEST, and took 3 more days to complete.
250
extra missing incorrect unknown
word content filler disamb. form style term order word term all
syscomb 4 13 20 47 62 8 18 21 1 11 205
OTF+VSM+SOUL 4 4 31 44 82 6 20 42 3 12 248
Table 7: Results for manual error analysis following (Vilar et al., 2006) for the first 100 test sentences.
NCODE outperforming OTF by 2.8 BLEU points
on the TEST set. VSM does not yield any signifi-
cant improvement, contrarily to the work of Chen
et al. (2013); it may be the case all individual sub-
corpus are equally good (or bad) at approximating
the stylistic preferences of the TEST set.
4.6 Integrating SOUL
Table 6 shows the substantial impact of adding
SOUL models for both baseline systems. With
only the SOUL LM, improvements on the test set
range from 0.5 BLEU points for NCODE system
to 1.2 points for the OTF system. The adaptation
of SOUL LM with the medical data brings an ad-
ditional improvement of about 0.2 BLEU points.
Adding all SOUL translation models yield an
improvement of 1.8 BLEU points for NCODE and
of 2.4 BLEU points with the OTF system using
VSM models. However, the SOUL adaptation step
has then only a modest impact. In future work, we
plan to also adapt the translation models in order
to increase the benefit of using in-domain data.
4.7 System Combination
Table 6 shows that performing the system combi-
nation allows a gain up to 0.6 BLEU points on the
DEVEL set. However this gain does not transfer to
the TEST set, where instead a drop of 0.5 BLEU
is observed. The system combination using SOUL
scores showed the best result over all of our other
systems on the DEVEL set, so we chose this (a
posteriori sub-obtimal) configuration as our main
system submission.
Our system combination strategy chose for DE-
VEL about 50% hypotheses among those produced
by NCODE and 25% hypotheses from OTF, the
remainder been common to both systems. As ex-
pected, the system combination prefers hypothe-
ses coming from the best system. We can observe
nearly the same distribution for TEST.
5 Error Analysis
The high level of scores for automatic metrics
encouraged us to perform a detailed, small-scale
analysis of our system output, using the error types
proposed by Vilar et al. (2006). A single annota-
tor analyzed the output of our main submission, as
well as our OTF variant. Results are in Table 7.
Looking at the most important types of errors,
assuming the translation hypotheses were to be
used for rapid assimilation of the text content, we
find a moderate number of unknown terms and in-
correctly translated terms. The most frequent er-
ror types include missing fillers, incorrect disam-
biguation, form and order, which all have some
significant impact on automatic metrics. Compar-
ing more specifically the two systems used in this
small-scale study, we find that our combination
(which reused more than 70% of hypotheses from
NCODE) mostly improves over the OTF variant on
the choice of correct word form and word order.
We may attribute this in part to a more efficient
reordering strategy that better exploits POS tags.
6 Conclusion
In this paper, we have demonstrated a successful
approach that makes use of two flexible transla-
tion systems, an n-gram system and an on-the-fly
phrase-based model, in a new medical translation
task, through various approaches to perform do-
main adaptation. When combined with continu-
ous language models, which yield additional gains
of up to 2 BLEU points, moderate to high-quality
translations are obtained, as confirmed by a fine-
grained error analysis. The most challenging part
of the task was undoubtedly the lack on an internal
test to guide system development. Another inter-
esting negative result lies in the absence of success
for our configuration of the vector space model
of Chen et al. (2013) for adaptation. Lastly, a more
careful integration of medical terminology, as pro-
vided by the UMLS, proved necessary.
7 Acknowledgements
We would like to thank Guillaume Wisniewski and
the anonymous reviewers for their helpful com-
ments and suggestions.
251
References
Alexandre Allauzen, Nicolas P?echeux, Quoc Khanh
Do, Marco Dinarelli, Thomas Lavergne, Aur?elien
Max, Hai-son Le, and Franc?ois Yvon. 2013. LIMSI
@ WMT13. In Proceedings of the Workshkop on
Statistical Machine Translation, pages 62?69, Sofia,
Bulgaria.
Yoshua Bengio, R?ejean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Re-
search, 3(6):1137?1155.
Chris Callison-Burch, Colin Bannard, and Josh
Schroeder. 2005. Scaling phrase-based statisti-
cal machine translation to larger corpora and longer
phrases. In Proceedings of ACL, Ann Arbor, USA.
Francesco Casacuberta and Enrique Vidal. 2004. Ma-
chine translation with inferred stochastic finite-state
transducers. Computational Linguistics, 30(3):205?
225.
Ciprian Chelba and Alex Acero. 2004. Adaptation
of maximum entropy classifier: Little data can help
a lot. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), Barcelona, Spain.
Stanley F. Chen and Joshua T. Goodman. 1996. An
empirical study of smoothing techniques for lan-
guage modeling. In Proceedings of the 34th Annual
Meeting of the Association for Computational Lin-
guistics (ACL), pages 310?318, Santa Cruz, NM.
Boxing Chen, Roland Kuhn, and George Foster. 2013.
Vector space model for adaptation in statistical ma-
chine translation. In Proceedings of ACL, Sofia,
Bulgaria.
Colin Cherry and George Foster. 2012. Batch tun-
ing strategies for statistical machine translation. In
Proceedings of the 2012 Conference of the North
American Chapter of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 427?436. Association for Computational Lin-
guistics.
Jonathan H Clark, Chris Dyer, Alon Lavie, and Noah A
Smith. 2011. Better Hypothesis Testing for Statisti-
cal Machine Translation : Controlling for Optimizer
Instability. In Better Hypothesis Testing for Statisti-
cal Machine Translation : Controlling for Optimizer
Instability, pages 176?181, Portland, Oregon.
Josep M. Crego and Jos?e B. Mari?no. 2006. Improving
statistical MT by coupling reordering and decoding.
Machine Translation, 20(3):199?215.
Josep M. Crego, Franc?ois Yvon, and Jos?e B. Mari?no.
2011. N-code: an open-source bilingual N-gram
SMT toolkit. Prague Bulletin of Mathematical Lin-
guistics, 96:49?58.
Daniel D?echelotte, Gilles Adda, Alexandre Allauzen,
Olivier Galibert, Jean-Luc Gauvain, H?el`ene May-
nard, and Franc?ois Yvon. 2008. LIMSI?s statisti-
cal translation systems for WMT?08. In Proc. of the
NAACL-HTL Statistical Machine Translation Work-
shop, Columbus, Ohio.
Kenneth Heafield. 2011. KenLM: Faster and Smaller
Language Model Queries. In Proceedings of the
Sixth Workshop on Statistical Machine Translation,
pages 187?197, Edinburgh, Scotland, July. Associa-
tion for Computational Linguistics.
Mark Hopkins and Jonathan May. 2011. Tuning as
ranking. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing,
EMNLP ?11, pages 1352?1362, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Reinhard Kneser and Herman Ney. 1995. Improved
backing-off for m-gram language modeling. In Pro-
ceedings of the International Conference on Acous-
tics, Speech, and Signal Processing, ICASSP?95,
pages 181?184, Detroit, MI.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constantin, and Evan Herbst. 2007. Moses:
Open source toolkit for statistical machine transla-
tion. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics Com-
panion Volume Proceedings of the Demo and Poster
Sessions, pages 177?180, Prague, Czech Republic,
June. Association for Computational Linguistics.
Thomas Lavergne, Olivier Capp?e, and Franc?ois Yvon.
2010. Practical very large scale CRFs. In Proceed-
ings the 48th Annual Meeting of the Association for
Computational Linguistics (ACL), pages 504?513.
Association for Computational Linguistics, July.
Thomas Lavergne, Hai-Son Le, Alexandre Allauzen,
and Franc?ois Yvon. 2011. LIMSI?s experiments
in domain adaptation for IWSLT11. In Mei-Yuh
Hwang and Sebastian St?uker, editors, Proceedings
of the heigth International Workshop on Spoken
Language Translation (IWSLT), San Francisco, CA.
Hai-Son Le, Ilya Oparin, Alexandre Allauzen, Jean-
Luc Gauvain, and Franc?ois Yvon. 2011. Structured
output layer neural network language model. In Pro-
ceedings of ICASSP, pages 5524?5527.
Hai-Son Le, Alexandre Allauzen, and Franc?ois Yvon.
2012a. Continuous space translation models with
neural networks. In Proceedings of the 2012 confer-
ence of the north american chapter of the associa-
tion for computational linguistics: Human language
technologies, pages 39?48, Montr?eal, Canada, June.
Association for Computational Linguistics.
Hai-Son Le, Thomas Lavergne, Alexandre Al-
lauzen, Marianna Apidianaki, Li Gong, Aur?elien
252
Max, Artem Sokolov, Guillaume Wisniewski, and
Franc?ois Yvon. 2012b. LIMSI @ WMT12. In
Proceedings of the Seventh Workshop on Statisti-
cal Machine Translation, pages 330?337, Montr?eal,
Canada.
Adam Lopez. 2008. Tera-Scale Translation Models
via Pattern Matching. In Proceedings of COLING,
Manchester, UK.
Jos?e B. Mari?no, Rafael E. Banchs, Josep M. Crego,
Adri`a de Gispert, Patrick Lambert, Jos?e A.R. Fonol-
losa, and Marta R. Costa-Juss`a. 2006. N-gram-
based machine translation. Computational Linguis-
tics, 32(4):527?549.
Franz Josef Och. 2003. Minimum error rate training in
statistical machine translation. In Proceedings of the
41st Annual Meeting on Association for Computa-
tional Linguistics, pages 160?167, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a Method for Automatic
Evaluation of Machine Translation. In Proceedings
of 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311?318, Philadelphia,
USA, July. Association for Computational Linguis-
tics.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of
International Conference on New Methods in Lan-
guage Processing, September.
Holger Schwenk, Daniel Dchelotte, and Jean-Luc Gau-
vain. 2006. Continuous space language models for
statistical machine translation. In Proceedings of the
COLING/ACL on Main conference poster sessions,
pages 723?730, Morristown, NJ, USA. Association
for Computational Linguistics.
L. Smith, T. Rindflesch, and W. J. Wilbur. 2004. Med-
post: a part of speech tagger for biomedical text.
Bioinformatics, 20(14):2320?2321.
A. Stolcke. 2002. SRILM - an extensible lan-
guage modeling toolkit. In Proceedings of the In-
ternational Conference on Spoken Language Pro-
cessing (ICSLP), pages 901?904, Denver, Colorado,
September.
Christoph Tillmann. 2004. A unigram orientation
model for statistical machine translation. In Pro-
ceedings of HLT-NAACL, pages 101?104.
David Vilar, Jia Xu, Luis Fernando D?Haro, and Her-
mann Ney. 2006. Error Analysis of Statistical Ma-
chine Translation Output. In LREC, Genoa, Italy.
Richard Zens, Franz Joseph Och, and Herman Ney.
2002. Phrase-based statistical machine translation.
In M. Jarke, J. Koehler, and G. Lakemeyer, editors,
KI-2002: Advances in artificial intelligence, volume
2479 of LNAI, pages 18?32. Springer Verlag.
253
